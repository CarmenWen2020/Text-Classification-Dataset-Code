The field of succinct data structures has flourished over the past 16 years. Starting from the compressed suffix
array by Grossi and Vitter (STOC 2000) and the FM-index by Ferragina and Manzini (FOCS 2000), a number of generalizations and applications of string indexes based on the Burrows-Wheeler transform (BWT)
have been developed, all taking an amount of space that is close to the input size in bits. In many largescale applications, the construction of the index and its usage need to be considered as one unit of computation. For example, one can compare two genomes by building a common index for their concatenation and
by detecting common substructures by querying the index. Efficient string indexing and analysis in small
space lies also at the core of a number of primitives in the data-intensive field of high-throughput DNA
sequencing.
We report the following advances in string indexing and analysis: We show that the BWT of a string
T ∈ {1,..., σ }
n can be built in deterministic O(n) time using just O(n log σ ) bits of space, where σ ≤ n. Deterministic linear time is achieved by exploiting a new partial rank data structure that supports queries in
constant time and that might have independent interest. Within the same time and space budget, we can
build an index based on the BWT that allows one to enumerate all the internal nodes of the suffix tree
of T . Many fundamental string analysis problems, such as maximal repeats, maximal unique matches, and
string kernels, can be mapped to such enumeration and can thus be solved in deterministic O(n) time and in
O(n log σ ) bits of space from the input string by tailoring the enumeration algorithm to some problem-specific
computations.
We also show how to build many of the existing indexes based on the BWT, such as the compressed suffix
array, the compressed suffix tree, and the bidirectional BWT index, in randomized O(n) time and in O(n log σ )
bits of space. The previously fastest construction algorithms for BWT, compressed suffix array and compressed suffix tree, which used O(n log σ ) bits of space, took O(n log log σ ) time for the first two structures
and O(n logϵ n) time for the third, where ϵ is any positive constant smaller than one. Alternatively, the BWT
could be previously built in linear time if one was willing to spendO(n log σ log logσ n) bits of space. Contrary
to the state-of-the-art, our bidirectional BWT index supports every operation in constant time per element
in its output.
CCS Concepts: • Theory of computation→Data compression; Pattern matching; Sorting and searching; • Mathematics of computing→Combinatorial algorithms; •Information systems→Data structures; • Applied computing→ Molecular sequence analysis;
1 INTRODUCTION
The suffix tree [74] is a fundamental text indexing data structure that has been used for solving
a large number of string processing problems over the past 40 years [2, 33]. The suffix array [49]
is another widely popular data structure in text indexing, and although not as versatile as the
suffix tree, its space usage is bounded by a smaller constant: Specifically, given a string of length n
over an alphabet of size σ, a suffix tree occupies O(n logn) bits of space, while a suffix array takes
exactly n logn bits.1
The past decade has witnessed the rise of compressed versions of the suffix array [25, 32] and
of the suffix tree [69]. In contrast to their plain versions, they occupy just O(n log σ ) bits of space;
this shaves a Θ(logσ n) factor, thus space becomes just a constant times larger than the original
text, which is encoded in exactly n log σ bits. Any operation that can be implemented on a suffix
tree (and thus any algorithm or data structure that uses the suffix tree) can be implemented on the
compressed suffix tree (henceforth denoted by CST) as well, at the price of a slowdown that ranges
from O(1) to O(logϵ n), depending on the operation. Building a CST, however, suffers from a large
slowdown if we are restricted to use an amount of space that is only a constant factor away from
the space taken by the CST itself. More precisely, a CST can be built in deterministic O(n logϵ n)
time (where ϵ is any constant such that 0 < ϵ < 1) andO(n log σ ) bits of space [39], or alternatively
in deterministic O(n) time and O(n logn) bits by first employing a linear-time deterministic suffix
tree construction algorithm to build the plain suffix tree [23] and then compressing the resulting
representation. It can also be built in deterministic O(n log logn) time and O(n log σ log logn) bits
of space (by combining Reference [39] with Reference [38]).
The compressed version of the suffix array (denoted by CSA in what follows) does not suffer
from the same slowdown in construction as the compressed suffix tree, since it can be built in
deterministicO(n log log σ ) time and O(n log σ ) bits of space [39], or alternatively in deterministic
O(n) time and in O(n log σ log logσ n) bits of space [61].
In this article, we show that the CSA can be built in deterministic O(n) time using O(n log σ )
bits of space, and that the CST can be built in randomized O(n) time using O(n log σ ) bits of space,
where randomization comes from the use of monotone minimal perfect hash functions.2 This seems
in contrast to the plain suffix tree, which can be built in deterministic O(n) time. However, hashing
is also necessary for building a representation of the plain suffix tree that supports the fundamental
child operation in constant time3: Building such a plain representation of the suffix tree takes itself
randomized O(n) time. If one insists on achieving deterministic linear construction time, then the
fastest bound known so far for the child operation is roughly4 O(log log σ ).
1In this article, log n stands for max{1, log2 (n) }. 2Monotone minimal perfect hash functions are defined in Section 3.6. 3The constant-time child operation enables, e.g., matching a pattern of length m against the suffix tree in O (m) time. 4The child operation is predecessor bound, so one could achieve O (log(log σ / logw)) time using more sophisticated predecessor data structures for word size w and universe size σ .
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.
Linear-time String Indexing and Analysis in Small Space 17:3
We also show that the key ingredient of compressed text indexes—namely, the Burrows-Wheeler
transform (BWT) of a string [16]—can be built in deterministic O(n) time and O(n log σ ) bits of
space. Our result is optimal when the input string is encoded as a black box that allows constanttime random access to individual characters. The previously fastest construction algorithms for
the BWT that used O(n log σ ) bits of space took O(n log log σ ) time [39], and previous algorithms
that achieved linear time required O(n log σ log logσ n) bits of space [61]. Otherwise, when the
string is encoded in O(n/ logσ n) machine words of Ω(logn) bits each, the BWT can be built in
O(n/ logσ n + r polylog n) time, where r is the number of runs of identical characters in the BWT
itself [41]; this is optimal when n/r ∈ Ω(polylogn), i.e., when the string is compressible. A related
result is that the BWT of a binary string of n bits, packed inO(n/ logn) machine words, can be built
inO(n/

logn) time andO(n/ logn) words of space [42]. When the word-packed string of length n
is on an alphabet of size σ such that log σ ≤
logn, the space becomesO(n/ logσ n) words, i.e., the
same as ours, and the time becomes O(n log σ/

logn) [42], which is not better than ours when
log σ >
logn. Other works on space-efficient construction include an in-place5 quadratic-time
algorithm that uses additional O(1) words of space [18] and a O(n logn/ log logn)-time algorithm
that uses nHk + o(n log σ ) bits of space [57], where Hk is the kth order empirical entropy of the
input (see, e.g., Reference [50]).
The main contributions of the article derive from the following results, which we believe have
independent technical interest and wide applicability to string processing and biological sequence
analysis problems: The first result is a data structure that takes at most n log σ + O(n) bits of space,
and that supports access and partial rank6 queries in constant time, and a related data structure
that takes n log σ (1 + 1/k) + O(n) bits of space for any positive integer k, and that supports either
access and partial rank queries in constant time and select queries in O(k) time, or select queries
in constant time and access and partial rank queries in O(k) time (Lemma 3.5). Both such data
structures can be built in deterministic O(n) time and o(n) bits of space.
In turn, the latter data structure enables an index that takes n log σ + O(n) bits of space, and that
allows one to enumerate a rich representation of all the internal nodes of a suffix tree in overall
O(n) time and inO(σ2 log2 n) bits of additional space (Lemmas 3.17 and 4.1, Theorems 4.3 and 4.7).
Such index is our second result of independent interest; we call it the unidirectional BWT index.
Our enumeration algorithm is easy to implement, to parallelize, and to apply to multiple strings,
and it performs a depth-first traversal of the suffix-link tree7 using a stack that contains at every
time at most σ logn nodes. A similar enumeration algorithm, which performs, however, a breadthfirst traversal of the suffix-link tree, was described in Reference [14]; such algorithm uses a queue
that takes Θ(n) bits of space, and that contains Θ(n) nodes in the worst case. This number of nodes
might be too large for applications that require storing, e.g., a real number per node, like weighted
string kernels (see, e.g., Reference [9] and references therein).
We plug a multi-string version of our unidirectional index into the recursive BWT construction algorithm of Reference [39], using our index for merging the BWT of a blocked version of
the text, to the BWT of a blocked version of a circular rotation of the text (Theorem 5.3). By
combining the BWT with the data structures in Lemma 3.5, we immediately get a deterministic construction of the CSA (Theorem 6.1). We also describe a data structure that takes at most
n log σ (1 + 1/k) + O(n log log σ ) bits of space for any positive integer k, and that supports access
and partial rank in constant time, select inO(k) time, and rank inO(log log σ + k) time (Lemmas 3.7
5An algorithm is called in-place if it overwrites the input (in this case, the text) with the output (in this case, the BWT). 6Access, rank, partial rank, and select queries are defined in Section 2. 7The suffix-link tree is defined in Section 2.3.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.    
17:4 D. Belazzougui et al.
and 3.9). By combining such a data structure with our BWT construction algorithm, we achieve
the deterministic construction of the succinct suffix array and of the BWT index (Theorem 6.2).
To build the CST, we make use of the bidirectional BWT index, a data structure consisting of two
BWTs that has a number of applications in high-throughput sequencing [46, 47, 72]. Our third
result of independent interest consists in showing that, in randomized O(n) time and in O(n log σ )
bits of space, one can build a bidirectional BWT index that takes O(n log σ ) bits of space and
that supports every operation in constant time per element in the output (Theorem 6.7). This is
in contrast to the O(σ ) or O(log σ ) time per element in the output required by existing bidirectional indexes for some key operations. Randomization comes from monotone minimal perfect
hash functions, which we use for implementing Weiner links and count-smaller operations8 in
constant time (Theorem 6.3). Finally, we employ our constant-time bidirectional BWT index to
build a key component of the CST, i.e. the permuted LCP array (defined in Section 2.5), in randomized O(n) time and O(logn) bits of space (Lemmas 6.10 and 7.5). A similar algorithm allows
building the matching statistics array (defined in Section 7), and both methods are practical.
In addition to describing new algorithms for index construction, we show that many fundamental primitives in string analysis and comparison, with a number of applications to genomics and
high-throughput sequencing, can all be performed by enumerating the internal nodes of a suffix
tree regardless of their order. This allows one to implement all such operations in deterministic
O(n) time on top of our unidirectional index, and thus in deterministic O(n) time and O(n log σ )
bits of space directly from the input string; this is our fourth result of independent interest. Implementing such string analysis procedures on top of our enumeration algorithm is also practical, as
it amounts to few lines of code invoked by a callback function. Using the enumeration procedure,
we also give a practical algorithm for building the BWT of the reverse of a string, given the BWT
of the string itself. Contrary to Reference [59], our algorithm does not need the suffix array and
the original string in addition to the BWT.
The article consists of several other intermediate results, whose logical dependencies are summarized in Figure 1 (we suggest keeping this figure at hand, in particular while reading Section 6).
Concurrently to the writing of this article (and partially building on Reference [5]), the deterministic construction of the BWT, CSA, and CST, inO(n) time andO(n log σ ) bits of space, was achieved
using a deterministic dictionary and batched rank queries, which enable building the permuted
LCP array in deterministic time (see Reference [52] for details). Other deterministic algorithms for
building the LCP array from the BWT have been proposed recently [65].
2 DEFINITIONS AND PRELIMINARIES
We work in the RAM model, and we index arrays starting from one. We denote by i (mod1 n) the
function that returns n if i = 0, that returns i if i ∈ [1..n], and that returns 1 if i = n + 1.
2.1 Temporary Space and Working Space
We call temporary space the size of any region of memory that: (1) is given in input to an algorithm,
initialized to a specific state; (2) is read and written (possibly only in part) by the algorithm during
its execution; (3) is restored to the original state by the algorithm before it terminates. We call
working space the maximum amount of memory that an algorithm uses in addition to its input, its
output, and its temporary space (if any). The temporary space of an algorithm can be bigger than
its working space.
8Count-smaller queries are defined in Section 6.3.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.
Linear-time String Indexing and Analysis in Small Space 17:5 Fig. 1. Map of the data structures (rectangles) and algorithms described in the article. Arcs indicate dependencies, and thick arcs highlight the main derivations of new results. Data structures whose construction algorithm works in randomized time are highlighted in red. Algorithms that use the static allocation strategy are marked with a white circle (see Section 3.1). Algorithms that use the logarithmic stack technique described in the proof of Lemma 4.2 are marked with a black circle. Algorithms that are easy to implement in practice are marked with a triangle. A dashed arc (v,w) means that data structure
v is used to build data structure
w, but some of the components of v can be discarded after the construction of
w.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.
17:6 D. Belazzougui et al.
2.2 Strings
A string T of length n is a sequence of symbols from the compact alphabet Σ = [1..σ], i.e., T ∈ Σn.
We assume σ ∈ o(
√
n/ logn), since for larger alphabets there already exist algorithms for building
the data structures described in this article, in linear time and in O(n log σ ) = O(n logn) bits of
working space (for example, the linear-time suffix array construction algorithms in References [40,
43, 44]). The reason behind our choice of o(
√
n/ logn) will become apparent in Section 4. We also
assume # to be a separator that does not belong to [1..σ], and, specifically, we set # = 0. In some
cases, we use multiple distinct separators, denoted by #i = −i + 1 for integers i > 0.
Given a string T ∈ [1..σ]
n, we denote by T [i..j] (with i and j in [1..n]) a substring of T , with
the convention that T [i..j] equals the empty string if i > j. As customary, we denote by V ·W the
concatenation of two strings V and W . We call T [1..i] (with i ∈ [1..n]) a prefix of T , and T [j..n]
(with j ∈ [1..n]) a suffix of T .
A rotation of T is a string T [i..n] · T [1..i − 1] for i ∈ [1..n]. We denote by R(T ) the set of all
lexicographically distinctrotations ofT . Note that |R(T )| can be smaller thann, since some rotations
of T can be lexicographically identical; this happens if and only if T = W k for some W ∈ [1..σ]
+
and k > 1. We are interested only in strings for which all rotations are lexicographically distinct9;
we often enforce this property by terminating a string with #. We denote by S(T ) the set of all
distinct, not necessarily proper, prefixes of rotations of T . In what follows, we will use rotations
to define a set of notions (like maximal repeats, suffix tree, suffix array, longest common prefix
array) that are typically defined in terms of the suffixes of a string terminated by #. We do so to
highlight the connection between such notions and the Burrows-Wheeler transform, one of the key
tools used in the following sections, which is defined in terms of rotations. Note that there is a
one-to-one correspondence between the ith rotation ofT # in lexicographic order and the ith suffix
of T # in lexicographic order.
A repeat of T is a string W ∈ S(T ) such that there are two rotations T 1 = T [i1..n] · T [1..i1 − 1]
and T 2 = T [i2..n] · T [1..i2 − 1], with i1  i2, such that T 1[1..|W |] = T 2[1..|W |] = W . Repeats are
substrings of T if T ∈ [1..σ]
n−1#. A repeat W is right-maximal if |W | < n and there are two rotationsT 1 = T [i1..n] · T [1..i1 − 1] andT 2 = T [i2..n] · T [1..i2 − 1], with i1  i2, such thatT 1[1..|W |] =
T 2[1..|W |] = W and T 1[|W | + 1]  T 2[|W | + 1]. A repeat W is left-maximal if |W | < n and there
are two rotations T 1 = T [i1..n] · T [1..i1 − 1] and T 2 = T [i2..n] · T [1..i2 − 1], with i1  i2, such that
T 1[2..|W | + 1] = T 2[2..|W | + 1] = W and T 1[1]  T 2[1]. Intuitively, a right-maximal (respectively,
left-maximal) repeat cannot be extended to the right (respectively, to the left) by a single character,
without losing at least one of its occurrences in T . A repeat is maximal if it is both left- and rightmaximal. If T ∈ [1..σ]
n−1#, repeats are substrings of T , and we use the terms left- (respectively,
right-) maximal substring. Given a string W ∈ S(T ), we call μ(W ) the number of (not necessarily
proper) suffixes of W that are maximal repeats of T , and we set μT = max{μ(T 
) : T  ∈ R(T )}. We
say that a repeat W of T is strongly left-maximal if there are at least two distinct characters a and
b in [1..σ] such that both aW and bW are right-maximal repeats of T . This definition will be useful in Section 4. Since only a right-maximal repeat W of T can be strongly left-maximal, the set
of strongly left-maximal repeats of T is a subset of the maximal repeats of T . Let W ∈ S(T ), and
let λ(W ) be the number of (not necessarily proper) suffixes of W that are strongly left-maximal
repeats of T . We set λT = max{λ(T 
) : T  ∈ R(T )}. Note that λT ≤ μT . Other types of repeat will
be described in Section 7.
9Some results in this article can be extended with little effort to strings W k with W ∈ Σ+ and k > 1, using the fact that
the Burrows-Wheeler transform ofW k can be obtained from the Burrows-Wheeler transform ofW by transforming every
character into a run of k identical characters.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.     
Linear-time String Indexing and Analysis in Small Space 17:7
2.3 Suffix Tree
Let T = {T 1,T 2,...,T m } be a set of strings on alphabet [1..σ]. The trie of T is the tree G =
(V, E, ), with set of nodes V , set of edges E, and labeling function , defined as follows: (1) every edge e ∈ E is labeled by exactly one character (e) ∈ [1..σ]; (2) the edges that connect a node
to its children have distinct labels; (3) the children of a node are sorted lexicographically according
to the labels of the corresponding edges; (4) there is a one-to-one correspondence between V and
the set of distinct prefixes of strings in T . Note that, if no string in T is a prefix of another string
in T , there is a one-to-one correspondence between the elements of T and the leaves of the trie
of T .
Given a trie, we call unary path a maximal sequence v1,v2,...,vk such that vi ∈ V and vi has
exactly one child, for all i ∈ [1..k]. By collapsing a unary path, we mean transformingG = (V, E, )
into a tree G = (V \ {v1,...,vk }, (E \ {(v0,v1), (v1,v2),..., (vk ,vk+1)}) ∪ {(v0,vk+1)}, 
), where
v0 is the parent of v1 in G, vk+1 is the only child of vk in G, 
(e) = (e) for all e ∈ E ∩ E
,
and 
((v0,vk+1)) is the concatenation (v0,v1) · (v1,v2) ····· (vk ,vk+1). Note that  labels the
edges of G with strings rather than with single characters. Given a trie, we call compact trie the
labeled tree obtained by collapsing all unary paths in the trie. Every node of a compact trie has
either zero or at least two children.
Definition 2.1 ([74]). Let T ∈ [1..σ]
n be a string such that |R(T )| = n. The suffix tree STT =
(V, E, ) of T is the compact trie of R(T ).
Note that STT is not defined if some rotations ofT are lexicographically identical, and that there
is a one-to-one correspondence between the leaves of the suffix tree of T and the elements of
R(T ). Since the suffix tree of T has precisely n leaves, and since every internal node is branching,
there are at most n − 1 internal nodes. We denote by sp(v), ep(v), and range(v) the left-most leaf,
the right-most leaf, and the set of all leaves in the subtree of an internal node v, respectively. We
denote by (e) the label of an edge e ∈ E, and by (v) the string (r,v1) · (v1,v2) ····· (vk−1,v),
where r ∈ V is the root of the tree, and r,v1,v2,...,vk−1,v is the path of v ∈ V in the tree. We say
that nodev hasstring depth |(v)|. We callw the proper locus of stringW if the search forW starting
from the root of STT ends at an edge (v,w) ∈ E. Note that there is a one-to-one correspondence
between the set of internal nodes of STT and the set of right-maximal repeats of T . Moreover, the
set of all left-maximal repeats of T enjoys the prefix closure property, in the sense that if a repeat
is left-maximal, so is any of its prefixes. It follows that the maximal repeats of T form an induced
subgraph of the suffix tree of T , rooted at r.
Given strings T 1,T 2,...,T m with Ti ∈ [1..σ]
ni for i ∈ [1..m], assume that |R(Ti )| = ni for all
i ∈ [1..m], and that R(Ti ) ∩ R(T j
) = ∅ for all i  j in [1..m]. We call generalized suffix tree the
compact trie of R(T 1) ∪ R(T 2) ∪···∪R(T m ). Note that, if stringW labels an internal node of the
suffix tree of a stringTi
, then it also labels an internal node of the generalized suffix tree. However,
there could be an internal node v in the generalized suffix tree G = (V, E, ) such that (v) does
not label an internal node in anyTi
. This means that: (1) if (v) ∈ S(Ti ), then it is always followed
by the same character ai in every rotation of Ti
; (2) there are at least two strings Ti and T j
, with
i  j, such that ai  aj . A node v in the generalized suffix tree could be such that all leaves in the
subtree rooted at v are rotations of the same string Ti
; we call such a node pure, and we call it
impure otherwise.
Let the label (v) of an internal node v of STT = (V, E, ) be aW , with a ∈ Σ and W ∈ Σ∗. Since
W occurs at all positions where aW occurs, there must be a nodew ∈ V with (w) = W , otherwise
v would not be a node of the suffix tree. We say that there is a suffix link from v to w labelled by a,
and we write suffixLink(v) = w. More generally, we say that the set of labels of internal nodes
of STT enjoys the suffix closure property, in the sense that if a stringW belongs to this set, so does
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.                            
17:8 D. Belazzougui et al.
every one of its suffixes. If T ∈ [1..σ]
n−1#, then we define suffixLink(v) for leaves v of STT as
well; the suffix link from a leaf leads either to another leaf, or to the root of STT . The graph that
consists of the set of internal nodes of STT and of the set of suffix links, is a trie rooted at the
same root node as STT ; we call such trie the suffix-link tree SLTT of T . Note that the suffix-link
tree might contain unary paths, and that traversing the suffix-link tree allows one to enumerate all
nodes of the suffix tree. Note also that extending to the left a repeat that is not right-maximal does
not lead to a right-maximal repeat. We exploit this property in Section 4 to enumerate all nodes of
the suffix tree in small space, storing neither the suffix tree nor the suffix-link tree explicitly.
Inverting the direction of all suffix links yields the so-called explicit Weiner links. Given a node
v ∈ V and a character a ∈ Σ, it might happen that string a(v) ∈ S(T ), but that it does not label
any internal node of STT ; we call all such extensions of internal nodes implicit Weiner links. An
internal node might have multiple outgoing Weiner links (possibly both explicit and implicit), and
all such Weiner links have distinct labels. The constructions described in this article rest on the
fact that the total number of explicit and implicit Weiner links is small:
Observation 1. Let T ∈ [1..σ]
n be a string such that |R(T )| = n. The number of suffix links,
explicit Weiner links, and implicit Weiner links in the suffix tree of T are upper bounded by n − 2,
n − 2, and 3n − 3, respectively.
Proof. Each of the at most n − 2 internal nodes of the suffix tree (other than the root) has a
suffix link. Each explicit Weiner link is the inverse of a suffix link, so their total number is also at
most n − 2.
Consider an internal node v with only one implicit Weiner link e = ((v), a(v)). The number
of such nodes, and thus the number of such implicit Weiner links, is bounded by n − 1. Call these
the implicit Weiner links of type I, and the remaining the implicit Weiner links of type II. Consider
an internal node v with two or more implicit Weiner links, and let Σv be the set of labels of all
Weiner links fromv. Since |Σv | > 1, there is an internal nodew in the suffix tree STT of the reverse
T of T , labeled by the reverse (v) of (v); every c ∈ Σv can be mapped to a distinct edge of STT
connecting w to one of its children. This is an injective mapping from all type II implicit Weiner
links to the at most 2n − 2 edges of the suffix tree of T . The sum of type I and type II Weiner links,
i.e., the number of all implicit Weiner links, is hence bounded by 3n − 3.
Slightly more involved arguments push the upper bound on the number of implicit Weiner links
down to n. Note that every leaf of the suffix-link tree has more than one Weiner link. Thus, the set
of all maximal repeats of T coincides with the set of all the internal nodes of the suffix-link tree
with at least two (implicit or explicit) Weiner links, and with the leaves of the suffix-link tree.
2.4 Rank and Select
Given a string S ∈ [1..σ]
n, we denote by rankc (S,i) the number of occurrences of character c ∈
[1..σ] in S[1..i], and we denote by selectc (S, j) the position i of the jth occurrence of c in S,
i.e., j = rankc (S, selectc (S, j)). We use partialRank(S,i) as a shorthand for rankS[i](S,i). Data
structures for supporting such operations efficiently will be described in Section 3.4. Here, we
just recall that it is possible to represent a bitvector of length n using n + o(n) bits of space, such
that rank and select queries can be supported in constant time (see, e.g., References [17, 51]).
This representation can be built in O(n) time and in o(n) bits of working space. Rank and select
data structures can be used to implement a representation of a string S that supports operation
access(S,i) = S[i] without storing S itself.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.      
Linear-time String Indexing and Analysis in Small Space 17:9
2.5 String Indexes
Sorting the set of rotations of a string yields an index that can be used for supporting pattern
matching by binary search:
Definition 2.2 ([49]). Let T ∈ [1..σ]
n be a string such that |R(T )| = n. The suffix array SAT [1..n]
of T is the permutation of [1..n] such that SAT [i] = j iff rotation T [j..n] · T [1..j − 1] has rank i in
the list of all rotations of T taken in lexicographic order.
Note that SAT is not defined if some rotations of T are lexicographically identical. We denote
by range(W ) = [sp(W )..ep(W )] the maximal interval of SAT whose rotations are prefixed by W .
Note that range(W ), sp(W ), and ep(W ) are in one-to-one correspondence with range(v), sp(v),
and ep(v) of a node v of the suffix tree of T such that (v) = W . We will often use such notions
interchangeably.
The longest common prefix array stores the length of the longest common prefix between every
two consecutive rotations in the suffix array:
Definition 2.3 ([49]). Let T ∈ [1..σ]
n be a string such that |R(T )| = n, and let p(i, j) be the function that returns the longest common prefix between the rotation that starts at position SAT [i] in
T and the rotation that starts at position SAT [j] inT . The longest common prefix array ofT , denoted
by LCPT [1..n], is defined as follows: LCPT [1] = 0 and LCPT [i] = p(i,i − 1) for all i ∈ [2..n]. The
permuted longest common prefix array of T , denoted by PLCPT [1..n], is the permutation of LCPT
in string order, i.e., PLCPT [SAT [i]] = LCPT [i] for all i ∈ [1..n].
The main tool that we use in this article for obtaining space-efficient index structures is a permutation of T induced by its suffix array:
Definition 2.4 ([16]). Let T ∈ [1..σ]
n be a string such that |R(T )| = n. The Burrows-Wheeler
transform of T , denoted by BWTT , is the permutation L[1..n] of T such that L[i] = T [SAT [i] −
1 (mod1 n)] for all i ∈ [1..n].
Like SAT , BWTT cannot be uniquely defined if some rotations of T are lexicographically identical. Given two strings S and T such that R(S) ∩ R(T ) = ∅, we say that the BWT of R(S) ∪ R(T )
is the string obtained by sorting R(S) ∪ R(T ) lexicographically, and by printing the character that
precedes the starting position of each rotation. Note that either R(S) ∩ R(T ) = ∅, or R(S) = R(T ).
A key feature of the BWT is that it isreversible: Given BWTT = L, one can reconstruct the unique
T of which L is the Burrows-Wheeler transform. Indeed, letV andW be two rotations ofT such that
V is lexicographically smaller than W , and assume that both V and W are preceded by character
a in T . It follows that rotation aV is lexicographically smaller than rotation aW , thus there is a
bijection between rotations preceded by a and rotations that start with a that preserves the relative
order among such rotations. Consider, thus, the rotation that starts at position i in T , and assume
that it corresponds to position pi in SAT (i.e SAT [pi] = i). If L[pi] = a is the kth occurrence of a
in L, then the rotation that starts at position i − 1 in T must be the kth rotation that starts with a
in SAT , and its position pi−1 in SAT must belong to the compact interval range(a) that contains
all rotations that start with a. For historical reasons, the function that projects the position pi in
SAT of a rotation that starts at position i, to the position pi−1 in SAT of the rotation that starts at
position i − 1 (mod1 n), is called LF (or last-to-first) mapping [24, 25], and it is defined as LF(i) = j,
where SA[j] = SA[i] − 1 (mod1 n). Note that reconstructing T from its BWT requires to know the
starting position in T of its lexicographically smallest rotation.
Let again L be the Burrows-Wheeler transform of a string T ∈ [1..σ]
n, and assume that we have
an arrayC[1..σ] that stores inC[c] the number of occurrences inT of all characters strictly smaller
than c, that is the sum of the frequencies of all characters in [1..c − 1]. Note that C[1] = 0, and that
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
17:10 D. Belazzougui et al.
C[c] + 1 is the position in SAT of the first rotation that starts with character c. It follows that
LF(i) = C[L[i]] + rankL[i](L,i).
Function LF can be extended to a backward search algorithm that counts the number of occurrences inT of a stringW , inO(|W |) steps, considering iteratively suffixesW [i..|W |] with i that goes
from |W | to one [24, 25]. Given the interval [i1..j1] that corresponds to a stringV and a characterc,
the interval [i2..j2] that corresponds to string cV can be computed asi2 = rankc (i1 − 1) + C[c] + 1
and j2 = rankc (j1) + C[c]. If i2 > j2, then cV  S(T ). Note that, if W is a right-maximal repeat of
T , a step of backward search corresponds to taking an explicit or implicit Weiner link in STT . The
time for computing a backward step is dominated by the time needed to perform a rank query,
which is typically O(log log σ ) [29] or O(log σ ) [30].
The inverse of function LF is called ψ for historical reasons [32, 67], and it is defined as follows:
Assume that position i in SAT corresponds to rotation aW with a ∈ [1..σ]; since a satisfies C[a] <
i ≤ C[a + 1], it can be computed from i by performing select0 (C
,i) − i + 1 on a bitvector C that
represents C with σ − 1 ones and n zeros, and that is built as follows: We append C[i + 1] − C[i]
zeros followed by a one for all i ∈ [1..σ − 1], and we append n − C[σ] zeros at the end. Function
ψ (i) returns the lexicographic rank of rotation W , given the lexicographic rank i of rotation aW ,
as follows: ψ (i) = selecta (BWTT ,i − C[a]).
Combining the BWT and the C array gives rise to the following index, which is known as FMindex in the literature [24, 25]:
Definition 2.5. Given a string T ∈ [1..σ]
n, a BWT index on T is a data structure that consists of:
• BWTT #, with support for rank (and select) queries;
• the integer array C[0..σ], which stores in C[c] the number of occurrences in T # of all characters strictly smaller than c.
The following lemma derives immediately from function LF:
Lemma 2.6. Given the BWT index of a string T ∈ [1..σ]
n−1#, there is an algorithm that outputs
the sequence SA−1
T [n], SA−1
T [n − 1],..., SA−1
T [1], in O(t) time per value in the output, in O(nt) total
time, and in O(logn) bits of working space, where t is the time for performing function LF.
So far, we have only described how to support counting queries, and we are still not able to
locate the starting positions of a pattern P in string T . One way of doing this is to sample suffix
array values, and to extract the missing values using the LF mapping. Adjusting the sampling
rate r gives different space/time tradeoffs. Specifically, we sample all the values of SAT #[i] that
satisfy SAT #[i] = 1 + rk for 0 ≤ k < n/r, and we store such samples consecutively, in the same
order as in SAT #, in array samples[1..n/r]. Note that this is equivalent to sampling every r
position in string order. We also mark in a bitvector B[1..n] the positions of the suffix array that
have been sampled; that is, we set B[i] = 1 if SAT #[i] = 1 + rk, and we set B[i] = 0 otherwise.
Combined with the LF mapping, this allows one to compute SAT #[i] in O(rt) time, where t is
the time required for function LF. One can set r = log1+ϵ n/ log σ for any given ϵ > 0 to have the
samples fit in (n/r) logn = n log σ/ logϵ n = o(n log σ ) bits, which is asymptotically the same as the
space required for supporting counting queries. This setting implies that the extraction of SAT #[i]
takes O(log1+ϵ nt/ log σ ) time. The resulting collection of data structures is called succinct suffix
array (see, e.g., Reference [56]).
Succinct suffix arrays can be further extended into self-indexes. A self-index is a succinct representation of a string T that, in addition to supporting count and locate queries on arbitrary strings
provided in input, allows one to access any substring of T by specifying its starting and ending
position; in other words, a self-index for T completely replaces the original string T , which can be
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
Linear-time String Indexing and Analysis in Small Space 17:11
discarded. Recall that we can reconstruct the whole string T # from BWTT # by applying function
LF iteratively. To reconstruct arbitrary substrings efficiently, it suffices to store, for every sampled position 1 + ri in string T #, the position of suffix T [1 + ri..n] in SAT #: Specifically, we use an
additional array pos2rank[1..n/r] such that pos2rank[i] = j if SAT #[j] = 1 + ri [25]. Note that
pos2rank can be seen itself as a sampling of the inverse suffix array at positions 1 + ri, and that
it takes the same amount of space as array samples. Given an interval [e..f ] in string T #, we can
use pos2rank[k] to go to the position i of suffix T [1 + rk..n] in SAT #, where k = (f − 1)/r and
1 + rk is the smallest sampled position greater than or equal to f in T #. We can then apply LF
mapping rk − e times starting from i, retrieving the characters from the BWT, and possibly using
one select operation on array C to derive T [1 + rk]; the result is the whole substring T [e..1 + rk]
printed from right to left, thus, we can return its prefixT [e..f ]. The running time of this procedure
is O((f − e + r)t).
Making a succinct suffix array a self-index does not increase its asymptotic space complexity.
We can thus define the succinct suffix array as follows:
Definition 2.7. Given a string T ∈ [1..σ]
n, the succinct suffix array of T is a data structure that
takes n log σ (1 + o(1)) + O((n/r) logn) bits of space, where r is the sampling rate, and that supports
the following queries:
• count(P): returns the number of occurrences of string P ∈ [1..σ]
m in T .
• locate(i): returns SAT #[i].
• substring(e, f ): returns T [e..f ].
The following result is an immediate consequence of Lemma 2.6:
Lemma 2.8. The succinct suffix array of a string T ∈ [1..σ]
n can be built from the BWT index of T
in O(nt) time and in O(logn) bits of working space, where t is the time for performing function LF.
In Section 6, we will define additional string indexes used in this article, like the compressed
suffix array, the compressed suffix tree, and the bidirectional BWT index.
3 BUILDING BLOCKS AND TECHNIQUES
In this section, we summarize several operations and corresponding data structures and algorithms, which will be used repeatedly throughout the article. In addition to known results, we
describe new algorithms for batched locate queries (Lemma 3.1), for access, rank, partial rank, and
select queries (Lemmas 3.5, 3.7, 3.9), and for building the balanced parentheses representation of a
suffix tree (Lemma 3.12).
3.1 Static Memory Allocation
Let A be an algorithm that builds a set of arrays by iteratively appending new elements to their
end. In all cases described in this article, the final size of all growing arrays built by A can be
precomputed by running a slightly modified version A of A that has the same time and space
complexity as A. Thus, we always restructure A as follows: First, we run A to precompute
the final size of all growing arrays built by A; then, we allocate a single, contiguous region of
memory that is large enough to contain all the arrays built by A, and we compute the starting
position of each array inside the region; finally, we run A using such positions. This strategy avoids
memory fragmentation in practice, and in some cases, for example in Section 3.5, it even allows us
to achieve better space bounds. See Figure 1 for a list of all algorithms in the article that use this
technique.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.
17:12 D. Belazzougui et al.
3.2 Batched Locate Queries
In several string analysis applications—for example, when computing maximal repeats and maximal matches in Section 7.2—one needs to resolve a batch of queries locate(i) = SAT #[i] issued on
a set of distinct values of i in [1..n], where T ∈ [1..σ]
n−1. The following lemma describes how to
answer such queries using just the BWT of T and a data structure that supports function LF:
Lemma 3.1. Let T ∈ [1..σ]
n−1 be a string. Given the BWT of T #, a data structure that supports
function LF, and a list pairs[1..occ] of pairs (ik ,pk ), where ik ∈ [1..n] is a position in SAT # and pk
is an integer for all k ∈ [1..occ], we can transform every pair (ik ,pk ) in pairs into the corresponding
pair (SAT #[ik ],pk ), possibly altering the order of list pairs, inO(nt + occ) time and inO(occ · logn)
bits of working space, where t is the time taken to perform function LF.
Proof. Assume that we could use a bitvector marked[1..n] such that marked[ik ] = 1 for all the
distinct ik that appear in pairs. Building marked from pairs takes O(n + occ) time. Then, we
invert BWTT # in O(nt) time. During this process, whenever we are at a position i in BWTT #, we
also know the corresponding position SAT #[i] in T #: if marked[i] = 1, we append pair (i, SAT #[i])
to a temporary array translate[1..occ]. At the end of this process, the pairs in translate are
in reverse string order; thus, we sort both translate and pairs in suffix array order. Finally,
we perform a linear, simultaneous scan of the two sorted arrays, replacing (ik ,pk ) in pairs with
(SAT #[ik ],pk ) using the corresponding pair (ik , SAT #[ik ]) in translate.
If occ ≥ n/ logn, then marked fits in O(occ · logn) bits. Otherwise, rather than storing
marked[1..n], we use a smaller bitvector marked
[1..n/h] in which we set marked
[i] = 1 iff there
is an ik ∈ [hi..h(i + 1) − 1]. As we invert BWTT #, we check whether the block i/h that contains
the current position i in the BWT, is such that marked
[i/h] = 1. If this is the case, then we binary search i in pairs. Every such binary search takes O(log occ) time, and we perform at most
h · occ binary searches in total. Setting h = n/(occ · logn) makes marked fit in occ · logn bits,
and it makes the total time spent in binary searches O(n/(logn/ log occ)) ∈ O(n).
Now if occ ≥ √
n, we sort array pairs[1..occ] using radix sort: Specifically, we interpret each
pair (ik ,pk ) as a triple (msb(ik ), lsb(ik ),pk ), where msb(x) is a function that returns the most
significant (logn)/2 bits of x and lsb(x) is a function that returns the least significant (logn)/2
bits of x. Since the resulting primary and secondary keys belong to the range [1..2
√
n], sorting
both pairs and translate takes O(
√
n + occ) time and O((√
n + occ) logn) ∈ O(occ logn) bits
of working space. If occ < √
n, we just sort array pairs[1..occ] using standard comparison sort,
then in time O(occ logn) ∈ O(
√
n logn).
3.3 Data Structures for Prefix-sum Queries
A prefix-sum data structure supports the following query on an array of numbers A[1..n]: given
i ∈ [1..n], return i
j=1 A[j]. The following well-known result, which we will use extensively in this
section and for building string indexes in Section 6, derives from combining Elias-Fano coding [20,
22] with bitvectors indexed to support the select operation in constant time:
Lemma 3.2 ([60]). Given a representation of an array of integers A[1..n] whose total sum isU , that
allows one to access its entries from left to right, we can build in O(n) time and in O(logU ) bits of
working space a data structure that takes n(2 + log(U/n)) + o(n) bits of space and that answers
prefix-sum queries in constant time.
3.4 Data Structures for Access, Rank, and Select Queries
In Section 6, we will describe algorithms for building the compressed suffix array, the BWT index,
and the succinct suffix array, in deterministic linear time and in O(n log σ ) bits of working space.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
Linear-time String Indexing and Analysis in Small Space 17:13
Such algorithms rely on the new partial rank and rank data structures of Lemmas 3.5 and 3.9,
which we now derive in detail.
We conceptually split a string S of length n into N = n/σ blocks of size σ each, except possibly
for the last block, which might be smaller. Specifically, block numberi ∈ [1..N − 1], denoted by Si
,
covers substring S[σ (i − 1) + 1..σi], and the last block SN covers substring S[σ (N − 1) + 1..n]. The
purpose of splitting S into blocks consists in translating global operations on S into local operations
on a block; for example, access(i) can be implemented by issuing access(i − σ (b − 1)) on block
b = i/σ. The construction we describe in this section largely overlaps with Reference [29].
We use f (c) to denote the frequency of character c in S, f (c,b) to denote the frequency of
character c in Sb , and Cb [c] as a shorthand for c−1 a=1 f (a,b). We encode the block structure of
S using bitvector freq = freq1freq2 ··· freqσ , where bitvector freqc [1..f (c) + N] is defined as
follows:
freqc = 10f (c,1)
10f (c,2)
1 ... 10f (c,N )
.
Note that freq takes at most 2n + σ − 1 bits of space: Indeed, every freqc contains exactly N ones,
thus the total number of ones in all bitvectors is σ n/σ ≤ n + σ − 1, and the total number of zeros
in all bitvectors is
c ∈[1..σ ] f (c) = n. Note also that a rank or select operation on a specific freqc
can be translated in constant time into a rank or select operation on freq. Bitvector freq can
be computed efficiently:
Lemma 3.3. Given a string S ∈ [1..σ]
n, vector freq can be built in O(n) time and in o(n) bits of
working space.
Proof. We use the static allocation strategy described in Section 3.1: Specifically, we first compute f (c) for all c ∈ [1..σ] by scanning S and incrementing corresponding counters. Then, we
compute the size of each bitvector freqc and we allocate a contiguous region of memory for freq.
Storing all f (c) counters takes σ logn ≤ (
√
n/ logn) logn = o(n) bits of space. Finally, we scan S
once again: Whenever we see the beginning of a new block, we append a one to the end of every
freqc , and whenever we see an occurrence of character c, we append a zero to the end of freqc .
The total time taken by this process is O(n), and the pointers to the current end of each freqc in
freq take o(n) bits of space overall.
Vector freqc , indexed to support rank or select operations in constant time, is all we need to
translate in constant time a rank or select operation on S into a corresponding operation on a
block of S; thus, we focus just on supporting rank and select operations inside a block of S in
what follows.
For this purpose, let Xb be the string 1f (1,b)
2f (2,b) ··· σf (σ,b)
. Sb can be seen as a permutation of
Xb : let πb : [1..σ] → [1..σ] be the function that maps a position in Sb onto a position in Xb , and
let π−1
b : [1..σ] → [1..σ] be the function that maps a position in Xb to a position in Sb . A possible
choice for such permutation functions is:
πb (i) = Cb [Sb [i]] + rankSb (Sb [i],i), (1)
π−1
b (i) = selectSb (i − Cb [c],c), (2)
where c = Xb [i] is the only character that satisfies Cb [c] < i ≤ Cb [c + 1]. We store explicitly just
one of πb and π−1
b , so that random access to any element of the stored permutation takes constant
time, and we represent the other permutation implicitly, as described in the following lemma:
Lemma 3.4 ([53]). Given a permutation π[1..n] of sequence 1, 2,...,n, there is a data structure
that takes (n/k) logn + n + o(n) bits of space in addition to π itself, and that supports query π−1[i]
for any i ∈ [1..n] in O(k) time, for any integer k ≥ 1. This data structure can be built in O(n) time
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.   
17:14 D. Belazzougui et al.
and in o(n) bits of working space. The query and the construction algorithms assume constant time
access to any element π[i].
Proof. A permutation π[1..n] of sequence 1, 2,...,n can be seen as a collection ofcycles, where
the number of such cycles ranges between one and n. Indeed, consider the following iterated version of the permutation operator:
πt
[i] =

i if t = 0,
π[πt−1[i]] if t > 0.
We say that a position i in π belongs to a cycle of length t, where t is the smallest positive integer
such that πt[i] = i. Note that π can be decomposed into cycles in linear time and using n bits
of working space, by iterating operator π from position one, by marking in a bitvector all the
positions that have been touched by such iteration, and by repeating the process from the next
position in π that has not been marked.
If a cycle contains a number of arcs t greater than a predefined threshold k, then it can be
subdivided into t/k paths containing at most k arcs each. We store in a dictionary the first vertex
of each path, and we associate with it a pointer to the first vertex of the path that precedes it. That
is, given a cycle x, π[x], π2[x],..., πt−1[x], x, the dictionary stores the set of (key, value) pairs:

(x, πk (t /k −1)
[x])

∪

(πik [x], π (i−1)k [x]) : i ∈ [1..(t/k − 1)]

.
Then, we can determine π−1[i] for any value i in O(k) time, by successively computing i, π[i],
π2[i], ... , πk [i] and by querying the dictionary for every vertex in such sequence. As soon as
the query is successful for some π j
[i] with j ∈ [0..k], we get π j−k [i] from the dictionary and we
compute the sequence π j−k [i], π j−k+1[i], π j−k+2[i],..., π−1[i],i, returning π−1[i]. The dictionary
can be implemented using a table and a bitvector of size n with rank support, which marks the
first element of each path of length k of each cycle.
By combining Lemma 3.3 and Lemma 3.4 with Equations (1) and (2), we obtain the key result of
this section:
Lemma 3.5. Given a string of length n over alphabet [1..σ], we can build the following data structures in O(n) time and in o(n) bits of working space:
• a data structure that takes at most n log σ + 4n + o(n) bits of space, and that supports access
and partialRank in constant time;
• a data structure that takes n log σ (1 + 1/k) + 5n + o(n) bits of space for any positive integer
k, and that supports either access and partialRank in constant time and select in O(k)
time, or select in constant time and access and partialRank in O(k) time.
Neither of these data structures requires the original string to support access, partialRank and
select.
Proof. In addition to the data structures built in Lemma 3.3, we store πb explicitly for every
Sb , spending overall n log σ bits of space. Note that πb can be computed from Sb in linear time for
all b ∈ [1..N]. We also store Cb for every Sb as a bitvector of 2σ bits that coincides with a unary
encoding of Xb (that is, we store 10f (1,b)
10f (2,b) ··· 10f (σ,b)
): Given a position i in block Sb , we
can determine the character c that satisfies Cb [c] < πb [i] ≤ Cb [c + 1] using a select and a rank
query on such bitvector, thus implementing access to Sb [i] in constant time. In turn, this allows
one to implement partialRankSb (i) in constant time using Equation (1).
Aselect query onCb , combined with the implicit representation of π−1
b described in Lemma 3.4,
allows one to implement select on Sb in O(k) time, at the cost of (σ/k) log σ + σ + o(σ ) bits of
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.     
Linear-time String Indexing and Analysis in Small Space 17:15
additional space per block. The complexity of selectSb can be exchanged with that of accessSb
and partialRankSb , by storing explicitly π−1
b rather than πb .
Note that the individual lower-order terms o(σ ) needed to support rank and select queries on
the bitvectors that encodeCb , and in the structures implemented by Lemma 3.4, do not necessarily
add up to o(n). Thus, for each of the two cases, we concatenate all the individual bitvectors, we
index them for rank and/or select queries, and we simulate operations on each individual bitvector
using operations on the bitvectors that result from the concatenation.
For the BWT index and the succinct suffix array of Section 6, we will need an implementation
of rank rather than of partialRank: To support this operation efficiently, we will use predecessor
queries. Given a set of sorted integers, a predecessor query returns the index of the largest integer in the set that is smaller than or equal to a given integer provided in input. It is known that
predecessor queries can be implemented efficiently, for example with the following data structure:
Lemma 3.6 ([34, 75]). Given a sorted sequence of n integers x1 < x2 < ··· < xn, where xi is encoded
in logU bits for all i ∈ [1..n], we can build in O(n) time and in O(n logU ) bits of working space a
data structure that takesO(n logU ) bits of space, and that answers predecessor queries inO(log logU )
time. This data structure does not require the original sequence of integers to answer queries.
The original predecessor data structure described in Reference [75] (called y-fast trie) has an expected linear time construction algorithm. Construction time is randomized, since the data structure uses a hash table. To obtain deterministic linear construction time, one can replace the hash
table with a deterministic dictionary [34].
Implementing rank queries amounts to plugging Lemma 3.6 into the block partitioning scheme
of Lemma 3.5:
Lemma 3.7. Given a string of length n over alphabet [1..σ] and an integer c > 1, we can build a
data structure that takes n log σ (1 + 1/k) + 6n + O(n/ logc−1 σ ) + o(n) bits of space for any positive
integer k, and that supports:
• either access and partialRank in constant time, select in O(k) time, and rank in
O(kc log log σ ) time;
• or access and partialRank inO(k) time, select in constant time, and rank inO(c log log σ )
time.
This data structure can be built in O(n) time and in o(n) bits of working space, and, once constructed,
it no longer requires the original string to support access, rank, and select.
Proof. As described in Lemma 3.5, we divide the string T into blocks of size σ and we build
bitvectors freqa for every a ∈ [1..σ]. We support ranka (i) as follows: Let b be the block that
contains position i, where blocks are indexed from zero. First, we determine the number of zeros
in freqa that precede the bth one, by computing selectfreqa (b, 1) − b. Then, if character a occurs
at most logc σ times inside block b, we binary-search the list of zeros in block b of freqa, using at
each step a select query to convert the position of a zero inside block b of freqa into an occurrence
of character a in string T . This process takes O(τc log log σ ) time, where τ is the time to perform
a select query on T .
If character a occurs more than logc σ times inside block b of freqa, then we use a sampling
strategy similar to the one described in Reference [75]. Specifically, we sample the relative positions
at which a occurs inside block b, every logc σ occurrences of a zero in freqa, and we encode such
positions in the data structure described in Lemma 3.6. Let us call the sampled positions of a block
red positions, and all other positions blue positions. Since positions are relative to a block, the size
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
17:16 D. Belazzougui et al.
of the universe is σ, thus the data structure of every block takes O(m log σ ) bits of space and it
answers queries in time O(log log σ ), where m is the number of red positions of the block. We use
the data structure of Lemma 3.6 to find the index j of the red position of a that immediately precedes
position i inside block b; this takes O(log log σ ) time. Since we sampled red positions every logc σ
occurrences of a in block b, we know that there are exactly (j + 1) logc σ − 1 zeros inside block b
before the jth red position. Finally, we find the blue position that immediately precedes position
i inside block b by binary-searching the set of logc σ − 1 blue positions between two consecutive
red positions, as described above, in time O(τc log log σ ).
With this strategy, we build O(n/ logc σ ) data structures of Lemma 3.6, containing in total
O(n/ logc σ ) elements, thus the total space taken by all such data structures is O(n/ logc−1 σ ) bits.
Note also that all such data structures can be built using just O(σ/ logc−1 σ ) bits of working space.
For every character a ∈ [1..σ], we store all data structures consecutively in memory, and we encode their starting positions in the prefix-sum data structure described in Lemma 3.2. All such
prefix-sum data structures take overall O(n log log σ/ logc σ ) bits of space, and they can be built
in O(logn) bits of working space. We use also a bitvector whicha of size n/σ to mark the blocks
of freqa for which we built a data structure of Lemma 3.6. To locate the starting position of the
data structure of a given block and character a, we use a rank query on whicha and we query the
prefix-sum data structure in constant time. The bitvectors for all characters take overall n + o(n)
bits of space.
In the space complexity of Lemma 3.7, we can achieve 5n rather than 6n by replacing the
plain bitvectors whicha with the compressed bitvector representation described in Reference [64],
which supports constant-time rank queries using (c log log σ/ logc σ )n + O(n/polylog(n)) bits.
Lemma 3.7 can be further improved by replacing binary searches with queries to the following
data structure:
Lemma 3.8 ([31]). GivenU ∈ 2O (w) and a constant ϵ < 1, we can precompute a lookup table of size
O(Uϵ ) bits, so that given any sorted sequence of integers x1 < x2 < ··· < xn, where n ∈ polylog(U )
and xi ∈ [1..U ] for all i ∈ [1..n], we can build inO(n) time and inO(n logU ) bits of working space, a
data structure that takes O(n log logU ) bits of space, and that (in combination with the precomputed
lookup table) answers predecessor queries in O(t/ϵ ) time, where t is the time to access an (arbitrary)
element of the sorted sequence of integers. The lookup table can be built in polynomial time on its size.
Lemma 3.9. Given a string of length n over alphabet [1..σ], we can build a data structure that takes
n log σ (1 + 1/k) + O(n log log σ ) bits of space for any positive integer k, and that supports:
• either access and partialRank in constant time, select in O(k) time, and rank in
O(log log σ + k) time;
• or access and partialRank in O(k) time, select in constant time, and rank in O(log log σ )
time.
This data structure can be built in O(n) time and in o(n) bits of working space, and, once constructed,
it no longer requires the original string to support access, rank, and select.
Proof. We proceed as in Lemma 3.7, but we build the data structure of Lemma 3.8 on every
sequence of consecutive logc σ − 1 blue occurrences of a inside the same block b. Every such data
structure uses O(logc σ · log log σ ) bits of space, and a O(τ /ϵ )-time predecessor query to such a
data structure replaces the binary search over the blue positions performed in Lemma 3.8, where
τ is the time to perform a select query on T . The total time for building all the data structures
of Lemma 3.8, for all blocks and for all characters, is O(n). All such data structures take overall
O(n log log σ ) bits of space, and they all share the same lookup table of size o(σ ) bits, which can
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
Linear-time String Indexing and Analysis in Small Space 17:17
be built in o(σ ) time by choosing ϵ small enough. We also build, in O(n) time, the prefix-sum data
structure of Lemma 3.2, which allows constant-time access to each data structure of Lemma 3.8.
3.5 Representing the Topology of Suffix Trees
It is well known that the topology of an ordered treeT with n nodes can be represented using 2n +
o(n) bits, as a sequence of 2n balanced parentheses built by opening a parenthesis, by recurring on
every child of the current node in order, and by closing a parenthesis [54]. For building the string
indexes of Section 6, we will need support for several tree operations on such a representation;
thus, we will use the following data structure:
Lemma 3.10 ([58, 71]). Let T be an ordered tree with n nodes, and let id(v) be the rank of a node v
in the preorder traversal ofT . Given the balanced parentheses representation ofT encoded in 2n + o(n)
bits, we can build a data structure that takes 2n + o(n) bits, and that supports the following operations
in constant time:
• child(id(v),i): returns id(w), wherew is the ith child of node v (i ≥ 1), or ∅ if v has less than
i children;
• parent(id(v)): returns id(u), where u is the parent of v, or ∅ if v is the root of T ;
• lca(id(v), id(w)): returns id(u), where u is the lowest common ancestor of nodes v and w;
• leftmostLeaf(id(v)), rightmostLeaf(id(v)): returns one plus the number of leaves that,
in the preorder traversal ofT , are visited before the first (respectively, the last) leaf that belongs
to the subtree of T rooted at v;
• selectLeaf(i): returns id(v), where v is the ith leaf visited in the preorder traversal of T ;
• depth(id(v)), height(id(v)): returns the distance of v from the root or from its deepest descendant, respectively;
• ancestor(id(v),d): returns id(u), where u is the ancestor of v at depth d.
This data structure can be built in O(n) time and in O(n) bits of working space.
Note that the operations supported by Lemma 3.10 are enough to implement a preorder traversal
of T in small space, as described by the following folklore lemma:
Lemma 3.11. Let T be an ordered tree with n nodes, and let id(v) be the rank of a node v in
the preorder traversal of T . Assume that we have a representation of T that supports the following
operations:
• firstChild(id(v)): returns the identifier of the first child of node v in the order of T ;
• nextSibling(id(v)): returns the identifier of the child of the parent of node v that follows v
in the order of T ;
• parent(id(v)): returns the identifier of the parent of node v.
Then, a preorder traversal of T can be implemented using O(logn) bits of working space.
Proof. During a preorder traversal of T , we visit every leaf exactly once and every internal
node exactly twice. Specifically, we visit a node v from its parent, from its previous sibling, or
from its last child in the order of T . If we visit v from its parent or from its previous sibling, then
in the next step of the traversal, we will visit the first child of v from its parent—or, if v has no
child, then we will visit the next sibling of v from its previous sibling if v is not the last child of its
parent; otherwise, we will visit the parent of v from its last child. If we visit v from its last child,
then in the next step of the traversal, we will visit the next sibling ofv from its previous sibling—or,
if v has no next sibling, then we will visit the parent of v from its last child. Thus, at each step of
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
17:18 D. Belazzougui et al.
the traversal, we need to store just id(v) and a single bit that encodes the direction in which we
visited v.
In Section 6, we will repeatedly traverse trees in preorder. Not surprisingly, the trees we will
be interested in are suffix trees or contractions of suffix trees, induced by selecting a subset of the
nodes of a suffix tree and by connecting such nodes using their ancestry relationship (the parent
of a node in the contracted tree is the nearest selected ancestor in the original tree). We will thus
need the following space-efficient algorithm for building the balanced parentheses representation
of a suffix tree:
Lemma 3.12. Let S ∈ [1..σ]
n−1 be a string. Assume that we are given an algorithm that enumerates
all the intervals of SAS# that correspond to an internal node of STS#, in t time per interval. Then, we
can build the balanced parentheses representation of the topology of STS# in O(nt) time and in O(n)
bits of working space.
Proof. We assume without loss of generality that logn is a power of two. We associate two
counters to every position i ∈ [1..n], one containing the number of open parentheses and the other
containing the number of closed parentheses at i. We implement such counters with two arrays
Co [1..n] and Cc [1..n]. Given the interval [i..j] of an internal node of STS#, we just increment Co [i]
and Cc [j]. Once all such intervals have been enumerated, we scan Co and Cc synchronously, and
for each i ∈ [1..n], we write Co [i] + 1 open parentheses followed by Cc [i] + 1 closed parentheses.
The total number of parentheses in the output is at most 2(2n − 1).
A naive implementation of this algorithm would use O(n logn) bits of working space: We
achieve O(n) bits using the static allocation strategy described in Section 3.1. Specifically, we partitionCo [1..n] into n/b blocks containing b > 1 positions each (except possibly for the last block,
which might be smaller), and we assign to each block a counter of c bits. Then, we enumerate the
intervals of all internal nodes of the suffix tree, incrementing counter i/b every time we want
to increment position i in Co . If a counter reaches its maximum value 2c − 1, then we stop incrementing it and we call saturated the corresponding block. The space used by all such counters is
n/b ·c bits, which is O(n) if c is a constant multiple of b. At the end of this process, we allocate
a memory area of size b logn bits to each saturated block, so that every position i in a saturated
block has logn bits available to store Co [i]. Note that there can be at most (n − 1)/(2c − 1) saturated blocks, so the total memory allocated to saturated blocks is at most nb logn/(2c − 1) bits;
this quantity is o(n) if 2c grows faster than b logn.
To every non-saturated block, we assign a memory area in which we will store the counters for
all the b positions inside the block. Specifically, we will use Elias gamma coding to store a counter
value x ≥ 0 in exactly 1 + 2log2 (x + 1) ≤ 3 + 2 log(x + 1) bits [21], and we will concatenate the
encodings of all the counters in the same block. The space taken by the memory area of a nonsaturated block j whose counter has value t < 2c − 1 is at most:

jb
i=(j−1)b+1

3 + 2 log(Co [i] + 1)

≤ 3b + 2b log 


jb
i=(j−1)b+1 (Co [i] + 1)
b


	
(3)
= 3b + 2b log 
t + b
b
	
(4)
≤ 5b + 2t, (5)
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
Linear-time String Indexing and Analysis in Small Space 17:19
where Equation (3) derives from applying Jensen’s inequality to the logarithm, and Equation (5)
comes from the fact that log x ≤ x for all x ≥ 1. Since n/b 
i=1 t ≤ n − 1, it follows that the total
number of bits allocated to non-saturated blocks is at most 7n for any choice of b (tighter bounds
might be possible, but for clarity, we do not consider them here). We concatenate the memory areas
of all blocks, and we store a prefix-sum data structure that takes o(n) bits and that returns in constant time the starting position of the memory area allocated to any given block (see Lemma 3.2).
We also store a bitvector isSaturated[1..n/b], which marks every saturated block with a one
and index it for rank queries.
Once memory allocation is complete, we enumerate again the intervals of all internal nodes of
the suffix tree, and for every such interval [i..j], we increment Co [i], as follows: First, we compute the block that contains position i, we use isSaturated to determine whether the block is
saturated or not, and we use the prefix-sum data structure to retrieve in constant time the starting position of the region of memory assigned to the block. If the block is saturated, then we
increment the counter that corresponds to position i directly. Otherwise, we access a precomputed table Ts [1..2s , 1..b] such that Ts [i, j] stores, for every possible configuration i of s bits interpreted as the concatenation of the Elias gamma coding of b counter values x1, x2,..., xb , the
configuration of s bits that represents the concatenation of the Elias gamma coding of counter
values x1, x2,..., xj−1, xj + 1, xj+1,..., xb . The total number of bits used by all such tables is at
most y
s=1 2s
bs = 2b + b(y − 1)2y+1, wherey = 3b + 2b log((t + b)/b) with t = 2c − 2 is from Equation (4). Thus, we need to choose b and c so that by2y ∈ o(n): setting b = log logn and c = db for
any constant d ≥ 1 makes y ∈ O((log logn)
2), and thus by2y ∈ o(n). The same choice of b and c
guarantees that a cell of Ts can be read in constant time for any s, and that the space for the counters in the memory allocation phase of the algorithm isO(n). Finally, setting d ≥ 2 guarantees that
2c grows faster than b logn, thus putting the total memory allocated to saturated blocks in o(n).
Tables Ts for all s ∈ [1..y] can be precomputed in time linear in their size.
Array Cc of closed parentheses can be handled in the same way as array Co .
In Section 4, we will combine this algorithm to a specific way of enumerating the suffix array
intervals of the nodes of a suffix tree (see Theorem 4.4). During index construction in Section 6,
we will also need to follow the suffix link that starts from any node of a suffix tree. Specifically, let
operation suffixLink(id(v)) return the identifier of the destination w of a suffix link from node
v of STS#. The topology of STS# can be augmented to support operation suffixLink, using just
BWTS#:
Lemma 3.13 ([69]). Let S ∈ [1..σ]
n−1 be a string. Assume that we are given the representation of
the topology of STS# described in Lemma 3.10, the BWT of S# indexed to support select operations
in time t, and the C array of S. Then, we can implement function suffixLink(id(v)) for any node v
of STS# (possibly a leaf) in O(t) time.
Proof. Let w be the destination of the suffix link from v, let [i..j] be the interval of node v in
BWTS#, and let (v) = aW and (w) = W , where a ∈ [0..σ] and W ∈ [0..σ]
∗. We convert id(v) to
[i..j] using operations leftmostLeaf and rightmostLeaf of the topology. Let aW X and aW Y be
the suffixes of S# that correspond to positions i and j in BWTS#, respectively, where X and Y are
strings on alphabet [0..σ]. Note that the position i ofW X in BWTS# is selecta (BWTS#,i − C[a]),
the position j
 of W Y in BWTS# is selecta (BWTS#, j − C[a]), and W is the longest prefix of the
suffixes that correspond to positions i and j
 in BWTS#, which also labels a node of STS#. We use
operation selectLeaf provided by the topology of STS# to convert i and j
 to identifiers of leaves
in STS#, and we compute id(w) using operation lca on such leaves.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.      
17:20 D. Belazzougui et al.
Note that, if aW is neither a suffix nor a right-maximal substring of S#, i.e., if aW is always
followed by the same character b ∈ [1..σ], the algorithm in Lemma 3.13 maps the locus of aW to
the locus of WbX in STS#, where X ∈ [1..σ]
∗ and aW bX is the (unique) shortest right-extension
of aW that is right-maximal. The locus of WbX might not be the same as the locus of W . As we
will see in Section 6, this is the reason why the bidirectional BWT index of Definition 6.5 does
not support operation contractLeft (respectively, contractRight) for strings that are neither
suffixes nor right-maximal substrings of S# (respectively, of S#). However, one can support such
cases using simple properties of maximal repeats [10].
3.6 Data Structures for Monotone Minimal Perfect Hash Functions
Given a set S ⊆ [1..U ] of size n, a monotone minimal perfect hash function (denoted by MMPHF
in what follows) is a function f : [1..U ] → [1..n] such that x < y implies f (x) < f (y) for every
x,y ∈ S. In other words, if the set of elements of S is x1 < x2 < ··· < xn, then f (xi ) = i, i.e., the
function returns the rank inside S of the element it takes as an argument. The function is allowed
to return an arbitrary value for any x ∈ [1..U ] \ S.
In our implementations of the bidirectional BWT index and of the compressed suffix tree in
Section 6, and for supporting Weiner links, we will need efficient implementations of MMPHFs;
thus, we will take advantage of the following lemma:
Lemma 3.14 ([6]). Let S ⊆ [1..U ] be a set represented in sorted order by the sequence x1 < x2 <
··· < xn, where xi is encoded in logU bits for all i ∈ [1..n] and logU < n. There is an implementation
of an MMPHF on S that takes O(n log logU ) bits of space, which evaluates f (x) in constant time for
any x ∈ [1..U ], and which can be built in randomized O(n) time and in O(n logU ) bits of working
space.
Proof. We use a technique known as most-significant-bit bucketing [7]. Specifically, we partition the sequence that represents S into n/b blocks of consecutive elements, where each
block Bi = x (i−1)b+1,..., xib contains exactly b = logn elements (except possibly for the last block
x (i−1)b+1,..., xn, which might be smaller). Then, we compute the length of the longest common
prefix pi of the elements in every Bi
, starting from the most significant bit. To do so, it suffices
to compute the longest prefix that is common to the first and to the last element of Bi
; this can
be done in constant time using the mostSignificantBit operation, which can be implemented
using a constant number of multiplications [15]. The length of the longest common prefix of a
block is at most logU − log logn ∈ O(logU ).
Then, we build an implementation of a minimal perfect hash function F that maps every element in S onto a number in [1..n]. This can be done in O(n logU ) bits of working space and in
randomized O(n) time (see Reference [35]). We also use a table lcp[1..n] that stores at index F (xi )
the length of the longest common prefix of the block to which xi belongs, and a table pos[1..n]
that stores at index F (xi ) the relative position of xi inside its block. Formally:
lcp[F (xi
)] = |p (i−1)/b +1 |,
pos[F (xi
)] = i − b · (i − 1)/b.
The implementation of F takes O(n + log logU ) bits of space, lcp takes O(n log logU ) bits, and
pos takes O(n log logn) bits.
It is folklore that all pi values are distinct, thus each pi identifies block i uniquely. We build
an implementation of a minimal perfect hash function G on set p1,p2,...,p n/b , and an inversion table lcp2block[1..n/b] that stores value i at index G(pi ). The implementation of G takes
O(n/ logn + log logU ) bits of space, and it can be built in O((n/ logn) logU ) bits of working space
and in randomized O(n/ logn) time. Table lcp2block takes O((n/ logn) · log(n/ logn)) = O(n)
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.
Linear-time String Indexing and Analysis in Small Space 17:21
bits. With this setup of data structures, we can return in constant time the rank i in S of any xi
,
by issuing:
i = b · lcp2block[G(xi
[1..lcp[F (xi
)]]] + pos[F (xi
)],
where xi
[д..h] denotes the substring of the binary representation of xi in logU bits that starts at
position д and ends at position h.
We will mostly use Lemma 3.14 inside the following construction, which is based on partitioning
the universe rather than the set of numbers:
Lemma 3.15. Let S ⊆ [1..U ] be a set represented in sorted order by the sequence x1 < x2 < ··· <
xn, where xi is encoded in logU bits for all i ∈ [1..n]. There is an implementation of an MMPHF on S
that takes O(n log logb) + U/b(2 + log2 (nb/U )) + o(U/b) bits of space, which evaluates f (x) in
constant time for any x ∈ [1..U ], and which can be built in randomized O(n) time and in O(b logb)
bits of working space, for any choice of b.
Proof. We will make use of a partitioning technique known as quotienting [63]. We partition
interval [1..U ] into n ≤ n blocks of size b each, except for the last block, which might be smaller.
Note that the most significant logU − logb bits are identical in all elements of S that belong to
the same block. For each block i that contains more than one element of S, we build an implementation of a monotone minimal perfect hash function f i on the elements inside the block, as
described in Lemma 3.14, restricted to their least significant logb bits; all such implementations
take O(n log logb) bits of space in total, and constructing each of them takes O(b logb) bits of
working space. Then, we use Lemma 3.2 to build a prefix-sum data structure that encodes in
U/b(2 + log2 (nb/U )) + o(U/b) bits of space the number of elements in every block. Given
an element x ∈ [1..U ], we first find the block it belongs to, by computing i = x/b, then we use
the prefix-sum data structure to compute the number r of elements in S that belong to blocks
smaller than i, and finally, we return r + f i (x[logU − logb + 1.. logU ]), where x[д..h] denotes
the substring of the binary representation of x in logU bits that starts at position д and ends at
position h.
The construction used in Lemma 3.15 is a slight generalization of one initially described
in Reference [6]. Setting b = U/n in Lemma 3.15 makes the MMPHF implementation fit in
O(n log log(U/n)) bits of space.
3.7 Data Structures for Range-minimum and Range-distinct Queries
Given an array of integers A[1..n], let function rmq(i, j) return an index k ∈ [i..j] such that A[k] =
min{A[x] : x ∈ [i..j]}, with ties broken arbitrarily. We call this function a range minimum query
(RMQ) over A. It is known that range-minimum queries can be answered by a data structure that
is small and efficient to compute:
Lemma 3.16 ([27]). Assume that we have a representation of an array of integers A[1..n] that
supports accessing the value A[i] stored at any position i ∈ [1..n] in time t. Then, we can build a data
structure that takes 2n + o(n) bits of space, and that answers rmq(i, j) for any pair of integers i < j in
[1..n] in constant time, without accessing the representation of A. This data structure can be built
in O(nt) time and in n + o(n) bits of working space.
Assume now that the elements of array A[1..n] belong to alphabet [1..σ], and let Σi,j be the set
of distinct characters that occur inside subarray A[i..j]. Let function rangeDistinct(i, j) return
the set of tuples {(c, rankA(c,pc ), rankA(c,qc )) : c ∈ Σi,j} in any order, where pc and qc are the first
and the last occurrence of c in A[i..j], respectively. The frequency of any c ∈ Σi,j inside A[i..j] is
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
17:22 D. Belazzougui et al.
rankA (c,qc ) − rankA(c,pc ) + 1. Support for rangeDistinct queries will be a key building block
of the suffix tree enumerators in Section 4.
It is well known that rangeDistinct queries can be implemented using rmq queries on a specific
array, as described in the following lemma:
Lemma 3.17 ([12, 55, 70]). Given a stringA ∈ [1..σ]
n, we can build a data structure of size n log σ +
8n + o(n) bits that answers rangeDistinct(i, j) for any pair of integersi < j in [1..n] inO(occ) time
and in σ (2 logn + 1) bits of temporary space, where occ = |Σi,j |. This data structure can be built in
O(kn) time and in (n/k) log σ + 2n + o(n) bits of working space, for any positive integer k, and, once
constructed, it no longer requires accessing A to answer rangeDistinct queries.
Proof. To return just the distinct characters in Σi,j it suffices to build a data structure that supports RMQs on an auxiliary array P[1..n], where P[i] stores the position of the previous occurrence
of character A[i] in A. Since rmq(i, j) is the leftmost occurrence of character A[rmq(i, j)] in A[i..j],
it is well known that Σi,j can be built by issuingO(occ) rmq queries on P andO(occ) accesses to A,
using a stack of occ · 2 logn bits and a bitvector of size σ. This is achieved by setting k = rmq(i, j),
by recurring on subintervals [i..k − 1] and [k + 1..j], and by using the bitvector to mark the distinct characters observed during the recursion and to stop the process if A[k] is already marked
[55]. Random access to array P can be simulated in constant time using partialRank and select
operations on A, which can be implemented as described in Lemma 3.5 setting k to a constant.
We use the data structures of Lemma 3.5 also to simulate access to A without storing A itself. We
build the RMQ data structure using Lemma 3.16. After construction, we will never need to answer
select queries on A; thus, we do not output the (n/k) log σ + n + o(n) bits that encode the inverse
permutation in Lemma 3.5.
To report partial ranks in addition to characters, we adapt this construction as follows: We build
a data structure that supports RMQs on an auxiliary array N[1..n], where N[i] stores the position
of the next occurrence of character A[i] in A. Given an interval [i..j], we first use the RMQ data
structure on P and a vector chars[1..σ] of σ log(n + 1) bits to store the first occurrence pc of
every c ∈ Σi,j . Then, we use the RMQ data structure on N to detect the last occurrence qc of every
c ∈ Σi,j , and we access chars[c] both to retrieve the corresponding pc and to clean up cell chars[c]
for the next query. Finally, we compute rankA (c,pc ) and rankA (c,qc ) using the partialRank data
structure of Lemma 3.5. To build the data structure that supports RMQs on N, we can use the same
memory area of n + o(n) bits used to build the data structure that supports RMQs on P.
The temporary space used to answer a rangeDistinct query can be reduced to σ bits by more
involved arguments [12]. Note that, rather than using partialRank, select, and access, one
can implement the rangeDistinct operation using MMPHFs; the following lemma details this
approach, whose main technique will be used again for supporting Weiner links in Section 6.
Lemma 3.18 ([12]). We can augment a string A ∈ [1..σ]
n with a data structure of size
O(n log log σ ) bits that answers rangeDistinct(i, j) for any pair of integersi < j in [1..n] inO(occ)
time and in σ (2 logn + 1) bits of temporary space, where occ = |Σi,j |. This data structure can be built
in O(n) randomized time and in O(n log σ ) bits of working space.
Proof. We build the set of sequences {Pc : c ∈ [1..σ]}, such that Pc contains all the positions
p1,p2,...,pk of characterc in A in increasing order. We encode Pc as a bitvector such that position
pi fori > 1 is represented by the Elias gamma coding of pi − pi−1. The total space taken by all such
sequences is O(n log σ ) bits, by applying Jensen’s inequality twice. Let |Pc | be the bit-size of Pc :
We compute |Pc | and we allocate a corresponding region of memory using the static allocation
strategy described in Section 3.1. We also mark in an additional bitvector startc [1..|Pc |] the first
bit of every representation of a pi in Pc , and we index startc to support select queries.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
Linear-time String Indexing and Analysis in Small Space 17:23
Then, we build an implementation of an MMPHF for every Pc , using Lemma 3.15 with U =
n and b = σk for some positive integer k. Specifically, for every c, we perform a single scan of
sequence Pc , decoding all the positions that fall inside the same block of A of size σk , and building
an implementation of an MMPHF for the positions inside the block. Once all such MMPHFs have
been built, we discard all Pc sequences. The total space used by all MMPHF implementations is at
most O(n(log log σ + log k)) + (nk/σk ) log σ + 2n/σk + o(n/σk ) bits; any k ≥ 1 makes such space
fit in O(n log log σ ) bits, and it makes the working space of the construction fit in O(σk log σ ) bits.
Since we assumed σ ∈ o(
√
n/ logn), setting k ∈ {1, 2} makes this additional space fit in O(n log σ )
bits.
Finally, we proceed as in Lemma 3.17. Given a position i, we can compute rankA (A[i],i) by
querying the MMPHF data structure of character A[i], and we can simulate random access to P[i]
by querying the MMPHF data structure of characterA[i] and by accessing pi − P[i] using a select
operation on startA[i].
Lemma 3.17 builds an internal representation of A, and the original representation of A provided
in the input can be discarded. However, Lemma 3.18 uses the input representation of A to answer
queries, thus it can be combined with any representation ofA that allows constant-time access—for
example with those that represent A up to its kth order empirical entropy for k ∈ o(logσ n) [26].
4 ENUMERATING ALL RIGHT-MAXIMAL SUBSTRINGS
The following problem lies at the core of our construction algorithms in Section 6 and, as we will
see in Section 7, it also captures the requirements of a number of fundamental string analysis
applications:
Problem 1. Given a string T ∈ [1..σ]
n−1#, return the following information for all right-maximal
substrings W of T :
• |W | and range(W ) in SAT ;
• the sorted sequence b1 < b2 <...< bk of all the distinct characters in [0..σ] such that W bi is
a substring of T ;
• the sequence of intervals range(W b1),..., range(W bk );
• a sequence a1, a2,..., ah that lists all the h distinct characters in [0..σ] such that aiW is a
prefix of a rotation of T—the sequence a1, a2,..., ah is not necessarily in lexicographic order;
• the sequence of intervals range(a1W ),..., range(ahW ).
Problem 1 does not specify the order in which the right-maximal substrings ofT (or equivalently,
the internal nodes of STT ) must be enumerated, nor the order in which the left-extensions aiW of
a right-maximal substring W must be returned. It does, however, specify the order in which the
right-extensions W bi of W must be returned.
The first step for solving Problem 1 consists in devising a suitable representation for a rightmaximal substring W of T . Let γ (a,W ) be the number of distinct strings W b such that aW b is a
prefix of a rotation ofT , where a ∈ [0..σ] and b ∈ {b1,...,bk }. Note that there are preciselyγ (a,W )
distinct characters to the right of aW when it is a prefix of a rotation of T ; thus, if γ (a,W ) = 0,
then aW is not a prefix of any rotation of T ; if γ (a,W ) = 1 (for example, when a = #), then aW is
not a right-maximal substring of T ; and if γ (a,W ) ≥ 2, then aW is a right-maximal substring of T .
This suggests to represent a substring W of T with the following pair:
repr(W ) = (chars[1..k], first[1..k + 1]),
where chars[i] = bi and range(W bi ) = [first[i]..first[i + 1] − 1] for i ∈ [1..k]. Note that
range(W ) = [first[1]..first[k + 1] − 1], since it coincides with the concatenation of the
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
17:24 D. Belazzougui et al.
intervals of the right-extensions ofW in lexicographic order. IfW is not right-maximal, then array
chars and first in repr(W ) have length one and two, respectively.
Given repr(W ), repr(aiW ) can be precomputed for all i ∈ [1..h], as follows:
Lemma 4.1. Assume the notation of Problem 1. Given a data structure that supports rangeDistinct queries on BWTT , given the C array of T , and given repr(W ) =
(chars[1..k], first[1..k + 1]) for a substring W of T , we can compute the sequence a1,..., ah and
the corresponding sequence repr(a1W ),..., repr(ahW ), in O(t · occ) time and in O(σ2 logn) bits
of temporary space, where t is the time taken by the rangeDistinct operation per element in its
output, and occ is the number of distinct strings aiW bj that are the prefix of a rotation of T , where
i ∈ [1..h] and j ∈ [1..k].
Proof. Let leftExtensions[1..σ + 1] be a vector of characters given in input to the algorithm
and initialized to all zeros, and let h be the number of nonempty cells in this vector. We will
store in vector leftExtensions all characters a1, a2,..., ah, not necessarily in lexicographic order.
Consider also matrices A[0..σ, 1..σ + 1], F [0..σ, 1..σ + 1], and L[0..σ, 1..σ + 1], given in input to
the algorithm and initialized to all zeros, whose rows correspond to possible left-extensions ofW .
We will store character bj in cell A[ai,p], for increasing values of p starting from one, iff aiW bj
is the prefix of a rotation of T ; in this case, we will also set F [ai,p] = sp(aiW bj) and L[ai,p] =
ep(aiW bj). In other words, every triplet (A[ai,p], F [ai,p], L[ai,p]) identifies the right-extension
W bj of W associated with character bj = A[ai,p], and it specifies the interval of aiW bj in BWTT
(see Figure 2). We use array gamma[0..σ], given in input to the algorithm and initialized to all zeros,
to maintain, for every a ∈ [0..σ], the number of distinct characters b ∈ {b1,...,bk } such that aW b
is the prefix of a rotation of T , or equivalently the number of nonempty cells in row a of matrices
A, F , and L. In other words, gamma[a] = γ (a,W ).
For every j ∈ [1..k], we enumerate all the distinct characters that occur inside the interval
BWTT [first[j]..first[j + 1] − 1] of string W bj = W · chars[j], along with the corresponding
partial ranks, using operation rangeDistinct. Recall that rangeDistinct does not necessarily
return such characters in lexicographic order. For every character a returned by rangeDistinct,
we compute range(aW bj) in constant time using the C array and the partial ranks, we increment
counter gamma[a] by one, and we set:
A[a, gamma[a]] = chars[j],
F [a, gamma[a]] = sp(aW bj),
L[a, gamma[a]] = ep(aW bj).
See Figure 2 for an example. If gamma[a] transitioned from zero to one, then we increment h by
one and we set leftExtensions[h] = a. At the end of this process, leftExtensions[i] = ai for
i ∈ [1..h] (note again that the characters in leftExtensions[1..h] are not necessarily sorted lexicographically); the nonempty rows in A, F , and L correspond to such characters; the characters
that appear in row ai of matrix A are sorted lexicographically; and the corresponding intervals
[F [ai,p]..L[ai,p]] are precisely the intervals of string aiW · A[ai,p] in BWTT . It follows that such
intervals are adjacent in BWTT , thus:
repr(aiW ) = (A[ai, 1..gamma[ai]], F [ai, 1..gamma[ai]] • (L[ai, gamma[ai]] + 1)),
where X • y denotes appending number y to the end of array X. We can restore all matrices
and vectors to their original state within the claimed time budget, by scanning over all cells of
leftExtensions, using their value to address matrices A, F, and L, and using array gamma to determine how many cells must be cleaned in each row of such matrices.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
Linear-time String Indexing and Analysis in Small Space 17:25
Fig. 2. Lemma 4.1 applied to the right-maximal substring W = (v). Gray directed arcs represent implicit
and explicit Weiner links. White dots represent the destinations of implicit Weiner links. Child vk of node v
in STT has interval [ik ..jk ] in BWTT , where k ∈ [1..3]. Among all strings prefixed by string W , only those
prefixed by W GAG are preceded by C; it follows that CW is always followed by G and it is not right-maximal,
thus the Weiner link from v labeled by C is implicit. Conversely,W AGCG,W CG, andW GAG are all preceded by
an A, so AW is right-maximal and the Weiner link from v labeled by A is explicit.
Iterated applications of Lemma 4.1 are almost all we need to solve Problem 1 efficiently, as
described in the following lemma:
Lemma 4.2. Given a data structure that supports rangeDistinct queries on the BWT of a string
T ∈ [1..σ]
n−1#, and given the C array of T , there is an algorithm that solves Problem 1 in O(nt) time
and in O(σ2 log2 n) bits of working space, where t is the time taken by the rangeDistinct operation
per element in its output.
Proof. We use again the notation of Problem 1. Assume by induction that we know repr(W ) =
(chars[1..k], first[1..k + 1]) and |W | for some right-maximal substringW ofT . Using Lemma 4.1,
we compute ai and repr(aiW ) = (charsi[1..ki], firsti[1..ki + 1]) for all i ∈ [1..h], and we determine whether aiW is right-maximal by checking whether |charsi | > 1, or equivalently whether
gamma[ai] > 1 in Lemma 4.1; if this is the case, then we push pair (repr(aiW ), |W | + 1) to a stack
S. In the next iteration, we pop the representation of a string from the stack and we repeat the
process, until the stack becomes empty. Note that this is equivalent to following all the explicit
Weiner links from (or equivalently, all the reverse suffix links to) the node v of STT with (v) = W ,
not necessarily in lexicographic order. Thus, running the algorithm from a stack initialized with
repr(ε) is equivalent to a depth-first traversal of the suffix-link tree of T (not necessarily following the lexicographic order of Weiner link labels); recall from Section 2.3 that a traversal of SLTT
guarantees to enumerate all the right-maximal substrings of T . Triplet repr(ε) can be easily built
from the C array of T .
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
17:26 D. Belazzougui et al.
Every rangeDistinct query performed by the algorithm can be charged to a distinct node of
STT , and every tuple in the output of all such rangeDistinct queries can be charged to a distinct
(explicit or implicit) Weiner link. It follows from Observation 1 that the algorithm runs in O(nt)
time. Since the algorithm performs a depth-first traversal of the suffix-link tree of T , the depth
of the stack is bounded by the length of a longest right-maximal substring of T . More precisely,
since we always pop the element at the top of the stack, the depth of the stack is bounded by
quantity μT defined in Section 2.2, i.e., by the largest number of (not necessarily proper) suffixes
of a maximal repeat that are themselves maximal repeats. Even more precisely, since we push just
right-maximal substrings, the depth of the stack is bounded by quantity λT defined in Section 2.2.
Unfortunately, λT might be O(n). We reduce this depth to O(logn) by pushing at every iteration
the pair (repr(aiW ), |aiW |) with largest range(aiW ) first (a technique already described in Reference [37]): The interval of every other aW is necessarily at most half of range(W ), thus stack
S contains at any time pairs from O(logn) suffix-link tree levels. Every such level contains O(σ )
pairs, and every pair takes O(σ logn) bits, thus the total space used by the stack is O(σ2 log2 n)
bits.
Algorithm 2 summarizes Lemma 4.2 in pseudocode. Combining Lemma 4.2 with the
rangeDistinct data structure of Lemma 3.17, we obtain the following result:
Theorem 4.3. Given the BWT of a string T ∈ [1..σ]
n−1#, we can solve Problem 1 in O(nk) time,
and in n log σ (1 + 1/k) + 10n + 3σ logn + σ + logn + o(n) = n log σ (1 + 1/k) + O(n) bits of working space, for any positive integer k.
Proof. Lemma 4.2 needs just the C array, which takes (σ + 1) logn bits, and a rangeDistinct
data structure; the one in Lemma 3.17 takes n log σ + 8n + o(n) bits of space, and it answers queries
in time linear in the size of their output and in σ (2 logn + 1) bits of space in addition to the output.
Building the C array from BWTT takes O(n) time, and building the rangeDistinct data structure
of Lemma 3.17 takesO(nk) time and (n/k) log σ + 2n + o(n) bits of working space, for any positive
integer k.
Note that replacing Lemma 3.17 in Theorem 4.3 with the alternative construction of Lemma 3.18
introduces randomization and does not improve space complexity.
As we saw in Section 3.5, having an efficient algorithm for enumerating all intervals in BWTT
of right-maximal substrings of T has an immediate effect on the construction of the balanced
parentheses representation of STT . The following result derives from plugging Theorem 4.3 into
Lemma 3.12, and will be used extensively for constructing string indexes in Section 6:
Theorem 4.4. Given the BWT of a string T ∈ [1..σ]
n−1#, we can build the balanced parentheses
representation of the topology of STT in O(nk) time and in n log σ (1 + 1/k) + O(n) bits of working
space, for any positive integer k.
Building the BWT efficiently in Section 5, and comparing strings in Section 7, will rely on
enumerating all the right-maximal substrings of a concatenation T = T 1
T 2 ···T m of m strings
T 1,T 2,...,T m, where Ti ∈ [1..σ]
ni−1#i for i ∈ [1..m]. Recall that the right-maximal substrings
of T correspond to the internal nodes of the generalized suffix tree of T 1,T 2,...,T m; thus, we
can solve Problem 2 by applying Lemma 4.2 to the BWT of T . If we are just given the BWT
of each Ti separately, however, we can represent a substring W as a pair of sets of arrays
repr
(W ) = ({chars1,..., charsm }, {first1 ... firstm }), where charsi collects all the distinct
characters b such thatW b is observed in stringTi
, in lexicographic order, and the interval of string
W · charsi
[j] in BWTT i is [firsti
[j]..firsti
[j + 1] − 1]. If W does not occur in Ti
, then we assume that |charsi | = 0 and that firsti
[1] equals one plus the number of suffixes of Ti that are
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
Linear-time String Indexing and Analysis in Small Space 17:27
lexicographically smaller thanW . If necessary, this representation can be converted inO(mσ ) time
into a representation based on intervals of BWTT . We can thus adapt the approach of Lemma 4.2
to solve the following generalization of Problem 1, as described in Lemma 4.5 below:
Problem 2. Given stringsT 1,T 2,...,T m withTi ∈ [1..σ]
ni−1#i fori ∈ [1..m], return the following
information for all right-maximal substrings W of T = T 1
T 2 ···T m:
• |W | and range(W ) in SAT ;
• the sorted sequence b1 < b2 < ··· < bk of all the distinct characters in [−m + 1..σ] such that
W bi is a substring of T ;
• the sequence of intervals range(W b1),..., range(W bk );
• a sequence a1, a2,..., ah that lists all the h distinct characters in [−m + 1..σ] such that aiW
is the prefix of a rotation of T—the sequence a1, a2,..., ah is not necessarily in lexicographic
order;
• the sequence of intervals range(a1W ),..., range(ahW ).
Lemma 4.5. Assume that we are given a data structure that supports rangeDistinct queries
on the BWT of a string Ti
, and the C array of Ti
, for all strings in a set {T 1,T 2,...,T m }, where
Ti ∈ [1..σ]
ni−1#i for i ∈ [1..m]. There is an algorithm that solves Problem 2 in O(mnt) time and in
O(mσ2 log2 n) bits of working space, where t is the time taken by the rangeDistinct operation per
element in its output, and n = m
i=1 ni .
Proof. To keep the presentation as simple as possible, we omit the details on how to handle
strings that occur in someTi but that do not occur in someT j with j  i. We use the same algorithm
as in Lemma 4.2, but this time with the following data structures:
• m distinct arrays gamma1, gamma2,..., gammam;
• m distinct matrices A1,A2,...,Am, F 1, F 2,..., Fm, and L1, L2,..., Lm;
• a single stack, in which we push repr
(W ) tuples;
• a single array leftExtensions[1..σ + m], which stores all the distinct left-extensions of
a string W that are the prefix of a rotation of a string Ti
, not necessarily in lexicographic
order.
Given repr
(W ) for a right-maximal substringW ofT , we apply Lemma 4.1 to eachTi to compute
the corresponding repr(aW ) for all strings aW that are the prefix of a rotation ofTi
, updating rowa
inAi
, Fi
, Li
, and gammai accordingly, and adding a character a to the shared array leftExtensions
whenever we see a for the first time in any Ti (see Algorithm 4). If a = #i , then we assume it
is actually #i−1 if i > 1, and we assume it is #m if i = 1. We push to the shared stack the pair
repr
(aW ) = ({chars1 ... charsm }, {first1 ... firstm }) such that charsi = Ai
[a, 1..gammai
[a]],
firsti = Fi
[a, 1..gammai
[a]] • (Li
[a, gammai
[a]] + 1) for all i ∈ [1..m], if and only if aW is rightmaximal in T , or equivalently iff there is an i ∈ [1..m] such that gammai
[a] > 1, or alternatively
if there are two integers i  j in [1..m] such that gammai
[a] = 1, gammaj
[a] = 1, and Ai
[a][1]
Aj
[a][1] (see Algorithm 3). Note that we never push repr
(aW ) with a = #i in the stack, thus the
space taken by the stack is O(mσ2 log2 n) bits. In analogy to Lemma 4.2, we push first to the stack
the left-extension aW ofW that maximizes m
i=1 |I(aW ,Ti )| = m
i=1 Li
[a, gammai
[a]] − Fi
[a, 1] + 1.
The result of this process is a traversal of the suffix-link tree of T , not necessarily following the
lexicographic order of its Weiner link labels. The total cost of translating every repr
(W ) into the
quantities required by Problem 2 is O(mn).
Recall that we say that a node (possibly a leaf) of the suffix tree of T = T 1
T 2 ···T m is pure if
all the leaves in its subtree are suffixes of exactly one string Ti
, and we call it impure otherwise.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.       
17:28 D. Belazzougui et al.
Lemma 4.5 can be adapted to traverse only impure nodes of the generalized suffix tree. This leads
to the following algorithm for building the BWT of T from the BWT of T 1,T 2,...,T m:
Lemma 4.6. Assume that we are given a data structure that supports rangeDistinct queries
on the BWT of a string Ti
, and the C array of Ti
, for all strings in a set {T 1,T 2,...,T m }, where
Ti ∈ [1..σ]
ni−1#i for i ∈ [1..m]. There is an algorithm that builds the BWT of string T = T 1
T 2 ···T m
in O(mnt) time and in O(mσ2 log2 n) bits of working space, where t is the time taken by the
rangeDistinct operation per element in its output, and n = m
i=1 ni .
Proof. The BWT of T can be partitioned into disjoint intervals that correspond to pure nodes
of minimal depth in STT , i.e., to pure nodes whose parent is impure. In STT , suffix links from
impure nodes lead to other impure nodes, so the set of all impure nodes is a subgraph of the suffixlink tree of T , it includes the root, and it can be traversed by iteratively taking explicit Weiner
links from the root. We modify Algorithm 3 to traverse only impure internal nodes of STT , by
pushing to the stack repr
(aW ) = ({charsi
}, {firsti
}), where charsi = Ai
[a, 1..gammai
[a]] and
firsti = Fi
[a, 1..gammai
[a]] • (Li
[a, gammai
[a]] + 1) for all i ∈ [1..m], iff it represents an internal
node of STT , and moreover if there are two integers i  j in [1..m] such that gammai
[a] > 0 and
gammaj
[a] > 0.
Assume that we enumerate an impure internal node of STT with label W , and let repr
(W ) =
({charsi
}, {firsti
}). We merge in linear time the set of sorted arrays {charsi
}. Assume that character b = charsi
[j] occurs only in charsi
. It follows that the locus of W b in STT is a pure node of
minimal depth, and we can copy BWTT i[firsti
[j]..firsti
[j + 1] − 1] to BWTT [x..x + firsti
[j +
1] − firsti
[j] − 1], where x = 1 + m
i=1 smaller(b,i) and
smaller(b,i) =

firsti
[1] − 1 if (|charsi | = 0) or (charsi
[1] ≥ b),
maxj:charsi[j]<b {firsti
[j + 1] − 1} otherwise.
The value of x can be easily maintained while merging set {charsi
}. If character b occurs in
more than one charsi array, then the locus of W b in STT is impure, and it will be enumerated (or
it has already been enumerated) by the traversal algorithm.
In the rest of the article, we will focus on the case m = 2. The following theorem, which we
will use extensively in Section 7, combines Lemma 4.5 for m = 2 with the rangeDistinct data
structure of Lemma 3.17:
Theorem 4.7. Given the BWT of a string S1 ∈ [1..σ]
n1−1#1 and the BWT of a string S2 ∈
[1..σ]
n2−1#2, we can solve Problem 2 in O(nk) time and in n log σ (1 + 1/k) + O(n) bits of working
space, for any positive integer k, where n = n1 + n2.
Finally, in Section 5, we will work on strings that are not terminated by a special character; thus,
we will need the following version of Lemma 4.6 that works on sets of rotations rather than on
sets of suffixes:
Theorem 4.8. Let S1 ∈ [1..σ]
n1 and S2 ∈ [1..σ]
n2 be two strings such that |R(S1)| = n1, |R(S2)| =
n2, and R(S1) ∩ R(S2) = ∅. Given the BWT of R(S1) and the BWT of R(S2), we can build the BWT of
R(S1) ∪ R(S2) inO(nk) time and in n log σ (1 + 1/k) + O(n) bits of working space, where n = n1 + n2.
Proof. Since all rotations of Si are lexicographically distinct, the compact trie of all such rotations is well defined, and every leaf of such trie corresponds to a distinct rotation of Si
. Since no
rotation of S1 is lexicographically identical to a rotation of S2, the generalized compact trie that
contains all rotations of S1 and all rotations of S2 is well defined, and every leaf of such trie corresponds to a distinct rotation of S1 or of S2. We can thus traverse such generalized compact trie using
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.    
Linear-time String Indexing and Analysis in Small Space 17:29
BWTS1 and BWTS2 as described in Lemma 4.6, using Lemma 3.17 to implement rangeDistinct
data structures.
ALGORITHM 1: Building repr(aW ) from repr(W ) for all a ∈ [0..σ] such that aW is a prefix of a
rotation of T ∈ [1..σ]
n−1#.
Input: repr(W ) for a substring W of T . Support for rangeDistinct queries on the BWT of T . C array
of T . Empty matrices A, F , and L, empty arrays gamma and leftExtensions, and a pointer h.
Output: Matrices A, F , L, pointer h, arrays gamma and leftExtensions, filled as described in
Lemma 4.2.
1 (chars, first) ← repr(W );
2 h ← 0;
3 for j ∈ [1..|chars|] do
4 I ← BWTT .rangeDistinct(first[j], first[j + 1] − 1);
5 for (a,pa,qa ) ∈ I do
6 if gamma[a] = 0 then
7 h ← h + 1;
8 leftExtensions[h] ← a;
9 end
10 gamma[a] ← gamma[a] + 1;
11 A[a, gamma[a]] ← chars[j];
12 F [a, gamma[a]] ← C[a] + pa;
13 L[a, gamma[a]] ← C[a] + qa;
14 end
15 end
5 BUILDING THE BURROWS-WHEELER TRANSFORM
It is well known that the Burrows-Wheeler transform of a string T # such that T ∈ [1..σ]
n and
# = 0  [1..σ], can be built in O(n log log σ ) time and in O(n log σ ) bits of working space [39]. In
this section, we bring construction time down to O(n) by plugging Theorem 4.8 into the recursive
algorithm described in Reference [39], which we summarize here for completeness.
Specifically, we partition T into blocks of equals size B. For convenience, we work with a version of T whose length is a multiple of B, by appending to the end of T the smallest number of
occurrences of character # such that the length of the resulting padded string is an integer multiple
of B, and such that the padded string contains at least one occurrence of #. Recall that B · x/B
is the smallest multiple of B that is at least x. Thus, we append n − n copies of character # to T ,
where n = B · (n + 1)/B. To simplify notation, we call the resulting string X, and we use n to
denote the length of X.
We interpret a partitioning of X into blocks as a new string XB of length n
/B, defined on the
alphabet [1..(σ + 1)
B] of all strings of length B on alphabet [0..σ]: The “characters” of XB correspond to the blocks of X. In other words, XB[i] = X[(i − 1)B + 1..iB]. We assume B to be even,
and we denote by left (respectively, right) the function from [1..(σ + 1)
B] to [1..(σ + 1)
B/2]
such that left(W ) returns the first (respectively, the second) half of block W . In other words, if
W = w1 ···wB, left(W ) = w1 ···wB/2 and right(W ) = wB/2+1 ···wB. We also work with circular rotations of X (see Section 2.2): Specifically, we denote by ←−
X string X[B/2 + 1..n
] · X[1..B/2],
or equivalently string X circularly rotated to the left by B/2 positions, and we denote by ←−
X B the
string on alphabet [1..(σ + 1)
B] induced by partitioning ←−
X into blocks of size B.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
17:30 D. Belazzougui et al.
ALGORITHM 2: Enumerating all right-maximal substrings of T ∈ [1..σ]
n−1#. See Lemma 4.1 for
a definition of operator •. The callback function callback highlighted in gray just prints the pair
(repr(W ), |W |) given in input. Section 7 describes other implementations of callback.
Input: BWT transform and C array of T . Array distinctChars of all the distinct characters that occur
in T , in lexicographic order, and array start of starting positions of the corresponding intervals
in BWTT . Support for rangeDistinct queries on BWTT , and an implementation of
Algorithm 1 (function extendLeft).
Output: (repr(W ), |W |) for all right-maximal substrings W of T .
1 S ← empty stack;
2 A ← zeros[0..σ, 1..σ + 1];
3 F ← zeros[0..σ, 1..σ + 1];
4 L ← zeros[0..σ, 1..σ + 1];
5 gamma ← zeros[0..σ];
6 leftExtensions ← zeros[1..σ + 1];
7 repr(ε) ← (distinctChars, start • (n + 1));
8 S.push
(repr(ε), 0)

;
9 while not S.isEmpty() do
10 (repr(W ), |W |) ← S.pop();
11 h ← 0;
12 extendLeft(repr(W ), BWTT ,C,A, F, L, gamma, leftExtensions,h);
13 callback(repr(W ), |W |, BWTT ,C,A, F, L, gamma, leftExtensions,h);
/* Pushing right-maximal left-extensions on the stack */
14 C←{c : c = leftExtensions[i],i ∈ [1..h], gamma[c] > 1};
15 if C  ∅ then
16 c ← argmax{L[c, gamma[c]] − F [c, 1] : c ∈ C};
17 repr(cW ) ← (A[c, 1..gamma[c]], F [c, 1..gamma[c]] • (L[c, gamma[c]] + 1));
18 S.push(repr(cW ), |W | + 1);
19 for a ∈C\{c} do
20 repr(aW ) ← (A[a, 1..gamma[a]], F [a, 1..gamma[a]] • (L[a, gamma[a]] + 1));
21 S.push(repr(aW ), |W | + 1);
22 end
23 end
/* Cleaning up for the next iteration */
24 for i ∈ [1..h] do
25 a ← leftExtensions[i];
26 for j ∈ [1..gamma[a]] do
27 A[a, j] ← 0;
28 F [a, j] ← 0;
29 L[a, j] ← 0;
30 end
31 gamma[a] ← 0;
32 end
33 end
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
Linear-time String Indexing and Analysis in Small Space 17:31
ALGORITHM 3: Enumerating all right-maximal substrings of T = T 1#1T 2#2 ···Tm#m, where Ti ∈
[1..σ]
ni−1 for i ∈ [1..m], m ≥ 1. The key differences from Algorithm 2 are highlighted in gray. To iterate over all impure right-maximal substrings of T , it suffices to replace just the gray lines (see Lemma
4.6). See Lemma 4.1 for a definition of operator •. The callback function callback just prints its input
(repr
(W ), |W |). For brevity, the case in which a string occurs in some Ti but does not occur in some T j
is not handled.
Input: BWT transform and C array of string T i #. Array distinctCharsi of all the distinct characters that occur in
T i #, in lexicographic order, and array starti of starting positions of the corresponding intervals in BWTT i #.
Support for rangeDistinct queries on BWTT i #, and an implementation of Algorithm 4 (function
extendLeft
).
Output: (repr
(W ), |W |) for all right-maximal substrings W of T .
1 S ← empty stack;
2 for i ∈ [1..m] do
3 Ai ← zeros[0..σ, 1..σ + 1], F i ← zeros[0..σ, 1..σ + 1];
4 Li ← zeros[0..σ, 1..σ + 1], gammai ← zeros[0..σ ];
5 end
6 leftExtensions ← zeros[1..σ + m];
7 seen ← zeros[1..σ ];
8 repr
(ε ) ← ({distinctCharsi }, {starti • (ni + 1)});
9 S .push((repr
(ε ), 0));
10 while not S .isEmpty() do
11 (repr
(W ), |W |) ← S .pop();
12 h ← 0;
13 extendLeft
(repr
(W ), {BWTT i }, {Ci }, {Ai }, {F i }, {Li }, {gammai }, leftExtensions, seen, h);
14 callback(repr
(W ), |W |, {BWTT i }, {Ci }, {Ai }, {F i }, {Li }, {gammai }, leftExtensions, h);
/* Pushing right-maximal left-extensions on the stack */
15 C←{c > 0 : c = leftExtensions[i], i ∈ [1..h], (∃ p ∈ [1..m] : gammap [c] > 1)
16 or (∃ p  q : gammap [c] = 1, gammaq[c] = 1, Ap [c, 1]  Aq[c, 1])};
17 if C  ∅ then
18 c ← argmax m
i=1 Li[c, gammai[c]] − F i[c, 1] : c ∈ C
;
19 repr
(cW ) ← ({Ai[c, 1..gammai[c]]}, {F i[c, 1..gammai[c]] • (Li[c, gammai[c]] + 1)});
20 S .push(repr
(cW ), |W | + 1);
21 for a ∈C\{c } do
22 repr
(aW ) ← ({Ai[a, 1..gammai[a]]}, {F i[a, 1..gammai[a]] • (Li[a, gammai[a]] + 1)});
23 S .push(repr
(aW ), |W | + 1);
24 end
25 end
/* Cleaning up for the next iteration */
26 for i ∈ [1..h] do
27 a ← leftExtensions[i];
28 if a ≤ 0 then
29 k ← −a + 2 (mod1 m);
30 Ak [0, 1] ← 0, F k [0, 1] ← 0, Lk [0, 1] ← 0, gammak [0] ← 0;
31 end
32 else
33 seen[a] ← 0;
34 for j ∈ [1..m] do
35 for k ∈ [1..gammaj[a]] do
36 Aj[a, k] ← 0, F j[a, k] ← 0, Lj[a, k] ← 0;
37 end
38 gammaj[a] ← 0;
39 end
40 end
41 end
42 end
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.      
17:32 D. Belazzougui et al.
ALGORITHM 4: Building repr
(aW ) from repr
(W ) for all a ∈ [−m + 1..σ] such that aW is a prefix
of a rotation of T = T 1#1T 2#2 ···Tm#m, where m ≥ 1 and Ti ∈ [1..σ]
ni−1. The lines highlighted in gray
are the key differences from Algorithm 1.
Input: repr
(W ) for a substring W of T . Support for rangeDistinct queries on BWTT i #, C array of
Ti , empty matrices Ai , Fi and Li , and empty array gammai of string Ti , for all i ∈ [1..m]. A
single empty array leftExtensions, a single bitvector seen, and a single pointer h.
Output: Matrices Ai , Fi , Li , pointer h, and arrays gammai and leftExtensions, for all i ∈ [1..m], filled
as described in Lemma 4.5.
1 ({charsi}, {firsti}) ← repr
(W );
2 h ← 0;
3 for i ∈ [1..m] do
4 for j ∈ [1..|charsi |] do
5 I ← BWTT i #.rangeDistinct(firsti[j], firsti[j + 1] − 1);
6 for (a,pa,qa ) ∈ I do
7 if a = 0 then
8 h ← h + 1;
9 leftExtensions[h] ← −(i − 1 (mod1 m)) + 1;
10 end
11 else
12 if seen[a] = 0 then
13 seen[a] = 1;
14 h ← h + 1;
15 leftExtensions[h] ← a;
16 end
17 end
18 gammai[a] ← gammai[a] + 1;
19 Ai[a, gammai[a]] ← charsi[j];
20 Fi[a, gammai[a]] ← Ci[a] + pa;
21 Li[a, gammai[a]] ← Ci[a] + qa;
22 end
23 end
24 end
Note that the suffix that starts at position i in ←−
X B equals the half-block Pi = X[B/2 + (i − 1)B +
1..iB], followed by string Si = Fi+1 · X[1..B/2], where Fi+1 is the suffix of XB that starts at position
i + 1 in XB, if any. Thus, it is not surprising that we can derive the BWT of string ←−
X B from the
BWT of string XB:
Lemma 5.1 ([39]). The BWT of string ←−
X B can be derived from the BWT of string XB in O(n
/B +
σ B ) time and O(σ B · log(n
/B)) bits of working space, where n = |X |.
The second key observation that we exploit for building the BWT ofX is the fact that the suffixes
of XB/2 that start at odd positions coincide with the suffixes of XB, and the suffixes of XB/2 that
start at even positions coincide with the suffixes of ←−
X B. Thus, we can reconstruct the BWT of XB/2
by merging the BWT of XB with the BWT of ←−
X B; this is where Theorem 4.8 comes into play.
Lemma 5.2. Assume that we can read in constant time a block of B characters. Then, the BWT of
string XB/2 can be derived from the BWT of string XB and from the BWT of string ←−
X B, in O(n
/B)
time and O(n log σ ) bits of working space, where n = |X |.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.
Linear-time String Indexing and Analysis in Small Space 17:33
Proof. All rotations of XB (respectively, of ←−
X B) are lexicographically distinct, and no rotation
of XB is lexicographically identical to a rotation of ←−
X B. Thus, we can use Theorem 4.8 to build the
BWT of R(XB ) ∪ R(
←−
X B ) in O(kn
/B) time and in 2n
(1 + 1/k) log(σ + 1) + O(n
/B) ∈ O(n log σ )
bits of working space. Inside the algorithm of Theorem 4.8, we apply the constant-time operator
right to the characters of the input BWTs. There is a bijection between set R(XB ) ∪ R(
←−
X B ) and
set R(XB/2) that preserves lexicographic order, thus the BWT of R(XB/2) coincides with the BWT
of R(XB ) ∪ R(
←−
X B ) in which each character is processed with operator right.
Lemmas 5.1 and 5.2 suggest setting B to a power of two and building the BWT of X in O(log B)
steps, where at step i, we compute the BWT of string XB/2i , stopping when B/2i = 1. Note that
the key requirement of Lemma 5.2, i.e., that all rotations of XB/2i (respectively, of ←−
X B/2i ) are lexicographically distinct, and that no rotation of XB/2i is lexicographically identical to a rotation of ←−
X B/2i , holds for all i. The time for completing step i is O(n
/(B/2i )), and the Burrows-Wheeler
transforms of XB/2i and of ←−
X B/2i take O(n log σ ) bits of space for every i.
The base case of the recursion is the BWT of string XB for some initial block size B: We
build it using any suffix array construction algorithm that works in O(σ B + n
/B) time and in
O((n
/B) log(n
/B)) bits of space (for example, those described in References [40, 43, 44]). We
want this first phase to take O(n
) time and O(n log σ ) bits of space; or, in other words, we want
to satisfy the following constraints:
(1) σ B ∈ O(n
);
(2) (n
/B) log(n
/B) ∈ O(n log σ ), or more strictly (n
/B) logn ∈ O(n log σ ).
We also want B to be a power of two. Recall that 2log2 x  is the smallest power of two that
is at least x. Assume, thus, that we set B = 2log2 (log2 n
/(c log2 σ )) for some constant c. Then B ≥
logn
/(c log σ ), thus Constraint 2 is satisfied by any choice of c. Since x < x + 1, we have that
B < (2/c) logn
/ log σ, thus Constraint 1 is satisfied for any c ≥ 2. For this choice of B, the number
of steps in the recursion becomesO(log logn
), and we can read a block of size B in constant time as
required by Lemma 5.2, since the machine word is assumed to be Ω(logn
). It follows that building
the BWT of X takes O(n + (n
/B + σ B log(n
/B)) log B
i=1 2i ) = O(n
) time and O(n log σ ) bits of
working space. Since the BWT of T # can be derived from the BWT of X at no extra asymptotic
cost (see Reference [39]), we have the following result:
Theorem 5.3. The BWT of a string T # such that T ∈ [1..σ]
n and # = 0 can be built in O(n) time
and in O(n log σ ) bits of working space.
6 BUILDING STRING INDEXES
6.1 Building the Compressed Suffix Array
The compressed suffix array of a string T ∈ [1..σ]
n−1# (abbreviated to CSA in what follows) is a
representation of SAT that uses just O((n log σ )/ϵ ) bits space for any given constant ϵ, at the
cost of increasing access time to any position of SAT to t = O((logσ n)
ϵ /ϵ ) [32]. Without loss of
generality, let B be a block size such that Bi divides n for any setting of i that we will consider, and
let Ti be the (suitably terminated) string of length n/Bi defined on the alphabet [1..(σ + 1)
Bi
] of
all strings of length Bi on alphabet [0..σ], and such that the “characters” of Ti correspond to the
consecutive blocks of size Bi of T . In other words, Ti[j] = T [(j − 1)Bi + 1..jBi
]. Note that T0 = T ,
and Ti with i > 0 is the string obtained by grouping every consecutive B characters of Ti−1. The
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
17:34 D. Belazzougui et al.
CSA of T with parameter ϵ consists of the suffix array of T1/ϵ , and of 1/ϵ layers, where layer
i ∈ [0..1/ϵ − 1] is composed of the following elements:
(1) A data structure that supports access and partialRank operations10 on BWTTi .
(2) TheC array ofTi , defined on alphabet [1..(σ + 1)
Bi
], encoded as a bitvector with (σ + 1)
Bi
ones and n/Bi zeros, and indexed to support select queries.
(3) A bitvector markedi , of size n/Bi
, that marks every position j such that SATi[j] is a multiple
of B.
For concreteness, let ϵ = 2−c for some constant c > 0. Note that layer i contains enough information to support function LF on Ti . To compute SATi[j], we first check whether markedi[j] = 1;
if so, then SATi[j] = B · SATi+1 [j

], where j
 = rank1 (markedi, j). Otherwise, we iteratively set j
to LF(j) in constant time and we test whether markedi[j] = 1. If it takes t iterations to reach a
j
∗ such that markedi[j
∗] = 1, then SATi[j] = B · SATi+1 [rank1 (markedi, j
∗)] + t. Since t ≤ B − 1 at
any layer, the time spent in a layer is O(B), and the time to traverse all layers is O(B/ϵ ). Setting
B = (logσ n)
ϵ achieves the claimed time complexity, and assuming without loss of generality that
σ is a power of two and n = σ22a
for some integer a ≥ c ensures that Bi for any i ∈ [1..1/ϵ] is an
integer that divides n. Using Lemma 3.5, every layer takes O(n log σ ) bits of space, irrespective
of B, so the whole data structure takes O((n log σ )/ϵ ) bits of space. Counting the number occ of
occurrences of a pattern P in T can be performed in a number of ways with the CSA. A simple
O(|P | logn · (logσ n)
ϵ /ϵ )) time solution consists in performing binary searches on the suffix array;
this allows one to locate all such occurrences in O(|P |(logn + occ) · (logσ n)
ϵ /ϵ )) time. Alternatively, count queries could be implemented with backward steps as in the BWT index, in overall
O(|P | log log σ ) time, using Lemma 3.9.
The CSA takes in general at least n log σ + o(n) bits, or even nHk + o(n) bits for k = o(logσ n)
[30]. The CSA has a number of variants, the fastest of which can be built in O(n log log σ ) time
usingO(n log σ ) bits of working space [39]. Combining the setup of data structures described above
with Theorem 5.3 allows one to build the CSA more efficiently:
Theorem 6.1. Given a string T = [1..σ]
n, we can build the compressed suffix array in O(n) time
and in O(n log σ ) bits of working space.
Note that the BWT of all stringsTi in the CSA ofT , as well as all bitvectors markedi , can be built
in a single invocation of Theorem 5.3, rather than by invoking Theorem 5.3 1/ϵ times. Note also
that combining Theorem 5.3 with Lemma 2.8 and with the first data structure of Lemma 3.9, yields
immediately a BWT index and a succinct suffix array that can be built in deterministic linear time:
Theorem 6.2. Given a string T = [1..σ]
n, we can build the following data structures:
• A BWT index that takes n log σ (1 + 1/k) + O(n log log σ ) bits of space for any positive integer k, and that implements operation LF(i) in constant time for any i ∈ [1..n], and operation
count(P) in O(m(log log σ + k)) time for any P ∈ [1..σ]
m. The index can be built in O(n)
time and in O(n log σ ) bits of working space.
• A succinct suffix array that takes n log σ (1 + 1/k) + O(n log log σ ) + O((n/r) logn) bits
of space for any positive integers k and r, and that implements operation count(P) in
O(m(log log σ + k)) time for any P ∈ [1..σ]
m, operation locate(i) in O(r) time, and operation substring(i, j) in O(j − i + r) time for any i < j in [1..n]. The index can be built in
O(n) time and in O(n log σ ) bits of working space.
10The CSA was originally defined in terms of the ψ function; in this case, support for select queries would be needed.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.
Linear-time String Indexing and Analysis in Small Space 17:35
6.2 Building BWT Indexes
To reduce the time complexity of a backward step to a constant, however, we need to augment the
representation of the topology of STT described in Lemma 3.10 with an additional operation, where
T = [1..σ]
n−1#. Recall that the identifier id(v) of a nodev of STT is the rank ofv in the preorder traversal of STT . Given a nodev of STT and a character a ∈ [0..σ], let operation weinerLink(id(v), a)
return zero if string a(v) is not the prefix of a rotation of T , and return id(w) otherwise, where
w is the locus of string a(v) in STT . The following lemma describes how to answer weinerLink
queries efficiently:
Lemma 6.3. Assume that we are given a data structure that supports access queries on the BWT
of a string T = [1..σ]
n−1# in constant time, a data structure that supports rangeDistinct queries
on BWTT in constant time per element in the output, and a data structure that supports select
queries on BWTT in time t. Assume also that we are given the representation of the topology of STT
described in Lemma 3.10. Then, we can build a data structure that takes O(n log log σ ) bits of space
and that supports operation weinerLink(id(v), a) in O(t) time for any node v of STT (including
leaves) and for any character a ∈ [0..σ]. This data structure can be built in randomized O(nt) time
and in O(n log σ ) bits of working space.
Proof. We show how to build efficiently the data structure described in Reference [11], which
we summarize here for completeness. We use the suffix tree topology to convert in constant time
id(v) to range(v) (using operations leftmostLeaf and rightmostLeaf), and vice versa (using
operations selectLeaf and lca). We traverse STT in preorder using the suffix tree topology, as
described in Lemma 3.11. For every internal node v of STT , we use a rangeDistinct query to
compute all the h distinct characters a1,..., ah that appear in BWTT [range(v)], and for every such
character the interval of ai (v) in BWTT , in overall O(h) time. Note that the sequence a1,..., ah
returned by a rangeDistinct query is not necessarily sorted in lexicographic order. We determine
whether ai (v) is the label of a node w of STT by taking a suffix link from the locus of ai (v) in
O(t) time, using Lemma 3.13, and by checking whether the destination of such link is indeed v.
For every character c ∈ [0..σ], we use vector sourcesc to store all nodes v of STT (including
leaves) that are the source of an implicit or explicit Weiner link labeled by c, in the order induced
by the preorder traversal of STT . We encode the difference between the preorder ranks of two
consecutive nodes in the same sourcesc using Elias delta or gamma coding [21]. We also store
a bitvector explicitc that marks with a one every explicit Weiner link in sourcesc (recall that
Weiner links from leaves are explicit). Bitvectors sourcesc and explicitc can be filled during the
preorder traversal of STT . Once explicitc has been filled, we index it to answer rank queries. The
space used by such indexed bivectors explicitc for all c ∈ [0..σ] is O(n) bits by Observation 1,
and the space used by vectors sourcesc for all c ∈ [0..σ] is O(n log σ ) bits, by applying Jensen’s
inequality twice as in Lemma 3.18. We follow the static allocation strategy described in Section 3.1:
Specifically, we compute the number of bits needed by sourcesc and explicitc during a preliminary pass over STT , in which we increment the size of the arrays by keeping the preorder position
of the last internal node with a Weiner link labeled by c, for all c ∈ [0..σ]. This preprocessing
takes O(n) time and O(σ logn) ∈ o(n) bits of space. Once such sizes are known, we allocate a large
enough contiguous region of memory.
Finally, we build an arrayC
[1..σ] whereC
[a] is the number of nodesv in STT (including leaves)
such that (v) starts with a character strictly smaller than a. We also build an implementation of an
MMPHF f c for every sourcesc using the technique described in the proof of Lemma 3.18, and we
discard sourcesc . All such MMPHF implementations take O(n log log σ ) bits of space, and they
can be built in overall O(n) randomized time and in O(σk log σ ) bits of working space, for any
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.      
17:36 D. Belazzougui et al.
integer k > 1. Note that C takes O(σ logn) ∈ o(n) bits of space, since σ ∈ o(
√
n/ logn), and it can
be built with a linear-time preorder traversal of STT .
Given a node v of STT and a character c ∈ [0..σ], we determine whether the Weiner link from
v labeled by c is explicit or implicit by accessing explicitc (f c (id(v))), and we compute the
identifier of the locusw of the destination of the Weiner link (which might be a leaf) by computing:
C
[c] + rank1 (explicitc , f c (id(v)) − 1) + 1.
If there is no Weiner link from v labeled by c, then v does not belong to sourcesc , but f c (id(v))
still returns a valid pointer in sourcesc ; to check whether this pointer corresponds to v, we
convert v and w to intervals in BWTT using the suffix tree topology, and we check whether
selectc (BWTT , sp(w) − C[c]) ∈ range(v).
The output of this construction consists in arrays C
, explicitc , and in the implementation of
f c , for all c ∈ [0..σ].
Since operation weinerLink(id(v), a) coincides with a backward step with character a from
range(v) in BWTT , Lemma 6.3 enables the construction of space-efficient BWT indexes with
constant-time LF:
Theorem 6.4. Given a string T = [1..σ]
n−1#, we can build any of the following data structures in
randomized O(n) time and in O(n log σ ) bits of working space:
• A BWT index that takes n log σ (1 + 1/k) + O(n log log σ ) bits of space for any positive integer k, and that implements operation LF(i) in constant time for any i ∈ [1..n], and operation
count(P) in O(mk) time for any P ∈ [1..σ]
m.
• A succinct suffix array that takes n log σ (1 + 1/k) + O(n log log σ ) + O((n/r) logn) bits of
space for any positive integers k and r, and that implements operation count(P) in O(mk)
time for any P ∈ [1..σ]
m, operation locate(i) in O(r) time, and operation substring(i, j) in
O(j − i + r) time for any i < j in [1..n].
Alternatively, for the same construction space and time, we can build analogous data structures that
support LF(i) in O(k) time, count(P) in O(m) time, locate(i) in O(r) time, and substring(i, j) in
O(j − i + r) time; such data structures take the same space as those described above.
Proof. In this proof, we combine a number of results described earlier in the article (see Figure 1
for a summary of their mutual dependencies).
We use Theorem 5.3 to build BWTT fromT , and Lemma 3.5 to build a data structure that supports
access, partialRank, and select queries on BWTT . Then, we discard BWTT . Together with
the C array of T , this is already enough to implement function LF and to build arrays samples
and pos2rank for the succinct suffix array, using Lemma 2.8. We either use the data structure
of Lemma 3.5 that supports select queries in O(k) time (in which case, we implement locate and
substring queries with function LF), or the data structure that supports select queries in constant
time (in which case, we implement locate and substring queries with function ψ).
To implement backward steps, we need support for weinerLink operations from internal nodes
of STT . We use Lemma 3.17 to build a rangeDistinct data structure on BWTT from the access,
partialRank, and select data structure built by Lemma 3.5. We use rangeDistinct queries inside the algorithm to enumerate the BWT intervals of all internal nodes of STT described in Theorem 4.3, and we use such algorithm to build the balanced parentheses representation of STT as
described in Theorem 4.4. To support operations on the topology of STT , we feed the balanced
parentheses representation of STT to Lemma 3.10. Finally, we use the rangeDistinct data structure, the tree topology, and the support for access, partialRank, and select queries on BWTT ,
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
Linear-time String Indexing and Analysis in Small Space 17:37
to build the data structures that support weinerLink operations described in Lemma 6.3. At the
end of this process, we discard the rangeDistinct data structure.
The output of this construction consists of the data structures that support access,
partialRank, and select on BWTT , and weinerLink on STT .
6.3 Building the Bidirectional BWT Index
The BWT index can be made bidirectional, in the sense that it can be adapted to support both left
and right extension by a single character [72]. In addition to having a number of applications in
high-throughput sequencing (see, e.g., References [46, 47]), this index can be used to implement
a number of string analysis algorithms, and as an intermediate step for building the compressed
suffix tree.
Given a stringT = t1t2 ···tn−1 on alphabet [1..σ], consider two BWT transforms, one built onT #
and one built onT # = tntn−1 ···t1#. Let I(W ,T ) be the function that returns the interval in BWTT #
of the suffixes ofT # that are prefixed by stringW ∈ [1..σ]
+. Note that interval I(W ,T ) in the suffix
array of T # contains all the starting positions of string W in T . Symmetrically, interval I(W ,T ) in
the suffix array ofT # contains all those positionsi such that n − i + 1 is an ending position of string
W in T .
Definition 6.5. Given a string T ∈ [1..σ]
n−1, a bidirectional BWT index on T is a data structure
that supports the following operations on pairs of integers 1 ≤ i ≤ j ≤ n and on substringsW ofT :
• isLeftMaximal(i, j): returns 1 if substring BWTT #[i..j] contains at least two distinct characters, and 0 otherwise.
• isRightMaximal(i, j): returns 1 if substring BWTT #[i..j] contains at least two distinct characters, and 0 otherwise.
• enumerateLeft(i, j): returns all the distinct characters that appear in substring
BWTT #[i..j], in lexicographic order.
• enumerateRight(i, j): returns all the distinct characters that appear in BWTT #[i..j], in lexicographic order.
• extendLeft(c, I(W ,T ), I(W ,T )): returns pair (I(cW ,T ), I(Wc,T )) for c ∈ [0..σ].
• extendRight(c, I(W ,T ), I(W ,T )): returns (I(W c,T ), I(cW ,T )) for c ∈ [0..σ].
• contractLeft(I(aW ,T ), I(W a,T )), where a ∈ [1..σ] and aW is right-maximal: returns pair
(I(W ,T ), I(W ,T ));
• contractRight(I(W b,T ), I(bW ,T )), where b ∈ [1..σ] andW b is left-maximal: returns pair
(I(W ,T ), I(W ,T )).
Operations extendLeft and extendRight are analogous to a standard backward step in BWTT #
or BWTT #, but they keep the interval of a string W in one BWT synchronized with the interval of
its reverse W in the other BWT.
To build a bidirectional BWT index on string T , we also need to support operation
countSmaller(range(v),c), which returns the number of occurrences of characters smaller than
c in BWTT #[range(v)], where v is a node of STT # and c is the label of an explicit or implicit
Weiner link from v. Note that, when v is a leaf of STT #, countSmaller(range(v), BWTT #[x]) = 0,
where x = sp(v) = ep(v). The construction of Lemma 6.3 can be extended to support constanttime countSmaller queries, as described in the following:
Lemma 6.6. Assume that we are given a data structure that supports access queries on the BWT
of a string T = [1..σ]
n−1# in constant time, a data structure that supports rangeDistinct queries on
BWTT in constant time per element in the output, and a data structure that supports select queries
on BWTT in time t. Assume also that we are given the representation of the topology of STT described
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
17:38 D. Belazzougui et al.
in Lemma 3.10. Then, we can build a data structure that takes 3n log σ + O(n log log σ ) bits of space,
and that supports operation weinerLink(id(v), a) in O(t) time for any node v of STT (including
leaves) and for any character a ∈ [0..σ], and operation countSmaller(range(v),c) in constant time
for any internal node v of STT and for any character a ∈ [1..σ] that labels a Weiner link from v. This
data structure can be built in randomized O(nt) time and in O(n log σ ) bits of working space.
Proof. We run the algorithm described in the proof of Lemma 6.3. Specifically, we traverse
STT in preorder, we print arrays sourcesc for all c ∈ [0..σ], and we build the implementation
of an MMPHF f c for every sourcesc . Before discarding sourcesc , we build the prefix-sum data
structure of Lemma 3.2 on every sourcesc with c > 0; by Jensen’s inequality and Observation 1,
all such data structures take at most 3n log σ + 6n + o(n) bits of space in total.
Let STc = (Vc , Ec ) be the contraction of STT induced by all the nc nodes (including leaves) that
have an explicit or implicit Weiner link labeled by character c ∈ [1..σ]. During the preorder traversal of STT , we also concatenate to a bitvector parenthesesc an open parenthesis every time
we visit an internal node v with a Weiner link labeled by character c from its parent, and a closed
parenthesis every time we visit v from its last child. Note that parenthesesc represents the topology of STc . By Observation 1, building all bitvectors parenthesesc takes O(n) time and 6n + o(n)
bits of space in total, since every pair of corresponding parentheses can be charged to an explicit
or implicit Weiner link of STT . We feed parenthesesc to Lemma 3.10 to obtain support for tree
operations, and we discard parenthesesc . Following the strategy described in Section 3.1, we preallocate the space required by sourcesc and parenthesesc for allc ∈ [1..σ] during a preliminary
pass over STT . Note that the preorder rank in STc of a node v, denoted by idc (v), equals its position in array sourcesc . Note also that the set of idc (w) values for all the descendants w of v in
STc , including v itself, forms a contiguous range.
We allocate σ empty arrays diffc [1..nc ], which, at the end of the algorithm, will contain the
following information:
diffc [idc (v)] = countSmaller(range(v),c)
−

(v,w)∈Ec
countSmaller(range(w),c),
i.e., diffc [k] will encode the difference between the number of characters smaller than c in the
BWT interval of the node v of STT that is mapped to position k in sourcesc , and the number of
characters smaller than c in the BWT intervals of all the descendants of v in the contracted suffix
tree STc . To compute countSmaller(range(v),c) for some internal node v of STT , we proceed
as follows: We use the implementation of the MMPHF f c built on sourcesc to compute idc (v),
we retrieve the smallest and the largest idc (w) value assumed by a descendant w of v in STc
using operations leftmostLeaf and rightmostLeaf provided by the topology of STc , and we sum
diffc [k] for all k in this range. We compute this sum in constant time by encoding diffc with the
prefix-sum data structure described in Lemma 3.2. Since nc
k=1 diffc [k] ≤ n, the total space taken
by all such prefix-sum data structures is at most 3n log σ + 6n + o(n) bits, by Observation 1 and
Jensen’s inequality.
To build the diffc arrays, we scan the sequence of all characters c1 < c2 < ··· < ck such
that ci ∈ [1..σ] and STci has at least one node, for all i ∈ [1..k]. We use a temporary vector
lastChar with one element per node of STT ; after having processed characterci , lastChar[id(v)]
stores the largest cj ≤ ci that labels a Weiner link from v. We also assume to be able to answer countSmaller(range(v),c) queries in constant time. Note that lastChar takes at most
(2n − 1) log σ bits of space. We process character ci as follows: We traverse STci in preorder using its topology, as described in Lemma 3.11. For each node v of STci , we use idci (v) and the
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
Linear-time String Indexing and Analysis in Small Space 17:39
prefix-sum data structure on sourcesci to compute id(v). If v is an internal node of STT , then
we use id(v) to access b = lastChar[id(v)]. We compute the number of occurrences of character
b in range(v) using the O(t)-time operation weinerLink(id(v),b), and we compute the number of occurrences of characters smaller than b in range(v) using the constant-time operation
countSmaller(range(v),b). We do the same for all children of v in STci , which we can access
using the topology of STci . Finally, we sum the values of all children and we subtract this sum
from the value of v, appending the result to the end of diffci using Elias delta or gamma coding.
Finally, we set lastChar[id(v)] = ci . The total number of accesses to a node v of STT is a constant
multiplied by the number of Weiner links from v, thus the algorithm runs in O(nt) time.
The output of the construction consists in the topology of STci for all i ∈ [1..k], in arraysC and
explicitc of Lemma 6.3 for all c ∈ [0..σ], in the implementation of f c for all c ∈ [0..σ], and in
the prefix-sum data structure on diffci for all i ∈ [1..k].
Lemma 6.6 immediately yields the following result:
Theorem 6.7. Given a stringT = [1..σ]
n, we can build in randomizedO(n) time and inO(n log σ )
bits of working space a bidirectional BWT index that takes O(n log σ ) bits of space and that implements every operation in time linear in the size of its output.
Proof. Let W be a substring of T such that I(W , BWTT #) = [i..j] and I(W , BWTT #) = [i
..j

],
let v be the node of STT # such that I(v, BWTT #) = [i..j], and let v be the node of STT # such
that I(v
, BWTT #) = [i
..j

]. We plug the countSmaller support provided by Lemma 6.6 in the
construction of the BWT index described in Theorem 6.4, and we build the corresponding data
structures on both BWTT # and BWTT #.
Operation extendLeft(a, (i, j), (i
, j

)) = ((p,q), (p
,q
)) can be implemented as follows: We
compute (p,q) using weinerLink(id(v), a), and we set (p
,q
) = (i + countSmaller(i, j, a),i +
countSmaller(i, j, a) + q − p).
To support isLeftMaximal, we build a bitvector runs[2..n + 1] such that runs[i] = 1 if and only
if BWTT #[i]  BWTT #[i − 1]. We build this vector by a linear scan of BWTT #, and we index it to
support rank queries in constant time. We implement isLeftMaximal(i, j) by checking whether
there is a one in runs[i + 1..j], i.e., whether rank1 (runs, j) − rank1 (runs,i) ≥ 1. This technique
was already described in, e.g., References [45, 62].
Assuming that W is right-maximal, we support contractLeft((i, j), (i
, j

)) = ((p,q), (p
,q
))
as follows: Let W = aV for some a ∈ [0..σ] and V ∈ [1..σ]
∗. We compute (p,q) = I(V, BWTT #)
using operation suffixLink(id(v)) described in Lemma 3.13, and we check the result of operation
isLeftMaximal(p,q): If V is not left-maximal, then (p
,q
) = (i
, j

), otherwise V is the label of an
internal node of STT #, and this node is the parent of v
.
To implement enumerateLeft(i, j), we first check whether isLeftMaximal(i, j) returns true;
otherwise, there is just character BWTT #[i] to the left of W in T #. Recall that operation
rangeDistinct(i, j) on BWTT # returns the distinct characters that occur in BWTT #[i..j] as a sequence a1,..., ah, which is not necessarily sorted lexicographically. Note that characters a1,..., ah
are precisely the distinct right-extensions of stringW in T #: SinceW is left-maximal, we have that
W = (v
), and a1,..., ah are the labels associated with the children of v in STT #. Thus, if we
had an MMPHF f v
that maps a1,..., ah to their rank among the children of v in STT #, we could
sort the output of rangeDistinct(i, j) in linear time. We can build the implementation of f v
for
all internal nodes v of STT # using the enumeration algorithm described in Theorem 4.3, and by
applying to array chars of repr((v
)) the implementation of the MMPHF described in
Lemma 3.14. Since every character in every chars array can be charged to a distinct node of
STT #, the set of all such MMPHF implementations takes O(n log log σ ) bits of space, and building
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.    
17:40 D. Belazzougui et al.
it takes randomized O(n) time and O(σ log σ ) bits of working space. Operation enumerateLeft
can be combined with extendLeft to return intervals in addition to distinct characters.
We support enumerateRight, isRightMaximal, contractRight, and extendRight symmetrically.
Operation contractLeft (respectively, contractRight) can be supported with the same time
and space complexities also for strings that are not right-maximal (respectively, not left-maximal),
using simple properties of maximal repeats [10]. Before describing the construction of other indexes, we note that the constant-time countSmaller support of Lemma 6.6, combined with the
enumeration algorithm of Lemma 4.2, enables an efficient way of building BWTT # from BWTT #:
Lemma 6.8. LetT ∈ [1..σ]
n be a string. Given BWTT #, indexed to support rangeDistinct queries
in constant time per element in their output, and countSmaller queries in constant time, we can build
BWTT # in O(n) time and in O(σ2 log2 n) bits of working space, and we can build BWTT # from left to
right, in O(n) time and in O(λT · σ2 logn) bits of working space, where λT is defined in Section 2.2.
Proof. We use Lemma 4.2 to iterate over all right-maximal substrings W of T , and we use
countSmaller queries to keep at every step, in addition to repr(W ), the interval ofW in BWTT #,
as described in Theorem 6.7.
Let a ∈ [1..σ], let I(W , BWTT #) = [i..j], and let I(aW , BWTT #) = [i
..j

]. Recall that [i
..j

] ⊆
[i..j], and that we can test whether aW is right-maximal by checking whether gamma[a] > 1 in
Lemma 4.1. If aW is not right-maximal, i.e., if the Weiner link labelled by a from the locus of W
in STT # is implicit, then BWTT #[i
..j

] is a run of character A[a][1], where A is the matrix used
in Lemma 4.1. If aW is right-maximal, then it will be processed in the same way as W during the
iteration, and its corresponding interval [i
..j

] in BWTT # will be recursively filled.
To build BWTT # from left to right, it suffices to replace the traversal strategy of Lemma 4.2,
based on the logarithmic stack technique, with a traversal based on the lexicographic order of the
left-extensions of every right-maximal substring. This makes the depth of the traversal stack of
Lemma 4.2 become O(λT ).
Contrary to the algorithm described in Reference [59], Lemma 6.8 does not need T and SAT # in
addition to BWTT #.
We also note that a fast bidirectional BWT index, such as the one in Theorem 6.7, enables a
number of applications, which we will describe in more detail in Section 7. For example, we can
enumerate all the right-maximal substrings of T as in Section 4, but with the additional advantage
of providing access to their left extensions in lexicographic order:
Lemma 6.9. Given the bidirectional BWT index ofT ∈ [1..σ]
n described in Theorem 6.7, there is an
algorithm that solves Problem 1 inO(n) time, and inO(σ log2 n) bits of working space andO(σ2 logn)
bits of temporary space, where the sequence a1,..., ah of left-extensions of every right-maximal string
W is in lexicographic order.
Proof. By adapting Lemma 4.2 to use operations enumerateLeft, extendLeft, and
isRightMaximal provided by the bidirectional BWT index. The smaller working space with respect to Lemma 4.2 derives from the fact that the representation of a stringW is now the constantspace pair of intervals (I(W ,T #), I(W ,T #)).
6.4 Building the Permuted LCP Array
We can use the bidirectional BWT index to compute the permuted LCP array as well:
Lemma 6.10. Given the bidirectional BWT index of T ∈ [1..σ]
n described in Theorem 6.7, we can
build PLCPT # in O(n) time and in O(logn) bits of working space.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.   
Linear-time String Indexing and Analysis in Small Space 17:41
Proof. We scan T  = T # from left to right. By inverting BWTT #, we know in constant time the
position ri in BWTT # that corresponds to every position i in T #. Assume that we know PLCP[i]
and the interval of aW = T [i..i + PLCP[i] − 1] in BWTT # and in BWTT #, where a ∈ [1..σ]. Note
that aW is right-maximal; thus, we can take the suffix link from the internal node of the suffix
tree of T # labeled by aW to the internal node labeled by W , using operation contractLeft. Let
([x..y],[x 
..y
]) be the intervals ofW in BWTT # and in BWTT #, respectively. Ifi = 0 or PLCP[i] = 0,
then rather than taking the suffix link from aW , we setW = ε, x = x  = 1 and y = y = n + 1. Since
PLCP[i + 1] ≥ PLCP[i] − 1, we set PLCP[i + 1] to its lower bound |W |. Then, we issue:
([x..y],[x 
..y
]) ← extendRight(T 
[i + PLCP[i]],[x..y],[x 
..y
])
and we check whether x = ri+1; if this is the case, we stop, since neither W · T 
[i + PLCP[i]] nor
any of its right-extensions are prefixes of the suffix at position ri+1 − 1 in BWTT #. Otherwise, we
increment PLCP[i + 1] by one and we continue issuing extendRight operations with the following
character of T 
. At the end of this process, we know the interval of T 
[i + 1..i + 1 + PLCP[i + 1] −
1] in BWTT # and BWTT #; thus, we can repeat the algorithm from position i + 2.
This algorithm can be easily adapted to compute the distinguishing statistics array of a string T
given its bidirectional BWT index, and to compute the matching statistics array of a string T 2 with
respect to a string T 1, given the bidirectional BWT index of T 1#1T 2#2 (see Section 7.1).
6.5 Building the Compressed Suffix Tree
The compressed suffix tree of a string T ∈ [1..σ]
n−1 [69], abbreviated to CST in what follows, is an
index that consists of the following elements:
(1) The compressed suffix array of T #.
(2) The topology of the suffix tree of T #. This takes 4n + o(n) bits of space, but it can be
reduced to 2.54n + o(n) bits [28].
(3) The permuted LCP array of T #, which takes 2n + o(n) bits of space [69].
The CST is designed to support the same set of operations as the suffix tree. Specifically, all operations that involve just the suffix tree topology can be supported in constant time, including
taking the parent of a node and the lowest common ancestor of two nodes. Most of the remaining
operations are instead supported in time t, i.e., in the time required for accessing the value stored
at a given suffix array position. Some operations are supported by augmenting the CST with other
data structures; for example, following the edge that connects a node to its child with a given label
(and returning an error if no such edge exists) needs additional O(n log log σ ) bits, and runs in t
time. Some operations take even more time; for example, string level ancestor queries (defined in
Section 6.5) need additional o(n) bits of space, and are supported in O(t log logn) time.
By just combining Lemma 6.10 with Theorems 6.7, 6.1, 4.4, and 5.3, we can prove the key result
of Section 6:
Theorem 6.11. Given a string T = [1..σ]
n, we can build the three main components of the compressed suffix tree (i.e., the compressed suffix array, the suffix tree topology, and the permuted LCP
array) in randomized O(n) time and in O(n log σ ) bits of working space. Such components take overall O(n log σ ) bits of space.
A number of applications of the suffix tree depend on the following additional operations:
stringDepth(id(v)), which returns the length of the label of a node v of the suffix tree;
blindChild(id(v), a), which returns the identifier of the child w of a node v of the suffix tree
such that (v,w) = aW for some W ∈ Σ∗ and a ∈ Σ, and whose output is undefined if v has no
outgoing edge whose label starts with a; child(id(v), a), which is analogous to blindChild but
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
17:42 D. Belazzougui et al.
returns ∅ if v has no outgoing edge whose label starts with a; and stringAncestor(id(v),d),
which returns the locus of the prefix of length d of (v). The latter operation is called string level
ancestor query. Operation stringDepth can be supported inO((logϵ
σ n)/ϵ ) time using just the three
main components of the compressed suffix tree. To support blindChild and child, we need the
following additional structure:
Lemma 6.12. Given a string T = [1..σ]
n, we can build in randomized O(n) time and in O(n log σ )
bits of working space, a data structure that allows a compressed suffix tree to support operation
blindChild in constant time, and operation child in O((logϵ
σ n)/ϵ ) time. Such data structure takes
O(n log log σ ) bits of space.
Proof. We build the following data structures, described in Reference [11]. We use an array
nChildren[1..2n − 1], of (2n − 1) log σ bits, to store the number of children of every suffix tree
node, in preorder, and we use an array labels[1..2n − 2] of (2n − 2) log σ bits to store the sorted
labels of the children of every node, in preorder. We enumerate the BWT intervals of every rightmaximal substring W of T , as well as the number k of distinct right-extensions of W , using Theorem 4.3. We convert range(W ) into the preorder identifier i of the corresponding suffix tree node
using the tree topology, and we set nChildren[i] = k. Then, we build the prefix-sum data structure of Lemma 3.2 on array nChildren (recall that this structure takes O(n) bits of space), and we
enumerate again the BWT interval of every right-maximal substring W of T , along with its rightextensions b1,b2,...,bk , using Theorem 4.3. For every such W , we set labels[i + j] = bj for all
j ∈ [0..k − 1], where i is computed from the prefix-sum data structure. Finally, we scan nChildren
and labels using pointersi and j, respectively, both initialized to one, we iteratively build a monotone minimal perfect hash function on labels[j..j + nChildren[i] − 1] using Lemma 3.15, and we
set i to i + 1 and j to j + nChildren[i]. All such MMPHFs fit inO(n log log σ ) bits of space and they
can be built in randomized O(n) time.
The output of blindChild can be checked in O((logϵ
σ n)/ϵ ) time using the compressed suffix
array and the stringDepth operation, assuming we store the original string. Finally, the data
structures that support string level ancestor queries can be built in deterministic linear time and
in O(n log σ ) bits of space:
Lemma 6.13. Given the compressed suffix tree of a string T = [1..σ]
n, we can build in O(n) time
and in O(n log σ ) bits of working space, a data structure that allows the compressed suffix tree to
answer stringAncestor queries in O(((logϵ
σ n)/ϵ ) · log logn) time. Such data structure takes o(n)
bits of space.
Proof. We call depth of a node the number of edges in the path that connects it to the root,
and we call height of an internal node v the difference between the depth of the deepest leaf in the
subtree rooted at v, and the depth of v. To build the data structure, we first sample a node every
Ω(b) nodes in the suffix tree. Specifically, we sample a node iff its depth is a multiple of b and its
height is at least b. Note that the number of sampled nodes is at most n/b, since we can associate
at least b − 1 non-sampled nodes to every sampled node. Specifically, let v be a sampled node at
depth ib for some i. If no descendant of v is sampled, then we can assign to v all the at least b
nodes in the path fromv to its deepest leaf. If at least one descendant of v is sampled, then v has at
least one sampled descendantw at depth (i + 1)b, and we can assign to v all the b − 1 non-sampled
nodes in the path from v to w.
We perform the sampling using just operations supported by the balanced parentheses representation of the topology of the suffix tree (see Lemma 3.10). Specifically, we perform a preorder
traversal of the suffix tree topology using Lemma 3.11, we compute the depth and the height of
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
Linear-time String Indexing and Analysis in Small Space 17:43
every node v using operations depth and height provided by the balanced parentheses representation, and, if v has to be sampled, we append pair (id(v), stringDepth(id(v))) to a temporary
list pairs. Building pairs takes O((n/b) · (logϵ
σ n)/ϵ ) time, and pairs itself takes O((n/b) logn)
bits of space. During the traversal, we also build a sequence of balanced parentheses S that encodes the topology of the subgraph of the suffix tree induced by sampled nodes: Every time we
traverse a sampled node from its parent, we append to S an open parenthesis, and every time we
traverse a sampled node from its last child, we append to S a closed parenthesis. At the end of this
process, we build a weighted level ancestor data structure (WLA, see, e.g., Reference [1]) on the
set of sampled nodes, where the weight assigned to a node equals its string depth. To do so, we
build the data structure of Lemma 3.10 on S, and we feed S and pairs to the algorithm described
in Reference [1]. The WLA data structure takes O(n/b) words of space, it can be built in O(n/b)
time, and it answers queries in O(log logn) time. Finally, we build a deterministic dictionary D
that stores the identifiers of all sampled nodes; this dictionary has size O((n/b) logn) bits, it can
be built in O((n/b) logn) time, and it answers queries in constant time [34]. The dictionary and
the WLA data structure are the output of our construction.
We now describe how to answer stringAncestor(id(v),d), omitting details on corner cases
for brevity. We first check whether w, the lowest ancestor of v at depth ib for some i, is sampled;
to do so, we compute e = depth(id(v)), we issue ancestor(id(v),ib), where i = e/b, using the
suffix tree topology, and we query D with id(w). If w is not sampled, then we replace w with its
ancestor at depth (i − 1)b, which is necessarily sampled. If the string depth of w equals d, then we
return id(w). Otherwise, if the string depth of w is less than d, we perform a binary search over
the range of tree depths between the depth of w plus one and the depth of v, using operations
ancestor and stringDepth. Otherwise, we query the WLA data structure to determine u, the
deepest sampled ancestor of w whose depth is less than d, and we perform a binary search over
the range of depths between the depth ofu plus one and the depth ofw, using operations ancestor
and stringDepth. Note that the range explored by the binary search is of size at most 2b, thus
the search takesO(logb) steps andO(logb · ((logϵ
σ n)/ϵ )) time. Setting b = log2 n makes the query
time O(log logn · ((logϵ
σ n)/ϵ )), the time to build pairs O(n), and the space taken by pairs and
by the WLA data structure o(n) bits.
7 STRING ANALYSIS
In this section, we use the enumerators of right-maximal substrings described in Theorems 4.3
and 4.7 to solve a number of fundamental string analysis problems in optimal deterministic time
and small space. Specifically, we show that all such problems can be solved efficiently by just
implementing function callback invoked by Algorithms 2 and 3. We also show how to compute matching statistics and distinguishing statistics (defined below) using a bidirectional BWT
index.
7.1 Matching Statistics
Definition 7.1 ([73, 74]). Given two strings S ∈ [1..σ]
n and T ∈ [1..σ]
m, and an integer threshold
τ > 0, the matching statistics MST,S,τ of T with respect to S is a vector of length m that stores at
index i ∈ [1..m] the length of the longest prefix of T [i..m] that occurs at least τ times in S.
Definition 7.2 ([73, 74]). Given S ∈ [1..σ]
n and an integer threshold τ > 0, the distinguishing
statistics DSS,τ of S is a vector of length |S | that stores at index i ∈ [1..|S |] the length of the shortest
prefix of S[i..|S |] that occurs at most τ times in S.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020. 
17:44 D. Belazzougui et al.
We drop a subscript from DSS,τ whenever it is clear from the context. Note that DSS,τ [i] ≥ 1
for all i and τ . The key additional property of DSS,τ , which is shared by PLCPS#, is called δmonotonicity:
Definition 7.3 ([66]). Let a = a0a1 ... an and δ = δ1δ2 ... δn be two sequences of nonnegative
integers. Sequence a is said to be δ-monotone if ai − ai−1 ≥ −δi for all i ∈ [1..n].
Specifically, MST,S,τ [i] − MST,S,τ [i − 1] ≥ −1 for all i ∈ [2..m], DST,τ [i] − DST,τ [i − 1] ≥ −1
and PLCPT #[i] − PLCPT #[i − 1] ≥ −1 for all i ∈ [2..m + 1]. This property allows all three of these
vectors to be encoded in 2x bits, where x is the length of the corresponding input string [8, 68].
The matching statistics array and the distinguishing statistics array of a string can be built in
linear time from the bidirectional BWT index of Theorem 6.7:
Lemma 7.4. Given a bidirectional BWT index of T ∈ [1..σ]
n that supports every operation in time
linear in the size of its output, we can build DST,τ in O(n) time and in O(logn) bits of working space.
Proof. We proceed as in the proof of Lemma 6.10, scanning T  = T # from left to right. Assume
that we are at position i of T 
, and assume that we know DS[i]. Then, aW = T 
[i..i + DS[i] − 2]
occurs more than τ times in T  and it is a right-maximal substring of T 
. To compute DS[i + 1], we
take the suffix link from the node of the suffix tree of T  that corresponds to aW , using operation
contractLeft, and we issue extendRight operations on stringW using charactersT 
[i + DS[i] −
1], T 
[i + DS[i]], and so on, until the frequency of the right-extension of W drops again below
τ + 1.
Lemma 7.5. Let S ∈ [1..σ]
n and T ∈ [1..σ]
m be two strings. Given a bidirectional BWT index of
their concatenation S#1T #2 that supports every operation in time linear in the size of its output, we
can build MST,S,τ in O(n + m) time and in n + m + o(n + m) bits of working space.
Proof. We use the same algorithm as in Lemma 7.4, scanning T from left to right and checking
at each step the frequency of the current string in S. This can be done in constant time using a
bitvector which[1..n + m + 2] indexed to support rank operations, such that which[i] = 1 iff the
suffix of S#1T #2 with lexicographic rank i starts inside S.
By plugging Theorem 6.7 into Lemmas 7.4 and 7.5, we immediately get the following result:
Theorem 7.6. Let S ∈ [1..σ]
n and T ∈ [1..σ]
m be two strings. We can build DST,τ in randomized
O(m) time and inO(m log σ ) bits of working space, and we can build MST,S,τ in randomizedO(n + m)
time and in O((n + m) log σ ) bits of working space.
Moreover, using Algorithm 3, we can achieve the same bounds in deterministic linear time:
Theorem 7.7. Let S ∈ [1..σ]
n and T ∈ [1..σ]
m be two strings. We can build DST,τ in O(m) time
and in O(m log σ ) bits of working space, and we can build MST,S,τ in O(n + m) time and in O((n +
m) log σ ) bits of working space.
Proof. For simplicity, we describe only how to compute MST,S,1. Note that array MST,S can
be built in linear time from two bitvectors start and end, of size |T | each, where start[i] = 1 iff
MST,S [i] > MST,S [i − 1] − 1, and where end[j] = 1 iff there is an i such that j = i + MST,S [i] − 1.
To build start, we use an auxiliary bitvector start of size |T | + 1, initialized to zeros, and
we run Algorithm 3 to iterate over all right-maximal substrings W of S#1T #2 that occur both in
S and in T . Let repr
(W ) = ({charsS , charsT }, {firstS , firstT }). If charsT \ charsS = ∅, then
we do not process W further and we continue the iteration. Otherwise, for every character b ∈
charsT \ charsS , we enumerate all the distinct characters a that occur to the left of W b in T ,
and their corresponding intervals in BWTT #, as described in Lemma 4.1. If aW is not a prefix of
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
Linear-time String Indexing and Analysis in Small Space 17:45
a rotation of S, then we set to one all positions in start
[i..j], where [i..j] is the interval of aW b
in BWTT #. At the end of this process, we invert BWTT # and we set start[i + 1] = 1 for every i
such that start
[j] = 1 and j is the lexicographic rank of suffix T [i..|T |]# among all suffixes of
T #. Finally, we repeat the entire process using BWTT #, BWTS#, and end
. The claimed complexity
comes from Theorems 5.3 and 4.7.
7.2 Maximal Repeats, Maximal Unique Matches, Maximal Exact Matches
Recall from Section 2 that string W is a maximal repeat of string T ∈ [1..σ]
n if W is both leftmaximal and right-maximal in T . Let {W 1,W 2,...,W occ} be the set of all occ distinct maximal
repeats of T . We encode such set as a list of occ pairs of words (pi
, |W i |), where pi is the starting
position of an occurrence of W i in T .
Theorem 7.8. Given a string T ∈ [1..σ]
n, we can compute an encoding of all its occ distinct maximal repeats in O(n + occ) time and in O(n log σ ) bits of working space.
Proof. Recall from Section 4 the representation repr(W ) of a substring W of T . Algorithm 2
invokes function callback on every right-maximal substring W of T ; inside such function, we
can determine the left-maximality of W by checking whether h > 1, and in the positive case, we
append pair (first[1], |W |) to a list pairs, where first[1] in repr(W ) is the first position of the
interval of W in BWTT # (see Algorithm 5). After the execution of the whole Algorithm 2, we feed
pairs to Lemma 3.1, obtaining in output a list of lengths and starting positions in T that uniquely
identifies the set of all maximal repeats of T .
We build BWTT # from T using Theorem 5.3. Then, we use Lemma 3.5 to build a data structure
that supports access and partialRank queries on BWTT #, and we discard BWTT #. We use this
structure both to implement function LF in Lemma 3.1, and to build the rangeDistinct data
structure of Lemma 3.17. Finally, as described in Theorem 4.3, we implement Lemma 4.2 with
this rangeDistinct data structure. We allocate the space for pairs and for related data structures
in Lemma 3.1 using the static allocation strategy described in Section 3.1. We charge to the output
the space taken by pairs. In Lemma 3.1, we charge to the output the space taken by list translate,
as well as part of the working space used by radix sort.
Maximal repeats have been detected from the input string in O(n log σ ) bits of working space
before, but not in overall O(n) time. Specifically, it is possible to achieve overall running time
O(n log σ ) by combining the BWT construction algorithm described in Reference [39], which runs
in O(n log log σ ) time, with the maximal repeat detection algorithm described in Reference [13],
which runs inO(n log σ ) time. The claim of Theorem 7.8 holds also for an encoding of the maximal
repeats that contains, for every maximal repeat, the starting position of all its occurrences in T . In
this case, occ becomes the number of occurrences of all maximal repeats of T . Specifically, given
the BWT interval of a maximal repeatW , it suffices to mark all the positions inside the interval in
a bitvector marked[1..n], to assign a unique identifier to every distinct maximal repeat, and to sort
the translated list pairs by such identifier before returning it in output. Bitvector marked can be
replaced by a smaller bitvector marked as described in Lemma 3.1.
Once we have the encoding (pi
, |W i |) of every maximal repeat W i
, we can return the corresponding string W i by scanning T in blocks of size logn, i.e., outputting logσ n characters in constant time; this allows us to print the C total characters in the output in overall C/ logσ n time.
Alternatively, we can discard the original string altogether, and maintain instead an auxiliary stack
of characters while we traverse the suffix-link tree in Lemma 4.2. Once we detect a maximal repeat, we print its string to the output by scanning the auxiliary stack in blocks of size logn. Recall
from Section 2.3 that the leaves of the suffix-link tree are maximal repeats; this implies that the
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.  
17:46 D. Belazzougui et al.
ALGORITHM 5: Function callback for maximal repeats. See Theorem 7.8 and Algorithm 2.
Input: repr(W ), |W |, BWTT , and C array of string T ∈ [1..σ]
n−1#. Matrices A, F , L, gamma,
leftExtensions, and counter h, from Lemma 4.1. List pairs.
1 if h < 2 then
2 return;
3 end
4 pairs.append((first[1], |W |));
depth d of the auxiliary stack is at most equal to the length of the longest maximal repeat, thus
the maximum size d log σ of the auxiliary stack can be charged to the output.
A supermaximal repeat is a maximal repeat that is not a substring of another maximal repeat,
and a near-supermaximal repeat is a maximal repeat that has at least one occurrence that is not
contained inside an occurrence of another maximal repeat (see, e.g., Reference [33]). The proof of
Theorem 7.8 can be adapted to detect supermaximal and near-supermaximal repeats within the
same bounds; we leave the details to the reader.
Consider two string S and T . For a maximal unique match (MUM) W between S ∈ [1..σ]
n and
T ∈ [1..σ]
m, it holds that: (1) W = S[i..i + k − 1] and W = T [j..j + k − 1] for exactly one i ∈ [1..n]
and for exactly one j ∈ [1..m]; (2) if i − 1 ≥ 1 and j − 1 ≥ 1, then S[i − 1]  T [j − 1]; (4) if i + k ≤ n
and j + k ≤ m, then S[i + k]  T [j + k] (see, e.g., Reference [33]). To detect all the MUMs of S andT ,
it would suffice to build the suffix tree of the concatenation C = S#1T #2 and to traverse its internal
nodes, since MUMs are right-maximal substrings ofC, like maximal repeats. More specifically, only
internal nodes v with exactly two leaves as children can be MUMs. Let the two leaves of a node
v be associated with suffixes C[i..|C|] and C[j..|C|], respectively. Then, i and j must be such that
i ≤ |S | and j > |S + 1|, and the left-maximality of v can be checked by accessing S[i − 1 (mod1 n)]
and T [j − 1 (mod1 m)] in constant time.
This notion extends naturally to a set of strings: A string W is a maximal unique match (MUM)
of d strings T 1,T 2,...,Td , where Ti ∈ [1..σ]
ni , if W occurs exactly once in Ti for all i ∈ [1..d],
and if W cannot be extended to the left or to the right without losing one of its occurrences. We
encode the set of all maximal unique matches W of T 1,T 2,...,Td as a list of occ triplets of words
(pi
, |W |, id), where pi is the first position of the occurrence of W in string Ti
, and id is a number
that uniquely identifies W . Note that the maximal unique matches of T 1,T 2,...,Td are maximal
repeats of the concatenation T = T 1#1T 2#1 ··· #1Td #2; thus, we can adapt Theorem 7.8 as follows:
Theorem 7.9. Given a set of strings T 1,T 2,...,Td where d > 1 and Ti ∈ [1..σ]
+ for all i ∈ [1..d],
we can compute an encoding of all the distinct maximal unique matches of the set in O(n + occ) time
and in O(n log σ ) bits of working space, where n = d
i=1 |Ti | and occ is the number of words in the
encoding.
Proof. We build the same data structures as in Theorem 7.8, but on string T =
T 1#1T 2#1 ··· #1Td #2, and we enumerate all the maximal repeats of T using Algorithm 2. Whenever we find a maximal repeat W with exactly d occurrences in T , we set to one in a bitvector
intervals[1..|T |] the first and the last position of the interval of W in BWTT (see Algorithm 6).
Note that the BWT intervals of all the maximal repeats ofT with exactly d occurrences are disjoint.
Then, we index intervals to support rank queries in constant time, we allocate another bitvector
documents[1..|T |], and we invert BWTT . Assume that, at the generic step of the inversion, we are
at position i in T and at position j in BWTT . We decide whether j belongs to the interval of a maximal repeat with d occurrences, by checking whether rank1 (intervals, j) is odd, or, if it is even,
whether intervals[j] = 1. If j belongs to an interval [x..y] that has been marked in intervals,
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.   
Linear-time String Indexing and Analysis in Small Space 17:47
ALGORITHM 6: First callback function for maximal unique matches. See Theorem 7.8 and
Algorithm 2.
Input: repr(W ), |W |, BWTT , and C array of string T ∈ [1..σ]
n−1#. Matrices A, F , L, gamma,
leftExtensions, and counter h, from Lemma 4.1. Bitvector intervals[1..|T |].
1 if h < 2 or first[|first|] − first[1]  d then
2 return;
3 end
4 intervals[first[1]] ← 1;
5 intervals[first[|first|] − 1] ← 1;
ALGORITHM 7: Second callback function for maximal unique matches. See Theorem 7.9 and
Algorithm 2.
Input: repr(W ), |W |, BWTT , and C array of string T ∈ [1..σ]
n−1#. Matrices A, F , L, gamma,
leftExtensions, and counter h, from Lemma 4.1. Bitvector intervals[1..|T |]. List pairs.
Integer id.
1 if h < 2 or first[|first|] − first[1]  d or intervals[first[1]]  1
or intervals[first[|first|] − 1]  1 then
2 return;
3 end
4 for i ∈ [first[1]..first[|first|] − 1] do
5 pairs.append
(i, |W |, id)

;
6 id ← id + 1;
7 end
then we compute x using rank queries on intervals, and we set documents[x + p − 1] to one,
where p is the identifier of the document that contains position i in T . Finally, we scan bitvectors intervals and documents synchronously; for each interval [x..y] that has been marked
in intervals and such that documents[i] = 0 for some i ∈ [x..y], we reset intervals[x] and
intervals[y] to zero. Finally, we iterate again over all the maximal repeats of T with exactly d
occurrences, using Algorithm 2. Let W be such a maximal repeat, with interval [x..y] in BWTT :
if intervals[x] = intervals[y] = 1, we append to list pairs of Theorem 7.8 a triplet (i, |W |, id)
for all i ∈ [x..y], where id is a number that uniquely identifies W (see Algorithm 7). Then, we
continue as in Theorem 7.8.
Maximal exact matches (MEMs) are related to maximal repeats as well. A triplet (i, j, ) is a
maximal exact match (also called maximal pair) of two strings T 1 and T 2 if: (1) T 1[i ... i +  − 1] =
T 2[j ... j +  − 1] = W ; (2) if i − 1 ≥ 1 and j − 1 ≥ 1, then T 1[i − 1]  T 2[j − 1]; (3) if i +  ≤ |T 1 |
and j +  ≤ |T 2 |, then T 1[i + ]  T 2[j + ] (see, e.g., References [3, 33]). We encode the set of all
maximal exact matches of strings T 1 and T 2 as a list of occ such triplets. Since W is a maximal
repeat of T 1#1T 2#2 that occurs both in T 1 and in T 2, we can build a detection algorithm on top of
the generalized iterator of Algorithm 3, as follows:
Theorem 7.10. Given two strings T 1 and T 2 in [1..σ]
+, we can compute an encoding of all their
occ maximal exact matches inO(|T 1 | + |T 2 | + occ) time and inO((|T 1 | + |T 2 |) log σ ) bits of working
space.
Proof. Recall that Algorithm 3 uses a rangeDistinct data structure built on top of the BWT
of T 1, and a rangeDistinct data structure built on top of the BWT of T 2, to iterate over all the
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.              
17:48 D. Belazzougui et al.
right-maximal substrings W of T 1#1T 2#2. For every such W , the algorithm gives to function
callback the intervals of all strings aW b such that a ∈ [1..σ], b ∈ [1..σ], and aW b is a prefix
of a rotation of T 1, and with the intervals of all strings cW d such that c ∈ [1..σ], d ∈ [1..σ], and
cW d is a prefix of a rotation ofT 2. Recall from Section 4 the representation repr
(W ) of a substring
W ofT 1#1T 2#2. Inside function callback, it suffices to determine whetherW occurs in bothT 1 and
T 2, using arrays first1 and first2 of repr
(W ), and to determine whether W is left-maximal in
T 1#1T 2#2, by checking whether h > 1 (see Algorithm 8). If both such tests succeed, then we build
a set X that represents all strings aW b that are the prefix of a rotation of T 1, and a set X2 that
represents all strings cW d that are the prefix of a rotation of T 2:
X1 = { (a,b,i, j) : a = leftExtensions[p], p ≤ h, gamma1
[a] > 0, b = A1
[a][q],
q ≤ gamma1
[a], i = F 1
[a][q], j = L1
[a][q] },
X2 = { (c,d,i

, j

) : c = leftExtensions[p], p ≤ h, gamma2
[c] > 0, d = A2
[c][q],
q ≤ gamma1
[c], i
 = F 2
[c][q], j
 = L2
[c][q] }.
In such sets, [i..j] is the interval of aW b in the BWT ofT 1#, and [i
..j

] is the interval ofcW d in the
BWT of T 2#. Building X1 and X2 for all maximal repeats W of T 1#1T 2#2 takes overall linear time
in the size of the input, since every element of X1 (respectively, of X2) can be charged to a distinct
edge or implicit Weiner link of the generalized suffix tree of T 1#1T 2#2, and the number of such
objects is linear in the size of the input (see Observation 1). Then, we use Lemma 7.11 to compute
the set of all quadruplets (i, j,i
, j

) such that (a,b,i, j) ∈ X1, (c,d,i
, j

) ∈ X2, a  c, and b  d, in
overall linear time in the size of the input and of the output, and for every such quadruplet, we
append all triplets (x,y, |W |) to list pairs of Theorem 7.8, where x ∈ [i..j] and y ∈ [i
..j

]. Running
Algorithm 3 and building its input data structures fromT 1 andT 2 takes overall O(|T 1 | + |T 2 |) time
andO((|T 1 | + |T 2 |) log σ ) bits of working space, by combining Theorem 5.3 with Lemmas 3.5, 3.17,
and 4.5.
Finally, we translate every x and y in pairs to a string position, as described in Theorem 7.8. We
allocate the space for pairs and for related data structures in Lemma 3.1 using the static allocation
strategy described in Section 3.1. We charge to the output the space taken by pairs. In Lemma 3.1,
we charge to the output the space taken by list translate, as well as part of the working space
used by radix sort.
Lemma 7.11. Let Σ be a set, and let A and B be two subsets of Σ × Σ. We can compute A ⊗ B =
{(a,b,c,d) | (a,b) ∈ A, (c,d) ∈ B, a  c,b  d} in O(|A| + |B| + |A ⊗ B|) time.
Proof. We assume without loss of generality that |A| < |B|. We say that two pairs (a,b), (c,d)
are compatible if a  c and b  d. Note that, if (a,b) and (c,d) are compatible, then the only elements of Σ × Σ that are incompatible with both (a,b) and (c,d) are (a,d) and (c,b). We iteratively
select a pair (a,b) ∈ A and scan A in O(|A|) = O(|B|) time to find another compatible pair (c,d);
if we find one, we scan B and report every pair in B that is compatible with either (a,b) or (c,d).
The output will be of size |B| − 2 or larger, thus the time to scan A and B can be charged to the
output. Then, we remove (a,b) and (c,d) from A and repeat the process. If A becomes empty, then
we stop. If all the remaining pairs in A are incompatible with our selected pair (a,b)—that is, if
c = a or d = b for every (c,d) ∈ A, then we build subsetsAa andAb whereAa = {(a, x) : x  b} ⊆ A
and Ab = {(x,b) : x  a} ⊆ A. Then, we scan B, and for every pair (x,y) ∈ B different from (a,b),
we do the following: If x  a and y  b, then we report (a,b, x,y), {(a, z, x,y) : (a, z) ∈ Aa, z  y}
and {(z,b, x,y) : (z,b) ∈ Ab , z  x}. Pairs (a,y) ∈ Aa and (x,b) ∈ Ab are the only ones that do
not produce output, thus the cost of scanning Aa and Ab can be charged to printing the result.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.             
Linear-time String Indexing and Analysis in Small Space 17:49
ALGORITHM 8: Function callback for maximal exact matches. See Theorem 7.10 and Algorithm 3.
Operator ⊗ is from Lemma 7.11.
Input: repr
(W ), |W |, {BWTT i #}, {Ci} arrays. Matrices {Ai}, {Fi}, {Li}, {gammai}. Array
leftExtensions and counter h. List pairs.
1 if h < 2 or |chars1 | = 0 or |chars2 | = 0 then
2 return;
3 end
4 X1 ← ∅;
5 X2 ← ∅;
6 for i ∈ [1..h] do
7 a ← leftExtensions[i];
8 if gamma1[a] > 0 then
9 for j ∈ [1..gamma1[a]] do
10 X1 ← X1 ∪ {(a,A1[a][j], F 1[a][j], L1[a][j])};
11 end
12 end
13 if gamma2[a] > 0 then
14 for j ∈ [1..gamma2[a]] do
15 X2 ← X2 ∪ {(a,A2[a][j], F 2[a][j], L2[a][j])};
16 end
17 end
18 end
19 Y ← X1 ⊗ X2;
20 for (i, j,i
, j

) ∈ Y do
21 for x ∈ [i..j], y ∈ [i
..j

] do
22 pairs.append
(x,y, |W |)

;
23 end
24 end
If x = a and y  b, then we report {(z,b, x,y) : (z,b) ∈ Ab }. If x  a and y = b, then we report
{(a, z, x,y) : (a, z) ∈ Aa }.
Theorem 7.10 uses the matrices and arrays of Lemma 4.1 to access all the left-extensions aW
of a string W , and for every such left-extension to access all its right-extensions aW b. A similar
approach can be used to compute all the minimal absent words of a string T . StringW is a minimal
absent word of a string T ∈ Σ+ if W is not a substring of T and if every proper substring of W is
a substring of T (see, e.g., Reference [19]). To decide whether aW b is a minimal absent word of
T , where {a,b} ⊆ Σ, it suffices to check that aW b does not occur in T , and that both aW and W b
occur in T . Only a maximal repeat of T can be the infixW of a minimal absent word aW b: We can
enumerate all the maximal repeats W of T as in Theorem 7.8. Recall also that aW b is a minimal
absent word ofT only if both aW andW b occur inT . We can use repr(W ) to enumerate all strings
W b that occur in T , we can use vector leftExtensions to enumerate all strings aW that occur
in T , and finally, we can use matrix A to discard all strings aW b that occur in T . Algorithm 9 uses
this approach to output an encoding of all distinct minimal absent words of T as a list of triplets
(i, ,b), where each triplet encodes minimal absent word T [i..i +  − 1] · b. Every operation of this
algorithm can be charged to an element of the output, to an edge of the suffix tree of T , or to a
Weiner link. The following theorem holds by this observation and by applying the same steps as
in Theorem 7.8; we leave its proof to the reader.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.     
17:50 D. Belazzougui et al.
ALGORITHM 9: Function callback for minimal absent words. See Theorem 7.12 and Algorithm 2.
Input: repr(W ), |W |, BWTT , and C array of string T ∈ [1..σ]
n−1#. Matrices A, F , L, gamma,
leftExtensions, and counter h, from Lemma 4.1. Bitvector used[1..σ] initialized to all zeros.
List pairs.
1 if h < 2 then
2 return;
3 end
4 for i ∈ [1..|chars|] do
5 used[chars[i]] ← 1;
6 end
7 for i ∈ [1..h] do
8 a ← leftExtensions[i];
9 for j ∈ [1..gamma[a]] do
10 used[A[a][j]] ← 0;
11 end
12 for j ∈ [1..|chars|] do
13 b ← chars[j];
14 if used[b] = 0 then
15 pairs.append
(F [a][1], |W | + 1,b)

;
16 used[b] ← 1;
17 end
18 end
19 end
20 for i ∈ [1..|chars|] do
21 used[chars[i]] ← 0;
22 end
Theorem 7.12. Given a string T ∈ [1..σ]
n, we can compute an encoding of all its occ minimal
absent words in O(n + occ) time and in O(n log σ ) bits of working space.
Recall from Section 2 that occ can be of size Θ(nσ ) in this case. Minimal absent words have been
detected in linear time in the length of the input before, but using a suffix array (see Reference [4]
and references therein).
7.3 String Kernels
Another way of comparing and analyzing strings consists in studying the composition and abundance of all the distinct strings that occur in them. Given two strings T 1 and T 2, a string kernel
is a function that simultaneously converts T 1 and T 2 to composition vectors {T1, T2} ⊂ Rn, indexed by a given set of n > 0 distinct strings, and that computes a similarity or a distance measure between T1 and T2 (see, e.g., References [36, 48]). Value Ti
[W ] is typically a function of the
number fT i (W ) of (possibly overlapping) occurrences of stringW in Ti (for example, the estimate
pi (W ) = fT i (W )/(|Ti |−|W | + 1) of the empirical probability of observingW inTi
). In this section,
we focus on computing the cosine of the angle between T1 and T2, defined as:
κ(T1, T2) =

W T1[W ]T2[W ]

(

W T1[W ]2) (
W T2[W ]2)
.
Specifically, we consider the case in which Ti is indexed by all distinct strings of a given length k
(called k-mers), and the case in which Ti is indexed by all distinct strings of any length:
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 17. Publication date: March 2020.    
Linear-time String Indexing and Analysis in Small Space 17:51
Definition 7.13. Given a string T ∈ [1..σ]
+ and a length k > 0, let vector Tk = [1..σk ] be such
that Tk[W ] = fT (W ) for everyW ∈ [1..σ]
k . The k-mer complexityC(T, k) of stringT is the number
of nonzero components of Tk. The k-mer kernel between two strings T 1 and T 2 is κ(T1
k, T2
k).
Definition 7.14. Given a stringT ∈ [1..σ]
+, consider the infinite-dimensional vector T∞, indexed
by all distinct substrings W ∈ [1..σ]
+, such that T∞[W ] = fT (W ). The substring complexity C(T )
of string T is the number of nonzero components of T∞. The substring kernel between two strings
T 1 and T 2 is κ(T1
∞, T2
∞).
Substring complexity and substring kernels, with or without a constraint on string length, can
be computed using the suffix tree of a single string or the generalized suffix tree of two strings,
using a telescoping technique that works by adding and subtracting terms to and from a sum, and
that does not depend on the order in which the nodes of the suffix tree are enumerated [9]. We
can thus implement all such algorithms as callback functions of Algorithms 2 and 3:
Theorem 7.15. Given a string T ∈ [1..σ]
n, there is an algorithm that computes:
• the k-mer complexity C(T, k) of T , in O(n) time and in O(n log σ ) bits of working space, for a
given integer k;
• the substring complexity C(T ), in O(n) time and in O(n log σ ) bits of working space.
Given two strings T 1 and T 2 in [1..σ]
+, there is an algorithm that computes:
• the k-mer kernel between T 1 and T 2, in O(|T 1 | + |T 2 |) time and O((|T 1 | + |T 2 |) log σ ) bits of
working space, for a given integer k;
• the substring kernel between T 1 and T 2, in O(|T 1 | + |T 2 |) time and in O((|T 1 | + |T 2 |) log σ )
bits of working space.
Proof. To make the article self-contained, we just sketch the proof of k-mer complexity given
in Reference [9]; the same telescoping technique can be applied to solve all other problems (see
Reference [9]).
A k-mer of T is either the label of a node of the suffix tree of T , or it ends in the middle of an
edge (u,v) of the suffix tree. In the latter case, we assume that the k-mer is represented by its
locus v, which might be a leaf. Let C(T, k) be initialized to |T | + 1 − k, i.e., to the number of leaves
that correspond to suffixes of T # of length at least k, excluding suffix T [|T | − k + 2..|T |]#. We use
Algorithm 2 to enumerate the internal nodes of STT #, and every time we enumerate a node v,
we proceed as follows: Let (v) = W . If |W | < k, then we leave C(T, k) unaltered; otherwise, we
increment C(T, k) by one and we decrement C(T, k) by the number of children of v in STT #, which
is equal to |chars| in repr(W ). It follows that every node v of STT # that is located at depth at
least k and that is not the locus of a k-mer is both added to C(T, k) (when the algorithm visits v)
and subtracted from C(T, k) (when the algorithm visits parent(v)). Leaves at depth at least k are
added by the initialization of C(T, k), and subtracted during the enumeration. Conversely, every
locus v of a k-mer of T (including leaves) is just added to C(T, k), because |(parent(v))| < k. The
claimed complexity comes from Theorem 5.3 and Theorem 4.3.
A number of other kernels and complexity measures can be implemented on top of Algorithms 2
and 3 (see Reference [9] for details). Since such iterators work on data structures that can be built
from the input strings in deterministic linear time, all such kernels and complexity measures can
be computed from the input strings in deterministic O(n) time and in O(n log σ ) bits of working
space, where n is the total length of the input strings.   