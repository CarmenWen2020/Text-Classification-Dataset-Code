Consider a forest that evolves via link operations that make the root of one tree the child of a node in another
tree. Intermixed with link operations are nca operations, which return the nearest common ancestor of two
given nodes when such exists. This article shows that a sequence of m such nca and link operations on a
forest of n nodes can be processed online in time O(mα (m,n) + n). This was previously known only for a
restricted type of link operation.
The special case where a link only extends a tree by adding a new leaf occurs in Edmonds’ algorithm
for finding a maximum weight matching on a general graph. Incorporating our algorithm into the implementation of Edmonds’ algorithm in [9] achieves time O(n(m + n logn)) for weighted matching, an arguably
optimum asymptotic bound (n and m are the number of vertices and edges, respectively). Our data structure
also provides a simple alternative implementation of the incremental-tree set merging algorithm of Gabow
and Tarjan [11].
CCS Concepts: • Theory of computation → Data structures design and analysis; Graph algorithms
analysis; • Mathematics of computing → Trees;
Additional Key Words and Phrases: Least common ancestors, matching algorithms, set merging, disjoint-set
data structure
1 INTRODUCTION
Finding nearest common ancestors (ncas) is a basic data structure operation. It is used to solve
many string problems via the suffix tree [4, 12, 18]. It arises in trees that grow by adding leaves,
in the efficient implementation of algorithms for weighted matching and its generalizations [8, 9].
This application is the jumping-off point for this article.
We summarize our algorithms for dynamic ncas starting with the most general version of the
problem. Recall that the nearest common ancestor of two nodes x and y in a rooted tree is the
ancestor of x and y that has greatest depth. Consider a rooted forest F that is subject to two
operations. In both operations, x and y are nodes of F :
link(x,y) – make y a child of x, where y is the root of a tree not containing x;
nca(x,y) – return the nearest common ancestor of x and y;
if x and y are in different trees, return ϵ.
The problem of nearest common ancestors with linking is to process (online) an arbitrary sequence
of m link and nca operations starting from an initial forest of n nodes (m,n ≥ 1).
A number of data structures have been proposed for this problem and various special cases.
See [13] for the early history, which dates back to the offline algorithm of Aho, Hopcroft, and
Ullman [1]. The problem of nearest common ancestors for static trees is when F is given initially
(equivalently, all links precede all ncas). Harel and Tarjan give an algorithm that answers each nca
query in time O(1), after O(n) time to preprocess F [13]. Several algorithms that achieve the same
asymptotic bounds but appear to be simpler have also been given (e.g., [15, 3, 2]). Harel and Tarjan
also give an algorithm for the case where link and nca operations are intermixed but where both
arguments of every link are roots [13]. The running time is O(mα (m,n) + n). In this article, we
remove the restriction on links: We show that the general nearest common ancestors with linking
problem can be solved in time O(mα (m,n) + n) and space O(m + n). The previous best solution
uses dynamic trees [17]. This data structure performs each operation in time O(logn), achieving
total time O(m logn + n). This is not as fast as our algorithm, although dynamic trees have the
advantage that they can process cut operations.
Dynamic ncas arise in Edmonds’ algorithm for weighted matching on graphs [6, 9, 16]. The
algorithm is based on the notion of “blossom,” a type of subgraph that can be contracted in the
search for an augmenting path. Blossoms get contracted using a trivial nca computation in a search
graph. But only the “cheapest blossom” can be contracted in Edmonds’ algorithm. An efficient
matching algorithm must track all possible blossoms and contract only the best. This necessitates
answering an nca query for every possible blossom. Furthermore, the query is made in the search
graph, a forest that grows by adding new leaves as the search progresses.
To be precise, consider the following operations on a rooted tree T . x is a node already in T , and
y is a new node not yet in T :
add_leaf (x,y) – add a new leaf y, with parent x, to T ;
add_root(y) – make the current root of T a child of new root y.
The problem of incremental-tree nearest common ancestors is to process (online) an arbitrary sequence of n add_leaf and add_root operations intermixed with m nca operations, starting with
an empty tree (more precisely, T starts with one node, an artificial root). Edmonds’ algorithm
actually only uses add_leaf and nca operations. We give an algorithm that solves this problem
in O(m + n logn) time. This achieves the desired time bound for Edmonds’ algorithm: It finds a
maximum weight matching in time O(n(m + logn)) [8, 9].1
We refine our nca algorithm to use timeO(m + n). This incremental-tree algorithm becomes the
starting point of our algorithm for general links.
A part of our data structure can be used to implement the incremental-tree set merging algorithm of Gabow and Tarjan [11]. The new implementation does not change the asymptotic performance, but it uses simpler primitive operations that must be precomputed (specifically, mostsignificant-bit, powers of two, and logical and), which might afford advantages in practice.
After the conference version of this article Cole and Hariharan presented a more powerful algorithm for the incremental-tree nca problem. Our algorithm answers each nca query in worst-case
time O(1), but add operations are O(1) only in an amortized sense. Cole and Hariharan achieve
worst-case time O(1) for all operations, even allowing insertion of internal nodes and deletion
1This time bound is optimal in an appropriate model of computation [8, 9]. See [9] for a survey of previous work on
weighted matching.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.
A Data Structure for Nearest Common Ancestors with Linking 45:3
of nodes with ≤ 1 child [4]. The starting point of their algorithm is a version of our approach
presented in Section 2.
2
The model of computation throughout this article is a random access machine with a word size
of logn bits. Harel and Tarjan [13] give a lower bound indicating that it is unlikely that our results
for nearest common ancestors can be achieved on a pointer machine. However, it still might be
possible to achieve our results for Edmonds’ algorithm on a pointer machine.
The rest of the article is organized as follows. This section concludes with some terminology.
Section 2 introduces the basic idea, a generalization of preorder numbering of trees. It solves the
incremental-tree nca problem inO(m + n log2 n) time. Section 3 improves the time bound toO(m +
n logn). This is all that is needed to complete the implementation of Edmonds’ search algorithm in
time O(m + n logn). (Readers interested only in the application to matching need go no further.)
The approach of Section 3 is extended in the next two sections: Section 4 achieves linear time for
the incremental-tree nca problem. Section 5 achieves time O(mα (m,n) + n) to process m nca and
link operations on a set of n nodes.
Some further details are presented in appendices: Appendix A shows how to compute the logarithms needed in Section 2. Appendix B proves a simple lemma for dynamic space allocation.
Appendix C gives our algorithm for incremental-tree set merging. Appendix D proves simple properties of Ackermann’s function used in Section 5. Appendix E extends our algorithm for ncas and
links (Section 5) to allow make_node operations (i.e., the number of nodes is not known in advance).
Terminology. We use interval notation for sets of integers: for i, j ∈ Z, [i..j] = {k ∈Z : i ≤ k ≤ j}.
logn denotes logarithm to the base two.
As usual, we assume a RAM machine does truncating integer division. We compare two rational
numbers a/b and c/d for positive integers a,b,c,d by comparing ad and bc. Assume that for a given
integer r ∈ [1..n] the value log r can be computed in O(1) time. This can be done if we precompute these n values and store them in a table. The precomputation time is O(n). More generally,
for a fixed rational number β > 1 logβ r can be computed in O(1) time (see Appendix A).
We use the following tree terminology. Let T be a tree. V (T ) denotes its vertex set. A subtree of
T is a connected subgraph. The root of T is denoted ρ(T ). Let v be a node of T . The ancestors of v
are the nodes on the path from v to ρ(T ). The ancestors are ordered as in this path. This indicates
how to interpret expressions like “the first ancestor of v such that.” The descendants of v are all
nodes that have v as an ancestor. Tv denotes the subtree of all descendants of v. The parent of v
is denoted π (v). For any function f defined on nodes of a tree, we write fT when the tree T is not
clear from context (e.g., πT (v)).
2 FAT PREORDER FOR DYNAMIC TREES
This section introduces the basic idea for dynamic trees, the fat preorder numbering that generalizes preorder numbering of trees. It starts with an algorithm to find nearest common ancestors on
a tree that is given in advance. Then it extends that algorithm to solve the incremental-tree nca
problem in O(m + n log2 n) time.
Our main auxiliary tree is the compressed tree, so we begin by reviewing the basic definitions
[13, 19]. Let T be a tree with root ρ(T ). The size s(v) of a node v is the number of its descendants.
(As usual, a node is a descendant of itself.) A child w of v is light if s(w) ≤ s(v)/2; otherwise, it
is heavy. Deleting each edge from a light child to its parent partitions the nodes of T into paths
of nonnegative length, called the heavy paths of T . (Thus, an isolated node is considered a heavy
2The reorganize operation defined in Section 2 makes our time bound for adds amortized. Cole and Hariharan do reorganizations piecemeal in future operations, so each operation does constant work [4].
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.
45:4 H. N. Gabow
Fig. 1. Fat preorder interval for vertex v. The two empty intervals guard the preorder numbers of the vertices
of Tv , which are in [p(v),q(v)).
path, and a heavy path has at most one node at each depth.) A node is an apex if it is not a heavy
child (e.g., ρ(T )). Equivalently, an apex is the highest node on its heavy path.
To generalize this, consider an arbitrary partition of V (T ) into a family P of disjoint paths of
nonnegative length. Length 0 paths are allowed, and we require that each path has at most one
node at each depth. Call the highest node on each path its apex. The compressed tree for T and
P, C(T, P), has nodes V (T ); its root is ρ(T ) and the parent of a node v  ρ(T ) is the first proper
ancestor ofv that is an apex. Any apexv has the same descendants inC(T, P) andT , so in particular
sC(T, P) (v) = sT (v). When P consists of the heavy paths of T , we call C(T, P) the compressed tree
(for T ), denoted C(T ). As extreme examples, C(T ) is T if T is a complete binary tree, and C(T ) has
height 1 if T is a path of one or more edges. (In a complete binary tree, every node is an apex; in a
path the apexes are the two ends.)
Let T be an arbitrary tree and let C be its compressed tree C(T ). C has height ≤ logn. This
follows from the fact that in C, the parent v of node w has sC (v) ≥ 2sC (w). (This is clear if w is a
light child in T . If w is a heavy child then sC (w) = 1 and sC (v) ≥ 2.)
For any nodes x,y in the given treeT , we compute ncaT (x,y) by starting with the corresponding
node ncaC (x,y) in the compressed tree C. Harel and Tarjan [13] compute ncaC (x,y) in O(1) time
by embedding C in a complete binary tree B; ncas in B are calculated using the binary expansion
of the inorder numbers of the nodes. Schieber and Vishkin [15] use a similar approach. We now
present a different strategy that seems to give simpler algorithms (see Section 3).
For simplicity, we begin by just discussing the algorithm for ncaC. We then extend that algorithm
to also find the closely related “characteristic ancestors”(see later discussion).
Our main tool, the fat preordering, is defined for special trees C that generalize the compressed
tree. Choose a real-valued constant β > 1 and integers e > 1, c > 2 such that
2
βe−1 − 1
≤ c − 2 ≤ βe . (1)
Note that the left inequality is satisfied when βe−1 ≥ 2 and c ≥ 4, so a convenient choice is e =
β = 2 and c = 4. LetC be equipped with a function3 σ : V (C) → [1..n] such that every node v with
child w satisfies
σ (v) ≥ βσ (w). (2)
For functions p,q,p,q : V (C) → Z+, p is a fat preorder numbering of C if, for any node v, as illustrated in Figure 1,
(i) the descendants of v in C are the nodes w with p(w) ∈ [p(v),q(v));
(ii) no node w has p(w) ∈ [p(v),p(v)) ∪ [q(v),q(v));
(iii) q(v) − p(v) = cσ (v)
e and p(v) − p(v) = q(v) − q(v) = σ (v)
e .
3In this definition, n is not necessarily equal to |V (C)|. Eventually, we will have n either equal to |V (C)| with σ = sC, β =
2 or n ≤ |V (C)| and σ a variant of sC .
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017. 
A Data Structure for Nearest Common Ancestors with Linking 45:5
Note that (i) is equivalent to p being a preorder numbering. Also, the definition allows “guarding
intervals” to overlap; for example, we may have q(w) ∈ [p(v),q(v)) for w not descending from v.
However, our algorithms will maintain the intervals [p(v),q(v)) as a laminar family. Without loss
of generality, we can assume p(ρ(C)) = 0, so q(ρ(C)) ≤ cne and all p-numbers are in [0,cne ). The
fat preorders that we construct take σ to be the size function sC (for static trees) or a close variant
of sC (for dynamic trees).
Given a fat preordering, the following high-level algorithm returns ncaC (x,y):
Let b be the first ancestor of x that has
(c − 2)σ (b)
e > |p(x) − p(y)|.
If b is an ancestor of y (i.e., p(b) ≤ p(y) < q(b)), then return b ; else return πC (b)
(the parent of b).
Lemma 2.1. The ncaC algorithm is correct.
Proof. We first show that any common ancestor d of x and y satisfies
(c − 2)σ (d)
e > |p(x) − p(y)|.
By (i) the interval [p(d),q(d)) contains both p(x) and p(y), so its length is at least 1 + |p(x) − p(y)|.
By (iii), its length is (c − 2)σ (d)
e . The preceding inequality follows.
Now, to prove the lemma we need only show that b	 = πC (b) is a common ancestor. (Clearly,
we can assume b is not the root.) This amounts to showing b	 is an ancestor of y. By (i)–(iii), a
nondescendant of b	 and a descendant of b	 differ in p-number by more than σ (b	
)
e . Using Equation (2) and the right inequality of Equation (1), σ (b	
)
e ≥ βeσ (b)
e ≥ (c − 2)σ (b)
e > |p(x) − p(y)|.
Since x descends from b	
, so must y.
Before implementing the high-level algorithm, we note that computing ncas using the compressed tree (or using other auxiliary trees that we shall see) requires more than just the nca node.
Fix any tree and consider nodes x,y. Let a = nca(x,y). For z = x,y, let az be the ancestor of z immediately preceding a; if a = z then take az = a. Define ca(x,y), the characteristic ancestors of x
and y, as the ordered triplet (a, ax , ay ). Our nca algorithms actually compute ca.
We will use the following data structure. Every vertex x stores an ancestor table,
ancestorx [0.. logβcne ], where
ancestorx [i] is the last ancestor b of x that has (c − 2)σ (b)
e < βi
.
If no such ancestor exists (i.e., (c − 2)σ (x)
e ≥ βi
), then the entry is ϵ.
Figure 2 gives the core of the algorithm for characteristic ancestors. As in the high-level ncaC
algorithm, it computes the node b and converts b to the desired nearest common ancestor a. It also
computes the characteristic ancestor ax , using an auxiliary node bx . We omit the computation of
ay , which is symmetric to ax . We write caC to refer to the entire algorithm that uses Figure 2 to
compute the entire tuple (a, ax , ay ).C continues to refer to an arbitrary tree that has a fat preorder.
Lemma 2.2. The caC algorithm is correct and uses O(1) time.
Proof. We start by showing that the first two lines of Figure 2 make
w = the first ancestor of x that has (c − 2)σ (w)
e ≥ βi
. (3)
The assignment to i makes βi ≤ |p(x) − p(y)|. Thus, the inequality of Equation (3) is weaker than
the definition of b in ncaC, so w exists. Consider two cases for w, first v  ϵ. The definition of
ancestorx [i] shows that v is the last ancestor of x with (c − 2)σ (v)
e < βi
. Thus, w = π (v) satisfies
Equation (3). Next, supposev = ϵ. The definition of ancestorx [i] showsw = x satisfies Equation (3).
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.  
45:6 H. N. Gabow
Fig. 2. The abbreviated caC algorithm.
As mentioned, Equation (3) implies that b is an ancestor of w (possibly b = w). We next show
that b is either w or its parent. Clearly, this holds if w = ρ(C). Suppose w  ρ(C). For w	 = π (w),
Equation (2) and e ≥ 1 show (c − 2)σ (w	
)
e ≥ (c − 2)βeσ (w)
e ≥ βi+1. Since βi+1 > |p(x) − p(y)|,
w	 satisfies the inequality (of the ncaC algorithm) that defines b. So b is either w or w	
.
This implies the test on σ (w) in Figure 2 correctly sets b to w or π (w). The test on b shows a
is set correctly (according to the ncaC algorithm). It remains to analyze ax . Examining the code
shows the following:
When a = π (b), ax is set correctly to b, so assume a = b and ax = bx .
When b = π (w), ax is set correctly to w, so assume b = w.
If v  ϵ, ax is set correctly, since b = w = π (v) and bx = v. So assume v = ϵ.
The code sets w = x and bx = x. So the algorithm now has a = b = w = x and ax = x. Clearly
this is correct.
The time bound of O(1) easily follows. (The value i is computed as in Appendix A.)
Now let C be a tree on n nodes satisfying Equation (2) for σ = s = sC. (An example is the compressed tree, with β = 2.) We show that a fat preordering ofC exists and can be constructed inO(n)
time. We use a recursive numbering procedure. It traverses C top-down. When visiting a node u, u
will have already been assigned an interval [p(u),q(u)) with q(u) − p(u) = cs(u)
e . Initially assign
ρ(C) the interval [0,cne ). Each child of u will get the leftmost possible interval (i.e., the intervals
of u’s children will form a partition of an interval beginning at p(u) + 1). To visit u, execute the
following procedure:
Assign p(u) ← p(u) + s(u)
e and q(u) ← q(u) − s(u)
e . Then assign intervals to the
children of u, starting at p(v) + 1, as follows:
For each child v of u, assign the next interval in [p(u),q(u)) of length cs(v)
e to v,
and then visit v.
Lemma 2.3. The numbering algorithm gives a valid fat preordering with σ = sC when C is a tree
satisfying Equation (2) for σ = sC. The time is O(n) for n = |V (C)|.
Proof. It is clear that the algorithm achieves properties (i)–(iii) of fat preorder, and it runs in
O(n) time. We must show that the intervals assigned by u all fit into the interval [p(u),q(u)) given
to u. For u a leaf this holds, since the interval’s size isc ≥ 3. Assume u is an interior node and letU
denote the set of children of u. Starting with the relation s(u) = 1 +
v ∈U s(v), multiply by s(u)
e−1
and use Equation (2) (and its consequence s(u) ≥ β) to obtain s(u)
e ≥ βe−1 + βe−1
v ∈U s(v)
e . This
implies cs(u)
e /βe−1 ≥ 1 +
v ∈U cs(v)
e . The right-hand side is the total size of intervals assigned
in [p(u),q(u)) (the term 1 accounts for the number p(u)). Since [p(u),q(u)) has length (c − 2)s(u)
e ,
it suffices to have c − 2 ≥ c/βe−1. This is equivalent to the left inequality of Equation (1).
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.       
A Data Structure for Nearest Common Ancestors with Linking 45:7
After numbering the tree in fat preorder, we construct the ancestor tables: For every node x we
find successive entries of ancestorx [i] by traversing the path from x to ρ(C). The time and space
for this is O(n logn) and dominates the entire algorithm.
The last important step in the nca algorithm is a procedure (due to [13]) that computes characteristic ancestors in T from those in the compressed tree C(T ). To state it, let C = C(T, P) for an
arbitrary set of paths P. Suppose the characteristic ancestorscaC (x,y) = (c,cx ,cy ) are known and
we seek the characteristic ancestors caT (x,y) = (a, ax , ay ).
Let P be the path of P with apex c. The definition of C implies that a = ncaT (x,y) is the first
common ancestor of x and y on P. For z ∈ {x,y}, let bz denote the first ancestor of cz on P (i.e., bz
is cz or πT (cz )). Then a is the shallower vertex of bx and by .
Next, we show how to find ax (the same procedure applies to ay ). Consider three cases.
Case a  bx : This makes the predecessor a− of a on P an ancestor of bx (possibly a− = bx ). Thus,
a− is an ancestor of x. Clearly, this makes ax = a−.
Case a = bx  cx : This makes bx = πT (cx ). Combining equations gives a = bx = πT (cx ). So, by
definition, ax = cx .
Case a = bx = cx : We will show
cx = x.
Combining this with the case definition gives a = x. So the definition of caT shows ax = x. (This
also makes ax = cx , as in the previous case.)
Ifcx is a leaf of C, the displayed equation follows since cx is an ancestor of x. Ifcx is not a leaf of
C, then cx is the unique vertex of P that is nonleaf (i.e.,cx = c). The definition ofcaC shows x = c.
Combining gives cx = c = x.
Putting these pieces together gives our algorithm for nca queries on a static tree. Let us summarize the algorithm. A preprocessing step computes the compressed tree C = C(T ). It is numbered
in fat preorder (β = 2). In addition, the order of nodes in each heavy path is recorded (so we can
find a− in the first case described earlier). The ancestor tables for C are constructed. The query
algorithm computes characteristic ancestors by finding caC (x,y) and using it to find caT (x,y).
Lemma 2.4. A tree T with n nodes can be preprocessed using O(n logn) time and space so that ca
queries can be answered in O(1) time.
Note that the preprocessing time and the space are both O(n) except for the resources needed
to compute and store the ancestor tables.
Incremental Trees
We extend these ideas to trees that grow by add_leaf operations. It is then to easy to complete the
incremental tree data structure by incorporating add_root operations.
We start by presenting the high-level strategy. Then we give the data structure and detailed
algorithm, and, finally, we prove that it works correctly.
We use a dynamic version D of the compressed tree, maintaining a fat preordering and computing cas as before. In more detail, D is maintained to be C(T, P) for a time-varying collection
of paths P that always partitions V (T ). add_leaf makes the new leaf a singleton path of P. The
algorithm to maintain D is based on this operation: Let v be an apex of D (i.e., a shallowest vertex
on some path of P). Thus, V (Dv ) = V (Tv ). To recompress v means to replace Dv in D by C(Tv ). As
usual,C(Tv ) is defined using the heavy paths ofTv , and the recompression changes P accordingly.
Each node of Tv gets reorganized in this recompression.
Recompressing v updates the fat preordering of D as follows. Let s be the size function on the
recompressed subtree Dv = C(Tv ). The fat preordering will take σ to be s. The other parameters
of the ordering are specified later. If v = ρ(D), the recompression uses the (static) fat preordering
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.  
45:8 H. N. Gabow
algorithm to assign new numbers to the nodes of Dv in the interval [0,cs(v)
e ). If v  ρ(D), let
u be the parent u = πD (v). Let Q(u) be the currently largest value q(z) for a child z of u in D.
The expansion interval for u is [Q(u),q(u)). Clearly, no numbers have been assigned in this interval. Use the fat preordering algorithm to assign new numbers to the nodes of Tv in the interval
[Q(u),Q(u) + cs(v)
e ). This updates Q(u) to q(v), decreasing the size of u’s expansion interval by
cs(v)
e . The old interval for v, [p(v),q(v)), will no longer be used; in effect, it is discarded.
The last part of the high-level description is based on a parameter α, 3/2 > α > 1. For any node
u let s(u) denote its current size in D and let σ (u) denote the σ-value for u in the current fat
preordering (i.e., u’s interval [p(u),q(u)) has size cσ (u)
e ). s(u) equals σ (u) plus the number of
descendants that u has gained since its reorganization. D is maintained to always have
s(u) < ασ (u)
for every node u.
We turn to the data structure. In D the data structure maintains the values of p,p,q,q,Q,s, and
σ for every vertex u. It also marks the apexes. The D tree is represented by parent pointers. The
tree T is represented by children lists (i.e., each node has a list of its children). Also, each path of
P is recorded (for the ca algorithm).
Now we give the detailed algorithms. The ca algorithm is the same as the static case.
add_leaf (x,y) proceeds as follows:
Add y to the list of children of x inT . Make y a singleton path of P by marking it an
apex and setting πD (y) to x if x is an apex; else πD (x). Increase s(a) by 1 for each
ancestor a of y in D. Letv be the last ancestor of y that now hass(v) ≥ ασ (v). (This
condition holds for v = y by convention.) Recompress v. (Start by computing the
valuessT (z) for z ∈ Tv .) Then construct the new ancestor table for each node ofTv .
Note that if v = y in this algorithm, recompressing v does not change D but assigns v its fat
preorder values.
Finally, we give the parameters for the fat preorder. As we shall show, they are selected so the
preceding strategy can be carried out; in particular, expansion intervals are large enough. Starting
with the preceding parameter α ∈ (1, 3/2), we will use
β = 2
2α − 1
as the constant of Equation (2) and, in addition to Equation (1), we require
c
(α − 1/2)
e + 1/2e
1 − 1/αe ≤ c − 2. (4)
Notice α ∈ (1, 3/2) implies the fraction of the left-hand side approaches 0 as e → ∞,so this inequality can always be achieved. For example, take α = 6/5, β = 10/7, e = 4, c = 5. (Then Equation (1)
amounts to 1.1 ≤ 3 ≤ 4.1 and Equation (4) amounts to 2.93 ≤ 3.) The fat preorder must satisfy the
defining properties (i)–(iii) for these parameters.
To show the algorithm is correct, we must first ensure that expansion intervals are large enough.
More precisely, consider an apex u of D that has just been reorganized. The algorithm adds <
(α − 1)σ (u) descendants of u in D before recompressing Du . The additions may cause various
children v of u to be recompressed and thus assigned new intervals in [p(u),q(u)). We must show
that the total length of all intervals ever assigned to children of u is < q(u) − p(u) = (c − 2)σ (u)
e .
Here, strict inequality accounts for the fact that the integer p(u) is assigned to u. Also note that
the “total length” includes both the original intervals assigned when u is reorganized and the new
intervals.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017. 
A Data Structure for Nearest Common Ancestors with Linking 45:9
We will use the following notation. We continue to let σ (u) denote its value when u is reorganized. Let v be a child of u in D. If v is an apex, it may be recompressed some number of times,
say  times. Let σi (v), i = 0,..., be the various values of σ (v). For example, σ0 (v) is the value of
σ (v) when u is reorganized. For i ≥ 1,
σi (v) ≥ ασi−1 (v). (5)
(If v is not an apex, then  = 0 and σ0 (v) = 1.)
Lemma 2.5. From whenu is reorganized until its next reorganization, the total length of all intervals
ever assigned to children of u is < q(u) − p(u).
Proof. For any child v of u, Equation (5) implies σ (v) ≥ αi
σ−i (v) for every i = 0,...,. Thus,
the total size of all intervals ever assigned to v is strictly less than
cσ (v)
e (1 + (1/α)
e + (1/α)
2e + ...) = cσ (v)
e
1 − 1/αe .
Obviously, this holds for children with  = 0, too. So the total size of intervals ever assigned to
children of u is strictly less than
S = c

v ∈U σ (v)
e
1 − 1/αe .
We can assume u has at least two children when it is initially reorganized. (If u starts with ≤ 1
child, u gets reorganized as soon as it gains a child since ασ (u) < (3/2)σ (u) ≤ σ (u) + 1.) Right
after u was reorganized, every child v had σ0 (v) = s(v) ≤ s(u)/2 = σ (u)/2 descendants in D. u
gets < (α − 1)σ (u) new descendants before reorganization. The sum of all σ (v) values is less than
ασ (u). Since e > 1, simple calculus shows S is maximized when u starts with exactly two children,
each with ≤ σ (u)/2 children, and every new node descends from the same initial child. (For any
initial configuration, S is maximized when all new nodes descend from the child that starts with
the greatest value σ0 (v). This value in turn is maximized when there are only two descendants
and each starts with ≤ σ (u)/2 descendants.) Thus, the maximum value of S is
cσ (u)
e (α − 1/2)
e + 1/2e
1 − 1/αe .
The left inequality of Equation (4) implies S ≤ (c − 2)σ (u)
e as desired.
Now we complete the correctness proof.
Lemma 2.6. The add_leaf and ca algorithms are correct.
Proof. We start by verifying Equation (2); that is, at any time when v is a child of u in D,
the current σ function satisfies σ (u) ≥ βσ (v). Immediately after u was last reorganized we have
σ (u) = s(u) ≥ 2s(v) = 2σ (v). This inequality holds even if v has not been added by add_leafsince
we take σ (v) = 0. After the reorganization, σ (u) does not change, and u gets less than (α − 1)σ (u)
new descendants. Thus, at any time s(v) ≤ σ (u)/2 + (α − 1)σ (u) = (α − 1/2)σ (u). We always have
σ (v) ≤ s(v). Thus, σ (v)/(α − 1/2) ≤ σ (u) (i.e., βσ (v) ≤ σ (u)).
The defining properties (i)–(iii) of fat preorder numbers hold since recompression uses the static
preorder numbering algorithm. (Since n is nondecreasing, the current σ always has values ≤ n.)
The rest of the data structure consists of the ancestor tables and the orderings of the paths of P.
If x ∈ Tv , ancestorx is constructed in entirety in the recompression. If x  Tv the recompression
does not change ancestorx . This table remains valid since no ancestor b of x is in Tv , so σ (b) does
not change. Similarly, a path of P with an apex not in Tv is vertex-disjoint from Tv . So it does not
change when v is recompressed, and the data structure representing it remains valid.
The ca algorithm works correctly since the data structure is correct.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.               
45:10 H. N. Gabow
Lemma 2.7. The nearest common ancestors problem with add_leaf and ca operations can be solved
in O(m + n log2 n) time and O(n logn) space.
Proof. A ca operation uses O(1) time, so the cas use O(m) time total.
In add_leaf (x,y), examining each ancestor of y uses O(logn) time. Recompressing a node v
uses O(s(v)) time for all processing except constructing the new ancestor tables, which uses
O(s(v) logn) time. Hence, the time for an add_leaf operation is O(s(v) logn). The algorithm’s recompression strategy implies s(v) < 1 + ασ (v) ≤ (1 + α)σ (v). So the time is
O(σ (v) logn).
When v is recompressed, ≥ (α − 1)σ (v) descendants y	 of v have been added since the last
reorganization of v. Charge the above time O(σ (v) logn) to these new descendants y	
, at the rate
of O(logn) per node. Since α > 1, this accounts for the time recompressing v.
A given node y	 is charged at most once from a node v that was an ancestor of y	 when add_leaf
added y	
. (In proof, recompressing v reorganizes every new ancestor w of y	
. So y	 will not be
charged fromw. In other words, after y	 is charged fromv, it is only charged from proper ancestors
of v.) Thus, Equation (2) implies any y	 is charged ≤ logβcne times total. So the total time charged
to y	 is O(log2 n). The time bound follows.
For the space bound, note that the ancestor tables use space O(n logn). The remaining space (as
specified in the data structure for D) is linear.
The lemma does not require n (the number of add_leaf operations) to be known in advance.
(This is the case in most of our applications of this algorithm, although not in the implementation
of Edmonds’ algorithm.) The timing analysis still applies verbatim. The use of ancestor tables
necessitates a storage management system to achieve the lemma’s space bound. We use a standard
doubling strategy. Since we use this several times, the following lemma gives a precise statement.
For completeness, the lemma is proved in Appendix B, a simple application of Cormen et al. [5,
Sec. 17.4.1].
Consider a collection of arrays Ai[1..ni] that grow by adding entries. Each such operation enlarges Ai with a new entry (i.e., ni increases by 1 and the contents of old entries in Ai do not
change). We implement this operation by allocating space for all arrays Ai sequentially within a
larger array S. When the current version of Ai in S becomes full, we allocate a new version at the
end of S with twice the size. We will also allow an operation that creates a new array Ai .
Lemma 2.8. A collection of arrays Ai[1..ni],i = 1,..., k that grows by adding new array entries
and creating new arrays can be maintained within an array S[1..4n], for n =
i ni . The extra time is
O(n).
Returning to Lemma 2.7, when the final value of n is unknown, note that the space usage consists
of ancestor tables and single values associated with each vertex. (The children lists forT are stored
as vertex values: v points to its first child, and each child of v points to the next child. The paths
of P are also stored as vertex values pred(v): a vertex v on a heavy path has pred(v) equal to the
predecessor of v on the heavy path.) The single values are updated in add_leaf operations and
recompressions. Lemma 2.8 is used to manage all the space (including single vertex values). We
conclude that Lemma 2.7 holds in entirety when n is not known in advance.
Now we extend these algorithms to allow add_root operations in addition to add_leaf. We show
that the general incremental tree problem reduces to add_leaf and ca operations. First extend the
characteristic ancestor operation. For an arbitrary node r of T , let nca(x,y;r) denote the nearest
common ancestor of x and y when T is rerooted at vertex r. Define ca(x,y;r) similarly. All other
terminology is unchanged (e.g., ca(x,y) denotes the characteristic ancestors in T with its original
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.  
A Data Structure for Nearest Common Ancestors with Linking 45:11
root and similarly for the term “ancestor.”) The following lemma shows that we can compute
ca(x,y;r) just using the ca functions on T .
Lemma 2.9. (i) Any 3 nodes in a tree x,y, z have |{nca(x,y),nca(x, z),nca(y, z)}| ≤ 2.
(ii) ca(x,y; z) =

ca(x,y) if nca(x, z) = nca(y, z),
(a, π (a), ay ) if nca(x, z) = nca(x,y)  nca(y, z) and ca(y, z) = (a, ay, az ).
Remark: Part (i) and symmetry of x and y show that part (ii) gives a complete definition of
ca(x,y; z).
Proof. (i) Let a be the shallowest of the three nodes nca(x,y), nca(x, z), nca(y, z). So wlog
a = nca(x,y), and letca(x,y) = (a, ax , ay ). If a  nca(x, z) then nca(x, z) is an ancestor of x deeper
than a. So ax  a, and z descends from ax . Thus, the path from y to z goes through a and ax , so
a = nca(y, z).
(ii) Let ca(x, z) = (b,bx ,bz ).
Suppose nca(x, z) = nca(y, z). b = nca(x, z) is an ancestor of y. If bx  b and bx is an ancestor of y, then nca(y, z) descends from bx , contradicting b = nca(y, z). Thus, b = nca(x,y) and
b = nca(x,y; z). Clearly, ca(x,y; z) = ca(x,y).
Next suppose nca(x, z) = nca(x,y)  nca(y, z). The equality implies b is an ancestor of y, and
the inequality implies b  bz and bz is an ancestor of y. Thus, nca(x,y; z) = nca(y, z). Letca(y, z) =
(a, ay, az ). Then ca(x,y; z) = (a, π (a), ay ).
It is now a simple matter to implement add_root operations in terms of add_leaf: We never
change the children lists of the data structure representing T . Instead, we maintain a pointer ϱ
that gives the current root of T as defined by add_root operations. The operation add_root(y) is
implemented as
add_lea f (ϱ,y); ϱ ← y. (6)
The algorithm for ca(x,y) is ca(x,y; ϱ).
We close this section by noting that add_root can be implemented directly, without the general
reduction. The main observation is that add_root changes the compressed tree in a simple way:
Let T 	 be the result of performing add_root(y) on T , a tree with root x. If |V (T )| > 1, then y plus
the heavy path with apex x in T forms a heavy path in T 	
. Thus, C(T 	
) can be constructed from
C(T ) by changing the name of the root from x to y and giving the root a new child named x. (This
works when |V (T )| = 1, too.) This transformation is easily implemented in our data structure.
Corollary 2.10. The nearest common ancestors problem with add_leaf, add_root, and ca operations can be solved in O(m + n log2 n) time and O(n logn) space.
As before, the corollary does not require that n be known in advance.
3 NCAS FOR EDMONDS’ ALGORITHM
This section gives a simple algorithm to find the ncas needed in Edmonds’ matching algorithm.
Each nca operation uses O(1) time, and the total time for all add_leaf ’s is O(n logn). The extra
space is O(n). So this completes our efficient implementation of the weighted matching algorithm.
(Readers interested only in matching needn’t go beyond this section.)
This section also introduces the multilevel approach, wherein the incremental tree algorithm
is used on a number of trees derived from the given tree, each at a given “level.” We use three
versions of the approach. The simplest is for Edmonds’ algorithm, and the two more elaborate
versions are presented in the next two sections.
Finally, the bitstring data structure presented in this section is applied in Appendix C to show a
simple implementation of the incremental-tree set merging algorithm of Gabow and Tarjan [11].
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.        
45:12 H. N. Gabow
Fig. 3. (a) Data structure for ncas in Edmonds’ algorithm. μ = logn. y is a 1-node, z is nonfull. (b) Generalization to L levels, TL = T , L ≥  > 1.
The idea is to reduce the number of tree nodes by contracting small subtrees. The terms vertex
and the incremental treeT refer to the objects of the given problem (e.g., an operation add_leaf (x,y)
makes vertex x the parent of vertex y in T ). We use two trees, illustrated in Figure 3(a). T2 is
the incremental tree T , enhanced with its data structure. T1 is a smaller version of T , derived by
contractions and deletions.
This indexing is a special case of our second multilevel structure, illustrated in Figure 3(b): A
tree T is represented on L > 1 levels, with T = TL; and, for level  ∈ [2..L], tree T−1 is a minor of
T. Edmonds’ algorithm uses L = 2.
For Edmonds’ algorithm, define μ = logn. The algorithm maintains a partition of the vertices
ofT = T2 into subtrees of ≤ μ vertices called 2-subtrees. A 2-subtree containing exactly μ vertices is
full. T1 is formed fromT = T2 by discarding the nonfull 2-subtrees and contracting the full ones. A
node of T1 (i.e., a contracted 2-subtree) is called a 1-node. (Figure 3(b) illustrates the general case.)
We use this additional notation, illustrated in Figure 3: For any vertex x, xdenotes the 2-subtree
containing x. As an example, 2-subtrees are created and maintained in add_leaf (x,y) as follows:
If xis a full 2-subtree, then y is made the root of a new 2-subtree. Otherwise, y is added to x. If
this makes xfull, then a 1-node is created for x. It is added to T1 either as the root if x = T or with
parent the 1-node containing π (ρ(x)). (Note that this description guarantees that T1 is a tree and
not a forest.)
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.            
A Data Structure for Nearest Common Ancestors with Linking 45:13
Fig. 4. Finding ncas in Edmonds’ algorithm.
When x is full, →−x denotes the 1-node containing x.
4 If x is a 1-node (i.e., the contraction of a
2-subtree S), ←−x denotes the root vertex of S (
←−x is a vertex of T2 = T ). Also note that we write
functions of nodes like π (x) and ca(x,y), relying on context (i.e., the identity of arguments x and
y) to indicate which tree Ti is being used.
T1 is processed using the incremental-tree nca algorithm of Lemma 2.7. Clearly, there are
O(n/ logn) 1-nodes, so the time spent on T1 is O(m + n logn) and the space is O(n).
T uses a simple data structure: Each root x of a 2-subtree is marked as such and stores the size
of its tree |V (x)|. If xis full, then x has a pointer to its 1-node y =→−x ; also, y has a pointer to x = ←−y .
Each nonroot has a pointer to its 2-subtree root. Each node x of T has a parent pointer as well as
child pointers; the children of x that belong to xall occur before the children not in x.
Figure 4 gives the algorithm for nca(x,y). Note that the ca operation takes place in T1 and the
nca operation is in a 2-subtree, in T2.
We complete the data structure by showing how to process add_leaf and nca operations in 2-
subtrees. We do this by maintaining a representation of ancestors as bitstrings in trees that grow
by add_leaf operations, assuming their size remains ≤ logn. Edmonds’ algorithm uses this data
structure on every 2-subtree. The details of the data structure are as follows.
Let T be a tree that grows by add_leaf operations, of size at most |V (T )| ≤ logn. The nodes of
T are numbered sequentially as they get added, starting at 1. The number of node x is stored as its
“identifier” id[x] ∈ [1..|V (T )|]. T also has an array v[1..|V (T )|] that translates identifiers to their
corresponding node (i.e., v[i] specifies the vertex of T whose identifier is i).
Each vertex x ∈ T has a RAM word anc[x] that stores a string of ≤ logn bits. The ith bit of
anc[x] (corresponding to 2i
) is 1 if and only if node number i is an ancestor of x. So, for example,
bits 1 and id[x] are always 1. The key property is that reading the bits most-significant-first gives
the ancestors of x in their proper order (i.e., decreasing depth).
For add_leaf , we maintain a value s as the current size of T . add_leaf (x,y) is implemented as
π (y) ← x; id[y], s ← s + 1; v[s] ← y; anc[y] ← anc[x] + 2s .
We precompute a table that gives most-significant bits. Specifically, for any bitstring b  0 of logn
bits, msb[b] is the index of the most significant bit of b. The operation nca(x,y) is implemented as
v [msb [anc[x] ∧ anc[y]]].
It is easy to see that add_leaf and nca both use O(1) time. To use this data structure in
Edmonds’ algorithm, we keep space usage linear by using the doubling strategy of Lemma 2.8 on
the collection of v arrays of all 2-subtrees.
Using the results of Gabow [9], which leaves the incremental-tree nca problem as the last detail
of Edmonds’ algorithm, we get the following:
4The forward arrow notation corresponds to the arrows in Figure 3.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.      
45:14 H. N. Gabow
Theorem 3.1. A search of Edmonds’ algorithm can be implemented in time O(m + n logn) and
space O(m).
The bitstring data structure is used in the next section, where we need the complete ca vector and
not just nca. To accomplish this, we precompute another table for least-significant bit. Specifically,
for any bitstring b  0 of logn bits, lsb[b] is the index of the least significant bit of b. Recalling
ca(x,y) = (nca(x,y), ax , ay ), node ax is found by
ax ← if nca(x,y) = x then x else v [lsb (anc[x] ∧ ¬anc[y])],
and similarly for ay .
4 MULTILEVEL INCREMENTAL-TREE ALGORITHMS
This section begins by giving the details of the multilevel approach illustrated in Figure 3(b). Then
it presents a 3-level algorithm to solve the incremental-tree nca problem in time O(1) for nca
queries, total time O(n) for add_leaf and add_root operations, and space O(n). That algorithm is
used in the next section to construct our most general nca algorithm. It uses the multilevel structure
presented in this section, with an unbounded number of levels and some changes that we note.
The Framework
This section gives the high-level organization of an incremental nca algorithm with an arbitrary
number of levels. We will only need three levels in the next section, but the number of levels is
unbounded in Section 5.
The terms vertex and the incremental tree T refer to the objects of the given problem (e.g., an
operation add_leaf (x,y) makes vertex x the parent of vertex y inT ). A multilevel algorithm works
on a number of levels designated  = L, L − 1,..., 1.
The incremental tree T is represented by a tree T on every level (a small tree T may have T
empty for levels  less than some threshold). TL is T . Every other T is a smaller tree derived from
T+1 by deletions and contractions. Each T is composed of -nodes (called nodes if the level is
clear). The algorithm maintains a partition of the nodes of T into subtrees called -subtrees. Each
level is provided with given algorithms that solve the incremental problem in any -subtree; the
multilevel algorithm described in this section sews these given algorithms together to solve the
incremental problem on the given tree T .
Every level  has an integral size parameter μ.
5 Every -subtree S contains ≤ μ -nodes. S is
full if equality holds. μ1 = n + 1, so T1 is always nonfull (if it exists). For L ≥  > 1, T−1 is formed
from T by discarding every nonfull -subtree and contracting the full ones. The fact that any T
is a tree (i.e., not a forest) follows from this invariant: For every level , the nonfull -subtrees of
T are at its frontier (i.e., any node x in a nonfull -subtree S has all its T-children in S).
Efficiency in a multilevel algorithm is achieved using the shrinkage of the tree from level to
level. Specifically, an -node with  < L contains ΠL
+1μi vertices of T . So the number of -nodes is
|V (T )| ≤ n/ΠL
+1μi . (7)
We use this additional notation: Let x be an -node, L ≥  ≥ 1. xdenotes the -subtree containing
x. If x is full (in particular  > 1), then→−x denotes the ( − 1)-node that is the contraction of x. If
 < L, then x is the contraction of an ( + 1)-subtree S, and ←−x denotes the root node of S. As
before, we write functions of nodes like π (x), relying on context (i.e., the identity of argument x)
to indicate which tree T is referenced.
5In Section 5, these size parameters are replaced by a notion of “stage.”
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.                                            
A Data Structure for Nearest Common Ancestors with Linking 45:15
Fig. 5. Procedure for c(x,y, ) when xand yare full.
Each T uses this data structure: Let x be an -node. If x is the root of its -subtree, it stores the
subtree size |V (x)| (1 ≤ |V (x)| ≤ μ). It also has pointers to ←−x if  < L and→−x if  > 1. A nonroot
x has a pointer to its -subtree root. Every x has a parent pointer.
The main routines for incremental trees are
a(x,y, ), a (x,y, ), c(x,y, ), c (x,y, ).
a(x,y, ) is a recursive routine that performs the add_leaf operation for -node x and new -node
y. It may call a(
→−x ,
→−y ,  − 1). For vertices x,y in the given graph, the operation add_leaf (x,y) is
performed by a(x,y, L). The a routine makes use of a to grow -subtrees. Specifically for a (x,y, ),
x is an -node and y a new -node that is made a child of x in the -subtree x.
The ca operation is organized similarly. It uses the recursive routine c(x,y, ), which returns
the characteristic ancestors of -nodes x and y, possibly invoking c(
→−x ,
→−y ,  − 1). For vertices x,y
in the given graph, the operation nca(x,y) is performed by c(x,y, L). The c routine makes use of
c , which returns the characteristic ancestors of -nodes x and y that belong to the same -subtree
x = y.
We will extend these operations later to allow add_root. Also looking ahead, in Section 5, the
nca routine will use c and c with some obvious modifications. The link routine will use the same
overall structure as a and a for add_leaf .
Now we describe the two recursive algorithms starting with a(x,y, ):
Case xis full: Make x the parent of node y, and make y a singleton -subtree.
Case xis nonfull: Execute a (x,y, ). If xis still not full, we are done; but suppose
xhas become full. Create a new ( − 1)-node z. Make z the node→−x . Now there are
two subcases:
Subcase x = T: Make z the unique ( − 1)-node, as well as a singleton ( − 1)-
subtree.
Subcase x  T: w = π (ρ(x)) is in a full -subtree. Execute a(
→−
w, z,  − 1) to add z
as a new ( − 1)-leaf.
The add_leaf algorithm preserves the defining properties of T trees and so is correct. The total
time for all add_leaf operations is dominated by the time used by a to build all the -subtrees,
L ≥  ≥ 1. (This includes the time to create a new singleton subtree.)
We turn to the c routine. The high-level strategy is simple: Use c(
→−x ,
→−y ,  − 1) to find the -
subtree S containing nca(x,y), and then use the c routine in S to find the desired level  characteristic ancestors (c,cx ,cy ). There are various special cases, depending on whether or not xand y
are full, whether or not S contains cx or cy , etc. The details of c(x,y, ) are as follows.
Case xandyare both full: Figure 5 gives pseudocode for this case. It handles special
cases such as x = y or x = y.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.                                                                           
45:16 H. N. Gabow
Case One or both of x,yis nonfull: If x = y, we use c (x,y, ) directly. (This includes
the special case where there are no ( − 1)-nodes.) Assuming x  y, when x is
nonfull we replace x by π (ρ(x)), and similarly for y. We then execute the code of
Figure 5. If the returned bx is the replacement for x, we change bx to ρ(x), and
similarly for y.
The analysis of this algorithm is similar to add_leaf : Correctness follows from the defining
properties of T trees. The time for an operation nca(x,y) is dominated by the time used by the
routines c (x,y, ), L ≥  ≥ 1.
We extend the routines to allow add_root, similar to the extension for Corollary 2.10, as follows.
add_root is still implemented by Equation (6), where now ϱ is a pointer to a node in the tree TL.
The routine for nca(x,y), instead of immediately calling c(x,y, L), is modified to use Lemma 2.9(ii)
as before. Specifically, it callsc(x,y, L),c(x, ϱ, L), and c(y, ϱ, L), and chooses nca(x,y) according to
the lemma.
Linear-Time Incremental Trees
Take L = 3 levels with
μ3 = μ2 = logn, μ1 = n + 1. (8)
Level 1 uses the incremental-tree algorithm of Section 2. It uses O(m + n) time and O(n) space.
This follows since Equation (7) shows there are ≤ n
log2 n 1-nodes. Thus, Corollary 2.10 shows the
time on level 1 is O(m + n log2 n
log2 n ) = O(m + n). The space in level 1 is O(
n log n
log2 n ) = O(n).
Levels 3 and 2 both use the bitstring data structure of Section 3. For level 2, we compute the
complete characteristic ancestor vector ca(x,y), as described in Section 3. The analysis of that
section shows that levels 3 and 2 use O(m + n) time. The space is kept linear, O(n), by using the
doubling strategy of Lemma 2.8 for the v tables of nonfull -subtrees.
This completes the three-level incremental tree algorithm.
Theorem 4.1. The incremental-tree nearest common ancestors problem with add_leaf , add_root,
and ca operations can be solved in O(m + n) time and O(n) space.
As in Corollary 2.10, the theorem does not require n (the number of add_leaf and add_root
operations) to be known in advance. To achieve this, first consider Equation (8) defining the μi .
One approach is to update these values every time n doubles. Instead, we will simply interpret n in
Equation (8) to be N, the maximum integer that can be represented in the RAM. So μ3 = μ2 = log N
is the number of bits in a RAM word. The timing estimates are unchanged. For instance, the time
in level 1 is O(m + n log2 n
log2 N ) = O(m + n). The space in level 1 is O(
n log n
log2 N ) = O(n). The space for all
three levels is maintained in one array S, using Lemma 2.8.
5 LINK OPERATIONS
This section extends the multilevel data structure to solve our most general dynamic nca problem.
The algorithm processes m nca and link operations on a set of n nodes in time O(mα (m,n) + n)
and linear space O(n).
The multilevel structure shares many details with that of the previous section: The levels  =
L,..., 1 and the notions of -tree, -node, and -subtree are all unchanged. A difference is that a
tree T at level  > 1 gives its level  − 1 counterpart T−1 by contracting every -subtree (i.e., no
subtrees are deleted). The notations x,
→−x , and ←−x are defined without change.
nca operations are implemented using the c and c routines, as in last section. link operations
are implemented using a recursive routine l similar to a of last section. The analog of a for link is
folded into l (i.e., there is no ˆ
l). It is convenient to use an extra argument forl: We write l(r, x,y, )
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.                               
A Data Structure for Nearest Common Ancestors with Linking 45:17
where r is the root of the -tree containing x. Call a tree built up by link operations a link tree. The
operation link(x,y) is performed by l(ρ, x,y, L) for ρ the root of the link tree containing x.
Example Algorithms
For motivation, we start by sketching the two simplest versions of our algorithm.
Algorithm 1 Every link tree is represented as an incremental tree. The link operation uses add_leaf
and add_root operations to transfer the nodes of the smaller tree into the larger (for trees of equal
size, break the tie arbitrarily). It then discards the incremental tree of the smaller tree. The number
of node transfers is O(n logn). So Theorem 4.1 shows the total time is O(m + n logn).
The analysis of Algorithm 1 is based on what we will call the “stage” of the link tree: A tree
in stage σ has between 2σ and 2σ+1 vertices. We view the analysis as charging a vertex O(1)
to advance from one stage to the next. (This accounts for the total time spent on add_leaf and
add_root operations since Theorem 4.1 shows the time spent on a tree that ultimately grows to ns
nodes is O(ns ); i.e., the time is proportional to the number of node transfers.) Our more efficient
algorithms maintain explicit stages, and these stages will require faster growth in the tree size.
Algorithm 2 Algorithm 1 can be improved using a two-level stategy similar to previous ones.
Level 2 classifies each tree as stage 1 or 2: A 2-tree is in stage 1 if it has < logn nodes and stage 2
if it has ≥ logn nodes.
A stage 2 2-tree is partitioned into 2-subtrees, each of which contains ≥ logn nodes. Each 2-
subtree is represented as an incremental tree using the data structure of Theorem 4.1. Contracting
all these 2-subtrees gives its corresponding 1-tree.
A stage 1 2-tree is also a 1-tree. For consistent terminology in stage 1, view each 2-node as a
2-subtree.
Level 1 uses Algorithm 1 on all 1-trees.
The l routine works as follows on level 2: It sets π (y) ← x. Then, letting X and Y denote the
2-trees containing x and y, respectively, it executes the case that applies:
Case Both trees are in stage 2: Link the level-1 trees using Algorithm 1.
Case Only one tree is stage 2: If X is stage 2, transfer the nodes of Y to x, using add_leaf operations. Then discard the data structures for Y on levels 1 and 2.
If Y is stage 2, do the same, using appropriate add_root operations in the transfer of X to y.
Case Both trees are stage 1: If the combined trees contain ≥ logn nodes, initialize the 2-tree as a
new stage 2 tree, with one 2-subtree consisting of all nodes of X and Y. Discard the data structures
for X and Y on both levels.
Otherwise, link the level 1 trees using Algorithm 1.
The total time is dominated by the time spent for all incremental trees on both levels 1 and 2.
On level 2, a 2-subtree that grows to contain ni nodes (as in the last two cases) uses time O(ni ) for
all add_leaf and add_root operations. So, all 2-subtrees use total time O(n).
Consider level 1. The 1-trees for stage 2 2-trees collectively contain ≤ n/ logn nodes. Each node
is transferred by Algorithm 1 at most logn times. So the total time is O(n). The stage 1 2-trees
collectively contain n nodes. Each is transferred ≤ log logn times by Algorithm 1. So the total
time is O(n log logn). This term strictly dominates the algorithm’s time bound.
Clearly, we can improve this algorithm by adding another stage for 2-trees with ≤ log logn
nodes. The time becomes O(n log(3) n). Continuing in this fashion, we can achieve time
O(n log∗ n).
6 Let us sketch this algorithm. (The detailed version of the algorithm is the case  = 2
6log(i) n and log∗ n are defined as in Cormen et al [5].
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.    
45:18 H. N. Gabow
of algorithm A presented later.) It is convenient to switch notation from small functions like log
to large ones like exponentiation. Recall the superexponentiation function, defined by 2↑1 = 2,
2↑(s + 1) = 2(2↑s )
.
In Algorithm 2, level 2 has log∗ n stages. A stage σ 2-tree has between 2↑σ and 2↑(σ + 1) nodes.
It is partitioned into 2-subtrees, each of which contains ≥ 2↑σ nodes. The remaining properties of
2-trees are essentially the same as in the previous algorithm.
The l routine uses new criteria to determine the cases but is otherwise unchanged. In more
detail, let X be in stage σ (X) and similarly for σ (Y ), and let σ = max{σ (X), σ (Y )}.
If the combined trees contain ≥ 2↑(σ + 1) nodes, a new stage σ + 1 tree is initialized (as in the
last case just given).
Otherwise, if σ (X)  σ (Y ), the nodes of the smaller 2-tree are transferred to the larger (as in
the preceding middle case).
Otherwise, (σ (X) = σ (Y )) and Algorithm 1 links the images of the two 2-trees (as in the first
and last cases).
The time for all link operations isO(n log∗ n). This holds because the time on each stage isO(n).
Let us sketch a proof. Consider level 2. As before, a 2-subtree that grows to contain ni nodes uses
time O(ni ) for all add_leaf and add_root operations. This gives O(n) time total for each stage on
level 2. There are log∗ n stages, so the total time is O(n log∗ n).
As for level 1, a 1-node is a contracted 2-subtree. A fixed stage σ of level 2 contains a total of
≤ n/(2↑σ ) 2-subtrees. Thus, over the entire algorithm, stage σ has ≤ n/(2↑σ ) 1-nodes x. After
being transferred 2↑σ times by Algorithm 1, x’s 1-tree has grown to ≥ 22↑σ = 2↑(σ + 1) 1-nodes.
So the 2-tree containing ←−x has advanced to stage σ + 1. So O(2↑σ ) time total is spent on x in level
1. Thus, the time for Algorithm 1 to process all stage σ 1-nodes is O( n
2↑σ · 2↑σ ) = O(n). Again,
there are log∗ n stages, so the total time on level 1 is O(n log∗ n).
We conclude that Algorithm 2 uses total time O(m + n log∗ n).
The General Algorithm
The construction can be repeated using Algorithm 2 to get an even faster Algorithm 3, and so on.
We now present the complete algorithm.
Define Ackermann’s function Ai (j) for i, j ≥ 1 by
A1 (j) = 2j
, for j ≥ 1;
Ai (1) = 2, for i ≥ 2;
Ai (j) = Ai−1 (Ai (j − 1)), for i, j ≥ 2.
Define two inverse functions,
ai (n) = min{j : Ai (j) ≥ n};
α (m,n) = min{i : Ai (4m/n) ≥ n}, for m,n ≥ 1.
These definitions differ slightly from those of Tarjan [20], but this does not change asymptotic
estimates. The most significant difference is that our function Ai (1) is constant compared to a
rapidly growing function in Tarjan [20]. This makes for a more convenient treatment of the base
case in our algorithms. We use some very weak properties of Ackermann’s function, including
these inequalities, which are proved in Appendix D:
Ai (j + 1) ≥ 2Ai (j), for i, j ≥ 1; (9)
Ai+1 (j) ≥ Ai (2j), for i ≥ 1, j ≥ 4; (10)
α (m	
,n	
) ≥ α (m,n) − 1, for m	 ≤ 2m,n	 ≥ n. (11)
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.  
A Data Structure for Nearest Common Ancestors with Linking 45:19
A preprocessing step tabulates the relevant values of Ackermann’s function. We use the values
Ai (j) that are ≤ n fori ≤ α (m,n). Define an array ackermann[1.. logn, 1.. logn]: If Ai (j) ≤ n, then
ackermann[i, j] = Ai (j); else ackermann[i, j] = ϵ. This table also allows us to find α (m,n), which is
≤ logn (since Equation (10) shows Alog n (4) ≥ A1 (21+log n )). The table is initialized, and α (m,n) is
found, in time O(log2 n). The table allows any desired value of Ackermann’s function to be found
in O(1) time.
We use the linear-time incremental tree data structure of Theorem 4.1. Call a tree that is represented by this data structure an incremental tree. The preprocessing step computes all the tables
for this algorithm in time O(n).
The approach is similar to that of Gabow [7] for a list-splitting problem. We construct a family
of algorithms A,  ≥ 1. A is a multilevel algorithm based on the function A. It calls A−1 if
 > 1. A runs in time O(m + na (n)).
Algorithm A works on level . The terms -node and -tree refer to the objects manipulated
by A. Every link tree corresponds to an L-tree with the same nodes and edges. Every level  has
a (n) stages σ, σ = 0,..., a (n) − 1. Each -tree T belongs to a unique stage σ defined as follows:
Case |V (T )| < 4: T is in stage σ = 0. Stage 0 uses a trivial algorithm so an invocation of c or
uses time O(1).
Case |V (T )| ≥ 4: T is in the stage σ ≥ 1 satisfying |V (T )| ∈ [2A (σ ), 2A (σ + 1)). (This is possible since A (1) = 2.) An -subtree in stage σ is a subtree that has ≥ 2A (σ ) nodes. The nodes of
T are partitioned into -subtrees. If  > 1, then T , with each -subtree contracted, is represented
on level  − 1.
Note that the contracted tree on level  − 1 may be a trivial tree in stage 0 (of level  − 1). Also,
if  = 1, there is no need to store the contracted tree since T has only one -subtree. This follows,
since an -subtree has ≥ 2A1 (σ ) = 2σ+1 nodes and |V (T )| < 2A1 (σ + 1) = 2σ+2 nodes.
Algorithm A uses the following data structure. Each -tree and -subtree is represented by
its root. An -tree T is stored using parent pointers and children lists. If r is the root of T, then
s(r) equals the size of T (the number of its -nodes). For any node x, σ (x) equals the stage of x’s
-tree; if σ (x) > 0, then xpoints to the -subtree containing x. Each -subtree is represented as an
incremental tree. Recall (from Theorem 4.1) that it has a root pointer ϱ that is updated by add_root
operations.
We turn to the link and nca operations. Initially, every node is a singleton link tree, in stage
0 of level L. Recall that the operation link(x,y) is processed by invoking the recursive algorithm
l(r, x,y, ) with arguments  = L and r equal to the root of the link tree containing x. r is found
by a simple recursive algorithm: If ρ(x) is the root of its -tree, then it is r. Otherwise, recursively
compute r as the root of the ( − 1)-tree containing→−x and set r ← ←−r .
The algorithm for l(r, x,y, ) is as follows. Let X and Y denote the -trees with root r and y,
respectively, on entry to l.
Combine Step: Combine X and Y into a new -treeT by setting π (y) ← x and adding y to the child
list of x.
The rest of the algorithm determines the stage of T and its decomposition into -subtrees. Start
by increasing s(r) by s(y). Let σ = max{σ (x), σ (y)}. Execute the first of the following cases that
applies and then return.
Case 1 s(r) ≥ 2A (σ + 1): Make T a new stage σ + 1 -tree consisting of one -subtree, as follows: Initialize a new incremental tree r. Traverse T top down; when visiting a node v, do an
add_leaf operation to add v tor. Discard the data structures for X and Y on all levels ≤ . If  > 1,
then create an ( − 1)-tree in stage 0 for T, consisting of one node.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.                                                                
45:20 H. N. Gabow
Fig. 6. Examples for link. L = 3. (a) Trees for link(x,y): (a.1) 3-trees X and Y. (a.2) 3-subtrees for X and Y. X
has one 3-subtree. (a.3) 2-trees for X and Y. (b) T3 formed for link (x,y), and link tree Z. (b.1)–(b.3) 3-trees, 3-
subtrees, and 2-trees, as before. (c)T3 formed forlink(z,r). (c.1)–(c.3) 3-tree, 3-subtrees, 2-tree. (c.4) 2-subtree
contains entire 2-tree.
Case 2 σ (x) > σ (y): Traverse Y top down, doing add_leaf operations to add each node of Y to
the incremental tree x. Discard the data structures for Y on all levels ≤ .
Case 3 σ (x) < σ (y): Traverse the path from x to r in X, doing add_root operations to add each
node to the incremental tree y. Then traverse X top down, doing add_leaf operations to add the
other nodes to y. Discard the data structures for X on all levels ≤  and set σ (r) ← σ (y).
Case 4 σ (x) = σ (y): If σ > 0, then do l(
→−r ,
→−x ,
→−y ,  − 1). If σ = 0, then combine X and Y using a
trivial algorithm.
Figure 6 illustrates two link operations.link(x,y) starts with the trees of Figure 6(a) and executes
Case 3 in level 3. The resulting link tree link(x,y) has root r, which is the nonroot node ϱ in
the incremental tree of Figure 6(b.2). link(z,r) starts with the trees of Figure 6(b) and executes
Case 4 in level 3. Then it executes Case 1 for the two stage 0 trees in level 2. The resulting 2-
tree (Figure 6(c.3)) is in stage 1 since it has 5 ≥ 4 nodes. All 2-nodes are in one 2-subtree, as in
Figure 6(c.4). Figure 6(b.2) and (c.2) illustrate that, in general, any incremental tree may have its
root pointer ϱ pointing to a node at arbitrary depth.
Lemma 5.1. The algorithm for link(x,y) preserves all the defining properties of the data structure.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.      
A Data Structure for Nearest Common Ancestors with Linking 45:21
Proof. We sketch the argument giving only the most interesting details. Assume the bookkeeping fields σ (v) and vare updated when node v is added to a new incremental tree.
Consider a link tree T . For every level  ∈ [1..L],T denotes the corresponding tree as defined
by the data structure’s parent and child pointers. T is represented correctly by TL, by a simple
induction using the Combine Step. Furthermore, for every level  ∈ [2..L], T−1 is formed from T
by contracting every -subtree. (Notice that in Case 2 T−1 does not change since Y is absorbed
into x. Similarly for Case 3 and y.) This implies that the vertex y is the root of its treeTL, and→−y and
all lower images are roots of their -trees (see especially Figure 6(b.2)). This justifies the argument →−y in the recursive call of Case 4.
Now consider the four cases that determine the stage and the -subtrees.
Case 1: The new -tree belongs in stage σ + 1 because s(r) < 4A (σ + 1) ≤ 2A (σ + 2) by Equation (9).
Cases 2–3: In Case 2, the incremental tree x exists, since x is in a positive stage. Similarly, in
Case 3, yexists.
Case 4: The new -treeT (formed in the Combine Step) is correctly partitioned into -subtrees,
since X and Y were. Also, Case 4 always has  > 1. (So level  − 1 actually exists.) This is because
if  = 1 and σ (r) = σ (y) = σ, then Case 1 applies since s(r) ≥ 2(2A1 (σ )) = 2σ+2 = 2A1 (σ + 1).
If σ = 0, then s(r) < 4; so, having updated T to the combined tree, we are done.
The algorithm for ca(x,y) is trivial in universe zero. In positive universes, it is the multi-level
algorithm c(x,y, ) of Section 4. The first case of the c algorithm is always used (since every -
subtree is contracted to an ( − 1)-node). It executes the code of Figure 5.
Lemma 5.2. Algorithm A executes a sequence of m ca and link operations on a set of n nodes in
O(m + na (n)) time and O(na (n)) space.
Proof. First consider the time. A ca query uses O() time in a positive stage, since O(1) time is
spent on each of  levels of recursion. The time is O(1) in universe zero.
The time for links is estimated as follows. Charge each link operation O() time to account for
the initial computation of the root r plus the  levels of recursion and associated processing in
routine l (e.g., Case 4 and the Combine Step). So far, all charges are included in the term O(m) of
the lemma.
For the rest of the time, call an add_leaf or add_root operation an add operation and define
η = the total number of add operations.
(η includes all add operations done in recursive calls.) The rest of the time for l is proportional to
η. Here, we are using Theorem 4.1, which shows that an incremental tree that grows to contain
ni nodes uses time O(ni ) for all add_leaf and add_root operations. (Also note that discarding data
structures in Cases 1–3 is just a no-op.) For the time bound of the lemma, it suffices to show
η = O(na (n)). In fact, we will show by induction on  that
η ≤ 2na (n). (12)
First consider the add operations in Cases 1–3 of level  (i.e., we exclude the operations that result
from a recursive call made in Case 4 from level ). Each such add is done for a node previously in a
lower stage of level . So, at most one add is done for each node in each stage. This gives ≤ na (n)
adds total. (In particular, this establishes the base case of the induction,  = 1.)
To bound the number of adds in all levels < , fix a stage σ > 0 of level . We will show there
are ≤ n adds total in recursive calls made from stage σ of level . So, the a (n) stages contribute
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.                                                 
45:22 H. N. Gabow
a total of ≤ na (n) adds in levels < . Adding together the two bounds gives Equation (12) and
completes the induction.
First note an approach that does not work. The inductive assumption holds for A−1. So we could
estimate the total number of ( − 1)-nodes, say n−1, and use the inductive bound 2n−1a−1 (n−1).
But this overestimates the number of adds, since A discards the entire data structure for an -tree
as soon as it moves to a higher stage (i.e., when it has size ≥ 2A (σ + 1) in Case 1, or earlier in
Cases 2 and 3). So, instead, our approach is to count the number of adds for each maximal -tree
Mi of stage σ.
Let Mi have ni vertices and si -subtrees. Then
si ≤ ni/2A (σ ) ≤ A (σ + 1)/A (σ ), (13)
where the first inequality uses the lower bound 2A (σ ) on the size of an -subtree and the second inequality uses the upper bound 2A (σ + 1) on the size of an -tree. The ( − 1)-tree for Mi
has si nodes. The inductive assumption shows the number of adds to form this ( − 1)-tree is
≤ 2sia−1 (si ).
Using the second inequality of Equation (13) gives
a−1 (si ) ≤ a−1 (A (σ + 1)/A (σ )) ≤ a−1 (A (σ + 1)) = a−1 (A−1 (A (σ ))) = A (σ ).
Using this and the first inequality of Equation (13) shows that the total number of adds for all
( − 1)-trees of stage σ is at most

i
2sia−1 (si ) ≤ 2A (σ )

i
si ≤ 2A (σ )

i
ni /2A (σ ) =

i
ni ≤ n.
This bound of n adds per stage implies ≤ na (n) recursive adds total. This completes the induction.
Now consider the space. There are initially n nodes on level L. Additional nodes are only created
in Case 1. The number of these nodes is obviously bounded by the number of add_leaf operations
and so is ≤ η. Theorem 4.1 shows the space used for incremental trees is proportional to η. (As
usual, all space is allocated from one global array S using Lemma 2.8.) So Equation (12) implies the
desired space bound.
The remaining issue is how to choose the number of levels . Consider the usual case where
bounds on m and n are known when the algorithm begins. Take  = α (m,n). By definition,
aα (m,n) (n) ≤ 4m/n ≤ m/n + 4. So the lemma implies the following:
Theorem 5.3. A sequence of ≤ m nca and link operations on a universe of ≤ n nodes can be processed in time O(mα (m,n) + n) and space O(m + n).
Appendix E extends this result to settings where m and n are not known in advance.
The multilevel method we have used can be applied to achieve the same time and space bounds
for several other problems. As mentioned earlier, Gabow [7] applies it to solve the list splitting
problem that arises in expand steps of Edmonds’ algorithm. The technique was rediscovered by
Han La Poutré [14]: He presents a multilevel algorithm for the set merging problem (this application is noted in Gabow [7, p. 99]). Also, a result similar to Corollary E.1 was independently arrived
at [J.A. La Poutré, personal communication]. Other applications include the static cocycle problem introduced in Gabow and Stallmann [10], both for graphic matroids and the job scheduling
matroid; the former is useful for various problems involving spanning trees.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.                                          
A Data Structure for Nearest Common Ancestors with Linking 45:23
APPENDICES
A COMPUTING LOGARITHMS
We show how to compute logβ r for a given integer r ∈ [1..cne ] in time O(1). Here, β = a/b is a
fixed rational number for positive integers a > b, c and e are fixed integers, c ≥ 1, e > 1.
Let k = logβ n. We precompute these values:
• k, ak ,bk .
• a table [1..n] with [r] = logβ r for r ∈ [1..n].
We show the precomputation time is O(n).
The following code precomputes the  table:
a	 = 1; b	 = 1; k = −1
for r = 1 to n do
while r ≥ a	
/b	 do
a	 = aa	
; b	 = bb	
; k = k + 1
[r] = k
On exit, k is the desired value logβ n, and the desired values ak and bk are given by a	
/a and
b	
/b, respectively. It is clear that the time is O(n).
Now we give the algorithm to compute logβ r for a given integer r ∈ [1..cne ]. Let h be the
unique integer satisfying
βhk ≤ r < β(h+1)k .
So,
logβ r = hk + logβ (r/βhk ). (14)
Taking floors gives the desired value.
We find h by testing successive values βhk . The desired h is at most e + (e + logβ c)/k = O(1).
This follows forr ≤ cne since c = βlogβ c and n < βk+1 implies ne < βke+e . Using the values ak ,bk ,
the time is O(e) = O(1).
The desired floor of the logarithmic term in Equation (14) is logβ r/βhk . This corresponds
to an entry in the  table since r/βhk < βk ≤ n. The desired entry is found as [rbhk /ahk ] (since
division is truncating). Again, the time is O(1).
B PROOF OF SPACE ALLOCATION LEMMA 2.8
Proof. We maintain the invariant that each current array Ai has size 2si in S when ni ∈ [si +
1..2si], and, furthermore, the total amount of space in S used for versions of Ai is ≤ 4si . (Thus, at
every point in time, every array Ai has used ≤ 4si < 4ni entries of S; i.e., all allocated storage is
within S[1..4n].)
When a new Ai is created, we set si = 1, allocate 2 cells, and set ni = 2.
When a new entry is added to Ai and ni = 2si, we allocate a new copy of Ai of size 4si at the end
of S, copying the contents of Ai[1..ni] into it. Setting s	
i = 2si and ni = 2si + 1 the new Ai has size
4si = 2s	
i with ni = s	
i + 1. Ai has used ≤ 4si + 4si = 4s	
i entries of S. So the invariant is preserved.
The time for the operation (copying ni entries) is O(ni ) and can be charged to the si = ni/2
elements added to Ai since the last allocation.
C INCREMENTAL-TREE SET MERGING
Incremental-tree set merging, introduced by Gabow and Tarjan [11], is a special case of the general
union-find problem. It achieves a slight asymptotic improvement of the best-known general unionfind algorithm, reducing time O(mα (m,n)) for m ≥ n finds in a universe of n elements to linear
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.       
45:24 H. N. Gabow
time O(m). It has a variety of applications [11]. It is more general than its special case, static-tree
set merging. The implementation of this section simplifies the tables that must be precomputed.
Specifically, Gabow and Tarjan [11] store forests in single memory cells. The precomputation step
checks that bitstrings correspond to forests and performs a depth-first search of such forests. Our
precomputed tables are for several natural bit operations, specifically most-significant-bit, powers
of two, and logical and.
The incremental-tree set merging data structure maintains a collection of disjoint sets, called
i-sets, specified by these operations:
add_leaf (x,y) - make a new node y the child of node x, and make {y} a singleton i-set;
union(x) - merge the i-set containing x with the i-set containing its parent π (x);
find(x) - return the root of the i-set containing x.
T denotes the tree that is grown by add_leaf operations. We allow the operation add_leaf (ε, x) to
initializeT to consist of the root x. Our algorithm is similar to Section 3, and we use its terminology.
As in Figure 3(a), T = T2, and a full 2-subtree gets added to T1 as a 1-node. We use the following
data structure.
Each 2-subtree uses a bitstring data structure based on Section 3 and specified as follows. id,
v, and anc are defined and maintained as in Section 3. In addition, the 2-subtree has a bitstring
roots of logn bits. The ith bit of roots is 1 if and only if node number i is the root of its i-set. In
other words, the ith bit is 0 if and only if union(v[i]) has been performed. The roots bitstring is
initialized to 2log n − 1 when the 2-subtree is created. (This makes the 0th bit of roots equal to 1,
but this is irrelevant.)
The operation union(x) is implemented as
roots[x] ← roots[x] − 2id[x]
.
Nothing is done in T1 even if x = ρ(x): Unions in T1 are done in a lazy fashion, as explained later.
For any vertex x, define
β (x) = anc[x] ∧ roots[x].
If β (x)  0, it is easy to see f ind(x) = v [msb [β (x)]]. Here, v denotes the v table for x.
T1 is processed using a set-merging data structure on the 1-nodes. A simple choice is the “relabelthe-smaller-half” algorithm, where a union operation uses O(logn) time and find is O(1) [5]. For
clarity, call the operations inT1 union1 and find1. We will manipulateT1-sets like i-sets (i.e., union1
will always merge a T1-set rooted at x into the T1-set containing π (x)).
We will maintain T1-sets to preserve this invariant:
(I) For arbitrary 1-nodes x and y = find1 (x),
←−x and ←−y are in the same i-set. Furthermore,
y  x implies ←−y is not an i-set root.
For a vertex x, the operation find(x) is implemented as follows.
if xis nonfull and β (x) = 0 then x ← π (ρ(x))
if β (x) = 0 then
loop /∗ x is always a vertex of T , y is always a 1-node ∗/
y ← find1 (
→−x ) /∗ β (x) = 0 ∗/
x ← π (
←−y )
if β (x)  0 then break
union1 (y,
→−x )
return v [msb [β (x)]]
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.          
A Data Structure for Nearest Common Ancestors with Linking 45:25
It is easy to see invariant (I) is maintained, and the find algorithm is correct. (Note that ←−y is
never an i-set root. This follows from invariant (I) when →−x  find1 (
→−x ) and from the condition
β (x) = 0 when→−x = find1 (
→−x ).)
Next, we show the time for m find operations and n unions is O(m + n).
Aunion operation is done in a 2-subtree data structure and so takesO(1) time. At most n/logn
union1 operations are done in executions of find. (Each union1 is nontrivial, i.e., it starts with y
and→−x in different T1-sets. This follows from the preceding find1 operation and redefinition of x.)
So union1 operations use total time O((n/ logn) logn) = O(n).
We turn to find operations. It is easy to see that, ignoring time for union1s, a find usesO(1) time
plus the time for all executions of find1. The total number of executions of find1 is ≤ m + n/logn.
(Them term accounts for the first execution of find1 in all finds. The remaining executions of find1
all follow a union1, so they number ≤ n/logn as earlier.) This implies that finds use total time
O(m + n) as desired.
D SIMPLE INEQUALITIES FOR ACKERMANN’S FUNCTION
Proof of Eqation (9), Ai (j + 1) ≥ 2Ai (j) for i, j ≥ 1:
First note the trivial inequality 2i ≥ 2i for i ≥ 1. Also, for every i ≥ 1, Ai (2) = 4.
Next, we show Ai (j) ≥ 2j by induction on i, with the inductive step inducting on j. The base
case j = 2 of the inductive step is Ai (2) = 4 = 22. For the inductive step
Ai (j) = Ai−1 (Ai (j − 1)) ≥ 2Ai (j−1) ≥ 22j−1
≥ 2 · 2j−1 = 2j
.
Now, Equation (9) itself follows from the first ≥ relation displayed earlier and 2Ai (j−1) ≥
2Ai (j − 1).
Proof of (10), Ai+1 (j) ≥ Ai (2j) for i ≥ 1, j ≥ 4:
Note that Equation (10) needn’t hold for j < 4: Recalling that A2 is superexponentiation A2 (j) =
2 ↑ j, A2 (3) = 16 < 64 = A1 (6). However, Equation (10) holds for i = 1, j = 4: A2 (4) = 216 > 28 =
A1 (8). In general, for i ≥ 1,
Ai+1 (4) = Ai (Ai+1 (3)) = Ai (Ai (Ai+1 (2))) = Ai (Ai (4)).
Also, Ai is an increasing function by Equation (9).
We prove Equation (10) by induction on i. For the base case, Ai+1 (4) = Ai (Ai (4)) ≥ Ai (24) >
Ai (8). For the inductive step, Ai+1 (j + 1) = Ai (Ai+1 (j)). The argument to Ai is Ai+1 (j) > Ai (2j) ≥
22j ≥ 2(2j) ≥ 2j + 2.
Proof of Eqation (11), α (m	
,n	
) ≥ α (m,n) − 1 for m	 ≤ 2m,n	 ≥ n:
Let i = α (m,n). We wish to show α (m	
,n	
) ≥ i − 1 (i.e., Ai−2 (4m	
n	 ) < n	
). This follows since
Ai−2

4

m	
n	

≤ Ai−2

8

m
n

≤ Ai−1

4

m
n

< n ≤ n	
.
The first inequality uses m	
/n	
≤2m/n ≤ 2m/n. The second uses Equation (10), which applies since 4m
n  ≥ 4. (A slightly more involved calculation shows α (m	
,n	
) ≤ α (m,n) + 2 when
m	 ≥ m,n	 ≤ 2n, but we do not use this fact.)
E NCAS WITH LINKS AND MAKE_NODES
This appendix extends the dynamic nca algorithm of Section 5 to settings where m and n are not
known in advance. More precisely, in addition to nca and link operations, we allow the operation make_node(x), which creates a new node x in a singleton tree. We prove the exact analog of
Theorem 5.3 in the new setting (Corollary E.1).
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.       
45:26 H. N. Gabow
It is convenient to assume that new nodes x are not counted in n until they are involved in a
link operation. (Since α (m,n) is increasing with n, this can only make the desired time bound of
Theorem 5.3 stronger.) Since a link increases n by ≤ 2, we always have
m ≥ n/2.
Our basic technique is a doubling strategy. But two difficulties must be overcome: First, α (m,n) is
decreasing withm. So, the time bound for a ca operation can decrease as the algorithm progresses.
Second, the term na (n) in the bound of Lemma 5.2 does not change in a predictable way.
We begin by describing our new procedure. It uses algorithm A where  is repeatedly modified.
In precise terms, the sequence of operations is divided into periods. The parameters n andm denote
their values at the start of a period. The period processes nca and link operations using algorithm
A where  = α (m,n). (Note that the first period begins with the execution of the firstlink; i.e.,m =
1, n = 2,  = 1.) The period continues as long as the value of α remains in { − 1, }. In other words,
we declare a new period whenever n	 (the current number oflinks),m	 (the current number oflinks
and ncas), and 	 = α (m	
,n	
), have 	 >  or 	 <  − 1. The last period ends at the conclusion of
the algorithm (α need not have changed). In precise terms, the algorithm is as follows.
Before executing the current nca/link operation, update n	
,m	
, and 	 to include
that operation. If 	 ∈ { − 1, }, then execute the operation. Otherwise, do the
following:
Set n ← n	
, m ← m	
,  ← 	
. Reorganize the entire data structure to use algorithm
A. Do this by making each current link tree T an incremental tree and placing it
in the appropriate stage for A. If  > 1, add a corresponding node in stage 0 of
level  − 1. Finally, execute the current nca/link operation.
This procedure clearly handles links and ncas correctly. The time to start a new period is O(n)
for the new value of n. (This includes the time to compute a new ackermann table, find , compute
new incremental tree tables, and find the stage for each incremental tree. All tables are computed
for the value 2n; e.g., ackermann stores all values Ai (j) ≤ 2n.)
Now we prove that the procedure achieves our goal. Note that the resource bounds of the following corollary are essentially the same as Theorem 5.3.
Corollary E.1. A sequence of nca and link operations can be processed in time O(mα (m,n)) and
space O(m). Here, m is the number of ncas and links, n is the number of links, and neither is known
in advance.
Proof. The bulk of the argument establishes the time bound. We will charge processing time to
the counts n and m. To do this, define a unit of time to be enough to pay for any constant amount
of computing in the algorithm’s time bound.
As in the algorithm, the analysis uses n, m, and  to denote the values when the first nca/link
operation of the period is executed. In addition, we use n and m to denote the counts up to and
including the last operation of the period. (So α (n,m) ∈ { − 1, }.) We also use n	
,m	
, 	 to denote
those values for the first operation of the next period. (So m	 = m + 1. This holds for the last period by convention. We also take n	 = n for the last period.) Note that m	 − m is the number of
operations in the period.
The proof of the time bound consists of these three claims.
Claim 1. The time for any period is at most (m	 − m) + 12m units.
Claim 2. The total time from the start of the algorithm to the end of any period but the last is at
most (12	 + 50)m units. (As earlier, 	 = α (m	
,n	
).)
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.                               
A Data Structure for Nearest Common Ancestors with Linking 45:27
Claim 3. When the last period ends, the total time for the entire algorithm is at most (12 + 62)m
units. The time bound of the corollary holds.
In Claim 1, all parameters (e.g., ) are defined for the period under consideration. In Claim 3, all
parameters are defined for the last period.
Proof of Claim 1. Lemma 5.2 shows the time for a period isO((m	 − m) + na (n)). So we need
only show na (n) ≤ 12m.
First observe
a (n) ≤ 4

m
n

. (15)
This is equivalent to A (4m
n ) ≥ n. This inequality holds by definition if α (m,n) = . The other
possibility is α (m,n) =  − 1. But this also implies the same inequality since we have A (4m
n ) ≥
A−1 (4m
n ) ≥ n.
The right-hand side of Equation (15) is < 4(m/n + 1). Thus, na (n) ≤ 4(m + n). Using m ≥ n/2,
the last quantity is bounded by 4m + 8m = 12m. ♦
We prove Claims 2 and 3 by charging each nca/link at most γ time units, where γ is 12	 + 50
in Claim 2 and 12 + 62 in Claim 3.
Claim 2 implies Claim 3. Claim 2 shows that the total time from the start of the algorithm to
the beginning of the last period is accounted for by a charge of γ = 12 + 50, where  = α (m,n)
is the value used in the data structure of the last period. (This holds a fortiori if the last period is
actually the first period.)
Account for the time in the last period in two steps. First account for the term (m	 − m) in
Claim 1 by charging each nca/link of the last period  units. Since  < γ , every nca/link is charged
≤ γ units. Next account for the term 12m in Claim 1 by increasing γ by 12, so the new charge is
12 + 62 units. This gives the first part of Claim 3.
The final value of α is ≥  − 1. So, using the first part of Claim 3 and changingm to the parameter
m of the corollary, the time bound for the entire algorithm is O(m(α (m,n) + 1) = O(mα (m,n)).
Claim 3 is now completely proved. ♦
Proof of Claim 2. Assume Claim 2 holds at the end of the previous period. Now switch to
the notation of the current period. (The value 	 in Claim 2 becomes the parameter of the current
period, .) So, each operation preceding the current period is chargedγ = 12 + 50 units. Let 	 now
denote the value of α after the current period ends. We wish to show the total time is accounted
for by charging every operation γ 	 = 12	 + 50 units. Consider the two possibilities for 	
.
Case 	 ≥  + 1: Account for the first term of Claim 1 by charging each nca/link of the current
period  units. Certainly,  < 12 + 50. So now every nca/link is charged ≤ γ units. Account for
the second term by increasing γ to γ + 12 = 12( + 1) + 50 ≤ 12	 + 50. This gives Claim 2 for the
current period. (This case applies when the current period is the first period, since  = 1.)
Case 	 ≤  − 2: If m	 ≤ 2m, then Equation (11) implies 	 = α (m	
,n	
) ≥ α (m,n) − 1 =  − 1. So
m	 > 2m.
Equation (11) also implies 	 = α (m	
,n	
) ≥ α (m,n) − 1 since m	 = m + 1 ≤ 2m. Using α (m,n) ≥
 − 1 gives 	 = α (m	
,n	
) ≥  − 2. With the inequality assumed for the current case, we get
	 =  − 2.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 45. Publication date: September 2017.                                              
45:28 H. N. Gabow
Since m	
/2 > m, we have m	 − m > m	
/2. So, we can account for the second term of Claim 1 by
charging each nca/link of this period 24 units (24(m	
/2) = 12m	 > 12m). The charge for the first
term of Claim 1 is . So, the total charge to each new time period is  + 24 = (	 + 2) + 24 = 	 + 26.
Each of the first m − 1 operations is currently charged γ = 12 + 50 = 12(	 + 2) + 50. Transfer
24 units to an operation of the current period. (Permissible since m	 − m > m > m − 1.) The first
m − 1 operations are now each charged γ 	 = 12	 + 50. The operations of the current period are
each charged ≤ (	 + 26) + 24 < 12	 + 50 = γ 	
. So, γ 	 accounts for all the time so far. ♦
Finally, consider the space. Lemma 5.2 shows the space for each period is O(na (n)). The argument of Claim 1 shows that any period has na (n) = O(m). Since m is at most the final value of m,
and the space for a period is always reused, the space bound follows.           