We consider the problem of online scheduling of jobs on unrelated machines with dynamic speed scaling to
minimize the sum of energy and weighted flow-time. We give an algorithm with an almost optimal competitive ratio for arbitrary power functions. (No earlier results handled arbitrary power functions for unrelated
machines.) For power functions of the form f (s) = sα for some constant α > 1, we get a competitive ratio of
O( α
log α ), improving upon a previous competitive ratio of O(α2) by Anand et al. (2012), along with a matching lower bound of Ω( α
log α ). Further, in the resource augmentation model, with a 1+ ϵ speed up, we give a
2( 1
ϵ + 1) competitive algorithm, with essentially the same techniques, improving the bound of 1 + O( 1
ϵ 2 ) by
Gupta et al. (2010) and matching the bound of Anand et al. (2012) for the special case of fixed speed unrelated
machines. Unlike the previous results most of which used an amortized local competitiveness argument or
dual fitting methods, we use a primal-dual method, which is useful not only to analyze the algorithms but
also to design the algorithm itself.
CCS Concepts: • Theory of computation → Scheduling algorithms; Convex optimization; Online
algorithms;
Additional Key Words and Phrases: Online algorithm, scheduling, flow-time, energy efficiency
1 INTRODUCTION
The design of online algorithms for scheduling problems has been an active area of research. Typically, in such problems jobs arrive online, over time, and to complete a job it must be assigned
a certain amount of processing, its processing volume. The algorithm has to schedule the jobs on
one or more machines to complete them as soon as possible. A standard objective is (a weighted)
sum of flow-times; the flow-time of a job is the duration of time between its release and completion. In the unrelated machines version of the problem, each job can have a different volume and a
different weight for each machine. Preemption/resumption is allowed, but migration of a job from
one machine to another is not.
Of late, an important consideration in such problems has been the energy consumption. A popular approach to model the energy consumption is via the dynamic speed scaling model: a machine
can run at many different speeds, higher speeds process jobs faster but consume more energy. The
rate of consumption of energy w.r.t. time is the power consumption, which is given as a function
of speed. Typical power functions are of the form sα , where s is the speed and α is some constant,
commonly equal to 2 or 3. The objective now is to minimize the sum of energy consumed and
weighted flow-time. In this article, we show that a natural and principled primal-dual approach
gives almost optimal competitive ratios for the most general version of the problem, with unrelated
machines and arbitrary monotone non-decreasing and convex power functions.
3 We summarize our
contributions in the following.
For power functions f (s) = sα , we give an algorithm with competitive ratio 8α
log2 α . We also give
a corresponding lower bound of α
4 log2 α . This improves upon a previous O(α2) competitive ratio
of Anand et al. (2012). We also show bounds for specific values of α; for α = 2 and 3, we show
competitive ratios of 4 and 5.581, respectively. It is worth noting that these bounds improve upon
the previous best bounds known for these values of α, even for a single machine (which were 5.24
and 8, respectively, for α = 2 and 3, due to Bansal et al. (2009)). For an arbitrary power function f ,
we define a quantity Γf such that for the power function f (s) = sα , Γf = α. Therefore, Γf can be
thought of as a generalization of the quantity α. We show analogous bounds for arbitrary power
functions, an upper bound of 8Γf
log2 Γf
and a lower bound of Γf
4 log2 Γf . No previous bounds were known
for arbitrary power functions. These results are summarized in Table 1.
An alternative objective function often considered, because it is sometimes easier to deal with,
is the fractional flow-time. For the fractional flow-time, imagine that a job is broken into infinitesimally small pieces and each piece has an independent flow-time, which is equal to the time between
its own completion time and its release time. The fractional flow-time is the average flow-time of all
the pieces put together. (Formally, it is defined as an integral.) Partially completing a job decreases
its fractional flow-time, whereas integral flow-time gives no “partial credit.” Our guarantees for
the fractional flow-time are essentially the same as those for the integral flow-time and they are
summarized in Table 2.
1(Bansal et al. 2009). 2(Bansal et al. 2009). 3The assumption that the power functions are monotone non-decreasing and convex is w.l.o.g. Via a standard technique
(e.g., see Section 3.1 of Bansal et al. (2009)), we can convert problems with arbitrary power functions to problems with
non-decreasing and convex power functions.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:3
Table 2. Competitive Ratios for Minimizing Fractional Flow-Time Plus Energy (Section 3)
Best known This article
General α < 2 α = 2 α = 3 General
Single machine
arbitrary f (s)
2 2 α 2 2.791 O( α
log2 α )
Unrelated machines
f (s) = sα
O(α) 4 2α 4 5.581 Θ( α
log2 α )
(Theorem 3.12, 6.1)
Unrelated machines
arbitrary f (s) (new) Θ
 Γf
log2 Γf

(Theorem 3.12, 6.1)
Table 3. Competitive Ratios for Minimizing Fractional/Integral Flow-Time Plus Energy (Any Power
Function f (s)) with Resource Augmentation ((1 + ϵ )-speed) (Section 5)
Best known This article
Integral flow-time plus energy arbitrary f (s) 1 + O( 1
ϵ 2 ) 5 2 + 2
ϵ (Theorem 5.1)
Fractional flow-time plus energy arbitrary f (s) 1 + 5
ϵ 5
Integral flow-time, fixed speed 2 + 2
ϵ 4 2 + 2
ϵ (Theorem 5.2)
Fractional flow-time, fixed speed 2 + 2
ϵ 6
We also consider the resource augmentation model, where for the same given power, machines
used by the algorithm run 1 + ϵ times faster than the machines used by the offline optimum. Our
techniques (essentially the same proof with minor modifications) extend to this model as well. We
show a competitive ratio of 2( 1
ϵ + 1), which improves upon the previous best known bound of
1 + O( 1
ϵ 2 ) by Gupta et al. (2010). As a special case when the power function is a 0–∞ step function,
this bound matches the best known competitive ratio for minimizing the weighted flow-time on
fixed speed unrelated machines (obtained in Anand et al. (2012) and Chadha et al. (2009)). These
results are summarized in Table 3.
Different machines can have different power functions, in which case the competitive ratio is
determined by the worst one.
Techniques and Proof Overview. The most common and successful technique for analyzing online
algorithms for such scheduling problems has been the amortized local competitiveness argument.
The technique calls for a potential function that “stores” the excess cost incurred by the optimum
solution and uses it as needed to pay for the algorithm’s solution. More recently Anand et al. (2012)
used the dual fitting method to give several improved competitive ratios. We use a primal-dual
approach, which is a principled approach that is used to guide the design of the algorithm itself
in addition to being a tool for the analysis. Our algorithms are based on a convex programming
relaxation. Most of the work done is in understanding the structure of the convex program and the
properties of the optimal primal and dual solutions; the algorithm and the analysis follow naturally
after that. In other words, we derive the algorithm from the structure of the convex program.
Almost all the special cases of our problem, such as related machines, a single machine,
unweighted flow-time, and so on, use the following speed scaling rule introduced by Albers and
4(Anand et al. 2012). 5(Gupta et al. 2010). 6(Chadha et al. 2009).
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.  
5:4 N. R. Devanur and Z. Huang
Fujiwara (2007): set the speed so that the power consumed is equal to the remaining weight
(PERW). The power is the rate of energy consumed and the remaining weight can be thought of as
the rate of the weighted flow-time (since the weighted flow-time is the integral of the remaining
weight over time). What the PERW rule, therefore, ensures is that the energy consumed and the
weighted flow-time are both equal, which is convenient for the analysis. PERW may also appear
to be the greedy choice, that is, the optimal choice if no more jobs are released.7 A more careful
analysis shows that the optimal speed to set, if no more jobs are released, is s so that f ∗ (f 
(s))
equals the remaining weight. Here, f ∗ is the Fenchel conjugate of f , and f ∗ (f 
(s)) has the following
geometric interpretation: if you draw a tangent to f ats, then this is the length of the y-intercept of
the tangent. We note that in the most-studied case of f (s) = sα , the speed given by PERW is only
a constant factor away from the speed characterized by f ∗ (f 
(s)) being equal to the remaining
weight. However, the two characterizations could be quite far off for general power functions.
We first analyze the natural greedy algorithm that follows from this speed scaling rule coupled
with a greedy job assignment policy: a job is assigned to the machine for which the increase in the
energy plus flow-time is the smallest. This gives an O(α) competitive ratio, which already beats
the previous bound of O(α2), which uses the greedy job assignment policy along with the PERW
speed scaling rule.
Setting the speed so that no other job arrives in the future is really a conservative approach.
The algorithm should anticipate the arrival of some jobs in the future and run faster. Such an
aggressive algorithm faces a trade-off: the improvement obtained when there are more jobs in the
future versus the degradation when there aren’t. The algorithm should hedge against both the
cases and balance the competitive ratio. A systematic way to do this is to set the speed s so that
f ∗ (f 
(s/c)) equals remaining weight, for some constantc, obtain a competitive ratio as a function
ofc and then setc to minimize the competitive ratio. For instance, for the power function f (s) = sα ,
the best choice of c we have turns out to be 1 + ln α−1
α , giving a competitive ratio of 8α
log2 α .
We start our analysis by considering the weighted fractional flow-time plus energy as an objective (Section 3). The fractional flow-time is commonly used, since it is easier to handle than
integral flow-time. Typically, algorithms designed for the fractional flow-time also work for the
integral flow-time with some loss in the competitive ratio. However, our analysis for the fractional
flow-time also goes through for the integral flow-time (with small modifications) without any loss
in the competitive ratio!
For the fractional flow-time, we start by giving a convex programming relaxation and its dual
using Fenchel conjugates. We then analyze the simple case of scheduling a single job on a single
machine and derive the speed scaling rule f ∗ (f 
(s)) = remaining (fractional) weight as optimal.
We then consider many jobs on a single machine, all released at time 0, and show that the same
speed scaling rule is optimal, along with the job selection rule of highest density first (HDF, density
= weight/volume). We also characterize the optimal dual solution for the same and show several
structural properties of the optimal primal and dual solutions. Finally, we consider the unrelated
machines case, which calls for a job assignment rule (which assigns jobs to machines). At any time,
given the job assignments, the algorithm uses the optimal schedule for each machine assuming no
future jobs arrive, and the corresponding dual. The job assignment rule follows from the complementary slackness conditions using these duals. This assignment rule is almost the same as the
greedy assignment rule, but is slightly different. We state the job assignment rule derived from
7We know of at least one notable researcher in the area who thought so. A source of this misconception may be that this
rule appears to achieve the optimal balance between energy and flow-time by equalizing their rates, respectively, power
and remaining weight. However, the speed chosen at any time affects the remaining weight for all future but only the
instantaneous power, so this is misleading. It would be optimal if both quantities were instantaneous for instance.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:5
the complementary slackness conditions in the algorithm, since that seems more principled given
our methodology. In any case, the analysis remains the same for both job assignment rules. The
competitive ratio of O(α) follows from a simple local charging of the primal to the dual cost, along
with some of the structural properties established earlier.
We then consider a systematically aggressive speed scaling rule, f ∗ (f 
(s/c)) = remaining weight
for some constantc (Section 3.5). With the rest of the algorithm/proof more or less identical, we derive a competitive ratio as a function ofc. On optimizing, we get a competitive ratio ofO(α/ log α).
Almost the same proof structure works for the integral case, and we outline what minor modifications are required (Section 4). Once again essentially the same proof goes through for the
resource augmentation model as well (Section 5). Finally, we show a simple example with two
machines that gives almost tight lower bounds (Section 6).
Comparison with Previous Algorithms. Our algorithms use the same job selection rule, namely
HDF, as in previous algorithms.
Our job assignment rule, which is driven by the complementary slackness conditions, is almost
the same as the greedy assignment rule in previous work. The greedy rule assign a job to the
machine that incurs the minimum increase in weighted flow-time plus energy. Our algorithm is
slightly different, since the complementary slackness conditions capture the marginal cost of fractional instead of integral assignment of jobs. In any case, our analysis (and, hence, the competitive
ratio bound) is also valid for the greedy assignment rule. We state the job assignment rule derived
from the complementary slackness conditions, since that is more consistent with the primal-dual
principle that we adopt in this article.
The main difference between our algorithms and previous algorithms lies in the speed-scaling
rule. Previous algorithms use the PERW rule, that is, choosing speed s such that f (s) equals the
remaining weight. Our algorithms, on the other hand, choose speed s such that f ∗ (f 
(s/c)) equals
the remaining weight for some parameterc ≥ 1. This rule is driven by the optimality conditions of
the convex programs. For the special case when f (s) = sα , the speed chosen by our speed scaling
rule and that by PERW coincide up to a constant factor. In fact, our framework allows us to also analyze the PERW rule for power functions f (s) = sα , which corresponds to a choice ofc = (α − 1)
1/α .
We show that the competitive ratio with the PERW speed scaling rule is still O( α
log α ), thus giving
a justification for using this rule as well. For general power functions, however, the two speed
scaling rules may pick very different speeds. We leave as an interesting open question whether
PERW can also obtain near optimal competitive ratio for general power functions.
Related Work
We summarize a selection of the related work that is most relevant to this article here. Energy
considerations in scheduling problems are motivated by the fact that energy costs are now a substantial part of the overall cost in data centers (see Barroso (2005)). Mobile devices with limited
battery also call for careful energy management. Most of the early work on energy-efficient algorithms was for the case of a single machine. Albers and Fujiwara (2007) introduced the objective of
energy plus flow-time in the dynamic speed scaling model, and the speed scaling rule of power =
remaining number of jobs. Generalizations to weighted flow-time followed (Bansal et al. 2009,
2008; Lam et al. 2008) with the current best competitive ratios given by Bansal et al. (2009) and
Andrew et al. (2009). For other variants in the dynamic speed scaling model, and other models
such as the power-down model and the importance of energy-efficient algorithms, see the survey
by Albers (2010).
Motivated by the design of architectures with heterogenous cores/processors, Gupta et al. (2010)
considered the case of related machines with different power functions. (See the references therein
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
5:6 N. R. Devanur and Z. Huang
for more reasons to consider heterogenous machines.) All these algorithms use the PERW speed
scaling rule and use potential functions with amortized local competitiveness arguments. Anand
et al. (2012) used a dual fitting based argument to generalize this to unrelated machines and the
fixed speed case in the resource augmentation model (improving upon a previous algorithm by
Chadha et al. (2009)). The major difference between dual fitting and primal dual is that dual fitting is only used as an analysis tool for a given algorithm while primal dual guides the design of
the algorithm itself. Also in recent work (Gupta et al. 2012) showed that the natural extensions of
several well known algorithms that work for homogeneous machines fail for heterogeneous machines, thus justifying the use of “non-standard” algorithms of Gupta et al. (2010) and Anand et al.
(2012). As summarized in Tables 1-3, our work unifies and improves upon several of these results,
most prominently (Bansal et al. 2009, 2009; Anand et al. 2012; Gupta et al. 2010; Chadha et al. 2009).
Buchbinder and Naor (2009) established the primal-dual approach for packing and covering
problems, unifying several previous potential function based analysis. In spite of the extensive research done in the problem of minimizing flow-time plus energy, this is the first time a primal-dual
algorithm has been proposed for these problems. To our knowledge, there is only one other work
that applies the primal-dual technique on energy-efficient online scheduling due to Gupta et al.
(2012). Gupta et al. (2012) considered a scheduling problem with load-balancing as the objective,
which is fundamentally different from the problems considered in this article. In concurrent and
independent work, Nguyen (2013) showed an O(α/ log α)-competitive algorithm for minimizing
flow-time plus energy on unrelated machines when the power function is f (s) = sα . The analysis
of Nguyen (2013) uses dual fitting based on a non-convex program and Lagrangian duality.
Subsequent to the conference version of this article, Angelopoulos et al. (2015) used the primaldual approach to derive competitive online algorithms for a family of objectives that generalize
minimizing weighted flow-time on a single machine.
Our work can also be seen as extending the primal-dual technique to convex programs, although
our contribution is more towards understanding the structure of the convex programs under consideration. The one novel aspect we introduce is considering a parameterized family of algorithms
with the parameter (c) controlling the aggressiveness of the algorithm and optimizing the parameter to get the best competitive ratio. This seems like a general technique that must be applicable
elsewhere. The primal-dual approach with convex programs was also used for an online concave
matching problem to get optimal competitive ratios by Devanur and Jain (2012). However, the
main technical difficulty in Devanur and Jain (2012) was in proving the lower bound; in contrast
our lower bounds are quite simple. Also the convex programs considered here are structurally very
different from the ones in Devanur and Jain (2012), therefore the similarities in the approaches are
only superficial.
2 CONVEX CONJUGATES AND FENCHEL DUALITY
Since the natural formulation of the objective of minimizing flow-time plus energy is a convex
(instead of linear) function of the decision variables, we consider an appropriate convex programming relaxation of the problem that is similar to that in Anand et al. (2012). Anand et al. (2012)
consider the Lagrangian dual in their dual fitting approach. Instead, we consider the Fenchel dual.
Fenchel duality is essentially the same as Lagrangian duality, but it adopts the concept of convex conjugates of convex functions, which significantly simplifies the notations and analysis and
makes it much easier to derive the optimal algorithms. Interested readers may try to duplicate our
results using Lagrangian duality.
We include a brief explanation of how to obtain the dual in this section and refer the reader to the
Appendix or Boyd and Vandenberghe (2004) and Cole et al. (2010) for a more detailed explanation
and other examples.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:7
Suppose that f : R → R is a function. The conjugate of f is a function f ∗ : R → R such that
f ∗ (μ) := sup
x

μx − f (x)

.
Although the conjugate is defined for any function f , for the rest of the article, we will assume that
f is strictly convex and differentiable, since this is the most interesting and most commonly studied
case by the theoretical computer science community to the applications we discuss. (Nevertheless,
there are interesting applications where the power function is not strictly convex, e.g., discrete
speed levels. We leave these cases for future study.)
Properties of f ∗:
— f ∗ is strictly convex and differentiable. (This property holds even if f is not strictly convex
and differentiable.)
— f ∗∗ = f . (Here, we use the assumption that f is strictly convex and differentiable.)
—If μ and x are such that f (x) + f ∗ (μ) = μx, then f 
(x) = μ and (f ∗)
(μ) = x.
—Vice versa, if f 
(x) = μ, then (f ∗)
(μ) = x and f (x) + f ∗ (μ) = μx.
We say that (x, μ) form a complementary pair wrt f if they satisfy one of the last two conditions
stated previously.
Convex programs with linear constraints. Consider the following (primal) optimization problem:
Minimize 
i
cixi + fi (xi ) s.t.
∀j :

i
aijxi ≥ bj ,
∀i : xi ≥ 0.
We can derive a minimization problem that is the dual of this, using Lagrangian duality. This is
usually a long calculation. Here, we identify a shortcut for the same:
Maximize 
j
bjλj −

i
f ∗
i (μi ) s.t.
∀i :

j
aijλj ≤ ci + μi,
∀j : λj ≥ 0.
Note the similarity to LP duality. The differences are as follows. Suppose the convex part of the
primal objective is 
i fi (xi ). There is an extra variable μi for every variable xi that occurs in f . In
the constraint corresponding to xi , μi appears on the right-hand side (RHS) along with the constant
term. Finally, the dual objective has − 
i f ∗
i (μi ) in addition to the linear terms. In other words, we
relax the constraint corresponding to xi by allowing a slack of μi , and charge − 
i f ∗
i (μi ) to the
objective function.
The optimum for the primal program is lower than the optimum for the dual program (weak
duality). In fact, if the primal constraints are strictly feasible, that is there exist xi such that for all
j,

i aijxi < bj , then the two optima are the same (strong duality) and the following generalized
complementary slackness conditions characterize them:
—xi > 0 ⇒ 
j aijλj = ci + μi ;
—λj > 0 ⇒ 
i aijxi = bi ; and
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
5:8 N. R. Devanur and Z. Huang
—xi and μi form a complementary pair w.r.t. fi , that is, μi = f 
i (xi ), xi = (f ∗)
i (μi ) and fi (xi ) +
f ∗
i (μi ) = μixi .
3 WEIGHTED FRACTIONAL FLOW-TIME PLUS ENERGY
In this section, we consider the objective of fractional flow-time plus energy and obtain competitive
algorithms for it. Suppose that we are given a power function, f : R+ → R+, which is monotonically non-decreasing, convex, and is 0 at 0. Further, we assume that f is also differentiable. For
ease of presentation, we assume that all the machines have the same power function, although all
of our results go through easily with different power functions for different machines.
Input: A set of jobs J and a set of machines M. For each job j ∈ J its release time rj ∈ R+. For
each job j ∈ J and each machine i ∈ M, the volume vij and the weight wij of job j if scheduled on
machine i. Let the density of job j on machine i be ρij = wij/vij .
Output: An assignment of each job to a single machine (no migration). For each machine i and
time t ∈ R+, the job scheduled at time t on machine i, denoted by ji (t) and the speed at which the
machine is run, denoted by sit .
Constraints: Job j must be scheduled only after its release time. It must receive a total amount of
vij units of computation if it is assigned to machine i:

t ∈[rj,+∞]:ji (t)=j
sit dt = vij .
Objectives: The objective has two components, energy and fractional flow-time. Recall that f is
the power function, which gives the power consumption as a function of the speed. Power is the
rate at which energy is consumed therefore energy consumed is the integral of power over time.
The energy consumed by machine i is therefore
Ei =
 ∞
0
f (sit )dt.
The fractional flow-time is an aggregated measure of the waiting time of a job. Suppose job j is
scheduled on machine i. Let vˆj (t) be the remaining volume of job j at time t, that is,
vˆj (t) = vij −

t∈[rj,t]:ji (t)=j
si (t
)dt
.
The fractional flow-time of job j is then defined to be
Fj := 1
vij  ∞
rj
vˆjdt.
The objective is to minimize the total energy consumed by all the machines and a weighted sum
of the flow-times of all the jobs:

i
	
Ei +

j:j→i
wij Fj


.
In the online version of the problem the details of job j are given only at time rj . The algorithm
has to make decisions at time t without knowing anything about the jobs released in the future.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:9
3.1 Convex Programming Relaxation and the Dual
The algorithms we design are based on a convex programming relaxation of the problem and its
dual, which are as follows. The dual convex program is obtained using Fenchel duality. (We will
omit the trivial constraints such as sijt ≥ 0 throughout this article.)
(Pfrac) Minimize 
i,j
ρij  ∞
rj
(t − rj)sijtdt +

i
 ∞
0
f (sit )dt
+

i,j
 ∞
rj
sijt
wij 	 wi j
0
(f ∗)
−1 (w)dw

dt s.t.
∀i,t : sit =

j:rj ≤t
sijt, (1)
∀j :

i
 ∞
rj
sijt
vij
dt ≥ 1, (2)
(Dfrac) Maximize 
j
αj −

i
 ∞
0
f ∗ (βit )dt s.t.
∀i, j,t ≥ rj : αj
vij
≤ ρij (t − rj) + βit +
1
wij  wi j
0
(f ∗)
−1 (w)dw.
The variables sijt denote the speed at which job j is scheduled on machine i at time t. sit = 
j sijt is the total speed of machine i at time t. The first summation in the objective function
corresponds to the fractional flow-time: sijtdt units of job j is processed between t and t + dt, all
of which waited for a duration of t − rj resulting in (t − rj)
sijt
vi j dt amount of fractional flow-time.
The second summation is the total energy consumed. The third summation is required, because
the convex program allows a job to be split among many machines and even have different parts
run in parallel. This sometimes allows the convex program to have a much lower objective than
the optimal solution to the problem. We will explain how the third term fixes this problem a little
later. Constraint (1) simply defines sit . For each job j, constraint (2) enforces that the scheduling
must complete job j. Hence, the primal program is a valid relaxation of the scheduling problem
and the first two terms in the objective capture the fractional flow-time and energy cost of the
given schedule.
Next, we explain why the first two terms in the primal objective are not enough to give a good
lower bound for the cost of the optimal schedule and why we introduce the third term. Note that
we do not enforce that all of job j must be processed on the same machine therefore both job
migrations and parallel processing of the same job on multiple machines are allowed. Consider
an instance with only one job released at time 0 and a large number of machines. The optimal
solution to the convex program schedules the job simultaneously on all the machines and the total
cost w.r.t. the first two terms will tend to zero as the number of machines tends to infinity. The
optimal algorithm has to schedule the job on a single machine and hence pays a fixed non-zero
cost. Without the third term, the convex program fails to provide a good lower bound on the cost
of the optimal solution.
Finally, we demonstrate how we derive the third term in the primal objective. Consider a modified instance where we have multiple copies of each machine (as many as the number of jobs);
the cost of the optimal solution to this instance is only lower. In this modified instance, w.l.o.g.,
no two jobs are ever scheduled on the same machine. It can be shown (Lemma 3.3) that if job j is
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
5:10 N. R. Devanur and Z. Huang
scheduled on a copy of machine i all by itself, then the optimal cost (energy + flow-time) due to
job j is vi j
wi j
 wi j
0 (f ∗)
−1 (w)dw. Now, still allowing a job to be split among different machines, an
 ∞
rj
sijt
vi j dt fraction of job j is scheduled on machine i. Thus, 
i
 ∞
rj
sijt
wi j (
 wi j
0 (f ∗)
−1 (w)dw)dt is a
lower bound on the cost of scheduling job j.
The preceding discussion implies that the optimum of the convex program with an additional
factor of 2 is a lower bound on opt, the optimum offline solution to the problem. We note this in
the following theorem.
Theorem 3.1. The optimum value of the convex crogram (Pfrac) is at most 2opt.
The algorithm heavily uses the structure of the optimal solutions to the primal and the dual programs. We explain this structure in stages. There is a natural decomposition of the problem itself:
at the highest level is the decision to allocate a job to one of the machines. Given these choices,
the rest of the problem decomposes into a separate one for each machine. For each machine, given
the set of jobs that have to be scheduled on it and the volumes and densities, there is the problem
of picking the job to schedule and the speed to set at any time, to minimize the total energy and
flow-time on that machine. Further, given the choice of the job to schedule on a machine, there
is an even simpler problem of setting the speed. We start with the simplest problem of all, given
just a single job and a single machine, what is the optimal speed schedule that minimizes the total
energy and fractional flow-time.
3.2 Optimal Scheduling for Single Job
We obtain a simpler convex program and its dual for the problem of scheduling a single job on a
single machine. We drop the third term in the objective, since that deals with non-integral assignment of jobs to machines. Since there is only one job in this subsection, we assume w.l.o.g. that
rj = 0:
Minimize  ∞
0
ρijtsijtdt +
 ∞
0
f (sit )dt s.t. (3)
∀t : sit = sijt,  ∞
0
sijt
vj
dt ≥ 1.
Maximize αj −
 ∞
0
f ∗ (βit )dt s.t.
∀t : αj
vij
≤ ρijt + βit .
Recall that the conjugate function f ∗ is defined as
f ∗ (β) := sup
s
{βs − f (s)}.
f ∗ is also convex and monotonically non-decreasing. If f is strictly convex, then so is f ∗. The most
important property we use about f ∗ is the notion of a complementary pair. β and s are said to be
a complementary pair if any one of the following conditions hold. (It can be shown that if one of
them holds, then so do the others.)
(C1) f 
(s) = β.
(C2) (f ∗)
(β) = s.
(C3) f (s) + f ∗ (β) = sβ.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:11
The optimal solutions to these programs are characterized by the (generalized) complementary
slackness or KKT conditions. These are
(K1) ∀ t, sijt > 0 ⇒ αj = vij (ρijt + βit ).
(K2) αj > 0 ⇒  ∞
0
sijt
vj dt = 1.
(K3) βit and sit are a complementary pair for all t.
The first condition implies that for the entire duration that the machine is running (with non-zero
speed), the quantity ρijt + βit remains the same, since it must always equal αj . In other words, βit
must linearly decrease with time, at the rate of ρij :
dβit
dt = −ρij . (4)
The main result in this section is that the optimum solution has a closed form expression where
sit and βit are set as a function of the remaining weight of the job at time t, which we denote by
wˆit . (More generally, wˆit will denote the total remaining weight of all the jobs on machine i.) Also
the remaining volume at time t is denoted vˆit .
Lemma 3.2. The optimum solution to the convex program Equation (3) is such that
(1) f ∗ (f 
(sit )) = f ∗ (βit ) = wˆit ,
(2) αj = vij f ∗−1 (wij).
Proof. Since βit and sit form a complementary pair, we have that
d f ∗ (βit )
dβit
= sit .
We start by multiplying this with Equation (4):
d f ∗ (βit )
dβit
dβit
dt = −sit ρij ,
d f ∗ (βit )
dt = ρij
dvˆit
dt = dwˆit
dt .
When wˆit = 0, then sijt = 0, f (sijt ) = 0 and therefore f ∗ (βit ) = 0 (by the third property of complementary pairs). Therefore, at any time f ∗ (βit ) = wˆit . The rest of the assertions in the lemma
follow immediately.
We also give a closed form for the value of the optimum. This form justifies the inclusion of the
third term in the objective for the convex program (Pfrac). We will also use this lemma later to
analyze how the cost of the optimum solution changes as we add new jobs.
Lemma 3.3. The cost of the optimal solution is
1
ρij  wi j
0
(f ∗)
−1 (w)dw.
Proof. Recall that the total weighted flow-time is equal to
 ∞
0
wˆitdt
and the total energy is equal to
 ∞
0
f (sit )dt.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017. 
5:12 N. R. Devanur and Z. Huang
From Lemma 3.2, f ∗ (βit ) = wˆit . Using this and the properties of complementary pairs, we get the
following sequence of equalities:
(wˆit + f (sit )) dt = (f ∗ (βit ) + f (sit )) dt = βitsitdt.
Further, by the definition of sit and βit , the preceding equals
−βitdvˆit = − 1
ρij
βitdwˆit = − 1
ρij
f ∗−1 (wˆit )dwˆit .
The lemma follows from observing that as t goes from 0 to ∞, wˆit goes from wij to 0.
3.3 Optimal Scheduling for a Single Machine
We now consider the next stage where there are multiple jobs to be scheduled on a single machine,
and the corresponding convex programs. We will continue to assume that rj = 0 for all jobs j:
Minimize 
j
 ∞
0
ρijtsijtdt +
 ∞
0
f (sit )dt s.t. (5)
∀t : sit =

j
sijt,
∀j :
 ∞
0
sijt
vij
dt ≥ 1;
Maximize 
j
αj −
 ∞
0
f ∗ (βit )dt s.t.
∀t, j : αj
vij
≤ ρijt + βit .
Lemma 3.4. The optimum solution to the convex program Equation (5) is such that
(1) Jobs are scheduled in the decreasing order of density.
(2) f ∗ (f 
(sit )) = f ∗ (βit ) = wˆit .
(3) αj = wijt ∗ +vij f ∗−1 (wˆit ∗ ) where t ∗ is the first time job j is scheduled.
Proof. The complementary slackness conditions for these pair of programs are essentially the
same as in the single-job case. To begin with,
sijt > 0 ⇒
αj
vij
= ρijt + βit .
As before, this implies that βit decreases at rate ρij whenever job j is scheduled, but the main new
issue is the choice of jobs to schedule. The preceding complementary slackness condition implies
that job j must be scheduled when the term ρijt + βit attains its minimum. The first part, ρijt,
always increases at rate ρij , while the second part, βit , decreases at rate ρij(t), where j(t) is the job
scheduled at time t. So, if ρij < ρij(t), then ρijt + βit is decreasing and vice-versa; if ρij > ρij(t),
then ρijt + βit is increasing. This implies that the “highest density first” (HDF) rule is optimal, that
is, schedule the jobs in the decreasing order of the density. For any j, ρijt + βit first decreases when
higher-density jobs are scheduled, then remains constant as job j is scheduled and then increases
as lower density jobs are scheduled.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017. 
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:13
Given the choice of jobs scheduled, the choice of speed is very similar to the single-job case.
Since βit and sit form a complementary pair, we have that
d f ∗ (βit )
dβit
= sit .
Multiply this with d βi t
d t = −ρij(t), where j(t) is the job scheduled at time t, we get
d f ∗ (βit )
dβit
dβit
dt = −sit ρij(t),
d f ∗ (βit )
dt = ρij
dvˆit
dt = dwˆit
dt .
When wˆit = 0, then sijt = 0, f (sijt ) = 0, and therefore f ∗ (βit ) = 0 (by the third property of complementary pairs). Therefore, at any time f ∗ (βit ) = wˆit .
Finally, assertion (3) follows by that the optimal αj must be the smallest number that satisfies
the dual constraints given the values of βit ’s.
Unlike the single-job case, we no longer have a closed form expression for the optimal cost.
Instead, we consider the marginal increase in the optimal cost due to a single job, and we show
the following generalization of Lemma 3.3.
Lemma 3.5. The increase in the cost of the optimal solution on introducing a new job j is at most
wijt
∗ +
1
ρij  wi j
0
f ∗−1 (wˆit ∗ + w)dw,
where t ∗ is the first time job j is scheduled and wˆit ∗ is the remaining weight at time t ∗ in the original
instance, without job j.
Proof. It suffices to show that even if we use a sub-optimal speed-scaling after introducing the
new job j, the increase in the cost is only the amount as stated in the lemma. In particular, consider
the sub-optimal scheduling in which we keep the schedule till time t ∗ unchanged, and then start
to schedule job j and use the optimal speed scaling.
First, note that job j waits till time t ∗, contributing wijt ∗ to the total flow-time.
Next, we consider the rest of the increase of the objective. Instead of introducing job j in one shot,
consider introducing it continuously by infinitesimal amounts. For any 0 ≤ w ≤ wij , suppose we
have already introduced weightw of job j. Consider introducing an additional infinitesimal weight
dw at time t ∗. This causes the flow-time to increase by
dF = (wˆit ∗ + w)dt,
as the jobs delayed by to this new infinitesimal amount of job j, including the first w weight of job
j itself, is wˆit ∗ + w.
Further, this also causes the energy consumption to increase by
dE = f (sit ∗ )dt,
where at this point βit ∗ and sit ∗ satisfy that f ∗ (βit ∗ ) = wˆit ∗ + w and f ∗ (βit ∗ ) + f (sit ∗ ) = βit ∗sit ∗
due to complementary slackness (recall that we use the optimal speed scaling after time t ∗).
Similar to the proof of Lemma 3.4, we now use these conditions to derive a sequence of equalities
as follows:
(wˆit ∗ + w + f (sit ∗ )) dt = (f ∗ (βit ∗ ) + f (sit ∗ )) dt = βit ∗sit ∗dt.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017. 
5:14 N. R. Devanur and Z. Huang
Further, by the definition of sit ∗ and βit ∗ , the preceding equals
−βit ∗dvˆit ∗ = 1
ρij
βit ∗dw = 1
ρij
f ∗−1 (wˆit ∗ + w)dw.
The lemma then follows by integrating w from 0 to wij .
Finally, we note a couple of simple observations that follow almost immediately from Lemma 3.4.
Lemma 3.6.  ∞
0 f ∗ (βit )dt is equal to the total flow-time.
Let
Γf = maxs
f ∗ (f 
(s))
f (s) + 1.
Lemma 3.7. The total flow-time is at most Γf − 1 times the total energy consumed.
Proof. The total energy used is  ∞
0 f (sit )dt. The total flow-time is  ∞
0 wˆitdt. Recall that by
Lemma 3.4, sit is chosen such that wˆit = f ∗ (f 
(sit )). So the ratio between the fractional flow-time
and the energy is  ∞
0 f ∗ (f 
(sit ))dt divided by  ∞
0 f (sit )dt, which is at most maxs f ∗ (f 
(s ))
f (s ) .
3.4 Conservative Greedy Algorithm
In this section, we analyze a primal-dual algorithm that we call conservative-greedy. The basic idea
is that given the choice of job assignments to machines, the algorithm schedules the jobs as if no
other jobs will be released in the future. That is, it schedules the jobs as per the optimal schedule
for the current set of jobs, as detailed in the previous section. The choice of job assignments to machines is done via a natural primal-dual method, the one dictated by the complementary slackness
conditions. We will later consider a more aggressive algorithm; if a job is released in the future,
then it is better to run faster than what the current optimal solution suggests. To the contrary, if
there is no future job then running faster is sub-optimal. The aggressive algorithm balances these
trade-offs.
First, though, we describe the conservative-greedy algorithm. At any point, given the jobs already released and their assignment to machines, the algorithm picks the optimal scheduling on
each machine, assuming no future jobs are released. This also gives dual solutions, in particular
the variables βit for all i and t in the future. When a new job j is released, its assignment to a machine is naturally driven by the following dual constraints and the corresponding complementary
slackness conditions. For all i,t,
αj
vij
≤ ρij (t − rj) + βit +
1
wij  wi j
0
(f ∗)
−1 (w)dw.
For a given machine i, we saw earlier that the RHS is minimized (over all t) at t ∗
i , where t ∗
i would be
the first time job j is scheduled on i given the HDF rule. That still holds true, since the third term is
independent of t. Now, we need to minimize over all i as well, and the algorithm does exactly this.
It assigns job j to the machine i that minimizes the RHS of the preceding inequality with t = t ∗
i .
It sets the dual αj so the corresponding constraint is tight. It then updates the schedule and βit ’s
for machine i. Note that as we add more jobs, the βit ’s can only increase, thus preserving dual
feasibility. The entire algorithm is summarized in Figure 1.
We will show that, surprisingly, such a conservative approach already achieves a meaningful
competitive ratio for arbitrary power functions and a near optimal competitive ratio for polynomial
power functions. Formally, we will show the following theorem in this section.
Theorem 3.8. The fractional conservative greedy algorithm is 2Γf -competitive for minimizing
weighted fractional flow-time plus energy.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.  
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:15
Fig. 1. The conservative greedy online scheduling algorithm for minimizing fractional flow-time plus energy
with arbitrary power functions.
The preceding competitive ratio might be unbounded if the function is highly skewed. Theorem 3.8 does not contradict known lower bounds for fixed-speed online scheduling, which is a
special case when the power function is a 0-∞ step function. For nice power functions such as polynomial power functions, the preceding theorem gives meaningful competitive ratios. In particular,
when we plug in the polynomial power functions, we have the following corollary of Theorem 3.8.
Corollary 3.9. The fractional conservative greedy algorithm is 2α-competitive for power function
f (s) = sα for minimizing weighted fractional flow-time plus energy.
Proof of Theorem 3.8. In the remainder of this section, we will present the primal-dual analysis of Theorem 3.8. It is easy to see that the algorithm constructs feasible primal and dual solutions
and the ratio is obtained by relating the cost of the primal to that of the dual. In fact, for every job
released, we relate the increase in the cost of the primal to the increase in the cost of the dual.
Lemma 3.10. When job j is released, the increase in the total cost of the algorithm is at most αj .
Proof. Suppose job j is assigned to machine i and will be scheduled at t ∗ (dropping subscript
i) according to the HDF rule on the current set of jobs assigned to i. Then,
αj = wij (t
∗ − rj) +vij βit ∗ +
1
ρij  wi j
0
(f ∗)
−1 (w)dw.
The increase in the total cost of the algorithm is only the increase in that for machine i. From
Lemma 3.5, this is at most
wij (t
∗ − rj) +
1
ρij  wi j
0
(f ∗)
−1 (w + wˆit ∗ )dw.
Comparing the two, it suffices to show that
βit ∗ +
1
wij  wi j
0
(f ∗)
−1 (w)dw ≥
1
wij  wi j
0
(f ∗)
−1 (w + wˆit ∗ )dw. (6)
Recall that by Lemma 3.4, we have βit ∗ = (f ∗)
−1 (wˆit ∗ ). Further, by the convexity of f ∗, we get
that (f ∗)
−1 is concave. So, we have
(f ∗)
−1 (wˆit ∗ ) + (f ∗)
−1 (w) ≥ (f ∗)
−1 (w + wˆit ∗ ),
for all 0 ≤ w ≤ wij . Integrate this inequality for w from 0 to wij , and we get Equation (6) as
desired.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017. 
5:16 N. R. Devanur and Z. Huang
Proof Theorem 3.8. Consider the release of a new job j. The change in the dual cost, ΔD is
equal to αj plus the change in the contribution of βit ’s for the machine job j was assigned to, say
machine i:
ΔD = αj − Δ
	 ∞
0
f ∗ (βit )dt

.
Let the change in the total energy and the total flow-time of the algorithm be ΔE and ΔF , respectively. From Lemma 3.10,
αj ≥ ΔE + ΔF .
Earlier, in Lemma 3.6, we showed that the total flow-time is always equal to  f ∗ (βit )dt. Thus, the
same holds for the difference:
Δ
	
f ∗ (βit )dt

= ΔF .
From the preceding three inequalities, we observe that the change in the dual cost is at least the
change in the energy cost of the algorithm. Since this holds for every change, and both the dual
cost and the energy cost of the algorithm are zero to begin with, it follows that the final dual cost
is at least the final energy cost of the algorithm:
ΔD ≥ ΔE ⇒ D ≥ E.
We also observed in Lemma 3.7 that the total flow-time and the energy are within a factor of Γf − 1.
Even though that was for a single machine with no future jobs, it is easy to see that the same holds
true for the conservative-greedy algorithm as well. Therefore,
F ≤ (Γf − 1)E.
The total cost of the algorithm, alg, can now be bounded in terms of the energy cost alone, which
is bound by the preceding dual:
alg = E + F ≤ Γf E.
Also, by Theorem 3.1 the dual is a lower bound on 2opt:
D ≤ 2opt.
Putting it all together, we get the conclusion as needed:
alg ≤ 2Γf opt.
An alternative algorithm with essentially the same analysis is the following: assign job j to
machine i for which the increase in the total cost is the minimum. The dual αj must, however, be
set as we do currently, so there might be a disconnect between which machine the job is assigned to
and which machine dictates the dual solution. The analysis still is pretty much the same, however.
3.5 Aggressive Greedy Algorithm
In the conservative greedy algorithm, the speed is scaled to the conservative extreme as the speed
is optimal assuming no future jobs will arrive. However, in an online instance there might be future
jobs and some of these future jobs will be effectively delayed by the current job. Therefore, a good
online algorithm should take this into account when choosing the speed. In this section, we will
consider a family of algorithms with different aggressiveness in terms of speed scaling. Given any
parameterC ≥ 1, theC-aggressive greedy algorithm for minimizing weighted fractional flow-time
plus energy is given in Figure 2.
We will need the following property of the βit ’s, which implies that our choice of βit ’s can be
derived from the primal-dual analysis and ensures dual feasibility.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017. 
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:17
Fig. 2. The aggressive greedy online scheduling algorithm for minimizing weighted fractional flow-time plus
energy with arbitrary power functions.
Lemma 3.11. For any time t at which no new jobs are released, βit must linearly decrease with
time, at the rate of ρij , where j is the job being processed on i at time t, that is,
dβit
dt = −ρij . (7)
Proof. First, note that Equation (7) is satisfied by the conservative greedy algorithm due to
Equation (4). Next, further note that the C-aggressive greedy algorithm is running at exactly C
times the speed of the conservative greedy algorithm given the same remaining weight of jobs. So,
if we set βit ’s the same way as in conservative greedy, that is, f ∗ (βit ) = wˆit equals the remaining
weight at time t, we would have that βit decreasesC times faster, that is, at rate −Cρij . Finally, note
that βit ’s are set to be 1
C fraction of those in the conservative greedy given the same remaining
weight of jobs. This cancels the factorC increase in the rate at which βit decreases. Putting together
these observations proves the lemma.
We will show that by choosing the optimal aggressiveness, we can improve the competitive
ratio for polynomial power functions by a logarithmic factor. Concretely, we show the following
(recall that Γf = maxs f ∗ (f 
(s ))
f (s ) + 1):
Theorem 3.12. The fractional C-aggressive greedy algorithm is 2Γf ,C-competitive for minimizing
weighted fractional flow-time plus energy with power function f , where
Γf ,C = C(CΓf + Γf − 1)
Γf C − Γf + 1 .
Optimal Choice of Aggressiveness. By optimizing our choice of C for 1 < Γf ≤ 2, Γf = 3, and
asymptotically optimizing it for general Γf , we get the following corollary, whose (asymptotic)
optimality will be presented in Section 6.
Corollary 3.13. The fractional C-aggressive greedy algorithm is
—2Γf -competitive for 1 < Γf ≤ 2, with C = 1;
—5.581-competitive for Γf = 3, with C ≈ 1.168;
— 8Γf
log2 Γf -competitive for general Γf , with C = 1 +
ln Γf −1
Γf .
Proof Corollary 3.13. The competitive ratios for 1 < Γf ≤ 2 and Γf = 3 are easy to verify. So,
we omit the tedious calculation here.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017. 
5:18 N. R. Devanur and Z. Huang
Next, consider the asymptotical bound for general Γf . With our choice of C = 1 +
ln Γf −1
Γf , the
denominator of Γf ,C is
Γf C − Γf + 1 = ln Γf = log2 Γf / log2 (e).
On the other hand, note that
C = 1 +
ln Γf − 1
Γf
= 1 +
ln(1 + (Γf − 1)) − 1
Γf
≤ 1 +
Γf − 2
Γf
and
CΓf =
	
1 +
ln Γf − 1
Γf

 Γf
< eln Γf −1 = 1
e
Γf .
So, putting it all together, we get that the numerator of Γf ,C is
C(CΓf + Γf − 1) <
	
1 +
Γf − 2
Γf

  1
e
Γf + Γf − 1

=
	
2 − 2
Γf

 	 1
e + 1 − 1
Γf


Γf ≤
 2
e + 2

Γf .
Finally, by the preceding discussion and that ( 2
e + 2) log2 (e) < 4 and Theorem 3.12, we prove the
claimed asymptotic competitive ratio for arbitrary Γf .
In particular, for polynomial power functions, we have
Corollary 3.14. For minimizing weighted fractional flow-time plus energy w.r.t. power function
f (s) = sα , the fractional C-aggressive greedy algorithm is
—2α-competitive for 1 < α ≤ 2, with C = 1;
—5.581-competitive for α = 3, with C ≈ 1.168;
— 8α
log2 α -competitive for general α, with C = 1 + ln α−1
α .
Remark 3.15. For polynomial power functions f (s) = sα , the speed scaling of PERW also falls
into the framework of C-aggressive greedy algorithm. Further, its choice of Cα = (α − 1) is also
asymptotically optimal when α ≥ 2. (We omit this calculation in this article.) So our result can be
viewed as a formal justification of the optimality of the heuristic speed scaling. Similar remark
applies to the integral C-aggressive greedy algorithm as well.
Remark 3.16. We note that for minimizing fractional flow-time plus energy on a single machine,
we can drop the third term in the primal objective and drop a factor of 2 in the competitive ratio. As
a result, we can get an α-competitive algorithm for 1 < α ≤ 2, and a 2.791-competitive algorithm
for α = 3. We omit the details in this article.
Proof of Theorem 3.12. In the rest of this section, we will present of proof of Theorem 3.12.
Given Lemma 3.11, it is easy to check that the way we set the primal and dual variables guarantees
feasibility. The competitive ratio comes from analyzing the ratio between the incremental costs of
the algorithm and the dual due to the arrival of new jobs.
We will first develop a few lemmas that are needed in the analysis. We start by showing that the
parameter Γf for arbitrary function f plays a similar role as the degree of polynomial functions as
the following lemma holds. (Recall that we have Γf = α for f (s) = sα .)
Lemma 3.17. For any power function f , any s > 0 and any C > 1, we have
f (Cs) ≤ CΓf f (s).
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.     
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:19
Proof. Note that for any s1 > 0, we have that
f 
(s1)s1
f (s1) = f ∗ (f 
(s1)) + f (s1)
f (s1) = f ∗ (f 
(s1))
f (s1) + 1 ≤ Γf ,
where the first equality is because s1 and f 
(s1) are a complementary pair and the inequality is by
definition of Γf . So, we have that
ln(f (Cs)) − ln(f (s)) =
 C
1
d ln(f (xs)) =
 C
1
f 
(xs)s
f (xs) dx
=
 C
1
f 
(xs)xs
f (xs)
1
x
dx ≤
 C
1
Γf
1
x
dx = Γf ln(C).
The lemma follows.
Proxy Dual Objective. For the convenience of analysis, we will consider a proxy dual objective,
Maximize 
j
αj −

i
 ∞
0
1
C f ∗ (Cβit )dt,
subject to the same constraints. We let D denote the value of the preceding dual objective. By the
convexity of f ∗, we have the following lemma.
Lemma 3.18. For any values of the dual variables, D ≤ D.
Proxy Power Function. We will consider the total cost of our algorithm w.r.t. the power function
ˆ
f (s) = CΓf f ( s
C ). We let E
denote the energy cost of the algorithm w.r.t. this power function, and
let alg  = F + E
denote the total cost of the algorithm w.r.t. this power function, noting that the
flow-time is unaffected. By Lemma 3.17, we have
Lemma 3.19. For any instance, E
≥ E and alg  ≥ alg.
By Lemmas 3.18 and 3.19, it suffices to bound the ratio between the increase in the total cost of
the algorithm w.r.t. power function ˆ
f when a new job arrives and the increase in the proxy dual
objective, denoted as Δalg and  ΔD, respectively.
Next, suppose a new job j arrives at time t and is assigned to machine i. We will account for
the incremental costs of the algorithm and the dual by relating them to the incremental cost in
an imaginary instance in which we are using the conservative greedy algorithm. We will call the
current instance I and the imaginary instance I 
.
More precisely, let there be a single machine in I  that is identical to machine i in I. For each
incomplete job in I at time t (before the arrival of job j), we put an identical job with the same
remaining volume in I  at time 0. Then, we will consider also releasing job j in I  at time 0. By our
construction of I  and the fact that the C-aggressive greedy algorithm is running exactly C-times
faster than the conservative one, there is a one-to-one mapping between the timeline after t in I
and the timeline in I  as specified in the next lemma, whose proof is straightforward and omitted.
Lemma 3.20. For any time t ≥ t, the remaining weight of each job in I at time t is the same as
that in I  at time C(t − t), both before and after the release of job j.
Let ΔF  and ΔE denote the increase in weighted fractional flow-time and energy, respectively,
in I  conditioned on running the conservative greedy both before and after releasing job j. Recall
that in the conservative greedy algorithm, we have ΔF  = (Γf − 1)ΔE
.
We let ΔF and ΔE
denote the increase in weighted fractional flow-time and energy (w.r.t. ˆ
f ) in
I due to the arrival of job j.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.           
5:20 N. R. Devanur and Z. Huang
The next lemma accounts for the the increase in flow-time due to the arrival of job j as a fraction
of ΔE
. The proof is rather straightforward from Lemma 3.20, so we omit it.
Lemma 3.21. ΔF = 1
C ΔF  = 1
C (Γf − 1)ΔE
.
The next lemma bounds the increase in energy due to the arrival of job j by a fraction of ΔE
.
Lemma 3.22. ΔE
= CΓf −1ΔE
.
Proof. For any t ≥ t, suppose the C-aggressive greedy algorithm runs at speed s at time t
.
Then, the energy cost (w.r.t. ˆ
f ) in I from time t to t + dt is ˆ
f (s)dt = CΓf f ( s
C )dt
. Further, by
Lemma 3.20 and that the C-aggressive greedy algorithm is running C-times faster than the conservative one, the conservative greedy algorithm runs at speed s
C at time C(t − t) in I 
. So, the
energy cost of from time C(t − t) to C(t − t) + Cdt in I  is f ( s
C )Cdt
, a C−Γf +1 fraction of the
corresponding energy cost in I. Integrating for all t ≥ t, we get that the energy cost (w.r.t. ˆ
f ) in I
after time t is exactly CΓf −1 times the total energy cost in I 
. As this relation holds both before and
after the release of job j, the lemma follows.
The next lemma establishes the relation between the incremental cost of the Proxy dual due to
βit ’s and ΔE
.
Lemma 3.23. Δ
 ∞
0
1
C f ∗ (Cβit )dt = 1
C2 (Γf − 1)E
.
Proof. Recall that βit = 1
C (f ∗)
−1 (wˆit ). By the convexity of f ∗, we have 1
C f ∗ (Cβit ) = 1
Cwˆit . So,
we have that  ∞
0
1
C f ∗ (Cβit )dt equals 1
C times the weighted fractional flow-time of instance I.
Hence, we have
Δ
 ∞
0
1
C f ∗ (Cβit )dt = 1
C
ΔF = 1
C2 ΔF 
.
The lemma then follows from ΔF  = (Γf − 1)ΔE
.
The next lemma follows from the fact that theC-aggressive greedy is running at exactlyC times
the speed of conservative greedy and our choice of βit .
Lemma 3.24. αj ≥ 1
C (ΔF  + ΔE
) = 1
C Γf ΔE
.
Proof. Consider the time t ∗ at which the new job j would be inserted according to HDF
w.r.t. the original instance (without job j). Let t denote the time at which j would be inserted
in the imaginary instance. By our choice of speed scaling, we have (t ∗ − t) = 1
C t
. Moreover,
βit ∗ = 1
C (f ∗)
−1 (wˆit ). So by our choice of αj , it is equal to
ρij (t
∗ − t) + βit ∗ +
1
wij  wi j
0
(f ∗)
−1 (w)dw = 1
C

t + (f ∗)
−1 (wˆit )

+
1
wij  wi j
0
(f ∗)
−1 (w)dw.
For any C ≥ 1, this is greater than or equal to 1
C times
t + (f ∗)
−1 (wˆit ) +
1
wij  wi j
0
(f ∗)
−1 (w)dw,
which equals the value of αj in conservative greedy and, thus, equals the total increase in weighted
fractional flow-time plus energy in I 
. So the lemma follows.
Finally, we are ready to derive the competitive ratio of the C-aggressive greedy algorithm.
Proof Theorem 3.12. Putting together Lemma 3.21 to Lemma 3.24, we have that the incremental costs of the algorithm (w.r.t. ˆ
f ) is
Δalg  = ΔF + ΔE
=
 1
C (Γf − 1) + CΓf −1

ΔE
,
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.        
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:21
and the incremental costs of the proxy dual is
ΔD = αj − Δ
1
C
 ∞
0
f ∗ (Cβit )dt ≥
 1
C
Γf − 1
C2 (Γf − 1)

ΔE
.
Simplifying the ratio between Δalg and  ΔD from the preceding equations proves Theorem 3.12.
4 WEIGHTED INTEGRAL FLOW-TIME PLUS ENERGY
In this section, we will discuss the problem of online scheduling for minimizing weighted (integral)
flow-time plus energy. The problem for weighted integral flow-time has the same input, output,
and constraints as the fractional flow-time version. The only difference is the objective. Next let us
formally define the weighted integral flow-time of an instance given a schedule. Let At denote the
set of jobs such that have been released before or at time t but have not been completed according
to the schedule till time t, that is,
rj ≤ t and 
t ∈[rj,∞]:ji (t)=j
sitdt < vij .
The weighted flow-time is defined as
 ∞
0

i

j ∈At :j→i
wijdt.
So the main difference is that when a job is partially completed, the entire weight of the job will
contribute to the weighted integral flow-time, while only the incomplete fraction will contribute to
the weighted fractional flow-time.
Convex Programming Relaxation and the Dual
Similar to the fractional, we will use the primal-dual analysis via the following convex program
for the problem of minimizing integral flow-time plus energy and consider its dual program:
(Pint) Minimize 
i

j
 ∞
rj
ρij (t − rj)sijtdt
+
 ∞
0
f (sit )dt +

i

j
 ∞
rj
(f ∗)
−1 (wij)sijtdt,
∀i,t :

j:rj ≤t
sijt = sit, (8)
∀j :

i
 ∞
rj
sijt
vij
≥ 1, (9)
(Dint) Maximize 
j
αj −

i
 ∞
0
f ∗ (βit )dt,
∀i, j,t ≥ rj : αj
vij
≤ ρij (t − rj) + βit + (f ∗)
−1 (wij). (10)
Here, we use the same notation as in the fractional case, so we will omit the detail explanations
of the convex programs. The only change is the third term in the primal program (and the corresponding part in the dual). This is because conditioned on being allocated to machine i, the optimal
cost for job j in a single-job instance w.r.t. integral flow-time plus energy is vij (f ∗)
−1 (wij). Hence,
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.      
5:22 N. R. Devanur and Z. Huang
Fig. 3. The conservative greedy online scheduling algorithm for minimizing weighted integral flow-time plus
energy with arbitrary power functions.
Fig. 4. TheC-aggressive greedy online scheduling algorithm for minimizing weighted integral flow-time plus
energy with arbitrary power function.
the share of the optimal single-job cost for the sijt
vi j dt fraction of job j is processed on machine i
from t to t + dt is (f ∗)
−1 (wij)sijtdt.
Algorithms
Similar to the fractional case, we will consider the conservative greedy algorithm, which will use
the optimal speed scaling assuming there are no future jobs, and a more general family of Caggressive greedy algorithms. The main difference comparing to the fractional case is the job
selection rule on a single machine is no longer HDF. Instead, the algorithms will combine the
job assignment rule job selection rule by maintaining a processing queue for each machine. The
machines will process the jobs in their queues in order. When a new job arrives, the algorithm will
insert the new job to an position in one of the processing queue according to the dual variables.
The formal descriptions of the algorithms are presented in Figures 3 and 4.
The integral conservative/aggressive greedy algorithms obtain the following competitive ratio
for general power functions and polynomial power functions, respectively.
Theorem 4.1. The integral conservative greedy algorithm is 2Γf -competitive for minimizing
weighted integral flow-time plus energy, where recall that Γf = maxs f ∗ (f 
(s ))
f (s ) + 1.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:23
Corollary 4.2. The integral conservative greedy algorithm is 2α-competitive for power function
f (s) = sα for minimizing weighted integral flow-time plus energy.
Theorem 4.3. The integral C-aggressive greedy algorithm is 2Γf ,C-competitive for minimizing
weighted integral flow-time plus energy with power function f , where
Γf ,C = C(CΓf + Γf − 1)
Γf C − Γf + 1 .
By asymptotically optimizing our choice ofC, we get the following corollaries, whose optimality
will be presented in Section 6. The proofs are identical to the integral case and hence omitted.
Corollary 4.4. The integral C-aggressive greedy algorithm is
—2Γf -competitive for 1 < Γf ≤ 2, with C = 1;
—5.581-competitive for Γf = 3, with C ≈ 1.168;
— 8Γf
log2 Γf -competitive for general Γf , with C = 1 +
ln Γf −1
Γf .
Corollary 4.5. For minimizing weighted integral flow-time plus energy w.r.t. power function
f (s) = sα , the fractional C-aggressive greedy algorithm is
—2α-competitive for 1 < α ≤ 2, with C = 1;
—5.581-competitive for α = 3, with C ≈ 1.168;
— 8α
log2 α -competitive for general α, with C = 1 + ln α−1
α .
Competitive Ratio
The primal-dual analysis of the integral case is almost identical to the fractional case. So, here,
we will only sketch the analysis of the conservative algorithm and explain the main differences
between the integral case and the fractional case. Extending the analysis from conservative to
aggressive algorithms is fairly straightforward using techniques we introduce in Section 3.5. So
the details will be omitted.
Next, we will show that αj is an upper bound on the increase in flow-time plus energy due the
job j. This is the main technical component of the analysis of the competitive ratio. The claim
follows easily from the next lemma, whose proof is similar to Lemma 3.10 and will be sketched in
the following.
Lemma 4.6. Suppose j
 is a job in the queue of machine i whose scheduled completion time is tij
before the arrival of job j. Then, the increase in total costs if we insert job j to the queue of machine i
right after j
 is at most
vij 
ρij (tij − rj) + βiti j + (f ∗)
−1 (wij)

.
Proof. Note that the algorithm uses the optimal speed scaling assuming no future jobs. To
bound the increase in flow-time plus energy when we insert job j to the queue of machine i right
after j

, it suffices to bound the increase in flow-time plus energy when we use a sub-optimal
speed scaling. In particular, we will let the jobs scheduled before j use the same speed as before
the arrival of j, and use the optimal speed scaling after that. In this case, the increase in flow-time
plus energy will be the flow-time due to job j waiting until time tij, that is, wij (tij − rj), plus the
increase in flow-time (of job j and jobs scheduled after j) plus energy due to processing job j, that
is, vij (f ∗)
−1 (W (tij ) + wij), whereW (tij ) is the integral remaining weight of jobs on machine i at
time tij. By the concavity of (f ∗)
−1, increase in flow-time is at most
vij 
(f ∗)
−1 (W (tij )) + (f ∗)
−1 (wij)

.
The lemma then follows by f ∗ (βiti j ) = W (tij ).
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017. 
5:24 N. R. Devanur and Z. Huang
Note that βit remains the same while machine i is processing the same job. So the minimal
value of vij (ρij (t − rj) + βit + (f ∗)
−1 (wij)) must be achieved in the completion time tij of one of
the jobs on i. As a simple corollary of Lemma 4.6, we have
Corollary 4.7. The value of αj is at least the increase in flow-time plus energy due to job j.
Now, we are ready to analyze the competitive ratio of the integral conservative greedy
algorithm.
Proof Theorem 4.1. First, note that it follows from our definition of the primal program Pint
that its optimal objective is at most twice that of the optimal flow-time plus energy by any (offline)
algorithm. Further, by our choice of βit ’s, the contribution of 
i
 ∞
0 f ∗ (βit )dt is the weighted
integral flow-time of the algorithm. So, combining with Corollary 4.7, we get that upon the arrival
of new jobs, the increase in the dual objective is at least the increase in energy of the algorithm.
Finally, via the same argument as in the fractional case, we have that the weighted integral flowtime of the algorithm is at most Γf times the energy. So the theorem follows.
5 RESOURCE AUGMENTATION
Further, as a simple application of our primal-dual approach, we manage to give simpler proofs
for either matching or improved competitive ratios for several online scheduling problems with
resource augmentation. In the resource augmentation setting, we will compare to a weaker offline
benchmark in the sense that given the same energy, the offline algorithm can only run at (1 + ϵ )
−1
fraction of the speed of the online algorithm.
Theorem 5.1. The fractional/integral conservative greedy algorithm is (1 + ϵ )-speed and 2( 1
ϵ + 1)-
competitive for minimizing weighted fractional/integral flow-time plus energy with arbitrary power
functions.
As a simple corollary of the preceding theorem when the power functions are step functions,
we have the following theorem for minimizing fractional/integral flow-time (fixed speed) with
resource augmentation.
Theorem 5.2. The fractional/integral conservative greedy algorithm is (1 + ϵ )-speed and 2( 1
ϵ + 1)-
competitive for minimizing weighted fractional/integral flow-time.
Primal-Dual Analysis with Resource Augmentation
We only need to slightly modify the primal-dual analysis in Sections 3 and 4 to prove Theorem 5.1.
Here, we will sketch the proof for the minimizing weighted integral flow-time. The analysis for
weighted fractional flow-time is almost identical and omitted.
First, we will need to change the objectives of the primal and dual programs to capture a weaker
offline benchmark whose speed is only a (1 + ϵ )
−1 fraction of that of the online algorithm using
the same energy. In particular, we will consider the following modified primal and dual convex
programs:
Maximize 
i

j
 ∞
rj
ρij (t − rj)sijtdt +
 ∞
0
f ((1 + ϵ )sit )dt
+

i

j
 ∞
rj
(f ∗)
−1 (wij)sijtdt,
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017. 
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:25
subject to Equations (8), (9);
Minimize 
j
αj −

i
 ∞
0
f ∗ ((1 + ϵ )
−1
βit )dt,
subject to Equation (10).
We will use the same rule as in Section 4 for setting the primal and dual variables. Since the
constraints remain the same, primal and dual feasibilities will be satisfied. Next, let us analyze the
competitive ratio. Recall that by Corollary 4.7 we have that αj is at least the increase in weighted
flow-time plus energy of the algorithm due the job j. Further, by our choice of βit and the concavity
of f ∗ (·), we have that

i
 ∞
0
f ∗ ((1 + ϵ )
−1
βit )dt ≤ (1 + ϵ )
−1

i
 ∞
0
f ∗ (βit )dt
= (1 + ϵ )
−1

i
 ∞
0
W ∗
i (t)dt
is at most a (1 + ϵ )
−1 fraction of the weighted flow-time of the algorithm. So, the increase in the
dual objective due to job j is at least the increase in energy plus a 1 − (1 + ϵ )
−1 = ϵ
1+ϵ fraction of
the increase in flow-time of the algorithm. Finally, recall that the optimal primal objective is at
most twice the optimal weighted flow-time plus energy of any (offline) algorithm. So, we get that
the integral conservative algorithm with (1 + ϵ )-speed-up is 2( 1
ϵ + 1)-competitive for minimizing
weighted flow-time plus energy.
6 ALMOST TIGHT LOWER BOUNDS
In this section, we complement our algorithmic results in Sections 3 and 4 by providing asymptotically tight lower bounds for minimizing weighted fractional/integral flow-time plus energy with
arbitrary power functions. Formally, we prove the following:
Theorem 6.1. Suppose there exists Γ ≥ 1 and s > 0 such that for any s ∈ [s,s + log2 Γ
Γ s], we have
f ∗ (f 
(s
))
f (s) + 1 ≥ Γ. Then, any online algorithm for scheduling jobs on unrelated machines with speed
scaling to minimize weighted fractional/integral flow-time plus energy must admit a competitive ratio
at least 1
4
Γ
log2 Γ .
In particular, we have the following asymptotically tight lower bound for polynomial power
functions as a corollary of Theorem 6.1.
Corollary 6.2. Any online algorithm for scheduling jobs on unrelated machines with speed scaling to minimize weighted fractional/integral flow-time plus energy w.r.t. polynomial power function
f (s) = sα (α ≥ 2) must admit a competitive ratio at least 1
4
α
log2 α .
Proof Theorem 6.1. Note that a lower bound for fractional flow-time will also imply the same
asymptotically tight lower bound for integral flow-time plus energy, because we can view each job
in the fractional instance as continuously many infinitesimally small jobs with the same density
in the integral instance. So in this proof, we only need to present the lower bound for fractional
flow-time.
Consider a randomized instance consists of two machines. Consider two types of jobs, both of
which have density ρ. The value of ρ will be determined later. The first type consists of only 1
job of size 1 that arrives at time 0. This job can be processed by both machines. The second type
consists of a sequence of jobs of total volume Γ that comes at rate s, that is, from t to t + dt we
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
5:26 N. R. Devanur and Z. Huang
have s · dt volume of such jobs arriving, from time 0+ (after the release of the type-1 job) to Γ
s . The
type-2 jobs can only be processed by one of the machines, which is randomly chosen when we
construct the instance (but it will be the same machine for all type-2 jobs).
Now, we will show that even if the online algorithm knows the instance, but not the random
coin flip, it must admit a competitive ratio at least Ω( Γ
log2 Γ ). By Yao’s minimax principle (Yao 1977),
it suffices to consider deterministic algorithm A and lower bounds its competitive ratio.
Let us first upper bound the cost of the offline optimal. The offline optimal could have processed
all type-2 jobs on the feasible machine with a fixed speed s, and processed the type-1 job on the
other machine also at a fixed speed s. The energy cost will be Γ
s f (s) in total. The weighted fractional
flow-time will be ρ
2s , because the type-1 job incurs a weighted fractional flow-time of ρ
2s , while the
type-2 jobs are completed immediately and hence have 0 weighted fractional flow-time. We will
choose the density ρ to balance the two types of costs, that is, Γ
s f (s) = ρ
2s . So the total cost is ρ
s .
Now consider the deterministic online algorithm. At time 0 the algorithm needs to decide which
machine the type-1 job is assigned to. So with probability 1
2 the algorithm will make a mistake and
assign the type-1 job to the only machine that can process the type-2 jobs. We will show that in
this case the cost of the online algorithm will be at least Ω( Γ
log2 Γ
ρ
s ). Choose t ∗ > 0 such that
f

s +
1
2t ∗

= Γf (s). (11)
We claim that the following lemma about t ∗ holds:
Lemma 6.3. t ∗ ≥ 1
2s
Γ
log2 Γ .
We will need the following lemma about f , whose proof is similar to that of Lemma 3.17 and
hence omitted:
Lemma 6.4. For any C ∈ [1, 1 + log2 Γ
Γ ], we have that
f (Cs) ≥ CΓ f (s).
Proof Lemma 6.3. First note that Lemma 6.4 implies
f
	
s + log2 Γ
Γ s


≥
	
1 + log2 Γ
Γ

 Γ
f (s).
Further, by 1 + x ≥ 2x for x ∈ [0, 1], the preceding is at least 2log2 Γ f (s) = Γf (s). So, by the definition of t ∗ and the monotonicity of f , we get that
1
2t ∗ ≤ log2 Γ
Γ s.
The lemma then follows.
Consider the first time t after time 0 that the total size of the jobs waiting in the online algorithm
drops to 1
2 . If t ≥ t ∗, then the fractional flow-time is at least ρt ∗
2 ≥ 1
4
Γ
log2 Γ
ρ
s . If t < t ∗, then from time
0 to t ∗ the algorithm has processed jobs of total size at least t ∗
s + 1
2 . So the average speed in this
period is s + 1
2t ∗ , and the energy cost is at least
t
∗ f

s +
1
2t ∗

= t
∗Γf (s) (by Equation (11))
= t
∗ ρ
2 (by definition of ρ)
≥
1
4
Γ
log2 Γ
ρ
s
. (by Lemma 6.3)
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.     
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:27
In sum, the expected total costs of the algorithm is at least 1
4
Γ
log2 Γ
ρ
s . So the competitive ratio is at
least 1
4
Γ
log2 Γ .
We remark that the preceding lower bound instance only consists of two machines and two types
of jobs with unit density, while our algorithm achieves asymptotically optimal competitive ratios
for the most general settings of weighted fractional/integral flow-times with unrelated machines
APPENDIX
A CONVEX CONJUGATES AND FENCHEL DUALITY
Suppose that f : R → R is a function. The conjugate of f is a function f ∗ : R → R such that
f ∗ (μ) := sup
x

μx − f (x)

.
Although the conjugate is defined for any function f , for the rest of the article, we will assume
that f is strictly convex and differentiable, since this is the case that is most interesting to the
applications we discuss.
Properties of f ∗:
— f ∗ is strictly convex and differentiable. (This property holds even if f is not strictly convex
and differentiable.)
— f ∗∗ = f . (Here, we use the assumption that f is strictly convex and differentiable.)
—If д(x) = c f (x) for some constant c, then д∗ (μ) = c f ∗ (μ/c).
—If д(x) = f (cx) for some constant c, then д∗ (μ) = f ∗ (μ/c).
—If д(x) = f (x + a) for some constant a, then д∗ (μ) = f ∗ (μ) − μa.
—If μ and x are such that f (x) + f ∗ (μ) = μx, then f 
(x) = μ and (f ∗)
(μ) = x.
—Vice versa, if f 
(x) = μ then (f ∗)
(μ) = x and f (x) + f ∗ (μ) = μx.
We say that (x, μ) form a complementary pair wrt f if they satisfy one of the last two conditions
stated previously. We now calculate the conjugates of some simple strictly convex and differentiable functions for illustration.
—If f (x) = 1
2x2, then f 
(x) = x. Thus, f ∗ (μ) is obtained by letting μ = x in μx − f (x), which
is then equal to 1
2 μ2.
—If f (x) = − log(x), then f 
(x) = −1/x. Set μ = −1/x to get f ∗ (μ) = −1 + log(x) = −1 −
log(−μ).
—Suppose f (x) = x log x. Then, f 
(x) = log x + 1 = μ. So, x = e μ−1. f ∗ (μ) = μx − f (x) =
x (log x + 1) − x log x = x = e μ−1. That is, f ∗ (μ) = e μ−1.
Convex Programs with Linear Constraints
Consider the following (primal) optimization problem.
Maximize 
i
cixi − fi (xi ) s.t.
∀j :

i
aijxi ≤ bj .
We will derive a minimization problem that is the dual of this, using Lagrangian duality. This is
usually a long calculation. The goal of this exercise is to identify a shortcut for the same.
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
5:28 N. R. Devanur and Z. Huang
Define the Lagrangian function
L(x, λ) :=

i
cixi − fi (xi ) +

j
λj 

bj −

i
aijxi


.
We say that x is feasible if it satisfies all the constraints of the primal problem. Note that for all
λ ≥ 0 and x feasible, L(x, λ) ≥ 
i cixi − fi (xi ). Define the dual function
д(λ) = max x L(x, λ).
So, for all λ, x, д(λ) ≥ L(x, λ). Thus, minλ≥0 д(λ) ≥ the optimum value for the primal program. The
dual program is essentially minλ≥0 д(λ). We further simplify it as follows. Rewriting the expression
for L,
L =

i
μixi − fi (xi ) +

j
bjλj , where μi = ci −

j
aijλj .
Now note that
д(λ) = max x L(x, λ) = max x
⎧⎪
⎨
⎪
⎩

i
μixi − fi (xi )
⎫⎪
⎬
⎪
⎭
+

j
bjλj =

i
f ∗
i (μi ) +

j
bjλj .
Thus, we get the dual optimization problem:
Minimize 
j
bjλj +

i
f ∗
i (μi ) s.t.
∀i :

j
aijλj = ci − μi,
∀j : λj ≥ 0.
Note the similarity to LP duality. The differences are as follows. Suppose the concave part of the
primal objective is − 
i fi (xi ). There is an extra variable μi for every variable xi that occurs in
f . In the constraint corresponding to xi , −μi appears on the RHS along with the constant term.
Finally, the dual objective has 
i f ∗
i (μi ) in addition to the linear terms. In other words, we relax
the constraint corresponding to xi by allowing a slack of μi , and charge 
i f ∗
i (μi ) to the objective
function.
Similarly, suppose we start with the primal problem,
Maximize 
i
cixi − fi (xi ) s.t.
∀j :

i
aijxi ≤ bj ,
∀i : xi ≥ 0.
Then, the dual problem is
Minimize 
j
bjλj +

i
f ∗
i (μi ) s.t.
∀i :

j
aijλj ≥ ci − μi,
∀j : λj ≥ 0.
As we saw, the optimum for the primal program is lower than the optimum for the dual program
(weak duality). In fact, if the primal constraints are strictly feasible, that is there exist xi such
ACM Transactions on Algorithms, Vol. 14, No. 1, Article 5. Publication date: December 2017.
Primal Dual Gives Almost Optimal Energy-Efficient Online Algorithms 5:29
that for all j

i aijxij < bj , then the two optima are the same (strong duality) and the following
generalized complementary slackness conditions characterize them:
—xi > 0 ⇒ 
j aijλj = ci − μi ;
—λj > 0 ⇒ 
i aijxi = bi ; and
—xi and μi form a complementary pair w.r.t. fi , that is, μi = f 
i (xi ), xi = (f ∗)
i (μi ) and fi (xi ) +
f ∗
i (μi ) = μixi .
Similarly, suppose we start from a minimization problem of the form
Minimize 
i
cixi + fi (xi ) s.t.
∀j :

i
aijxi ≥ bj ,
∀i : xi ≥ 0.
Then, the dual of this is
Maximize 
j
bjλj −

i
f ∗
i (μi ) s.t.
∀i :

j
aijλj ≤ ci + μi,
∀j : λj ≥ 0.