We study the problem of testing whether an unknown n-variable Boolean function is a k-junta in the
distribution-free property testing model, where the distance between functions is measured with respect to
an arbitrary and unknown probability distribution over {0, 1}
n. Our first main result is that distributionfree k-junta testing can be performed, with one-sided error, by an adaptive algorithm that uses O˜ (k2)/ϵ
queries (independent of n). Complementing this, our second main result is a lower bound showing that any
non-adaptive distribution-free k-junta testing algorithm must make Ω(2k/3) queries even to test to accuracy
ϵ = 1/3. These bounds establish that while the optimal query complexity of non-adaptive k-junta testing is
2Θ(k)
, for adaptive testing it is poly(k), and thus show that adaptivity provides an exponential improvement
in the distribution-free query complexity of testing juntas.
CCS Concepts: • Theory of computation → Streaming, sublinear and near linear time algorithms;
Additional Key Words and Phrases: Property testing, distribution-free model
1 INTRODUCTION
Property testing of Boolean functions was first considered in the seminal works of Blum et al. (1993)
and Rubinfeld and Sudan (1996) and has developed into a robust research area at the intersection
of sub-linear algorithms and complexity theory. Roughly speaking, a property tester for a class C
of functions from {0, 1}
n to {0, 1} is a randomized algorithm that is given some form of access to
the (unknown) input Boolean function f and must with high probability distinguish the case that
f ∈ C versus the case that f is ϵ-far from every function д ∈ C. In the usual (uniform-distribution)
property testing scenario, the testing algorithm may access f by making black-box queries on
inputs x ∈ {0, 1}
n, and the distance between two functions f and д is measured with respect to
the uniform distribution on {0, 1}
n; the goal is to develop algorithms that make as few queries
as possible. Many different classes of Boolean functions have been studied from this perspective
(Blum et al. 1993; Alon et al. 2005; Bhattacharyya et al. 2010; Parnas et al. 2002; Diakonikolas et al.2007; Goldreich et al. 2000; Fischer et al. 2002; Chakrabarty and Seshadhri 2013; Chen et al. 2014,
2015; Khot et al. 2015; Belovs and Blais 2016; Khot and Shinkar 2016; Chakrabarty and Seshadhri
2016; Baleshzar et al. 2016; Chen et al. 2017b, 2017c; Matulef et al. 2010, 2009; Blais et al. 2011; Blais
and Kane 2012; Gopalan et al. 2011), and see other works referenced in the surveys by Ron (2008,
2010) and Goldreich (2010). Among these, the class of k-juntas—Boolean functions that depend only
on (an unknown set of) at most k of their n input variables—is one of the best-known and most
intensively investigated such classes (Fischer et al. 2004; Chockler and Gutfreund 2004; Blais 2008,
2009; Buhrman et al. 2013; Servedio et al. 2015), with ongoing research on junta testing continuing
right up to the present (Chen et al. 2017a).
The query complexity of junta testing in the uniform distribution framework is now well understood. Improving on poly(k)/ϵ-query algorithms given in Fischer et al. (2004) (which introduced the junta testing problem), in Blais (2008), Blais gave a non-adaptive algorithm that makes
O˜ (k3/2)/ϵ queries, and in Blais (2009), Blais gave anO(k log k + k/ϵ )-query adaptive algorithm. On
the lower bounds side, Fischer et al. (2004) initially gave an Ω(√
k) lower bound for non-adaptively
testing k-juntas, which also implies an Ω(log k) lower bound for adaptive testing. Chockler and
Gutfreund improved the adaptive lower bound to Ω(k) in Chockler and Gutfreund (2004), and
very recently Chen et al. (2017a) gave an Ω( ˜ k3/2)/ϵ non-adaptive lower bound. Thus, in both the
adaptive and non-adaptive uniform distribution settings, the query complexity of k-junta testing
has now been pinned down to within logarithmic factors.
Distribution-free property testing. This work studies the junta testing problem in the
distribution-free property testing model that was first introduced in Goldreich et al. (1998). In this
model, the distance between Boolean functions is measured with respect to a distribution D over
{0, 1}
n, which is arbitrary and unknown to the testing algorithm. Since the distribution is unknown, in this model the testing algorithm is allowed (in addition to making black-box queries)
to draw random labeled samples (x, f (x)), where each x is independently distributed according
to D. The query complexity of an algorithm in this framework is the worst-case total number of
black-box oracle calls plus random labeled samples that are used, across all possible distributions.
(It follows that distribution-free testing of a class C requires at least as many queries as testing C
in the standard uniform-distribution model.)
Distribution-free property testing is in the spirit of similar distribution-free models in computational learning theory such as Valiant’s celebrated PAC learning model (Valiant 1984). Such
models are attractive because of their minimal assumptions; they are well motivated both because
in many natural settings the uniform distribution over {0, 1}
n may not be the best way to measure distances, and because they capture the notion of an algorithm dealing with an unknown and
arbitrary environment (modeled here by the unknown and arbitrary distribution D over {0, 1}
n
and the unknown and arbitrary Boolean function f : {0, 1}
n → {0, 1}). Researchers have studied
distribution-free testing of a number of Boolean function classes, including monotone functions,
low-degree polynomials, dictators (1-juntas) and k-juntas (Halevy and Kushilevitz 2007), disjunctions and conjunctions (monotone and non-monotone), decision lists, and linear threshold functions (Glasner and Servedio 2009; Dolev and Ron 2011; Chen and Xie 2016). Since depending on
few variables is an appealingly flexible “real-world” property in comparison with more highly
structured syntactically defined properties, we feel that junta testing is a particularly natural task
to study in the distribution-free model.
Prior results on distribution-free junta testing. Given how thoroughly junta testing has
been studied in the uniform distribution model, surprisingly little was known in the distributionfree setting. The adaptive Ω(k) and non-adaptive Ω( ˜ k3/2)/ϵ uniform-distribution lower bounds
from Chockler and Gutfreund (2004) and Chen et al. (2017a) mentioned earlier trivially extend
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.
Distribution-free Junta Testing 1:3
to the distribution-free model, but no other lower bounds on distribution-free junta testing were
known prior to this work. On the positive side, Halevy and Kushilevitz (2007) showed that any class
C that has (i) a one-sided error uniform-distribution testing algorithm and (ii) a self-corrector, has
a one-sided error distribution-free testing algorithm. As poly(k)/ϵ-query one-sided junta testers
were given already in Fischer et al. (2004), and k-juntas have O(2k )-query self-correctors (Alon
and Weinstein 2012), this yields a one-sided non-adaptive distribution-free junta tester with query
complexity O(2k /ϵ ). No other results were known.
Thus, prior to this work there were major gaps in our understanding of distribution-free k-junta
testing: Is the query complexity of this problem polynomial in k, exponential in k, or somewhere
in between? Does adaptivity confer an exponential advantage, a sub-exponential advantage, or no
advantage at all? Our results, described below, answer both these questions.
1.1 Our Results
Our main positive result is a poly(k)/ϵ-query one-sided adaptive algorithm for distribution-free
k-junta testing:
Theorem 1.1 (Upper bound). For any ϵ > 0, there is a one-sided distribution-free adaptive
algorithm for ϵ-testing k-juntas with O˜ (k2)/ϵ queries.
Theorem 1.1 shows that k-juntas stand in interesting contrast with many other well-studied
classes of Boolean functions in property testing such as conjunctions, decision lists, linear threshold functions, and monotone functions. For each of these classes, distribution-free testing requires
dramatically more queries than uniform-distribution testing: for the first three classes the separation is poly(1/ϵ ) queries in the uniform setting (Parnas et al. 2002; Matulef et al. 2010) versus
nΩ(1) queries in the distribution-free setting (Glasner and Servedio 2009; Chen and Xie 2016); for
n-variable monotone functions poly(n) queries suffice in the uniform setting (Goldreich et al. 2000;
Khot et al. 2015), whereas Halevy and Kushilevitz (2007) show that 2Ω(n) queries are required in
the distribution-free setting. In contrast, Theorem 1.1 shows that for k-juntas the query complexities of uniform-distribution and distribution-free testing are polynomially related (indeed, within
at most a quadratic factor of each other).
Complementing the strong upper bound that Theorem 1.1 gives for adaptive testers, our main
negative result is an Ω(2k/3)-query lower bound for non-adaptive testers:
Theorem 1.2 (Lower bound). For k ≤ n/200, any non-adaptive algorithm that distribution-free
ϵ-tests k-juntas over {0, 1}
n, for ϵ = 1/3, must have query complexity Ω(2k/3).
Theorems 1.1 and 1.2 together show that adaptivity enables an exponential improvement in
the distribution-free query complexity of testing juntas. This is in sharp contrast with uniformdistribution junta testing, where the adaptive and non-adaptive query complexities are polynomially related (with an exponent of only 3/2). To the best of our knowledge, this is the first example
of a exponential separation between adaptive and nonadaptive distribution-free testers.
1.2 Ideas and Techniques
The algorithm. As a first step toward ourO˜ (k2)/ϵ-query algorithm, in Section 3, we first present a
simple one-sided adaptive algorithm, which we call SimpleDJunta, that distribution-free tests kjuntas using O((k/ϵ ) + k logn) queries. SimpleDJunta uses binary search and is an adaptation to
the distribution-free setting of theO((k/ϵ ) + k logn)-query uniform-distribution algorithm, which
is implicit in Blais (2009). The algorithm maintains a set I of relevant variables: a string x ∈ {0, 1}
n
has been found for each i ∈ I such that f (x)  f (x (i)
) (we use x (i) to denote the string obtained
by flipping the ith bit of x), and the algorithm rejects only when |I | becomes larger than k. In each
round, the algorithm samples a string x ← D and a subset R of I := [n] \ I uniformly at random.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018. 
1:4 Z. Liu et al.
A simple lemma, Lemma 3.2, states that if f is far from every k-junta with respect to D, then
f (x)  f (x(R)
) with at least some moderately large probability as long as |I | ≤ k, where we use
x(R) to denote the string obtained from x by flipping every coordinate in R. With such a pair
(x, x(R)
) in hand, it is straightforward to find a new relevant variable using binary search over
coordinates in R (see Figure 1), with at most logn additional queries.
To achieve a query complexity that is independent of n, clearly one must employ a more efficient
approach than binary search over Ω(n) coordinates (since most likely the set R has size Ω(n) for
the range of k we are interested in). In the uniform-distribution setting this is accomplished in
Blais (2009) by first randomly partitioning the variable space [n] into s = poly(k/ϵ ) disjoint blocks
B1,..., Bs of variables and carrying out binary search over blocks (see Figure 2) rather than over
individual coordinates; this reduces the cost of each binary search to log(k/ϵ ) rather than logn.
The algorithm maintains a set of relevant blocks: two strings x,y ∈ {0, 1}
n have been found for
each such block B that satisfy f (x)  f (y) and y = x (S ) with S ⊆ B, and the algorithm rejects when
more than k relevant blocks have been found. In each round the algorithm samples two strings x, y
uniformly at random conditioned on their agreeing with each other on the relevant blocks that
have already been found in previous rounds; if f (x)  f (y), then the binary search over blocks is
performed to find a new relevant block. To establish the correctness of this approach, Blais (2009)
employs a detailed and technical analytic argument based on the influence of coordinates and the
Efron-Stein orthogonal decomposition of functions over product spaces. This machinery is well
suited for dealing with product distributions, and indeed the analysis of Blais (2009) goes through
for any product distribution over {0, 1}
n (and even for more general finite domains and ranges).
However, it is far from clear how to extend this machinery to work for the completely unstructured
distributions D that must be handled in the distribution-free model.
Our main distribution-free junta testing algorithm, denoted MainDJunta, draws ideas from
both SimpleDJunta (mainly Lemma 3.2) and the uniform distribution tester of (Blais 2009). To
avoid the logn cost, the algorithm carries out binary search over blocks rather than over individual
coordinates, and maintains a set of disjoint relevant blocks B1,..., B, i.e., for each Bj a pair of
strings xj and yj have been found such that they agree with each other over Bj and satisfy f (xj
)
f (yj
). Letwj be the projection of xj (and yj
) over Bj and let дj be the Boolean function over {0, 1}Bj
obtained from f by setting variables in Bj towj
. For clarity, we assume further that every function
дj is very close to a literal (i.e., for some τ ∈ {xij , xij }, we have дj (x) = τ for all x ∈ {0, 1}Bj for some
ij ∈ Bj) under the uniform distribution. (To justify this assumption, we note that if дj is far from
every literal under the uniform distribution, then it is easy to split Bj further into two relevant
blocks using the uniform distribution algorithm of (Blais 2009).) Let I = {ij : j ∈ []}. Even though
the algorithm does not know I, there is indeed a way to draw uniformly random subsets R of I.
First, we draw a partition of Bj into Pj and Qj uniformly at random, for each j. Since дj is close
to a literal, it is not difficult to figure out whether Pj or Qj contains the hidden ij , say it is Pj
for every j. Then the union of all Qj ’s together with a uniformly random subset of B1 ∪···∪ B,
denoted by R, turns out to be a uniformly random subset of I. With R in hand, Lemma 3.2 implies
that f (x)  f (x(R)
) with high probability when x ← D, and when this happens, one can carry
out binary search over blocks to increase the number of relevant blocks by one. In Section 4.1, we
explain the intuition behind the main algorithm in more detail.
The lower bound. As we explain in Section 2, a q-query non-adaptive distribution-free tester is
a randomized algorithm A that works as follows. When A is run on an input pair (ϕ, D)
1 it is first
1For clarity, throughout our discussion of lower bounds, we write ϕ to indicate a function that may be either a “yesfunction” or a “no-function,” f to denote a “yes-function” and д to denote a “no-function.”
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.        
Distribution-free Junta Testing 1:5
given the result (y1,ϕ(y1)),..., (yq,ϕ(yq )) of q queries from the sampling oracle. Based on them,
it queries the black-box oracle q times on strings z1,..., zq . The zj
’s may depend on the random
pairs (yi
,ϕ(yi )) received from the sampling oracle, but the jth black-box query string zj may not
depend on the responses ϕ(z1),...,ϕ(zj−1) to any of the j − 1 earlier black-box queries.
As is standard in property testing lower bounds, our argument employs a distribution YES
over yes-instances and a distribution N O over no-instances. Here, YES is a distribution over
(function, distribution) pairs (f, D) in which f is guaranteed to be a k-junta; N O is a distribution
over pairs (g, D) such that with probability 1 − o(1), g is 1/3-far from every k-junta with respect
to D. To prove the desired lower bound against non-adaptive distribution-free testers, it suffices to
show that for q = 2k/3, any deterministic non-adaptive algorithm A as described above is roughly
equally likely to accept whether it is run on an input drawn from YES or from N O.
Our construction of the YES and N O distributions is essentially as follows. In making a draw
either from YES or from N O, first m = Θ(2k logn) strings are selected uniformly at random
from {0, 1}
n to form a set S, and the distribution D in both YES and N O is set to be the uniform
distribution over S. Also, in both YES and N O, a “background” k-junta h is selected uniformly
at random by first picking a set J of k variables at random and then a random truth table for h
over the variables in J. We view the variables in J as partitioning {0, 1}
n into 2k disjoint “sections”
depending on how they are set.
In the case of a draw from YES, the Boolean function f that goes with the above-described D
is simply the background junta f = h. In the case of a draw from N O, the function g that goes
with D is formed by modifying the background junta h in the following way (roughly speaking;
see Section 5.1 for precise details): for each z ∈ S, we toss a fair coin b(z) and set the value of all
the strings in z’s section that lie within Hamming distance 0.4n from z (including z itself) to b(z)
(see Figure 7). Note that the value of g at each string in S is a fair coin toss, which is completely
independent of the background junta h. Using the choice of m it can be argued (see Section 5.1)
that with high probability g is 1/3-far from every k-junta with respect to D as (g, D) ←NO.
The rough idea of why a pair (f, D) ← YES is difficult for a (q = 2k/3)-query non-adaptive
algorithm A to distinguish from a pair (g, D) ←NO is as follows. Intuitively, in order for A to
distinguish the no-case from the yes-case, it must obtain two strings x1, x2 that belong to the same
section but are labeled differently. Since there are 2k sections but q is only 2k/3, by the birthday
paradox it is very unlikely that A obtains two such strings among the q samples y1,..., yq that it
is given from the distribution D. In fact, in both the yes and no cases, writing (ϕ, D) to denote
the (function, distribution) pair, the distribution of the q pairs (y1,ϕ(y1)),..., (yq,ϕ(yq )) will be
statistically very close to (x1, b1),..., (xq, bq ), where each pair (xj
, bj) is independently drawn
uniformly from {0, 1}
n × {0, 1}. Intuitively, this translates into the examples (yi
,ϕ(yi )) from the
sampling oracle “having no useful information” about the set J of variables that the background
junta depends on.
What about the q strings z1,..., zq that A feeds to the black-box oracle? It is also unlikely that
any two elements of y1,..., yq, z1,..., zq belong to the same section but are labeled differently.
Fix an i ∈ [q], we give some intuition here as to why it is very unlikely that there is any j such
that zi lies in the same section as yj but has f (zi )  f (yj
) (via a union bound, the same intuition
handles all i ∈ [q]). Intuitively, since the random examples from the sampling oracle provide no
useful information about the set J defining the background junta, the only thing that A can do in
selecting zi is to choose how far it lies, in terms of Hamming distance, from the points in y1,..., yq
(which, recall, are uniform random). Fix j ∈ [q], if zi is within Hamming distance 0.4n from yj
, then
even if zi lies in the same section as yj it will be labeled the same way as yj
, whether we are in the
yes-case or the no-case. However, if zi is farther than 0.4n in Hamming distance from yj
, then it is
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018. 
1:6 Z. Liu et al.
overwhelmingly likely that zi will lie in a different section from yj (since it is very unlikely that all
0.4n of the flipped coordinates avoid the k-element set J). We prove Theorem 1.2 in Section 5 via a
formal argument that proceeds somewhat differently from but is informed by the above intuitions.
Organization. In Section 2, we define the distribution-free testing model and introduce some
useful notation. In Section 3 we present SimpleDJunta and prove Lemma 3.2 in its analysis. In
Section 4, we present our main algorithm MainDJunta and prove Theorem 1.1, and in Section 5,
we prove Theorem 1.2.
2 PRELIMINARIES
Notation. We use [n] to denote {1,...,n}. We use f and д to denote Boolean functions, which are
maps from {0, 1}
n to {0, 1} for some positive integer n. We use the calligraphic font (e.g., D and
N O) to denote probability distributions, boldface letters such as x to denote random variables,
and write “x ← D” to indicate that x is a random variable drawn from a distribution D. We write
x ← {0, 1}
n to denote that x is a string drawn uniformly at random. Given S ⊆ [n], we also write
R ← S to indicate that R is a subset of S drawn uniformly at random, i.e., each index i ∈ S is
included in R independently with probability 1/2.
Given a subset B ⊆ [n], we use B to denote its compliment with respect to [n], and {0, 1}B to
denote the set of all binary strings of length |B| with coordinates indexed by i ∈ B. Given an x ∈
{0, 1}
n and a B ⊆ [n], we write xB ∈ {0, 1}B to denote the projection of x over coordinates in B and
x (B) ∈ {0, 1}
n to denote the string obtained from x by flipping coordinates in B. Given x ∈ {0, 1}B
and y ∈ {0, 1}B, we write x ◦ y ∈ {0, 1}
n to denote their concatenation, a string that agrees with
x over coordinates in B and agrees with y over B. (As an example of the notation, given x,y ∈
{0, 1}
n and B ⊆ [n], xB ◦ yB denotes the string that agrees with x over B and with y over B.) Given
x,y ∈ {0, 1}
n, we write d(x,y) to denote the Hamming distance between x and y, and diff(x,y) ⊆
[n] to denote the set of coordinates i ∈ [n] with xi  yi .
Given f ,д : {0, 1}
n → {0, 1} and a probability distribution D over {0, 1}
n, we write
distD (f ,д) := Pr
z←D[f (z)  д(z)]
to denote the distance between f and д with respect to D. Given a class C of Boolean functions,
distD (f , C) := min
д∈C
(distD (f ,д))
denotes the distance between f and C with respect to D, where the minimum is taken over д with
the same number of variables as f . We say f is ϵ-far from C with respect to D if distD (f , C) ≥ ϵ.
We will often work with restrictions of Boolean functions. Given f : {0, 1}
n → {0, 1}, S ⊆ [n]
and a string z ∈ {0, 1}B, the restriction of f over B by z, denoted by fz , is the Boolean function
д : {0, 1}B → {0, 1} defined by д(x) = f (x ◦ z) for all x ∈ {0, 1}B.
Distribution-free property testing. Now, we can define distribution-free property testing:
Definition 2.1. We say an algorithm A has oracle access to a pair (f , D), where f : {0, 1}
n →
{0, 1} is an unknown Boolean function and D is an unknown probability distribution over {0, 1}
n,
if it can (1) access f via a black-box oracle that returns f (x) when a string x ∈ {0, 1}
n is queried,
and (2) access D via a sampling oracle that, upon each request, returns a pair (x, f (x)), where
x ← D independently.
Let C be a class of Boolean functions. A distribution-free testing algorithmA for C is a randomized
algorithm that, given as input a distance parameter ϵ > 0 and oracle access to a pair (f , D), accepts
with probability at least 2/3 if f ∈ C and rejects with probability at least 2/3 if f is ϵ-far from C
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.   
Distribution-free Junta Testing 1:7
with respect to D. We say A is one-sided if it always accepts when f ∈ C. The query complexity
of a distribution-free testing algorithm is the number of queries made on f plus the number of
samples drawn from D.
One may assume without loss of generality that a distribution-free testing algorithm consists
of two phases: In the first phase, the algorithm draws a certain number of sample pairs (x, f (x))
from D; in the second phase, it makes black-box queries to f . In general, a query x ∈ {0, 1}
n made
by the algorithm in the second phase may depend on sample pairs it receives in the first phase (e.g.
it can choose to query a string that is close to a sample received in the first phase) and results of
queries to f made so far. In Section 5, we will prove lower bounds on non-adaptive distributionfree testing algorithms. An algorithm is said to be non-adaptive if its black-box queries made in
the second phase do not depend on results of previous black-box queries, i.e., all queries during
the second phase can be made in a single batch (though we emphasize that they may depend on
samples the algorithm received in the first phase).
Juntas and literals. We study the distribution-free testing of the class of k-juntas. Recall that a
Boolean function f is a k-junta if it depends on at most k variables. More precisely, f is a k-junta
if there exists a list 1 ≤ i1 < ··· < ik ≤ n of k indices and a Boolean function д : {0, 1}
k → {0, 1}
over k variables such that f (x1,..., xn ) = д(xi1 ,..., xik ) for all x ∈ {0, 1}
n.
We say that a Boolean function f is a literal if f depends on exactly one variable, i.e. for some
i ∈ [n], we have that either f (x) = xi for all x or f (x) = xi for all x. Note that the two constant
(all-1 and all-0) functions are one-juntas but are not literals.
We often use the term “block” to refer to a nonempty subset of [n], which should be interpreted
as a nonempty subset of the n variables of a Boolean function f : {0, 1}
n → {0, 1}. The following
definition of distinguishing pairs and relevant blocks will be heavily used in our algorithms.
Definition 2.2 (Distinguishing Pairs and Relevant Blocks). Given x,y ∈ {0, 1}
n and a block B ⊆ [n],
we say that (x,y) is a distinguishing pair for B if xB = yB and f (x)  f (y). We say B is a relevant
block of f if such a distinguishing pair exists for B (or equivalently, the influence of B in f is
positive).
When B = {i} is a relevant block, we simply say that the ith variable is relevant to f .
As will become clear later, all our algorithms reject a function f only when they have found k + 1
pairwise disjoint blocks B1,..., Bk+1 and a distinguishing pair for each Bi . When this occurs, it
means that B1,..., Bk+1 are pairwise disjoint relevant blocks of f , which implies that f cannot be
a k-junta. As a result, our algorithms are one-sided. To prove their correctness, it suffices to show
that they reject with probability at least 2/3 when f is ϵ-far from k-juntas with respect to D.
For the standard property testing model under the uniform distribution, Blais (2009) obtained a
nearly optimal algorithm:
Theorem 2.3 (Blais 2009). There exists a one-sided, O((k/ϵ ) + k log k)-query algorithm UniformJunta(f , k, ϵ ) that rejects f with probability at least 2/3 when it is ϵ-far from k-juntas under the uniform distribution. Moreover, it rejects only when it has found k + 1 pairwise disjoint blocks
and a distinguishing pair of f for each of them.
Binary Search. The standard binary search procedure (see Figure 1) takes as input two strings
x,y ∈ {0, 1}
n with f (x)  f (y), makes O(logn) queries on f , and returns a pair of strings x 
,y ∈
{0, 1}
n with f (x 
)  f (y
) and x  = y(i) for some i ∈ diff(x,y), i.e., a distinguishing pair for the
ith variable for some i ∈ diff(x,y).
However, we cannot afford to use the standard binary search procedure directly in our main
algorithm due to its query complexity of O(logn); recall, our goal is to have the query complexity
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.   
1:8 Z. Liu et al.
Fig. 1. Description of the standard binary search procedure.
Fig. 2. Description of the blockwise version of the binary search procedure.
depend on k only. Instead, we will employ a blockwise version of the binary search procedure, as
described in Figure 2. It takes as input two strings x,y ∈ {0, 1}
n with f (x)  f (y) and a sequence
of pairwise disjoint blocks B1,..., Br such that
diff(x,y) ⊆ B1 ∪···∪ Br
(i.e., (x,y) is a distinguishing pair for B1 ∪···∪ Br ), makes O(log r) queries on f , and returns two
strings x 
,y ∈ {0, 1}
n satisfying f (x 
)  f (y
) and diff(x 
,y
) ⊆ Bi for some i ∈ [r] (i.e., (x 
,y
) is
a distinguishing pair for one of the blocks Bi in the input).
3 WARMUP: A TESTER WITH O((K/ϵ ) + K LOG N ) QUERIES
As a warmup, we present in this section a simple, one-sided distribution-free algorithm for testing
k-juntas (SimpleDJunta, where the capital letter D is a shorthand for distribution-free). It uses
O((k/ϵ ) + k logn) queries, where n as usual denotes the number of variables of the function being
tested. The idea behind SimpleDJunta and its analysis (Lemma 3.2 below) will be useful in the
next section where we present our main algorithm to remove the dependency on n.
The algorithm SimpleDJunta maintains a set I ⊂ [n], which is such that a distinguishing pair
has been found for each i ∈ I (i.e., I is a set of relevant variables of f discovered so far). The
algorithm sets I = ∅ at the beginning and rejects only when |I | reaches k + 1, which implies immediately that the algorithm is one-sided. SimpleDJunta proceeds round by round. In each round
it draws a pair of random strings x and y with xI = yI . If f (x)  f (y), then the standard binary
search procedure is used on x and y to find a distinguishing pair for a new variable i ∈ I, which is
then added to I.
The description of the algorithm can be found in Figure 3. The following theorem establishes
its correctness.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.   
Distribution-free Junta Testing 1:9
Fig. 3. Description of the distribution-free testing algorithm SimpleDJunta for k-juntas.
Theorem 3.1. (i) The algorithm SimpleDJunta makes O((k/ϵ ) + k logn) queries and always accepts when f is a k-junta. (ii) It rejects with probability at least 2/3 if f is ϵ-far from k-juntas with
respect to D.
Proof. For part (i), note that the algorithm only runs binary search (and spends O(logn)
queries) when f (x)  f (y) and this happens at most k + 1 times (even though the algorithm has
O(k/ϵ ) rounds). The rest of part (i) is immediate from the description of the algorithm.
For part (ii), it suffices to show that when |I | ≤ k at the beginning of a round, a new relevant
variable is discovered in this round with at least a modestly large probability. For this purpose,
we use the following simple but crucial lemma and note the fact that x and y on line 3 can be
equivalently drawn by first sampling x ← D and w ← {0, 1}
n and then setting y = xI ◦ wI (the
way we draw x and y in Figure 3 via R ← I makes it easier to connect with the main algorithm in
the next section).
Lemma 3.2. If f is ϵ-far from k-juntas with respect to D, then for any I ⊂ [n] of size at most k, we
have
Pr x←D,w← {0,1}n
[f (x)  f (xI ◦ wI )] ≥ ϵ/2. (1)
Before proving Lemma 3.2, we use it to finish the proof of part (ii). Assuming Lemma 3.2 and
that f is ϵ-far from k-juntas with respect to D, for each round in which |I | ≤ k the algorithm finds
a new relevant variable with probability at least ϵ/2. Using a coupling argument, the probability
that the algorithm rejects f (i.e., |I | reaches k + 1 during the 8(k + 1)/ϵ rounds) is at least the
probability that
8(k
+1)/ϵ
i=1
Zi ≥ k + 1,
where Zi ’s are i.i.d. {0, 1}-variables that are 1 with probability ϵ/2. It follows from the Chernoff
bound that the latter probability is at least 2/3. This finishes the proof of the theorem.
Proof of Lemma 3.2. Let I be a subset of [n] of size at most k. To prove Equation (1) forI, we use
I to define the following Boolean function h : {0, 1}
n → {0, 1} over n variables: for each x ∈ {0, 1}
n,
we set
h(x) := arg max
b ∈ {0,1}

Pr w← {0,1}n
[f (xI ◦ wI ) = b]

,
where we break ties arbitrarily. Then for any x ∈ {0, 1}
n, we have
Pr w← {0,1}n
[f (xI ◦ wI ) = h(x)] ≥ 1/2. (2)
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.     
1:10 Z. Liu et al.
Furthermore, we have
Pr x←D,w← {0,1}n
[f (x)  f (xI ◦ wI )]
=

z ∈ {0,1}n
Pr
x←D[x = z] · Pr w← {0,1}n
[f (z)  f (zI ◦ wI )]
≥

z ∈ {0,1}n
Pr
x←D[x = z] · ((1/2) · 1[f (z)  h(z)])
= (1/2) · Pr
x←D[f (x)  h(x)] ≥ ϵ/2,
where the first inequality follows from Equation (2) and the second inequality follows from the
assumption that f is ϵ-far from every k-junta with respect to D and the fact that h is a k-junta
(since it only depends on variables in I and |I | ≤ k). This finishes the proof of the lemma.
4 PROOF OF THEOREM 1.1: A TESTER WITH O˜ (K2)/ϵ QUERIES
In this section, we present our main O˜ (k2)/ϵ-query algorithm for the distribution-free testing of
k-juntas. We start with some intuition behind the algorithm.
4.1 Intuition
Recall that the factor of logn in the query complexity of SimpleDJunta from the previous section
is due to the use of the standard binary search procedure. To avoid it, one could choose to terminate
each call to binary search early but this ends up giving us relevant blocks of variables instead of
relevant variables. To highlight the challenge, imagine that the algorithm has found so far  ≤ k
many pairwise disjoint relevant blocks Bj , j ∈ []; i.e., it has found a distinguishing pair for each
block Bj . By definition, each Bj must contain at least one relevant variable ij ∈ Bj . However, we
do not know exactly which variable in Bj is ij , and thus it is not clear how to draw a set R from I
uniformly at random, where I = {ij : j ∈ []}, as on line 3 of SimpleDJunta, to apply Lemma 3.2
to discover a new relevant block. It seems that we are facing a dilemma when trying to improve
SimpleDJunta to remove the logn factor: unless we pin down a set of relevant variables, it is
not clear how to draw a random set from their complement, but pinning down a single relevant
variable using the standard binary search procedure would already cost logn queries.
To explain the main idea behind our O˜ (k2)/ϵ-query algorithm, let us assume again that  ≤ k
many disjoint relevant blocks Bj have been found so far, with a distinguishing pair (x[j]
,y[j]
) for
each Bj (satisfying that diff(x[j]
,y[j]
) ⊆ Bj and f (x[j]
)  f (y[j]
) by definition). Let
w[j] = 
x[j]

Bj
= 
y[j]

Bj
∈ {0, 1}
Bj .
Next let us assume further that the function дj := fw[j] , for each j ∈ [], is a literal, i.e., either
дj (z) = zij for all z ∈ {0, 1}Bj or дj (z) = zij for all z ∈ {0, 1}Bj , for some variable ij ∈ Bj , but the
variable ij is of course unknown to the algorithm. (While this may seems very implausible, we
make this assumption for now and explain below why it is not too far from real situations.)
To make progress, we draw a random two-way partition of each Bj into Pj and Qj , i.e., each
i ∈ Bj is added to Pj or Qj with probability 1/2 (so they are disjoint and Bj = Pj ∪ Qj). We make
three simple but crucial observations to increase the number of disjoint relevant blocks by one.
(1) Since дj is assumed to be a literal on the ijth variable (and by the definition of дj we
have query access to дj), it is easy to tell whether ij ∈ Pj or ij ∈ Qj , simply by picking an
arbitrary string x ∈ {0, 1}Bj and comparing дj (x) with дj (x (Pj )
). Below, we assume that the
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.              
Distribution-free Junta Testing 1:11
algorithm correctly determines whether ij is in Pj or Qj for all j ∈ []. We let Sj denote
the element of {Pj, Qj} that contains ij and let Tj denote the other one. We also assume
below that the algorithm has obtained a distinguishing pair of дj for each block Sj .
(2) Next, we draw a subset T of B1 ∪···∪ B uniformly at random. Crucially, the way that Pj
and Qj were drawn, and the above assumption that Sj contains ij , implies that
R := T ∪ T1 ∪···∪ T
is indeed a subset of I drawn uniformly at random (recall that I = {ij : j ∈ []}), since other
than those in I, each variable is included in R independently with probability 1/2. If we
draw a random string x ← D, then Lemma 3.2 implies that f (x)  f (y), where y = x(R)
,
with probability at least ϵ/2.
(3) Finally, assuming that f (x)  f (y) (with diff(x, y) = R), running the blockwise binary
search on x, y and blocks T, T1,..., T will lead to a distinguishing pair for one of these
blocks and will only require O(log ) ≤ O(log k) queries. If it is a distinguishing pair for
T, then we can add T to the list of relevant blocks B1,..., B and they remain pairwise
disjoint. If it is Tj for some j ∈ [], then we can replace Bj in the list by Sj and Tj , for
each of which we have found a distinguishing pair (recall that a distinguishing pair has
already been found for each Sj in the first step). In either case, we have that the number
of pairwise disjoint relevant blocks grows by one.
Coming back to the assumption we made earlier, although дj is very unlikely to be a literal, it
must fall into one of the following three cases: (1) close to a literal; (2) close to a (all-0 or all-1)
constant function; or (3) far from 1-juntas. Here in all cases “close” and “far” means with respect
to the uniform distribution over {0, 1}Bj . As we discuss in more detail in the rest of the section,
with some more careful probability analysis the above arguments generalize to the case in which
every дj is only close to (rather than exactly) a literal. However, if one of the blocks Bj is in case
(2) or (3), then (using the fact that we have a distinguishing pair for Bj) it is easy to split Bj into
two blocks and find a distinguishing pair for each of them. (For example, for case (3) this can be
done by running Blais’s uniform distribution junta testing algorithm.) As a result, we can always
make progress by increasing the number of pairwise disjoint relevant blocks by one. Our algorithm
basically keep repeating these steps until the number of such blocks reaches k + 1.
4.2 Description of the Main Algorithm and the Proof of Correctness
Our algorithm MainDJunta(f , D, k, ϵ ) is described in Figure 4. It maintains two collections of
blocks V = {B1,..., Bv } (V for “verified”) and U = {C1,...,Cu } (U for “unverified”) for some nonnegative integers v and u. They are set to be ∅ at initialization and always satisfy the following
properties:
(A) B1,..., Bv ,C1,...,Cu ⊆ [n] are pairwise disjoint (nonempty) blocks of variables;
(B) A distinguishing pair has been found for each of these blocks. For notational convenience,
we use (x[j]
,y[j]
) to denote the distinguishing pair for each Bj and (x[C]
,y[C]
) to denote the
distinguishing pair for each block C ∈ U . We also use the notation
w[j] := 
x[j]

Bj
= 
y[j]

Bj
∈ {0, 1}
Bj and w[C] := 
x[C]

C = 
y[C]

C ∈ {0, 1}
C,
and we let дj := fw[j] and дC := fw[C] , Boolean functions over {0, 1}Bj and {0, 1}
C,
respectively.
The algorithm rejects only when the total number of blocks v + u ≥ k + 1 so it is one-sided.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.            
1:12 Z. Liu et al.
Fig. 4. Description of the distribution-free testing algorithm MainDJunta for k-juntas.
Throughout the algorithm and its analysis, we set a key parameter γ := 1/(8k). Blocks in V are
intended to be those that have been “verified” to satisfy the condition that дj is γ -close to a literal
(for some unknown variable ij ∈ Bj) under the uniform distribution, while blocks in U have not
been verified yet so they may or may not satisfy the condition. More formally, at any point in the
execution of the algorithm we say that the algorithm is in good condition if its current collections
V and U satisfy conditions (A), (B), and
(C) Every дj , j ∈ [v], is γ -close to a literal under the uniform distribution over {0, 1}Bj .
The algorithm MainDJunta(f , k, ϵ ) starts with V = U = ∅ and proceeds round by round. For
each round, we consider two different types that the round may have: type 1 is that u = 0, and
type 2 is that u > 0. In a type-1 round (with u = 0), we follow the idea sketched in Section 4.1 to
increase the total number of disjoint relevant blocks by one. We prove the following lemma for
this case in Section 4.3.
Lemma 4.1. Assume that MainDJunta is in good condition at the beginning of a round, withu = 0
and v ≤ k. Then it must remain in good condition at the end of this round. Moreover, letting V  and
U  be the two collections of blocks at the end of this round, we have either |V 
| = v and |U 
| = 1, or
|V 
| = v − 1 and |U 
| = 2 with probability at least ϵ/4.
In a type-2 round (with u ≥ 1), we pick an arbitrary block C from U and check whether дC
is close to a literal under the uniform distribution. The following lemma, which we prove in
Section 4.4, shows that with high probability, either C is moved to collection V and the algorithm
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.
Distribution-free Junta Testing 1:13
remains in good condition, or the algorithm finds two disjoint subsets of C and a distinguishing
pair for each of them so that V stays the same but |U | goes up by one (we add these two blocks to
U , since they have not yet been verified).
Lemma 4.2. Assume that MainDJunta is in good condition at the beginning of a round, with u > 0
and v + u ≤ k. Then with probability at least 1 − 1/(64k), one of the following two events occurs at
the end of this round (letting V  and U  be the two collections of blocks at the end of this round):
(1) The algorithm remains in good condition with |V 
| = |V | + 1 and |U 
| = |U | − 1; or
(2) The algorithm remains in good condition with V  = V and |U 
| = |U | + 1.
Assuming Lemmas 4.1 and 4.2, we are ready to prove the correctness of MainDJunta.
Theorem 4.3. (i) The algorithm MainDJunta makes O˜ (k2)/ϵ queries and always accepts f when
it is a k-junta. (ii) It rejects with probability at least 2/3 when f is ϵ-far from every k-junta with
respect to D.
Proof of Theorem 4.3 Assuming Lemmas 4.1 and 4.2. MainDJunta is one-sided, since it rejects f only when it has found k + 1 pairwise disjoint relevant blocks of f . Its query complexity is
(# type-1 rounds) · (# queries per type-1 round) + (# type-2 rounds) · (# queries per type-2 round)
= O(k/ϵ ) · (O(k) + O(log k)) + O(k) · O(log k) · O(k) = O(k2
/ϵ ) + O(k2 log k) = O(k2 log k)/ϵ.
In the rest of the proof we show that it rejects f with probability at least 2/3 when f is ϵ-far from
every k-junta with respect to D.
For this purpose, we introduce a simple potential function F to measure the progress:
F (V,U ) := 3|V | + 2|U |.
Each round of the algorithm is either of type-1 (when |U | = 0) or of type-2 (when |U | > 0). By
Lemma 4.1, if the algorithm is in good condition at the beginning of a type-1 round, then the
algorithm ends the round in good condition and the potential function F goes up by at least one
with probability at least ϵ/4 (in which case, we say that the algorithm succeeds in this type-1
round). By Lemma 4.2, if the algorithm is in good condition at the beginning of a type-2 round,
then the algorithm ends the round in good condition and F goes up by at least one with probability
at least 1 − 1/(32k) (in which case, we say it succeeds in this type-2 round).
Note that F is 0 at the beginning (V = U = ∅) and that we must have |U | + |V | ≥ k + 1 (and
thus, the algorithm rejects) when the potential function F reaches 3(k + 1) or above. As a result, a
necessary condition for the algorithm to accept is that one of the following two events occurs:
E1: At least one of the (no more than 3(k + 1) many) type-2 rounds fails.
E2: E1 does not occur (so the algorithm ends every round in good condition, and the reason
that the algorithm accepts cannot be that it uses up all the 3(k + 1) many type-2 rounds),
and the algorithm uses up all the 64k/ϵ many type-1 rounds but at most 3k + 2 of them
succeed.
By a union bound, the probability of E1 is at most
3(k + 1) · 1/(64k) ≤ 6k · 1/(64k) < 1/8.
As the algorithm ends every round in good condition, it follows from Lemma 4.1 from a coupling
argument that the probability of E2 is at most the probability that
64

k/ϵ
i=1
Zi ≤ 3k + 2,
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018. 
1:14 Z. Liu et al.
Fig. 5. Description of the subroutine WhereIsTheLiteral.
where Zi ’s are i.i.d. {0, 1}-valued random variables that take 1 with probability ϵ/4. It follows from
the Chernoff bound the probability is at most (using 3k + 2 ≤ 5k)
exp 
−
 11
16 	2
·
16k
2


= exp 
−121k
32 

< exp(−3) < 1/8.
Finally, it follows from a union bound that the algorithm rejects with probability at least 3/4.
4.3 Proof of Lemma 4.1
We start with a lemma for the subroutine WhereIsTheLiteral, which is described in Figure 5.
Lemma 4.4. Assume that д : {0, 1}B → {0, 1} is γ -close (with respect to the uniform distribution) to
a literal xi or xi for some i ∈ B. If i ∈ P, then WhereIsTheLiteral(д, P,Q) returns a distinguishing
pair of д for P with probability at least 1 − 4γ ; If i ∈ Q, then it returns a distinguishing pair of д for
Q with probability at least 1 − 4γ .
Proof. Let K be the set of strings x ∈ {0, 1}B such that д(x) disagrees with the literal to
which it is γ -close (so |K| ≤ γ · 2|B |
). We work on the case when i ∈ Q; the case when i ∈ P is
similar.
By the description of WhereIsTheLiteral, it returns a distinguishing pair for Q if
д(w ◦ z) = д(w ◦ z(P )
) and д(w ◦ z
)  д(w ◦ z(Q)
).
Note that this holds if all four strings fall outside of K and thus, the probability that it does not
hold is at most the probability that at least one of these four strings falls inside K. The latter by
a union bound is at most 4γ , since each of these four strings is drawn uniformly at random from
{0, 1}B by itself. This finishes the proof of the lemma.
We are now ready to prove Lemma 4.1.
Proof of Lemma 4.1. First, it is easy to verify that if the algorithm starts a round in good
condition, then it ends it in good condition. This is because whenever a block is added to U , it is
disjoint from other blocks, and we have found a distinguishing pair for it.
Next it follows directly from Lemma 4.4 and a union bound that, for any sequence of partitions
Pj and Qj of Bj picked on line 6, the probability that the for-loop correctly sets Sj to be the one
that contains the special variable ij for all j ∈ [v] is at least (recalling that γ = 1/(8k))
1 − 4γ · v ≥ 1 − 4γ · k = 1/2.
Now, we can view the process equivalently as follows. First, we draw x ← D, T ← B1 ∪···∪ Bv ,
and random partitions Pj , Qj of each Bj . If we let T∗
j be the set in Pj , Qj that does not contain the special variable, then R∗ = T ∪ T∗
1 ∪···∪ T∗
v is a set drawn uniformly at random from I,
where I = {ij : j ∈ [v]} consists of the special variables. Therefore, it follows from Lemma 3.2 that
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.   
Distribution-free Junta Testing 1:15
Fig. 6. Description of the subroutine Literal.
f (x)  f (x(R∗ )
) with probability at least ϵ/2. Since with probability at least 1/2, the set R on line
11 agrees with R∗, we have that the algorithm reaches line 12 with f (x)  f (y) with probability
at least ϵ/4. Given this, the lemma is immediate by inspection of lines 12-17 of the algorithm.
4.4 Proof of Lemma 4.2
First it follows from the description of the subroutine Literal(д) in Figure 6 that it either returns
“true” or a pair of nonempty disjoint subsets C
,C∗ of C and a distinguishing pair of д for each of
them (see Theorem 2.3). Next, let C ∈ V be the block picked in line 20. If д is γ -close to a literal,
then it is easy to verify that one of the two events described in Lemma 4.2 must hold (using the
property of Literal(д) above). So, we focus on the other two cases in the rest of the proof: д is
γ -far from 1-juntas or д is γ -close to a (all-1 or all-0) constant function. In both cases, we show
below that the second event happens with high probability.
When д is γ -far from 1-juntas under the uniform distribution, we have that one of the log k + 6
calls to UniformJunta in Literal rejects with probability at least
1 − (1/3)
log k+6 > 1 − 1/(64k).
The second event in Lemma 4.2 occurs when this happens.
When д is γ -close to a constant function (say the all-1 function), we have that either string x
or y in the distinguishing pair for C disagrees with the function (say д(x) = 0, since д(x)  д(y)).
Let K be the set of strings in {0, 1}
C that disagree with the all-1 function. Then line 7 of Literal(д)
does not hold only when one of x (C
) or x (C∗ ) lies in K. As both strings are distributed uniformly
over {0, 1}
C by themselves, this happens with probability at most 2γ by a union bound. Therefore,
the probability that line 7 holds at least once is at least
1 − (2γ )
log k+3 = 1 − (1/(4k))log k+3 > 1 − (1/4)
log k+3 = 1 − 1/(64k2).
As a result, the second event in Lemma 4.2 occurs with probability at least 1 − 1/(64k2).
This finishes the proof of Lemma 4.2.
5 PROOF OF THEOREM 1.2: AN Ω(2K /3)-QUERY NON-ADAPTIVE LOWER BOUND
In this section, we prove the Ω(2k/3) lower bound for the non-adaptive distribution-free testing of
k-juntas that was stated as Theorem 1.2. We start with some notation. Given a sequence Y = (yi :
i ∈ [q]) of q strings in {0, 1}
n and a Boolean function ϕ : {0, 1}
n → {0, 1}, we write ϕ(Y ) to denote
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.    
1:16 Z. Liu et al.
the q-bit string α with αi = ϕ(yi ) for each i ∈ [q]. We also write Y = (yi : i ∈ [q]) ← Dq to denote
a sequence of q independent draws from the same probability distribution D.
Let k and n be two positive integers that satisfy k ≤ n/200. We may further assume that k is at
least some absolute constant C (to be specified later), since otherwise, the claimed Ω(2k/3) lower
bound on query complexity holds trivially due to the constant hidden behind the Ω. Let q = 2k/3.
For convenience, we refer to an algorithm as a q-query algorithm if it makes q sample queries and
q black-box queries each. Such algorithms are clearly at least as powerful as those that make q
queries in total. Our goal is then to show that there exists no q-query non-adaptive (randomized)
algorithm for the distribution-free testing of k-juntas over Boolean functions of n variables, even
when the distance parameter ϵ is 1/3.
By Yao’s minimax principle, we focus on q-query non-adaptive deterministic algorithms. Such
an algorithm A (which consists of two deterministic maps A1 and A2 as discussed below) works as
follows. Upon an input pair (ϕ, D), where ϕ : {0, 1}
n → {0, 1} and D is a probability distribution
over {0, 1}
n, the algorithm receives in the first phase a sequence Y = (yi : i ∈ [q]) of q strings
(which should be thought of as samples from D) and a binary string α = ϕ(Y ) of length q. In the
second phase, the algorithm A uses the first map A1 to obtain a sequence of q strings Z = (zi : i ∈
[q]) = A1 (Y, α) and feeds them to the black-box oracle. Once the query results β = ϕ(Z) are back,
A2 (Y, α, β) returns either 0 or 1 (notice that we do not need to include Z as an input of A2, since it
is determined by Y and α), in which cases the algorithm A either rejects or accepts, respectively. A
randomized algorithm T works similarly and consists of two similar maps T1 and T2 but both are
randomized.
Given the description above, unlike typical deterministic algorithms, whether A accepts or not
depends on not only (ϕ, D) but also the sample strings Y ← Dq it draws. Formally, we have
Pr[A accepts (ϕ, D)] = Pr
Y←Dq
[A2 (Y,ϕ(Y),ϕ(A1 (Y,ϕ(Y)))) = 1].
The plan of the rest of the section is as follows. We define in Section 5.1 a pair of probability
distributions YES and N O over pairs (ϕ, D), where ϕ is a Boolean function over n variables and
D is a distribution over {0, 1}
n. For clarity, we use (f , D) to denote pairs in the support of YES and
(д, D) to denote pairs in the support of N O. We show that (1) Every (f , D) in the support of YES
satisfies that f is a k-junta (Lemma 5.2); (2) With probability 1 − ok (1), (g, D) ←NO satisfies
that g is 1/3-far from every k-junta with respect to D (Lemma 5.3). To obtain Theorem 1.2, it
suffices to prove the following main technical lemma, which informally says that any q-query
non-adaptive deterministic algorithm must behave similarly when it is run on (f, D) ← YES
versus (g, D) ←NO:
Lemma 5.1. Any q-query deterministic algorithm A satisfies
| E(f, D)←YES[Pr [A accepts (f, D)]] − E(g, D)←N O[Pr [A accepts (g, D)]] | ≤ 1/4. (3)
Proof of Theorem 1.2 Assuming Lemmas 5.1, 5.2, and 5.3. Assume for a contradiction that
there exists a q-query non-adaptive randomized algorithm T for the distribution-free testing of
k-juntas over n-variable Boolean functions when ϵ = 1/3. Then it follows from Lemmas 5.2 and
5.3 that
E(f, D)←YES[Pr [T accepts (f, D)]] − E(g, D)←N O[Pr [T accepts (g, D)]] ≥ 1/3 − ok (1),
since the first expectation is at least 2/3 and the second is at most
1/3(1 − ok (1)) + ok (1) ≤ 1/3 + ok (1).
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.
Distribution-free Junta Testing 1:17
As T is a probability distribution over deterministic algorithms, there must exist a q-query nonadaptive deterministic algorithm A that satisfies
E(f, D)←YES[Pr [A accepts (f, D)]] − E(g, D)←N O[Pr [A accepts (g, D)]] ≥ 1/3 − ok (1),
a contradiction with Lemma 5.1 when k is sufficiently large.
5.1 The YES and N O Distributions
Given J ⊆ [n], we partition {0, 1}
n into sections (with respect to J) where the z-section, z ∈ {0, 1}J ,
consists of those x ∈ {0, 1}
n that have xJ = z. We write JUNTAJ to denote the uniform distribution over all juntas over J. More precisely, a Boolean function h : {0, 1}
n → {0, 1} drawn
from JUNTAJ is generated as follows: For each z ∈ {0, 1}J , a bit b(z) is chosen independently
and uniformly at random, and for each x ∈ {0, 1}
n the value of h(x) is set to b(xJ ). Let
m := 36 · 2k lnn.
We start with YES. A pair (f, D) drawn from YES is generated as follows:
(1) First, we draw independently a subset J of [n] of size k uniformly at random and a subset
S of {0, 1}
n of size m uniformly at random.
(2) Next, we draw f ← JUNTAJ and set D to be the uniform distribution over S.
For technical reasons that will become clear in Section 5.2 we use YES∗ to denote the probability distribution supported over triples (f , D, J), with (f, D,J) ← YES∗ being generated by the
same two steps above (so, the only difference is that we include J in elements of YES∗
).
The following observation is straightforward from the definition of YES.
Lemma 5.2. The function f is a k-junta for every pair (f , D) in the support of YES.
We now describe N O. A pair (g, D) drawn from N O is generated as follows:
(1) We draw J and S in the same way as the first step of YES.
(2) Next, we draw h ← JUNTAJ and a map γ : S → {0, 1} uniformly at random by choosing a bit independently and uniformly at random for each string in S. We usually refer to
h as the “background junta.”
(3) The distribution D is set to be the uniform distribution over S, which is the same as YES.
The function g : {0, 1}
n → {0, 1} is defined using h, S and γ as follows:
(a) For each string y ∈ S, set g(y) = γ (y);
(b) For each string x  S, if there exists no y ∈ S with yJ = xJ and d(x,y) ≤ 0.4n, set
g(x) = h(x); otherwise, we set g(x) = 1 if there exists such a y ∈ S with γ (y) = 1,
and set g(x) = 0 if every such y ∈ S has γ (y) = 0. (The choice of the tie-breaking
rule here is not important; we just pick one to make sure that g is well defined in all
cases.)
Similarly, we let N O∗ denote the distribution supported on triples (д, D, J) as generated above.
See Figure 7 for an illustration of a function drawn from N O. To gain some intuition, we first
note that about half of the strings z ∈ S have д(z) disagree with the value of the background junta
on the section it lies in. With such a string z in hand (from one of the samples received in the first
phase), an algorithm may attempt to find a string w that lies in the same section as z but satisfies
д(z)  д(w). If such a string is found, then the algorithm knows for sure that (д, D) is from the
N O distribution. However, finding such a w is not easy, because one must flip more than 0.4n bits
of z, but without knowing the variables in J it is hard to keep w in the same section as z after
flipping this many bits.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.   
1:18 Z. Liu et al.
Fig. 7. A schematic depiction of how {0, 1}
n is labeled by a function д from N O. The domain {0, 1}
n is
partitioned into 2k sections corresponding to different settings of the variables in J; each section is a vertical
strip in the figure. Shaded regions correspond to strings where д evaluates to 1 and unshaded regions to
strings where д evaluates to 0. Each string in S is a black dot and the value of д on each such string is chosen
uniformly at random. Since in this figure the truncated circles are disjoint, the tie-breaking rule does not
come into effect, and for each z ∈ S, all strings in its section within distance at most 0.4n (the points in the
truncated circle around z) have the same value as z. The value of д on other points is determined by the
background junta h, which assigns a uniform random bit to each section.
Next, we prove that with high probability, (g, D) ←NO satisfies that g is 1/3-far from every
k-junta with respect to D:
Lemma 5.3. With probability at least 1 − ok (1), (g, D) ←NO is such that g is 1/3-far from every
k-junta with respect to the distribution D.
Proof. Fix a k-junta h, i.e., any set I ⊂ [n] with |I | = k and any 2k -bit truth table over variables in I. We have that distD (g,h) is precisely the fraction of strings z ∈ S such that γ (z)  h(z).
Since each bit γ (z) is drawn independently and uniformly at random, we have that
Pr
(g, D)←N O [ distD (g,h) ≤ 1/3 ] = Pr
j←Bin(m,1/2)
[j ≤ m/3 ],
which, recalling that m = 36 · 2k lnn, by a standard Chernoff bound is at most e−m/36 = n−2k
. The
result follows by a union bound over all (at most)

n
k


· 22k
≤ nk · 22k
= ok

n2k 
possible k-juntas h over n variables. This finishes the proof of the lemma.
Given Lemmas 5.2 and 5.3, to prove Theorem 1.2 it remains only to prove Lemma 5.1.
5.2 Proof of Lemma 5.1
The following definitions will be useful. Let Y = (yi : i ∈ [q]) be a sequence of q strings in {0, 1}
n,
α be a q-bit string, and J ⊂ [n] be a set of variables of size k. We say that (Y, α, J) is consistent if
αi = αj for all i, j ∈ [q] with yi
J = yj
J . (4)
Given a consistent triple (Y, α, J), we write JUNTAY,α, J to denote the uniform distribution over all juntas h over J that are consistent with (Y, α). More precisely, a draw of h ←
JUNTAY,α, J is generated as follows: For each z ∈ {0, 1}J , if there exists a yi such that yi
J = z,
then h(x) is set to αi for all x ∈ {0, 1}
n with xJ = z; if no such yi exists, then a uniform random bit
b(z) is chosen independently and h(x) is set to b(z) for all x with xJ = z.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.  
Distribution-free Junta Testing 1:19
To prove Lemma 5.1, we first derive from A a new randomized algorithm A that works on triples
(ϕ, D, J) from the support of either YES∗ or N O∗
. Again for clarity we use ϕ to denote a function
from the support of YES/YES∗ or N O/N O∗
, f to denote a function from YES/YES∗ and д to
denote a function from N O/N O∗
.
In addition to being randomized, A differs from A in two important ways:
(1) Like A, A receives samples Y ← Dq and ϕ(Y), but unlike A, A also receives J for free.
(2) UnlikeA,A does not make any black-box queries but simply runs on the triple (Y,ϕ(Y), J)
it receives at the beginning. So formally A is a randomized algorithm that runs on triples
(Y, α, J), where Y = (yi : i ∈ [q]) is a sequence of q strings, α is a q-bit string, and J ⊂ [n]
is a set of variables of size k, and outputs “accept” or “reject.”
A detailed description of the randomized algorithm A running on (Y, α, J) is as follows:
(1) First, if (Y, α, J) is not consistent, then A immediately halts and rejects (simply because
this can never occur if (Y, α, J) is obtained from a triple (f , D, J) in the support of YES∗
).
Otherwise, A applies A1 on (Y, α) to obtain a sequence Z = (Zi : i ∈ [q]) of q strings.
(2) Next, A draws h ← JUNTAY,α, J . (This is the only part of A that is randomized.)
(3) Finally, A runs A2 (Y, α, h
(Z)) and outputs the same result (accept or reject).
From the description of A above, whether it accepts a triple (ϕ, D, J) or not depends on both
the randomness of Y and h
. Formally, we have
Pr [A accepts (ϕ, D, J)] = Pr
Y,h
[(Y,α, J) is consistent and A2 (Y,α, h
(Z)) = 1].
Lemma 5.1 follows immediately from the following three lemmas (note that the marginal distribution of (f, D) in YES∗ (or (g, D) in N O∗
) is the same as YES (or N O)). In all three lemmas,
we assume that A is a q-query non-adaptive deterministic algorithm while A is the randomized
algorithm derived from A as described above.
Lemma 5.4 (A behaves similarly on YES∗ and N O∗
). We have
| E(f, D,J)←YES∗ [Pr [A accepts (f, D,J)]] − E(g, D,J)←N O∗ [Pr[A accepts (g, D,J)]]| ≤ 1/8.
Lemma 5.5 (A and A behave identically on YES and YES∗
, respectively). We have
E(f, D,J)←YES∗ [Pr [A accepts (f, D)]] = E(f, D,J)←YES∗ [Pr [A accepts (f, D,J)]]. (5)
Lemma 5.6 (A and A behave similarly on N O and N O∗
, respectively). We have
| E(g, D,J)←N O∗ [Pr [A accepts (g, D)]] − E(g, D,J)←N O∗ [Pr [A accepts (g, D,J)]]| ≤ 1/8.
We start with the proof of Lemma 5.4, which says that a limited algorithm such as A cannot
effectively distinguish between a draw from YES∗ versus N O∗
:
Proof of Lemma 5.4. Since A runs on (Y, α, J), it suffices to show that the distributions of
(Y,α,J) induced from YES∗ and N O∗ have small total variation distance. For this purpose, we
first note that the distributions of (Y,J) induced from YES∗ and N O∗ are identical: In both cases,
Y and J are independent; J is a random subset of [n] of size k; Y is obtained by first sampling a
subset S of {0, 1}
n of size m and then drawing a sequence of q strings from S with replacement.
Fix a pair (Y, J) in the support of (Y,J). We say Y is scattered by J if yi
J  yj
J for all i  j ∈ [q].
In particular, this implies that no string appears more than once in Y. The following claim, whose
proof we defer, shows that Y is scattered by J with high probability.
Claim 5.7. We have that Y is scattered by J with probability at least 1 − O(2−k/3).
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.  
1:20 Z. Liu et al.
Fix any (Y, J) in the support of (Y,J) such that Y is scattered by J. We claim that the distributions
of α conditioning on (Y,J) = (Y, J) in the YES∗ case and the N O∗ case are identical, from which
it follows that the total variation distance between the distributions of (Y,α,J) in the two cases is
at most O(2−k/3) ≤ 1/8 when k is sufficiently large. Indeed, α is uniform over strings of length q
in both cases. This is trivial for N O∗
. For YES∗ note that α is determined by the random k-junta
f ← JUNTAJ ; the claim follows from the assumption that Y is scattered by J.
Proof of Claim 5.7. We fix J and show that Y is scattered by J with high probability. As strings
of Y are drawn one by one, the probability of yi colliding with one of the previous samples is at
most (i − 1)/m ≤ q/m. By a union bound, all strings in Y are distinct with probability at least
1 − q · (q/m) = 1 − q2
/m = 1 − O

2−k/3

.
Conditioning on this event, Y = (yi ) is distributed precisely as a uniform random sequence from
{0, 1}
n with no repetition and thus each pair (yi
, yj
) is distributed uniformly over pairs of distinct
strings in {0, 1}
n. As a result, we have
Pr
yi
J = yj
J

= 2n−k − 1
2n − 1
≤
1
2k .
By a union bound over (
q
2 ) pairs, we have that the probability of Y being scattered by J is at least
(1 − O(2−k/3)) ·

1 −

q
2


· 2−k


≥ 1 − O(2−k/3).
This finishes the proof of the claim.
Next, we prove Lemma 5.5.
Proof of Lemma 5.5. The first expectation in Equation (5) is equal to the probability that
A2 (Y,α,f(A1 (Y,α)) = 1,
where (f, D,J) ← YES∗
, Y ← Dq and α = f(Y). For the second expectation, since the triple on
which we run A is always consistent, we can rewrite it as the probability that
A2 (Y,α, h
(A1 (Y,α))) = 1,
where (f, D,J) ← YES∗
, Y ← Dq, α = f(Y) and h ← JUNTAY,α,J.
To show that these two probabilities are equal, we first note that the distributions of (Y,α,J) are
identical. Fixing any triple (Y, α, J) in the support of (Y,α,J), which must be consistent, we claim
that the distribution of f conditioning on (Y,α,J) = (Y, α, J) is exactly JUNTAY,α, J . This is
because, for each z ∈ {0, 1}J , if yi
J = z for some yi in Y, then we have f(x) = αi for all strings x
with xJ = z; otherwise, we have f(x) = b(z) for all x with xJ = z, where b(z) is an independent and
uniform bit. This is the same as how h ← JUNTAY,α, J is generated. It follows directly from
this claim that the two probabilities are the same. This finishes the proof of the lemma.
Finally, we prove Lemma 5.6, the most difficult among the three lemmas:
Proof of Lemma 5.6. Similar to the proof of Lemma 5.5, the first expectation is the probability of
A2 (Y,α, g(A1 (Y,α)) = 1,
where (g, D,J) ←NO∗ and α = g(Y), while the second expectation is the probability of
(Y,α,J) is consistent and A2 (Y,α, h
(A1 (Y,α))) = 1,
where (g, D,J) ←NO∗
, Y ← Dq, α = g(Y), and h ← JUNTAY,α,J. We note that the
distributions of (Y,α,J, D) in the two cases are identical.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.    
Distribution-free Junta Testing 1:21
The following definition is crucial. We say a tuple (Y, α, J, D) in the support of (Y,α,J, D) is
good if it satisfies the following three conditions (S below is the support of D):
E0: Y is scattered by J.
E1: Let Z = A1 (Y, α). Then every z in Z and every x in S \ Y have d(x, z) > 0.4n. (In S \ Y,
we abuse notation and use Y as a set that contains all strings in the sequence Y.)
E2: If a string z in Z satisfies zJ = yJ for some y in Y, then we have d(y, z) ≤ 0.4n.
We delay the proof of the following claim to the end.
Claim 5.8. We have that (Y,α,J, D) is good with probability at least 7/8.
Fix any good (Y, α, J, D) in the support and let Z = A1 (Y, α). We finish the proof by showing
that the distribution of g(Z), a binary string of length q, conditioning on (Y,α,J, D) = (Y, α, J, D)
is the same as that of h
(Z) with h ← JUNTAY,α, J . This combined with Claim 5.8 implies that
the difference of the two probabilities has absolute value at most 1/8.
To see this is the case, we partition strings of Z into Zw , where each Zw is a nonempty set that
contains all z in Z with zJ = w ∈ {0, 1}J . For each Zw , we consider the following two cases:
(1) If there exists no string y in Y with yJ = w, then by E1 strings in Zw are all far from strings
of S (i.e., the support of D) in this section and thus, g(z) = b(w) for some independent and
uniform bit b(w), for all strings z ∈ Zw .
(2) If there exists a y in Y with yJ = w (which must be unique by E0), say yi
, then by E1 and
E2 strings in Zw are all close to y and far from other strings of S in this section. As a result,
we have g(z) = αi for all strings z ∈ Zw .
So the conditional distribution of g(Z) is identical to that of h
(Z) with h ←JUNTAY,α, J .
This finishes the proof of the lemma.
Proof of Claim 5.8. We bound the probabilities of (Y,α,J, D) violating each of the three conditions E0, E1 and E2 and apply a union bound. By Claim 5.7, E0 is violated with probabilityO(2−k/3).
For E1, we fix a pair (Y, α) in the support and let  ≤ q be the number of distinct strings in Y
and Z = A1 (Y, α). Conditioning on Y = Y, S \ Y is a uniformly random subset of {0, 1}
n \ Y of size
m − . Instead of working with S \ Y, we let T denote a set obtained by making m −  draws from
{0, 1}
n uniformly at random (with replacements).
On the one hand, the total variation distance between S \ Y and T is exactly the probability that
either (1) T ∩ Y is nonempty or (2) |T| < m − . By two union bounds, (1) happens with probability
at most (m − ) · (/2n ) ≤ mq/2n and (2) happens with probability at most (m/2n ) · m. As a result,
the total variation distance is at most (mq + m2)/2n. On the other hand, the probability that one of
the strings of T has distance at most 0.4n with one of the strings of Z is at most mq · exp(−n/100)
by a Chernoff bound followed by a union bound. Thus, the probability of violating E1 is at most
(using the assumption that k ≤ n/200)
(mq + m2)/2n + mq · exp(−n/100) = O(2−n/300 lnn).
For E2, we fix a pair (Y, α) in the support and let Z = A2 (Y, α). Because J is independent from
(Y,α), it remains a subset of [n] of size k drawn uniformly at random. For each pair (y, z) with y
from Y and z from Z that satisfy d(y, z) > 0.4n, the probability of yJ = zJ is at most

0.6n
k


n
k
 ≤ (0.6)
k .
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 1. Publication date: September 2018.       
1:22 Z. Liu et al.
Since there are at most q2 many such pairs, it follows from a union bound that the probability of
violating E2 is at most q2 · (0.6)
k ≤ e−0.07k .
Finally, the lemma follows from a union bound when k (and thus, n) is sufficiently large.