The minimum path cover problem asks us to find a minimum-cardinality set of paths that cover all the nodes
of a directed acyclic graph (DAG). We study the case when the size k of a minimum path cover is small,
that is, when the DAG has a small width. This case is motivated by applications in pan-genomics, where
the genomic variation of a population is expressed as a DAG. We observe that classical alignment algorithms
exploiting sparse dynamic programming can be extended to the sequence-against-DAG case by mimicking the
algorithm for sequences on each path of a minimum path cover and handling an evaluation order anomaly
with reachability queries.
Namely, we introduce a general framework for DAG-extensions of sparse dynamic programming. This
framework produces algorithms that are slower than their counterparts on sequences only by a factor k. We
illustrate this on two classical problems extended to DAGs: longest increasing subsequence and longest common
subsequence. For the former, we obtain an algorithm with running time O(k|E| log |V |). This matches the
optimal solution to the classical problem variant when the input sequence is modeled as a path. We obtain an
analogous result for the longest common subsequence problem. We then apply this technique to the co-linear
chaining problem, which is a generalization of the above two problems. The algorithm for this problem turns
out to be more involved, needing further ingredients, such as an FM-index tailored for large alphabets and
a two-dimensional range search tree modified to support range maximum queries. We also study a general
sequence-to-DAG alignment formulation that allows affine gap costs in the sequence.
The main ingredient of the proposed framework is a new algorithm for finding a minimum path cover of
a DAG (V, E) in O(k|E| log |V |) time, improving all known time-bounds when k is small and the DAG is not
too dense. In addition to boosting the sparse dynamic programming framework, an immediate consequence
of this new minimum path cover algorithm is an improved space/time tradeoff for reachability queries in
arbitrary directed graphs.
CCS Concepts: • Theory of computation → Graph algorithms analysis; Packing and covering problems; Dynamic programming; • Applied computing → Computational genomics;
Additional Key Words and Phrases: Pattern matching, longest common subsequence, co-linear chaining, pangenomics
1 INTRODUCTION
A path cover of a directed acyclic graph (DAG) G = (V, E) is a set of paths such that every node of
G belongs to some path (note that the paths are not required to be disjoint). A minimum path cover
(MPC) is one having the minimum number of paths. The size of a MPC is also called the width ofG.
Many DAGs commonly used in genome research, such as graphs encoding human variations [10]
and graphs modeling gene transcripts [21], can consist, in the former case, of millions of nodes
and, in the latter case, of thousands of nodes (see Reference [31] for a survey of such pan-genomics
approaches). However, they generally have a small width on average; for example, splicing graphs
for most genes in human chromosome 2 have width at most 10 [46, Fig. 7]. To the best of our
knowledge, among the many MPC algorithms [8, 9, 18, 23, 36, 41], there are only three whose
complexities depend on the width of the DAG. Say the width of G is k. The first algorithm runs
in time O(|V ||E| + k|V |
2) and can be obtained by slightly modifying an algorithm for finding a
minimum chain cover in partial orders from Reference [15]. The other two algorithms are due to
Chen and Chen: the first one works in time O(|V |
2 + k
√
k|V |) [8], and the second one works in
time O(max(
√
|V ||E|, k
√
k|V |)) [9].
In this article, we present a MPC algorithm running in time O(k|E| log |V |). This is better than
all previous algorithms when k is small. For example, this holds for k = o(
√
|V |/ log |V |) and |E| =
O(|V |
3/2) (and with smaller k, the graph can be denser). Our algorithm is based on a standard
reduction of a minimum flow problem to a maximum flow problem [2].
We then proceed to show that some problems (like alignments) that admit efficient sparse dynamic programming solutions on sequences [13] can be extended to DAGs, so that their complexity
increases only by the minimum path cover size k. Namely, our improvement applies to many cases
where a data structure over previously computed solutions is maintained and queried for computing the next value. Our new MPC algorithm enables this, as its complexity is generally of the same
form as that of solving the extended problems. Careful bookkeeping is necessary due to the evaluation order not matching the reachability order among the nodes of the path cover. Given a path
cover, our technique first computes so-called forward propagation links indicating how the partial
solutions in each path in the cover must be synchronized.
To best illustrate the versatility of the technique itself, we show how to compute a longest
increasing subsequence (LIS) in a labeled DAG, in time O(k|E| log |V |). This matches the optimal
solution to the classical problem on a single sequence when it is modeled as a path (where k = 1).
We also illustrate our technique with the longest common subsequence (LCS) problem between a
labeled DAG G = (V, E) and a sequence S.
We then demonstrate the use of the framework on a more complex problem—co-linear chaining (CLC)—first introduced in Reference [32]. It has been proposed as a model of the sequence
alignment problem that scales to massive inputs and has been a subject of recent interest (see,
e.g., References [29, 38, 42, 47, 51–53]). In the CLC problem, the input is directly assumed to be
a set of N pairs of intervals in the two sequences that match (either exactly or approximately).
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.
Sparse Dynamic Programming on DAGs with Small Width 29:3
The CLC alignment solution asks for a subset of these plausible pairs that maximizes the coverage
in one of the sequences, and whose elements appear in increasing order in both sequences. The
fastest algorithm for this problem runs in the optimal O(N log N) time [1].
We define a generalization of the CLC problem between a sequence and a labeled DAG. As
motivation, we mention the problem of aligning a long sequence, or even an entire chromosome,
to a DAG storing all known variations of a population with respect to a reference genome (such
as the above-mentioned [10]). Here, the N input pairs match intervals in the sequence with paths
(also called anchors) in the DAG. This problem is not straightforward, as the topological order of
the DAG might not follow the reachability order between the anchors. Existing tools for aligning
DNA sequences to DAGs (BGREAT [27], vg [20]) rely on anchors, and our techniques have the
potential to impact the design of such tools.
The algorithm we propose for the CLC extension is more involved, and we we will develop it in
stages.
We conclude with a general alignment-specific formulation of the framework that admits affine
gap costs, and has the LIS, LCS, and a limited variant of CLC as special cases.
Related Work. Extending pattern matching to DAGs has been studied before [3, 30, 33, 37, 39].
On a graph with nodes labeled with characters, finding a path matching a pattern with a minimum
number of edit operation can be done in O(m|E|) time [33], where m is the pattern length and E
is the set of edges. There is a matching conditional lower bound holding already for the linear
case of the graph being a single path: The edit distance between two strings of length n cannot be
computed in O(n2−ϵ ) time, for any constant ϵ > 0, unless the Strong Exponential Time Hypothesis (seth) fails [4]. This quadratic boundary has been the motivation to study sparse dynamic
programming solutions on sequences [13]. We extend the work on sparse dynamic programming
to the case where one of the inputs is a DAG. To compare the obtained results to the non-sparse
setting, let us consider the longest common subsequence problem. Being a special case of the problem studied in Reference [33], finding the longest common subsequence of an input sequence of
length n and a path in a DAG can be done in O(n|E|) time. Our sparse dynamic algorithm for the
same problem works in O(k|E| log |V | + (|V | + n) logn + k|M| log logn) time, where M is the set
of matching character pairs between the sequence and the graph. For small k and |M| this can be
significantly better than the non-sparse solution.
Notation. To simplify notation, for any DAG G = (V, E), we will assume that V is always
{1,..., |V |} and that 1,..., |V | is a topological order on V (so that for every edge (u,v), we have
u < v). We will also assume that |E|≥|V | − 1. A labeled DAG is a tuple (V, E, , Σ) where (V, E) is
a DAG, and  : V → Σ assigns to the nodes labels from Σ, Σ being an ordered alphabet.
For a path P = (v1,...,vt ) in G, let the label of P, denoted (P), be the concatenation of the
labels of the nodes of P, namely (v1) ··· (vt ). Moreover, the first node of P will be called its
startpoint, and its last node will be called its endpoint. For a node v ∈ V , we denote by N−(v) the
set of in-neighbors of v and by N+(v) the set of out-neighbors of v. If there is a (possibly empty)
path from node u to node v, then we say that u reaches v. We denote by R−(v) the set of nodes
that reach v.
We denote a set of consecutive integers with interval notation [i..j], meaning {i,i + 1,..., j}. For
a pair of intervals m = ([x..y],[c..d]), we use m.x, m.y, m.c, and m.d to denote the four respective
endpoints. We also consider pairs of the form m = (P,[c..d]) where P is a path, and use m.P to
access P. For a set M, we may fix an order, to access an element as M[i].
In Table 1, we summarize some of these notations, together with some other notions that will
be introduced later.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.     
29:4 V. Mäkinen et al.
Table 1. Key Notations Used in This Article
Notation Definition
(V, E, , Σ) A labeled DAG, where (V, E) is a DAG, and  : V → Σ assigns labels
from an ordered alphabet Σ to the nodes.
(P) For a labeled DAG (V, E, , Σ) and a path P = (v1,...,vt ) in it, the label
of P is (P) = (v1) ··· (vt ).
R−(v) For a DAG G and a node v, R−(v) is the set of nodes that reach v,
namely that have a (possibly empty) path to v.
last2reach[v,i] For a path cover P1,..., PK of a DAG, last2reach[v,i] is the last node
on Pi different from node v, that reaches v, if this node exists.
forward[u]
In a DAG, the forward propagation links from a node u is the set
forward[u] of pairs (v,i), such that (v,i) ∈ forward[u] holds for any
node v and index i with last2reach[v,i] = u.
index[v,i] For a path cover P1,..., PK , index[v,i] is the position of v in Pi ,
starting from 0, or −1 if v does not appear in Pi .
T .update(k, val) Given a search tree T , for the leaf w with key(w) = k, update
value(w) = val.
T .RMaxQ(l,r) Given a search tree T , return maxw : l ≤key(w)≤r value(w) (Range
Maximum Query).
2 THE MPC ALGORITHM
In this section, we assume basic familiarity with network flow concepts; see Reference [2] for
further details. In the minimum flow problem, we are given a directed graph G = (V, E) with a
single source and a single sink, with a demand d : E → Z for every edge. The task is to find a flow
of minimum value (the value is the sum of the flow on the edges exiting the source) that satisfies
all demands (such a flow is called feasible). The standard reduction from the minimum path cover
problem to a minimum flow one (see, e.g., Reference [35]) creates a new DAGG∗ by replacing each
nodev with two nodesv−,v+, adds the edge (v−,v+) and adds all in-neighbors ofv as in-neighbors
of v−, and all out-neighbors of v as out-neighbors of v+. Finally, the reduction adds a global source
with an out-going edge to every node, and a global sink with an in-coming edge from every node.
Edges of type (v−,v+) get demand 1, and all other edges get demand 0. The value of the minimum
flow equals k, the width of G, and any decomposition of it into source-to-sink paths induces a
minimum path cover in G.
Our MPC algorithm is based on the following simple reduction of a minimum flow problem to
a maximum flow one (see, e.g., Reference [2]): (i) find a feasible flow f : E → Z; (ii) transform this
into a minimum feasible flow, by finding a maximum flow f 	 in G in which every e ∈ E now has
capacity f (e) − d(e). The final minimum flow solution is obtained as f (e) − f 	
(e), for every e ∈ E.
We solve step (i) in time O(k|E| log |V |) by finding a path cover in G∗ whose size is larger than k
only by a factor O(log |V |). This is based on the classical greedy set cover algorithm (see, e.g., Reference [50, Chapter 2]): At each step, select a path covering most of the remaining uncovered
nodes. The following lemma shows how to do this, by dynamic programming.
Lemma 2.1. Let G = (V, E) be a DAG, and let k be the width of G. In time O(k|E| log |V |), we can
compute a path cover P1,..., PK of G, such that K = O(k log |V |).
Proof. The algorithm works by choosing, at each step, a path that covers the most uncovered
nodes. For every node v ∈ V , we store m[v] = 1, if v is not covered by any path, and m[v] = 0
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.       
Sparse Dynamic Programming on DAGs with Small Width 29:5
otherwise. We also store u[v] as the largest number of uncovered nodes on a path starting at
v. The values u[·] are computed by dynamic programming, by traversing the nodes in inverse
topological order and setting u[v] = m[v] + maxw ∈N +(v) u[v]. Initially, we have m[v] = 1 for all v.
We then compute u[v] for all v, in time O(|E|). By taking the node v with the maximum u[v], and
tracing back along the optimal path starting at v, we obtain our first path in time O(|E|). We then
update m[v] = 0 for all nodes on this path, and iterate this process until all nodes are covered. This
takes overall time O(K|E|), where K is the number of paths found.
This algorithm analysis is identical to that of the classical greedy approximation algorithm for
the set cover problem [50, Chapter 2], because the universe to be covered is V and each possible
path in G is a possible covering set, which implies that K = O(k log |V |).
This path cover induces a flow of value O(k log |V |). Thus, for step (ii), we need to shrink this
flow into a flow of value k. If we run the Ford-Fulkerson algorithm, then this means there are
O(k log |V |) successive augmenting paths, each of which can be found in time O(|E|). This gives a
time bound for step (ii) of O(k|E| log |V |). Combining Lemma 2.1 with this observation, we obtain
our first result:
Theorem 2.2. Given a DAG G = (V, E) of width k, the MPC problem on G can be solved in time
O(k|E| log |V |).
Our approach for Theorem 2.2 is similar to the one from [15] for finding the minimum number
k of chains to cover a partial order of size n. A chain is a set of pairwise comparable elements. The
algorithm from Reference [15] runs in time O(kn2), and it has the same features as ours: It first
finds a set of O(k logn) chains in the same way as we do (longest chains covering most uncovered
elements) and then in a second step reduces these to k. However, if we were to apply this algorithm
to DAGs, then it would run in timeO(|V ||E| + k|V |
2), which is slower than our algorithm for small
k. This is because it uses the classical reduction given by Fulkerson [18] to a bipartite graph, where
each edge of the graph encodes a pair of elements in the relation. Since DAGs are not transitive
in general, to use this reduction one needs first to compute the transitive closure of the DAG,
in time O(|V ||E|). Finally, we mention that such an approximation-refinement approach has also
been applied to other covering problems on graphs, such as a 2-hop cover [11].
3 THE DYNAMIC PROGRAMMING FRAMEWORK
In this section, we give an overview of our approach and introduce a few technical notions. Our aim
is to single out its key features, so that it is easier to check if other problems can also benefit from
it. However, we do not prove general necessary or sufficient conditions, nor general correctness
results. Correctness will be proved for each application of this framework.
Suppose we have a problem involving DAGs that is solvable, for example by dynamic programming, by traversing the nodes in topological order. Thus, assume also that a partial solution at
each node v is obtainable from all (and only) nodes R−(v) of the DAG that can reach v, plus some
other independent objects, such as another sequence. Furthermore, suppose that at each node v
we need to query (and maintain) a data structure T that depends on R−(v), so that the query result
TR−(v).Query(·) of the data structure at nodev is obtainable from the query results TR−(vi ).Query(·)
at previous nodes vi , where {v} ∪
i R−(vi ) = R−(v) and the sets R−(vi ) are not necessarily disjoint. We express this with the following formula:
TR−(v).Query(·) =

i
TR−(vi ).Query(·). (1)
In Equation (1), we let  be some operation on the query results, such as min or max. See
Figure 1 for a conceptual illustration.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.   
29:6 V. Mäkinen et al.
Fig. 1. A conceptual representation of the decomposition of the set R−(v) of nodes that reach v into three
sets R−(v1), R−(v2), R−(v3).
Fig. 2. A path cover P1, P2, P3 of a DAG. We mark in gray the set R−(v) of nodes that reach v.
Fig. 3. The same DAG as in Figure 2. For each path Pi , we marked the node vi = last2reach[v,i],
namely, the last node on path Pi , different from v, that reaches v. We show in gray the three sets
R−(v1), R−(v2), R−(v3) into which R−(v) is decomposed. We draw with dashed lines the forward links from
each vi to v. Note that the nodes vi also have other outgoing forward links; see Example 4.3 for all forward
links from v3.
To obtain such sets R−(vi ), our key idea is to decompose the graph into a path cover P1,..., PK ,
and perform the computation only along these paths. We will employ K data structures T1,..., TK ,
such that, after processing node v on path Pi , data structure Ti stores the correct result for R−(v).
See Figures 2 and 3 for an example.
Our second idea concerns the order in which the nodes on these K paths are processed. Because
the answer at v depends on R−(v), we cannot process the nodes on the K paths (and update the
corresponding Ti ’s) in an arbitrary order. As such, for every path i and every node v, we distinguish the last node on path i different from v that reaches v (if it exists). We will call this node
last2reach[v,i]. See Figure 3 for an example. We note that this insight is the same as in Reference [24], which symmetrically identified the first node on a chain i that can be reached from v
(a chain is a subsequence of a path). The following observation is the first element for using the
decomposition Equation (1).
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.
Sparse Dynamic Programming on DAGs with Small Width 29:7
Observation 1. Let P1,..., PK be a path cover of a DAG G = (V, E), and let v ∈ V . For every
i ∈ [1..K], if last2reach[v,i] exists, let vi = last2reach[v,i] and let Ri = R−(vi ). Otherwise, if
last2reach[v,i] does not exist, let Ri = ∅. Then it holds that R−(v) = {v} ∪ K
i=1 Ri .
Proof. It is clear that {v} ∪ K
i=1 Ri ⊆ R−(v). To show the reverse inclusion, consider a node
u ∈ R−(v). Since P1,..., PK is a path cover, then u appears on some Pi . Since u reaches v, then
u = v, or u appears on Pi before last2reach[v,i], or u = last2reach[v,i]. Therefore u = v, or u
appears in some Ri , as desired.
This allows us to identify, for every node u, a set of forward propagation links forward[u],
where (v,i) ∈ forward[u] holds for any node v and index i with last2reach[v,i] = u (see
Figure 3 and also Example 4.3 for some illustrations of this concept). Observe also that every node
can have at most K incoming forward links, for each of the K paths of the path cover, and thus
|

u ∈V forward[u]| = O(K|V |).
These propagation links are the second element in the decomposition. Once we have computed
the correct value at u, we update the corresponding data structures Ti for all paths i to which u
belongs. We also propagate the query value of Ti in the decomposition (1) for all nodes v with
(v,i) ∈ forward[u]. This means that when we come to process v, we have already correctly computed all terms in the decomposition (1) and it suffices to apply the operation  to these terms.
The next lemma shows how to compute the values last2reach (and, as a consequence, all
forward propagation links), also by dynamic programming.
Lemma 3.1. Let G = (V, E) be a DAG, and let P1,..., PK be a path cover of G. For every v ∈ V and
every i ∈ [1..K], we can compute last2reach[v,i] in overall time O(K|E|).
Proof. For each Pi and every node v on Pi , let index[v,i] be the position of v in Pi , starting
from 0. Our algorithm actually computes last2reach[v,i] as the index of this node in Pi . Initially,
we set last2reach[v,i] = −1 for all v and i. At the end of the algorithm, last2reach[v,i] = −1
will hold precisely for those nodes v that cannot be reached by any node of Pi (different than v
itself).
We traverse the nodes in topological order. For each node v and every i ∈ [1..K], we do as
follows. If v is on Pi , then we set last2reach[v,i] = index[v,i]. Otherwise, we set
last2reach[v,i] = max
u ∈N −(v)
last2reach[u,i].
Observe that after computing the values last2reach[v,i] for each node v and every i ∈ [1..K],
we incorrectly have that for every path Pi on which v appears, we have last2reach[v,i] =
index[v]. As such, we must set last2reach[v,i] to the index of the previous node of v, namely
last2reach[v,i] = index[v] − 1 (so that last2reach[v,i] also gets set to −1 if v is the first node
on path Pi ).
An immediate application of Theorem 2.2 and of the values last2reach[v,i] is for solving
reachability queries (see Section 4.1). Another simple application is an extension of the longest
increasing subsequence (LIS) problem to labeled DAGs (see Section 4.2).
The LIS problem, the longest common subsequence (LCS) problem of Section 5, co-linear chaining
(CLC) problem of Section 6, as well as anchored global alignment under affine gaps costs of Section 7
make use of the following standard data structure (see, e.g., Reference [28, p. 20]).
Lemma 3.2. The following two operations can be supported with a balanced binary search tree T
in time O(logn), where n is the number of leaves in the tree.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.     
29:8 V. Mäkinen et al.
• update(k, val): For the leaf w with key(w) = k, update value(w) = val.
• RMaxQ(l,r): Return maxw : l ≤key(w)≤r value(w) (Range Maximum Query).
Moreover, the balanced binary search tree can be constructed in O(n) time, given the n pairs (key,
value) sorted by component key.
In some of the solutions, we can exploit a faster data structure summarized below.
Lemma 3.3. Consider the operations of Lemma 3.2 for keys in range [0..n] so that the query range is
semi-infinite (0,r) and updates are only allowed to increase the value. Then there is a data structure T
that can be constructed in O(n log logn) time and that supports these restricted versions of operations
in amortized O(log logn) time.
Proof. Recall that a van Emde Boas tree [48, 49] supports maintaining a subset S of [1..n] under
insertions (S = S ∪ {i}) and deletions (S = S \ {i}). It also supports operations predecessor(i) and
successor(i), that give the largest element of S smaller than i, and smallest element larger than i,
respectively. These operations take O(log logn) time. There is a simple reduction to support semiinfinite range maximum queries [19]: We store the values in an auxiliary array V [0..n] (with all
values initialized to −∞) and keep an invariant that S contains only the keys k such that V [k]
is a left-most answer to some semi-infinite range maximum query. This means that values V [k],
k ∈ S, form a strictly increasing series in V [1..n]. That is, query RMaxQ(0,r) can be answered
by V [predecessor(r + 1)] (or by −∞ if no such predecessor exists). To maintain this invariant, we
initially set S = {0} and update it on each operation update(k, val) as follows. If val ≤ V [k] or
val ≤ V [predecessor(k)], then we do nothing. Otherwise, we insert i to S (if it is not yet there),
setV [k] = val, and delete all elements j of S succeeding k withV [j] ≤ V [k]. These deletions (each
preceded by a successor-operation) can be amortized to the update-operations as each such operation causes at most one insertion.
Throughout the article, we assume that the data structure of Lemmas 3.2 is initialized with
(key, value) pairs with each value = −∞. (The structure of Lemma 3.3 gets implicitly initialized
accordingly.)
4 TWO SIMPLE APPLICATIONS
4.1 Reachability Queries
Recall Theorem 2.2 and the values last2reach[v,i]. If we have all these O(k|V |) values, then
we can answer in constant time whether a node y is reachable from a node x (with x  y), as in
Reference [24]: We check
index[x,i] ≤ index[last2reach[y,i],i],
where index[v,i] is defined as the position of v in Pi , starting from 0 (defined in the proof of
Lemma 3.1), Pi is any path containing x, and we take by convention index[−1,i] = −1. Recall also
that reachability queries in an arbitrary graph can be reduced to solving reachability queries in
its DAG of strongly connected components, because nodes in the same component are pairwise
reachable. See Table 2 for existing tradeoffs for solving reachability queries.1
Corollary 4.1. Let G = (V, E) be an arbitrary directed graph and let the width of its DAG of
strongly connected components be k. In time O(k|E| log |V |), we can construct from G an index of size
O(k|V |), so that for any x,y ∈ V , we can answer in O(1) time whether y is reachable from x.
1Note that Reference [8] incorrectly attributes to Reference [24] query time O (log k), and as a consequence References
[25, 45] incorrectly mention query time O (log k) for Reference [8].
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.  
Sparse Dynamic Programming on DAGs with Small Width 29:9
Table 2. Previous Comparable Space/Time Tradeoffs for Reachability Queries
Construction time Index size Query time Reference
O(k|E| log |V |) O(k|V |) O(1) this article
O(|V |
2 + k
√
k|V |) O(k|V |) O(1) [8]
O(k|E|) O(k|V |) O(log2 k) [25]
O(k(|V | + |E|)) O(k|V |) O(k) or O(|V | + |E|) [54]
Compiled from Reference [45, Table 1].
4.2 The LIS Problem
The longest increasing subsequence (LIS) problem asks us to delete the minimum number of values
from an input sequence s1 ···sn such that the remaining values form a strictly increasing series of
values. Here the input sequence is assumed to come from an ordered alphabet Σ. For example, on
input sequence 1, 4, 2, 3, 7, 5, 6, from the alphabet Σ = {1, 2, 3, 4, 5, 6, 7}, the unique optimal solution
is 1, 2, 3, 5, 6. Such a longest increasing subsequence can be found in O(n logn) time [17]. This is
optimal in the comparison model of computation, but can be improved to O(n log log L) [12] on an
integer alphabet and the RAM model of computation, where L is the length of the solution.
Let us consider an O(n logn) solution that can be easily modified to O(n log logn) time on an
integer alphabet. We first map Σ to a subset of {1, 2,...,n} with an order-preserving mapping, in
O(n logn) time (by, e.g., sorting the sequence elements, and relabeling by the ranks in the order
of distinct values). We then store, at every index i of the input sequence, the value LLIS[i] defined
as the length of the longest strictly increasing subsequence ending at i and using the i-th symbol.
The values LLIS[i] can be computed by dynamic programming, by storing all previous key-value
pairs (sj, LLIS[j]) in the data structure T of Lemma 3.3, and querying T .RMaxQ(0,si − 1). Notice
that, by definition, we have LLIS[j
	
] ≤ LLIS[j] for all j
	 < j such thatsj	 = sj . Hence, updates on T
never try to decrease the value and condition of Lemma 3.3 holds. In fact, this feature is common
to all the subsequent algorithms, and we will omit mentioning this in the sequel.
Consider the following extension of the LIS problem to a labeled DAG G = (V, E, , Σ) of width
k. Among all paths P in G, and among all subsequences of the label (P) of P, we need to find a
longest strictly increasing subsequence.
We now explain how to extend the previous dynamic programming algorithm for this problem. We analogously map Σ to a subset of {1, 2,..., |V |} with an order-preserving mapping in
O(|V | log |V |) time, as above. Recall that we assume V = {1,..., |V |}, where 1,..., |V | is a topological order. Assume also that we have K paths to cover V and forward[u] is computed for all
u ∈ V .
For each node v, we aim to analogously compute LLIS[v] as the length of a longest strictly
increasing subsequence of the labels of all paths ending at v, with the property that (v) is the last
element of this subsequence.
For each i ∈ [1..K], we let Ti be the data structure of Lemma 3.3. We start with setting
Ti .update(0, 0) to indicate that any position can start a new increasing subsequence. For each
v ∈ V , we also initialize LLIS[v] = 0. The algorithm proceeds in the fixed topological ordering. Assume now that we are at some position u, and have already updated all search trees
associated with the covering paths going through u. For every (v,i) ∈ forward[u], we update
LLIS[v] = max(LLIS[v], Ti .RMaxQ(0, (v) − 1) + 1). Once the algorithm reaches v in the topological ordering, value LLIS[v] has been updated from all u	 such that (v,i) ∈ forward[u	
]. It remains to show how to update each Ti when reaching v, for all covering pathsi on which v occurs.
This is done as Ti .update((v), LLIS[v]). See Example 4.3 and Figure 4 for examples.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.     
29:10 V. Mäkinen et al.
Fig. 4. Example of the LIS problem on a DAG where each node is labeled by an integer. One seeks a longest
increasing sequence of a path label, among all paths of the DAG.
The final answer to the problem is maxv ∈V LLIS[v], with the actual LIS to be found with a standard traceback. The algorithm runs in O(|V | log |V | + K|V | log log |V |) time. With Theorem 2.2
plugged in, we have K = k and the total running time becomes O(k|E| log |V | + |V | log |V | +
k|V | log log |V |) = O(k|E| log |V |), under our assumption |E|≥|V | − 1. The following theorem
summarizes this result.
Theorem 4.2. Let G = (V, E, , Σ) be a labeled DAG of width k, where Σ is an ordered alphabet.
We can find a longest increasing subsequence in G in time O(k|E| log |V |).
When the DAG is just a labeled path with |E| = |V | − 1 (modeling the standard LIS problem),
then the algorithm from Lemma 2.1 returns one path (K = 1). The complexity is thenO(|V | log |V |),
matching the best possible bound for the standard LIS problem [17]. For integer alphabets Σ =
{1, 2,...,n} the standard LIS problem can be solved in O(n log log L) time [12], for L ≤ n being
the length of the solution. In this setting our algorithm comes close with its O(n log logn) bound
when restricted to a single path.
Example 4.3. We detail the steps needed for solving a LIS instance on the DAG from Figures 2
and 3, with node labels as shown in Figure 4 above. As an example, observe that the forward links
exitingv3, namely the set forward[v3], is the set {(a, 3), (b, 3), (v2, 3), (c, 3), (v, 3)}. We do not draw
these links, but we draw instead the forward links incoming to v.
Assume now that the topological order first contains the nodes of P3 until v3, followed by the
nodes of P2 until v2, and then by the nodes of P1 until v1.
• When we reach v3 on P3, we have computed LLIS[v3] = 5 and have performed
T3.update(7, 5). At this point, T3 contains (0, 0), (1, −∞), (2, −∞), (3, 1), (4, 2), (5, 3), (6, 4),
(7, 5),..., (10, −∞). We then perform updates along the forward links exiting v3, including the forward link from v3 to v. As such, we update LLIS[v] = max(LLIS[v],
T3.RMaxQ(0, 7) + 1) = max(0, 6) = 6.
• When we reach v2 on P2, we have computed LLIS[v2] = 4 and have performed
T2.update(6, 4). At this point, T2 contains (0, 0), (1, −∞), (2, −∞), (3, 1), (4, −∞), (5, 3), (6, 4),
..., (10, −∞). (Keep in mind that there is also a forward link from v3 to v2, not drawn.) We
then perform updates along the forward links exiting v2, including the forward link fromv2
tov. As such, we update LLIS[v] = max(LLIS[v], T2.RMaxQ(0, 7) + 1) = max(6, 4 + 1) = 6.
• When we reach v1 on P1, we have computed LLIS[v1] = 3 and have performed
T1.update(9, 3). At this point, T1 contains (0, 0), (1, −∞),..., (5, 2), (6, −∞), (7, 1),
(8, −∞), (9, 3), (10, −∞). We then perform updates along the forward links exiting v1,
including the forward link from v1 to v. As such, we update LLIS[v] = max(LLIS[v],
T1.RMaxQ(0, 7) + 1) = max(6, 3 + 1) = 6.
• When we finally reachv, all forward links tov have been processed, and the value LLIS[v] =
6 is final and correct.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019. 
Sparse Dynamic Programming on DAGs with Small Width 29:11
5 THE LCS PROBLEM
Consider a labeled DAG G = (V, E, , Σ) and a sequence S ∈ Σ∗, where Σ is an ordered alphabet.
We say that the longest common subsequence (LCS) between G and S is a longest subsequence C of
any path label in G such that C is also a subsequence of S.
We will modify the LIS algorithm of Section 4.2 minimally to find an LCS between a DAGG and
a sequence S.
For any c ∈ Σ, let S (c) denote set {j | S[j] = c}. For each node v and each j ∈ S ((v)), we aim to
store in LLCS[v, j] the length of the longest common subsequence between S[1..j] and any label
of a path ending at v, among all subsequences having (v) = S[j] as the last symbol.
Assume we have a path cover of size K and forward[u] computed for all u ∈ V . Assume also
we have mapped Σ to {0, 1, 2,..., |S | + 1} in O((|V | + |S |) log |S |) time (e.g., by sorting the symbols
of S, binary searching labels of V , and then relabeling by ranks, with the exception that, if a node
label does not appear in S, it is replaced by |S | + 1).
Let Ti be a data structure of Lemma 3.3. We start by setting Ti .update(0, 0), for each i ∈ [1..K],
so that any (v, j) with (v) = S[j] can start a new common subsequence. We also initialize
LLCS[v, j] = 0 for all (v, j). The algorithm proceeds in fixed topological ordering on G. At a node
u, for every (v,i) ∈ forward[u], we now update an array LLCS[v, j] for all j ∈ S ((v)) as follows:
LLCS[v, j] = max(LLCS[v, j], Ti .RMaxQ(0, j − 1) + 1). The update step of Ti when the algorithm
reaches a node v, for each covering path i containing v, is done as Ti .update(j
	
, LLCS[v, j
	
]) for all
j
	 such that j
	 ∈ S ((v)).
The final answer to the problem is maxv ∈V,j ∈S ((v)) LLCS[v, j], with the actual LCS to be found
with a standard traceback. The algorithm runs in O((|V | + |S |) log |S | + K|M| log log |S |) time,
where M = {(v, j) | v ∈ V, j ∈ [1..|S |], (v) = S[j]}, and assuming a cover of K paths is given. Notice that |M| can be Ω(|V ||S |). With Theorem 2.2 plugged in, the total running time becomes
O(k|E| log |V | + (|V | + |S |) log |S | + k|M| log log |S |).
Theorem 5.1. Let G = (V, E, , Σ) be a labeled DAG of width k, and let S ∈ Σ∗, where Σ is an ordered alphabet. We can find a longest common subsequence between G and S in time O(k|E| log |V | +
(|V | + |S |) log |S | + k|M| log log |S |), where M is the set of characters in the sequences that match,
namely M = {(v, j) | v ∈ V, j ∈ [1..|S |], (v) = S[j]}.
When G is a path, the bound improves to O((|V | + |S |) log |S | + |M| log log |S |), which nearly
matches the fastest sparse dynamic programming algorithm for the LCS on two sequences [13].
On two sequences of length n and m, respectively, the algorithm of Reference [13] runs in time
O(n log(min(m, |Σ|) + d log log(min(d,nm/d))), where d ≤ |M| is the number of so-called dominant matches, which are a subset of the |M| total pairs of characters in the two sequences that
match. Notice that our algorithm has a difference in the log log-factor due to a different data structure, which does not work for this order of computation.
6 CO-LINEAR CHAINING
We start with a formal definition of the co-linear chaining problem (see Figure 5 for an illustration),
following the notions introduced in Reference [28, Section 15.4].
Problem 1 (Co-linear Chaining (CLC)). Let T and R be two sequences over an alphabet Σ, and
let M be a set of N pairs ([x..y],[c..d]). Find an ordered subset S = s1s2 ···sp of pairs from M such
that
• sj−1.y < sj .y and sj−1.d < sj .d, for all 1 ≤ j ≤ p, and
• S maximizes the ordered coverage of R, defined as
coverage(R, S) = |{i ∈ [1..|R|] | i ∈ [sj .c..sj .d] for some 1 ≤ j ≤ p}|.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.          
29:12 V. Mäkinen et al.
Fig. 5. In the co-linear chaining problem between two sequences T and R, we need to find a subset of pairs
of intervals (i.e., anchors) so that (i) the selected intervals in each sequence appear in increasing order; and
(ii) the selected intervals cover in R the maximum number of positions. The figure shows an input for the
problem, and highlights in gray an optimal subset of anchors. Figure taken from Reference [28].
Fig. 6. An illustration of the CLC problem between a sequence and a DAG. We show three substrings of R
and their matches in G, and highlight with thick lines an optimal subset of pairs.
The definition of ordered coverage between two sequences is symmetric, as we can simply exchange the roles of T and R. But when solving the CLC problem between a DAG and a sequence,
we must choose whether we want to maximize the ordered coverage on the sequence R or on the
DAG G. We will consider the former variant.
First, we define the following precedence relation:
Definition 6.1. Given two paths P1 and P2 in a DAG G, we say that P1 precedes P2, and write
P1 ≺ P2, if one of the following conditions holds:
• P1 and P2 do not share nodes and there is a path inG from the endpoint of P1 to the startpoint
of P2, or
• P1 and P2 have a suffix-prefix overlap and P2 is not fully contained in P1; that is, if P1 =
(a1,..., ai ) and P2 = (b1,...,bj) then there exists a k ∈ {max(1, 2 + i − j),...,i} such that
ak = b1, ak+1 = b2,..., ai = b1+i−k .
We then extend the formulation of Problem 1 to handle a sequence and a DAG (see Figure 6 for
an illustration).
Problem 2 (CLC between a Seqence and a DAG). Let R be a sequence, let G be a labeled
DAG, and let M be a set of N pairs of the form (P,[c..d]), where P is a path in G and c ≤ d are
non-negative integers (with the interpretation that the path label (P) matches the substring R[c..d]).
Find an ordered subset S = s1s2 ···sp of pairs from M such that
• for all 2 ≤ j ≤ p, it holds that sj−1.P ≺ sj .P and sj−1.d < sj .d, and
• S maximizes the ordered coverage of R, analogously defined as coverage(R, S) = |{i ∈
[1..|R|] | i ∈ [sj .c..sj .d] for some 1 ≤ j ≤ p}|.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019. 
Sparse Dynamic Programming on DAGs with Small Width 29:13
To illustrate the main technique of this article, let us for now only seek solutions where paths
in consecutive pairs in a solution do not overlap in the DAG. Suffix-prefix overlaps between paths
turn out to be challenging; we will postpone this case until Section 6.3.
Problem 3 (Overlap-limited CLC between a Seqence and a DAG). Under the same hypotheses as in Problem 2, find an ordered subset S = s1s2 ···sp of pairs from M such that
• for all 2 ≤ j ≤ p, it holds that there is a non-empty path from the last node of sj−1.P to the first
node of sj .P and sj−1.d < sj .d, and
• S maximizes coverage(R, S).
First, let us consider a trivial approach to solve Problem 3. Assume we have ordered in
O(|E| + N) time the N input pairs as M[1], M[2],..., M[N], so that the endpoints of M[1].P,
M[2].P,..., M[N].P are in topological order, breaking ties arbitrarily. We denote by C[j] the
maximum ordered coverage of R[1..M[j].d] using the pair M[j] and any subset of pairs from
{M[1], M[2],..., M[j − 1]}.
Theorem 6.2. Overlap-limited co-linear chaining between a sequence and a labeled DAG G =
(V, E, , Σ) (Problem 3) on N input pairs can be solved in O((|V | + |E|)N) time.
Proof. First, we reverse the edges of G. Then, we mark the nodes that correspond to the path
endpoints for every pair. After this preprocessing, we can start computing the maximum ordered
coverage for the pairs as follows: for every pair M[j] in topological order of their path endpoints for
j ∈ {1,..., N}, we do a depth-first traversal starting at the startpoint of path M[j].P. Note that since
the edges are reversed, the depth-first traversal checks only pairs whose paths are predecessors of
M[j].P.
Whenever we encounter a node that corresponds to the path endpoint of a pair M[j
	
], we first
examine whether it fulfills the criterion M[j
	
].d < M[j].c (call this case (a)). The best ordered coverage using pair M[j] after all such M[j
	
] is then
Ca
[j] = max j	 : M[j	].d<M[j].c
{C[j
	
] + (M[j].d − M[j].c + 1)}, (2)
where C[j]	 is the best ordered coverage when using pairs M[j
	
] last.
If pair M[j
	
] does not fulfill the criterion for case (a), then we check whether M[j].c ≤ M[j
	
].d ≤
M[j].d (call this case (b)). The best ordered coverage using pair M[j] after all such M[j
	
] with
M[j
	
].c < M[j].c is then
Cb
[j] = max j	 : M[j].c ≤M[j	].d ≤M[j].d
{C[j
	
] + (M[j].d − M[j
	
].d)}. (3)
Inclusions, i.e., M[j].c ≤ M[j
	
].c, can be left computed incorrectly in Cb[j], since there is a better
or equally good solution computed in Ca[j] or Cb[j] that does not use them [1].
Finally, we take C[j] = max(Ca[j],Cb[j]). Depth-first traversal takes O(|V | + |E|) time and is
executed N times, for O((|V | + |E|)N) total time.
However, we can do significantly better than O((|V | + |E|)N) time. In the next sections, we will
describe how to apply the framework from Section 3 here.
6.1 Co-linear Chaining on Sequences Revisited
We now describe the dynamic programming algorithm from Reference [1] for the case of two
sequences, as we will then reuse this same algorithm in our MPC approach.
First, sort input pairs in M by the coordinate y into the sequence M[1], M[2],..., M[N], so that
M[i].y ≤ M[j].y holds for all i < j. This will ensure that we consider the overlapping ranges in
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.  
29:14 V. Mäkinen et al.
sequence T in the correct order. Then, we fill a table C[1..N] analogous to that of Theorem 6.2
so that C[j] gives the maximum ordered coverage of R[1..M[j].d] using the pair M[j] and any
subset of pairs from {M[1], M[2],..., M[j − 1]}. Hence, maxj C[j] gives the total maximum ordered
coverage of R.
Consider Equations (2) and (3). Now, we can use an invariant technique to convert these recurrence relations, so that we can exploit the range maximum queries of Lemma 3.2:
Ca[j] = (M[j].d − M[j].c + 1) + max j	 : M[j	].d<M[j].c
C[j
	
]
= (M[j].d − M[j].c + 1) + T .RMaxQ(0, M[j].c − 1),
Cb[j] = M[j].d + max j	 : M[j].c ≤M[j	].d ≤M[j].d
{C[j
	
] − M[j
	
].d}
= M[j].d + I.RMaxQ(M[j].c, M[j].d),
C[j] = max(Ca
[j],Cb[j]).
For these to work correctly, we need to have properly updated the trees T and I for all j
	 ∈
[1..j − 1]. That is, we need to call T .update(M[j
	
].d,C[j
	
]) and I.update(M[j
	
].d,C[j
	
] − M[j
	
].d)
after computing each C[j
	
]. The running time is O(N log N).
Figure 5 illustrates the optimal chain on our schematic example. This chain can be extracted by
modifying the algorithm to store traceback pointers.
Theorem 6.3 ([1, 42]). Problem 1 on N input pairs can be solved in the optimal O(N log N) time.
6.2 Co-linear Chaining on DAGs Using a Minimum Path Cover
Let us now modify the above algorithm to work with DAGs, using the main technique of this
article.
Theorem 6.4. Problem 3 on a labeled DAG G = (V, E, , Σ) of width k and a set of N input pairs
can be solved in time O(k|E| log |V | + kN log N) time.
Proof. Assume we have a path cover of size K and forward[u] computed for all u ∈ V . For each
path i ∈ [1..K], we create two binary search trees Ti and Ii . As a reminder, these trees correspond
to coverages for pairs that do not, and do overlap, respectively, on the sequence. Moreover, recall
that in Problem 3, we do not consider solutions where consecutive paths in the graph overlap.
As keys, we use M[j].d, for every pair M[j], and additionally the key 0. Recall that the value of
every key is initialized to −∞.
After these preprocessing steps, we process the nodes in topological order, as detailed in
Algorithm 1. If node v corresponds to the endpoint of some M[j].P, then we update the trees
Ti and Ii for all covering pathsi containing node v. Then, we follow all forward propagation links
(w,i) ∈ forward[v] and updateC[j] for each path M[j].P starting atw, taking into account all pairs
whose path endpoints are in covering path i. Before the main loop visits w, we have processed all
forward propagation links to w, and the computation of C[j] has taken all previous pairs into account, as in the naive algorithm, but now indirectly through the K search trees. Exceptions are the
pairs overlapping in the graph, which we omit in this problem statement. The forward propagation ensures that the search tree query results are indeed taking only reachable pairs into account.
WhileC[j] is already computed when visitingw, the startpoint of M[j].P, the added coverage with
the pair is updated to the search trees only when visiting the endpoint.
There are NK forward propagation links, and both search trees are queried in O(log N) time.
All the search trees containing a path endpoint of a pair are updated. Each endpoint can be contained in at most K paths, so this also gives the same bound 2NK on the number of updates.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019. 
Sparse Dynamic Programming on DAGs with Small Width 29:15
With Theorem 2.2 plugged in, we have K = k and the total running time becomes O(k|E| log |V | +
kN log N).
ALGORITHM 1: Co-linear chaining between a sequence and a DAG using a path cover.
Input: DAG G = (V, E), a path cover P1, P2,..., PK of G, and N pairs M[1], M[2],..., M[N] of the form
(P,[c..d]).
Output: The index j giving maxj C[j].
Use Lemma 3.1 to find all forward propagation links;
for i ← 1 to K do
Initialize search trees Ti and Ii with keys M[j].d, 1 ≤ j ≤ N, and with key 0, all keys associated with
values −∞;
Ti .update(0, 0);
Ii .update(0, 0);
/* Save to start[i] (respectively, end[i]) the indexes of all pairs whose path starts
(respectively, ends) at i. */
for j ← 1 to N do
start[M[j].P.f irst].push(j);
end[M[j].P.last].push(j);
C[j] ← 0;
for v ∈ V in topological order do
for j ∈ end[v] do
/* Update the search trees for every path that covers v, stored in paths[v]. */
for i ∈ paths[v] do
Ti .update(M[j].d,C[j]);
Ii .update(M[j].d,C[j] − M[j].d);
for (w,i) ∈ forward[v] do
for j ∈ start[w] do
Ca[j] ← (M[j].d − M[j].c + 1) + Ti .RMaxQ(0, M[j].c − 1);
Cb[j] ← M[j].d + Ii .RMaxQ(M[j].c, M[j].d);
C[j] ← max(C[j],Ca[j],Cb[j]);
return argmaxjC[j];
6.3 Co-linear Chaining with Path Overlaps
We now consider how to extend the algorithms we developed for Problem 3 to work for the more
general case of Problem 2, where overlaps between paths are allowed in a solution. The detection
and merging of such path overlaps has been studied in Reference [40], and we tailor a similar
approach for our purposes.
We use an FM-index [16] tailored for large alphabets [22], and a two-dimensional range
search tree [7] modified to support range maximum queries. The former is used for obtaining all ranges [i	
..i] in the coverage array C such that all input pairs M[i	
],..., M[i] have a
path M[i		].P, i	 ≤ i		 ≤ i, overlapping with the path M[j].P of j-th input pair M[j]. Here, the
endpoint of the j-th input pair is at node v visited in topological order. This implies that the
paths of the input pairs [i	
..i] have already been visited, and thus, by induction, that C[i	
..i]
values have been correctly computed (subject to the modification we are about to study). The
sequence ranges [M[i		].c..M[i		].d] for all i		 ∈ [i	
..i] may be arbitrarily located with respect to
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019. 
29:16 V. Mäkinen et al.
Fig. 7. Answering two-dimensional range maximum queries. A search interval [i	
..i] is split into O(log(i −
i	 + 1)) intervals/subtrees. In each subtree root, we store a type T and a type I search tree of Lemma 3.2
containing values C[i		] or C[i		] − M[i		].d, respectively, for indexes i		 in the corresponding subinterval.
Figure modified from one in Reference [28].
interval [M[j].c..M[j].d], so we need to maintain an analogous mechanism with search trees of
type T and I as in our co-linear chaining algorithm based on a path cover. This time we cannot,
in advance, separate the input pairs to K paths with different search trees, but we have a dynamic
setting, with interval [i	
..i] deciding which values should be taken into account. This is where a
two-dimensional range search tree is used to support these queries in O(log2 N) time: Figure 7
illustrates this.
In what follows, we show that O(L) queries are sufficient to take all overlaps into account
throughout the algorithm execution (holding for both the trivial algorithm and for the one based
on a path cover), where L = 
i |M[i].P |—the sum of the path lengths—is at most the total input
length. The construction will actually induce an order for the input pairs such that O(L) queries
are sufficient: Since the other parts of the algorithms do not use the order of input pairs directly,
we can safely reorganize the input accordingly.
With this introduction, we are ready to consider how all the intervals [i	
..i] related to jth pair
are obtained. We build in O(L log log |V |) time the FM-index version proposed in Reference [22] of
sequences T = (
i #(M[i].P)
−1)#, where # is a symbol not in alphabet {1, 2,..., |V |} and considered smaller than other symbols, e.g., # = 0, and X −1 denotes the reverse X[|X |]X[|X − 1]]···X[1]
of X.
For our purposes it is sufficient to know that the FM-index of T , when given an interval I (X)
corresponding to lexicographically-ordered suffixes that start with X, can determine the interval
I (cX) in O(log log |V |) time [22]. This operation is called backward step.
We use the index to search M[j].P in the forward direction by searching its reverse with
backward steps. Consider we have found interval I ((M[j].P[1..k])
−1), for some k, such that
backward step I (#(M[j].P[1..k])
−1) results in a non-empty interval [i	
..i]. This interval [i	
..i]
corresponds to all suffixes of T that have #(M[j].P[1..k])
−1 as a prefix. That is, [i	
..i] corresponds
to input pairs whose path suffix has a length k overlap with the path prefix of jth input pair.
For this interval to match with coverage array C, we just need to rearrange the input pairs
according to their order in the first N rows of the array storing the lexicographic order of suffixes
of T .
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.
Sparse Dynamic Programming on DAGs with Small Width 29:17
Since each backward step on the index may induce a range search on exactly one interval [i	
..i],
the running time is dominated by the range queries.2 However, this also gives the bound L on the
number of range queries, as claimed earlier.
Alternatively, one can omit the expensive range queries and process each overlapping pair
separately, to compute in constant time its contribution to C[j]. This gives another bound
O(L log log |V | + #overlaps), where #overlaps is the number of overlaps between the input
paths. This can be improved to O(L + #overlaps) by using a generalized suffix tree to compute
the overlaps in advance [40, proof of Theorem 2]. We obtain the following result:
Theorem 6.5. Let G = (V, E, , Σ) be a labeled DAG and let M be a set of N pairs of the form
(P,[c..d]). The algorithms from Theorems 6.2 and 6.4 can be modified to solve Problem 2 with additional time O(L log2 |V |) or O(L + #overlaps), where L is at most the input length and #overlaps
is the number of overlaps between the input paths.
7 GENERAL ALIGNMENT-SPECIFIC FRAMEWORK
We will now express the framework of Section 3 in the light of alignments, removing special
features of CLC, but adding different gap costs to the model, as in References [13, 14].
We assume the reader is familiar with the concept of alignments (see, e.g., Reference [28, Section 6]). Consider an input sequence R[1..m], a labeled input DAG G = (V = [1..n], E, , Σ), and
a sparse set A ⊆ [1..m] × [1..n] of alignment anchors. We wish to compute the global alignment
score S (m + 1,n + 1) of the best alignment of R with a source-to-sink path of G among alignments
whose substitutions are a subset of A. We call such alignment anchored. With proper initialization,
one can compute the score using recurrence
S (j,v) = s(j,v) + max j	<j,u ∈R−(v),(j	
,u)∈A
{S (j
	
,u) + w(j
	
, j,u,v)}, (4)
where s(j,v) is the substitution score of aligning R[j] with (v), and w is the gap cost function
associated to P[j
	 + 1..j − 1] and to the shortest path from u to v.
Our general framework applies at least when the gap cost is defined as a linear function of
the gap length in R, that is, under the affine gap cost model. For α and β constants, consider the
following cost function:
w(j
	
, j,u,v) =
⎧⎪
⎨
⎪
⎩
0, for j
	 = j − 1, and
−α − β (j − j
	 − 1), for j
	 < j − 1.
Then, one can write Equation (4) as
S (j,v) = max
⎧⎪⎪⎪
⎨
⎪⎪⎪
⎩
s(j,v)+ max u ∈R−(v),(j−1,u)∈A
{S (j − 1,u)},
s(j,v) − α − βj+ max j	<j,u ∈R−(v),(j	
,u)∈A
{S (j
	
,u) + βj	
}, (5)
where the first part handles the case of two consecutive substitutions in the sequence and the
second part the case of a gap between two substitution in the sequence. Here, we used the same
invariant trick as in co-linear chaining.
The rest of the details are analogous to the LIS, LCS, and CLC algorithms, except that the first
part of Equation (5) needs a different approach. Assume we have a path cover of G of size K
and forward[u] computed for all u ∈ V . Let Ti be a data structure of Lemma 3.3 initialized with
2A simple wavelet tree-based FM-index would provide the same bound, but in case the range search part is later improved,
we used the best bound for the subroutine.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.   
29:18 V. Mäkinen et al.
Ti .update(0, 0), for each i ∈ [1..K]. These will be used for handling the second part of Equation (5).
Let Mi[1..m] be an array initialized with values −∞, for each i ∈ [1..K]. These will be used for
handling the first part of Equation (5). The algorithm proceeds in a fixed topological ordering on
G. At a node u, for every (v,i) ∈ forward[u], we now update S (j,v) for all j such that (j,v) ∈ A
as follows:
S (j,v) = max
⎧⎪⎪⎪
⎨
⎪⎪⎪
⎩
S (j,v),
s(j,v) + Mi[j − 1],
s(j,v) − α − βj + Ti .RMaxQ(0, j − 1).
The update step of Ti when the algorithm reaches a node v, for each covering path i containing
v, is done as Ti .update(j
	
, S (j
	
,v) + βj	
) for all j
	 such that (j
	
,v) ∈ A. We also update Mi[j
	
] =
max(Mi[j
	
], S (j
	
,v)) for all j
	 such that (j
	
,v) ∈ A. Initialization is handled by the (0, 0) key-value
at each Ti so that an alignment can start with a gap, and with values −∞ in each Mi so that the
first anchored substitution is handled properly.
Combined with the path cover algorithm, this global alignment generalization to the sequenceto-DAG case can thus be solved in the same running time as the LCS problem, with the match set
M replaced now by the given set of anchors A.
Theorem 7.1. Let G = (V, E, , Σ) be a labeled DAG of width k, let R ∈ Σ∗, and A be a set of
alignment anchors. We can compute the maximum anchored global alignment score of the sequence
R and a source-to-sink path in G in time O(k|E| log |V | + (|V | + |R|) log |R| + k|A| log log |R|), under
affine gap costs in R.
8 DISCUSSION
For applying our solutions to Problem 2 in practice, one first needs to find the alignment anchors.
As explained in the problem formulation, alignment anchors are such pairs (P,[c..d]) where P is
a path in G and (P) matches R[c..d]. With sequence inputs, such pairs are usually taken to be
maximal exact matches (MEMs) between the two sequences and can be retrieved in small space
in linear time [5, 6]. It is largely an open problem how to retrieve MEMs between a sequence and
a DAG efficiently: The case of length-limited MEMs is studied in Reference [43], based on an extension of [44] with features such as suffix tree functionality. On the practical side, anchor finding
has already been incorporated into tools for conducting alignment of a sequence to a DAG [20, 27,
34]. We also implemented the co-linear chaining algorithm,3 and we reported some preliminary
experimental results in the conference version of this article [26].
On the theoretical side, it remains open whether the MPC algorithm could benefit from a better initial approximation and/or one that is faster to compute. More generally, it remains open
whether the overall bound O(k|E| log |V |) for the MPC problem can be improved. Also, many of
the non-sparse solutions to pattern matching on graphs work also on general graphs (see, e.g.,
Reference [33]), while our solutions are restricted to DAGs. NP-hardness of path cover on general
graphs is the main bottleneck, but even if a path cover is given, it is not clear how to extend the
sparse dynamic programming framework to handle cycles.
In addition to the case of cycles, our dynamic programming framework could be extended to
various gap cost models [13, 14]. We demonstrated that the framework applies to the case of affine
(linear) gap costs in the input sequence, but it remains open how to handle more complex costs
functions, and gap costs in the graph.
3https://github.com/Anna-Kuosmanen/Seq2DagChainer.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 29. Publication date: February 2019.  
Sparse Dynamic Programming on DAGs with Small Width 29:19
Finally, we have studied a sparse dynamic programming framework for sequence-to-DAG alignment problems, but it would also be natural to consider DAG-to-DAG alignment. Finding a path
PA from DAG A = (V A, EA) and path PB from DAG B = (V B, EB ) that minimizes the edit distance
between (PA) and (PB ) can easily be done in O(|EA||EB |) time [28, Section 6.6.5]. Extending
the sparse dynamic programming framework to this problem area is an interesting direction to
consider.  