Often in a scheduling problem, there is uncertainty about the jobs to be processed. The issue of uncertainty
regarding the machines has been much less studied. In this article, we study a scheduling environment in
which jobs first need to be grouped into some sets before the number of machines is known, and then the sets
need to be scheduled on machines without being separated. To evaluate algorithms in such an environment,
we introduce the idea of an α-robust algorithm, one that is guaranteed to return a schedule on any number
m of machines that is within an α factor of the optimal schedule on m machine, where the optimum is not
subject to the restriction that the sets cannot be separated. Under such environment, we give a ( 5
3 + ϵ )-
robust algorithm for scheduling on parallel machines to minimize makespan and show a lower bound 4
3 . For
the special case when the jobs are infinitesimal, we give a 1.233-robust algorithm with an asymptotic lower
bound of 1.207. We also study a case of fair allocation, where the objective is to minimize the difference
between the maximum and minimum machine load.
CCS Concepts: • Theory of computation → Scheduling algorithms; Adversary models;
Additional Key Words and Phrases: Robust algorithms
1 INTRODUCTION
For many problems, one does not know the entire input accurately and completely in advance.
There are different ways of addressing such uncertainty, e.g., via online algorithms (assuming the
input arrives over time), dynamic algorithms (assuming the input changes over time), stochastic
optimization (assuming the input includes random variables), or robust optimization (assuming
that there is bounded uncertainty in the data). Another way of addressing uncertainty is to require
one solution that is good against all possible values of the uncertain parameters. Examples of work
in this direction include the universal traveling salesman problem (one tour that is good no matter
which subset of points arrive) [14], robust matchings (one matching is chosen and then evaluated
by its top k edges, where k is unknown) [9, 11], a knapsack of unknown capacity (one policy
of packing that is good irrespective of the actual capacity) [6], and two-stage scheduling (some
decisions must be made before the actual scenario is known) [5, 17]. In scheduling problems, there
are many ways to model uncertainty in the jobs, including online algorithms [1, 2], in which the
set of jobs is not known in advance, stochastic scheduling [13], in which the jobs are modeled as
random variables, and work on schedules that are good against multiple objective functions [4, 12,
15]. But there is much less work studying the possibility of uncertainty in the machines, and the
work we are aware of studies uncertainty in speed or reliability (breakdowns) [3, 7].
Motivated by the need to understand how to make scheduling decisions without knowing how
many machines we will have, we consider a different notion of uncertainty—a scenario in which
you don’t know how many machines you are going to have, but you still have to commit (partially) to a schedule by making significant decisions about partitioning the jobs before knowing
the number of machines.
This type of decision arises in a variety of settings. For example, many scheduling problems
are fundamentally about packing items onto machines, and there are many examples of problems
that concern packing items where there are multiple levels of commitment to be made with partial
information. For example, in a warehouse, a large order may need to be placed into multiple boxes,
without knowing exactly how many trucks there will be to ship the items. You therefore want to
be able to pack the items well, given the various possible number of trucks. Another example
involves problems in modern data centers. In data centers, there are some systems that require
you to group work together into “bundles” without knowing exactly how many machines will be
available. For example, in a map-reduce type computation, the mapping function naturally breaks
the data into some number of groups д. However, there are some unknown number of available
machinesm, and you typically have to design your mapping function, choosing a д and associated
grouping, without knowing m. You may know a range of possible values for m, or it may vary
widely depending on the availability of machines at the time you run the map-reduce computation
(and the availability is typically not under your control). As more and more computing moves to
the “cloud,” that is, moves to large shared data centers, we anticipate that this problem of grouping
work without knowing the number of machines will become more widespread.
In this article, we consider the following model. We are given a set of n jobs, J, with a known
processing times p(j) for each job j, and a number M, which is an upper bound on the number of
machines we might have. An algorithm must commit, before knowing how many machines there
are, to grouping the jobs into M baдs, where each job is assigned to exactly one of the bags. We
call this step the packing step. Only after completing the packing step do we learn the number of
machines m. We now need to compute a schedule, with the restriction that we must keep the bags
together, that is, we will assign one or more bags to each machine. We call this step the scheduling
step. As in other robust problems, we want to do well against all possible numbers of machines.
We therefore evaluate our schedule by the ratio of the makespan of our schedule, ALG(m, M), to
the makespan of a schedule that knew m in advance, optm, taking the worst case over all possible
values of m. If an algorithm always provides a ratio of at most α, where α = max1≤m ≤M
ALG(m,M)
optm ,
then we call it α-robust. (We may also consider scenarios in which there are different upper and
lower bounds on the range of m; the definition of robustness extends in the obvious way.)
Our main result is an algorithm, which is ( 5
3 + ϵ )-robust, for minimizing makespan on parallel
machines; and we show a lower bound of 4/3 on the robustness of any algorithm for minimizing
makespan on parallel machines. As with many scheduling problems, there are two different aspects
to address. One is the load-balancing aspect, but in this two-stage problem, it seems that one wants
to create bags of a variety of different sizes, to allow a more balanced final schedule. The second is
to deal with large jobs and to handle cases where one or several large jobs are the dominant term
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.
Scheduling When You Do Not Know the Number of Machines 9:3
in the makespan. Large jobs seem to provide a particular challenge in this problem, and much of
our algorithm and analysis are devoted to handling various cases involving large jobs.
To focus on the load balancing issues, we also consider the “continuous” case where we have a
set of infinitesimal jobs. That is, in the packing stage, we simply need to divide our total load into M
bags. In traditional makespan scheduling, this case is trivial, we would just divide the load into M
equal pieces and achieve an optimal makespan. But in this two stage-problem, even the continuous
case is challenging. We can, however, obtain significantly stronger results than in the discrete case,
showing an upper bound of 1.233 and a lower bound of 1.207 on the robustness. It is worth noting
that in Reference [16], the author considered a similar model of sequential partitioning on an
interval with the objective of keeping the largest subinterval small at all the stages of the partition
process.
The continuous case also models a problem in fair allocation. In fair allocation, you typically
have resources that you want to split “fairly” among several parties. The literature on this problem is vast, and we will not attempt to summarize it here. We will only observe that we are solving a problem in fair allocation that has not previously been studied, to our knowledge. We are
given some objects to share, and everyone agrees on the values, but we do not know how many
people will be sharing them. We place the objects into bags, and have the restriction that each
person must take a subset of the bags. In the makespan variant, we are minimizing the maximum amount that anyone gets. Motivated by fairness, we also consider a version where you want
to minimize the difference between the maximum allocation and the minimum allocation (this
objective makes sense in fair allocation, but not necessarily in scheduling). Here, we consider a
case where we know a lower bound of αM on the eventual number of machines, and can show
a lower bound of min{2/3, 2/(4α + 1)}A on the difference, and we can obtain an upper bound of
min{2/3, 1/(α + 1)}A, where A is the average load.
1.1 Overview of this Chapter and a Lower Bound
We give a brief overview of this chapter, and for intuition, a simple lower bound. The order of our
article is different than the order presented in the Introduction. We present the continuous case
first, because the proofs are simpler and it gives some intuition for the discrete case.
In Section 2, we consider the case when all jobs are infinitesimal. We give an algorithm that
is approximately 1.233 robust. And we also show a lower bound that is approximately 1.207. For
the infinitesimal case, we also consider the objective of minimizing the maximum difference between the most loaded and least loaded machine. For this case, and the average load is A, if we
know that the eventual number of machines is in the range [αM, M], we can show an asymptotic
lower bound of min{2/3, 2/(4α + 1)}A on the difference, and we can obtain an upper bound of
min{2/3, 1/(α + 1)}A.
In Section 3, we consider the general case with arbitrary sized jobs. We give an algorithm that
gives a robust ratio of 5
3 + ϵ, breaking the simple 2 bound that can be obtained by running LPT on
M sets and then repeatedly merging the two smallest sets untilm sets remain. In our algorithm, we
first calculate the optimal schedule form ∈ { M
2 , 3M
4 , M} within a factor of 1 + ϵ using the algorithm
in Reference [10]. We then take one of these schedules and partition the jobs that were scheduled
on some machines into a larger number of sets, which we call bags. After learning how many
machines we have, we place the bags on the machines. This algorithm is more involved than
the previous ones, and has several cases, based on the values of optM , opt3M/4, and optM/2 and
demonstrates how a more careful investigation of the packing and scheduling steps can lead to
improved bounds.
We conclude the Introduction with a simple lower bound. Consider the following example. Let
the upper bound on the number of machines, M, be 3 and assume we have n = 6 identical jobs,
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.
9:4 C. Stein and M. Zhong
Fig. 1. k as a function of M.
each of which has processing time 1. We will prepare 3 bags that will ultimately have to be scheduled on either 1, 2, or 3 machines. Note that the unconstrained optimal makespan for 1, 2, and 3
machines are 6, 3, and 2, respectively. First, consider the packing that gives each bag 2 jobs. Then
the makespans are 6, 4, and 2, respectively, if there are 1, 2, and 3 machines. Hence, this algorithm
is max{6/6, 4/3, 2/2} = 4/3-robust. Next, consider the packing that places {1, 2, 3} jobs in each bag,
respectively. Then the makespans are 6, 3, and 3, respectively, if there are 1, 2, and 3 machines,
which makes this algorithm max{6/6, 3/3, 3/2} = 3/2-robust. In this example, the former algorithm
is better. Moreover, this example demonstrates that 4/3 is a lower bound on the robustness of any
algorithm. Note that there exists a same lower bound for any number of M: Consider an arbitrary
M and n = 2M identical jobs with each processing time 1, if we put at least one bag with at least
3 jobs, then the robust ratio is at least 3/2; otherwise, we put 2 jobs in each bag and it provides a
robust ratio that is at least 4/3.
2 SCHEDULING INFINITESIMAL JOBS
Throughout this article, we use p(j) to denote the processing time of job j. For any set of jobs S,
we use p(S) =
pi ∈S p(i) to denote the sum of the processing times of jobs in S. We informally say
a job set S is big if the value of p(S) is large and small otherwise.
We now consider the case of infinitesimal jobs. Suppose we are given a job set J with all infinitesimal jobs such that p(J) = s, for some s > 0. We first pack the jobs to M ≥ 3 sets and then
schedule the bags on m machines, where m ∈ {1, 2,..., M} is only known after we pack the jobs.
Let ALG(m, M) be the makespan of scheduling the bags on m machines, then our objective is to
minimize the robust ratio, α = maxm {
ALG(m,M)
optm }. Recall optm is the makespan of a schedule that
knew m in advance, and specifically optm = s/m when all jobs are infinitesimal.
The main idea in the packing is to produce a set of bags with a diverse set of sizes. More
precisely, we consider the following packing, which we call packing PC. S1, S2,..., SM are the
bags: for i = 1, 2, 3,..., 2M/3, p(Si ) = ks
M− i
2  − ks
2(M−1) ; for j = 2M/3 + 1, 2M/3 + 2,..., M,
p(Sj) = ks
M , where k is a parameter that only depends on M, chosen to ensure that M
i=1 p(Si ) = s.
Specifically, k = 1/(2 ·
M/3
i=1 1
M−i −  M
3  1
M (M−1) + 3{ M
3 } 1
M ) ≈ 1.233 when M is large (here, we
use {} to denote the integer remainder). And we will show that for all M, k ≤ 1.2333. See Figure 1
to see how k changes with M and Figure 2 to see the size of bags when M = 100 and s = 1000.
Using packing PC to put the jobs into bags, we obtain the following theorem.
Theorem 2.1. Form ∈ [1, M], there exists a schedule with makespan at most koptm ≤ 1.2333optm,
which schedules {S1, S2,..., SM } on m machines.
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.   
Scheduling When You Do Not Know the Number of Machines 9:5
Fig. 2. The size of the bags when M = 100 and s = 1000.
Proof. We first consider the case when m ≥ M/2, and we schedule the bags as follows. Let
t = M − m. For machine i = 1, 2,...,t, we schedule bags Si and S2t−i+1 on machine i; for machine j = t + 1,...,m, we schedule bag Sj+t on machine i. The machines with one bag are all
within the bound, since fori ≤ 2 M
3 ,p(Si ) = ks
M− i
2  − ks
2(M−1) ≤ ks
M−M/3 − ks
2M = ks
M ≤ ks
m = koptm.
Therefore, it remains to bound the load on machines with two bags. We will use L(i,t) to denote the load on the machine i for a particular value of t. We therefore need to prove that
L(i,t) = p(Si ) + p(S2t−i+1) ≤ koptm for 1 ≤ i ≤ t, t = M − m ≤  M
2 . Observe that when i is even,
L(i,t) = p(Si ) + p(S2t−i+1) = p(Si−1) + p(S2t−i+2) = L(i − 1,t), hence we may assume that i is odd.
Since i ≤  M
2  ≤ 2 M
3 , p(Si ) = ks
M− i+1
2
− ks
2(M−1) .
We first consider the subcase when 2t − i + 1 ≥ 2 M
3  + 1, then p(S2t−i+1) = ks
M . Since i is odd,
i ≤ 2t − 2 M
3  − 1. Hence, we have
L(i,t) = ks
M − i+1
2
− ks
2(M − 1)
+
ks
M (1)
≤
ks
M − t + M/3
+
ks(M − 2)
2(M − 1)M .
Recall that optm = ks
m = ks
M−t . Since t ≥  M
3  + i
2 , t ≥  M
3  + 1. Using Equation (1), it follows
that
L(i,t) − ks
M − t
= ks
M + M/3 − t
+
ks(M − 2)
2(M − 1)M − ks
M − t
≤
ks(M − 2)
2(M − 1)M − ks M/3
(M + M/3 − t)(M − t)
≤
ks(M − 2)
2(M − 1)M − ks M/3
(M − 1)(M − M/3 − 1)
= ks M2 − 3M + 2 − (3M − 2) M/3
2M(M − 1)(M − M/3 − 1)
≤ ks
M2 − 3M + 2 − (3M − 2)(M/3 − 2
3 )
2M(M − 1)(M − M/3 − 1)
= ks −M/3 + 2
3
2M(M − 1)(M − M/3 − 1) < 0.
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.
9:6 C. Stein and M. Zhong
That is, L(i,t) ≤ ks
M−t = koptm. Next, we consider the subcase that that 2t − i + 1 ≤ 2 M
3 . Now,
we have p(S2t−i+1) = ks
M− 2t−i+1 2
− ks
2(M−1) = ks
M+(i+1)/2−(t+1) − ks
2(M−1) (Recall that we assume i is odd
then 2t − i + 1 is even). Hence, we have
L(i,t) = ks
M − i+1
2
+
ks
M + i+1
2 − (t + 1)
− ks
M − 1
= ks(2M − t − 1)
(M − i+1
2 )(M + i+1
2 − (t + 1)) − ks
M − 1
= ks(2M − t − 1)
−1
4 (i − t)2 + 1
4 (t + 1)2 + M2 − (t + 1)M − ks
M − 1
≤
ks(2M − t − 1)
−1
4 (1 − t)2 + 1
4 (t + 1)2 + M2 − (t + 1)M − ks
M − 1
= ks(2M − t − 1)
(M − 1)(M − t)
− ks
M − 1 = ks
M − t
.
The inequality holds, because 1 ≤ i ≤ t. This proves that for the case m ≥ M/2, the makespan
of our schedule is at most ks
M−t = koptm.
Next, we consider the case when 1 ≤ m < M/2. Let x ≥ 2 be the integer such that
M
x+1 ≤ m < M
x . Let t = M − mx. For machine i = 1, 2,...,t, we schedule bags Si , S2t−i+1,
Si+2t,..., Si+x t on such machine; for machine j = t + 1,...,m, we schedule bags Sj+x t, Sj+x t+(m−t),
Sj+x t+2(m−t) ..., Sj+x t+(x−1)(m−t) on such machine. Recall that ∀i ≤ M, p(Si ) ≤ ks
M . Observe that
we schedule x bags on the last m − t machines, hence the processing times of jobs on such those
machines are at most xks
M = ks
M/x ≤ ks
m = koptm. The processing time of the bags scheduled on
machine i ≤ t is (note that m = (M − t)/x):
p(Si ) + p(S2t−i+1) +
x
j=2
p(Si+jt ) = L(i,t) +
x
j=2
p(Si+jt )
≤
ks
M − t
+ (x − 1)
ks
M
≤
kxs
M − t = koptm .
The last is to show a bound of k. Recall that k is chosen to ensure that M
i=1 p(Si ) = s, so we have
(recall that we use {} to denote the integer remainder)

M
i=1
p(Si ) = 2 ·
M

/3
i=1
 ks
M − i
− ks
2(M − 1)

+ (M − 2 M/3) ·
ks
M
= 2 ·
M

/3
i=1
ks
M − i
− M/3 ks
M(M − 1)
+ 3{M/3}
ks
M . (2)
We can solve Equation (2) for k and obtain that k = 1/b(M), where b(M) = 2 ·
M/3
i=1 1
M−i −
 M
3  1
M (M−1) + 3{ M
3 } 1
M . Note that when M is large,
b(M) ≈ 2
M
−1
i=M−M/3
1
i ≈ 2(ln(M − 1) − ln(2M/3)).
≈ 2 ln 1.5
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.        
Scheduling When You Do Not Know the Number of Machines 9:7
Hence, k = 1/b(M) ≈ 1/(2 ln 1.5) ≈ 1.233 when M is large. We verify by computer that when M <
10,000, k ≤ 1.2333. And for M ≥ 10,000,
b(M) ≥ 2 ·
 M/3
0
1
M − i
− 1
3(M − 1)
≥ 2ln M − M/3
M − 4 · 10−5 ≥ 0.81089.
So, k ≤ 1/0.81089 ≤ 1.2333.
We can also show a lower bound. That is, we can show that, no matter how you divide the jobs,
you cannot achieve a robust ratio below 1.207, which is close to (but does not exactly match) our
upper bound. We state the theorem in terms of a function Q(M), which we define precisely below
and which, for large M, is 1.207.
Theorem 2.2. Let S1, S2,..., SM be M bags of infinitesimal jobs such that M
i=1 p(Si ) = s for
some s > 0. Assume there exists a constant k such for all m ∈ {1, 2,..., M}, we can schedule
{S1, S2,..., SM } on m machines with makespan at most k
optm. Then k ≥ Q(M), where Q(M) =
maxt ≤ M
2 ,t ∈N 1 t
M−t + M−2t M
≈ 1.207.
Proof. We may assume that p(S1) ≤ p(S2) ≤ ··· ≤ p(SM ). We first prove the following stateme
Fort ≤ M/2, there exists a schedule of {S1, S2,..., SM }on m = M − t machines with
minimum makespan such that S1, S2,..., S2t are on the first t machines. (3)
Let {T1,T2,...,Tm } be a schedule of {S1, S2,..., SM } on m machines with minimum makespan,
where Ti is a set of bags scheduled on machine i. We may assume that every Ti contains at least
one bag. Since we schedule M bags on M − t machines, there are at least M − 2t machines contain
exactly one bag. We rename the machines such that Tt+1,Tt+2,...,Tt+(M−2t) = Tm contain only
one bag. Suppose that at least one of S1, S2,..., S2t is not scheduled on the first t machines, that
is, there exists i ≤ 2t, j ≥ t + 1 such that Tj = {Si}. Then, since |
t
i=1 Ti | = M − | m
i=t+1 Ti | = 2t,
there exists i ≥ 2t + 1, j
 ≤ t such that Si ∈ Tj. Define T 
j = {Si }, T 
j = Tj \ {Si }∪{Si}, and
T 
 = T for   j, j

. Since p(Si ) ≥ p(Si ), p(T 
j ) ≤ p(Tj ). Note also that p(T 
j ) ≤ maxM
i=1{p(Si )} ≤
maxm
i=1{p(Ti )}. Hence, maxm
i=1{p(T 
i )} ≤ maxm
i=1{p(Ti )}. This implies that {T 
1 ,...,T 
m } is also a
schedule with minimum makespan. Note that the first t machines of schedule {T 
1 ,...,T 
m } contain
more bags from {S1, S2,..., S2t } than the first t machines of schedule {T1,...,Tm }. By repeating
the above process, we can get a schedule with minimum makespan such that S1, S2,..., S2t are all
scheduled on the first t machines. This completes the proof of Equation (1).
By choosing m = M, we know that p(Si ) ≤ k
optM = k
s
M for any 1 ≤ i ≤ M. For t ≤ M/2, consider the schedule with minimum makespan on m = M − t machines such that S1, S2,..., S2t are
scheduled on the first t machines. It follows that 2t
i=1 p(Si ) ≤ t · k
optm = t k
s
M−t
Hence, for any t ≤ M/2, we have
s =

M
i=1
p(Si ) =
2t
i=1
p(Si ) +

M
i=2t+1
p(Si )
≤
tk
s
M − t
+ (M − 2t) ·
k
s
M .
It follow that k ≥ Q(M) = maxt ≤ M
2 ,t ∈N 1 t
M−t + M−2t M
. Let L(t) = t
M−t + M−2t
M . Then by taking the
derivative, it is not hard to obtain that min0≤t ≤ M
2 L(t) = L((1 −
√
2
2 )M) = 2(
√
2 − 1). And Q(M) ≈
1
min0≤t≤ M
2
L(t) =
√
2+1
2 ≈ 1.207.
Figure 3 shows how Q(M) changes with M.
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.           
9:8 C. Stein and M. Zhong
Fig. 3. Q(M) as a function of M.
2.1 Minimizing the Maximum Difference
Another objective we consider is to minimize the difference between the load of the most loaded
and least loaded machines. This objective is particularly relevant to settings in which we want to
achieve fairness. First, minimizing the maximum difference is a well-studied scheduling objective in
situations where fairness is important. Second, it models a type of fair allocation problem. Suppose
that we want to split some goods among m people, where we do not know what m is in advance.
To simplify the process, we can first divide the goods into M groups or bags, and then have the
restriction that each person must take a subset of the bags.
For this problem, we consider a case where after packing the jobs into M bags, we are given
m ∈ [αM, M] machines on which to schedule. Our bounds (see below) will depend on α, with,
not surprisingly, better bounds for larger α. Since we assume every job is infinitesimal, the minimum difference between the load of the most loaded and least loaded machines is always 0 if
we know the number of machines in advance. So, we cannot use the α-robust settings to evaluate our algorithms in this section, and we introduce some new definitions here. Let s denote
the sum of processing time of all jobs and let A = s/M. For a set of bags S = {S1, S2,..., SM }, we
use Dm (M, α, S,A) to denote the minimum difference between the load of the most loaded and
least loaded machines when we schedule S on m machines. Our goal is to choose S to minimize
D(M, α, S,A) = maxm∈[α M,M]{Dm (M, α, S,A)}.
Assume that αM is an integer with value at most M − 1 and M ≥ 3. Let D∗ (M, α,A) =
minS {D(M, α, S,A)}. We now give a lower bound of D∗ (M, α,A).
Theorem 2.3. If αM > M
2 , then
D∗ (M, α,A) ≥
2(1 − α)M
1 + (4α + 1)(1 − α)M · A.
If αM ≤ M
2 , then
D∗ (M, α,A) ≥
2M2 − 2M
3M2 + M − 2 · A, if M is odd;
D∗ (M, α,A) ≥
2M2 − 4M
3M2 − 8 · A, if M is even.
Proof. We may assume that p(S1) ≤ p(S2) ≤ ··· ≤ p(SM ). For i = 1, 2,..., M, let Ai = i
j=1 p(Sj)/i. We first prove the following statement:
For m > M/2,Dm (M, α, S,A) ≥ 2A2M−2m − p(S2M−2m+1). (4)
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019. 
Scheduling When You Do Not Know the Number of Machines 9:9
Assume the optimal way to schedule {Si} on m machines is by scheduling a set of bags,
Yi , on the ith machine. We may assume that every Yi contains at least one bag. Note that
are at least 2m − M machine receive exactly one bag. We rename the machines so that
YM−m+1,YM−m+2,...,YM−m+(2m−M) = Ym contain only one bag. If the 2m − M bags on those machines are exactly the biggest 2m − M bags, S2M−2m+1, S2M−2m+2,..., S2M−2m+2m−M = SM , then
S1, S2,..., S2M−2m are on the first M − m machines and it follow that the maximum load on a
machines is at least 2M−2m i=1 Si /(M − m) = 2A2M−2m and the minimum load on a machine is at
most S2M−2m+1, which implies that (4) holds. So, we may assume not, and moreover among all
optimal schedule, {Yi} is the schedule with the largest number of the biggest 2m − M bags on
the last 2m − M machines. Then there exists i ≤ 2M − 2m, j ≥ M − m + 1 such that Yj = {Si}.
It follow that there also exists i ≥ 2M − 2m + 1, j
 ≤ M − m such that Si ∈ Yj. Define Y 
j =
{Si }, Y 
j = (Yj \ {Si }) ∪ {Si}, and Y 
 = Y for   j, j

. Since p(Si ) ≥ p(Si ), p(Y 
j ) ≥ p(Yj ). Note
also that p(Y 
j ) ≤ maxM
i=1{p(Si )} ≤ maxM
i=1{p(Yi )}. Hence, maxm
i=1{p(Y 
i )} ≤ maxm
i=1{p(Yi )}. Since
p(Y 
j ) ≥ p(Si ) = p(Yj) andp(Y 
j ) = p(Si ) ≥ p(Yj), minm
i=1{p(Y 
i )} ≥ minm
i=1{p(Yi )}. This implies that
{Y 
i } is also an optimal way of scheduling the bags. But in the last 2m − M machines of {Y 
i }, they
have more bags come from the biggest 2m − M bags than the last 2m − M machines of {Yi}, a
contradiction. This completes the proof of Equation (4).
Let D = D∗ (M, α,A) and let S = {Si} be the set of bags that reaches the optima. First, we assume that αM > M
2 . LetA = A2M−2α M . By Equation (4), p(S(2−2α )M+1) ≥ 2A − Dα M (S) ≥ 2A − D.
Since D ≥ DM (S) = p(SM ) − p(S1) ≥ p(S(2−2α )M+1) − p(S1), it follows that p(S1) ≥ 2A − 2D and
p(SM ) ≥ p(S(2−2α )M+1) ≥ 2A − D.
We know thatA2 ≥ p(S1) ≥ 2A − 2D. For 1 ≤ k < (1 − α)M, assume we know thatA2k ≥ 2A −
2D + k−1
2 (2A − 3D). Then by Equation (4), we have p(S2k+1) ≥ 2A2k − DM−k (S) ≥ 2(2A − 2D +
k−1
2 (2A − 3D)) − D = 2A − 2D + k(2A − 3D). Also, p(S2k+2) ≥ p(S2k+1) ≥ 2A − 2D + k(2A −
3D). Therefore, A2k+2 ≥ A2k 2k
2k+2 + (2A − 2D + k(2A − 3D)) 2
2k+2 ≥ 2A − 2D + k
2 (2A − 3D). Inductively, this implies that A = A2M−2α M ≥ 2A − 2D + (1−α )M−1
2 (2A − 3D), which is equivalent
to
D ≥
2(1 − α)M
1 + 3(1 − α)M
A
.
Recall that p(Si ) ≤ p(S1) + D ≤ A + D for any i, we have
MA = s ≤ 2(1 − α)MA + (2α − 1)M(A + D)
= MA + (2α − 1)MD
≤
1 + 3(1 − α)M
2(1 − α) D + (2α − 1)MD.
This is equivalent to
D ≥
2(1 − α)M
1 + (4α + 1)(1 − α)M · A.
For the case of αM ≤ M
2 , if M is odd, then
D∗ (M, α,A) ≥ D∗

M,
M + 1
2M ,A
	
≥
2M2 − 2M
3M2 + M − 2 · A.
If M is even, then
D∗ (M, α,A) ≥ D∗

M,
M + 2
2M ,A
	
≥
2M2 − 4M
3M2 − 8 · A.
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.      
9:10 C. Stein and M. Zhong
These bounds can most simply be parsed by saying that if M is large, then the lower bound is
given by min{2/3, 2/(4α + 1)}A. Note that when α ≤ 1
2 , the lower bound goes to 2
3A as M goes
large. If M is even, then the bound of 2
3A can be reached by setting the bags as follows. Choose S =
{S1, S2,..., SM } such that p(S1) = p(S2) = ··· = p(SM/2) = 2
3A and p(SM/2+1) = p(SM/2+2) = ··· =
p(SM ) = 4
3A, then it is easy to verify that Dm (S) ≤ 2
3A for any m ∈ [1, M].
However, we can also get an upper bound of D∗ (M, α,A) as follows when α > 1
2 (note that
D ≈ 1
α+1 · A if M is large).
Theorem 2.4. If αM > M
2 , then
D∗ (M, α,A) ≤ (1 − α)M
α + (1 − α)(α + 1)M · A.
Proof. Let D = (1−α )M
α+(1−α )(α+1)M · A and A = 1+3(1−α )M
2(1−α )M · D. We choose the bags,
T = {T1,T2,...,TM }, as follows: for k = 0, 1, 2,..., (1 − α)M − 1, set p(T2k+1) = p(T2k+2) =
2A − 2D + k(2A − 3D); for i ≥ 2(1 − α)M + 1, set p(Ti ) = 2A − D.
It is not hard to check that M
i=1 p(Ti ) = MA, and that the maximum difference of p(Ti ) is D. To
show that D∗ (M, α,A) ≤ D, it is sufficient to show that Dm (M, α,T,A) ≤ D for any m ∈ [αM, M).
Consider the following schedule on m machines with Si being the set of bags on machine i: S1 =
{T1,T2M−2m }, S2 = {T2,T2M−2m−1},..., SM−m = {TM−m,TM−m+1}, SM−m+1 = {T2M−2m+1}, SM−m+2 =
{T2M−2m+2},..., Sm = {TM }. Then maxi{p(Si )} = p(T1) + p(T2M−2m ) = 4A − 4D + (M − m − 1)
(2A − 3D) and mini{p(Si )} = p(T2M−2m+1) = 2A − 2D + (M − m)(2A − 3D). So Dm (M, α,T,A) ≤
(4A − 4D + (M − m − 1)(2A − 3D)) − (2A − 2D + (M − m)(2A − 3D)) = D, as required.
It is worth remarking that another well-studied fairness objective function is to maximize the
minimum loads of the machines. The analysis of this objective is easy in our model. Suppose we
are given a job set J with all infinitesimal jobs such that p(J) = s, for some s > 0. We first pack
the jobs to M ≥ 3 sets, {S1, S2,..., SM }, and then schedule the bags on m machines, where m ∈
{1, 2,..., M} is only known after we pack the jobs, such that the minimum load on the machines is
maximized. Let Yi be the set of the bags on machine i with Im = mini p(Yi ), then our objective is to
deciding the size of the bags such that R = minm {Im/(s/m)} is maximized, where s/m is the optimal
minimum load. We may assume p(S1) ≤ p(S2) ≤ ··· ≤ p(SM ). Consider the case when m = M − k
for k = 0, 1,..., M/2 − 1, then it is clear that IM−k ≤ p(S2k+1), and then p(S2k+2) ≥ p(S2k+1) ≥
Im−k ≥ R · s/(M − k). We assume M is even for simplicity, then s = M
i=1 p(Si ) ≥ 2Rs M/2−1
k=0
1
M−k .
It follows that R ≤ 1/(2
M/2−1
k=0
1
M−k ) = 1/(2
M
j=M/2+1
1
j ) ≈ 1/(2ln2) ≈ 0.72. And this upper bound
can indeed be reached by taking R = 1/(2
M/2−1
k=0
1
M−k ) andp(S2k+2) = p(S2k+1) = R · s/(M − k) for
k = 0, 1,..., M/2 − 1.
3 SCHEDULING DISCRETE JOBS
In this section, we will provide an algorithm that, for any set of jobs, gives a 5
3 + ϵ robust ratio for
m ∈ {1, 2,..., M}. For ease of presentation, we first assume M is divisible by 4 and prove Lemma 3.8.
Then, we discuss the changes that are needed when M is not divisible by 4 and prove Lemma 3.9.
The two lemmas together gives the main theorem.
Our algorithm will start by computing minimum makespan schedules for M, 3M/4 and M/2
machines. We can use a PTAS for minimizing makespan on parallel machines to compute a (1 + ϵ)-
approximate schedule. We use opt
i to denote the value of the makespan obtained by running the
PTAS on a scheduling instance with i machines. We will especially use opt
M/2 and will denote
this value by b. We use PTAS (i) = {S1, S2, ..Si} to denote the result of the PTAS on a scheduling
instance with i machines, where Sj is the set of jobs assigned to machine j.
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.       
Scheduling When You Do Not Know the Number of Machines 9:11
Based on the values opt
M , opt
3M/4 and opt
M/2, we will consider several cases. Each case will
have the same structure. First, we will compute sets of jobs Si from a PTAS. Second, we will use
partitioning routines to split the job set into M bags. We then learn the number of machines and
need to schedule the bags on machines. In different cases, we will use different combinations of
partitioning and scheduling routines. We then show that, for each case, the resulting schedule is
5
3 (1 + ϵ ) robust.
In Section 3.1, we will give our partitioning routines and prove some properties about each one.
In Section 3.2, we will describe how to assemble the jobs into bags and then schedule them on
machines.
3.1 Partitioning
In this subsection, we will describe three useful partitioning algorithms, which will be used as
subroutines in the main algorithm. Each one will take one or four sets of jobs and partition them
into multiple bags with special properties, achieving some balance between the sizes of the bags
and controlling the placement of large jobs. In all routines, we will assume that we process the
jobs in sorted size order, largest-to-smallest.
We begin with the first partitioning algorithm, Partition I, which partitions a job set into two
bags such that neither of them is too big. This partitioning algorithm will be useful when we want
to partition a job set “evenly.” It implements the LPT algorithm, sorting the jobs in non-increasing
order and then repeatedly placing the next job in the least loaded bag. It appears in Algorithm 1.
ALGORITHM 1: Partition I (S)
Input: A set of jobs S = {j1, j2, .., jK } with p(j1) ≥ p(j2) ≥ ··· ≥ p(jK ) and
p(S) ≤ b.
Main Process: Run LPT on 2 machines, A and B, breaking ties in favor of A.
If p(A) ≤ p(B), then swap the names of A and B.
Output: (A, B).
We now bound the sizes of the bags. Recall that the standard analysis of LPT shows that it is
a 4/3 − 1/3m = 7/6-approximation algorithm on two machines [8]. The bounds that we give are
tight and are not implied by the standard analysis of LPT.
Lemma 3.1. Let (A, B) be the output of Partition I, then p(A) = max{p(A),p(B)} ≤
max{p(j1), 2b/3}. If the maximum is achieved by p(j1), then A = {j1}.
Proof. If j1 has larger processing time than all the other jobs combined, i.e., p(j1) ≥ K
i=2 p(ji ),
then for i = 2, ..,K, LPT will put ji in B and max{p(A),p(B)} = p(A) = p(j1).
Next consider the case when p(j1) < K
i=2 p(ji ). We use dt to denote the difference in the loads
of A and B after adding job jt (note that dK = p(A) − p(B)). Pick k as the smallest integer such that
p(j1) < k
i=2 p(ji ), clearly k ≥ 3. Then, p(jk ) ≥ dk = k
i=2 p(ji ) − p(j1), since k−1 i=2 p(ji ) ≤ p(j1).
When we decide where to put ji+1, the difference between the loads of A and B is di , and we
put ji+1 in the bag with less load. Hence, we must have di+1 ≤ max{p(ji+1),di}. Then inductively
dK ≤ max{p(jK ),p(jK−1), ..,p(jk+1),p(jk ),dk }. Because the jobs are indexed largest to smallest, and
because k ≥ 3, we can simplify the previous inequality to dK ≤ p(j3) ≤ b/3 (recall that dk ≤ jk ).
Since p(A) + p(B) = b, it immediately follows that max{p(A),p(B)} ≤ 2b/3.
Since we almost always need to put multiple bags on one machine, we want to create enough
small bags to control the makespan. This demand leads to the next two partitioning algorithms,
in which we partition job sets to bags such that half of them are small enough and the others are
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.      
9:12 C. Stein and M. Zhong
not too big. The first one, Partition II, works for the case when we have a job set with at most one
large job. We will place the job with the largest processing time in set A and then fill set B greedily
up to b/3 and then place the remaining jobs in A. The details appear in Algorithm 2.
ALGORITHM 2: Partition II(S)
Input: A set of jobs S = {j1, j2,..., jK } with 5b/6 ≥ p(j1) ≥ p(j2) ≥ ··· ≥ p(jK ),
p(j2) ≤ b/3 and p(S) ≤ b.
Main Process:
1 B = ∅, A = {j1}.
2 Let t be the largest integer such that p(j2) + p(j3) + ··· + p(jt ) ≤ b
3 ; put j2,..., jt in B.
3 Add jt+1,..., jK into A if t  K.
Output: (A, B).
Lemma 3.2. Let (A, B) be the output of Partition II, then p(A) ≤ 5b/6 and p(B) ≤ b/3.
Proof. By line 2, clearly we have p(B) ≤ b/3. Suppose, for a contradiction, that p(A) > 5b/6.
Then t  K and it follows that p(j1) + K
i=t+1 p(ji ) > 5b/6. Since K
i=1 p(ji ) ≤ b, we must have
t
i=2 p(ji ) = K
i=1 p(ji ) − (p(j1) + K
i=t+1 p(ji )) < b/6.
But, since t+1 i=2 p(ji ) > b/3, we also have p(jt+1) = t+1 i=2 p(ji ) − t
i=2 p(ji ) > b/3 − b/6 = b/6.
Note that t ≥ 2, hence p(j2) ≥ p(jt+1) > b/6 > t
i=2 p(ji ), a contradiction.
The last partitioning algorithm, Partition III, will handle the case when we have at least two
large jobs and Partition II is not working. Specifically, we will take four job sets as input; one of
them, S1, has two large (≥b/3) jobs, another, S2, has only small (<b/3) jobs, and the other two, S3
and S4, have no jobs greater than 2b/3. We partition them into eight bags, such that we have four
small bags and four bags that are not too big. The algorithm first puts the second biggest job from
S1 into A1 and the remaining jobs from S1 into A2. It then greedily puts jobs from S2 into A1 and
A2 as long as they do not cause the load to go over 5b/6. By greedily, here, we mean that it goes
through the jobs in non-increasing order of processing time, and if the job can fit on A1 or A2, we
place it on one that it fits. We then use Partition I on the remaining jobs of S2, running LPT to
place jobs on B1 and B2. We also use Partition I to partition S3 to (A3, B3), and S4 to (A4, B4). We
will show that either p(B1) + p(B3) and p(B2) + p(B4) are both not too big or we can switch jobs
to ensure that neither is. The details of the algorithm appear in Algorithm 3.
Lemma 3.3. Let (A1,A2,A3,A4, B1, B2, B3, B4) be the output of Partition III. These bags satisfy:
(1) p(Ai ) ≤ 5b/6, for i = 1, 2, 3, 4;
(2) p(Bi ) ≤ b/2, for i = 1, 2, 3, 4;
(3) p(B1) + p(B3) ≤ 5b/6, p(B2) + p(B4) ≤ 5b/6.
Proof. Let kij denote the load of jobs from Si that are placed in bag Aj . Note that the jobs in
bags B1 and B2 all come from S2. Thus, k11 + k12 ≤ b and k21 + k22 + p(B1) + p(B2) ≤ b. Fori = 1, 2,
define the non-negative slack in each Ai as δi = 5b/6 − k1i − k2i and δ = max{δ1, δ2}.
Note that at line 1, p(A1) = p(x2) ≤ b/2 and p(A2) = p(S1) − p(x2) ≤ b − b/3 ≤ 2b/3. Since we
call Partition I to separate S3 and S4, by Lemma 3.1, p(A3) ≤ 2b/3, p(A4) ≤ 2b/3 at line 6. Since for
i = 1, 2, 3, 4, we will only add job to Ai as long as the load does not exceed 5b/6, condition 1 holds.
We now consider conditions 2 and 3. By line 2, any job in B1 ∪ B2 has load greater than δ. First,
consider the case when B1 ∪ B2 contains at most two job. Then, p(B1) ≤ b/3 and p(B2) ≤ b/3. Note
that also p(B3) ≤ p(S3)/2 ≤ b/2 and similarly p(B4) ≤ b/2, hence the claim follows.
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.            
Scheduling When You Do Not Know the Number of Machines 9:13
ALGORITHM 3: Partition III(S1, S2, S3, S4)
Input: Four sets of jobs: S1 = {x1, x2, .., xp } such that p(x1) ≥ p(x2) ≥ b/3, p(S1) ≤ b,
S2 = {y1, ..,yq } such that b/3 ≥ p(y1) ≥ ··· ≥ p(yq ), p(S2) ≤ b, S3 with p(S3) ≤ b and S4
with p(S4) ≤ b. Moreover for i = 1, 2, 3, 4, p(j) ≤ 2b/3 for every j ∈ Si .
Main Process:
1 A1 = {x2}. A2 = S1 − {x2}, i = 1.
2 while (i ≤ q)
3 if p(A1) + p(yi ) ≤ 5b/6, add yi to A1; else if p(A2) + p(yi ) ≤ 5b/6, add yi to A2,
4 i + +
5 Let S be the jobs remaining in S2.
6 Set (B1, B2) = Partition I(S
), (A3, B3) = Partition I(S3), (A4, B4) = Partition I(S4).
7 If B1 ∪ B2 contains 3 jobs and B2 contains only one job, then let j3 be the job with the least processing
time in B1. If p(j3) + p(A3) ≤ 5b/6, then move j3 from B1 to A3; Else if p(j3) + p(A4) ≤ 5b/6, move
j3 from B1 to A4; Else set temp = B1, B1 = B4, B4 = temp.
8 Else, if B1 ∪ B2 contains more than 3 jobs and B2 contains one job, then let j
 denote the job in B2
with the least processing time. Move j
 from B2 to A3 ifp(j

) + p(A3) ≤ 5b/6.
Output: (A1,A2,A3,A4, B1, B2, B3, B4).
Next, we consider the case when B1 ∪ B2 contains l ≥ 3 job before line 7. Note that
p(B1) + p(B2) ≤ b − k21 − k22
= b −

5b
6 − δ1 − k11
−

5b
6 − δ2 − k12
≤
b
3
+ 2δ .
If both B1 and B2 contain at least 2 jobs, then p(B1) ≥ 2δ and p(B2) ≤ b/3 + 2δ − p(B1) ≤ b/3.
Similarly, p(B2) ≤ b/3, hence the claim follows.
Next, we consider the case when l = 3. If B1 contains only one job, then b/3 ≥ p(B1) ≥ p(B2)
and the claim follows. We may assume B1 = {j2, j3}, B2 = {j1} and p(j1) ≥ p(j2) ≥ p(j3). Since
k11 = p(x2) ≤ b/2 and 5b/6 − k11 ≥ b/3 ≥ p(y1), we will always put y1 in A1. It implies that
p(y1) + p(j1) + p(j2) + p(j3) ≤ p(S2) ≤ b jobs and j3 ≤ (p(y1) + p(j1) + p(j2) + p(j3))/4 ≤ b/4. If in
line 7 we move j3 to either A3 or A4, then clearly claim follows, since after line 7, both B1 and
B2 contains one job and thus have load at most b/3. So, we may assume that before line 7,
p(A3) > 5b/6 − p(j3) ≥ 5b/6 − b/4 = 7b/12, and then p(B3) ≤ b − p(A3) < 5b/12. Similarly, we
have p(B4) < 5b/12. Hence, p(B3) + p(B4) < 5b/6 before line 7. Note also that p(j2) + p(j3) ≤
p(j1) + p(y1), hence p(j2) + p(j3) ≤ p(S2)/2 ≤ b/2. So before line 7, p(B1) ≤ b/2 and p(B2) ≤ b/3.
Recall that we will swap the name of B1 and B4, hence the claim follows.
The last case is when l ≥ 4 and one of B1 and B2 contains only one job. If B1 contains only
one job, then b/3 ≥ p(B1) ≥ p(B2) and the claim follows. We may assume that B2 = {j1} and B1 =
{j2, j3,..., jl} with p(j1) ≥ p(j2) ≥ ··· ≥ p(jl ). Then by Partition I, p(j2) + ··· + p(jl−1) ≤ p(B1) ≤
b/3. Recall that l ≥ 4, hence p(jl ) ≤ (p(j2) + ··· + p(jl−1))/(l − 2) ≤ b/6. By Lemma 3.1, p(A3) ≤
2b/3. Thus, p(A3) + p(jk ) ≤ 5b/6. So, in line 8 we will always move jk to A3. After line 8, p(B2) =
p(j2) + ··· + p(jl−1) ≤ b/3. Hence, the claim also follows.
3.2 Packing and Scheduling
In the previous subsection, we gave different algorithms to partition jobs into bags. We will now
show how to use these algorithms in conjunction with additional algorithms to schedule the bags
(and thus jobs) onto machines. Recall that opt
M/2 is the value of the makespan obtained by running
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019. 
9:14 C. Stein and M. Zhong
the PTAS on a scheduling instance with M/2 machines and b = opt
M/2. Two simple facts we will
use throughout this section are that optm is non-increasing with m, the number of machine, and
that 2optM ≥ optM/2, which implies that optM ≥ opt
M/2
2(1+ϵ ) = b
2(1+ϵ ) .
We now prove our bound of 5
3 + ϵ by considering three different cases. The first case is when
opt
M ≥ 3b/5, which implies that the PTAS of the jobs on M/2 is good enough for M machines.
We remark that if this case does not hold then there is no job with processing time greater than
3b/5. The next case is when opt
3M/4 ≤ 4b/5, which implies that we can start with the PTAS of the
jobs on 3M/4 machines and do not need to split all job sets. The last and the hardest case is when
opt
3M/4 > 4b/5 and opt
M < 3b/5. In this case, we will start with the PTAS on M/2 and split each
job sets according to its structure. Before going to the detail of those three cases, we start with a
lemma that gives a sufficient condition for bags to give a 5
3 (1 + ϵ ) robust ratio when the number
of machine is less than M/2. It will be used in both the second and the last case.
Lemma 3.4. Let {S1,..., SM } denotes M bags such that fori = 1, 2,..., M/2, p(Si ) ≤ 5b/6, and for
i = M/2 + 1, M/2 + 2,..., M, p(Si ) ≤ 2b/3. Then, the following algorithm yields a schedule for the
bags with a makespan of at most 5(1 + ϵ )optm/3 for any m ∈ [1, M/2).
(1) If M/4 ≤ m < M/2, then for i = 1, 2,...,m, put Si on machine i; for i = m + 1,m +
2,..., M/2, put Si on machine i − m; for i = M/2 + 1,..., M, put Si on the machine with
the least load.
(2) If M
2(k+1) ≤ m < M
2k for some integer k ≥ 2, then sort {S1, S2,..., SM } in decreasing order
according to their processing times. For i = 1,...,m, put Si on machine i; for i = m +
1,..., 2m, put Si on machine i − m; for i = 2m + 1,..., M, put Si on the machine with the
least load.
Proof. First note that since m < M/2, optm ≥ optM/2 ≥ b/(1 + ϵ ). Let machine r be the machine with the largest load and among the bags scheduled on machine r, St is the one with
the largest index. For the case M/4 ≤ m < M/2, if t > M/2, then we know p(St ) ≤ 2b/3 ≤ 2(1 +
ϵ )optm/3 and the makespan of our algorithm is at most p(St ) + optm ≤ 5(1 + ϵ )optm/3. Else
t ≤ M/2, then we know that we only put two bags on machine r, and therefore the makespan
is at most 2 · 5b/6 = 5b/3 ≤ 5(1 + ϵ )optm/3.
For the case M
2(k+1) ≤ m < M
2k with k ≥ 2, if t ≤ 2m, then similarly we know that we only put two
bags on machine r, and therefore the makespan is at most 2 · 5b/6 = 5b/3 ≤ 5(1 + ϵ )optm/3. So,
we may assume that t ≥ 2m + 1. Since 2m
i=1 p(Si ) ≥ 2m
i=1 p(St ) = 2mp(St ), optm ≥ 2m
i=1 p(Si )/m ≥
2p(St ). This implies that p(St ) ≤ optm/2 and the makespan of our algorithm is at most p(St ) +
optm ≤ 3optm/2.
3.2.1 Case I: opt
M ≥ 3b/5. The first case is quite simple. We start with PTAS (M/2), and partition each job set into two bags using Partition I. Those are our bags. For scheduling, if m ≥ M/2,
then we just revert to the M/2 machine schedule, leaving the remaining machines empty. If
m < M/2, then we can run LPT to the bags on the machines. The details appear in Algorithm 4.
Lemma 3.5. If opt
M ≥ 3b/5, then Algorithm 4 is 5
3 (1 + ϵ )-robust.
Proof. For m ≥ M/2,
ALG(m, M)
optm
≤
opt
M/2
optM
≤
b
opt
M
1+ϵ
≤
b
3b/5
1+ϵ
= (1 + ϵ )
5
3
.
For m < M/2, optm ≥ optM/2 ≥ b/(1 + ϵ ). We may assume the bags are S
1,..., S
M such that
p(S
1) ≥ p(S
2) ··· ≥ p(S
M ). Let k be the number such that for i ≤ k, p(S
i ) > 2b/3 and for i > k,
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.    
Scheduling When You Do Not Know the Number of Machines 9:15
ALGORITHM 4: Algorithm for Case I
Packing:
1 Let S = PTAS (M/2) = {S1,..., SM/2}.
2 for k = 1 ..., M/2, let (Ak , Bk ) = Partition I(Sk ).
3 Return the M bags {A1,...,AM/2, B1,..., BM/2}.
Scheduling:
1 if m ≥ M/2, then schedule Ai and Bi on machine i for i ≤ M/2. Leave the remaining machines empty.
2 if m < M/2, then run LPT to schedule the bags on m machines.
p(S
i ) ≤ 2b/3. Recall that each bag S
i comes from running Partition I on some job set Sj with
p(Sj) ≤ b. Hence, by Lemma 3.1, either p(S
i ) ≤ 2b/3 or it is a bag with a single job. Specifically,
S
1,..., S
k all contain only one job. Let the jobs be j1, j2,..., jk , respectively. Let machine r be the
machine with the largest load and among the bags scheduled on machine r, St is the one with the
largest index. If t > k, then we know p(St ) ≤ 2b/3 ≤ 2(1 + ϵ )optm/3. By the property of LPT, the
makespan of our algorithm is at most p(St ) + optm ≤ 5(1 + ϵ )optm/3. If t ≤ k, then the makespan
of our algorithm is equal to the makespan of running LPT on jobs {j1, j2,..., jk }, which is at most
4/3 times the minimum makespan of scheduling {j1, j2,..., jk } on m machines [8]. Since the minimum makespan of scheduling {j1, j2,..., jk } on m machines is at most optm, our makespan in this
case is at most 4optm/3.
3.2.2 Case II: opt
M < 3b
5 and opt
3M/4 ≤ 4b/5. In this case, we will use the PTAS schedule for
3M/4 machines as input to Partition I. For Si ∈ PTAS (3M/4), we call Sibad if Si contains a job with
processing time at least 2
3opt
3M/4. For the packing step, we will run Partition I on M/4 job sets,
try to avoid a bad set if possible, and then schedule the bags on machines. For the scheduling step,
we will have several cases, based on how many bad machines that we have. The details appear in
Algorithm 5.
ALGORITHM 5: Algorithm for Case II
Packing:
1 Let S = PTAS (3M/4) = {S1, .., S3M/4}. Let v be the number of bad sets in S. Rename the sets
so that S1,..., Sv are bad sets.
2 For M/2 < k ≤ 3M/4, let (Ak , Bk ) = Partition I(Sk ).
3 Return the M bags {S1, S2, .., SM/2,AM/2+1,...,A3M/4, BM/2+1, .., B3M/4}.
Scheduling:
1 if m ≥ 3M/4, then schedule Si on machine i for i ≤ M/2, and schedule Aj , Bj on
machine j for j > M/2 . Leave the remaining machines empty.
2 if v > M/2 and v ≤ m < 3M/4, then first put S1, S2,..., SM/2,AM/2+1 ∪ BM/2+1,...,Av ∪ Bv on v
machines, respectively, then schedule the rest of the bags with at most one in each machine.
3 if v > M/2 and M/2 ≤ m < v − 1, or v ≤ M/2 and M/2 ≤ m < 3M/4, then schedule the bags
arbitrarily such that each machine contains at most two bags and at most one bag comes from
{S1,..., SM/2}.
4 if m < M/2, then we follow the strategy in Lemma 3.4.
Lemma 3.6. If opt
M < 3b
5 and opt
3M/4 ≤ 4b/5, then Algorithm 5 is 5
3 (1 + ϵ )-robust.
Proof. For m ≤ 3M/4, the load on each machine is at most opt
3M/4 ≤ 4b/5; hence, we have
ALG(m, M)
optm
≤
opt
3M/4
optM
≤
4b/5
b
2(1+ϵ )
< (1 + ϵ )
5
3
.
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019. 
9:16 C. Stein and M. Zhong
By Lemma 3.1, for k > max{v, M/2}, p(Bk ) ≤ p(Ak ) ≤ 2
3opt
3M/4. If v > M/2 and v ≤ m <
3M/4 or v ≤ M/2 and M/2 ≤ m < 3M/4, then the load on each machine is at most opt
3M/4 +
maxi>v {p(Ai )} ≤ 5
3opt
3M/4, so we have
ALG(m, M)
optm
≤
5
3opt
3M/4
opt3M/4
≤ (1 + ϵ )
5
3
.
If v > M/2 and M/2 ≤ m < v − 1, then since the number of jobs with processing time at least 2
3opt
3M/4 is at least v, optv−1 ≥ 4
3opt
3M/4. However, each machine has load at most 2opt
3M/4. So
we obtain
ALG(m, M)
optm
≤
2opt
3M/4
optv−1
≤
2opt
3M/4
4
3opt
3M/4
< (1 + ϵ )
5
3
.
Since opt
M < 3b/5, every job has processing time at most 3b/5. Notice that for i ≤ M/2 and
j > M/2, p(Si ) ≤ opt
3M/4 ≤ 4b/5 and p(Bj) ≤ p(Aj) ≤ max{3b/5, 2opt
3M/4/3} = 3b/5 < 2b/3, we
can apply Lemma 3.4 on the case m < M/2.
3.2.3 Case III: opt
M < 3b
5 and opt
3M/4 > 4b/5. This final case is the most complicated one. We
say a job is big if it has processing time at least b/3. Let S be a set of jobs with sum of the processing
time at most b, we call the set 2-big if it has at least two big jobs. We call the set 1-big if it is not
2-big and has one big job. We call a set that is neither 1-big nor 2-big, 0-big. Give a set S of job
sets, we let vi (S) denote the number of i-big sets, and we assume that we have functions i-big(S),
which return an i-big set from S, assuming one exists. For a job set Si , we use Si (j) to denote the
jth largest job in set Si .
The main idea for packing here is to use the optimal schedule for M/2 machines, but to split
the jobs assigned to each machine into 2 bags. For each Si , we want to partition it into two sets
such that one is small enough and the other is not too big. We can use Partition II to achieve this
goal, if and only if Si contains at most one big job, that is, Si is 0-big or 1-big. If Si is 2-big and thus
contain two big jobs, then we try to group three 2-big job sets with one 0-big job set, and then
partition them into eight sets such that four are small enough and four not too big, using Partition
III. Roughly speaking, if many Si contain two big jobs, then we can give a good lower bound on
opt, else we will have a good partition. The details appear in Algorithm 6.
Lemma 3.7. If opt
M < 3b/5 and opt
3M/4 > 4b/5, then Algorithm 6 is 5
3 (1 + ϵ )-robust.
Proof. Since opt
M < 3b/5, every job has processing time at most 3b/5. For k = 1, 2, 3,..., 4u,
by Lemma 3.2, we have p(Ak ) ≤ 5b/6 and p(Bk ) ≤ b/2 and if moreover k = 4t + 1 or 4t + 2 for an
integer t, p(Bk ) + p(Bk+2) ≤ 5b/6, and we call such Bk and Bk+2 paired bags. For k = 4u + 1, 4u +
2, .., 4u + w, we partition a 2-big job set Si into {Ak , Bk } by putting the second biggest job in Bk
and the rest in Ak . Hence, b/3 ≤ p(Bk ) ≤ b/2, p(Ak ) = p(Si ) − p(Bk ) ≤ b − b/3 = 2b/3. For k =
4u + w + 1, 4u + w + 2, .., M
2 , by Lemma 3.3, we have p(Ak ) ≤ 5b/6,p(Bk ) ≤ b/3 (note that after
line 3 there is no 2-big job set left, and hence we can use Partition II in line 4).
Next consider the scheduling process in Algorithm 4. First consider the case m ≥ max
{3M/4, M/2 + w + 2u}. For i ≤ M/2, the load of each machine is at most maxi{p(Ai )} ≤ 5b/6. For
i = M/2 + 1, M/2 + 2,..., M/2 + 2u, we always schedule a paired bags on the machines, so the
load is still at most 5b/6. For the rest of the machines, we schedule B4u+1,..., BM/2 on them such
that there are at most two bags on each machine, and if there are two bags, at most one of them is
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019. 
Scheduling When You Do Not Know the Number of Machines 9:17
ALGORITHM 6: Algorithm for Case III
Packing:
1 Let S = PTAS ( M
2 ) = {S1, .., S M
2
}, u = min 

v2 (S )
3

,v0 (S)

, w = max{v2 (S) − 3u, 0} and S = S.
2 if u ≥ 1, for k = 1, 5, 9,..., 4u − 3,
C = 2-big(S
);D = 0-big(S
); E = 2-big(S
); F = 2-big(S
);
(Ak ,...,Ak+3, Bk ,..., Bk+3) = Partition III(C,D, E, F );
S = S − C − D − E − F .
3 if w ≥ 1, for k = 4u + 1, 4u + 2,..., 4u + w,
C = 2-big(S
); Bk = {C(2)};Ak = C/Bk ; S = S − C.
4 for k = 4u + w + 1, 4u + w + 2,..., M
2 .
Let C be any set in S
. (Ak , Bk ) = Partition II(C); S = S − C.
5 Return the bags {A1,A2,...,A M
2
, B1,..., B M
2
}.
Scheduling:
1 if m ≥ max{3M/4, M/2 + w + 2u}
for i ≤ M/2, schedule Ai on machine i;
for i = M/2 + 1, M/2 + 3,..., M/2 + 2u − 1, schedule B2i−M−1 and B2i−M+1 on
machine i and B2i−M and B2i−M+2 on machine i + 1;
for i = M/2 + 2u + 1,...,m, schedule the rest of the bags on those machine such that
each machine contains at most two bags and if there are two bags on a machine, at
most one of them is from B4u+1,..., B4u+w ;
2 if w + 2u > M/4 and 3M/4 ≤ m < M/2 + w + 2u
for i ≤ M/2, schedule Ai on machine i;
for i = M/2 + 1,...,m, schedule the rest of the bags on the machines so that
each machine contains at most two bags;
3 if M/2 ≤ m < 3M/4
Schedule all bags amongst the machines so that there are at most two
bags on each machine and if there are 2 bags on a machine, at most one of
them is Ai for some i.
4 if m < M/2, then we follow the strategy in Lemma 3.4.
from B4u+1,..., B4u+w ; hence, the load is at most b/3 + b/2 = 5b/6. Therefore,
ALG(m, M)
optm
≤
5b/6
optM
≤
5b/6
b
2(1+ϵ )
= (1 + ϵ )
5
3
.
Next, consider the case w + 2u > M/4 and 3M/4 ≤ m < M/2 + w + 2u. Since 4u ≤ v2 (S) +
v0 (S) ≤ M/2, 2u ≤ M/4 and v2 (S) − 3u = w ≥ 1. If u = v2 (S)/3 < v0 (S), since v2 (S) ≥ 3u +
1, then v2 (S) = 3u + 1 or 3u + 2. If v2 (S) = 3u + 1, then M/4 < w + 2u = v2 (S) − u = 2u + 1
and M/4 ≤ 2u, but M/2 ≥ v2 (S) +v0 (S) > 3u + 1 + u, a contradiction. If v2 (S) = 3u + 2, then
M/4 < v2 (S) − u = 2u + 2 and M/4 ≤ 2u + 1, but M/2 ≥ v2 (S) +v0 (S) > 3u + 2 + u, a contradiction. So, we have u ≥ v0 (S) and w + 2u = v2 (S) − u ≤ v2 (S) −v0 (S). Note that in total we
have at least 2v2 (S) +v1 (S) = M/2 +v2 (S) −v0 (S) jobs with processing time at least b/3.
Hence, optM/2+w+2u−1 ≥ 2b/3. However, in this case the load on each machine is at most
max{maxi{Ai}, 2 maxi{Bi}} ≤ b:
ALG(m, M)
optm
≤
b
optM/2+w+2u−1
≤
b
2b/3
< (1 + ϵ )
5
3
.
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019.  
9:18 C. Stein and M. Zhong
In the case M/2 ≤ m < 3M/4, the load on each machine is at most maxi{Ai} + maxi{Bi} ≤
5b/6 + b/2 = 4b/3. Therefore, we have
ALG(m, M)
optm
≤
4b/3
opt3M/4
≤ (1 + ϵ )
4b/3
4b/5 = (1 + ϵ )
5
3
.
For the case when m < M/2, by Lemma 3.4 the claim follows.
By combining Lemmas 3.5, 3.6, and 3.7, we can deduce the following lemma. The running time
comes from calling the PTAS [10] three times, various sorting and heap data structure operations
needed to implement LPT and the other algorithms. The M term is in the running time because of
the need to perform operations on the M bags.
Lemma 3.8. When M is divisible by 4, Algorithms 4, 5, and 6 together give a 5
3 (1 + ϵ )-robust algorithm with running time 2O˜ (1/ϵ ) + O((M + n) logn).
Recall that the algorithm uses the terms M/2 and 3M/4 and assumes that they are integers.
When M is not divisible by 4, the algorithm and analysis still work with some small changes.
Lemma 3.9. When M is not divisible by 4, there exists an algorithm gives a 5
3 (1 + ϵ )-robust algorithm with running time 2O˜ (1/ϵ ) + O((M + n) logn).
Proof. We prove this Lemma by making necessary modifications on Algorithms 4, 5, and 6
depending on the remainder of M divided by 4. We first consider the case when M = 4r + 2,
for some nonnegative integer r. Then, we can still follow Algorithm 4. For Algorithm 6, we replace every M/4 by r + 1 and every 3M/4 by 3r + 2. And then Lemma 3.7 still holds with the
same argument under the assumption that opt
3r+2 > 4b/5 instead of opt
3M/4 > 4b/5. So the only
remaining case is opt
3r+2 ≤ 4b/5, and we can follow Algorithm 5 with small modifications. In
the packing phase of Algorithm 5, we will only partition r sets and return with M = 4r + 2 bags
{S1,..., S2r+2,A2r+3,...,A3r+2, B2r+3,..., B3r+2}. Then, in the scheduling phase, line 1 holds for
m ≥ 3r + 2, line 2 holds for v > 2r + 2 and v ≤ m < 3r + 2, and line 3 holds for v > 2r + 2 and
2r + 2 ≤ m < v − 1, or v ≤ 2r + 2 and 2r + 2 ≤ m < 3r + 2. The corresponding part of Lemma 3.6
holds accordingly. The last case is when m ≤ 2r + 1, and we can follow Lemma 3.4 with the same
strategy except for replacing M/2 with 2r + 2 and M/4 with r + 1.
Next, we consider the case when M = 4r + 1, for some nonnegative integer r. In this case, we
denote b = opt
2r+1 instead of opt
M/2, and we replace M/2 by 2r + 1 in all three algorithm. Then,
we still have that optM ≥ b
1+ϵ . For Algorithm 4, in the packing phase, we start with S = PTAS (2r +
1) in line 1, and we run partition I for S1, S2 ..., S2r in line 2. Then, we can still return M bags
{A1,...,A2r, B1,..., B2r, S2r+1} in line 3. We may assume that either S2r+1 contains a job with
processing time greater than 2b/3 or every job has processing time at most 2b/3. For the case
m ≥ 2r + 1, we follow line 1 of the scheduling phase of Algorithm 4 and clearly the robust ratio still
holds. Next, we consider the case form ≤ 2r, and we run LPT to schedule the bags onm machines.
Recall that fori = 1, 2,..., 2r, either p(Ai ) ≤ 2b/3 or Ai is a bag with a single job, and same for Bi .
To prove the ratio still hold, it is sufficient to show that the bags finish last has size at most 2b/3. If
S2r+1 does not contain a job with processing time greater than 2b/3, then every job has processing
time at most 2b/3. We may assume that we run LPT with S2r+1 first (otherwise p(S2r+1) ≤ 2b/3)
and that S2r+1 is not the bag finishes last, since otherwise the makespan is at most b. But now the
bags finishes last has size at most 2b/3 and the claim holds. So, we may assume S2r+1 contains a job
with processing time greater than 2b/3, say j. Then the proof of Lemma 3.5 works except for the
case when the machine finishes last contains only bags with single jobs and S2r+1. But its makespan
is at most 4optm/3 + p(S2r+1 \ {j}) ≤ 5optm/3. For Algorithm 6, recall that we do it under the case
ACM Transactions on Algorithms, Vol. 16, No. 1, Article 9. Publication date: November 2019. 
Scheduling When You Do Not Know the Number of Machines 9:19
when opt4r+1 < 3b/5 < 2b/3, then it implies that for the sets in {S1, S2,..., S2r+1} = PTAS (2r + 1),
at most 2r of which contain at least 2 big jobs. As we can only provide 4r + 1 bag, in the packing
phase, we will return with the bags {A1,...,A2r+1,A = B1 ∪ B3, B2, B4, B5,..., B2r+1} if u ≥ 1 and
{A1,...,A2r+1,A = B1 ∪ B2r+1, B2, B3,..., B2r } otherwise. Then, p(A
) ≤ 5b/6, and we will treat
it like an A-bag. Then in the scheduling phase, line 1 holds for m ≥ max{3r + 2, 2r + 1 + w + 2u},
line 2 holds for w + 2u > r + 1 and 3r + 2 ≤ m < 2r + 1 + w + 2u, and line 3 holds for 2r + 2 ≤
m ≤ 3r + 1. The corresponding part of Lemma 3.7 holds accordingly under the assumption that
opt3r+1 > 4b/5. Also for the case whenm ≤ 2r + 1, we can follow Lemma 3.4 with the same strategy
except for replacing M/2 with 2r + 2 and M/4 with r + 1. It left the case when opt3r+1 ≤ 4b/5, and
we follow Algorithm 5 with modifications. In the packing phase of Algorithm 5, we will only
partition r sets and return with M = 4r + 1 bags {S1,..., S2r+1,A2r+2,...,A3r+1, B2r+2,..., B3r+1}.
Then in the scheduling phase, line 1 holds for m ≥ 3r + 1, line 2 holds for v > 2r + 1 and v ≤ m <
3r + 1, and line 3 holds for v > 2r + 1 and 2r + 1 ≤ m < v − 1, or v ≤ 2r + 1 and 2r + 1 ≤ m <
3r + 1. The corresponding part of Lemma 3.6 holds accordingly. Also for the case whenm ≤ 2r + 1,
we can follow Lemma 3.4 similarly.
For the last case when M = 4r + 3, for some nonnegative integer r. In this case, we denote b =
opt
2r+2 instead of optM/2, and we replace M/2 by 2r + 2 in all three algorithm. The modification
of Algorithm 4 is exactly the same as in the case when M = 4r + 1. For Algorithm 6, similarly,
we have at most 2r + 1 sets in PTAS (2r + 2), which contain at least 2 big jobs. in the packing
phase, we will return with the bags {A1,...,A2r+2,A = B1 ∪ B3, B2, B4, B5,..., B2r+2} if u ≥ 1 and
{A1,...,A2r+2,A = B1 ∪ B2r+2, B2, B3,..., B2r+1} otherwise. Then in the scheduling phase, line 1
holds for m ≥ max{3r + 3, 2r + 2 + w + 2u}, line 2 holds for w + 2u > r + 1 and 3r + 3 ≤ m < 2r +
2 + w + 2u, and line 3 holds for 2r + 3 ≤ m ≤ 3r + 2. The corresponding part of Lemma 3.7 holds
accordingly. Also for the case when m ≤ 2r + 2, we can follow Lemma 3.4 with the same strategy
except for replacing M/2 with 2r + 3 and M/4 with r + 2. It left the case when opt3r+2 ≤ 4b/5, and
we follow Algorithm 5 with similar modifications as in the previous case. This finishes the proof
of 3.9.
In sum, we have the following main theorem.
Theorem 3.10. There exists a ( 5
3 + ϵ )-robust algorithm with running time 2O˜ (1/ϵ ) + O((M +
n) logn).
4 CONCLUSION AND OPEN PROBLEMS
We initiate the idea of scheduling with uncertainty in a number of machines and give several
results. In this article, we focus on the case when m ∈ {1, 2, 3,..., M}, but there still exists a gap
between the lower bound of 4/3 and the upper bound of 5/3. It would be very interesting to close
the gap. We can also generalize the idea to consider the case when m ∈ [αM, βM] to see whether
we can give a better upper bound if we know a restriction on the possible number of machines
in advance. Some of our partitioning lemmas in Section 3.1 can be generalized and may be useful
in such analysis. For example, if we have a set S = {j1, j2, .., jK }, with jobs satisfying 1 − αb/2 ≥
p(j1) ≥ p(j2) ≥ ··· ≥ p(jK ), p(j2) ≤ αb, and p(S) ≤ b, then we can use the idea of Partition II to
partition S to one set with load at most 1 − αb/2 and another set with load at most αb. Partition
III can also be generalized similarly.
It would be natural to consider other scheduling problems and other objectives, such as average
completion time or flow time. Makespan is really a partitioning problem, but other objectives
raise additional questions regarding the ordering of the jobs, which would be interesting to study.
Finally, it might be interesting to consider the case where the number of machines come from a
distribution and you need to schedule to minimize the expected makespan. 