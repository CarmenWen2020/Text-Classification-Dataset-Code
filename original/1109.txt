Clustering problems are well studied in a variety of fields, such as data science, operations research, and
computer science. Such problems include variants of center location problems, k-median and k-means to
name a few. In some cases, not all data points need to be clustered; some may be discarded for various
reasons. For instance, some points may arise from noise in a dataset or one might be willing to discard a
certain fraction of the points to avoid incurring unnecessary overhead in the cost of a clustering solution.
We study clustering problems with outliers. More specifically, we look at uncapacitated facility location (UFL), k-median, and k-means. In these problems, we are given a set X of data points in a metric space
δ (., .), a set C of possible centers (each maybe with an opening cost), maybe an integer parameter k, plus
an additional parameter z as the number of outliers. In uncapacitated facility location with outliers, we
have to open some centers, discard up to z points of X, and assign every other point to the nearest open
center, minimizing the total assignment cost plus center opening costs. In k-median and k-means, we have
to open up to k centers, but there are no opening costs. In k-means, the cost of assigning j to i is δ 2 (j,i).
We present several results. Our main focus is on cases where δ is a doubling metric (this includes fixed
dimensional Euclidean metrics as a special case) or is the shortest path metrics of graphs from a minor-closed
family of graphs. For uniform-cost UFL with outliers on such metrics, we show that a multiswap simple
local search heuristic yields a PTAS. With a bit more work, we extend this to bicriteria approximations for the
k-median and k-means problems in the same metrics where, for any constant ϵ > 0, we can find a solution
using (1 + ϵ )k centers whose cost is at most a (1 + ϵ )-factor of the optimum and uses at most z outliers.
Our algorithms are all based on natural multiswap local search heuristics. We also show that natural local
search heuristics that do not violate the number of clusters and outliers for k-median (or k-means) will have
unbounded gap even in Euclidean metrics.
Furthermore, we show how our analysis can be extended to general metrics for k-means with outliers to
obtain a (25 + ϵ, 1 + ϵ )-approximation: an algorithm that uses at most (1 + ϵ )k clusters and whose cost is at
most 25 + ϵ of optimum and uses no more than z outliers.
CCS Concepts: • Theory of computation → Facility location and clustering;
Additional Key Words and Phrases: k-means, outliers, local search
1 INTRODUCTION
Clustering is a fundamental problem in the field of data analysis with a long history and a wide
range of applications in very different areas, including data mining [10], image processing [45],
biology [31], and database systems [20]. Clustering is the task of partitioning a given set of data
points into clusters based on a specified similarity measure between the data points such that the
points within the same cluster are more similar to each other than those in different clusters.
In a typical clustering problem, we are given a set of n data points in a metric space and an integer
k that specifies the desired number of clusters. We wish to find a set of k points to act as centers and
then assign each point to its nearest center, thereby forming k clusters. The quality of the clustering solution can be measured by using different objectives. For example, in the k-means clustering
(which is the most widely used clustering model), the goal (objective function) is to minimize the
sum of squared distances of each data point to its center, while in k-median, the goal is to minimize
the sum of distances of each data point to its center. The uncapacitated facility location problem is the same as k-median except that instead of a cardinality constraint bounding the number
of centers, there is an additional cost for each center included in the solution. Minimizing these
objective functions exactly is NP-hard [3, 19, 24, 32, 42, 46], so there has been substantial work
on obtaining provable upper bounds (approximability) and lower bounds (inapproximability) for
these objectives; see References [1, 11, 24, 32, 38, 39] for the currently best bounds. Although inapproximability results [24, 32, 38] prevent getting polynomial time approximation schemes (PTASs)
for these problems in general metrics, PTASs are known for these problems in fixed dimensional
Euclidean metrics [5, 17, 22]. Indeed, PTASs for k-median and uncapacitated facility location in fixed dimension Euclidean space [5] have been known for almost two decades, but getting
a PTAS for k-means in fixed dimension Euclidean space had been an open problem until recent
results in References [17, 22].
In spite of the fact that these popular (center-based) clustering models are reasonably good
for noise-free datasets, their objective functions (specially the k-means objective function) are
extremely sensitive to the existence of points far from cluster centers. Therefore, a small number
of very distant data points, called outliers—if not discarded—can dramatically affect the clustering
cost and also the quality of the final clustering solution. Dealing with such outliers is indeed the
main focus of this article.
Clustering with outliers has a natural motivation in applications where outliers and unexpected
data contained in the data may apply a strong influence over the clustering quality. In some specific
applications, we need to find out the anomaly patterns (outliers) in the data for further investigation [29]. This finds applications, for example, in detecting fraudulent usage of credit cards—by
monitoring transactions data to detect exceptional cases perhaps in type of purchase, location,
timeframe, and so on—or in detecting suspicious trades in the equity markets or in monitoring
medical condition or more; see Reference [29] for more details.
We restrict our attention to the outlier version of the three well-studied clustering problems: kmeans with outliers (k-means-out), k-median with outliers (k-median-out), and uncapacitated
facility location with outliers (UFL-out). Formally, in these problems, we are given a set X of
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.
Approximation Schemes for Clustering with Outliers 26:3
n data points in a metric space, a set C of possible centers, and the number of desired outliers z.
Both k-means-out and k-median-out aim at finding k centers C = {c1,...,ck }⊆C and a set of
(up to) z points Z to act as outliers. The objective is to minimize the clustering cost. In k-meansout, this is the sum of squared distances of each data point in X \ Z to its nearest center, i.e.,

x ∈X\Z δ (x,C)
2, while in k-median-out this is just the sum of distances, i.e.,
x ∈X\Z δ (x,C),
where δ (x,C) indicates the distance of point x to its nearest center in C. UFL-out is the same as
k-median-out except that instead of a cardinality constraint, we are given opening cost fc for
each centerc ∈ C. The problem hence consists of finding centers (facilities) C and z outliers Z that
minimizes
x ∈X\Z δ (x,C) +
c ∈C fc . We present PTASs for these problems on doubling metrics
i.e., metrics with fixed doubling dimensions (which include fixed dimension Euclidean metrics as
special case) and shortest path metrics of minor closed graphs (which includes planar graphs as
special case).1 Recall that a metric (V, δ ) has doubling dimension d if each ball of radius 2r can be
covered with 2d balls of radiusr inV . We call it a doubling metric if d can be regarded as a constant;
Euclidean metrics of constant (Euclidean) dimension are doubling metrics.
Despite a very large amount of work on the clustering problems, there has been only little
work on their outlier versions. To the best of our knowledge, the clustering problem with outliers
was introduced by Charikar et al. [13]. They devised a factor 3-approximation for UFL-out and
a bicriteria 4(1 + 1
ϵ )-approximation scheme for k-median-out that drops z(1 + ϵ ) outliers. They
obtained these results via some modifications of the Jain-Vazirani algorithm [33]. The first true
approximation algorithm for k-median-out was given by Chen [15], who obtained this by combining very carefully the Jain-Vazirani algorithm and local search; the approximation factor is not
specified but seems to be a very large constant. Very recently,2 Krishnaswamy et al. [35] used iterative rounding techniques to obtain a (7.081 + ϵ )-approximation algorithm for k-median-out
and a (53.002 + ϵ )-approximation algorithm for k-means-out. It should be noted that their result
provides the first true constant factor approximation of k-means-out.
The first bicriteria approximation algorithm for k-means-out is obtained by Gupta et al. [26].
They devised a bicriteria 274-approximation algorithm for k-means-out that dropsO(kz log(nΔ))
outliers, where Δ denotes the maximum distance between data points. This is obtained by a simple
local search heuristic for the problem.
1.1 Related Work
k-means is one of the most widely studied problems in the computer science literature. The problem is usually considered on d-dimensional Euclidean space Rd , where the objective becomes minimizing the variance of the data points with respect to the centers they are assigned to. The most
commonly used algorithm for k-means is a simple heuristic known as Lloyd’s algorithm (commonly referred to as the k-means algorithm) [41]. Although this algorithm works well in practice,
it is known that the the cost of the solutions computed by this algorithm can be arbitrarily large
compared to the optimum solution [34]. Under some additional assumptions about the initially
chosen centers, however, Arthur and Vassilvitskii [6] show that the approximation ratio of Lloyd’s
algorithm is O(log k). Later, Ostrovsky et al. [44] show that the approximation ratio is bounded
by a constant if the input points obey some special properties. Under no such assumptions,
Kanungo et al. [34] proved that a simple local search heuristic (that swaps only a constant number of centers in each iteration) yields an (9 + ϵ )-approximation algorithm for Euclidean k-means.
1For brevity, we will call such graphs minor closed, understanding this means they belong to a fixed family of graphs that
closed under minors.
2Krishnaswamy et al. made a version of their work publicly available on November 3, 2017, few months after an extended
abstract of this work was submitted to SODA 2018.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.    
26:4 Z. Friggstad et al.
Recently, Ahmadian et al. [1] improved the approximation ratio to 6.357 + ϵ by primal-dual algorithms. For general metrics, Gupta and Tangwongsan [25] proved that the local search algorithm
is a (25 + ϵ )-approximation. This was also recently improved to 9 + ϵ via primal-dual algorithms
[1].
To obtain algorithms with arbitrary small approximation ratios for Euclidean k-means, many
researchers restrict their focus on cases when k or d is constant. For the case when both k and d
are constant, Inaba et al. [30] showed that k-means can be solved in polynomial time. For fixed
k (but arbitrary d), several PTASs have been proposed, each with some improvement over past
results in terms of running time; e.g., see References [18, 21, 27, 28, 36, 37]. Despite a large number
of PTASs for k-means with fixed k, obtaining a PTAS for k-means in fixed-dimensional Euclidean
space had been an open problem for a long time. Bandyapadhyay and Varadarajan [9] presented
a bicriteria PTAS for the problem that finds a (1 + ϵ )-approximation solution that might use up to
(1 + ϵ )k clusters. The first true PTAS for the problem was recently obtained in References [17, 22]
via local search. The authors show that their analysis also works for metrics with fixed doubling
dimension [22] and the shortest path metrics of minor closed graphs [17]. Soon after Cohen-Addad
presented a PTAS with improved running time [16].
There are several constant factor approximation algorithms for k-median in general metrics.
The simple local search (identical with the one for k-means) is known to give a 3 + ϵ approximation by Arya et al. [7, 8]. The current best approximation uses different techniques and has
an approximation ratio of 2.675 + ϵ [11, 40]. For Euclidean metrics, this was recently improved to
2.633 + ϵ via primal-dual algorithms [1]. Arora et al. [5], based on Arora’s quadtree dissection [4],
gave the first PTAS for k-median in fixed dimensional Euclidean metrics. We note that Reference
[5] also gives a PTAS for UFL-out and k-median-out in constant-dimensional Euclidean metrics;
our results for Euclidean metrics in particular are therefore most meaningful for k-means-out. The
recent PTASs (based on local search) for k-median in References [17, 22] work also for doubling
metrics [22] and also for minor-closed metrics [17]. No PTAS or even a bicriteria PTAS was known
for such metrics for even uniform-cost UFL with outliers (Uniform-UFL-out) or k-median-out.
Currently, the best approximation for uncapacitated facility location in general metrics is
a 1.488-approximation [39]. As with k-median, PTASs are known for uncapacitated facility
location in fixed dimensional Euclidean metrics [5], metrics with fixed doubling dimension [22],
and for the shortest path metrics of minor closed graphs [17]; however, the results in Reference
[17] only work for uncapacitated facility location with uniform opening cost. Ahmadian
and Swamy gave approximation algorithms for some clustering problems with lower bounds and
outliers [2]. For approximating the k-center problem with non-uniform capacities see Chakrabarty
et al. [12].
1.2 Our Results
We present a general method for converting local search analysis for clustering problems without
outliers to problems with outliers. Roughly speaking, we preprocess and then aggregate test swaps
used in the analysis of such problems to incorporate outliers. We demonstrate this by applying our
ideas to Uniform-UFL-out, k-median-out, and k-means-out.
A quick comment on the running times of our procedures is in order. In each theorem statement
below, we mention that a ρ-swap local search algorithm provides some approximation where ρ
is some constant (depending on ϵ and perhaps some other quantity like the dimension of the
Euclidean space or the size of an excluded minor). This is an algorithm that tries all possible ways
to close up to ρ centers from the local optimum and open up to ρ centers not currently in the
local optimum. There are |C|O (ρ) such swaps to consider, which is polynomial. The number of
iterations is also polynomial in the input size when using the standard trick of performing a swap
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.
Approximation Schemes for Clustering with Outliers 26:5
only if it improves by a (1 + ϵ/|C|)-factor (mentioned in Section 2), so the overall algorithms run
in polynomial time.
Most of our results are for metrics of fixed doubling dimensions as well as shortest path metrics
of minor-closed graphs. First, we show that on such metrics a simple multi-swap local search
heuristic yields a PTAS for Uniform-UFL-out.
Theorem 1.1. A ρ = ρ(ϵ,d)-swap local search algorithm yields a PTAS for Uniform-UFL-out for
doubling metrics and minor-closed graphs. Here, d is either the doubling constant of the metric or a
constant that depends on a minor that is exclude from the minor-closed family.
We then extend this result to k-median and k-means with outliers (k-median-out and k-meansout) and obtain bicriteria PTASs for them. More specifically:
Theorem 1.2. A ρ = ρ(ϵ,d)-swap local search algorithm yields a bicriteria PTAS for k-medianout and k-means-out on doubling metrics and minor-closed graphs; i.e., finds a solutions of cost at
most (1 + ϵ ) · OPT with at most (1 + ϵ )k clusters that uses at most z outliers where OPT is the cost
of optimum k-clustering with z outliers.
In fact, in minor-closed metrics a true local optimum in the local search algorithm would find a
solution using (1 + ϵ )k clusters with cost at most OPT in both k-median-out and k-means-out,
but a (1 + ϵ )-factor must be lost due to a standard procedure to ensure the local search algorithm
terminates in polynomial time.
We show how these results can be extended to the setting where the metric is the
q
q -norm, i.e.,
cost of connecting two points i, j is δq (i, j) (e.g., k-median-out is when q = 1, k-means-out is
when q = 2). Finally, we show that in general metrics we still recover bicriteria constant-factor
approximation schemes for k-median-out and k-means-out. While not our main result, it gives
much more reasonable constants under bicriteria approximations for these problems.
Theorem 1.3. A 1/ϵO (1)
-swap local search algorithm finds a solution using (1 + ϵ )k clusters and
has cost at most (3 + ϵ ) · OPT for k-median-out or cost at most (25 + ϵ ) · OPT for k-means-out.
As mentioned earlier, before the results of Krishnaswamy et al. [35], a true constant-factor approximation for k-median-out was given by Chen [14], though the unspecified constant seemed
to be very large. More interestingly, even a constant-factor bicriteria approximation scheme for
k-means-out that uses (1 + ϵ )k clusters and discards the correct number of outliers had not been
observed before (recall that the algorithm of Reference [13] for k-median-out has ratioO(1/ϵ ) for
k-median using at most (1 + ϵ )z outliers). It is not clear that Chen’s algorithm can be extended to
give a true constant-factor approximation for k-median-out; one technical challenge is that part
of the algorithm reassigns points multiple times over a series of O(logn) iterations. So it is not
clear that the algorithm in Reference [14] extends to k-means.
To complement these results, we show that for UFL-out (i.e., non-uniform opening costs) any
multi-swap local search has unbounded gap. Also, for k-median-out and k-means-out, we show
that without violating the number of clusters or outliers, any multi-swap local search will have
unbounded gap even on Euclidean metrics.
Theorem 1.4. Multi-swap local search has unbounded gap for UFL-out and for k-median-out
and k-means-out on Euclidean metrics.
Outline of the article: We start with preliminaries and notation. Then in Section 2, we prove
Theorem 1.1. In Section 3, we prove Theorem 1.2 for the case of k-median on doubling metrics and then in Section 4, we show how to extend these theorems to
q
q -norm distances as
well as minor-closed families of graphs. Theorem 1.3 is proven in Section 5. Finally, the proof of
Theorem 1.4 appears in Section 6.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.  
26:6 Z. Friggstad et al.
ALGORITHM 1: uniform-cost UFL ρ
-Swap Local Search
Let S be an arbitrary non-empty subset of C;
while ∃ sets P ⊆C−S, Q ⊆ S with |P |, |Q| ≤ ρ s.t. cost((S − Q) ∪ P) < cost(S) do
S ← (S − Q) ∪ P;
end
return S
1.3 Preliminaries and Notation
In Uniform-UFL-out, we are given a set of X points, a set C of centers and z for the number
of outliers. Our goal is to select a set C ⊂ C to open and a set Z ⊂ X to discard and assign each
j ∈X− Z to the nearest center inC to minimize
x ∈X\Z δ (x,C) + |C|, where δ (x,C) is the distance
of x to nearest c ∈ C. In k-median-out and (discrete) k-means-out, along with X, C, and z, we
have an integer k as the number of clusters. We like to find k centers C = {c1,...,ck }⊆C and a
set of (up to) z points Z to act as outliers. In k-median-out, we like to minimize
x ∈X\Z δ (x,C)
and in k-means-out, we want to minimize
x ∈X\Z δ (x,C)
2.
For all these three problems, if we have the
q
q -norm then we like to minimize
x ∈X\Z δq (x,C).
We should note that in classical k-means (in Rd ), one is not given a candidate set of potential
centers, but they can be chosen anywhere. However, by using the classical result in Reference [43],
at a loss of (1 + ϵ ) factor we can assume we have a set C of “candidate” centers from which the
centers can be chosen (i.e., reduce to the discrete case considered here). This set can be computed
in time O(nϵ−d log(1/ϵ )) and |C| = O(nϵ−d log(1/ϵ )).
2 UNIFORM-COST UFL WITH OUTLIERS IN DOUBLING METRICS
We start with presenting an approximation scheme for Uniform-UFL-out in doubling metrics
(Theorem 1.1). Recall there is already a PTAS for UFL-out in constant-dimensional Euclidean
metrics using dynamic programming for uncapacitated facility location through quadtree
decompositions [5]. However, our approach generalizes to many settings where quadtree decompositions are not known to succeed such as when the assignment cost between a point j and a
center i is δ (j,i)
q for constant 1 < q < ∞ including k-means distances (q = 2, as seen in the next
section) and also to shortest-path metrics of edge-weighted minor-closed graphs. Still, we will
initially present our approximation scheme in this simpler setting to lay the groundwork and introduce the main ideas.
Recall that we are given a set X of points and a set C of possible centers in a metric space
with doubling dimension d and a number z bounding the number of admissible outliers. As the
opening costs are uniform, we may scale all distances and opening costs so the opening cost of a
center is 1. For any ∅  S⊆C, order the points j ∈ X as j
S
1 , j
S
2 ,..., jS
n in nondecreasing order of
distance δ (j
S
i ,S). The cost of S is then cost(S) := n−z
=1 δ (j
S
 ,S) + |S|. That is, after discarding
the z points that are furthest from S the others are assigned to the nearest center in S: We pay
this total assignment cost for all points that are not outliers and also the total center opening cost
|S|. The goal is to find ∅  S⊆C minimizing cost(S).
Let ϵ > 0 be a constant. Let ρ := ρ
(ϵ,d) be some constant we will specify later. We consider a
natural multiswap heuristic for Uniform-UFL-out, described in Algorithm 1.
Each iteration can be executed in time |X| · |C|O (ρ
)
. It is not clear that the number of iterations
is bounded by a polynomial. However, the standard trick from References [7, 34] works in our
setting. That is, in the loop condition we instead perform the swap only if cost((S − Q) ∪ P) ≤
(1 − ϵ/|C|) · cost(S). This ensures the running time is polynomial in the input size as every |C|/ϵ
iterations the cost decreases by a constant factor.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.          
Approximation Schemes for Clustering with Outliers 26:7
Our analysis of the local optimum follows the standard template of using test swaps to generate
inequalities to bound cost(S). The total number of swaps we use to generate the final bound is at
most |C|, so (as in References [7, 34]) the approximation guarantee of a local optimum will only
be degraded by an additional (1 + ϵ )-factor. For the sake of simplicity, in our presentation we will
bound the cost of a local optimum solution returned by Algorithm 1.
2.1 Notation and Supporting Results from Previous Work
We use many results from Reference [22], so we use the same notation. In this section, these results are recalled and a quick overview of the analysis of the multiswap local search heuristic for
uniform-cost UFL is provided; this is simply Algorithm 1, where the cost function is defined appropriately for uniform-cost UFL. Recall that in uncapacitated facility location, each center
i ∈ C has an opening cost fi ≥ 0. Also let cost(S) =
j ∈X δ (j,S) +
i ∈S fi . In uniform-cost UFL,
all opening costs are uniform.
Let S be a local optimum solution returned by Algorithm 1 and O be a global optimum solution.
As it is standard in local search algorithms for uncapacitated clustering problems, we may assume
S∩O = ∅. This can be assumed by duplicating each i ∈ C, asserting S uses only the original
centers and O uses only the copies. It is easy to see S would still be a local optimum solution. Let
σ : X→S be the point assignment in the local optimum and σ∗ : X→O be the point assignment
in the global optimum. For j ∈ X, letcj = δ (j,S) and c∗
j = δ (j, O) (remember there are no outliers
in this review in Reference [22]).
For each i ∈S∪O, let Di be the distance from i to the nearest center of the other type. That
is, for i ∈ S let Di = δ (i, O) and for i
∗ ∈ O let Di ∗ = δ (i
∗,S). Note for every j ∈ X and every i ∈
{σ (j), σ∗ (j)} that
Di ≤ δ (σ (j), σ∗ (j)) ≤ c∗
j + cj . (1)
The following lemma is is restated from Reference [22]:
Lemma 2.1 (Lemma 3 in Reference [22], Paraphrased). There is a special pairing T ⊆S×O
such that each i ∈S∪O appears at most once among pairs in T and for any A ⊆S∪O containing
at least one center from every pair in T , δ (i,A) ≤ 5 · Di for every i ∈S∪O.
Next, a net is cast around each i ∈ S. The idea is that any swap that hasi ∈ S being closed would
have something open near every i
∗ ∈ O that itself is close to i. More precisely, Reference [22]
identifies a set N ⊆S×O with the following properties. For each i ∈ S and i
∗ ∈ O with δ (i,i
∗) ≤
Di /ϵ and Di ∗ ≥ ϵ · Di there is some pair (i,i
) ∈ N with δ (i
,i
∗) ≤ ϵ · Di ∗ . The set N contains
further properties to enable Theorem 2.2 (below), but these are sufficient for our discussion.
The last major step in Reference [22] before the final analysis was to provide a structure theorem
showing S∪O can be partitioned into test swaps that mostly allow the redirections discussed
above.
Theorem 2.2 (Theorem 4 in Reference [22], Slightly Adjusted). For any ϵ > 0, there is a
constant ρ := ρ(ϵ,d) and a randomized algorithm that samples a partitioning π of O∪S such that:
• For each part P ∈ π, |P ∩ O|, |P ∩ S| ≤ ρ.
• For each part P ∈ π, SP includes at least one center from every pair in T , where  indicates
the symmetric difference operation.
• For each (i
∗,i) ∈ N , Pr[i,i
∗ lie in different parts of π] ≤ ϵ.
There is only a slight difference between this statement and the original statement in Reference
[22]. Namely, the first condition of Theorem 4 in Reference [22] also asserted |P ∩ O| = |P ∩ S|. As
noted at the end of Reference [22], this part can be dropped by skipping one final step of the proof
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.  
26:8 Z. Friggstad et al.
that “balanced” parts of the partition that were constructed (also see Reference [23] for further
details).
The analysis in Reference [22] used Theorem 2.2 to show cost(S) ≤ (1 + O(ϵ )) · OPT generated
an inequality by swapping each part P ∈ π. Roughly speaking, for each j ∈ X with probability at
least 1 − ϵ (over the random construction of π), the swap that closes σ (j) will open something
very close to σ (j) or very close to σ∗ (j). With the remaining probability, we can at least move j a
distance of at most O(c∗
j + cj). Finally, if j was never moved to something that was close to σ∗ (j)
this way, then we ensure we move j from σ (j) to σ∗ (j) when σ∗ (j) is swapped in.
In our analysis for clustering with outliers, our reassignments for points that are not outliers in
either the local or global optimum are motivated by this approach. Details will appear below in
our analysis of Algorithm 1.
2.2 Analysis for Uniform-Cost UFL with Outliers: An Outline
Now let S be a locally optimum solution for Algorithm 1, let Xa be the points in X that are assigned
to S and Xo be the points in X that are outliers when opening S. Similarly, let O be a globally
optimum solution, and let Xa∗
be the points in X that are assigned to O and Xo∗
be the points in
X that are outliers when opening O. Note |Xo | = |Xo∗
| = z.
Let σ : X → S ∪ {⊥} assign j ∈ Xa to the nearest center in S and j ∈ Xo to ⊥. Similarly, let
σ∗ : X → O ∪ {⊥} map each j ∈ Xa∗
to the nearest center in O and each j ∈ Xo∗
to ⊥. For j ∈ X,
we let cj = 0 if j ∈ Xo and, otherwise, let cj = δ (j,S) = δ (j, σ (j)). Similarly, let c∗
j = 0 if j ∈ Xo∗
and, otherwise, let c∗
j = δ (j, O) = δ (j, σ (j)).
Our starting point is the partitioning scheme described in Theorem 2.2. The new issue to be
handled is in reassigning the outliers when a part is swapped. That is, for any j ∈ Xo∗
any swap
that has σ (j) swapped out cannot, in general, be reassigned anywhere cheaply.
Really the only thing we can do to upper bound the assignment cost change for j is to make it
an outlier. We can try assigning each j ∈ Xo to σ∗ (j) if it is opened, thereby allowing one j ∈ Xo∗
with σ (j) being swapped out to become an outlier. However, there may not be enough j
 ∈ Xo that
have σ∗ (j

) opened after the swap. That is, we might not remove enough outliers from the solution
S to be able to let all such j become outliers.
Our approach is to further combine parts P of the partition π and perform the swaps simultaneously for many of these parts. Two of these parts in a larger grouping will not actually be swapped
out: Their centers in O will be opened to free up more spaces for outliers yet their centers in S
will not be swapped out. Doing this carefully, we ensure that the total number of j ∈ Xo∗
that have
σ (j) being closed is at most the total number of j ∈ Xo that have σ∗ (j) being opened.
These larger groups that are obtained by combining parts of π are not disjoint. However, the
overlap of centers between larger groups will be negligible compared to |S| + |O|.
2.3 Grouping the Parts
We will assume Xo ∩ Xo∗
= ∅. This is without loss of generality as S would still be a local optimum
in the instance with Xo ∩ Xo∗
removed and z adjusted. Recall we are also assuming S∩O = ∅.
For each part P of π, let ΔP := |{j ∈ Xo : σ∗ (j) ∈ P}| − |{j ∈ Xo∗
: σ (j) ∈ P}|. This is the difference between the number of outliers we can reclaim by moving them to σ∗ (j) (if it is open after
swapping P) and the number of outliers j that we must create, because σ (j) was closed when
swapping P.
Consider the following refinements of π: π+ = {P ∈ π : ΔP > 0}, π− = {P ∈ π : ΔP < 0}, and
π0 = {P ∈ π : ΔP = 0}. Intuitively, nothing more needs to be done to prepare parts P ∈ π0 for swapping, as this would create as many outliers as it would reclaim in our analysis framework. We work
toward handling π+ and π−.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.
Approximation Schemes for Clustering with Outliers 26:9
Fig. 1. The bipartite graph with sides π+, π− and edges P. The edges are ordered left to right in order of
their creation time, so e0 is the leftmost edge. A grouping with group size α = 6 is depicted. The edges in
E1 and E3 are bold. Note the last group has more than α edges. The parts that are split by some group are
shaded. While not depicted, a part could be split by many groups (if it has very high degree in the bipartite
graph).
Next we construct a bijection κ : Xo → Xo∗
. We will ensure when σ (j) is swapped out for some
j ∈ Xo∗
that σ∗ (κ−1 (j)) will be swapped in. So there is space to make j an outlier in the analysis.
There are some cases in our analysis where we never swap out σ (j) for some j ∈ Xo∗
, but we will
still ensure σ∗ (κ−1 (j)) is swapped in at some point so we can still make j an outlier while removing
κ−1 (j) as an outlier to get the negative dependence on cj in the final inequality.
To start defining κ, for each P ∈ π we pair up points in {j ∈ Xo∗
: σ (j) ∈ P} and {j ∈ Xo : σ∗ (j) ∈
P} arbitrarily until one of these two groups is exhausted. These pairs define some mapping of κ.
The number of unpaired points in {j ∈ Xo∗
: σ (j) ∈ P} is exactly −ΔP if ΔP < 0 and the number of
unpaired points in {j ∈ Xo : σ∗ (j) ∈ P} is exactly ΔP .
Having done this for each P, we begin pairing unpaired points in Xo ∪ Xo∗
between parts. Arbitrarily order π+ as P+
1 , P+
2 ,..., P+
m and π− as P−
1 , P−
2 ,..., P−
 . We will complete the pairing κ and
also construct edges in a bipartite graph H with π+ on one side and π− on the other side using Algorithm 2. To avoid confusion with edges in the distance metric, we call the edges of H between π+
and π− superedges. Note that in first line of the while loop in 2, we are, in fact, pairing up unpaired
points between {j ∈ Xo : σ∗ (j) ∈ P+
a } and {j ∈ Xo∗
: σ (j) ∈ P−
b }. This follows from the definitions
of π+ and π−.
In Algorithm 2, we say a part P ∈ π+ ∪ π− has an unpaired point j ∈ Xo∗
∪ Xo and that j is an
unpaired point of P if, currently, κ has not paired j and σ (j) ∈ P or σ∗ (j) ∈ P (whatever is relevant).
The resulting graph over π+, π− is depicted in Figure 1, along with other features described below.
We now group some superedges together. Let α = 4ρ/ϵ denote the group size. We will assume
|P| > α, as otherwise |π+ ∪ π−| ≤ 2α, and we could simply merge all parts in π+ ∪ π− into a single
part P with ΔP = 0 with |P ∩ S|, |P ∩ O| ≤ 2αρ. The final local search algorithm will use swap
sizes greater than 2αρ and the analysis showing cost(S) ≤ (1 + O(ϵ )) · cost(O) would then follow
almost exactly as we show.3
Order edges e0, e1,..., e |P |−1 of P according to when they were formed. For each integer s ≥ 0,
let Es = {ei : α · s ≤ i < α · (s + 1)}. Let s be the largest index with Es+1  ∅ (which exists by the
assumption |P| > α). Merge the last two groups by replacing Es with Es ∪ Es+1. Finally, for each
0 ≤ s ≤ s let Gs ⊆S∪O consist of all centersi ∈S∪O belonging to a part P that is an endpoint
of some superedge in Es . The grouping is G = {Gs : 0 ≤ s ≤ s
}. The groups of superedges Es are
depicted in Figure 1. Note each part P ∈ π+ ∪ π− is contained in at least one group of G because
3One very minor modification is that the presented proof of Lemma 2.6 in the final analysis ultimately uses |P| ≥ α. It
still holds otherwise in a trivial way as there would be no overlap between groups.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.  
26:10 Z. Friggstad et al.
ALGORITHM 2: Pairing Unpaired Outliers
P←∅  A set of superedges between π+ and π−;
a ← 1,b ← 1;
while there are unpaired points do
Arbitrarily pair up (via κ) unpaired points between P+
a and P−
b until no longer possible;
P←P∪{(P+
a , P−
b )};
if P+
a has no unpaired point then
a ← a + 1;
end
if P−
b has no unpaired point then
b ← b + 1;
end
end
return P;
ΔP  0 (so a superedge edge in P was created with P as an endpoint). We first show the following
lemma.
Lemma 2.3. For each Gs ∈ G, α − 1 ≤ |Gs | ≤ 8ρα.
Proof. The upper bound is simply because the number of parts used to form Gs is at most
2|Es | ≤ 4α and each part has at most 2ρ centers. For the lower bound, we argue the graph (π+ ∪
π−, P) is acyclic. If so, then (Gs , Es ) is also acyclic and the result follows, because any acyclic graph
(V, E) has |V |≥|E| − 1 (and also the fact that |P | ≥ 1 for any part).
To show (π+ ∪ π−, P) is acyclic, we first remove all nodes (and incident edges) with degree
1. Call this graph H. After this, the maximum degree of a vertex/part in H is at least 2. To see
this, note for any P ∈ π+ ∪ π−, the incident edges are indexed consecutively by how the algorithm
constructed super edges. If ei, ei+1,..., ej denotes these super edges incident to P, then, again
by how the algorithm constructed super edges, for any i < c < j the other endpoint of ec only
appeared in one iteration so the corresponding part has degree 1.
Finally, consider any simple path in H with at least four vertices starting in π+ and ending in
π−. Let P+
a1
, P−
b1
, P+
a2
, P−
b2
,..., P+
ac , P−
bc be, in order, the parts visited by the path. Then both the ai
and bi sequences are strictly increasing or strictly decreasing. As c ≥ 2 (because there are at least
four vertices on the path) then either (i) a1 < ac and b1 < bc or (ii) a1 > ac and b1 > bc . Suppose,
without loss of generality, it is the former case. There is no edge of the form (P+
a1
, P−
bc
) in P, because
the only other edge (P+
a , P−
bc
) incident to P−
bc besides (P+
ac , P−
bc
) has a > ac .
Ultimately, this shows there is no cycle in H. As H is obtained by removing only the degree-1
vertices of (π+ ∪ π−, P), this graph is also acyclic.
Note: In fact, it is not hard to see that H is a forest where each component of it is a caterpillar
(a tree in which every vertex is either a leaf node or is adjacent to a “stalk” node; stalk nodes form
a path). The parts P that are split between different groups Gs are the high degree nodes in H and
these parts belong to the “stalk” of H.
Definition 1. For each group Gs ∈ G, say a part P ∈ π is split by Gs if P ⊆ Gs and NP (P)  Es ,
where NP (P) are those parts that have a superedge to P.
We simply say P is split if the group Gs is clear from the context.
Lemma 2.4. For each Gs ∈ G, there are at most two parts P split by Gs .
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.    
Approximation Schemes for Clustering with Outliers 26:11
Proof. Say Es = {ei, ei+1,..., ej}. First consider some index i < c < j, where ec has a split endpoint P. Then some edge ec incident to P is not in Es so either c < i or c > j. Suppose c < i;
the other case c > j is similar. By construction, all edges in NP (P) for any P ∈ π+ ∪ π− appear
consecutively. As c < i < c and ec, ec ∈ NP (P), then ei ∈ NP (P). Thus, the only split parts are
endpoints of either ei or ej .
We claim that if both endpoints of ei are split, then ei and ej share an endpoint and the other
endpoint of ej is not split. Say ei−1 = (P+
ai−1
, P−
bi−1
). Then either P+
ai−1  P+
ai or P−
bi−1  P−
bi
. Suppose it
is the former case, the latter again being similar. So if P+
ai is split, it must be that ej+1 has P+
ai as an
endpoint. Therefore, ei and ej share P+
ai as an endpoint. Further, since ej+1 has P+
ai as an endpoint,
then NP (P−
bj
) = {ej}, so P−
bj
, the other endpoint of ej , has degree 1 and so is not split.
Similarly, if both endpoints of ej are split, then one is in common with ei and the other endpoint
of ei is not split. Overall, we see Gs splits at most two vertices.
2.4 Analyzing Algorithm 1
Now suppose we run Algorithm 1 using ρ = 4αρ. For each P ∈ π0, extend G to include a “simple”
group Gs = P for each P ∈ π0, where s is the next unused index. Any P ∈ π0 is not split by any
group. Each P ∈ π is then contained in at least one group of G and |Gs ∩ S|, |Gs ∩ O| ≤ ρ for each
Gs ∈ G by Lemma 2.3.
We are now ready to describe the swaps used in our analysis of Algorithm 1. Simply, forGs ∈ G,
let Ss be the centers in Gs ∩ S that are not in a part P that is split and let Os simply be Gs ∩ O.
We consider the swap S → (S−Ss ) ∪ Os .
To analyze these swaps, we further classify each j ∈ Xa ∩ Xa∗ in one of four ways. This is the
same classification from Reference [22]. Label j according to the first property that it satisfies
below.
• lucky: both σ (j) and σ∗ (j) in the same part P of π.
• long: δ (σ (j), σ∗ (j)) > Dσ (j)/ϵ.
• good: either Dσ ∗ (j) ≤ ϵDσ (j) or there is some i ∈ O with δ (σ∗ (j),i
) ≤ ϵ · Dσ ∗ (j) and
(σ (j),i
) ∈ N where both σ (j) and i lie in the same part.
• bad: j is not lucky, long, or good. Note, by Theorem 2.2, that Pr[j is bad] ≤ ϵ over the
random construction of π.
Finally, as a technicality for each j ∈ Xa ∩ Xa∗
, where σ∗ (j) lies in a part that is split by some
group and j is either lucky or long, let s(j) be any index such that group Gs (j) ∈ G contains the
part with σ∗ (j). The idea is that we will reassign j to σ∗ (j) only when group Gs (j) is processed.
Similarly, for any j ∈ Xo , let s(j) be any index such that σ∗ (j), σ (κ(j)) ∈ Gs (j). The following
lemma shows this is always possible.
Lemma 2.5. For each j ∈ Xo there is at least one group Gs ∈ G, where σ∗ (j), σ (κ(j)) ∈ Gs .
Proof. If σ∗ (j) and σ (κ(j)) lie in the same part P, then this holds, because each part is a subset
of some group. Otherwise, j is paired with κ(j) at some point in Algorithm 2, and an edge (P+
a , P−
b )
is added to P, where σ∗ (j) ∈ P+
a , σ (κ(j)) ∈ P−
b . The centers in both endpoints of this super edge
were added to some group Gs .
We now place a bound on the cost change in each swap. Recall |Gs ∩ S|, |Gs ∩ O| ≤ ρ and S is
a local optimum, so
0 ≤ cost((SSs ) ∪ Os ) − cost(S).
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.    
26:12 Z. Friggstad et al.
We describe a feasible reassignment of points to upper bound the cost change. This may result in
some points in Xa becoming outliers and other points in Xo now being assigned. We take care to
ensure the number of points that are outliers in our reassignment is exactly z, as required.
Consider the following instructions describing one possible way to reassign a point j ∈ X when
processing Gs . This may not describe an optimal reassignment, but it places an upper bound on
the cost change. We begin by describing how to reassign points in Xo ∪ Xo∗
. Recall they are paired
via κ and Xo ∩ Xo∗
= ∅.
• For each j ∈ Xo such that s = s(j), make κ(j) an outlier and connect j to σ∗ (j), which is
now open. The total assignment cost change for j and κ(j) is c∗
j − cκ(j).
So far, this reassignment still uses z outliers, because each new outlier j
 has its paired point κ−1 (j

)
become connected. Observe that, in total over all swaps, each j ∈ Xo will have κ(j) becoming an
outlier and have j connected to σ∗ (j) in precisely one swap.
The rest of the analysis will not create any more outliers, it will simply reassign some clients.
The remaining cases essentially repeat the reassignment directions from Reference [22]. Consider
some j ∈ Xa ∩ Xa∗
and move it according to the appropriate case below.
• If j is lucky or long and s = s(j), then reassign j from σ (j) to σ∗ (j). The assignment cost
change for j is c∗
j − cj .
• If j is long, then move j to the open center that is nearest to σ (j). By Lemma 2.1 and because
j is long, the assignment cost increase for j can be bounded as follows:
5 · Dσ (j) ≤ 5ϵ · δ (σ (j), σ∗ (j)) ≤ 5ϵ · (c∗
j + cj).
• If j is good and Dσ ∗ (j) ≤ ϵ · Dσ (j), then move j to the open center that is nearest to σ∗ (j).
By Equation (1) in Section 2.1 and Lemma 2.1, the assignment cost increase for j can be
bounded as follows:
c∗
j + 5 · Dσ ∗ (j) − cj
≤ c∗
j + 5ϵ · Dσ (j) − cj
≤ c∗
j + 5ϵ · (c∗
j + cj) − cj
= (1 + 5ϵ ) ·c∗
j − (1 − 5ϵ ) ·cj .
• If j is good but Dσ ∗ (j) > ϵ · Dσ (j), then let i be such that σ (j),i both lie in Gs and
δ (σ∗ (j),i
) ≤ ϵ · Dσ ∗ (j). Reassigning j from to i bounds its assignment cost change by
c∗
j + δ (σ∗ (j),i

) − cj
≤ c∗
j + ϵ · Dσ ∗ (j) − cj
≤ (1 + ϵ ) ·c∗
j − (1 − ϵ ) ·cj .
• If j is bad, then simply reassign j to the open center that is nearest to σ (j). By Equation (1)
and Lemma 2.1, the assignment cost for j increases by at most 5 · Dσ (j) ≤ 5 · (c∗
j + cj). This
looks large, but its overall contribution to the final analysis will be scaled by an ϵ-factor,
because a point is bad only with probability at most ϵ over the random sampling of π.
Note this accounts for all points j where σ (j) is closed (among others). Every other point j that is
not moved according to one of the cases above may stay assigned to σ (j) to bound its assignment
cost change by 0.
For j ∈ X, let Δj denote the total reassignment cost change over all swaps when moving j as
described above. This should not be confused with the previously used notation ΔP for a part
P ∈ π. We bound Δj on a case-by-case basis below.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.
Approximation Schemes for Clustering with Outliers 26:13
• If j ∈ Xo , then the only time j is moved is for the swap involving Gs (j). So Δj = c∗
j .
• If j ∈ Xo∗
, then the only time j is moved is for the swap involving Gs (κ−1 (j)). So Δj = −cj .
• If j is lucky, then it is only moved when Gs (j) is processed so Δj = c∗
j − cj .
• If j is long, then it is moved to σ∗ (j) when Gs (j) is processed and it is moved near σ (j)
when σ (j) is closed, so Δj ≤ c∗
j − cj + 5ϵ · (c∗
j + cj) = (1 + 5ϵ ) ·c∗
j − (1 − 5ϵ ) ·cj .
• If j is good, then it is only moved when σ (j) is closed so Δj ≤ (1 + 5ϵ ) ·c∗
j − (1 − 5ϵ ) ·cj .
• If j is bad, then it is only moved when σ (j) is closed so Δj ≤ 5 · (c∗
j + cj).
To handle the center opening cost change, we use the following fact.
Lemma 2.6.
Gs ∈G |Os | − |Ss | ≤ (1 + 2ϵ ) · |O| − (1 − 2ϵ ) · |S|
Proof. For each Gs ∈ G, let Ps be the union of all parts used to form Gs that are not split by
Gs . Let Ps = Gs − Ps , these are centers in Gs that lie in a part split by Gs .
Now, |Ps | ≤ 2ρ, because at most two parts are split byGs by Lemma 2.4. However, by Lemmas 2.3
and 2.4 there are at least α − 3 parts that were used to form Gs that were not split by Gs . As each
part contains at least one center, then |Ps | ≥ α − 3. Thus, for small-enough ϵ we have
|Ps | ≤ 2ρ ≤ ϵ · (α − 3) ≤ ϵ |Ps |.
Note
Gs ∈G |Ps | ≤ |S| + |O|, because no center appears in more than one set of the form Ps .
Also note |Os |≤|Ps ∩ O| + |Ps | and |Ss |≥|Gs ∩ S| − |Ps |,
Gs ∈G
|Os | − |Ss | ≤
Gs ∈G
|Ps ∩ O| − |Gs ∩ S| + 2ϵ |Ps |
≤ |O| − |S| + 2ϵ · (|O| + |S|).
Putting this all together,
0 ≤

Gs ∈G
cost((S−Ss ) ∪ Os ) − cost(S)
≤

j ∈X
Δj +

Gs ∈G
|Os | − |Ss |
≤

j ∈ Xa ∩ Xa∗
j is not bad

(1 + 5ϵ ) ·c∗
j − (1 − 5ϵ )cj

+

j bad
5(c∗
j + cj) +

j ∈Xo
c∗
j −

j ∈Xo∗
cj
+ (1 + 2ϵ ) · |O| − (1 − 2ϵ ) · |S|.
This holds for any π supported by the partitioning scheme described in Theorem 2.2. Taking
expectations over the random choice of π and recalling Pr[j is bad] ≤ ϵ for any j ∈ Xa ∩ Xa∗
,
0 ≤

j ∈X

(1 + 10ϵ ) ·c∗
j − (1 − 10ϵ ) ·cj

+ (1 + 2ϵ ) · |O| − (1 − 2ϵ ) · |S|.
Rearranging and relaxing slightly further shows
(1 − 10ϵ ) · cost(S) ≤ (1 + 10ϵ ) · cost(O).
Ultimately, cost(S) ≤ (1 + 30ϵ ) · OPT .
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.                 
26:14 Z. Friggstad et al.
ALGORITHM 3: k-median ρ
-Swap Local Search
Let S be an arbitrary set of (1 + ϵ )k centers from C;
while ∃ sets P ⊆C−S, Q ⊆ S with |P |, |Q| ≤ ρ s.t. cost((S − Q) ∪ P) < cost(S) and
|(S − Q) ∪ P | ≤ (1 + ϵ )k do
S ← (S − Q) ∪ P;
end
return S;
3 k-MEDIAN AND k-MEANS WITH OUTLIERS
In this section, we show how the results of the previous section can be extended to get a bicriteria approximation scheme for k-median-out and k-means-out with outliers in doubling metrics
(Theorem 1.2). For ease of exposition we present the result for k-median-out. More specifically,
given a set X of points, set C of possible centers, positive integers k as the number of clusters,
and z as the number of outliers for k-median, we show that a ρ
-swap local search (for some
ρ = ρ(ϵ,d) to be specified) returns a solution of cost at most (1 + ϵ )OPT using at most (1 + ϵ )k
centers (clusters) and has at most z outliers. Note that a local optimum S satisfies |S| = (1 + ϵ ) · k
unless cost(S) = 0 already, in which case our analysis is already done. Extension of the result to
k-means or in general to a clustering where the distance metric is the
q
q -norm is fairly easy and
is discussed in the next section, where we also show how we can prove the same result for the
shortest path metric of minor-closed families of graphs.
The proof uses ideas from both [22] for the PTAS for k-means as well as the results of the
previous section for Uniform-UFL-out. LetS be a local optimum solution returned by Algorithm 3
that uses at most (1 + ϵ )k clusters and has at most z outliers, and let O be a global optimum solution
to k-median-out with k clusters and z outliers. Again, we use σ : X → S ∪ {⊥} and σ∗ : X →
O ∪ {⊥} as the assignment of points to cluster centers, where σ (j) = ⊥ (or σ∗ (j) = ⊥) indicates
an outlier. For j ∈ X, we use cj = δ (j,S) and c∗
j = δ (j, O). The special pairing T ⊆S×O and
Lemma 2.1 as well as the notion of nets N ⊆S∪O are still used in our setting. A key component
of our proof is the following slightly modified version of Theorem 4 in Reference [22]:
Theorem 3.1. For any ϵ > 0, there is a constant ρ = ρ(ϵ,d) and a randomized algorithm that
samples a partitioning π of S∪O such that:
• For each part P ∈ π, |P ∩ O| < |P ∩ S| ≤ ρ
• For each part P ∈ π, SΔP contains at least one center from every pair in T
• For each (i
∗,i) ∈ N , Pr[i,i
∗ lie in different parts of π] ≤ ϵ.
The only difference of this version and the one in Reference [22] is that in Theorem 4 in Reference [22], for the first condition we have |P ∩ O| = |P ∩ S| ≤ ρ. The theorem was proved by
showing a randomized partitioning that satisfies conditions 2 and 3. To satisfy the first condition
|P ∩ O| = |P ∩ S| ≤ ρ a balancing step was performed at the end of the proof that would merge
several (but constant) number of parts to get parts with equal number of centers from S and O
satisfying the two other conditions. Here, since |S| = (1 + ϵ )k and |O| = k, we can modify the
balancing step to make sure that each part has at least one more center from S than from O.
Proof Sketch. Here we show how we can modify proof of Theorem 4 in Reference [22] to
prove Theorem 3.1. The proof of Theorem 4 in Reference [22] starts by showing the existence of a
randomized partitioning scheme of S∪O, where each part has size at most (d/ϵ )
Θ(d/ϵ )
, satisfying
the second and third conditions. We can ensure that each part has size at least 1/ϵ by merging
small parts if needed. That part of the proof remains unchanged. Let us call the parts generated
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019. 
Approximation Schemes for Clustering with Outliers 26:15
so far P1,..., P. Then we have to show how we can combine constant number of parts to satisfy
condition 1 for some ρ = ρ(ϵ,d). Since we have (1 + ϵ )k centers in S and k centers in O, we can
simply add one “dummy” optimum center to each part Pi so that each part has now one dummy
center, noting that  < ϵk (because each part has size at least 1/ϵ). We then perform the balancing
step of proof of Theorem 4 in Reference [22] to obtain parts of size ρ = (d/ϵ )
Θ(d/ϵ ) with each part
having the same number of centers from S and O satisfying conditions 2 and 3. Removing the
“dummy” centers, we satisfy condition 1 of Theorem 3.1.
We define Xa, Xo, Xa∗
, and Xo∗
as in the previoius section for Uniform-UFL-out. Note that
|Xo | = |Xo∗
| = z. As before, we assume Xo ∩ Xo∗
= ∅ and S∩O = ∅. Let π be a partitioning as
in Theorem 3.1. Recall that for each P ∈ π, ΔP := |{j ∈ Xo : σ∗ (j) ∈ P}| − |{j ∈ Xo∗
: σ (j) ∈ P}|;
we define π+, π−, π0 to be the parts with positive, negative, and zero ΔP values. We define the
bijection κ : Xo → Xo∗
as before: For each P ∈ π, we pair up points (via κ) in {j ∈ Xo∗
: σ (j) ∈ P}
and {j ∈ Xo : σ∗ (j)} ∈ P arbitrarily. Then (after this is done for each P) we begin pairing unpaired
points in Xo ∪ Xo∗
between groups using Algorithm 2. The only slight change w.r.t. the previous
section is in the grouping the super-edges of the bipartite graph defined over π+ ∪ π−: We use
parameter α = 2ρ + 3 instead. Lemmas 2.3 and 2.4 still hold.
Suppose we run Algorithm 3 with ρ = αρ. As in the case of Uniform-UFL-out, we add each
P ∈ π0 as a separate group to G and so each P ∈ π is now contained in at least one group Gs ∈ G
and |Gs ∩S|, |Gs ∩ O| ≤ ρ
. For the points j ∈ Xa ∩ Xa∗
(i.e., those that are not outlier in neither
S nor O), the analysis is pretty much the same as the PTAS for standard k-means (without outliers)
in Reference [22]. We need extra care to handle the points j that are outliers in one of S or O (recall
that Xo ∩ Xo∗
= ∅).
As in the analysis of Uniform-UFL-out, for Gs ∈ G, let Ss be the centers in Gs ∩ S that are not
in a part P that is split, and Os = Gs ∩ O; consider the test swap that replacesS with (S−Ss ) ∪ Os .
To see this is a valid swap considered by our algorithm, note that the size of a test swap is at most
ρ
. Furthermore, there are at least α − 3 unsplit parts P and at most two split parts in each Gs ,
and each unsplit part has at least one more center from S than O (condition 1 of Theorem 3.1);
therefore, there are at least α − 3 more centers that are swapped out in Ss , and these can account
for the at most 2ρ centers ofS∪O in the split parts that are not swapped out (note that α − 3 = 2ρ).
Thus, the total number of centers of S and O after the test swap is still at most (1 + ϵ )k and k,
respectively.
We classify each j ∈ Xa ∩ Xa∗
to lucky, long, good, and bad in the same way as in the case of
Uniform-UFL-out. Furthermore,s(j) is defined in the same manner: For each j ∈ Xa ∩ Xa∗
where
σ∗ (j) lies a part that is split by some group and j is either lucky or long, let s(j) be any index such
that group Gs (j) ∈ G contains the part with σ∗ (j). Similarly, for any j ∈ Xo let s(j) be any index
such that σ∗ (j), σ (κ(j)) ∈ Gs (j) (note that since Xo ∩ Xo∗
= ∅, if j ∈ Xo , then j ∈ Xa∗
). Lemma 2.5
still holds. For each Gs ∈ G, since |Gs ∩ S|, |Gs ∩ O| ≤ ρ and S is a local optimum, any test swap
based on a group Gs is not improving, hence 0 ≤ cost((SSs ) ∪ Os ) − cost(S).
For each test swap Gs , we describe how we could re-assign each point j for which σ (j) becomes
closed and bound the cost of each re-assignment depending on the type of j. This case analysis is
essentially the same as the one we had for Uniform-UFL-out. Note that the points in Xo ∪ Xo∗
are paired via κ.
Below we specify what to do for each point j ∈ Xo and κ(j) together.
• If j ∈ Xo and s = s(j), then by Lemma 2.5 σ∗ (j) ∈ Gs and it is open, we assign j to σ∗ (j)
and make κ(j) an outlier. The total assignment cost change for j and κ(j) will be c∗
j − cκ(j).
The subsequent cases are when j ∈ Xa ∩ Xa∗
.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.   
26:16 Z. Friggstad et al.
• If j is lucky and s = s(j), then we reassign j from σ (j) to σ∗ (j). The total reassignment cost
change is c∗
j − cj .
• If j is long, then we assign j to the nearest open center to σ (j). Again using Lemma 2.1 and
since j is long, the total cost change of the reassignment is at most
5 · Dσ (j) ≤ 5ϵ · δ (σ (j), σ∗ (j)) ≤ 5ϵ · (c∗
j + cj).
• If j is good and Dσ ∗ (j) ≤ ϵ · Dσ (j), then we assign j to the open center that is nearest to
σ∗ (j). By Equation (1) in Section 2.1 and Lemma 2.1, the assignment cost change for j is at
most
c∗
j + 5 · Dσ ∗ (j) − cj ≤ c∗
j + 5ϵ · Dσ (j) − cj
≤ c∗
j + 5ϵ · (c∗
j + cj) − cj
≤ (1 + 5ϵ ) ·c∗
j − (1 − 5ϵ ) ·cj .
• If j is good but Dσ ∗ (j) > ϵ · Dσ (j), then let i be such that σ (j),i both lie in Gs and
δ (σ∗ (j),i
) ≤ ϵ · Dσ ∗ (j). Reassigning j from σ (j) to i bounds its assignment cost change
by
c∗
j + δ (σ∗ (j),i

) − cj ≤ c∗
j + ϵ · Dσ ∗ (j) − cj ≤ (1 + ϵ ) ·c∗
j − (1 − ϵ ) ·cj .
• Finally, if j is bad, then simply reassign j to the open center that is nearest to σ (j). By
Equation (1) and Lemma 2.1, the total reassignment cost change for j is at most
5 · Dσ (j) ≤ 5 · (c∗
j + cj).
For every point j for which σ (j) is still open, we keep it assigned to σ (j). Considering all cases,
if Δj denotes the net cost change for re-assignment of j ∈ X, then:
• If j ∈ Xo , then the only time j is moved is for the swap involving Gs (j) and Δj = c∗
j .
• If j ∈ Xo∗
, then the only time j is moved is for the swap involving Gs (κ−1 (j)). So Δj = −cj .
• If j is lucky, then it is only moved when Gs (j) is processed so Δj = c∗
j − cj .
• If j is long, then it is moved to σ∗ (j) when Gs (j) is processed and it is moved near σ (j)
when σ (j) is closed, so
Δj ≤ c∗
j − cj + 5ϵ · (c∗
j + cj) = (1 + 5ϵ ) ·c∗
j − (1 − 5ϵ ) ·cj .
• If j is good, then it is only moved when σ (j) is closed so Δj ≤ (1 + 5ϵ ) ·c∗
j − (1 − 5ϵ ) ·cj .
• If j is bad, then it is only moved when σ (j) is closed so Δj ≤ 5 · (c∗
j + cj).
Considering all Gs ∈ G (and considering all test-swaps),
0 ≤

Gs ∈G
cost((S−Ss ) ∪ Os ) − cost(S)
≤

j ∈X
Δj
≤

j ∈ Xa ∩ Xa∗
j is not bad

(1 + 5ϵ ) ·c∗
j − (1 − 5ϵ )cj

+

j bad
5(c∗
j + cj) +

j ∈Xo
c∗
j −

j ∈Xo∗
cj . (2)
Using the fact that the probability of a point j being bad is at most ϵ we get
0 ≤

j ∈X

(1 + 10ϵ ) ·c∗
j − (1 − 10ϵ ) ·cj

.
Rearranging and relaxing slightly further shows the same bound cost(S) ≤ (1 + 30ϵ ) · OPT .
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.           
Approximation Schemes for Clustering with Outliers 26:17
4 EXTENSIONS
4.1 Extension to
q
q
-Norms
In this section, we show how the results for Uniform-UFL-out and k-median-out can be extended to the setting where distances are
q
q -norm for any q ≥ 1. For example, if we have 2
2-norm
distances instead of 1
1, then we get k-means (instead of k-median). Let us consider modifying the
analysis of k-median-out to the
q
q -norm distances. In this setting, for any local and global solutions S and O, let δj = δ (j, σ (j)) and δ ∗
j = δ (j, σ∗ (j)); thencj = δq
j and c∗
j = δ ∗q
j . It is easy to see that
throughout the analysis of Uniform-UFL-out and k-median-out, for the cases that j ∈ Xa ∩ Xa∗
is lucky, long, or good (but not bad) when we consider a test swap for a group P, we can bound the
distance from j to some point in SP by first moving it to either σ (j) or σ∗ (j) and then moving it
a distance of O(ϵ ) · (δj + δ ∗
j ) to reach an open center. Considering that we reassigned j from σ (j),
the reassignment cost will be at most
(δj + O(ϵ ) · (δj + δ ∗
j ))q − cj = O(2qϵ ) · (cj + c∗
j ),
which, after rearranging, shows
(δ ∗
j + O(ϵ ) · (δj + δ ∗
j ))q − cj = (1 + O(2qϵ )) ·c∗
j − (1 − O(2qϵ )) ·cj .
For the case that j is bad, the reassignment cost can be bounded by O(2q (cj + c∗
j )) but since the
probability of being bad is still bounded by ϵ, the bound in Equation (2) can be re-written as
0 ≤

j ∈ Xa ∩ Xa∗
j is not bad

(1 + O(2qϵ )) ·c∗
j − (1 − O(2qϵ ))cj

+

j bad
O(2q (c∗
j + cj)) +

j ∈Xo
c∗
j −

j ∈Xo∗
cj , (3)
which would imply
0 ≤

j ∈X

(1 + O(2qϵ )) ·c∗
j − (1 − O(2qϵ )) ·cj

and cost(S) ≤ (1 + O(2qϵ )) · OPT . It is enough to choose ϵ sufficiently small compared to q to
obtain a (1 + ϵ 
)-approximation. A similar argument shows that for the case of Uniform-UFLout with
q
q -norm distances, we can get a PTAS.
4.2 Minor-Closed Families of Graphs
We consider the problem k-median-out in families of graphs that exclude a fixed minor H. Recall
that a family of graphs is closed under minors if and only if all graphs in that family exclude some
fixed minor.
Let G = (V, E) be an edge-weighted graph excluding H as a minor, where X, C ⊆ V and let δ
denote the shortest-path metric of G. We will argue Algorithm 3 for some appropriate constant
ρ := ρ
(ϵ,H) returns a set S⊆C with |S| = (1 + ϵ ) · k where cost(S) ≤ (1 + ϵ ) · OPT . This can
be readily adapted to k-means-out using ideas from Section 4.1. We focus on k-median-out for
simplicity. This will also complete the proof of Theorem 1.2 for minor-closed metrics. The proof
of Theorem 1.1 for minor-closed metrics is proven similarly and is slightly simpler.
We use the same notation as our analysis for k-median-out in doubling metrics. Namely, S⊆C
is a local optimum solution, O⊆C is a global optimum solution, Xa are the points assigned in the
local optimum solution, Xo are the outliers in the local optimum solution, and so on. We assume
S∩O = ∅ (one can “duplicate” a vertex by adding a copy with a 0-cost edge to the original and
preserve the property of excluding H as a minor) and Xo ∩ Xo∗
= ∅.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.               
26:18 Z. Friggstad et al.
A key difference is that we do not start with a perfect partitioning of S∪O, as we did with
doubling metrics. Rather, we start with the r-divisions described in Reference [17], which provides “regions” that consist of subsets of S∪O with limited overlap. We present a brief summary,
without proof, of their partitioning scheme and how it is used to analyze the multiswap heuristic
for uncapacitated facility location. Note that their setting is slightly different in that they
show local search provides a true PTAS for k-median and k-means, whereas we are demonstrating a bicriteria PTAS for k-median-out and k-means-out. It is much easier to describe a PTAS
using their framework if (1 + ϵ ) · k centers are opened in the algorithm. Also, as the locality gap
examples in the next section show, Algorithm 3 may not be a good approximation when using
solutions S of size exactly k.
First, the nodes in V are partitioned according to their nearest center in S∪O, breaking ties in
a consistent manner. Each part (i.e., Voronoi cell) is then a connected component so each can be
contracted to get a graph G with vertices S∪O. Note G also excludes H as a minor. Then, for
r = dH /ϵ2, where dH is a constant depending only on H, they consider an r-division ofG
. Namely,
they consider “regions” R1,..., Rm ⊆S∪O with the following properties (Definition III.1.1
in Reference [17]). First, define the boundary ∂(Ra ) for each region to be all centers i ∈ Ra incident to an edge (i,i
) of G with i  Ra.
• Each edge of G has both endpoints in exactly one region.
• There are at most cH /r · (|S| + |O|) regions where cH is a constant depending only on H.
• Each region has at most r vertices.
• m
a=1 |∂(Ra )| ≤ ϵ · (|S| + |O|).
In general, the regions are not vertex-disjoint.
For each region Ra, the test swap S→S− ((Ra − ∂(Ra )) ∩ S) ∪ (Ra ∩ O) is considered. Each
j with σ (j) ∈ Ra − ∂(Ra ) is moved in one of two ways:
• If σ∗ (j) ∈ Ra, then move j to σ∗ (j) for a cost change of c∗
j − cj .
• Otherwise, if point j is in the Voronoi cell for some i ∈ Ra, then δ (j, ∂(Ra )) ≤ c∗
j , because
the shortest path from j to σ∗ (j) must include a vertex v in the Voronoi cell of some i ∈
∂(Ra ). By definition of the Voronoi partitioning, i is closer to v than σ∗ (j). So the cost
change for j is at most c∗
j − cj again.
• Finally, if point j does not lie in the Voronoi cell for any i ∈ Ra ∪ ∂(Ra ), then δ (j, ∂(Ra )) ≤
cj , because the shortest path from j to σ (j) again crosses the boundary of Ra. So the cost
change for j is at most 0.
Last, for each j ∈ X, if no bound of the form c∗
j − cj is generated for j according to the above rules,
then j should move from σ (j) to σ∗ (j) in some swap that opens σ∗ (j).
We use this approach as our starting point for k-median-out. Let ϵ  > 0 be a constant such that
we run Algorithm 3 using (1 + ϵ 
) · k centers in S. We will fix the constant ρ dictating the size of
the neighborhood to be searched momentarily.
Form the same graph G obtained by contracting the Voronoi diagram of G
, let R1,..., Rm be
the subsets with the same properties as listed above for ϵ := ϵ 
/10. The following can be proven
in a similar manner to Theorem 3.1.
Lemma 4.1. We can find regions R
1,..., R
 such that the following hold:
• Each edge lies in exactly one region.
• |R
a | ≤ O(r) for each 1 ≤ a ≤ .
• |(R
a − ∂(R
a )) ∩ S| > |R
a ∩ O|.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.    
Approximation Schemes for Clustering with Outliers 26:19
Proof. This is similar to the proof of Theorem 3.1. The main difference in this setting is that
the sets Ra are not necessarily disjoint but are only guaranteed have limited overlap.
First, note
m
a=1
|(Ra − ∂(Ra ) ∩ S| − |Ra ∩ O| ≥ [|S| − ϵ · (|S| + |O|)] − [|O| + ϵ · (|S| + O|)]
= |S| − |O| − 2ϵ · (|S| + |O|)
= ϵ  · k − 2ϵ · (2 + ϵ 
) · k
≥ ϵ 
2 · k, (4)
because m
a=1 |∂(Ra )| ≤ ϵ · |S ∪ O|.
Now add “dummy” optimum centers to each region to form regions Ra satisfying m
a=1 |Ra ∩
O| − |(Ra − ∂(Ra ) ∩ S| = 0, where the boundary ∂(Ra ) is the boundary of the non-dummy vertices. The number to be added overall is at bounded as follows:
m
a=1
|(Ra − ∂(Ra ) ∩ S| − |Ra ∩ O| ≤ |S| − |O| − ϵ · (|S| + O|)
≤ ϵ 
k − 2ϵk
= 4ϵ 
5 · k.
Now, as |Ra | ≤ r for each a, then there are at least 2k/r regions, so we may add at most 4ϵ
5 / 2
r =
O(dH /ϵ ) centers per region to achieve this. That is, |Ra |≤|Ra | + O(dH /ϵ ) = O(r).
However, we can guarantee each Ra has at least one dummy center. This is because there are
at least ϵ
2 · k dummy centers to add, and the number of regions is at most cH /r · 3k = 3cH
dH ϵ2 · k =
300cH
dH ϵ 2 · k. For small-enough ϵ 
, the number of centers to add is at least the number of regions.
Finally, using Theorem 4 in Reference [22], we can partition {R1,..., Ra } into parts R∞,..., R,
where each part Rb consists of O(r 2) regions and
Ra ∈Rb |(Ra − ∂(Ra ) ∩ S| − |Ra ∩ O| = 0. For
each 1 ≤ b ≤ , let R
b = ∪Ra ∈Rb Ra (discarding the dummies). Note N (R
b ) ⊆ ∪Ra ∈RbN (Ra ). So
|R
b ∩ O| < |(R
b − N (R
b )) ∩ S|, as required.
The rest of the proof proceeds as with our analysis of k-median-out in doubling metrics. For
each j ∈ Xo , let τ (j) be any index such that σ∗ (j) ∈ R
a, and for each j ∈ Xo∗
, let τ ∗ (j) be any
index such that σ (j) ∈ Ra. For each R
b , define the imbalance of outliers Δb to be |{j ∈ Xo : τ (j) =
b}| − |{j ∈ Xo∗
: τ ∗ (j) = b}|. Note
b=1 Δb = 0.
Everything now proceeds as before, so we only briefly summarize: We find groups Gs each
consisting of Θ(r 2/ϵ 
) regions of the form R
b along with similar a similar bijection κ : Xo → Xo∗
such that for each j ∈ Xo , we have R
τ (j) and R
τ ∗ (κ(j)) appearing in a common group. Finally, at
most two regions are split by this grouping. At this point, we determine that swaps of size ρ =
O(r 3/ϵ 
) = O(d3
H /ϵ 7
) in Algorithm 3 suffice for the analysis.
For each group Gs , consider the test swap that opens all global optimum centers lying in some
region R
a used to formGs and closes all local optimum centers inGs that are neither in the boundary ∂(R
a ) of any region forming Gs or in a split region. Outliers are reassigned exactly as with
k-median-out, and points in Xa ∩ Xa∗
are moved as they would be in the analysis for uncapacitated facility location described above.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.          
26:20 Z. Friggstad et al.
Fig. 2. Depiction of the formation of the parts Ps where the arrows between them indicate the mapping ϕ.
The white nodes are in the initial blocks, and the blocks are separated by white dashed lines. The first and
third blocks are large, so another center (black) not yet in any block is added to them. The grey rectangles
are the parts that are formed from the blocks.
For each j ∈ Xa ∩ Xa∗
, the analysis above shows the total cost change for j can be bounded by
c∗
j − cj . Similarly, for each j ∈ Xo , we bound the total cost change of both j and κ(j) together by
c∗
j − cκ(j). So, in fact, cost(S) ≤ cost(O).
5 GENERAL METRICS
Here we prove Theorem 1.3. That is, we show how to apply our framework for k-median-out
and k-means-out to the local search analysis in Reference [25] for k-median and k-means in
general metrics where no assumptions are made about the distance function δ apart from the
metric properties. The algorithm and the redirections of the clients are the same with both kmedian-out and k-means-out. We describe the analysis and summarize the different bounds at
the end to handle outliers.
Let ϵ > 0 be a constant and suppose Algorithm 3 is run using solutions S with |S| ≤ (1 + ϵ ) ·
k and neighborhood size ρ for some large constant ρ > 0 to be determined. We use the same
notation as before and, as always, assume S∩O = ∅ and Xo ∩ Xo∗
= ∅.
Let ϕ : O→S map each i
∗ ∈ O to its nearest center in S. Using a trivial adaptation of Algorithm 1 in Reference [25] (the only difference being we have |S| = (1 + ϵ ) · k rather than |S| = k),
we find blocks B1,..., Bm ⊆S∪O with the following properties.
• The blocks are disjoint and each i
∗ ∈ O lies in some block.
• For each block Ba, |Ba ∩ S| = |Ba ∩ O|.
• For each block Ba, there is exactly one i ∈ Ba ∩ S with ϕ−1 (i)  ∅. For this i we have
Ba ∩ O = ϕ−1 (i).
Call a block small if |Ba ∩ S| ≤ 2/ϵ; otherwise, it is large. Note there are at most ϵ
2 · k large
blocks. However, there are ϵ · k centers in S that do not appear in any block. Assign one such
unused center to each large block; note that these centersi satisfy ϕ−1 (i) = ∅, and there are still at
least ϵ
2 · k centers in S not appearing in any block.
Now we create parts Ps . For each large block Ba, consider any paring between Ba ∩ O and
{i ∈ Ba ∩ S : ϕ−1 (i) = ∅}. Each pair forms a part on its own. Finally, each small block is a part on
its own. This is illustrated in Figure 2.
Note that each part Ps satisfies |Ps ∩ S| = |Ps ∩ O| ≤ 2/ϵ. Then perform the following procedure: While there are at least two parts with size at most 2/ϵ, merge them into a larger part. If there
is one remaining part with size at most 2/ϵ, then merge it with any part created so far. Now all parts
have size between 2/ϵ and 6/ϵ, call these groups P
1,..., P
. Note  ≤ ϵ
2 · k, because the sets P
a ∩ O
partition O. To each part, add one more i ∈ S that does not yet appear in a part. Summarizing, the
parts P
1,..., P
 have the following properties:
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.    
Approximation Schemes for Clustering with Outliers 26:21
• Each i
∗ ∈ O appears in precisely one part. Each i ∈ S appears in at most one part.
• |P
a ∩ O| < |P
a ∩ S| ≤ 6
ϵ + 1 ≤ 7
ϵ .
Construct a pairing κ : Xo → Xo∗
almost as before. First, pair up outliers within a group arbitrarily. Then arbitrarily pair up unpaired j ∈ Xo with points j
 ∈ Xo∗
such that σ (j) does not
appear in any part P
a. Finally, pair up the remaining unpaired outliers using Algorithm 2 applied
to these parts. Form groups Gs with these parts in the same way as before. Again, note some i ∈ S
do not appear in any group, this is not important. What is important is that each i
∗ ∈ O appears
in some group.
The swaps used in the analysis are of the following form. For each group Gs we swap in all
global optimum centers appearing in Gs and swap out all local optimum centers appearing in
Gs that are not in a part split by Gs . As each group is the union of Θ(1/ϵ ) parts, the number of
centers swapped is O(ϵ−2). This determines ρ := O(ϵ−2) in Algorithm 3 for the case of general
metrics.
The clients are reassigned as follows. Note by construction that if j ∈ Xo∗
has σ (j) in a part that
is not split, then σ∗ (κ−1 (j)) lies in the same group as σ (j). Say that this is the group when κ−1 (j)
should be connected. Otherwise, pick any group containing σ∗ (κ−1 (j)) and say this is when κ−1 (j)
should be connected.
For each group Gs ,
• for each j ∈ Xo , if j should be connected in this group, then connect j to σ∗ (j) and make
κ(j) an outlier for a cost change of c∗
j − cκ(j).
• for any j ∈ Xa ∩ Xa∗
where σ (j) is closed, move j as follows:
—If σ∗ (j) is now open, then move j to σ∗ (j) for a cost change bound of c∗
j − cj .
—Otherwise, move j to ϕ(σ∗ (j)), which is guaranteed to be open by how we constructed
the parts. The cost change here is summarized below for the different cases of kmedian and k-means.
Finally, for any j ∈ Xa ∩ Xa∗
that has not had its c∗
j − cj bound generated yet, move j to σ∗ (j) in
any one swap, where σ∗ (j) is opened to get a bound of c∗
j − cj on its movement cost.
The analysis of the changes in cost follows directly from the analysis in Reference [25], so we
simply summarize.
• For k-median-out, cost(S) ≤ (3 + O(ϵ )) · OPT .
• For k-means-out, cost(S) ≤ (25 + O(ϵ )) · OPT .
• For k-clustering with
q
q -norms of the distances, cost(S) ≤ ((3 + O(ϵ )) · q)
q.
These are slightly different than the values reported in Reference [25], because they are considering
the q norm, whereas we are considering
q
q . This concludes the proof of Theorem 1.3.
6 THE LOCALITY GAP
In this section, we show that the natural local search heuristic has unbounded gap for UFL-out
(with non-uniform opening costs). We also show that local search multiswaps that do not violate
the number of clusters, and outliers can have arbitrarily large locality gap for k-median-out and
k-means, even in the Euclidean metrics. We further strengthen our example by showing a similar
large locality gap even if a small violation of the number of outliers is permitted. Locality gap here
refers to the ratio of any local optimum solution produced by the local search heuristics to the
global optimum solution.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.   
26:22 Z. Friggstad et al.
6.1 UFL-out with Non-uniform Opening Costs
First, we consider a multi-swap local search heuristic for the UFL-out problem with non-uniform
center opening costs. We show this for any local search that does ρ-swaps and does not discard
more than z points as outliers, where ρ is a constant and z is part of the input has unbounded ratio.
Assume the set of X∪C is partitioned into disjoint sets A, B1, B2,..., Bz , where
• The points in different sets are at a large distance from one another.
• A has one center i with the cost of ρ and z points that are colocated at i.
• For each of  = 1, 2,..., z, the set B has one center i with the cost of 1 and one point
located at i.
The set of centers S = {i1, i2,..., iz } is a local optimum for the ρ-swap local search heuristic;
any ρ-swap between B’s will not reduce the cost, and any ρ-swaps that opens i will incur an
opening cost of ρ, which is already as expensive as the potential savings from closing ρ of the
B’s. Note that the z points of A are discarded as outliers in S. It is straightforward to verify that
the global optimum in this case is O = {i}, which discards the points in B1,..., Bz ’s as outliers.
The locality gap for this instance is cost(S)/cost(O) = z
ρ , which can be arbitrarily large for any
fixed ρ.
Note this can also be viewed as a planar metric, showing local search has an unbounded locality
gap for UFL-out in planar graphs.
6.2 k-median-out and k-means-out
Chen [14] presents a bad gap example for local search for k-median-out in general metrics. The
example shows an unbounded locality gap for the multiswap local search heuristic that does not
violate the number of clusters and outliers. We adapt this example to Euclidean metrics and prove
Claim 1 below. The same example shows standard multiswap local search for k-means-out has
an unbounded locality gap.
Claim 1. The ρ-swap local search heuristic that generates no more than k clusters and discards at
most z outliers has an unbounded locality gap in Euclidean metrics if ρ < k − 1.
Proof. Consider an input in which n  z  k > 1. The set of points X is partitioned into disjoint sets B, C1, C2,..., Ck−1, D1, D2,..., Dk−2, and E:
• The distance between every pair of points from different sets is large.
• B has n − 2z colocated points.
• For each i = 1, 2,..., k − 1, Ci has one point in the center and u − 1 points evenly distributed on the perimeter of a circle with radius β from the center.
• For each j = 1, 2,..., k − 2, Dj has u − 1 colocated points.
• E has one point at the center and u + k − 3 points evenly distributed on the perimeter of
the circle with radius γ ,
where u = z/(k − 1), and β and γ are chosen such that γ < (u − 1)β < 2γ (see Figure 3). Let f (.)
denote the center point of a set (in the case of colocated sets, any point from the set). Then, the
set S = {f (B), f (D1), f (D2),..., f (Dk−2), f (E)} is a local optimum for the ρ-swap local search if
ρ < k − 1 with cost (u + k − 3)γ . The reason is that since the distance between the sets is large, we
would incur a large cost by closing f (B) or f (E). Therefore, we need to close some points in the
sets D1,..., Dk−2, and open some points in C1,..., Ck−1 to ensure we do not violate the number
of outliers. Since z  k, we can assume u > k − 1. We can show via some straightforward algebra
that if we close ρ points from Dj ’s, then we need to open points from exactly ρ different Ci ’s to
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019.      
Approximation Schemes for Clustering with Outliers 26:23
Fig. 3. The locality gap counter example for k-median in Euclidean metrics.
keep the number of outliers below z. Since the points on the perimeter are distributed evenly, we
incur the minimum cost by opening f (Ci )’s. So, we only need to show that swapping at most ρ
points from f (Dj)’s with ρ points in f (Ci )’s does not reduce the cost. Assume w.l.o.g. that we
swap f (D1), f (D2),..., f (Dρ ) with f (C1), f (C2),..., f (Cρ ). The new cost is ρ(u − 1)β + (u +
k − ρ − 3)γ , since as a result of the swap, we can reclaim ρ points from the set E as outliers. Notice
that cost(S) = (u + k − 3)γ . Therefore, the cost has, in fact, increased as a result of the ρ-swap,
since (u − 1)β > γ . Now, we can show the claim for k-median-out.
Consider the solution O = {f (B), f (C1), f (C2),..., f (Ck−1)} that costs (k − 1)(u − 1)β. This is
indeed the global optimum for the given instance. The locality gap then would be
(u + k − 3)γ
(k − 1)(u − 1)β > u + k − 2
2(k − 1)
,
since (u − 1)β < 2γ . This ratio can be arbitrarily large as z (and, consequently, u) grows. A slight
modification of this example to planar metrics also shows local search has an unbounded locality
gap for k-median-out and k-means-out in planar graphs. In particular, the sets of collocated
points can be viewed as stars with 0-cost edges and the sets E and all Ci can be viewed as stars
where the leaves are in X and have distance γ or β (respectively) to the middle of the star, which
lies in C.
Small Violation on the Number of Outliers: Now, we consider a setting where we are allowed
to violate the number of outliers by z1−δ , for a given small constant δ.
Claim 2. The ρ-swap local search heuristic that generates no more than k clusters and discards at
most z + z1−δ outliers for any δ, 0 < δ ≤ 1, has an unbounded locality gap in Euclidean metrics, if
ρ < k − 1.
Proof. We again consider the solution S = {f (B), f (E), F (D1), f (D2),..., f (Dk−2)}, this time
with z1−δ outliers, namely the sets C1, C2,..., Ck−1, and any z−δ distinct points from the perimeter of the set E. First, note that S is locally optimal. For the same reasons as before, we do
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 26. Publication date: February 2019. 
26:24 Z. Friggstad et al.
not swap out f (B) and f (E). Therefore, any other solutoin obtained by a ρ-swap will be of
the form S = {f (B), f (E), f (C1), f (C2),..., f (Cρ ), f (Dρ+1),..., f (Dk−2)}, i.e., it closes some
points from the Dj ’s and opens the same number from Ci ’s. The set of outliers consists of the sets
D1, D2,..., Dρ , Cρ+1,..., Ck−1, and any z−δ + ρ distinct perimeter points from E. Comparing the
cost of S and S
, we have:
cost(S) − cost(S
)
= (u + k − 3 − z1−δ )γ − 
ρ(u − 1)β + (u + k − 3 − z1−δ − ρ)γ

= (γ − (u − 1)β) ρ < 0,
where the inequality is due to our choice of β and γ . Finally, comparing the cost of S against the
global optimal O, we get
cost(S)
cost(O) = (u + k − 3 − z1−δ )γ
(k − 1)(u − 1)β
> u + k − 3 − z1−δ
2(k − 1)
=
z
k−1 − z1−δ
2(k − 1) +
k − 3
2(k − 1)
> (zδ − k + 1)
2(k − 1)2 · z1−δ ,
where the first inequality is due to the choice of the parameters β and γ . Since z  k, this ratio grows with z for arbitrarily small values of δ. We conclude that the cost of a local optimal
that violates the number of outliers by z1−δ can be unbounded compared to the cost of a global
optimum.