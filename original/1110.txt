We study the problem of computing maximin share allocations, a recently introduced fairness notion. Given
a set of n agents and a set of goods, the maximin share of an agent is the best she can guarantee to herself,
if she is allowed to partition the goods in any way she prefers, into n bundles, and then receive her least
desirable bundle. The objective then is to find a partition, where each agent is guaranteed her maximin share.
Such allocations do not always exist, hence we resort to approximation algorithms. Our main result is a 2/3-
approximation that runs in polynomial time for any number of agents and goods. This improves upon the
algorithm of Procaccia and Wang (2014), which is also a 2/3-approximation but runs in polynomial time only
for a constant number of agents. To achieve this, we redesign certain parts of the algorithm in Procaccia and
Wang (2014), exploiting the construction of carefully selected matchings in a bipartite graph representation
of the problem. Furthermore, motivated by the apparent difficulty in establishing lower bounds, we undertake
a probabilistic analysis. We prove that in randomly generated instances, maximin share allocations exist with
high probability. This can be seen as a justification of previously reported experimental evidence. Finally, we
provide further positive results for two special cases arising from previous works. The first is the intriguing
case of three agents, where we provide an improved 7/8-approximation. The second case is when all item
values belong to {0, 1, 2}, where we obtain an exact algorithm.
CCS Concepts: • Theory of computation → Approximation algorithms analysis; Algorithmic game
theory; • Mathematics of computing → Combinatorial algorithms;
Additional Key Words and Phrases: Fair division, maximin share

1 INTRODUCTION
We study a recently proposed fair division problem in the context of allocating indivisible goods.
Fair division has attracted the attention of various scientific disciplines, including, among others,
mathematics, economics, and political science. Ever since the first attempt for a formal treatment
by Steinhaus, Banach, and Knaster (Steinhaus 1948), many interesting and challenging questions
have emerged. Over the past decades, a vast literature has developed, see, for example, Brams and
Taylor (1996) and Robertson and Webb (1998), and several notions of fairness have been suggested.
The area gradually gained popularity in computer science as well, as most of the questions are
inherently algorithmic, see, for example, Even and Paz (1984), Edmonds and Pruhs (2006), and
Woeginger and Sgall (2007), among others, for earlier works and the surveys by Procaccia (2015)
and by Bouveret et al. (2016) on more recent results.
The objective in fair division problems is to allocate a set of resources to a set of n agents in a
way that leaves every agent satisfied. In the continuous case, the available resources are typically
represented by the interval [0, 1], whereas in the discrete case, we have a set of distinct, indivisible
goods. The preferences of each agent are represented by a valuation function, which is usually an
additive function (additive on the set of goods in the discrete case, or a probability distribution on
[0, 1] in the continuous case). Given such a setup, many solution concepts have been proposed as
to what constitutes a fair solution. Some of the standard ones include proportionality, envy-freeness,
equitability, and several variants of them. The most related concept to our work is proportionality,
where an allocation is called proportional, if each agent receives a bundle of goods that is worth
at least 1/n of the total value according to her valuation function.
Interestingly, all the aforementioned solutions and several others can be attained in the continuous case. Apart from mere existence, in some cases we can also have efficient algorithms, see, for
example, Even and Paz (1984) for proportionality and Aziz and MacKenzie (2016) for some recent
progress on envy-freeness. In the presence of indivisible goods, however, the picture is quite different. We cannot guarantee existence, and it is even NP-hard to decide whether a given instance
admits fair allocations. In fact, in most cases it is hard to produce decent approximation guarantees.
Motivated by the question of what can we guarantee in the discrete case, we focus on a concept
recently introduced by Budish (2011), which can be seen as a relaxation of proportionality. The
rationale is as follows: suppose that an agent, say agent i, is asked to partition the goods into n
bundles and then the rest of the agents choose a bundle before i. In the worst case, agenti will be left
with her least-valuable bundle. Hence, a risk-averse agent would choose a partition that maximizes
the minimum value of a bundle in the partition. This value is called the maximin share of agent i.
The objective then is to find an allocation where every agent receives at least her maximin share.
Even for this notion, existence is not guaranteed under indivisible goods (Procaccia and Wang 2014;
Kurokawa et al. 2016), despite the encouraging experimental evidence (Bouveret and Lemaître
2016; Procaccia and Wang 2014). However, it is possible to have constant factor approximations,
as has been recently shown (Procaccia and Wang 2014) (see also our related work section).
Contribution: Our main result, in Section 4, is a (2/3 − ε)-approximation algorithm, for any constant ε > 0, that runs in polynomial time for any number of agents and any number of goods.
That is, the algorithm produces an allocation where every agent receives a bundle worth at least
2/3 − ε of her maximin share. Our result improves upon the 2/3-approximation of Procaccia and
Wang (2014), which runs in polynomial time only for a constant number of agents. To achieve this,
we redesign certain parts of their algorithm, arguing about the existence of appropriate, carefully
constructed matchings in a bipartite graph representation of the problem. Before that, in Section 3,
we provide a much simpler and faster 1/2-approximation algorithm. Despite the worse factor, this
algorithm still has its own merit due to its simplicity.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.
Approximation Algorithms for Computing Maximin Share Allocations 52:3
Moreover, we study two special cases, motivated by previous works. The first one is the case
of n = 3 agents. This is an interesting turning point on the approximability of the problem; for
n = 2, there always exist maximin share allocations, but adding a third agent makes the problem
significantly more complex, and the best known ratio was 3/4 (Procaccia and Wang 2014). We
provide an algorithm with an approximation guarantee of 7/8, by examining more deeply the set
of allowed matchings that we can use to satisfy the agents. The second case is the setting where
all item values belong to {0, 1, 2}. This is an extension of the {0, 1} setting studied by Bouveret and
Lemaître (2016), and we show that there always exists a maximin share allocation, for any number
of agents.
Finally, motivated by the apparent difficulty in finding impossibility results on the approximability of the problem, we undertake a probabilistic analysis in Section 6. Our analysis shows that
in randomly generated instances, maximin share allocations exist with high probability. This may
be seen as a justification of the reported experimental evidence (Bouveret and Lemaître 2016;
Procaccia and Wang 2014), which show that maximin share allocations exist in most cases.
1.1 Related Work
For an overview of the classic fairness notions and related results, we refer the reader to the books
of Brams and Taylor (1996) and Robertson and Webb (1998). The notion we study here was introduced by Budish (2011) for ordinal utilities (i.e., agents have rankings over alternatives), building
on concepts by Moulin (1990). Later on, Bouveret and Lemaître (2016) defined the notion for cardinal utilities, in the form that we study it here, and provided many important insights as well as
experimental evidence. The first constant factor approximation algorithm was given by Procaccia
and Wang (2014), achieving a 2/3-approximation but in time exponential in the number of agents.
On the negative side, constructions of instances where no maximin share allocation exists, even
for n = 3, have been provided by Procaccia and Wang (2014) and Kurokawa et al. (2016). These elaborate constructions, along with the extensive experimentation of Bouveret and Lemaître (2016),
reveal that it has been challenging to produce better lower bounds, that is, instances where no
α-approximation of a maximin share allocation exists, even for α very close to 1. Driven by these
observations, a probabilistic analysis, similar in spirit but more general than ours, is carried out
by Kurokawa et al. (2016). In our analysis in Section 6, all values are uniformly drawn from [0, 1];
Kurokawa et al. (2016) show a similar result with ours but for a a wide range of distributions over
[0, 1], establishing that maximin share allocations exist with high probability under all such distributions. However, their analysis, general as it may be, needs very large values of n to guarantee
relatively high probability; hence, it does not fully justify the experimental results discussed earlier.
Recently, some variants of the problem have also been considered. Barman and Murthy (2017)
gave a constant factor approximation of 1/10 for the case where the agents have submodular
valuation functions. It remains an interesting open problem to determine whether better factors
are achievable for submodular or other non-additive functions. Along a different direction,
Caragiannis et al. (2016) introduced the notion of pairwise maximin share guarantee and provided
approximation algorithms. Although conceptually this is not too far apart from maximin shares,
the two notions are incomparable.
Another aspect that has been studied is the design of truthful mechanisms providing approximate maximin share fairness guarantees. Note that our work here does not deal with incentive
issues. Looking at this as a mechanism design problem without money, Amanatidis et al. (2016) provide both positive and negative results exhibiting a clear separation between what can be achieved
with and without the truthfulness constraint. Even further, Amanatidis et al. (2017) completely
characterized truthful mechanisms for two agents, which in turn implied tight bounds on the approximability of maximin share fairness by truthful mechanisms.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.
52:4 G. Amanatidis et al.
Finally, a seemingly related problem is that of max-min fairness (also known as the Santa
Claus problem) (Asadpour and Saberi 2007; Bansal and Sviridenko 2006; Bezakova and Dani
2005). In this problem, we want to find an allocation where the value of the least happy person is
maximized. With identical agents, this coincides with our problem, but beyond this special case
the two problems exhibit very different behavior.
2 DEFINITIONS AND NOTATION
For any k ∈ N, we denote by [k] the set {1,..., k}. Let N = [n] be a set of n agents and M = [m]
be a set of indivisible items. Following the usual setup in the fair division literature, we assume
each agent has an additive valuation function vi (·), so for every S ⊆ M, vi (S) =
j ∈S vi ({j}). For
j ∈ M, we will use vij instead of vi ({j}).
Given any subset S ⊆ M, an allocation of S to the n agents is a partition T = (T1,...,Tn ), where
Ti ∩Tj = ∅ and Ti = S. Let Πn (S) be the set of all partitions of a set S into n bundles.
Definition 2.1. Given a set of n agents, and any set S ⊆ M, the n-maximin share of an agent i
with respect to S, is
μi (n, S) = max
T ∈Πn (S )
min
Tj ∈T
vi (Tj).
Note that μi (n, S) depends on the valuation function vi (·) but is independent of any other function vj (·) for j  i. When S = M, we refer to μi (n, M) as the maximin share of agent i. The solution
concept we study asks for a partition that gives each agent her maximin share.
Definition 2.2. Given a set of agents N, and a set of goods M, a partition T = (T1,...,Tn ) ∈
Πn (M) is called a maximin share (MMS) allocation if vi (Ti ) ≥ μi (n, M) , for every agent i ∈ N.
Before we continue, a few words are in order regarding the appeal of this new concept. First,
it is very easy to see that having a maximin share guarantee to every agent forms a relaxation of
proportionality; see Claim 3.1. Given the known impossibility results for proportional allocations
under indivisible items, it is worth investigating whether such relaxations are easier to attain.
Second, the maximin share guarantee has an intuitive interpretation; for an agent i, it is the value
that could be achieved if we run the generalization of the cut-and-choose protocol for multiple
agents, with i being the cutter. In other words, it is the value that agent i can guarantee to himself,
if he were given the advantage to control the partition of the items into bundles, but not the
allocation of the bundles to the agents.
Example 2.3. Consider an instance with three agents and five items:
abcde
Agent 1 1/2 1/2 1/3 1/3 1/3
Agent 2 1/2 1/4 1/4 1/4 0
Agent 3 1/2 1/211/2 1/2
If M = {a,b,c,d, e} is the set of items, then one can see that μ1 (3, M) = 1/2, μ2 (3, M) = 1/4,
μ3 (3, M) = 1. For example, for agent 1, no matter how she partitions the items into three bundles, the worst bundle will be worth at most 1/2 for her, and she achieves this with the partition
({a}, {b,c}, {d, e}). Similarly, agent 3 can guarantee a value of 1 (which is best possible as it is equal
to v3 (M)/n) by the partition ({a,b}, {c}, {d, e}).
Note that this instance admits a maximin share allocation, for example, ({a}, {b,c}, {d, e}), and in
fact this is not unique. Note also that if we remove some agent, say agent 2, the maximin values for
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.   
Approximation Algorithms for Computing Maximin Share Allocations 52:5
the other two agents increase. For example, μ1 (2, M) = 1, achieved by the partition ({a,b}, {c,d, e}).
Similarly, μ3 (2, M) = 3/2.
As shown in Procaccia and Wang (2014), maximin share allocations do not always exist. Hence,
our focus is on approximation algorithms, that is, on algorithms that produce a partition where
each agent i receives a bundle worth (according to vi ) at least ρ · μi (n, M), for some ρ ≤ 1.
3 WARMUP: SOME USEFUL PROPERTIES AND A POLYNOMIAL
TIME 1/2-APPROXIMATION
We find it instructive to provide first a simpler and faster algorithm that achieves a worse approximation of 1/2. In the course of obtaining this algorithm, we also identify some important
properties and insights that we will use in the next sections.
We start with an upper bound on our solution for each agent. The maximin share guarantee is
a relaxation of proportionality, so we trivially have
Claim 3.1. For every i ∈ N and every S ⊆ M, μi (n, S) ≤ vi (S)
n =

j ∈S vij
n .
Proof. This follows by the definition of maximin share. If there existed a partition where the
minimum value for agent i exceeded the above bound, then the total value for agent i would be
more than
j ∈S vij .
Based on this, we now show how to get an additive approximation. Algorithm 1 achieves an
additive approximation of vmax , where vmax = maxi,j vij . This simple algorithm, which we will
refer to as the Greedy Round-Robin Algorithm, has also been discussed by Bouveret and Lemaître
(2016), where it was shown that when all item values are in {0, 1}, it produces an exact maximin
share allocation. At the same time, we note that the algorithm also achieves envy-freeness up to
one item, another solution concept defined by Budish (2011), and further discussed in Caragiannis
et al. (2016). Finally, some variations of this algorithm have also been used in other allocation
problems, see, for example, Brams and King (2005) or the protocol in Bouveret and Lang (2011).
We discuss further the properties of Greedy Round-Robin in Section 6.
In the statement of the following algorithm, the set VN is the set of valuation functions VN =
{vi : i ∈ N}, which can be encoded as a valuation matrix, since the functions are additive.
ALGORITHM 1: Greedy Round-Robin(N, M,VN )
1 Set Si = ∅ for each i ∈ N.
2 Fix an ordering of the agents arbitrarily.
3 while ∃ unallocated items do
4 Si = Si ∪ {j}, where i is the next agent to be examined in the current round (proceeding in a
round-robin fashion) and j is i’s most desired item among the currently unallocated items.
5 return (S1,..., Sn )
Theorem 3.2. If (S1,..., Sn ) is the output of Algorithm 1, then for every i ∈ N,
vi (Si ) ≥

j ∈M vij
n −vmax ≥ μi (n, M) −vmax .
Proof. Let (S1,..., Sn ) be the output of Algorithm 1. We first prove the following claim about
the envy of each agent towards the rest of the agents:
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.    
52:6 G. Amanatidis et al.
Claim 3.3. For every i, j ∈ N, vi (Si ) ≥ vi (Sj) −vmax .
Proof. Fix an agenti, and let j  i. We will upper bound the differencevi (Sj) −vi (Si ). If j comes
after i in the order chosen by the algorithm, then the statement of the claim trivially holds, since
i always picks an item at least as desirable as the one j picks. Suppose that j precedes i in the
ordering. The algorithm proceeds in  = 
m/n rounds. In each round k, let rk and r
k be the items
allocated to j and i, respectively. Then,
vi (Sj) −vi (Si ) = (vi,r1 −vi,r
1
) + (vi,r2 −vi,r
2
) + ··· + (vi,r −vi,r

).
Note that there may be no item r
 in the last round if the algorithm runs out of goods but this does
not affect the analysis (simply set vi,r
 = 0).
Since agent i picks her most desirable item when it is her turn to choose, this means that for
two consecutive rounds k and k + 1 it holds that vi,r
k ≥ vi,rk+1 . This directly implies that
vi (Sj) −vi (Si ) ≤ vi,r1 −vi,r
 ≤ vi,r1 ≤ vmax .
If we now sum up the statement of Claim 3.3 for each j, then we get: nvi (Si ) ≥
j vi (Sj) −
nvmax , which implies
vi (Si ) ≥

j vi (Sj)
n −vmax =

j ∈M vij
n −vmax ≥ μi (n, M) −vmax ,
where the last inequality holds by Claim 3.1.
The next important ingredient is the following monotonicity property, which says that we can
allocate a single good to an agent without decreasing the maximin share of other agents. Note that
this lemma also follows from Lemma 1 of Bouveret and Lemaître (2016), yet, for completeness, we
prove it here as well.
Lemma 3.4 (Monotonicity Property). For any agent i and any good j, it holds that
μi (n − 1, M \ {j}) ≥ μi (n, M).
Proof. Let us look at agent i, and consider a partition of M that attains her maximin share.
Let (S1,..., Sn ) be this partition. Without loss of generality, suppose j ∈ S1. Consider the remaining partition (S2,..., Sn ) enhanced in an arbitrary way by the items of S1 \ {j}. This is a (n − 1)-
partition of M \ {j} where the value of agent i for any bundle is at least μi (n, M). Thus, we have
μi (n − 1, M \ {j}) ≥ μi (n, M).
We are now ready for the 1/2-approximation, obtained by Algorithm 2, which is based on using Greedy Round-Robin, but only after we allocate first the most valuable goods. This is done
so the value of vmax drops to an extent that Greedy Round-Robin can achieve a multiplicative
approximation.
Theorem 3.5. Let N be a set of n agents, and let M be a set of goods. Algorithm 2 produces an
allocation (S1,..., Sn ) such that
vi (Si ) ≥
1
2
μi (n, M), ∀i ∈ N.
Proof. We will distinguish two cases. Consider an agent i who was allocated a single item
during the first phase of the algorithm (lines 4–8). Suppose that at the time when i was given her
item, there were n1 active agents, n1 ≤ n, and that S was the set of currently unallocated items. By
the design of the algorithm, this means that the value of what i received is at least

j ∈S vij
2n1
≥
1
2
μi (n1, S),
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.              
Approximation Algorithms for Computing Maximin Share Allocations 52:7
ALGORITHM 2: apx-mms1/2 (N, M,VN )
1 Set S = M
2 for i = 1 to |N | do
3 Let αi =

j∈S vi j
|N |
4 while ∃i, j s.t. vij ≥ αi /2 do
5 Allocate j to i.
6 S = S  {j}
7 N = N  {i}
8 Recompute the αis.
9 Run Greedy Round-Robin on the remaining instance.
where the inequality follows by Claim 3.1. But now if we apply the monotonicity property
(Lemma 3.4) n − n1 times, then we get that μi (n1, S) ≥ μi (n, M), and we are done.
Consider now an agent i, who gets a bundle of goods according to Greedy Round-Robin, in the
second phase of the algorithm. Let n2 be the number of active agents at that point, and S be the
set of goods that are unallocated before Greedy Round-Robin is executed. We know that vmax at
that point is less than half the current value of αi for agent i. Hence, by the additive guarantee of
Greedy Round-Robin, we have that the bundle received by agent i has value at least

j ∈S vij
n2
−vmax >

j ∈S vij
n2
− αi
2 =

j ∈S vij
2n2
≥
1
2
μi (n2, S).
Again, after applying the monotonicity property repeatedly, we get that μi (n2, S) ≥ μi (n, M),
which completes the proof.
4 A POLYNOMIAL TIME ( 2
3 − ε)-APPROXIMATION
The main result of this section is Theorem 4.1, establishing a polynomial time algorithm for achieving a 2/3-approximation to the maximin share of each agent.
Theorem 4.1. Let N be a set of n agents, and let M be a set of goods. For any constant ε > 0,
Algorithm 3 produces in polynomial time an allocation (S1,..., Sn ), such that
vi (Si ) ≥
 2
3 − ε

μi (n, M), ∀i ∈ N.
Our result is based on the algorithm by Procaccia and Wang (2014), which also guarantees to
each agent a 2/3-approximation. However, their algorithm runs in polynomial time only for a
constant number of agents. Here, we identify the source of exponentiality and take a different
approach regarding certain parts of the algorithm. For the sake of completeness, we first present
the necessary related results of Procaccia and Wang (2014), before we discuss the steps that are
needed to obtain our result.
First, we note that even the computation of the maximin share values is already a hard problem.
For a single agent i, the problem of deciding whether μi (n, M) ≥ k for a given k is NP-complete.
However, a polynomial-time approximation scheme (PTAS) follows by the work of Woeginger
(1997). In the original article, which is in the context of job scheduling, Woeginger gave a PTAS
for maximizing the minimum completion time on identical machines. But this scheduling problem
is identical to computing a maximin partition with respect to a given agent i. Indeed, from agent
i’s perspective, it is enough to think of the machines as identical agents (the only input that we
need for computing μi (n, M) is the valuation function of i). Hence:
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.       
52:8 G. Amanatidis et al.
Theorem 4.2 (Follows by (Woeginger 1997)). Suppose we have a set M of goods to be divided
among n agents. Then, for each agent i, there exists a PTAS for approximating μi (n, M).
A central quantity in the algorithm of Procaccia and Wang (2014) is the n-density balance parameter, denoted by ρn and defined in the following. Before stating the definition, we give for
clarity the high-level idea, which can be seen as an attempt to generalize the monotonicity property of Lemma 3.4. Assume that in the course of an algorithm, we have used a subset of the items
to “satisfy” some of the agents, and that those items do not have “too much” value for the rest of
the agents. If k is the number of remaining agents, and S is the remaining set of goods, then we
should expect to be able to “satisfy” these k agents using the items in S. A good approximation in
this reduced instance, however, would only be an approximation with respect to μi (k, S). Hence,
to hope for an approximation algorithm for the original instance, we would need to examine how
μi (k, S) relates to μi (n, M). Essentially, the parameter ρn is the best guarantee one can hope to
achieve for the remaining agents, based only on the fact that the complement of the set left to be
shared is of relatively small value. Formally:
Definition 4.3 ((Procaccia and Wang 2014)). For any number n of agents, let
ρn = max ⎧⎪
⎨
⎪
⎩
λ






∀M,∀ additive vi ∈ (R+)
2M
,∀S ⊆ M,∀k,  s.t. k +  = n,
vi (M \ S) ≤ λμi (n, M) ⇒ μi (k, S) ≥ λμi (n, M)
⎫⎪
⎬
⎪
⎭
.
After a quite technical analysis, Procaccia and Wang calculate the exact value of ρn in the following lemma.
Lemma 4.4 (Lemma 3.2 of (Procaccia and Wang 2014)). For any n ≥ 2,
ρn = 2nodd
3nodd − 1
>
2
3
,
where nodd denotes the largest odd integer less than or equal to n.
We are now ready to state our algorithm, referred to as apx-mms (Algorithm 3). We elaborate
on the crucial differences between Algorithm 3 and the result of Procaccia and Wang (2014) after
the algorithm description (namely after Lemma 4.5). At first, the algorithm computes each agent’s
(1 − ε
)-approximate maximin value using Woeginger’s PTAS, where ε = 3ε
4 . Let ξ = (ξ1,...,ξn )
be the vector of these values. Hence, ∀i, μi (n, M) ≥ ξi ≥ (1 − ε
)μi (n, M). Then, apx-mms makes a
call to the recursive algorithm rec-mms (Algorithm 4) to compute a ( 2
3 − ε)-approximate partition.
rec-mms takes the arguments ε
,n = |N |, ξ, S (the set of items that have not been allocated yet),
K (the set of agents that have not received a share of items yet), and the valuation functions VK =
{vi |i ∈ K}. The guarantee provided by rec-mms is that as long as the already allocated goods are
not worth too much for the currently active agents of K, we can satisfy them with the remaining
goods. More formally, under the assumption that
∀i ∈ K, vi (M \ S) ≤ (n − |K|)ρn μi (n, M), (1)
which we will show that it holds before each call, rec-mms(ε
,n,ξ, S,K,VK ) computes a |K|-
partition of S, so each agent receives items of value at least (1 − ε
)ρnξi .
The initial call of the recursion is, of course, rec-mms(ε
,n,ξ, M, N,VN ). Before moving on to
the next recursive call, rec-mms appropriately allocates some of the items to some of the agents,
so they receive value at least (1 − ε
)ρnξi each. This is achieved by identifying an appropriate
matching between some currently unsatisfied agents and certain bundles of items, as described in
the algorithm. In particular, the most important step in the algorithm is to first compute the set X +
(line 6), which is the set of agents that will not be matched in the current call. The remaining active
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.   
Approximation Algorithms for Computing Maximin Share Allocations 52:9
agents, that is, K \ X +, are then guaranteed to get matched in the current round, whereas X + will
be satisfied in the next recursive calls. To ensure this for X +, rec-mms guarantees that inequality
Equation (1) holds for K = X + and with S being the rest of the items. Note that Equation (1) trivially
holds for the initial call of rec-mms, where K = N and S = M.
ALGORITHM 3: apx-mms(ε, N, M,VN )
1 ε = 3ε
4
2 for i = 1 to |N | do
3 Use Woeginger’s PTAS to compute a (1 − ε
)-approximation ξi of μi (|N |, M). Let ξ = (ξ1,...,ξn ).
4 return rec-mms(ε
, |N |, ξ , M, N,VN )
For simplicity, in the description of rec-mms, we assume that K = {1, 2,... , |K|}. Also, for the
bipartite graph defined in the following algorithm, by Γ(X +), we denote the set of neighbors of the
vertices in X +.
ALGORITHM 4: rec-mms(ε
,n, ξ , S,K,VK )
1 if |K| = 1 then
2 Allocate all of S to agent 1.
3 else
4 Use Woeginger’s PTAS to compute a (1 − ε
)-approximate |K|-maximinpartition of S with respect
to agent 1 from K, say (S1,..., S |K |).
5 Create a bipartite graph G = (X ∪ Y, E), where X = Y = K andE = {(i, j) | i ∈ X, j ∈ Y,
vi (Sj ) ≥ (1 − ε
)ρnξi}.
6 Find a set X+ ⊂ X, as described in Lemma 4.5.
7 Given a perfect matching A, between X  X+ and a subset of Y  Γ(X+), allocate Sj to agent i iff
(i, j) ∈ A(the matching is a byproduct of line 6).
8 if X+ = ∅ then
9 Output the above allocation.
10 else
11 Output the above allocation, together with rec-mms(ε
,n, ξ , S∗,X+,VX+ ), where S∗ is the
subset of S not allocated in line 7.
To proceed with the analysis, and since the choice of X + plays an important role (line 6 of
Algorithm 4), we should first clarify what properties of X + are needed for the algorithm to work.
The following lemma is the most crucial part in the design of our algorithm.
Lemma 4.5. Assume that for n, M, S, K, VK inequality (1) holds and let G = (X ∪ Y, E) be the
bipartite graph defined in line 5 of rec-mms. Then, there exists a subset X + of X \ {1}, such that:
(i) X + can be found efficiently.
(ii) There exists a perfect matching between X \ X + and a subset of Y \ Γ(X +).
(iii) If we allocate subsets to agents according to such a matching (as described in line 7) andX +  ∅,
then inequality (1) holds for n, M, S∗, X +, VX+ where S∗ ⊆ S is the unallocated set of items,
that is:
∀i ∈ X +, vi (M \ S∗) ≤ (n − |X +|)ρn μi (n, M).
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.   
52:10 G. Amanatidis et al.
Before we prove Lemma 4.5, we elaborate on the main differences between our setup and the
approach of Procaccia and Wang (2014):
Choice of X+. In Procaccia and Wang (2014), X + is defined as arg maxZ ⊆K\ {1}{|Z |||Z |≥|Γ(Z)|}.
Clearly, when n is constant, so is |K|, and thus the computation of X + is trivial. However, it is not
clear how to efficiently find such a set in general, when n is not constant. We propose a definition
of X +, which is efficiently computable and has the desired properties. In short, our X + is any
appropriately selected counterexample to Hall’s Theorem for the graph G constructed in line 5.
Choice of ε. The algorithm works for any ε > 0, but Procaccia and Wang (2014) choose an ε that
depends on n, and it is such that (1 − ε)ρn ≥ 2
3 . This is possible, since for any n, ρn ≥ 2
3 (1 + 1
3n−1 ).
However, in this case, the running time of Woeginger’s PTAS (line 4) is not polynomial in n. Here,
we consider any fixed ε independent of n, hence the approximation ratio of 2
3 − ε.
The formal definition of X + is given within the proof of Lemma 4.5 that follows.
Proof of Lemma 4.5. We will show that either X + = ∅ (in the case where G has a perfect
matching), or some set X + with X + ∈ 
Z ⊆ X : |Z | > |Γ(Z)|∧∃ matching of size |X \ Z | in G\
{Z ∪ Γ(Z)} } has the desired properties. Moreover, we propose a way to find such a set efficiently.
We first find a maximum matching B of G. If |B| = |K|, then we are done, since for X + = ∅, properties (i) and (ii) of Lemma 4.5 hold, while we need not check (iii). If |B| < |K|, then there must
be a subset of X violating the condition of Hall’s Theorem.1 Let Xu,Xm be the partition of X in
unmatched and matched vertices, respectively, according to B, with Xu  ∅, Xm  ∅. Similarly, we
define Yu,Ym.
We now construct a directed graphG = (X ∪ Y, E
), where we direct all edges ofG from X to Y,
and on top of that, we add one copy of each edge of the matching but with direction from Y to X.
In particular, ∀i ∈ X,∀j ∈ Y, if (i, j) ∈ E then (i, j) ∈ E
, and moreover if (i, j) ∈ B then (j,i) ∈ E
.
We claim that the following set satisfies the desired properties
X + := Xu ∪ {v ∈ X : v is reachable from Xu in G
}.
Note that X + is easy to compute; after finding the maximum matching in G, and constructing G
,
we can run a depth-first search in each connected component of G
, starting from the vertices of
Xu . See also Figure 1, after the proof of Theorem 4.1 for an illustration.
Given the definition of X +, we now show property (ii). Back to the original graph G, we first
claim that |X +| > |Γ(X +)|. To prove this, note that if j ∈ Γ(X +) in G, then j ∈ Ym. If not, then it is
not difficult to see that there is an augmenting path from a vertex in Xu to j, which contradicts the
maximality of B. Indeed, since j ∈ Γ(X +), let i be a neighbor of j in X +. If i ∈ Xu , then the edge (i, j)
would enlarge the matching. Otherwise, i ∈ Xm, and since also i ∈ X +, there is a path in G from
some vertex of Xu to i. But this path by construction of the directed graph G must consist of an
alternation of unmatched and matched edges; hence, together with (i, j), we have an augmenting
path.
Therefore, Γ(X +) ⊆ Ym, that is, for any j ∈ Γ(X +), there is an edge (i, j) in the matching B. But
then i has to belong to X + by the construction of G (and since j ∈ Γ(X +)). To sum up: for any j ∈
Γ(X +), there is exactly one distinct vertex i, with (i, j) ∈ E, and i ∈ X + ∩ Xm, that is, |X + ∩ Xm | ≥
|Γ(X +)|. In fact, we have equality here, because it is also true that for any i ∈ X + ∩ Xm, there is
a distinct vertex j ∈ Ym, which is trivially reachable from X +. Hence, |X + ∩ Xm | = |Γ(X +)|. Since
Xu  ∅, we have |X +| = |Xu | + |X + ∩ Xm | ≥ 1 + |Γ(X +)|. So, |X +| > |Γ(X +)|.
1The special case of Hall’s Theorem (Hall 1935) used here states that given a bipartite graph G = (X ∪ Y, E), where X, Y
are disjoint independent sets with |X | = |Y |, there is a perfect matching in G if and only if |W |≤|Γ(W )| for every
W ⊆ X.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.   
Approximation Algorithms for Computing Maximin Share Allocations 52:11
Fig. 1. Ilustration of G, G and X+.
Also, note that X + ⊆ X \ {1}, because for any Z ⊆ X that contains vertex 1, we have |Γ(Z)| =
|K|≥|Z |. This is due to the fact that for any vertex j ∈ Y, the edge (1, j) is present by the construction, since v1 (Sj) ≥ (1 − ε
)μ1 (k, S) ≥ (1 − ε
)ρnμ1 (n, M) ≥ (1 − ε
)ρnξ1, for all 1 ≤ j ≤ |K|.
We now claim that if we removeX + and Γ(X +) fromG, then the restriction of B on the remaining
graph, still matches all vertices of X \ X +, establishing property (ii). Indeed, note first that for any
i ∈ X \ X +, it has to hold that i ∈ Xm, since X + contains Xu . Also, for any edge (i, j) ∈ B with
i ∈ X and j ∈ Γ(X +), we have i ∈ X + by the construction of X +. So, for any i ∈ X \ X +, its pair in
B belongs to Y \ Γ(X +). Equivalently, B induces a perfect matching between X \ X + and a subset
of Y \ Γ(X +) (this is the matching A in line 7 of the algorithm).
What is left to prove is that property (iii) also holds for X +. This can be done by the same
arguments as in Procaccia and Wang (2014), specifically by the following lemma, which can be
inferred from their work.
Lemma 4.6 ((Procaccia and Wang 2014), end of Subsection 3.1). Assume that inequality (1)
holds for n, M, S, K, VK , and let G be the graph defined in line 5. For any Z ⊆ X, if there exists a
perfect matching between X \ Z and a subset of Y \ Γ(Z), say Y∗, and there are no edges between Z
and Y∗ in G, then property (iii) holds as well.
Clearly, there are no edges between X + and Y \ Γ(X +). Hence, Lemma 4.6 can be applied to X +,
completing the proof.
Given Lemma 4.5, we can now prove the main result of this section, the correctness of apx-mms.
Proof of Theorem 4.1. It is clear that the running time of the algorithm is polynomial. Its
correctness is based on the correctness of rec-mms. The latter can be proven with strong induction
on |K|, the number of still active agents that rec-mms receives as input, under the assumption that
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017. 
52:12 G. Amanatidis et al.
Equation (1) holds before each new call of rec-mms (which we have established by Lemma 4.5).
For |K| = 1, assuming that inequality Equation (1) holds, we have for agent 1 of K:
v1 (S) = v1 (M) −v1 (M \ S) ≥ nμ1 (n, M) − (n − 1)ρnμ1 (n, M)
≥ μ1 (n, M) ≥
 2
3 − ε

μ1 (n, M).
For the inductive step, Lemma 4.5 and the choice of X + are crucial. Consider an execution of
rec-mms during which some agents will receive a subset of items and the rest will form the set X +
to be handled recursively. For all the agents in X +—if any—we are guaranteed ( 2
3 − ε)-approximate
shares by property (iii) of Lemma 4.5 and by the inductive hypothesis. On the other hand, for each
agent i that receives a subset Sj of items in line 7, we have
vi (Sj) ≥ (1 − ε
)ρnξi ≥ (1 − ε
)
2
ρn μi (n, M) > (1 − 2ε
)
2
3
μi (n, M) =
 2
3 − ε

μi (n, M),
where the first inequality holds, because (i, j) ∈ E(G).
In Figure 1, we give a simple snapshot to illustrate a recursive call of rec-mms. In particular,
in Subfigure 1(a), we see a bipartite graph G that could be the current configuration for rec-mms,
along with a maximum matching. In Subfigure 1(b), we see the construction of G
, as described in
Lemma 4.5, and the setX +. The bold (black) edges inG signify that both directions are present. The
set X + consists then of Xu and all other vertices of X reachable from Xu . Finally, Subfigure 1(b) also
shows the set of agents that are satisfied in the current call along with the corresponding perfect
matching, as claimed in Lemma 4.5.
We note that the analysis of the algorithm is tight, given the analysis on ρn (see Section 3.3
of Procaccia and Wang (2014)). Improving further on the approximation ratio of 2/3 seems to
require drastically new ideas and it is a challenging open problem. We stress that even a PTAS
is not currently ruled out by the lower bound constructions (Kurokawa et al. 2016; Procaccia and
Wang 2014). Related to this, in the next section, we consider two special cases in which we can
obtain better positive results.
5 TWO SPECIAL CASES
In this section, we consider two interesting special cases, where we have improved approximations.
The first is the case of n = 3 agents, where we obtain a 7/8-approximation, improving on the 3/4-
approximation of Procaccia and Wang (2014). The second is the case where all values for the goods
belong to {0, 1, 2}. This is an extension of the {0, 1} setting discussed in Bouveret and Lemaître
(2016), and we show how to get an exact allocation without any approximation loss.
5.1 The Case of n = 3 Agents
For n = 2, it is pointed out in Bouveret and Lemaître (2016) that maximin share allocations exist via
an analog of the cut and choose protocol. Using the PTAS of Woeginger (1997), we can then have a
(1 − ε)-approximation in polynomial time. In contrast, as soon as we move to n = 3, things become
more interesting. It is proven that with three agents there exist instances where no maximin share
allocation exists (Procaccia and Wang 2014). The best-known approximation guarantee is 3
4 by
observing that the quantity ρn, defined in Section 4, satisfies ρ3 ≥ 3
4 .
We provide a different algorithm, improving the approximation to 7
8 − ε. To do this, we combine
ideas from both algorithms presented so far in Sections 3 and 4. The main result of this subsection
is as follows:
Theorem 5.1. Let N = {1, 2, 3} be a set of three agents with additive valuations, and let M be a set
of goods. For any constant ε > 0, Algorithm 5 produces in polynomial time an allocation (S1, S2, S3),
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017. 
Approximation Algorithms for Computing Maximin Share Allocations 52:13
such that
vi (Si ) ≥
 7
8 − ε

μi (3, M), ∀i ∈ N.
The algorithm is shown in the following. Before we prove Theorem 5.1, we provide here a brief
outline of how the algorithm works.
Algorithm Outline: First, approximate values for the μis are calculated as before. Then, if there
are items with large value to some agent, in analogy to Algorithm 2, we first allocate one of those
reducing this way the problem to the simple case of n = 2. If there are no items of large value,
then the first agent partitions the items as in Algorithm 4. In the case where this partition does not
satisfy all three agents, then the second agent repartitions two of the bundles of the first agent.
Actually, she tries two different such repartitions, and we show that at least one of them works out.
The definition of a bipartite preference graph and a corresponding matching (as in Algorithm 4) is
never mentioned explicitly here. However, the main idea (and the difference with Algorithm 4) is
that if there are several ways to pick a perfect matching between X \ X + and a subset of Y \ Γ(X +),
then we try them all and choose the best one. Of course, since n = 3, if there is no perfect matching
in the preference graph, thenX \ X + is going to be just a single vertex, and we only have to examine
two possible perfect matchings between X \ X + and a subset of Y \ Γ(X +).
ALGORITHM 5: apx-3-mms(ε, M,v1,v2,v3)
1 ε = 8
7 ε
2 Compute a (1 − ε)-approximation ξi of μi (3, M) for i ∈ {1, 2, 3}.
3 if ∃i ∈ {1, 2, 3}, j ∈ M such that vij ≥ 7
8 ξi then
4 Give item j to agent i and divide M  {j} among the other two agents in a "cut-and-choose"
fashion.
5 else
6 Agent 1 computes a (1 − ε)-approximate maximin partition of M into three sets, say (A1,A2,A3).
7 if ∃j2, j3 ∈ {1, 2, 3} such that j2  j3, v2 (Aj2 ) ≥ 7
8 ξ2and v3 (Aj3 ) ≥ 7
8 ξ3 then
8 Give set Aj2 to agent 2, set Aj3 to agent 3, and the last set to agent 1.
9 else
10 There are two sets that have value less than 7
8 ξ2 w.r.t. agent 2, say for simplicity A2 and A3.
11 Agent 2 computes (1 − ε
)-approximate 2-maximin partitions of A1 ∪ A2 and A1 ∪ A3, say
(B1, B2) and (B
1, B
2), respectively, and discards the partition with the smallest maximin value.
Let (D1,D2) be the partition she keeps.
12 Agent 3 takes the set she prefers from (D1,D2); agent 2 gets the other, and agent 1 gets
M  (D1 ∪ D2).
Proof of Theorem 5.1. First, note that for constant ε the algorithm runs in time polynomial in
|M|. Next, we prove the correctness of the algorithm.
If the output is computed in lines 3 and 4 then for agent i, then as defined in line 3, the value
she receives is at least 7
8 ξi ≥ 7
8 (1 − ε)μi (3, M) > ( 7
8 − ε)μi (3, M). The remaining two agents i1,i2
essentially apply an approximate version of a cut and choose protocol. Agent i1 computes a
(1 − ε)-approximate 2-maximin partition of M \ {j}, say (C1,C2), then agent i2 takes the set she
prefers among C1 and C2, and agent i1 gets the other. By the monotonicity lemma (Lemma 3.4),
we know that μi1 (2, M \ {j}) ≥ μi1 (3, M), and thus no matter which set is left for agent i1, she
is guaranteed a total value of at least (1 − ε)μi1 (3, M) > ( 7
8 − ε)μi1 (3, M). Similarly, we have
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.   
52:14 G. Amanatidis et al.
μi2 (2, M \ {j}) ≥ μi2 (3, M), and therefore vi2 (M \ {j}) ≥ 2μi2 (3, M). Since i2 chooses before i1, she
is guaranteed a total value that is at least μi2 (3, M) > ( 7
8 − ε)μi2 (3, M).
If the output is computed in lines 6–8, then clearly all agents receive a (7/8 − ε)-approximation,
since for agent 1 it does not matter which of the Ais she gets.
The most challenging case is when the output is computed in lines 10–12 (starting with the partition from line 6). Then, as before, agent 1 receives a value that is at least a (7/8 − ε)-approximation
no matter which of the three sets she gets. For agents 2 and 3, however, the analysis is not straightforward. We need the following lemma.
Lemma 5.2. Let N, M,ε be as earlier, such that for all j ∈ M, we have v2j < 7
8 ξ2. Consider any
partition of M into 3 sets A1,A2,A3 and assume that there are no j2, j3 ∈ {1, 2, 3} such that j2  j3,
v2 (Aj2 ) ≥ 7
8 ξ2 and v3 (Aj3 ) ≥ 7
8 ξ3. Then, lines 10–12 of Algorithm 5 produce an allocation (S2, S3) for
agents 2 and 3, such that for i ∈ {2, 3}: vi (Si ) ≥ ( 7
8 − ε)μi (3, M). Moreover, if agent 1 is given set Ak ,
then S2 ∪ S3 =
∈N \k A.
Clearly, Lemma 5.2 completes the proof.
Before stating the proof of Lemma 5.2, we should mention how it is possible to go beyond the
previously known 3
4 -approximation. As noted earlier, ρn is by definition the best guarantee we can
get, based only on the fact that the complement of the set left to be shared is not too large. As a
result, the 7
8 ratio cannot be guaranteed just by the excess value. Instead, in addition to making sure
that the remaining items are valuable enough for the remaining agents, we further argue about
how a maximin partition would distribute those items.
There is an alternative interpretation of Algorithm 5 in terms of Algorithm 3. Whenever only a
single agent (i.e., agent 1) is going to become satisfied in the first recursive call, we try all possible
maximum matchings of the graph G for the calculation of X +. Then, we proceed with the “best”
such matching. Here, for n = 3, this means we only have to consider two possibilities for the set
agent 1 is going to get matched to; it is either A2 or A3 (subject to the assumptions in Algorithm 5).
Proof of Lemma 5.2. First, recall that v2 (M) ≥ 3μ2 (3, M) ≥ 3ξ2. Like in the description of the
algorithm, we may assume that agent 1 gets set A3, without loss of generality. Before we move to
the analysis, we should lay down some facts. Let (B1, B2) be agent 2’s (1 − ε
)-approximate maximin partition of A1 ∪ A2 computed in line 11; similarly (B
1, B
2) is agent 2’s (1 − ε
)-approximate
maximin partition of A1 ∪ A3. We may assume that v2 (B1) ≥ v2 (B2). Also, assume that in line 11
of the algorithm, we have (D1,D2) = (B1, B2), that is, min{v2 (B
1),v2 (B
2)} ≤ v2 (B2) and M \ (D1 ∪
D2) = A3. The case where (D1,D2) = (B
1, B
2) is symmetric. Our goal is to show that v2 (B2) ≥
( 7
8 − ε)μ2 (3, M). For simplicity, we write μ2 instead of μ2 (3, M).
Note, toward a contradiction, that
v2 (B2) <
 7
8 − ε

μ2 ⇒
(1 − ε
)μ2 (2,A1 ∪ A2) <
 7
8 − ε

μ2 ⇒
(1 − ε
)μ2 (2,A1 ∪ A2) <
 7
8 − 7
8
ε

μ2 ⇒
μ2 (2,A1 ∪ A2) <
7
8
μ2.
Moreover, this means min{v2 (B
1),v2 (B
2)} < ( 7
8 − ε)μ2 as well, which leads to μ2 (2,A1 ∪ A3) <
7
8 μ2. So, it suffices to show that either μ2 (2,A1 ∪ A2) or μ2 (2,A1 ∪ A3) is at least 7
8 μ2. This statement
is independent of the Bis, and in what follows, we consider exact maximin partitions with respect
to agent 2. Before we proceed, we should make clear that for the case we are analyzing there are
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.     
Approximation Algorithms for Computing Maximin Share Allocations 52:15
Fig. 2. Assuming that the set of items M is represented by a rectangle, here is a depiction of several sets
involved in the proof of Lemma 5.2. Recall that (A1,A2,A3) and (A
1,A
2,A
3) are partitions of M, (C1,C2) is
a partition of A1, and Fi = A
i ∩ A3 for i = 1, 2, 3.
indeed exactly two sets in {A1,A2,A3} each with value less than 7
8 μ2 with respect to agent 2, as
claimed in line 10 of the algorithm. Indeed, notice that in any partition of M there is at least one set
with value at least μ2 with respect to agent 2, due to the fact thatv2 (M) ≥ 3μ2 and by the definition
of a maximin partition. If, however, there were at least 2 sets in {A1,A2,A3} with value at least 7
8 ξ2,
then we would be at the case handled in steps 6–8. Hence, there will be exactly two sets each with
value less than 7
8 ξ2 ≤ 7
8 μ2 for agent 2 and as stated in the algorithm, we assume these are the sets
A2,A3.
Consider a 3-maximin share allocation (A
1,A
2,A
3) of M with respect to agent 2. Let Fi = A
i ∩ A3
for i = 1, 2, 3. Without loss of generality, we may assume that v2 (F1) ≤ v2 (F2) ≤ v2 (F3).
If v2 (F1) ≤ 1
8 μ2, then the partition (A
1 \ A3, (A
2 ∪ A
3) \ A3) is a partition of A1 ∪ A2, such that
v2 (A
1 \ A3) = v2 (A
1) −v2 (F1) ≥ μ2 − 1
8
μ2 = 7
8
μ2
and
v2 ((A
2 ∪ A
3) \ A3) ≥ v2 (A
2) +v2 (A
3) −v2 (A3) ≥ 2μ2 − 7
8
μ2 = 9
8
μ2.
So, in this case, we conclude that μ2 (2,A1 ∪ A2) ≥ 7
8 μ2.
On the other hand, if v2 (F1) > 1
8 μ2, we are going to show that μ2 (2,A1 ∪ A3) ≥ 7
8 μ2. Toward
this, we consider a 2-maximin share allocation (C1,C2) of A1 with respect to agent 2 and let us
assume that v2 (C1) ≥ v2 (C2). For a rough depiction of the different sets involved in the following
arguments; see Figure 2.
Claim 5.3. For C1,C2,A3, F1, F2, F3 as earlier, we have
(i) v2 (A3) +v2 (C2) ≥ 7
8 μ2, and
(ii) v2 (F1) +v2 (F2) +v2 (C1) > 7
8 μ2.
Proof. Note that
v2 (C1) +v2 (C2) +v2 (A3) = v2 (M) −v2 (A2) > 3μ2 − 7
8
μ2 = 17
8 μ2.
If v2 (A3) +v2 (C2) < 7
8 μ2, then v2 (C1) > 10
8 μ2. Moreover,
v2 (A3) = v2 (F1) +v2 (F2) +v2 (F3) ≥ 3v2 (F1) >
3
8
μ2,
so v2 (A3) +v2 (C2) < 7
8 μ2 implies that v2 (C2) < 4
8 μ2.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.
52:16 G. Amanatidis et al.
Let d denote the difference v2 (C1) −v2 (C2); clearly d > 6
8 μ2. It is not hard to see that
minj ∈C1 v2j ≥ d. Indeed, suppose there existed some j ∈ C1 such that v2j < d. Then, by moving
j from C1 to C2, we increase the minimum value of the partition, which contradicts the choice of
(C1,C2).
Sincev2 (C1) > 10
8 μ2 and no item has value more than 7
8 μ2 for agent 2, this means thatC1 contains
at least two items. Thus, v2 (C1) ≥ minj ∈C1 v2j > 12
8 μ2.
Now, for any item д ∈ arg minj ∈C1 v2j , the partition ({д},A1 \ {д}) is strictly better than (C1,C2),
since v2д > 6
8 μ2 > v2 (C2) and v2 (A1 \ {д}) = v2 (A1) −v2д ≥ v2 (C1) −v2д > 12
8 μ2 − 6
8 μ2 = 6
8 μ2 >
v2 (C2). Again, this contradicts the choice of (C1,C2). Hence, it must be that v2 (A3) +v2 (C2) ≥ 7
8 μ2.
The proof of (ii) is simpler. Notice that
v2 (F1) +v2 (F2) +v2 (C1) ≥ v2 (F1) +v2 (F1) +
1
2
v2 (A1)
>
1
8
μ2 +
1
8
μ2 +
1
2

3μ2 − 7
8
μ2 − 7
8
μ2

= 7
8
μ2.
Now, ifv2 (C1) ≥ 7
8 μ2 then (i) of Claim 5.3 implies that min{v2 (C1),v2 (A3 ∪C2)} ≥ 7
8 μ2. Similarly,
if v2 (F3) +v2 (C2) ≥ 7
8 μ2 then (ii) of Claim 5.3 implies that min{v2 (F1 ∪ F2 ∪C1),v2 (F3 ∪C2)} ≥
7
8 μ2. In both cases, we have μ2 (2,A1 ∪ A3) ≥ 7
8 μ2. So, it is left to examine the case where both
v2 (C1) and v2 (F3) +v2 (C2) are less than 7
8 μ2.
Claim 5.4. Let C1,C2,A3, F1, F2, F3 be as earlier and max{v2 (C1),v2 (F3 ∪C2)} < 7
8 μ2. Then,
min{v2 (F1 ∪C1),v2 (F2 ∪ F3 ∪C2)} ≥ 7
8 μ2.
Proof. Recall that v2 (A1) +v2 (A3) > 17
8 μ2. Suppose v2 (F1 ∪C1) < 7
8 μ2. Then, v2 (F2 ∪ F3 ∪
C2) > 10
8 μ2. Since v2 (F3 ∪C2) < 7
8 μ2, we have v2 (F2) > 3
8 μ2. But then we get the contradiction
7
8
μ2 > v2 (A3) = v2 (F1) +v2 (F2) +v2 (F3) ≥
1
8
μ2 +
3
8
μ2 +
3
8
μ2 = 7
8
μ2.
Hence, v2 (F1 ∪C1) ≥ 7
8 μ2. Similarly, suppose v2 (F2 ∪ F3 ∪C2) < 7
8 μ2. Then, v2 (F1 ∪C1) > 10
8 μ2.
Since v2 (C1) < 7
8 μ2, we have v2 (F1) > 3
8 μ2. Then, we get the contradiction
7
8
μ2 > v2 (A3) = v2 (F1) +v2 (F2) +v2 (F3) ≥
3
8
μ2 +
3
8
μ2 +
3
8
μ2 = 9
8
μ2.
Hence, v2 (F2 ∪ F3 ∪C2) ≥ 7
8 μ2.
Claim 5.4 implies μ2 (2,A1 ∪ A3) ≥ 7
8 μ2 and this concludes the proof.
5.2 Values in {0, 1, 2}
Bouveret and Lemaître (2016) consider a binary setting where all valuation functions take values
in {0, 1}, that is, for each i ∈ N, and j ∈ M, vij ∈ {0, 1}. This can correspond to expressing approval
or disapproval for each item. It is then shown that it is always possible to find a maximin share allocation in polynomial time. In fact, they show that the Greedy Round-Robin algorithm, presented
in Section 3, computes such an allocation in this case.
Here, we extend this result to the setting where each vij is in {0, 1, 2}, allowing the agents to
express two types of approval for the items. Enlarging the set of possible values from {0, 1} to
{0, 1, 2} by just one extra possible value makes the problem significantly more complex. Greedy
Round-Robin does not work in this case, so a different algorithm is developed.
Theorem 5.5. Let N = [n] be a set of agents and M = [m] be a set of items. If for any i ∈ N,
agent i has a valuation function vi , such that vij ∈ {0, 1, 2} for any j ∈ M, then we can find, in time
O(nm logm), an allocation (T1,...,Tn ) of M so vi (Ti ) ≥ μi (n, M) for every i ∈ [n].
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.  
Approximation Algorithms for Computing Maximin Share Allocations 52:17
To design our algorithm, we make use of an important observation by Bouveret and Lemaître
(2016) that allows us to reduce appropriately the space of valuation functions that we are interested in. We say that the agents have fully correlated valuation functions if they agree on a common
ranking of the items in decreasing order of values. That is, ∀i ∈ N, if M = {1, 2,...,m}, we have
vi1 ≥ vi2 ≥ ··· ≥ vim. In Bouveret and Lemaître (2016), the authors show that to find a maximin
share allocation for any set of valuation functions, it suffices to do so in an instance where the
valuation functions are fully correlated. This family of instances seems to be the difficulty in computing such allocations. Actually, their result preserves approximation ratios as well (with the
same proof); hence, we state this stronger version. For a valuation function vi let σi be a permutation on the items such that vi (σi (j)) ≥ vi (σi (j + 1)) for j ∈ {1,...,m − 1}. We denote the function
vi (σi (·)) by v↑
i . Note that v↑
1,v↑
2,...,v↑
n are now fully correlated.
Theorem 5.6 ((Bouveret and Lemaître 2016)). Let N = [n] be a set of agents with additive
valuation functions, M = [m] be a set of goods and ρ ∈ (0, 1]. Given an allocation (T1,...,Tn ) of M
so v↑
i (Ti ) ≥ ρμi (n, M) for every i, one can produce in linear time an allocation (T 
1 ,...,T 
n ) of M so
vi (T 
i ) ≥ ρμi (n, M) for every i.
We are ready to state a high-level description of our algorithm. The detailed description,
however, is deferred to the end of this subsection. The reason for this is that the terminology
needed is gradually introduced through a series of lemmas motivating the idea behind the
algorithm and proving its correctness. In fact, the remainder of the subsection is the proof of
Theorem 5.5. Algorithm 6 in the end summarizes all the steps.
Algorithm Outline: We first construct v↑
1,v↑
2,...,v↑
n and work with them instead. The Greedy
Round-Robin algorithm may not directly work, but we partition the items in a similar fashion,
although without giving them to the agents. Then, we show that it is possible to choose some
subsets of items and redistribute them in a way that guarantees that everyone can get a bundle of
items with enough value. At a higher level, we could say that the algorithm simulates a variant
of the Greedy Round-Robin, where for an appropriately selected set of rounds the agents choose
in the reverse order. Finally, a maximin share allocation can be obtained for the original vis, as
described in Bouveret and Lemaître (2016).
Proof of Theorem 5.5. According to Theorem 5.6 it suffices to focus on instances where the
valuation functions take values in {0, 1, 2} and are fully correlated. Given such an instance we
distribute the m objects into n buckets in decreasing order, that is, bucket i will get items i,n +
i, 2n + i,... . Notice that this is compatible with how the Greedy Round-Robin algorithm could
distribute the items; however, we do not assign any buckets to any agents yet. We may assume
that m = kn for some k ∈ N; if not, we just add a few extra items with 0 value to everyone. It is
convenient to picture the collection of buckets as the matrix
B =








(k − 1)n + 1 (k − 1)n + 2 ··· kn
.
.
. .
.
. . . . .
.
.
n + 1 n + 2 ··· 2n
1 2 ··· n







,
since our algorithm will systematically redistribute groups of items corresponding to rows
of B.
Before we state the algorithm, we establish some properties regarding these buckets and the
way each agent views the values of these bundles. First, we introduce some terminology.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.
52:18 G. Amanatidis et al.
Definition 5.7. We say that agent i is
—satisfied with respect to the current buckets, if all the buckets have value at least μi (n, M)
according to vi .
—left-satisfied with respect to the current buckets, if she is not satisfied, but at least the n/2
leftmost buckets have value at least μi (n, M) according to vi .
—right-satisfied if the same as earlier hold, but for the rightmost n/2 buckets.
Now, suppose that we see agent i’s view of the values in the buckets. A typical view would have
the following form (recall the goods are ranked from highest to lowest value):
















000000 ··· 000
· · · · · · ··· · · ·
111110 ··· 000
· · · · · · ··· · · ·
111111 ··· 111
222111 ··· 111
· · · · · · ··· · · ·
222222 ··· 222















.
A row that has only 2s fori will be called a 2-row fori. A row that has both 2s and 1s will be called
a 2/1-row for i, and so forth. An agent can also have a 2/1/0-row. It is not necessary, of course, that
an agent will have all possible types of rows in her view. Note, however, that there can be at most
one 1/0-row and at most one 2/1-row in her view. We first prove the following lemma for agents
that are not initially satisfied.
Lemma 5.8. Any agent not satisfied with respect to the initial buckets must have both a 1/0-row
and a 2/1-row in her view of B. Moreover, initially all agents are either satisfied or left-satisfied.
Proof. Let us focus on the multiset of values of an agent that is not satisfied, say i. It is straightforward to see that if i has no 1s, or the number of 2s is a multiple of n (including 0), then agent i
gets value μi (n, M) from any bucket. So, i must have a row with both 2s and 1s. If this is a 2/1/0-row,
then again it is easy to see that the initial allocation is a maximin share allocation for i. So, i has
a 2/1-row. The only case where she does not have a 1/0-row is if the total number of 1s and 2s is
a multiple of n. But then the maximum and the minimum value of the initial buckets differ by 1;
hence, we have a maximin share allocation and i is satisfied.
Next, we show that an agent i who is not initially satisfied is left-satisfied. In what follows, we
only refer to i’s view. Buckets B1 and Bn, indexed by the corresponding columns of B, have maximum and minimum total value, respectively. Since i is not satisfied, we have vi (B1) ≥ vi (Bn ) + 2,
but the way we distributed the items guarantees that the difference between any two buckets is at
most the largest value of an item; so, vi (B1) = vi (Bn ) + 2. Moreover, since vi (M) ≥ nμi (n, M) and
vi (Bn ) < μi (n, M), we must have vi (B1) > μi (n, M). This implies that vi (B1) = μi (n, M) + 1 and
vi (Bn ) = μi (n, M) − 1.
More generally, we have buckets of value μi (n, M) + 1 (leftmost columns), we have buckets of
value μi (n, M) − 1 (rightmost columns), and maybe some other buckets of value μi (n, M) (columns
in the middle). We know that the total value of all the items is at least nμi (n, M), so, by summing up
the values of the buckets, we conclude that there must be at most n/2 buckets of value μi (n, M) − 1.
Therefore, i is left-satisfied.
So far, we may have some agents that could take any bucket and some agents that would take
any of the n/2 (at least) first buckets. Clearly, if the left-satisfied agents are at most n/2, then we can
easily find a maximin share allocation. However, there is no guarantee that there are not too many
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017. 
Approximation Algorithms for Computing Maximin Share Allocations 52:19
left-satisfied agents initially, so we try to fix this by reversing some of the rows of B. To make this
precise, we say that we reverse the ith row of B when we take items (i − 1)n + 1, (i − 1)n + 2,...,in,
and we put item in in bucket 1, item in − 1 in bucket 2, and so on.
The algorithm then proceeds by picking a subset of rows of B and reversing them. The rows
are chosen appropriately so the resulting buckets (i.e., the columns of B) can be easily paired with
the agents to get a maximin share allocation. First, it is crucial to understand the effect that the
reversal of a set of rows has to an agent.
Lemma 5.9. Any agent satisfied with respect to the initial buckets remains satisfied independently
of the rows of B that we may reverse. On the other hand, any agent not satisfied with respect to the
initial buckets, say agent i, is affected if we reverse her 1/0-row or her 2/1-row. If we reverse only one
of those, then i becomes satisfied with respect to the new buckets; if we reverse both, then i becomes
right-satisfied. The reversal of any other rows is irrelevant to agent i.
Proof. Fix an agent i. First notice that, due to symmetry, reversing any row that fori is a 2-row,
a 1-row, or a 0-row does not improve or worsen the initial allocation from i’s point of view. Also,
clearly, reversing both the 1/0-row and the 2/1-row of a left-satisfied agent makes her right-satisfied.
Similarly, if i is satisfied and has a 2/1/0-row, or has a 2/1-row but no 1/0-row, or has a 1/0-row but
no 2/1-row, then reversing those keeps i satisfied.
The interesting case is when i has both a 1/0-row and a 2/1-row. If i is satisfied, then even removing her 1/0-row leaves all the buckets with at least as much value as the last bucket; so reversing
it keeps i satisfied. A similar argument holds for i’s 2/1-row as well. If i is not satisfied, then the
difference of the values of the first and the last bucket will be 2. Like in the proof of Lemma 5.8,
the number of columns that have 1 in i’s 1/0-row and 2 in i’s 2/1-row (i.e., total value μi (n, M) + 1)
are at least as many as the columns that have 0 in i’s 1/0-row and 1 in i’s 2/1-row (i.e., total value
μi (n, M) − 1). So, by reversing her 1/0-row, the values of all the “worst” (rightmost) buckets increase by 1, the values of some of the “best” (leftmost) buckets decrease by 1, and the values of the
buckets in the middle either remain the same or increase by 1. The difference between the best and
the worst buckets now is 1 (at most), so this is a maximin share allocation for i and she becomes
satisfied. Due to symmetry, the same holds for reversing i’s 2/1-row only.
Now, what Lemma 5.9 guarantees is that when we reverse some of the rows of the initial B,
we are left with agents that are either satisfied, left-satisfied, or right-satisfied. If the rows are
chosen so there are at most n/2 left-satisfied and at most n/2 right-satisfied agents, then there is
an obvious maximin share allocation: to any left-satisfied agent we arbitrarily give one of the first
n/2 buckets, to any right-satisfied agent we arbitrarily give one of the last n/2 buckets, and to each
of the remaining agents we arbitrarily give one of the remaining buckets. In Lemma 5.10, we prove
that it is easy to find which rows to reverse to achieve that.
We use a graph theoretic formulation of the problem for clarity. With respect to the initial
buckets, we define a graph G = (V, E) with V = [k], that is, G has a vertex for each row of B.
Also, for each left-satisfied agent i, G has an edge connecting i’s 1/0-row and 2/1-row. We delete,
if necessary, any multiple edges to get a simple graph with n edges at most. We want to color
the vertices of G with two colors, “red” (for reversed rows) and “blue” (for non reversed), so the
number of edges having both endpoints red is at most n/2 and at the same time the number of
edges having both endpoints blue is at most n/2. Note that if we reverse the rows that correspond
to red vertices, then the agents with red endpoints become right-satisfied, the agents with blue
endpoints remain left-satisfied and the agents with both colors become satisfied. Moreover, the
initially satisfied agents are not affected, and we can find a maximin share allocation as previously
discussed. This is illustrated in Figure 3.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017. 
52:20 G. Amanatidis et al.
Fig. 3. Assuming an instance with 3 agents and 11 items, the tables on top are the three different views on
the initial buckets. This results in the graph shown in the middle—before and after the coloring. By reversing
row c that corresponds to a red vertex, every agent becomes satisfied and thus any matching of the columns
to the agents defines an MMS allocation.
Lemma 5.10. Given graph G defined earlier, in time O(k + n), we can color the vertices with two
colors, red and blue, so the number of edges with two red endpoints is less than n/2 and the number
of edges with two blue endpoints is at most n/2.
Proof. We start with all the vertices colored blue, and we arbitrarily recolor vertices red, one at
a time, until the number of edges with two blue endpoints becomes at most |E|/2 for the first time.
Assume this happens after recoloring vertex u. Before turning u from blue to red, the number of
edges with at most one blue endpoint was strictly less than |E|/2. Also, the recoloring of u did not
force any of the edges with two blue endpoints to become edges with two red endpoints. So, the
number of edges with two red endpoints after the recoloring of u is at most equal to the number of
edges with at most one blue endpoint before the recoloring ofu, that is, less than |E|/2. To complete
the proof, notice that |E| ≤ n. For the running time, notice that each vertex changes color at most
once, and when this happens, we only need to examine the adjacent vertices to update the counters
on each type of edges (only red, only blue, or both).
Lemma 5.10 completes the proof of correctness for Algorithm 6 below that summarizes all the
steps involved. For the running time notice that v↑
1,...,v↑
n can be computed in O(nm logm), since
we get v↑
i by sorting vi1,...,vim. Also step 5 can be computed in O(nm); for each agent i, we scan
the first column of B to find her (possible) 1/0-row and 2/1-row, and then inO(n), we check whether
she is left-satisfied by checking that the positions that have 1 in i’s 1/0-row and 2 in i’s 2/1-row are
at least as many as the positions that have 0 in i’s 1/0-row and 1 in i’s 2/1-row.
6 A PROBABILISTIC ANALYSIS
As argued in the previous works (Bouveret and Lemaître 2016; Procaccia and Wang 2014), it has
been quite challenging to prove impossibility results. Setting efficient computation aside, what
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017. 
Approximation Algorithms for Computing Maximin Share Allocations 52:21
ALGORITHM 6: exact-mms0,1,2 (N, M,VN )
1 Let k = 
m
n . Add kn − m dummy items with value 0 for everyone.
2 if v1,...,vn are not fully correlated then
3 Compute v↑
1,...,v↑
n and use them instead.
4 Construct a k × n matrix B so Bij is the (i − 1)n + jth item.
5 Find the set of left-satisfied agents and their corresponding 1/0-rowsand 2/1-rows.
6 Construct a graph G = ([k], E) with E = {{i, j}|∃ left-satisfied agent that i and j are her
1/0-row and 2/1-row}.
7 Color the vertices of G with two colors, red and blue, so the number of edges having both endpoints
red, and the number of edges having both endpoints blue, each is ≤ n/2.
8 Reverse the rows of B that correspond to red vertices, and keep track of who is satisfied, left-satisfied,
or right-satisfied.
9 Arbitrarily give some of the first n/2 buckets (columns of B) to each of the left-satisfied agentsand
some of the last n/2 buckets to each of the right-satisfied agents. Arbitrarily give the rest ofthe buckets
to the satisfied agents.
10 if v↑
1,...,v↑
n were used then
11 Based on the allocation in step 9 compute and return a maximin share allocation for the original
vis as described in Bouveret and Lemaître [2016].
12 else
13 Return the allocation in step 9.
is the best ρ for which a ρ-approximate allocation does exist? All we know so far is that ρ  1
by the elaborate constructions by Kurokawa et al. (2016) and Procaccia and Wang (2014). However, extensive experimentation by Bouveret and Lemaître (2016) (and also by Procaccia and Wang
(2014)) showed that in all generated instances, there always existed a maximin share allocation.
Motivated by these experimental observations and by the lack of impossibility results, we present
a probabilistic analysis, showing that indeed we expect that in most cases there exist allocations
where every agent receives her maximin share. In particular, we analyze the Greedy Round-Robin
algorithm from Section 3 when each vij is drawn from the uniform distribution over [0, 1].
Recently, Kurokawa et al. (2016) show similar results for a large set of distributions over [0, 1],
includingU [0, 1]. Although, asymptotically, their results yield a theorem that is more general than
ours, we consider our analysis to be of independent interest, since we have much better bounds
on the probabilities for the special case of U [0, 1], even for relatively small values of n.
For completeness, before stating and proving our results, we include the version of Hoeffding’s
inequality we are going to use.
Theorem 6.1 ((Hoeffding 1963)). Let X1,X2,...,Xn be independent random variables with Xi ∈
[0, 1] for i ∈ [n]. Then, for the empirical mean X¯ = 1
n (X1 + ··· + Xn ), we have P(X¯ − E[X¯] ≥ t) ≤
exp(−2nt 2).
We start with Theorem 6.2. Its proof is based on tools like Hoeffding’s and Chebyshev’s inequalities, and on a careful estimation of the probabilities when m < 3n. Note that for m ≥ 2n, the
theorem provides an even stronger guarantee than the maximin share (by Claim 3.1).
Theorem 6.2. Let N = [n] be a set of agents and M = [m] be a set of goods, and assume that the
vijs are i.i.d. random variables that follow U [0, 1]. Then, for m ≥ 2n and large enough n, the Greedy
Round-Robin algorithm allocates to each agent i a set of goods of total value at least 1
n
m
j=1 vij with
probability 1 − o(1). The o(1) term is O(1/n) when m > 2n and O(logn/n) when m = 2n.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.  
52:22 G. Amanatidis et al.
Proof. In what follows, we assume that agent 1 chooses first, agent 2 chooses second, and so
forth. We consider several cases for the different ranges of m. We first assume that 2n ≤ m < 3n.
It is illustrative to consider the case of m = 2n and examine the nth agent that chooses last.
Like all the agents in this case, she receives exactly two items; let Yn be the total value of those
items. From her perspective, she sees n + 1 values chosen uniformly from [0, 1], picks the maximum of those, then u.a.r. n − 1 of the rest are removed, and she takes the last one as well. If
we isolate this random experiment, then it is as if we take Yn = max{X1,...,Xn+1} + XY , where
Y ∼ U ({1, 2,...,n + 1}\{μ}), μ ∈ arg max{X1,...,Xn+1},Xi ∼ U [0, 1] ∀i ∈ [n + 1], and all the Xis
are independent. We estimate now the probability P(Yn ≤ a) for 1 < a < 2. We will set a to a particular value in this interval later on. In fact, we bound this probability using the corresponding
probability for Zn = max{X1,...,Xn+1} + XY, where Y  ∼ U {1, 2,...,n + 1}. For Zn, we have
P(Zn ≤ a) =
n+1
i=1
 a
0
P
	
max 1≤j ≤n+1
Xj ≤ t ∧ Y = i ∧ Xi ≤ a − t


dt
= (n + 1)
 a
0
P
	
max 1≤j ≤n+1
Xj ≤ t ∧ Y = 1 ∧ X1 ≤ a − t


dt
=
 a
0
P
	
max 1≤j ≤n+1
Xj ≤ t ∧ X1 ≤ a − t


dt
=
 a
0
P(X1 ≤ t ∧ X1 ≤ a − t ∧ X2 ≤ t ∧ ... ∧ Xn+1 ≤ t)dt
=
 a/2
0
P(X1 ≤ t ∧ X2 ≤ t ∧ ... ∧ Xn+1 ≤ t)dt
+
 1
a/2
P(X1 ≤ a − t ∧ X2 ≤ t ∧ ... ∧ Xn+1 ≤ t)dt
+
 a
1
P(X1 ≤ a − t ∧ X2 ≤ t ∧ ... ∧ Xn+1 ≤ t)dt
=
 a/2
0
t
n+1
dt +
 1
a/2
(a − t)t
ndt +
 a
1
(a − t)dt.
Also, by the definition of Y 
, we have P(Y   arg max{X1,...,Xn+1}) = n/(n + 1). Therefore, for
Yn, we get
P(Yn ≤ a) = P(Zn ≤ a | Y   arg max{X1,...,Xn+1})
= P(Zn ≤ a ∧ Y   arg max{X1,...,Xn+1})
P(Y   arg max{X1,...,Xn+1})
≤
P(Zn ≤ a)
P(Y   arg max{X1,...,Xn+1}) = n + 1
n P(Zn ≤ a)
= n + 1
n
	 a/2
0
t
n+1
dt +
 1
a/2
(a − t)t
ndt +
 a
1
(a − t)dt

,
where for the inequality we used the fact that P(A ∩ B) ≤ P(A) for any events A, B.
A similar analysis for the jth agent yields
P(Yj ≤ a) ≤
2n − j + 1
n
	 a/2
0
t
2n−j+1
dt +
 1
a/2
(a − t)
n−j+1
t
ndt +
 a
1
(a − t)
n−j+1
dt

.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.     
Approximation Algorithms for Computing Maximin Share Allocations 52:23
In the more general case where m = 2n + κ(n), 0 ≤ κ(n) < n, we have a similar calculation for the
agents that receive only two items in the Greedy Round-Robin algorithm, as well as for the first
two items of the first κ(n) agents (who receive three items each). Let Yi be the total value agent i
receives, andWi be the value of her first two items. Of course, for the last 2n players, Yi = Wi . Also,
recall that m
j=1 vij = vi (M). We now relate the probability that we are interested in estimating,
with the probabilities P(Yi ≤ a) that we have already bounded. We will then proceed by setting α
appropriately. We have
P
	
∃i such that Yi <
1
n
m
j=1
vij

≤
n
i=1
P
	
Yi < vi (M)
n


=
n
i=1
P
	
Yi < min 
vi (M)
n , a

∨ vi (M)
n > max {Yi, a}


≤
n
i=1
P
	
Yi < min 
vi (M)
n , a

 +
n
i=1
P
	
vi (M)
n > max {Yi, a}


≤
n
i=1
P(Yi < a) +
n
i=1
P
	
vi (M)
n > a


.
To upper bound the first sum, we use the Wis, that is, we do not take into account the third item
that the first κ(n) agents receive. By the definition of Yi,Wi , for these first κ(n) agents we have
P(Yi < a) ≤ P(Wi < a), while for the remaining agents we have P(Yi < a) = P(Wi < a). Note that
the bounds for P(Yi ≤ a) calculated earlier, here hold for κ(n) + 1 ≤ i ≤ n. For 1 ≤ i ≤ κ(n), the
same bounds hold for P(Wi ≤ a):
n
i=1
P(Yi < a) ≤
κ

(n)
i=1
P(Wi < a) +
n
i=κ(n)+1
P(Yi < a)
≤
n
i=1
m − i + 1
n
	  a/2
0
t
m−i+1
dt +
 1
a/2
(a − t)
n+κ(n)−i+1
t
ndt +
 a
1
(a − t)
n+κ(n)−i+1
dt

≤ 3
n
j=1
	  a/2
0
t
n+κ(n)+j
dt +
 1
a/2
(a − t)
κ(n)+j
t
ndt +
 a
1
(a − t)
κ(n)+j
dt

= 3 



n
j=1
(a/2)
n+κ(n)+j+1
n + κ(n) + j + 1
+
n
j=1
 1
a/2
(a − t)
κ(n)+j
t
ndt +
n
j=1
 a−1
0
uκ(n)+j
du


.
We are going to bound each sum separately. We set a = 1 + κ(n)
2n +
3 ln n
n = m
2n +
3 ln n
n . Note
that for n ≥ 46, we have a ∈ (1, 2). Consider the first sum:
n
j=1
(a/2)
n+κ(n)+j+1
n + κ(n) + j + 1
≤ (a/2)
n+κ(n)+2
n + κ(n) + 2
n−1
i=0
(a/2)
i
<
1
n + κ(n) + 2 ·
(a/2)
n+κ(n)+2
1 − a/2 = O(1/n),
where we got O(1/n), because the bound is at most 3
n for n ≥ 57 and for any value of κ(n).
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017. 
52:24 G. Amanatidis et al.
Next, we deal with the second sum:
n
j=1
 1
a/2
(a − t)
κ(n)+j
t
ndt <
 1
a/2
t
n 



∞
j=0
(a − t)
κ(n)+j 


dt ≤
 1
a/2
t
n (a/2)
κ(n) 1
1 − a + t
dt
≤ (a/2)
κ(n)
1 − a/2
 1
a/2
t
ndt = (a/2)
κ(n)
1 − a/2
	
1 − (a/2)
n+1
n + 1


= O(1/n).
Here, for n ≥ 58 the bound is at most 10
n for any κ(n).
Finally, for the third sum, we rewrite it as
n
j=1
 a−1
0
uκ(n)+j
du =
n
j=1
(a − 1)
κ(n)+j+1
κ(n) + j + 1 =
n+κ

(n)+1
i=κ(n)+2
(a − 1)
i
i .
We are going to bound each term separately. Consider the case where κ(n) ≥ 5
√
n. For n ≥ 64, it
can be shown that 1
5 (
κ(n)
2n +
3 ln n
n )
5
√
n < 10
n3/2 . So,
n+κ

(n)+1
i=κ(n)+2
(a − 1)
i
i
≤
n
i=1
(a − 1)
κ(n)
κ(n) ≤ n ·

κ(n)
2n +
3 ln n
n
5
√
n
5
√
n ≤ n ·
10
n2 = 10
n .
On the other hand, when κ(n) < 5
√
n, we have a − 1 < 2.5+
√
3 ln n √
n . For n ≥ 59 and j ≥ 10, it can be
shown that 1
j ( 2.5+
√
3 ln n √
n )j < 30
n2 . Of course, for 3 ≤ j ≤ 9 it is true that 1
j ( 2.5+
√
3 ln n √
n )j = o(1/n), and
particularly for n ≥ 59 the sum 9
i=3
1
j ( 2.5+
√
3 ln n √
n )j is bounded by 25
n . In general, it is to be expected
to have relatively large hidden constants whenm is very close to 2n. This changes quickly though;
when κ(n) > 21 the whole sum is less than 1/n. In any case, if κ(n) > 0, then
n+κ

(n)+1
i=κ(n)+2
(a − 1)
i
i
≤
n+2
i=3
(a − 1)
i
i
≤

9
i=3
1
i
 2.5 + √
3 lnn √
n

i
+
n+2
i=10
1
i
 2.5 + √
3 lnn √
n

i
≤ O(1/n) + (n − 7)
30
n2 = O(1/n).
However, if κ(n) = 0, then we have
n+1
i=2
(a − 1)
i
i = 


2.5 + √
3 lnn √
n


2
+
n+1
i=3
(a − 1)
i
i = O
	
logn
n


.
So far, we have n
i=1 P(Yi < a) = O(1/n) (or O(logn/n) when m = 2n). To complete the proof
for this case, we use Hoeffding’s inequality to bound the probability that the average of the values
for any agent is too large:
n
i=1
P
	
vi (M)
n > a


≤ n · P
	
v1 (M)
n > a


= n · P
	
v1 (M)
m > n
m
 m
2n +
3 ln n
n


= n · P
	
v1 (M)
m − 1
2
> n
m
3 ln n
n


≤ n · e
−2m

n
m
 3 lnn
n
2
= n · e−2 n
m ·3 ln n ≤ n · e−2 ln n = 1
n
.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.  
Approximation Algorithms for Computing Maximin Share Allocations 52:25
Hence,
P
	
∃i such that Yi < vi (M)
n


=
⎧⎪⎪⎪
⎨
⎪⎪⎪
⎩
O

log n
n

if m = 2n
O

1
n

if 2n < m < 3n
.
The remaining cases are for m ≥ 3n. We give the proof for m ≥ 4n. The cases for 3n ≤ m < 3.5n
and 3.5n ≤ m < 4n differ in small details but they essentially follow the same analysis. We briefly
discuss these cases at the end of the proof.
Assume that kn ≤ m < (k + 1)n, k ≥ 4. We focus on the agent that chooses last, that is, agent n,
who has the smallest expected value. She gets exactly k items, and like before let Yn be the total
value she receives. To bound P (Yn < β), we introduce the random variables Zn and Wn. Consider
the following random experiment involving the independent random variables X1,...,Xm−n+1,
Xi ∼ U [0, 1] ∀i ∈ [m − n + 1]. Given a realization of the Xis, that is, some values x1,..., xm−n+1 in
[0, 1], Zn is defined similarly to Yn:
—Initially, Zn = 0.
—While there are still xis left, take the maximum of the remaining xis, add it to Zn, remove
it from the available numbers, and then remove the xis with the n − 1 highest indices.
—Return Zn.
On the other hand, Wn = k−1 i=1 X(m+1−in,m−i(n−1)), where X(j,t) is the jth order statistic of
X1,...,Xt . That is, Wn is defined as the sum of the largest of all xis, the second largest of the
first m − n + 1 xis, the third largest of the first m − 2n + 2 xis, and so on.
It is not hard to see that always Wn ≤ Zn (in fact, each term of Wn is less than or equal to
the corresponding term of Zn) and that Zn follows the same distribution as Yn. So, P (Yn < β) =
P (Zn < β) ≤ P (Wn < β). Using the fact that the ith order statistic in a sample of size  drawn
independently from U [0, 1] has expected value i
+1 and variance i(−i+1)
(+1)2 (+2) (Gentle 2009), we get
E[Wn] = m − n + 1
m − n + 2
+ m − 2n + 1
m − 2n + 3
+ ··· + m − (k − 1)n + 1
m − (k − 1)n + k
≥ (k − 1)n + 1
(k − 1)n + 2
+ (k − 2)n + 1
(k − 2)n + 3
+ ··· + n + 1
n + k
> k − 1 − 1
(k − 1)n − 2
(k − 2)n −···− k − 1
n > k − 1 − (k − 1)Hk−1
n .
Moreover, if X
i = X(m+1−in,m−i(n−1)), we have
σ2
Wn = Var(Wn ) =

k−1
i=1

k−1
j=1
Cov(Xi,Xj) ≤

k−1
i=1

k−1
j=1

Var(Xi )Var(Xj) ≤ 



k−1
i=1

Var(Xi )

2
< 



k−1
i=1
√
i
m − in + i + 1


2
< 


√
k − 1

k−1
i=1
1
(k − i)n


2
= (k − 1)H2
k−1
n2 ,
where Hk−1 is the (k − 1)-th harmonic number. Now we can bound the probability that any agent
receives value less than 1/n of her total value:
P
	
Yi < vi (M)
n


≤ P
	
Yn < vn (M)
n


≤ P
	
Yn <
13k
20 

+ P
	
vn (M)
n >
13k
20 

.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.      
52:26 G. Amanatidis et al.
Next, using Chebyshev’s inequality, we have
P
	
Yn <
13k
20 

≤ P
	
Wn <
13k
20 

= P
	
E[Wn] −Wn > E[Wn] − 13k
20 

≤ P
	
|E[Wn] −Wn | > k − 1 − (k − 1)Hk−1
n − 13k
20 

≤ P 



|E[Wn] −Wn | >
7k
20 − 1 − (k−1)Hk−1
n √
k−1Hk−1
n
σWn



≤
(k − 1)H2
k−1   7k−20
20 
n − (k − 1)Hk−1
2 .
On the other hand, using Hoeffding’s inequality,
P
	
vn (M)
n >
13k
20 

= P
	
vn (M)
m − 1
2
> n
m
13k
20 − 1
2


≤ P
	
vn (M)
m − 1
2
>
13k
20(k + 1)
− 1
2


≤ e
−2m 3k−10
20(k+1)
2
≤ e
−2kn 3k−10
20(k+1)
2
.
Finally, we take a union bound to get
P
	
∃i s.t. Yi < vi (M)
n


≤
n
i=1
P
	
Yi < vi (M)
n


≤ n
	 (k−1)H2
k−1
(( 7k−20
20 )n−(k−1)Hk−1 )
2 + e
−2kn 3k−10
20(k+1)
2


= O(1/n).
The exact same proof works when 3n ≤ m < 3.5n, but instead of 3k−10
20(k+1) in Hoeffding’s inequality,
we have 3·3−5
20(3+0.5) , and of course we should adjust E[Wn] and Var(Wn ) accordingly. When 3.5n ≤
m < 4n, on the other hand, we need to consider three items in Wn instead of two, since two items
are not enough anymore to guarantee separation of Yi and 1
n
m
j=1 vij with high probability. That
said, the proof is the same, but we should adjust E[Wn] and Var(Wn ), and instead of 13k
20 = 39
20 we
may choose 2.5.
We now state a similar result for any m, generalizing Theorem 6.2 that only holds when m ≥
2n. We use a modification of Greedy Round-Robin. While m < 2n, the algorithm picks any agent
uniformly at random and gives her only her “best” item (phase 1). When the number of available
items becomes two times the number of active agents, the algorithm proceeds as usual (phase 2).
We note that while for m ≥ 2n Theorem 6.2 gives the stronger guarantee of vi (M)
n for each agent
i, here we can only have a guarantee of μi (n, M).
Theorem 6.3. Let N = [n], M = [m], and the vijs be as in Theorem 6.2. Then, for any m and large
enough n, the Modified Greedy Round-Robin algorithm allocates to each agent i a set of items of total
value at least μi (n, M) with probability 1 − o(1). The o(1) term isO(1/n) whenm > 2n andO(logn/n)
when m ≤ 2n.
Proof. If m ≥ 2n, then this is a corollary of Theorem 6.2. When m < 2n, then for any agent
i, we have maxj{vij} ≥ μi (n, M). So the first agent that receives only her most valuable item has
total value at least μi (n, M). If Na, Ma are the sets of remaining agents and items, respectively, after
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 52. Publication date: December 2017.  
Approximation Algorithms for Computing Maximin Share Allocations 52:27
several agents were assigned one item in phase 1 of the algorithm, then by Lemma 3.4, for any agent
i ∈ Na, we have μi (|Na |, Ma ) ≥ μi (n, M). If |Ma | < 2|Na |, then it is also true that maxj ∈Ma vij ≥
μi (|Na |, Ma ), so correctness of phase 1 follows by induction. If |Ma | = 2|Na |, then by Theorem 6.2
phase 2 guarantees that with high probability each agent i ∈ Na will receive a set of items with
total value at least 1
|Na |
vi (Ma ) ≥ μi (|Na |, Ma ) ≥ μi (n, M).
Remark 1. The implicit constants in the probability bounds of Theorems 6.2 and 6.3 depend heavily
on n and m, as well as on the point one uses to separate Yi and 1
n
m
j=1 vij in the proof of Theorem 6.2. Our analysis gives good bounds for the case 2n ≤ m < 3n without requiring very large
values for n (especially whenκ(n) in the proof of Theorem 6.2 is not small). For example, ifm = 2.4n
an appropriate adjustment of our bounds gives a o(1) term less than 1.7/n for n ≥ 41. When we
switch from the detailed analysis of the 2n ≤ m < 3n case to the sloppier general treatment for
m ≥ 3n, there is definitely some loss, for example, form = 4n, we get that the o(1) term is less than
130/n for n > 450. This is corrected relatively quickly as m grows, for example, for m = 13n the
o(1) term can be made less than 8/n for n ≥ 59. One can significantly improve the constants by
breaking the interval kn ≤ m < (k + 1)n into smaller intervals (not unlike the 3n ≤ m < 3.5n case).
Theorems 6.2 and 6.3 may leave the impression that n has to be large. Actually, there is no reason
why we cannot consider n fixed and let m grow. Following closely the proof of Theorem 6.2 for
m ≥ 4n, we get the next corollary. Notice that now we can use E[Wn] ≥ 0.7k and σ2
Wn < k.
Corollary 6.4. Let N = [n], M = [m], and the vijs be as in Theorem 6.2. Then, for fixed n and
large enough m, the Greedy Round-Robin algorithm allocates to each agent i a set of goods of total
value at least 1
n
m
j=1 vij with probability 1 − O(1/m).
7 CONCLUSIONS
The most interesting open question is undoubtedly whether one can improve on the 2/3-
approximation. Going beyond 2/3 seems to require a drastically different approach. One idea that
may deserve further exploration is to pick in each step of Algorithm 4 the best out of all possible matchings (and not just an arbitrary matching as is done in line 7 of the algorithm). This is
essentially what we exploit for the special case of n = 3 agents. However, for a larger number of
agents, this seems to result in a heavy case analysis without any visible benefits. In terms of noncombinatorial techniques, we are not currently aware of any promising LP-based approach to the
problem.
Even establishing better ratios for special cases could still provide new insights into the problem. It would be interesting, for example, to see if we can have an improved ratio for the special
case studied in Bansal and Sviridenko (2006) for the Santa Claus problem. In this case of additive
functions, the value of a good j takes only two distinct values, 0 or vj . On the other hand, obtaining negative results seems to be an even more challenging task, given our probabilistic analysis
and the results of related works. The negative results (Kurokawa et al. 2016; Procaccia and Wang
2014) require very elaborate constructions, which still do not yield an inapproximability factor far
away from 1. Apart from improving the approximation quality, exploring practical aspects of our
algorithms is another direction, see, for example, Spliddit (2015). Finally, we have not addressed
here the issues of truthfulness and mechanism design, a stimulating topic for future work, studied
recently by (Amanatidis et al. 2017, 2016). These works still leave several open questions regarding the approximability that can be achieved under truthfulness (without payments) for more than
two agents. It is also not clear if more positive results can arise when payments are allowed. Similar mechanism design questions also remain open for a related problem studied by Markakis and
Psomas (2011).   