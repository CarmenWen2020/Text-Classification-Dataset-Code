The k-center problem is a canonical and long-studied facility location and clustering problem with many
applications in both its symmetric and asymmetric forms. Both versions of the problem have tight approximation factors on worst case instances: a 2-approximation for symmetric k-center and an O(log∗ (k))-
approximation for the asymmetric version. Therefore, to improve on these ratios, one must go beyond the
worst case.
In this work, we take this approach and provide strong positive results both for the asymmetric and symmetric k-center problems under a natural input stability (promise) condition called α-perturbation resilience
[15], which states that the optimal solution does not change under any α-factor perturbation to the input distances. We provide algorithms that give strong guarantees simultaneously for stable and non-stable instances:
Our algorithms always inherit the worst-case guarantees of clustering approximation algorithms and output
the optimal solution if the input is 2-perturbation resilient. In particular, we show that if the input is only
perturbation resilient on part of the data, our algorithm will return the optimal clusters from the region of
the data that is perturbation resilient while achieving the best worst-case approximation guarantee on the
remainder of the data. Furthermore, we prove that our result is tight by showing symmetric k-center under
(2 − ϵ )-perturbation resilience is hard unless N P = RP.
The impact of our results is multifaceted. First, to our knowledge, asymmetric k-center is the first problem that is hard to approximate to any constant factor in the worst case, yet can be optimally solved in
polynomial time under perturbation resilience for a constant value of α. This is also the first tight result
for any problem under perturbation resilience, i.e., this is the first time the exact value of α for which the
problem switches from being NP-hard to efficiently computable has been found. Furthermore, our results
illustrate a surprising relationship between symmetric and asymmetric k-center instances under perturbation resilience. Unlike approximation ratio, for which symmetric k-center is easily solved to a factor of 2
but asymmetric k-center cannot be approximated to any constant factor, both symmetric and asymmetric
k-center can be solved optimally under resilience to 2-perturbations. Finally, our guarantees in the setting
where only part of the data satisfies perturbation resilience make these algorithms more applicable to real-life
instances.
CCS Concepts: • Theory of computation → Facility location and clustering;
Additional Key Words and Phrases: Beyond worst-case analysis, clustering, perturbation resilience
1 INTRODUCTION
Clustering is a fundamental problem in combinatorial optimization with a wide range of applications including bioinformatics, computer vision, text analysis, and countless others. The underlying goal is to partition a given set of points to maximize similarity within a partition and minimize
similarity across different partitions. A common approach to clustering is to consider an objective
function over all possible partitionings and seek solutions that are optimal according to the objective. Given a set of points (and a distance metric), common clustering objectives include finding k
centers to minimize the sum of the distance from each point to its closest center (k-median) or to
minimize the maximum distance from a point to its closest center (k-center).
Traditionally, the theory of clustering (and more generally, the theory of algorithms) has focused
on the analysis of worst-case instances [4, 16–18, 20, 27, 39]. For example, it is well known that
popular objective functions are provably NP-hard to optimize exactly or even approximately (APXhard) [27, 32, 38], so research has focused on finding approximation algorithms. While this perspective has led to many elegant approximation algorithms and lower bounds for worst-case instances,
it is often overly pessimistic of an algorithm’s performance on “typical” instances or real-world
instances. A rapidly developing line of work in the algorithms community, the so-called beyond
worst-case analysis of algorithms (BWCA), considers the design and analysis of problem instances
under natural structural properties that may be satisfied in real-world applications. For example,
the popular notion of α-perturbation resilience, introduced by Bilu and Linial [15], considers instances such that the optimal solution does not change when the input distances are allowed to
increase by up to a factor of ≥ α. The goals of BWCA are twofold: (1) to design new algorithms
with strong performance guarantees under the added assumptions [8, 29, 36, 44] and (2) to prove
strong guarantees under BWCA assumptions for existing algorithms used in practice [40, 43, 45].
An example of goal (1) is a series of works focused on finding exact algorithms for k-median, kmeans, and k-center clustering under α-perturbation resilience [1, 6, 11]. The goal in this line of
work is to find the minimum value of α ≥ 1 for which optimal clusterings of α-perturbation resilient instances can be found efficiently. Two examples of goal (2) are as follows: Ostrovsky et al.
showed that k-means++ outputs a near-optimal clustering as long as the data satisfy a natural
clusterability criterion [43]; and Spielman and Teng [45] established that the expected runtime of
the simplex method is O(n) under smoothed analysis.
In approaches for answering goals (1) and (2), researchers have developed an array of sophisticated tools exploiting the structural properties of such instances leading to algorithms that output the optimal solution. However, overly exploiting a BWCA assumption can lead to algorithms
that perform poorly when the input data do not exactly satisfy the given assumption. Indeed,
recent analyses and technical tools are susceptible to small deviations from the BWCA assumptions that can propagate when just a small fraction of the data does not satisfy the assumption.
For example, some recent algorithms make use of a dynamic programming subroutine that crucially needs the entire instance to satisfy the specific structure guaranteed by the BWCA assumption. To continue the efforts of BWCA in bridging the theory-practice gap, it is essential to study
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.
k-center Clustering under Perturbation Resilience 22:3
Table 1. Our Results over All Variants of k-center under Perturbation Resilience
Problem Guarantee α Metric Local Theorem
Symmetric k-center under α-PR OPT 2 Yes Yes Theorem 5.1
Asymmetric k-center under α-PR OPT 2 Yes Yes Theorem 5.2
Symmetric k-center under (α, ϵ )-PR OPT 3 No Yes Theorem 6.8
Asymmetric k-center under (α, ϵ )-PR ϵ-close 3 No No Theorem 6.17
algorithms whose guarantees degrade gracefully to address scenarios that present mild deviations
from the standard BWCA assumptions. Another downside of existing approaches is that BWCA
assumptions are often not efficiently verifiable. This creates a catch-22 scenario: It is only useful to
run the algorithms if the data satisfy certain assumptions, but a user cannot check these assumptions efficiently. For example, by nature of α-perturbation resilience (that the optimal clustering
does not change under all α-perturbations of the input), it is not known how to test this condition
without computing the optimal clustering over Ω (2n ) different perturbations. To alleviate these
issues, in this work, we also focus on what we propose should be a third goal for BWCA: (3) to
show (new or existing) algorithms whose performance degrades gracefully on instances that only
partially meet the BWCA assumptions.
1.1 Our Results and Techniques
In this work, we address goals (1), (2), and (3) of BWCA by providing algorithms that give the
optimal solution under perturbation resilience and also perform well when the data are partially
perturbation resilient or not at all perturbation resilient. These algorithms act as an interpolation
between worst-case and beyond worst-case analysis. We focus on the symmetric/asymmetric kcenter objective under perturbation resilience. Our algorithms simultaneously output the optimal
clusters from the stable regions of the data while achieving state-of-the-art approximation ratios
over the rest of the data. In most cases, our algorithms are natural modifications to existing approximation algorithms, thus achieving goal (2) of BWCA. To achieve these two-part guarantees,
we define the notion of perturbation resilience on a subset of the datapoints. All prior work has
only studied perturbation resilience as it applies to the entire dataset. Informally, a subset S ⊆ S
satisfies α-perturbation resilience if all points v ∈ S remain in the same optimal cluster under any
α-perturbation to the input. We show that our algorithms return all optimal clusters from these
locally stable regions. Most of our results also apply under the recently defined, weaker condition
of α-metric perturbation resilience [1], which states that the optimal solution cannot change under
the metric closure of any α-perturbation. We list all our results in Table 1 and give a summary of
the results and techniques below.
k-center under 2-perturbation resilience. In Section 3, we show that any 2-approximation algorithm for k-center will always return the clusters satisfying 2-perturbation resilience. Therefore, since there are well-known 2-approximation algorithms for symmetric k-center, our analysis
shows that these will output the optimal clustering under 2-perturbation resilience. For asymmetric k-center, we give a new algorithm that outputs the optimal clustering under 2-perturbation
resilience. It works by first computing the “symmetrized set,” or the points that demonstrate a
rough symmetry. We show how to optimally cluster the symmetrized set, and then we show how
to add back the highly asymmetric points into their correct clusters.
Hardness of symmetric k-center under (2 − δ )-perturbation resilience. In Section 3.3, we prove
that there is no polynomial time algorithm for symmetric k-center under (2 − δ )-perturbation resilience unless NP = RP, which shows that our perturbation resilience results are tight for both
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.
22:4 M.-F. Balcan et al.
symmetric and asymmetric k-center. In particular, it implies that we have identified the exact
threshold (α = 2) where the problem switches from efficiently computable to NP-hard for both
symmetric and asymmetric k-center. For this hardness result, we use a reduction from a variant of
perfect dominating set. To show that this variant is itself hard, we construct a chain of parsimonious reductions (reductions that preserve the number of solutions) from 3-dimensional matching
to perfect dominating set.
Our upper bound for asymmetric k-center under 2-perturbation resilience and lower bound for
symmetric k-center under (2 − δ )-perturbation resilience illustrate a surprising relationship between symmetric and asymmetric k-center instances under perturbation resilience. Unlike approximation ratio, for which symmetric k-center is easily solved to a factor of 2 but asymmetric k-center
cannot be approximated to any constant factor, both symmetric and asymmetric k-center can be
solved optimally under resilience to 2-perturbations. Overall, this is the first tight result quantifying the power of perturbation resilience for a canonical combinatorial optimization problem.
Local perturbation resilience. In Section 5, we apply our results from Section 3 to the local perturbation resilience setting. For symmetric k-center, we show that any 2-approximation algorithm outputs all optimal clusters from 2-perturbation resilient regions. For asymmetric k-center,
we design a new algorithm based on the worst-case O(log∗ n) approximation algorithm due to
Vishwanathan [48], which is tight [21]. We give new insights into this algorithm, which allow
us to show that a modification of the algorithm outputs all optimal clusters from 2-perturbation
resilient regions, while keeping the worst-case O(log∗ n) guarantee overall. If the entire dataset
satisfies 2-perturbation resilience, then our algorithm outputs the optimal clustering. We combine the tools of Vishwanathan with the perturbation resilience assumption to prove this two-part
guarantee. Specifically, we use the notion of a center-capturing vertex (CCV), which is used in the
first phase of the approximation algorithm to pull out supersets of clusters. We show that each optimal center from a 2-perturbation resilient subset is a CCV and satisfies a separation property; we
prove this by carefully constructing a 2-perturbation in which points from other clusters cannot
be too close to the center without causing a contradiction. The structure allows us to modify the
approximation algorithm of Vishwanathan [48] to ensure that optimal clusters from perturbation
resilient subsets are pulled out separately in the first phase. All of our guarantees hold under the
weaker notion of metric perturbation resilience.
Efficient algorithms for symmetric and asymmetric k-center under (3, ϵ )-perturbation resilience. In
Section 6, we consider (α, ϵ )-perturbation resilience, which states that at most ϵn total points can
swap in or out of each cluster under any α-perturbation. For symmetric k-center, we show that
any 2-approximation algorithm will return the optimal clusters from (3, ϵ )-perturbation resilient
regions, assuming a mild lower bound on optimal cluster sizes; and for asymmetric k-center, we
give an algorithm that outputs a clustering that is ϵ-close to the optimal clustering (see Section 2
for the formal definition). Our main structural tool is showing that if any single point v is close
to an optimal cluster other than its own, then k − 1 centers achieve the optimal radius under a
carefully constructed 3-perturbation. Any other point we add to the set of centers must create
a clustering that is ϵ-close to the optimal clustering, and we show that all of these sets cannot
simultaneously be consistent with one another, thus causing a contradiction. A key concept in
our analysis is defining the notion of a cluster-capturing center, which allows us to reason about
which points can capture a cluster when its center is removed.
1.2 Related Work
k-center. There are three classic 2-approximation algorithms for k-center from the 1980s [24, 27,
30] that are known to be tight [30]. Asymmetric k-center proved to be a much harder problem. The
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.
k-center Clustering under Perturbation Resilience 22:5
first nontrivial result was an O(log∗ n) approximation algorithm [48], and this was later improved
to O(log∗ k) [2]. This result was later proven to be asymptotically tight [21].
Perturbation resilience. Perturbation resilience was introduced by Bilu and Linial [15], who
showed algorithms that output the optimal solution for Max Cut underO(n)-perturbation resilient
instances. This was later improved by Bilu et al. [14] to O(
√
n)-perturbation resilience and by
Makarychev et al. [40] to O(

logn log logn)-perturbation resilience). The study of clustering under perturbation resilience was initiated by Awasthi et al. [6], who provided an optimal algorithm
for center-based clustering objectives (which include k-median, k-means, and k-center clustering,
as well as other objectives) under 3-perturbation resilience. This result was improved by Balcan
and Liang [11], who showed an algorithm for center-based clustering under (1 + √
2)-perturbation
resilience. They also gave a near-optimal algorithm for k-median under (2 + √
3, ϵ )-perturbation
resilience when the optimal clusters are not too small.
Recently, Angelidakis et al. [1] gave algorithms for center-based clustering (including k-median,
k-means, and k-center) under 2-perturbation resilience and defined the more general notion of
metric perturbation resilience, although their algorithm does not extend to the (α, ϵ )-perturbation
resilience or local perturbation resilience settings. Cohen-Addad and Schwiegelshohn [22] showed
that local search outputs the optimal k-median, k-means, and k-center solution when the data
satisfy a stronger variant of 3-perturbation resilience, in which both the optimal clustering and
optimal centers are not allowed to change under any 3-perturbation. Perturbation resilience has
also been applied to other problems, such as Min Multiway Cut, the Traveling Salesman Problem,
finding Nash Equilibria, Metric Labeling, and Facility Location [10, 37, 40–42].
Subsequent work. Vijayaraghavan et al. [47] study k-means under additive perturbation resilience, in which the optimal solution cannot change under additive perturbations to the input
distances. The notion of additive perturbation resilience is similar but orthogonal to the more
common notion of (multiplicative) perturbation resilience. Deshpande et al. [23] gave an algorithm for Euclidean k-means under α-perturbation resilience, which runs in time linear in n and
the dimension d, and exponentially in k and 1
α−1 . Chekuri and Gupta [19] showed the natural LP
relaxation of k-center and asymmetric k-center is integral for 2-perturbation resilient instances.
They also define a new model of perturbation resilience for clustering with outliers, and they show
the algorithm of Angelidakis et al. [1] exactly solves clustering with outliers under 2-perturbation
resilience; and they further show the natural LP relaxation for k-center with outliers is integral for
2-perturbation resilient instances. Their algorithms have the desirable property that either they
output the optimal solution or they guarantee the input did not satisfy 2-perturbation resilience
(but note this is not the same thing as determining whether or not a given instance satisfies perturbation resilience). Friggstad et al. [26] show that for any fixed ϵ > 0, (1 + ϵ )-perturbation resilient
instances of k-means in doubling metrics can be solved in polynomial time. They also show that
in Euclidean space for a non-constant dimension, there exists a fixed ϵ0 > 0 such that there is no
PTAS for (1 + ϵ0)-perturbation resilient k-means, unless NP = RP, assuming a conjecture that they
call stable SAT.
Other stability notions. A related notion, approximation stability [8], states that any αapproxima- tion to the objective must be ϵ-close to the target clustering. There are several positive
results for k-means, k-median [8, 12, 28], and min-sum [8, 9, 49] under approximation stability. Approximation stability is a stronger notion than perturbation resilience. Formally, approximation
stability implies (α, ϵ )-perturbation resilience when the parameters α and ϵ are the same (consequently, when ϵ = 0, approximation stability implies α-perturbation resilience) [8]. Ostrovsky
et al. [43] show how to efficiently cluster instances in which the k-means clustering cost is much
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020. 
22:6 M.-F. Balcan et al.
lower than the (k − 1)-means cost. Kumar and Kannan [35] give an efficient clustering algorithm
for instances in which the projection of any point onto the line between its cluster center to any
other cluster center is a large additive factor closer to its own center than the other center. This
result was later improved along multiple axes by Awasthi and Sheffet [7]. These other notions
of stability are similar but orthogonal to perturbation resilience in the sense that neither definition implies perturbation resilience or vice versa. There are many other works that show positive
results for different natural notions of stability in various settings [3, 5, 28, 29, 35, 36, 44].
2 PRELIMINARIES AND BASIC PROPERTIES
A clustering instance (S,d) consists of a set S of n points, a distance function d : S × S → R≥0, and
an integer k. For a point u ∈ S and a set A ⊆ S, we define d(A,u) = minv ∈A d(v,u). The k-center
objective is to find a set of points X = {x1,..., xk } ⊆ S called centers to minimize maxv ∈S d(X,v).
We denote VorX,d (x) = {v ∈ S | x = argminy ∈Xd(y,v)},
1 the Voronoi tile of x ∈ X induced byX on
the set of points S in metric d, and we denote VorX,d (X
) =
x ∈X VorX (x) for a subsetX ⊆ X. We
often write VorX (x) and VorX (X
) when d is clear from context. We refer to the Voronoi partition
induced by X as a clustering (we only consider Voronoi partitions as valid k-center solutions).
Throughout the article, we denote the clustering with the minimum cost with respect to d by
OPT = {C1,...,Ck }, we denote the radius of OPT by r ∗, and we denote the optimal centers by
c1,...,ck , where ci is the center of Ci for all 1 ≤ i ≤ k. We use Br (c) to denote a ball of radius r
centered at point c.
Some of our results assume distance functions that are metrics, and some of our results assume
asymmetric distance functions. A distance function d is a metric if
(1) for all u,v, d(u,v) ≥ 0,
(2) for all u,v, d(u,v) = 0 if and only if u = v,
(3) for all u,v,w, d(u,w) ≤ d(u,v) + d(v,w), and
(4) for all u,v, d(u,v) = d(v,u).
An asymmetric distance function satisifies (1), (2), and (3), but not (4).
Now, we formally define perturbation resilience, a notion introduced by Bilu and Linial [15] for
Max Cut and by Awasthi et al. [6] for clustering. We say that d is an α-perturbation of the distance
function d, if for all u,v ∈ S, d(u,v) ≤ d
(u,v) ≤ αd(u,v).
2
Definition 2.1 (Perturbation Resilience). A clustering instance (S,d) satisfies α-perturbation resilience (α-PR) if for any α-perturbation d of d, the optimal clustering C under d is unique and
equal to OPT .
Note that the optimal centers might change under an α-perturbation, but the optimal clustering
must stay the same. This is a well-studied assumption for clustering problems; however, one downside is that it assumes every point must stay in its own optimal cluster following a perturbation.
This motivates a relaxed variant of α-perturbation resilience, called (α, ϵ )-perturbation resilience,
that allows a small change in the optimal clustering when distances are perturbed. We say that
two clusterings C and C are ϵ-close if minσ
k
i=1



Ci \ C
σ (i)


 ≤ ϵn, where σ is a permutation on
[k] = {1,..., k}.
1In general, argminy∈X d (y, v) might be a set. All of the Voronoi tilings defined in this work (unless otherwise noted) are
provably unique due to the perturbation resilience assumptions defined later.
2We only consider perturbations in which the distances increase, because without loss of generality, we can scale the
distances to simulate decreasing distances.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.       
k-center Clustering under Perturbation Resilience 22:7
Definition 2.2 ((α, ϵ )-perturbation Resilience). A clustering instance (S,d) satisfies (α, ϵ )- perturbation resilience if for any α-perturbation d of d, each optimal clustering C under d is ϵ-close to
OPT .
In Definitions 2.1 and 2.2, we do not assume that the α-perturbations satisfy the triangle inequality. Angelidakis et al. [1] recently studied the weaker definition in which the α-perturbations
must satisfy the triangle inequality, called metric perturbation resilience. We can update these definitions accordingly. For symmetric clustering objectives, α-metric perturbations are restricted to
metrics. For asymmetric clustering objectives, the α-metric perturbations must satisfy the directed
triangle inequality.
Definition 2.3 (Metric Perturbation Resilience). A clustering instance (S,d) satisfies α-metric perturbation resilience (α-MPR) if for any α-metric perturbationd of d the optimal clustering C under
d is unique and equal to OPT .
In our arguments, we will sometimes convert a non-metric perturbation d into a metric perturbation by taking the metric completion d of d (also referred to as the shortest-path metric on d
)
by setting the distances in d as the length of the shortest path on the graph whose edges are the
lengths in d
. Note that for all u,v, we have d(u,v) ≤ d(u,v), since d was originally a metric.
2.1 Local Perturbation Resilience
In the previous section, we defined α-perturbation resilience and the more relaxed (α, ϵ )-
perturbation resilience. However, even assuming (α, ϵ )-perturbation resilience is strong in the
sense that it applies to every cluster, e.g., the entire clustering instance cannot change by more
than an ϵ fraction after a perturbation. Now, we define perturbation resilience for an optimal cluster rather than the entire dataset. All prior works have considered perturbation resilience with
respect to the entire dataset.
Definition 2.4 (Local Perturbation Resilience). Given a clustering instance (S,d), a clusterC satisfies α-perturbation resilience (α-PR) if for each α-perturbation d of d each optimal clustering C
under d contains C.
As a sanity check, we show that a clustering is perturbation resilient if and only if every optimal
cluster satisfies perturbation resilience.
Fact 2.5. A clustering instance (S,d) satisfies α-PR if and only if each optimal cluster satisfies
α-PR.
Proof. Given a clustering instance (S,d), the forward direction follows by definition: Assume
(S,d) satisfies α-PR, and given an optimal cluster Ci , then for each α-perturbation d
, the optimal clustering stays the same under d
; therefore, Ci is contained in the optimal clustering under
d
. Now, we prove the reverse direction. Given a clustering instance with optimal clustering C,
and given an α-perturbation d
, let the optimal clustering under d be C
. For each Ci ∈ C, by
assumption, Ci satisfies α-PR, so Ci ∈ C
. Therefore, C = C
.
Next, we define the local version of (α, ϵ )-PR. Two clusters Ci and Cj are ϵ-close if |Ci \ Cj | +
|Cj \ Ci | ≤ ϵ.
Definition 2.6 (Local (α, ϵ )-perturbation Resilience). Given a clustering instance (S,d), an optimal
cluster C satisfies (α, ϵ )-PR if for any α-perturbation d of d each optimal clustering C under d
contains a cluster C that is ϵ-close to C.
In Sections 5 and 6, we will consider a slightly stronger notion of local perturbation resilience.
Informally, an optimal cluster satisfies α-strong local perturbation resilience if it is α-PR and all
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020. 
22:8 M.-F. Balcan et al.
nearby optimal clusters are also α-PR. We will sometimes be able to prove guarantees for clusters
satisfying strong local perturbation resilience that are not true under standard local perturbation
resilience.
Definition 2.7 (Strong Local Perturbation Resilience). Given a clustering instance (S,d), a clusterC
satisfies α-strong local perturbation resilience (α-SLPR) if for each C such that there exists u ∈ C,
v ∈ C
, and d(u,v) ≤ r ∗, then C is α-PR (any cluster that is close to C must be α-PR).
Note that a typical use for local perturbation resilience is when we can divide the input point
set into two (or more) sections and some sections consist of clusters that satisfy local perturbation
resilience while other sections do not. In this case, all clusters in the perturbation resilient section
also satisfy strong local perturbation resilience, except potentially the clusters on the border of the
section (see Figure 5).
To conclude this section, we state a lemma for asymmetric (and symmetric) k-center, which
allows us to reason about a specific class of α-perturbations that will be important throughout the
article. We give two versions of the lemma, each of which will be useful in different sections of the
article.
Lemma 2.8. Given a clustering instance (S,d) and α ≥ 1,
(1) assume we have an α-perturbation d of d with the following property: for all p,q, if d(p,q) ≥
r ∗, then d
(p,q) ≥ αr ∗. Then the optimal cost under d is αr ∗.
(2) assume we have an α-perturbation d of d with the following property: for all u,v, either
d
(u,v) = min(αr ∗, αd(u,v)) or d
(u,v) = αd(u,v). Then the optimal cost under d is αr ∗.
Proof.
(1) Assume there exists a set of centers C = {c
1,...,c
k } whose k-center cost under d is
strictly less than αr ∗. Then for all i and s ∈ VorC
,d (c
i ), d
(c
i,s) < αr ∗, implying d(c
i,s) <
r ∗ by construction. It follows that the k-center cost of C under d is r ∗, which is a contradiction. Therefore, the optimal cost under d must be αr ∗.
(2) Given u,v such that d(u,v) ≥ r ∗, then d
(u,v) ≥ αr ∗ by construction. Now the proof follows from part one.
3 k-CENTER UNDER PERTURBATION RESILIENCE
In this section, we provide efficient algorithms for finding the optimal clustering for symmetric and asymmetric instances of k-center under 2-perturbation resilience. Our results directly
improve on the result by Balcan and Liang [11] for symmetric k-center under (1 + √
2)-
perturbation resilience. We also show that it is NP-hard to recover OPT even for symmetric
k-center instance under (2 − δ )-perturbation resilience. As an immediate consequence, our results
are tight for both symmetric and asymmetric k-center instances. This is the first problem for which
the exact value of perturbation resilience is found (α = 2), where the problem switches from efficiently computable to NP-hard.
First, we show that any α-approximation algorithm returns the optimal solution for αperturbation resilient instances. An immediate consequence is an algorithm for symmetric kcenter under 2-perturbation resilience. Next, we provide a novel algorithm for asymmetric k-center
under 2-perturbation resilience. Finally, we show hardness of k-center under (2 − δ )-PR.
3.1 α-approximations are Optimal under α-PR
The following theorem shows that any α-approximation algorithm for k-center will return the
optimal solution on clustering instances that are α-perturbation resilient:
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020. 
k-center Clustering under Perturbation Resilience 22:9
Theorem 3.1. Given a clustering instance (S,d) satisfying α-perturbation resilience for asymmetric k-center and a set C of k centers that is an α-approximation, i.e., for all p ∈ S, d(C,p) ≤ αr ∗, then
the Voronoi partition induced by C is the optimal clustering.
Proof. For a point p ∈ S, letc(p) := argminc ∈Cd(c,p), the closest center inC to p. The idea is to
construct an α-perturbation in which C is the optimal solution by increasing all distances except
between p and c(p) for all p. Then the theorem will follow by using the definition of perturbation
resilience.
By assumption, ∀p ∈ S, d(c(p),p) ≤ αr ∗. Create a perturbation d as follows: Increase all distances by a factor of α, except for all p ∈ S, set d
(c(p),p) = min(αd(c(p),p), αr ∗) (recall in Definition 2.1, the perturbation need not satisfy the triangle inequality). Then no distances were increased by more than a factor of α. And, since we had that d(c(p),p) ≤ αr ∗, no distances decrease
either. Therefore, d is an α-perturbation of d. By Lemma 2.8, the optimal cost for d is αr ∗. Also,
C achieves cost at most αr ∗ by construction, so C is an optimal set of centers under d
. Then, by
α-perturbation resilience, the Voronoi partition induced by C under d is the optimal clustering.
Finally, we show the Voronoi partition of C under d is the same as the Voronoi partition of C
under d
. Given p ∈ S whose closest point in C is c(p) under d, then under d
, all distances from p
to C \ {c(p)} increased by exactly α, and d(p,c(p)) increased by at most α. Therefore, the closest
point in C to p under d is still c(p).
3.2 Asymmetric k-center under 2-PR
An immediate consequence of Theorem 3.1 is that we have an exact algorithm for symmetric kcenter under 2-perturbation resilience by running a simple 2-approximation algorithm (e.g., References [24, 27, 30]). However, Theorem 3.1 only gives an algorithm for asymmetric k-center under
O(log∗ (k))-perturbation resilience. Next, we show it is possible to substantially improve the latter
result.
One of the challenges involved in dealing with asymmetric k-center instances is the fact that
even though for all p ∈ Ci , d(ci,p) ≤ r ∗, the reverse distance, d(p,ci ), might be arbitrarily large.
Such points for which d(p,ci )  r ∗ pose a challenge to the structure of the clusters, as they can be
very close to points or even centers of other clusters. To deal with this challenge, we first define
the notion of a center-capturing vertex [48].
Definition 3.2. Given an asymmetric k-center clustering instance (S,d), a point v ∈ S is a centercapturing vertex (CCV) if for all u ∈ S, d(u,v) ≤ r ∗ implies d(v,u) ≤ r ∗.
As the name suggests, each CCV p ∈ Ci “captures” its center in the sense that d(p,ci ) ≤ r ∗. We
define the set of center-capturing vertices A = {p | p is a CCV}. Intuitively speaking, these points
behave similarly to a set of points with symmetric distances up to a distance r ∗. To explore this,
we define a desirable property of A with respect to the optimal clustering.
Definition 3.3. A is said to respect the structure of OPT if
(1) ci ∈ A for all i ∈ [k], and
(2) for all p ∈ S \ A, if A(p) := arg minq ∈A d(q,p) ∈ Ci , then p ∈ Ci .
For all i, defineC
i = Ci ∩ A(which is in fact the optimal clustering ofA). Satisfying Definition 3.3
implies that if we can optimally cluster A, then we can optimally cluster the entire instance (formalized in Theorem 3.6). Thus, our goal is to show that A does indeed respect the structure of
OPT and to show how to return C
1,...,C
k .
Intuitively, A is similar to a symmetric 2-perturbation resilient clustering instance. However,
some structure is no longer there; for instance, a point p may be at distance ≤ 2r ∗ from every
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020. 
22:10 M.-F. Balcan et al.
Fig. 1. Properties of a 2-perturbation resilient instance of aymmetric k-center that are used for clustering.
point in a different cluster, which is not true for 2-perturbation resilient instances. This implies
we cannot simply run a 2-approximation algorithm on the set A, as we did in the previous section.
However, we show that the remaining structural properties are sufficient to optimally cluster A. To
this end, we define two properties and show how they lead to an algorithm that returnsC
1,...,C
k
and help us prove that A respects the structure of OPT .
The first of these properties requires each point to be closer to its center than any point in
another cluster [6].
Property (1): For all p ∈ C
i and q ∈ C
ji , d(ci,p) < d(q,p).
The second property requires that any point within distance r ∗ of a cluster center belongs to
that cluster.
Property (2): For all i  j and q ∈ Cj , d(q,ci ) > r ∗ (see Figure 1). A weaker version of this property
was introduced by Balcan and Liang [11].
Let us illustrate how these properties allow us to optimally cluster A.
3 Consider a ball of radius
r ∗ around a center ci . By Property 2, such a ball exactly captures C
i . Furthermore, by Property 1,
any point in this ball is closer to the center than to points outside of the ball. Is this true for a ball
of radius r ∗ around a general point p? Not necessarily. If this ball contains a point q ∈ C
j from a
different cluster, then q will be closer to a point outside the ball than to p (namely, cj , which is
guaranteed to be outside of the ball by Property 2). This allows us to determine that the center of
such a ball must not be an optimal center.
This structure motivates our Algorithm 1 for asymmetric k-center under 2-perturbation resilience. At a high level, we start by constructing the set A, which can be done in polynomial
time (if r ∗ is not known, then we can use a guess-and-check wrapper). Then, we create the set of
all balls of radius r ∗ around all points in A. Next, we prune this set by throwing out any ball that
contains a point farther from its center than to a point outside the ball. We also throw out any ball
that is a subset of another one. Our claim is that the remaining balls are exactlyC
1,...,C
k . Finally,
we add the points in S \ A to their closest point in A.
Formal details of our analysis.
Lemma 3.4. Properties 1 and 2 hold for asymmetric k-center instances satisfying 2-perturbation
resilience.
Proof. Property 1: Assume false, d(q,p) ≤ d(ci,p). The idea will be that, since q is in A, it is
close to its own center, so we can construct a perturbation in which q replaces its center cj . Then
p will join q’s cluster, causing a contradiction. Construct the following d
:
d
(s,t) =

min(2r ∗, 2d(s,t)) if s = q, t ∈ Cj ∪ {p},
2d(s,t) otherwise.
3Other algorithms work, such as single linkage with dynamic programming at the end to find the minimum cost pruning
of k clusters. However, our algorithm is able to recognize optimal clusters locally (without a complete view of the point
set).
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.  
k-center Clustering under Perturbation Resilience 22:11
ALGORITHM 1: Asymmetric k-center algorithm under 2-PR
Input: Asymmetric k-center instance (S,d), distance r ∗ (or try all possible candidates)
Create symmetric set
• Build set A = {p | ∀q,d(q,p) ≤ r ∗ =⇒ d(p,q) ≤ r ∗}
Create candidate balls
• ∀c ∈ A, construct Gc = {p ∈ A | d(c,p) ≤ r ∗}.
• Define G = {Gc }c ∈A.
Prune balls
• ∀Gc , if ∃p ∈ Gc , q ∈ A \ Gc such that d(q,p) < d(c,p), then set G = G \ Gc .
• ∀p,q such that Gp ⊆ Gq, set G = G \ Gp .
Insert remaining points
• ∀p  A, add p to Gq, where q = arg mins ∈A d(s,p).
Output: G
This is a 2-perturbation, because for all q ∈ Cj ∪ {p}, d(q,q
) ≤ 2r ∗. Then, by Lemma 2.8, the
optimal cost is 2r ∗. The set of centers {c }
k
i=1 \ {cj}∪{q} achieves the optimal cost, since q is distance 2r ∗ from Cj , and all other clusters have the same center as in OPT (achieving radius 2r ∗).
Then for all c, d
(q,p) ≤ d
(ci,p) ≤ d
(c,p). Then, we can construct a 2-perturbation in which q
becomes the center of Cj , and then q is the best center for p, so we have a contradiction.
Property 2: Assume on the contrary that there exists q ∈ Cj , i  j such that d(q,ci ) ≤ r ∗. Now,
we will define a d in which q can become a center for Ci .
d
(s,t) =

min(2r ∗, 2d(s,t)) if s = q, t ∈ Ci,
2d(s,t) otherwise.
This is a 2-perturbation, because for all p ∈ Ci , d(q,p) ≤ 2r ∗. Then, by Lemma 2.8, the optimal
cost is 2r ∗. The set of centers {c }
k
i=1 \ {ci}∪{q} achieves the optimal cost, since q is distance 2r ∗
from Ci , and all other clusters have the same center as in OPT (achieving radius 2r ∗). But the
clustering with centers {c }
k
i=1 \ {ci}∪{q} is different from OPT , since (at the very least) q and ci
are in different clusters. This contradicts 2-perturbation resilience.
Lemma 3.5. The set A respects the structure of OPT .
Proof. From Lemma 3.4, we can use Property 2 in our analysis. First, we show that ci ∈ A for
all i ∈ [k]. Given ci , ∀p ∈ Ci , then d(ci,p) ≤ r ∗ by definition of OPT . ∀q  Ci , then by Property 2,
d(q,ci ) > r ∗. It follows that for any point p ∈ S, it cannot be the case that d(p,ci ) ≤ r ∗ andd(ci,p) >
r ∗. Therefore, ci ∈ A.
Now, we show that for all p ∈ S \ A, if A(p) ∈ Ci , then p ∈ Ci . Given p ∈ S \ A, let p ∈ Ci and
assume towards contradiction that q = A(p) ∈ Cj for some i  j. We will construct a 2-perturbation
d in which q replacescj as the center for Cj and p switches from Ci to Cj , causing a contradiction.
We construct d as follows: All distances are increased by a factor of 2 except for d(q,p) and d(q,q
)
for all q ∈ Cj . These distances are increased by a factor of 2 up to 2r ∗. Formally,
d
(s,t) =

min(2r ∗, 2d(s,t)) if s = q, t ∈ Cj ∪ {p},
2d(s,t) otherwise.
This is a 2-perturbation, because d(q,Cj) ≤ 2r ∗. Then, by Lemma 2.8, the optimal cost is 2r ∗.
The set of centers {c }
k
i=1 \ {cj}∪{q} achieves the optimal cost, since q is distance 2r ∗ fromCj , and
all other clusters have the same center as in OPT (achieving radius 2r ∗). But consider the point
p. Since all centers are in A and q is the closest point to p in A, then q is the center for p under d
.
Therefore, the optimal clustering under d is different from OPT , so we have a contradiction.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.            
22:12 M.-F. Balcan et al.
Now, we are ready to show Algorithm 1 returns the optimal clustering.
Theorem 3.6. Algorithm 1 returns the exact solution for asymmetric k-center under 2-perturbation
resilience.
Proof. In this proof, we refer to the first line of Prune balls in Algorithm 1 as Pruning step 1
and the second line as Pruning step 2. First, we must show that after Pruning step 2, the remaining sets are exactlyC
i = Ci ∩ A for all i ∈ [k]. We prove this in three steps: The setsGci correspond
to C
i , these sets are not thrown out in Pruning step 1 and Pruning step 2, and all other sets are
thrown out in steps Pruning step 1 and Pruning step 2. Because of Lemma 3.4, we can use
Properties 1 and 2.
From Lemma 3.5, all centers are in A, soGci will be created in step 2. For all p ∈ Ci , d(ci,p) ≤ r ∗.
For all q  C
i , by Property 2, we have d(q,ci ) > r ∗, and, since ci and q are inA, we have d(ci,q) > r ∗
as well. It follows that Gci = C
i .
Given s ∈ Gci and t ∈ A \ Gci , we have s ∈ C
i and t ∈ C
j for some j  i. If d(t,s) < d(ci,s), then
we get a contradiction from Property 1. Therefore, for all i, Gci is not thrown out in step Pruning
step 1.
Recall we showed that Gci = C
i for all i. If Gp ⊆ Gci , then Gp will be thrown out in Pruning
step 2 (ifGp = Gci , it does not matter which set we keep, so without loss of generality, say that we
keep Gci ). If Gp is not thrown out in Pruning step 2, then there must exist s ∈ Gp ∩C
j for some
j  i. If s = cj , then d(p,cj) ≤ r ∗ and we get a contradiction from Property 2. So, we can assume
s is a non-center (and that cj  Gp ). But d(cj,s) < d(p,s) from Property 1, and therefore Gp will
be thrown out in Pruning step 1. We conclude that for all non-centers p, Gp is thrown out in
Pruning step 1 or Pruning step 2. Thus, the remaining sets after Pruning step 2 are exactly
C
1,...,C
k .
Finally, by Lemma 3.5, for each p ∈ Ci \ A, A(p) ∈ Ci , so p will be added to Gci . Therefore, the
final output is C1,...,Ck .
3.3 Hardness of k-center under Perturbation Resilience
In this section, we show NP-hardness for k-center under (2 − δ )-perturbation resilience. We show
that if there exists a polynomial time algorithm that returns the optimal solution for symmetric
k-center under (2 − δ )-perturbation resilience,4 then NP = RP even under the condition that the
optimal clusters all have size at least n
2k . Because symmetric k-center is a special case of asymmetric
k-center, we have the same hardness results for asymmetric k-center. This proves Theorem 3.6 is
tight with respect to the level of perturbation resilience assumed.
Theorem 3.7. There is no polynomial time algorithm for finding the optimal k-center clustering
under (2 − δ )-perturbation resilience, even when assuming all optimal clusters have size at least n
2k ,
unless NP = RP.
We show a reduction from a special case of Dominating Set that we call Unambiguous-BalancedPerfect Dominating Set. Below, we formally define this problem and all intermediate problems. Part
of our reduction is based on the proof of Ben-David and Reyzin [13], who showed a reduction from
a variant of dominating set to the weaker problem of clustering under (2 − δ )-center proximity. αcenter proximity is the property that for all p ∈ Ci and j  i, αd(ci,p) < d(cj,p), and it follows from
α-perturbation resilience. We use four NP-hard problems in a chain of reductions. Here, we define
all of these problems up front. We introduce the “balanced” variants of two existing problems.
4In fact, our result holds even under the strictly stronger notion of approximation stability [8].
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.      
k-center Clustering under Perturbation Resilience 22:13
Definition 3.8 (3-Dimensional Matching (3DM) [33]). We are given three disjoint sets X1, X2, and
X3 each of size m, and a set T such that t ∈ T is a triple t = (x1, x2, x3) where x1 ∈ X1, x2 ∈ X2,
and x3 ∈ X3. The problem is to find a set M ⊆ T of size m that exactly hits all the elements in
X1 ∪ X2 ∪ X3. In other words, for all pairs (x1, x2, x3), (y1,y2,y3) ∈ M, it is the case that x1  y1,
x2  y2, and x3  y3.
Definition 3.9 (Balanced-3-Dimensional Matching (B3DM)). This is the 3DM problem
(X1,X2,X3,T ) with the additional constraint that 2m ≤ |T | ≤ 3m, where |X1 | = |X2 | = |X3 | = m.
Definition 3.10 (Perfect Dominating Set (PDS) [13]). Given a graph G = (V, E) and an integer k,
the problem is to find a set of vertices D ⊆ V of size k such that for all v ∈ V \ D, there exists
exactly one d ∈ D such that (d,v) ∈ E (then, we say d “hits” v).
Definition 3.11 (Balanced-Perfect-Dominating Set (BPDS)). This is the PDS problem (G, k) with
the additional assumption that if the graph has n vertices and a dominating set of size k exists,
then each vertex in the dominating set hits at least n
2k vertices.
Additionally, each problem has an “Unambiguous” variant, which is the added constraint that
the problem has at most one solution. Valiant and Vazirani [46] showed that Unambiguous-3SAT
is hard unless NP = RP. To show the Unambiguous version of another problem is hard, one must
establish a parsimonious polynomial time reduction from Unambiguous-3SAT to that problem. A
parsimonious reduction is one that preserves the number of solutions. For two problems A and B,
we denote A ≤par B to mean there is a reduction from A to B that is parsimonious and polynomial
time. Some common reductions involve 1-to-1 mappings and are therefore trivially parsimonious,
but many other common reductions are not parsimonious. For instance, the standard reduction
from 3SAT to 3DM is not parsimonious [34], yet there is a more roundabout series of reductions
that are all parsimonious. To prove Theorem 3.7, we start with the claim that Unambiguous-BPDS
is hard unless NP = RP. We use a parsimonious series of reductions from 3SAT to B3DM to BPDS.
All of these reductions are from prior work, yet we verify parsimony and balancedness.
Lemma 3.12. There is no polynomial time algorithm for Unambiguous-BPDS unless NP = RP.
Proof. We use a series of parsimonious reductions from 3SAT to B3DM to BPDS. Then it follows from the result by Valiant and Vazirani [46] that there is no polynomial time algorithm for
Unambiguous-BPDS unless NP = RP.
To show that B3DM is NP-hard, we use the reduction of Dyer and Frieze [25], who showed that
Planar-3DM is NP-hard. While planarity is not important for the purpose of our problems, their
reduction from 3SAT has two other nice properties that we crucially use. First, the reduction is
parsimonious, as pointed out by Hunt III et al. [31]. Second, given their 3DM instance X1,X2,X3,T ,
each element in X1 ∪ X2 ∪ X3 appears in either two or three tuples in T . (Dyer and Frieze [25]
mention this observation just before their Theorem 2.3.) From this, it follows that 2m ≤ |T | ≤ 3m,
and so their reduction proves that B3DM is NP-hard via a parsimonious reduction from 3SAT.
Next, we reduce B3DM to BPDS using a reduction similar to the reduction by Ben-David and
Reyzin [13]. Their reduction maps every element in X1 ∪ X2 ∪ X3 ∪T to a vertex in V and adds
one extra vertex v to V . There is an edge from each element (x1, x2, x3) ∈ T to the corresponding
elements x1 ∈ X1, x2 ∈ X2, and x3 ∈ X3. Furthermore, there is an edge fromv to every element inT .
Ben-David and Reyzin [13] show that if the 3DM instance is a yes instance with matching M ⊆ T ,
then the minimum dominating set is v ∪ M. Now, we will verify this same reduction can be used
to reduce B3DM to BPDS. If we start with B3DM, then our graph has |X1 | + |X2 | + |X3 | + |T | + 1 ≤
6m + 1 vertices, since |T | ≤ 3m, so n ≤ 6m + 1. Also note that in the yes instance, the dominating
set is size m + 1 by construction. Therefore, to verify the reduction to BPDS, we must show that
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.   
22:14 M.-F. Balcan et al.
in the yes instance, each node in the dominating set hits ≥ 6m+1
2(m+1) nodes. Given t ∈ M, t hits 3
nodes in the graph and n
2(m+1) ≤ 6m+1
2m+2 ≤ 3. The final node in the dominating set is v, and v hits
|T | − m ≥ 2m − m = m nodes, and 6m+1
2(m+1) ≤ m when m ≥ 3. Therefore, the resulting instance is a
BPDS instance.
Now, we have verified that there exists a parsimonious reduction 3SAT ≤par BPDS, so it follows
that there is no polynomial time algorithm for Unambiguous-BPDS unless NP = RP.
Now, we can prove Theorem 3.7 by giving a reduction from Unambiguous-BPDS to k-center
clustering under (2 − δ )-perturbation resilience, where all clusters are size ≥ n
2k . We use the same
reduction as Ben-David and Reyzin [13], but we must verify that the resulting instance is (2 −
δ )-perturbation resilient. Note that our reduction requires the Unambiguous variant while the
reduction of Ben-David and Reyzin [13] does not, since we are reducing to a stronger problem.
Proof of Theorem 3.7. From Lemma 3.12, there is no polynomial time algorithm for Unambiguous BPDS unless NP = RP. Now for all δ > 0, we reduce from Unambiguous-BPDS to kcenter clustering and show the resulting instance has all cluster sizes ≥ n
2k and satisfies (2 − δ )-
perturbation resilience.
Given an instance of Unambiguous-BPDS, for every v ∈ V , create a point v ∈ S in the clustering
instance. For every edge (u,v) ∈ E, let d(u,v) = 1, otherwise let d(u,v) = 2. Since all distances are
either 1 or 2, the triangle inequality is trivially satisfied. Then a k-center solution of cost 1 exists
if and only if there exists a dominating set of size k.
Since each vertex in the dominating set hits at least n
2k vertices, the resulting clusters will be
size at least n
2k + 1. Additionally, if there exists a dominating set of size k, then the corresponding
optimal k-center clustering has cost 1. Because this dominating set is perfect and unique, any other
clustering has cost 2. It follows that the k-center instance is (2 − δ )-perturbation resilient.
4 k-CENTER UNDER METRIC PERTURBATION RESILIENCE
In this section, we extend the results from Section 3 to the metric perturbation resilience setting
[1]. We first give a generalization of Lemma 2.8 to show that it can be extended to metric perturbation resilience. Then, we show how this immediately leads to corollaries of Theorem 3.1 and
Theorem 3.6 extended to the metric perturbation resilience setting.
Recall that in the proofs from the previous section, we created α-perturbations d by increasing
all distances by α, except a few distances d(u,v) ≤ αr ∗ that we increased to min(αd(u,v), αr ∗). In
this specific type of α-perturbation, we used the crucial property that the optimal clustering has
cost αr ∗ (Lemma 2.8). However, d may be highly non-metric, so our challenge is arguing that the
proof still goes through after taking the metric completion of d (recall the metric completion of
d is defined as the shortest path metric on d
). In the following lemma, we show that Lemma 2.8
remains true after taking the metric completion of the perturbation.
Lemma 4.1. Given α ≥ 1 and an asymmetric k-center clustering instance (S,d) with optimal radius
r ∗, let d denote an α-perturbation such that for all u,v, either d(u,v) = min(αr ∗, αd(u,v)) or
d(u,v) = αd(u,v). Let d denote the metric completion of d. Then d is an α-metric perturbation
of d, and the optimal cost under d is αr ∗.
Proof. By construction, d
(u,v) ≤ d(u,v) ≤ αd(u,v). Since d satisfies the triangle inequality,
we have that d(u,v) ≤ d
(u,v), so d is a valid α-metric perturbation of d.
Now given u,v such that d(u,v) ≥ r ∗, we will prove that d
(u,v) ≥ αr ∗. By construction,
d(u,v) ≥ αr ∗. Then, since d is the metric completion of d, there exists a path u = u0–
u1–··· –us−1–us = v such that d
(u,v) = s−1 i=0 d
(ui,ui+1) and for all 0 ≤ i ≤ s − 1, d
(ui,ui+1) =
d(ui,ui+1).
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.  
k-center Clustering under Perturbation Resilience 22:15
Case 1: there exists an i such that d(ui,ui+1) ≥ αr ∗. Then d
(u,v) ≥ αr ∗ and we are done.
Case 2: for all 0 ≤ i ≤ s − 1, d(ui,ui+1) < αr ∗. Then, by construction, d
(ui,ui+1) =
d(ui,ui+1) = αd(ui,ui+1), and so d
(u,v) = s−1 i=0 d
(ui,ui+1) = α s−1 i=0 d(ui,ui+1) ≥ αd(u,v) ≥
αr ∗.
We have proven that for all u,v, if d(u,v) ≥ r ∗, then d
(u,v) ≥ αr ∗. Then, by Lemma 2.8, the
optimal cost under d must be αr ∗.
Recall that metric perturbation resilience states that the optimal solution does not change under
any metric perturbation to the input distances. In the proofs of Theorems 3.1 and 3.6, the only
perturbations constructed were the type as in Lemma 2.8. Since Lemma 4.1 shows that even the
metric closures of these perturbations still have cost at most αr ∗, Theorems 3.1 and 3.6 are true
even under metric perturbation resilience.
Corollary 4.2. Given a clustering instance (S,d) satisfying α-metric perturbation resilience for
asymmetric k-center and a set C of k centers that is an α-approximation, i.e., ∀p ∈ S, ∃c ∈ C such
that d(c,p) ≤ αr ∗, then the Voronoi partition induced by C is the optimal clustering.
Corollary 4.3. Algorithm 1 returns the exact solution for asymmetric k-center under 2-metric
perturbation resilience.
5 k-CENTER UNDER LOCAL PERTURBATION RESILIENCE
In this section, we further extend the results from Sections 3 and 4 to the local perturbation resilience setting. First, we show that any α-approximation to k-center will return each optimal
α-MPR cluster, i.e., Corollary 4.2 holds even in the local perturbation resilience setting. Then for
asymmetric k-center, we show that a natural modification to the O(log∗ n) approximation algorithm of Vishwanathan [48] leads to an algorithm that maintains its performance in the worst
case while exactly returning each optimal cluster located within a 2-MPR region of the dataset.
This generalizes Corollary 4.3.
5.1 Symmetric k-center
In Section 3, we showed that any α-approximation algorithm for k-center returns the optimal
solution for instances satisfying α-perturbation resilience (and this was generalized to metric perturbation resilience in the previous section). In this section, we extend this result to the local
perturbation resilience setting. We show that any α-approximation will return each (local) α-MPR
cluster. For example, if a clustering instance is half 2-perturbation resilient, then running a 2-
approximation algorithm will return the optimal clusters for half the dataset and a 2-approximation
for the other half.
Theorem 5.1. Given an asymmetric k-center clustering instance (S,d), a set C of k centers that
is an α-approximation, and a clustering C defined as the Voronoi partition induced by C, then each
α-MPR cluster is contained in C.
The proof is very similar to the proof of Theorem 3.1. The key difference is that we reason about
each perturbation resilient cluster individually rather than reasoning about the global structure of
perturbation resilience.
Proof of Theorem 5.1. Given an α-approximate solution C to a clustering instance (S,d),
and given an α-MPR cluster Ci , we will create an α-perturbation as follows: Define C(v) :=
argminc ∈Cd(c,v). For allv ∈ S, set d(v, C(v)) = min{αr ∗, αd(v, C(v))}. For all other pointsu ∈ S,
set d(v,u) = αd(v,u). Then, by Lemma 4.1, the metric completion d of d is an α-perturbation
of d with optimal cost αr ∗. By construction, the cost of C is ≤ αr ∗ under d
; therefore, C is an
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020. 
22:16 M.-F. Balcan et al.
optimal clustering. Denote the set of centers of C by C. By definition of α-MPR, there exists
vi ∈ C such that VorC,d (vi ) = Ci . Now, given v ∈ Ci , argminu ∈Cd
(u,v) = vi , so, by construction,
argminu ∈Cd(u,v) = vi . Therefore, VorC,d (vi ) = Ci , so Ci ∈ C.
5.2 Asymmetric k-center
In Section 3, we gave an algorithm that outputs the optimal clustering for asymmetric k-center under 2-perturbation resilience (Algorithm 1 and Theorem 3.6), and we extended it to metric perturbation resilience in Section 4. In this section, we extend the result further to the local perturbation
resilience setting, and we show how to add a worst-case guarantee of O(log∗ n). Specifically, we
give a new algorithm, which is a natural modification to the O(log∗ n) approximation algorithm
of Vishwanathan [48], and show that it maintains the O(log∗ n) guarantee in the worst case and
furthermore, for each perturbation resilient clusterCi , there is a cluster outputted by the algorithm
that is a superset of Ci and does not contain any other perturbation resilient cluster. As a consequence, if the entire clustering instance satisfies 2-metric perturbation resilience, then the output
of our algorithm is the optimal clustering.
Theorem 5.2. Given an asymmetric k-center clustering instance (S,d) of size n with optimal clustering {C1,...,Ck }, for each 2-MPR cluster Ci , there exists a cluster outputted by Algorithm 3 that
is a superset of Ci and does not contain any other 2-MPR cluster. Furthermore, the overall clustering
returned by Algorithm 3 is an O(log∗ n)-approximation.
At the end of this section, we will also show an algorithm that outputs an optimal cluster Ci
exactly, if Ci and any optimal cluster near Ci are 2-MPR.
Approximation algorithm for asymmetric k-center. We start with a recap of the O(log∗ n) approximation algorithm by Vishwanathan [48]. This was the first nontrivial algorithm for asymmetric k-center, and the approximation ratio was later proven to be tight by Reference [21]. To
explain the algorithm, it is convenient to think of asymmetric k-center as a set covering problem.
Given an asymmetric k-center instance (S,d), define the directed graph D(S,d) = (S,A), where
A = {(u,v) | d(u,v) ≤ r ∗}. For a point v ∈ S, we define Γin(v) and Γout(v) as the set of vertices with
an arc to and from v, respectively, in D(S,d). The asymmetric k-center problem is equivalent to
finding a subset C ⊆ S of size k such that ∪c ∈C Γout(c) = S. We also define Γx
in (v) and Γx
out(v) as the
set of vertices that have a path of length ≤ x to and from v in D(S,d), respectively, and we define
Γx
out(A) =
v ∈A Γx
out(v) for a set A ⊆ S and similarly for Γx
in (A). It is standard to assume the value
of r ∗ is known; since it is one of O(n2) distances, the algorithm can search for the correct value in
polynomial time.
Recall the notion of a center-capturing vertex from Section 3.2: A pointv is a CCV if for allu ∈ S,
d(u,v) ≤ r ∗ implies d(v,u) ≤ r ∗. In other words, v is a CCV if Γin(v) ⊆ Γout(v). Therefore, v’s entire cluster is contained inside Γ2
out(v), which is a nice property that the approximation algorithm
exploits (see Figure 2(a)). At a high level, the approximation algorithm has two phases. In the first
phase, the algorithm iteratively picks a CCV v arbitrarily and removes all points in Γ2
out(v). This
continues until there are no more CCVs. For every CCV picked, the algorithm is guaranteed to remove an entire optimal cluster. In the second phase, the algorithm runs log∗ n rounds of a greedy
set-cover subroutine on the remaining points (see Algorithm 2). To prove the second phase terminates in O(log∗ n) rounds, the analysis crucially assumes there are no CCVs among the remaining
points. We refer the reader to Reference [48] for these details.
Description of our algorithm and analysis. We show a modification to the approximation algorithm of Vishwanathan [48] leads to simultaneous guarantees in the worst case and under local
perturbation resilience. Note that the set of all CCVs is identical to the symmetric set A defined
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.  
k-center Clustering under Perturbation Resilience 22:17
Fig. 2. Examples of a center-capturing vertex (left), and CCV-proximity (right).
ALGORITHM 2: O(log∗ n) approximation algorithm for asymmetric k-center [48]
Input: Asymmetric k-center instance (S,d), optimal radius r ∗ (or try all possible candidates)
Set C = ∅
Phase I: Remove arbitrary CCVs
While there exists an unmarked CCV
• Pick an unmarked CCV c, add c to C, and mark all vertices in Γ2
out(c)
Phase II: Recursive set cover
Set A0 = S \ Γ5
out(C), i = 0.
While |Ai | > k:
• Set A
i+1 = ∅.
• While there exists an unmarked point in Ai :
—Pick v ∈ S, which maximizes Γ5
out(v) ∩ Ai , mark points in Γ5
out(v) ∩ Ai , and add v to A
i+1.
• Set Ai+1 = A
i+1 ∩ A0 and i = i + 1
Output: Centers C ∪ Ai+1
in Section 3.2. In Section 3.2, we showed that all centers are in A; therefore, all centers are CCVs,
assuming 2-PR. In this section, we have that the center of a 2-MPR cluster is a CCV (Lemma 5.4),
which is true by definition ofr ∗, (Ci ⊆ Γout(ci )) and by using the definition of 2-MPR (Γin(ci ) ⊆ Ci ).
Since each 2-MPR center is a CCV, we might hope that we can output the 2-MPR clusters by iteratively choosing a CCV v and removing all points in Γ2
out(v). However, using this approach, we
might remove two or more centers from 2-MPR clusters in the same iteration, which means we
would not output one separate cluster for each 2-MPR cluster. If we try to get around this problem
by iteratively choosing a CCV v and removing all points in Γ1
out(v), then we may not remove one
full cluster in each iteration; so, for example, some of the 2-MPR clusters may be cut in half.
The key challenge is thus carefully specifying which nearby points get marked by each CCV c
chosen by the algorithm. We fix this problem with two modifications that carefully balance the two
guarantees. First, any CCVc chosen will mark points in the following way: For allc ∈ Γin(c), mark
all points in Γout(c
). Intuitively, we still mark points that are two hops fromc, but the first hop must
go backwards, i.e., mark v such that there exists c and d(c
,c) ≤ r ∗ and d(c
,v) ≤ r ∗. This gives
us a useful property: If the algorithm picks a CCV c ∈ Ci and it marks a different 2-MPR center
cj , then the middle hop must be a point q in Cj . However, we know from perturbation resilience
that d(cj,q) < d(c,q). This fact motivates the final modification to the algorithm. Instead of picking
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.
22:18 M.-F. Balcan et al.
Fig. 3. Examples of center-separation (left) and weak CCV-proximity (right).
arbitrary CCVs, we require the algorithm to choose CCVs with an extra structural property that we
call CCV-proximity (Definition 5.3) (see Figure 2(b)). Intuitively, a pointc satisfying CCV-proximity
must be closer than other CCVs to each point in Γin(c). Going back to our previous example, c
will NOT satisfy CCV-proximity, because cj is closer to q, but we will be able to show that all 2-
MPR centers do satisfy CCV-proximity. Thus, Algorithm 3 works as follows: It first chooses points
satisfying CCV-proximity and marks points according to the rule mentioned earlier. When there
are no more points satisfying CCV-proximity, the algorithm chooses regular CCVs. Finally, it runs
Phase II as in Algorithm 2. This ensures that Algorithm 3 will output each 2-MPR center in its own
cluster.
Details for Theorem 5.2. Now, we formally define CCV-proximity. The other properties in the
following definition,center-separation and weak CCV-proximity, are defined in terms of the optimal
clustering, so they cannot be explicitly used by an algorithm, but they will simplify all of our proofs.
Definition 5.3.
(1) An optimal centerci satisfiescenter-separation if any point within distance r ∗ ofci belongs
to its cluster Ci . That is, Γin(ci ) ⊆ Ci (see Figure 3(a)).5
(2) A CCV c ∈ Ci satisfies weak CCV-proximity if, given a CCV c  Ci and a point v ∈ Ci , we
have d(c,v) < d(c
,v) (see Figure 3(b)).6
(3) A point c satisfies CCV-proximity if it is a CCV, and each point in Γin(c) is closer to c
than any CCV outside of Γout(c). That is, for all points v ∈ Γin(c) and CCVs c  Γout(c),
d(c,v) < d(c
,v) (see Figure 2(b)).7
Next, we prove that all 2-MPR centers satisfy center-separation, and all CCVs from a 2-MPR
cluster satisfy CCV-proximity and weak CCV-proximity.
Lemma 5.4. Given an asymmetric k-center clustering instance (S,d) and a 2-MPR cluster Ci ,
(1) ci satisfies center-separation,
(2) any CCV c ∈ Ci satisfies CCV-proximity,
(3) any CCV c ∈ Ci satisfies weak CCV-proximity.
Proof. Given an instance (S,d) and a 2-MPR cluster Ci , we show that Ci has the desired properties.
5Center-separation is the local-PR equivalent of property 2 from Section 3.2. 6This is a variant of α-center proximity [6], a property defined over an entire clustering instance, which states for all i, for
all v ∈ Ci , j  i, we have αd (ci, v) < d (cj, v). Our variant generalizes to local-PR, asymmetric instances, and general
CCVs.
7This is similar to a property in Reference [11].
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.   
k-center Clustering under Perturbation Resilience 22:19
Center separation: Assume there exists a point v ∈ Cj for j  i such that d(v,ci ) ≤ r ∗. The idea
is to construct a 2-perturbation in which v becomes the center for Ci .
d(s,t) =

min(2r ∗, 2d(s,t)) if s = v, t ∈ Ci,
2d(s,t) otherwise.
d is a valid 2-perturbation of d, because for each point u ∈ Ci , d(v,u) ≤ d(v,ci ) + d(ci,u) ≤ 2r ∗.
Define d as the metric completion of d. Then, by Lemma 4.1, d is a 2-metric perturbation with
optimal cost 2r ∗. The set of centers {ci }
k
i=1 \ {ci}∪{v} achieves the optimal cost, sincev is distance
2r ∗ from Ci , and all other clusters have the same center as in OPT (achieving radius 2r ∗). If v is
a noncenter, then {ci }
k
i=1 \ {ci}∪{v} is a valid set of k centers. If v = cj , then add an arbitrary
point v ∈ Cj to this set of centers (it still achieves the optimal cost, since adding another center
can only decrease the cost). Then in this new optimal clustering, ci ’s center is a point in {ci }
k
i=1 \
{ci}∪{v,v
}, none of which are from Ci . We conclude that Ci is no longer an optimal cluster,
contradicting 2-MPR.
Weak CCV-proximity: Given a CCV c ∈ Ci , a CCV c ∈ Cj such that j  i, and a point v ∈ Ci ,
assume to the contrary that d(c
,v) ≤ d(c,v). We will construct a perturbation in which c and
c become centers of their respective clusters, and then v switches clusters. Define the following
perturbation d:
d(s,t) =

min(2r ∗, 2d(s,t)) if s = c, t ∈ Ci or s = c
, t ∈ Cj ∪ {v},
2d(s,t) otherwise.
d is a valid 2-perturbation of d, because for each point u ∈ Ci , d(c,u) ≤ d(c,ci ) + d(ci,u) ≤
2r ∗, for each point u ∈ Cj , d(c
,u) ≤ d(c
,cj) + d(cj ,u) ≤ 2r ∗, and d(c
,v) ≤ d(c,v) ≤ d(c,ci ) +
d(ci,v) ≤ 2r ∗. Define d as the metric completion of d. Then, by Lemma 4.1, d is a 2-metric perturbation with optimal cost 2r ∗. The set of centers {ci }
k
i=1 \ {ci,cj}∪{c,c
} achieves the optimal
cost, since c and c are distance 2r ∗ from Ci and Cj , and all other clusters have the same center as
in OPT (achieving radius 2r ∗). Then, since d
(c
,v) ≤ d(c,v), v can switch clusters, contradicting
perturbation resilience.
CCV-proximity: First, we show thatci is a CCV. By center-separation, we have that Γin(ci ) ⊆ Ci ,
and by definition ofr ∗, we have thatCi ⊆ Γout(ci ). Therefore, Γin(ci ) ⊆ Ci ⊆ Γout(ci ), so ci is a CCV.
Now given a point v ∈ Γin(ci ) and a CCV c  Γout(ci ), from center-separation and definition of r ∗,
we have v ∈ Ci and c ∈ Cj for j  i. Then, from weak CCV-proximity, d(ci,v) < d(c,v).
Now using Lemma 5.4, we can prove Theorem 5.2.
Proof of Theorem 5.2. First, we explain why Algorithm 3 retains the approximation guarantee
of Algorithm 2. Given any CCV c ∈ Ci chosen in Phase I, since c is a CCV, then ci ∈ Γout(c), and by
definition of r ∗, Ci ⊆ Γout(ci ). Therefore, each chosen CCV always marks its cluster, and we start
Phase II with no remaining CCVs. This condition is sufficient for Phase II to return an O(log∗ n)
approximation (Theorem 3.1 from Vishwanathan [48]).
Next, we claim that for each 2-MPR cluster Ci , there exists a cluster outputted by Algorithm 3
that is a superset of Ci and does not contain any other 2-MPR cluster. To prove this claim, we
first show there exists a point from Ci satisfying CCV-proximity that cannot be marked by any
point from a different cluster in Phase I. From Lemma 5.4, ci satisfies CCV-proximity and centerseparation. If a point c  Ci marks ci , then there exists v ∈ Γin(c) ∩ Γin(ci ). By center-separation,
ci  Γout(c), and therefore, since c is a CCV, c  Γout(ci ). But then, from the definition of CCVproximity for ci and c, we have d(c,v) < d(ci,v) and d(ci,v) < d(c,v), so we have reached a contradiction (see Figure 4(a)).
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.        
22:20 M.-F. Balcan et al.
Fig. 4. Example of Algorithm 3 (left) and the proof of Theorem 5.5 (right).
ALGORITHM 3: Algorithm for asymmetric k-center under perturbation resilience
Input: Asymmetric k-center instance (S,d), distance r ∗ (or try all possible candidates)
Set C = ∅.
Phase I: Remove special CCVs
• While there exists an unmarked CCV:
—Pick an unmarked point c that satisfies CCV-proximity. If no such c exists, then pick an
arbitrary unmarked CCV instead. Add c to C, and ∀c ∈ Γin(c), mark Γout(c
).
• For each c ∈ C, let Vc denote c’s Voronoi tile of the marked points induced by C.
Phase II: Recursive set cover
• Run Phase II as in Algorithm 2, outputting Ai+1.
• Compute the Voronoi diagram {V 
c }c ∈C∪Ai+1 of S \ Γ5
out(C) induced by C ∪ Ai+1
• For each c in C, set V 
c = Vc ∪V 
c
Output: Sets {V 
c }c ∈C∪Ai+1
At this point, we know a point c ∈ Ci will always be chosen by the algorithm in Phase I. To
finish the proof, we show that each point v from Ci is closer to c than to any other point c  Ci
chosen as a center in Phase I. Since c and c are both CCVs, this follows directly from weak CCVproximity.8
5.2.1 Strong local perturbation resilience. Theorem 5.2 shows that Algorithm 3 will output each
2-PR center in its own cluster. Given some 2-PR center ci , it is unavoidable that ci might mark a
non 2-PR center cj and capture all points in its cluster. In this section, we show that Algorithm 3
with a slight modification outputs each 2-strong local perturbation resilient cluster exactly. Recall
that intuitively an optimal cluster Ci satisfies α-strong local perturbation resilience if all nearby
clusters satisfy α-perturbation resilience (Definition 2.7).
Intuitively, the nearby 2-PR clusters “shield” Ci from all other points (see Figure 5). The only
modification is that at the end of Phase II, instead of calculating the Voronoi diagram using the
metric d, we assign each point v ∈ S \ Γ5
out(C) to the point in C ∪ Ai+1, which minimizes the path
length in D(S,d), breaking ties by distance to first common vertex in the shortest path.
Theorem 5.5. Given an asymmetric k-center clustering instance (S,d) with optimal clustering
C = {C1,...,Ck }, consider a 2-PR cluster Ci . Assume that for all Cj for which there is v ∈ Cj , u ∈ Ci ,
and d(u,v) ≤ r ∗, we have thatCj is also 2-PR (Ci satisfies 2-strong local perturbation resilience). Then
Algorithm 4 returns Ci exactly.
8It is possible that a center c chosen in Phase 2 may be closer to v than c is to v, causing c to “steal” v; this is unavoidable.
This is why Algorithm 3 separately computes the Voronoi tiling from Phase I and Phase II, and so the final output is
technically not a valid Voronoi tiling over the entire instance S.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.  
k-center Clustering under Perturbation Resilience 22:21
Fig. 5. The white clusters are optimal clusters with no structure, the gray clusters are 2-PR clusters, and
the black clusters are 2-PR clusters that only have neighbors that are also 2-PR (Theorem 5.5). Algorithm 4
outputs the black clusters exactly.
ALGORITHM 4: Outputting optimal clusters for asymmetric k-center under stability
Input: Asymmetric k-center instance (S,d), distance r ∗ (or try all possible candidates)
Set C = ∅.
Phase I: Remove special CCVs
• While there exists an unmarked CCV:
—Pick an unmarked point c that satisfies CCV-proximity. If no such c exists, then pick an
arbitrary unmarked CCV instead. Add c to C, and ∀c ∈ Γin(c), mark Γout(c
).
• For each c ∈ C, let Vc denote c’s Voronoi tile of the marked points induced by C.
Phase II: Recursive set cover
• Run Phase II as in Algorithm 2, outputting Ai+1.
Phase III: Assign points to centers
• For each v ∈ S \ Γ5
out(C), assign v to the centerc ∈ C ∪ Ai+1 with the minimum path length in
D(S,d) from c to v, breaking ties by distance to first common vertex in the shortest path.
• Let V 
c denote the set of vertices in v ∈ S \ Γ5
out(C) assigned to c.
• For each c in C, set V 
c = Vc ∪V 
c
Output: Sets {V 
c }c ∈C∪Ai+1
Proof. Given a 2-PR cluster Ci with the property in the theorem statement, by Theorem 5.2,
there exists a CCV c ∈ Ci from Phase I satisfying CCV-proximity such that Ci ⊆ Vc . Our goal is to
show that Vc = Ci . First, we show that Γin(c) ⊆ Ci , which will help us prove the theorem. Assume
towards contradiction that there exists a point v ∈ Γin(c) \ Ci . Let v ∈ Cj . Since c is a CCV, we have
v ∈ Γout(c), so Cj must be 2-PR by definition. By Lemma 5.4,cj is a CCV and d(cj,v) < d(c,v). But
this violates CCV-proximity of c, so we have reached a contradiction. Therefore, Γin(c) ⊆ Ci .
To finish the proof, we must show that Vc ⊆ Ci . Assume towards contradiction there exists
v ∈ Vc \ Ci at the end of the algorithm.
Case 1: v was marked by c in Phase I. Let v ∈ Cj . Then there exists a point u ∈ Γin(c) such that
v ∈ Γout(u). From the previous paragraph, we have that Γin(c) ⊆ Ci , sou ∈ Ci . Therefore,v ∈ Γout(u)
implies Cj must be 2-PR. Since v is from a different 2-PR cluster, it cannot be contained in Vc , so
we have reached a contradiction.
Case 2: v was not marked by c in Phase I. Denote the shortest path in D(S,d) from c to v by
(c = v0,v1,...,vL−1,vL = v). Let v ∈ Cj denote the first vertex on the shortest path that is not in
Ci (such a vertex must exist, because v  Ci ). Then v−1 ∈ Ci and d(v−1,v ) ≤ r ∗, so Cj is 2-PR by
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.     
22:22 M.-F. Balcan et al.
the assumption in Theorem 5.5. Letc denote the CCV chosen in Phase I such that Cj ⊆ Vc. Then,
by weak CCV-proximity from Lemma 5.4, we have d(c
,v ) < d(c,v ).
Case 2a: d(c,v ) ≤ r ∗. Then v is the first vertex on the shortest path from c to v and c to v, so
v is the first common vertex on the shortest paths. Since d(c
,v ) < d(c,v ), the algorithm will
choose c as the center for v (see Figure 4(b)).
Case 2b: d(c,v ) > 2r ∗. But, since c ∈ Cj is a CCV, we have d(c
,v ) ≤ d(c
,cj) + d(cj,v ) ≤ 2r ∗,
so the shortest path from c to v on D(S,d) is at most 2, and the shortest path from c to v on D(S,d)
is at least 3. Since v is on the shortest path from c to v, it follows that the shortest path from c to
v is strictly shorter than the shortest path from c to v.
Case 2c: r ∗ < d(c,v ) ≤ 2r ∗. In this case, we will show that d(c
,v ) ≤ r ∗, and therefore, we
conclude the shortest path from c to v is strictly shorter than the shortest path from c to v, as in
Case 2b. Assume towards contradiction that d(c
,v ) > r ∗. Then, we will create a 2-perturbation in
which c and c become centers for their own clusters andv switches clusters. Define the following
perturbation d
:
d
(s,t) =
⎧⎪⎪
⎨
⎪⎪
⎩
min(2r ∗, 2d(s,t)) if s = c, t ∈ Ci or s = c
, t ∈ Cj \ {v },
d(s,t) if s = c, t = v,
2d(s,t) otherwise.
d is a valid 2-perturbation of d, because for each point u ∈ Ci , d(c,u) ≤ d(c,ci ) + d(ci,u) ≤ 2r ∗,
for each point u ∈ Cj , d(c
,u) ≤ d(c
,cj) + d(cj,u) ≤ 2r ∗ and d(c,v ) ≤ 2r ∗. Therefore, d does not
decrease any distances (and, by construction, d does not increase any distance by more than a
factor of 2). If the optimal cost is 2r ∗, then the set of centers {ci }
k
i=1 \ {ci,cj}∪{c,c
} achieves
the optimal cost, since c and c are distance 2r ∗ from all points in Ci ∪ {v } and Cj , and all other
clusters have the same center as in OPT (achieving radius 2r ∗). Then, by perturbation resilience,
it must be the case that d
(c
,v ) < d
(c,v ), which implies 2d(c
,v ) < d(c,v ). But d(c
,v ) > r ∗
and d(c,v ) ≤ 2r ∗, so, we have a contradiction. Now, we assume the optimal cost of d is less than
2r ∗. Note that all distances d(s,t) were increased to 2d(s,t) or min(2d(s,t), 2r ∗) except for d(c,v ).
Therefore, c must be a center for v under d
, or else the optimal cost would be exactly 2r ∗ by
Lemma 4.1. But it contradicts perturbation resilience to have c and c in the same optimal cluster
under a 2-perturbation. This completes the proof.
6 k-CENTER UNDER (α, ϵ )-PERTURBATION RESILIENCE
In this section, we consider (α, ϵ )-perturbation resilience. First, we show that any 2-approximation
algorithm for symmetric k-center must be optimal under (3, ϵ )-perturbation resilience (Theorem 6.1). Next, we show how to extend this result to local perturbation resilience (Theorem 6.8).
Then, we give an algorithm for asymmetric k-center, which returns a clustering that is ϵ-close
to OPT under (3, ϵ )-perturbation resilience (Theorem 6.17). For all of these results, we assume
a lower bound on the size of the optimal clusters, |Ci | > 2ϵn for all i ∈ [k]. We show the lower
bound on cluster sizes is necessary; in its absence, the problem becomes NP-hard for all values
of α ≥ 1 and ϵ > 0 (Theorem 6.18). The theorems in this section require a careful reasoning about
sets of centers under different perturbations that cannot all simultaneously be optimal.
6.1 Symmetric k-center
We show that for any (3, ϵ )-perturbation resilient k-center instance such that |Ci | > 2ϵn for all i ∈
[k], there cannot be any pair of points from different clusters that are distance ≤ r ∗. This structural
result implies that simple algorithms will return the optimal clustering, such as running any 2-
approximation algorithm or running the Single Linkage algorithm, which is a fast algorithm widely
used in practice for its simplicity.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.                               
k-center Clustering under Perturbation Resilience 22:23
Theorem 6.1. Given a (3, ϵ )-perturbation resilient symmetric k-center instance (S,d) where all
optimal clusters are size > max(2ϵn, 3), then the optimal clusters in OPT are exactly the connected
components of the threshold graph Gr ∗ = (S, E), where E = {(u,v) | d(u,v) ≤ r ∗}.
First, we explain the high-level idea behind the proof.
Proof idea. Since each optimal cluster center is distance r ∗ from all points in its cluster, it suffices
to show that any two points in different clusters are greater than r ∗ apart from each other. Assume
on the contrary that there exist p ∈ Ci and q ∈ Cji such that d(p,q) ≤ r ∗. First, we find a set of
k + 2 points and a 3-perturbation d
, such that every size k subset of the points are optimal centers
under d
. Then, we show how this leads to a contradiction under (3, ϵ )-perturbation resilience.
Here is how we find a set of k + 2 points and a perturbation d such that all size k subsets are
optimal centers under d
. From our assumption, p is distance ≤ 3r ∗ from every point inCi ∪Cj (by
the triangle inequality). Under a 3-perturbation in which all distances are blown up by a factor of
3 except the distances from p to Ci ∪Cj , then replacing ci and cj with p would still give us a set of
k − 1 centers that achieve the optimal cost. But, would this contradict (3, ϵ )-perturbation resilience?
Not necessarily! Perturbation resilience requires exactly k distinct centers.9 The key challenge is
to pick a final “dummy” center to guarantee that the Voronoi partition is ϵ-far from OPT . The
dummy center might “accidentally” be the closest center for almost all points in Ci or Cj . Even
worse, it might be the case that the new center sets off a chain reaction in which it becomes center
to a cluster Cx , and cx becomes center to Cj , which would also result in a partition that is not ϵ-far
from OPT .
To deal with the chain reactions, we crucially introduce the notion of a cluster capturing center
(CCC). A cluster capturing center (CCC) is not to be confused with a center-capturing vertex
(CCV), defined by Vishwanathan [48] and used in the previous section.cx is a CCC for Cy if for all
but ϵn points p ∈ Cy , d(cx ,p) ≤ r ∗ and for all i  x,y, d(cx ,p) < d(ci,p). Intuitively, a CCC exists
if and only if cx is a valid center for Cy when cy is taken out of the set of optimal centers (i.e., a
chain reaction will occur). We argue that if a CCC does not exist, then every dummy center we
pick must be close to eitherCi orCj , since there are no chain reactions. If there does exist a CCCcx
for Cy , then it is much harder to reason about what happens to the dummy centers under d
, since
there may be chain reactions. However, we can define a new d by increasing all distances except
d(cx ,Cy ), which allows us to take cy out of the set of optimal centers, and then any dummy center
must be close to Cx or Cy . There are no chain reactions, because we already know cx is the best
center for Cy among the original optimal centers. Thus, whether or not there exists a CCC, we can
find k + 2 points close to the entire dataset by picking points from both Ci and Cj (respectively, Cx
and Cy ).
Because of the assumption that all clusters are size > 2ϵn, for every 3-perturbation there must be
a bijection between clusters and centers, where the center is closest to the majority of points in the
corresponding cluster. We show that all size k subsets of the k + 2 points cannot simultaneously
admit bijections that are consistent with one another.
Formal analysis. We start out with a simple implication from the assumption that |Ci | > 2ϵn for
all i.
Fact 6.2. Given a clustering instance that is (α, ϵ )-perturbation resilient for α ≥ 1, and all optimal
clusters have size > 2ϵn, then for any α-perturbation d
, for any set of optimal centers c
1,...,c
k of
9This distinction is well motivated; if for some application the best k-center solution is to put two centers at the same
location, then we could achieve the exact same solution with k − 1 centers. That implies we should have been running
k
-center for k = k − 1 instead of k.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.  
22:24 M.-F. Balcan et al.
Fig. 6. (a) Definition of a CCC and (b) definition of a CCC2.
d
, for each optimal cluster Ci , there must be a unique center c
i that is the center for more than half
of the points in Ci under d
.
This fact follows simply from the definition of (α, ϵ )-perturbation resilience (under d
, at most
ϵn points in the optimal solution can change clusters) and the assumption that all optimal clusters
are size > 2ϵn. Now, we formally define a CCC.
Definition 6.3. A centerci is a first-order cluster-capturing center (CCC) for Cj if for all x  j, for
more than half of the points p ∈ Cj , d(ci,p) < d(cx ,p) and d(ci,p) ≤ r ∗ (see Figure 6 (a)). ci is a
second-order cluster-capturing center (CCC2) for Cj if there exists a cl such that for all x  j,l, for
more than half of points p ∈ Cj , d(ci,p) < d(cx ,p) and d(ci,p) ≤ r ∗ (see Figure 6(b)).
Each cluster Cj can have at most one CCC ci , because ci is closer than any other center to
more than half of Cj . Every CCC is a CCC2, since the former is a stronger condition. However, it
is possible for a cluster to have multiple CCC2’s.10 We needed to define CCC2 for the following
reason: Assuming there exist p ∈ Ci and q ∈ Cj that are close, and we replace ci and cj with p in
the set of centers. It is possible that cj is a CCC for Ci , but this does not help us, since we want to
analyze the set of centers after removing cj . However, if we know that cx is a CCC2 for Ci (it is
the best center for Ci , disregarding cj), then we know that cx will be the best center for Ci after
replacing ci and cj with p. Now, we use this definition to show that if two points from different
clusters are close, then we can find a set of k + 2 points and a 3-perturbation d
, such that every
size k subset of the points are optimal centers under d
. To formalize this notion, we give one more
definition.
Definition 6.4. A set C ⊆ S (β,γ )-hits S if for all s ∈ S there exist β points in C at distance ≤ γ r ∗
to s.
Note that if a set C of k + 2 points (3, 3)-hits S, then any size k subset of C is still 3r ∗ from every
point in S, and later, we will show that means there exists a perturbation d such that every size k
subset must be an optimal set of centers.
Lemma 6.5. Given a clustering instance satisfying (3, ϵ )-perturbation resilience such that all optimal clusters are size > 2ϵn and there are two points from different clusters that are ≤ r ∗ apart from
each other, then there exists a set C ⊆ S of size k + 2 that (3, 3)-hits S.
Proof. First, we prove the lemma assuming that a CCC2 exists, and then we prove the other
case. When a CCC2 exists, we do not need the assumption that two points from different clusters
are close.
10In fact, a cluster can have at most three CCC2’s, but we do not use this in our analysis.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.  
k-center Clustering under Perturbation Resilience 22:25
Fig. 7. Case 1 of Lemma 6.5.
Case 1: There exists a CCC2. If there exists a CCC, then denote cx as a CCC for Cy . If there
does not exist a CCC, then denote cx as a CCC2 for Cy . We will show that all points are close to
either Cx or Cy . cx is distance ≤ r ∗ to all but ϵn points in Cy . Therefore, d(cx ,cy ) ≤ 2r ∗ and so cx
is distance ≤ 3r ∗ to all points in Cy . Consider the following d
.
d
(s,t) =

min(3r ∗, 3d(s,t)) if s = cx , t ∈ Cy,
3d(s,t) otherwise.
This is a 3-perturbation, because d(cx ,t) ≤ 3r ∗ for all t ∈ Cy . Then, by Lemma 2.8, the optimal
cost is 3r ∗. Given any u ∈ S, the set of centers {cl}
k
i=1 \ {cy }∪{u} achieves the optimal cost, since cx
is distance 3r ∗ from Cy , and all other clusters have the same center as in OPT (achieving radius
3r ∗). Therefore, this set of centers must create a partition that is ϵ-close to OPT , or else there
would be a contradiction. Then, from Fact 6.2, one of the centers in {cl}
k
i=1 \ {cy }∪{u} must be the
center for the majority of points in Cy under d
. If this center is c,   x,y, then for the majority
of points v ∈ Cy , d(c,v) ≤ r ∗ and d(c,v) < d(cz ,v) for all z  ,y. Then, by definition, c is a
CCC for Cy . But then  must equal x, so we have a contradiction. Note that if some c has for the
majority of v ∈ Cy , d(c,v) ≤ d(cz ,v) (non-strict inequality) for all z  ,y, then there is another
equally good partition in which c is not the center for the majority of points in Cy , so we still
obtain a contradiction. Therefore, either u or cx must be the center for the majority of points in
Cy under d
.
If cx is the center for the majority of points in Cy , then u must be the center for the majority
of points in Cx (it cannot be a different center c, since cx is a better center for Cx than c by
definition). Therefore, each u ∈ S is distance ≤ r ∗ to all but ϵn points in either Cx or Cy .
Now partition all the non-centers into two sets Sx and Sy , such that
Sx = {u | for the majority of points v ∈ Cx , d(u
,v
) ≤ r ∗}, and
Sy = {u | u  Sx and for the majority of points v ∈ Cy, d(u
,v
) ≤ r ∗}.
Then givenu
,v ∈ Sx , there exists an s ∈ Cx such that d(u
,v
) ≤ d(u
,s) + d(s,v
) ≤ 2r ∗ (since
both points are close to more than half of points in Cx ). Similarly, any two points u
,v ∈ Sy are
≤ 2r ∗ apart (see Figure 7).
Now, we will find a set of k + 2 points that (3, 3)-hits S. For now, assume that Sx and Sy are both
nonempty. Given an arbitrary pair p ∈ Sx , q ∈ Sy , we claim that {c }
k
=1 ∪ {p,q} (3, 3)-hits S. Given
a non-center s ∈ Ci such that i  x and i  y, without loss of generality, let s ∈ Sx . Then ci , p, and
cx are all distance 3r ∗ to s. Furthermore,ci , p, and cx are all distance 3r ∗ to ci . Given a points ∈ Cx ,
then cx , cy , and p are distance 3r ∗ to s, because d(cx ,cy ) ≤ 2r ∗, and a similar argument holds for
s ∈ Cy . Therefore, {c }
k
=1 ∪ {p,q} (3, 3)-hits S.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.                       
22:26 M.-F. Balcan et al.
If Sx = ∅ or Sy = ∅, then we can prove a slightly stronger statement: For each pair of non-centers
{p,q}, {c }
k
=1 ∪ {p,q} (3, 3)-hits S. Without loss of generality, let Sy = ∅. Given a point s ∈ Ci such
that i  x and i  y, then ci ,cx , and p are all distance 3r ∗ to s. Given a point s ∈ Cx , then p,cx , and
cy are all distance ≤ 3r ∗ to s. Given a point s ∈ Cy , then p, cx , and cy are all distance ≤ 3r ∗ to s,
because s,p ∈ Sx implies d(s,p) ≤ 2r ∗. Thus, we have proven case 1.
Case 2: There does not exist a CCC2. Now, we use the assumption that there exist p ∈ Cx , q ∈ Cy ,
x  y, such that d(p,q) ≤ r ∗. Then, by the triangle inequality, p is distance ≤ 3r ∗ to all points in
Cx and Cy . Consider the following d
:
d
(s,t) =

min(3r ∗, 3d(s,t)) if s = p, t ∈ Cx ∪Cy,
3d(s,t) otherwise.
This is a 3-perturbation, because d(p,t) ≤ 3r ∗ for all t ∈ Cx ∪Cy . Then, by Lemma 2.8, the optimal cost is 3r ∗. Given any s ∈ S, the set of centers {cl}
k
i=1 \ {cx ,cy }∪{p,s} achieves the optimal cost,
since p is distance 3r ∗ fromCx ∪Cy , and all other clusters have the same center as in OPT (achieving radius 3r ∗). Therefore, this set of centers must create a partition that is ϵ-close to OPT , or else
there would be a contradiction. Then, from Fact 6.2, one of the centers in {cl}
k
i=1 \ {cx ,cy }∪{p,s}
must be the center for the majority of points in Cx under d
.
If this center is c for   x and   y, then for the majority of points t ∈ Cx , d(c,t) ≤ r ∗ and
d(c,t) < d(cz ,t) for all z  , x,y. Then, by definition,c is a CCC2 for Cx , and we have a contradiction. Note that if some c has for the majority of t ∈ Cx , d(c,t) ≤ d(cz ,t) (non-strict inequality)
for all z  ,y, then there is another equally good partition in which c is not the center for the
majority of points in Cy , so we still obtain a contradiction.
Similar logic applies to the center for the majority of points in Cy . Therefore, p and s must be
the centers for Cx and Cy . Since s was an arbitrary noncenter, all noncenters are distance ≤ r ∗ to
all but ϵn points in either Cx or Cy .
Similar to Case 1, we now partition all the non-centers into two sets Sx and Sy , such that
Sx = {u | for the majority of points v ∈ Cx , d(u,v) ≤ r ∗}, and
Sy = {u | u  Sx and for the majority of points v ∈ Cy, d(u,v) ≤ r ∗}.
As before, each pair of points in Sx are distance ≤ 2r ∗ apart and similarly for Sy . It is no longer
true that d(cx ,cy ) ≤ 2r ∗, however, we can prove that for both Sx and Sy , there exist points from
two distinct clusters each. From the previous paragraph, given a non-center s ∈ Ci for i  x,y, we
know that p and s are centers forCx andCy . With an identical argument, given t ∈ Cj for j  x,y,i,
we can show that q and t are centers for Cx and Cy . It follows that Sx and Sy both contain points
from at least two distinct clusters.
Now, we finish the proof by showing that for each pair u ∈ Sx , v ∈ Sy , {c }
k
=1 ∪ {u,v} (3, 3)-
hits S. Given a non-center s ∈ Ci , without loss of generality, s ∈ Sx , then there exists j  i and
t ∈ Cj ∩ Sx . Then ci , cj , and u are 3r ∗ to s and ci , cx , and u are 3r ∗ to ci . In the case where i = x,
then ci , cj , and u are 3r ∗ to ci . This concludes the proof.
So far, we have shown that by just assuming two points from different clusters are close, we can
find a set of k + 2 points that (3, 3)-hits S. Now, we will show that such a set leads to a contradiction
under (3, ϵ )-perturbation resilience. Specifically, we will show there exists a perturbation d such
that any size k subset can be an optimal set of centers. But it is not possible that all (
k+2
k ) of these
sets of centers simultaneously create partitions that are ϵ-close to OPT . First, we state a lemma
that proves there does exist a perturbationd such that any size k subset is an optimal set of centers.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.                           
k-center Clustering under Perturbation Resilience 22:27
Lemma 6.6. Given a k-center clustering instance (S,d), given z ≥ 0, and given a set C ⊆ S of size
k + z that (z + 1, α)-hits S, there exists an α-metric perturbation d such that all size k subsets of C
are optimal sets of centers under d
.
Proof. Consider the following perturbation d:
d(s,t) =

min(αr ∗, αd(s,t)) if s ∈ C and d(s,t) ≤ αr ∗,
αd(s,t) otherwise.
By Lemma 4.1, the metric closure d of d is an α-metric perturbation with optimal cost αr ∗.
Given any size k subsetC ⊆ C, then for allv ∈ S, there is still at least one c ∈ C such that d(c,v) ≤
αr ∗; therefore, by construction, d
(c,v) ≤ αr ∗. It follows that C is a set of optimal centers under
d
.
Next, we state a fact that helps clusters rank their best centers from the set of k + 2 points. For
each cluster Ci , we would like to have a ranking of all points such that for a given d and set of k
centers, the center forCi is the highest point in the ranking. The following fact shows this ranking
is well defined:
Fact 6.7. Given a k-center clustering instance (S,d) with optimal clustering C = {C1,...,Ck } such
that for all i ∈ [k], |Ci | > 2ϵn, let d denote an α-perturbation of d. There are rankings Rx,d for all
x ∈ [k] such that for any optimal set of centers c
1,...,c
k under d
, the center that is closest in d to
all but ϵn points in Cx is the highest-ranked point in Rx,d.
11
Proof. Assume the fact is false. Then there exists a d
, a cluster Ci , two points p and q, and two
sets of k centers p,q ∈ C and p,q ∈ C that achieve the optimal cost under d
, but p is the center
for Ci in C while q is the center for Ci in C
. Then p is closer than all other points in C to all but
ϵn points in Ci . Similarly, q is closer than all other points in C to all but ϵn points in Ci . Since
|Ci | > 2ϵn, this causes a contradiction.
We also define Rx,d
,C : C → [n
] as the ranking specific to a set of centers C, where |C| = n
.
Now, we can prove Theorem 6.1.
Proof of Theorem 6.1. It suffices to prove that any two points from different clusters are at distance > r ∗ from each other. Assume towards contradiction that this is not true. Then, by Lemma 6.5,
there exists a set C of size k + 2 that (3, 3)-hits S. From Lemma 6.6, there exists a 3-metric perturbation d such that all size k subsets of C are optimal sets of centers under d
. Consider the ranking of each cluster for C over d guaranteed from Fact 6.7. We will show this ranking leads to a
contradiction.
Consider the set of all points ranked 1 or 2 by any cluster, formally, {p ∈ C | ∃i s.t. Ri,d
,C ≤ 2}.
This set is a subset of C, since we are only considering the rankings of points in C, so it is size
≤ k + 2. Note that a point cannot be ranked both 1 and 2 by a cluster. Then as long as k > 2,
it follows by the Pigeonhole Principle that there exists a point c ∈ C that is ranked in the top
two by two different clusters. Formally, there exists x and y such that x  y, Rx,d
,C (c) ≤ 2 and
Ry,d
,C (c) ≤ 2. Denote u and v such that Rx,d
,C (u) = 1 and Ry,d
,C (v) = 1. If u or v is equal to c,
then redefine it to an arbitrary center in C \ {c,u,v}. Consider the set of centers C = C \ {u,v},
which is optimal under d by construction. But then, from Fact 6.7, c is the center for all but ϵn
points in both Cx and Cy , contradicting Fact 6.2. This completes the proof.
11Formally, for eachCx , there exists a bijection Rx,d : S → [n] such that for all sets of k centersC that achieve the optimal
cost under d
, we have c = argminc∈C Rx,d (c
) if and only if VorC (c ) is ϵ-close to Cx .
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.    
22:28 M.-F. Balcan et al.
6.2 Local Perturbation Resilience
Now, we extend the argument from the previous section to local perturbation resilience. First, we
state our main structural result, which is that any pair of points from different (3, ϵ )-PR clusters
must be distance > r ∗ from each other. Then, we will show how the structural result easily leads
to an algorithm for (3, ϵ )-SLPR clusters.
Theorem 6.8. Given a k-center clustering instance (S,d) with optimal radius r ∗ such that all optimal clusters are size > 2ϵn and there are at least three (3, ϵ )-PR clusters, then for each pair of (3, ϵ )-PR
clusters Ci and Cj , for all u ∈ Ci and v ∈ Cj , we have d(u,v) > r ∗.
Before we prove this theorem, we show how it implies an algorithm to output the optimal (3, ϵ )-
SLPR clusters exactly. Since the distance from each point to its closest center is ≤ r ∗, a corollary
of Theorem 6.8 is that any 2-approximate solution must contain the optimal (3, ϵ )-SLPR clusters,
as long as the 2-approximation satisfies two sensible conditions: (1) for every point v and its assigned center u (so we know d(u,v) ≤ 2r ∗), ∃w such that d(u,w) and d(w,v) are ≤ r ∗ and (2) there
cannot be multiple clusters outputted in the 2-approximation that can be combined into one cluster with radius smaller than r ∗. Both of these properties are easily satisfied using quick pre- or
post-processing steps.12
Theorem 6.9. Given a k-center clustering instance (S,d) such that all optimal clusters are size
> 2ϵn and there are at least three (3, ϵ )-PR clusters, then any 2-approximate solution satisfying conditions (1) and (2) must contain all optimal (3, ϵ )-SLPR clusters.
Proof. Given such a clustering instance, then Theorem 6.8 ensures that there is no edge of
length r ∗ between points from two different (3, ϵ )-PR clusters. Given a (3, ϵ )-SLPR cluster Ci , it
follows that there is no point v  Ci such that d(v,Ci ) ≤ r ∗. Therefore, given a 2-approximate
solution C satisfying condition (1), any u ∈ Ci and v  Ci cannot be in the same cluster. This is
because in the graph of datapoints where edges signify a distance ≤ r ∗,Ci is an isolated component.
Finally, by condition (2), Ci must not be split into two clusters. Therefore, Ci ∈ C.
Proof idea for Theorem 6.8. The high-level idea of this proof is similar to the proof of Theorem 6.1.
In fact, the first half is very similar to Lemma 6.5: We show that if two points from different PR
clusters are close together, then there must exist a set of k + 2 points C that (3, 3)-hits the entire
point set. In the previous section, we arrived at a contradiction by showing that it is not possible
that all (
k+2
k )0 subsets of C can be centers that are ϵ-close to OPT . However, the weaker local PR
assumption poses a new challenge.
As in the previous section, we will still argue that all size k subsets of C cannot stay consistent
with the (3, ϵ )-PR clusters using a ranking argument that maps optimal clusters to optimal centers,
but our argument will be to establish conditional claims that narrow down the possible sets of
ranking lists. For instance, assume there is a (3, ϵ )-PR cluster Ci that ranks ci first and ranks cj
second. Then under subsets C
, which do not contain ci , cj is the center for a cluster C
i , which is
ϵ-close to Ci . Therefore, a different point in C must be the center for all but ϵn points in Cj (and it
cannot be a different center c without causing a contradiction). This is the basis for Lemma 6.13,
which is the main workhorse lemma in the proof of Theorem 6.8. By building up conditional
statements, we are able to analyze every possibility of the ranking lists for the three (3, ϵ )-PR
clusters and show that all of them lead to contradictions, proving Theorem 6.8.
12For condition (1), before running the algorithm, remove all edges of distance > r ∗, and then take the metric completion
of the resulting graph. For condition (2), given the radius rˆ of the outputted solution, for each v ∈ S, check if the ball of
radius rˆ around v captures multiple clusters. If so, then combine them.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.    
k-center Clustering under Perturbation Resilience 22:29
Formal analysis of Theorem 6.8. We start with a local perturbation resilience variant of Fact 6.2.
Fact 6.10. Given a k-center clustering instance (S,d) such that all optimal clusters have size >
2ϵn, let d denote an α-perturbation with optimal centers C = {c
1,...,c
k }. Let C denote the set
of (α, ϵ )-PR clusters. Then there exists a one-to-one function f : C → C such that for all Ci ∈ C
,
|VorC,d (f (Ci )) ∩Ci |≥|Ci | − ϵn. That is, the optimal cluster in d whose center is f (Ci ) contains all
but ϵn of the points in Ci .
In words, for any set of optimal centers under an α-perturbation, each PR cluster can be paired
to a unique center. This follows simply because all optimal clusters are size > 2ϵn, yet under a
perturbation, < ϵn points can switch out of each PR cluster. Because of this fact, for a perturbation
d with set of optimal centers C and an (α, ϵ )-PR cluster Cx , we will say that c is the center for Cx
under d if c is the center for all but ϵn points in Cx . Now, we are ready to prove the first half of
Theorem 6.8, stated in the following lemma. The proof is similar to Lemma 6.5.
Lemma 6.11. Given a k-center clustering instance (S,d) such that all optimal clusters are size > 2ϵn
and there exist two points at distance r ∗ from different (3, ϵ )-PR clusters, then there exists a partition
Sx ∪ Sy of the non-centers S \ {c }
k
=1 such that for all pairs p ∈ Sx , q ∈ Sy , {c }
k
=1 ∪ {p,q} (3, 3)-hits
S.
Proof. This proof is split into two main cases. The first case is the following: There exists a
CCC2 for a (3, ϵ )-PR cluster, disregarding a (3, ϵ )-PR cluster. In fact, in this case, we do not need
the assumption that two points from different PR clusters are close. If there exists a CCC to a (3, ϵ )-
PR cluster, then denote the CCC by cx and the cluster by Cy . Otherwise, let cx denote a CCC2 to
a (3, ϵ )-PR cluster Cy , disregarding a (3, ϵ )-PR center cz . Then cx is at distance ≤ r ∗ to all but ϵn
points in Cy . Therefore, d(cx ,cy ) ≤ 2r ∗ and so cx is at distance ≤ 3r ∗ to all points in Cy . Consider
the following perturbation d:
d(s,t) =

min(3r ∗, 3d(s,t)) if s = cx , t ∈ Cy,
3d(s,t) otherwise.
This is a 3-perturbation, because for all v ∈ Cy , d(cx ,v) ≤ 3r ∗. Define d as the metric completion of d. Then, by Lemma 4.1, d is a 3-metric perturbation with optimal cost 3r ∗. Given any
non-center v ∈ S, the set of centers {c }
k
=1 \ {cy }∪{v} achieves the optimal score, since cx is at
distance 3r ∗ fromCy , and all other clusters have the same center as in OPT (achieving radius 3r ∗).
Therefore, from Fact 6.10, one of the centers in {c }
k
=1 \ {cy }∪{v} must be the center for all but
ϵn points inCy under d
. If this center isc,   x,y, then for all but ϵn points u ∈ Cy , d(c,u) ≤ r ∗,
and d(c,u) < d(cz ,u) for all z  ,y. Then, by definition,c is a CCC for the (3, ϵ )-PR cluster, Cy .
But then, by construction,  must equal x, so we have a contradiction. Note that if some c has
for all but ϵn points u ∈ Cy , d(c,u) ≤ d(cz ,u) (non-strict inequality) for all z  ,y, then there is
another equally good partition in which c is not the center for all but ϵn points in Cy , so we still
obtain a contradiction. Therefore, eitherv orcx must be the center for all but ϵn points inCy under
d
.
If cx is the center for all but ϵn points in Cy , then, because Cy is (3, ϵ )-PR, the corresponding
cluster must contain fewer than ϵn points from Cx . Furthermore, since for all   x and u ∈ Cx ,
d(u,cx ) < d(u,c ), it follows that v must be the center for all but ϵn points in Cx . Therefore, every
non-center v ∈ S is at distance ≤ r ∗ to all but ϵn points in either Cx or Cy .
Now partition all the non-centers into two sets Sx and Sy , such that
Sx = {p | for the majority of points q ∈ Cx , d(p,q) ≤ r ∗}
and
Sy = {p | p  Sx and for the majority of points q ∈ Cy, d(p,q) ≤ r ∗}.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.                          
22:30 M.-F. Balcan et al.
Then given p,q ∈ Sx , there exists an s ∈ Cx such that d(p,q) ≤ d(p,s) + d(s,q) ≤ 2r ∗ (since both
points are close to more than half of points in Cx ). Similarly, any two points p,q ∈ Sy are ≤ 2r ∗
apart.
Now, we will find a set of k + 2 points that (3, 3)-hits S. For now, assume that Sx and Sy are
both nonempty. Given a pair p ∈ Sx , q ∈ Sy , we claim that {c }
k
=1 ∪ {p,q} (3, 3)-hits S. Given a
non-center s ∈ Ci such that i  x,y, without loss of generality, let s ∈ Sx . Then ci , p, and cx are all
distance 3r ∗ to s. Furthermore,ci ,cx , and p are all distance 3r ∗ to ci . Given a point s ∈ Cx , then cx ,
cy , and p are distance 3r ∗ to s, because d(cx ,cy ) ≤ 2r ∗. Finally, cx , cy, and p are distance 3r ∗ to cx ,
and similar arguments hold for s ∈ Cy and cy . Therefore, {c }
k
=1 ∪ {p,q} (3, 3)-hits S.
If Sx = ∅ or Sy = ∅, then we can prove a slightly stronger statement: For each pair of non-centers
{p,q}, {c }
k
=1 ∪ {p,q} (3, 3)-hits S. Without loss of generality, let Sy = ∅. Given a point s ∈ Ci such
that i  x and i  y, then ci ,cx , and p are all distance 3r ∗ to s. Given a point s ∈ Cx , then p,cx , and
cy are all distance ≤ 3r ∗ to s. Given a point s ∈ Cy , then p, cx , and cy are all distance ≤ 3r ∗ to s,
because s,p ∈ Sx implies d(s,p) ≤ 2r ∗. Thus, we have proven case 1.
Now, we turn to the other case. Assume there does not exist a CCC2 to a PR cluster, disregarding
a PR center. In this case, we need to use the assumption that there exist (3, ϵ )-PR clusters Cx and
Cy , and p ∈ Cx , q ∈ Cy such that d(p,q) ≤ r ∗. Then, by the triangle inequality, p is distance ≤ 3r ∗
to all points in Cx and Cy . Consider the following d:
d(s,t) =

min(3r ∗, 3d(s,t)) if s = p, t ∈ Cx ∪Cy,
3d(s,t) otherwise.
This is a 3-perturbation, because d(p,v) ≤ 3r ∗ for all v ∈ Cx ∪Cy . Define d as the metric completion of d. Then, by Lemma 4.1, d is a 3-metric perturbation with optimal cost 3r ∗. Given any
non-center s ∈ S, the set of centers {c }
k
=1 \ {cx ,cy }∪{p,s} achieves the optimal cost, since p is
distance 3r ∗ fromCx ∪Cy , and all other clusters have the same center as in OPT (achieving radius
3r ∗).
From Fact 6.10, one of the centers in {c }
k
=1 \ {cx ,cy }∪{p,s} must be the center for all but ϵn
points in Cx under d
. If this center isc for   x,y, then for all but ϵn points t ∈ Cx , d(c,t) ≤ r ∗
and d(c,t) < d(cz ,t) for all z  , x,y. So, by definition,c is a CCC2 forCx disregarding cy , which
contradicts our assumption. Similar logic applies to the center for all but ϵn points inCy . Therefore,
p and s must be the centers for Cx and Cy . Since s was an arbitrary non-center, all non-centers are
distance ≤ r ∗ to all but ϵn points in either Cx or Cy .
Similar to Case 1, we now partition all the non-centers into two sets Sx and Sy , such that
Sx = {u | for the majority of points v ∈ Cx , d(u,v) ≤ r ∗}
and
Sy = {u | u  Sx and for the majority of points v ∈ Cy, d(u,v) ≤ r ∗}.
As before, each pair of points in Sx are distance ≤ 2r ∗ apart and similarly for Sy . It is no longer
true that d(cx ,cy ) ≤ 2r ∗; however, we can prove that for both Sx and Sy , there exist points from
two distinct clusters each. From the previous paragraph, given a non-center s ∈ Ci for i  x,y, we
know that p and s are centers forCx andCy . With an identical argument, given t ∈ Cj for j  x,y,i,
we can show that q and t are centers for Cx and Cy . It follows that Sx and Sy both contain points
from at least two distinct clusters.
Now, we finish the proof by showing that for each pair u ∈ Sx , v ∈ Sy , {c }
k
=1 ∪ {u,v} (3, 3)-
hits S. Given a non-center s ∈ Ci , without loss of generality, s ∈ Sx , then there exists j  i and
t ∈ Cj ∩ Sx . Then ci , cj , and u are 3r ∗ to s and ci , cx , and u are 3r ∗ to ci . In the case where i = x,
then ci , cj , and u are 3r ∗ to ci . This concludes the proof.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.                            
k-center Clustering under Perturbation Resilience 22:31
Now, we move to the second half of the proof of Theorem 6.8. Recall that the proof from the
previous section relied on a ranking argument, in which optimal clusters were mapped to their
closest centers from the set C of k + 2 points from the first half of the proof. This is the basis for
the following fact:
Fact 6.12. Given a k-center clustering instance (S,d) with optimal clustering C = {C1,...,Ck }
such that for all i ∈ [k], |Ci | > 2ϵn, let d denote an α-perturbation of d and let C denote the set of
(α, ϵ )-PR clusters. For each Cx ∈ C
, there exists a ranking Rx,d of S such that for any set of optimal
centers C = {c
1,...,c
k } under d
, the center that is closest in d to all but ϵn points in Cx is the
highest-ranked point in Rx,d.
13
Proof. Assume the lemma is false. Then there exists an (α, ϵ )-PR clusterCi , two distinct points
u,v ∈ S, and two sets of k centers C and C both containing u and v, and both sets achieve the
optimal score under an α-perturbation d
, but u is the center for Ci in C while v is the center for
Ci in C
. Then VorC (u) is ϵ-close to Ci ; similarly, VorC (v) is ϵ-close to Ci . This implies u is closer
to all but ϵn points in Ci than v, and v is closer to all but ϵn points in Ci than u. Since |Ci | > 2ϵn,
this causes a contradiction.
We also define Rx,d
,C : C → [n
] as the ranking specific to C. Recall that our goal is to show
a contradiction assuming two points from different PR clusters are close. From Lemma 6.6 and
Lemma 6.11, we know there is a set of k + 2 points, and any size k subset is optimal under a
suitable perturbation. By Lemma 6.10, each size k subset must have a mapping from PR clusters
to centers, and from Fact 6.12, these mappings are derived from a ranking of all possible center
points by the PR clusters. In other words, each PR cluster Cx can rank all the points in S, so for
any set of optimal centers for an α-perturbation, the top-ranked center is the one whose cluster is
ϵ-close to Cx . Now, using Fact 6.12, we can try to give a contradiction by showing that there is no
set of rankings for the PR clusters that is consistent with all the optimal sets of centers guaranteed
by Lemmas 6.6 and 6.11. The following lemma gives relationships among the possible rankings.
These will be our main tools for contradicting PR and thus finishing the proof of Theorem 6.8.
Lemma 6.13. Given a k-center clustering instance (S,d) such that all optimal clusters are size >
2ϵn, and given non-centers p,q ∈ S such that C = {c }
k
=1 ∪ {p,q} (3, 3)-hits S, let the set C denote
the set of (3, ϵ )-PR clusters. Consider a 3-perturbation d such that all size k subsets of C are optimal
sets of centers under d
. The following are true:
(1) Given Cx ∈ C and Ci such that i  x, Rx,d (cx ) < Rx,d (ci ).
(2) There do not exist s ∈ C and Cx ,Cy ∈ C such that x  y, and Rx,d
,C (s) + Ry,d
,C (s) ≤ 4.
(3) Given Ci and Cx ∈ C such that x  i, if Rx,d
,C (ci ) ≤ 3, then for all Cy ∈ C such that y
x,i, Ry,d
,C (p) ≥ 3 and Ry,d
,C (q) ≥ 3.
Proof.
(1) By definition of the optimal clusters, for each s ∈ Cx , d(cx ,s) < d(ci,s), and therefore, by
construction, d
(cx ,s) < d
(ci,s). It follows that Rx,d (cx ) < Rx,d (ci ).
(2) Assume there exists s ∈ C and Cx ,Cy ∈ C such that Rx,d
,C (s) + Ry,d
,C (s) ≤ 4.
Case 1: Rx,d
,C (s) = 1 and Ry,d
,C (s) ≤ 3. Define u and v such that Ry,d
,C (u) = 1 and
Ry,d
,C (v) = 2. (If u or v is equal to s, then redefine it to an arbitrary center inC \ {s,u,v}.)
Consider the set of centers C = C \ {u,v}, which is optimal under d by Lemma 6.6. By
Fact 6.12, s is the center for all but ϵn points in both Cx and Cy , causing a contradiction.
13Formally, for each Cx ∈ C
, there exists a bijection Rx,d : S → [n] such that for all sets of k centers C that achieve the
optimal cost under d
, then c = argminc∈C Rx,d (c
) if and only if VorC (c ) is ϵ-close to Cx .
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.       
22:32 M.-F. Balcan et al.
Case 2: Rx,d
,C (s) = 2 and Ry,d
,C (s) = 2. Define u and v such that Rx,d
,C (u) = 1 and
Ry,d
,C (v) = 1. (Again, if u or v is equal to s, then redefine it to an arbitrary center in
C \ {s,u,v}.) Consider the set of centers C = C \ {u,v}, which is optimal under d by
Lemma 6.6. However, by Fact 6.12, s is the center for all but ϵn points in both Cx and
Cy , causing a contradiction.
(3) Assume Rx,d
,C (ci ) ≤ 3.
Case 1: Rx,d
,C (ci ) = 2. Then, by Lemma 6.13 part 1, Rx,d
,C (cx ) = 1. Consider the set
of centers C = C \ {cx ,p}, which is optimal under d
. By Fact 6.12, VorC (ci ) must be ϵclose to Cx . In particular, VorC (ci ) cannot contain more than ϵn points from Ci . But by
definition, for all j  i and s ∈ Ci , d(ci,s) < d(cj,s). It follows that VorC (q) must contain all but ϵn points from Ci . Therefore, for all but ϵn points s ∈ Ci , for all j, d
(q,s) <
d
(cj,s). If Ry,d
,C (q) ≤ 2, then Cy ranks cy or p number one. Then for the set of centers
C = C \ {cy,p}, VorC (q) contains more than ϵn points from Cy and Ci , contradicting the
fact that Cy is (3, ϵ )-PR. Therefore, Ry,d
,C (q) ≥ 3. The argument to show Ry,d
,C (p) ≥ 3 is
symmetric.
Case 2: Rx,d
,C (ci ) = 3. If there exists j  i, x such that Rx,d
,C (ci ) = 2, then without loss
of generality, we are back in case 1. By Lemma 6.13 part 1, Rx,d
,C (cx ) ≤ 2. Then either
p or q are ranked top two, without loss of generality, Rx,d
,C (p) ≤ 2. Consider the set
C = C \ {cx ,p}. Then as in the previous case, VorC (ci ) must be ϵ-close to Cx , implying
for all but ϵn points s ∈ Ci , for all j, d
(q,s) < d
(cj,s). If Ry,d
,C (q) ≤ 2, again, then Cy
ranks cy or p as number one. Let C = C \ {cy,p}, and then VorC (q) contains more than
ϵn points from Cy and Ci , causing a contradiction. Furthermore, if Ry,d
,C (p) ≤ 2, then we
arrive at a contradiction by Lemma 6.13 part 2.
We are almost ready to bring everything together to give a contradiction. Recall that Lemma 6.11
allows us to choose a pair (p,q) such that {c }
k
=1 ∪ {p,q} (3, 3)-hits S. For an arbitrary choice of
p and q, we may not end up with a contradiction. It turns out, we will need to make sure one of
the points comes from a PR cluster and is very high in the ranking list of its own cluster. This
motivates the following fact, which is the final piece to the puzzle:
Fact 6.14. Given a k-center clustering instance (S,d) such that all optimal clusters are size > 2ϵn,
given an (α, ϵ )-PR cluster Cx , and given i  x, then there are fewer than ϵn points s ∈ Cx such that
d(ci,s) ≤ min(r ∗, αd(cx ,s)).
Proof. Assume the fact is false. Then let B ⊆ Cx denote a set of size ϵn such that for all s ∈ B,
d(ci,s) ≤ min(r ∗, αd(cx ,s)). Construct the following perturbation d
: For all s ∈ B, set d
(cx ,s) =
αd(cx ,s). For all other pairss,t, set d
(s,t) = d(s,t). This is clearly an α-perturbation by construction. Then the original set of optimal centers still achieves cost r ∗ under d
, because for all s ∈ B,
d
(ci,s) ≤ r ∗. Clearly, the optimal cost under d cannot be < r ∗. It follows that the original set of
optimal centersC is still optimal under d
. However, all points in B are no longer in VorC (cx ) under
d
, contradicting the fact that Cx is (α, ϵ )-PR.
Now, we are ready to prove Theorem 6.8.
Proof of Theorem 6.8. Assume towards contradiction that there are two points at distance
≤ r ∗ from different (3, ϵ )-PR clusters. Then, by Lemma 6.11, there exists a partition S1, S2 of noncenters of S such that for all pairs p ∈ S1, q ∈ S2, {c }
k
=1 ∪ {p,q} (3, 3)-hit S. Given three (3, ϵ )-PR
clustersCx ,Cy , andCz , letc
x ,c
y , and c
z denote the centers in {c1,...,ck } ranked highest byCx ,Cy ,
andCz disregarding cx ,cy , and cz , respectively. Define p = argmins ∈Cx d(cx ,s), and without loss of
generality, let p ∈ S1. Then pick an arbitrary point q from S2 and defineC = {c }
k
=1 ∪ {p,q}. Define
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.           
k-center Clustering under Perturbation Resilience 22:33
d as in Lemma 6.6 (i.e., we define d so all size k subsets of C are optimal sets of centers under
d
). We claim that Rx,d
,C (p) < Rx,d
,C (c
x ): From Fact 6.14, there are fewer than ϵn points s ∈ Cx
such that d(c
x ,s) ≤ min(r ∗, 3d(cx ,s)). Among each remaining points ∈ Cx , we will showd
(p,s) ≤
d
(c
x ,s). Recall that d(p,s) ≤ d(p,cx ) + d(cx ,s) ≤ 2r ∗, so d
(p,s) = min(3r ∗, 3d(p,s)). There are
two cases to consider.
Case 1: d(c
x ,s) > r ∗. Then, by construction, d
(c
x ,s) ≥ 3r ∗, and so d
(p,s) ≤ d
(c
x ,s).
Case 2: 3d(cx ,s) < d(c
x ,s). Then
d
(p,s) ≤ 3d(p,s) by construction of d
,
≤ 3(d(p,cx ) + d(cx ,s)) by triangle inequality,
≤ 6d(cx ,s) by definition of p,
≤ 2d(c
x ,s) by assumption,
≤ min(3r ∗
, 3d(c
x ,s)) by construction of d
,
= d
(c
x ,s),
and this proves our claim.
Because Rx,d
,C (p) < Rx,d
,C (c
x ) and Rx,d
,C (cx ) < Rx,d
,C (c
x ), it follows that the top two can
only be cx , p, or q. Therefore, either Rx,d
,C (p) ≤ 2 or Rx,d
,C (q) ≤ 2. The rest of the argument is
broken up into cases.
Case 1: Rx,d
,C (c
x ) ≤ 3. From Lemma 6.13 part 3, then Ry,d
,C (p) ≥ 3 and Ry,d
,C (q) ≥ 3. It follows by process of elimination that Ry,d
,C (cy ) = 1 and Ry,d
,C (cy ) = 2. Again, by Lemma 6.13
part 3, Rx,d
,C (p) ≥ 3 and Rx,d
,C (q) ≥ 3, causing a contradiction.
Case 2: Rx,d
,C (cx ) > 3 and Ry,d
,C (cy ) ≤ 3. Then Rx,d
,C (p) ≤ 3 and Rx,d
,C (q) ≤ 3. From
Lemma 6.13 part 3, Rx,d
,C (p) ≥ 3 and Rx,d
,C (q) ≥ 3, therefore, we have a contradiction. Note,
the case where Rx,d
,C (cx ) > 3 and Rz,d
,C (cz ) ≤ 3 is identical to this case.
Case 3: The final case is when Rx,d
,C (cx ) > 3, Ry,d
,C (cy ) > 3, and Rz,d
,C (cz ) > 3. So for each
i ∈ {x,y, z}, the top three for Ci in C is a permutation of {ci,p,q}. Then each i ∈ {x,y, z} must rank
p or q in the top two, so by the Pigeonhole Principle, either p or q is ranked top two by two different
PR clusters, contradicting Lemma 6.13. This completes the proof.
We note that Case 3 in Theorem 6.8 is the reason why we need to assume there are at least three
(3, ϵ )-PR clusters. If there are only two,Cx andCy , then it is possible that there exist u ∈ Cx ,v ∈ Cy
such that d(u,v) ≤ r ∗. In this case, for p,q,d
, and C as defined in the proof of Theorem 6.8, if Cx
ranks cx , p, q as its top three and Cy ranks cy , q, p as its top three, then there is no contradiction.
6.3 Asymmetric k-center
Now, we consider asymmetric k-center under (3, ϵ )-PR. The asymmetric case is a more challenging
setting, and our algorithm does not return the optimal solution; however, our algorithm outputs a
clustering that is ϵ-close to the optimal solution.
Recall the definition of the symmetric set A from Section 3, A = {p | ∀q,d(q,p) ≤ r ∗ =⇒
d(p,q) ≤ r ∗}, equivalently, the set of all CCV’s. We might first ask whether A respects the structure of OPT , as it did under 2-perturbation resilience. Namely, whether Condition 1: all optimal
centers are in A and Condition 2: arg minq ∈A d(q,p) ∈ Ci =⇒ p ∈ Ci hold. In fact, we will show
that neither conditions hold in the asymmetric case, but both conditions are only slightly violated.
6.3.1 Structure of optimal centers. First, we give upper and lower bounds on the number of
optimal centers in A, which will help us construct an algorithm for (3, ϵ )-PR later on. We call a
centerci “bad” if it is not in the set A, i.e., ∃q such that d(q,ci ) ≤ r ∗ but d(ci,q) > r ∗. First, we give
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020. 
22:34 M.-F. Balcan et al.
Fig. 8. An (α, ϵ )-perturbation resilient asymmetric k-center instance with one bad center (cy ). The dotted
arrows are distance 1, and the solid arrows are distance 1
α .
an example of a (3, ϵ )-PR instance with at least one bad center, and then we show that all (3, ϵ )-PR
instances must have at most six bad centers.
Lemma 6.15. For all α,n, k ≥ 1 such that n
k ∈ N, there exists a clustering instance with one bad
center satisfying (α, 2
n )-perturbation resilience.
Proof. Given α,n, k ≥ 1, we construct a clustering instance such that all clusters are size n
k .
Denote the clusters by C1,...,Ck and the centers by c1,...,ck . For each i, denote the non-centers
in Ci by pi,1,...,pi,L. Now, we define the distances as follows: For convenience, set L = n
k − 1.
For all 2 ≤ i ≤ k and 1 ≤ j ≤ L, let d(ci,pi,j) = 1. For all 2 ≤ i ≤ k, 1 ≤ j,  ≤ L, let d(pi,j,p1,  ) = 1
α
and d(c1,p1,  ) = 1
α . Finally, let d(p2,1,c1) = 1. All other distances are the maximum allowed by the
triangle inequality. In particular, the distance between two points p and q is set to infinity unless
there exists a path from p to q with finite distance edges defined above (see Figure 8).
The optimal clusters and centers are C1,...,Ck and c1,...,ck , achieving a radius of 1, and
c1 is a bad center, because d(p2,1,c1) = 1 but d(c1,p2,1) = ∞. It is left to show that this instance
satisfies (α, 2
n )-perturbation resilience. Given an arbitrary α-perturbation d
, we must show that
at most 2
n · n = 2 points switch clusters. By definition of an α-perturbation, for all p,q, we have
d(p,q) ≤ d
(p,q) ≤ αd(p,q) (recall that, without loss of generality, a perturbation only increases
the distances). The centersc2,...,ck must remain optimal centers under d
, since for all 2 ≤ i ≤ k,
d
(ci,pi,1) ≤ α and no other point q  ci,pi,1 satisfies d(q,pi,1) < ∞. Now, we must determine the
final optimal center. Note that for all 2 ≤ i, j ≤ k and 1 ≤ ,m ≤ L, we have
d
(pi, ,p1,m ) ≤ αd(pi, ,p1,m ),
≤ α ·
1
α ,
< d(cj,p1,m ),
≤ d
(cj,p1,m ).
Therefore, cj cannot be a center for pi, , for all 2 ≤ i, j ≤ k and 1 ≤  ≤ L. Therefore, the final
optimal center c under d must be either c1 or pi,  for 2 ≤ i ≤ k and 1 ≤  ≤ L. Furthermore, it
follows thatc’s cluster at least containsC1 \ {c1} and for each 2 ≤ i ≤ k,ci ’s cluster at least contains
Ci \ {c}. Therefore, the optimal clustering under d differs from OPT by at most two points. This
concludes the proof.
Now, we show there are at most six bad centers for any asymmetric k-center instance satisfying
(3, ϵ )-PR.
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.            
k-center Clustering under Perturbation Resilience 22:35
Lemma 6.16. Given a (3, ϵ )-perturbation resilient asymmetric k-center instance such that all optimal clusters are size > 2ϵn, there are at most 6 bad centers, i.e., at most six centers ci such that ∃q
with d(q,ci ) ≤ r ∗ and d(ci,q) > r ∗.
Proof. Assume the lemma is false. By assumption, there exists a set B, |B| ≥ 7, of centers ci
such that ∃q with d(q,ci ) ≤ r ∗ and d(ci,q) > r ∗. The first step is to use this set of bad centers to
construct a set C of ≤ k − 3 points that are ≤ 3r ∗ from every point in S. Once we find C, we will
show how this set cannot exist under (3, ϵ )-perturbation resilience, causing a contradiction.
Given a center ci ∈ B and q such that d(q,ci ) ≤ r ∗ and d(ci,q) > r ∗, note that d(ci,q) > r ∗ implies q  Ci . For each ci ∈ B, define a(i) as the center of q’s cluster. Then d(a(i),ci ) ≤ d(a(i),q) +
d(q,ci ) ≤ 2r ∗ and so for all p ∈ Ci , we have d(a(i),p) ≤ d(a(i),ci ) + d(ci,p) ≤ 3r ∗. If for each
ci ∈ B, a(i) is not in B, then we would be able to remove B from the set of optimal centers, and the
remaining centers are still distance 3r ∗ from all points in S (finishing the first half of the proof).
However, we need to consider the case where there exist centers ci in B such that a(i) is also in
B. Our goal is to show there exists a subset B ⊆ B of size 3, such that for each ci ∈ B
, a(i)  B
;
therefore, the set of optimal centers without B is still distance 3r ∗ from all points in S.
Construct a directed graph G = (B, E) where E = {(ci,c) | c = a(i)}. Then every point has outdegree ≤ 1. Finding B corresponds to finding ≥ 3 points with no edges to one another, i.e., an
independent set of G. Consider a connected component G = (V 
, E
) of G. Since V  is connected,
we have |E
|≥|V 
| − 1. Since every vertex has out-degree ≤ 1, |E
|≤|V 
|. Then, we have two
cases.
Case 1: |E
| = |V 
| − 1. ThenG is a tree, and so there must exist an independent set of size  |V  |
2

.
Case 2: |E
| = |V 
|. ThenG contains a cycle, and so there exists an independent set of size 	 |V  |
2


.
It follows that we can always find an independent set of size 	 |V  |
2


for the entire graph G. For
|B| ≥ 7, there exists such a set B of size ≥ 3. Then, we have the property thatci ∈ B =⇒ a(i)  B
.
Now let C = {c }
k
=1 \ B
. By construction, B is distance ≤ 3r ∗ to all points in S. Consider the
following 3-perturbation d
: Increase all distances by a factor of 3, except d(a(i),p), for i such that
ci ∈ B and p ∈ Ci , which we increase to min(3r ∗, 3d(a(i),p)). Then, by Lemma 2.8, the optimal
radius is 3r ∗. Therefore, the set C achieves the optimal cost over d even though |C| ≤ k − 3. Then,
we can pick any combination of three dummy centers, and they must all result in clusterings that
are ϵ-close to OPT . We will show this contradicts (3, ϵ )-perturbation resilience.
We pick five arbitrary points p1,p2,p3,p4,p5 ∈ S \ C and define C = C ∪ {p1,p2,p3,p4,p5}. From
the above paragraph, each size 3 subset P ⊆ {p1,p2,p3,p4,p5} added to C will result in a set of
optimal centers under d
. Then, by Fact 6.2, each point inC ∪ P must be the center for the majority
of points in exactly one cluster. To obtain a contradiction, we consider the ranking defined by
Fact 6.7 of C over d
.
We start with a claim about the rankings: For eachc ∈ C
, for all pairs x,y such that x  y, if c ∈
C such that Rx,d
,C (c) < Rx,d
,C (c
) or Ry,d
,C (c) < Ry,d
,C (c
), then Rx,d
,C (c
) + Ry,d
,C (c
) ≥ 5.
In words, there cannot be two clusters such that c is ranked first among C ∪ {c
} and top two (or
first and third) among C for both clusters. Assume this is false. Then there exist x  y such that
Rx,d
,C (c
) + Ry,d
,C (c
) ≤ 4, so there are at most two total points ranked above c in Rx,d
,C and
Ry,d
,C, and these points must be from the set {p1,p2,p3,p4,p5}. Without loss of generality, denote
these points by p and p (if there are one or zero points ranked above c
, then let one or both of p
and p be arbitrary). Then consider the set of centers C \ {p,p
} that is size k and must be optimal
under d as described earlier. However, the partitioning is not ϵ-close to OPT , since c is the best
center (ranked 1) for both Cx and Cy . This completes the proof of the claim.
Now consider the set D = {ci ∈ C | ∃x s.t. Rx,d
,C (ci ) = 1}, i.e., the set of points in C that are
ranked 1 for some cluster. Denote m = (k − 3) − |D|, which is the number of points in C that are
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.        
22:36 M.-F. Balcan et al.
ALGORITHM 5: (3, ϵ )-Perturbation Resilient Asymmetric k-center
Input: Asymmetric k-center instance (S,d), r ∗ (or try all possible candidates).
(1) Build set A = {p | ∀q,d(q,p) ≤ r ∗ =⇒ d(p,q) ≤ r ∗}.
(2) Create the threshold graph G = (A, E) where E = {(u,v) | d(u,v) ≤ r ∗}. Define a new
symmetric k-center instance (S,A,d
) where d
(u,v) = distG (u,v).
(3) For all k − 6 ≤ k ≤ k, run a symmetric k-center 2-approximation algorithm on (S,A,d
).
If the output is a set of centers C achieving cost ≤ 2r ∗, then go to step 4.
(4) For all C ⊆ C of size k − 6 and S ⊆ S of size 6, return if cost(C ∪ S
) ≤ 3r ∗.
Output: Voronoi tiling G1,...,Gkusing C ∪ S as the centers.
not ranked 1 for any cluster. By the claim and since |C| = k − 3, there are exactly m + 3 clusters
whose top-ranked point is not in C. Given one such cluster Cx , again by the claim, the top two
ranked points must not be from the set D. Therefore, there are 2(m + 3) slots that must be filled
by m + 5 points, so (for all m ≥ 0) by the Pigeonhole Principle, there must exist a point p ∈ C
ranked in the top two by two different clusters. This directly contradicts the claim, so we have a
contradiction that completes the proof.
6.3.2 Algorithm under (3, ϵ )-PR. From the previous lemma, we know that at most a constant
number of centers are bad. Essentially, our algorithm runs a symmetric 2-approximation algorithm
on A, for all k − 6 ≤ k ≤ k, to find a 2-approximation for the clusters in A. For instance, iteratively
pick an unmarked point, and mark all points distance 2r ∗ away from it [30]. Then, we use brute
force to find the remaining six centers, which will give us a 3-approximation for the entire point
set. Under (3, ϵ )-perturbation resilience, this 3-approximation must be ϵ-close to OPT . We are
not able to output OPT exactly, since Condition 2 may not be satisfied for up to ϵn points. The
asymmetric k-center algorithm runs an approximation algorithm for symmetric k-center as a subroutine. The symmetric k-center instance (S,A,d) is a generalization: The set of allowable centers
A is a subset of the points S to be clustered. The classic 2-approximation algorithms for k-center
apply to this setting as well.
Theorem 6.17. Algorithm 5 runs in polynomial time and outputs a clustering that is ϵ-close to
OPT for (3, ϵ )-perturbation resilient asymmetric k-center instances such that all optimal clusters
are size > 2ϵn.
Proof. We define three types of clusters. A cluster Ci is green if ci ∈ A, it is yellow if ci  A
but Ci ∩ A  ∅, and it is red if Ci ∩ A = ∅. Denote the number of yellow clusters by y and the
number of red clusters by x. From Lemma 6.16, we know that x + y ≤ 6. The symmetric k-center
instance (S,A,d
) constructed in step 2 of the algorithm is a subset of an instance with k − x
optimal clusters of costr ∗, so the (k − x)-center cost of (S,A,d
) is at mostr ∗. Therefore, step 3 will
return a set of centers achieving cost ≤ 2r ∗ for some k ≤ k − x. By definition of green clusters,
we know that k − x − y clusters have their optimal center in A. For each green cluster Ci , let
c(i) ∈ C denote the center that is distance ≤ 2r ∗ to ci (if there is more than one point in C, then
denote c(i) by one of them arbitrarily). Let C = {c(i) | Ci is green} and |C
| ≤ k − x − y. Then the
set C ∪ {cx | x is not green} is cost ≤ 3r ∗, and the algorithm is guaranteed to encounter this set
in the final step.
Finally, we explain why C ∪ {cx | x is not green} must be ϵ-close to OPT . Let B = {cx |
x is not green}. Create a 3-perturbation in which we increase all distances by 3, except for the
distances from C ∪ B to all points in their Voronoi tile, which we increase up to 3r ∗. Then, the
ACM Transactions on Algorithms, Vol. 16, No. 2, Article 22. Publication date: March 2020.   
k-center Clustering under Perturbation Resilience 22:37
optimal score is 3r ∗ by Lemma 4.1, andC ∪ B achieves this score. Therefore, by (3, ϵ )-perturbation
resilience, the Voronoi tiling of C ∪ B must be ϵ-close to OPT . This completes the proof.
6.4 APX-hardness under Perturbation Resilience
Now, we show hardness of approximation even when it is guaranteed the clustering satisfies (α, ϵ )-
perturbation resilience for α ≥ 1 and ϵ > 0. The hardness is based on a reduction from the general
clustering instances, so the APX-hardness constants match the non-stable APX-hardness results.
This shows the condition on the cluster sizes in Theorem 6.1 is tight.14
Theorem 6.18. Given α ≥ 1, ϵ > 0, it is NP-hard to approximate k-center to 2, k-median to 1.73,
or k-means to 3.94, even when it is guaranteed the instance satisfies (α, ϵ )-perturbation resilience.
Proof. Given α ≥ 1, ϵ > 0, assume there exists a β-approximation algorithm A for k-median
under (α, ϵ )-perturbation resilience. We will show a reduction to k-median without perturbation
resilience. Given a k-median clustering instance (S,d) of size n, we will create a new instance
(S
,d
) for k = k + n/ϵ with size n = n/ϵ as follows: First, set S = S and d = d and then add
n/ϵ new points to S
, such that their distance to every other point is 2αn maxu,v ∈S d(u,v). Let
OPT denote the optimal solution of (S,d). Then the optimal solution to (S
,d
) is to use OPT
for the vertices in S and make each of the n/ϵ added points a center. Note that the cost of OPT
and the optimal clustering for (S
,d
) are identical, since the added points are distance 0 to their
center. Given a clustering C on (S,d), let C denote the clustering of (S
,d
) that clusters S as in
C and then adds n/ϵ extra centers on each of the added points. Then the cost of C and C are
the same, so it follows that C is a β-approximation to (S,d) if and only if C is a β-approximation
to (S
,d
). Next, we claim that (S
,d
) satisfies (α, ϵ )-perturbation resilience. Given a clustering
C that is an α-approximation to (S
,d
), then there must be a center located at all n/ϵ of the
added points, otherwise the cost of C would be > αOPT . Therefore, C agrees with the optimal
solution on all points except for S; therefore, C must be ϵ-close to the optimal solution. Now that
we have established a reduction, the theorem follows from hardness of 1.73-approximation for kmedian [32]. The proofs for k-center and k-means are identical, using hardness from Reference [27]
and Reference [32], respectively.
7 CONCLUSION
Our work pushes the understanding of (promise) stability conditions farther in several ways. We
are the first to design computationally efficient algorithms to find the optimal clustering under
α-perturbation resilience with a constant value of α for a problem that is hard to approximate to
any constant factor in the worst case, thereby demonstrating the power of perturbation resilience.
Furthermore, we demonstrate the limits of this power by showing the first tight results in this
space for perturbation resilience. Our work also shows a surprising relation between symmetric
and asymmetric instances, in that they are equivalent under resilience to 2-perturbations, which
is in stark contrast to their widely differing tight approximation factors. Finally, we initiate the
study of clustering under local stability. We define a local notion of perturbation resilience, and
we give algorithms that simultaneously output all optimal clusters satisfying local stability, while
ensuring the worst-case approximation guarantee. Although α = 2 is tight for k-center, the best
value of perturbation resilience for symmetric k-median and other center-based objectives is not
known. Currently, the best upper bound is α = 2 [1], but no lower bounds are known for α = 1 + ϵ,
for constant ϵ > 0.  