We consider a new construction of locality-sensitive hash functions for Hamming space that is covering in
the sense that is it guaranteed to produce a collision for every pair of vectors within a given radius r. The
construction is efficient in the sense that the expected number of hash collisions between vectors at distance cr, for a given c > 1, comes close to that of the best possible data independent LSH without the covering guarantee, namely, the seminal LSH construction of Indyk and Motwani (STOC’98). The efficiency of
the new construction essentially matches their bound when the search radius is not too large—e.g., when
cr = o(log(n)/ log logn), where n is the number of points in the dataset, and when cr = log(n)/k, where k
is an integer constant. In general, it differs by at most a factor ln(4) in the exponent of the time bounds. As
a consequence, LSH-based similarity search in Hamming space can avoid the problem of false negatives at
little or no cost in efficiency.
CCS Concepts: • Theory of computation → Nearest neighbor algorithms; Sorting and searching;
Additional Key Words and Phrases: Similarity search, high-dimensional, locality-sensitive hashing, recall
1 INTRODUCTION
Similarity search in high dimensions has been a subject of intense research for the last decades
in several research communities including theory of computation, databases, machine learning,
and information retrieval. In this article, we consider nearest-neighbor search in Hamming space,
where the task is to find a vector in a preprocessed set S ⊆ {0, 1}
d that has minimum Hamming
distance to a query vector y ∈ {0, 1}
d .
It is known that efficient data structures for this problem, i.e., whose query and preprocessing
time does not increase exponentially with d, would disprove the strong exponential time hypothesis (Williams 2005; Alman and Williams 2015). For this reason, the algorithms community has
studied the problem of finding a c-approximate nearest neighbor, i.e., a point whose distance to
y is bounded by c times the distance to a nearest neighbor, where c > 1 is a user-specified parameter. If the exact nearest neighbor is sought, then the approximation factor c can be seen as a
bound on the relative distance between the nearest and the second-nearest neighbor. All existing
c-approximate nearest-neighbor data structures that have been rigorously analyzed have one or
more of the following drawbacks:
(1) Worst-case query time linear in the number of points in the dataset, or
(2) Worst-case query time that grows exponentially with d, or
(3) Multiplicative space overhead that grows exponentially with d, or
(4) Lack of unconditional guarantee to return a nearest neighbor (or c-approximate nearest
neighbor).
Arguably, the data structures that come closest to overcoming these drawbacks are based on
locality-sensitive hashing (LSH). For many metrics, including the Hamming metric discussed in
this article, LSH yields sublinear query time (even ford  logn) and space usage that is polynomial
in n and linear in the number of dimensions (Indyk and Motwani 1998; Gionis et al. 1999). If the
approximation factorc is larger than a certain constant (currently known to be at most 3), then the
space can even be made O (nd), still with sublinear query time (Panigrahy 2006; Kapralov 2015;
Andoni et al. 2017).
However, these methods come with a Monte Carlo–type guarantee: A c-approximate nearest
neighbor is returned only with high probability, and there is no efficient way of detecting if the
computed result is incorrect. This means that they do not overcome the 4th drawback above.
Contribution. In this article, we investigate the possibility of Las Vegas-type guarantees for (capproximate) nearest-neighbor search in Hamming space. Traditional LSH schemes pick the sequence of hash functions independently, which inherently implies that we can only hope for high
probability bounds. Extending and improving results by Greene et al. (1994) and Arasu et al. (2006),
we show that in Hamming space, by suitably correlating hash functions, we can “cover” all possible
positions of r differences and thus eliminate false negatives, while achieving performance bounds
comparable to those of traditional LSH methods. Since our methods are based on combinatorial
objects called coverings, we refer to the approach as CoveringLSH.
Let ||x − y|| denote the Hamming distance between vectors x andy. Our results imply the following theorem on similarity search (specifically c-approximate near-neighbor search) in a standard
unit cost (word RAM) model:
Theorem 1.1. Given S ⊆ {0, 1}
d , c > 1, and r ∈ N+, we can construct a data structure such that
for n = |S | and a value f (n,r,c) bounded by
f (n,r,c) =
⎧⎪⎪
⎨
⎪⎪
⎩
O (1) if log(n)/(cr) ∈ N
(logn) O(1) if cr ≤ log(n)/(3 log logn)
O(min(n0.4/c r, 2r )) for all parameters
,
the following holds:
—On query y ∈ {0, 1}
d the data structure is guaranteed to return x ∈ S with ||x − y|| < cr if
there exists x  ∈ S with ||x  − y|| ≤ r.
—The expected query time is O(f (n,r,c) n1/c (1 + d/w)), where w is the word length.
—The size of the data structure is O(f (n,r,c) n1+1/c logn + nd) bits.
Our techniques, like traditional LSH, extend to efficiently solve other variants of similarity
search. For example, we can (1) handle nearest-neighbor search without knowing a bound on the
distance to the nearest neighbor, (2) return all near neighbors instead of just one, and (3) achieve
high probability bounds on query time rather than just an expected time bound.
When f (n,r,c) = O (1) the performance of our data structure matches that of classical LSH with
constant probability of a false negative (Indyk and Motwani 1998; Gionis et al. 1999), so f (n,r,c)
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.
CoveringLSH: Locality-Sensitive Hashing without False Negatives 29:3
is the multiplicative overhead compared to classical LSH. In fact, O’Donnell et al. (2014) showed
that the exponent of 1/c in query time is optimal for methods based on (data-independent) LSH.
1.1 Notation
For a set S and function f , we let f (S) = {f (x) | x ∈ S}. We use 0 and 1 to denote vectors of all 0s
and 1s, respectively. For x,y ∈ {0, 1}
d , we use x ∧y and x ∨y to denote bit-wise conjunction and
disjunction, respectively, and x ⊕ y to denote the bitwise exclusive-or. Let I (x) = {i | xi = 1}. We
use ||x || = |I (x)| to denote the Hamming weight of a vector x, and
||x − y|| = |I (x ⊕ y)|
to denote the Hamming distance between x and y. For S ⊆ {0, 1}
d , let ΔS be an upper bound on the
time required to produce a representation of the nozero entries of a vector in S in a standard (word
RAM) model (Hagerup 1998). Observe that in general ΔS depends on the representation of vectors
(e.g., bit vectors for dense vectors or sparse representations if d is much larger than the largest
Hamming weight). For bit vectors we have ΔS = O (1 + d/w) if we assume the ability to count
the number of 1s in a word in constant time1, and this is where the term 1 + d/w in Theorem 1.1
comes from. We use “x mod b” to refer to the integer in {0,...,b − 1} whose difference from x is
divisible by b. Finally, let 
x,y denote ||x ∧y||, i.e., the dot product of x and y.
2 BACKGROUND AND RELATED WORK
Given S ⊆ {0, 1}
d the problem of searching for a vector in S within Hamming distance r from a
given query vector y was introduced by Minsky and Papert as the approximate dictionary problem (Minsky and Papert 1987). The generalization to arbitrary spaces is now known as the nearneighbor problem (or sometimes as point location in balls). It is known that a solution to the approximate near-neighbor problem for fixed r (known before query time) implies a solution to
the nearest-neighbor problem with comparable performance (Indyk and Motwani 1998; Har-Peled
et al. 2012). In our case, this is somewhat simpler to see, so we give the argument for completeness. Two reductions are of interest, depending on the size of d. If d is small, then we can obtain
a nearest-neighbor data structure by having a data structure for every radius r, at a cost of factor
d in space and logd in query time, using binary search. Alternatively, if d is large we can restrict
the set of radii to the O (log(n) log(d)) radii of the form (1 + 1/ logn)
i
 < d. This decreases the
approximation factor needed for the near -neighbor data structures by a factor 1 + 1/ logn, which
can be done with no asymptotic cost in the data structures we consider. For this reason, in the
following, we focus on the near -neighbor problem in Hamming space where r is assumed to be
known when the data structure is created.
2.1 Deterministic Algorithms
For simplicity, we will restrict our attention to the case r ≤ d/2. A baseline is the brute force algorithm that looks up all (
d
r ) bit vectors of Hamming distance at most r from y. The time usage is
at least (d/r)
r , assuming r ≤ d/2, so this method is not attractive unless dr is quite small. The dependence on d was reduced by Cole et al. (2004), who achieve query time O (d + logr n) and space
O (nd + n logr n). Again, because of the exponential dependence on r, this method is interesting
only for small values of r.
1This is true on modern computers using the popcnt instruction and implementable with table lookups if w = O (log n). If
only a minimal instruction set is available, then it is possible (at least) to get ΔS = O (d/w + logw) by a folklore recursive
construction, see, e.g., Hagerup et al. (2001, Lemma 3.2).
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.
29:4 R. Pagh
2.2 Randomized Filtering with False Negatives
In a seminal article, Indyk and Motwani (1998) presented a randomized solution to the capproximate near-neighbor problem where the search stops as soon as a vector within distance
cr from y is found. Their technique can also be used to solve the approximate dictionary problem,
but the time will then depend on the number of points at distance between r + 1 and cr that we
inspect. Their data structure, like all LSH methods for Hamming space we consider in this article,
uses a set of functions from a Hamming projection family:
HA = {x → x ∧ a | a ∈ A}, (1)
where A⊆{0, 1}
d . The vectors in A will be referred to as bit masks. Given a query y, the idea is
to iterate through all functions h ∈ HA and identify collisions h(x) = h(y) for x ∈ S, e.g., using a
hash table. This procedure covers a query y if at least one collision is produced when there exists
x ∈ S with ||x − y|| ≤ r, and it is efficient if the number of hash function evaluations and collisions
with ||x − y|| > cr is not too large. The procedure can be thought of as a randomized filter that
attempts to catch data items of interest while filtering away data items that are not even close to
being interesting. The filtering efficiency with respect to vectors x and y is the expected number
of collisions h(x) = h(y) summed over all functions h ∈ HA, with expectation taken over any
randomness in the choice of A. We can argue that without loss of generality it can be assumed
that the filtering efficiency depends only on ||x − y|| and not on the location of the differences. To
see this, using an idea from Arasu et al. (2006), consider replacing each a ∈ A by a vector π (a)
defined by π (a)i = aπ (i), where π : {1,...,d}→{1,...,d} is a random permutation used for all
vectors in A. This does not affect distances and means that collision probabilities will depend
solely on ||x − y||, d, and the Hamming weights of vectors in A.
Remark. If vectors in A are sparse, then it is beneficial to work with a sparse representation of
the input and output of functions in HA, and indeed this is what is done by Indyk and Motwani,
who consider functions that concatenate a suitable number of 1-bit samples from x. However,
we find it convenient to work with d-dimensional vectors, with the understanding that a sparse
representation can be used if d is large. 
Classical Hamming LSH. Indyk and Motwani use a collection
A(R) = {a(v) | v ∈ R},
where R ⊆ {1,...,d}
k is a set of uniformly random and independent k-dimensional vectors. Each
vector v encodes a sequence of k samples from {1,...,d}, and a(v) is the projection vector that
selects the sampled bits. That is, a(v)i = 1 if and only if vj = i for some j ∈ {1,..., k}. By choosing
k appropriately, we can achieve a tradeoff that balances the size of R (i.e., the number of hash functions) with the expected number of collisions at distance cr. It turns out that |R| = O(n1/c log(1/δ ))
suffices to achieve collision probability 1 − δ at distance r while keeping the expected total number
of collisions with “far” vectors (at distance cr or more) linear in |R|.
Newer Developments. In a recent advance of Andoni and Razenshteyn (2015), extending preliminary ideas from Andoni et al. (2014), it was shown how data-dependent LSH can achieve the same
guarantee with a smaller family (having no(1) space usage and evaluation time). Specifically, it suffices to check collisions of O (nρ log(1/δ )) hash values, where ρ = 1
2c−1 + o(1). We will not attempt
to generalize the new method to the data dependent setting, though that is certainly an interesting
possible extension.
In a surprising development, it was recently shown Alman and Williams (2015) that even with
no approximation of distances (c = 1) it is possible to obtain truly sublinear time per query if (1)
d = O (logn) and (2) we are concerned with the answers to a batch of n queries.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.
CoveringLSH: Locality-Sensitive Hashing without False Negatives 29:5
2.3 Filtering Methods without False Negatives
The literature on filtering methods for Hamming distance that do not introduce false negatives,
but still yield formal guarantees, is relatively small. As in Section 2.2, the previous results can be
stated in the form of Hamming projection families (1). We consider constructions of sets A that
ensure collision for every pair of vectors at distance at most r, while at the same time achieving
nontrivial filtering efficiency for larger distances.
Choosing error probability δ < 1/(
d
r ) in the construction of Indyk and Motwani, we see that
there must exist a set R∗ of size O(log(1/δ )n1/c ) that works for every choice of r mismatching
coordinates, i.e., ensures collision under some h ∈ HA(R∗ ) for all pairs of vectors within distance
r. In particular, we have |A(R∗)| = O(dn1/c ). However, this existence argument is of little help
to design an algorithm, and hence we will be interested in explicit constructions of LSH families
without false negatives.2
Kuzjurin has given such explicit constructions of “covering” vectors (Kuzjurin 2000) but in general the bounds achieved are far from what is possible existentially (Kuzjurin 1995). Independently, Greene et al. (1994) linked the question of similarity search without false negatives to
the TurÃ¡n problem in extremal graph theory. While optimal TurÃ¡n numbers are not known
in general, Greene et al. construct a family A (based on corrector hypergraphs) that will incur few
collisions with random vectors, i.e., vectors at distance about d/2 from the query point.3 Gordon
et al. (1995) presented near-optimal coverings for certain parameters based on finite geometries—in
Section 5, we will use their construction to achieve good data structures for small r.
Arasu et al. (2006) give a construction that is able to achieve, for example, o(1) filtering efficiency
for approximation factor c > 7.5 with |A| = O(r 2.39). Observe that there is no dependence on
d in these bounds, which is crucial for high-dimensional (sparse) data. The technique of Arasu
et al. (2006) allows a range of tradeoffs between |A| and the filtering efficiency, determined by
parameters n1 and n2. No theoretical analysis is made of how close to 1 the filtering efficiency can
be made for a given c, but it seems difficult to significantly improve the constant 7.5 mentioned
above.
Independently of the work of Arasu et al. (2006), “lossless” methods for near-neighbor search
have been studied in the contexts of approximate pattern matching (Kucherov et al. 2005) and
computer vision (Norouzi et al. 2012). The analytical part of these articles differs from our setting
by focusing on filtering efficiency for random vectors, which means that differences between a
data vector and the query appear in random locations. In particular, there is no need to permute
the dimensions as described in Section 2.2. Such schemes aimed at random (or, more generally,
“high entropy”) data become efficient when there are few vectors within distance r log |S | of a
query point. Another variation of the scheme of Arasu et al. (2006) recently appeared in Deng
et al. (2015).
3 BASIC CONSTRUCTION
Our basic CoveringLSH construction is a Hamming projection family of the form (1). We start by
observing the following simple property of Hamming projection families:
Lemma 3.1. For every A⊆{0, 1}
d , every h ∈ HA, and all x,y ∈ {0, 1}
d , we have h(x) = h(y) if
and only if h(x ⊕ y) = 0.
2Indyk (2000) sketched a way to verify that a random family contains a colliding function for every pair of vectors within
distance r, but, unfortunately, the construction is incorrect (Indyk 2015). 3It appears that Theorem 3 of Greene et al. (1994) does not follow from the calculations of the article—a factor of about 4
is missing in the exponent of space and time bounds (Parnas 2015).
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.
29:6 R. Pagh
Fig. 1. The collection A7 corresponding to nonzero vectors of the Hadamard code of message length 3. The
resulting Hamming projection family HA7 , see Equation (1), is 2-covering, since for every pair of columns
there exists a row with 0s in these columns. It has weight 4/7, since there are four 1s in each row. Every row
covers 3 of the 21 pairs of columns, so no smaller 2-covering family of weight 4/7 exists.
Proof. Let a ∈ A be the vector such that h(x) = x ∧ a. We have h(x) = h(y) if and only if ai
0 ⇒ xi = yi . Since xi = yi ⇔ (x ⊕ y)i = 0 the claim follows.
Thus, to make sure all pairs of vectors within distance r collide for some function, we need our
family to have the property (implicit in the work of Arasu et al. (2006)) that every vector with 1s
in r bit positions is mapped to zero by some function, i.e., the set of 1s is “covered” by zeros in a
vector from A.
Definition 3.2. For A⊆{0, 1}
d , the Hamming projection family HA is r-covering if for every
x ∈ {0, 1}
d with ||x || ≤ r, there exists h ∈ HA such that h(x) = 0. The family is said to have weight
ω if ||a|| ≥ ωd for every a ∈ A.
A trivial r-covering family uses A = {0}. We are interested in r-covering families that have a
nonzero weight chosen to make collisions rare among vectors that are not close. Vectors in our
basic r-covering family, which aims at weight around 1/2, will be indexed by nonzero vectors in
{0, 1}
r+1. The family depends on a functionm : {1,...,d}→{0, 1}
r+1 that maps bit positions to bit
vectors of length r + 1. (We remark that if d ≤ 2r+1 − 1 and m is the function that maps an integer
to its binary representation, then our construction is identical to known coverings based on finite
geometry (Gordon et al. 1995); however, we give an elementary presentation that does not require
knowledge of finite geometry.) Define a family of bit vectors a(v) ∈ {0, 1}
d by
a˜(v)i =

0 if 
m(i),v ≡ 0 mod 2
1 otherwise , (2)
where 
m(i),v is the dot product of vectors m(i) and v. We will consider the family of all such
vectors with nonzero v:
A(m) = {a(v) | v ∈ {0, 1}
r+1
\{0}}.
Figure 1 shows the family A(m) for r = 2 and m(i) equal to the binary representation of i.
Lemma 3.3. For every m : {1,...,d}→{0, 1}
r+1, the Hamming projection family HA(m) is rcovering.
Proof. Let x ∈ {0, 1}
d satisfy ||x || ≤ r and consider a(v) ∈ A(m) as defined in (2). It is clear that
whenever i ∈ {1,...,d}\I (x) we have (a(v) ∧ x)i = 0 (recall that I (x) = {i | xi = 1}). To consider
(a(v) ∧ x)i fori ∈ I (x), let Mx = m(I (x)), where elements are interpreted asr + 1-dimensional vectors over the field F2. The span of Mx has dimension at most |Mx | ≤ ||x || ≤ r, and since the space
is r + 1-dimensional, there exists a vector vx  0 that is orthogonal to span(Mx ). In particular,

vx ,m(i) mod 2 = 0 for all i ∈ I (x). In turn, this means that a(vx ) ∧ x = 0, as desired.
If the values of the function m are “balanced” over nonzero vectors, then the family HA(m) has
weight close to 1/2 for d  2r . More precisely we have:
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.     
CoveringLSH: Locality-Sensitive Hashing without False Negatives 29:7
Lemma 3.4. Suppose |m−1 (v)|≥d/2r+1 for each v ∈ {0, 1}
r+1 andm−1 (0) = ∅. Then HA(m) has
weight at least 2r d/2r+1/d > (1 − 2r+1
d )/2.
Proof. It must be shown that ||a(v)|| ≥ 2r d/2r+1 for each nonzero vector v. Note that v has
a dot product of 1 with a set V ⊆ {0, 1}
r+1 of exactly 2r vectors (namely the nontrivial coset of v’s
orthogonal complement). For each v ∈ V , we have a(v)i = 1 for all i ∈ m−1 (v
). Thus the number
of 1s in a(v) is

v∈V
|m−1 (v
)| ≥ 2r d/2r+1
 > 
1 − 2r+1
d

d/2.
Comment on Optimality. We note that the size |HA(m) | = 2r+1 − 1 is close to the smallest possible
for an r-covering families with weight around 1/2. To see this, observe that (
d
r ) possible sets of
errors need to be covered, and each hash function can cover at most (
d/2
r ) such sets. This means
that the number of hash functions needed is at least

d
r


d/2
r
 > 2r,
which is within a factor of 2 from the upper bound. 
Lemmas 3.3 and 3.4 leave open the choice of mapping m. We will analyze the setting where m
maps to values chosen uniformly and independently from {0, 1}
r+1. In this setting the condition of
Lemma 3.4 will in general not be satisfied, but it turns out that it suffices for m to have balance in
an expected sense. We can relate collision probabilities to Hamming distances as follows:
Theorem 3.5. For all x,y ∈ {0, 1}
d , x  y, and for random m : {1,...,d}→{0, 1}
r+1,
(1) If ||x − y|| ≤ r, then Pr[∃h ∈ HA(m) : h(x) = h(y)] = 1.
(2) E[|{h ∈ HA(m) | h(x) = h(y)}|] < 2r+1−||x−y | |.
Proof. Let z = x ⊕ y. For the first part, we have ||x − y|| = ||z|| ≤ r. Lemma 3.3 states that there
exists h ∈ HA(m) such that h(z) = 0. By Lemma 3.1, this implies h(x) = h(y).
To show the second part, we fix v ∈ {0, 1}
r+1\{0}. Now consider a(v) ∈ A(m), defined in Equation (2), and the corresponding functionh(x) = x ∧ a(v) ∈ HA(m). Fori ∈ I (z), we haveh(z)i = 0 if
and only if a(v)i = 0. Since m is random and v  0 the a(v)i values are independent and random,
so the probability that a(v)i = 0 for all i ∈ I (z) is 2−||z | | = 2−||x−y | |. By linearity of expectation,
summing over 2r+1 − 1 choices of v the claim follows.
Comments. A few remarks on Theorem 3.5 (that can be skipped if the reader wishes to proceed
to the algorithmic results):
—The vectors in A(m) can be seen as samples from a Hadamard code consisting of 2r+1 vectors
of dimension 2r+1, where bit i of vector j is defined by 
i, j mod 2, again interpreting the
integersi and j as vectors in Fd
2 . Nonzero Hadamard codewords have Hamming weight and
minimum distance 2r+1. However, it does not seem that error-correcting ability in general
yields nontrivial r-covering families.
—The construction can be improved by changing m to map to {0, 1}
r+1\{0} and/or requiring
the function values of m to be balanced such that the number of bit positions mapping to
each vector in {0, 1}
r+1 is roughly the same. This gives an improved version of Lemma 3.4,
but the improvement is not significant when d is much smaller or much larger than 2r . To
keep the exposition simple, we do not analyze this variant.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.     
29:8 R. Pagh
—At first glance it appears that the ability to avoid collision for CoveringLSH (“filtering”)
is not significant when ||x − y|| = r + 1. However, we observe that for similarity search in
Hamming space it can be assumed without loss of generality that either all distances from
the query point are even or all distances are odd. This can be achieved by splitting the
dataset into two parts, having even and odd Hamming weight, respectively, and handling
them separately. For a given query y and radius r, we then perform a search in each part,
one with radius r and one with radius r − 1 (in the part of data where distance r to y is not
possible). This reduces the expected number of collisions at distance r + 1 to at most 1/2. 
Nearest Neighbor. Above we have assumed that the maximum distance cr to a point we can report was given in advance. With a small change in the algorithm, not affecting efficiency, we can
improve the maximum distance to cr1, where r1 is the distance to the nearest neighbor. The distance r1 does not need to be known when building the data structure, only the upper bound r ≥ r1.
To see this, consider the subfamily of A(m) indexed by vectors of the form v = 0r−r1v1, where
v1 ∈ {0, 1}
r1+1\{0}. We observe that collision is guaranteed up to distance r1 for some function in
this subfamily. Going through these subfamilies forr1 = 1, 2, 3,... can be done in a natural way by
lettingm map randomly to {0, 1}
r+1 and choosing v as the binary representation of 1, 2, 3,... Theorem 3.5 implies the invariant that the nearest-neighbor distance r1 satisfies r1 ≥ logv, where
v is interpreted as an integer. This means that when a point x at distance at most c log(v + 1) is
found, we can stop after finishing iteration v and return x which is guaranteed to be at distance at
most cr1. Figure 2 gives pseudocode for data structure construction and nearest-neighbor queries
using CoveringLSH.4
3.1 Approximation Factor c = log(n)/r
We first consider a case in which the method above directly gives a strong result, namely when
the threshold cr for being an approximate near neighbor equals logn. Such a threshold may be
appropriate for high-entropy datasets of dimension d > 2 logn where most distances tend to be
large (see Kucherov et al. (2005) and Norouzi et al. (2012) for discussion of such settings). In
this case, Theorem 3.5 implies efficient c-approximate near-neighbor search in expected time
O(ΔS 2r ) = O(ΔS n1/c ), where ΔS bounds the time to compute the Hamming distance between
query vector y and a vector x ∈ S. This matches the asymptotic time complexity of Indyk and
Motwani (1998).
To show this bound observe that the expected total number of collisions h(x) = h(y), summed
over all h ∈ HA(m) and x ∈ S with ||x − y|| ≥ logn, is at most 2r+1. This means that computing
h(y) for each h ∈ HA(m) and computing the distance to the vectors that are not within distance
cr but collide with y under some h ∈ HA(m) can be done in expected time O (ΔS 2r ). The expected
bound can be supplemented by a high probability bound as follows: Restart the search in a new
data structure if the expected time is exceeded by a factor of 2. Use O (logn) data structures and
resort to brute force if this fails, which happens with polynomially small probability in n.
What we have bounded is in fact performance on a worst case dataset in which most data points
are just above the threshold for being a c-approximate near neighbor. In general, the amount of
time needed for a search will depend on the distribution of distances between y and data points
and may be significantly lower.
The space required is O (2rn) = O(n1+1/c ) words plus the space required to store the vectors
in S, again matching the bound of Indyk and Motwani. In a straightforward implementation, we
need additional space O (d) to store the function m, but if d is large (for sets of sparse vectors)
4A corresponding Python implementation is available on github, https://github.com/rasmus-pagh/coveringLSH.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.
CoveringLSH: Locality-Sensitive Hashing without False Negatives 29:9
Fig. 2. Pseudocode for constructing (left) and querying (right) a nearest-neighbor data structure on a set
S ⊆ {0, 1}
d as described in Section 3.1. Parameter r controls the largest radius for which a nearest neighbor
is returned. This is the simplest instantiation of CoveringLSH—it works well on high-entropy data where
there are few points within distance r + log2 |S | of a query point. In this setting, given a query point y, the
expected search time for finding a nearest neighbor x is O(2| |x−y | |). If only a c-approxiate nearest neighbor
is sought, then the condition best ≤ log(v + 1) should be changed to best ≤ clog(v + 1).
Notation: The function Random returns a random element from a given set. The inner product 
m,v can
be computed by a bitwise conjunction followed by counting the number of bits set (popcnt). D[i] is used to
denote the information associated with key i in the dictionary D that is the main part of the data structure;
if i is not a key in D, then D[i] = ∅. The function call BitVec(v,r + 1) typecasts an integer to a bit vector of
dimension r + 1. Finally, ||x − y|| denotes the Hamming distance between x and y.
Other comments: Vectors are stored 2r+1 − 1 times in D but may be represented as references to a single
occurrence in memory to achieve better space complexity for large d. The global dictionary A, which contains
a covering independent of the set S, must be initialized by InitializeCovering before BuildDataStructure
is called. Note that the function m is not stored, as it is not needed after constructing the covering.
we may reduce this by only storing m(i) if there exists x ∈ S with xi  0. With this modification,
storingm does not change the asymptotic space usage. For dense vectors, it may be more desirable
to explicitly store the set of covering vectors A(m) rather than the function m, and indeed this is
the approach taken in the pseudocode.
Example. Suppose we have a set S of n = 230 vectors from {0, 1}
128 and wish to search for a
vector at distance at mostr = 10 from a query vector y. A brute-force search within radiusr would
take much more time than linear search, so we settle for 3-approximate similarity search. Vectors
at distance larger than 3r have collision probability at most 1/(2n) under each of the 2r+1 − 1
functions in h ∈ HA(m), so in expectation there will be less than 2r = 1024 hash collisions between
y and vectors in S. The time to answer a query is bounded by the time to compute 2, 047 hash values
for y and inspect the hash collisions.
It is instructive to compare to the family HA(R) of Indyk and Motwani, described in Section 2.2,
with the same performance parameters (2, 047 hash evaluations, collision probability 1/(2n) at
distance 31). A simple computation shows that for k = 78 samples we get the desired collision
probability, and collision probability (1 − r/128)
78 ≈ 0.0018 at distance r = 10. This means that
the probability of a false negative by not producing a hash collision for a point at distance r is
(1 − (1 − r/128)
78)
2047 > 0.027. So the risk of a false negative is nontrivial given the same time and
space requirements as our “covering” LSH scheme. 
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018. 
29:10 R. Pagh
Fig. 3. The collection A2×7 containing two copies of the collection A7 from figure 1, one for each half of the
dimensions. The resulting Hamming projection family HA2×7 , see (1), is 5-covering, since for set of 5 columns
there exists a row with 0s in these columns. It has weight 4/14, since there are four 1s in each row. Every
row covers (
10
5 ) sets of 5 columns, so a lower bound on the size of a 5-covering collection of weight 4/14 is
(
14
5 )/(
10
5 ) = 8.
4 CONSTRUCTION FOR LARGE DISTANCES
Our basic construction is only efficient when cr has the “right” size (not too small, not too large).
We now generalize the construction to arbitrary values of r, cr, and n, with a focus on efficiency
for large distances. In a nutshell:
—For an arbitrary choice of cr (even much larger than logn), we can achieve performance
that differs from classical LSH by a factor of ln(4) < 1.4 in the exponent.
—We can match the exponent of classical LSH for the c-approximate near-neighbor problem
whenever logn/(cr) is (close to) integer.
We still use a Hamming projection family (1), changing only the set A of bit masks used. Our
data structure will depend on parametersc and r, i.e., these cannot be specified as part of a query.
Without loss of generality, we assume that cr is integer.
Intuition. When cr < logn we need to increase the average number of 1s in the bit masks to
reduce collision probabilities. The increase should happen in a correlated fashion to maintain the
guarantee of collision at distance r. The main idea is to increase the fraction of 1s from 1/2 to
1 − 2−t , for t ∈ N, by essentially repeating the sampling from the Hadamard code t times and
selecting those positions where at least one sample hits a 1.
On the other hand, when cr > logn, we need to decrease the average number of 1s in the bit
masks to increase collision probabilities. This is done using a refinement of the partitioning method
of Arasu et al. (2006) which distributes the dimensions across partitions in a balanced way. The reason this step does not introduce false negatives is that for each data point x there will always exist
a partition in which the distance between query y and x is at most the average across partitions.
An example is shown in Figure 3. 
We use b,q ∈ N to denote, respectively, the number of partitions and the number of partitions to
which each dimension belongs. Conceptually, we first expand every dimension into q dimensions,
simply copying the bit value, increasing all distances by a factor q. Second, we distribute the qd
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.
CoveringLSH: Locality-Sensitive Hashing without False Negatives 29:11
dimensions of the expanded vectors among b partitions. If two vectors have distance at most r,
then there will always exist a partition where the expanded vectors have distance at most r =
rq/b. Let Intervals(b,q) denote the set of intervals in {1,...,b} of length q, where intervals are
considered modulo b (i.e., with wraparound). We will use two random functions,
m : {1,...,d} → 
{0, 1}
t r
+1
t
s : {1,...,d} → Intervals(b,q),
to define a family of bit vectors a(v, k) ∈ {0, 1}
d , indexed by vectors v ∈ {0, 1}
t r
+1 and k ∈
{1,...,b}. We define a family of bit vectors a(v, k) ∈ {0, 1}
d by
a(v, k)i = s−1 (k)i ∧ 



j

m(i)j,v mod 2  0
	



, (3)
where s−1 (k) is the preimage of k under s represented as a vector in {0, 1}
d (that is, s−1 (k)i = 1 if
and only if k ∈ s(i)) and 
m(i)j,v is the dot product of vectors m(i)j and v. We will consider the
family of all such vectors with nonzero v:
A(m,s) = 
a(v, k) | v ∈ {0, 1}
t r
+1
\{0}, k ∈ {1,...,b}

.
Note that the size of A(m,s) is b (2t r
+1 − 1) < 2b 2trq/b .
Lemma 4.1. For every choice of b,d,q,t ∈ N, and every choice of functions m and s as defined
above, the Hamming projection family HA(m,s ) is r-covering.
Proof. Let x ∈ {0, 1}
d satisfy ||x || ≤ r. We must argue that there exists a vector v∗ ∈
{0, 1}
t r
+1\{0} and k∗ ∈ {1,...,b} such that a(v∗, k∗) ∧ x = 0, i.e., by Equation (3)
∀i : xi ∧ s−1 (k)i ∧ 



j

m(i)j,v mod 2  0
	



= 0.
We let k∗ = arg min ||x ∧ s−1 (k)||, breaking ties arbitrarily. Informally, k∗ is the partition with the
smallest number of 1s in x. Note that b
k=1 ||x ∧ s−1 (k)|| = qr so by the pigeonhole principle, ||x ∧
s−1 (k∗)|| ≤ rq/b = r
. Now consider the “problematic” set I (x ∧ s−1 (k∗)) of positions of 1s in
x ∧ s−1 (k∗), and the set of vectors that m associates with it:
Mx = {m(I (x ∧ s−1 (k∗)))j | j ∈ {1,...,t}}.
The span of Mx has dimension at most tr
, since |Mx | ≤ tr
. This means that there must exist
v∗ ∈ {0, 1}
t r
+1\{0} that is orthogonal to all vectors in Mx . In particular, this implies that for each
i ∈ Ix we have 
j
m(i)j,v∗ mod 2  0 is false, as desired.
We are now ready to show the following extension of Theorem 3.5:
Theorem 4.2. For random m and s, for every b,d,q,r,t ∈ N and x,y ∈ {0, 1}
d , x  y:
(1) ||x − y|| ≤ r ⇒ Pr[∃h ∈ HA(m,s ) : h(x) = h(y)] = 1.
(2) E[|{h ∈ HA(m,s ) | h(x) = h(y)}|] < (1 − (1 − 2−t )q/b)| |x−y | |b 2trq/b+1.
Proof. By Lemma 3.1, we have h(x) = h(y) if and only if h(z) = 0, where z = x ⊕ y. So the first
part of the theorem is a consequence of Lemma 4.1. For the second part, consider a particular
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.     
29:12 R. Pagh
vector a(v, k), where v is nonzero, and the corresponding hash value h(z) = z ∧ a(v, k). We argue
that over the random choice of m and s we have, for each i:
Pr[a(v, k)i = 0] = Pr[s−1 (k)i = 0] + Pr[s−1 (k)i = 1 ∧ ∀j : 
m(i)j,v ≡ 0 mod 2]
= (1 − q/b) + 2−t
q/b (4)
= 1 − (1 − 2−t )q/b.
The second equality uses independence of the vectors {m(i)j | j = 1,...,t} and s(i) and that for
each j we have Pr[
m(i)j,v ≡ 0 mod 2] = 1/2. Observe also that a(v, k)i depends only on s(i) and
m(i). Since function values of s and m are independent, so are the values
{a(v, k)i | i ∈ {1,...,d}}.
This means that the probability of having a(v, k)i = 0 for all i, where zi = 1 is a product of probabilities from Equation (4):
Pr[h(x) = h(y)] =
	
i ∈Iz
(1 − (1 − 2−t )q/b) = (1 − (1 − 2−t )q/b)| |x−y | |.
The second part of the theorem follows by linearity of expectation, summing over the vectors in
A(m,s).
4.1 Choice of Parameters
The expected time complexity of c-approximate near-neighbor search with radius r is bounded
by the size |A| of the hash family plus the expected number κA of hash collisions between the
query y and vectors S that are not c-approximate near neighbors. Define
Sfar = {x ∈ S | ||x − y|| > cr} and κA = E [|{(x,h) ∈ Sfar × HA | h(x) = h(y)}|],
where the expectation is over the choice of family A. Choosing parameters t, b, and q in Theorem 4.2 to get a family A that minimizes |A| + κA is nontrivial. Ideally, we would like to balance
the two costs, but integrality of the parameters means that there are “jumps” in the possible sizes
and filtering efficiencies of HA(m,s ). Figure 4 shows bounds achieved by numerically selecting the
best parameters in different settings. We give a theoretical analysis of some choices of interest below. In the most general case, the strategy is to reduce to a set of subproblems that hit the “sweet
spot” of the method, i.e., where |A| and κA can be made equal. 
Corollary 4.3. For every c > 1, there exist explicit, randomized r-covering Hamming projection
families HA1 , HA2 such that for every y ∈ {0, 1}
d :
(1) |A1 | ≤ 2r+1
n1/c and κA1 < 2r+1
n1/c .
(2) If log(n)/(cr) + ε ∈ N, for ε > 0, then |A1 | ≤ 2εr+1
n1/c and κA1 < 2εr+1
n1/c .
(3) If r > ln(n)/c, then |A2 | ≤ 8r nln(4)/c and κA2 < 8r nln(4)/c .
Proof. We let A1 = A(m,s) with b = q = 1 and t = log(n)/(cr). Then
|A1 | < 2b 2trq/b = b 2t r+1 ≤ 2(log(n)/(cr)+1)r+1 = 2r+1
n1/c .
Summing over x ∈ Sfar, the second part of Theorem 4.2 yields
κA1 < n2−tcr 2t r+1 ≤ 2t r+1 ≤ 2r+1
n1/c .
For the second bound on A1 we notice that the factor 2r is caused by the rounding in the
definition of t, which can cause 2t r to jump by a factor 2r . When log(n)/(cr) + ε is integer, we
instead get a factor 2εr .
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018. 
CoveringLSH: Locality-Sensitive Hashing without False Negatives 29:13
Fig. 4. Expected number of memory accesses for different similarity search methods for finding a vector
within Hamming distance r of a query vector y. The plots are for r = 16 and r = 256, respectively, and are
for a worst-case dataset where all points have distance 2r from y, i.e., there exists no c-approximate near
neighbor for an approximation factor c < 2. The bound for exhaustive search in a Hamming ball of radius r
optimistically assumes that the number of dimensions islog2 n, which is smallest possible for a dataset of size
n (for r = 256 this number is so large that it is not even shown). Two bounds are shown for the classical LSH
method of Indyk and Motwani: A small fixed false-negative probability of 1% and a false-negative probability
of 1/n. The latter is what is needed to ensure no false negatives in a sequence of n searches. The bound for
CoveringLSH in the case r = 16 uses a single partition (b = 1), while for r = 256 multiple partitions are used.
Finally, we let A2 = A(m,s) with b = r, q = 2ln(n)/c, and t = 1. The size of A2 is bounded
by b 2trq/b+1 ≤ r 22 ln(n)/c+3 = 8r nln(4)/c . Again, by Theorem 4.2 and summing over x ∈ Sfar:
κA2 < n (1 − q/(2r))
cr r 2q+1 < n exp (−qc/2)r 2q+1 < r 2q+1 < 8r nln(4)/c ,
where the second inequality follows from the fact that 1 − α < exp(−α) when α > 0.
5 CONSTRUCTION FOR SMALL DISTANCES
In this section, we present a different generalization of the basic construction of Section 3 that
is more efficient for small distances, cr ≤ log(n)/(3 log logn), than the construction of Section 4.
The existence of asymptotically good near-neighbor data structures for small distances is not a
big surprise: For r = o(log(n)/ log logn), it is known how to achieve query time no(1) (Cole et al.
2004), even with c = 1. In practice, this is unlikely to be faster than linear search for realistic values
of n except when r is a small constant. In contrast, we seek a method that has reasonable constant
factors and may be useful in practice.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018. 
29:14 R. Pagh
The idea behind the generalization is to consider vectors and dot products modulo p for some
prime p > 2. This corresponds to using finite geometry coverings over the field of size p (Gordon
et al. 1995), but, like in Section 3, we make an elementary presentation without explicitly referring to finite geometry. Vectors in the r-covering family, which aims at weight around 1 − 1/p,
will be indexed by nonzero vectors in {0,...,p − 1}
r+1. Generalizing the setting of Section 3, the
family depends on a function m : {1,...,d}→{0,...,p − 1}
r+1 that maps bit positions to vectors
of length r + 1. Define a family of bit vectors a˜(v) ∈ {0, 1}
d , v ∈ {0,...,p − 1}
r+1 by
a˜(v)i =

0 if 
m(i),v ≡ 0 mod p,
1 otherwise (5)
for all i ∈ {1,...,d}, where 
m(i),v is the dot product of vectors m(i) and v. We will consider the
family of all such vectors with nonzero v:
A˜(m) = {a˜(v) | v ∈ {0,...,p − 1}
r+1
\{0}}.
Lemma 5.1. For every m : {1,...,d}→{0,...,p − 1}
r+1, the Hamming projection family HA˜(m)
is r-covering.
Proof. Identical to the proof of Lemma 3.3. The only difference is that we consider the field Fp
of size p.
Next, we relate collision probabilities to Hamming distances as follows:
Theorem 5.2. For all x,y ∈ {0, 1}
d and for random m : {1,...,d}→{0,...,p − 1}
r+1,
(1) If ||x − y|| ≤ r, then Pr[∃h ∈ HA˜(m) : h(x) = h(y)] = 1.
(2) E[|{h ∈ HA˜(m) | h(x) = h(y)}|] < pr+1−||x−y | |.
Proof. The proof is completely analogous to that of Theorem 3.5. The first part follows from
Lemma 5.1. For the second part, we use that Pr[
m(i),v ≡ 0 mod p] = 1/p for each v  0 and that
we are summing over pr+1 − 1 values of v.
Now suppose that cr ≤ log(n)/(3 log logn) and let p be the smallest prime number such that
pcr > n or, in other words, the smallest prime p > n1/(cr)
. We refer to the family A˜(m) with this
choice of p as A3, and note that |A3 | < pr+1.
By the second part of Theorem 5.2, the expected total number of collisions h(x) = h(y), summed
over all h ∈ HA3 and x ∈ S with ||x − y|| ≥ cr, is at most pr+1. This means that computing h(y)
for each h ∈ HA3 and computing the distance to the vectors that are not within distance cr but
collide with y under some h ∈ HA3 can be done in expected time O (ΔS pr ).
What remains is to bound pr+1 in terms of n and c. According to results on prime gaps (see,
e.g., Dudek (2016) and its references), there exists a prime between every pair of cubes α3 and
(α + 1)
3 for α larger than an explicit constant. Assuming α > 4, this implies that there exists a
prime between α3 and (1 + 4/α)α3. If n exceeds a certain constant, since p is the smallest such
prime, then, choosing α = n1/(3cr)
, we have p < (1 + 4/α)α3. By our upper bound on cr, we have
α > nlog log(n)/ log n = logn. Using r + 1 ≤ log(n), we have
|A3 | < pr+1 < ((1 + 4/α)α3)
r+1 < (1 + 4/ logn)
log nn
r+1
cr < e4
n
r+1 cr . (6)
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.    
CoveringLSH: Locality-Sensitive Hashing without False Negatives 29:15
Improvement for Small r. To asymptotically improve this bound for small r, we observe that
without loss of generality we can assume thatcr ≥ log(n)/(6 log logn): If this is not the case, then
move to vectors of dimension dt by repeating all vectorst times, where t is the largest integer with
crt ≤ log(n)/(3 log logn). This increases all distances by a factor exactly t < logn and increases
ΔS by at most a factor t < logn. Then we have
|A3 | < pr+1 < n
r+1
cr = n1/c+1/(cr) ≤ n1/c (logn)
6
. (7)
That is, the expected time usage of pr+1 matches the asymptotic time complexity of Indyk and
Motwani (1998) up to a polylogarithmic factor.
Comments. In principle, we could combine the construction of this section with partitioning to
achieve improved results for some parameter choices. However, it appears difficult to use this for
improved bounds in general, so we have chosen to not go in that direction. The constant 3 in the
upper bound on cr comes from bounds on the maximum gap between primes. A proof of Cram’s
conjecture on the size of prime gaps would imply that 3 can be replaced by any constant larger
than 1, which in turn would lead to a smaller exponent in the polylogarithmic overhead.
6 PROOF OF THEOREM 1.1
The data structure will choose either A1 or A2 of Corollary 4.3 or A3 of Section 5 with size
bounded in Ref. (7), depending on which i ∈ {1, 2, 3} minimizes |Ai | + κAi . The term n0.4/c comes
from part (3) of Corollary 4.3 and the inequality ln(4) < 1.4.
The resulting space usage is O (|Ai |n logn + nd) bits, representing buckets by list of pointers to
an array of all vectors in S. Also observe that the expected query time is bounded by |Ai | + κAi .
7 CONCLUSION AND OPEN PROBLEMS
We have seen that, at least in Hamming space, LSH-based similarity search can be implemented
to avoid the problem of false negatives at little or no cost in efficiency compared to conventional
LSH-based methods. The methods presented are simple enough that they may be practical, and
indeed practicality for small Hamming distances was recently demonstrated in Pham and Pagh
(2016). An obvious open problem is to completely close the gap or show that a certain loss of
efficiency is necessary (the non-constructive bound in Section 2.3 shows that the gap is at most
a factor O (d)). Recently, Ahle (2017) managed to close the gap up to a o(1) additive term in the
exponent.
It is of interest to investigate the possible time-space tradeoffs. CoveringLSH uses superlinear
space and employs a data independent family of functions. Is it possible to achieve covering guarantees in linear or near-linear space? Can data structures with very fast queries and polynomial
space usage match the performance achievable with false negatives (Andoni et al. 2017)?
Another interesting question is what results are possible in this direction for other spaces and
distance measures, e.g., 1, 2, ∞, and set similarity search. See Sankowski and Wygocki (2017)
and Ahle (2017) for recent progress in this direction.
Finally, CoveringLSH is data independent. Is it possible to improve performance by using datadependent techniques?
ACKNOWLEDGMENTS
The author thanks Ilya Razenshteyn for useful comments; Thomas Dybdal Ahle, Ugo Vaccaro,
and Annalisa De Bonis for providing references to work on explicit covering designs; Piotr Indyk
for information on reduction from 1 and 2 metrics to the Hamming metric; and members of the
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 29. Publication date: June 2018.      
29:16 R. Pagh
Scalable Similarity Search project for many rewarding discussions of this material. Finally, thanks
to the anonymous reviewers for numerous constructive suggestions.