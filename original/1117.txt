We investigate the complexity of several fundamental polynomial-time solvable problems on graphs and on
matrices, when the given instance has low treewidth; in the case of matrices, we consider the treewidth of
the graph formed by non-zero entries. In each of the considered cases, the best known algorithms working
on general graphs run in polynomial time; however, the exponent of the polynomial is large. Therefore, our
main goal is to construct algorithms with running time of the form poly(k) · n or poly(k) · n logn, where
k is the width of the tree decomposition given on the input. Such procedures would outperform the best
known algorithms for the considered problems already for moderate values of the treewidth, like O(n1/c ) for
a constant c.
Our results include the following:
—an algorithm for computing the determinant and the rank of an n × n matrix using O(k3 · n) time and
arithmetic operations;
—an algorithm for solving a system of linear equations using O(k3 · n) time and arithmetic operations;
—an O(k3 · n logn)-time randomized algorithm for finding the cardinality of a maximum matching in
a graph;
—an O(k4 · n log2 n)-time randomized algorithm for constructing a maximum matching in a graph;
—an O(k2 · n logn)-time algorithm for finding a maximum vertex flow in a directed graph.
Moreover, we give an approximation algorithm for treewidth with time complexity suited to the running
times as above. Namely, the algorithm, when given a graph G and integer k, runs in time O(k7 · n logn) and
either correctly reports that the treewidth of G is larger than k, or constructs a tree decomposition of G of
width O(k2).
The above results stand in contrast with the recent work of Abboud et al. (SODA 2016), which shows that
the existence of algorithms with similar running times is unlikely for the problems of finding the diameter
and the radius of a graph of low treewidth.
CCS Concepts: • Theory of computation → Graph algorithms analysis; Parameterized complexity
and exact algorithms; Approximation algorithms analysis; • Mathematics of computing→Computations
on matrices; Graph algorithms;
Additional Key Words and Phrases: Treewidth, Gaussian elimination, maximum matching, parameterized
complexity
1 INTRODUCTION
The idea of exploiting small separators in graphs dates back to the early days of algorithm
design, and in particular to the work of Lipton and Tarjan [50]. Namely, the Lipton-Tarjan planar
separator theorem states that every n-vertex planar graph admits a separator of size O(
√
n) that
splits it in a balanced way, and which moreover can be found efficiently. Applying Divide &
Conquer on small balanced separators is now a basic technique, commonly used when working
with algorithms on “well-decomposable” graphs, including polynomial-time, approximation, and
parameterized paradigms.
In structural graph theory, the concept of graphs that are well-decomposable using small separators, is captured by the notion of treewidth. Informally, the treewidth of a graph is the optimum width of its tree decomposition, which expresses the idea of breaking it into small pieces
using small separators. Because discovering a low-width tree decomposition of a graph provides
a uniform way to exploit the properties of small separators algorithmically, usually by means of
dynamic programming or Divide & Conquer, treewidth became one of the fundamental concepts
in graph algorithms. We refer to chapters in textbooks [22, Chapter 7], [73, Chapter 10], and [47,
Chapter 10] for an introduction to treewidth and its algorithmic applications.
Treewidth is also important from the point of view of applications, as graphs of low treewidth
do appear in practice. For instance, the control-flow graphs of programs in popular programming
languages have constant treewidth [39, 69]. On the other hand, topologically constrained graphs,
like planar graphs or H-minor free graphs, have treewidth O(
√
n).
Arguably, the usefulness of treewidth as a robust abstraction for the concept of being welldecomposable has been very well understood in the design of algorithms for NP-hard problems,
and in particular in parameterized complexity. However, it seems that the applicability of treewidth
for solving fundamental polynomial-time solvable problems, like Diameter, Maximum Matching, or Maximum Flow, is relatively unexplored. Prior to our work, the research on polynomialtime algorithms on well-decomposable graphs was mainly focused on using Lipton-Tarjan-style
separator theorems for specific graph classes, rather than treewidth and tree decompositions (see,
e.g., [3, 17, 56, 77]). We see two main reasons for this phenomenon.
First, the standard dynamic programming approach on graphs of low treewidth inherently requires time which is exponential in the treewidth of the graph (as one considers all possible behaviors with respect to separators of size related to treewidth). While the exponential dependence on
the treewidth is unavoidable for NP-hard problems (unless P=NP), for polynomial-time solvable
problems it would be much more desirable to have algorithms on a graph of treewidth k with
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
Fully Polynomial-Time Parameterized Computations for Graphs 34:3
running times of the form f (k) · nc , for some very small constant c and a polynomial function f .
Consider, for example, the Maximum Matching problem. There is a wide variety of algorithms
for finding a maximum matching in a graph; however, their running times are far from linear. On
the other hand, it is not hard to obtain an algorithm for Maximum Matching on a graph, given
together with a tree decomposition of width k, that runs in time O(3k · k O(1) · n); this outperforms all the general-purpose algorithms for constant values of the treewidth. But is it possible
to obtain an algorithm with running time O(kd · n) or O(kd · n logn) for some constant d, implying a significant speed-up already for moderate values of treewidth, like k = O(n1/3d )? This
question can be asked also for a number of other problems for which the known general-purpose
polynomial-time algorithms have an unsatisfactory running time. As mentioned earlier, standard
dynamic programming on tree decompositions seems difficult to apply here, due to the inherently
exponential number of states. Interestingly, the recent work of Abboud et al. [1] indicates that for
some polynomial-time solvable problems this seems to be a real obstacle. In particular, Abboud
et al. [1] proved that the Diameter and Radius problems can be solved in time 2O(k log k) · n1+o(1)
on graphs of treewidth k, but achieving running time of the form 2o(k) · n2−ε for any ε > 0 for Diameter would already contradict the Strong Exponential Time Hypothesis (SETH) of Impagliazzo
et al. [43]; the same lower bound is also given for Radius, but under a stronger assumption.
Second, in order to use the treewidth effectively, we need efficient algorithms to construct low
width decompositions. Computing treewidth exactly is NP-hard [5]. Standard parameterized algorithms for approximating treewidth [8, 10, 64] have exponential running time dependency on
the target width, while known polynomial-time approximation algorithms, e.g., [30], are based on
heavy tools like semi-definite programming, and hence their running time is far from linear with
respect to the graph size. Coming back to our example with Maximum Matching, to find a tree
decomposition of width at most k, we either have to run an algorithm with running time exponential in k or to use a polynomial-time treewidth approximation whose running time is much worse
than the time of any reasonable algorithm solving Maximum Matching directly.
Thus, in order to understand the applicability of treewidth as a tool for polynomial-time solvable
problems, we have to develop a new algorithmic toolbox and new approximation algorithms suitable for such purposes. The goal of this article is to provide basic answers to the questions above,
and thus to initiate a systematic study of the treewidth parameterization for fundamental problems
that are solvable in polynomial time, but for which the fastest known general-purpose algorithms
have an unsatisfactory running time. Examples of such problems include Maximum Matching,
Maximum Flow, and various algebraic problems on matrices, like computing determinants or
solving systems of linear equations. For such problems, our main concrete goal is to design an algorithm with running time of the form O(kd · p(n)) for some constant d and polynomial p(n) that
would be much smaller than the running time bound of the fastest known unparameterized algorithm. Mirroring the terminology of parameterized complexity, we will call such algorithms fully
polynomial FPT (FPT stands for fixed-parameter tractable). Although several results of this kind are
scattered throughout the literature [2, 14–16, 60], mostly concerning shortest path problems, no
systematic investigations have been made so far.
1.1 Our Contribution
Our first main result is a new approximation algorithm for treewidth, which is suited to the type
of running times at which we aim.
Theorem 1.1. There exists an algorithm that, given a graphG on n vertices and a positive integer k,
in time O(k7 · n logn) either provides a tree decomposition of G of width at most O(k2), or correctly
concludes that tw(G) ≥ k.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
34:4 F. V. Fomin et al.
Thus, Theorem 1.1 can serve the same role for fully polynomial FPT algorithms parameterized
by treewidth, as Bodlaender’s linear-time algorithm for treewidth [8] serves for Courcelle’s theorem [21]: it can be used to remove the assumption that a suitable tree decomposition is given on
the input, because such a decomposition can be approximated roughly within the same running
time.
Next, we turn to algebraic problems on matrices. Given an n × m matrix A over some field, we
can construct a bipartite graph GA as follows: the vertices on the opposite sides of the bipartition
correspond to rows and columns of A, respectively, and a row is adjacent to a column if and only
if the entry on their intersection is non-zero in A. Then, we can investigate the complexity of
computational problems when a tree decomposition ofGA of (small) width k is given on the input.
As a graph on n vertices and of treewidth k has at most kn edges, it follows that such matrices
are sparse—they contain only O(kn) non-zero entries. It is perhaps more convenient to think of
them as edge-weighted bipartite graphs: we label each edge of GA with the element placed in the
corresponding entry of the matrix. We assume the matrix is given in sparse form, e.g., as adjacency
lists for GA.
Our main result here is a pivoting scheme that essentially enables us to perform Gaussian elimination on matrices of small treewidth. In particular, we are able to obtain useful factorizations of
such matrices, which gives us information about the determinant and rank, and the possibility to
solve linear equations efficiently. We cannot expect to invert matrices in near-linear time, as even
very simple matrices have inverses with Ω(n2) entries (e.g., the square matrix with M[j,i] = 1 for
i − j ∈ {0, 1}, 0 elsewhere). The following theorems gather the main corollaries of our results; we
refer to Sections 2 and 4 for definitions of pathwidth, tree-partition width, and more details on
the form of factorizations that we obtain. Note that for square matrices, the same results can be
applied to decompositions of the usual symmetric graph, as defined and explained in Section 2.
Theorem 1.2. Given an n × m matrix M over a field F and a path or tree-partition decomposition of
its bipartite graphGM of width k, Gaussian elimination on M can be performed using O(k2 · (n + m))
field operations and time. In particular, the rank, determinant, a maximal nonsingular submatrix, and
a PLUQ-factorization can be computed in this time. Furthermore, for every r ∈ Fn, the system of linear
equations Mx = r can be solved in O(k · (n + m)) additional field operations and time.
Theorem 1.3. Given an n × m matrix M over a field F and a tree-decomposition of its bipartite
graph GM of width k, we can calculate the rank, determinant, and a generalized LU -factorization of
M in O(k3 · (n + m)) field operations and time. Furthermore, for every r ∈ Fn, the system of linear
equations Mx = r can be solved in O(k2 · (n + m)) additional field operations and time.
Our algorithms work more efficiently for parameters pathwidth and tree-partition width, which
can be larger than treewidth. The reason is the pivoting scheme underlying Theorems 1.2 and 1.3
works perfectly for path and tree-partition decompositions, whereas for standard tree decomposition the scheme can possibly create a lot of new non-zero entries in the matrix. However, we
show how to reduce the case of tree decompositions to tree-partition decompositions by adjusting
the idea of matrix sparsification for nested dissection of Alon and Yuster [3] to the setting of tree
decompositions. Unfortunately, this reduction incurs an additional k factor in the running times
of our algorithms, and we obtain a less robust factorization.
Observe that one can also use the known inequality pw(G) ≤ tw(G) · log2 n (see, e.g., [11]; here
n = |V (G)|) to reduce the treewidth case to the pathwidth case. This trades the additional factor
k for a factor (logn)
2; depending on the actual value of k, this might be beneficial for the overall
running time.
Note that Theorems 1.2 and 1.3 work over any field F. Hence, we can use them to develop an
algebraic algorithm for the maximum matching problem, using the classic approach via the Tutte
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
Fully Polynomial-Time Parameterized Computations for Graphs 34:5
matrix. This requires working in a field F (say, Fp ) of polynomial size, and hence the complexity
of performing arithmetic operations in this field depends on the computation model. Below, we
count all such operations as constant time, and elaborate on this issue in Section 5.
Theorem 1.4. There exists an algorithm that, given a graph G together with its tree decomposition
of width at most k, uses O(k3 · n) time and field operations and computes the size of a maximum
matching in G. The algorithm is randomized with one-sided error: it is correct with probability at
least 1 − 1/nc for an arbitrarily chosen constant c, and in the case of an error it reports a suboptimal
value.
Theorem 1.4 only provides the size of a maximum matching; to construct the matching itself,
we need some more work.
Theorem 1.5. There exists an algorithm that, given a graph G together with its tree decomposition
of width at most k, uses O(k4 · n logn) time and field operations and computes a maximum matching
in G. The algorithm is randomized with one-sided error: it is correct with probability at least 1 − 1/nc
for an arbitrarily chosen constant c, and in the case of an error it reports a failure or a suboptimal
matching.
We remark that our algebraic approach is tailored to unweighted graphs, and cannot be easily
extended to the weighted setting.
Finally, we turn our attention to the maximum flow problem. We prove that for vertex-disjoint
flows we can also design a fully polynomial FPT algorithm with near-linear running time dependence on the size of the input. The algorithm works even on directed graphs (given a tree
decomposition of the underlying undirected graph), but only in the unweighted setting (i.e., with
unit vertex capacities, which boils down to finding vertex-disjoint paths).
Theorem 1.6. There exists an algorithm that given an unweighted directed graph G on n vertices,
distinct terminals s,t ∈ V (G) with (s,t)  E(G), and a tree decomposition of G of width at most k,
works in time O(k2 · n logn) and computes a maximum (s,t)-vertex flow together with a minimum
(s,t)-vertex cut in G.
Theorem 1.6 states the result only for single-source and single-sink flows, but it is easy to reduce
other variants, like (S,T )-flows, to this setting. Note that, in particular, Theorem 1.6 provides an
algorithm for the maximum matching problem in bipartite graphs that is faster than the general
one from Theorem 1.5: one just needs to add a new source s and a new sink t to the graph, and
make s and t fully adjacent to the opposite sides of the bipartition.
1.2 Related Work on Polynomial-Time Algorithms on Small Treewidth Graphs
The reachability and shortest paths problems on low treewidth graphs have received considerable
attention in the literature, especially from the point of view of data structures [2, 14–16, 60]. In
these works, the running time dependence on treewidth is either exponential or polynomial, which
often leads to interesting tradeoff questions. For most of these problems, classical algorithms already gave optimal time bounds up to logarithmic factors, but empirical studies suggest practical
applicability of leveraging low treewidth even in that case [60]. However, the only instance we are
aware of where an asymptotic improvement follows already for polynomially small treewidth is
an O(k2 · n logn) algorithm for computing the so-called almost-sure reachability set in Markov decision processes by Chatterjee and Łącki [14], improving over a general O(m√
m) = O(k1.5 · n1.5)
algorithm.
As far as computation of maximum flows is concerned, we are aware only of the work of
Hagerup et al. on multicommodity flows [40]; however, their approach inevitably leads to exponential running time dependence on the treewidth. The work of Hagerup et al. [40] was later
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018. 
34:6 F. V. Fomin et al.
used by Chambers and Eppstein [13] for the maximum flow problem in one-crossing-minor-free
graphs; unfortunately, the exponential dependency on the size of the excluded minor persists.
1.3 Related Work on Approximating Treewidth
Computing treewidth exactly is NP-hard [5], and moreover there is no constant-factor approximation for treewidth unless the Small Set Expansion Hypothesis fails [76]. However, when we allow
the algorithm to run in FPT time when parameterized by the target width, then there is a wide variety of exact and approximation algorithms. Perhaps the best known are the four-approximation
algorithm in 2O(k) · n2 time of Robertson and Seymour [64] (see [22, 47] for an exposition of this
algorithm) and the linear-time exact algorithm of Bodlaender with running time k O(k3 ) · n [8].
Recently, Bodlaender et al. [10] obtained a three-approximation in time 2O(k) · n logn and a fiveapproximation in time 2O(k) · n. Essentially, all the known approximation algorithms for treewidth
follow the approach of Robertson and Seymour [64], which is based on recursively decomposing
subgraphs by breaking them using balanced separators.
As far as polynomial-time approximation algorithms are concerned, the best known algorithm
is due to Feige et al. [30] and it achieves approximation factor O(

logOPT ) in polynomial time.
Unfortunately, the running time is far from linear due to the use of semi-definite programming for
the crucial subroutine of finding balanced separators; this is also the case in previous works [4,
49], which are based on linear programming as well.
For this reason, in the proof of Theorem 1.1 we develop a purely combinatorial O(OPT )-factor
approximation algorithm for finding balanced separators. This algorithm is based on the techniques of Feige and Mahdian [31], which are basic enough so that they can be implemented within
the required running time. The new approximation algorithm for balanced separators is then combined with a trick of Reed [63]. Essentially, the original algorithm of Robertson and Seymour [64]
only breaks the (small) interface between the subgraph being decomposed and the rest of the graph,
which may result in Ω(n) recursion depth. Reed [63] observed that one can add an additional step
of breaking the whole subgraph in a balanced way, which reduces the recursion depth to logarithmic and results in improving the running time dependence on the input size from O(n2) to
O(n logn), at the cost of solving a more general (and usually more difficult) subproblem concerning balanced separators. Fortunately, our new approximation algorithm for balanced separators is
flexible enough to solve this more general problem as well, so we arrive at O(n logn) running time
dependence on n.
1.4 Related Work on Matrix Computations
Solving systems of linear equations and computing the determinant and rank of a matrix are ubiquitous, thoroughly explored topics in computer science, with a variety of well-known applications.
Since sparse matrices often arise both in theory and in practice, the possibility (and often necessity)
of exploiting their sparsity has been deeply studied as well. Here, we consider matrices as sparse
when their non-zero entries are not only few (i.e., o(n2)), but furthermore they are structured in a
way that could potentially be exploited using graph-theoretical techniques. The two best known
classical approaches in this direction are standard Gaussian elimination with a perfect elimination
ordering, and nested dissection.
A common assumption in both approaches is that throughout the execution of an algorithm,
no accidental cancellation occurs—that is, except for situations guaranteed and required by the
algorithm, arithmetic operations never change a non-zero value to a zero. This can be assumed in
some settings, such as when the input matrix is positive definite. Otherwise, as soon as accidental
zeros occur (e.g., simply starting with a zero diagonal), some circumvention is required by finding
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018. 
Fully Polynomial-Time Parameterized Computations for Graphs 34:7
a different pivot than originally planned. In practice, especially when working over real-valued
matrices, one may expect this not to extend resource usage too much, but when working over finite
fields it is clear that this assumption cannot be used to justify any resource bounds. Surprisingly,
we are not aware of any work bounding the worst-case running time of an algorithm for the
determinant of a matrix of small treewidth (or pathwidth) without this assumption. This may in
part be explained by the fact that a better understanding of sparseness in graph theory and the rise
of treewidth in popularity came after the classical work on sparse matrices, and by the reliance on
heuristics in practice.
Perfect elimination ordering, generally speaking, refers to an ordering of rows and columns of
a matrix such that Gaussian elimination introduces no fill-in—entries in the matrix where a zero
entry becomes non-zero. A seminal result of Parter [58] and Rose [65] says that such an ordering
exists if and only if the (symmetric) graph of the matrix is chordal (or triangulated, i.e., every cycle
with more than three edges has a chord). This assumes no accidental cancellation occurs. Thus, if
one wanted to minimize space usage, one would search for a minimum completion to a chordal
graph (this is the classical Minimum Fill-in problem), while to put a guarantee on the time spent
on eliminating, one could demand a chordal completion with small cliques (a.k.a. small frontsize),
which is equivalent to the graph of the original matrix having small treewidth [11]. Radhakrishnan
et al. [62] use this approach to give an O(k2
n) algorithm for solving systems of linear equations
defined by matrices of treewidth k, assuming no accidental cancellation.
To lift this assumption one has to consider arbitrary pivoting and the bipartite graph of a matrix
instead (with separate vertices for each row and each column), which also allows the study of
non-symmetric, non-square matrices. A Γ-free ordering is an ordering of rows and columns of a
matrix such that no  
 0

submatrix occurs—it can be seen that such an ordering allows one to
perform Gaussian elimination with no fill-in ( represents a non-zero entry). This corresponds to
a strong ordering of the bipartite graph GM of the matrix—an ordering  of vertices such that for
all vertices i, j, k, , if i  , j  k, and ji, ki, j are edges, then k must be an edge too. A bipartite
graph is known to be chordal bipartite graph (defined as a bipartite graph with no chordless cycles
strictly longer than four—note it need not be chordal) if and only if it admits a strong ordering (see,
e.g., [26]). Golumbic and Goss [38] first related chordal bipartite graphs to Gaussian elimination
with no fill-in, but assuming no accidental cancellation. Bakonyi and Bono [6] showed that when
an accidental cancellation occurs and a pivot cannot be used, a different pivot can always be found.
However, they do not consider the running time needed for finding the pivot, nor the number of
arithmetic operations performed.
Nested dissection is a Divide & Conquer approach introduced by Lipton et al. [50] to solve a
system of linear equations whose matrix is symmetric positive definite and whose graph admits a
certain separator structure. Intuitively, a weak separator tree for a graph gives a small separator of
the graph that partitions its vertices into two balanced parts, which after removing the separator,
are recursively partitioned in the same way. In work related to nested dissection, a small separator
means one of size O(nγ ), where n is the number of remaining vertices of the graph and γ < 1
is a constant (γ = 1
2 for planar and H-minor-free graphs). Thus, an algorithm needs to handle a
logarithmic number of separators whose total size is a geometric series bounded again by O(nγ ). In
modern graph-theoretic language, this most closely corresponds to a (balanced, binary) tree-depth
decomposition of depth O(nγ ).
To use nested dissection for matrices A that are not positive definite (so without assuming no
accidental cancellation), Mucha and Sankowski [56] used it on AAT instead, carefully recovering
some properties ofA afterward. In order to guarantee a good separator structure forAAT , however,
they first need to decrease the degree of the graph of A by an approach called vertex splitting,
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.            
34:8 F. V. Fomin et al.
introduced by Wilson [74]. Vertex splitting is the operation of replacing a vertex v with a path on
three vertices v
,w,v and replacing each incident edge uv with either uv or uv. It is easy to see
that this operation preserves the number of perfect matchings, for example. The operation applied
to the graph of a matrix can in fact be performed on the matrix, preserving its determinant too. By
repeatedly splitting a vertex, we can transform it, together with incident edges, into a tree of degree
bounded by 3. Choosing an appropriate partition of the incident edges, the structure of the graph
can be preserved; for example, the knowledge of a planar embedding can be used to stay in the
class of planar graphs. This allowed Mucha and Sankowski [56] to find maximum matchings via
Gaussian elimination in planar graphs in O(nω/2) time, where ω < 2.38 is the exponent of the best
known matrix multiplication algorithm. Yuster and Zwick [77] showed that the weak separator
tree structure can be preserved too, which allowed them to extend this result to H-minor-free
graphs.
These methods were further extended by Alon and Yuster [3] to use vertex splitting and nested
dissection on AAT for solving arbitrary systems of linear equations over any field, for matrices
whose graphs admit a weak separator tree structure. If the separators are of size O(nβ ) and can
be efficiently found, the algorithm works in O(nω β ) time. However, it is randomized and very involved, in particular requiring arithmetic computations in field extensions of polynomial size. A
careful translation of their proofs to tree decompositions could only give an O(k5 · n log3 n) randomized algorithm for matrices of treewidth k (i.e., O(n
k2) [62] where n = nk and k = k2 logn
after vertex splitting, with the recursion in [3] giving an additional logn factor).
Our approach differs in that for matrices with path or tree-partition decompositions of small
width we show that a strong ordering respecting the decomposition can be easily found, and standard Gaussian elimination is enough, as long as the ordering is properly used when pivoting. For
matrices with small treewidth, this does not seem possible (an apparent obstacle here is that not
all chordal graphs have strong orderings). However, a variant of the vertex splitting technique
guided with a tree-decomposition allows us to simply (in particular, deterministically) reduce to
the tree-partition case (instead of considering AAT ).
1.5 Related Work on Maximum Matchings
The existence of a perfect matching in a graph can be tested by calculating the determinant of
the Tutte matrix [70]. Lovász [51] showed that the size of a maximum matching can be found
by computing the rank, while Mucha and Sankowski [55, 56] gave a randomized algorithm for
extracting a maximum matching: in O(nω ) time for general graphs and O(nω/2) for planar graphs.
The results on general graphs were later simplified by Harvey [42]. Before that, Edmonds [27]
gave the first polynomial-time algorithm, then bested by combinatorial algorithms of Micali and
Vazirani [61, 72], Blum [7], and Gabow and Tarjan [35], each running in O(m√
n) time. Recently,
Mądry [53] gave an O(m10/7) algorithm for the unweighted bipartite case, then generalized to
the weighted bipartite case by Cohen et al. [19]. For graphs of treewidth k, simply because their
number of edges ism = O(kn), the above gives O(kn1.5) and O(k1.42n1.42) algorithms, respectively.
1.6 Related Work on Maximum Flows
The maximum flow problem is a classic subject with a long and rich literature. Starting with the
first algorithm of Ford and Fulkerson [34], which works in time O(F · (n + m)) for integer capacities, where F is the maximum size of the flow, a long chain of improvements and generalizations was proposed throughout the years (see, e.g., [19, 25, 28, 29, 37, 45, 53, 54, 57]). The running
times of these algorithms vary depending on the variants they solve, but all of them are far larger
than linear. In particular, the fastest known algorithm in the directed unit-weight setting, which
is the case considered in this work, is due to Mądry [53, 54] and works in time O(m10/7). For this
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
Fully Polynomial-Time Parameterized Computations for Graphs 34:9
reason, recently there was a line of work on finding near-linear (1 + ε)-approximation algorithms
for the maximum flow problem [18, 46, 48, 59, 66], culminating in a (1 + ε)-approximation algorithm working in undirected graphs in time O(ε−2 · m log11 n), proposed by Peng [59].
1.7 Outline
In Section 2, we establish notation and recall basic facts about matrices, flows, and tree-like decompositions of graphs. Section 3 is devoted to the approximation algorithm for treewidth, that is,
Theorem 1.1. In Section 4, we give our results for problems on matrices of low width, and in particular we prove Theorems 1.2 and 1.3. In Section 5, we apply these results to the maximum matching
problem, proving Theorems 1.4 and 1.5. Section 6 is focused on the maximum vertex flow problem
and contains a proof of Theorem 1.6. Finally, in Section 7 we gather short concluding remarks and
state a number of open problems stemming from our work.
2 PRELIMINARIES
2.1 Notation
We use standard graph notation (cf. [22]). All the graphs considered in this article are simple,
that is, they have no loops or multiple edges connecting the same endpoints. For a graph G and
X ⊆ V (G), by NG [X] we denote the closed neighborhood of X, that is, all the vertices that are either
in X or are adjacent to vertices of X, and by NG (X) = NG [X] \ X we denote the open neighborhood
of X. When G is clear from the context, we drop the subscript. For a path P, the internal vertices
of P are all the vertices of P apart from the endpoints. Paths P and Q are internally vertex-disjoint
if no vertex of P is an internal vertex of Q and vice versa. The set of connected components of a
graph G is denoted by cc(G).
ByG[X] we denote the subgraph ofG induced by X, and we defineG − X = G[V (G) \ X]. Graph
H is a subgraph of G, denoted H ⊆ G, if V (H) ⊆ V (G) and E(H) ⊆ E(G). We say H is a completion
of G if V (H) = V (G) and E(H) ⊇ E(G).
A 1-subdivision of a graph G is obtained from G by taking every edge uv ∈ E(G), and replacing
it with a new vertex wuv and edges uwuv and wuvv.
2.2 Matrices
For an n × m matrix M, the entry at the intersection of the r-th row and c-th column is denoted
as M[r,c]. For sets X ⊆ {1,...,n} and Y ⊆ {1,...,m}, by [M]X,Y we denote the |X |×|Y | matrix
formed by the entries of M appearing on the intersections of rows of X and columns of Y.
The symmetric graph of an n × n matrix (i.e., square, but not necessarily symmetric) is the
undirected graph with vertices {1,...,n} and an edge between i and j whenever M[i, j]  0
or M[j,i]  0 (loops are allowed). The bipartite graph of an n × m matrix is a bipartite, undirected graph with vertices in {r1,...,rn }∪{c1,...,cm } and an edge between ri and cj whenever
M[i, j]  0.
In this work, for describing the structure of a matrix M, we use the bipartite graph exclusively
and denote itGM , as it allows us to express our results for arbitrary (not necessarily square) matrices. Note that any tree decomposition of the symmetric graph of a square matrix M can be turned
into a decomposition ofGM of twice the width plus 1, by putting both the i-th row and i-th column
in the same bag where index i was.
A matrix is in (non-reduced) row-echelon form if all zero rows (with only zero entries) are below
all non-zero rows, and the leftmost non-zero coefficient of each row is strictly to the right of
the leftmost non-zero coefficients of rows above it. In particular, there are no non-zero entries
below the diagonal. We define column-echelon form analogously. A permutation matrix is a square
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.   
34:10 F. V. Fomin et al.
{0, 1}-matrix with exactly one entry equal to 1 in each row and each column. A PLUQ-factorization
of an n × m matrix M (also known as an LU-factorization with full pivoting) is a quadruple of
matrices where P is a permutation n × n matrix, L is an n × n matrix in column-echelon form
with ones on the diagonal, U is an n × m matrix in row-echelon form, Q is a permutation m × m
matrix, and M = PLUQ. A generalized LU -factorization of M is a sequence of matrices such that
their product is M and each is either a permutation matrix or a matrix in row- or column-echelon
form.
2.3 Flows and Cuts
For a graph G and disjoint subsets of vertices S,T ⊆ V (G), an (S,T )-path is a path in G that starts
in a vertex of S, ends in a vertex of T , and whose internal vertices do not belong to S ∪T . In
this article, an (S,T )-vertex flow is a family of (S,T )-paths F = {P1, P2,..., Pk } that are internally
vertex-disjoint; note that we do allow the paths to share endpoints in S or T . The size of a flow
F , denoted |F |, is the number of paths in it. A subset X ⊆ V (G) \ (S ∪T ) is an (S,T )-vertex cut
if no vertex of T is reachable by a path from some vertex of S in the graph G − X. A variant of
the well-known Menger’s theorem states that the maximum size of an (S,T )-vertex flow is always
equal to the minimum size of an (S,T )-vertex cut, provided there is no edge between S and T .
In case G is directed, instead of undirected paths, we consider directed paths starting from S and
ending in T , and the same statement of Menger’s theorem holds (the last condition translates to
the nonexistence of edges from S to T ). Note that in this definition we are only interested in flows
and cuts in unweighted graphs, or in other words, we put unit capacities on all the vertices.
There is a wide variety of algorithms for computing the maximum vertex flows and minimum
vertex cuts in undirected/directed graphs in polynomial time. Among them, the most basic is the
classic algorithm of Ford and Fulkerson, which uses the technique of finding consecutive augmentations of an (S,T )-vertex flow, up to the moment when a maximum flow is found. More precisely,
the following well-known result is used.
Theorem 2.1 (Max-flow Augmentation). There exists an algorithm that, given a directed graph
G on n vertices and m edges, disjoint subsets S,T ⊆ V (G) with no edge from S to T , and some (S,T )-
vertex flow F , works in O(n + m) time and either certifies that F is maximum by providing an
(S,T )-vertex cut of size |F |, or finds an (S,T )-vertex flow F  with |F 
| = |F | + 1.
The classic proof of Theorem 2.1 works as follows: the algorithm first constructs the residual
network that encodes where more flow could be pushed. Then, using a single BFS it looks for an
augmenting path. If such an augmenting path can be found, then it can be used to modify the flow
so that its size increases by one. On the other hand, the nonexistence of such a path uncovers an
(S,T )-vertex cut of size |F |. Of course, the analog of Theorem 2.1 for undirected graphs follows
by turning an undirected graph into a directed one by replacing every edge uv with arcs (u,v) and
(v,u).
The next well-known corollary follows by applying the algorithm of Theorem 2.1 at most k +
1 times.
Corollary 2.2. There exists an algorithm that, given an undirected/directed graphG on n vertices
and m edges, disjoint subsets S,T ⊆ V (G) with no edge from S to T , and a positive integer k, works in
time O(k · (n + m)) and provides one of the following outcomes:
(a) a maximum (S,T )-vertex flow of size  together with a minimum (S,T )-vertex cut size , for
some  ≤ k; or
(b) a correct conclusion that the size of the maximum (S,T )-vertex flow (equivalently, of the
minimum (S,T )-vertex cut) is larger than k.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.   
Fully Polynomial-Time Parameterized Computations for Graphs 34:11
2.4 Tree Decompositions
We now recall the main concepts of graph decompositions used in this article. First, we recall
standard tree and path decompositions.
Definition 2.3. A tree decomposition of a graphG is a pair (T , {Bx }x ∈V (T ) ), where T is a tree and
each node x of T is associated with a subset of vertices Bx ⊆ V (G), called the bag at x. Moreover,
the following conditions have to be satisfied:
—For each edge uv ∈ E(G), there is some x ∈ V (T ) such that {u,v} ⊆ Bx .
—For each vertex u ∈ V (G), define T [u] to be the subgraph of T induced by nodes whose
bags contain u. Then T [u] is a non-empty and connected subtree of T .
The width of T is equal to maxx ∈V (T ) |Bx | − 1, and the treewidth of G, denoted tw(G), is the minimum possible width of a tree decomposition of G. In case T is a path, we call T also a path
decomposition of G. The pathwidth of G, denoted pw(G), is the minimum possible width of a tree
decomposition of G.
We follow the convention that whenever (T , {Bx }x ∈V (T ) ) is a tree decomposition of G, then
elements of V (T ) are called nodes, whereas elements of V (G) are called vertices. Moreover, we
often identify the nodes of T with bags associated with them, and hence we can talk about adjacent
bags, and so forth. Also, we often refer to the tree T only as a tree decomposition, thus making
the associated family {Bx }x ∈V (T ) of bags implicit.
Throughout this article, we assume that all tree or path decompositions of widthk given on input
have O(|V (G)|) nodes. In fact, we will assume that the input decompositions are clean, defined as
follows.
Definition 2.4. A tree decomposition (T , {Bx }x ∈V (T )) ofG is called clean if for every xy ∈ V (T ),
it holds that Bx  By and By  Bx .
It is known that a clean tree decomposition of a graph on n vertices has at most n nodes, and
that any tree (path) decomposition T of width k can be transformed in time O(k|V (T )|) to a
clean tree (path) decomposition of the same width (see, e.g., [32, Lemma 11.9]). Thus, the input
decomposition can be always made clean in time linear in its size.
Finally, we recall the definition of another width parameter we use (see, e.g., [75]).
Definition 2.5. A tree-partition decomposition of a graph G is a pair (T , {Bx }x ∈V (T ) ), where T is
a tree and each node x of T is associated with a subset of vertices Bx ⊆ V (G), called the bag at x.
Moreover, the following conditions have to be satisfied:
—The sets {Bx }x ∈V (T ) form a partition of V (G), and in particular are pairwise disjoint.
—For each edge uv ∈ E(G), either there is some x ∈ V (T ) such that {u,v} ⊆ Bx , or there is
some xy ∈ E(T ) such that u ∈ Bx and v ∈ By .
The width of T is equal to maxx ∈V (T ) |Bx |, and the tree-partition width of G, denoted tpw(G), is
the minimum possible width of a tree-partition decomposition of G.
It is easy to see that empty bags in a tree-partition decomposition can be disposed of, implying |V (T )|≤|V (G)| without loss of generality. For all G, 1 + tw(G) ≤ 2 · tpw(G), but tpw(G)
can be arbitrarily large already for graphs of constant treewidth, unless the maximum degree is
bounded [75]. We also need the following well-known fact.
Lemma 2.6 (cf. Exercise 7.15 in [22]). A graph on n vertices of treewidth k has at most kn edges.
Hence, a graph on n vertices of tree-partition width k has at most 2kn edges.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.  
34:12 F. V. Fomin et al.
2.5 Measures and Balanced Separators
In several parts of the article, we will be introducing auxiliary weight functions on the vertices of
graphs, which we call measures.
Definition 2.7. Let G be a graph. Any function μ : V (G) → R+ ∪ {0} that is positive on at least
one vertex is called a measure on V (G). For a subset A ⊆ V (G), we denote μ(A) = 
u ∈A μ(u).
First, we need the following simple folklore lemma about the existence of balanced nodes of
trees.
Lemma 2.8. Let T be a tree on q nodes, with a measure μ given on V (T ). Then a node x ∈ V (T )
such that μ(V (C)) ≤ μ(V (T ))/2 for every C ∈ cc(T − x) can be found in time O(q).
Proof. Consider any edge yz ∈ E(T ), and let the removal of yz split T into subtrees Ty and
Tz , where y ∈ V (Ty ) and z ∈ V (Tz ). Orient yz from y to z if μ(V (Ty )) < μ(V (Tz )), from z to y
if μ(V (Ty )) > μ(V (Tz )), and arbitrarily if μ(V (Ty )) = μ(V (Tz )). The obtained oriented tree has q
nodes and q − 1 directed edges, which means that there is a node x that has outdegree 0. For every
neighbor y of x we have that the edge xy was directed toward x. This means that μ(V (Tx )) ≥
μ(V (Ty )); equivalently, μ(V (Ty )) ≤ μ(V (T ))/2. As y was an arbitrarily chosen neighbor of x, it
follows that x satisfies the required property.
As for the algorithmic claim, given the value of μ(x) for each x ∈ V (T ), it is easy to implement
the procedure orienting the edges in time O(q) by using a recursive depth-first search procedure on
T that returns the total weight of nodes in the explored subtree. Having the orientation computed,
suitable x can be retrieved by a simple indegree count in time O(q).
In graphs of bounded treewidth, Lemma 2.8 can be generalized to find balanced bags instead of
balanced nodes.
Definition 2.9. Let G be a graph, let μ be a measure on V (G), and let α ∈ [0, 1]. A set X ⊆ V (G)
is called an α-balanced separator w.r.t. μ if for each C ∈ cc(G − X), it holds that μ(V (C)) ≤ α ·
μ(V (G)).
Lemma 2.10 (Lemma 7.19 of [22]). Let G be a graph with tw(G) < k, and let μ be a measure on
V (G). Then there exists a 1
2 -balanced separator X w.r.t. μ with |X | ≤ k.
3 APPROXIMATING TREEWIDTH
In this section, we show our approximation algorithm for treewidth, that is, prove Theorem 1.1.
For the reader’s convenience, we restate it here.
Theorem 1.1. There exists an algorithm that, given a graphG on n vertices and a positive integer k,
in time O(k7 · n logn) either provides a tree decomposition of G of width at most O(k2), or correctly
concludes that tw(G) ≥ k.
In our proof of Theorem 1.1, we obtain an upper bound of 1, 800k2 on the width of the computed
tree decomposition. While this number could of course be improved by a more careful analysis of
different parameters used throughout the algorithm, we refrain from performing a tighter analysis
in order to simplify the presentation.
We first prove the backbone technical result, that is, an approximation algorithm for finding
balanced separators. This algorithm will be used as a subroutine in every step of the algorithm of
Theorem 1.1. Our approach for approximating balanced separators is based on the work of Feige
and Mahdian in [31].
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018. 
Fully Polynomial-Time Parameterized Computations for Graphs 34:13
Lemma 3.1. There exists an algorithm that, given a graph G on n vertices and m edges with a
measure μ on V (G), and a positive integer k, works in time O(k4 · (n + m)) and returns one of the
following outcomes:
(1) A 7
8 -balanced separator Y w.r.t. μ with |Y | ≤ 100k2.
(2) A (1 − 1
100k )-balanced separator X w.r.t. μ with |X | ≤ k.
(3) A correct conclusion that tw(G) ≥ k.
Proof. By rescaling μ if necessary, we assume that μ(V (G)) = 1. Throughout the proof, we
assume that tw(G) < k and hence, by Lemma 2.10, there exists some 1
2 -balanced separatorW w.r.t.
μ, which is of course unknown to the algorithm. We will prove that whenever such a W exists,
the algorithm reaches one of the outcomes (1) or (2). If none of these outcomes is reached, then no
such W exists and, by Lemma 2.10, the algorithm can safely report that tw(G) ≥ k, that is, reach
outcome (3). We also assume that for every vertex u ∈ V (G) it holds that μ(u) < 1
100k , because
otherwise we can immediately provide outcome (2) by setting X = {u}. Note that, in particular,
this implies that μ(W ) < 1
100 .
We first generalize the problem slightly. Suppose that for some i ≤ k we are given a set Yi such
that the following invariants are satisfied: (i) |Yi | ≤ 100ik and (ii) |W ∩ Yi | ≥ i. Then, the claim is
as follows.
Claim 3.2. GivenYi satisfying invariants (i) and (ii) for some i < k, one can in time O(k3 · (n + m))
either arrive at one of the outcomes (1) or (2), or find a set Z with |Z | ≤ 100k and Z ∩ Yi = ∅, such
that Z ∩W  ∅.
Before we proceed to the proof of Claim 3.2, we observe how Lemma 3.1 follows from it. We
start with Y0 = ∅, which clearly satisfies the invariants (i) and (ii). Then we iteratively compute
Y1,Y2,Y3,... as follows: when computing Yi+1, we use the algorithm of Claim 3.2 to either provide
outcome (1) or (2), in which case we terminate the whole computation, or find a suitable set Z.
Then Yi+1 = Yi ∪ Z satisfies the invariants (i) and (ii) for the next iteration, and hence we can
proceed. Suppose that this algorithm successfully performed k iterations, that is, it constructed
Yk . Then we have that |W ∩ Yk | ≥ k, so since |W | ≤ k, we have W ⊆ Yk . Then Yk should be a 1
2 -
balanced separator w.r.t. μ and |Yk | ≤ 100k2, so it can be reported as Y in outcome (1). If Yk is not
a 1
2 -balanced separator, then W did not exist in the first place and the algorithm can safely report
outcome (3). Since the algorithm of Claim 3.2 works in time O(k3 · (n + m)) and we apply it at
most k times, the running time promised in the lemma statement follows.
We now proceed to the proof of Claim 3.2. LetG = G − Yi . First, let us investigate the connected
components of G
. If μ(V (C)) ≤ 7
8 for each C ∈ cc(G
), then we can reach outcome (1) by taking
Y = Yi , because |Yi | ≤ 100ik ≤ 100k2. Hence, suppose there is a connected componentC0 ∈ cc(G
)
with μ(C0) > 7
8 . Let T0 be an arbitrary spanning tree of C0, and let V0 = V (C0). We first prove an
auxiliary claim that will imply that we can find a nice partitioning of T0. This is almost exactly the
notion of Steiner decompositions used by Feige and Mahdian [31] (see Definition 5.1 and Lemma 5.2
in [31]), but we choose to reprove the result for the sake of completeness.
Claim 3.3. Suppose we are given a number λ ∈ R+ and a tree T on n vertices with a measure
μ on V (T ), such that μ(u) < λ for each u ∈ V (T ). Then one can in time O(n) find a family F =
{(R1,u1), (R2,u2),..., (Rp,up )} (with ui ’s not necessarily distinct) such that the following holds (in
the following, we denote R˜
i = Ri \ {ui}):
(a) for each i = 1, 2,...,p, we have that ui ∈ Ri ⊆ V (T ), T [Ri] is connected, and λ ≤ μ(R˜
i ) <
4λ;
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018. 
34:14 F. V. Fomin et al.
(b) μ(V (T ) \
p
i=1 R˜
i ) < 2λ;
(c) sets R˜
i are pairwise disjoint for i = 1, 2,...,p.
Proof. We first provide a combinatorial proof of the existence of F , which proceeds by induction on n. Then we will argue how the proof gives rise to an algorithm constructing F in linear
time.
Let us rootT in an arbitrary vertex r, which imposes a parent-child relation on the vertices ofT .
Forv ∈ V (T ), letTv be the subtree rooted atv. If μ(V (T )) < 2λ, then we can take F = ∅, so suppose
otherwise. Let then u be the deepest vertex of T for which μ(V (Tu )) ≥ 2λ, and let u1,u2,...,uq be
the children ofu. Let us denote aj = μ(V (Tuj )), forj = 1, 2,...,q. Asu was chosen to be the deepest,
we have that aj < 2λ for each j = 1, 2,...,q. Moreover, since μ(u) < 2λ and μ(V (Tu )) ≥ 2λ, we
have that u has at least one child, that is, q > 0.
Scan the sequence a1, a2,..., aq from left to right, and during this scan iteratively extract minimal prefixes for which the sum of entries is at least λ, up to the point when the total sum of the
remaining numbers is smaller than λ. Let these extracted sequences be
{a1, a2,..., aj2−1}, {aj2 , aj2+1,..., aj3−1},..., {ajs , ajs+1,..., ajs+1−1},
where s is their number. Then, denoting aj1 = 1, we infer from the construction and the fact that
aj < 2λ for all j that the following assertions hold:
—λ ≤ j+1−1
i=j ai < 3λ for all  = 1, 2,...,s; and
—q
i=js+1 ai < λ.
Moreover, since μ(V (Tu )) ≥ 2λ, we have that q
i=1 ai ≥ 2λ − μ(u) > λ, and hence at least one sequence has been extracted; that is, s ≥ 1.
For  = 1,...,s − 1, let I = {j, j + 1,..., j+1 − 1}, and let Is = {js , js + 1,...,q}. Concluding,
from the assertions above it follows that we have partitioned {1,...,q} into contiguous sets of
indices I1, I2,..., Is such that s ≥ 1 and λ ≤ 
i ∈I ai < 4λ, for each  = 1, 2,...,s.
Let T  be T with all the subtrees Tuj removed, for j = 1, 2,...,q. Since q > 0, we have that
|V (T 
)| < |V (T )|, and hence by the induction hypothesis we can find a family F  = {(R1,u1),
(R2,u2),..., (Rp,up )} for the tree T  satisfying conditions (a), (b), and (c).
For  = p + 1,p + 2,...,p + s, let Rp+ = {u} ∪ 
i ∈I V (Tui ) and u = u. Observe that
μ(Rp+ \ {up+ }) = 
i ∈I ai , so we obtain that λ ≤ μ(R˜
p+ ) < 4λ. Consider F = F  ∪ {(Rp+1,
up+1), (Rp+2,up+2),..., (Rp+s ,up+s )}. It can be easily verified that conditions (a), (b), and (c) hold
for F using the induction assumption that they held for F 
. This concludes the inductive proof
of the existence of F .
Naively, the proof presented above gives rise to an O(n2) algorithm that iteratively finds a suitable vertex u, and applies itself recursively to T 
. However, it is very easy to see that the algorithm
can be implemented in time O(n) by processing T bottom-up, remembering the total measure of
the vertices in the processed subtree, and cutting new pairs (Ri,ui ) whenever the accumulated
measure exceeds 2λ.
Armed with Claim 3.3, we proceed with the proof of Claim 3.2. Apply the algorithm of Claim 3.3
to treeT0 and λ = 1
100k . Note that the premise of the claim holds by the assumption that μ(u) < 1
100k
for each u ∈ V (G). Therefore, we obtain a family F = {(R1,u1), (R2,u2),..., (Rp,up )} satisfying
conditions (a), (b), and (c) for T0. For i = 1, 2,...,p, let R˜
i = Ri \ {ui} and let Z = {u1,u2,...,up }.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.                   
Fully Polynomial-Time Parameterized Computations for Graphs 34:15
Note that by conditions (a) and (c) we have that
1 = μ(V (G)) ≥

p
i=1
μ(R˜
i ) ≥ pλ = p
100k
,
so |Z | ≤ p ≤ 100k.
We need the following known fact.
Claim 3.4 (cf. The Proof of Lemma 7.20 in [22]). Let b1,b2,...,bq be nonnegative reals such
that q
i=1 bi ≤ 1 and bi ≤ 1/2 for each i = 1, 2,...,q. Then {1,...,q} can be partitioned into two sets
J1 and J2 such that 
i ∈Jz bi ≤ 2
3 for each z ∈ {1, 2}.
Let B1, B2,..., Bq be the connected components of C0 −W , and let bi = μ(V (Bi )) for each i =
1, 2,...,q. Clearly, q
i=1 bi ≤ μ(V (G)) = 1. Moreover, each component Bi is contained in some
component of G −W , and hence, sinceW is a 1
2 -balanced separator of G w.r.t. measure μ, we have
that bi ≤ 1/2 for each i = 1, 2,...,q. By Claim 3.4 we can find a partition (J1, J2) of {1,...,q} such
that 
i ∈Jz bi ≤ 2
3 for each z ∈ {1, 2}. Let A1 = 
i ∈J1 V (Bi ) and A2 = 
i ∈J2 V (Bi ). Then vertex sets
A1 andA2 are non-adjacent inG and μ(A1), μ(A2) ≤ 2
3 . Recall that 7
8 < μ(V (C0)) = μ(A1) + μ(A2) +
μ(W ∩V (C0)). Since μ(W ) < 1
100 , we have that μ(A1) + μ(A2) > 7
8 − 1
100 > 5
6 . Hence, it follows that
μ(A1), μ(A2) > 5
6 − 2
3 = 1
6 .
Let K1 ⊆ {1,...,p} be the set of those indices i for which R˜
i ∩ A1  ∅. By properties (a), (b),
and (c), we obtain that
1
6
< μ(A1) ≤ μ

V (C0) \

p
i=1
R˜
i


+

i ∈K1
μ(R˜
i ) < 2λ + 4λ|K1 |.
Since λ = 1
100k , we have that
|K1 | >
1
24λ − 1
2
> 3k.
Since |W | ≤ k and sets R˜
i are pairwise disjoint, there is L1 ⊆ K1 of size at least 2k such that additionally R˜
i ∩W = ∅ for each i ∈ L1. Symmetrically, we prove that there is a set L2 ⊆ {1,...,p} of
at least 2k indices such that R˜
i ∩ A2  ∅ and R˜
i ∩W = ∅, for each i ∈ L2.
Suppose for a moment that Z ∩W = ∅. Then, for each i ∈ L1 we in fact have that Ri ∩W = ∅.
Since G[Ri] is connected, R˜
i ∩ A1  ∅, and there is no edge between A1 and A2, it follows that
Ri ⊆ A1. Similarly, Ri ⊆ A2 for each i ∈ L2.
Therefore, the algorithm does as follows. For each pair of distinct indices i, j ∈ {1,...,p} for
which Ri ∩ Rj = ∅, we verify whether the size of a minimum vertex cut in G between Ri and Rj
does not exceed k. If we find such a pair and the corresponding vertex cut X, then X separates Ri
from Rj , so X is a (1 − 1
100k )-balanced separator w.r.t. μ in G, due to μ(Ri ), μ(Rj) ≥ 1
100k . Therefore,
such X can be reported as outcome (2). The argumentation of the previous paragraph ensures us
that at least one such pair (i, j) will be found provided Z ∩W = ∅.
Hence, if for every such pair (i, j) the minimum vertex cut in G between Ri and Rj is larger than
k, then we have a guarantee that Z ∩W  ∅. Since |Z | ≤ 100k, we can provide the set Z as the
outcome of the algorithm of Claim 3.2. This concludes the description of the algorithm of Claim 3.2.
To bound its running time, observe that the application of the algorithm of Claim 3.3 takes time
O(n + m), whereas later we verify the minimum value of a vertex cut for at most p2 = O(k2) pairs
(i, j). By Corollary 2.2, each such verification can be implemented in time O(k · (n + m)). Hence,
the whole algorithm of Claim 3.2 indeed runs in time O(k3 · (n + m)). As argued before, Lemma 3.1
follows from Claim 3.2.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.       
34:16 F. V. Fomin et al.
Having an approximation algorithm for balanced separators, we can proceed with the proof of
Theorem 1.1. Following the approach introduced by Robertson and Seymour [64], we solve a more
general problem. Let η = 100k2. Assume we are given a graph H together with a subset S ⊆ V (H),
S  V (H), with the invariant that |S | ≤ 17η. The goal is to either conclude that tw(H) ≥ k, or to
compute a rooted tree decomposition of H, i.e., a tree decomposition rooted at some node r, that
has width at most 18η and where S is a subset of the root bag. Note that if we find an algorithm
with running time O(k7 · n logn) for the generalized problem, then Theorem 1.1 will follow by
applying it to G and S = ∅.
The algorithm for the more general problem works as follows. First, observe that we can assume
that |E(H)| ≤ kn, where we denote n = |V (H)|. Indeed, by Lemma 2.6 we can immediately answer
that tw(H) ≥ k if this assertion does not hold. Having this assumption, we consider three cases:
either (i) |V (H) \ S | ≤ η, or, if this case does not hold, (ii) |S | ≤ 16η, or (iii) 16η < |S | ≤ 17η.
Case (i): If |V (H) \ S | ≤ η, we can output a trivial tree decomposition with one bag containing
V (H), because the facts that |V (H) \ S | ≤ η and |S | ≤ 17η imply that |V (H)| ≤ 18η. In the other
cases, we assume that (i) does not hold, that is, |V (H) \ S | > η.
Case (ii): Suppose |S | ≤ 16η. Define a measure μS onV (H) as follows: μS (u) = 0 for eachu ∈ S, and
μS (u) = 1 for each u  S. Run the algorithm of Lemma 3.1 on H with measure μS . If it concluded
that tw(H) ≥ k, then we can terminate and pass this answer. Otherwise, let X be the obtained
subset of vertices. Regardless whether outcome (1) or (2) was given, we have that X is a (1 − 1
100k )-balanced separator in H w.r.t. measure μS , and moreover |X | ≤ η. Let B = S ∪ X and observe
that |B|≤|S | + |X | ≤ 17η. Consider the connected components of H − B. For each C ∈ cc(H − B),
define a new instance (HC, SC ) of the generalized problem as follows:HC = H[NH [V (C)]] and SC =
NH (V (C)). Observe that SC ⊆ B, hence |SC |≤|B| ≤ 17η, and thus the invariant that |SC | ≤ 17η is
satisfied in the instance (HC, SC ). Apply the algorithm recursively to (HC, SC ), yielding either a
conclusion that tw(HC ) ≥ k, in which case we can report that tw(H) ≥ k as well, or a rooted tree
decomposition TC of HC that has width at most 18η and where SC is a subset of the root bag.
Construct a tree decomposition T of H as follows: create a root node r associated with bag B, and,
for each C ∈ cc(H − B), attach the decomposition TC below r by making the root of TC a child of
r. It easy to verify that T created in this manner indeed is a tree decomposition of H, and its width
does not exceed 18η because |B| ≤ 17η ≤ 18η.
Case (iii): Suppose 16η < |S | ≤ 17η. Define a measure μS on V (H) as follows: μS (u) = 1 for each
u ∈ S, and μS (u) = 0 for each u  S. Run the algorithm of Lemma 3.1 on H with measure μS . Again,
if it concluded that tw(H) ≥ k, then we can terminate and pass this answer. Otherwise, we obtain
either a 7
8 -balanced separatorY with |Y | ≤ η, or a (1 − 1
100k )-balanced separatorX with |X | ≤ k. Let
Z be this separator, being either X or Y depending on the subcase. The algorithm proceeds as in the
previous case. Define B = S ∪ Z; then |B|≤|S | + |Z | ≤ 18η. For each C ∈ cc(H − B), define a new
instance (HC, SC ) of the generalized problem by taking (HC, SC ) = (H[NH [V (C)]], NH (V (C))). Apply the algorithm recursively to each (HC, SC ), yielding either a conclusion that tw(HC ) ≥ k, which
implies tw(H) ≥ k, or a tree decomposition TC of HC that has width at most 18η and SC is contained in its root bag. Construct the output decomposition T by taking B as the bag of the root
node r, and attaching all the decompositions TC below r by making their roots children ofr. Again,
it can be easily verified that T is a tree decomposition of H of width at most 18η. The only verification that was not performed is that in the new instances (HC, SC ), the invariant that |SC | ≤ 17η
still holds. We shall prove an even stronger fact: that |SC |≤|S | − k, for each C ∈ cc(H − B), so
the needed invariant will follow by |S | ≤ 17η. However, we will need this stronger property in the
future. The proof investigates the subcases Z = X and Z = Y separately.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.   
Fully Polynomial-Time Parameterized Computations for Graphs 34:17
Suppose first that Z = X, that is, the algorithm of Lemma 3.1 has found a (1 − 1
100k )-balanced
separator X with |X | ≤ k. Consider any connected component C ∈ cc(H − B), and let D be the
connected component of H − X in which C is contained. Since X is (1 − 1
100k )-balanced w.r.t. μS ,
we have that
|S ∩ D| = μS (D) ≤
	
1 − 1
100k


· μS (V (H)) =
	
1 − 1
100k


· |S | = |S | − |S |
100k < |S | − 16k;
the last inequality follows from the assumption that |S | > 16η = 1600k2. Now observe that
NH (C) ⊆ X ∪ (S ∩ D), so |NH (C)| < k + (|S | − 16k) = |S | − 15k < |S | − k.
Suppose second that Z = Y, that is, the algorithm of Lemma 3.1 found a 7
8 -balanced separator Y
with |Y | ≤ η. Again, consider any connected componentC ∈ cc(H − B), and let D be the connected
component of H − Y in which C is contained. Since Y is 7
8 -balanced w.r.t. μS , we have that
|S ∩ D| = μS (D) ≤
7
8 · μS (V (H)) = 7
8
|S | = |S | − |S |
8
< |S | − 2η;
the last inequality follows from the assumption that |S | > 16η. Again, observe that NH (C) ⊆ Y ∪
(S ∩ D), so |NH (C)| < η + (|S | − 2η) = |S | − η < |S | − k.
This concludes the description of the algorithm. Its partial correctness is clear: if the algorithm
concludes that tw(H) ≥ k, then it is always correct, and similarly any tree decomposition of H
output by the algorithm satisfies the specification. We are left with arguing that the algorithm
always stops (i.e., it does not loop in the recursion) and its running time is bounded by O(k7 ·
n logn). We argue both these properties simultaneously.
Let T be the recursion tree yielded by the algorithm. That is, T is a rooted tree with nodes
corresponding to recursive subcalls to the algorithm (in case the algorithm loops, T would be
infinite). Thus, each node is labeled by the instance (H
, S
) which is being solved in the subcall. The
root of T corresponds to the original instance (H, S), and the children of each node x correspond
to subcalls invoked in the call corresponding to x. Observe that if the algorithm returns some
decomposition T ofH, then T is isomorphic to T, because every subcall produces exactly one new
bag, and the bags are arranged into T exactly according to the recursion tree of the algorithm.
The leaves of T correspond to calls where no subcall was invoked: either ones conforming to
Case (i), or ones conforming to Case (ii) or (iii) when B = V (H). Partition the node set of T into
sets A(i), A(ii), A(iii), depending whether the corresponding subcall falls into Case (i), (ii), or (iii). For
each node x ∈ V (T), let (Hx , Sx ) be its corresponding subcall, let hx = |V (Hx ) \ Sx |, sx = |Sx |, and
nx = hx + sx = |V (Hx )|. By chld(x) we denote the set of children of x. The following claim shows
the crucial properties of T that follow from the algorithm.
Claim 3.5. The following holds:
(a) For each x ∈ A(ii) and each y ∈ chld(x), we have hy ≤ (1 − 1
100k ) · hx .
(b) For each x ∈ A(iii) and each y ∈ chld(x), we have sy ≤ sx − k.
Proof. Assertion (b) is exactly the stronger property |NH (C)|≤|S | − k that we have ensured
in Case (iii). Hence, it remains to prove assertion (a). Let X ⊆ V (Hx ) be the separator found by
the algorithm when investigating the subcall in node x. Suppose that child y corresponds to some
subcall (Hy, Sy ) = (HC, SC ) = (NHx [V (C)], NHx (V (C))), for some component C ∈ cc(Hx − (Sx ∪
X)). Let D be the connected component of H − X in which C is contained. Since X is (1 − 1
100k )-
balanced in Hx w.r.t. measure μS , we have that
hy = |C|≤|D \ S | = μS (D) ≤
	
1 − 1
100k


· μS (V (Hx )) =
	
1 − 1
100k


· hx .
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018. 
34:18 F. V. Fomin et al.
Using Claim 3.5, we can show an upper bound on the depth of T. This, in particular, proves that
the algorithm always stops (equivalently, T is finite).
Claim 3.6. The depth of T is bounded by O(k2 · logn).
Proof. Take any finite path P in T that starts in its root and travels down the tree. Since each
node of A(i) is a leaf in T, at most one node of P can belong to A(i). We now estimate how many
nodes of A(ii) and A(iii) can appear on P.
First, we claim that there are at most O(k · logn) nodes of A(ii) on P. Let x0, x1, x2,..., xp be
consecutive vertices of A(ii) on P, ordered from the root. By Claim 3.5(a), we have that hxi+1 ≤
(1 − 1
100k ) · hxi for each nonnegative integeri. Since hx0 ≤ n, by induction it follows that hxi ≤ (1 − 1
100k )
i · n. However, hx ≥ 1 for each x ∈ V (T), so p ≤ log1− 1
100k 1/n ≤ O(k · logn). Consequently,
|V (P) ∩ A(ii)| = p + 1 ≤ O(k · logn).
Second, we claim that on P there cannot be more than 100k consecutive vertices from A(iii).
Assume otherwise, that there exists a sequence x0, x1,..., x100k where xi ∈ V (P) ∩ A(iii) for each
0 ≤ i ≤ 100k and xi+1 ∈ chld(xi ) for each 0 ≤ i < 100k. By Claim 3.5(b), we have thatsxi+1 ≤ sxi −
k, for each 0 ≤ i < 100k. By induction, we obtain thatsxi ≤ sx0 − ik. However,sx0 ≤ 17η, so sx100k ≤
17η − 100k · k = 16η. This is a contradiction with x100k ∈ A(iii), because Case (ii) should apply to
instance (Hx100k , Sx100k ) instead.
Combining the observations of the previous paragraphs, we see that P has at most O(k · logn)
vertices ofA(ii), the segments of consecutive vertices of A(iii) have length at most 100k, and only the
last vertex can belong to A(i). It follows that |V (P)|≤O(k2 · logn). Since P was chosen arbitrarily,
we infer that the depth of T is bounded by O(k2 · logn).
By Claim 3.6, we already know that the algorithm stops and outputs some tree decomposition T .
As we noted before, T and T are isomorphic by a mapping that associates a bag of T with the
subcall in which it was constructed. By somewhat abusing the notation, we will identify T with
T in the sequel. It remains to argue that the running time of the algorithm is bounded by O(k7 ·
n logn).
We partition the total work used by the algorithm between the nodes of T . For x ∈ V (T ),
we assign to x the operations performed by the algorithm when constructing the bag Bx associated with x: running the algorithm of Lemma 3.1 to compute an appropriate balanced separator,
construction of the subcalls, and gluing the subdecompositions obtained from the subcalls below
the constructed bag Bx . From the description of the algorithm and Lemma 3.1 it readily follows
that the work associated with node x ∈ V (T ) is bounded by O(nx ) whenever x belongs to A(i),
and by O(k5 · nx ) whenever x belongs to A(ii) or A(iii). Note that here we use the assumption that
|E(Hx )| ≤ k · nx for each node x ∈ V (T ); we could assume this because otherwise the application
of the algorithm to instance (Hx , Sx ) would immediately reveal that tw(Hx ) ≥ k.
Take any node x ∈ A(i), and observe that, since Sx  V (Hx ), we have that Bx = V (Hx ) contains
some vertex of H which does not belong to any other bag of T : any vertex of V (Hx ) \ Sx has
this property. Therefore, the total number of nodes in A(i) is at most n. Since nx ≤ O(k2) for each
such x, we infer that the total work associated with bags of A(i) is bounded by O(k2 · n).
We now bound the work associated with the nodes of A(ii) ∪ A(iii). Let d be the depth of T ; by
Claim 3.6, we know that d ≤ O(k2 · logn). For 0 ≤ i ≤ d, let Li be the set of nodes of A(ii) ∪ A(iii)
that are at depth exactly i in T . Fix some i with 0 ≤ i ≤ d. Observe that among x ∈ Li , the sets
V (Hx ) \ Sx are pairwise disjoint. Hence, 
x ∈Li hx ≤ n. However, for every node x ∈ A(ii) ∪ A(iii) we
have that hx > η (or otherwise case (i) would apply in the subcall corresponding to x) and sx ≤ 17η.
This implies that nx = hx + sx ≤ 18hx , and hence 
x ∈Li nx ≤ 18n. Consequently, the total work
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.  
Fully Polynomial-Time Parameterized Computations for Graphs 34:19
associated with the nodes of Li is bounded by 
x ∈Li O(k5 · nx ) ≤ O(k5 · n). Since d = O(k2 logn),
we conclude that the total work associated with bags of A(ii) ∪ A(iii) is O(k7 · n logn).
Concluding, we have shown that the algorithm for the generalized problem is correct, always
terminates, and in total uses time O(k7 · n logn). Theorem 1.1 follows by applying it to H = G and
S = ∅.
4 GAUSSIAN ELIMINATION
First, in Section 4.1 we describe our Gaussian elimination algorithm guided by an ordering of
rows and columns of the matrix. Next, in Section 4.2 we show how suitable orderings of low width
can be recovered from low-width path and tree-partition decompositions of the bipartite graph
associated with the matrix. Finally, in Section 4.3, we present how the approach can be lifted to
tree decompositions using an adaptation of the vertex-splitting/matrix sparsification technique.
4.1 Gaussian Elimination with Strong Orderings
4.1.1 Algebraic Description. We first describe our algorithm purely algebraically, showing what
arithmetic operations will be performed and in which order. We will address implementation details later.
We assume that the algorithm is given an n × m matrix M over some field F, and moreover
there is an ordering  imposed on the rows and columns of M. We will never compare rows with
columns using , so actually we are only interested in the orders imposed by  on the rows and
on the columns of M. The following Algorithm 1 presents our procedure.
ALGORITHM 1: Gaussian elimination of (M, ) (see Figure 1).
Input: An n × m matrix M with an order  on rows and columns of M
Output: Matrices U , L, and removal orders of rows and columns of M
Set I , J to be the sets of all columns and all rows of M, respectively.
Set L to be the n × n identity matrix.
while I is not empty do
Let i be the earliest column in I (in the ordering );
if it is empty, remove i from I and choose i again.
Let j be the earliest row in J such that M[j,i]  0 (in the ordering ).
for every row k  j in J such that M[k,i]  0 do
/* Eliminate entry M[k, i] using row j. */
Set L[k, j] := M[k,i]
M[j,i] .
for every column  such that M[j, ]  0 do
Set M[k, ] := M[k, ] − L[k, j] · M[j, ]. /* Entry Update */
end
end
Remove j from J and i from I. /* After this, M[k, i] = 0 for all k ∈ J. */
end
Remove all remaining rows from J. /* Note these must be empty rows. */
Set U := M.
Return U , L, and orders in which the columns/rows were removed from I/J, respectively.
In Algorithm 1, we consider consecutive columns of M (in the order of ), and for each column
i we find the first row j (in the order of ) that has a non-zero element in column i. Row j is
then used to eliminate all the other non-zero entries in column i. Hence, whenever some column
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.                  
34:20 F. V. Fomin et al.
Fig. 1. The positions of non-zero entries (black circles) after a few steps of the algorithm. In the current step
the entry in column i, row k will be eliminated using the earlier row j—entries that will become non-zero are
marked as white circles.
i is removed, it has a zero intersection with all the rows apart from the said row j. Inductively, it
follows that in the Entry Update step, we have that j ∈ J with M[j, ]  0 imply that  ∈ I and
columns are never changed after removal from I. Clearly, rows are never changed after removal.
The algorithm also returns the removal orders of columns and rows of M, that is, the orders in
which they were removed from I and J, respectively. For I this order coincides with the order
, but for J it depends on the positioning of non-zero entries in the matrix, and can differ from
.
We now verify that the output of the algorithm forms a suitable factorization of M and can be
used for solving linear equations. In the following, we assume that L andU are given as sequences
of non-zero entries in consecutive rows (in any order), so their description takes O(N) indices and
field values, where N is the number of non-zero entries in L and U .
Lemma 4.1. Let M be an n × m matrix over a field F and  be any ordering of its rows and
columns. Within the same time bounds as needed for executing Algorithm 1, we can compute a PLUQfactorization of M, and hence the rank, determinant (if n = m), and a maximal nonsingular submatrix
of M. Furthermore, for any r ∈ Fn, the system of linear equations Mx = r can be solved in O(N) additional time and field operations (either outputting an example solution x ∈ Fm or concluding that
none exists), where N is the number of non-zero entries in L and U .
Proof. Let U, L be the matrices output by the algorithm. The sequence of row operations performed in the algorithm implies that each row U [k, ·] of the final matrix is obtained from the row
M[k, ·] of the original matrix by adding row U [j, ·] with multiplier −L[k, j], for each j removed
earlier (note that rows j removed later are never added and have L[k, j] = 0). That is, for each k
we have U [k, ·] = M[k, ·] − 
jk L[k, j] · U [j, ·], from which it immediately follows that M = LU
(recall that L was initialized as an identity matrix).
Let P,Q be n × n and m × m permutation matrices, respectively, such that U  = PUQ is the matrix U with rows and columns reordered in the removal order output by the algorithm. Then
L = PLP−1 is the matrix L reordered so that its rows and its columns both correspond to the rows
of M in removal order. We claim thatU 
, LT are in row-echelon form, and hence M = P−1L
U 
Q−1
gives the desired factorization.
For matrix L
, this follows directly from the fact that L has ones on the diagonal, which are
preserved by the permutation P of both rows and columns, and L is lower-triangular, because
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.       
Fully Polynomial-Time Parameterized Computations for Graphs 34:21
L[k, j] can be non-zero only if row k is removed after row j of M in the algorithm. Hence, LT is in
row-echelon form.
For matrix U 
, observe that, when a row j is removed in the algorithm, it is removed together
with a column i that has a non-zero intersection with j, and all rows below, that is, all rows that
remain to be removed and are currently in J, have a zero intersection with i. Hence, also any
column i to the left of i has a zero intersection with row j, since j was removed strictly after
removing i
. This means that U 
[j,i] is the first non-zero element of row j and all elements below
it are zero. Hence, U  is in row-echelon form.
It follows that since L has only ones on the diagonal and since U  is in row-echelon form,
the determinant of M (in case n = m) is equal to the product of diagonal values of U  multiplied
by the signs of the permutations P and Q. Similarly, P,Q, L are of full rank, so the rank of M is
the number of non-zero rows in U 
. A maximum nonsingular submatrix of U  is given by the
non-empty rows and the columns containing the leftmost non-zero element of each such row—
the same submatrix is maximal nonsingular for L
U 
. As L is lower-triangular with ones on the
diagonal, the corresponding positions in M = P−1L
U 
Q−1 give a maximal nonsingular submatrix
of M. Clearly, all of the above can be computed within the same time bounds.
Given r, to solve Mx = P−1L
U 
Q−1
x = r it is enough to solve Q−1
y = r trivially to get y, then
U 
z = y to get z and similarly with L and P−1 to get x. The system U 
z = y (for U  in row-echelon
form, analogously for L
) can be easily solved by back-substitution. That is, start from zi = 0 for
every column i of U 
. If for any empty row j of U 
, yj is non-zero, output that the system has no
solutions. For every non-empty row j of U  from the lowest to the highest (so in order opposite
to the removal order, with shortest rows first), let U 
[j,i] be the first non-zero entry of the row j
and set zi := yj−

i U
[j, ]·z
U[j,i] . A standard check shows that U 
z = y. Exactly one multiplication or
division and one addition or subtraction is done for every non-zero element of U  and L
.
4.1.2 Using Strong Orderings of Small Width. We now introduce width parameters for the ordering , and show how to bound the number of fill-in entries using these parameters.
Definition 4.2. LetH be a graph. AnH-ordering is an ordering ofV (H). AnH-ordering  isstrong
if for every i, j, k ∈ V (H) with ij,ik ∈ E(H) and j  k, any neighbor of j that comes afteri in the ordering is also a neighbor of k; in other words, { ∈ NH (j) | i  }⊆{ ∈ NH (k) | i  }. The width
of an H-ordering  is defined as maxij ∈E(H ) min (|{ ∈ NH (j) | i  }|, |{ ∈ NH (i) | j  }|). The
degeneracy of an H-ordering  is defined as maxi ∈V (H ) |{ ∈ NH (i) | i  }|.
Note that for any H-ordering , the width of  is upper-bounded by its degeneracy d, as for
each ij ∈ E(H), without loss of generality i  j, we have that
min 	
|{ ∈ NH (j) | i  }|,|{ ∈ NH (i) | j  }|

≤|{ ∈ NH (i) | j  }| ≤ |{ ∈ NH (i) | i  }| ≤ d.
If H is bipartite, then it can be observed that the definitions of a strong ordering and width do not
depend on the relative ordering in  of vertices on different sides of the bipartition.
Lemma 4.3. Let M be a matrix and let  be a strong H-ordering of width b, for some completion
H of GM . Consider how the matrix M is modified throughout Algorithm 1 applied on M with order
. Then the following invariant is maintained: throughout the algorithm, H is always a completion
of GM . Furthermore, the output matrix L has at most |E(H)| + |V (H)| non-zero entries.
Proof. Zero entries of M can become non-zero only in the Entry Update step of Algorithm 1.
Specifically, an entry M[k, ] can become non-zero when M[j,i], M[k,i], M[j, ] were already nonzero, i   (by choice of i), and j  k (by choice of j), for certain rows and columnsi, j, k, . Before
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.                                                 
34:22 F. V. Fomin et al.
this step, by inductive assumption the invariant held and thus ij,ik ∈ E(H) and  is a neighbor of
j in H. Therefore, the strongness of the H-ordering  implies that  is also a neighbor of k in H,
that is, k ∈ E(H). Thus, the invariant is maintained after each step.
To bound the number of non-zero entries of L, observe that every non-diagonal entry L[k, j] for
rows k, j of M is filled only when there is an edge ijk ∈ E(H), where ij is the column together with
which j was removed. Hence, the number of non-zero entries in each column j of L is bounded by
1 plus the number of rows adjacent to ij in H (or just 1 if ij does not exist for j). Since the column
ij is different for different j, the total number of non-zero entries in L is bounded by |E(H)| +
|V (H)|.
4.1.3 Implementation. We now show how to implement Algorithm 1 on a RAM machine so that
the total number of field operations (arithmetic, comparison, and assignment on matrix values) and
time used for looping through rows and columns is small. In the following, we consider the RAM
model with Ω(log(n))-bit registers (on input length n) and a second type of registers for storing
field values, with an oracle that performs field operations in constant time, given positions of
registers containing the field values. The input matrix M is given in any form that allows one to
enumerate non-zero entries (as triples indicating a row j, column i, and the position of a register
containing M[j,i]) in linear time. The ordering  is given in any form that allows comparison
in constant time and enumeration of columns in the order in linear time (otherwise, we need
O(|V (H)| log |V (H)|) additional time to sort the columns). The graph H is given in any form that
allows one to enumerate edges in linear time.
Lemma 4.4. Let M be a matrix and let  be a strongH-ordering of widthb, for some completionH of
the bipartite graph of M. Then, Algorithm 1 on M with pivoting order  makes O(|E(H)| · b + |V (H)|)
field operations.
Furthermore, if H and a list of columns sorted with  are given, then all other operations can be
done in time O(|E(H)| · b + |V (H)|) on a RAM machine.
Proof. By enumerating all edges of H, we can construct adjacency lists for the graph in
O(|E(H)|) time. For every vertex i of H, we store the following:
— two auxiliary values a[i], a
[i], set initially to −1;
—a bit remembering whether i was removed from I or J,
—and we additionally compute an array p[i] containing its last b neighbors in  order, not
necessarily sorted.
Note that all the arrays p[i] can be constructed in a total of O(

i ∈V (H ) |NH [i]| · b) = O(|E(H)| · b)
time, by finding the last b elements of the adjacency list of i in time O(|NH [i]| · b), for each i ∈
V (H).
For every edge ij of H, we store a record containing
— the position of the registers containing values a[i], a
[i], a[j], a
[j], and M[j,i]; and
—an auxiliary value a[j,i], set initially to −1.
The record is pointed to by each occurrence of the edge in the lists and p-arrays of i and j (just as
one would store an edge’s weight).
Claim 4.5. Every edge ij of H occurs in at least one of the arrays p[i] or p[j].
Proof. By the definition of the width of ordering , either |{ ∈ NH (j) | i  }| ≤ b or |{ ∈
NH (i) | j  }| ≤ b. In the first case, i can be found among the last b neighbors of j, symmetrically
in the second case.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.                  
Fully Polynomial-Time Parameterized Computations for Graphs 34:23
Note that by Claim 4.5, for each given edge ij we can access the record of ij in time O(b), by
iterating through p[i] and p[j].
Let us consider the number of basic operations (field and constant-time RAM operations) executed throughout the algorithm. The outermost loop executes one iteration for each column i
in the  order. Since a sorted list is given, their enumeration takes O(|V (H)|) time. In every iteration of the outermost loop, the rows with non-zero intersection with i are neighbors of i in
the bipartite graph of M and hence in H, by Lemma 4.3. Therefore, they can be enumerated by
following the adjacency list for i and comparing the corresponding entries with zero, for a total of
at most O(

i ∈V (H ) |NH [i]|) = O(|V (H)| + |E(H)|) basic operations. This also suffices for making
comparisons to find the earliest row j among them.
For any fixed i, after choosing j, the inner loops perform the Entry Update step for every
intersection of a row k ∈ R and a column  ∈ C, where R is the set of rows in J having nonzero intersection with i and C is the set of columns in I having non-zero intersection with j.
By Lemma 4.3, R ⊆ NH (i) and C ⊆ NH (j). We find R and C by iterating over NH (i) to find R,
and iterating over NH (j) to find C. Note that the time spent on all these iterations amortizes to
O(

i ∈V (H ) |NH (i)|) = O(|E(H)|).
To access all the values M[k, ] occurring at intersections k ∈ R and  ∈ C, we iterate over the
arrays p[k] and p[] of each k ∈ R and each  ∈ C. By Claim 4.5, each edge k for k ∈ R,  ∈ C will
be in at least one of these arrays (in particular, |R|·|C| ≤ b · (|R| + |C|)). Thus, the total time used
on Entry Update steps will amount to O(b · (|R| + |C|)) basic operations. More precisely, for each
k ∈ R, we set a[k] := M[k,i] and a
[k] := i. Similarly, for each  ∈ C, we set a[] := M[j, ] and
a
[] := i. Then, we iterate over all edges k with k ∈ R and  ∈ C using the p-arrays, and perform
the Entry Update step for each such edge. Given k and , we can check that they are indeed in R and
C by testing a
[k] = i and a
[] = i. We can ensure that the update is not performed twice on the
same entry by setting a[k, ] := i when the first update is performed, and then not performing it
again when value i is seen in a[k, ]. Values M[k,i] and M[j, ] can be accessed in constant time
via a[k] and a[].
Since |R|≤|NH (i)| and |C|≤|NH (j)|, the total number of basic operations used for the Entry
Update steps is O(b · (|NH (i)| + |NH (j)|)). Since i and j are afterward removed from I and J,
this amortizes to O(b ·

i ∈V (H ) |NH (i)|) = O(b · |E(H)|) basic operations in total. Therefore, the
total number of basic operations made throughout the algorithm is bounded by O(|E(H)| · b +
|V (H)|).
4.2 Orderings for Path and Tree-Partition Decompositions
The aim of this section is to show that small path or tree-partition decompositions of the bipartite
graph associated with a matrix can be used to find a completion with a strong ordering of small
width. In both cases, it is enough to complete the graph to a maximal graph admitting the same
decomposition and take a natural ordering (corresponding to “forget nodes”—the rightmost or
topmost bags that contain each vertex).
Lemma 4.6. Given a matrix M and a path decomposition of width b of the bipartite graph G = GM
of M, one can construct a completionH ofG with at most 2b · |V (G)| edges and list a strongH-ordering
of degeneracy (and hence width) at most b, in time O(b · |V (G)|).
Proof. Consider a path decomposition of G with consecutive bags B1, B2,..., Bq; since we can
make it a clean decomposition in linear time, we can assume that q ≤ n. For every vertexv ofG, let
Bb(v), Be (v) be the first and last bag containingv, respectively. Fori = 1, 2,...,n, let B
i be the set of
all the vertices v with e (v) = i. Let H be the graph obtained from G by adding edges between any
two vertices in the same bag Bi (i.e., sets Bi become cliques in H). The graph H still has pathwidth
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.                     
34:24 F. V. Fomin et al.
b so by Lemma 2.6 it has at most b · |V (G)| edges. Let  be any ordering that places all vertices
in B
i before all vertices B
j , for i < j (vertices within one set B
i can be ordered arbitrarily); that is,
e (u) < e (v) implies u  v. It is straightforward to perform the construction in time O(b · |V (G)|).
We claim  is a strong H-ordering of degeneracy b.
To show this, first observe that uv ∈ E(H) (u and v were in a common bag of the decomposition)
if and only if b(u) ≤ e (v) and b(v) ≤ e (u)—if uv ∈ E(H), the vertices were in a common bag and
the implication is clear, while in the other case either b(u), e (u) < b(v) or b(v), e (v) < b(u), giving
the converse. To check strongness, let i, j, k ∈ V (H) be such that ij,ik ∈ E(H) and j  k. Then
ik ∈ E(H) implies b(k) ≤ e (i) and j  k implies e (j) ≤ e (k). Let  be any neighbor of j that comes
after i. Then j ∈ E(H) implies b() ≤ e (j) and i   implies e (i) ≤ e (). Together, we have b() ≤
e (j) ≤ e (k) and b(k) ≤ e (i) ≤ e (), hence k ∈ E(H), concluding the proof of strongness.
To bound the degeneracy of , for each i ∈ V (H) we want to bound the number of neighbors
 of i with i  . Such a neighbor must satisfy b() ≤ e (i) and e (i) ≤ e (). By the properties of a
decomposition,  must be contained in all bags from Bb() to Be (), hence both i and  are contained
in the bag Be (i). Therefore, the number of such neighbors  is bounded by |Be (i) \ {i}| ≤ b for each
i ∈ V (H), which shows degeneracy is at most b.
Lemma 4.7. Given a matrix M and a tree-partition decomposition of width b of the bipartite graph
G of M, one can construct a completion H of G with at most b · |V (G)| edges and list a strong Hordering of degeneracy (and hence width) at most 2b, in time O(b · |V (G)|).
Proof. Let (T , {Bt }t ∈V (T ) ) be the given tree-partition decomposition of G. Let H be the graph
obtained from H by adding all edges between vertices in the same or in adjacent (in T ) bags. The
graph H still has tree-partition width b and hence at most 2b · |V (G)| edges, by Corollary 2.6. For
a vertex i ∈ V (H), by t(i) we denote the node of T whose bag contains i.
We root T arbitrarily, which imposes an ancestor-descendant relation on the nodes of T . Let
 be any ordering that goes “upward” the decomposition, that is, places all vertices of Bt after all
vertices of Bt for any t and its descendantt in T . It is straightforward to perform the construction
of any such  in O(b · |V (G)|). We claim  is a strong H-ordering of degeneracy 2b.
The bound on degeneracy follows from the fact that the neighbors of a vertex i in H occurring
later in the ordering  must be either in the same bag asi, or in the parent bag of the bag containing
i. To show strongness, let i, j, k ∈ V (H) be such that ij,ik ∈ E(H) and j  k. Let  be any neighbor
of j that comes afteri in . We want to show that  is a neighbor of k too. If it is not, then t()  t(i)
(as otherwise, NH [] = NH [i]  k), and similarly t(k)  t(j) (as otherwise, NH [k] = NH [j]  ).
Since j and i are adjacent, t(i) and t(j) are either equal or adjacent (in T ).
If t(i) is a child of t(j), then since t(k) is either equal or adjacent to t(i) (due to ik ∈ E(H)) and it
is not a descendant of t(j) (by j  k), it must be equal to t(j), contradicting the above inequalities.
If t(j) is a child of t(i), then since t() is either equal or adjacent to t(j) (due to j ∈ E(H)) and it
is not a descendant of t(i) (by i  ), it must be equal to t(i), contradicting the above inequalities.
If t(i) = t(j), then t(k) is either equal or adjacent to t(i) = t(j) (by ik ∈ E(H)); they cannot be
equal (by the above inequalities) and t(k) is not a child of t(j) (by j  k), hence t(k) must be the
parent of t(i) = t(j). Similarly, t() is either equal or adjacent to t(j) = t(i) (by j ∈ E(H)), they
cannot be equal (by the above inequalities), and t() is not a child of t(i) (by i  ), hence t()
must also be the parent of t(j) = t(i), implying t() = t(k). Hence, in any case,  is a neighbor of
k too, proving strongness of the H-ordering .
Given Lemmas 4.6 and 4.7, from an n × m matrix M and a path or tree-partition decomposition
of GM of width b, we can construct a completion H of GM with at most (n + m) · b edges and a
strong H-ordering of width at most 2b. Therefore, Gaussian elimination can be performed using
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.                                                       
Fully Polynomial-Time Parameterized Computations for Graphs 34:25
O((n + m) · b2) field operations and time by Lemma 4.4, yielding matrices with at most (n + m) · b
non-zero entries by Lemma 4.3. Together with Lemma 4.1, this concludes the proof of Theorem 1.2.
Theorem 1.2. Given an n × m matrix M over a field F and a path or tree-partition decomposition of
its bipartite graphGM of width k, Gaussian elimination on M can be performed using O(k2 · (n + m))
field operations and time. In particular, the rank, determinant, a maximal nonsingular submatrix, and
a PLUQ-factorization can be computed in this time. Furthermore, for every r ∈ Fn, the system of linear
equations Mx = r can be solved in O(k · (n + m)) additional field operations and time.
It is tempting to try to perform the same construction as in Lemmas 4.6 and 4.7 also for standard
tree decompositions that correspond to treewidth. That is, complete the graph to a chordal graph
H according to the given tree decomposition, root the decomposition in an arbitrary bag, and
order the vertices in a bottom-up manner according to their forget nodes (i.e., highest nodes of
the tree containing them). Unfortunately, it is not hard to construct an example showing that
this construction does not yield a strong ordering. In fact, there are chordal graphs that admit
no strong ordering [26]. For this reason, in the next section we show how to circumvent this
difficulty by reducing the case of tree decompositions to tree-partition decompositions using the
vertex-splitting technique.
4.3 Vertex Splitting for Low Treewidth Matrices
In this subsection, we show how vertex splitting can be used to expand a matrix of small treewidth
into an equivalent matrix of small tree-partition width. This allows us to extend our results to
treewidth as well and prove Theorem 1.3, without assuming the existence of a good ordering for
the original matrix.
The vertex-splitting operation, as described in the Introduction, can be used repeatedly to
change vertices into arbitrary trees, with original edges moved quite arbitrarily. While algorithms
will not need to perform a series of splittings, as the final outcome in our application can be easily described and constructed directly from a given tree decomposition, we nevertheless define
possible outcomes in full generality to perform inductive proofs more easily.
In this section, each tree decomposition has a tree that is arbitrarily rooted and the set of all
children of each node is arbitrarily ordered, so that the following ordering can be defined. The
pre-order of an ordered tree is the ordering of nodes which places a parent before its children and
the children in the same order as defined by the tree.
A tree-split E of a graphG is an assignment of an ordered tree E(v) to every vertexv ∈ V (G) and
of a node pair E(uv) = (t,t
) for every edge uv ∈ E(G), such that t ∈ V (E(u)) and t ∈ V (E(v)).
A rooted tree decomposition (T , {Bt }t ∈V (T ) ) (with an ordered tree T ) of a graph G gives rise to
a tree-split ET as follows: for v ∈ V (G), E(v) is the subtree of T induced by those nodes whose
bags containv; for uv ∈ E(G), E(uv) = (t,t), where t is the topmost node of T whose bag contains
both u and v (it is easy to see that there is always a unique such node). See Figure 2 for an example.
For a matrix M and a tree-split E of G = GM , we define below the E-split of M, denoted ME. We
later show that this operation preserves the determinant up to sign, for example. We will denote
the E-split of M corresponding to a tree decomposition T of the bipartite graph of M simply
as MT —we aim to show that MT preserves the algebraic properties of M and strengthens the
structure given by T to that of a tree-partition decomposition. The ordering and sign choices in
the definition are only needed to preserve the sign of the determinant.
Definition 2.8 (E-split of M). Let M be a matrix with rows r1,...,rn, columns c1,...,cm, and
bipartite graph G = GM , and let E be a tree-split of G. The matrix ME has the following rows, in
order: for every row ri of the original matrix (in original order), we have a row indexed with the
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
34:26 F. V. Fomin et al.
Fig. 2. On the left: a matrix M, its bipartite graph GM , and the matrix after splitting the row r once (into r0
and r1, creating the auxiliary column r01). In the middle: a tree decomposition of GM with nodes 0, 1, 2, 3.
On the right: the matrix after splitting according to this decomposition, and its graph in a tree-partition
decomposition. The resulting graph is also a tree-split of GM , with the constituent trees colored with the
color of the corresponding vertex (since c,c
,c appear in unique bags of the decompositions, their splittings
are trivial, with one vertex and no edge, so no corresponding row appears).
pair (ri,t), where t is the root of E(ri ). After this, for every original row ri we have consecutively,
for every non-root node t of the tree E(ri ), a row indexed with the pair (ri,t) (different t occurring
according to the pre-order of the tree). Then, for every original column ci (in original order), we
have consecutively for every edge tt of the tree E(ci ) a row indexed with the pair (ci,tt
) (different
tt occurring according to the pre-order of the lower node in the tree E(ci )). The columns of ME
are defined symmetrically, that is, they are indexed with (ci,t) and (ri,tt
).
We define the entries of ME. For every non-zero entry of the original matrix, that is, every edge ricj of its bipartite graph, set ME[(ri,t), (cj,t
)] = M[ri,cj], where (t,t
) = E(ricj). For
each row indexed as (ci,tt
), with t being the parent of t in E(ci ), set M
[(ci,tt
), (ci,t
)] =
−M
[(ci,tt
), (ci,t)] = (−1)
n
, where n is the number of rows preceding (ci,t
). Symmetrically,
for each column indexed as (ri,tt
), with t the parent of t in E(ri ), set M
[(ri,t
), (ri,tt
)] =
−M
[(ri,t), (ci,tt
)] = ±1 analogously. We set all other entries to 0. This concludes the definition.
For a tree-split E of a graph G, we write E for the total number of edges in all trees E(v),
for v ∈ V (G). Note that if E is a tree-split of the bipartite graph of an n × m matrix M, then ME
has exactly n + E rows and m + E columns. For a set of rows (analogously for columns) I, we
write IE for the set of all rows of ME, except those of the form (v,t) where v is a row of M not
in I and t is the root of E(v). In other words, IE is obtained from I by taking rows with the same
positions, and adding all the last E rows of ME. In particular, |IE | = |I | + E.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
Fully Polynomial-Time Parameterized Computations for Graphs 34:27
Lemma 4.9. Let E be a tree-split of the bipartite graph G of an n × m matrix M. Then rk M =
rk ME − E and for any sets I and J of rows and columns of M of equal size,
det[M]I, J = (−1) E·(n+m) · det[ME]IE, JE .
In particular, det ME = det M, if n = m.
To prove the above lemma, we need the following definition and claim to perform an inductive
step. For a tree-split E of a graph G and two adjacent nodes a,b of a tree E(v) assigned to some
vertex v ∈ V (G), define the contracted tree-split Ev/ab as E with the two nodes a,b identified in
E(v) (contracting the tree edge connecting a and b) and identified in any pair E(vw) that contains
them.
Claim 4.10. Let E be a tree-split of the bipartite graph G = GM of a matrix M. Let (v,tt
) be the
last column or the last row of ME (so t is the last node in pre-order of the tree E(v) and t is its
parent). Then rk MEv/t t  = rk ME − 1 and if I, J are any sets of rows and columns of M of equal size,
then det[MEv/t t ]IEv/t t  , JEv/t t  = (−1)
n
· det[ME]IE, JE , where n is the number of rows (if v is a row)
or columns (if v is a column) of MEv/t t  .
Proof. Without loss of generality assume v is a row of M. Note that the signs in the definition
of ME were chosen so that removing the last column or last row does not change any of them.
Hence, the matrices ME and MEv/t t  only differ at rows (v,t), (v,t
), and column (v,tt
). Observe
that the row (v,t) of MEv/t t  is obtained by adding rows (v,t
) and (v,t) of ME and deleting column
(v,tt
). Thus, adding row (v,t
) to row (v,t) of ME yields a matrix equal to MEv/t t  , but with an
additional column (v,tt
) and an additional row (v,t
). After this row operation, the column (v,tt
)
has only one non-zero entry σ = ±1 at the intersection with row (v,t
) (the other has just been
canceled). Hence, all other entries of this additional row can be eliminated using this entry. This
makes ME[(v,t
), (v,tt
)] = σ the only non-zero entry in its row and column (after applying the
above row and column operations). Hence, rk ME = rk MEv/t t  + 1.
To check minor determinants, consider any sets I, J of rows and columns of M of equal size.
The sets IE and JE always contain (v,t
) and (v,tt
), so the above row and column operations
have the same effect after deleting rows and columns outside those sets, and do not change the
determinant. Moving row (v,t
) to the last position multiplies the determinant by (−1)
n1 , where n1
is the number of rows below it—since IE contains all rows below (v,t
), this is independent of I and
J and we can count rows in ME just as well. Then, deleting this last row and the last column (whose
only non-zero entry is their intersection σ) multiplies the determinant by σ = (−1)
n2 , where n2
was defined to count the rows originally above (v,t
) in ME. This deletion yields a matrix equal
to MEv/t t  , hence det ME = (−1)
n
· det MEv/t t  , where n = n1 + n2 counts all rows of ME except
the deleted one, which is equal to the number of all rows in MEv/t t  .
Proof of Lemma 4.9. Observe that if E is a trivial tree-split assigning a single node tree E(v) to
every vertexv ∈ V (G), then ME defines the same matrix as M. Hence, M can be obtained from ME
by repeatedly contracting the edge corresponding to the last row or the last column. By Claim 4.10,
the rank decreases by exactly one with every contraction, so in total it decreases by E.
Similarly, each minor’s determinant changes sign i times with every contraction, where
i counts the rows or columns of the matrix after contraction. Hence in total, the number
of sign changes is n
i=n+E −1 i (for contractions corresponding to rows) plus m
i=m+E −1 i
(for columns), which is equal to E·(n+E −1+n)
2 + E·(m+E −1+m)
2 = E · (n + m + E − 1) ≡
E · (n + m) (mod 2).
The explicit construction of the split of a matrix allows us to easily bound its size, number of
non-zero entries in each row, and in total. Before this, to optimize these parameters, we need the
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.  
34:28 F. V. Fomin et al.
following easy adaption of a standard lemma about constructing so-called “nice tree decompositions” (see [9]).
Lemma 4.11. Given a tree decomposition of a graph G of width b, one can find in time O(bn) a tree
decomposition of G of width b with a rooted tree of at most 5n nodes, each with at most two children.
Furthermore, for every node t of the decomposition, there are at most b edges uv ∈ E(G) such that t
is the topmost node whose bag contains both u and v.
Proof. Root the given tree decomposition in an arbitrary node. For v ∈ V (G), let t(v) be the
topmost node whose bag contains v—there is such a bag by properties of tree decompositions (the
so-called forget bag for v). Bodlaender et al. [9, Lemma 6] describe how to transform a tree decomposition in time O(n · b) to a “nice tree decomposition,” which in particular has the following
properties: it has at most four |V (G)| nodes, each with at most two children, and furthermore t(v)
has at most one child for each v ∈ V (G), and for u  v, either t(u)  t(v) or t(u) = t(v) is the root
of the decomposition tree. To remedy this last possibility, if the root node’s bag is B = {b1,...,b },
add atop of it a path of nodes with bags {b1,...,bi} for i = ,  − 1,..., 1, 0, rooted at the last,
empty bag. This adds at most  ≤ |V (G)| nodes to the decomposition tree. Now, t(v)  t(u) for all
vertex pairs u  v ∈ V (G).
Consider now any edge uv ∈ E(G). Since the sets of nodes whose bags contain u and those
whose contain v induce connected subtrees of the decomposition tree, their intersection is also
a connected subtree whose topmost bag cannot be a descendant of both t(u) and t(v). That is,
the topmost node whose bag contains both u and v is either t(u) or t(v). Therefore, if we assign
each edge to the topmost bag that contains both its endpoints, then for each node t, the edges
assigned to it must be incident to v, where v ∈ V (G) is such that t = t(v). Since such edges have
both endpoints in the bag of t, there can be at most b of them.
Lemma 4.12. Let M be an n × m matrix with bipartite graphG = GM . Let T be a tree decomposition
of G of width b obtained from Lemma 4.11. Then MT has the following properties, for some N =
O(b · (n + m)):
(a) MT has n + N rows and m + N columns.
(b) Every row and column of MT has at most b + 3 non-zero entries, and MT has at most
|E(G)| + 4N such entries in total.
(c) The bipartite graph of MT has a tree-partition decomposition of width b, with a tree that is
the 1-subdivision of the tree of T .
(d) MT and the decomposition can be constructed in time O(N).
(e) rk M = rk MT − N.
(f) det[M]I, J = (−1)N ·(n+m) · det[MT ]I
, J for any sets I and J of rows and columns of M of
equal size, where I  (J 
) is obtained from I (J) by adding the last N rows (columns) of MT to
it,
(in particular, det MT = det M, if n = m).
Proof. Let E be the tree-split corresponding to T , let MT = ME, and let N = E. Property (a)
follows directly from the definition of ME. Since by definition trees E(v) are subtrees of T , and,
by the definition of a decomposition’s width, at most b + 1 such subtrees can share each edge of
this tree, we have that N = E = O(b · (n + m)).
Property (b) follows from the fact that every row of MT is either indexed as (c,tt
) (for some
column c of M)—in which case it has exactly two non-zero entries—or it is indexed as (r,t) for
some row r of M and some node t of T whose bag contains r. The only non-zero entries of row
(r,t) are MT [(r,t), (c,t)] for edges rc ∈ E(G) such that E(rc) = (t,t); and MT [(r,t), (r,tt
)] for
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.         
Fully Polynomial-Time Parameterized Computations for Graphs 34:29
neighbors t of t in T . By the guarantees of Lemma 4.11, there are at most b edges rc such that t
is the topmost bag containing both endpoints, that is, such that E(rc) = (t,t). Furthermore, every
node has at most three neighbors in the tree of T , thus the row (r,t) has at most b + 3 non-zero
entries in total. The proof is symmetric for columns. Similarly, the total number of non-zero entries
can be bounded by 2N for entries in rows indexed as (c,tt
), 2N for entries in columns indexed
as (r,tt
), and |E(G)| for the remaining entries, which must be of the form MT [(r,t), (c,t)] where
rc ∈ E(G) and (t,t) = E(rc).
Properties (c) and (d) follow easily from the construction: the tree-partition decomposition of
MT ’s bipartite graph will have a bag Vt for every node t of T and a bag Vt t for every edge tt of
T —these bags are naturally assigned to the nodes of the 1-subdivision of T ’s tree. Each bag Vt
contains all rows and columns indexed as (v,t), for some v ∈ V (G) (contained in the bag of t in T )
and each bag Vt t contains all rows and columns indexed as (v,tt
), for some v ∈ V (G) (contained
in both bags of t,t in T ). This defines a valid tree-partition decomposition of width at most b, as
non-zero entries are either of the form MT [(r,t), (c,t)] and hence its row and column fall into the
same bag Vt , or of the form MT [(r,t), (r,tt
)] (or symmetrically for columns) and hence its row
and column fall into adjacent bags Vt , Vt t.
Properties (e) and (f) follow directly from Lemma 4.9.
We remark that the construction of Lemma 4.12 actually preserves the symmetry of the matrix,
that is, if M is symmetric, then so is ME as well. The construction can be also easily adapted to
preserve skew-symmetry, if needed.
As a final ingredient for Theorem 1.3, we need to provide a generalized LU -factorization for the
original matrix and retrieve a maximal nonsingular submatrix.
Lemma 4.13. Let M be an n × m matrix over a field F and let E be a tree-split of the bipartite
graph G = GM . Given a PLUQ-factorization of ME with N non-zero entries in total, a generalized
LU -factorization of M with at most N  = N + 2E + 2n + 2m non-zero entries can be constructed
in O(N 
) time. Hence, given any vector r ∈ Fm, the system of linear equation Mx = r can be solved
in O(N 
) additional field operations and time.
Proof. DefineUE as the following matrix with n rows corresponding to rows of M and n + E
columns corresponding to rows of ME (indexed as (r,t) or (c,tt
)). The only non-zero entries ofUE
areUE[r, (r,t)] = 1 for rowsr of M and nodest of the tree E(r). Analogously, define LE as the following matrix with m columns corresponding to columns of M and m + E rows corresponding
to columns of ME (indexed as (c,t) or (r,tt
)). The only non-zero entries of LE are LE[(c,t),c] = 1
for columns c of M and nodes t of the tree E(c). It is straightforward from the definition of ME
that M = UEMELE.
Let P, L,U,Q define the given PLUQ-factorization of ME; then M = UEPLUQLE. Observe that
for any ordering of columns of UE, the matrix can be given in row-echelon form by ordering rows
so thatr1 comes before r2 whenever the first column indexed as (r1,t) (fort ∈ E(r1)) comes before
the first column indexed as (r2,t
) (for t ∈ E(r2)). Hence, we can construct from P a permutation
n × n matrix P such that U  = P
UEP is in row-echelon form. Similarly, we can find an m × m
permutation matrixQ and a reordering L of LE such that L = QLEQ is in column-echelon form.
Then, M = P−1
U 
LU L
Q−1 gives a generalized LU -factorization. The number of non-zero entries
in LE and inUE is equal to n + E andm + E, respectively. Hence, the total number of non-zero
entries in the factorization is at most N  = 2n + E + N + E + 2m.
To solve a system of linear equations Mx = r with input vector r, we proceed with each matrix
of the generalized factorization just as in Lemma 4.1, either using back-substitution or permuting
entries, using a total number of field operations equal to twice the number of non-zero entries and
O(N 
) time.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.  
34:30 F. V. Fomin et al.
Given Lemma 4.12, from an n × m matrix M and a tree decomposition of GM of width at most b,
we can construct an (n + N) × (m + N) matrix MT with a tree-partition decomposition of width
at most b, in time O(N), for N = O((n + m) · b). Then, by Lemma 4.7 we can find a completion H
of GMT with at most N  = (n + m + 2N) · b edges and a strong H-ordering of width at most 2b.
Therefore, Gaussian elimination can be performed on MT using O(N  · b) = O((n + m) · b3) field
operations and time by Lemma 4.4, yielding matrices with at most 2N  + n + m + 2N = O((n +
m) · b2) non-zero entries by Lemma 4.3. This allows us to retrieve the rank and determinant of
MT , by Lemma 4.1, and hence also of the original matrix M, by Lemma 4.12. We can also retrieve
a PLUQ-factorization of MT and hence, by Lemma 4.13, a generalized LU -factorization of M with
O((n + m) · b2) non-zero entries, which allows us to solve a system of linear equations Mx = r
with given r in O((n + m) · b2) additional field operations and time. This concludes the proof of
Theorem 1.3.
Theorem 1.3. Given an n × m matrix M over a field F and a tree-decomposition of its bipartite
graph GM of width k, we can calculate the rank, determinant, and a generalized LU -factorization of
M in O(k3 · (n + m)) field operations and time. Furthermore, for every r ∈ Fn, the system of linear
equations Mx = r can be solved in O(k2 · (n + m)) additional field operations and time.
5 MAXIMUM MATCHING
5.1 Computation Model
As we have already mentioned in the Introduction, in this section we will consider arithmetic
operations in a field F of size O(nc ) for some constantc. Although any such field would suffice, let
us focus our attention on F = Fp for some prime p = O(nc ). Then an element of F can be stored in
a constant number of machine words of length logn, and the following arithmetic operations in Fp
can be easily implemented in constant time, assuming constant-time arithmetic on machine words
of length logn: addition, subtraction, multiplication, testing versus zero. The only operation that is
not easily implementable in constant time is inversion, and hence also division. In a standard RAM
machine, we need to apply the extended Euclid’s algorithm, which takes O(logn) time. However,
in real-life applications, operations such as arithmetic in a field are likely to be heavily optimized,
and hence we find it useful to separate this part of the running time.
In the algorithms in the sequel, we state the consumed time resources in terms of time (standard
operations performed by the algorithm) and field operations, each time meaning operations in some
Fp for a polynomially bounded p.
5.2 Computing the Size of a Maximum Matching
To address the problem of computing the maximum matching in a graph, we need to recall some
classic results on expressing this problem algebraically. Recall that a matrix A is called skewsymmetric if A = −AT . For a graph G with vertex set {v1,v2,...,vn }, let E = {xij : i < j, vivj ∈
E(G)} be a set of indeterminates associated with edges of G. With G we can associate the Tutte
matrix ofG, denotedA˜(G) and defined as follows. The matrixA˜(G) = [aij]1≤i,j ≤n is an n × n matrix
over the field Z(E), that is, the field of fractions of multivariate polynomials over E with integer
coefficients. Its entries are defined as follows:
aij =
⎧⎪⎪
⎨
⎪⎪
⎩
xij if i < j and vivj ∈ E(G);
−xji if i > j and vivj ∈ E(G);
0 otherwise.
Clearly, A˜(G) is skew-symmetric. The following result is due to Tutte [70] (for perfect matchings)
and Lovász [51] (for maximum matchings).
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
Fully Polynomial-Time Parameterized Computations for Graphs 34:31
Theorem 5.1 ([51, 70]). A˜(G) is nonsingular if and only if G has a perfect matching. Moreover,
rkA˜(G) is equal to twice the maximum size of a matching in G.
From Theorem 5.1, we can derive our first result on finding the cardinality of a maximum matching, that is, Theorem 1.4, which we recall below.
Theorem 1.4. There exists an algorithm that, given a graph G together with its tree decomposition
of width at most k, uses O(k3 · n) time and field operations and computes the size of a maximum
matching in G. The algorithm is randomized with one-sided error: it is correct with probability at
least 1 − 1/nc for an arbitrarily chosen constant c, and in the case of an error it reports a suboptimal
value.
Proof. Arbitrarily enumerate V (G) as {v1,v2,...,vn }. Let A˜(G) be the Tutte matrix of G. Let
p be a prime with nc+1 ≤ p < 2 · nc+1, and let F = Fp (p can be found in polylogarithmic expected
time by iteratively sampling a number and checking whether it is prime using the AKS algorithm).
Construct matrix A(G) from A˜(G) by substituting each indeterminate from E with a value chosen uniformly and independently at random from F. Since the determinant of the largest nonsingular square submatrix of A˜(G) is a polynomial over E of degree at most n, from the SchwarzZippel lemma it follows that this submatrix remains nonsingular in A(G) with probability at least
1 − n
p ≥ 1 − 1
nc . Moreover, for any square submatrix of A(G) that is nonsingular, the corresponding submatrix of A˜(G) is nonsingular as well. Hence, with probability at least 1 − 1
nc we have that
matrix A(G) has the same rank as A˜(G), and otherwise the rank of A(G) is smaller than that of
A˜(G).
Let H be the bipartite graph GA(G) associated with matrix A(G). Then H has 2n vertices: for
every vertex u of G, H contains a row-copy and a column-copy of u. Based on a decomposition of G
of width k, it is easy to construct a tree decomposition of H of width at most 2k + 1 as follows: the
decomposition has the same tree, and in each bag we replace each vertex of G by both its copies
in H. The construction of H and its tree decomposition takes time O(kn).
We now use the algorithm of Theorem 1.3 to compute the rank of A(G); this uses O(k3 · n) time
and field operations. Supposing that A(G) indeed has the same rank as A˜(G) (which happens with
probability at least 1 − 1
nc ), we have by Theorem 5.1 that the size of a maximum matching in G is
equal to the half of this rank, so we report this value. In the case when rkA(G) < rkA˜(G), which
happens with probability at most 1
nc , the algorithm will report a value smaller than the maximum
size of a matching in G.
5.3 Reconstructing a Maximum Matching
Theorem 1.4 gives only the maximum size of a matching, but not the matching itself. To recover
the maximum matching, we need some extra work.
First, we need to perform an analog of the splitting operation from Section 4 in order to reduce
the case of tree decompositions to tree-partition decompositions. The reason is that Theorem 1.3
does not give us a maximum nonsingular submatrix of the Tutte matrix of the graph, which we
need in order to reduce finding a maximum matching to finding a perfect matching in a subgraph.
Lemma 5.2. There exists an algorithm that given a graph G together with its clean tree decomposition (T , {Bx }x ∈V (T ) ) of width at most k, constructs another graph G together with its tree-partition
decomposition (T 
, {Cx }x ∈V (T )) of width at most k, such that the following holds:
(i) The algorithm runs in time O(kn).
(ii) |V (G
)| ≤ 2kn.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018. 
34:32 F. V. Fomin et al.
(iii) Given a matching M in G, one can construct in O(kn) time a matching M in G with
|M
| = |M| + Λ/2, where Λ = |V (G
)|−|V (G)|.
(iv) Given a matching M in G
, one can construct in O(kn) time a matching M in G with
|M|≥|M
| − Λ/2.
Proof. The vertex set of G consists of the following vertices:
—For every vertex u ∈ V (G) and every node t ∈ V (T ) with u ∈ Bt , create a vertex (u,t).
—For every vertex u ∈ V (G) and every pair of adjacent nodes t,t ∈ V (T ) with u ∈ Bt ∩ Bt,
create a vertex (u,tt
).
The edge set of G is defined as follows:
—For every vertex u ∈ V (G) and every pair of adjacent nodes t,t ∈ V (T ) with u ∈ Bt ∩ Bt,
add edges (u,tt
)(u,t) and (u,tt
)(u,t
).
—For every edge uv ∈ E(G) and every node t ∈ V (T ) with {u,v} ⊆ Bt , create an edge
(u,t)(v,t).
This finishes the description of G
. Construct T  from T by subdividing every edge of T once,
that is, T  is the 1-subdivision of T . Let the subdivision node used to subdivide edge tt be also
called tt
. Place all the vertices of G of the form (u,t) in a bag Ct , for each t ∈ V (T ), and place
all the vertices of G of the form (u,tt
) in a bag Ct t, for each tt ∈ E(T ). Then it readily follows
that (T 
, {Cx }x ∈V (T )) is a tree-partition decomposition of G of width at most k. Moreover, since
T was clean, it has at most most n nodes , and hence also at most n − 1 edges. Hence, G has at
most 2kn vertices, and (ii) is satisfied. It is straightforward to implement the construction above in
time O(kn), and hence (i) also follows. We are left with showing how matchings in G and G can
be transformed one to the other.
Before we proceed, let us introduce some notation. Let W V be the set of all the vertices of G
of the form (u,t) for some t ∈ V (T ), and let W E be the set of all the vertices of G of the form
(u,tt
) for some tt ∈ E(T ). Then (W V ,W E ) is a partition of V (G
). For some u ∈ V (G), let Tu
be the subgraph of G induced by all the vertices with u on the first coordinate, that is, of the
form (u,t) or (u,tt
). It follows that Tu is a tree. Moreover, if we denote W V
u = W V ∩V (Tu ) and
W E
u = W E ∩V (Tu ), then (W V
u ,W E
u ) is a bipartition of Tu , all the vertices of W E
u have degrees 2 in
Tu and no neighbors outside Tu , and |W V
u | = |W E
u | + 1. Then, we have that
|V (G
)| =

u ∈V (G)
|W V
u | + |W E
u | =

u ∈V (G)
(2|W E
u | + 1) = |V (G)| + 2

u ∈V (G)
|W E
u |,
so Λ/2 = 
u ∈V (G) |W E
u |.
Claim 5.3. For each u ∈ V (G) and each w ∈ W V
u , there is a matching Mu,w in Tu that matches all
the vertices of Tu apart from w. Moreover, Mu,w can be constructed in time O(|V (Tu )|).
Proof. RootTu inw. This imposes a parent-child relation on the vertices ofTu , and in particular,
each vertex of W E
u has exactly one child, which of course belongs to W V
u . Construct Mu,w by
matching each vertex of W E
u with its only child. Then only the root w is left unmatched in Tu .
Now, the first direction is apparent.
Claim 5.4. Condition (iii) holds.
Proof. Let M be a given matching in G. Construct M as follows: For every uv ∈ M, pick an
arbitrary node t of T with {u,v} ⊆ Bt , and add (u,t)(v,t) to M
. After this, from each subtree Tu ,
for u ∈ V (G), at most one vertex is matched so far, and it must belong to W V
u . Let wu ∈ V (Tu ) be
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018. 
Fully Polynomial-Time Parameterized Computations for Graphs 34:33
this matched vertex in Tu ; in case no vertex of Tu is matched so far, we take wu to be an arbitrary
vertex of W V
u . For each u ∈ V (G), we add to M the matching Mu,wu , which has cardinality |W E
u |;
this concludes the construction of M
. It is clear that M is a matching in G
, and moreover
|M
| = |M| +

u ∈V (G)
|W E
u | = |M| + Λ/2.
It is easy to see that a straightforward construction of M takes time O(kn).
We now proceed to the proof of the second direction.
Claim 5.5. Condition (iv) holds.
Proof. Let M be a given matching in G
. We first transform it to a matching M in G such
that |M|≥|M
| and for each u ∈ V (G) at most one vertex fromTu is matched in M with a vertex
outside Tu . If for some vertex u this is not true, we arbitrarily select one edge ww of those in
the matching with exactly one endpoint, say w, in Tu (and hence in W V
u ). We then remove all
edges incident to Tu from the matching except ww and add Mu,w ⊆ E(Tu ) from Claim 5.3 to the
matching. We removed at most |W V
u | − 1 edges, as every edge of a matching incident to Tu must
contain a vertex of W V
u . However, we added |W E
u | = |W V
u | − 1 edges, hence the matching could
only increase in size.
Construct the matching M in G as follows: Inspect all the edges of M, and for each edge of
the form (u,t)(v,t) ∈ M for some u,v ∈ V (G), add the edge uv to M. The construction of M
ensures that M is indeed a matching in G, and no edge of G is added twice to M. Moreover, for
eachu ∈ V (G), we have that M ∩ E(Tu ) is a matching inTu , so in particular, |M ∩ E(Tu )|≤|W E
u |.
Hence, from the construction we infer that
|M
|≤|M| = |M| +

u ∈V (G)
|M ∩ E(Tu )|≤|M| +

u ∈V (G)
|W E
u | = |M| + Λ/2.
A straightforward implementation of the construction of M runs in time O(kn).
Claims 5.3 and 5.3 conclude the proof.
Next, we need to recall how finding a maximum matching can be reduced to finding a perfect
matching using a theorem of Frobenius and the ability to efficiently compute a largest nonsingular
submatrix; this strategy was used, for example, by Mucha and Sankowski in [56].
Theorem 5.6 (Frobenius Theorem). Suppose A is an n × n skew-symmetric matrix, and suppose
X,Y ⊆ {1,...,n} are such that |X | = |Y | = rkA. Then
det[A]X,X · det[A]Y,Y = (−1)|X | ·

det[A]X,Y
2 .
Corollary 5.7. Let G be a graph with vertex set {v1,v2,...,vn }, and suppose [A˜(G)]X,Y is a
maximal nonsingular submatrix of A˜(G). Then X = {vi : i ∈ X} is a subset of V (G) of maximum size
for which G[X] contains a perfect matching.
Proof. By the assumption about X, we have that |X | = rkA˜(G). Since [A˜(G)]X,Y is nonsingular, by the Frobenius theorem both [A˜(G)]X,X and [A˜(G)]Y,Y are nonsingular as well. Observe
that [A˜(G)]X,X = A˜(G[X]), so by Theorem 5.1 we infer that G[X] contains a perfect matching.
Moreover, X has to be the largest possible set with this property, because |X | = rkA˜(G).
Now we are ready to prove the analog of Theorem 1.5 for tree-partition width. This proof contains the main novel idea of this section. Namely, it is known if a perfect matching is present in the
graph, then from the inverse of the Tutte matrix one can derive the information about which edges
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.    
34:34 F. V. Fomin et al.
are contained in some perfect matching. Finding one column of the inverse amounts to solving one
system of linear equations, so in time roughly O(k2
n) we are able to find a suitable matching edge
for any vertex of the graph. Having found such an edge, both the vertex and its match can be
removed; we call this operation ousting. However, ousting iteratively on all the vertices would result in a quadratic dependence on n. Instead, we apply a Divide & Conquer scheme: we first oust
the vertices of a balanced bag of the given tree-partition decomposition, and then recurse on the
connected components of the remaining graph. This results in O(logn) levels of recursion, and
the total work used on each level is O(k3 · n). We remark that in Section 6, we apply a similar
approach to the maximum flow problem.
Lemma 5.8. There exists an algorithm that, given a graph G together with its tree-partition decomposition of width at most k, uses O(k3 · n logn) time and field operations and computes a maximum
matching in G. The algorithm is randomized with one-sided error: it is correct with probability at
least 1 − 1
nc for an arbitrarily chosen constant c, and in the case of an error it reports a failure or a
suboptimal matching.
Proof. Let {v1,v2,...,vn } be an arbitrary enumeration of the vertices of G and let A˜(G) be
the Tutte matrix of G. For the entire proof, we fix a field F = Fp for some prime p with nc+5 ≤ p <
2 · nc+5. Construct A(G) from A˜(G) by substituting each indeterminate from E with a value chosen
uniformly and independently at random from F. Since the determinant of the largest nonsingular
submatrix of A˜(G) is a polynomial over E of degree at most n, from the Schwarz-Zippel lemma
we obtain that this submatrix remains nonsingular in A(G) with probability at least 1 − n
nc+5 =
1 − 1
nc+4 ; of course, in this case A(G) has the same rank as A˜(G).
Similarly as in the proof of Theorem 1.4, let H = GA(G) be the bipartite graph associated with
A(G). Then, a tree-partition decomposition of H of width at most 2k can be constructed from the
given tree-partition decomposition of G of width at most k by substituting every vertex with its
row-copy and column-copy in H in the corresponding bag. Therefore, we can apply the algorithm
of Theorem 1.2 to A(G) with this decomposition of H, and retrieve a largest nonsingular submatrix
of A(G) using O(k2 · n) time and field operations. Suppose that this submatrix is [A(G)]X,Y for
some X,Y ⊆ {1,...,n}. Then |X | = |Y | = rkA(G) and of course [A˜(G)]X,Y is nonsingular as well.
Recall that with probability at least 1 − 1
nc+4 , we have rkA(G) = rkA˜(G), so if this holds, then
[A˜(G)]X,Y is also the largest nonsingular submatrix of A˜(G). Then the rows of A˜(G) with indices
of X form a base of the subspace of Z(E)
n spanned by all the rows of A˜(G). By Corollary 5.7, we
infer that X = {vi : i ∈ X} is the maximum size subset of V (G) for which G[X] contains a perfect
matching.
We now constrain our attention to the graph G[X], and our goal is to find a perfect matching
there. Observe that a tree-partition decomposition of G[X] of width at most k can be obtained
by taking the input tree-partition decomposition of G, removing all the vertices of V (G) \ X from
all the bags, and deleting bags that became empty. Hence, in this manner we effectively reduced
finding a maximum matching to finding a perfect matching, and from now on we can assume
w.l.o.g. that the input graph G has a perfect matching.
If G has a perfect matching, then from Theorem 5.1 we infer that A˜(G) is nonsingular. Again,
construct A(G) from A˜(G) by substituting each indeterminate from E with a value chosen uniformly and independently at random from F. Since detA˜(G) is a polynomial of degree at most n
over E, from the Schwarz-Zippel lemma we infer that with probability at least 1 − n
nc+5 = 1 − 1
nc+4
matrix A(G) remains nonsingular. Similarly as before, a suitable tree-partition decomposition of
the bipartite graphH = GA(G) can be constructed from the input tree-partition decomposition ofG,
and hence we can apply Theorem 1.2 to A(G). In particular, we can check using O(k2 · n) time and
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
Fully Polynomial-Time Parameterized Computations for Graphs 34:35
field operations whether A(G) is indeed nonsingular, and otherwise we abort the computations
and report failure.
Call an edge e ∈ E(G) allowed if there is some perfect matching M in G that contains e. The
following result of Rabin and Vazirani [61] shows how it can be retrieved from the inverse of A˜(G)
whether an edge is allowed; it is also the cornerstone of the approach of Mucha and Sankowski [55,
56].
Claim 5.9 ([61]). Edge vivj is allowed if and only if the entry (A˜(G)
−1)[j,i] is non-zero.
This very easily leads to an algorithm that identifies some allowed edge incident on a vertex.
Claim 5.10. For any vertex u ∈ V (G), one can find an allowed edge incident on u using O(k2
n)
time and field operations.
Proof. Suppose u = vi for some i ∈ {1,...,n}. We recover the i-th column ci of A(G)
−1 by solving the system of equations A(G)ci = ei , where ei is the unit vector with 1F on the i-th coordinate,
and zeros on the other coordinates. Using Theorem 1.2, this takes time O(k2
n), including the time
needed for Gaussian elimination. Now observe that ci must have a non-zero entry on some coordinate, say the j-th, which corresponds to an edge vivj ∈ E(G). Indeed, the i-th row ri of A(G)
has non-zero entries only on the coordinates that correspond to neighbors of vi and we have that
rici = 1F, so it cannot happen that all the coordinates of ci corresponding to the neighbors of vi
have zero values.
This means that (A(G)
−1)[j,i]  0, which implies that (A˜(G)
−1)[j,i]  0. Since vivj ∈ E(G), by
Claim 5.3 this means that vivj is allowed.
Let us introduce the operation of ousting a vertex u, defined as follows: Apply the algorithm of
Claim 5.10 to find an allowed edge uv incident on u, add uv to a constructed matching M, and
remove both u and v from the graph. Clearly, the resulting graph G still has a perfect matching
due to uv being allowed, so we can proceed on G
. Note that we can compute a suitable tree
decomposition of G by deleting u and v from the corresponding bags of the current tree-partition
decomposition, and removing bags that became empty. By Claim 5.10, any vertex can be ousted
within O(k2 · n) time and field operations.
We can now describe the whole algorithm. Let (T , {Bx }x ∈V (T ) ) be the given tree-partition decomposition of G, and let q = |V (T )|; as each bag can be assumed to be non-empty, we have
q ≤ n. Define a uniform measure μ on V (T ): μ(x) = 1 for each x ∈ V (T ). Using the algorithm of
Lemma 2.10, find in time O(q) a node x ∈ V (T ) such that each connected component of T − x
has at most q/2 nodes. Iteratively apply the ousting procedure to all the vertices of Bx ; each application boils down to solving a system of equations using O(k2 · n) time and field operations, so
all the ousting procedures in total use O(k3 · n) time and field operations. Note that in consecutive oustings, we work on a graph with fewer and fewer vertices, so after each ousting we again
resample the Tutte matrix of the current graph, and perform the Gaussian elimination again.
Let G be the graph after applying all the ousting procedures. For every subtree C ∈ cc(T − x),
let GC be the subgraph of G induced by the vertices of G that are placed in the bags of C. Then
graphs GC for C ∈ cc(T − x) are pairwise disjoint and non-adjacent, and their union is G
. Since
G has a perfect matching (by the properties of ousting), so does each GC. Observe that a treepartition decomposition TC of GC of width at most k can be obtained by inspecting C, removing
from each bag all the vertices not belonging toGC, and removing all the bags that became empty in
this manner. Hence, to retrieve a perfect matching ofG it suffices to apply the algorithm recursively
on all the instances (GC, TC), and take the union of the retrieved matchings together with the edges
gathered during ousting.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.   
34:36 F. V. Fomin et al.
We first argue that the whole algorithm runs in time O(k3 · n logn). Let T denote the recursion
tree of the algorithm—a rooted tree with nodes labelled by instances solved in recursive subcalls,
where the root corresponds to the original instance (G, T ) and the children of each node correspond to subcalls invoked when solving the instance associated with this node. Note that the
leaves of T correspond to instances where there is only one bag. Since the number of bags is
halved in each recursive subcall, and at the beginning we have q ≤ n bags, we immediately obtain
the following.
Claim 5.11. The height of T is at most log2 n.
We partition the work used by the algorithm among the nodes of T. Each node a is charged
by the work needed to (i) find the balanced node x, (ii) oust the vertices of Bx , (iii) construct
the subcalls (GC, TC) for C ∈ cc(T − x), and (iv) gather the retrieved matchings and the ousted
edges into a perfect matching of the graph. From the description above, it follows that if in the
instance associated with a node a the graph has na vertices, then the total work associated with a
is O(k3 · na ). Since subinstances solved at each level of the recursion correspond to subgraphs of
G that are pairwise disjoint, and since there are at most log2 n levels, the total work used by the
algorithm is O(k3 · n logn).
We now estimate the error probability of the algorithm. Since on each level Li all the graphs
associated with the subcalls are disjoint, we infer that the total number of subcalls is O(n logn).
In each subcall we apply ousting at most k times, and each time we resample new elements of F
to the Tutte matrix, which can lead to aborting the computations with probability at most 1
nc+4 in
case the sampling led to making the matrix singular. By union bound, no such event will occur
with probability at least 1 − k ·n log n
nc+4 ≥ 1 − 1
nc+1 . Together with the error probability of at most 1
nc+4
when initially reducing finding a maximum matching to finding a perfect matching, this proves
that the error probability is at most 1
nc+1 + 1
nc+4 ≤ 1
nc .
We can now put all the pieces together and prove Theorem 1.5, which we now restate for the
reader’s convenience.
Theorem 1.5. There exists an algorithm that, given a graph G together with its tree decomposition
of width at most k, uses O(k4 · n logn) time and field operations and computes a maximum matching
in G. The algorithm is randomized with one-sided error: it is correct with probability at least 1 − 1/nc
for an arbitrarily chosen constant c, and in the case of an error it reports a failure or a suboptimal
matching.
Proof. Apply the algorithm of Lemma 5.2 to construct a graphG together with a tree-partition
decomposition T  of G of width at most k; this takes time O(kn). Apply the algorithm of
Lemma 5.8 to construct a maximum matching M in G
; this uses O(k4 · n logn) time and field
operations, because G has O(kn) vertices. Using Lemma 5.2(iv), construct in time O(kn) a matching M in G of size at least |M
| − Λ/2, where Λ = |V (G
)|−|V (G)|. Observe now that M has to be
a maximum matching in G, because otherwise, by Lemma 5.2(iii), there would be a matching in
G of size larger than |M| + Λ/2 ≥ |M
|, which is a contradiction with the maximality of M
. In
case the algorithm of Lemma 5.8 reported failure or returned a suboptimal matching of G
, which
happens with probability at most 1
nc , the same outcome is given by the constructed procedure.
Finally, we remark that actually the number of performed inversions in the field in the algorithms of Theorems 1.4 and 1.5 is O(kn) and O(k2
n), respectively. Hence, if inversion is assumed
to cost O(logn) time, while all the other operations take constant time, the running times of
Theorems 1.4 and 1.5 can be stated as O(k3 · n + k · n logn) and O(k4 · n logn + k2 · n log2 n), respectively.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.  
Fully Polynomial-Time Parameterized Computations for Graphs 34:37
6 MAXIMUM FLOW
In this section, we prove Theorem 1.6. Let us remind one that our definition of (s,t)-vertex flows
corresponds to a family of vertex-disjoint paths from s to t; that is, we work with directed graphs
with unit capacities on vertices.
Theorem 1.6. There exists an algorithm that given an unweighted directed graph G on n vertices,
distinct terminals s,t ∈ V (G) with (s,t)  E(G), and a tree decomposition of G of width at most k,
works in time O(k2 · n logn) and computes a maximum (s,t)-vertex flow together with a minimum
(s,t)-vertex cut in G.
Proof. Assume without loss of generality that the given tree decomposition (T , {Bx }x ∈V (T ))
of G is clean (otherwise, clean it in linear time) and let q ≤ n be the number of its nodes. For a
graph H containing s and t, by κH (s,t) we will denote the maximum size of a flow from s to t in
H.
Let L be the set of all the nodes x ∈ V (T ) for which {s,t} ⊆ Bx ; note that L can be constructed in
time O(kq). By the properties of a tree decomposition, L is either empty or it induces a connected
subtree of T . Let  = |L|. We will design an algorithm with running time O(k2 · n log( + 2)),
which clearly suffices, due to  ≤ q ≤ n.
First, we need to introduce several definitions and prove some auxiliary claims.
Claim 6.1. If  = 0, then κG (s,t) ≤ k.
Proof. The assumption that  = 0 means that there is no bag where s and t appear simultaneously. Since T [s] and T [t] are disjoint connected subtrees of T , it follows that there is an edge
xy of T whose removal disconnects T into subtrees Tx (containing x) and Ty (containing y) such
that T [s] ⊆ Tx and T [t] ⊆ Ty . By the properties of a tree decomposition, we have that Bx ∩ By is
an (s,t)-vertex separator. Since Bx  By due to T being clean, we have that |Bx ∩ By | ≤ k. Hence,
in particular, κ(s,t) ≤ |Bx ∩ By | ≤ k.
Suppose now that  > 0 and let us fix some arbitrary node x belonging to L. Let us examine a
component C ∈ cc(T − x), which is a subtree of T . LetWC consist of all the vertices u ∈ V (G) for
which T [u] ⊆ C, and let yC be the (unique) neighbor node of x contained in C. We call subtree
C important if L ∩V (C)  ∅, that is, C has at least one node that contains both s and t; since L
induces a connected subtree of T , this is equivalent to yC ∈ L. Otherwise, C is called unimportant.
A path P from s to t is called expensive if V (P) ∩ (Bx \ {s,t})  ∅, that is, P traverses at least one
vertex of Bx other than s and t; otherwise, P is called cheap.
Claim 6.2. For any x ∈ L and any (s,t)-vertex flow F , the number of expensive paths in F is at
most k − 1.
Proof. The paths from F are internally vertex-disjoint and each of them traverses a vertex of
Bx \ {s,t}. Since |Bx \ {s,t}| ≤ k − 1, the claim follows.
Claim 6.3. For any x ∈ L and any cheap (s,t)-path P, there exists a subtree CP ∈ cc(T − x) such
that V (P) ⊆ WCP ∪ {s,t}. Moreover, CP is important.
Proof. By the properties of a tree decomposition, we have that the vertex set of each connected
component of G − Bx is contained in WC for some subtree C ∈ cc(T − x). Since P is cheap, we
have that V (P) ∩ Bx = {s,t}, and hence all the internal vertices of P have to belong to the same
connected component H of G − Bx . Therefore, we can take CP to be the connected component of
T − x for which V (H) ⊆ WCP . It remains to show that CP is important.
For the sake of contradiction, suppose CP is not important. Lety = yCP ; then {s,t}  By . Without
loss of generality, suppose t  By , as the second case is symmetric. Since t ∈ Bx , this means that
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.              
34:38 F. V. Fomin et al.
no bag of CP containst. Consequently, there is no edge from any vertex ofWCP to t inG. However,
all the internal vertices of P are contained in WCP , which is a contradiction.
We can now describe the whole algorithm. First, the algorithm computes L in time O(kq). If
L = ∅, then by Claim 6.1 we have that κG (s,t) ≤ k. Hence, by starting with an empty flow and
applying at most k times flow augmentation (Lemma 2.1) we obtain both a maximum (s,t)-vertex
flow and a minimum (s,t)-vertex cut in time O(k · (n + m)) = O(k2 · n). Therefore, suppose that
L  ∅.
The crux of our approach is to take x to be a node that splits L in a balanced way. For this,
Lemma 2.8 will be useful. Define measure μL on V (T ) as follows: for x ∈ V (T ), let μL (x) = 1 if
x ∈ L and μL (x) = 0 otherwise. Since L  ∅, μL is indeed a measure. Run the algorithm of Lemma 2.8
on T with measure μL; let x be the obtained balanced node. Then for each C ∈ cc(T − x) we have
that |L ∩V (C)| = μL (V (C)) ≤ μL (V (T ))/2 = |L|/2. In the following, the notions of expensive and
cheap components refer to components of T − x.
For each C ∈ cc(T − x), decide whether C is important by checking whether {s,t} ⊆ ByC ; this
takes at most O(kq) time in total. Let I ⊆ cc(T − x) be the family of all the important subtrees of
T − x. For each C ∈ I, construct an instance (GC,s,t, TC) of the maximum vertex flow problem as
follows: takeGC = G[WC ∪ {s,t}], construct its tree decomposition TC from C by removing all the
vertices not contained inWC ∪ {s,t} from all the bags of C, and make it clean in linear time. Clearly,
TC constructed in this manner is a tree decomposition of GC of width at most k, and has at most
half as many bags containing both s and t as T . Moreover, a straightforward implementation of
the construction of all the instances (GC,s,t, TC) for C ∈ I runs in total time O(kn), due to q ≤ n.
Now, apply the algorithm recursively to all the constructed instances (GC,s,t, TC), for all C ∈ I.
Each application returns a maximum (s,t)-vertex flow FC in GC. Observe that all the internal
vertices of all the paths of FC are contained in WC, and all the sets WC for C ∈ I are pairwise
disjoint. Therefore, taking F apx = C ∈I FC defines an (s,t)-vertex flow, because the paths of F apx
are internally vertex-disjoint.
Now comes the crucial observation: Fapx is not far from the maximum flow.
Claim 6.4. |F apx | ≥ κG (s,t) − (k − 1).
Proof. Let F ◦ be a maximum (s,t)-vertex flow. Partition F ◦ as F ◦
exp  C ∈I F ◦
C , where F ◦
exp
is the set of the expensive paths from F ◦, whereas for C ∈ I we define F ◦
C to be the set of cheap
paths from F ◦ whose internal vertices are contained in WC. Claim 6.3 ensures us that, indeed,
each path of F ◦ falls into one of these sets. By Claim 6.2 we have that
|F ◦
exp| ≤ k − 1. (1)
On the other hand, for each C ∈ I we have that
|FC | = κGC (s,t) ≥ |F ◦
C |. (2)
By combining Equations (1) and (2), we infer that
|F apx | =

C ∈I
|FC | ≥ 
C ∈I
|F ◦
C | = |F ◦| − |F ◦
exp| ≥ κG (s,t) − (k − 1).
From Claim 6.4 it follows that in order to compute a maximum (s,t)-vertex flow and a minimum (s,t)-vertex cut in G, it suffices to start with the flow F apx and apply flow augmentation
(Lemma 2.1) at most k − 1 times. This takes time O(k2 · n), since by Lemma 2.6, G has at most kn
edges. The algorithm clearly terminates, because the number of bags containing both s and t is
strictly smaller in each recursive subcall. From the description it is clear that it correctly reports a
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.    
Fully Polynomial-Time Parameterized Computations for Graphs 34:39
maximum (s,t)-vertex flow and a minimum (s,t)-vertex cut in G. We are left with estimating the
running time of the algorithm.
Let T denote the recursion tree of the algorithm—a rooted tree with nodes labeled by instances
solved in recursive subcalls, where the root corresponds to the original instance (G,s,t, T ) and the
children of each node correspond to subcalls invoked when solving the instance associated with
this node. Note that the leaves of T correspond to instances where there is only one bag containing
both s and t. Since the number of bags containing both s and t is halved in each recursive subcall,
we immediately obtain the following.
Claim 6.5. The height of T is at most log2 .
Let d ≤ log2  be the height of T, and let Li be the set of nodes of T contained on level i (at
distance i from the root), for 0 ≤ i ≤ d.
Similarly as in the proof of Theorem 1.1, we partition the work used by the algorithm among
the nodes of T. Each node a is charged by the work needed to (i) find the balanced node x, (ii)
investigate the connected components of T − x and find the important ones, (iii) construct the
subcalls (GC,s,t, TC) for C ∈ I, (iv) construct F apx by putting together the flows returned from
subcalls, and (v) run at most k − 1 iterations of flow augmentation to obtain a maximum flow and
a minimum cut. From the description above it follows that if in the instance associated with a node
a the graph has na vertices (excluding s and t), then the total work associated with a is O(k2 · na ).
Note that here we use the fact that na > 0, because in every subproblem solved by the algorithm
there is at least one vertex other than s and t. This follows from the fact that each leaf of a clean
decomposition in any subcall contains a vertex that does not belong to the leaf’s parent (or any
other bag).
Obviously, each subinstance solved in the recursion corresponds to some subgraph of G. For
a level i, 0 ≤ i ≤ d, examine all the nodes a ∈ Li . Observe that in the instances corresponding to
these nodes, all the considered subgraphs share only s and t among the nodes of G. This proves
that 
a ∈Li na ≤ n. Consequently, the total work associated with the nodes of Li is 
a ∈Li O(k2 ·
na ) ≤ O(k2 · n). By Claim 6.5 the number of levels is at most log2 , so the total work used by the
algorithm is O(k2 · n log ) ≤ O(k2 · n logn).
As mentioned in the Introduction, the single-source single-sink result of Theorem 1.6 can be
easily generalized to the multiple-source multiple-sink setting. When we want to compute a maximum (S,T )-vertex flow together with a minimum (S,T )-vertex cut, it suffices to collapse the whole
sets S and T into single vertices s and t, and apply the algorithm to s and t. It is easy to see that
this operation increases the treewidth of the graph by at most 2, because the new vertices can be
placed in every bag of the given tree decomposition. Similarly, in the setting when the minimum
cut can contain vertices of S ∪T , which corresponds to finding the maximum number of completely vertex-disjoint paths from S to T , it suffices to add two new vertices s and t, and introduce
edges (s,u) for all u ∈ S and (v,t) for all T . Again, the new vertices can be placed in every bag of
the given tree decomposition, which increases its width by at most 2.
7 CONCLUSIONS
In this work, we have laid foundations for a systematic exploration of fully polynomial FPT algorithms on graphs of low treewidth. For a number of important problems, we gave the first such
algorithms with linear or quasi-linear running time dependence on the input size, which can serve
as vital primitives in future works. Of particular interest is the new pivoting scheme for matrices
of low treewidth, presented in Section 4, and the general Divide & Conquer approach based on
pruning a balanced bag, which was used for reconstructing a maximum matching (Theorem 1.5)
and computing the maximum vertex flow (Theorem 1.6).
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.     
34:40 F. V. Fomin et al.
We believe that this work is but a first step in a much larger program, since our results raise
a large number of very concrete research questions. In order to facilitate further discussion, we
would like to state some of them explicitly in this section.
In Section 3, we have designed an approximation algorithm for treewidth that yields an O(OPT )-
approximation in time O(k7 · n logn). We did not attempt to optimize the running time dependence
on k; already a better analysis of the current algorithm shows that the recursion tree in fact has
depth O(k · logn), which gives an improved running time bound of O(k6 · n logn). It is interesting
whether this dependence on k can be reduced significantly, say to O(k3). However, we believe that
much more relevant questions concern improving the approximation factor and removing the logn
factor from the running time bound.
Q1. Is there an O(OPT )-approximation algorithm for treewidth with running time O(k3 ·
n logn)?
Q2. Is there an O(logc OPT )-approximation algorithm for treewidth with running time O(kd ·
n logn), for some constants c and d?
Q3. Is there an O(OPTc )-approximation algorithm for treewidth with running time O(kd · n),
for some constants c and d?
In Section 4, we have presented a new pivoting scheme for Gaussian elimination that is based
on the prior knowledge of a suitable decomposition of the row-column incidence graph. The
scheme works well for decompositions corresponding to parameters pathwidth and tree-partition
width, but for treewidth it breaks. We can remedy the situation by reducing the treewidth case to
the tree-partition width case using ideas originating in the sparsification technique of Alon and
Yuster [3], but this incurs an additional factor k to the running time. Finally, there has been a lot
of work on improving the running time of Gaussian elimination using fast matrix multiplication;
in particular, it can be performed in time O(nω ) on general graphs [12] and in time O(nω/2) on
sparse graph classes admitting with O(
√
n) separators [3], like planar and H-minor-free graphs.
When we substitute k = n or k = n1/2 in the running time of our algorithms, we fall short of these
results.
Q4. Can a PLUQ-factorization of a given matrix be computed using O(k2
n) arithmetic operations also when a tree decomposition of width k is given, similarly as for path and treepartition decompositions?
Q5. Can the techniques of Hopcroft and Bunch [12] be used to obtain an O(kc · n)-time algorithm for computing, say, the determinant of a matrix of treewidth k, so that the running
time would match O(nω ) whenever k = Θ(n), and O(nω/2) whenever the matrix has a planar graph and k = Θ(n1/2)?
In Section 5, we presented how our algebraic results can be used to obtain fully polynomial FPT
algorithms for finding the size and constructing a maximum matching in a graph of low treewidth,
where the running time dependence on the size of the graph is almost linear. In both cases, we
needed to perform computations in a finite field of size poly(n), which resulted in an unexpected
technicality: the appearance of an additional logn factor, depending on the computation model. We
believe that this additional factor should not be necessary. Also, when reconstructing the matching
itself, we used an additional O(k logn) factor. Perhaps more importantly, we do not see how the
presented technique can be extended to the weighted setting, even in the very simple case of only
having weights 1 and 2.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
Fully Polynomial-Time Parameterized Computations for Graphs 34:41
Q6. Can one find the size of a maximum matching in a graph without the additional logn factor
incurred by algebraic operations in a finite field of size poly(n)?
Q7. Can one construct a maximum matching in a graph of low treewidth in the same time as
for finding its size?
Q8. Can one compute a maximum matching in a weighted graph in time O(kc · n logn), where
k is the width of a given tree decomposition and c is some constant, at least for integer
weights?
In Section 6, we showed how a Divide & Conquer approach can be used to design algorithms for
finding maximum vertex flows in low treewidth graphs with quasi-linear running time dependence
on the size of the graph. Our technique is crucially based on the fact that we work with unweighted
vertex flows, because we use the property that the size of a bag upper bounds the number of paths
that can use any of its vertices. Hence, we do not see how our techniques can be extended to the
setting with capacities on vertices, or to edge-disjoint flows. Of course, there is also a question of
removing the logn factor from the running time bound.
Q9. Is there an algorithm for computing a maximum (s,t)-vertex flow in a directed graph with
a tree decomposition of width k that would run in time O(kc · n) for some constant c?
Q10. Can one compute a maximum (s,t)-vertex flow in a (directed) graph with capacities on
vertices in time O(kc · n), where k is the width of a given tree decomposition and c is some
constant?
Q11. Can one compute a maximum (s,t)-edge flow in a (directed) graph in time O(kc · n logn),
where k is the width of a given tree decomposition and c is some constant? What about
edge capacities?
Of course, one can look for other cases when developing fully polynomial FPT algorithms can
lead to an improvement over the fastest known general-usage algorithms when the given instance
has low treewidth. Let us propose one important example of such a question.
Q12. Can one design an algorithm for Linear Programming that would have running time
O(kc · (n + m) log(n + m)) for some constant c, when the n × m matrix of the given program has treewidth at most k?
We remark that Giannopoulou et al. [36] proposed the following complexity formalism for fully
polynomial FPT algorithms. For a polynomial function p(n), we say that a parameterized problem
Π is in P-FPT(p(n)) (for polynomial-FPT) if it can be solved in time O(kc · p(n)), where k is the
parameter and c is some constant. The class P-FPT(n) for p(n) = n is called PL-FPT (for polynomiallinear FPT). We can also define class PQL-FPT as 
d ≥1 P-FPT(n logd n), that is, PQL-FPT comprises
problems solvable in fully polynomial FPT time where the dependence on the input size is quasilinear. In this work, we were not interested in studying any deeper complexity theory related to
fully polynomial FPT algorithms, but our algorithms can be, of course, interpreted as a fundamental
toolbox of positive results for PL-FPT and PQL-FPT algorithms for the treewidth parameterization.
On the other hand, the results of Abboud et al. [1] on Radius and Diameter are the first attempts of
building a lower bound methodology for these complexity classes. Therefore, further investigation
of the complexity theory related to P-FPT classes looks like a very promising direction.
To make our algorithms practical, further work is necessary. For computing treewidth, our results are deeply impractical and we can only hope the new insights may inspire future approaches.
Only very recently, the PACE challenge [23, 24] kickstarted research into practical implementations for computing tree decompositions (see [41, 67, 68, 71]). Our techniques on matrices rely
on a simple variant of Gaussian elimination (and a new analysis thereof). However, as far as
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 34. Publication date: June 2018.
34:42 F. V. Fomin et al.
we know, practical approaches rely on additional techniques including, above all, parallelization
on dedicated GPUs, hence we believe it is an interesting research direction to explore parallel
parameterized algorithms in a similar fashion. The same question applies to algorithms relying
on matrix computations. As for the divide and conquer algorithm for vertex flow, we believe the
technique to be simple enough that a direct implementation should not incur excessively high
constants, and may thus already be applicable, although a decent tree decomposition would have
to be available first.
Very recently, Iwata et al. [44] answered some of the above questions by showing a divide and
conquer algorithm for finding maximum matchings in time O(k · m logn) and weighted matchings
in time O(k · m log2 n). Coudert et al. [20] considered the parameter clique-width and related ones
such as modular-width, in place of treewidth. Finally, Mertzios et al. [52] introduced linear-time
kernelization algorithms for maximum matching, outputting an equivalent instance of size poly(k)
(although for parameters weaker than treewidth). Such a reduced instance can be solved with any
other technique, in particular, yielding running times of the form O(n + m + poly(k)).