Submodular function maximization has been studied extensively in recent years under various constraints
and models. The problem plays a major role in various disciplines. We study a natural online variant of this
problem in which elements arrive one by one and the algorithm has to maintain a solution obeying certain
constraints at all times. Upon arrival of an element, the algorithm has to decide whether to accept the element
into its solution and may preempt previously chosen elements. The goal is to maximize a submodular function
over the set of elements in the solution.
We study two special cases of this general problem and derive upper and lower bounds on the competitive
ratio. Specifically, we design a 1/e-competitive algorithm for the unconstrained case in which the algorithm
may hold any subset of the elements, and constant competitive ratio algorithms for the case where the algorithm may hold at most k elements in its solution.
CCS Concepts: • Mathematics of computing → Combinatorial optimization; • Theory of computation → Online algorithms; Streaming, sublinear and near linear time algorithms;
Additional Key Words and Phrases: Online algorithms, submodular maximization, competitive analysis,
preemption
1 INTRODUCTION
Submodular function maximization has been studied extensively in recent years under various
constraints and models. Submodular functions, which capture well the intuitive idea of diminishing returns, play a major role in various disciplines, including combinatorics, optimization, economics, information theory, operations research, algorithmic game theory, and machine learning
Submodular maximization captures well known combinatorial optimization problems such as:
Max-Cut [31, 35, 38, 39, 52], Max-DiCut [21, 31, 33], Generalized Assignment [12, 13, 24, 28],
and Max-Facility-Location [2, 15, 16]. Additionally, one can find submodular maximization problems in many other settings. In machine learning, maximization of submodular functions has been
used for document summarization [45, 46], sensor placement [40, 41, 43], and information gathering [42]. In algorithmic game theory, calculating market expansion [17] and computing core values
of certain types of games [51] are two examples where the problem can be reduced to submodular
maximization.
It is natural to consider online variants of submodular maximization such as the following setting. There is an unknown ground set of elements N = {u1,u2,...,un }, a non-negative submodular
function f : 2N → R+ and perhaps a constraint determining the feasible sets of elements that may
be chosen. The elements of N arrive one by one in an online fashion. Upon arrival, the online algorithm must decide whether to accept each revealed element into its solution and this decision
is irrevocable. As in the offline case, the algorithm has access to the function f via a value oracle,
but in the online case it may only query subsets of elements that have already been revealed. More
concretely, one may think of items as goods arriving online one by one. The algorithm may accept
goods so as to maximize a (possibly non-monotone) submodular function of the chosen goods.
Although natural, it is not difficult to see that no algorithm has a constant competitive ratio for
the above naïve model even with a simple cardinality constraint.1 Therefore, in order to obtain
meaningful results, one must relax the model. One possible relaxation is to consider a random
arrival model in which elements arrive at a random order [7, 25, 32]. This relaxation leads to
“secretary-type” algorithms. We propose here a different approach that still allows for an adversarial arrival. Specifically, we allow the algorithm to reject (preempt) previously accepted elements.
Preemption appears as part of many online models (see, e.g., [1, 18, 19, 29]). The use of preemption
in our model is mathematically elegant and fits well with natural scenarios.
1.1 Our Results
We study online submodular maximization problems with two simple constraints on the feasible
sets. The unconstrained case in which no restrictions are imposed on the sets of elements that the
algorithm may choose, and the cardinality constraint in which the online algorithm may hold at
any time at most k elements. We provide positive results as well as hardness results. Our hardness
results apply both to polynomial- and non-polynomial-time algorithms, but all our algorithms
are polynomial (in some of them, a polynomial-time implementation loses an additive ε in the
competitive ratio).
The Unconstrained Case. The first special case we consider is, arguably, the most basic case in
which there is no constraint on the sets of elements that may be chosen. In this case, the problem is trivial if the submodular function f is monotone2 as the algorithm may take all elements.
Therefore, we only consider non-monotone (and non-negative) functions f . As noted above, in the
presence of a constraint, no constant approximation ratio can be guaranteed when preemption is
1To see that, assume that the algorithm is allowed to pick only a single element and that the adversary uses the following
strategy. For a fixed choice of positive integer constants c and k, the adversary sends the algorithm up to k + 1 elements,
where the ith element is of value ci and is sent to the algorithm if and only if each one of the elements sent to the algorithm
before it has been accepted with probability at least 1/k. Clearly this strategy guarantees that the algorithm accepts the
last element sent by the adversary with probability at most 1/k, and since this element is heavier than all the previously
sent elements by a factor of at least c, the competitive ratio of the algorithm cannot be better than 1/k + 1/c, which can
be made arbitrarily small by picking c and k to be large enough. 2A set function f : 2N → R is monotone if A ⊆ B ⊆ N implies f (A) ≤ f (B).
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.
Online Submodular Maximization with Preemption 30:3
not allowed. However, in the absence of a constraint, there are such algorithms. Specifically, Feige
et al. [23] proved that the algorithm that selects each revealed element independently with probability 1/2, and does not use preemption, is 1/4-competitive. Nevertheless, we prove in the next
theorem that this simple algorithm is essentially optimal without using the additional power of
preemption.
Theorem 1.1. An online algorithm using no preemption cannot be (1/4 + ε)-competitive for any
constant ε > 0, even if f is guaranteed to be a cut function of a weighted directed graph.
On the positive side, we prove that preemption is beneficial for unconstrained-model.
3
Theorem 1.2. There exists a 1/e-competitive algorithm for unconstrained-model, which can
be implemented in polynomial time at the cost of an additive ε loss in the competitive ratio (for an
arbitrary small constant ε > 0).
A special case of unconstrained-model that we study is dicut-model. In this model, every set
is feasible and the objective function f is a cut function of a weighted directed graph G = (V,A)
having N ⊆ V (i.e., a subset of the nodes of G form the ground set). We assume an algorithm for
dicut-model knows V (but not N ) and can query the weight (and existence) of every arc leaving a revealed node. Note that this means that the algorithm can, in particular, query the weight
of arcs going from revealed nodes to nodes that are still unrevealed. This ability might feel a
bit unnatural, but it is crucial in order to keep this model as a special case of our more general
unconstrained-model. It is also important to observe that the information available to the algorithm is sufficient for calculating the value of f for every set of revealed elements, and thus, there
is no need for the algorithm to have also oracle access to f .
The dicut-model can be viewed as an online model of the well-known Max-DiCut problem (see Section 1.2 for a discussion of another online model of Max-DiCut). Additionally, since
dicut-model is a special case of unconstrained-model (with more power for the algorithm), it
inherits the positive result given by Theorem 1.2. The next theorem gives a stronger result.
Theorem 1.3. There exists a polynomial time 0.483-competitive algorithm for dicut-model.
Theorem 1.3 is proved by showing that an offline algorithm suggested by [22] can be implemented under dicut-model. We complement Theorems 1.2 and 1.3 by the following theorem
which gives hardness results for dicut-model (and thus, also for unconstrained-model).
Theorem 1.4. No randomized (deterministic) algorithm for dicut-model has a competitive ratio
better than 4/5 (
5−
√
17
2 ≈ 0.438).4
Notice that, for polynomial-time algorithms, a hardness result of 1/2 proved by [23] for offline algorithms extends immediately to unconstrained-model. For dicut-model, there exists a
polynomial-time 0.874-approximation offline algorithm [44], thus, polynomial-time offline algorithms are strictly stronger than online algorithms in this model.
Cardinality Constraint. The second case we consider is cardinality-model in which a set is
feasible if and only if its size is at most k, for some parameter 0 ≤ k ≤ n. Our positive results for
cardinality-model are summarized by the following theorem:
3As usual in online algorithms, we assume that the solution maintained by the algorithm must be competitive throughout
the execution (because the input might end at every point). This assumption is necessary to exclude trivialities such as an
algorithm that originally keeps all the elements and then removes unnecessary elements after viewing the last element.
4Theorem 1.4 holds even if we allow the algorithm access to all arcs of G, including arcs leaving unrevealed elements.
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.
30:4 N. Buchbinder et al.
Table 1. Summary of the Results for cardinality-model
Monotone Objective General Objective
Deterministic Randomized Deterministic Randomized
Algorithm 1/4 [11, 53] 1/4 [11, 53] − 56
627 ≈ 0.0893 (1.5)
Hardness 1/2 + ε (1.6) 3/4 + ε (1.6) 5−
√
17
2 ≈ 0.438 (1.4) 1/2 + ε (1.6)
Polynomial 1/2 + ε (1.6) 1 − 1/e [49] 5−
√
17
2 ≈ 0.438 (1.4) 0.491 [30]
time hardness
To the right of each result appears the number of the theorem (or references) proving it.
Theorem 1.5. There exists a randomized (56/627 ≈ 0.0893)-competitive algorithm for
cardinality-model, which can be implemented in polynomial time at the cost of an additive ε loss
in the competitive ratio (for an arbitrary small constant ε > 0). Moreover, if the objective function
is restricted to be monotone, then there exists a polynomial-time deterministic (1/4)-approximation
algorithm for this model.
The second part of Theorem 1.5 follows from the works of Badanidiyuru Varadaraja [53] and
Chakrabarti and Kale [11]. More specifically, these works present a streaming 1/4-competitive
algorithm for maximizing a monotone submodular function subject to a cardinality constraint,5
and this algorithm can be implemented also under cardinality-model. We describe a different
1/4-competitive algorithm for monotone objectives under cardinality-model, and then use additional ideas to prove the first part of Theorem 1.5.
It is interesting to note that both algorithms guaranteed by Theorem 1.5 can be implemented
in the streaming model of Chakrabarti and Kale [11]. Thus, we give also the first algorithm for
maximizing a general non-negative submodular function subject to a cardinality constraint under
this streaming model.
On the negative side, notice that cardinality-model generalizes unconstrained-model (by
setting k = n). Hence, both hardness results given by Theorem 1.4 extend to cardinality-model.
The following theorem gives a few additional hardness results for this model.
Theorem 1.6. No algorithm for cardinality-model is (1/2 + ε)-competitive for any constantε >
0. Moreover, even if the objective function is restricted to be monotone, no randomized (deterministic)
algorithm for cardinality-model is (3/4 + ε)-competitive ((1/2 + ε)-competitive).
Notice that polynomial-time hardness results of 0.491 and 1 − 1/e for cardinality-model
and cardinality-model with a monotone objective follow from Gharan and Vondrák [30]
and Nemhauser and Wolsey [49], respectively. All the results for cardinality-model are summarized in Table 1.
1.2 Related Work
The literature on submodular maximization problems is very large, and therefore, we mention
below only a few of the most relevant works. The classical result of Nemhauser et al. [50] states
that the simple discrete greedy algorithm is a (1 − 1/e)-approximation algorithm for maximizing
a monotone submodular function subject to a cardinality constraint. This result is known to be
tight [49], even in the case where the objective function is a coverage function [20]. However,
when one considers submodular objectives which are not monotone, less is known. An approximation of 0.309 was given by [55], which was later improved to 0.325 [30] using a simulated
5In fact, the algorithm of [53] and [11] is (4m)
−1-competitive for the more general constraint of m-matroids intersection.
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.
Online Submodular Maximization with Preemption 30:5
annealing technique. Extending the continuous greedy algorithm of Calinescu et al. [10] to general non-negative submodular objectives, Feldman et al. [27] obtained an improved approximation
of 1/e − o(1). Finally, Buchbinder et al. [9] gave a fast 1/e-approximation algorithm and a much
slower (1/e + 0.004)-approximation algorithm, demonstrating that 1/e is not the right approximation ratio for the problem. On the hardness side, it is known that no polynomial-time algorithm
can have an approximation ratio better than 0.491 [30].
For the problem of maximizing a non-negative submodular function subject to no constraints,
the first approximation algorithm was presented by Feige et al. [23] who gave a 2/5-approximation.
This was improved in Gharan and Vondrák [30] and Feldman et al. [26] to 0.41 and 0.42, respectively. Finally, Buchbinder et al. [8] described a 1/2-approximation linear time algorithm, matching
the hardness result given by Feige et al. [23]. Huang and Borodin [36] study an online model of this
problem where the algorithm can access both f and ¯f . This model is powerful enough to implement the randomized 1/2-approximation algorithm of Buchbinder et al. [8], and therefore, Huang
and Borodin [36] consider only deterministic algorithms. Azar et al. [3] consider the Submodular Max-SAT problem, and provide a 2/3-approximation algorithm for it that can be implemented
also in a natural online model. We are not aware of any additional works on online models of
submodular maximization.
For the problem of Max-DiCut, Goemans and Williamson [31] obtained 0.796-approximation using semi-definite programming. This was improved through a series of works [21, 44, 47] to 0.874.
On the hardness side, a (12/13 + ε)-approximation algorithm will imply P = NP [35]. Assuming the
Unique Games Conjecture, the best possible approximation for Max-Cut is 0.878 [39, 48], and this
hardness result holds also for Max-DiCut since the last generalizes Max-Cut.
Bar-Noy and Lampis [6] gave a (1/3)-competitive deterministic algorithm for an online model of
Max-Cut where every revealed node is accompanied by its input and output degrees. For the case
of a directed acyclic graph, they provide an improved deterministic algorithm with a competitive
ratio of 2/31.5 ≈ 0.385, which is optimal against an adaptive adversary. Huang and Borodin [36]
noticed that the (1/3)-competitive deterministic algorithm of Bar-Noy and Lampis [6] is in
fact identical to the (1/3)-approximation deterministic algorithm of Buchbinder et al. [8] for
unconstrained submodular maximization. Using the same ideas, it is not difficult to show that the
(1/2)-approximation randomized algorithm of Buchbinder et al. [8] implies a (1/2)-competitive
algorithm in this online model. Finally, Feige and Jozeph [22] considered oblivious algorithms
for Max-DiCut—algorithms in which every node is selected into the cut independently with a
probability depending solely on its input and output degrees. They showed a 0.483-approximation
oblivious algorithm, and proved that no oblivious algorithm has an approximation ratio of 0.4899.
Finally, the vast literature on buyback problems considers problems that are similar to our model,
but assume a linear objective function. Many of these problems are non-trivial only when preemption has a cost, which is usually assumed to be either constant or linear in the value of the preempted element. The work from this literature most closely related to ours is the work of Babaioff
et al. [4, 5] who considered a matroid constraint. For other buyback results, see, e.g., [14, 34, 37,
53, 54].
Organization of the Article. Section 2 defines additional notation. Section 3 gives our results
for unconstrained-model and dicut-model, except for Theorem 1.1 whose proof is deferred to
Appendix A. Finally, Section 4 describes our results for cardinality-model.
2 PRELIMINARIES
We study the following online variant of the submodular maximization problem. There is an
unknown ground set of elements N = {u1,u2,...,un }, a non-negative submodular function
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.
30:6 N. Buchbinder et al.
f : 2N → R+ and a down-closed6 collection of feasible sets I ⊆ 2N . The objective of the instance
is to find a feasible set maximizing f . The elements of N are revealed one by one. The algorithm
creates n + 1 feasible solutions: S0, S1,..., Sn (each Si ∈ I). The solution S0 is the empty set ∅.
For every 1 ≤ i ≤ n, Si is the solution selected by the algorithm immediately after element ui is
revealed and the algorithm can choose it to be any feasible subset of Si−1 + ui .
7 It is important to
note that the algorithm does not know n (the size of the ground set) in advance; hence, the input
might end after every element from the algorithm’s point of view.
3 The UNCONSTRAINED-MODEL AND DICUT-MODEL
Our positive results for unconstrained-model and dicut-model (i.e., Theorems 1.2 and 1.3) are
proved in Section 3.1. The negative result for dicut-model (Theorem 1.4), which applies also to
unconstrained-model, is proved in Section 3.2.
3.1 Algorithms for unconstrained-model and dicut-model
Before describing our algorithm for unconstrained-model, we need some notation. For two vectors x,y ∈ [0, 1]N , we use x ∨y and x ∧y to denote the coordinate-wise maximum and minimum,
respectively, of x and y (formally, (x ∨y)u = max{xu,yu } and (x ∧y)u = min{xu,yu }). We abuse
notation both in the description of the algorithm and in its analysis, and unify a set with its characteristic vector and an element with the characteristic vector of the singleton set containing it.
The multilinear extension of a set function f : 2N → R+ is a function F : [0, 1]N → R+ defined by
F (x) = E[f (R(x))], where R(x) is a random set containing every element u ∈ N with probability
xu , independently. The multilinear extension is an important tool used in many previous works
on submodular maximization (see, e.g., [10, 27, 55]). We denote by ∂u F (x) the derivative of F at
point x with respect to the coordinate corresponding to u. It can be checked that F is a multilinear
function, and thus:
∂u F (x) = F (x ∨u) − F (x ∧ (N − u))
(note that we have used here the above-mentioned abuse of notation, i.e., F (x ∨u) and F (x ∧ (N −
u)) should be understood as F (x ∨ 1{u }) and F (x ∧ 1N −u ), respectively, where 1{u } and 1N −u are
the characteristic vectors of the sets {u} and N − u, respectively). Moreover, when xu < 1, it also
holds that
∂u F (x) = F (x ∨u) − F (x)
1 − xu
.
Consider Algorithm 1. Recall that Si is the solution that the algorithm produces after seeing
element ui .
ALGORITHM 1: Marginal Choice
1 foreach element ui revealed do
2 Choose a uniformly random threshold θi ∈ [0, 1].
3 Let Ni ← {u1,u2,...,ui}.
4 Let Si ← {uj ∈ Ni | ∂uj F (θj · Ni ) ≥ 0}.
Our first objective is to show that Algorithm 1 is an online algorithm according to
unconstrained-model.
6A collection I of sets is down-closed if A ⊆ B ⊆ N and B ∈ I imply A ∈ I. 7Given a set S and an element u, we use S + u and S − u as shorthands for S ∪ {u } and S \ {u }, respectively.
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.
Online Submodular Maximization with Preemption 30:7
Lemma 3.1. For every 1 ≤ i ≤ n, Si ⊆ Si−1 + ui .
Proof. By definition, Si contains only elements of Ni . Fix an element uj ∈ Ni − ui . Then:
uj ∈ Si ⇒ ∂uj F (θj · Ni ) ≥ 0 ⇒ ∂uj F (θj · Ni−1) ≥ 0 ⇒ uj ∈ Si−1,
where the second derivation follows from submodularity.
Next, we bound the competitive ratio of Algorithm 1. Fix an element ui ∈ N . By submodularity,
д(z) = ∂ui F (z · N ) is a continuous non-increasing function of z (formally, this holds because the
submodularity of f implies that all the second order partial derivatives of F are non-positive [10]).
Hence, by the intermediate value theorem, one of the following must hold: д(z) is always positive
in the range [0, 1], д(z) is always negative in the range [0, 1] or д(z) has at least one root z0 ∈
[0, 1]. In the last case, the set I0 (ui ) ⊆ [0, 1] of the roots of д(z) is non-empty. Moreover, by the
monotonicity and continuity of д(z), I0 (ui ) is a closed range. Using these observations, we define
a vector yˆ ∈ [0, 1]N as follows: For every element ui ∈ N ,
yˆui =
⎧⎪⎪
⎨
⎪⎪
⎩
0 if ∂ui F (0 · N ) < 0,
1 if ∂ui F (1 · N ) > 0,
max I0 (ui ) otherwise.
Remark. Notice that the case ∂ui F (0 · N ) < 0 in the definition of yˆ can happen only when
f (∅) > 0.
Observation 3.2. Every element ui ∈ N belongs to Sn with probability yˆui , independently. Hence,
E[f (Sn )] = F (yˆ).
Proof. An element ui ∈ N belongs to Sn if and only if ∂ui F (θi · N ) ≥ 0, which is equivalent to
the condition θi ≤ yˆui . Clearly, the last condition happens with probability yˆui , and is independent
for different elements.
The last observation implies that analyzing Algorithm 1 is equivalent to lower bounding F (yˆ).
One way to think of yˆ is to view it as the vector that would have been obtained by the continuous greedy algorithm of [10] if one tried to solve the offline problem using it, and the algorithm
was modified to use at time t the partial derivative ∂u F (t · N ) as the weight of element u instead
of ∂u F (y ∧ (t · N )). Accordingly, Corollary 3.5 lower bounds F (yˆ) by combining ideas from the
analysis of [10] for continuous greedy with ideas from [27] regarding the extension of this analysis to non-monotone functions. The following lemma is a technical claim which is necessary for
handling the difference between the weights used in the construction of yˆ according to the above
view and the weights that would have been used by a real execution of continuous greedy.
Lemma 3.3. For every λ ∈ [0, 1], F (yˆ ∧ (λ · N )) ≥ F (λ · N ).
Proof. Observe that:
F (yˆ ∧ (λ · N )) = f (∅) +
 λ
0
dF (yˆ ∧ (z · N ))
dz dz = f (∅) +
 λ
0

u ∈N
z ≤yˆu
∂u F (yˆ ∧ (z · N ))dz,
where the second equality is due to the chain rule. By submodularity and the observation that
∂u F (z · N ) is non-positive for every z > yˆu , we get:

u ∈N
z ≤yˆu
∂u F (yˆ ∧ (z · N )) ≥

u ∈N
z ≤yˆu
∂u F (z · N ) ≥

u ∈N
∂u F (z · N ).
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.        
30:8 N. Buchbinder et al.
Combining the above equality and inequality, and using the chain rule again, gives:
F (yˆ ∧ (λ · N )) ≥ f (∅) +
 λ
0
⎡
⎢
⎢
⎢
⎢
⎣

u ∈N
∂u F (z · N )
⎤
⎥
⎥
⎥
⎥
⎦
dz = f (∅) +
 λ
0
dF (z · N )
dz dz = F (λ · N ).
We also need a lemma proved by [27].
Lemma 3.4 (Lemma III.5 of [27]). Consider a vector x ∈ [0, 1]N . Assuming xu ≤ a for everyu ∈ N ,
then for every set S ⊆ N , F (x ∨ S) ≥ (1 − a)f (S).
The following corollary follows from the last two lemmata.
Corollary 3.5. F (yˆ) ≥ e−1 · F (OPT ), where OPT is the subset of N maximizing f (OPT ).
Proof. Let 0 ≤ a < 1 be a parameter whose value is chosen later. Observe that
F (yˆ ∧ (a · N )) − f (∅) =
 a
0
dF (yˆ ∧ (z · N ))
dz dz (1)
=
 a
0

u ∈N
z ≤yˆu
∂u F (yˆ ∧ (z · N ))dz ≥
 a
0

u ∈N
z ≤yˆu
∂u F (z · N )dz,
where the second equality follows by the chain rule, and the inequality follows from the submodularity of f . Let us now lower bound the rightmost integrand of the last inequality. We claim that
whenever z is not equal to any coordinate of yˆ (i.e., it does not take one of a finite set of values),
the following inequality is true.

u ∈N
z ≤yˆu
∂u F (z · N ) ≥

u ∈OPT
∂u F (z · N ). (2)
To see why this is the case, observe that ∂u F (z · N ) ≥ 0 whenever z < yˆu and ∂u F (z · N ) ≤ 0
whenever z > yˆu . This means that the left-hand side sums the term ∂u F (z · N ) for all the elements
u ∈ N for which this term is positive, and excludes from the sum the elements for which it is
negative. Hence, this left-hand side is equal to the maximum sum of the form 
u ∈S ∂u F (z · N ) for
any set S ⊆ N of elements, and in particular, it is at least the right-hand side.
The right-hand side of Inequality (2) can be further lower bounded by

u ∈N
z ≤yˆu
∂u F (z · N ) ≥

u ∈OPT
∂u F (z · N )dz =

u ∈OPT [F (u ∨ (z · N )) − F (z · N )]
1 − z
dz (3)
≥
F (OPT ∨ (z · N )) − F (z · N )
1 − z
≥ f (OPT ) − F (yˆ ∧ (z · N ))
1 − z ,
where the penultimate inequality follows from the submodularity of f , and the final inequality
follows from Lemmata 3.3 and 3.4.
Combining Inequalities (1) and (3), we get
F (yˆ ∧ (a · N )) ≥ f (∅) +
 a
0

f (OPT ) − F (yˆ ∧ (z · N ))
1 − z

dz.
The solution of this differential equation is:
F (yˆ ∧ (a · N )) ≥ −(1 − a) · ln(1 − a) · f (OPT ).
Note that the right-hand side of the last inequality is maximized for a = 1 − e−1, which gives
F (yˆ ∧ ((1 − e−1) · N )) ≥ e−1 · f (OPT ).
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.              
Online Submodular Maximization with Preemption 30:9
To complete the proof of the corollary, we need to get back to Inequality (1). This inequality shows,
in particular, that F (yˆ ∧ (a · N )) is equal to a non-negative constant (f (∅)) plus an integral from
0 to a over the integrand 
u ∈N,z ≤yˆu ∂u F (yˆ ∧ (z · N )). Note that this integrand is non-negative
whenever z is not equal to any coordinate of yˆ since, for such values of z, the submodularity of f
and the fact that ∂u F (z · N ) ≥ 0 whenever z < yˆu imply together

u ∈N
z ≤yˆu
∂u F (yˆ ∧ (z · N )) ≥

u ∈N
z ≤yˆu
∂u F (z · N ) ≥ 0.
Hence, F (yˆ ∧ (z · N )) is a non-decreasing function of z, which implies
F (yˆ) = F (yˆ ∧ (1 · N )) ≥ F (yˆ ∧ ((1 − e−1) · N )) ≥ e−1 · f (OPT ).
The last corollary completes the proof of the first part of Theorem 1.2. A naïve implementation of Algorithm 1 requires exponential time to evaluate F . However, for every constant c > 0,
it is possible to approximate, in polynomial time, ∂uj F (θj · Ni ) with probability 1 − cεi−2 up to an
additive error of cεi−2 · f (OPT ) using sampling (see [10] for an example of a similar estimate).8
For a small enough c, such an approximation is good enough to affect the competitive ratio of the
algorithm by only an ε. To keep the algorithm within unconstrained-model, the samples used
to approximate ∂uj F (θj · Ni ) for different values of i need to be correlated to guarantee that the
approximation is a decreasing function of i. For more details, see Appendix B.
The rest of this section considers dicut-model and is devoted to proving Theorem 1.3. Recall
that in this model f is the cut function of some weighted directed graph G = (V,A). Let win(u)
(wout(u)) denote the total weight of the arcs entering (leaving) element u. Throughout this section,
we assume there are no arcs of G leaving nodes of V \ N . Removing such arcs does not affect the
value of any solution, and thus, our results hold also without the assumption. Feige and Jozeph [22]
proved the following theorem (we rephrased it in our notation).
Theorem 3.6. There exists a non-decreasing function h : [0, 1] → [0, 1] such that the vector yˆ ∈
[0, 1]N defined by:
yˆu = h
 wout(u)
win(u) + wout(u)

obeys F (yˆ) ≥ 0.483 · f (OPT ). Moreover, h is independent of G and can be computed in constant time
(assuming numbers comparison takes constant time).
Notice that in the last theorem yˆu is undefined when win(u) = wout(u) = 0. However, in this
case, because such an element u is isolated, we may use any value yu and still have F (yˆ) ≥ 0.483 ·
f (OPT ). We can now present our algorithm for dicut-model which is depicted as Algorithm 2.
Observation 3.7. The competitive ratio of Algorithm 2 is at least 0.483.
Proof. Notice that the vector y(n) defined by the algorithm is equal to the vector yˆ defined
by Theorem 3.6 (wherever the last is defined). Notice also that every element u ∈ N belongs to
Sn with probability yu (n), independently. Hence, E[f (Sn )] = F (yˆ). The observation now follows
from Theorem 3.6.
To complete the proof of Theorem 1.3, we only need the following lemma which shows that
Algorithm 2 is an online algorithm of dicut-model.
Lemma 3.8. For every 1 ≤ i ≤ n, Si ⊆ Si−1 + ui .
8It is not possible to decrease the error to something that depends on n because n is unknown to the algorithm when it
calculates these estimations.
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.    
30:10 N. Buchbinder et al.
ALGORITHM 2: Degrees Ratio Choice
1 foreach element ui revealed do
2 Choose a uniformly random threshold θi ∈ [0, 1].
3 Initialize Si ← ∅.
4 for j = 1 to i do
5 Let win(uj ,i) denote the total weight of arcs from revealed elements to uj .
6 if win(uj ,i) + wout(uj ) > 0 then Let yuj (i) ← h
 wout(uj )
win (uj,i)+wout(uj )

.
7 else Let yuj (i) ← 1.
8 if yuj (i) ≥ θj then Add uj to Si .
Proof. Fori = 1, there is nothing to prove since S1 ⊆ {u1}. Thus, we assume from now on i ≥ 2.
By definition, Si contains only elements of Ni . Fix an element uj ∈ Ni − ui . We have to show that
uj ∈ Si−1. First, let us prove that:
yuj (i) ≤ yuj (i − 1). (4)
There are two cases to consider. If win(uj,i − 1) + wout(uj) ≤ 0, then yuj (i − 1) = 1, which
proves (4) since yuj (i) ∈ [0, 1]. Otherwise, since win(uj,i) is a non-decreasing function of i, we
get win(uj,i) + wout(uj) > 0. Inequality (4) now follows since h is also non-decreasing. Using (4),
we now get:
uj ∈ Si ⇒ yuj (i) ≥ θj ⇒ yuj (i − 1) ≥ θj ⇒ uj ∈ Si−1.
3.2 Hardness Results for dicut-model
In this section, we prove Theorem 1.4. The proof of the theorem is split between two lemmata.
Lemma 3.9 proves the part of Theorem 1.4 referring to deterministic algorithms, and Lemma 3.10
proves the part referring to randomized algorithms. Both lemmata present an absolute graph G =
(V,A), and then fix an algorithm ALG and describe an adversary that reveals some of the nodes
of V . The choice which nodes to reveal is done iteratively, i.e., the adversary initially reveals some
nodes and then can reveal additional nodes based on the decisions of ALG (or the probabilities of
various decisions in case ALG is randomized). Formally, in the hard instance for ALG, the set N is
the set of nodes revealed by the adversary, and the hard instance reveals these nodes in the same
order they are revealed by the adversary.
Lemma 3.9. No deterministic online algorithm has a competitive ratio better than (5 − √
17)/2 for
dicut-model.
Proof. Consider the directed graph G described in Figure 1, and fix a deterministic algorithm
ALG for dicut-model. We describe an adversary that reveals some of the nodes of G and forces
ALG to be no better than ((5 − √
17)/2)-competitive. For every pair of nodes x and y of G, we
denote by c(xy) the weight of the arc from x to y. Our adversary begins by revealing u1 and u2
and stops if ALG’s solution at this point is either empty or equal to {u1,u2}. If ALG’s solution is
empty, then ALG is not competitive at all. On the other hand, if ALG’s solution is {u1,u2}, then its
solution has a value of:
c(u1v1) + c(u2v2) = 2 ·
√
17 − 3
4 =
√
17 − 3
2 .
On the other hand, the optimal solution is {u1}, whose value is:
c(u1v1) + c(u1u2) =
√
17 − 3
4
+ 1 =
√
17 + 1
4 .
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019. 
Online Submodular Maximization with Preemption 30:11
Fig. 1. Graph for Lemma 3.9. The number beside each arc represents its weight.
Thus, in this case, the competitive ratio of ALG is at most:
√
17 − 3
2 :
√
17 + 1
4 = 2
√
17 − 6
√
17 + 1 = 5 − √
17
2 .
By the above discussion, the only interesting case is when ALG’s solution contains exactly one
of the elements u1 or u2. Notice that G is symmetric in the sense that switching every node having
the index 1 with the corresponding node having the weight 2 does not change the graph. Hence,
it is safe to assume ALG’s solution is exactly {u1}. The next step of the adversary is to reveal v1.
If v1 enters the solution of ALG, then, regardless of whether u1 is kept in the solution, the value
of ALG’s solution isc(u1u2) = c(v1u1) = 1. On the other hand, the optimal solution at this point is
{u2,v1} whose value is:
c(u2u1) + c(u2v2) + c(v1u1) = 1 +
√
17 − 3
4
+ 1 =
√
17 + 5
4 .
Hence, the competitive ratio of ALG is at most:
4
√
17 + 5 = 5 − √
17
2 .
We are left to handle the case in which v1 does not enter ALG’s solution. In this case, the adversary also reveals w1. ALG’s solution at this point must be a subset of {u1,w1} and every such
subset has a value of at most:
c(u1v1) + c(u1u2) = c(w1u1) =
√
17 + 1
4 .
On the other hand, the optimal solution at this point is {u2,v1,w1} whose value is:
c(u2u1) + c(u2v2) + c(v1u1) + c(w1u1) = 1 +
√
17 − 3
4
+ 1 +
√
17 + 1
4 =
√
17 + 3
2 .
Hence, the competitive ratio of ALG is at most:
√
17 + 1
4 :
√
17 + 3
2 =
√
17 + 1
2
√
17 + 6 = 7 − √
17
8
<
5 − √
17
2 .
Lemma 3.10. No randomized online algorithm has a competitive ratio better than 4/5 for
dicut-model.
Proof. Consider the directed graph G describe in Figure 2, and fix an algorithm ALG for
dicut-model. We describe an adversary that reveals some of the nodes of G and forces ALG to
be no better than 4/5-competitive. The adversary begins by revealing the nodes u1 and u2 of the
graph. At this point, let p1 be the probability that u1 is alone in the solution of ALG and p2 be the
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019. 
30:12 N. Buchbinder et al.
Fig. 2. Graph for Lemma 3.10. All arcs have an identical weight assumed to be 1.
probability that u2 is alone in this solution. If p1 + p2 ≤ 4/5, then the adversary stops here, and the
competitive ratio of ALG is no better than 4/5 since the optimal solution at this point has a value
of 1 and the expected value of the ALG’s solution is only p1 + p2. Thus, we may assume from now
on p1 + p2 > 4/5.
By symmetry, we may also assume, without loss of generality, p2 ≤ p1, which implies 2p1 ≥
p1 + p2 > 4/5. The next step of the adversary is revealing w1. The optimal solution at this point is
{u2,w1}, whose value is 2. Let us analyze the best solution ALG can produce. With probability p1,
ALG must pick a subset of {u1,w1}, and thus, cannot end up with a solution of value better than 1.
With the remaining probability, ALG produces a solution no better than the optimal solution, i.e.,
its value is at most 2. Hence, the expected value of ALG’s solution is at most:
p1 + 2(1 − p1) = 2 − p1 < 2 − 2/5 = 2 · (4/5).
4 THE CARDINALITY-MODEL
Our positive and negative results for cardinality-model (i.e., Theorems 1.5 and 1.6, respectively)
are proved in Sections 4.1 and 4.2, respectively.
4.1 Algorithms for cardinality-model
We first consider the special case of cardinality-model where the objective function f is monotone (in addition to being non-negative and submodular). For this case, it is possible to derive a
1/4-competitive algorithm from the works of Varadaraja [53] and Chakrabarti and Kale [11] on
streaming algorithms. We describe an alternative algorithm which achieves the same competitive
ratio and has an easier analysis. A variant of this algorithm is applied later in this section to the
general cardinality-model.
Our alternative algorithm has a parameter c > 0 and is given as Algorithm 3. Intuitively, the
algorithm gets the first k revealed elements unconditionally. For every element ui arriving after
the first k elements, the algorithm finds the best swap replacing an element of the current solution
withui . If this swap increases the value of the current solution by enough to pass a given threshold,
then it is carried out and ui enters the solution.
ALGORITHM 3: Greedy with Threshold(c)
1 Let S0 ← ∅.
2 foreach element ui revealed do
3 if i ≤ k then
4 Let Si ← Si−1 + ui .
5 else
6 Let u
i be the element of Si−1 maximizing f (Si−1 + ui − u
i ).
7 if f (Si−1 + ui − u
i ) − f (Si−1) ≥ c · f (Si−1)/k then
8 Let Si ← Si−1 + ui − u
i .
9 else
10 Let Si ← Si−1.
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019. 
Online Submodular Maximization with Preemption 30:13
Let us define some notation. For every 0 ≤ i ≤ n, let Ai = ∪i
j=1Sj . Informally, Ai is the set of
all elements of {u1,u2,...,ui} originally accepted by Algorithm 3, regardless of whether they are
preempted at a later stage or not. We also use f (u | S) to denote the marginal contribution of an
element u to a set S. Formally, f (u | S) = f (S + u) − f (S). The following technical lemma is used
later in the proof. Intuitively, it shows that the gain that can be obtained by adding an element to
the solution is related to the marginal gain of adding this element to the set of all the elements
that have ever been in the solution.
Lemma 4.1. For every k < i ≤ n, f (Si−1 + ui − u
i ) − f (Si−1) ≥ f (ui | Ai−1) − f (Si−1)/k.
Proof. By the choice of u
i and an averaging argument,
f (Si−1 + ui − u
i ) − f (Si−1) ≥

u∈Si−1 [f (Si−1 + ui − u
) − f (Si−1)]
k
≥

u∈Si−1 [f (Si−1 + ui ) − f (Si−1)]
k +

u∈Si−1 [f (Si−1 − u
) − f (Si−1)]
k ,
where the second inequality follows from submodularity. The first term on the rightmost side is
f (ui | Si−1), which can be lower bounded by f (ui | Ai−1) because Si−1 ⊆ Ai−1. The second term
on the rightmost side can be lower bounded using the submodularity and non-negativity of f as
follows:

u∈Si−1 [f (Si−1 − u
) − f (Si−1)]
k ≥ f (∅) − f (Si−1)
k ≥ − f (Si−1)
k .
The following lemma relates Si and Ai .
Lemma 4.2. For every 0 ≤ i ≤ n, f (Si ) ≥ c
c+1 · f (Ai ).
Proof. For i = 0, the lemma clearly holds since f (A0) = f (∅) = f (S0). Thus, it is enough to
prove that for every 1 ≤ i ≤ n, f (Si ) − f (Si−1) ≥ c
c+1 · [f (Ai ) − f (Ai−1)]. If the algorithm does not
accept ui, then Si = Si−1 and Ai = Ai−1, and the claim is trivial. Hence, we only need to consider
the case in which the algorithm accepts element ui .
If i ≤ k, then f (Si ) − f (Si−1) = f (ui | Si−1) ≥ f (ui | Ai−1) = f (Ai ) − f (Ai−1), where the inequality follows from submodularity since Si−1 ⊆ Ai−1. If i > k, then it is possible to lower bound
f (Si ) − f (Si−1) = f (Si−1 + ui − u
i ) − f (Si−1) in two ways. First, a lower bound of f (ui | Ai−1) −
f (Si−1)/k is given by Lemma 4.1. A second lower bound ofc · f (Si−1)/k follows since the algorithm
accepts ui . Using both lower bounds, we get:
f (Si ) − f (Si−1) ≥ max{f (ui | Ai−1) − f (Si−1)/k,c · f (Si−1)/k}
≥ c · [f (ui | Ai−1) − f (Si−1)/k] + 1 · [c · f (Si−1)/k]
c + 1
≥ c
c + 1 · f (ui | Ai−1) = c
c + 1 · [f (Ai ) − f (Ai−1)].
We are now ready to prove the competitive ratio of Algorithm 3, and thus, also the second
part of Theorem 1.5. Our proof is based on the observation that every element of OPT that was
never added to the solution must have a relatively minor contribution to An, and thus, An (and
consequently also Sn) must have a large value with respect to f (OPT ∪ An ).
Corollary 4.3. The competitive ratio of Algorithm 3 is at least c
(c+1)2 . Hence, for c = 1, the competitive ratio of Algorithm 3 is at least 1/4.
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.  
30:14 N. Buchbinder et al.
Proof. Let OPT be the optimal solution, and consider an element ui ∈ OPT \ Ai . Since ui was
rejected by Algorithm 3, the following inequality must hold:
c · f (Si−1)
k > f (Si−1 + ui − u
i ) − f (Si−1) ≥ f (ui | Ai−1) − f (Si−1)
k ,
where the second inequality follows from Lemma 4.1. Rearranging yields:
f (ui | Ai−1) < c + 1
k · f (Si−1) ≤ c + 1
k · f (Sn ),
where the last inequality uses the monotonicity of f (Si ) (as a function of i). In conclusion, by the
submodularity and monotonicity of f and Lemma 4.2,
f (OPT ) ≤ f (An ) +

u ∈OPT \An
f (u | An ) < f (An ) +

u ∈OPT \An

c + 1
k · f (Sn )

≤ f (An ) + (c + 1) · f (Sn ) ≤ (c + 1)
2
c · f (Sn ).
At this point, we return the focus to the general cardinality-model allowing the objective f
to be non-monotone. For the algorithm and analysis presented, we need the notation described
at the beginning of Section 3. More specifically, we use the multilinear extension F of f and the
assumption that a set S ⊆ N represents also its characteristic vector. As a first step, consider a
variant of Algorithm 3 modified in two ways:
• The value k is replaced with pk for an integer parameter p ≥ 1.
• The algorithm uses an auxiliary function д : 2N → R+ instead of the real objective function
f . The auxiliary function д is defined as д = F (p−1 · S).
Notice that the modified algorithm does not produce a feasible solution. Still, we are interested
in analyzing it because we use it later as a building block in an algorithm that does produce a
feasible solution. The proofs of Lemmata 4.1 and 4.2 do not use the monotonicity of f . Hence,
both lemmata still hold with pk and д taking the roles of k and f , respectively (notice that д is
non-negative and submodular). The resulting lemmata after the replacements are given below:
Lemma 4.4. For every pk < i ≤ n, д(Si−1 + ui − u
i ) − д(Si−1) ≥ д(ui | Ai−1) − д(Si−1)/(pk).
Lemma 4.5. For every 0 ≤ i ≤ n, д(Si ) ≥ c
c+1 · д(Ai ).
We also need the following lemma of [23].
Lemma 4.6 (Lemma 2.3 of [23]). Let f : 2N → R be submodular; A, B ⊆ N two (not necessarily
disjoint) sets; and A(p), B(q) their independently sampled subsets, where each element of A appears
in A(p) with probability p and each element of B appears in B(q) with probability q. Then
E[f (A(p) ∪ B(q))] ≥ (1 − p)(1 − q) · f (∅) + p(1 − q) · f (A) + (1 − p)q · f (B) + pq · f (A ∪ B).
Using the above lemmata, it is possible to get a guarantee on д(Sn ). The proof of the next corollary is very similar to the proof of Corollary 4.3, with the main difference being that we now need
to use Lemma 4.6 rather than monotonicity to lower bound д(An ∪ OPT ).
Corollary 4.7. Let OPT be the optimal solution. The modified Algorithm 3 guarantees:
д(Sn ) ≥ cp−1 (1 − p−1)
(c + 1)(1 + p−1c) · f (OPT ).
Hence, for c = 7/4 and p = 3, д(Sn ) ≥ (56/627) · f (OPT ).
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.   
Online Submodular Maximization with Preemption 30:15
Proof. Consider an element ui ∈ OPT \ Ai . Since ui was rejected by Algorithm 3, the following
inequality must hold:
c · д(Si−1)
pk > д(Si−1 + ui − u
i ) − д(Si−1) ≥ д(ui | Ai−1) − д(Si−1)
pk ,
where the second inequality follows from Lemma 4.4. Rearranging yields:
д(ui | Ai−1) < c + 1
pk · д(Si−1) ≤ c + 1
pk · д(Sn ),
where the last inequality uses the monotonicity of д(Si ) (as a function of i) for i ≥ pk. By the
submodularity of д and Lemma 4.5, we now get:
д(An ∪ OPT ) ≤ д(An ) +

u ∈OPT \An
д(ui | An ) < д(An ) +

u ∈OPT \An

c + 1
pk · д(Sn )

≤ д(An ) + p−1 (c + 1) · д(Sn ) ≤ (c + 1)(1 + p−1
c)
c · д(Sn ).
On the other hand, by Lemma 4.6,
д(An ∪ OPT ) = F (p−1 · OPT + p−1 · (An \ OPT )) ≥ p−1 (1 − p−1) · f (OPT ).
A naïve implementation of Algorithm 3 requires exponential time to evaluate F . However, this
can be fixed, at a loss of an arbitrary small additive constantε > 0 in the guarantee of Corollary 4.7.
See the above discussion about a polynomial-time implementation of Algorithm 1 for the main
ideas necessary for such an implementation. For more details, see Appendix C.
Consider a collection of vectors {yi}
n
i=0 obtained from the sets {Si}
n
i=0 via the following rule:
yi = p−1 · Si . The vectors{yi}
n
i=0 form a fractionalsolution for cardinality-model in the following
sense. First, for every 0 ≤ i ≤ n, the sum of the coordinates of yi is at most k. Second, for every
1 ≤ i ≤ n, yi ≤ yi−1 ∨u, where the inequality is element-wise. To convert the vectors {yi}
n
i=0 into
an integral solution {S¯
i}
n
i=0 for cardinality-model, we need an online rounding method.
The rounding we suggest works as follows: For every 1 ≤  ≤ k, select a uniformly random
value r from the range {p( − 1) + 1,p( − 1) + 2,...,p}. Intuitively, the values r specify the
indexes of the elements from the set {uj}
pk
j=1 that get into the rounded solution. Later elements can
get into the rounded solution only if they “take the place” of one of these elements. More formally,
we say that an element uj ∈ Si uses place a if there exists a series of indexes: j1, j2,..., jm such
that:
j = j1, u
ji = uji−1 (i.e., uji replaced uji−1 in Sji ) and jm = a.
The element uj ∈ Si gets into S¯
i if and only if it uses a place a equal to r for some  ∈ {1, 2,..., k}.
Observation 4.8. For every 0 ≤ i ≤ n, at most one element of Si uses every place a ∈
{1, 2,...,pk}.
Proof. The observation follows immediately from the definition when i ≤ pk because for every
1 ≤ j ≤ i, the element uj uses place j. For larger values of i, we prove the observation by induction.
Assume the claim holds for i − 1 ≥ pk, and let us prove it for i. Since ui got into Si , the place used
by ui is defined as the place used by u
i . As Si = Si−1 + ui − u
i , every place is used at most once
also in Si .
Corollary 4.9. The sets {S¯
i}
n
i=0 form a feasible online solution for cardinality-model.
Proof. Observation 4.8 implies immediately that |S¯
i | ≤ k since only k places make elements of
Si using them appear in S¯
i . Observe also that the rule whether an element of Si gets into {S¯
i} is
independent of i. Hence, for every 1 ≤ i ≤ k, S¯
i ⊆ S¯
i−1 + ui simply because Si ⊆ Si−1 + ui .
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.             
30:16 N. Buchbinder et al.
Next, we want to show that f (S¯
n ) is as good approximation for f (OPT ) as д(Sn ). This is done by
the following lemma, which exploits the negative correlations in our online rounding technique.
Lemma 4.10. E[f (S¯
n )] ≥ д(Sn ).
Proof. For every 1 ≤  ≤ k, let R be the set of elements of Sn using places from {p( − 1) +
1,p( − 1) + 2,p}. By Observation 4.8, the size of R is at most p. Let R¯ be a set containing the
single element of R using the place r, or the empty set if no element uses this place. By definition,
S¯
n = 	k
=1 R¯.
Recall also that given a vector x ∈ [0, 1]N , R(x) is a random set containing every element u ∈ N
with probability xu . Using this notation,
д(Sn ) = E[f (R(p−1 · Sn ))] = E
⎡
⎢
⎢
⎢
⎢
⎣
f 



k
=1
R(p−1 · R )

⎤
⎥
⎥
⎥
⎥
⎦
.
Hence, the lemma is equivalent to the inequality:
E
⎡
⎢
⎢
⎢
⎢
⎣
f 



k
=1
R¯

⎤
⎥
⎥
⎥
⎥
⎦
≥ E
⎡
⎢
⎢
⎢
⎢
⎣
f 



k
=1
R(p−1 · R )

⎤
⎥
⎥
⎥
⎥
⎦
. (5)
As a step toward proving Inequality (5), fix 1 ≤  ≤ k, and let D be a random subset of N \ R
with an arbitrary distribution. We use the notation v1,v2,...,vp to denote the elements of R
(p = |R | ≤ p), and R,i to denote the set {v1,v2,...,vi} ⊆ R. By submodularity,
E 
f

D ∪ R¯

= E[f (D )] +
p
i=1 E[f (vi | D )]
p
≥ E[f (D )] +
p
i=1 E[f (vi | D ∪ R(p−1 · R,i−1)]
p
= E[f (D )] +
p

i=1

E[D ∪ R(p−1 · R,i )] − E[D ∪ R(p−1 · R,i−1)]

= E 
f

D ∪ R(p−1 · R )

.
Inequality (5) follows by repeated applications of the above inequality, once for every 1 ≤  ≤
k.
The first part of Theorem 1.5 follows by combining the modified Algorithm 3 with the rounding
method described above.
4.2 Hardness Results for cardinality-model
In this section, we prove Theorem 1.6. The proof of the theorem is split between two lemmata.
Lemma 4.11 proves the part of Theorem 1.6 referring to monotone objectives, and Lemma 4.12
proves the part referring to general objective functions.
Lemma 4.11. No deterministic (randomized) online algorithm has a competitive ratio better than
1/2 + ε (3/4 + ε) for cardinality-model, even if the objective function is restricted to be monotone.
Proof. Let k = (2ε)
−1 and consider the ground set N = {ui}
k
i=1 ∪ {vi}
k
i=1 ∪ {w} and the monotone submodular function f : 2N → R+ defined as follows:
f (S) = |S ∩ {ui}
k
i=1 | + min{k, |S ∩ {vi}
k
i=1 | + k · |S ∩ {w}|}.
Intuitively, f gains a “point” for every element of the forms ui and vi . The element w gives k
“points”, but these are the same points of the elements of the formvi , i.e., vi gives no “points” once
w is in the solution.
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.                                         
Online Submodular Maximization with Preemption 30:17
Fix an algorithm ALG, and set the element w to be revealed after all the other elements of N .
Observe that as long as w is not revealed, there is no way for ALG to distinguish between the
elements of {ui}
k
i=1 and {vi}
k
i=1. Thus, if ALG is deterministic, the set S2k is determined solely based
on the order in which the elements are revealed. By choosing the right order, one can guarantee
that S2k ⊆ {vi}
k
i=1, which implies that S2k+1 is a subset of {vi}
k
i=1 ∪ {w}, and thus, has a value of at
most k. On the other hand, the optimal solution is {ui}
k−1 i=1 ∪ {w}, and its value is 2k − 1. Hence,
the competitive ratio of ALG is at most:
k
2k − 1
≤
k + 1
2k ≤
1
2
+ ε.
If ALG is randomized, S2k depends also on the random choices of ALG. However, by symmetry,
it is still possible to choose a revelation order for the elements of {ui}
k
i=1 and {vi}
k
i=1 guaranteeing
that in expectation S2k contains no more than k/2 elements of {ui}
k
i=1. Thus, the expected value of
f (S2k+1) can be upper bounded by 3k/2, and the competitive ratio of ALG is no more than:
3k/2
2k − 1
≤
3k/2 + 1
2k ≤
3
4
+ ε.
Lemma 4.12. No online algorithm has a competitive ratio better than 1/2 + ε for
cardinality-model.
Proof. Intuitively, the proof of Lemma 4.12 provides a weaker bound for randomized algorithms because the number of elements of the form vi in the ground set is quite small.
In this proof, we fix this problem. Let k = ε−1 and h = 2k · ε−1. Consider the ground set
N = {ui}
k
i=1 ∪ {vi}
h
i=1 ∪ {w} and the non-negative submodular function f : 2N → R+ defined as
follows:
f (S) =

|S | if w  S,
k + |S ∩ {ui}
k
i=1 | otherwise.
Fix an algorithm ALG, and set the element w to be revealed after all the other elements of N .
Observe that, as long as w is not revealed, there is no way for ALG to distinguish between the
elements of {ui}
k
i=1 and {vi}
h
i=1. Hence, by symmetry, it is possible to choose a revelation order for
these elements guaranteeing that in expectation Sk+h contains no more than k2/(k + h) elements
of {ui}
k
i=1. The final solution of ALG is a subset of Sk+h + w. Since the size of Sk+h is at most k, no
element of Sk+h + w has a negative marginal contribution, and thus, the value of ALG’s solution
can be upper bounded by f (Sk+h + w). Hence, the total expected value ALG achieves is at most:
E[f (Sk+h + w)] = k + E[|Sk+h ∩ {ui}
k
i=1 |] ≤ k +
k2
k + h .
On the other hand, the optimal solution is {ui}
k−1 i=1 ∪ {w}, and its value is 2k − 1. Hence, the
competitive ratio of ALG is at most:
k + k2
k+h
2k − 1
≤
k + 1
2k +
k
k + h ≤
1
2
+ ε.
APPENDICES
A PROOF OF THEOREM 1.1
In this appendix, we prove Theorem 1.1. Namely, we need to show that an algorithm that uses no
preemption cannot be (1/4 + ε)-competitive for any constant ε > 0 even when f is guaranteed to
be a cut function of a weighted directed graph and there is no constraint on the set of elements
that can be picked. Assume for the sake of contradiction that there exists such an algorithm ALG
which is (1/4 + ε)-competitive for some, fixed, ε > 0. We design an adversary that constructs an
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.   
30:18 N. Buchbinder et al.
instance for ALG that leads to a contradiction. The adversary uses a ground set N with n = n(ε)
elements (where n(ε) is function that depends only on ε and is described later).
Each time an element ui is revealed the adversary uses the properties of ALG to choose the
weighted set Ei of arcs that leaves ui . The objective function f is defined as the weighted cut
function of the graph (N,
	n
i=1 Ei ). Notice that any query of f made by ALG before ui is revealed
does not depend on Ei , and thus, the behavior of ALG up to the point when ui is revealed can be
described before Ei, Ei+1,..., En are determined. An exact description of the adversary is given as
Algorithm 4. For ease of notation, we denote by Ni the set {uj | 1 ≤ j ≤ i} for every 0 ≤ i ≤ n.
ALGORITHM 4: Adversary for ALG
1 Let m ← 1 + 4/ε, n0 ← 8/ε and n = e8/ε · n0.
2 Let Mode ← A.
3 for i = 1 to n do
4 if Mode = A then
5 Reveal element ui with arcs of weight mi going to all nodes revealed so far.
6 else // If Mode = B
7 Reveal element ui with no exiting arcs.
8 for every set S ⊆ Ni−1 do
9 Let Ai,S be the event that the algorithm accepted exactly the elements of S before ui is
revealed.
10 Let pi,S be the probability of Ai,S .
11 Let qi,S be the probability ui is accepted given Ai,S .
12 if i ≥ n0 and 
S ⊆Ni−1

pi,S · qi,S ·

1 − |S |
i−1

< 1/4 + ε/4 then
13 Mode ← B.
Observe that Algorithm 4 operates in two modes. In mode A, every revealed element has an arc
going to every previously revealed element. In mode B, the revealed elements have no exiting arcs
(i.e., their output degrees are 0). The adversary starts in mode A and then switches permanently
to mode B when some condition holds for the first time.
Lemma A.1. The adversary given by Algorithm 4 never switches to mode B.
Proof. Assume for the sake of contradiction that the adversary switches to mode B immediately
after element ui is revealed. Let us upper bound the weight of the cut C produced by ALG in this
case. If ALG accepted a set S ⊆ Si−1 of elements before ui is revealed, then the expected number
of arcs leaving ui crossing C is (i − 1) · qi,S · (1 − |S |
i−1 ). By the linearity of the expectation, the
expected number of arcs leaving ui crossing C is:
(i − 1) ·

S ⊆Ni−1

pi,S · qi,S ·

1 − |S |
i − 1

< (i − 1) · (1/4 + ε/4).
The total weight of all arcs leaving elements other than ui is:

i−1
j=1
[(j − 1) · mj
] ≤ (i − 1) ·

i−1
j=1
mj ≤ (i − 1) ·
∞
j=0
mi−1−j = (i − 1) · mi
m − 1 = (i − 1)mi ·
ε
4
.
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.    
Online Submodular Maximization with Preemption 30:19
Hence, even if all of the above arcs cross C, the total weight of C is still less than (i − 1)mi · (1/4 +
ε/2). On the other hand, the optimal solution may accept ui and no other elements, which results
in a cut of weight (i − 1)mi
. Hence, the competitive ratio of ALG cannot be 1/4 + ε.
Let us denote ti,S = 1 − |S |
i−1 . Using this notation, the previous lemma implies that for every
n0 ≤ i ≤ n:
S ⊆Ni−1

pi,S · qi,S · ti,S
 ≥ 1/4 + ε/4.
Consider the potential function Φ(i) = 
S ⊆Ni−1 (pi,S · t 2
i,S ).
Observation A.2. For every 1 ≤ i ≤ n − 1,
Φ(i + 1) − Φ(i) =

S ⊆Ni−1

pi,S ·

[(i − 1)ti,S + (1 − qi,S )]
2
i2 − t
2
i,S

=

S ⊆Ni−1


pi,S ·
(1 − 2i)t 2
i,S + 2(i − 1)ti,S (1 − qi,S ) + (1 − qi,S )
2
i2 

.
Proof. Let Bi be the expected set of elements accepted by ALG immediately after ui is revealed.
Notice that:
Φ(i + 1) = E
⎡
⎢
⎢
⎢
⎢
⎣

1 − |Bi |
i
2⎤
⎥
⎥
⎥
⎥
⎦
=

S ⊆Ni−1


pi,S · E
⎡
⎢
⎢
⎢
⎢
⎣

1 − |Bi |
i
2





Ai,S
⎤
⎥
⎥
⎥
⎥
⎦


.
Observe also that E[|Bi | | Ai,S ] = E[|S | + qi,S | Ai,S ]. Hence, by the linearity of the expectation:
Φ(i + 1) =

S ⊆Ni−1


pi,S · E
⎡
⎢
⎢
⎢
⎢
⎣

1 − |S | + qi,S
i
2





Ai,S
⎤
⎥
⎥
⎥
⎥
⎦


=

S ⊆Ni−1


pi,S ·

1 − (1 − ti,S )(i − 1) + qi,S
i
2


=

S ⊆Ni−1


pi,S ·

ti,S (i − 1) + (1 − qi,S )
i
2


.
The observation follows by combining the above equality with the definition of Φ(i).
Using the above observation, we can now prove the following lemma:
Lemma A.3. For every n0 ≤ i ≤ n − 1, Φ(i + 1) − Φ(i) ≤ −ε/(4i).
Proof. By Observation A.2,
Φ(i + 1) − Φ(i) =

S ⊆Ni−1


pi,S ·
(1 − 2i)t 2
i,S + 2(i − 1)ti,S (1 − qi,S ) + (1 − qi,S )
2
i2 

≤ 2 ·

S ⊆Ni−1


pi,S ·
−i · t 2
i,S + i · ti,S · (1 − qi,S ) + 1
i2 

= 2
i ·

S ⊆Ni−1

pi,S · ti,S · (1 − qi,S − ti,S )

+
2
i2 .
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.            
30:20 N. Buchbinder et al.
Notice that ti,S · (1 − qi,S − ti,S ) = ti,S · (1 − ti,S ) − ti,S · qi,S ≤ 1/4 − ti,S · qi,S . Plugging this inequality into the previous one yields:
Φ(i + 1) − Φ(i) ≤
2
i ·

S ⊆Ni−1

pi,S · (1/4 − ti,S · qi,S )

+
2
i2 = 2
i ·
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1/4 −

S ⊆Ni−1
pi,S · ti,S · qi,S
⎤
⎥
⎥
⎥
⎥
⎥
⎦
+
2
i2
≤
2
i · [1/4 − (1/4 + ε/4)] +
2
i2 = − ε
2i
+
2
i2 ≤ −
ε
2i
+
ε
4i = − ε
4i
,
where the third inequality holds since i ≥ n0 ≥ 8/ε.
Corollary A.4. Φ(n) ≤ Φ(n0) − 2.
Proof. By Lemma A.3,
Φ(n) − Φ(n0) =
n−1
i=n0
[Φ(i + 1) − Φ(i)] ≤ − n−1
i=n0
ε
4i
≤ −
ε
4 ·
 n
n0
dx
x = −ε
4 · ln(n/n0) ≤ −
ε
4 · ln(n0 · e8/ε
/n0) = −2.
Corollary A.4 leads to an immediate contradiction since Φ can only take values from the range
[0, 1].
B A POLYNOMIAL-TIME IMPLEMENTATION OF ALGORITHM 1
In this appendix, we prove the second part of Theorem 1.2 by giving a polynomial-time implementation of Algorithm 1 having the same competitive ratio up to an arbitrary constant ε > 0.
Our implementation is given as Algorithm 5. This algorithm uses the notation Tθ (x), where x is
a vector and θ ∈ [0, 1]. This notation stands for {u | xu ≥ θ }, i.e., the set of all elements whose
coordinate in x is at least θ.
ALGORITHM 5: Marginal Choice - Polynomial Time Implementation
1 foreach element ui revealed do
2 Choose a uniformly random threshold θi ∈ [0, 1].
3 Let Ni ← {u1,u2,...,ui}.
4 Let Vi be a collections of 3072 · ε−2i
4 ln[2ε−1 (4i)
2] random vectors from [0, 1]Ni .
5 for j = 1 to i − 1 do
6 for every vector v ∈ Vj do
7 Extend v to be a vector of [0, 1]Ni by setting vui to be an independent uniformly random
value from [0, 1].
8 for j = 1 to i do
9 mj ← |Vj |
−1 ·

v ∈Vj[f (Tθj (v) + uj ) − f (Tθj (v) − uj )]. /* mj ≈ ∂uj F (θj · Ni ). */
10 Let Si ← {uj ∈ Ni | mj ≥ 0}.
Our first objective is to show that Algorithm 5 is an online algorithm according to
unconstrained-model.
Lemma B.1. For every 1 ≤ i ≤ n, Si ⊆ Si−1 + ui .
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.       
Online Submodular Maximization with Preemption 30:21
Proof. For i = 1, the claim is trivial since S1 can contain only u1. Thus, we assume from now
on i > 1. For every 1 ≤ j ≤ i − 1, let mj and Vj denote the value mj and the set Vj after iteration
i − 1, and let m
j and V
j denote these entities after iteration i.
By definition, Si contains only elements of Ni . Fix an element uj ∈ Ni − ui . Then:
uj ∈ Si ⇒

v ∈V
j
[f (Tθj (v) + uj) − f (Tθj (v) − uj)]
|Vj | = m
j ≥ 0
⇒

v ∈Vj[f (Tθj (v) + uj) − f (Tθj (v) − uj)]
|Vj | = mj ≥ 0 ⇒ uj ∈ Si−1,
where the second derivation follows from submodularity since V
j is produced from Vj by extending vectors.
To bound the loss in the competitive ratio of Algorithm 5, we need the following claims:
Observation B.2. For every 1 ≤ i ≤ n, each set in Vi is an independent sample of [0, 1]N .
Proof. Follows immediately by the way Algorithm 5 constructs Vi and the way this collection
is updated when new elements arrive.
Lemma B.3. Let X1,X2,...,X be independent random variables such that for each i, Xi ∈ [−1, 1].
Let X = 1


i=1 Xi and μ = E[X]. Then:
Pr[X > μ + α] ≤ e− α2
12 and Pr[X < μ − α] ≤ e− α2
8 ,
for every α > 0.
Proof. For every 1 ≤ i ≤ , letYi = (1 + Xi )/2. Additionally, letY = 
i=1 Yi . Clearly,Yi ∈ [0, 1].
Hence, by the Chernoff bound:
Pr[X > μ + α] = Pr[2Y/ − 1 > 2E[Y]/ − 1 + α] = Pr 
Y > E[Y] +
α
2

≤ e− E[Y](α /(2·E[Y]))2
3 = e
− α22
12·E[Y] ≤ e− α2
12 ,
and
Pr[X < μ − α] = Pr[2Y/ − 1 < 2E[Y]/ − 1 − α] = Pr 
Y < E[Y] − α
2

≤ e− E[Y](α /(2·E[Y]))2
2 = e
− α22
8·E[Y] ≤ e− α2
8 .
Let mi (z) be the value that mi gets after the last iteration of Algorithm 5 if the random variable
θi is z.
Lemma B.4. For every 1 ≤ i ≤ n and z ∈ [0, 1], Pr[|mi (z) − ∂ui F (z · N )| > ε(4i)
−2 · f (OPT )] ≤
ε(4i)
−2.
Proof. Observe that mi is obtained by averaging |Vi | independent samples of f (R(z · N ) +
ui ) − f (R(z · N ) − ui ), and the expectation of the last expression is ∂ui F (z · N ). Additionally, observe that every sample belongs to the range [−f (OPT ), f (OPT )] since f (OPT ) = maxS ⊆N f (S).
Hence, by Lemma B.3:
Pr[|mi (z) − ∂ui F (z · N )| > ε(4i)
−2 · f (OPT )] ≤ 2e
−|Vi |·[ε (4i)
−2]
2
12 ≤ 2e− ln[2ε−1 (4i)
2] = ε(4i)
−2
.
Fix the vectors V = (V1, V2,..., Vn ). For every element ui ∈ N , mi (z) is continuous nonincreasing function of z by submodularity. Hence, by the intermediate value theorem, one of the
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.                       
30:22 N. Buchbinder et al.
following must hold: mi (z) is always positive in the range [0, 1], mi (z) is always negative in the
range [0, 1] or mi (z) has at least one root z0 ∈ [0, 1]. In the last case, the set I0 (ui ) ⊆ [0, 1] of the
roots of mi (z) is non-empty. Moreover, by the monotonicity and continuity of mi (z), I0 (ui ) is a
closed range. Using these observations, we define a vector y(V) ∈ [0, 1]N as follows: For every
element ui ∈ N ,
y(V)ui =
⎧⎪⎪
⎨
⎪⎪
⎩
0 if mi (0) < 0,
1 if mi (1) > 0,
max I0 (ui ) otherwise.
Let Ei be the event that ∂ui F (z · N ) ≥ −ε(4i)
−2 · f (OPT ) for every z < y(V)ui and ∂ui F (z ·
N ) ≤ ε(4i)
−2 · f (OPT ) for every z > y(V)ui .
Lemma B.5. For every 1 ≤ i ≤ n, the event Ei holds with probability at least 1 − 2ε(4i)
−2.
Proof. Let us first bound the probability that ∂ui F (z · N ) ≥ −ε(4i)
−2 · f (OPT ) for every z <
y(V)ui . If ∂ui F (z · N ) is always at least −ε(4i)
−2 · f (OPT ), then there is nothing to prove. Otherwise, if ∂ui F (z · N ) is always at most −ε(4i)
−2 · f (OPT ), then ∂ui F (∅) ≤ 0, which guarantees that
y(V)ui = 0, and thus, makes the claim that we want to prove void. Hence, it is enough to consider the case in which ∂ui F (z · N ) is at least −ε(4i)
−2 · f (OPT ) for some value of z and at most
−ε(4i)
−2 · f (OPT ) for another value of z. In this case the intermediate value theorem and the continuity of ∂ui F (z · N ) imply the existence of a value y
i such that ∂ui F (y
i · N ) = −ε(4i)
−2 · f (OPT ).
By Lemma B.4, mi (y
i ) ≤ 0 with probability at least 1 − ε(4i)
−2, which implies y(V)ui ≤ y
i , and
thus:
∂ui F (y(V)ui · N ) ≥ ∂ui F (y
i · N ) = −ε(4i)
−2 · f (OPT ).
Observe that the last inequality implies that ∂ui F (z · N ) ≥ −ε(4i)
−2 · f (OPT ) for every z <
y(V)ui .
A similar argument shows that ∂ui F (z · N ) ≤ ε(4i)
−2 · f (OPT ) for every z > y(V)ui with probability at least 1 − ε(4i)
−2. The lemma now follows by the union bound.
Let E be the event that Ei holds for every 1 ≤ i ≤ n at the same time.
Corollary B.6. E happens with probability at least 1 − ε/2.
Proof. Combining Lemma B.5 and the union bound gives:
Pr[E] ≥ 1 −
n
i=1
2ε(4i)
−2 ≥ 1 − ε
8 −
 n
1
2ε(4x)
−2
dx = 1 − ε
8
+
ε
8x




n
1
≥ 1 − ε
2
.
It is important to notice that the event E depends only on the vectors V. Thus, fixing the vectors V does not affect the distribution of θ1, θ2,..., θn. Next, we analyze the competitive ratio of
Algorithm 5 assuming V is fixed in a way that makes the event E hold. The next observation
corresponds to Observation 3.2.
Observation B.7. For every given vectors V, every element ui ∈ N belongs to Sn with probability
y(V)ui , independently. Hence, E[f (Sn )] = F (y(V)).
Proof. An element ui ∈ N belongs to Sn if and only if mi (θi ) ≥ 0, which is equivalent to the
condition θi ≤ y(V)ui since mi is a non-increasing function. Clearly, the last condition happens
with probability y(V)ui , and is independent for different elements.
The last observation implies that analyzing Algorithm 5 is equivalent to lower bounding
F (y(V)). The next lemma and corollary correspond to Lemma 3.3 and Corollary 3.5, respectively.
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.     
Online Submodular Maximization with Preemption 30:23
Lemma B.8. For every given vectors V for which the event E holds and for every λ ∈ [0, 1],
F (y(V) ∧ (λ · N )) ≥ F (λ · N ) − ε ·
n
i=1 (4i)
−2 · f (OPT ).
Proof. For ease of notation, we use y as a shorthand for y(V) in this proof. Observe that:
F (y ∧ (λ · N )) = f (∅) +
 λ
0
dF (y ∧ (z · N ))
dz dz = f (∅) +
 λ
0

u ∈N
z ≤yu
∂u F (y ∧ (z · N ))dz,
where the second equality is due to the chain rule. By submodularity and the observation that E
implies ∂ui F (z · N ) ≤ ε(4i)
−2 · f (OPT ) for every z > yui , we get:

u ∈N
z ≤yu
∂u F (y ∧ (z · N )) ≥

u ∈N
z ≤yu
∂u F (z · N ) ≥

u ∈N
∂u F (z · N ) − ε ·
n
i=1
(4i)
−2 · f (OPT ).
Combining the above equality and inequality, and using the chain rule again, gives:
F (y ∧ (λ · N )) ≥ f (∅) +
 λ
0
⎡
⎢
⎢
⎢
⎢
⎣

u ∈N
∂u F (z · N )
⎤
⎥
⎥
⎥
⎥
⎦
dz − ε ·
n
i=1
(4i)
−2 · f (OPT )
= f (∅) +
 λ
0
dF (z · N )
dz dz − ε ·
n
i=1
(4i)
−2 · f (OPT )
= F (λ · N ) − ε ·
n
i=1
(4i)
−2 · f (OPT ).
Corollary B.9. For every given vectors V for which the event E holds, F (y(V)) ≥ (e−1 − ε/2) ·
F (OPT ).
Proof. Again, we use y as a shorthand for y(V) in this proof. Let 0 ≤ a < 1 be a parameter
whose value is chosen later. Observe that
F (y ∧ (a · N )) − f (∅) =
 a
0
dF (y ∧ (z · N ))
dz dz (6)
=
 a
0

u ∈N
z ≤yu
∂u F (y ∧ (z · N ))dz ≥
 a
0

u ∈N
z ≤yu
∂u F (z · N )dz,
where the second equality follows by the chain rule, and the inequality follows from the submodularity of f . Let us now lower bound the integrand on the rightmost side of the last inequality. We
claim that whenever z is not equal to any coordinate of y (i.e., it does not take one of a finite set
of values), the following inequalities are true:

u ∈N
z ≤yu
∂u F (z · N ) ≥

u ∈N
∂u F (z ·N )≥0
∂u F (z · N ) − ε ·
n
i=1
(4i)
−2 · f (OPT ) (7)
≥

u ∈OPT
∂u F (z · N ) − ε ·
n
i=1
(4i)
−2 · f (OPT ).
The first inequality holds since ∂ui F (z · N ) ≥ −ε(4i)
−2 · f (OPT ) whenever z < yui and
∂ui F (z · N ) ≤ ε(4i)
−2 · f (OPT ) whenever z > yui , and the second inequality is true since
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.                        
30:24 N. Buchbinder et al.

u ∈N, ∂u F (z ·N )≥0 ∂u F (z · N ) is the maximal sum of the form 
u ∈S ∂u F (z · N ) for any set S ⊆ N
of elements (and in particular, it is at least 
u ∈OPT ∂u F (z · N )).
The first sum on the right-hand side of Inequality (7) can be further lower bounded by

u ∈OPT
∂u F (z · N )dz =

u ∈OPT [F (u ∨ (z · N )) − F (z · N )]
1 − z
dz (8)
≥
F (OPT ∨ (z · N )) − F (z · N )
1 − z
≥ f (OPT ) − F (y ∧ (z · N )) − ε ·
n
i=1 (4i)
−2 · f (OPT )
1 − z ,
where the first inequality follows from the submodularity of f , and the second inequality follows
from Lemmata 3.4 and B.8.
Combining Inequalities (6), (7), and (8), we get
F (y ∧ (a · N ))
≥ f (∅) +
 a
0
⎡
⎢
⎢
⎢
⎢
⎣
f (OPT ) − F (y ∧ (z · N )) − ε ·
n
i=1 (4i)
−2 · f (OPT )
1 − z − ε ·
n
i=1
(4i)
−2 · f (OPT )
⎤
⎥
⎥
⎥
⎥
⎦
dz
≥ f (∅) +
 a
0
⎡
⎢
⎢
⎢
⎢
⎣
f (OPT ) − F (y ∧ (z · N ))
1 − z − ε ·

1 +
1
1 − a

·
n
i=1
(4i)
−2 · f (OPT )
⎤
⎥
⎥
⎥
⎥
⎦
dz.
The solution of this differential equation is
F (y ∧ (z · N )) ≥ −(1 − z) · ln(1 − z) · 

1 − ε ·

1 +
1
1 − a

·
n
i=1
(4i)
−2

· f (OPT ),
and by plugging z = a we get
F (y ∧ (a · N )) ≥ −(1 − a) · ln(1 − a) · 

1 − ε ·

1 +
1
1 − a

·
n
i=1
(4i)
−2

· f (OPT ).
Choosing now a = 1 − e−1 yields
F (y ∧ ((1 − e−1) · N )) ≥ e−1 · 

1 − ε · (1 + e) ·
n
i=1
(4i)
−2

· f (OPT )
≥ e−1 · 

1 − 4ε ·
n
i=1
(4i)
−2

· f (OPT ).
To complete the proof of the corollary, we need to get back to Inequality (6). This inequality
shows, in particular, that F (y ∧ (a · N )) is equal to a non-negative constant (f (∅)) plus an integral
from 0 to a over the integrand 
u ∈N,z ≤yu ∂u F (y ∧ (z · N )). Since ∂ui F (z · N ) ≥ −ε(4i)
−2 · f (OPT )
whenever z < yui , it is possible to lower bound this integrand in the case that z is not equal to any
coordinate of y by:

u ∈N
z ≤yu
∂u F (y ∧ (z · N )) ≥

u ∈N
z ≤yu
∂u F (z · N ) ≥ −ε ·
n
i=1
(4i)
−2 · f (OPT ),
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.            
Online Submodular Maximization with Preemption 30:25
where the first inequality follows from the submodularity of f . Hence, the derivative of F (y ∧ (a ·
N )) with respect to a is at least −ε ·
n
i=1 (4i)
−2 · f (OPT ), which implies:
F (y) = F (y ∧ (1 · N )) ≥ F (y ∧ ((1 − e−1) · N )) − εe−1 ·
n
i=1
(4i)
−2 · f (OPT )
≥ e−1 

1 − 4ε ·
n
i=1
(4i)
−2

· f (OPT ) − εe−1 ·
n
i=1
(4i)
−2 · f (OPT )
≥ 

e−1 − 4ε ·
n
i=1
(4i)
−2

· f (OPT ).
The corollary now follows by observing that:
4 ·
n
i=1
(4i)
−2 ≤
1
4
+ 4 ·
 n
1
dx
(4x)2 = 1
4 − 1
4x




n
1
<
1
4
+
1
4 = 1
2
.
We are now ready to prove the competitive ratio of Algorithm 5, and complete the proof of
Theorem 1.2.
Lemma B.10. Algorithm 5 has a competitive ratio of at least e−1 − ε.
Proof. By Observation B.7, the expected value of the set produced by Algorithm 5 is
E[F (y(V))]. By the law of total expectation:
E[F (y(V))] ≥ Pr[E] · E[F (y(V)) | E] ≥ (1 − ε/2) · (e−1 − ε/2) · f (OPT ) ≥ (e−1 − ε) · f (OPT ),
where the second inequality holds by Corollaries B.6 and B.9.
C A POLYNOMIAL TIME IMPLEMENTATION OF ALGORITHM 3 FOR THE
OBJECTIVE д(S)
In this appendix, we present a polynomial-time implementation of Algorithm 3 that can be used to
get a set S obeying д(S) ≥ (56/627 − ε) · f (OPT ), for any constant ε > 0, and thus, together with
the rounding described in Section 4, proves the second part of Theorem 1.5. The polynomial-time
implementation is given as Algorithm 6.
Lemma C.1. Every value mu,i calculated by Algorithm 6 obeys:
Pr[|mu,i − д(Si−1 + ui − u
i ) + (1 + c/(pk)) · д(Si−1)| ≤ ε(4i)
−3 · f (OPT )] ≤ εp−1
k−1 (2i)
−2
.
Proof. The value mu,i is calculated by averaging  samples. For every set S of size at most pk,
submodularity implies f (S) ≤ p · f (OPT ). Hence, each sample must belong to the range [−(p +
c/k) · f (OPT ),p · f (OPT )]. Thus, by Lemma B.3,
Pr[|mu,i − д(Si−1 + ui − u
i ) + (1 + c/(pk)) · д(Si−1)| ≤ ε(4i)
−3 · f (OPT )] ≤ 2e− [ε (4i)
−3/(p+c )]
2
12
≤ 2e− ln[2ε−1pk (2i)
2] = εp−1
k−1 (2i)
−2
.
Corollary C.2. With probability at least 1 − ε/2, all the values mu,i calculated by Algorithm 6
are accurate approximations up to an error of ε(4i)
−3 · f (OPT ).
Proof. For every 1 ≤ i ≤ n, Algorithm 6 approximates at most pk different mu,i values. Hence,
by the union bound, the probability that all these approximations are correct up to an error of
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.           
30:26 N. Buchbinder et al.
ALGORITHM 6: Greedy with Theshold for Non-Monotone Functions - Polynomial-Time
Implementation(c,p)
1 Let S0 ← ∅.
2 foreach element ui revealed do
3 if i ≤ pk then
4 Let Si ← Si−1 + ui .
5 else
6 for each u ∈ Si−1 do
7 Approximate
mu,i = д(Si−1 + ui − u
i ) − (1 + c/(pk)) · д(Si−1)
= E[f (R(p−1 (Si−1 + ui − u
i ))) − (1 + c/(pk)) · f (R(p−1 · Si−1))]
using  = 50000ε−2i
6 (p + c)
2 ln[2ε−1pk(2i)
2] samples.
8 Let u
i be the element of Si−1 maximizing mu
i,i .
9 if mu
i,i ≥ 0 then
10 Let Si ← Si−1 + ui − u
i .
11 else
12 Let Si ← Si−1.
ε(4i)
−3 · f (OPT ) is at least:
1 − pk ·
n
i=1
(εp−1
k−1 (2i)
−2) = 1 − ε ·
n
i=1
(2i)
−2 ≥ 1 − ε
4 − ε ·
 n
1
dx
4x2 = 1 − ε
4
+
ε
4x




n
1
> 1 − ε
2
.

Let E be the event that Corollary C.2 holds. As in Section 4, for every 0 ≤ i ≤ n, we define
Ai = ∪i
j=1Sj . The following two lemmata correspond to Lemmata 4.4 and 4.5.
Lemma C.3. Assuming E holds, then for every pk < i ≤ n,
д(Si−1 + ui − u
i ) − д(Si−1) ≥ д(ui | Ai−1) − д(Si−1)/(pk) − 2ε(4i)
−3 · f (OPT ).
Proof. By the choice of u
i and an averaging argument,
д(Si−1 + ui − u
i ) − д(Si−1) ≥ mu
i,i + c/(pk) · д(Si−1) − ε(4i)
−3 · f (OPT )
≥

u∈Si−1 [mu
,i + c/(pk) · д(Si−1) − ε(4i)
−3 · f (OPT )]
pk
≥

u∈Si−1 [д(Si−1 + ui − u
) − д(Si−1) − 2ε(4i)
−3 · f (OPT )]
pk
≥

u∈Si−1 [д(Si−1 + ui ) − д(Si−1)]
pk +

u∈Si−1 [д(Si−1 − u
) − д(Si−1)]
pk − 2ε(4i)
−3 · f (OPT ),
where the last inequality follows from submodularity. The first term on the rightmost side is д(ui |
Si−1), which can be lower bounded by д(ui | Ai−1) because Si−1 ⊆ Ai−1. The second term on the
rightmost side can be lower bounded using the submodularity and non-negativity of д as follows:

u∈Si−1 [д(Si−1 − u
) − д(Si−1)]
pk ≥ д(∅) − д(Si−1)
pk ≥ −д(Si−1)
pk .
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.      
Online Submodular Maximization with Preemption 30:27
The following lemma relates Si and Ai :
Lemma C.4. Assuming E holds, then for every 0 ≤ i ≤ n,
д(Si ) ≥ c
c + 1 · д(Ai ) − 2ε ·

i
j=1
(4j)
−3 · f (OPT ) ≥ c
c + 1 · д(Ai ) − ε/4 · f (OPT ).
Proof. The second inequality holds since i
j=1 (4j)
−3 = 0 for i = 0 and for i > 0:
2 ·

i
j=1
(4j)
−3 ≤
1
32
+
 i
1
dx
32x3 = 1
32 − 1
64x2




i
1
<
1
32
+
1
64
<
1
4
.
Hence, in the rest of the proof, we concentrate on proving the first inequality.
For i = 0, the inequality clearly holds since д(A0) = д(∅) = д(S0). Thus, it is enough to prove
that, for every 1 ≤ i ≤ n, д(Si ) − д(Si−1) ≥ c
c+1 · [д(Ai ) − д(Ai−1)] − 2ε(4i)
−3 · f (OPT ). If the algorithm does not accept ui then Si = Si−1 and Ai = Ai−1, and the claim is trivial. Hence, we only need
to consider the case in which the algorithm accepts element ui .
If i ≤ k, then д(Si ) − д(Si−1) = д(ui | Si−1) ≥ д(ui | Ai−1) = д(Ai ) − д(Ai−1), where the inequality follows from submodularity since Si−1 ⊆ Ai−1. If i > k, then it is possible to lower bound
д(Si ) − д(Si−1) = д(Si−1 + ui − u
i ) − д(Si−1) in two ways. First, a lower bound of д(ui | Ai−1) −
д(Si−1)/(pk) − 2ε(4i)
−3 · f (OPT ) is given by Lemma C.3. The second lower bound is that, since
the algorithm accepts ui ,
д(Si−1 + ui − u
i ) − д(Si−1) ≥ mu
i,i + c · д(Si−1)/(pk) − ε(4i)
−3 · f (OPT )
≥ c · д(Si−1)/(pk) − ε(4i)
−3 · f (OPT ).
Using both lower bounds, we get:
д(Si ) − д(Si−1) ≥ max 
д(ui | Ai−1) − д(Si−1)
pk ,
c · д(Si−1)
pk 
− 2ε(4i)
−3 · f (OPT )
≥ c · [д(ui | Ai−1) − д(Si−1)/(pk)] + 1 · [c · д(Si−1)/(pk)]
c + 1 − 2ε(4i)
−3 · f (OPT )
≥ c
c + 1 · д(ui | Ai−1) − 2ε(4i)
−3 · f (OPT )
= c
c + 1 · [д(Ai ) − д(Ai−1)] − 2ε(4i)
−3 · f (OPT ).
We also need to show that д(Si ) is roughly monotone as a function of i.
Lemma C.5. Assuming E holds, then for pk < i ≤ n, д(Si ) − д(Si−1) ≥ −ε(4i)
−3 · f (OPT ).
Proof. If Si = Si−1, then the claim is trivial. Otherwise, we must have mu
i,i ≥ 0, and thus,
д(Si ) − д(Si−1) ≥ c · д(Si )
pk − ε(4i)
−3 · f (OPT ) ≥ −ε(4i)
−3 · f (OPT ).
Corollary C.6. Assuming E holds, then for pk ≤ i ≤ n, д(Sn ) ≥ д(Si ) − ε(8i)
2 · f (OPT ).
Proof. By Lemma C.5,
д(Sn ) − д(Si ) ≥ − ε ·
n
j=i+1
(4i)
−3 · f (OPT ) ≥ −ε ·
 n
i
dx
64x3 · f (OPT )
= ε · 1
128x2




n
i
· f (OPT ) ≥ −
ε
128i2 · f (OPT ) ≥ −
ε
(8i)2 · f (OPT ).
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.        
30:28 N. Buchbinder et al.
Using the above claims, it is possible to get a guarantee onд(Sn ) assuming E holds. The following
corollary corresponds to Corollary 4.7.
Corollary C.7. Assuming E holds, Algorithm 6 produces a set Sn such that:
д(Sn ) ≥
 cp−1 (1 − p−1)
(c + 1)(1 + p−1c)
− ε
2

· f (OPT ).
Proof. Consider an elementui ∈ OPT \ Ai . Sinceui was rejected by Algorithm 6, we must have
mu
i,i < 0, which implies:
c · д(Si−1)
pk + ε(4i)
−3 · f (OPT ) > д(Si−1 + ui − u
i ) − д(Si−1)
≥ д(ui | Ai−1) − д(Si−1)
pk − 2ε(4i)
−3 · f (OPT ),
where the second inequality follows from Lemma C.3. Rearranging yields:
д(ui | Ai−1) < c + 1
pk · д(Si−1) + 3ε(4i)
−3 · f (OPT ) ≤ c + 1
pk · д(Sn ) + 4ε(8i)
−2 · f (OPT ),
where the last inequality uses Corollary C.6. By the submodularity of д and Lemma C.4, we now
get:
д(An ∪ OPT ) ≤ д(An ) +

u ∈OPT \An
д(ui | An )
< д(An ) +

u ∈OPT \An

c + 1
pk · д(Sn )

+ 4ε ·
n
i=1
(8i)
−2 · f (OPT )
≤ д(An ) + p−1 (c + 1) · д(Sn ) + 4ε ·
n
i=1
(8i)
−2 · f (OPT )
≤ (c + 1)(1 + p−1
c)
c · д(Sn ) + c + 1
c ·
ε
4 · f (OPT ) + 4ε ·
n
i=1
(8i)
−2 · f (OPT ).
Observe now that:
4ε ·
n
i=1
(8i)
−2 · f (OPT ) ≤
ε
16
+ ε ·
 n
1
dx
16x2 = ε
16 − ε
16x




n
1
≤
ε
16
+
ε
16
<
ε
4
,
and, by Lemma 4.6,
д(An ∪ OPT ) = F (p−1 · OPT + p−1 · (An \ OPT )) ≥ p−1 (1 − p−1) · f (OPT ).
Combining the three last inequalities yields:

p−1 (1 − p−1) − c + 1
c ·
ε
2

· f (OPT ) ≤ (c + 1)(1 + p−1
c)
c · д(Sn ).
The corollary now follows by dividing the last inequality by (c+1)(1+p−1c )
c and observing that 1 +
p−1
c ≥ 1.
Corollary C.8. Algorithm 6 produces a set Sn such that:
E[д(Sn )] ≥
 cp−1 (1 − p−1)
(c + 1)(1 + p−1c)
− ε

· f (OPT ).
Hence, for c = 7/4 and p = 3, д(Sn ) ≥ (56/627 − ε) · f (OPT ).
ACM Transactions on Algorithms, Vol. 15, No. 3, Article 30. Publication date: May 2019.        
Online Submodular Maximization with Preemption 30:29
Proof. By the law of total expectation,
E[д(Sn )] ≥ Pr[E] · E[д(Sn ) | E] ≥ (1 − ε/2) ·
 cp−1 (1 − p−1)
(c + 1)(1 + p−1c)
− ε
2

· f (OPT )
≥
 cp−1 (1 − p−1)
(c + 1)(1 + p−1c)
− ε

· f (OPT ),
where the second inequality holds by Corollaries C.2 and C.7.