In the generalized caching problem, we have a set of pages and a cache of size k. Each page p has a size
wp ≥ 1 and fetching cost cp for loading the page into the cache. At any point in time, the sum of the sizes of
the pages stored in the cache cannot exceed k. The input consists of a sequence of page requests. If a page is
not present in the cache at the time it is requested, it has to be loaded into the cache, incurring a cost of cp .
We give a randomizedO(log k)-competitive online algorithm for the generalized caching problem, improving the previous bound ofO(log2 k) by Bansal, Buchbinder, and Naor (STOC’08). This improved bound is tight
and of the same order as the known bounds for the classic paging problem with uniform weights and sizes.
We use the same LP-based techniques as Bansal et al. but provide improved and slightly simplified methods
for rounding fractional solutions online.
CCS Concepts: • Theory of computation → Caching and paging algorithms; Design and analysis of
algorithms; Online algorithms;
Additional Key Words and Phrases: Online primal dual, knapsack cover inequalities
1 INTRODUCTION
In the basic two-level caching problem, we are given a collection of n pages and a cache (a fast
access memory). The cache has a limited capacity and can store up to k pages. At each time step,
a request to a specific page arrives and can be served directly if the corresponding page is in the
cache; in that case, no cost is incurred. If the requested page is not in the cache, a page fault occurs
and, in order to serve the request, the page must be fetched into the cache, possibly evicting some
other page, and a cost of one unit is incurred. The goal is to design an algorithm that specifies
which page to evict in case of a fault such that the total cost incurred on the request sequence is
minimized.
This classical problem can be naturally extended to the generalized caching problem by allowing
pages to have nonuniform fetching costs and nonuniform sizes. In the general model, we are given
a collection of n pages. Each page p is described by a fetching cost cp ≥ 0 and a size wp ≥ 1. The
cache has limited capacity and can only store pages up to a total size of at most k. The framework
of generalized caching has been motivated by applications in web caching and networking. The
nonuniform sizes of the pages can correspond to the scenarios of caching web pages of different
sizes, and the nonuniform costs of fetching a page can model scenarios in which the pages have
different locations in a large network.
Various special cost models have been proposed in the literature. In the bit model [4, 11], each
page p has cp = wp , and thus, for example, minimizing the fetching cost can correspond to minimizing the total traffic in the network. In the fault model [4, 11], for each page, we have the
fetching cost cp = 1, and the size wp may be arbitrary; in this case, the fetching cost corresponds
to the number of times a user has to wait for a page to be retrieved. In the weighted caching model
[3, 11], for each page p we have the size wp = 1 and the fetching cost cp may be arbitrary; this
models situations where some pages are more expensive to fetch than others because they may be
on far-away servers or slower disks.
We consider the online version of the generalized caching problem. In this version, as soon as a
page is requested, it has to be loaded into the cache, and while processing the request we have no
information about the sequence of pages which will be requested later.
1.1 Related Work
The study of the caching problem with uniform sizes and costs (the paging problem) in the online
setting has been initiated by Sleator and Tarjan [13] in their work that introduced the framework
of competitive analysis. They show that well-known paging rules like Least Recently Used (LRU)
or First In First Out (FIFO) are k-competitive and that this is the best competitive ratio that any
deterministic algorithm can achieve.
Fiat et al. [10] extend the study to the randomized setting and design a randomized 2Hk -
competitive algorithm, where Hk is the k-th Harmonic number. They also prove that no randomized online paging algorithm can be better than Hk -competitive. Subsequently, McGeoch and
Sleator [12], and Achlioptas et al. [1] design randomized online algorithms that achieve this competitive ratio.
Weighted caching, where pages have uniform sizes but can have arbitrary cost, has been studied
extensively because of its relation to the k-server problem. The results for the k-server problem
on trees due to Chrobak et al. [8] yield a tight deterministic k-competitive algorithm for weighted
caching. The randomized complexity of weighted caching has been resolved only recently, when
Bansal et al. [3] designed a randomized O(log k)-competitive algorithm.
The caching problem with nonuniform page sizes seems to be much harder. Already the offline
version is NP-hard [11], and there was a sequence of results [2, 9, 11] that led to the work of
Bar-Noy et al. [5] which gives a 4-approximation for the offline problem.
For the online version, the first results consider special cases of the problem. Irani [11] shows
that, for the bit model and for the fault model in the deterministic case, LRU is (k + 1)-competitive.
Cao and Irani [7] and Young [14] extend this result to the generalized caching problem.
In the randomized setting, Irani [11] gives an O(log2 k)-competitive algorithm for both bit and
fault models, but for the generalized caching problem no o(n)-competitive ratio was known until
the work of Bansal et al. [4]. They show how to obtain a competitive ratio of O(log2 k) for the
general model, and also a competitive ratio of O(log k) for the bit model and the fault model.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.
An O(log k)-Competitive Algorithm for Generalized Caching 6:3
Table 1. An Overview of the Results for the Online Caching Problem in the
Randomized Setting (New Result Highlighted in Bold)
uniform sizes arbitrary sizes
2Hk -competitive [10]
O(log2 k)-competitive [11]
uniform costs Hk -competitive [1, 12]
O(log k)-competitive [4]
lower bound Hk [10]
O(log2 k)-competitive [4]
arbitrary costs O(log k)-competitive [3]
O(log k)-competitive
Table 1 presents an overview of the results for the online caching problem in the randomized
setting.
Since it has been known that no randomized o(log k)-competitive online algorithm for generalized caching exists, the central open problem in this area was whether it is possible to design a
randomized O(log k)-competitive algorithm.
1.2 Result and Techniques
We present a randomizedO(log k)-competitive online algorithm for the generalized caching problem, improving the previous bound of O(log2 k) by Bansal et al. [4]. This improved bound unifies
all earlier results for special cases of the caching problem. It is asymptotically optimal as, already
for the problem with uniform page sizes and uniform fetching costs, there is a lower bound of
Ω(log k) on the competitive ratio of any randomized online algorithm [10].
Our approach is similar to the approach used by Bansal, Buchbinder, and Naor in their results on
weighted caching [3] and generalized caching [4]. In both those papers, the authors first formulate
a packing/covering linear program that forms a relaxation of the problem. They can solve this
linear program in an online manner by using the online primal-dual framework for packing and
covering problems introduced by Buchbinder and Naor [6]. However, using the framework as a
black-box only guarantees an O(logn)-factor between the cost of the solution obtained online and
the cost of an optimal solution. They obtain an O(log k)-guarantee by tweaking the framework
for this specific problem. Note that this O(log k)-factor is optimal; that is, in general, one needs to
lose a factor of Ω(log k) when solving the LP online.
In the second step, they give a randomized rounding algorithm to transform the fractional solution into a sequence of integral cache states. Unfortunately, this step is quite involved. Bansal
et al. [4] give three different rounding algorithms for the general model and the more restrictive
bit and fault models, where, in particular, the rounding for the fault model is quite complicated.
We use the same LP as Bansal et al. [4] and also the same algorithm for obtaining an online
fractional solution. Our contribution is a more efficient and also simpler method for rounding the
fractional solution online. In particular, our rounding algorithm maintains a distribution over only
k2 integral cache states. The approaches by Bansal et al. [3, 4] do not give a good bound on the
number of states maintained by the rounding algorithm.
We first give a rounding algorithm for monotone cost models (wherewp ≥ wp impliescp ≥ cp)
and then extend it to work for the general model. Our rounding algorithm only loses a constant
factor, and, hence, we obtain an O(log k)-competitive algorithm for generalized caching.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.  
6:4 A. Adamaszek et al.
2 THE LINEAR PROGRAM
This section describes an LP for the generalized caching problem. It also shows how to generate
good variable assignments which are used in the rounding algorithm of the next section. Although
there are some minor notational differences, this largely follows the work of Bansal, Buchbinder,
and Naor [4].
We assume that cost is incurred when a page is evicted from the cache, not when it is loaded into
the cache. This means that we will not pay anything for the last time we load a page into the cache
that remains in the cache at the end. In the general model, we may assume that the last request
is always to a page of size k and zero cost. This does not change the cost of any algorithm in the
original cost model. However, it does ensure that the cost in our alternate cost model matches the
cost in the original model.
We begin by describing an integer program IP for the generalized caching problem. The IP
has variables x (p,i) indicating if page p has been evicted from the cache after the page has been
requested for the ith time. If x (p,i) = 1, page p was evicted after the ith request to page p and has
to be loaded back into the cache when the page is requested for the (i + 1)-st time. The total cost
is then
p

i cpx (p,i).
Let B(t) denote the set of pages that have been requested at least once until and including time
t and let r(p,t) denote the number of requests to page p until and including time t. In a time step
t in which page pt is requested, the total size of pages other than pt in the cache can be at most
k − wpt . Thus, we require

p ∈B(t)\ {pt }
wp (1 − x (p,r(p,t))) ≤ k − wpt .
Rewriting the constraint gives

p ∈B(t)\ {pt }
wpx (p,r(p,t)) ≥

p ∈B(t)
wp − k.
To shorten the notation, we define the total weight of a set of pages S as W (S) :=
p ∈S wp . Altogether, this results in the following IP formulation for the generalized caching problem.
min
p

i
cp x (p,i)
s.t. ∀t :

p ∈B(t)\ {pt }
wp x (p,r(p,t)) ≥ W (B(t)) − k
∀p,i : x (p,i) ∈ {0, 1}
(IP1)
To decrease the integrality gap of our final LP relaxation, we add additional, redundant constraints
to the IP.
min
p

i
cp x (p,i)
s.t. ∀t∀S ⊆ B(t) : W (S) > k :

p ∈S\ {pt }
wp x (p,r(p,t)) ≥ W (S) − k
∀p,i : x (p,i) ∈ {0, 1}
(IP2)
Unfortunately, even the relaxation of this IP formulation can have an arbitrarily large integrality
gap. However, in an integral solution, any wp > W (S) − k cannot give any additional advantage
over wp = W (S) − k for a constraint involving set S. Therefore, it is possible to further strengthen
the constraints without affecting an integral solution. For this, we definew˜ S
p := min{W (S) − k,wp }.
Relaxing the integrality constraint, we obtain an LP. As shown in Observation 2.1 of Bansal
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.            
An O(log k)-Competitive Algorithm for Generalized Caching 6:5
PROCEDURE 1: fix-set(S,t, x,y)
Input: Current time step t, current variable assignments x and y, aminimal set S ⊆ B(t).
Output: Terminates if S becomes non-minimalor constraint t, S in primal-LP is satisfied and returns the new assignments for x and y.
1: while
p ∈S\ {pt } w˜ S
p · x (p,r(p,t)) < W (S) − k do {constraint for t, S violated}
2: infinitesimally increase yS (t)
3: for each p ∈ S do
4: v :=
τ :r(p,τ )=r(p,t),ppτ

S ⊆B(τ ):p ∈S,W (S )>k w˜ S
p yS (τ ) − cp
{v is a violation of the dual constraint for x (p,r(p,t))}
5: if v ≥ 0 then x (p,r(p,t)) := 1
k exp  v
cp

6: if x (p,r(p,t)) = 1 then return {S is not minimal any more}
7: end for
8: end while
9: return {the primal constraint for step t and set S is fulfilled}
et al. [4], we can assume without loss of generality that x (p,i) ≤ 1. This results in the final LP
formulation.
min
p

i
cp x (p,i)
s.t. ∀t ∀S ⊆ B(t) : W (S) > k :

p ∈S\ {pt }
w˜ S
p x (p,r(p,t)) ≥ W (S) − k
∀p,i : x (p,i) ≥ 0.
(primal-LP)
The dual of primal-LP is
max
t

S ⊆B(t):W (S )>k
(W (S) − k) yS (t)
s.t. ∀p,i :

t:r(p,t)=i,ppt

S ⊆B(t):p ∈S,W (S )>k
w˜ S
p yS (t) ≤ cp
∀t ∀S ⊆ B(t) : W (S) > k : yS (t) ≥ 0.
(dual-LP)
Procedure 1 will be called by our online rounding algorithm to generate assignments for the LP
variables. Note that, as the procedure will not be called for all violated constraints, the variable
assignments will not necessarily result in a feasible solution to primal-LP but will have properties
which are sufficient to guarantee that our rounding procedure produces a feasible solution. We
assume that all primal and dual variables are initially zero.
For a time step t, we say a set of pages S is minimal if, for every p ∈ S, x (p,r(p,t)) < 1. We note
that by Observation 2.1 of Bansal et al. [4], whenever there is a violated constraintt, S in primal-LP,
there is also a violated constraint t, S ⊆ S for a minimal set S
. The idea behind Procedure 1 is that
it is called with a minimal set S. The procedure then increases the primal (and dual) variables of
the current solution in such a way that one of two things happen: Either the set S is not minimal
any more because the value of x (p,r(p,t)) reaches 1 for some page p ∈ S, or the constraint t, S is
no longer violated. At the same time, the following theorem guarantees that the primal variables
are not increased too much; that is, that the final cost is still bounded by O(log k) times the cost
of an optimal solution. Its proof follows exactly the proof of Theorem 3.1 from Bansal et al. [4].
Theorem 2.1. Let x (p,i) be the final variable assignments generated by successive calls to Procedure 1 (with different subsets). The total cost
p

i cpx (p,i) is at most O(log k) times the cost of an
optimal solution to the caching problem.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.                
6:6 A. Adamaszek et al.
Observe that our online algorithm generates the calls to Procedure 1. Sometimes it may succeed
in constructing a rounded solution from an infeasible assignment to the x (p,i)’s. In this case, it
will not continue to call Procedure 1, and, hence, the final assignment to the x (p,i)’s may not be
feasible. However, this is not a problem for the analysis.
3 THE ONLINE ALGORITHM
The online algorithm for the generalized caching problem works as follows. It computes primal
and dual assignments x and y for LPs primal-LP and dual-LP, respectively, by repeatedly finding
violated primal constraints and passing the constraint together with the current primal and dual
solution to Procedure 1.
PROCEDURE 2: online-step(t)
1: x (pt,r(pt,t))  0 {put page pt into the cache}
{some constraints may be violated}
2: S  {p ∈ B(t) | γ · x (p,r(p,t)) < 1}
3: while constraint for S is violated do
4: fix-set(S,t, x,y) {change the current solution}
5: adjust distribution μ to mirror new x
6: S  {p ∈ B(t) | γ · x (p,r(p,t)) < 1} {recompute S}
7: end while
{the constraints are fulfilled}
In addition to the fractional solutions x and y, the online algorithm maintains a probability distribution over valid cache states. Specifically, μ will be the uniform distribution over k2 subsets,
with each subset specifying the set of pages that are not present in the cache. Some of the k2 subsets may be identical. A randomized algorithm then chooses a random number r from [1,..., k2]
and behaves according to the r-th subset (i.e., whenever the rth subset changes, it performs the
corresponding operations). Note that this rounding approach differs substantially from the results
by Bansal, Buchbinder, and Naor [3, 4] since it constructs a distribution over a small number of
cache states. The approaches in Bansal et al. [3, 4] do not allow us to obtain a good upper bound
on the number of states.
We will design the distribution μ in such a way that it closely mirrors the primal fractional
solution x. In Section 3.1, we will show that each set in the support of μ is a complement of a valid
cache state (i.e., the size constraints are fulfilled and the currently requested page is contained in
the cache). In Section 3.2, we will show a way of updating μ in such way that a change in the
fractional solution of the LP that increases the fractional cost by ε is accompanied by a change in
the distribution μ with (expected) cost at most O(ε).
The rounding algorithm loses only a constant factor, which gives us a O(log k)-competitive
algorithm for generalized caching.
Procedure 2 gives the outline of a single step of the online algorithm, where γ is a scaling factor
which is explained in the following.
3.1 Ensuring that Cache States are Valid
We will set up some constraints for the sets in the support of μ which guarantee that the sets
describe complements of valid cache states. In order to define these constraints, we introduce the
following notation.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.   
An O(log k)-Competitive Algorithm for Generalized Caching 6:7
Fig. 1. The size classes Si and the set of large pages L obtained from the set S.
Let t denote the current time step and set xp := x (p,r(p,t)). Let γ ≥ 2 denote a scaling factor to
be chosen later and define zp := min{γxp, 1}. The variable zp is a scaling of the primal fractional
solution xp . We also introduce a rounded version of the scaling: We define z¯p := k · zp 	/k, which
is simply the value of zp rounded down to the nearest multiple of 1/k. Note that due to the way the
LP-solution is generated, zp > 0 implies that zp ≥ γ /k. Therefore, rounding down can only change
the value of zp by a small factor. More precisely, we have z¯p ≥ (1 − 1/γ ) · zp .
We use S to denote the set of pages p that are fractional in the scaled solution (i.e., have zp < 1,
or, equivalently, z¯p < 1). We divide these pages into size classes as follows. The class Si contains
pages whose size falls into the range [2i
, 2i+1). See Figure 1 for an illustration.
We construct a set L ⊆ S of “large pages” by selecting pages from S in decreasing order of size
(ties broken according to page-id) until either the values of z¯ for the selected pages add up to at
least 1, or all pages in S have been selected. We use w to denote the size of the smallest page in L,
and i to denote its class-id. Note that this construction guarantees that either 1 ≤
p ∈L z¯p < 2 or
L = S. The following claim shows that the second possibility only occurs when the weight of S is
small compared to the size of the cache k or while the online algorithm is serving a request (e.g.,
when the online algorithm iterates through the while-loop of Procedure 2).
Claim 3.1. After a step of the online algorithm, we either have 1 ≤
p ∈L z¯p < 2 or W (S) ≤ k.
Proof. If W (S) ≤ k there is nothing to prove. Otherwise, we have to show that we do not run
out of pages during the construction of the set L. Observe that after the while-loop of Procedure 2
finishes, the linear program enforces the following condition for the subset S:

p ∈S
min{W (S) − k,wp } · xp ≥ W (S) − k.
In particular, this means that
p ∈S xp ≥ 1 and hence
p ∈S z¯p ≥ (1 − 1/γ )γ ≥ 1, as γ ≥ 2. Since the
values of z¯p for the pages p in S sum up to at least 1, we will not run out of pages when constructing L.
Let D denote a subset of pages that are evicted from the cache. With a slight abuse of notation
we also use D to denote the characteristic function of the set; that is, for a page p we write D(p) = 1
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.        
6:8 A. Adamaszek et al.
if p belongs to D and D(p) = 0 if it does not. We are interested whether at time t the set D describes
a complement of a valid cache state.
Definition 3.2. We say that a subset D of pages γ -mirrors the fractional solution x if:
(1) ∀p ∈ B(t) : z¯p = 0 implies D(p) = 0 (i.e., p is in the cache).
(2) ∀p ∈ B(t) : z¯p = 1 implies D(p) = 1 (i.e., p is evicted from the cache).
(3) For each class Si : 

p ∈Si z¯p 	 ≤
p ∈Si D(p). (class constraints)
(4) 

p ∈L z¯p 	 ≤
p ∈L D(p). (large pages constraint)
Here z¯ is the solution obtained after scaling x by γ and rounding down to multiples of 1/k.
We refer to the constraints in the first two properties as integrality constraints, to the constraints
in the third property as class constraints, and to the constraint in the fourth property as the large
pages constraint.
Lemma 3.3. A subset of pages that γ -mirrors the fractional solution x to the linear program describes a complement of a valid cache state for γ ≥ 16.
Proof. Let D denote a set that mirrors the fractional solution x. In order to show that D is a
complement of a valid cache state, we need to show that the page pt which is accessed at time t is
not contained in D and that the size of all pages which are not in D sums up to at most k.
Observe that the fractional solution is obtained by applying Procedure 1. Therefore, at time t
the variable xpt = x (pt,r(pt,t)) has value 0. (It is set to 0 when Procedure 2 is called for time t,
and it is not increased by Procedure 1.) Hence, we have z¯pt = 0 and Property 1 in Definition 3.2
guarantees that pt is not in D.
It remains to show that the size of all pages which are not in D sums up to at most k. This means
we have to show

p ∈B(t)\ {pt }
wpD(p) ≥ W (B(t)) − k. (1)
Because of the integrality constraints we have

p ∈B(t)\ {pt }
wpD(p)
=

p ∈B(t)\S
wpD(p) +

p ∈S\ {pt }
wpD(p)
=

p ∈B(t)\S
wp +

p ∈S
wpD(p)
= W (B(t)) −W (S) +

p ∈S
wpD(p).
In order to obtain Equation (1), it suffices to show that

p ∈S\ {pt }
wpD(p) ≥ W (S) − k.
For the case that W (S) ≤ k, this is immediate since the left-hand side is always non-negative.
Therefore, in the following, we can assume that W (S) > k, and, hence, 1 ≤
p ∈L z¯p < 2 due to
Claim 3.1. Recall that w is the size of the smallest page in L and that i denotes the corresponding
class-id.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.               
An O(log k)-Competitive Algorithm for Generalized Caching 6:9
If 2i ≥ W (S) − k, then

p ∈S
wpD(p) ≥

p ∈L
wpD(p) ≥ 2i

p ∈L
D(p) ≥ 2i ≥ W (S) − k,
where the third inequality follows from the large pages constraint and the fact that
p ∈L z¯p ≥ 1.
In the remainder of the proof, we can assume 2i < W (S) − k. We have

p ∈S
wpD(p) ≥

i ≤i

p ∈Si
wpD(p)
≥

i ≤i
2i ·

p ∈Si
D(p)
≥

i ≤i
2i ·




p ∈Si
z¯p − 1



= 1
2

i ≤i

p ∈Si
2i+1
z¯p −

i ≤i
2i
≥
1
2

i ≤i

p ∈Si
wpz¯p − 2i+1
≥ γ
4

i ≤i

p ∈Si
w˜ S
p xp − 2(W (S) − k).
(2)
Here, the second inequality follows since wp ≥ 2i for p ∈ Si , the third inequality follows from the
class constraints, and the fourth inequality holds since wp ≤ 2i+1 for p ∈ Si . The last inequality
uses the fact that z¯p ≥ (1 − 1/γ )γxp ≥ γ /2 · xp for every p ∈ S and that wp ≥ w˜ S
p .
Using the fact that z¯p ≥ γ /2 · xp we get
γ
4

p ∈L\Si
w˜ S
p xp ≤
1
2

p ∈L
w˜ S
p z¯p ≤
1
2
(W (S) − k)

p ∈L
z¯p ≤ W (S) − k,
where the last inequality uses
p ∈L z¯p ≤ 2. Adding the inequality 0 ≥ γ
4

p ∈L\Si w˜ S
p xp − (W (S) −
k) to Equation (2) gives

p ∈S
wpD(p) ≥ γ
4

p ∈S
w˜ S
p xp − 3(W (S) − k) ≥ (γ /4 − 3)(W (S) − k) ≥ W (S) − k,
for γ ≥ 16. Here, the second inequality holds because, after serving a request, the online algorithm
guarantees that the constraint
p ∈S w˜ S
p xp ≥ W (S) − k is fulfilled for the current set S.
3.2 Updating the Distribution Online
We will show how to update the distribution μ over subsets of pages online in such a way that
we can relate the update cost to the cost of our linear programming solution x. We show that, in
each step, the subsets in the support of μ mirror the current linear programming solution. Then
Lemma 3.3 guarantees that we have a distribution over complements of valid cache states.
However, directly ensuring all properties in Definition 3.2 leads to a very complicated algorithm.
Therefore, we partition this step into two parts. We first show how to maintain a distribution μ1
over subsets D that fulfill the first three properties in Definition 3.2 (i.e., the integrality constraints
and the class constraints). Then we show how to maintain a distribution μ2 over subsets that fulfill
the first and the last property.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.                                           
6:10 A. Adamaszek et al.
Fig. 2. Cost classes.
From these two distributions, we obtain the distribution μ as follows. We sample a subsetD1 from
the first distribution and a subset D2 from the second distribution, and we compute D = D1 ∪ D2
(or D = max{D1,D2} if D is viewed as the characteristic function of the set).
Clearly, if both sets D1 and D2 fulfill Property 1 from Definition 3.2, then the union fulfills Property 1. Furthermore, if one of D1, D2 fulfills one of the properties 2, 3, or 4, then the corresponding
property is fulfilled by the union as these properties only specify a lower bound on the characteristic function D.
We will construct μ1 and μ2 to be uniform distributions over k subsets. Then, the combined
distribution μ is a uniform distribution over k2 subsets, where some of the subsets may be identical.
In the following, we assume that the values of z¯p change in single steps by 1/k. This is actually
not true. Consider, for example, Line 1 of Procedure 2 where, after the time step t is increased,
the variable x (pt,r(pt,t)) is set to 0. As xpt is a shorthand for x (pt,r(pt,t)), the value of xpt , and
therefore also the value of z¯pt , is set to 0. However, the drop in the value of z¯pt larger than 1/k can
be simulated by several consecutive changes by a value of 1/k.
3.2.1 Maintaining Distribution μ1. In order to be able to maintain the distribution μ1 at a small
cost we strengthen the conditions that the sets D in the support of μ1 have to fulfill. For each size
class Si we introduce cost classes C0
i ,C1
i ,... where Cs
i = {p ∈ Si : cp ≥ 2s } (see Figure 2). Note that
we have Si = C0
i .
For the subsets D in the support of μ1 we require:
A. For each subset D, for each size class Si , and for all cost classes Cs
i

p ∈Cs
i
z¯p
	
≤

p ∈Cs
i
D(p) ≤


p ∈Cs
i
z¯p

.
B. For each page p

D
D(p) · μ1 (D) = z¯p .
Note that the first set of constraints ensures that class constraints are fulfilled. This holds because
C0
i = Si . Hence, the left inequality for C0
i is exactly the class constraint for class Si . The second set
of constraints implies the integrality constraints. An example of a distribution that satisfies the
constraints A and B is given in Figure 3.
Increasing z¯p . Suppose that for some page p the value of z¯p increases by 1/k (see Figure 4 for
an example). Assume that p ∈ Si and cp ∈ [2r, 2r+1) (i.e., p ∈ C0
i ,...,Cr
i ). As we have to satisfy
the property
D D(p)μ1 (D) = z¯p , we have to add the page p to a set D∗ in the distribution μ1,
which currently does not contain p (i.e., we have to set D∗ (p) = 1 for this set). We choose this set
arbitrarily.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.     
An O(log k)-Competitive Algorithm for Generalized Caching 6:11
Fig. 3. An example of distribution μ1 for the cache of size k = 4 and the set of pages Si = {p1,p2,...,p8}. The
values z¯pi are given at the top of the figure. μ1 is a uniform distribution over 4 sets: D1 = {p2,p3,p5,p7,p8},
D2 = {p1,p3,p4,p6,p7}, D3 = {p1,p2,p4,p6,p8}, and D4 = {p1,p5,p7,p8}. Constraints A and B are satisfied.
However, after this step, some of the constraints

p ∈Cs
i
z¯p
	
≤

p ∈Cs
i
D(p) ≤


p ∈Cs
i
z¯p

corresponding to the cost classes Cs
i for s ≤ r may become violated. We repair the violated constraints step by step from s = r to 0. We do that by moving the pages between the sets D in such
a way that, while repairing the constraints for the cost class Cs
i , we keep the following invariant:
All but one of the sets D from the support of μ has the same number of pages from the set Cs
i as
they had before we increased the value of z¯p . The remaining set, which we denote by D+, has one
additional page. At the beginning D+ = D∗.
Notice that
p ∈Cs
i z¯p =
D

p ∈Cs
i D(p) · μ1 (D) is equal to the average number of pages fromCs
i
that a set in the support of μ1 has. If the number of pages from Cs
i in the sets D in the support of
μ1 differs by at most one, each set has either 

p ∈Cs
i z¯p 	 or 

p ∈Cs
i z¯p  pages from Cs
i , and all the
constraints for Cs
i are satisfied.
Fix s and assume that the constraints hold for all s > s, but some are violated for s. Let a :=


p ∈Cs
i z¯p − 1
k ; that is, before increasing the value of z¯p each set D contained at most a, and at
least a − 1, pages from Cs
i . As the only set that has now a different number of pages from Cs
i is D+,
and some constraints for Cs
i are violated, it must be the case that D+ has now a + 1 pages from
Cs
i , and some set D with positive support in μ1 has a − 1 pages from Cs
i . The constraints for Cs+1 i
are satisfied, so the number of pages in class Cs+1 i differs by at most 1 between D+ and D
. Hence,
there must exist a page in Cs
i \ Cs+1 i that is in D+ but not in D
. We move this page to D
, which
incurs an expected cost of at most 2s+1/k. Now all the sets in the support of μ1 have either a − 1
or a pages from Cs
i , and so all the constraints for Cs
i are satisfied. As we did not modify any pages
from Cs+1 i , the constraints for values s > s remain satisfied. Now the set D has one additional
page, and it becomes the new set D+.
Performing the preceding procedure incurs expected cost of 2s+1/k fors from r to 0. In total, we
have expected cost O(2r /k). The increase of z¯p increases the LP-cost by at least 2r /k. Therefore,
the cost in maintaining the distribution μ1 can be amortized against the increase in LP-cost.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.                
6:12 A. Adamaszek et al.
Fig. 4. In this setting as in Figure 3, the value of z¯p6 increases by 1/k = 1/4. a) To satisfy Constraint B,
we add page p6 to the set D4. After this modification, Constraint A is not satisfied for the cost class C2
i =
{p5,p6,p7,p8} and the set D4. b) To satisfy Constraint A for the cost class C2
i , we move the page p5 from D4
to D3. Now Constraint A is satisfied for the cost classes C3
i ,C2
i , and C1
i , but it is violated for C0
i and the sets
D3,D4. c) To satisfy Constraint A for the cost class C0
i , we move the page p2 from D3 to D4. Now all the
constraints are satisfied.
Decreasing z¯p . When for some page p the value of z¯p decreases, we have to delete the page p
from a set D in the support of μ1 that currently contains p. The analysis for this case is completely
analogous to the case of an increase in z¯p . The resulting cost of O(2r /k), where cp ∈ [2r, 2r+1),
can be amortized against the cost of LP—at a loss of a constant factor, we can amortize O(2r /k)
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.
An O(log k)-Competitive Algorithm for Generalized Caching 6:13
Fig. 5. Each page p ∈ S with z¯p = i/k hasi corresponding points (here k = 6). The set of k points with highest
priority (Q) and the set of large pages (L) are marked in gray.
against the cost of LP when the value of z¯p increases and by the same amount when the value of
z¯p decreases.
Change of the Set S. The class constraints depend on the set S that is dynamically changing.
Therefore we have to check whether the constraints are fulfilled if a page enters or leaves the set
S. When a page p with cp ∈ [2r, 2r+1) increases its z¯p value to 1, we first add it to the only set in
the support of μ1 that does not contain it. This induces an expected cost of at most 2r+1/k. Then
we fix Constraint A, as described in the procedure for increasing a z¯p value. This also induces
expected cost O(2r /k). After that, we remove the page from the set S. Constraint A will still be
valid because, for every cost class Cs
i that contains p and for every set D in the support of μ1, the
values of
p ∈Cs
i z¯p and
p ∈Cs
i D(p) change by exactly 1.
3.2.2 Maintaining Distribution μ2. We will show how to maintain a distribution μ2 over subsets
of pages such that each set D in the support of μ2 fulfills the large pages constraint and does not
contain any page p for which z¯p = 0. Note that as
p ∈L z¯p < 2, the large pages constraint can be
reformulated as follows: If
p ∈L z¯p ≥ 1, then each subset D in the support of μ2 contains at least
one page from the set of large pages L.
In the following, we introduce an alternative way of thinking about this problem. A variable z¯p
can be in one of k + 1 different states {0, 1/k, 2/k,..., 1 − 1/k, 1}. We view the k − 1 nonintegral
states as points. We say that the th point for page p appears (or becomes active) if the value of z¯p
changes from ( − 1)/k to /k. Points can disappear for two reasons. Suppose the th point for
page p is active. We say that it disappears (or becomes inactive) if either the value of z¯p decreases
from /k to ( − 1)/k or the value of z¯p reaches 1.
Note that if for a page p we have z¯p = 1, the page p is not in the set S, and it only enters S once
the value of z¯p is decreased to 0 again. The appearance of a point for page p corresponds to a cost
of cp /k of the LP-solution. At a loss of a factor of 2, we can also amortize cp /k against the cost of
the LP-solution when a point for page p disappears.
Observation 3.4. The set of pages with active points is the set of pages in S with the value z¯p  0.
Observation 3.4 says that if we guarantee that a set D in the support of μ2 can contain only those
pages from S which have an active point, we guarantee one of our constraints—the set D—does
not contain any page p for which z¯p = 0.
We assign priorities to the active points according to the size of the corresponding page, where
points corresponding to larger pages have higher priorities. Ties are broken first according to pageids and then to the point-numbers. At any time step, Q denotes the set of the k active points with
highest priority or all active points if there are less than k (see Figure 5). The following observation
follows directly from the definition of Q and L, as we used the same tie-breaking mechanisms for
both constructions.
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.           
6:14 A. Adamaszek et al.
Observation 3.5. For any time step, the set of pages p in L that have a value of z¯p  0 is exactly
the set of pages that have at least one point in Q.
We assign to active points labels from the set {1,..., k}, with the meaning that if a point q has
label , then the th set in the support of μ2 contains the page corresponding to q. At each point in
time, the th set consists of pages for which one of the corresponding points has label . In general,
we will allow a point to have several labels. Note that this definition of the sets in the support of
μ2 directly ensures that a page that has z¯p = 0 is not contained in any set in the support of μ2,
because a page with this property does not have any active points.
Adding a label to a point q increases the expected cost of the online algorithm by at mostcp(q)/k,
where p(q) is the page corresponding to the point q. Deleting a label is free, and, in particular, if a
point disappears (meaning its labels also disappear), the online algorithm has no direct cost while
we can still amortize cp /k against the LP-cost.
The following observation forms the basis for our method of maintaining the distribution μ2.
Observation 3.6. If the points in Q have different labels, then all sets in the support of the distribution μ2 fulfill the large pages constraint.
This means that we only have to show that there is a labeling scheme that, on one hand, has a
small relabeling cost (i.e., the cost for relabeling can be related to the cost of the LP-solution) and
that, on the other hand, guarantees that at any point in time no two points from Q have the same
label. We first show that a very simple scheme exists if the cost function is monotone in the page
size (i.e., wp ≤ wp implies cp ≤ cp for any two pages p, p
). Note that the bit model and the fault
model that have been analyzed by Bansal et al. [4] have monotone cost functions. Therefore, the
following section gives an alternative proof for an O(log k)-competitive algorithm for these cost
models.
3.2.3 Maintaining μ2 for Monotone Cost. We show how to maintain a labeling of the set Q such
that all labels assigned to points are different. Assume that currently every point in the set Q has
a single unique label.
Appearance of a Point q. Suppose that a new point q arrives. If q does not belong to the k points
with the highest priority, it will not be added to Q and we do not have to do anything.
If the set Q currently contains strictly less than k points, then the new point will be contained
in the new set Q, but at least one of the k labels has not been used before and we can label q with
it. In the new set Q, all points have different labels. The online algorithm paid a cost of cp(q)/k,
where p(q) denotes the page corresponding to the point q.
If Q already contains k pages, then, upon appearance of q, a point q with lower priority is
removed from Q and q is added. We can assign the label of q to the new point q, and then all
points in the new set Q have different labels. Again, the online algorithm pays a cost of cp(q)/k.
In all cases, the online algorithm pays at most cp(q)/k whereas the LP-cost is cp(q)/k.
Disappearance of a Point q. Now, suppose that a point q in the current set Q is deleted. This
means that a point q with a lower priority than q may be added to the set Q (if there are at least k
points in total). We give q the same label that q had. This incurs a cost ofcp(q)/k ≤ cp(q)/k, where
the inequality holds due to the monotonicity of the cost function (note that this is the only place
where monotonicity is used). Since we can amortize cp(q)/k against the cost of the LP-solution we
are competitive.
3.2.4 Maintaining μ2 for General Cost. We want to assign labels to points in Q in such a way
that we are guaranteed to see k different labels if the set Q contains at least k points. In the last
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.             
An O(log k)-Competitive Algorithm for Generalized Caching 6:15
Fig. 6. Each page p ∈ S with z¯p = i/k hasi corresponding points (here k = 6). The sets Qi of points with the
highest priority that correspond to the pages with cost at least 2i have been marked. The sets Q0 and Q1
have k points each, and the set Q2 has less than k points.
Fig. 7. Labelings for the sets Qi (k = 4). The 8 active points are ordered increasingly with respect to the
priority. The size of the dots corresponds to the cost of 1, 2, or 4 of the respective pages: larger dots represent
larger costs. Each point has a set of labels assigned to it. Each set Qi contains 4 points, with the highest
priorities among the points with cost at least 2i . For each set Qi we are given a valid labeling.
section, we did this by always assigning different labels to points in Q. For the case of general cost
functions, we proceed differently.
Let Qi denote the set of k active points with the highest priority that correspond
to pages with cost at least 2i
. In case there are less than k such points, Qi contains
all active points corresponding to pages with cost at least 2i (see Figure 6). Note
that Q = Q0.
Essentially, our goal is to have a labeling scheme with small relabeling cost that guarantees that
each set Qi sees at least |Qi | different labels. Since Q = Q0, this gives the desired result. However,
for the case of general cost, it will no longer be sufficient to assign unique labels to points, but
we will sometimes be assigning several different labels to the same point. At first glance, this may
make a relabeling step very expensive in case a point with a lot of labels disappears.
To avoid this problem, we say that a set Qi has to commit to a unique label for every point q
contained in it, where the chosen label is from the set of all labels assigned to q (see Figure 7).
The constraint for Qi is that it commits to different labels for all points contained in it. If a point
currently has labels  and
, then a set Qi may either commit to  or
, but, furthermore, during
an update operation it may switch the label it commits to for free (i.e., no cost is charged to the
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.      
6:16 A. Adamaszek et al.
Fig. 8. In this setting as in Figure 7, a new point (marked in gray) arrives, which is assigned Label 4. Then
the point with lowest priority disappears from each Qi . Afterward, Q2 contains k different labels. For Q1, the
rightmost element recommits to Label 3, and afterward the label of the leftmost element is changed to 2 at
small cost. For Q0, only the label of the third element is changed.
online algorithm). Recall that if a point corresponding to a page p has several labels, then all sets
D corresponding to these labels contain the page p; therefore, committing to a different label is
free as no change has to be made for any set D from the support of μ2. The label to which a set Qi
commits for a point q ∈ Qi is denoted by Qi (q).
Appearance of a Point q. Suppose that a point q0 corresponding to a page p with cp ∈ [2r, 2r+1)
appears. This increases the cost of the LP by at least Ω(2r /k). We assign an arbitrary label 0 to
this point, and, as 0 is the only label of q0, we set Qs (q0) = 0 for all subsets Qs that contain q0.
Assigning a new label corresponds to evicting the page in some cache state D and, consequently,
induces an expected cost of at most 2r /k for the online algorithm.
It remains to adjust the labeling so that the labeling of every set Qi becomes valid again. The
sets Qs where s > r are not affected by the appearance of q0, and their labelings remain valid. We
only have to fix the labelings for the sets Qs where s ≤ r. We will do this while only paying at
most O(2s /k) for every s ≤ r. This cost can be amortized against the cost of the LP.
Assume that labelings for all sets Qs where s > s are valid, but the labeling for Qs is violated.
We want to fix it, paying only O(2s /k). We call a label  a duplicate label for Qs if there exist
two points in Qs for which Qs commits to . We call the corresponding points duplicate points.
The labeling is valid if and only if there exist no duplicate points. We call a label  free for Qs if
currently there is no point in Qs for which Qs commits to . When we start processing Qs there
exists at most one duplicate label; namely, the label 0 that we assigned to q0 and for which we
have Qs (q0) = 0.
Since the total number of labels is k and there are at most k points in Qs , there must exist a
free label free. We could fix the condition for Qs by assigning the label free to one of the duplicate
points q and setting Qs (q) = free. However, this would create a cost that depends on the cost of
the page corresponding to the chosen point q. This may be too large, as our aim is to only pay
O(2s /k) for fixing the condition for set Qs . Therefore, we will successively switch the labels that
Qs commits to for the duplicate points until the cost of one of the duplicate points q is in [2s , 2s+1).
During this process, we will maintain the invariant that there are at most two duplicate points for
Qs . Hence, in the end, we can assign the free label free to a duplicate point q with cost at most
2s+1, set Qs (q) = free, and obtain a valid labeling for Qs .
The process of switching the labels for the set Qs is as follows. Suppose that currently  denotes
the duplicate label and that the two duplicate points both correspond to pages with cost at least
ACM Transactions on Algorithms, Vol. 15, No. 1, Article 6. Publication date: November 2018.                 
An O(log k)-Competitive Algorithm for Generalized Caching 6:17
2s+1. This means that both points are in the set Qs+1. As the labeling for Qs+1 is valid, we know
that Qs+1 commits to different labels for these points. One of these labels must differ from . Let
q denote the duplicate point for which Qs+1 commits to a label   . We set Qs (q
) =
. Now,
may be the new duplicate label for the set Qs .
The preceding process can be iterated. With each iteration, the number of points in the intersection of Qs and Qs+1 for which both sets commit to the same label increases by one. Hence, after
at most k iterations we either end up with a set Qs that has no duplicate points, or one of the
duplicate points corresponds to a page with cost smaller than 2s+1.
An example of fixing the labeling after adding a new point can be seen in Figure 8. As we only
pay cost 2s+1/k for fixing the labeling of Qs , the total payment summed over all sets Qs with s ≤ r
is O(2r /k), which can be amortized to the cost of LP.
Disappearance of a Point q. Now, suppose that a point q corresponding to a page p with cp ∈
[2r, 2r+1) is deleted. Then, a new point may enter the sets Qs for which s ≤ r. The only case for
which this does not happen is when Qs already contains all active points corresponding to pages
with cost at least 2s . For each Qs we commit to an arbitrary label for this point (recall that this
doesn’t induce any cost because any point, when it becomes active, gets a label). Now, for each
Qs , we have the same situation as in the case when a new point appears. The set either fulfills its
condition or has exactly two duplicate points. As before, we can fix the condition for set Qs at cost
O(2s /k), and the total cost of O(2r /k) can be amortized to the cost of LP.           