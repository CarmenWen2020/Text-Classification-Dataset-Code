We consider the problem of estimating the size of a maximum matching when the edges are revealed in a
streaming fashion. When the input graph is planar, we present a simple and elegant streaming algorithm
that, with high probability, estimates the size of a maximum matching within a constant factor using O˜ (n2/3)
space, where n is the number of vertices. The approach generalizes to the family of graphs that have bounded
arboricity, which include graphs with an excluded constant-size minor. To the best of our knowledge, this
is the first result for estimating the size of a maximum matching in the adversarial-order streaming model
(as opposed to the random-order streaming model) in o(n) space. We circumvent the barriers inherent in
the adversarial-order model by exploiting several structural properties of planar graphs, and more generally,
graphs with bounded arboricity. We further reduce the required memory size to O˜ (
√
n) for three restricted
settings: (i) when the input graph is a forest; (ii) when we have 2-passes and the input graph has bounded
arboricity; and (iii) when the edges arrive in random order and the input graph has bounded arboricity.
Finally, we design a reduction from the Boolean Hidden Matching Problem to show that there is no randomized streaming algorithm that estimates the size of the maximum matching to within a factor better than
3/2 and uses only o(n1/2) bits of space. Using the same reduction, we show that there is no deterministic
algorithm that computes this kind of estimate in o(n) bits of space. The lower bounds hold even for graphs
that are collections of paths of constant length.
CCS Concepts: • Mathematics of computing → Matchings and factors; • Theory of computation →
Probabilistic computation; Streaming models; Sketching and sampling;
Additional Key Words and Phrases: Streaming algorithms, maximal matching, planar graphs, bounded arboricity, estimating matching size
1 INTRODUCTION
As noted by Lovasz and Plummer in their classic book [22],“Matching Theory is a central part of
graph theory, not only because of its applications, but also because it is the source of important
ideas developed during the rapid growth of combinatorics during the last several decades.” In the
classical offline model, where we assume we have enough space to store all vertices and edges of
a graph G = (V, E), the problem of computing the maximum matching of G has been extensively
studied. The best result in this model is the 30-year-old algorithm due to Micali and Vazirani [25]
with running time O(m√
n), where n = |V | and m = |E|.
In contrast, in the streaming model, the algorithm has access to a sequence of edges, called a
stream. The algorithm reads edges in the order in which they appear in the stream. The main goal
is to design an algorithm that solves a given problem, using as little space as possible. In particular,
we wish that the amount of space be sublinear in the size of the input.
Note that the size of a maximum matching in a graph can be as large as Ω(n). Hence, there
is little hope to solve the problem exactly or even with a relatively good approximation in o(n)
space, since the algorithm has to remember the labels of endpoints for each edge in the matching.
This and similar constraints with other graph problems were the main motivation behind the
semi-streaming model introduced by Feigenbaum, Kannan, McGregor, Suri, and Zhang [8]. In this
model, the streaming algorithm is allowed to use O˜ (n) space (where the O˜ (·) notation hides factors
in logn, 1/ϵ, and log(1/δ ), i.e., O˜ (·) = poly(logn, 1/ϵ, log(1/δ )) · O(·)). The semi-streaming model
allows for finding a maximal matching (a 2-approximation for the maximum matching) using O˜ (n)
space in a greedy manner. For every new edge (u,v), we add it to the current matching if both u
and v are unmatched; otherwise, we discard it.
Unfortunately, in many real-world applications where the data is modeled by a massive graph,
we may not be able to store all vertices in main memory. Hence, we investigate the following natural question: Given the stream of edges of a massive graph G, how well can the maximum matching
size be approximated in o(n) space? We stress again that we focus on approximating the size of the
maximum matching, not computing a large matching.
The problem of estimating the size of a maximum matching of a graph has been studied relatively well in the case of sublinear-time algorithms [15, 27, 28, 32, 34]. Surprisingly, very little is
known about this problem in the streaming model, which is one of the most fundamental models
of the area of algorithms for big data. The only known result is a recent algorithm by Kapralov,
Khanna, and Sudan [18], which computes an estimate within a factor of O(polylog(n)) in the
random-order streaming model using O(polylog(n)) space. In the random-order model, the input
stream is assumed to be chosen uniformly at random from the set of all possible permutations of
the edges. However, to the best of our knowledge, if the algorithm is required to provide a good
approximation with high constant probability for any ordering of edges (we refer to this setting
as the adversarial-order model), nothing is known for the problem of estimating the size of a maximum matching using o(n) space.
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:3
1.1 Our Results
In this article, we mainly focus on the family of graphs with bounded arboricity. A graphG = (V, E)
has arboricity c if
c = max
U ⊆V

|E(U )|
|U | − 1

,
where E(U ) is the subset of edges with both endpoints in U .
1 Several important families of
graphs have constant arboricity. Examples include planar graphs, bounded genus graphs, bounded
treewidth graphs, and more generally, graphs that exclude a fixed minor.2 Our main result is a simple and elegant streaming algorithm with a constant approximation factor in the adversarial-model
for estimating the size of a maximum matching in graphs with bounded arboricity using O˜ (n2/3)
space (see Theorem 2). The problem is non-trivial even when the underlying graph is a tree. For
the case of trees (or forests), we give a 2(1 + ϵ )-approximation algorithm that uses O˜ (
√
n) space,
where ϵ is an arbitrarily small constant.
We complement our upper bounds with polynomial space lower bounds. We show that no algorithm that uses o(n1/2) bits of space can estimate the maximum matching size to within a factor
of less than 3/2 with probability at least 3/4 on every input. A deterministic algorithm that is required to never err and always provide an estimate within a factor better than 3/2 cannot use only
o(n1/2) bits of space. In fact, both lower bounds hold even for the special case of forests.
1.2 Unsuccessful Approaches
As mentioned before, little is known about the adversarial-order version of the problem. Before
sketching our main ideas, we briefly mention main difficulties in applying successful techniques
from related areas of algorithms. In the area of sublinear-time algorithms [15, 27, 28, 32, 34], the
technique of choice has been local exploration. A number of dynamic algorithms for matchings [3,
12, 16, 26, 29] have applied various partitioning techniques. The polylog(n)-approximation algorithm for random-order streams [18] employs a combination of both partitioning and exploration.
Local exploration is applied by sublinear-time algorithms for the maximum matching size. They
locally apply greedy techniques for constructing a good matching. Given random access to a graph
G = (V, E) with degree bounded by d, they sample a small number of vertices and explore their
local neighborhoods using breadth-first search to bounded depth. Using the information gathered
from the exploration, they estimate the fraction of vertices in a locally constructed large matching. By exploring this approach, one can obtain an algorithm that approximates the maximum
matching size within an additive error of ϵ · n in time d O(1/ϵ 2 ) [34].
The partitioning approach used by some dynamic algorithms for maintaining a large matching [3, 29] is based on a hierarchical decomposition of the vertex set into a logarithmic number of
layers. Depending on a specific approach taken, the decomposition can be a complicated function
of the graph structure and the history of operations on the graph. In general, high-degree vertices
tend to be included in higher layers and low-degree vertices tend to be included in lower layers.
The exact placement of a vertex depends, however, on its connections to other vertices. Equipped
with additional information, such a decomposition allows for easy access to a large matching,
which is a result of combining together large matchings involving each layer. Kapralov, Khanna,
and Sudan [18] show that one can get O(polylog(n))-approximation to the size of a maximum
matching in the random-order model, using O(polylog(n)) space. The intuition behind their ap1Equivalently, the arboricity of a graph can be defined as the minimum number of forests into which its edges can be
partitioned.
2It can be shown that for an H-minor-free graph, the arboricity number are O(h
√
h), where h is the number of vertices of
H [21].
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.  
48:4 H. Esfandiari et al.
proach can be interpreted as a combination of the local exploration and partitioning approaches.
In the random-order variant, for any arbitrary edge e, we may assume that a large fraction of
edges adjacent3 to e comes after e. Because of that one can get samples from neighborhoods of
vertices and edges in the graph. This turns out to be enough to construct a recursive exploration
procedure for estimating the number of vertices in a specific layer of the decomposition.
These approaches do not seem to be effective in the adversarial-order model. Local exploration
may be made difficult by placing edges in order unsuitable for following sequences of edges. This
idea is used in works [9, 13] to show lower bounds for the amount of space necessary for building
a BFS tree from a specific node and for verifying if two nodes are at small distance, even with a
few passes over the stream.
The partitioning approach is also difficult to implement due to the relations between vertices.
Identifying them require some amount of exploration. For instance, the algorithm of Kapralov
et al. [18] for random-order streams uses a recursive procedure for determining which layer
a given vertex belongs to. Executing the procedure requires exploration from low-degree to
high-degree vertices and sampling random neighbors of each visited vertex. In a random-order
stream, after reaching a high-degree vertex, the algorithm is still likely to see sufficiently many
incident edges. This is not guaranteed for adversarial-order streams.
1.3 Our Techniques
Our approach consists of two main parts. As discussed before, it is inherently hard to probe a certain neighborhood of the graph in the adversarial-order streaming model. Therefore in the first
part, we present combinatorial parameters of bounded-arboricity graphs (including graphs that exclude a fixed-minor) that (i) provide a constant approximation for the size of a maximum matching;
and more crucially (ii) computing the parameters do not involve a neighborhood search. In the second part, we design estimators for these parameters in the streaming model by sampling subgraphs
with certain properties and carefully bounding the positive correlation between the samples. We
hope that this approach together with the structural properties provided in this article may help
in estimating other graph properties in the streaming model as well. For a vertex v ∈ V , let deg(v)
denote the degree of vertex v in G. Let βG denote an upper bound on the average degree of every
subgraph of G. (We may drop the index G when it is clear from the context.) Recall that for planar
graphs, βG ≤ 6. In fact, if the arboricity of a graph is ν, it is easy to verify that βG ≤ 2ν. Thus, for
the family of graphs with constant arboricity (including graphs that exclude a fixed minor), we
may assume that βG is constant. We use the following intuition behind the structural properties.
Suppose we sample a small subset of vertices V  ⊆ V . Let G[V 
] denote the subgraph induced by
V 
. If the maximum degree of V  is constant, then every edge is adjacent to at most a constant
number of other edges. Thus, the size of a maximum matching in G[V 
] can be, in fact, approximated by simply the number of edges in G[V 
]. On the other hand, if the degrees of vertices are
large, most of the edges fall between V  and V 
, since the subgraph G[V 
] is sparse. Therefore,
in this case, one may hope to find a large matching between V  and V  that covers most of V 
.
Hence, the size of such a matching can be approximated by |V 
|. This intuition can be formalized
as follows.
Let μ = β + 3 denote a constant threshold. A vertex v is light, if deg(v) ≤ μ; otherwise, the
vertex is heavy. An edge is shallow if both of its endpoints are light. Throughout the article, let hG
and sG denote the number of heavy vertices and the number of shallow edges inG, respectively. We
may drop the index G when it is clear from the context. The crux of our algorithm is the following
two-fold structural property of graphs.
3Two edges are adjacent if they share an endpoint.
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:5
Lemma 1. Let G be a graph with maximum matching size M∗. We have the following bounds:
—upper bound: M∗ ≤ hG + sG ,
—lower bound: M∗ ≥ max{hG,sG }
η ,
where η = 1.25μ + 0.75. For graphs with bounded arboricity ν, the constant μ is at most 2ν + 3.
It is not hard to verify the upper bound of Lemma 1. Observe that every edge of the maximum
matching is either shallow or saturates at least one heavy vertex. Thus, the size of a matching
cannot be more than the combined number of shallow edges and heavy vertices. On the other
hand, proving the lower bound turns out to be quite non-trivial. In Section 3, we use the sparsity
of graphs with bounded arboricity together with an extension of Hall’s Theorem to prove the lower
bound on M∗.
By Lemma 1, estimating hG and sG leads to a constant-factor approximation for the size of a
maximum matching. Thus, for the second part of our work, we use random sampling to estimate
these parameters. For estimating hG , we sample a set of vertices and we find the number of heavy
vertices among them. We show that a sample of size O(
√
n) is enough for estimating hG . For
estimating sG , a major difficulty is that, in the adversarial-order setting, one does not hope to
maintain information about independent (or negatively correlated) samples of edges: when an edge
arrives, we may have already seen all the edges adjacent to it! Therefore, instead of sampling edges,
we sample a set of verticesV  and maintain the shallow-subgraph induced byV 
. We then show that
although the probability of sampling edges is positively correlated, the degree of dependency is
constant and thus the variance can be bounded, showing that the output of the estimator is highly
concentrated. However, to obtain a good estimator forsG , we need to maintain a shallow-subgraph
of size O(n2/3), which is indeed the space bottleneck.
Theorem 2. Let G be a graph with arboricity ν and n = ω(ν 2) vertices. Let ϵ, δ ∈ (0, 1] be two
arbitrary positive values less than one. With probability at least (1 − δ ), our algorithm estimates the
size of the maximum matching in G within a ((5ν + 9)(1 + ϵ )
2)-factor in the streaming model using
O˜ (νϵ−2 log(δ−1)n2/3) space. Both the update time and final processing time are O(log(δ−1)).
In particular, for planar graphs, by choosing δ = n−1 and ϵ as a small constant, the output of our
algorithm is within 25-approximation of the size of the maximum matching with probability at least
1 − 1
n using at most O˜ (n2/3) space.
We further study the limits of the approximation factor by considering the simple but still nontrivial case when the graph is a tree, or more generally, a forest with no isolated vertices. We
improve the approximation factor of our general result to (roughly) 2 for the special case of trees
while reducing the required memory size to O˜ (
√
n). Furthermore, in Section 6, we show significant
improvements may not be possible: using only o(
√
n) bits of space, one cannot estimate the size of
a maximum matching in forests within a factor better than 1.5.
Theorem 3. Let F be a forest with no isolated vertices. Let δ ∈ (0, 1] and ϵ ∈ (0, 1/5] be arbitrary.
With probability at least (1 − δ ), our algorithm estimates the maximum matching size in F within a
factor of 2(1 + 3ϵ ) in the streaming model using O˜ (ϵ−2 log(δ−1)
√
n) space. Both the update time and
final processing time are O(log(δ−1)).
Remark 4. We should mention that the assumption of having a forest F with no isolated vertices
in Theorem 3 is inherent in the proof. The 2(1 + 3ϵ )-estimator that we develop for Theorem 3 has
linear dependency on the number of connected components of F . If the forest F includes isolated
vertices, the estimator will not be a 2(1 + 3ϵ )-approximation of the size of a maximum matching
of F .
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.
48:6 H. Esfandiari et al.
Table 1. Known Results for Estimating the Size of a Maximum Matching in Data Streams
Reference Graph class Stream ordering Approximation factor Space
Folklore General Adversarial √
n O(
√
n)
Greedy General Adversarial 2 O(n)
Ref. [18] General Random O(polylog(n)) O(polylog(n))
This work Planar Adversarial 25 O˜ (n2/3)
This work Forests Adversarial 2 O˜ (
√
n)
This work Bounded Arboricity Adversarial O(1) O˜ (n2/3)
This work Bounded Arboricity Random O(1) O˜ (
√
n)
This work Bounded Arboricity 2-Pass Adver. O(1) O˜ (
√
n)
Improving to O˜ (
√
n) Space. We further show that by relaxing the streaming model, one can improve the required space of our algorithm. In particular, given either (i) a random-order stream ; or
(ii) two passes over an adversarial-order stream, we can (1 + ϵ )-approximate the number of shallow
edges using only O˜ (
√
n) space. This, in turn, gives O˜ (
√
n)-space algorithm that approximates the
size of the maximum matching within O(β) in these models.
In the 2-pass streaming model, we first sample O˜ (
√
n) edges uniformly at random. In the second
pass, we extract the set of shallow edges out of this sample set, which leads to an estimation for
sG . If the number of shallow edges is at least Ω( ˜ √
n), this estimator gives a (1 + ϵ )-approximation
factor. Otherwise, if the number of shallow edges is small, one can argue that maintaining a small
maximal matching and estimating hG is enough to obtain a constant factor approximation factor
for the size of the maximum matching. See Section 5 for a detailed discussion. For a (one-pass)
random-order stream, we can indeed use a similar technique. The idea is that the first O˜ (
√
n) edges
of the stream are, in fact, a random sample set. Therefore, we maintain the first O˜ (
√
n) edges and
we use the rest of the stream to find out how many of them are shallow. This again gives an
estimation for the number of shallow edges, which leads to an O(β)-approximation factor (see
Section 5 for more details).
Table 1 summarizes the known results for estimating the size of a maximum matching.
1.4 Further Related Streaming Work
The question of approximating the maximum cardinality matching has been extensively studied in
the streaming model. An O(n)-space greedy algorithm trivially obtains a maximal matching, which
is a 2-approximation for the maximum cardinality matching [8]. A natural question is whether
one can beat the approximation factor of the greedy algorithm with O˜ (n) space. Very recently,
it was shown that obtaining an approximation factor better than e
e−1  1.58 in one pass requires
n1+Ω(1/ log log n) space [11, 17], even in bipartite graphs and in the vertex-arrival model.4
Closing the gap between the upper bound of 2 and the lower bound of e
e−1 remains one of the
most appealing open problems in the graph streaming area (see Ref. [30]). The factor of 2 can be
improved on if one either considers the random-order model or allows for two passes [20]. By
allowing even more passes, the approximation factor can be improved to multiplicative (1 − ϵ )-
approximation via finding and applying augmenting paths with successive passes [1, 5, 6, 23, 24].
4In the vertex-arrival model, the vertices arrive in the stream together with all their incident edges. This setting has also
been studied in the context of online algorithms, where each arriving vertex has to be either matched or discarded irrevocably upon arrival. Seminal work due to Karp, Vazirani, and Vazirani [19] gives an online algorithm with e
e−1 approximation
factor in the online model.
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:7
Another line of research [7, 8, 23, 35] has explored the question of approximating the maximumweight matching in one pass and O˜ (n) space. Currently, the best known approximation factor
equals 4.9108 + ϵ (for any positive constant ϵ) [7].
1.5 Preliminaries
The Streaming Model. Let S be a stream of edges of an underlying graphG = (V, E). We assume that
the vertex set V is fixed and given, and that |V | = n. We assume that there is a unique numbering
for the vertices in V so that we can treat v ∈ V as a unique number v for 1 ≤ v ≤ n. We denote
an undirected edge in E with two endpoints u,v ∈ V by (u,v). The graph G can have at most
(
n
2 ) = n(n − 1)/2 edges. Thus, each edge can also be thought of as referring to a unique number
between 1 and (
n
2 ).
Adversarial Order. We work in the most-popular adversarial-order model. In this model, a streaming algorithm has to compute a satisfying solution with satisfying probability for every ordering
of items in the input stream. This should be contrasted with the easier random-order model, where
an algorithm has to provide a satisfying solution with satisfying probability for an input stream
permuted uniformly at random.
Approximation Factor. Throughout the article, let M∗ denote the size of a maximum matching
in G. Our task is to output M, an estimate of M∗, after receiving all the edges. A (randomized)
algorithm has approximation factor α with probability (1 − δ ) if for every permutation of input
edges, with probability at least (1 − δ ), we have M ≤ M∗ ≤ αM.
Concentration Bounds. We use an extension of the Chernoff-Hoeffding bound for negatively
correlated Boolean variables. It was first proved by Panconesi and Srinivasan [31].
Theorem 5 ([31]). If the Boolean random variables X1, X2,...,Xr are negatively correlated then
for X = r
i=1 Xi , μ = E[X], and 0 < δ ≤ 1, we have Pr[X < (1 − δ )μ] < e−μδ 2/2.
We also use Chebyshev’s inequality, which we state here for completeness.
Theorem 6 (Chebyshev’s Ineqality). Let X be a random variable with finite expected value μ
and finite non-zero variance σ2, we have Pr[|X − μ| ≥ kσ] ≤ 1
k2 .
2 ALGORITHM
To estimate the size of a maximum matching, by Lemma 1, it is sufficient to estimate the two parameters h and s, if one is willing to lose a constant (2η) in the approximation factor. Our algorithm
consists of three parallel subroutines. Each subroutine reports an estimated lower bound for the
size of a maximum matching. The joined result is the maximum of the three estimated values. Since
the probabilities of success for the second and the third subroutines are small, we boost the success
probabilities by independently running a logarithmic number of copies of these subroutines. The
final output is the median of the values reported by all copies. In what follows, we describe these
subroutines (see Algorithm 1).
Since we are restricted to sublinear space, it is natural to use sampling for estimating h and s.
However, the first obstacle occurs in the instances where both h and s are small. Thus, we do not
hope to capture a heavy vertex (or a shallow edge) using sublinear samples. We overcome these
instances by maintaining a maximal matching of the graph, up to a sublinear size. Therefore, for
instances where M∗ = O˜ (n2/3), we can estimate M∗ up to a factor of two (see Subroutine 1).
We now focus on the instances where M∗ is relatively large, and thus we cannot keep a maximal
matching in the memory. By the upper bound of Lemma 1, at least one of h or s is large. If the
number of heavy vertices is large, we can estimate h using a vertex-set sample. We sample O˜ (n2/3)
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018. 
48:8 H. Esfandiari et al.
random vertices and we maintain their degrees throughout the execution of the algorithm. We
note that in a vertex-set sample, we hit roughly Ω( h
n1/3 ) heavy vertices. Therefore, h would be
proportional to the fraction of sampled vertices that are heavy (see Subroutine 2).
Now, we are left with the most subtle scenario that h is small, but the number of shallow edges is
large. In this case, we face a new obstacle inherent of the adversarial setting; it is often not possible
to independently sample edges and maintain information about their neighborhood. At the time
that an edge arrives, we may have already seen all the adjacent edges, and thus, we will not be able
to distinguish whether an edge has a desired property. Recall that an edge is shallow if both its
endpoints are light. Therefore, instead of sampling edges, we maintain a shallow-subgraph sample.
ALGORITHM 1: Maximum Matching Estimator
Input: A stream of edges of the graph e1,..., em.
Output: An estimation M of the size of a maximum matching M∗.
Initialization Process:
1: Set α1 = 3, α2 = 3ϵ−2, α3 = 4ϵ−1 and Q = 26ln( 1
δ ) + 1.
2: Set c1 = c2 = c3 = 2/3.
3: Initialize Subroutine 1 with factor α1 and c1.
4: Initialize Q independent executions of Subroutines 2 and 3, with the corresponding α and c factors.
Update Process, upon the arrival of ei = (u,v):
1: Update each subroutine with the new arrival.
Termination Process:
1: Let R denote the return value of Subroutine 1.
2: Fori ∈ [Q], let ˆ
hi denote the value returned by the i-th execution of Subroutine 2. Similarly, let sˆi denote
that of Subroutine 3.
3: For i ∈ [Q], let Mi = max{α1n2/3, ˆ
hi,sˆi}.
4: if R < α1n2/3 then
5: Output M = R.
6: else
7: Let Mmed denote the median value of Mi ’s.
8: Output M = Mmed
(1+ϵ )η .
We first sample a set V  of O(n2/3) vertices. We maintain the degrees of these vertices throughout the execution. At an iteration i (i.e., after receiving the i-th edge), let L ⊆ V  denote the set
of vertices of V  with degree at most μ. We maintain all the edges induced by L
. Observe that
vertices may become heavy and leave L
, in which case, we will ignore their adjacent edges (see
Subroutine 3). This guarantees that we only need O˜ (μn2/3) space for keeping all the edges in our
shallow-subgraph sample. After processing the entire input stream, the maintained edges are exactly the shallow edges induced by V 
. Every vertex is sampled with probability (roughly) n−1/3;
however, the probabilities of sampling edges using sampling incident vertices are not independent.
Fortunately since we only have negative correlation, we can still say that a shallow edge falls in
the induced subgraph with probability roughly n−2/3. However, as mentioned before, we do not
hope to have an efficient sublinear sampling that independently (or even with negative correlation)
samples the edges. Our algorithm is not an exception either. For two edges that share an endpoint,
the probabilities of hitting them in a shallow-subgraph sample is positively correlated. In general,
one cannot hope for strong concentration bounds at the presence of positive correlation. However,
the crux of our analysis is that since the light vertices have bounded degree, the degree of dependency is constant. Therefore, we are able to bound the variance of our estimator and show that,
with a constant probability, the estimated value is highly concentrated around its expectation. We
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:9
SUBROUTINE 1. [Bounded Maximal Matching]
Input: A stream of edges of the graph e1,..., em.
Return Value: R := min{|M
|, α1nc1 } where M is a maximal matching, c1 = 2/3, and α1 = 3.
Initialization Process:
1: Initialize M to an empty set.
Update Process, upon the arrival of ei = (u,v):
1: if |M| < α1nc1 and M ∪ {ei} is a valid matching then
2: M ← M ∪ {ei}.
3: else
4: Discard ei .
Termination Process:
1: Return |M|.
SUBROUTINE 2. [Heavy Vertices]
Input: A stream of edges of the graph e1,..., em.
Return Value: ˆ
h, an estimation for the number of heavy vertices in the graph.
Initialization Process:
1: Initialize V  ⊂ V as a random subset of size α2nc2 where α2 = 3ϵ−2 and c2 = 2/3.
2: For every v ∈ V 
, initialize deg(v) to zero.
Update Process, upon the arrival of ei = (u,v):
1: if u ∈ V  then
2: deg(u) ← deg(u) + 1.
3: if v ∈ V  then
4: deg(v) ← deg(v) + 1.
Termination Process:
1: Let h = 
{v ∈ V 
| deg(v) > μ}
 denote the number of heavy vertices in V 
.
2: Return ˆ
h = h × n1−c2
α2 .
believe this approach may be of independent interest for testing properties of (sub-)graphs with
bounded degree. Finally, to boost the probability of success, we maintain independent estimators
of subroutines in our algorithm (see Algorithm 1).
2.1 Main Subroutines
The subroutines of Algorithm 1 are as follows.
Subroutine 1. In this subroutine, we greedily construct a maximal matching M. However, we
stop the process if |M| exceeds the limit α1nc1 , where c1 = 2/3 and α1 = 3.
Subroutine 2. In this subroutine, we choose a subset of size α2nc2 from the set of vertices uniformly at random. Here, α2 = 3ϵ−2 and c2 = 2/3. During the execution, we maintain the degrees
of sampled vertices.
Subroutine 3. In this subroutine, we again choose a subset of size α3nc3 from the set of vertices
uniformly at random. We set α3 = 4ϵ−1 and c3 = 2/3 in the proof. However, in addition to the
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.
48:10 H. Esfandiari et al.
SUBROUTINE 3. [Shallow Edges]
Input: A stream of edges of the graph e1,..., em.
Return Value: sˆ, an estimation for the number of shallow edges in the graph.
Initialization Process:
1: Initialize V  ⊂ V as a random subset of size α3nc3 where α3 = 4ϵ−1 and c3 = 2/3.
2: For every v ∈ V 
, initialize deg(v) to zero and E to an empty set.
Update Process, upon the arrival of ei = (u,v):
1: if both u,v ∈ V  then
2: E ← E ∪ {ei}.
3: if u ∈ V  then
4: deg(u) ← deg(u) + 1.
5: If deg(u) > μ, remove all the edges adjacent to u from E
.
6: if v ∈ V  then
7: deg(v) ← deg(v) + 1.
8: If deg(v) > μ, remove all the edges adjacent to v from E
.
Termination Process:
1: Let s = 

E
 denote the number of shallow edges induced by V 
.
2: Return sˆ = s × n(n−1)
(|V  |)(|V  |−1) .
degrees of vertices, we also maintain the edges that the degrees of their endpoints are at most μ.
Thus, we require at most O˜ (μα3nc3 ) memory space.
2.2 Analysis
In this section, we analyze the approximation ratio and the success probability of our randomized
algorithm. We first distinguish between two primary cases, depending on the size of a maximum
matching M∗. Recall that the size of a maximal matching is always within a factor of 2 of a maximum matching. The first subroutine of our algorithm maintains the size R of a maximal matching
up to α1n2/3. Observe that in Algorithm 1, we completely ignore the second and third subroutines
if R < α1n2/3. Therefore, for the input scenarios that R < α1n2/3, we indeed have a maximal matching of size R and thus our algorithm deterministically estimates M∗ within a two factor. In the rest
of this section, we focus on the case where α1n2/3 ≤ R ≤ M∗.
By Lemma 1, we know that either h ors is large (respectively, the number of heavy vertices and
the number of shallow edges). Consider a single execution of Subroutine 2 (resp., Subroutine 3),
which returns an estimation for ˆ
h (resp., sˆ). In our algorithm, we maintain log( 1
δ ) executions of
the subroutines to boost the success probability. Therefore, the goal is to show that for a single
execution, with a constant probability, the output of our algorithm is within a constant factor of
M∗. Since we output the maximum of the two estimations, we need to show two concentration
bounds for each estimator:
(1) if the target value is large, with constant probability, we get a constant approximation;
and
(2) if the target value is small, with constant probability, the estimation is small, too.
We prove this for both estimators in the following lemmas.
Lemma 7. For Subroutine 2, where we set α1 = 3, we have:
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:11
—If h ≥ n2/3, Pr[| ˆ
h − h| ≥ ϵh] ≤ 2 exp(−n1/3); and
—If h < n2/3, Pr[ˆ
h > α1n2/3] ≤ exp(−n1/3).
Proof. Recall in the subroutine, we select a random sample V  of size n = α2n2/3. Recall that
α1 = 3, α2 = 3ϵ−2, and α3 = 4ϵ−1. Let V  = {v
1,...,v
n }. For i ∈ [n
], let Xi denote the random
variable whereXi = 1 ifv
i is a heavy vertex, andXi = 0, otherwise. Since we have h heavy vertices,
we have that Pr[Xi = 1] = h
n for every i ∈ [n
]. Let h =
i Xi . Clearly E [h
] = n·h
n . Recall that the
return value of Subroutine 2 is ˆ
h = h × n
n . Thus, in expectation, we return the value of h.
Now, since all Xi ’s are negatively correlated, we use the extension of Chernoff bound 5 to prove
the first bound of the lemma for h ≥ n2/3:
Pr 



ˆ
h − h

 ≥ ϵh
= Pr 



ˆ
h − E

ˆ
h


 ≥ ϵ E

ˆ
h
  = Pr


h − E
h

 ≥ ϵ E
h 
≤ 2 exp 
−ϵ2 E [h
]
3

≤ 2 exp(−n1/3).
We again use the Chernoff bound to prove the lemma for h < n2/3:
Pr 
ˆ
h > α1n2/3

≤ Pr 
ˆ
h >

α1n2/3
h

E

ˆ
h


= Pr 
h >

α1n2/3
h

E
h

≤ Pr 
h >

1 +
α1n2/3
2h

E
h

≤ exp 	


−
α2
1 · n4/3 · E [h
]
12h2 

≤ exp 
−

α1
2
2
n1/3

.
Indeed, for the shallow-edge estimator, the problem is more sophisticated. For a shallow edge
e = (u,v), the probability of hitting e in a shallow subgraph is positively correlated to that of all
edges adjacent to u and v. Thus, a normal application of (generalized variants of) the Chernoff
bound fails to satisfy the required concentration bounds. Indeed, in the next lemma, we use the
small dependency degree between the variables to prove the concentration bounds.
Lemma 8. For Subroutine 3, where we assume n ≥ α2
3 (2μ)
3 and we set α1 = 3, α2 = 3ϵ−2, we have:
—If s ≥ n2/3, Pr[|sˆ − s| ≥ ϵs] ≤ 1/4; and
—If s < n2/3, Pr[sˆ > α1n2/3] ≤ α−2
1 = 1/9.
Proof. Recall that in Subroutine 3, we sample a shallow subgraph (V 
, E
) where |V 
| = n =
α3n2/3. Recall that α1 = 3 and α3 = 4ϵ−1. Let e1,..., es denote the shallow edges in the original
graph. For a shallow edge ei , let Xi denote the random variable where Xi = 1 if ei ∈ E and Xi = 0,
otherwise. Let p = Pr[Xi = 1]. We have
E [Xi] = Pr[Xi = 1] = p =
	
n
2


	
n
2

 = n
(n − 1)
n(n − 1) .
We note that this probability is indeed very small p ∈ [
α2
3n−2/3
2 , α2
3n−2/3]. Lets =
i Xi ; note that
E [s
] = sp. In order to get a concentration bound for s
, we would need to bound the variance of
our estimator. We know that
Var (s
) =

i
Var (Xi ) +

ij
Cov(Xi,Xj).
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.         
48:12 H. Esfandiari et al.
Thus, we need to calculate Var(Xi ) and Cov(Xi,Xj) for which we have
Var (Xi ) = E

(Xi − E [Xi])
2

= Pr[Xi = 1] (1 − E [Xi])
2 + Pr[Xi = 0] (E [Xi])
2
= p(1 − p)
2 + (1 − p)(p)
2 = p(1 − p) ≤ p.
For the covariance, we have
Cov(Xi,Xj) = E[XiXj] − E[Xi] E[Xj] = Pr[Xi = Xj = 1] − p2
.
Thus, we have two cases based on whether the edges corresponding to Xi and Xj share an endpoint:
(i) if ei and ej share an endpoint, Pr[Xi = Xj = 1] is the probability of having three fixed
vertices in V 
; and
(ii) if ej and ej do not share an endpoint, Pr[Xi = Xj = 1] is the probability of having four
fixed vertices in V 
.
For Case (i), we have
Cov 	
Xi,Xj


=
	
n−3
n−3


	
n
n

 − p2 = n
(n − 1)(n − 2)
n(n − 1)(n − 2) − p2 < p3/2 − p2 ≤ p3/2
.
For Case (ii), we have
Cov 	
Xi,Xj


=
	
n−4
n−4


	
n
n

 − p2 = n ··· (n − 3)
n ··· (n − 3) − p2 < p2 − p2 = 0.
This implies that we can completely ignore the terms for Case (ii) since we are interested in an
upper bound on the variance. On the other hand, the degree of a light vertex is at most μ. Hence,
every shallow edge is adjacent to at most 2μ other edges. This leads to the following upper bound
for the variance of s for p = Θ(n−2/3) and n ≥ α2
3 (2μ)
3:
Var (s
) =
s
i=1
Var (Xi ) +

ij
Cov 	
Xi,Xj


≤ sp +

ij
Cov 	
Xi,Xj


≤ sp +

ij:ei and ej share an endpoint
p3/2 ≤ sp + s(2μ)(p3/2
) ≤ 2sp.
Now, we can indeed use the Chebyshev’s inequality (Theorem 6) to prove the desired concentration bounds. For s ≥ n2/3, and α3 = 4ϵ−1, we have
Pr[|sˆ − s| ≥ ϵs] = Pr[|sˆ − E [sˆ]| ≥ ϵ E [sˆ]] = Pr


s − E
s

 ≥ ϵ E
s 
≤
Var (s
)
ϵ2 (E [s
])2 ≤
2
ϵ2 · n2/3 · p
≤
4n2/3
ϵ2n2/3α2
3
≤ 1/4.
One can use a similar argument to prove the concentration bound for s < n2/3:
Pr 
sˆ > α1n2/3

= Pr 
sˆ >

α1n2/3
E [sˆ]

E [sˆ]

= Pr 
s >

α1n2/3
s

E
s

≤
Var (s
)
	
E [s
] (
α1n2/3
s − 1)

2 ≤
4s2 Var (s
)
s2p2α2
1n4/3 ≤ α−2
1 .
We are now ready to prove the main theorem.
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.        
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:13
Proof of Theorem 2. Consider the parameters in Algorithm 1. Recall that α1 = 3, α2 = 3ϵ−2,
and α3 = 4ϵ−1. As discussed before, if R < α1n2/3, we deterministically output the size of a maximal
matching, which is within a two-approximation of M∗. Thus, we focus on the instances where
M∗ ≥ α1n2/3. In this scenario, the output is proportional to the median of the Mi values where
Mi = max{α1n2/3, ˆ
hi,sˆi}. Let τ = max{α1n2/3,hG ,sG } = max{hG,sG }. By Lemma 1, we know that
at least one of hG and sG is at least M∗
2 > n2/3. We say the i
th execution of the sampling issuccessful,
if |Mi − τ | ≤ ϵτ . Let Xi denote the random variable where Xi = 1 if the i
th sample is successful and
Xi = 0 otherwise. Let us consider the event Xi = 1. We have two cases based on whether hG ≥ sд.
One can use the union bound to get a lower bound on the probability of success.
For τ = hG , we have
Pr[Xi = 1] ≥ 1 − Pr[| ˆ
h − hG | ≥ ϵhG ] − Pr[sˆ > α1n2/3
].
For τ = sGs, we have
Pr[Xi = 1] ≥ 1 − Pr[|sˆ − sG | ≥ ϵsG ] − Pr[ˆ
h > α1n2/3
].
Now we can use the concentration bounds given by Lemmas 7 and 8 to get a constant lower bound
for Pr[Xi = 1]. Note that we can assume n ≥ 125 since, otherwise, we can keep the (constant-size)
input in memory. Thus, we have n1/3 ≥ 5.
Pr[Xi = 1]
≥ min{1 − Pr[| ˆ
h − hG | ≥ ϵhG ] − Pr[sˆ > α1n2/3
], 1 − Pr[|sˆ − sG | ≥ ϵsG ] − Pr[ˆ
h > α1n2/3
]}
≥ min{1 − 2 exp(−n1/3) − 1
9
, 1 − 1
4 − exp(−n1/3)}
≥ min{8/9 − 2 exp(−5), 3/4 − exp(−5)} ≥ 0.743.
Observe if for some i ∈ [Q], Xi = 1, then by Lemma 1, Mi
η(1+ϵ ) ≤ M∗ ≤ 2Mi (1 + ϵ ). However, the
output of the algorithm is proportional to the median value of all Mi ’s. In the event that more than
half of Xi ’s are successful, the median is indeed successful. Let X =
i ∈[Q] X denote the number
of successful Mi ’s. With probability at least Pr[X ≥ 0.501Q], the output of our algorithm is within
2η(1 + ϵ )
2-approximation of M∗. Since the executions are independent, we can use a Chernoff
bound to get the success probability of our algorithm.
Pr[X ≥ 0.501Q] ≥ 1 − exp
	



−
(1 − 0.501Q
E[X] )
2 E [X]
2



≥ 1 − exp 
− (1 − 0.501/0.743)
2 E [X]
2

≥ 1 − exp(−0.053 E [X]) ≥ 1 − exp(−0.039Q)
≥ 1 − exp(− ln(δ−1)) = 1 − δ,
where E [X] = Pr[Xi = 1]Q ≥ 0.743Q and Q ≥ 26 ln(δ−1).
Therefore, with probability at least 1 − δ, the output of our algorithm is within (2η)(1 + ϵ )
2-
approximation of the size of the maximum matching. It only remains to analyze the running time
of our algorithm. Observe that the update process and the termination process of each single
subroutine can be implemented to run in O(1) time. Therefore, the update time of our algorithm is
linear to the number of subroutines O(Q) = O(ln(δ−1)). Furthermore, at the end of the execution
of the algorithm, we need to find the median of Q values, which can also be implemented in
O(ln(δ−1)) time.
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.  
48:14 H. Esfandiari et al.
3 PROOF OF LEMMA 1
In a maximum matching, the number of edges that are incident to at least one heavy vertex cannot
be more than the number of heavy vertices hG : two matching edges cannot share a (heavy) vertex.
On the other hand, the edges that are not adjacent to a heavy vertex are induced by light vertices,
i.e., such an edge is a shallow edge. The number of these edges is at mostsG , and thus, the maximum
matching size is at most hG + sG .
We recall that βG denotes an upper bound on the average degree of every subgraph of G. It is
easy to verify that βG ≤ 2νG for a graph G whose arboricity is νG . Here, we may assume that βG
is constant. We may drop the index G when it is clear from the context.
In order to prove the lower bound, we first show that there is a matching of size 4sG
5μ+3
in the induced subgraph of light vertices. This implies M∗ ≥ 4sG
5μ+3 . We then show a matching of size μ−β+1
2μ
hG in G, which means that M∗ ≥ μ−β+1
2μ
hG . Overall, this implies that M∗ ≥
max{
μ−β+1
2μ
hG , 4
5μ+3sG }. If we set μ = β + 3, we have
μ − β + 1
2μ = (β + 3) − β + 1
2(β + 3) = 4
β + 3
and 4
5μ + 3 = 4
5(β + 3) + 3 = 1
1.25β + 4.5
.
By comparing the above functions, we can see 4
5μ+3 ≤ μ−β+1
2μ . Thus, we have
M∗ ≥ max 
μ − β + 1
2μ
hG , 4
5μ + 3
sG

≥ max  4
5μ + 3
hG , 4
5μ + 3
sG

= max{hG,sG }
1.25μ + 0.75 ,
as desired.
In this section, we write L to denote the set of light vertices.
Claim 9. The subgraph induced by L contains a matching of size 4sG
5μ+3 .
Proof. Han in Ref. [14] proved (see Theorem 2.3) that for a graph ofm edges and the maximum
degree at most Δ, there exists a matching of size at least 4m
5Δ+3 . By definition, each vertex of L has
degree at most μ. Hence, we have a matching of size 4sд
5μ+3 in the graph induced by L.
In order to prove the next claim, we use the following generalization of Hall’s theorem.
Lemma 10 ([4]). Let G(X,Y ) be a bipartite graph with bipartition (X,Y ). The number of edges in a
maximum matching of G(X,Y ) is
|X | − max
R ⊆X (|R|−|N (R)|),
where N (R) is the set of all neighbors of vertices in R.
Claim 11. There is a matching of size μ−β+1
2μ
hG in H, where H is G excluding the edges with both
endpoints in L.
Proof. Let Mh be the set of vertices covered by a maximal matching in the graph induced by
heavy vertices and assume |Mh | = 2λ. Let U be the set of unmatched heavy vertices. Since Mh is
maximal, there is no edge between vertices in U , i.e., U is an independent set. Let G(U,L) denote
the bipartite graph consisting of edges connecting vertices in L and U . In the remainder, we use
Lemma 10 to show the size of a maximum matching inG(U,L) plus |Mh |
2 = λ is at least hG ( 1
2 − 2β−2
μ−1 ).
Consider the bipartite graph G(U,L) and let R be an arbitrary subset of U . We bound |N (R)| by
double counting the number of edges between R and N (R), namely E[R, N (R)]. On the one hand,
the degree of each vertex in N (R) is at most μ. Hence, we have E[R, N (R)] ≤ μN (R). On the other
hand, vertices in R are heavy, which means they have a degree of at least μ in graph G. However,
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018. 
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:15
vertices in R may have some neighbors in Mh that do not exist in G(U,L). Recall that β denotes an
upper bound on the average degree of every subgraph of G. Observe that R ∪ Mh is a subgraph of
G. Thus, the average degree of vertices in R ∪ Mh is at most β. Moreover, there is at least λ edges
between vertices in Mh. Thus, there is at most (|R |+2λ)β
2 − λ edges between R and Mh. Therefore,
we have μ|R| − (
(|R |+2λ)β
2 − λ) ≤ E[R, N (R)]. Thus, we have
μ|R| − 
|R|β
2
+ λβ − λ

≤ μN (R),
which gives us |R| − ( |R |β
2μ
+ λβ−λ
μ ) ≤ N (R). If we apply Lemma 10, the size of the maximum matching in G(U,L) is at least
|U | − max
R ⊆U (|R|−|N (R)|) ≥ hG − 2λ − max
R ⊆U

|R|−|R| +

|R|β
2μ
+ λβ − λ
μ
 
= hG − 2λ − λβ − λ
μ − max
R ⊆U
|R|β
2μ
= hG − 2λ − λβ − λ
μ − (hG − 2λ)β
2μ
= hG − hG β
2μ − 2λ +
λ
μ
.
Therefore, the total size of the matching is at least
λ + hG − hG β
2μ − 2λ +
λ
μ = hG − hG β
2μ − λ +
λ
μ
.
This quantity decreases as a function of λ. Thus, it is minimized for λ ≥ hG
2 . In this case, the size
of the matching is at least
hG − hG β
2μ − hG
2
+
hG
2μ = μ − β + 1
2μ
hG ,
which completes the proof.
4 MAXIMUM MATCHING IN FORESTS
In this section, we approximate the maximum matching size in a tree within a 2(1 + 3ϵ ) factor.
We use some structural property of trees to improve the lower and upper bounds of the maximum
matching in Lemma 12. Later, we generalize this lemma to forests with no isolated vertices and
approximate the maximum matching size of these forests by a 2(1 + 3ϵ ) factor. During this section,
we set μ = 1, i.e., vertices with degree 1 are light and vertices with degree 2 or more are heavy.
Lemma 12. Let T be a tree with maximum matching size M∗. We have the following bounds:
—upper bound: M∗ ≤ hT + 1,
—lower bound: M∗ ≥ hT +1
2 .
Proof. In a tree with more than two vertices, leaves can not be adjacent. Thus, each matching
edge shares at least one vertex with the heavy vertices. Thus, maximum matching size in a tree T
with more than two vertices is at most hT . On the other hand, the maximum matching size in a
tree with two vertices is 1. Therefore, for every tree, we have M∗ ≤ max(hT , 1) ≤ hT + 1.
In order to show the lower bound, we show that every tree has a matching with the following
two properties.
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018. 
48:16 H. Esfandiari et al.
—All of the heavy vertices are matched.
—At least one light vertex (leaf) is matched.
This immediately gives us M∗ ≥ hT +1
2 .
Let M be a maximum matching that matches the maximum possible number of heavy
vertices. Assume a heavy vertex v is not matched in M. Let Pv be a maximal alternative path starting from v. Recall that a path p = [u1,u2,...,uk ] is called alternating if edges
(u1,u2), (u3,u4),..., (u2j−1,u2j),... are unmatched, but (u2,u3), (u4,u5),..., (u2j,u2j+1),... are
matched. Recall that an alternating path p = [u1,u2,...,uk ] is called augmenting if both u1 and
uk are unmatched. Since, there is no cycle in the tree, the other end of Pv is a leaf. If we replace
the matched edges by the unmatched edges in Pv , the size of the matching remains the same, but
the number of matched heavy vertices increases by one. This contradicts the selection of M. Thus,
all of the heavy vertices in M are matched.
Let P be a maximal alternative path in T , i.e., an alternating path that is not a subset of another
alternating path in the tree T . We observe that both ends of P are leaves. On the other hand, since
M is a maximum matching, at least one end of P is matched. This means that at least one leaf is
matched in M, which completes the proof.
Lemma 12 shows that hF +1
2 approximates the maximum matching size with a factor of 2. Thus,
if we estimate the number of heavy vertices with a factor of 1 + ϵ, we can estimate the maximum
matching size with a factor of 2(1 + ϵ ). Later, we use Subroutine 2 to estimate the number of heavy
vertices.
Lemma 13 generalizes Lemma 12 to the forests with no isolated vertices.
Lemma 13. Let F be a forest with no isolated vertex and with maximum matching size M∗. We
have the following bounds:
—upper bound: M∗ ≤ hF + cF ,
—lower bound: M∗ ≥ hF +cF
2 ,
—lower bound: M∗ ≥ cF ,
where cF is the number of connected components in F .
Proof. Let T1,T2,...,TcF be the connected components of F . Let M∗
i be the size of the maximum of Ti . For all 1 ≤ i ≤ cF we know that hTi +1
2 ≤ M∗
i ≤ hTi + 1. By summing it up over all
i we have hF +cF
2 ≤ M∗ ≤ hF + cF . In addition, since we do not have any isolated vertex, each
connected component contains at least one edge. This give us M∗ ≥ cF , which completes the
proof.
The number of connected components in a forest is exactly n − m where n is the number of
vertices and m is the number of edges [4]. Thus, similar to the trees, if we estimate the number of
heavy vertices within a 1 + ϵ factor, we can estimate the maximum matching size within a 2(1 + ϵ )
factor. In the following algorithm, we reuse Subroutine 2 and estimate the maximum matching size
of a forest with no isolated vertex.
Lemma 14. For Subroutine 2 with parameters c2 = 0.5, α2 = 6ϵ−2, and α3 = 2, we have:
—If h ≥ √
n, Pr[| ˆ
h − h| ≥ ϵh] ≤ 1
e ; and
—If h < √
n, Pr[ˆ
h > α3
√
n] ≤ 1
e .
Proof. Recall, in the subroutine, we select a random sample V  of size n = α2
√
n. Let V  =
{v
1,...,v
n }. Fori ∈ [n
], letXi denote the random variable whereXi = 1 ifv
i is a heavy vertex, and
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.  
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:17
ALGORITHM 2: Maximum Matching Estimator for Forest
Input: A stream of edges of the forest e1,..., em.
Output: An estimation M of the size of maximum matching M∗.
Initialization Process:
1: Set α1 = 5, α2 = 6ϵ−2, α3 = 2, and Q = 21ln( 1
δ ) + 1.
2: Set c1 = c2 = 0.5.
3: Initialize Subroutine 1 with α1 and c1 factors.
4: Initialize Q independent executions of Subroutine 2, with α2 and c2 factors.
Update Process, upon the arrival of ei = (u,v):
1: Update each subroutine with the new arrival.
Termination Process:
1: Let R denote the return value of Subroutine 1.
2: For i ∈ [Q], let ˆ
hi denote the value returned by the i-th execution of Subroutine 2.
3: Let hmed denote the median value of ˆ
hi ’s.
4: Let n denote the number of vertices and m denote the number of edges
5: if R < α1
√
n then
6: Output M = R.
7: else if hmed ≤ α3
√
n then
8: Output M = n − m.
9: else
10: Output M = hmed+n−m
2(1+ϵ ) .
Xi = 0 otherwise. Since we have h heavy vertices, we have that Pr[Xi = 1] = h
n for every i ∈ [n
].
Let h =
i Xi . Clearly, E [h
] = n·h
n . Recall that the return value of Subroutine 2 is ˆ
h = h × n
n .
Thus, in expectation, we return the value of h. Now, since all Xi ’s are negatively correlated, we
use the extension of Chernoff bound 5 to prove the first bound of the lemma for h ≥ √
n:
Pr[| ˆ
h − h| ≥ ϵh] = Pr[| ˆ
h − E[ˆ
h]| ≥ ϵ E[ˆ
h]] = Pr


h − E
h

 ≥ ϵ E
h 
≤ 2 exp 
−ϵ2 E [h
]
3

≤
2
e2 ≤
1
e
.
We again use the Chernoff bound to prove the lemma for h < √
n:
Pr[ˆ
h > α3
√
n] ≤ Pr 
ˆ
h >

α3
√
n
h

E[ˆ
h]

= Pr 
h >

α3
√
n
h

E
h

≤ Pr 
h >

1 +
α3
√
n
2h

E
h

≤ exp 
−
α2
3 · n · E [h
]
12h2

≤ exp 
−

α3
2
2

= 1
e
.

Proof of Theorem 3. When the maximum matching size is less than α1
√
n, we report the size
of a maximal matching of F . This clearly estimates the maximum matching size within a factor of 2.
Thus, we can assume the maximum matching size is at least α1
√
n. First, we assume the following
two claims and prove the theorem in three cases. Later, we provide the proofs of the claims.
Claim 15. If h < √
n, Pr[hmed > α3
√
n] ≤ δ.
Claim 16. If h ≥ √
n, Pr[|hmed − h| ≥ ϵh] ≤ δ.
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.        
48:18 H. Esfandiari et al.
Case 1: h < √
n. In this case, using Claim 15, with probability 1 − δ, we have hmed ≤ α3
√
n and
the output is cF = n − m. Thus, with probability 1 − δ, we have
M∗ ≤ hF + cF ≤ √
n + cF ≤
M∗
α1
+ cF .
This, combined with M∗ ≥ cF , says that the output approximates the maximum matching size
within a α1
α1−1 = 5
4 factor.
Case 2: h ≥ √
n and hmed < α3
√
n. In this case, using Claim 16, with probability 1 − δ, we have
hF − ϵhF ≤ hmed. This, together with hmed < α3
√
n, gives us hF − ϵhF < α3
√
n. Therefore, with
probability 1 − δ, we have
M∗ ≤ hF + cF ≤
α3
1 − ϵ
√
n + cF ≤
α3M∗
α1 (1 − ϵ )
+ cF .
This, combined with M∗ ≥ cF , says that the output approximates the maximum matching size
within a α1 (1−ϵ )
α1 (1−ϵ )−α3 ≤ 2 factor, assuming ϵ ≤ 1/5.
Case 3: h ≥ √
n and hmed ≥ α3
√
n. In this case, using Claim 16, with probability 1 − δ, we have
hF − ϵhF ≤ hmed. Therefore, with probability 1 − δ, we have
M∗ ≤ hF + cF ≤
hmed
1 − ϵ + cF ≤
hmed + cf
1 − ϵ = hmed + cf
2(1 + ϵ )
2(1 + ϵ )
1 − ϵ ≤
hmed + cf
2(1 + ϵ ) 2(1 + 3ϵ ).
At the same time, we have
M∗ ≥
hF + cF
2
≥
hmed/(1 + ϵ ) + cF
2
≥
hmed + cf
2(1 + ϵ ) .
This means that the output approximates the maximum matching size within a 2(1 + 3ϵ ) factor,
assuming ϵ ≤ 1/5.
Proof of Claim 15. Let Xi denote the random variable where Xi = 1, if ˆ
hi > α3
√
n and Xi = 0
otherwise. Form Lemma 14 we know that the probability of Xi = 1 is 1/e. On the other hand, if
hmed > α3
√
n at least half of the hi ’s are greater than α3
√
n. Thus, we have
Pr[hmed > α3
√
n] ≤ Pr
⎡
⎢
⎢
⎢
⎢
⎢
⎣

i ∈Q
Xi ≥ 0.5|Q|
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
Using Chernoff bound, we can bound this probability as follows.
Pr[hmed > α3
√
n] ≤ exp 	


−
(1 − e
2 )
2 1
e 21 ln( 1
δ )
3


≤ δ .
Proof of Claim 15. Let Xi donate the random variable where Xi = 1, if |hi − h| ≥ ϵh and
Xi = 0, otherwise. From Lemma 14, we know that the probability of Xi = 1 is 1/e. Again here,
if |hmed − h| ≥ ϵh, at least half of the hi ’s are greater than α3
√
n. Hence, again we have
Pr[|hmed − h| ≥ ϵh] ≤ Pr
⎡
⎢
⎢
⎢
⎢
⎢
⎣

i ∈Q
Xi ≥ 0.5|Q|
⎤
⎥
⎥
⎥
⎥
⎥
⎦
≤ exp 	


−
(1 − e
2 )
2 1
e 21 ln( 1
δ )
3


≤ δ .
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.  
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:19
SUBROUTINE 4. [Shallow Edges Using Two Passes]
Input: A stream of edges of the graph e1,..., em.
Return Value: sˆ, an estimation for the number of shallow edges in the graph.
First Pass:
1: Let S be a random sample set of edges chosen with probability 3 log(4/δ )
ϵ 2
√
n .
Second Pass:
1: for each edge (u,v) ∈ S do
2: if (u,v) is a shallow edge then
3: Add (u,v) to set E
, which is initialized to zero in the beginning of this pass.
Termination Process:
1: Let s = 

E
 denote the number of shallow edges sampled using our algorithm.
2: Return sˆ = s × ϵ 2√
n
3 log(4/δ ) .
5 TWO-PASS AND RANDOM-ORDER STREAMING ALGORITHMS
Here, we approximate the number of shallow edges within (1 ± ϵ ) factor using O˜ (
√
n) space. Observe that in Lemma 7 we can replace h ≥ n2/3 by h ≥ n1/2 and the lemma works with constant
probability. Therefore, we can approximate h using O˜ (
√
n) space. The goal is to estimate s using
O˜ (
√
n) space as well.
Two-Pass Streaming Algorithm. When we are allowed to have two passes over the input, a natural
approach for estimating s is to get a random sample of edges in the first round and then distinguish
between shallow and heavy edges in the second round (see Subroutine 4).
Lemma 17. For Subroutine 4, we have Pr[(1 − ϵ ) · |S | ≤ sˆ ≤ (1 + ϵ ) · |S |] ≥ 1 − δ/2.
Proof. Let S be the set of shallow edges. Suppose that |S | ≥ √
n. If |S | ≤ √
n, we either maintain
a maximal matching if the size of matching is O˜ (n1/2) or charge the number of shallow edges to
the number of heavy vertices if the number of them is Ω( ˜ n1/2). Thus, we do not elaborate on them
here. For i ∈ [|S |], let Xi denote the random variable where Xi = 1 if ei ∈ S, and Xi = 0 otherwise.
We then have Pr[Xi] = 3 log(4/δ )
ϵ 2
√
n . Let s =
i ∈[|S |] Xi . Thus, E[s
] = E[
i ∈[|S |] Xi] = 3 log(4/δ )· |S |
ϵ 2
√
n ≥
3 log(4/δ )
ϵ 2 . We then use the Chernoff bound to prove that
Pr


s − E
s

 ≥ ϵ · E
s  ≤ 2 exp 
−ϵ2 E [s
]
3

≤ δ/2.
Let us condition on the event that |s − E [s
]| ≤ ϵ · E [s
], which happens with probability at
least 1 − δ/2. Since
(1 − ϵ ) · E
s
× ϵ2√
n
3 log(4/δ ) ≤ sˆ ≤ (1 + ϵ ) · E
s
× ϵ2√
n
3 log(4/δ )
,
we obtain (1 − ϵ ) · |S | ≤ sˆ ≤ (1 + ϵ ) · |S |.
Random Order Streams. Recall that in the 2-pass algorithm, we use the first pass simply to take a
random sampling of the edges. Although in the random order model, we cannot have two passes
over the input, the first few edges of the input is, in fact, a random sample (see Subroutine 5). Thus,
the following lemma can be proved in a manner similar to Lemma 17.
Lemma 18. For Subroutine 5, we have Pr[(1 − ϵ ) · |S | ≤ sˆ ≤ (1 + ϵ ) · |S |] ≥ 1 − δ/2.
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.        
48:20 H. Esfandiari et al.
SUBROUTINE 5. [Shallow Edges in Random-Order Streams]
Input: A stream of edges of the graph e1,..., em.
Return Value: sˆ, an estimation for the number of shallow edges in the graph.
1: Let S be the first 3 log(4/δ )
ϵ 2
√
n edges of the stream.
2: for each edge (u,v) ∈ S do
3: if (u,v) is a shallow edge then
4: Add (u,v) to set E
, which is initialized to zero in the beginning of this pass.
Termination Process:
1: Let s = 

E
 denote the number of shallow edges sampled using our algorithm.
2: Return sˆ = s × ϵ 2√
n
3 log(4/δ ) .
6 HARDNESS RESULTS
In this section, we show streaming lower bounds by reducing from the Boolean Hidden Matching
Problem [2, 10, 33], which we refer to as BHM.
BHMn (n ∈ Z+): Alice is given a binary string x ∈ {0, 1}
4n. Bob is given a perfect
matching Y = {(y1,y
1), (y2,y
2),..., (y2n,y
2n,2)} between numbers in [4n] and a
vector z ∈ {0, 1}
2n. It is guaranteed that there is a θ ∈ {0, 1} such that for for all
i ∈ [2n], it holds xyi ⊕ xy
i ⊕ zi = θ. Alice sends a single message to Bob and Bob’s
task is to output θ, the parity of the matching.
We use the following two communication lower bounds for BHMn.
Fact 19 ([33, Theorem 2.1]). Any randomized protocol for BHMn with Alice’s message of length
o(n1/2) errs with probability greater than 1/4 on some input.
Fact 20 ([2, Theorem 8]). Any randomized protocol for BHMn with Alice’s message of length o(n)
errs with non-zero probability on some input.
Fact 19 is used to show a weaker lower bound for randomized streaming algorithms and Fact 20
is used to show a stronger lower bound for deterministic streaming algorithms. In the proof, we
turn a streaming algorithm into a communication protocol. In particular, a small-space streaming
algorithm would imply an efficient communication protocol. Hence, a communication lower bound
for protocols implies a space lower bound for streaming algorithms.
Theorem 21. Let A be a streaming algorithm whose goal is to estimate the maximum matching
size to within a factor better than 3/2. Let n be the number of vertices in the input graph.
—If A errs with probability at most 1/4 on any input, then it cannot use o(n1/2) bits of space.
—If A is deterministic and never errs, then it cannot use o(n) bits of space.
The lower bounds hold even for graphs consisting of paths of length up to 3.
Proof. Let x ∈ {0, 1}
4n be Alice’s input in an instance of BHMn. Let Y = {(y1,y
1),
(y2,y
2),..., (y2n,y
2n,2)} be a perfect matching between numbers in [4n]. Y and z ∈ {0, 1}
2n are
Bob’s input in the same instance of BHMn.
We construct a graphG on 4n groups of vertices. Each group consists of ui ,wi,0, andwi,1, where
i ∈ [4n]. For each i ∈ [4n], Alice holds only one edge of G: (ui,wi,xi ), which connects two vertices
in the same group. For each edge (yi,y
i ) of the matching Y, where i ∈ [2n], Bob holds two edges
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.
Streaming Algorithms for Estimating the Matching Size in Planar Graphs and Beyond 48:21
Fig. 1. The first example of connected groups for different parities in the lower bound construction. Alice’s
edges and Bob’s edges are drawn using solid and dashed lines, respectively. The case of x4 = 1, x7 = 0, and
the corresponding zi = 1. The parity of the matching equals x4 ⊕ x7 ⊕ zi = 0.
Fig. 2. The first example of connected groups for different parities in the lower bound construction. Alice’s
edges and Bob’s edges are drawn using solid and dashed lines, respectively. The case of x4 = 0, x7 = 1, and
the corresponding zi = 0. The parity of the matching equals x4 ⊕ x7 ⊕ zi = 1.
of G: (wyi,0,wy
i,0⊕zi ) and (wyi,1,wy
i,1⊕zi ), which connect vertices in two groups. Note that both
Alice and Bob can compute the edges they hold by only looking at their respective inputs.
We now show that the size of the matching significantly depends on the parity of the matching
in the instance of BHMn. We consider each pair of connected groups. It is easy to show that if the
parity of the matching equals 0 (see Figure 1 for an example), then such pair consists of two disjoint
paths of length 3 and 1, respectively. The size of the maximum matching in the pair of connected
groups is then equal to 3, which implies that the maximum matching in the entire graph is of size
2n · 3 = 6n. If the parity of the matching equals 1 (as in Figure 2), then the pair of connected groups
consists of two edges of length 2 and has a maximum matching of size 2. In this case, the maximum
matching size in the entire graph G equals 2n · 2 = 4n. The multiplicative gap between these two
cases equals 3/2. Hence, to distinguish them and to solve the BHMn instance (i.e., to compute the
parity of the matching), it suffices to estimate the maximum matching size in G to within a factor
better than 3/2.
Suppose now that there is a streaming algorithm that uses f (n) space for some function f and,
on any input, estimates the maximum matching size within a factor better than 3/2 with probability at least p on all inputs. Then, BHMn can be solved with probability at least p by sending a
message consisting of f (n) bits. First, Alice simulates the streaming algorithm on the set of her
edges and passes the state of the algorithm to Bob. Bob continues the simulation on his set of
ACM Transactions on Algorithms, Vol. 14, No. 4, Article 48. Publication date: August 2018.
48:22 H. Esfandiari et al.
edges and outputs his prediction of the parity of the matching bases on the estimate to the maximum matching size produced by the streaming algorithm as described above. More precisely, if
the streaming algorithm produces an estimate within a factor better than 3/2, the protocol correctly computes the parity of the matching in the BHMn instance. By applying Fact 19, we obtain
that there is no o(n1/2)-bits-of-space randomized streaming algorithm that provides this kind of
guarantee with probability at least 3/4 for all inputs. Similarly, via Fact 20, we learn that there is
no o(n)-bits-of-space deterministic streaming algorithm that always manages to compute a multiplicative 3/2-estimate to the maximum matching size.