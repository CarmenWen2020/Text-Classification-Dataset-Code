We give a variant of the pairing heaps that achieves the following amortized costs: O(1) per find-min and
insert, O(log logn) per decrease-key and meld, O(logn) per delete-min; where n is the number of elements
in the resulting heap on which the operation is performed. These bounds are the best known for any selfadjusting heap and match two lower bounds, one established by Fredman and the other by Iacono and Özkan,
for a family of self-adjusting heaps that generalizes the pairing heaps but do not include our variant. We
further show how to reduce the amortized cost for meld to be paid by the other operations, on the expense of
increasing that of delete-min toO(logn + log log N ), where N is the total number of elements in the collection
of heaps of the data structure (not just the heap under consideration by the operation).
Categories and Subject Descriptors: E.1 [Data Structures]: Lists, Stacks, and Queues; E.2 [Data Storage
Representations]: Linked Representations; F.2.2 [Analysis of Algorithms and Problem Complexity]:
Sorting and Searching
General Terms: Algorithms, design, performance, theory
Additional Key Words and Phrases: Priority queues, heaps, pairing heaps, amortized analysis
1 INTRODUCTION
A self-adjusting data structure is a data representation that does not explicitly maintain structural
information (like position, size, or height) for every element in the data set, and still can adjust
itself to perform well and to theoretically compete with other structures. Avoiding the restrictions
governing other structures and the necessity of maintaining structural information, self-adjusting
data structures showed practical superiority over their counterparts.
Following splay trees (Sleator and Tarjan 1985), a self-adjusting alternative to balanced search
trees with many other interesting properties, the next move was a self-adjusting heap. Around
that time, Fibonacci heaps were discovered by Fredman and Tarjan (1987). The introduction of
Fibonacci heaps was a milestone in data-structural design; the reason is that they achieve optimal
amortized bounds for all heap operations including decrease-key, and, accordingly, lead to improving many basic graph algorithms. The amortized costs for Fibonacci heaps are: O(1) per find-min,
insert and decrease-key, O(logn) per delete-min, and no cost per meld. As a self-adjusting alternative to Fibonacci heaps, Fredman et al. introduced the pairing heaps (1986). Despite the simplicity
of their structure and ingenuity of their proofs, Fredman et al. (1986) were only able to illustrate an
amortized O(logn) cost for all heap operations. Lagging behind Fibonacci heaps, the time bounds
for various pairing-heap operations, except for delete-min, were to be improved.
The pairing heap is a heap-ordered multiway tree. The values in the heap are stored one key
value per node. A primitive operation of a pairing heap is the join operation in which two trees
are combined by linking the root with the larger key value to the other as its leftmost child. The
following operations are defined for the standard implementation of the pairing heaps:
—find-min(H). Return the value at the root of the heap H.
—insert(k,H). Create a single node with key value k and join it with H’s tree.
—decrease-key(x, δ,H). Decrease the value of node x in heap H by δ. If this node is not the
root, cut its subtree and join the two trees resulting from the cut.
—delete-min(H). Remove the root of heap H and return its value. The resulting trees are combined to form a single tree. Several variants have been suggested, each with a different way
of combining these trees. For the standard two-pass method, the joins are performed in
two passes. In the first pass, called the pairing pass, the trees are joined in pairs from left
to right (pairing these trees from right to left achieves the same amortized bounds). In the
second pass, called the right-to-left incremental pass, the resulting trees are joined in order
from right to left, where each tree is joined with the result of the joins of the trees to its
right.
—meld(H1,H2). Join the two trees of the two heaps H1 and H2 into one heap.
Another variant of the pairing heaps is called the multi-pass variant (Fredman et al. 1986). In the
multi-pass variant, the delete-min operation proceeds as follows. The children of the deleted root
are joined in pairs; then, instead of the second right-to-left incremental pass, the resulting trees are
joined in pairs again, and the process repeats until a single tree remains. Even the O(logn) bound
is not known to hold for the multi-pass variant of the pairing heaps. Other variants with different
joining strategies for the delete-min operation were also suggested in Fredman et al. (1986) and
Elmasry (2004). Elmasry (2004) showed that the O(logn) bound for delete-min can as well be
achieved by substituting a join with a multiway-join and combining the trees in groups rather
than pairs. Iacono (2000) and Elmasry (2001) studied some distribution-sensitive properties of the
pairing heaps.
At about the same time as the introduction of the pairing heaps, Sleator and Tarjan (1986) introduced the skew heaps, another self-adjusting heap structure. As first introduced, the skew heaps
did not support the decrease-key operation, but can be easily extended to support it in O(logn)
time. The amortized asymptotic time bounds for the skew-heap operations are O(logn), still lagging behind the time bounds of the Fibonacci heaps.
Theoretical improvements concerning the pairing heaps and their variants were later obtained
through the years. See Table 1. One year after the first paper about pairing heaps, Stasko and Vitter
(1987) were able to improve the amortized time bound for insert toO(1) by modifying the standard
implementation of the pairing heaps using an auxiliary buffer for insertions. With every insert
operation, a new single-node tree is added to the buffer. Accompanying a delete-min operation,
the nodes in this buffer are first combined into one tree using the multi-pass pairing; the resulting
tree is then joined with the main heap.
Despite the inability to prove better bounds for the decrease-key operation, several experiments
(Jones 1986; Stasko and Vitter 1987; Moret and Shapiro 1992; Larkin et al. 2014) were conducted
illustrating that the pairing heaps are practically superior to other heaps, including the Fibonacci
heaps, especially for applications that involve decrease-key operations! Other experiments
(Elmasry and Fredman 1997) were also conducted with an attempt to construct a worst-case
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.
Toward Optimal Self-Adjusting Heaps 55:3
Table 1. Upper Bounds on the Operations of the Pairing Heap (First Rows) and Its Variants (Last Rows)
insert delete-min decrease-key meld
Fredman et al. 1986 O(logn) O(logn) O(logn) O(logn)
Iacono 2000 O(1) O(logn) O(logn) zero
Pettie 2005 O(22
√log log n ) O(logn) O(22
√log log n ) O(22
√log log n )
Stasko & Vitter 1987 O(1) O(logn) O(logn) O(logn)
This article O(1) O(logn) O(log logn) O(log logn)
O(1) O(logn + log log N) O(log logn) zero
All the time bounds are amortized. The number of elements in the heap under consideration by the operation is n, and
the total number of elements in the collection of heaps is N .
scenario for the decrease-key operation, but the experimental results seemed consistent with a
constant time-bound for the operation.
An important step was taken by Fredman (1999a), when he showed that a constant amortized
cost for decrease-key is precluded by establishing a lower bound of Ω(log logn) for a family of
heap structures that generalizes the standard implementation of the pairing heaps. He (Fredman
1999a) also conducted experiments illustrating that this lower bound is subject to experimental
detection, by constructing scenarios where the pairing heap requires more than constant amortized
time to perform decrease-key. We have to point out that our data structure does not follow the
settings for this lower bound to apply. Though practically efficient, pairing heaps are therefore
not theoretically as efficient as Fibonacci heaps. Left with skew heaps, one would think about
the possibility of improving the bound for decrease-key. Proving a tighter bound for skew heaps
was also precluded by Fredman (1999b), who introduced a transformation, referred to as depletion,
which, when applied to skew heaps, induces the skew-pairing heaps. This implies that his lower
bound, as applying to skew-pairing heaps, also applies to skew heaps.
Coming up with a tighter analysis, the amortized bounds for the standard pairing-heaps implementation were improved by Iacono (2000) to: O(1) per find-min and insert, O(logn) per decreasekey and delete-min, and no cost per meld. A few years later, Pettie (2005) established the first
sub-logarithmic bound for the decrease-key operation by proving the following amortized bounds:
O(1) per find-min,O(logn) per delete-min, andO(22
√log log n ) for other operations. He (Pettie 2005)
also conjectured that Fredman’s lower bound is tight as for the pairing heaps.
Another lower bound was recently given by Iacono and Özkan for a family of self-adjusting
heaps that also generalizes the pairing heaps, albeit using different settings from those of
Fredman’s. Iacono and Özkan (2014b) showed that, if the amortized costs of insert and delete-min
are O(logn), the amortized cost of decrease-key is Ω(log logn/ log log logn), and subsequently,
Iacono and Özkan (2014a) improved the lower bound for decrease-key to Ω(log logn), again
matching our upper bounds. Still, our construction does not follow the settings for this lower
bound either. More precisely, one of the settings for this lower bound to hold is that the implementation of the decrease-key operation be similar to that of the standard implementation of the
pairing heaps, but our implementation is not.
In this article, we give a variant of the pairing heaps that accomplishes the following amortized
bounds: O(1) per find-min and insert, O(log logn) per decrease-key and meld, O(logn) per deletemin. We describe the data structure in Section 2 and prove the time bounds in Section 3. We
then show in Section 4 how to improve meld to have zero amortized cost while increasing the
amortized cost for delete-min to O(logn + log log N), where N is the total number of elements in
the collection of heaps of the structure. We conclude the article with some remarks in Section 5.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.
55:4 A. Elmasry
2 THE DATA STRUCTURE
Similar to the standard pairing heaps, our trees are heap-ordered multiway trees. We apply the
method of Stasko and Vitter (1987) of performing lazy insertions, using an auxiliary insertion
buffer and adding a single-node tree to it with every insertion. There are two new ideas behind
the implementation of our variant.
The first idea is that, when applying a decrease-key operation on a node x, we not only cut x’s
subtree from its parent, we also cut the subtree of the leftmost child of x from x’s subtree and glue
this leftmost subtree in the place of x’s subtree. The intuition is that cutting subtrees would result
in holes implying bad heap structures, while replacing a subtree with the subtree of the leftmost
child of its root would hopefully fill the hole with a subtree that is comparable in size. Each heap
will have, in addition to the insertion buffer, another pool of trees that contains the main tree and
the subtrees (without the leftmost subtrees of their roots) cut by the decrease-key operations. A
pointer to the root of the tree that has the minimum value among those in the insertion buffer and
the pool is maintained.
The second idea is on how to combine the trees of a heap into one tree to be considered as the
main tree of the heap in the next phase. We rely on an operation that we call consolidate. The
insertion buffer holds single-node trees that have been inserted since the last call to consolidate.
These nodes are combined using the multi-pass pairing; by joining them in pairs, the resulting
trees are joined again in pairs, and this repeats until a single tree remains. The pool holds at most
logn1 trees of various sizes, where n is the number of elements in this heap; these are the subtrees
that have been cut by the decrease-key operations, plus the remainings of the main tree, since the
last call to consolidate. Once the number of trees of this pool reaches the threshold, we apply a
sorting subroutine on the keys of the roots of these trees and combine them according to such
order. The intuition is that since we do not know the sizes of these trees, there is no guarantee on
how to combine them into an efficient structure. Linking the trees in order, each as the leftmost
child of the other, will result in such a good structure. Because sorting is expensive, we have to
involve a bounded number of at most logn such trees.
Next, we give the detailed implementations for various heap operations.
—find-min(H). Return the value of the node pointed to by H’s minimum pointer.
—insert(k,H). Create a single-node tree with key value k and add it to the insertion buffer of
heap H. Refer H’s minimum pointer to the new node if its key is smaller than the current
minimum.
We use the following procedure in implementing the upcoming operations:
—consolidate(H). Combine the trees of H’s pool into one tree by sorting the values of these
tree-roots and linking the trees in this order such that their roots form a path in the combined tree (make every root the leftmost child of the root with the next smaller value). See
Figure 1. As in Stasko and Vitter (1987), combine the nodes of H’s insertion buffer into one
tree using the multi-pass pairing. Join the combined tree of the insertion buffer and the
combined tree of the pool.
—decrease-key(x, δ,H). Decrease the key of the corresponding node x in heap H by δ. Refer
the minimum pointer of H to x if its new key is smaller than the current minimum. If x is
not a root: (1) Cut x’s subtree. (2) If x is not a leaf, cut the subtree of the leftmost child of x
and glue it in place of x’s subtree. (3) Add the rest of x’s subtree (excluding the subtree of
1log x def
= max{1, log2 x}.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.  
Toward Optimal Self-Adjusting Heaps 55:5
Fig. 1. A consolidate operation on the trees pool.
Fig. 2. A cut performed by the decrease-key operation on node x.
x’s leftmost child that has just been cut, if it exists) to the pool of H as a standalone tree. (4)
If the pool of H now has logn trees, call consolidate. See Figure 2.
—delete-min(H). Remove the root pointed to by the minimum pointer of heap H. Recombine the subtrees of this root into one tree using the two-pass method of the pairing heaps
(Fredman et al. 1986). Combine the trees of H into one tree by calling consolidate. Refer H’s
minimum pointer to the root of the resulting tree.
—meld(H1,H2). Combine the trees of the heap that has the smaller size between H1 and H2
into one tree by calling consolidate. Add the resulting tree to the pool of the larger heap,
and destroy the smaller heap. Refer the minimum pointer of the melded heap to the root of
this tree if its key is smaller than the current minimum. If the pool of the melded heap now
has logn trees, call consolidate.
3 ANALYSIS
In this section, we prove the following theorem, which implies the claimed bounds.
Theorem 1. Consider a sequence of operations S = o1, o2,... performed on a collection of heaps,
starting with no elements. Let A = {i | oi is a find-min or an insert operation}, B = {i | oi is a decreasekey or a meld operation}, and C = {i | oi is a delete-min operation}. The sequence S is executed on
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.  
55:6 A. Elmasry
our heaps in O(|A| +
i ∈B log logni +
i ∈C logni ) time, where ni is the number of elements in the
resulting heap on which oi is performed.
We shall make use of the following basic lemma.
Lemma 1. For any n ≥ 2, logn · (log log 2n − log logn) < log e.
Proof.
logn · (log log 2n − log logn) = logn · log
logn + 1
logn

= log
1 +
1
logn
log n
But, for n ≥ 2, (1 + 1
log n )
log n < e, where e is the base of the natural logarithm.
To prove Theorem 1, we use a combination of the potential-function and the accounting methods
(Tarjan 1985). A credit system is used on top of a potential function. Each unit of credit or potential
is equivalent to a constant amortized cost. It would be possible to glue the credit system into the
potential function, but we find our treatment simpler and more intuitive to present the result.
3.1 The Potential Function
In Fredman et al. (1986) and Stasko and Vitter (1987), the potential function Φ =
x log s(x) is used,
summing over all the nodes x, where s(x) is the size (number of nodes) of the subtree rooted at x
in the binary representation of the pairing heaps. Looking at the same thing in a different way, the
potential function Φ =
l log (s1 (l) + s2 (l)) is used in Elmasry (2004), summing over all the links
l, where s1 (l) and s2 (l) are the sizes (number of nodes) of the linked subtrees at the moment when
link l took place. Recall that a link is realized by joining, with pointers, the root of one subtree as
the leftmost child of the root of the other.
The drawback of these potential functions is that they do not reflect the efficiency of the links
performed. In other words, we want to reflect the fact that when a tree is linked to a smaller tree,
this link is efficient with respect to the amount of information gained. To accomplish this, we use
the potential function Φ =
l log s1 (l)+s2 (l)
s2 (l) , where a tree of size s2 (l) was linked to a tree of size
s1 (l) at the moment when link l took place. This potential function was used in Pettie (2005).
Despite the fact that the potential on a link may reach logn, there is good news concerning our
potential function. When two trees of equal sizes are linked, the potential on the formed link is 1.
Therefore, the sum of the potentials on the links of a binomial tree that has k nodes is k − 1. More
interesting is that the sum of the potentials on any path of links from a node x to a leaf telescopes
to at most log Δ, where Δ is the number of descendants of x. If the path is the left spine of the
subtree of x, the sum of potentials telescopes to exactly log Δ. These properties of the potential
function are basic ingredients in our analysis.
3.2 Credits
We maintain the following credit invariants in addition to proving the amortized bounds in terms
of the potential function:
—Insertion-buffer credits: Two credits per single-node tree in the insertion buffer.
—Pool credits: log logn + 2 credits per pool tree, where n is the current heap size.
—Heap credits: 2 logn credits per heap, where n is the current heap size.
The insertion-buffer credits are used to pay for the multi-pass pairing of the buffer. The pool
credits are mainly used to pay for the sorting of the values in the roots of the pool trees. The heap
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.        
Toward Optimal Self-Adjusting Heaps 55:7
credits are used to compensate for the increase in the potential and pool credits after the meld
operation.
3.3 The Time Bounds
Next, we analyze the time bounds for our operations. Each operation must maintain the potential
function, the credits, and pay for the work it performs.
find-min. No potential changes or extra credits are required. The actual work performed isO(1).
It follows that the cost of find-min is O(1).
insert. Two credits are given to the inserted node to maintain the insertion-buffer credits. As a
result of incrementing the size of the heap following an insertion, the pool credits must be supplied
by log log(n + 1) − log logn units for each tree in the pool. Since the number of trees of the pool is
at most logn, Lemma 1 indicates that less than two credits are enough to adjust the pool credits.
The 2 logn heap credits may require an adjustment of at most two credits. No potential changes
are required. The actual work performed to append the inserted node to the list of the insertion
buffer is O(1). It follows that the amortized cost of insert is O(1).
Consolidate. The following lemma shows that the insertion-buffer credits, in addition to paying
for the actual cost of combining the nodes of the insertion buffer using the multi-pass pairing, will
also compensate for the resulting change in potential.
Lemma 2. The extra potential as a result of performing a multi-pass pairing on the nodes of the
insertion buffer is less than 2b, where b is the size of the buffer.
Proof. If b is a power of two, the resulting tree from the multi-pass pairing is a binomial tree.
In this case, the value of the potential on each of the b − 1 links is one, for a total of b − 1 units.
The worst-case scenario that maximizes the potential function if b is not a power of two is when
the smaller tree is linked to the larger tree whenever two trees of different sizes are linked, as the
potential function will have the smaller value in the denominator. Performing a multi-pass pairing
on the nodes of the insertion buffer, and applying this worst-case scenario, the resulting tree will
be in the form of a set of binomial trees each linked as the leftmost child of the one larger in size.
Other than the links of the leftmost path in this combined tree, the potentials on the other links
add up to at most b − 1 (each of these at most b − 1 links holds one unit of potential). Since the
sum of the potential values on a path telescopes to the logarithm of the total number of nodes,
the sum of these values on the leftmost path of the combined tree is logb. Therefore, the total
potential on the links of the combined tree is at most b + logb − 1 (in fact, the maximum value
reaches b + logb − 2 when a single node is linked to a binomial tree of size b − 1).
The trees of the pool are combined by sorting the key values in their roots and linking them
accordingly in order, forming a new path of links. Since the sum of the potential values on a path
telescopes, the increase in potential is logn. Another logn increase in potential is due to linking
this combined tree with the combined insertion buffer. This potential increase is paid for by the
operation that calls consolidate. If consolidate is called within decrease-key, then the number of
nodes on which decrease-key has been performed since the previous call to consolidate is at least
logn, and the increase in potential is paid from the pool credits that will be released, two credits
per tree pool. If consolidate is called within delete-min, the delete-min operation pays the increase
in potential. If consolidate is called within meld, the heap credits of the destroyed heap pays for
the increase in potential. Since the number of trees of the pool isr ≤ logn, the actual work done
in sorting is O(r · log logn), which is paid from the pool credits, log logn credits per tree pool. It
follows that the amortized cost of consolidate is zero.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.    
55:8 A. Elmasry
decrease-key. Performing a decrease-key on a non-root node x results in reducing the number of
descendants of all the ancestors of x. Accordingly, there would be an increase in potential for the
links along the path of the ancestors of x. However, we are still on the safe side and can prove that
the total potential on the links decreases! Before the decrease-key operation, consider the path ℘ of
the nodes from the root including the h ≥ 1 ancestors of x followed by the nodes on the left spine
of the subtree of x. Since we cut the subtree of x and replace it with the subtree of its leftmost
child, the nodes of ℘ remain the same after the decrease-key except for x. Let P be the sum of
the potentials on the links of ℘ before the decrease-key. Recall that the potential on a link is the
logarithm of the ratio of the size of the two involved subtrees to the size of the linked subtree
only. The fact we use here is that the numerator of this ratio is an integer strictly larger than the
denominator.
Consider first the case where x is not a leaf. Then,
P = log
τ 

1
τ1
·
τ 

2
τ2
... τ 

h
τh
· Δ

,
for some positive integers τ 

1 > τ1 ≥ τ 

2 > τ2 ≥ ··· ≥ τ 

h > τh ≥ Δ, where Δ is the number of descendants of x (including x itself). Let P
 be the sum of the potentials on ℘ after the decrease-key
operation. Then,
P
 = log
τ 

1 − δ
τ1 − δ ·
τ 

2 − δ
τ2 − δ ... τ 

h − δ
τh − δ · (Δ − δ )

,
where δ is the number of the descendants of x excluding those of the subtree of the leftmost child
of x, i.e., Δ > δ > 0. As stated above, the change in potential for a link on the path of the ancestors
of x is log (
τ 

i −δ
τi−δ /
τ 

i
τi ), which is positive as τ 

i −δ
τi−δ /
τ 

i
τi > 1. Still, we can write
P
 − P = log
τ 

1 − δ
τ 

1
·
τ 

2 − δ
τ1 − δ
 τ 

2
τ1
·
τ 

3 − δ
τ2 − δ
 τ 

3
τ2
... Δ − δ
τh − δ
 Δ
τh

,
which is negative as τ 

1−δ
τ 

1
< 1, τ 

i+1−δ
τi−δ /
τ 

i+1
τi ≤ 1 and Δ−δ
τh −δ / Δ
τh ≤ 1, indicating that the total potential
of this path decreases.
Consider next the case where x is a leaf. Then,
P = log
τ 

1
τ1
·
τ 

2
τ2
... τ 

h−1
τh−1
· τ 

h

,
P
 = log
τ 

1 − 1
τ1 − 1 ·
τ 

2 − 1
τ2 − 1 ... τ 

h−1 − 1
τh−1 − 1

,
and hence we can write
P
 − P = log
τ 

1 − 1
τ 

1
·
τ 

2 − 1
τ1 − 1
 τ 

2
τ1
·
τ 

3 − 1
τ2 − 1
 τ 

3
τ2
... 1
τh−1 − 1
 τ 

h
τh−1

,
which is again negative since 1
τh−1−1 / τ 

h
τh−1 ≤ 1 as τ 

h ≥ 2.
In both cases, since the potentials on all the other links either decrease or remain the same, it
follows that the total potential on all the links decreases as a result of the cuts accompanying the
decrease-key operation.
When a tree is cut and added to the pool, an extra log logn + 2 credits are needed for such a
tree to maintain the pool credits. The consolidate operation will be called only when the number of
trees of the pool is logn. In such a case, the O(logn · log logn) cost of consolidate will be paid for
from the pool credits (each of the logn trees has log logn + 2 credits). The actual work done by
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.        
Toward Optimal Self-Adjusting Heaps 55:9
Fig. 3. A delete-min ((ci  bi )  ai ) step.
decrease-key, other than the call to consolidate if performed, is O(1). It follows that the amortized
cost of decrease-key is O(log logn).
meld. The cost of the consolidate operation performed on the smaller heap is paid for by the
insertion-buffer, pool, and heap credits of this heap as it is destroyed. When the combined tree of
the smaller heap is added to the pool of the larger heap, an extra log logn + 2 credits are needed
for such a tree to maintain the pool credits of the melded heap, and these credits are paid for by
the meld operation. The size of the larger heap now increases, but at most doubles. In accordance,
its pool credits need to be increased by log log 2n − log logn per tree. By Lemma 1, as there are
at most logn trees in the pool, less than two credits are needed to adjust the pool credits for
this increase in size. The heap credits of the melded heap require an adjustment of at most two
credits as well. A consolidate operation for the melded heap is called only when the number of
trees of the pool is logn. In such a case, the O(logn · log logn) cost of consolidate is paid for
from the logn · (log logn + 2) pool credits. The actual work done by meld, other than the calls
to consolidate, is O(1). It follows that the amortized cost of meld is O(log logn).
delete-min. We think about the two-pass pairing as being performed in steps. At the i-th step,
the pair of trees that is the i-th pair from the right among the subtrees of the deleted root are
joined, then the resulting tree is joined with the combined tree from the joins of all the previous
steps. Each step will then involve three trees and two joins. Let ai be the tree resulting from the
joins of the previous steps, and let Ai be the number of nodes in ai . Let bi and ci be the i-th pair
from the right among the subtrees of the deleted root to be joined at the i-th step, and let Bi and
Ci , respectively, be the number of nodes in their subtrees. It follows that Ai+1 = Ai + Bi + Ci . Let
(t2  t1) denote the tree resulting from the linking of tree t2 to tree t1 as its leftmost subtree.
During the delete-min, the two links that are cut at the i-th step have a potential
log Ai + Bi
Bi
+ log Ai + Bi + Ci
Ci
.
Four cases are possible for the linkings of bi,ci , and ai , depending on which trees are linked to
which. See Figure 3 for the first case.
Case 1. The tree ci is linked to bi , then the resulting tree is linked to ai :
The potential on the new links is:
log Bi + Ci
Ci
+ log Ai + Bi + Ci
Bi + Ci
= log Ai + Bi + Ci
Ci
.
The difference in potential is:
log Bi
Ai + Bi
< log Bi + Ci
Ai
.
Case 2. The tree bi is linked to ci , then the resulting tree is linked to ai :
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.   
55:10 A. Elmasry
The potential on the new links is:
log Bi + Ci
Bi
+ log Ai + Bi + Ci
Bi + Ci
= log Ai + Bi + Ci
Bi
.
The difference in potential is:
log Ci
Ai + Bi
< log Bi + Ci
Ai
.
Case 3. The tree ci is linked to bi , then ai is linked to the resulting tree:
The potential on the new links is:
log Bi + Ci
Ci
+ log Ai + Bi + Ci
Ai
.
The difference in potential is:
log (Bi + Ci ) · Bi
Ai · (Ai + Bi ) < 2 log Bi + Ci
Ai
.
Case 4. The tree bi is linked to ci , then ai is linked to the resulting tree:
The potential on the new links is:
log Bi + Ci
Bi
+ log Ai + Bi + Ci
Ai
.
The difference in potential is:
log (Bi + Ci ) · Ci
Ai · (Ai + Bi ) < 2 log Bi + Ci
Ai
.
We show in the next two items that the sum of the amortized costs of all the steps performed
within a delete-min operation is O(logn).
(i) If Bi+Ci
Ai ≤ 1
2 , then the i-th step for any of the four cases results in a change of potential of
less than log Bi+Ci
Ai ≤ −1. This released, at least one unit of potential is used to pay for the
work done at this step.
(ii) If Bi+Ci
Ai > 1
2 , we call this step a bad step. For all the four cases, the sum of the change in
potential resulting from all bad steps is at most 2
Bi+Ci >Ai log Bi+Ci
Ai (taking the summation
for positive terms only). Since Ai
 ≥ Bi + Ci when i
 > i, this sum telescopes to O(logn). It
remains to account for the actual work done at the bad steps. Since Ai+1 = Ai + Bi + Ci , a
bad step i results in Ai+1 > 3
2Ai . Then, the number of bad steps is O(logn). Since the actual
work done per step is O(1), the actual work done at the bad steps is O(logn).
The cost of the consolidate operation is O(r · log logn + logn), and r is the number of trees in
the pool at the time when the delete-min is performed. If r · log logn > logn, this cost is paid for
using the r · (log logn + 2) pool credits. Otherwise, the cost ofconsolidate isO(logn), which is paid
for by the delete-min operation. Summing up, it follows that the amortized cost of the delete-min
operation is O(logn).
4 COSTLESS MELD
To improve the amortized cost of meld, we rely on two basic ideas. The first idea is that combining
two insertion buffers is cheap as long as the insertion buffers are each implemented as a list that
could be simply appended one to the other. In accordance, when appending the insertion buffers,
we do not need to pay any cost for extra credits or potential units. The second idea is to make
sure that the pool of each heap has either hosted no new elements or has hosted a large number
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017. 
Toward Optimal Self-Adjusting Heaps 55:11
of elements—while still counting the deleted elements—since the previous meld. If this happens,
the extra credits needed when melding a consolidated pool to the other can be paid for by other
operations (the two pools to be melded are either empty, big enough, or have encountered many
element deletions). To be able to achieve this, we make sure that the size of the insertion buffer
reaches some threshold before it is combined into a large-enough tree that is moved to the pool.
The penalty paid in consequence is that delete-min has to each time scan the insertion buffer to
find its minimum, and hence, the delete-min cost would increase.
The delete-min operation will not call consolidate before it aborts. Alternatively, it has to scan all
the roots of the insertion buffer and the pool to refer the minimum pointer to the new minimum. As
before, once the number of trees of the pool becomes logn following an operation, we combine
the trees of the pool into one tree by sorting. However, we do not accompany this with combining
the trees of the insertion buffer. The insertion buffer, in accordance, keeps growing with new
insertions until its size becomes t = log log N, where N is the total number of elements in the
collection of heaps. Once the number of elements of the insertion buffer reaches this threshold,
the insertion buffer is combined using the multi-pass pairing into one tree, and the resulting tree
is moved to the pool. To meld two heaps, the pool of the smaller heap is combined into one tree
using sorting and the resulting tree is moved to the pool of the melded heap. The two insertion
buffers are appended to each other and moved to the insertion buffer of the melded heap. If the
size of the melded buffer reaches the threshold, it is combined using the multi-pass pairing into
one tree, which is subsequently moved to the pool.
We use the following two separate procedures for consolidating the heaps:
—combine-buffer(H). Combine the nodes of the insertion buffer of heap H into one tree using
the multi-pass pairing. Move the resulting tree to the pool of H.
—combine-pool(H). Combine the trees of the pool of heapH into one tree by sorting the values
of the roots of these trees and linking the trees in this order.
Here are the detailed operations.
—find-min(H). Return the value of the node pointed to by H

s minimum pointer.
—insert(k,H). Create a single-node tree with key value k and add it to the insertion buffer
of heap H. Refer the minimum pointer of H to the new node if its key is smaller than the
current minimum. If the number of elements ofH’s buffer is log log N, callcombine-buffer;
and if the number of trees of H’s pool becomes logn, call combine-pool.
—decrease-key(x, δ,H). Decrease the key of the corresponding node x by value δ. Refer the
minimum pointer of heap H to x if its new value is smaller than the current minimum. If x
is not a root: (1) Cut x’s subtree. (2) If x is not a leaf, cut the subtree of the leftmost child of
x and glue it in place of x’s subtree. (3) Add the rest of x’s subtree to the pool of H. (4) If
the pool now has logn trees, call combine-pool.
—delete-min(H). Remove the root pointed to by the minimum pointer of heap H. Recombine
its subtrees into one tree using the standard two-pass pairing. Scan H’s roots and refer the
minimum pointer to the one with the minimum value.
—meld (H1,H2). Combine the trees of the pool of the smaller heap between H1 and H2 calling
combine-pool. Add the resulting tree to the pool of the larger heap. Append the buffer of the
smaller heap to that of the larger, and destroy the smaller heap. If the melded buffer now has
at least log log N nodes, call combine-buffer. If the melded pool now has at least logn
trees, call combine-pool. Refer the minimum pointer of the melded heap to the smaller of
the minima of the two heaps.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.       
55:12 A. Elmasry
To ensure the claimed time bounds, we only need to illustrate who will pay for the pool credits of
the trees that are moved there, log logn + 2 credits per tree. If a tree is moved to the pool following
a consolidation of the insertion buffer, it must have t = log log N elements, so the credits are paid
by the preceding insert operations. Another reason for a tree to be moved to a pool is when two
heaps are melded. We bound the total number of such trees in terms of the overall number of
insertions. Since the trees that are moved from the insertion buffers to the pools are each of size
at least t, we can bound the total number of heaps that ever had non-empty pools by the overall
number of insertions divided by t. Since the number of these heaps decreases with every meld
that involves non-empty pools, the total number of these meld operations is also bounded by the
same quantity. The credits needed on each of the consolidated pools when moved to another pool
is log logn + 2, which is at most t + 2 by our choice of t (that is why we chose t = log log N,
which is larger than log logn for all the heaps). The overall credits needed are then proportional
to the number of insertions performed, and can be paid from the constant cost of each insertion.
The meld operation has to pay nothing on its own.
When the value of t changes while N is changing, we rebuild the whole data structure. For
this global rebuilding to happen, we must have already performed plenty of heap operations that
are enough to pay for the cost of the rebuilding and the required credits. Note that the value of
t remains the same as long as N is in the range {22j
· ·22j+1
− 1}, where j is a positive integer.
One way to proceed is to rebuild each heap by breaking down all the trees of its pool into single
nodes first. If the number of elements in the heap is less than t, we keep all of them as a list in
the insertion buffer. Otherwise, we combine all the elements into one tree using the multi-pass
pairing and keep the resulting tree in the pool. The objective is to maintain the size of every
buffer below the threshold t and every pool to be either empty or to contain one tree of size at
least t.
5 COMMENTS AND CONCLUSIONS
We have given a variation of the pairing heaps that achieves optimal amortized time bounds,
except for the decrease-key operation that still matches both Fredman’s (1999a) and Iacono-Özkan’s
(2014a) lower bounds for a family of self-adjusting heaps that generalizes the pairing heaps. One of
the settings for Fredman’s lower bound to apply is that a comparison is followed by a link between
the nodes containing the compared values. We note that our variation does not follow the settings
of this lower bound to hold; using sorting to combine the pool trees breaks such setting. One of the
settings for the lower bound of Iacono and Özkan is that the decrease-key operation is performed
by cutting the subtree of the node whose value decreased and adding it as the leftmost tree of
the heap, nothing more. We note that our variation also does not follow the settings of this lower
bound to hold; keeping the subtree of the leftmost child of the decreased node in its place breaks
such setting.
We need to point out that the possibility of inventing a self-adjusting heap that achieves an
asymptotically better bound for the decrease-key operation, while having the same optimal bounds
for other operations is still in question. Such self-adjusting heap structure should not follow the
settings of either Fredman’s or Iacono-Özkan’s lower bounds, though.
In Fredman (1999a), it was stated that the cost of m pairing-heap operations, including n deletemin operations, is O(m log2m/n n), which implies a constant amortized cost per decrease-key when
m = Ω(n1+ϵ ), for any constant ϵ > 0. The new variation performs a sequence of m operations,
including n delete-min operations, inO(m log logn + n logn) time, which implies a constant amortized cost per decrease-key when m = O(n logn/ log logn). This efficiency of the pairing heaps
when the ratio of the decrease-key to delete-min operations is either small or large (asymptotically)
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 55. Publication date: November 2017.  
Toward Optimal Self-Adjusting Heaps 55:13
illustrates the practically efficient performance for the decrease-key operation (Elmasry and
Fredman 1997).
We use sorting to implement our heap operations. Beyond the decision-tree model, breaking
the Ω(n logn) bound for sorting n integers can be done under several circumstances. Applying
these sorting methods to our heap structure would break the O(log logn) bound for decrease-key.
For example, if the number of bits of the integers representing the key values of the heap is a
constant multiple of logn, radix sort runs in linear time in the RAM model. In such a case, we
would achieve a constant cost per decrease-key. As pointed out in Iacono and Özkan (2014b), the
reason that Fibonacci heaps achieve optimal bounds is that sorting the ranks, which are small
integers, can be done in linear time in the RAM model.
It is debatable to what extent our data structure is self-adjusting. The sorting step is slightly
foreign to the spirit of self-adjustment, but it does not contradict with either the definition or the
practical intentions. There is also a need by our structure to maintain a couple of counters for
the number of elements per heap (although this could presumably be encoded somehow in the
structure). Our argument is that we only keep global information in few variables with simple
operations to maintain; there is still no need to keep extra information within the nodes of the
structure.
We make no claims concerning the practicality of our data structure. We expect our structure
to perform better than Fibonacci heaps and its relatives that achieve the same optimal theoretical
bounds. On the other hand, we expect the standard pairing heaps to be superior in practice to our
variant. Experiments are still to be conducted to verify those expectations.
Four important open questions are:
—Is there a self-adjusting heap that has an amortized o(log logn) cost for the decrease-key
operation?
—Is it possible that the standard implementation of the pairing heaps (Fredman et al. 1986)
has the same bounds as those we achieve in this article?
—Is it possible to achieve our results without keeping track of and using the sizes of the heaps
in operation?
—Which heap is the best in practice?