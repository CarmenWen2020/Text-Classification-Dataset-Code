Chazelle and Matoušek [J. Algorithms, 1996] presented a derandomization of Clarkson’s sampling-based algorithm [J. ACM, 1995] for solving linear programs with n constraints and d variables ind(7+o(1))dn deterministic time. The time bound can be improved to d(5+o(1))dn with subsequent work by Brönnimann, Chazelle,
and Matoušek [SIAM J. Comput., 1999]. We first point out a much simpler derandomization of Clarkson’s
algorithm that avoidsε-approximations and runs in d(3+o(1))dn time. We then describe a few additional ideas
that eventually improve the deterministic time bound to d(1/2+o(1))dn.
CCS Concepts: • Theory of computation → Linear programming; Computational geometry;
Additional Key Words and Phrases: Computational geometry, linear programming, derandomization, epsilon-nets
1 INTRODUCTION
This article studies the well-known linear programming problem and focuses on algorithms in
the real RAM model, where running time (the number of arithmetic operations) is analyzed as a
function of the number of constraints n and the number of variables d, but not the bit complexity
of the input. The major open question is whether a polynomial algorithm on the real RAM, also
known as a “strongly polynomial” algorithm, exists for linear programming. (In contrast, for the
latest advance on polynomial-time linear programming algorithms that depend on the input bit
complexity, see Reference [20], for example.)
In the early 1980s, Megiddo [24] discovered a linear programming algorithm that runs in O(n)
time for any constant dimension d; this result has become a cornerstone of computational geometry, with many applications in low-dimensional settings. Since Megiddo’s work, many other
linear-time algorithms have been proposed, which improve the (super)exponential dependency of
the hidden factor on d; the story has been recounted in many books and surveys [3, 7, 13, 16, 21,
25]. Table 1 summarizes the previous results and also states our new result.
Table 1. Deterministic and Randomized Time Bounds for Linear Programming on the Real RAM
Simplex method det. O(n/d)
d/2+O (1)
Megiddo [24] det. 2O (2d )
n
Clarkson [9]/Dyer [14] det. 3d2
n
Dyer and Frieze [15] rand. O(d)
3d (logd)
dn
Clarkson [10] rand. d2
n + O(d)
d/2+O (1) logn + d4√
n logn
Seidel [26] rand. d!n
Kalai [19]/Matoušek, Sharir, and Welzl [23] rand. min{d22dn, e2
√
d ln(n/
√
d)+O (
√
d+log n)
}
combination of [10] and [19, 23] rand. d2
n + 2O (
√d log d)
Hansen and Zwick [18] rand. 2O (
√d log((n−d)/d))n
Agarwal, Sharir, and Toledo [4] det. O(d)
10d (logd)
2dn
Chazelle and Matoušek [8] det. O(d)
7d (logd)
dn
Brönnimann, Chazelle, and Matoušek [5] det. O(d)
5d (logd)
dn
This article det. O(d)
d/2 (logd)
3dn
As one can see, our improvement is a modest one and does not alter the form dO (d)
n of the
best-known deterministic time bounds; also, our new algorithm does not beat existing randomized algorithms. However, we believe the result is still interesting for several reasons. First, linear
programming is among the most fundamental algorithmic problems of all time, and no progress
on deterministic real-RAM algorithms has been reported for years. Second, we obtain a dependency of d(1/2+o(1))d on the dimension, reaching a natural limit for the kinds of approach considered here and in Chazelle and Matoušek’s prior work [8]. These are based on derandomization of Clarkson’s sampling algorithms [10], which reduce the problem to subproblems with no
fewer than n ≈ d2 constraints, for which the currently best deterministic real-RAM upper bound
is about (n/d)
d/2 = dd/2, as guaranteed by the simplex method. (The best randomized algorithms
by Kalai [19] and Matoušek, Sharir, and Welzl [23] are faster, but no derandomization of results of
that kind is known.) Note that our new d(1/2+o(1))dn deterministic result even beats the well-loved
randomized O(d!n)-time algorithm by Seidel [26]; and it is superior to all existing deterministic
bounds on the real RAM for n  d2.
Our technical ideas are also noteworthy in several ways:
(1) We observe that derandomizing Clarkson’s recursive sampling algorithm [10] can lead
to a simple deterministic linear-time algorithm for constant dimensions. Chazelle and
Matoušek [8] previously derandomized Clarkson’s algorithm but needed a combination of
several ideas (including the method of conditional probabilities and a nontrivial merge-andreduce algorithm for computing ε-approximations). In contrast, our simplest derandomization can be described in under two pages, as we explain in Section 2, and is completely
self-contained except for the use of the standard greedy algorithm for set cover or hitting
set. The resulting algorithm is actually simpler than Megiddo’s original deterministic algorithm [24] and all its subsequent refinements [4, 14]. This is perhaps the main takeaway
of the article. (Megiddo’s algorithm grouped the input into pairs and invoked linear-time
median finding repeatedly; our algorithm uses groups of a larger size but does not need
median finding at all. Megiddo’s algorithm also required more complicated primitive operations; our algorithm only requires standard orientation tests on d + 1 input hyperplanes.)
Incidentally, even without attempting to optimize the dependency on d, our algorithm in
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.
Improved Deterministic Algorithms for Linear Programming in Low Dimensions 30:3
its simplest version has a running time at most d(3+o(1))dn, already improving all prior
published deterministic bounds.
(2) Clarkson’s article [10] described a second sampling algorithm, based on iterative reweighting (or in more trendy terms, the multiplicative weights update method). This second algorithm has expected n logn running time in terms of n but has better dependency on d.
We point out a simple variant of Clarkson’s second algorithm that has running time linear
in n; this variant appears new. If randomization is allowed, then this is not too interesting, since Clarkson [10] eventually combined his two algorithms to obtain his best results
with running time linear in n. However, the variant makes a difference for deterministic
algorithms, as we will see in Section 3.
(3) The heart of the matter in obtaining the best derandomization of Clarkson’s algorithms
lies in the construction of ε-nets. This was the focus of the previous article by Chazelle
and Matoušek [8]. In Section 4, we present an improved ε-net algorithm (for halfspace
ranges) that has running time about O(1/ε)
d/2 when n is polynomial in d and that may
be of independent interest. This is the most technical part of the article (here we need to
go back to previous derandomization concepts, namely, (sensitive)ε-approximations). Still,
the new idea can be described compactly with an interesting recursion.
2 CLARKSON’S FIRST ALGORITHM
In linear programming, we want to minimize a linear function over an intersection of n halfspaces.
Without loss of generality, we assume that the halfspaces are in general position (by standard perturbation techniques) and all contain (0,..., 0, −∞) (by adding an extra dimension if necessary).
The problem can then be restated as follows:
Given a set H of n hyperplanes in Rd , find a point p that lies on or below1 all
hyperplanes in H, while minimizing a given linear objective function.
All our algorithms rely on the concept of ε-nets, which in this context can be defined as follows:
Definition 2.1. Given p ∈ Rd , let Violatep (H) = {h ∈ H : h is strictly below p}. A subset R ⊆ H
is an ε-net of H if for every p ∈ Rd ,
Violatep (R) = ∅⇒|Violatep (H)| ≤ ε|H|.
Fact 1.
(a) (Mergability) If Ri is an ε-net of Hi for each i and the Hi ’s are disjoint, then
i Ri is an ε-net
of
i Hi .
(b) Given a set H of n ≥ d hyperplanes in Rd , we can construct an ε-net of H of size O( d
ε logn)
in O(n/d)
d+O (1) deterministic time.
Proof. (a) is clear from the definition. For (b), we want a subset R ⊆ H that hits the set
Violatep (H) for every p ∈ Rd of level > εn, where the level of a point p is defined to be
|Violatep (H)|. It suffices to consider just the vertices of the arrangement of H (since for every
p ∈ Rd , there is an equivalent vertex having the same set Violatep (H)). The number of vertices in
the arrangement is m = O(( n
d )) = O(n/d)
d . We can enumerate them trivially in dO (1)
m time. Afterwards, we can run the standard greedy algorithm [12] to compute a hitting set for m given sets
in a universe of size n in O(mn) time. In our case, when all given sets have size > εn, the greedy
algorithm produces a hitting set of size O( 1
ε logm) = O( d
ε logn).
1Throughout the article, “below” and “above” refer to the dth coordinate value.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.   
30:4 T. M. Chan
Let δ > 0 be a sufficiently small constant. We begin with a version of Clarkson’s first algorithm [10] for linear programming, described in the pseudocode below. (The original version used
a random sample R of size near d
√
n, which behaves like a (1/
√
n)-net; our version instead uses a
Θ(1/d2)-net R, following Chazelle and Matoušek [8].)
LP(H):
0. if |H| = O(d3 logc d) then return answer directly
1. compute a (δ/d2)-net R of H
2. for j = 1, 2,... {
3. p = LP(R ∪ X1 ∪···∪ Xj−1)
4. Xj = Violatep (H)
5. if Xj = ∅ then return p
}
Let B∗ denote the set of d hyperplanes defining the optimal solution. Observe that in each iteration of the for loop before the last, Xj must include a new member of B∗, because otherwise,
p would not be violated by any member of B∗ and would thus have a worse objective value than
LP(B∗) = LP(H), a contradiction. It follows that the loop has at most d + 1 iterations.
Much of the effort in Chazelle and Matoušek’s derandomization [8] is devoted to the construction of the net R in line 1. To this end, they generalized the problem to the construction of εapproximations and described a clever linear-time algorithm [8, 22] that used repeated merge and
reduce steps, with a base case that involved the method of conditional probabilities (another alternative is the bounded independence technique [17]). Our new idea for line 1 bypasses all this and
is a lot more straightforward—we just apply a standard grouping trick, based on Fact 1(a), and
construct a net for each group naively, as described in lines 1.1–1.3 below. The key observation
is that for Clarkson’s algorithm, we do not need a net of optimal size—slightly sublinear size is
already good enough.
1.1. arbitrarily divide H into groups H1,...,H|H |/b  of size at most b
1.2. for i = 1,..., |H|/b, compute a (δ/d2)-net Ri of Hi
1.3. R =
i Ri
Assume that there is an algorithm to construct an ε-net for n hyperplanes of size O( d
ε logc n)
in Tnet(n,ε) deterministic time for some constant c. Let n = |H|. Then line 1.2 takes O(n/b ·
Tnet(b, δ/d2)) time. The set R in line 1.3 has size O(n/b · d3 logc b), which can be made at most
n/(2d) + O(d3 logc d) by choosing a group size b = Θ(d4 logc d). Furthermore, each Xj has size
at most δn/d2, so R ∪ X1 ∪···∪ Xj−1 has size at most n/(2d) + O(d3 logc d) + δn/d. Line 0 takes
O(d2 logc d)
d/2 time by the simplex method. Line 4 takes O(dn) time. Thus, the running time of
LP(H) satisfies the recurrence
TLP(n) =
⎧⎪⎪⎪
⎨
⎪⎪⎪
⎩
O(d2 logc d)
d/2 if n = O(d3 logc d)
(d + 1)TLP
( 1
2 +δ ) n
d + O(d3 logc d)

+ O(Tnet(O(d4 logc d), δ/d2) n)
+ O(d2
n) else,
implying
TLP(n) = O(Tnet(O(d4 logc d), δ/d2) n) + O(d2 logc d)
d/2
n.
By Fact 1(b),Tnet(O(d4 logc d), δ/d2) = O(d3 logc d)
d with c = 1. We have therefore obtained a simple deterministic algorithm for linear programming with running time O(d3 logd)
dn.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.  
Improved Deterministic Algorithms for Linear Programming in Low Dimensions 30:5
3 CLARKSON’S SECOND ALGORITHM
Clarkson [10] gave a second randomized algorithm for linear programming, based on iterative
reweighting. Originally the algorithm required O(d logn) iterations and led to a running time
that is slightly superlinear in n. We describe a new variant that requires O(d logd) iterations and
maintains linear dependency on n. The main advantage of the second algorithm is that it works
with Θ(1/d)-nets instead of Θ(1/d2)-nets.
The algorithm is described in the pseudocode below, where initially all elements of H have
multiplicity 1. (The main novelty is that when the multiplicity of each element reaches a certain
limit, we move the element to a setX. Another change is that instead of a purely iterative algorithm,
our variant will continue to use recursion.)
LP(H):
0. if |H| = O(d2 logc d) then return answer directly
1. repeat {
2. compute a (δ/d)-net R of H
3. X = {h ∈ H : h has multiplicity ≥ d2 in H} (with multiplicities reset to 1 in X)
4. p = LP(R ∪ X)
5. for each h ∈ Violatep (H) do
6. double h’s multiplicity in H
7. if Violatep (H) = ∅ then return p
}
Let B∗ denote the set of d hyperplanes defining the optimal solution. Observe that in each iteration of the repeat loop before the last, at least one member of B∗ has its multiplicity doubled,
because otherwise, p would not be violated by any member of B∗, and would thus have a worse
objective value than LP(B∗) = LP(H). The multiplicity of an element stays unchanged in H when
it reaches d2 or above. Thus, the loop has at most d log2 (d2) + 1 = O(d logd) iterations.
Let |H| denote the size of H, counting multiplicities. Let n denote the initial size of H. In each
iteration, |H| increases by at most δ |H|/d. Thus, at the end of the loop, |H| ≤ (1 + δ/d)
O (d log d)
n ≤
dO (δ )
n. On the other hand, |X |≤|H|/d2 ≤ n/d2−O (δ )
.
As before, we replace line 2 with:
2.1. arbitrarily divide H into groups H1,...,H|H |/b  of size at most b
2.2. for i = 1,..., |H|/b, compute a (δ/d)-net Ri of Hi
2.3. R =
i Ri
Line 2.2 takes O(dO (δ )
n/b · Tnet(b, δ/d)) time. The set R in line 2.3 has size O(dO (δ )
n/b ·
(1/δ )d2 logc b), which can be made at most n/d1+Ω(δ ) + O(d2 logc d) by choosing a group size b =
d3+Θ(δ )
. Line 0 takes O(d logc d)
d/2 time by the simplex method. Lines 5–6 take O(dn) time. Thus,
the running time satisfies the recurrence
TLP(n) =
⎧⎪⎪⎪
⎨
⎪⎪⎪
⎩
O(d logc d)
d/2 if n = O(d2 logc d)
O(d logd)TLP  n
d1+Ω(δ ) + O(d2 logc d)

+ O(Tnet(d3+O (δ )
, δ/d) n)
+ O((d2 logd) n) else,
implying
TLP(n) = O(Tnet(d3+O (δ )
, δ/d) n) + O(d logc d)
d/2
n. (1)
By Fact 1(b), Tnet(d3+O (δ )
, δ/d) ≤ d(2+O (δ ))d with c = 1. We have therefore obtained an improved
simple deterministic algorithm for linear programming with running time O(d)(2+O (δ ))dn for any
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.  
30:6 T. M. Chan
constant δ > 0. (The small O(δ )d term in the exponent can probably be improved by using a
weighted version of ε-nets, but this will not matter at the end.)
4 NEW ε-NET CONSTRUCTION
In this section, we provide better bounds for Tnet(n,ε). Two ideas come to mind:
(1) we can improve Fact 1(b) by enumerating only vertices with level capped at εn + 1 rather
than all vertices in the entire arrangement;
(2) we can compute ε-nets faster by following Brönnimann, Chazelle, and Matoušek’s approach of using sensitive approximations [5].
Either idea can lead to an improvement, but for our best improvement, we need a combination of
both, which creates new technical challenges. Brönnimann, Chazelle, and Matoušek’s algorithm
for sensitive ε-approximations [5], based on earlier algorithms by Matoušek [22] and Chazelle and
Matoušek [8] for ε-approximations, involves repeatedly dividing into groups, and merging and
reducing approximations of groups; however, arbitrarily dividing into groups does not preserve
caps on levels. We suggest a new solution that adds another recursive call for the division step.
(The dependency on n is no longer linear, unlike the previous algorithms [5, 8, 22], but will not
matter at the end.)
We need to review technical facts concerning the concept of sensitive ε-approximations; the
construction in Fact 2(c) below requires the method of conditional probabilities [5].
Definition 4.1. Let ρp (H) = |Violatep (H)|/|H|. A subset R ⊆ H is a sensitive ε-approximation of
H w.r.t. P if for every p ∈ P,
|ρp (R) − ρp (H)| ≤ (ε/2)

ρp (H) + ε2
/2.
Fact 2 (Brönnimann, Chazelle, and Matoušek [5]).
(a) (Mergability) If Ri is a sensitive ε-approximation of Hi w.r.t. P and the Hi ’s are disjoint and
have equal size, then
i Ri is a sensitive ε-approximation of
i Hi w.r.t. P.
(b) (Reducibility) If R is a sensitive ε-approximation of H w.r.t. P and R is a sensitive ε
-
approximation of R w.r.t. P, then R is a sensitive (ε + 2ε
)-approximation of H w.r.t. P.
(c) Given a set H of n hyperplanes in Rd with n ≥ r ≥ c0
ε 2 log |P | for some constant c0, we can
construct a sensitive ε-approximation of H w.r.t. P of size r in O(|P |n) deterministic time.
For our purposes of constructing ε-nets, it suffices to construct sensitive approximation w.r.t.
points with level bounded by an appropriate cap.
Fact 3. Define Cap(H, α) = {p ∈ Rd : ρp (H) ≤ α}.
(d) If R is a sensitive √
ε-approximation of H w.r.t. Cap(H,ε + 1
|H |
), then R is an ε-net of H.
(e) If R is a sensitive cε-approximation of H w.r.t. Cap(H, (tε)
2), then Cap(H, (tε)
2) ⊆
Cap(R, ((t + c)ε)
2).
(f) Given a set H of n hyperplanes in Rd with n ≥ r ≥ c0d
ε 2 logn for some constantc0, and given
a parameter t ≥ 1, we can construct a sensitive ε-approximation of H w.r.t. Cap(H, (tε)
2) of
size r in O(tεn/d)
d+O (1) deterministic time.
Proof. For (d), observe that for every p with level ε|H| + 1 (which is in Cap(H,ε + 1
|H |
)), we
have ρp (H) > ε, implying that ρp (R) ≥ ρp (H) − (
√
ε/2)

ρp (H) − ε/2 > 0. This suffices to confirm
that R is an ε-net.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.  
Improved Deterministic Algorithms for Linear Programming in Low Dimensions 30:7
For (e), observe that if ρp (H) ≤ (tε)
2, then
ρp (R) ≤ ρp (H) + (cε/2)

ρp (H) + (cε)
2
/2 ≤ (t
2 + (c/2)t + (c2
/2))ε2 < ((t + c)ε)
2
.
For (f), it suffices to run the algorithm in Fact 2(c) w.r.t. just the vertices of level at most k in
the arrangement of H, with k = (tε)
2
n. Clarkson and Shor [11] (and also Wagner [27]) proved
that the number m of vertices of level at most k is m ≤ ( n
d/2 ) · O(k/d + 1)d/2 = O(n/d)d/2 ·
O(t 2
ε2
n/d + 1)d/2 = O(tεn/d)
d+O (1)
. We can enumerate all vertices of level at most k by an
output-sensitive algorithm that uses repeated ray shooting [1, 2] in O(dO (1)
mn) time (this can
viewed as a generalization of the standard gift-wrapping algorithm for convex hulls [6] in dual
space; the time bound gets better in small constant dimensions by using ray shooting data structures [1, 2], but this will not matter). Afterwards, we can run the algorithm in Fact 2(c) in O(mn)
time.
Lemma 4.2. Given a set H of n hyperplanes in Rd with n a power of 2 and n ≥ c1d
ε 2 log d
ε for some
constant c1, and given parameters ε > 0 and t ≥ 1, we can compute a sensitive ε-approximation of
size exactly n/2 w.r.t. Cap(H, (tε)
2) in deterministic time
Thalver(n,ε,t) = O t+log n
ε log d
ε
d
n1.59.
Proof. We describe our algorithm by the following deceptively concise pseudocode using
recursion:
Halver(H,ε,t):
0. if |H| = O( d
ε 2 log d
ε ) or ε ≥ 2 then return answer directly
1. A = Halver(H, 2ε, t/2)
2. return Halver(A,ε,t + 2) ∪ Halver(H − A,ε,t + 2)
Since A is a sensitive 2ε-approximation of H w.r.t. Cap(H, (tε)
2), from the definitions it follows that H − A is also a sensitive 2ε-approximation (since |H − A| = |A| = |H|/2 implies that
ρp (H − A) − ρp (H) = ρp (H) − ρp (A)). We thus have Cap(H, (tε)
2) ⊆ Cap(A, ((t + 2)ε)
2), Cap(H −
A, ((t + 2)ε)
2) by Fact 3(e). By induction hypothesis and Fact 2(a), the union in line 2 is a sensitive
ε-approximation of H w.r.t. Cap(H, (tε)
2). (To recap, the desired sensitive ε-approximation is computed by the recursive calls in line 2; the only purpose of the recursive call in line 1 is in providing
a good cap for line 2.)
By Fact 3(f), line 0 takesO( t
ε log d
ε )
d time. Assuming that Halver returns linked lists for both the
sensitive approximation and its complement, we see that cost of the algorithm excluding the three
recursive calls in lines 1–2 is O(1) by merging linked lists. The running time of Halver(H,ε,t)
satisfies the recurrence
Thalver(n,ε,t) = ⎧⎪
⎨
⎪
⎩
O( t
ε log d
ε )
d if n = O( d
ε 2 log d
ε )
Thalver(n, 2ε, t/2) + 2Thalver(n/2,ε,t + 2) + O(1) else.
Observe that the depth of the recursion is at most log2 n + log2 (1/ε), since n is either halved or
ε is doubled in each recursive call. The number of nodes in the recursion tree is thus at most
3log2 n+log2 (1/ε ) = O(n/ε)
1.59. The parameter t increases by at most O(logn) and the parameter ε
does not decrease. It follows that Thalver(n,ε,t) ≤ O(
t+log n
ε log d
ε )
dn1.59. (The n1.59 factor is improvable but will not matter at the end.)
Theorem 4.3. Given a setH of n hyperplanes in Rd with n a power of 2, and given parametersε > 0
and t ≥ 1, for some constantc1, we can compute a sensitive ε logn-approximation of size  c1d
ε 2 log d
ε 
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.   
30:8 T. M. Chan
w.r.t. Cap(H, (tε)
2) in deterministic time
T
approx (n,ε,t) = O(
t+log n
ε log d
ε )
dn1.59.
Proof. We describe another recursive algorithm:
Approx(H,ε,t):
0. if |H| = O( d
ε 2 log d
ε ) then return answer directly
1. A = Halver(H,ε,t)
2. R = Approx(A,ε,t + 1) ∪ Approx(H − A,ε,t + 1)
3. return Halver(R,ε/2,t + logn)
Since both A and H − A are sensitive ε-approximations of H w.r.t. Cap(H, (tε)
2), we have
Cap(H, (tε)
2) ⊆ Cap(A, ((t + 1)ε)
2), Cap(H − A, ((t + 1)ε)
2) by Fact 3(e). By induction hypothesis
and Fact 2(a), the union R in line 2 is a sensitive ε log(n/2)-approximation of H w.r.t. Cap(H, (tε)
2).
We have Cap(H, (tε)
2) ⊆ Cap(R, ((t + logn)ε)
2) by Fact 3(e). By Fact 2(b), the set in line 3 is a
sensitive (ε log(n/2) + 2(ε/2) = ε logn)-approximation of H w.r.t. Cap(H, (tε)
2).
By Fact 3(f), line 0 takesO( t
ε log d
ε )
d time. By Lemma 4.2, lines 1 and 3 takeO(
t+log n
ε log d
ε )
dn1.59
time. The running time of Approx(H,ε,t) satisfies the recurrence
T
approx (n,ε,t) = ⎧⎪
⎨
⎪
⎩
O( t
ε log d
ε )
d if n = O( d
ε 2 log d
ε )
2T
approx (n/2,ε,t + 1) + O(
t+log n
ε log d
ε )
dn1.59 else.
It follows that T
approx (n,ε,t) = O(
t+log n
ε log d
ε )
dn1.59.
Corollary 4.4. Given a set H of n hyperplanes in Rd with n a power of 2, and parameters ε > 0
and t ≥ 1, we can compute a sensitive ε-approximation of size O( d
ε 2 log2 n log d
ε ) w.r.t. Cap(H, (tε)
2)
in deterministic time
Tapprox (n,ε,t) ≤ T
approx (n,ε/ logn,t logn) = O( t
ε log2 n log d
ε )
dn1.59.
By Fact 3(d), we conclude:
Corollary 4.5. Given a set H of n hyperplanes in Rd with n a power of 2, and parameter ε > 0,
we can compute an ε-net of size O( d
ε log2 n log d
ε ) in deterministic time
Tnet(n,ε) ≤ Tapprox (n,
√
ε,

1 + 1
εn ) = O( 1
ε )
d/2 (log2 n log d
ε )
dn1.59.
Note the extra logarithmic factors on the net size in the above corollaries (which are improvable
if we increase the running time, but will not matter at the end).
Combining with the approach in Section 3 and applying Corollary 4.5 to (1) with c = 3,
we have finally obtained a deterministic algorithm for linear programming with running time
O(d)
d/2 (logd)
3dn.
5 REMARKS
The same results hold for the problem of computing the smallest enclosing ball of n points in Rd ;
this problem can be transformed to minimizing a convex function in an intersection of n halfspaces
in one dimension higher.
Like Chazelle and Matoušek’s derandomization [8] of Clarkson’s algorithm, our algorithms in
Sections 2 and 3 can more generally solve any LP-type problem [23] of combinatorial dimension d,
assuming just the availability of a subsystem oracle (see Reference [8] for the formal definitions).
The improvement in Section 4 does not completely generalize, because the same combinatorial
bound on “(≤ k)-levels” does not necessarily hold. However, sensitive approximations can still be
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018. 
Improved Deterministic Algorithms for Linear Programming in Low Dimensions 30:9
used and we can still obtain a d(1+o(1))dn deterministic time bound as follows: First, in Fact 3(f),
the time bound without caps is O(n/d)
d+O (1)
. In Theorem 4.3, we can apply a more naive divideand-conquer where in line 1 we simply choose an arbitrary subset A of size |H|/2. The recurrence
becomes
T
approx (n,ε) = ⎧⎪
⎨
⎪
⎩
O( 1
ε 2 log d
ε )
d if n = O( d
ε 2 log d
ε )
2T
approx (n/2,ε) + O( 1
ε 2 log d
ε )
d else,
yielding T
approx (n,ε) = O( 1
ε 2 log d
ε )
dn. In Corollaries 4.4 and 4.5, we then obtain running times of
Tapprox (n,ε) = O( 1
ε 2 logn log d
ε )
dn and Tnet(n,ε) = O( 1
ε logn log d
ε )
dn, implying a final time bound
of TLP(n) = O(d log2 d)
dn for LP-type problems.