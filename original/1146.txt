Chazelle and Matou≈°ek [J. Algorithms, 1996] presented a derandomization of Clarkson‚Äôs sampling-based algorithm [J. ACM, 1995] for solving linear programs with n constraints and d variables ind(7+o(1))dn deterministic time. The time bound can be improved to d(5+o(1))dn with subsequent work by Br√∂nnimann, Chazelle,
and Matou≈°ek [SIAM J. Comput., 1999]. We first point out a much simpler derandomization of Clarkson‚Äôs
algorithm that avoidsŒµ-approximations and runs in d(3+o(1))dn time. We then describe a few additional ideas
that eventually improve the deterministic time bound to d(1/2+o(1))dn.
CCS Concepts: ‚Ä¢ Theory of computation ‚Üí Linear programming; Computational geometry;
Additional Key Words and Phrases: Computational geometry, linear programming, derandomization, epsilon-nets
1 INTRODUCTION
This article studies the well-known linear programming problem and focuses on algorithms in
the real RAM model, where running time (the number of arithmetic operations) is analyzed as a
function of the number of constraints n and the number of variables d, but not the bit complexity
of the input. The major open question is whether a polynomial algorithm on the real RAM, also
known as a ‚Äústrongly polynomial‚Äù algorithm, exists for linear programming. (In contrast, for the
latest advance on polynomial-time linear programming algorithms that depend on the input bit
complexity, see Reference [20], for example.)
In the early 1980s, Megiddo [24] discovered a linear programming algorithm that runs in O(n)
time for any constant dimension d; this result has become a cornerstone of computational geometry, with many applications in low-dimensional settings. Since Megiddo‚Äôs work, many other
linear-time algorithms have been proposed, which improve the (super)exponential dependency of
the hidden factor on d; the story has been recounted in many books and surveys [3, 7, 13, 16, 21,
25]. Table 1 summarizes the previous results and also states our new result.
Table 1. Deterministic and Randomized Time Bounds for Linear Programming on the Real RAM
Simplex method det. O(n/d)
d/2+O (1)
Megiddo [24] det. 2O (2d )
n
Clarkson [9]/Dyer [14] det. 3d2
n
Dyer and Frieze [15] rand. O(d)
3d (logd)
dn
Clarkson [10] rand. d2
n + O(d)
d/2+O (1) logn + d4‚àö
n logn
Seidel [26] rand. d!n
Kalai [19]/Matou≈°ek, Sharir, and Welzl [23] rand. min{d22dn, e2
‚àö
d ln(n/
‚àö
d)+O (
‚àö
d+log n)
}
combination of [10] and [19, 23] rand. d2
n + 2O (
‚àöd log d)
Hansen and Zwick [18] rand. 2O (
‚àöd log((n‚àíd)/d))n
Agarwal, Sharir, and Toledo [4] det. O(d)
10d (logd)
2dn
Chazelle and Matou≈°ek [8] det. O(d)
7d (logd)
dn
Br√∂nnimann, Chazelle, and Matou≈°ek [5] det. O(d)
5d (logd)
dn
This article det. O(d)
d/2 (logd)
3dn
As one can see, our improvement is a modest one and does not alter the form dO (d)
n of the
best-known deterministic time bounds; also, our new algorithm does not beat existing randomized algorithms. However, we believe the result is still interesting for several reasons. First, linear
programming is among the most fundamental algorithmic problems of all time, and no progress
on deterministic real-RAM algorithms has been reported for years. Second, we obtain a dependency of d(1/2+o(1))d on the dimension, reaching a natural limit for the kinds of approach considered here and in Chazelle and Matou≈°ek‚Äôs prior work [8]. These are based on derandomization of Clarkson‚Äôs sampling algorithms [10], which reduce the problem to subproblems with no
fewer than n ‚âà d2 constraints, for which the currently best deterministic real-RAM upper bound
is about (n/d)
d/2 = dd/2, as guaranteed by the simplex method. (The best randomized algorithms
by Kalai [19] and Matou≈°ek, Sharir, and Welzl [23] are faster, but no derandomization of results of
that kind is known.) Note that our new d(1/2+o(1))dn deterministic result even beats the well-loved
randomized O(d!n)-time algorithm by Seidel [26]; and it is superior to all existing deterministic
bounds on the real RAM for n  d2.
Our technical ideas are also noteworthy in several ways:
(1) We observe that derandomizing Clarkson‚Äôs recursive sampling algorithm [10] can lead
to a simple deterministic linear-time algorithm for constant dimensions. Chazelle and
Matou≈°ek [8] previously derandomized Clarkson‚Äôs algorithm but needed a combination of
several ideas (including the method of conditional probabilities and a nontrivial merge-andreduce algorithm for computing Œµ-approximations). In contrast, our simplest derandomization can be described in under two pages, as we explain in Section 2, and is completely
self-contained except for the use of the standard greedy algorithm for set cover or hitting
set. The resulting algorithm is actually simpler than Megiddo‚Äôs original deterministic algorithm [24] and all its subsequent refinements [4, 14]. This is perhaps the main takeaway
of the article. (Megiddo‚Äôs algorithm grouped the input into pairs and invoked linear-time
median finding repeatedly; our algorithm uses groups of a larger size but does not need
median finding at all. Megiddo‚Äôs algorithm also required more complicated primitive operations; our algorithm only requires standard orientation tests on d + 1 input hyperplanes.)
Incidentally, even without attempting to optimize the dependency on d, our algorithm in
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.
Improved Deterministic Algorithms for Linear Programming in Low Dimensions 30:3
its simplest version has a running time at most d(3+o(1))dn, already improving all prior
published deterministic bounds.
(2) Clarkson‚Äôs article [10] described a second sampling algorithm, based on iterative reweighting (or in more trendy terms, the multiplicative weights update method). This second algorithm has expected n logn running time in terms of n but has better dependency on d.
We point out a simple variant of Clarkson‚Äôs second algorithm that has running time linear
in n; this variant appears new. If randomization is allowed, then this is not too interesting, since Clarkson [10] eventually combined his two algorithms to obtain his best results
with running time linear in n. However, the variant makes a difference for deterministic
algorithms, as we will see in Section 3.
(3) The heart of the matter in obtaining the best derandomization of Clarkson‚Äôs algorithms
lies in the construction of Œµ-nets. This was the focus of the previous article by Chazelle
and Matou≈°ek [8]. In Section 4, we present an improved Œµ-net algorithm (for halfspace
ranges) that has running time about O(1/Œµ)
d/2 when n is polynomial in d and that may
be of independent interest. This is the most technical part of the article (here we need to
go back to previous derandomization concepts, namely, (sensitive)Œµ-approximations). Still,
the new idea can be described compactly with an interesting recursion.
2 CLARKSON‚ÄôS FIRST ALGORITHM
In linear programming, we want to minimize a linear function over an intersection of n halfspaces.
Without loss of generality, we assume that the halfspaces are in general position (by standard perturbation techniques) and all contain (0,..., 0, ‚àí‚àû) (by adding an extra dimension if necessary).
The problem can then be restated as follows:
Given a set H of n hyperplanes in Rd , find a point p that lies on or below1 all
hyperplanes in H, while minimizing a given linear objective function.
All our algorithms rely on the concept of Œµ-nets, which in this context can be defined as follows:
Definition 2.1. Given p ‚àà Rd , let Violatep (H) = {h ‚àà H : h is strictly below p}. A subset R ‚äÜ H
is an Œµ-net of H if for every p ‚àà Rd ,
Violatep (R) = ‚àÖ‚áí|Violatep (H)| ‚â§ Œµ|H|.
Fact 1.
(a) (Mergability) If Ri is an Œµ-net of Hi for each i and the Hi ‚Äôs are disjoint, then
i Ri is an Œµ-net
of
i Hi .
(b) Given a set H of n ‚â• d hyperplanes in Rd , we can construct an Œµ-net of H of size O( d
Œµ logn)
in O(n/d)
d+O (1) deterministic time.
Proof. (a) is clear from the definition. For (b), we want a subset R ‚äÜ H that hits the set
Violatep (H) for every p ‚àà Rd of level > Œµn, where the level of a point p is defined to be
|Violatep (H)|. It suffices to consider just the vertices of the arrangement of H (since for every
p ‚àà Rd , there is an equivalent vertex having the same set Violatep (H)). The number of vertices in
the arrangement is m = O(( n
d )) = O(n/d)
d . We can enumerate them trivially in dO (1)
m time. Afterwards, we can run the standard greedy algorithm [12] to compute a hitting set for m given sets
in a universe of size n in O(mn) time. In our case, when all given sets have size > Œµn, the greedy
algorithm produces a hitting set of size O( 1
Œµ logm) = O( d
Œµ logn).
1Throughout the article, ‚Äúbelow‚Äù and ‚Äúabove‚Äù refer to the dth coordinate value.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.   
30:4 T. M. Chan
Let Œ¥ > 0 be a sufficiently small constant. We begin with a version of Clarkson‚Äôs first algorithm [10] for linear programming, described in the pseudocode below. (The original version used
a random sample R of size near d
‚àö
n, which behaves like a (1/
‚àö
n)-net; our version instead uses a
Œò(1/d2)-net R, following Chazelle and Matou≈°ek [8].)
LP(H):
0. if |H| = O(d3 logc d) then return answer directly
1. compute a (Œ¥/d2)-net R of H
2. for j = 1, 2,... {
3. p = LP(R ‚à™ X1 ‚à™¬∑¬∑¬∑‚à™ Xj‚àí1)
4. Xj = Violatep (H)
5. if Xj = ‚àÖ then return p
}
Let B‚àó denote the set of d hyperplanes defining the optimal solution. Observe that in each iteration of the for loop before the last, Xj must include a new member of B‚àó, because otherwise,
p would not be violated by any member of B‚àó and would thus have a worse objective value than
LP(B‚àó) = LP(H), a contradiction. It follows that the loop has at most d + 1 iterations.
Much of the effort in Chazelle and Matou≈°ek‚Äôs derandomization [8] is devoted to the construction of the net R in line 1. To this end, they generalized the problem to the construction of Œµapproximations and described a clever linear-time algorithm [8, 22] that used repeated merge and
reduce steps, with a base case that involved the method of conditional probabilities (another alternative is the bounded independence technique [17]). Our new idea for line 1 bypasses all this and
is a lot more straightforward‚Äîwe just apply a standard grouping trick, based on Fact 1(a), and
construct a net for each group naively, as described in lines 1.1‚Äì1.3 below. The key observation
is that for Clarkson‚Äôs algorithm, we do not need a net of optimal size‚Äîslightly sublinear size is
already good enough.
1.1. arbitrarily divide H into groups H1,...,H|H |/b  of size at most b
1.2. for i = 1,..., |H|/b, compute a (Œ¥/d2)-net Ri of Hi
1.3. R =
i Ri
Assume that there is an algorithm to construct an Œµ-net for n hyperplanes of size O( d
Œµ logc n)
in Tnet(n,Œµ) deterministic time for some constant c. Let n = |H|. Then line 1.2 takes O(n/b ¬∑
Tnet(b, Œ¥/d2)) time. The set R in line 1.3 has size O(n/b ¬∑ d3 logc b), which can be made at most
n/(2d) + O(d3 logc d) by choosing a group size b = Œò(d4 logc d). Furthermore, each Xj has size
at most Œ¥n/d2, so R ‚à™ X1 ‚à™¬∑¬∑¬∑‚à™ Xj‚àí1 has size at most n/(2d) + O(d3 logc d) + Œ¥n/d. Line 0 takes
O(d2 logc d)
d/2 time by the simplex method. Line 4 takes O(dn) time. Thus, the running time of
LP(H) satisfies the recurrence
TLP(n) =
‚éß‚é™‚é™‚é™
‚é®
‚é™‚é™‚é™
‚é©
O(d2 logc d)
d/2 if n = O(d3 logc d)
(d + 1)TLP
( 1
2 +Œ¥ ) n
d + O(d3 logc d)

+ O(Tnet(O(d4 logc d), Œ¥/d2) n)
+ O(d2
n) else,
implying
TLP(n) = O(Tnet(O(d4 logc d), Œ¥/d2) n) + O(d2 logc d)
d/2
n.
By Fact 1(b),Tnet(O(d4 logc d), Œ¥/d2) = O(d3 logc d)
d with c = 1. We have therefore obtained a simple deterministic algorithm for linear programming with running time O(d3 logd)
dn.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.  
Improved Deterministic Algorithms for Linear Programming in Low Dimensions 30:5
3 CLARKSON‚ÄôS SECOND ALGORITHM
Clarkson [10] gave a second randomized algorithm for linear programming, based on iterative
reweighting. Originally the algorithm required O(d logn) iterations and led to a running time
that is slightly superlinear in n. We describe a new variant that requires O(d logd) iterations and
maintains linear dependency on n. The main advantage of the second algorithm is that it works
with Œò(1/d)-nets instead of Œò(1/d2)-nets.
The algorithm is described in the pseudocode below, where initially all elements of H have
multiplicity 1. (The main novelty is that when the multiplicity of each element reaches a certain
limit, we move the element to a setX. Another change is that instead of a purely iterative algorithm,
our variant will continue to use recursion.)
LP(H):
0. if |H| = O(d2 logc d) then return answer directly
1. repeat {
2. compute a (Œ¥/d)-net R of H
3. X = {h ‚àà H : h has multiplicity ‚â• d2 in H} (with multiplicities reset to 1 in X)
4. p = LP(R ‚à™ X)
5. for each h ‚àà Violatep (H) do
6. double h‚Äôs multiplicity in H
7. if Violatep (H) = ‚àÖ then return p
}
Let B‚àó denote the set of d hyperplanes defining the optimal solution. Observe that in each iteration of the repeat loop before the last, at least one member of B‚àó has its multiplicity doubled,
because otherwise, p would not be violated by any member of B‚àó, and would thus have a worse
objective value than LP(B‚àó) = LP(H). The multiplicity of an element stays unchanged in H when
it reaches d2 or above. Thus, the loop has at most d log2 (d2) + 1 = O(d logd) iterations.
Let |H| denote the size of H, counting multiplicities. Let n denote the initial size of H. In each
iteration, |H| increases by at most Œ¥ |H|/d. Thus, at the end of the loop, |H| ‚â§ (1 + Œ¥/d)
O (d log d)
n ‚â§
dO (Œ¥ )
n. On the other hand, |X |‚â§|H|/d2 ‚â§ n/d2‚àíO (Œ¥ )
.
As before, we replace line 2 with:
2.1. arbitrarily divide H into groups H1,...,H|H |/b  of size at most b
2.2. for i = 1,..., |H|/b, compute a (Œ¥/d)-net Ri of Hi
2.3. R =
i Ri
Line 2.2 takes O(dO (Œ¥ )
n/b ¬∑ Tnet(b, Œ¥/d)) time. The set R in line 2.3 has size O(dO (Œ¥ )
n/b ¬∑
(1/Œ¥ )d2 logc b), which can be made at most n/d1+Œ©(Œ¥ ) + O(d2 logc d) by choosing a group size b =
d3+Œò(Œ¥ )
. Line 0 takes O(d logc d)
d/2 time by the simplex method. Lines 5‚Äì6 take O(dn) time. Thus,
the running time satisfies the recurrence
TLP(n) =
‚éß‚é™‚é™‚é™
‚é®
‚é™‚é™‚é™
‚é©
O(d logc d)
d/2 if n = O(d2 logc d)
O(d logd)TLP  n
d1+Œ©(Œ¥ ) + O(d2 logc d)

+ O(Tnet(d3+O (Œ¥ )
, Œ¥/d) n)
+ O((d2 logd) n) else,
implying
TLP(n) = O(Tnet(d3+O (Œ¥ )
, Œ¥/d) n) + O(d logc d)
d/2
n. (1)
By Fact 1(b), Tnet(d3+O (Œ¥ )
, Œ¥/d) ‚â§ d(2+O (Œ¥ ))d with c = 1. We have therefore obtained an improved
simple deterministic algorithm for linear programming with running time O(d)(2+O (Œ¥ ))dn for any
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.  
30:6 T. M. Chan
constant Œ¥ > 0. (The small O(Œ¥ )d term in the exponent can probably be improved by using a
weighted version of Œµ-nets, but this will not matter at the end.)
4 NEW Œµ-NET CONSTRUCTION
In this section, we provide better bounds for Tnet(n,Œµ). Two ideas come to mind:
(1) we can improve Fact 1(b) by enumerating only vertices with level capped at Œµn + 1 rather
than all vertices in the entire arrangement;
(2) we can compute Œµ-nets faster by following Br√∂nnimann, Chazelle, and Matou≈°ek‚Äôs approach of using sensitive approximations [5].
Either idea can lead to an improvement, but for our best improvement, we need a combination of
both, which creates new technical challenges. Br√∂nnimann, Chazelle, and Matou≈°ek‚Äôs algorithm
for sensitive Œµ-approximations [5], based on earlier algorithms by Matou≈°ek [22] and Chazelle and
Matou≈°ek [8] for Œµ-approximations, involves repeatedly dividing into groups, and merging and
reducing approximations of groups; however, arbitrarily dividing into groups does not preserve
caps on levels. We suggest a new solution that adds another recursive call for the division step.
(The dependency on n is no longer linear, unlike the previous algorithms [5, 8, 22], but will not
matter at the end.)
We need to review technical facts concerning the concept of sensitive Œµ-approximations; the
construction in Fact 2(c) below requires the method of conditional probabilities [5].
Definition 4.1. Let œÅp (H) = |Violatep (H)|/|H|. A subset R ‚äÜ H is a sensitive Œµ-approximation of
H w.r.t. P if for every p ‚àà P,
|œÅp (R) ‚àí œÅp (H)| ‚â§ (Œµ/2)

œÅp (H) + Œµ2
/2.
Fact 2 (Br√∂nnimann, Chazelle, and Matou≈°ek [5]).
(a) (Mergability) If Ri is a sensitive Œµ-approximation of Hi w.r.t. P and the Hi ‚Äôs are disjoint and
have equal size, then
i Ri is a sensitive Œµ-approximation of
i Hi w.r.t. P.
(b) (Reducibility) If R is a sensitive Œµ-approximation of H w.r.t. P and R is a sensitive Œµ
-
approximation of R w.r.t. P, then R is a sensitive (Œµ + 2Œµ
)-approximation of H w.r.t. P.
(c) Given a set H of n hyperplanes in Rd with n ‚â• r ‚â• c0
Œµ 2 log |P | for some constant c0, we can
construct a sensitive Œµ-approximation of H w.r.t. P of size r in O(|P |n) deterministic time.
For our purposes of constructing Œµ-nets, it suffices to construct sensitive approximation w.r.t.
points with level bounded by an appropriate cap.
Fact 3. Define Cap(H, Œ±) = {p ‚àà Rd : œÅp (H) ‚â§ Œ±}.
(d) If R is a sensitive ‚àö
Œµ-approximation of H w.r.t. Cap(H,Œµ + 1
|H |
), then R is an Œµ-net of H.
(e) If R is a sensitive cŒµ-approximation of H w.r.t. Cap(H, (tŒµ)
2), then Cap(H, (tŒµ)
2) ‚äÜ
Cap(R, ((t + c)Œµ)
2).
(f) Given a set H of n hyperplanes in Rd with n ‚â• r ‚â• c0d
Œµ 2 logn for some constantc0, and given
a parameter t ‚â• 1, we can construct a sensitive Œµ-approximation of H w.r.t. Cap(H, (tŒµ)
2) of
size r in O(tŒµn/d)
d+O (1) deterministic time.
Proof. For (d), observe that for every p with level Œµ|H| + 1 (which is in Cap(H,Œµ + 1
|H |
)), we
have œÅp (H) > Œµ, implying that œÅp (R) ‚â• œÅp (H) ‚àí (
‚àö
Œµ/2)

œÅp (H) ‚àí Œµ/2 > 0. This suffices to confirm
that R is an Œµ-net.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.  
Improved Deterministic Algorithms for Linear Programming in Low Dimensions 30:7
For (e), observe that if œÅp (H) ‚â§ (tŒµ)
2, then
œÅp (R) ‚â§ œÅp (H) + (cŒµ/2)

œÅp (H) + (cŒµ)
2
/2 ‚â§ (t
2 + (c/2)t + (c2
/2))Œµ2 < ((t + c)Œµ)
2
.
For (f), it suffices to run the algorithm in Fact 2(c) w.r.t. just the vertices of level at most k in
the arrangement of H, with k = (tŒµ)
2
n. Clarkson and Shor [11] (and also Wagner [27]) proved
that the number m of vertices of level at most k is m ‚â§ ( n
d/2 ) ¬∑ O(k/d + 1)d/2 = O(n/d)d/2 ¬∑
O(t 2
Œµ2
n/d + 1)d/2 = O(tŒµn/d)
d+O (1)
. We can enumerate all vertices of level at most k by an
output-sensitive algorithm that uses repeated ray shooting [1, 2] in O(dO (1)
mn) time (this can
viewed as a generalization of the standard gift-wrapping algorithm for convex hulls [6] in dual
space; the time bound gets better in small constant dimensions by using ray shooting data structures [1, 2], but this will not matter). Afterwards, we can run the algorithm in Fact 2(c) in O(mn)
time.
Lemma 4.2. Given a set H of n hyperplanes in Rd with n a power of 2 and n ‚â• c1d
Œµ 2 log d
Œµ for some
constant c1, and given parameters Œµ > 0 and t ‚â• 1, we can compute a sensitive Œµ-approximation of
size exactly n/2 w.r.t. Cap(H, (tŒµ)
2) in deterministic time
Thalver(n,Œµ,t) = O t+log n
Œµ log d
Œµ
d
n1.59.
Proof. We describe our algorithm by the following deceptively concise pseudocode using
recursion:
Halver(H,Œµ,t):
0. if |H| = O( d
Œµ 2 log d
Œµ ) or Œµ ‚â• 2 then return answer directly
1. A = Halver(H, 2Œµ, t/2)
2. return Halver(A,Œµ,t + 2) ‚à™ Halver(H ‚àí A,Œµ,t + 2)
Since A is a sensitive 2Œµ-approximation of H w.r.t. Cap(H, (tŒµ)
2), from the definitions it follows that H ‚àí A is also a sensitive 2Œµ-approximation (since |H ‚àí A| = |A| = |H|/2 implies that
œÅp (H ‚àí A) ‚àí œÅp (H) = œÅp (H) ‚àí œÅp (A)). We thus have Cap(H, (tŒµ)
2) ‚äÜ Cap(A, ((t + 2)Œµ)
2), Cap(H ‚àí
A, ((t + 2)Œµ)
2) by Fact 3(e). By induction hypothesis and Fact 2(a), the union in line 2 is a sensitive
Œµ-approximation of H w.r.t. Cap(H, (tŒµ)
2). (To recap, the desired sensitive Œµ-approximation is computed by the recursive calls in line 2; the only purpose of the recursive call in line 1 is in providing
a good cap for line 2.)
By Fact 3(f), line 0 takesO( t
Œµ log d
Œµ )
d time. Assuming that Halver returns linked lists for both the
sensitive approximation and its complement, we see that cost of the algorithm excluding the three
recursive calls in lines 1‚Äì2 is O(1) by merging linked lists. The running time of Halver(H,Œµ,t)
satisfies the recurrence
Thalver(n,Œµ,t) = ‚éß‚é™
‚é®
‚é™
‚é©
O( t
Œµ log d
Œµ )
d if n = O( d
Œµ 2 log d
Œµ )
Thalver(n, 2Œµ, t/2) + 2Thalver(n/2,Œµ,t + 2) + O(1) else.
Observe that the depth of the recursion is at most log2 n + log2 (1/Œµ), since n is either halved or
Œµ is doubled in each recursive call. The number of nodes in the recursion tree is thus at most
3log2 n+log2 (1/Œµ ) = O(n/Œµ)
1.59. The parameter t increases by at most O(logn) and the parameter Œµ
does not decrease. It follows that Thalver(n,Œµ,t) ‚â§ O(
t+log n
Œµ log d
Œµ )
dn1.59. (The n1.59 factor is improvable but will not matter at the end.)
Theorem 4.3. Given a setH of n hyperplanes in Rd with n a power of 2, and given parametersŒµ > 0
and t ‚â• 1, for some constantc1, we can compute a sensitive Œµ logn-approximation of size  c1d
Œµ 2 log d
Œµ 
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018.   
30:8 T. M. Chan
w.r.t. Cap(H, (tŒµ)
2) in deterministic time
T
approx (n,Œµ,t) = O(
t+log n
Œµ log d
Œµ )
dn1.59.
Proof. We describe another recursive algorithm:
Approx(H,Œµ,t):
0. if |H| = O( d
Œµ 2 log d
Œµ ) then return answer directly
1. A = Halver(H,Œµ,t)
2. R = Approx(A,Œµ,t + 1) ‚à™ Approx(H ‚àí A,Œµ,t + 1)
3. return Halver(R,Œµ/2,t + logn)
Since both A and H ‚àí A are sensitive Œµ-approximations of H w.r.t. Cap(H, (tŒµ)
2), we have
Cap(H, (tŒµ)
2) ‚äÜ Cap(A, ((t + 1)Œµ)
2), Cap(H ‚àí A, ((t + 1)Œµ)
2) by Fact 3(e). By induction hypothesis
and Fact 2(a), the union R in line 2 is a sensitive Œµ log(n/2)-approximation of H w.r.t. Cap(H, (tŒµ)
2).
We have Cap(H, (tŒµ)
2) ‚äÜ Cap(R, ((t + logn)Œµ)
2) by Fact 3(e). By Fact 2(b), the set in line 3 is a
sensitive (Œµ log(n/2) + 2(Œµ/2) = Œµ logn)-approximation of H w.r.t. Cap(H, (tŒµ)
2).
By Fact 3(f), line 0 takesO( t
Œµ log d
Œµ )
d time. By Lemma 4.2, lines 1 and 3 takeO(
t+log n
Œµ log d
Œµ )
dn1.59
time. The running time of Approx(H,Œµ,t) satisfies the recurrence
T
approx (n,Œµ,t) = ‚éß‚é™
‚é®
‚é™
‚é©
O( t
Œµ log d
Œµ )
d if n = O( d
Œµ 2 log d
Œµ )
2T
approx (n/2,Œµ,t + 1) + O(
t+log n
Œµ log d
Œµ )
dn1.59 else.
It follows that T
approx (n,Œµ,t) = O(
t+log n
Œµ log d
Œµ )
dn1.59.
Corollary 4.4. Given a set H of n hyperplanes in Rd with n a power of 2, and parameters Œµ > 0
and t ‚â• 1, we can compute a sensitive Œµ-approximation of size O( d
Œµ 2 log2 n log d
Œµ ) w.r.t. Cap(H, (tŒµ)
2)
in deterministic time
Tapprox (n,Œµ,t) ‚â§ T
approx (n,Œµ/ logn,t logn) = O( t
Œµ log2 n log d
Œµ )
dn1.59.
By Fact 3(d), we conclude:
Corollary 4.5. Given a set H of n hyperplanes in Rd with n a power of 2, and parameter Œµ > 0,
we can compute an Œµ-net of size O( d
Œµ log2 n log d
Œµ ) in deterministic time
Tnet(n,Œµ) ‚â§ Tapprox (n,
‚àö
Œµ,

1 + 1
Œµn ) = O( 1
Œµ )
d/2 (log2 n log d
Œµ )
dn1.59.
Note the extra logarithmic factors on the net size in the above corollaries (which are improvable
if we increase the running time, but will not matter at the end).
Combining with the approach in Section 3 and applying Corollary 4.5 to (1) with c = 3,
we have finally obtained a deterministic algorithm for linear programming with running time
O(d)
d/2 (logd)
3dn.
5 REMARKS
The same results hold for the problem of computing the smallest enclosing ball of n points in Rd ;
this problem can be transformed to minimizing a convex function in an intersection of n halfspaces
in one dimension higher.
Like Chazelle and Matou≈°ek‚Äôs derandomization [8] of Clarkson‚Äôs algorithm, our algorithms in
Sections 2 and 3 can more generally solve any LP-type problem [23] of combinatorial dimension d,
assuming just the availability of a subsystem oracle (see Reference [8] for the formal definitions).
The improvement in Section 4 does not completely generalize, because the same combinatorial
bound on ‚Äú(‚â§ k)-levels‚Äù does not necessarily hold. However, sensitive approximations can still be
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 30. Publication date: June 2018. 
Improved Deterministic Algorithms for Linear Programming in Low Dimensions 30:9
used and we can still obtain a d(1+o(1))dn deterministic time bound as follows: First, in Fact 3(f),
the time bound without caps is O(n/d)
d+O (1)
. In Theorem 4.3, we can apply a more naive divideand-conquer where in line 1 we simply choose an arbitrary subset A of size |H|/2. The recurrence
becomes
T
approx (n,Œµ) = ‚éß‚é™
‚é®
‚é™
‚é©
O( 1
Œµ 2 log d
Œµ )
d if n = O( d
Œµ 2 log d
Œµ )
2T
approx (n/2,Œµ) + O( 1
Œµ 2 log d
Œµ )
d else,
yielding T
approx (n,Œµ) = O( 1
Œµ 2 log d
Œµ )
dn. In Corollaries 4.4 and 4.5, we then obtain running times of
Tapprox (n,Œµ) = O( 1
Œµ 2 logn log d
Œµ )
dn and Tnet(n,Œµ) = O( 1
Œµ logn log d
Œµ )
dn, implying a final time bound
of TLP(n) = O(d log2 d)
dn for LP-type problems.