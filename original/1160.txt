Abstract
The clustering problem, in its many variants, has numerous applications in operations research and computer science (e.g., in applications in bioinformatics, image processing, social network analysis, etc.). As sizes of data sets have grown rapidly, researchers have focused on designing algorithms for clustering problems in models of computation suited for large-scale computation such as MapReduce, Pregel, and streaming models. The k-machine model (Klauck et al., (SODA 2015) [8]) is a simple, message-passing model for large-scale distributed graph processing. This paper considers three of the most prominent examples of clustering problems: the uncapacitated facility location problem, the p-median problem, and the p-center problem and presents O(1)-factor approximation algorithms for these problems running in O˜(n/k) rounds in the k-machine model. These algorithms are optimal up to polylogarithmic factors because this paper also shows Ω˜(n/k) lower bounds for obtaining polynomial-factor approximation algorithms for these problems. These are the first results for clustering problems in the k-machine model.

We assume that the metric provided as input for these clustering problems is only implicitly provided, as an edge-weighted graph and in a nutshell, our main technical contribution is to show that constant-factor approximation algorithms for all three clustering problems can be obtained by learning only a small portion of the input metric.

Previous articleNext article
Keywords
Clustering
Facility location
k-median
k-center
k-machine model
Large-scale clustering
Distributed clustering

1. Introduction
The problem of clustering data has a wide variety of applications in areas such as information retrieval, bioinformatics, image processing, and social network analysis. In general, clustering is a key component of data mining and machine learning algorithms. Informally speaking, the objective of data clustering is to partition data into groups such that data within each group are “close” to each other according to some similarity measure. For example, we might want to partition visitors to an online retail store (e.g., Amazon) into groups of customers who have expressed preferences for similar products. As the sizes of data sets have grown significantly over the last few years, it has become imperative that clustering problems be solved efficiently in models of computation that allow multiple machines to process data in parallel. Distributing input data across multiple machines is important not just for speeding up computation through parallelism, but also because no single machine may have sufficiently large memory to hold a full data set. Motivated by these concerns, recent research has considered problems of designing clustering algorithms [2], [3] in systems such as MapReduce [4] and Pregel [5]. Clustering algorithms [6] have also been designed for streaming models of computation [7].

In this paper we present distributed algorithms for three of the most prominent clustering problems: the uncapacitated metric facility location problem, the p-median problem, and the p-center problem. All three problems have been studied for several decades now and are well-known to be NP-hard. On the positive side, all three problems have constant-factor (polynomial-time) approximation algorithms. We consider these problems in the recently proposed k-machine model [8], a synchronous, message-passing model for large-scale distributed computation. This model cleanly abstracts essential features of systems such as Pregel [5] and Giraph (see http://giraph.apache.org/) that have been designed for large-scale graph processing,1 allowing researchers to prove precise upper and lower bounds. One of the main features of the k-machine model is that the input, consisting of n items, is randomly partitioned across k machines. Of particular interest are settings in which n is much larger than k. Communication occurs via bandwidth-restricted communication links between every pair of machines and thus the underlying communication network is a size-k clique. For all three problems, we present constant-factor approximation algorithms that run in 
 rounds in the k-machine model. We also show that these algorithms have optimal round complexity, to within polylogarithmic factors, by providing complementary 
2 lower bounds for 
-factor approximation algorithms for any constant . These are the first results on clustering problems in the k-machine model.

1.1. Problem definitions
The input to the uncapacitated metric facility location problem (in short, FacLoc) is a set V of points, a metric 
 that assigns distances to point-pairs, and a facility opening cost 
 associated with each point . The problem is to find a subset  of points (to “open” as “facilities”) so as to minimize the objective function 
, where 
. For convenience, we use 
 instead of  to denote facility opening cost of point i. In other words, for each facility  that we open, we need to pay its facility opening cost 
 and the open facility f can “serve” any point , potentially reducing the quantity  for many . FacLoc is NP-hard and is in fact hard to approximate with an approximation factor better than 1.463 [10]. There are several well-known constant-factor approximation algorithms for FacLoc including the primal-dual algorithm of Jain and Vazirani [11] and the greedy algorithm of Mettu and Plaxton [12]. The current best approximation factor for FacLoc is 1.488 [13].

The input to the p-median problem (in short, pMedian) is a set V of points and a metric 
 that assigns distances to point-pairs, and a positive integer p. The problem is to find a subset  of exactly p points to open (as “facilities”) so as to minimize the objective function 
. pMedian is NP-hard and is in fact hard to approximate with an approximation factor better than 
 
 [14]. A well-known approximation algorithm for the pMedian problem is due to Jain and Vazirani [11], who present a 6-approximation algorithm. This approximation factor has been improved by subsequent results – see [15], for example. The input to the p-center problem (in short, pCenter) is the same as the input to pMedian, but the objective function that is minimized is 
. Like FacLoc and pMedian, the pCenter problem is not only NP-hard, it is in fact hard to approximate with an approximation factor strictly better than 2 [16]. There is also an optimal 2-approximation algorithm for this problem [16] obtained via a simple, greedy technique called farthest first traversal.

In all three problems, it is assumed that each point is “connected” to the nearest open facility. So an open facility along with the “clients” that are connected to it forms a cluster.

1.2. The k-machine model and input-output specification
Let n denote . The k-machine model is a message-passing, synchronous model of distributed computation. Time proceeds in rounds and in each round, each of the k machine performs local computation and then sends, possibly distinct, messages to the remaining  machines. A fundamental constraint of the k-machine model is that each message is required to be small; as is standard, we assume here that each message is of size  bits. It is assumed that the k machines have unique ID's, that are represented by -bit strings. We assume that all the distances and facility opening costs fit into a constant number of messages and therefore all distances and facility opening costs are bounded above by some polynomial in n.

As per the random partition assumption of k-machine model [8], the points in V are distributed uniformly at random across the k machines. This results in 
 points per machine, with high probability (w.h.p.).3 We use 
, , to denote the machines and 
 to denote the subset of points “hosted” by 
. The natural way to distribute the rest of the input, namely 
 and 
 (in the case of FacLoc), is for each machine 
 to be given 
 and 
 for each point 
. The distribution of f in this manner is fine, but there is a problem with distributing 
 in this manner. Since n is extremely large, it is infeasible for 
 to hold the 
 elements in 
. (Recall that .) In general, this explicit knowledge of the metric space consumes too much memory, even when divided among k machines, to be feasible. So we make, what we call the graph-metric assumption, that the metric 
 is specified implicitly by an edge-weighted graph with vertex set V. Let  be the edge-weighted graph with non-negative edge weights representing the metric 
. Thus for any ,  is the shortest path distance between points i and j in G.

Klauck et al. [8] consider a number of graph problems in the k-machine model and we follow their lead in determining the initial distribution of G across machines. For each point 
, machine 
 knows all the edges in G incident on i and for each such edge , machine 
 knows the ID of the machine that hosts x. Thus, 
 elements are needed at each machine 
 to represent the metric space and if G is a sparse graph, this representation can be quite compact.

The graph-metric assumption fundamentally affects the algorithms we design. Since the metric d is provided implicitly, via G, access to the metric is provided through shortest path computations on G. In fact, it turns out that these shortest path computations are the costliest part of our algorithms. One way to view our main technical contribution is this: we show that for all three clustering problems, there are constant-factor approximation algorithms that only require a small (i.e., polylogarithmic) number of calls to a subroutine that solves the Single Source Shortest Path (SSSP) problem.

For all three problems, the output consists of F, the set of open facilities, and connections between clients (i.e., points that have not been open as facilities) and their nearest open facilities. More precisely, for any machine 
 and any point 
:

•
If , then 
 knows that i has been opened as a facility and furthermore 
 knows all 
-pairs where x is a client that connects to i and 
 is the ID of the machine hosting x.

•
If , then 
 knows that i is a client and it also knows the 
-pair, where x is the open facility that i connects to and 
 is the ID of the machine hosting x.

1.3. Our results
We first prove 
 lower bounds (in Section 3) for FacLoc, pMedian, and pCenter. For each problem, we show that obtaining an α-approximation algorithm in the k-machine model, for any , requires at least 
 rounds. In the subsequent three sections, we present 
-round, constant-factor approximation algorithms for the FacLoc, pMedian, and pCenter problem, respectively. Our lower bound results show that our algorithms have optimal round complexity, at least up to polylogarithmic factors.

We bring to bear a wide variety of old and new techniques to derive our upper bound results including the facility location algorithm of Mettu and Plaxton [12], the fast version of this algorithm due to Thorup [17], the neighborhood-size estimation framework of Cohen [18], [19], the p-median Lagrangian relaxation algorithm of Jain and Vazirani [11] and the recent distributed shortest path algorithms due to Becker et al. [20]. In our view, an important contribution of this paper is to show how all of these techniques can be utilized in the k-machine model.

We note that in a recent paper [21], we use techniques from the current paper to obtain 
-round, constant-factor approximation algorithms in the k-machine model for clustering problems with outliers. Specifically, we present algorithms for Robust Facility Location and Facility Location with Penalties, two versions of the facility location problem with outliers proposed by Charikar et al. [22].

1.4. Related work
Following Klauck et al. [8], the papers [23], [24], [21], [25] have studied graph problems in the k-machine model. In [24], the authors present an 
-round algorithm for graph connectivity, which then serves as the basis for 
-round algorithms for other graph problems such as minimum spanning tree (MST) and approximate min-cut. The upper bound for MST does not contradict the 
 lower bounds shown for this problem in Klauck et al. [8] because Pandurangan et al. [24] use a more relaxed notion of how the output MST is represented. Specifically, at the end of the algorithm in [24] every MST edge is known to some machine, whereas Klauck et al. [8] use the stricter requirement that every MST edge be known to the machines hosting the two end points of the edge. This phenomena in which the round complexity of the problem is quite sensitive to the output representation may be relevant to our results as well and is further discussed in Section 7.

Earlier in this section, we have mentioned models and systems for large-scale parallel computation such as MapReduce and Pregel. Another model of large-scale parallel computation, that is related to the k-machine model is the Massively Parallel Computation model (MPC) introduced in [26], which according to [27] is the “most commonly used theoretical model of computation on synchronous large-scale data processing platforms such as MapReduce and Spark.” Some examples of very recent results on graph problems in the MPC model appear in [28], [29], [30].

2. Technical preliminaries
2.1. Information theory
We use information-theoretic techniques to obtain our lower bounds (Section 3) and here we present a brief summary of the definitions we need. Let μ be a probability distribution over a finite set Ω and let X be a random variable distributed according to μ. The entropy of X is defined as 
. Given a random variable Y, the conditional entropy of X given Y is 
 where  is the entropy of the conditional distribution of X given the event . The joint entropy of two random variables X and Y, denoted by , is the entropy of their joint distribution.

The mutual information between random variables X and Y is  and the conditional mutual information between X and Y given Z is . See the first two chapters of [31] for an excellent introduction to the basics of information theory.

Conditional mutual information has been used to obtain lower bounds on the running time of k-machine algorithms in [8] and we follow their approach to derive lower bounds for the clustering problems considered here. Reoughly speaking, the overall idea is as follows. Consider a machine m and let X denote machine m's view of the problem's (global) input at the start of the algorithm. The input is obtained by sampling from a probability distribution and therefore X is a random variable. Let A denote the portion of the global input given to machine m and therefore  is the entropy of m's view of the global input, conditioned on its initial local input. Let Y denote the transcript of messages m sees during the course of an algorithm. Then,  is the entropy of m's view of the global input, conditioned on its initial private input and all the messages it receives over the course of the algorithm. Our goal now is to show that  is large relative to , i.e., machine m has a lot of uncertainty about X initially, but needs to learn a lot about X in order to produce its portion of the problem output. Showing this is equivalent to showing a strong lower bound on the conditional mutual information . This in turn, implies a strong lower bound on the total volume of information that machine m has to receive during the algorithm. Dividing this by the bandwidth of machine m, which if , leads to a lower bound on the round complexity of the algorithm.

2.2. Shortest paths
Since the input metric is only implicitly provided, as an edge-weighted graph, computing shortest path distances to learn parts of the metric space turns out to be a key element of our algorithms. The Single Source Shortest Path (SSSP) problem has been considered in the k-machine model in Klauck et al. [8] and they describe a -approximation algorithm that runs in the k-machine model in 
 rounds. This is too slow for our purpose, since we are looking for an overall running time of 
. We instead turn to a recent result of Becker at al. [32] and using this we can easily obtain an 
-round SSSP algorithm. Becker et al. do not work in the k-machine model; their result relevant to us is in the Broadcast Congested Clique model. Informally speaking, the Congested Clique model can be thought of as a special case of the k-machine model with . The Broadcast Congested Clique model imposes the additional restriction on communication that in each round each machine sends the same message (i.e., broadcasts) to the remaining  machines. For a precise and detailed description of the Congested Clique models, see [33], [34].

Theorem 2.1

Becker et al. [32]
For any , in the Broadcast Congested Clique model, a deterministic -approximation to the SSSP problem in undirected graphs with non-negative edge-weights can be computed in  rounds.

Any Broadcast Congested Clique algorithm that runs in T rounds can be simulated in the k-machine model in 
 rounds. This is because each of the T Broadcast Congested Clique rounds can be simulated in 
 rounds in the k-machine model. In this simulation, each machine will compute and communicate on behalf of each of the vertices it hosts, sequentially. However, all k machines will do this in parallel. Thus, a single Broadcast Congested Clique round can be simulated in the k-machine model is as many rounds as the maximum number of vertices that a machine hosts. Since this is 
 w.h.p. the claim follows. A more general version of this claim is proved in Klauck et al. in the Conversion Theorem (Theorem 4.1 [8]). This leads to the following result about the SSSP problem in the k-machine model.

Corollary 2.2

For any , there is a deterministic -approximation algorithm in the k-machine model for solving the SSSP problem in undirected graphs with non-negative edge-weights in  rounds.4

In addition to SSSP, our clustering algorithms require an efficient solution to a more general problem that we call Multi-Source Shortest Paths (in short, MSSP). The input to MSSP is an edge-weighted graph , with non-negative edge-weights, and a set  of sources. The output is required to be, for each vertex v, the distance  (i.e., ) and the vertex 
⁎
 that realizes this distance. The following lemma uses ideas from Thorup [17] to show that MSSP can be reduced to a single call to SSSP and can be solved in an approximate sense in the k-machine model in 
 rounds.

Lemma 2.3

Given a set  of sources known to the machines (i.e., each machine 
 knows 
), we can, for any value , compute a -approximation to MSSP in 
 rounds, w.h.p. Specifically, after the algorithm has ended, for each , the machine 
 that hosts v knows a pair 
, such that 
.

Proof

First, as in [17], we add a dummy source vertex s, and connecting s to each vertex  by 0-weight edges. The shortest path distance from s to any other vertex , is same as  in the original graph. This dummy source can be hosted by an arbitrary machine and the edge information can be exchanged in 
 rounds

Using Corollary 2.2, we can compute approximate shortest path distance 
 that satisfies the property 
, in 
 rounds. Following Becker at al. [32] (Section 2.3) we can compute an approximate shortest path tree in addition to approximate distances in the Broadcast Congested Clique in  rounds w.h.p. and hence in the k-machine model in 
 rounds w.h.p.

Since a tree contains linear (in n) number of edges, all machines can exchange this information in 
 rounds so that every machine knows the computed approximate shortest path tree. Now, each machine 
 can determine locally, for each vertex 
 the vertex  which satisfies the properties stated in the lemma. □

Note that in the solution to MSSP, for each , . For our algorithms, we also need the solution to a variant of MSSP that we call ExclusiveMSSP in which for each , we are required to output  and the vertex 
⁎
 that realizes this distance. The following lemma uses ideas from Thorup [17] to show that ExclusiveMSSP can be solved by making  calls to a subroutine that solves SSSP.

Lemma 2.4

Given a set  of sources known to the machines (i.e., each machine 
 knows 
), we can, for any value , compute a -approximation to ExclusiveMSSP in 
 rounds, w.h.p. Specifically, after the algorithm has ended, for each , the machine 
 that hosts v knows a pair 
, such that 
.

Proof

Each vertex in T is assigned a  size bit vector. This can be done in one round where each machine broadcasts the number of vertices in T that it hosts. Then the machines assign a bit vector to the vertices they host based on the machine ID using a simple rule like vertices hosted by machines with lower ID are assigned lower bit vectors. We create  subsets of T by making two sets 
 and 
 for each bit position i. The set 
 contains vertices whose 
 bit value is b. Note that for all pairs of vertices , there is at least one set 
 such that 
 and 
. Now we run an MSSP algorithm for each 
 using Lemma 2.3. Now for each vertex , 
 is the smallest 
 such that 
 and the vertex u is an arbitrary vertex that realizes the distance 
. □

3. Lower bound results
In this section, we derive 
 lower bounds for achieving 
-factor approximation algorithms (for any constant c) in the k-machine model for all three problems considered in this paper. Our lower bounds are inspired by the  lower bound result from [8] for the Spanning Tree Computation problem.

To prove the lower bounds we describe a family of lower bound graphs 
 where X and Y are sampled from the same distribution as the one used in [8]. That is,  is chosen uniformly at random from 
, satisfying the constraint that for every , 
. Let  and let 
 for some large enough constant γ. The graph 
 has  vertices 
. See Fig. 1. We fix the ID's of the vertices to be the first n natural numbers which means that each machine knows whether a vertex v is 
 just by knowing . For every  there are three edges in the graph of the form 
 and the weights of these edges depend on the bit values of 
 and 
 where 
. In particular, we assign weights to 
 as follows – if 
 and 
, the weights are , if 
 and 
, the weights are , and if 
 and 
, the weights are . There is no weight assignment for the case when 
 because the distribution of  places no probability mass on this case.

Fig. 1
Download : Download high-res image (39KB)
Download : Download full-size image
Fig. 1. This figure shows the structure of the graph family Fb(X,Y) and how edge weights are assigned as a function of the length-b binary vectors X and Y.

In the following lemma we show that any protocol that reveals X and Y to a single machine must do so by making it receive large messages from other machines. The proof is the same as the entropy argument made in Theorem 2.1 in [8] with the added simplification that the entropy at the end of the protocol is zero. Nevertheless, we prove the lemma for completeness.

Lemma 3.1

Let Π be a public-coin ε-error randomized protocol in the k-machine model  on an n-vertex input graph sampled uniformly at random from 
. If a machine knows both X and Y at the end of the protocol Π then it must receive  bit messages in expectation from other machines.

Proof

Let m be the machine that knows both X and Y at the end of the protocol. Since X and Y are encoded in the edge weights of the graph, if the machine m hosts u then it knows the string X via the edges 
 and similarly it knows Y if it hosts w. But if m hosts both u and w then it knows X and Y before the protocol even begins. This is a bad event so we condition on the event that no machine hosts both u and w which happens with probability .

Before the first round of communication, it can be shown that the entropy . The machine m also hosts some vertices 
 and 
 giving it access to some bits of X and Y. If m hosts at most  
's and 
's for , it cannot know more than  bits of X and Y by virtue of hosting these vertices. The Chernoff bound gives us that the event where m hosts more than  vertices happens with probability at most 
, for b large enough. Therefore, this event cannot decrease the entropy by more than . Hence, the entropy of  given this initial information (which we denote by a random variable A) is . Note that if m hosts either u or w then A will contain information about either X or Y respectively but that does not affect our lower bound on the initial entropy.

Let 
 be the messages received by the machine m during the course of the protocol Π. With probability , m knows both X and Y at the end of the protocol and therefore 
. This means that 
. With the rest of the probability ε, we could have 
. Therefore 
. This calculation was done under the assumption that different machines host u and w, therefore, the expected number of messages received by m must be at least . □

Lemma 3.2

Let  be an arbitrary constant. For any 
, every public-coin ε-error randomized protocol in the k-machine model that computes an α-factor approximate solution of FacLoc on an n-vertex input graph has an expected round complexity 
.

Proof

To prove the lemma we consider the family of lower bound graphs 
 with the additional property that the vertices u and w have facility opening cost 0 and every other vertex has opening cost L.

Consider the solution  to FacLoc where we open the vertices u and w and connect all other vertices to the closest open facility. The cost of this solution is  whereas any other solution will incur a cost of at least . By our choice of 
 for a large enough constant γ, the solution  is optimal and any α-approximate solution is forced to have the same form as .

After the facility location algorithm terminates, with probability , the machine m hosting u will know the ID's of the 
's that u serves in . This allows u to figure out Y because 
 if u serves 
 and 
 otherwise. Therefore, machine m knows X (since it is hosting u) and Y at the end of the algorithm. By Lemma 3.1, machine m receives  bit messages in expectation throughout the course of the algorithm. This implies an 
 lower bound on the expected round complexity. □

Lemma 3.3

Let  be an arbitrary constant. For any 
, every public-coin ε error randomized protocol on a k-machine network that computes an α-factor approximate solution of

Image 1
and
Image 2
on an n-vertices input graph has an expected round complexity of 
.
Proof

We show the lower bound for  on graphs that come from the family 
. An optimal solution in a graph from this family is to open u and w which gives a solution of cost  for

Image 3
and  for
Image 4
. But, we need to be a bit more careful because the
Image 3
or
Image 4
algorithms can choose to open some of the 
's and 
's instead of u and w with only a constant factor increase in the cost of the solution. More specifically, there are four possible cases where we can open different pairs of vertices to get an -approximate solution – , 
, 
, and 
 where 
 and 
 are connected by an edge of weight 1 to u and w respectively. In all these cases, each open vertex knows both X and Y at the end of the algorithm by virtue of knowing the vertices that it serves in the final solution. This is because the value of L is high enough to ensure that the two clusters formed in any α-approximate solution are the same as the optimal solution no matter what centers are chosen. Therefore, we can apply Lemma 3.1 to all these cases which gives us that the machine hosting one of these vertices will receive  bit messages in expectation during the course of the algorithm. This means that the expected round complexity for both the algorithms is 
. □
4. Facility location in 
 rounds
At the heart of our k-machine algorithm for FacLoc is the well-known sequential algorithm of Mettu and Plaxton [12], that computes a 3-approximation for FacLoc. To describe the Mettu-Plaxton algorithm (henceforth, MP algorithm), we need some notation. For each real  and vertex v, define the “ball”  as the set . For each vertex , we define a radius 
 as the solution r to the equation 
. Fig. 2 illustrates the definition of 
 (note that 
 is well-defined for every vertex v).

Fig. 2
Download : Download high-res image (26KB)
Download : Download full-size image
Fig. 2. This illustration, which originally appeared in [37], shows B(v,rv), the radius-rv ball centered at v. If we imagine the ball B(v,r) growing with increasing r and we reach a stage at which r = rv, then the sum of the 5 distances, denoted by solid line segments from points within the ball to the ball-boundary equals fv.

The MP algorithm is the following simple, 2-phase, greedy algorithm:

We will work with a slight variant of the MP algorithm, called β-MP in [38]. The only difference between the MP algorithm and the β-MP algorithm is in the definition of each radius 
, which is defined for the β-MP algorithm, as the value r satisfying 
. (Thus, the β-MP algorithm with  is just the MP algorithm.)

There are two challenges to implementing the β-MP algorithm efficiently in the k-machine model (and more generally in a distributed or parallel setting): (i) The calculation of the radius 
 by the machine hosting vertex v requires that the machine know distances 
; however the distance metric is initially unknown and is too costly to fully calculate, and (ii) the Greedy Phase seems inherently sequential because it considers vertices one-by-one in non-decreasing order of radii; implementing this algorithm as-is would be too slow. In the next three sections, we describe how to overcome these challenges and we end the section with a complete description of our FacLoc algorithm in the k-machine model.

4.1. Reducing radius computation to neighborhood-size computation
To deal with the challenge of computing radii efficiently, without full knowledge of the metric, we use Thorup's approach [17]. Thorup works in the sequential setting, but like us, he assumes that the distance metric is implicitly specified via an edge-weighted graph. He shows that it is possible to implement the MP algorithm in 
 time on an m-edge graph. In other words, it is possible to implement the MP algorithm without computing the full distance metric (e.g., by solving the All Pairs Shortest Path (APSP) problem). We now show how to translate Thorup's ideas into the k-machine model. (We note here that Thorup's ideas for the FacLoc problem have already been used to design algorithms in “Pregel-like” distributed systems [3].)

For some , we start by discretizing the range of possible radii values using non-negative integer powers of .5 For any vertex v and for any integer , let 
 denote 
, the size of the neighborhood of v within distance 
. Further, let  denote the sum 
. Now note that if r increases from 
 to 
, then  increases by at least 
. This implies that 
 is a lower bound on 
. This observation suggests that we might be able to use, as an approximation to 
, the smallest value 
 for which this lower bound on 
 exceeds 
. Denote by 
, this approximation of 
. In other words, 
, where  is the smallest integer such that 
. We now show that 
 is a good approximation to 
 in the following lemma.

Lemma 4.1

For all , 
 
.

Proof

The values 
 and 
 respectively depend on how 
 and 
 relate to 
.

Recall that 
. Following calculations show that 
 can be interpreted as 
 where 
 is  rounded up to nearest power of .
 
 
 
 
 

Therefore, we can say that,
 

Which implies,
 

Note that by definition of 
, if 
 then 
 and 
. Thus, there has to exist a value 
 such that 
 and this is the r-value computed by the MP algorithm. Since 
, the Lemma follows. □

From the definition of 
 one can see that in order to compute these values, we only require knowledge of 
 for all , rather than actual distances  for all . We now state the high-level k-machine model algorithm (Algorithm 2) for computing 
 values.
In Algorithm 2, Step 2 is just local computation, so we focus on Step 1 which requires the solution to the problem of computing neighborhood sizes. More specifically, we define the problem NbdSizeComputation as follows: given an edge-weighted graph, with non-negative edge weights, compute the size of  for each vertex v and positive real d. The output to the problem in the k-machine model is required to be a distributed data structure (distributed among the k machines) such that each machine 
 can answer any query “What is ?” for any 
 and any positive real d, using local computation. Note that a “trivial” way of solving NbdSizeComputation is to solve APSP, but as mentioned earlier this is too costly. In the next subsection we show how to solve a “relaxed” version of this problem in the k-machine model in 
 rounds, making only  calls to a k-machine SSSP algorithm.

4.2. Neighborhood-size estimation in the k-machine model
To solve NbdSizeComputation efficiently in the k-machine model, we turn to an elegant idea due to Cohen [18], [19]. Motivated by certain counting problems, Cohen [18] presents a “size-estimation framework,” a general randomized method in the sequential setting. Cohen's algorithm starts by assigning to each vertex v a rank  chosen uniformly from . These ranks induce a random permutation of the vertices. To compute the size estimate of a neighborhood, say , for a vertex v and real , Cohen's algorithm finds the smallest rank of a vertex in . It is then shown (in Section 6, [18]) that the expected value of the smallest rank in  is . Thus, in expectation, the reciprocal of the smallest rank in  is (almost) identical to . To obtain a good estimate of  with high probability, Cohen simply repeats the above-described procedure independently a bunch of times and shows the following concentration result (via a standard application of Chernoff bounds) on the average estimator.

Theorem 4.2

Implicit in Cohen [18]
Let v be a vertex and  a real. For , let 
 denote the smallest rank of a vertex in  obtained in the i-th repetition of Cohen's neighborhood-size estimation procedure. Let 
 be the average of 
. Let . Then, for any ,

This theorem implies that 
 repetitions suffice for obtaining -factor estimates w.h.p. of the sizes of  for all v and all d.

Cohen proposes a modified Dijkstra's SSSP algorithm to find smallest rank vertices in each neighborhood. Let 
 be the vertices of the graph in non-decreasing order of rank. Initiate Dijkstra's algorithm, first with source 
, then with source 
, and so on. During the search with source 
, if it is detected that for a vertex u, 
 for some , then the current search can be “pruned” at u. This is because the vertex 
 has ruled out 
 from being the lowest ranked vertex in any of u's neighborhoods. In fact, this is true not just for u, but for all vertices whose shortest paths to 
 pass through u. Even though this algorithm performs n SSSP computations, the fact that each search is pruned by the results of previous searches makes the overall running time much less than n times the worst case running time of an SSSP computation. In particular, by making critical use of the fact that the random vertex ranks induce a random permutation of the vertices, Cohen is able to show that the algorithm runs in 
 time, on n-vertex, m-edge graphs, w.h.p.

We don't know how to implement Cohen's algorithm, as is, efficiently in the k-machine model. In particular, it is not clear how to take advantage of pruning that occurs in later searches while simultaneously taking advantage of the parallelism provided by the k machines. A naive implementation of Cohen's algorithm in the k-machine model is equivalent to n different SSSP computations, which is too expensive. Below, in Algorithm NbdSizeEstimates (Algorithm 3), we show that we can reduce Cohen's algorithm to a polylogarithmic number of SSSP computations provided we are willing to relax the requirement that we find the smallest rank in each neighborhood.

The goal of Algorithm 3 is to estimate  for all  and all . In Step 3, each vertex  picks a rank uniformly at random from , which is rounded down to the closest value 
 for some integer i (
 is suitably chosen in the algorithm). In Steps 5-7, in each iteration i, 
, we consider the set 
 of vertices that have rounded rank equal to 
 and solve an instance of the MSSP problem (see Lemma 2.3) using the vertices in 
 as sources. We repeat the algorithm 
 times for a suitably chosen constant c, so that the neighborhood size estimates satisfy the property provided in Theorem 4.2 with high probability.

Notice that the algorithm's behavior is not well-defined if a rank falls in the range 
 However, since ranks are chosen uniformly at random from , the probability that the rank of a vertex falls in this range is 
. By union bound, no rank falls in the interval 
 with probability at least . We condition the correctness proof of this algorithm on this high probability event.

Running time. There are  calls to the subroutine solving MSSP. By Corollary 2.2, each of these calls takes 
 rounds. Since 
, the overall round complexity of this algorithm in the k-machine model is 
.

Answering queries. At the end of each iteration, each machine 
 holds, for each vertex 
, the sequence of distances, 
. Over ℓ repetitions, machine 
 holds ℓ such sequences for each vertex 
. Note that each distance 
 is associated with the rounded rank 
. For any vertex  and real , let us denote the query “What is the size of ?” by . To answer query , we consider one of the ℓ sequences 
 and find the smallest i, such that 
, and return the rounded rank 
. To get an estimate that has low relative error, we repeat this over the ℓ sequences and compute the average 
 
 of the ranks computed in each iteration. The estimator is obtained by subtracting 1 from the reciprocal of 
 
.

The following lemma shows the correctness of Algorithm 3 in the sense that even though we might not get an approximately correct answer to , the size  is guaranteed to be “sandwiched” between the answers to two queries with nearby distances. This guarantee is sufficient to ensure that the RadiusComputation Algorithm produces approximately correct radii (see Section 4.3).

Lemma 4.3

Let s denote  for some vertex v and real . For any , w.h.p., Algorithm 3 satisfies the following properties:

•
for the query , the algorithm returns an answer that is at most .

•
for the query , the algorithm returns an answer that is at least .

Proof

Fix a particular repetition j, , of the algorithm and a ranking of the vertices. Let 
 denote the smallest rank in  in repetition j. To answer query , the algorithm examines the sequence of approximate distances 
, finds the smallest i such that 
, and uses 
 as an approximation for 
. Since 
 there is a vertex 
 such that 
. Since we compute a -approximate solution to MSSP, the actual distance . Thus the rank of u is at least 
 and therefore the rounded-rank of u is at least 
. Since 
, the rounded-rank of u is simply 
 and so we get that 
.

Over all ℓ repetitions, the algorithm computes the average 
 
 of the sequence 
. Letting 
 denote the average of 
 over all ℓ repetitions, we see that 
 
. From Theorem 4.2, we know that w.h.p. 
. Combining these two inequalities, we get
 
 
 
 
 
 
 
 
 The second last inequality above follows from the fact , since . The last inequality follows from the setting 
.

Now we consider query . Again, fix a repetition j, , of the algorithm and a ranking of the vertices. Let  be a vertex with rank equal to 
. We get two immediate implications: (i) the rounded-rank of u is at most 
 and (ii) 
. Together these imply that 
, the approximate rank computed by the algorithm in repetition j is at most 
. Averaging over all ℓ repetitions we get that 
 
. Using Theorem 4.2, we know that w.h.p. 
. Combining these two inequalities, we that get 
 
. This leads to
 
 
 
 
 
 The second last inequality follows from the fact that . A little bit of algebra shows that 
 implies that 
 and the last inequality follows from this. □

4.3. Radius computation revisited
Having designed a k-machine algorithm that returns approximate neighborhood-size estimates we restate the RadiusComputation algorithm (Algorithm 2) below.

We show below that even though the computed neighborhood-sizes are approximate, in the sense of Lemma 4.3, the radii that are computed by the RadiusComputation algorithm (Version 2) are a close approximation of the actual radii.

Lemma 4.4

For every , 
 
.

Proof

By Lemma 4.3, we have the following bounds on 
:
 

Similar bounds will apply for the terms 
. Adding the respective inequalities for these terms, yields the following inequality:
 
 
 

Now we obtain the following bound using similar arguments as in Lemma 4.1:
 

This means that there must exist a value 
 such that 
. The lemma follows since 
. □

4.4. Implementing the greedy phase
Referring to the two phases in the MP Algorithm (Algorithm 1), we have now completed the implementation of the Radius Computation Phase in the k-machine model. Turning to the Greedy Phase, we note that discretizing the radius values results in 
 distinct values. If we can efficiently process each batch of vertices with the same (rounded) radius in the k-machine model, that would yield an efficient k-machine implementation of the Greedy Phase as well. Consider the set W of vertices with (rounded) radius 
. Note that a set  is opened as facilities by the Greedy Phase iff I satisfies two properties: (i) for any two vertices , 
 and (ii) for any , 
. Thus the set I can be identified by computing a maximal independent set (MIS) in the graph 
, where 
 is the graph with vertex set V and edge set 
. (
 denotes the subgraph of 
 induced by W.)

Algorithm 1
Download : Download high-res image (30KB)
Download : Download full-size image
Algorithm 1. MP Algorithm.

Algorithm 2
Download : Download high-res image (50KB)
Download : Download full-size image
Algorithm 2. RadiusComputation Algorithm (Version 1).

The well-known distributed MIS algorithm of Luby [39] runs in  rounds w.h.p. and it can be easily implemented in the k-machine model in  rounds. However, Luby's algorithm assumes that the graph on which the MIS is being computed is provided explicitly. This is not possible here because explicitly providing the edges of a graph 
 would require pairwise-distance computation, which we're trying to avoid. Another problem with using Luby's algorithm is that it uses randomization, where the probabilities of certain events depend on vertex-degrees. The degree of a vertex v in 
 is exactly  and this is the quantity we would need to estimate. Unfortunately, the correctness guarantees for Algorithm 3 proved in Lemma 4.3 are not strong enough to give good estimates for . We deal with these challenges by instead using the beeping model MIS algorithm of Afek et al. [40], which is quite similar to Luby's algorithm except that it does require knowledge of vertex-degrees. In Luby's algorithm vertices “mark” themselves at random as candidates for joining the MIS. After this step, if a marked vertex v detects that a neighbor has also marked itself, then v “backs off.” In the current setting, this step would require every marked vertex v to detect if there is another marked vertex within distance d. We use ideas from Thorup [17] to show that this problem can be solved using  calls to a subroutine that solves ExclusiveMSSP (Lemma 2.4). In Luby's algorithm marked vertices that do not back off, join the MIS (permanently). Then, any vertex v that has a neighbor who has joined the MIS will withdraw from the algorithm. Determining the set of vertices that should withdraw in each iteration requires a call to an MSSP subroutine. Because the calls to the ExclusiveMSSP and MSSP subroutines return only approximate shortest path distances, what Algorithm 5 computes is a relaxation of an MIS, that we call -approximate MIS.

Algorithm 3
Download : Download high-res image (95KB)
Download : Download full-size image
Algorithm 3. NbdSizeEstimates(G,ε).

Definition 4.5

-approximate MIS
For an edge-weighted graph , and parameters , an -approximate MIS is a subset  such that

1.
For all distinct vertices , 
 
.

2.
For any , there exists a  such that .

The algorithm consists of  stages and in each Stage i, we run a Luby-like MIS algorithm for  iterations (for some constant ) with fixed marking probability which we double in each stage. In each iteration of the two for loops, the set 
 is the set of marked vertices in machine 
. The machines solve an ExclusiveMSSP instance in Step 5 with all marked vertices to ensure that marked vertices that are within approximated distance d of each other back-off. The marked vertices in machine 
 that do not back-off (i.e., vertices in 
) join the MIS (U). The machines then solve an instance of the MSSP problem in Step 8 to remove the vertices that within approximate distance d from the vertices in the MIS. We formalize the correctness of Algorithm 5 in the following Lemma.

Lemma 4.6

For a given set , Algorithm 5 finds an -approximate MIS I of  whp in 
 rounds.

Proof

We first bound the running time of the algorithm. The double nested loop runs for 
 iterations. In each iteration, Steps 5 and 8 run in 
 rounds via Lemma 2.4, Lemma 2.3 respectively and all other steps are local computations. This means that the overall running time is 
.

By the analysis in [40] and the guarantees provided by the solution to ExclusiveMSSP, no two vertices in W at distance at most  end up in T. Similarly, by the analysis in [40] and the guarantees provided by the solution to MSSP, every vertex in  is at distance at most  from U. Thus U is an -approximate MIS and it is computed in the k-machine model in 
 rounds. □

4.5. Putting it all together
Our k-machine model algorithm for FacLoc is shown in Algorithm 6. We could analyze the algorithm as done in [17] to show the constant approximation guarantee. However, we also want to use this algorithm as the basis for obtaining a p-median algorithm in the next section. Therefore, we take an approach similar to the one used in [11], [38], to show a stronger approximation guarantee, as in Lemma 4.13. We require several claims, which are along the lines of those in Thorup [17], and Archer et al. [38].

Throughout this section, we condition on the event that the outcome of all the randomized algorithms is as expected (i.e. the “bad” events do not happen). Note that this happens with w.h.p. We first need the following facts along the lines of [17].

Lemma 4.7

Modified From Lemma 8 of [17]
There exists a total ordering ≺ on the vertices in V such that  implies that 
. Furthermore, v is added to S if and only if there is no previous  in S such that 
.

Proof Sketch

The ordering is obtained by listing in each iteration, the vertices in I that are included in S before the rest of the vertices of W. Note that the extra 
 factor appears because of the definition of -approximate MIS. □

Claim 4.8

Modified From Claim 9.2 of [17]
For any two distinct vertices , we have that 
.

Proof Sketch

Without loss of generality, assume that , so 
. Now the claim follows from Lemma 4.7, and the definition of -approximate MIS. □

In the rest of the section, we follow the primal-dual analysis of [38], again with necessary modifications arising from various approximations. For completeness, we state the LP relaxation for FacLoc (primal) below left and its dual (below right). We reserve the subscript i for facilities and j for clients. Note that in our case, . 
 
 
 
 
 

Let  be the parameter that is used in the FacLoc algorithm. (Recall that we are working with the β-MP algorithm.) Set 
 
. Say that j contributes to i if 
. Then, set 
. It is easy to see that the v and w values are dual feasible.

Define for each , 
 if there exists an  with 
 and 0 otherwise. Note that 
 is uniquely defined, if it is not zero. This is because of the fact that the balls 
 and 
 are disjoint using Claim 4.8. Also note that 
, therefore, 
.

For , call the facility  that determines the minimum in 
, the bottleneck of j. We say that a facility (or a vertex) is closed if it does not belong to the set S, and it is opened otherwise. Furthermore, we say that a facility  caused another facility  was closed, if at the time u was removed in the Algorithm 6, Line 5, 
. Before showing the approximation guarantee, we need the following four lemmas. (cf. Lemmas 1-4 from [38])

Lemma 4.9

For any , we have that 
. Furthermore, if for some 
, then 
 
.

Proof

We have that 
. Now using the appropriate upper and lower bounds from Lemma 4.4 for 
 to get the desired inequality. □

Lemma 4.10

If , and if i is a bottleneck for j, then 
.

Proof

Since i the bottleneck for j, we see that 
. Therefore,(Using the fact that 
 
)
(Using Lemma 4.9.)
 □

Lemma 4.11

If an open facility is a bottleneck for j, then j cannot contribute to any other open facility.

Proof

Suppose 
 is j's bottleneck. Also assume that j contributes to another , i.e. 
. Using triangle inequality, we have that 
. In the last inequality, we use the fact that 
, which means that 
. Now there are two cases, depending on whether 
 or 
.

In the first case, if 
, then again using similar reasoning, we have that 
. However, this implies that 
, which is a contradiction to Claim 4.8.

In the second case, 
. However, since 
 is also a bottleneck for j, this implies that 
. That is, 
 is the closest vertex to j, i.e. 
. However, this implies 
, which is again a contradiction to Claim 4.8. □

Lemma 4.12

If a closed facility  is a bottleneck for , and  is the open facility that caused i to close, then 
.

Proof

The result follows from the sequence of inequalities below.(Triangle inequality)(k caused i to close, so using Lemma 4.7.)
(Using Lemma 4.9.)
(Since i is the bottleneck for j)
 □

We are finally ready to prove the main guarantee of the modified β-MP algorithm, as in [38]. The basic idea is to show that  times the dual variable 
 pays for the distance traveled by j, as well as, 
, which is a part of the facility opening cost. We formalize this in the following lemma.

Lemma 4.13

For any vertex , there exists a facility  such that

Proof

Consider a vertex . We prove the theorem by doing a careful case analysis.

Case 1.
Some open facility  is the bottleneck for j. In this case, connect j to i. If 
, we have that 
. Also, 
. Otherwise, 
, and 
.

Case 2.
Some closed facility  is the bottleneck for j, and j does not contribute to any open facility (i.e. 
). There must be some open facility  that caused i to close. Connect j to k. By Lemma 4.12, we know that 
.

Case 3.
Some closed facility  is the bottleneck for j, and there exists an open facility  with 
, but ℓ was not the reason why i was closed. Since 
, 
, by the uniqueness of 
. Connect j to ℓ. By Lemma 4.9, we have that 
 
. Also, there must be some open facility  which prevented i from opening. Using similar reasoning as in the previous case, we have that 
. Now,(Using Claim 4.8 and Lemma 4.9.)
(Triangle inequality)

Case 4.
Some closed facility  is the bottleneck for j. Furthermore, there is an open facility  such that 
, and k caused i to be closed. Connect j to k. Again, by uniqueness of 
, we have that 
. Also, from Lemma 4.10, 
. Since k caused i to be closed, we have that 
 
 
, by Lemma 4.10. Combining the previous inequalities yields, 
.

Finally, we use the well-known fact that for any , 
 for some constant c, and the lemma follows. □

Using Lemma 4.13, we get the following theorem.

Theorem 4.14

In 
 rounds, whp, Algorithm 6 finds a factor  approximate solution S to the facility location problem. Furthermore, if F is the total facility cost of the algorithm's solution, C is the total connection cost of the algorithm's solution, OPT is the optimal solution cost, and  then 
 

Proof

As for the approximation guarantee, we note by Lemma 4.13, we get that for each vertex , we have shown that there exists an opened facility  such that 
 which gives the desired guarantee. Finally, we note that the cost of any feasible dual solution is a lower bound on the optimal cost. Then, by setting  appropriately, the theorem follows. □

5. A p-median algorithm
In this section, we describe an 
 round algorithm for the p-median problem. We will follow the randomized rounding algorithm of Jain and Vazirani [11] which shows an interesting connection between p-median and uniform facility location problems. As observed in [11], the similarities between the linear programming formulations of the uniform facility location problem, and the p-median problem can be exploited to obtain an  approximation algorithm for the p-median problem, if one has a subroutine that returns an  approximation for the uniform facility location problem, with a specific property. This is summarized in the following lemma.

Lemma 5.1

Modified from [11]
Let  be a polynomial time uniform facility location algorithm that takes the facility opening cost z as input and returns a solution such that,  where C is the total connection cost, F is the number of facilities opened by the algorithm, and OPT is the optimal solution cost. Then there exists a randomized p-median algorithm 
 that returns a solution with expected cost at most 2μ times the optimal p-median cost.

Note that the facility location algorithm described in Section 4 returns a solution satisfying the guarantee in Lemma 5.1 (cf. Theorem 4.14). All that we need to show is that the randomized rounding algorithm can be efficiently implemented in the k-machine model. In the following sections, we first describe the sequential randomized algorithm 
 [11], and then discuss how to implement it in k-machine model.

5.1. The sequential algorithm
Let 
 and 
 be the maximum and minimum inter-point distances respectively. Using a Facility Location algorithm that has the guarantee of Lemma 5.1, we perform binary search on the facility opening cost z in the range 
. If we come across a solution 
 such that 
, then we have a μ-approximate solution and we stop. Otherwise, we find two solutions A and B, such that , with 
, where 
 and 
 are the facility opening costs corresponding to the solutions A and B respectively. Let 
 and 
. We now obtain a solution C from A and B, such that .

Construct the set 
 as follows. Starting with an empty set, for each vertex in A, add the closest vertex in B to 
, breaking ties arbitrarily. If at this point, 
, add arbitrary vertices from 
 to 
 until 
. Set , with probability a, and 
 with probability b, where 
 
 
. Now, pick a set of 
 vertices from 
, and add it to C. It is clear that , and this is the claimed solution with expected cost 2μ times that of the optimal p-median cost.

5.2. Implementation in the k-machine model
In order to implement the sequential algorithm in the k-machine model, we will assign a special machine (say the machine with the smallest ID), which executes the key steps of the sequential algorithm. For convenience, we refer to this machine as 
. First, each machine sends the weights of minimum and maximum weight edges incident on any of the vertices hosted by it to 
. This allows 
 to figure out the smallest edge weight 
 and the largest edge weight 
 in the input graph and it sets 
 and 
 (which is a crude polynomial upper bound). The machines perform binary search on the facility opening cost to obtain two solutions A, and B by using Algorithm 6 (modified appropriately to take facility opening cost as input parameter). We assume that each machine knows the subsets of the vertices hosted by it that belong to A and B respectively.

Now, we show how the machines identify the set 
 in 
 rounds. Using Lemma 2.3, Lemma 2.4 with , for each vertex in A, we determine the approximately closest vertex from B in 
 rounds, and let 
 be this set. At this point, each machine also knows which of its vertices belongs to 
. In  rounds, each machine sends the number of its vertices belonging to , and 
, to 
. If 
 discovers that 
, then it decides arbitrary 
 vertices from B, and informs the respective machines to mark those vertices as belonging to 
, and update the counts accordingly. This takes 
 rounds.

Now, 
 locally determines whether A or 
 will be included in the solution set C (with probability a and b respectively) and informs all other machines. Note that 
 knows the number of vertices in 
 that belong to each of the machines so it can sample 
 vertices in the set 
 as follows. For a machine 
, 
 sends it the number 
 which is the number of vertices from 
 hosted by 
 that are chosen by 
 uniformly at random to be in 
. Finally, each machine 
 chooses a set of 
 vertices uniformly at random from the set 
 that it hosts. It is easy to see that this procedure guarantees that each vertex from the set 
 has probability b of getting chosen in the set 
. The set 
 is the final solution.

At this point, each machine knows the subset of C that is hosted by it. We use Lemma 2.3, Lemma 2.4 to identify for each vertex , the approximately closest vertex  in 
 rounds. In additional 
 rounds, 
 can compute the approximate cost of the solution. Note while computing 
, we can find only  approximate nearest neighbor, instead of an exact nearest neighbor. One can show that this does not increase the cost of the resulting solution by more than  factor. We omit the details because the p-median analysis of Jain and Vazirani [11] goes through with a multiplicative  factor to account for the approximate nearest neighbor computation. Combining this with Lemma 5.1 and by repeating  times, we get that the solution obtained by our algorithm has cost at most  times the optimal solution with high probability. Finally, setting the value of ε for the facility location algorithm appropriately yields the following theorem.

Theorem 5.2

For any constant , there exists a randomized algorithm to obtain a  factor approximation to the p-median problem in the k-machine model in 
 rounds w.h.p.

6. A p-center algorithm
In this section, we describe a constant factor approximation algorithm for the p-center problem. It is a well-known that (see for example [16]) if 
⁎
 is an optimal p-center cost, then any distance-
⁎
 MIS is a 2-approximation for the p-center. But since we do not know how to compute a distance-d MIS efficiently in the k-machine model, we show in the following Lemma that an 
⁎
-approximate MIS suffices to get an -approximation.

Lemma 6.1

For a graph , if 
⁎
 is an optimal p-center cost, then any 
⁎
-approximate MIS is an 
 approximation.

Proof

Let 
 be an optimal p-center solution (we assume without loss of generality that O contains exactly p centers). Define a partition 
 of the vertex set V, by defining the set 
 for each 
 as follows. For each 
, let 
 be the set of vertices, for which 
 is the closest center in O. Here we break ties arbitrarily, so that each vertex appears in exactly one of the sets 
. Note that if 
 for some j, then 
⁎
.

Now let  be any 
⁎
-approximate MIS. We first show that I is feasible, i.e. , by showing that for any , 
. Assume this is not the case, i.e. for some i, there exist distinct 
. But this implies that 
⁎
, which is a contradiction to the fact that I is an 
⁎
-approximate MIS.

Finally, the approximation guarantee follows from the definition of an approximate MIS – for any , there exists an  such that 
⁎
. □

Although we do not know the optimal p-center cost 
⁎
 we can find it by doing a binary search to get the largest d such that an -approximate MIS has size at most p. There are at most  iterations of the binary search because of our assumption that the distances bounded by . This along with Lemma 4.6 gives us the following theorem.

Theorem 6.2

For any constant , there exists a randomized algorithm to obtain a -factor approximation to the p-center problem in the k-machine model in 
 rounds w.h.p.

7. Conclusions and future work
This paper initiates the study of clustering problems in the k-machine model and presents near-optimal (in rounds) constant-factor approximation algorithms for these problems. The near-optimality of our algorithms is established via almost-matching lower bounds on the number of rounds needed to solve these problems in the k-machine model. However, the lower bounds critically depend on a certain assumption regarding how the output of the clustering algorithms is to be represented. Specifically, we require that every machine with an open facility knows all clients connecting to that facility. This requirement forces some machines to learn a large volume of information distributed across the network and this leads to our lower bounds.

We could alternately, impose a rather “light weight” output requirement and, for example, require each machine with an open facility to simply know the number of clients connecting to it or the aggregate connection cost of all the clients connecting to it. (Of course, independent of this change, the output requires that each client know the facility it connects to.) So the main open question that follows from our work is whether we can design optimal k-machine algorithms under this relaxed output requirement. Specifically, it may be the case that, with this relaxed output requirement, there are optimal 
-round algorithms for the clustering problems considered in this paper. Alternately, can we prove stronger lower bounds even in this, more relaxed, setting?