Abstract
In this article we consider finite automata networks and make some connections between the two most classical deterministic update schedules: the parallel one (all automata are updated all together) and the sequential ones (the automata are updated periodically one at a time according to a total order). Given a network h with n automata, where each automaton has q possible states, the cost of sequentialization of h is the minimum number of additional automata required for another automata network to simulate h with the same q states thanks to a sequential update schedule. We prove that this cost is at most ⌈n/2+logq⁡(n/2+1)⌉ and, conversely, we construct a network h with cost at least n/2−logq⁡(n)/2−2 for q≥4 and ⌊n/3⌋ for q≥2. Furthermore, we prove that, except in some marginal well identified cases, n plus the cost of sequentialization of h is exactly the procedural complexity of h with unlimited memory. Finally, we prove that the cost of sequentialization of h is at most the directed pathwidth of its interaction graph.

Keywords
Automata networks
Intrinsic simulation
Parallel update schedule
Sequential
Update schedules
Procedural complexity
Interaction graph

1. Introduction
In this article, we study finite automata networks. They are discrete dynamical systems introduced in the 1940s by McCulloch and Pitts [1], which are nowadays classical models for the dynamics of natural dynamical systems, like gene or neural networks [2], [3], [4]. Also, they are computational models used to study computability and complexity properties [5], [6], [7], which is the purpose of this paper.

A network of size n (i.e. with n automata) over a finite alphabet  can be regarded as a function 
. The automata of h are indexed with numbers named coordinates between 1 and n. Furthermore, 
 is the set of possible configurations for the network: in the configuration 
, the state of automaton i is 
. When updated, the state of the i-th automaton evolves according to 
, which is the i-th coordinate function of h (sometimes referred to as local function). Hence, 
. In the following, it is very convenient to extend the function 
 to the function 
 defined by 
. In this way, 
 is the configuration obtained from x when we only update the coordinate i. Given a word 
 over the alphabet , we then set 
. Hence, 
 is the configuration obtained from x when we update sequentially the coordinates from 
 to 
.

To study the dynamics of a network, we have to define an update schedule, namely, the order in which the coordinates are updated between two time steps. In this paper, we consider the two most classical deterministic update schedules. Firstly, we consider the parallel update schedule (sometimes referred to as synchronous), where all coordinates are updated all together at the same time. In other words, if 
 is the configuration of the system at time t, then 
. Secondly, we consider the sequential update schedules, where the coordinates are updated sequentially, according to a total order on the coordinates. More concretely, a sequential update schedule can be described by a word 
 of size n over the alphabet  without repeated letter. The word u can also be regarded as a permutation of . More precisely, given such a word u, the sequential dynamics is described by the recurrence 
.

There have been several works which study the set of functions computed by a given network depending on the update schedules used [8], [9], [10], [11], [12], [13]. Here, like in [14], we take the opposite approach. We consider a network with a parallel update schedule, and we search for a network with the same dynamics when updated sequentially. In other words, given 
, we want to find another network 
 and a sequential update schedule w such that 
. However, sometimes, it is impossible to find such a network f. To show this, consider the network 
 defined by 
. For the sake of contradiction, suppose that there exists 
 and a sequential update schedule w such that 
. We can suppose, without lose of generality, that . Hence,
 As a consequence, 
 and thus 
. But then 01 and 11 are both equal to 
, and it is a contradiction. However, what we can do is to consider the network 
 defined by 
 and the sequential update schedule . Then,
 The network f has one more automaton than h but with the sequential update schedule w, it computes h if we only consider the first two coordinates, whatever the value of 
. We then say that f sequentializes h thanks to the sequential update schedule .

More formally, and more generally, given  and a finite word w over , we say that 
 simulates (and sequentializes if w has no repetitions) 
 thanks to w if
 where 
 is the projection on the first n coordinates.

This paper is about the cost of sequentialization of a network h, namely the minimum number 
 of additional automata required for a network f to sequentialize h. We have seen above that we can have 
, and it is easy to see that 
. Indeed, for any 
, the network 
 defined by 
 and 
 for all  sequentializes h thanks to .

Our first main result shows that we can always do something much better, which is asymptotically optimal when the alphabet size increases. It widely generalizes and improves the results obtained in [15] (which gives weaker bounds in the case ).

Theorem 1

Let  be a finite set of size . For any 
 we have

Theorem 2

Let  be a finite set of size q. If , then there exists  such that
 If  then there exists  such that 
. Finally, for  and n large enough, 
 
.

There is another well-known way to simulate the network 
 such that 
 without adding any additional automaton. For example, we can take 
 such that 
 and compute h thanks to the update schedule . Indeed,
 This does not mean that the cost of sequentialization of h is 0 since the word  is not a sequential update schedule: the word w contains two times the letter 1. Such repetitions allow a drastic reduction of the minimum number of additional automata required to simulate h. Actually, at most one additional automaton is required, and the network f simulating h with this additional automaton can be defined independently of h: it is proved in [16] that there exists 
 such that, for any 
, there exists a word w over  (possibly exponentially long) such that f simulates h thanks to w.

Taking the opposite direction, we next study the minimum length of a word required to simulate h. More precisely, given 
, we denote by 
⁎
 the minimum integer ℓ such that there exists , a word w over  of length ℓ and a network 
 such that f simulates h thanks to w. This quantity 
⁎
 is equivalent to the procedural complexity of h with unlimited memory, introduced in [17]. Our second main result connects the cost of sequentialization and this procedural complexity.

Theorem 3

For any 
 we have
⁎
 where  is the number of coordinates  such that 
 is not the identity function.

The interaction digraph of a network is a digraph which represents the influence between the coordinates of a network. More precisely, the interaction digraph of 
 is the digraph with vertex set  such that, for all , there is an arc from j to i if the value of 
 depends on the value of 
. The interaction digraph is an important object when studying networks and is what motivates the network terminology. Finding properties of a network according to its interaction digraph is a classical exercise [18], [19], [20], [21], [22]. Our third and last main result gives an upper bound on the cost of sequentialization according to the interaction digraph only.

Theorem 4

For any 
, 
 is at most the directed pathwidth of the interaction digraph of h.

The paper is organized as follows. In Section 2, the main definitions and notations are introduced. In Section 3, the problem of finding the cost of sequentialization is proved to be linked to the problem of finding the chromatic number of a graph named confusion graph. In Section 4, the upper bound of 
 given in Theorem 1 is proved. In Section 5, the lower bounds of Theorem are proved. In Section 6 is shown the link between the cost of sequentialization and the procedural complexity. Theorem 3 is proved here. Section 7 is about bounding the cost of sequentialization of a network h with its interaction digraph. Theorem 4 is proved here. At least, Section 8 gives a conclusion and some research directions.

2. Definitions and notations
For all , with , the closed interval between i and j is denoted by  and the open one by . If n is a positive integer, we use  as an abbreviation of . The set of permutations of  is denoted by 
 and is regarded as the set of words over  of length n without repeated letter. Let 
. If 
 then we say that i is updated at step . For any word 
 and set 
 with 
, the projection of x on I is denoted either by 
 or by 
. In other words, 
. For all words 
 and 
, their concatenation is denoted by 
. Also, 
 is the concatenation of n times the vector x. In particular, 
 is a word composed of n times the letter 0. We say that 
 is a subword of 
 (or equivalently that y contains x) if there exists  such that 
. In the following, unless otherwise specified, n and q are integers with ,  and  is an alphabet of size q.

We denote by  the set of functions from 
 to 
 (or equivalently the set of networks of size n with an alphabet of size q). Let . For every , the i-th coordinate function of f is the function 
. This means that 
. We say that 
 is trivial if 
 for all 
. Similarly, we define 
 for every . In this paper, we make particular use of the superscript of a function f. Firstly, as mentioned in the introduction, for any , 
 is defined by
 and, if 
, then 
 is defined by
 The relation 
 is sometimes expressed by 
. Note that if  is the concatenation of two non-empty words u and v then 
. Secondly, for any , 
 is defined as follows. For all 
 and ,
  In other words, 
 is the configuration obtained from x when all the coordinates in I are updated synchronously.

The interaction digraph of a network  is the digraph  whose vertices are the coordinates of f (i.e. ) and whose arcs are the couples  such that the component i has an influence on the component j in f. More formally, for any arc , there exists 
 such that 
 and 
.

We now recall from the introduction our notion of simulation and the cost of sequentialization, which are the main concepts of this paper.

Definition 1 Simulation and sequentialization

Let ,  with  and let w be a finite word over . We say that f simulates h thanks to w if 
. Furthermore, if f simulates h thanks to w and 
 then f sequentializes h thanks to w.

Remark 1

All the results of this paper remain true if the definition of simulation 
 is replaced by the following weaker condition: , with  such that 
.

Definition 2

 and 
Let . The cost of sequentialization of h, denoted by 
, is the smallest integer k such that there exists  such that f sequentializes h. We set

In order to study the cost of sequentialization of h, it is convenient to first study a refined version of this cost, where we put constraints on the update schedule.

Definition 3

 and 
Let  and 
. The cost of sequentialization of h with the pattern u, denoted by 
, is the smallest integer k such that there exists  which sequentializes h thanks to a word 
 which contains u. We set

Clearly, 
 and thus 
. Example 1 below shows that the difference between 
 and 
 can be large.

Example 1

Consider the network  represented Fig. 1. The network h is of size  and it permutes 2 pairs of values. More specifically,
 Now, consider the canonical sequential update schedule . We want to find a network  and a word 
 which contains u such that f sequentialize h thanks to w. Let  be the network represented Fig. 2. In other words, let 
. Consider the word 
 whose prefix is a permutation of the 2 additional automata of f and which has u as suffix. By applying 
, we get
 
 
 
 Then, 
. As a result, f whose size is  sequentializes h with the pattern u and this means that 
. Furthermore, as we will see in Section 3 (in Proposition 2), there are no smaller networks f which would work. Consequently, 
.

Fig. 1
Download : Download high-res image (10KB)
Download : Download full-size image
Fig. 1. Network h of Example 1.

Fig. 2
Download : Download high-res image (13KB)
Download : Download full-size image
Fig. 2. Network f of Example 1.

Now, one can prove that there exists a network  (which has one additional automaton) which simulates h thanks to a word 
 (which does not contain u). First, let  which, instead of updating the coordinates of  in the same order than u, updates the pairs of coordinates  and  one after the other. Then, we define the network  as
 As always, the additions and subtractions are done modulo q. The network g is represented Fig. 3. One can check that 
. As a result, 
. Consequently, g sequentializes h and thus 
.

Fig. 3
Download : Download high-res image (25KB)
Download : Download full-size image
Fig. 3. Network g of Example 1.

More generally, one can show that, for all integer n even and , we have 
 and 
 with  the network defined by 
 
 
.

3. Confusion graph
In this section, we study a way to translate the problem of finding the cost of sequentialization into a simpler problem of graph coloring. To do so, we use a tool that was first introduced in [15] where it was called the  graph (standing for Not Equivalent and Confusable Configurations). We rather call it the confusion graph in this paper. Consider the network  such that 
 and the word 
. As seen in the introduction, there are no networks f such that 
. It is because any  cannot have the two following properties together:

-
 and,

-
.

Consequently, we could say that the pair of configurations  is an obstacle to the sequentialization without memory of h with the pattern u. The confusion graph 
 defined below represents any such pair 
 with an edge. We can see that if 
 has any edge then 
, but Lemma 1 goes further and shows that we can deduce 
 directly from the chromatic number of 
.
Definition 4 Confusion graph

Consider  and a word 
. The confusion graph 
 is the undirected graph whose vertices are the set of all configurations 
 and in which two configurations x and 
 are adjacent if and only if

-
, and

-
, 
.

An example of confusion graph is given in Fig. 4. In this example,  and  are adjacent in 
 because 
 and . Conversely, even if 
, the configurations  and  are not adjacent in 
 because .

Fig. 4
Download : Download high-res image (66KB)
Download : Download full-size image
Fig. 4. Example of a network h ∈ F(3,2) and its confusion graph with the order u = (1,2,3). (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)

In the sequel, we denote by  the chromatic number of a graph G, namely the minimum number of colors of a proper coloring of its vertices. In [15], the exact relation between the chromatic number of the confusion graph 
 and 
 was proved in the case where . We propose in Lemma 1 a straightforward generalization for any alphabet size.

Lemma 1

For every  and 
 we have 
.

Proof

Lower bound. Without loss of generality, suppose that u is the canonical sequential update schedule . Let 
,  and . Consider  and 
 such that f sequentializes h thanks to w which contains u.

Fix 
. Let 
 be the coloring function of the confusion graph 
 such that 
. The function c uses at most 
 colors. Hence, if c is proper, we have 
 and thus 
. As a result, it is now sufficient to prove that for each edge 
, the inequality 
 holds.

Let x and 
 be adjacent configurations of 
,  and 
. Let us prove that 
. For the sake of contradiction, suppose that 
. Since x and 
 are adjacent in 
, we know that 
 and 
 for some . Consider the biggest of these i (note that  since 
). So we have 
 and then 
. Let us decompose w as 
. Let us prove that 
. First, since f sequentializes h thanks to w, we have
 Furthermore, for each coordinate  that does not appear in v, we have 
. Finally, for each coordinate  that appears in v, we have 
 because we assumed that 
. As a result, 
. However,
 This is a contradiction. Consequently, we have, 
. Thus, we have proved that if x and 
 are adjacent in 
, then 
. In other words, 
 gives a proper coloring of the confusion graph 
. As a result, the confusion graph needs at most 
 colors because 
 is a word of size  on the alphabet . Thus, 
 and 
.

Upper bound. Let 
,  and . Let 
 such that 
. Let 
 be a proper coloring of 
. For all  and any configuration 
, let us define 
 as follows. First, 
 and then , 
 and 
. Let  such that:

-
, and

-
 

Let us prove that 
. Let 
 and 
 with 
. By, induction let us prove that,(1)
First, for , we have,
 Second, let  and suppose that,
 We have
 with 
. Furthermore, we have
 because, by induction hypothesis,
 and
 Let us prove that 
. For the sake of contradiction, suppose that 
. Thus, 
. However, 
 thus 
 and 
. Consequently, x and 
 are adjacent in the confusion graph, but they have the same color. This is a contradiction. □

If we look again at the example given in Fig. 4, we can see that the chromatic number of 
 is 2. Indeed, we give a proper coloring of 
 which uses only 2 colors and, since there is at least one edge in 
, there is no valid coloring with only 1 color. Thus, according to Lemma 1, we have

The Proposition 1 below states that the cost of sequentialization with or without pattern (
 or 
) can only increase when n increases. The proof is based on the fact that, from a network  and 
 with 
, one can construct 
 such that 
 (and such that for any 
 there exists 
 with 
). For instance, this proposition can be used to simplify the proof that for any n, 
. Indeed, using Proposition 1, it is sufficient to prove that for any k, 
.

Proposition 1

For any 
, if 
 then 
 and 
.

Proof

Let 
, 
 and . Let 
 such that for any 
,
 Consider 
 and let 
 such that u is a subword of 
. Let us prove that the confusion graph 
 restricted to the vertices in 
 is isomorphic to the confusion graph 
. Consider 
 and 
 such that 
, 
 and 
. Clearly  if and only if 
. Furthermore, if x is adjacent to y in 
 then there exists  such that 
. One can check that 
 is connected to 
 in 
. Indeed, for 
 such that 
, we have
 The other side is totally symmetrical. Hence, 
 contains an isomorphism of 
 as subgraph. Clearly, 
. As a result, for any 
, 
 for some 
. In particular, let  and 
 such that 
, then 
. The same way, for  such that 
, we have 
. □

4. Cost of sequentialization with a pattern
In this section, we study the cost of sequentialization with a pattern, denoted 
. First, we show in Theorem 1 that we can upper bound 
. Clearly, this gives us an upper bound for 
 as well. Second, Proposition 2 shows a lower bound for 
 which is really close to the upper bound.

In [15], the authors showed that 
. Theorem 1 below shows that we have in fact, 
 for any q. To prove it, all the configurations of the confusion graph 
 which are equal in their second half 
 and have the same image 
 are regrouped together. One can prove that a proper coloring of this factorized graph is a proper coloring of the confusion graph. Furthermore, the maximal degree of the factorized graph is at most 
. One can conclude using the Brooks' theorem which states that the chromatic number of a graph is at most its maximum degree plus one.

Theorem 1

.

Proof

Let  and 
. Let us prove that 
. Without loss of generality, suppose that u is the canonical sequential update schedule . Let 
. Let 
 be a partition of 
, such that 
 are in the same set 
 if and only if the two following conditions hold:

-
They are equal on the second half of the coordinates which will be updated in u. In other words, 
 or, more simply, 
 because we assumed that .

-
They have the same image by h. In other words, 
.

For all 
, let us denote by  the set 
 which contains x. Let 
. Let 
 be the graph where two sets 
 and 
 are adjacent in 
 if and only if there are two configurations 
 and 
 adjacent in the confusion graph 
. Let us prove that the maximum degree of 
 is at most 
. Let 
 be the vertex of 
 of maximum degree and  its set of neighbors in 
. Note that if a set 
 is in N then there exists two configurations 
 such that 
 and 
.
Let us decompose N in  sets (not necessarily disjoint) as follows.

-
Let 
 be defined so that for any 
 we have 
 and 
 such that
 Since 
, we have 
 and 
 because . In other words, 
, there exists 
 such that 
. However, there is only 
 such configurations 
. Thus, 
.

-
For all , let 
 be defined so that for any 
 we have
 Let 
 and let 
 such that 
. Thus, we have 
 because . However, the value of 
 is fixed on the interval  and can only vary on the interval . As a result, the second half of 
 can take 
 values. Furthermore, we know that 
, 
 and 
. Consequently, 
. Thus, the value of 
 is fixed on the interval  and can vary only on the interval . As a result, 
 can take 
 different values. Now, if two configurations 
 and 
 have the same image by h and are equal on their second half then they are in the same set 
. Thus, 
.

We have 
. Thus, 
. As a consequence, the degree of 
 in 
 is at most 
. Moreover, the degree of 
 is strictly less than 
 because 
 is not neighbor of itself but has been counted when we gave the upper bound of . As a result, 
 with 
 the maximum degree of 
. We can see that any coloring of this graph 
 gives a proper coloring of the confusion graph 
. Indeed, we can color all the configurations of a set 
 in 
 as we color 
 in 
. If two configurations x and 
 are adjacent in the confusion graph 
, then 
,  and 
 are adjacent in 
 and do not have the same color. Thus, 
. As a consequence, according to Lemma 1, we have, 
. Hence, 
. □
In [15], the authors proved that, for all n, one can construct  whose cost of sequentialization with the pattern 
 is . Proposition 2 below is a straightforward generalization for any alphabet size. To prove Proposition 2, one can construct a couple  such that 
 has a clique of size 
. Since the chromatic number of a graph is at least the size of its biggest clique, we have 
. As a result, 
, and we get Proposition 2 from that.

Proposition 2

.

Proof

Let . Let  such that:

-
,

-
, and

-
if n is odd, let 
.

We also consider the canonical sequential update schedule . Let 
 be the set of configurations which have only 0 in their second half. Let us prove that X is a clique in 
. To do that, consider two distinct configurations 
 and let us prove that 
. We have 
. Thus, 
 and  such that 
 and 
. Thus, 
. However, when we update the first half of the coordinates, x and 
 both become the configuration 
. Indeed, 
. Then, we have 
. As a result, 
 is an edge in 
. As a consequence, every two distinct vertices of X are adjacent. Thus, X is a clique. Moreover, X is a clique of size 
. Thus, 
 and 
. Hence, 
. □
Remark 2

Note that the network defined in the proof of Proposition 2 is bijective. In [17], Theorem 5 shows that if  is bijective, then for any 
 we have 
 if n is even and  otherwise. As a result, we know that the biggest 
 with h a bijective network of even size n is exactly .

5. Lower bound on the cost of sequentialization
The purpose of this section is to build networks with the greatest cost of sequentialization possible. This gives us lower bounds for 
 which are given in Theorem 2 below.

Theorem 2

For all ,
 For all ,
 Finally, if  and n is large enough, then
 

Theorem 2 is a direct consequence of Lemma 2, Lemma 3, Lemma 4, Lemma 5.

We define the following notation. For all sets I and all integers k,  
  is the set of subsets of I of size k.

In Lemma 2, we consider the integers k and  and suppose that the following hypothesis is respected. Firstly, each set 
  is associated to a set 
 of 
 configurations which are different on 
 only. Secondly, the sets 
 
 are disjoint. Lemma 2 states that in this case, there exists a network  with the two following properties. First, the network h has no trivial coordinate function 
 (). Secondly, for any word 
, there is a corresponding set 
 which is a clique in the confusion graph 
. Therefore, 
 and 
.

Lemma 2

Let ,  
 ,  
 
 and 
. For all , let
 If the sets 
 are disjoint then there exists a network  such that  and 
.

Proof

Let . For any , let 
. Let us define the network  as follows. Let 
. If 
 for some  then 
. Otherwise, 
.

We can check that all the coordinate functions of h are no trivial. Indeed, for any , the coordinate function 
 is not trivial because it always returns 0. Furthermore, for any  (resp. ), let  such that 
 (resp. 
) and consider the two configurations 
 such that 
 and 
. We have 
 but 
. Hence, the coordinate function 
 is not trivial. As a result, h has no trivial coordinate function and .

Now, let us prove that 
. To that end, it is sufficient to prove that we have 
 for all words 
. Consider a word 
. Let  such that 
 is the set of the k first letters of u which belong to . In other words, there exists t such that 
 and 
. Let us prove that 
 is a clique in the confusion graph 
. Consider a pair of configurations 
. We can prove that 
 as follows.

-
We have 
 because

-
We have 
 because

Therefore, 
. Moreover, 
 because 
. As a result, 
 and 
 is a clique in the confusion graph 
. Note that the set 
 is of size 
 and therefore 
. According to Lemma 1, for any 
, we have 
. Consequently, 
. □
Using Lemma 2, we can prove Proposition 3 below.

Proposition 3

For any , we have 
.

Proof

Let , , . Clearly, a subset 
  can be encoded with a word 
 such that, for all ,
  Now, consider the configurations 
 such that, for all , 
 and 
. One can check that the sets 
 (defined like in Lemma 2) are disjoint. Indeed, 
 for all . Therefore, for all 
 and 
 with 
, we have
 According to Lemma 2 and Proposition 1, we then have 
. □

One can see that, even for , the lower bound given in Proposition 3 is far from optimal. Indeed, firstly, there are only  
 
 subsets of  of size k. Therefore, the subset could be encoded in a subword of size 
  (instead of one of size 2k). However, this would not change much because  
  is close to 
 when k tends to infinity because  
 
. More importantly, the coordinates 
 of the word 
 are not used to encode the subset 
. Conversely, in Lemma 3 below, the coordinates 
 of 
 are used to encode the second half of 
 and consequently, the subword 
 is only of size k. One can check that the sets 
 defined in this way are disjoint, and we get Lemma 3.

Lemma 3

For all , we have 
.

Proof

Let , , ,  
  and  
 
. For all , let 
 and let 
 such that, for any ,
 

Let us define 
 such that
 For all , let 
. We have to prove that the sets 
 are disjoint.

For all  and , let 
 with . Note that 
 also equals 
 and for all 
, we have 
. For all , let us define 
 as follows. First, for all 
, 
. Secondly, for all 
 and ,
 
 To prove that the sets 
 are disjoint, it is sufficient to prove that, for all 
, we have 
. Indeed, if 
 there are two cases: if the difference is in the first k bits then 
 and therefore 
 and if the difference is in the last k bits then 
 and therefore 
.

Let 
. By induction on j, let us prove that, for all ,
 First,
 Secondly, let  and suppose that 
 with 
. We can prove that 
 as follows.

-
If  and 
 then 
. Note that 
 and then 
. Consequently,
 Finally, 
.

-
Otherwise, we have 
. One can check that 
 as follows.

–
First, suppose that  and that 
. Thus, 
 and then 
.

–
Now, suppose that . Note that 
. Consequently, 
 and 
. Furthermore, 
 by definition of 
. As a result, 
, 
 and 
.

Finally, we have
 As a result, 
. Moreover, 
 and 
. Consequently, 
.
As a result of the induction, for all , 
 and in particular 
. Therefore, the sets 
 are disjoint. Furthermore, by Lemma 2 and Proposition 1, we have 
. □

In Lemma 4, we study the case where . One can see that if , one can encode two letters 
 in each letter 
 with 
. Thus, one can encode the word 
 in the subword 
. To have a non ambiguous decoding, we only need to encode a “starting position” i judiciously selected in 
. Looking 
, one can get a part of 
 and then of 
. A bigger part of 
 can then be guessed observing the values of 
 newly revealed. This process can be repeated until finding all of 
. As a conclusion, when , one can find a lower bound for 
 witch is close to 
. This is close of the upper bound given in Theorem 1, which is close to 
.

Lemma 4

For all , 
.

Proof

In this proof only, to simplify the use of modulo operations, the coordinates of a network are indexed from 0 (instead of 1). In other words, any configuration 
, is written 
. Furthermore, we will note the empty interval as  with . Let  with 
, , ,  
  and  
 
. Let 
 for all . For all , let 
 such that, for all , 
 if 
 and 0 otherwise. For all , we define
 
 At least, let 
 such that 
.

For all , let 
 the function which take a coordinate  (resp. a subset  or a configuration 
) and returns the coordinate i (resp. the subset I of the configuration y) circularly shifted of m coordinates on the left. More formally, for all  and , we have 
. For all , we have 
. Finally, for all 
, we have
 Note that
  .

For all , let 
, 
 and 
 with 
.

Claim 1

Let , . We have 
.

Proof

First, let us suppose that 
. Then, 
. Furthermore,
 Therefore, 
 because otherwise this would contradict the minimality of 
. Secondly, suppose that 
. Let 
. We have 
. Note that 
 because 
. As a result,
 However,
 Therefore,
 As a consequence, 
 because otherwise this would contradict the minimality of 
. In all cases, we then have, 
. ■

Now, consider the two functions 
, 
 which can respectively encode or decode a coordinate . This means that, for all , 
. The same way, consider two functions 
 and 
 which can respectively encode or decode a couple of letters 
. In other words, for all 
, we have 
. Note that such functions exist because .

For all  and for all , let 
. Define 
 and 
 as follows. For all ,
 Let 
.

For all , let us define 
 as follows. First, for all configurations 
, 
 with  the empty word. Second, for all configurations 
,
 

Let 
⁎
 such that 
 with 
. To prove that the sets 
 are disjoint, it is sufficient to prove that, for all  and 
, we have 
.

Let  and 
. We have 
. Thus, it is sufficient to prove that 
 with 
.

To lighten the notation, let 
, 
, 
 and 
.

By induction on j, let us prove that, for all ,
 First,
 Secondly, let  and suppose that 
 with 
. One can prove that 
 as follows.

-
If  and 
 then 
. Note that 
 and then 
. Therefor,
 As a consequence, 
.

-
Otherwise, we have 
. One can check that 
 as follows.

–
Suppose that  and that 
. Thus, 
 and 
.

–
Now, suppose that . Then, 
. Note that
 Consequently,
 Furthermore, , 
 and 
. If  then
 which is a contradiction because, according to Claim 1, we have 
. As a result, 
.

In all cases, we have 
. Consequently,
 Thus,
 Furthermore,
 and
 Finally, 
.
By induction, for all 
 and in particular 
. Thus, for all  and 
, we have 
 and the sets 
 are disjoint. According to Lemma 2, we have 
 with 
 and .
Consider an integer  and the largest integer k such that  with 
. Then, one can check that 
 and we deduce (using Proposition 1) that
 ■

We can now study the special case of .

Lemma 5

For , 
 
.

Proof

Suppose that  and let 
. One can check that if 
 then 
 adapting the proof of Lemma 3. This shows that 
 and thus 
.

By doing this, the value of 
 is not as much taken advantage of that it could be. Indeed, each letter 
 with 
 could take 3 distinct values but is only used to encode a bit 
. On the other hand, there are not enough space to encode a couple of bits 
 as required on the proof of Lemma 4.

However, one can use the following trick. The coordinates of 
 are partitioned in sets of size λ, with λ a parameter which will be fixed later. In each word 
, one can encode the word 
 with μ the largest integer such that 
.

Adapting the proof of Lemma 4, we get the following inequality. We have 
 if  with 
 and 
 
. It is now an optimization problem where we want to maximize k. For this, we have to take λ big enough to minimize the difference between 
 and 
. However, we have to take λ small enough to minimize γ. One can take λ growing with k but negligible compared to k (for instance ).

We can then take 
,  and  with 
 
 and k maximal under these constraints. Then, we get  and thus 
 
. □

6. Procedural complexity
In this section, we study the relation between κ and the procedural complexity as defined in [17]. It is basically the number of step necessary to compute a function h, modifying the coordinate one by one. Let us call an instruction a function which updates at most one coordinate. In other words, if g is an instruction, then there exists  such that for any 
, 
 with . For any  and , 
⁎
 is the set of instructions from 
 to 
.

Definition 5 Procedural complexity

The procedural complexity of  with a memory k, denoted by , is the minimum number t of instructions 
⁎
 such that 
.

In particular, the procedural complexity with a memory  is called the memoryless procedural complexity and is denoted by . This notion of memoryless computation, was introduced in [23] and developed among others in [24], [25]. Here, we also study the procedural complexity with an arbitrarily big memory denoted by 
⁎
. Let  be the number of non-trivial coordinate functions of h. Theorem 3 shows that the procedural complexity of an AN (automata network) h equals 
. Furthermore, it shows that the minimal procedural complexity is reached when we use a memory of size 
.
Theorem 3

Let  and 
. We have 
⁎
.

Theorem 3 is a consequence of Lemma 6 and Lemma 7. In Lemma 6, we prove that 
⁎
. We use the fact that by definition of 
 there exist  and 
 such that the  instructions 
⁎
 compute h. With that, we already have . Furthermore, for each i such that 
 is trivial, we can remove the function 
 from the list of instructions and still compute h. As a result, we have , and by definition of 
⁎
 we have 
⁎
.
Lemma 6

Let  and 
. We have 
⁎
.

Proof

Let , 
, and . By definition of 
, there exists  and 
 such that f sequentializes h thanks to w. Thus, 
. By definition, , 
 does not update more than one coordinate. Then, 
⁎
. Consider the set T of coordinates of the trivial functions of h and let 
 be the word obtained from w by removing the letters that belong to T. In other words, 
, if  then 
. Let us prove that 
. Let 
 be a trivial coordinate function. Thus, 
. Also, for all 
 and , we have 
. Furthermore, since 
, the coordinate i is updated once in w at step . Thus, 
. Hence, since i is not updated before the step j, we have 
. As a result, 
, 
. Using the same method for all j such that 
 is trivial we get 
. The length 
 is . As a result, we have . Hence, by definition of 
⁎
 we have 
⁎
. □

In Lemma 7, we prove that 
⁎
 with 
. To do so, we take a list of functions 
⁎
 which compute h. We consider a permutation 
 which updates all coordinates of  in the same order as 
 update them for the last time. Then we prove that h can be sequentialized with the pattern w with less than 
⁎
 additional automata.

Lemma 7

Let  and 
. We have 
⁎
.

Proof

Let 
⁎
, ,  and 
⁎
 such that 
. We can assume that for all , the function 
 updates one coordinate. Otherwise, 
 would be the identity function, and we could remove it and have 
⁎
, which is absurd. Let 
 such that, for all , 
 is the coordinate updated by 
. Let 
 with 
 be the set of steps where a coordinate of  is updated for the last time in w. In other words, 
 and 
. Furthermore, let 
 with 
 be the set of the other steps. We know that  because, to compute h, each coordinate of a non-trivial function of h needs to be updated at least once. Indeed, if 
 is non-trivial, then 
. If i is not updated in w, then 
 and h is not computed. Let 
⁎
. Let v be a word which contains all the coordinates of  not updated in w and let 
. For all , let 
 be the function which returns the value of the coordinate updated by 
. In other words, 
. Let 
 (a word of size  containing only the letter 0). Let 
, such that, 
, 
. Let us prove that c gives a proper coloring of the confusion graph 
. Let 
 be adjacent in the confusion graph 
. For the sake of contradiction, suppose that 
. Let  and 
. Let 
⁎
. Let  be the last step of w in which the coordinate 
⁎
 is updated. Let 
 and 
. Let us prove that 
. For all  not yet updated in w at step b:

-
If , then 
 because 
⁎
⁎
, 
⁎
 and thus
⁎
⁎

-
If  then 
.

For all  already updated in w at step b:
-
If  and a is updated for the last time then 
 because 
⁎
⁎
, 
⁎
 and 
⁎
.

-
Otherwise if  or if a is not updated for the last time, let  be the last step in w before b such that  is updated. In other words, 
 and 
. We have 
.

Thus, we have 
 and thus 
. However, 
. This is absurd, so if two configurations 
 are adjacent in the confusion graph 
 then 
. Thus, c gives a proper coloring of the confusion graph 
 and it uses at most 
⁎
 colors. By Lemma 1, 
⁎
 and thus 
⁎
. □
In [17], Proposition 12 states that for any network , we have . In Corollary 1 below, we refine this bound using Theorem 1, Theorem 3 and the fact that .

Corollary 1

For all ,  with 
.

In the following Corollary 2, we give a lower bound for the procedural complexity with an arbitrarily big memory. It is a direct corollary of Theorem 3, Lemma 2, Lemma 3 and Lemma 4.

Corollary 2

For all  there is  such that 
⁎
. Furthermore, if , there is  such that 
⁎
.

7. Interaction digraph
In this section, we will show that one can upper bound the cost of sequentialization of a network using its interaction digraph.

Consider the interaction digraph  and the network . As a warm up, one can convince himself that if D is an acyclic digraph then 
. Indeed, consider a topological order 
 of the vertices of D. There is then no  such that . Consequently, for all 
 and , we have
 with let . This means that h can be sequentialized by himself using u: 
.

A feedback vertex set of a digraph  is a subset  of the vertices of D such that all the cycles of D pass through I and the transversal number of D is the size of its smallest feedback vertex set. If the transversal number of D is 0, then the empty set is a feedback vertex set of D and D is acyclical and then, like already said, for any networks , we have 
. Moreover, one can prove that the transversal number of D is an upper bound for 
.

Proposition 4

Consider a digraph D with a feedback vertex set I of size k. For all networks , we have 
.

Proof

Let  and . Consider the following network . For all 
 and 
, let

Consider a topological order 
 of  and two permutations 
 and 
. Let 
. By simple expansion of 
 we get
 As a result, 
 and thus 
. □

We are going to define a more precise notion that the transversal number: the directed vertex separation. This notion was introduced in [26] and follows the notion of vertex separation for the undirected graphs introduced in [27].

We give here a definition adapted to the notations of this paper.

Definition 6 Directed vertex separation

Consider the digraph  and a word 
. Let
 The directed vertex separation with the pattern u, denoted by 
, is defined as follows.
 
 The directed vertex separation, denoted by , is defined by  
 
.

In some digraph, the transversal number can be much greater that the directed vertex separation. For instance, the transversal number of a undirected squared grid of dimensions n is close to 
 (because all the neighbors of a vertex not in the feedback vertex must be in the feedback vertex set) but the directed vertex separation is only of n.

For all 
, let 
 the mirror word of u.

Theorem 4

Consider a digraph  and a network . For all 
, we have 
 and thus 
.

Proof

Consider a word 
. We have
 To lighten the notation, let 
 and 
. For all , let . Moreover, when , let .

Claim 2

For all ,  is either empty, or an interval .

Proof

Suppose that . By definition of , for all , there exists an arc  such that . Thus, .

Now, consider the coordinate j which allows  to be in . There is an arc  such that . Clearly, for all , we have  and thus . As a result, . ■

Define the following coloring ⁎. First, for all coordinates i such that , we have ⁎. Let 
⁎
⁎ and for all  let 
. Then, let us define the colors  of the coordinates 
⁎
 by increasing order of  (i.e. if 
 then we define 
 and later 
). Each color  is defined as the smallest color not used by another coordinate  already defined. In other words, for all 
⁎
 and , we have . Note that this definition of  is valid because we have  by definition of d. For all , let .

Claim 3

For all distinct , if ⁎ then .

Proof

Suppose . Suppose that there exists . Thus, . By Claim 2, we have  because . Consequently, we cannot have . ■

Claim 4

Let 
⁎
. For all , such that , the following property is respected. For all 
, .

Proof

Let  with ⁎, 
 and . Let us prove that . If  this is obvious, so suppose that . Since 
⁎
, the sets  and  are not empty and  and  are well-defined. One can see that . Indeed, otherwise we would have ,  and, by Claim 3, . Since , we have . Consequently,  and thus . ■

Let  and . Now, let us define a network  which sequentializes h with the pattern u.

-
For all , 
 
.

-
For all , 
,
 
 
 
 

-
For all 
⁎
, 
.

All additions and subtractions are done modulo q.
Consider a permutation 
 which updates M and then  in the order u.

Consider the configurations 
 and 
. Let 
 be the configuration such that, for all , 
 
. Let us show that 
. For this purpose, we prove by induction that for all ,

First, if  then we have 
. Then, for all , let 
, and suppose that 
. Let us show that 
. Let 
 (i.e. ). First, suppose that 
⁎
. We have 
. Since 
⁎
, we know that  is empty and that . Consequently, there are no arcs  such that . Thus, 
 and 
. Now, suppose that 
 with . We have
 
 
 
 
 First, we know that 
 
 
. Second, for all 
 with , we have 
. Third and ultimately, for all 
 with , we have 
. Indeed, by Claim 4, for all 
, . Thus, 
 and 
.

Consequently, we have
 
 
 
 
 
 
 
 

Then, 
 and by induction 
. Consequently, 
 and 
. ■

Consider a last digraph parameter: the directed pathwidth.

Definition 7 Directed path decomposition

Consider a digraph . A directed path decomposition is a set of sets 
 such that

-
 
.

-
For all , 
.

-
For all , the following condition is respected. There are two integers  and 
 such that 
, 
 and 
.

The size of a directed path decomposition 
, is the size of the biggest set 
 with  minus one.
The directed pathwidth of the digraph D, denoted by , is the size of the smallest directed path decomposition. By Theorem 4.4 in [26], for all digraphs D we have . This gives immediately the following corollary.

Corollary 3

Consider a digraph  and a network . For all 
, we have 
.

8. Conclusion and future research
We have seen that for , we have
 Thus, for any fixed n, the limit of 
 and 
 when q tends to infinity is . It is an argument in favor of the still open conjecture made in [15] which states that for any n and q we have 
. It would be interesting to investigate a variant of the problem presented in this paper, where additional automata are forbidden, but where a coordinate could be updated several times. As shown in the introduction, we can simulate sequentially (but with repetitions) 
 by 
. However, we can find networks that cannot be simulated sequentially (even with repetition) without memory. One of them is the network  such that
 The first question is, for what sizes of networks and alphabets  can we find networks  that absolutely cannot be sequentialized without memory. The second one is, when a network can be sequentialized without memory, how many updates are necessary to do so?