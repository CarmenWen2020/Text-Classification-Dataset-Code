Abstract
Spiking neural P systems with rules on synapses (RSSNP systems) are a class of computation models which are inspired by the information processing and communication manner of neurons. In this work, we consider labeled RSSNP systems (lRSSNP systems), where each rule is assigned either with a label chosen from an alphabet Σ or with the empty label λ. A string over an alphabet is accepted by an lRSSNP system if the string is processed from left to right by the system in the sense that in a step only rules labeled with the processed symbol or with λ are used (not both), and the system reaches a final configuration in the moment when the whole string is processed. The set of all accepted strings is categorized as the restricted (computations are done with the application of the rules labeled with symbols from the given alphabet) control language and the unrestricted (rules labeled with λ are allowed in the computations) control language of the system. We study the language accepting power of lRSSNP systems using only standard spiking rules by comparing the family of control languages of lRSSNP systems with the families of languages in the Chomsky hierarchy. It is proved that restricted lRSSNP systems can accept no more than context-sensitive languages, and unrestricted lRSSNP systems can accept recursively enumerable languages.


Keywords
Membrane computing
Spiking neural P system
Rule on synapse
Controlled mechanism
Language

1. Introduction
The computation models in membrane computing are called P systems, which are a class of parallel and distributed computation models inspired by cellular structure, internal states, and biological behaviors [24], [31]. P systems have been deeply and widely investigated both at the theoretical studies [2], [23], [30], [32] and application aspects [5], [7], [8], [18], [25], [43]. In the area of membrane computing, abstracting computing notions from the manner in which neurons process information and communicate with each other by sending spikes along synapses, spiking neural P systems (SNP systems) were introduced by Ionescu et al. [9]. Since then there has been an extensive research study on SNP systems and their variants inspired by neurobiology or motivated by computer science and mathematics. For instance, considering the biological phenomenon of the synapse creation and deletion, SNP systems with structural plasticity and SNP systems with neuron division and budding are introduced [6], [12], [21]; inspired by the biological characteristic of different kinds of neurons, SNP systems with astrocytes and axon P systems are respectively introduced [22], [44]; with the mathematical motivation of extending the way of neuron spiking, SNP systems with anti-spikes and SNP systems with extended rules are proposed [11], [20]; with computer science motivation of the use of rule strategies, SNP systems with I/O mode and asynchronous SNP systems with local synchronization are respectively proposed [1], [37].

A variant of SNP systems, SNP systems with rules on synapses (RSSNP systems), is introduced and investigated by Song et al. [36], where spikes are contained in neurons yet spiking and forgetting rules placed on synapses. In RSSNP systems, neurons represented by nodes could contain a number of spikes, and they are connected through synapses (shown in diagrams by directed edges) with spiking rules and forgetting rules. Each application of the enabled spiking and forgetting rules on synapses means that a piece of information (described by the multisets of the single object a) is processed and transmitted from upstream to downstream neurons. In this way, the system evolves in a parallel and nondeterministic way. Many achievements have resulted from RSSNP systems: RSSNP systems have been proved to be Turing universal working in the maximally parallel strategy [36], [38]; they have been verified to be Turing computable in other working modes, such as the maximum spikes consumption strategy [35], the asynchronous mode [39]; they also have been shown to be able to solve hard problems in feasible time [2].

The language generating/accepting power of a computation model is an essential issue in the fields of text mining, language processing, linguistic context analysis, and so forth [13]. The label controlled mechanism was introduced in P system to investigate the language generating/accepting power [16], [26], [28], [45], where each rule is associated with a label which is a symbol from a given alphabet or the empty label λ, and a computation of such a system is guided by a string over the alphabet of labels. Actually, great achievements have been made in investigating the computation power of P systems with label controlled mechanism [2], [15], [19], [27], [42].

To our best knowledge, there is no result on the language accepting power of RSSNP systems. Motivated by the study of control languages associated with classical SNP systems, we investigate the language accepting power of lRSSNP systems. For a given alphabet Σ of rule labels, a string over Σ is accepted by an lRSSNP system if the string is processed from left to right by the system in the sense that at a step only rules labeled with that processed symbol or with λ are used (not both), and the system reaches a final configuration in the moment when the whole string is processed. The set of all accepted strings is called the control language of the system. Here, we specify two cases of computations: (1) restricted mode: at each step, only rules with the same non-empty label can be used. In other words, rules with the empty label λ are not allowed to be used; (2) unrestricted mode: at each step, rules either with the same non-empty label or with the empty label λ (not both) can be used. We study the family of control languages of lRSSNP systems comparing with the families of languages in the Chomsky hierarchy [10]. Specifically, we prove the following results. In the restricted mode, lRSSNP systems of degree one can characterize the finite languages; lRSSNP systems can accept regular languages when there is no limitation on the number of neurons in the systems; control languages of spiking neural P systems are not beyond context-sensitive languages. In the unrestricted mode, recursively enumerable languages can be accepted by lRSSNP systems.

2. Labeled spiking neural P systems with rules on synapses
In this section, we introduce the labeled spiking neural P systems with rules on synapses (lRSSNP systems).

The necessary notions and notations is recalled first. An alphabet is a finite set of symbols. Let V be such an alphabet. A nonempty string w over V is a multiset of those symbols from V, and the empty string is denoted by λ. The set of all nonempty strings over V is denoted by 
, and the one including the empty string λ is denoted by 
⁎
. A set of strings in 
⁎
 denoted by a regular expression E forms a language . For the formal definition of regular expressions and regular languages relating to formal language theory and membrane computing, readers could refer to the Handbook of Formal Languages [29] and the Oxford Handbook of Membrane Computing [25].

In what follows, we give the definition of labeled spiking neural P system with rules on synapses, which is obtained by suitably modifying the definition of spiking neural P system with rules on synapses defined in [36].

Definition 1

A labeled spiking neural P system with rules on synapses (lRSSNP system) with a hypothetical input tape is a construct

(I)
 is the object alphabet with a single symbol a denoting a spike;

(II)
Σ is the input alphabet consisting of a finite set of symbols;

(III)
  is a neuron with 
 spikes initially inside;

(IV)
syn is a set of synapses of the form 
; for every i (, ), where  represents a synapse from neuron 
 to neuron 
, and 
 is a set of rules on synapse  of the following forms:

(i)
spiking rule: 
, where E is a regular expression over the singleton , and the rule could be written as 
 if 
;

(ii)
forgetting rule: 
 with the restriction that 
  for any spiking rule 
 from 
;

(V)
F is the set of final configurations;

(VI)
 is the labeling function, in which the collection of rules on synapses is denoted by  
 
 (), and the labeling function assigns to each rule on synapses a symbol from Σ or the empty label λ;

(VII)
 indicates there exists a distinguished synapse 
 from neuron 
 to the environment.

For an input string 
⁎
 supposed to be placed on the input tape in the lRSSNP system Π, there is an input pointer initially pointing to the leftmost symbol in w. System Π reads symbols of the string w one by one from left to right. Each symbol in w to be processed is accessible to all the rules on synapse(s) in system Π. A rule on the synapse with label  can be applied if and only if (1) system Π currently processes the symbol b, and (2) the rule with label b is enabled. We consider here that either the rules on synapses with some label 
 or the rules with label λ (not both) are enabled in a step. For instance, assume that the pointer in system Π points to 
 at a given time. If some rules on synapses with label 
 are enabled with all rules on synapses with label λ not enabled, then enabled rules with label 
 are applied with the pointer moving to the next symbol; if none of the rules on synapses with label 
 is enabled and some rules on synapses with label λ are enabled, then the step is called a λ-step when the enabled rules with label λ are applied with the pointer staying at 
.

For  ∪ , a spiking rule of the form 
 with label b written as 
 is applied as follows. If neuron 
 contains k spikes such that 
 , then the rule 
 is enabled. When Π points to the symbol b, the use of the enabled rule 
 on the synapse  makes c spikes be consumed from neuron 
 and leaving  spikes in the neuron, and a spike produced is immediately sent to neuron 
. If a spiking rule on synapse 
 is applied, the produced spike will be emitted to the environment.

The forgetting rule 
 with  is applied by removing s spikes from neuron 
, when system Π processes symbol b with neuron 
 having exactly s spikes.

By convention, a global clock is assumed to record the time for the whole system. Under the tick of the global clock, the system works in parallel at the level of the system, yet sequential at the level of each synapse. In other words, if synapse  can use one of rules on synapse 
 with label b when system Π processes the symbol b, then a rule from 
 with label b should be used; if there exist two (or more) enabled rules on the synapse  with label b when system Π processes the symbol b, then only one enabled rule on synapse  with label b can be nondeterministically chosen to be applied in a step. A delicate problem arises when several synapses with applicable rules emerge from the same neuron. We adopt the method used in [36]. Assume that there are two cases if there are two enabled rules on two different synapses 
 and 
 with neuron 
 having 
 spikes 
 are enabled when Π processes 
: if 
, then both of the enabled rules are applied with the same number of spikes being consumed; if 
, then only one of the two enabled rules can be non-deterministically chosen.

The configuration 
 of system Π is given by the current number of spikes in each neuron, where the value of i-th component is exactly the number of spikes in neuron 
  at the given time t . Applying the rules labeled , the system Π transits to 
 from configuration 
 written as 
. A sequence of transitions from the initial configuration 
 is a computation.

For a given input string 
⁎
, if it is completely read and the computation of Π reaches a final configuration in the set of F, then we say that the input string 
⁎
 is accepted. The language accepted by a system Π is denoted by . The family of languages accepted by lRSSNP systems with m neurons using standard rules is denoted by 
, where the subscript m can be replaced by ⁎ if the bound is not specified, and the subscript λ can be deleted (
) if the system applies only rule(s) labeled b ,  at each step.

3. Computation power of restricted lRSSNP systems
In this section, we investigate the computation power of restricted lRSSNP system by comparing the family of languages accepted by restricted lRSSNP systems (
) with the families of finite (FIN), regular (REG), context-free (CF), and context-sensitive languages (CS). For the detailed notions and notations about languages, please refer to [10], [29].

Theorem 3.1

.

Proof

For a given alphabet Σ, let 
 (
⁎
), where 
  is the length of the string 
 over the alphabet, and for each ,
 
  In what follows, we construct a restricted lRSSNP system 
 shown in Fig. 1 to accept the finite language 
,
 where

•
Σ is the set of distinct symbols in the words in 
;

•
;

•
 with 
 ∪ 
 
;

•
;

•
 such that 
 and 
 for 
.

Fig. 1
Download : Download high-res image (13KB)
Download : Download full-size image
Fig. 1. lRSSNP system Π1 for accepting Lfin.

System 
 starts a computation from the initial configuration 
 with n applicable rules 
 . When the first symbol 
 of the i-th string in 
 is processed, 
 spikes are consumed from neuron 
 leaving 
 spikes in it, and the produced spike is emitted to the environment.

If 
, system 
 reaches the final configuration 
 at step 1, then it accepts the string 
.

If 
, then the system points to 
 with the rule on synapse 
 (
, as ) enabled at the end of the first step. The application of the rule on synapse 
 means that one spike is consumed from the neuron 
 leaving 
 spikes in it, and the rule on synapse 
 (
, as ) can be used in the next step. Thus, in each step j (
), the pointer points to symbol 
, and the labeled spiking rule 
 is applied. At step 
, the last symbol 
 of the string 
 is processed by using the rule on synapse 
 (
 for ). Then, 
 reaches the final configuration 
 ultimately. In this way, system 
 can accept each of the strings in 
.

Therefore the inclusion 
 holds, because 
 is arbitrary.

For a language 
, there is an lRSSNP system of degree one
 accepting L, where 
⁎
; each rule on synapses has a label 
 with 
, 
. The only neuron 
 consists of a finite number of spikes, and has at most one synapse 
 pointing to the environment. System Π with the synapse 
 can only use rules on synapses finite number of times. Therefore, . □

Theorem 3.2

⁎
.

Proof

Let 
 and 
 be a deterministic finite automaton accepting 
, where Q denotes the set of states; δ denotes the transition function; 
 denotes the initial state; 
 denotes the set of final states. Without loss of generality, assume that the alphabet is 
, and the states are 
  (
 denotes the initial state). Denote the total number of loops by 
 (a loop at state 
,  is of the form 
).

In what follows, we construct the restricted lRSSNP system 
 of degree 
 (with no output neuron) to simulate automaton D, shown in Fig. 2.
 where

•
Σ is the input alphabet in automaton D;

•
F is the set of final configurations where all neurons are empty except those neurons associated with final states 
 (
);

•
 such that 
 (the details of how the labeled rules are constructed are list below).

Fig. 2
Download : Download high-res image (126KB)
Download : Download full-size image
Fig. 2. The component of restricted lRSSNP system Π2 constructed for simulating a transition δ(qi,bk)=qj in automaton D.

Each state 
 having no loop in automaton D is associated with a single neuron 
, and the state 
 having 
 loops in automaton D is associated with 
 neurons 
.

If automaton D reaches the state 
 having no loop (resp., having a loop or more loops), the associated neuron 
 (resp., each of the associated neurons 
) contains a spike.

Assume that automaton D is at state 
 with pointer pointing to symbol 
. The simulation of the transition functions 
 is described as follows:

•
if both of the states 
  have no loop, the enabled rule with label 
 on synapses 
, 
 shown in Fig. 2 (a) will be applied;

•
if 
 having one or more loops, the enabled rules on synapses 
 and 
 are applied (the case of 
 having two loops is shown in Fig. 2 (b));

•
if the state 
 has a loop (or more loops) with the state 
  having no loop, all the enabled rules on synapses of the form 
 on synapses will be applied (the case that state 
 has one loop with state 
 having no loop is shown in Fig. 2 (c));

•
if each of the states 
  has one loop (or more loops), all the enabled rules on synapses of the form 
 will be applied (the case of each of states 
 having a loop is shown in Fig. 2 (d));

•
if the state 
 has a loop (or more loops) with the state 
  having no loop, all the enabled rules on synapses of the form 
 will be applied (the case that state 
 has one loop with state 
 having no loop is shown in Fig. 2 (e)).

For a string placed on the input tape of the system 
, the system processed each of the symbols in the string from left to right according to the derivation in the automaton D (the simulations mentioned above).

System 
 starts a computation from the initial configuration, where each of the neurons associated with state 
 contains one spike with all the other neurons empty. It evolves by applying the corresponding rules on synapses to execute the operations of automaton D. As automaton D reaches the final state 
, system 
 also reaches a final configuration in F where each of the neurons associated with the final state 
 contains a spike, except that all the other neurons are empty. Thus, 
, and the proof is completed. □

We illustrate the construction of restricted lRSSNP system to simulate a deterministic finite automaton by Example 3.1.

Example 3.1

Consider the deterministic finite automaton
 shown in Fig. 3, where

Fig. 3
Download : Download high-res image (16KB)
Download : Download full-size image
Fig. 3. Deterministic finite automaton D3.

The restricted lRSSNP system 
 is constructed to simulate automaton 
, shown in Fig. 4,
 where
 
, 
.

Fig. 4
Download : Download high-res image (57KB)
Download : Download full-size image
Fig. 4. Restricted lRSSNP system Π3 for simulating automaton given in Example 3.1.

The string 
 can be accepted by system 
, and the process is summarized in Table 1.


Table 1. The evolution for accepting the string 001.

step	current input symbol	applied labeled rules	configuration
0	-	-	〈1,1,0,0,0〉
1	0	((1,3),0:a → a)	
((2,4),0:a → a)	
2	0	((3,4),0:a → a)	
((4,3),0:a → a)	
3	1	((3,5),1:a → a)	
((4,5),1:a → a)	
System 
 starts the computation from the initial configuration  with neurons 
 and 
 containing one spike each. As 
 processes the first symbol 0, the enabled rules on the synapses ,  are applied by consuming one spike from each of the neurons 
 and 
, and the produced spikes are delivered to neuron 
 and neuron 
. Then, the rules on the synapses ,  can be applied with the second symbol 0 consumed by system 
. Processing the last symbol 1 in the string 001, system 
 with the configuration  transits to the final configuration  by applying the enabled rules on the synapses , . Hence, the string 001 is accepted by 
.

In general, for a word w accepted by the deterministic finite automaton 
, system 
 starts from the initial configuration , processes each symbol in word w from left to right according to the derivation in automaton 
. As automaton 
 comes to the final state 
, system 
 reaches the final configuration  after processing the last symbol in word w. In this way, 
.

The following result shows that restricted lRSSNP systems can even recognize more than the regular languages when the number of neurons is not limited.

Theorem 3.3

⁎
.

Proof

For the context-free language 
, we construct the restricted lRSSNP system 
 to accept it, shown in Fig. 5.
 where , , , , , 
.

Fig. 5
Download : Download high-res image (25KB)
Download : Download full-size image
Fig. 5. Restricted lRSSNP system Π4 for accepting L2 = {0n1n|n ≥ 1}.

The restricted lRSSNP system 
 with the initial configuration  processes the string 
 as follows. As system 
 reads the initial two 0s, the rules on the synapses , , 
 are applied twice, and the configuration transits to  with neuron 
 accumulating two spikes. Then, system 
 processes the first 1 in the string 0011, the rules on the synapses , , 
 are applied delivering one spike to each of the neurons 
, 
, the environment, respectively. System 
 with the current configuration  processes the last symbol 1 in the string 0011 by applying the rules on the synapse 
. After processing the string 0011, the system 
 reaches the final configuration . The detailed process is displayed in Table 2.


Table 2. The evolution of system Π4 in the case of accepting the string 0011.

step	current input symbol	applied labeled rules	configuration
0	-	-	〈1,1,1,0〉
1	0	((2,3),0:a → a)	
((3,2),0:a → a)	
((2,4),0:a → a)	
2	0	((2,3),0:a → a)	
((3,2),0:a → a)	
((2,4),0:a → a)	
3	0	((1,2),1:a → a)	
((1,3),1:a → a)	
((4,env),1:a+/a → a)	
4	1	((4,env),1:a+/a → a)	〈0,2,2,0〉
In general, for a 
, system 
 processes the initial 
 by applying the rules on the synapses  for n times, and neuron 
 receives n spikes. When system 
 processes the first 1, system 
 with the current configuration  transits to  by applying all rules on synapses labeled 1. Then, the rule on synapse  can be used for  times, as neuron 
 has  spikes. The system reaches its final configuration , when system 
 completes processing the string 
. Hence, 
 is accepted by the restricted lRSSNP system 
. □

The following result shows that the languages accepted by the restricted lRSSNP systems are not beyond context-sensitive languages.

Theorem 3.4

⁎
.

Proof

For a language , there is a restricted lRSSNP system
 accepting L, with the set of rules 
 on synapses, 
 with 
, .

To simulate 
, a linear bounded automaton (LBA) B (readers can refer to [29] for the detailed definition of linear bounded automaton) with  stacks is constructed, where the input string is placed on the first stack of automaton B, and other m stacks are used for recording the numbers of spikes in m neurons, respectively. Let 
 be the set of configurations in the system 
, and 
 is the initial configuration. The configurations of B can be described by 
 where 
 is the description of input string.

System 
 accepts L as follows. For a string 
, 
 starts a computation from the initial configuration 
, and it processes the first symbol 
 by using the rule on synapse with label 
. Since system 
 consists of no λ-step, 
 processes one symbol in each step. By using spiking rules, each neuron receives no more than  spikes when system 
 processes the j-th symbol 
 . Each neuron 
 , which initially contains 
 spikes, contains no more than 
 spikes after applying rules with labels 
 in turn, and the total storage of the neurons  
 
 is in proportion to the length of the input string .

All the operations in 
 can be simulated by automaton B with  stacks, whose tape head is not allowed to move outside a bounded region according to the length of the input string. If the string w is accepted by system 
, automaton B which recognizes context-sensitive languages starting from 
 will reach its final configuration 
 where 
.

Therefore, any language accepted by a restricted lRSSNP system is context-sensitive. □

4. Computation power of unrestricted lRSSNP systems
In this section, we consider the computation power of unrestricted lRSSNP systems, where λ-step is allowed, that is, all the rules labeled λ are applied in a step. When a λ-step is applied, the supposed input pointer remains still processing no input symbol. The family of languages accepted by unrestricted RSSNP systems (
) is proven to be the family of recursively enumerable languages.

Theorem 4.1

⁎
.

Proof

Assume that 
⁎
 is a recursive language, where 
 . Define a function  such that 
  and 
 for 
. For a language L, . For a family of languages , .

It is known that register machines can accept the family of Turing computable sets of numbers NRE [17]. In what follows, for a language L accepted by a register machine, we construct an unrestricted lRSSNP system Π to accept . In this way, we will have 
⁎
. By Turing thesis, 
⁎
 is obvious, so the theorem will be proved.

Let 
 be a deterministic register machine with m registers, a set of instruction labels I which includes the initial instruction label 
 and the halting instruction label 
, and a set of instructions H which is one-to-one bijection to I consisting of three forms instructions:

•
 (the number 
 stored in register r is increased by one, and then M goes to instruction 
);

•
 (the positive number 
 stored in register r is decreased by one, and then M goes to instruction 
; otherwise, M with  from instruction 
 goes to instruction 
);

•
 (the system halts when the halt instruction 
 is applied).

A string  if and only if there is a deterministic register machine 
 that accepts 
 in the following way. Register machine 
 starts a computation by applying instruction 
 with register 1 containing the value  and all the other registers empty; proceeds forward according to the guidance of the instruction labels; eventually halts at the halting instruction 
 [14].

To simulate deterministic register machine 
, we construct a unrestricted lRSSNP system 
, which is shown in Fig. 6, where &  is a special symbol marking the end of an input string;  ∪ ; F is the set of final configurations in which all neurons associated with instructions are empty, and will not receive any spike; neuron 
 is the output neuron having the rule on synapse 
.

Fig. 6
Download : Download high-res image (134KB)
Download : Download full-size image
Fig. 6. Unrestricted lRSSNP system Π for accepting language L ∈ RE.

The input string 
 is supposed to be placed on the input tape, and the input pointer starts from 
 moving left to right. Each of the neurons 
  is associated with register u, i.e., neuron 
 contains 
 spikes if register u stores number 
. Neuron 
 is used for temporary storing when the system is processing input symbol 
. To be specific, neuron 
 accumulating 2i spikes corresponds to the processing symbol 
.

Initially, only neuron 
 in system Π contains  spikes with other neurons empty. Each neuron 
 containing two spikes indicates that the simulation of instruction 
 starts. If neuron 
 receives two spikes, then the string w is accepted by system Π. The acceptance of the string w by system Π is as follows.

•
The Storage stage starts when system Π reads an input symbol 
 (): neuron 
 initiates with empty, and accumulates 2i spikes at the end of the stage;

•
the Update stage in sub-module 
 follows each Storage stage: the number of spikes in neuron 
 increases from 
 to 
, and then the number of spikes in neuron 
 becomes zero again;

•
the Storage stage and the Update stage are repeated as the input pointer moves from left to right;

•
The Acceptance stage starts when the pointer completes processing of the last symbol &.

Storing stage.

Assume that system Π processes symbol 
 at step t with each of the rules on synapses 
 (, ) being enabled. The application of the enabled rules on synapses means that  spikes are consumed from neuron 
 with  spikes retained, and a spike is delivered to each of the neurons 
.

The rules on the synapses 
, 
 are applied at each step τ (). With neuron 
 containing one spike, each of the rules on synapses  () and 
 is applied at step . Meanwhile, the rules on synapses 
, 
 are continuously applied sending spikes to each of the neurons 
, 
 from step  to step . At step , the storage of neuron 
 is completed, and neuron 
 accumulates 2i spikes. Hence, each of the neurons 
 contains two spikes, and neuron 
 contains one spike at the end of step .

Update stage.

At step , each of the rules on synapses 
  is applied delivering 2p spikes to neuron 
, and the rule on synapse 
 is also applied delivering a spike to neuron 
.

When neuron 
 contains 
 spikes, each of the rules on synapses 
  is applied for 
 times continuously delivering spikes to each of neurons 
 , and each of the rules on synapses 
 is applied from step  to step 
 continuously delivering spikes to neuron 
.

At step 
, the rule on synapse 
 is applied producing one spike to neuron 
, and neuron 
 accumulates ⁎
 spikes at the same time.

Since neuron 
 contains 
 spikes, each of the rules on synapses 
 and 
 is applied from step 
 to step 
. Correspondingly, each of the rules on synapses 
 and 
 is applied continuously sending 
 spikes to neuron 
 from step 
 to step 
.

At step 
, neuron 
 accumulates 
 spikes, and neuron 
 contains one spike with the rule on synapse 
 being applied. Hence, neuron 
 accumulates 
 spikes with the enabled rule on synapses 
 and 
 being applied in the next 
 steps. From step 
 to step 
, the rules on synapses 
 and 
 are applied making neuron 
 have 
 spikes inside.

Repeat.

At step 
, the rule on synapse 
 is applied delivering a spike to neuron 
, because neuron 
 contains only one spike. Then, each of the rules on synapses 
 and 
 is applied at step 
 sending a spike to each of neurons 
, 
, and then each of the rules on synapses 
 and 
 is applied at the next step sending two spikes to neuron 
, which contains  spikes again. Hence, system Π is ready to process the next symbol.

Acceptance stage.

When system Π finishes reading the string 
 with neuron 
 containing  spikes, the rule on synapse 
 is applied, where the symbol & is the mark of the end of input (w&), with neuron 
 containing  spikes.

The application of the rule on synapse 
 means that 2p spikes are consumed from neuron 
 leaving two spikes in it, and one spike is delivering to neuron 
. Then, the rule on synapse 
 is applied by consuming the spikes left in neuron 
, and delivering another spike to neuron 
.

Neuron 
 receives two spikes from neuron 
 with  spikes in neuron 
, the sub-module M of unrestricted lRSSNP system Π starts to simulate the acceptance process of the number  by the sub-module labeled M displayed in Fig. 6.

The sub-module M consists of ADD modules for instruction of the form 
 and SUB modules for instruction of the form 
. Each instruction labeled 
 in register machine 
 is associated with a sub-network in sub-module M shown in Fig. 6, which includes neuron 
, several auxiliary neurons 
, and rules on synapses with (label λ) between them. As neuron 
 contains two spikes, the ADD module (resp., SUB module) starts to simulate the corresponding instruction with label 
. If neuron 
 receives two spikes, system Π will eventually halt (reaching the final configuration) after the application of the rule on synapse 
. Readers could refer to [36] to get the detailed simulation of register machine 
. In this way, system Π accepts the string w by a controlled computation. □

5. Conclusion and discussion
Spiking neural P systems are a class of computation models inspired by nets of neurons. The investigation of the computation power of generating and accepting languages of spiking neural P systems is a basic topic [3], [4], [40], [41], [42]. In this work, we investigate a variant of spiking neuron P systems, spiking neural P systems with labeled rules on synapses (lRSSNP systems), where each rule on synapses is associated with a label by a labeling function; the label is either a symbol in a given input alphabet or λ. The trace of labels of applied rules during a computation forms a string. We investigated the power of lRSSNP systems for accepting languages.

The way of rule application here has a subtle difference from that previously used in P system [15], [27], [28], where enabled rules with label λ, or enabled rules with input symbol label, instead of both of these two cases, can be applied when the system processes an input symbol. Using the more strict way of applying rules, we proved that the restricted lRSSNP systems accept no more than context-sensitive languages (Theorem 3.4) and the unrestricted lRSSNP systems accept all recursively enumerable languages (Theorem 4.1).

It would be interesting to examine the computation power of lRSSNP systems for generating languages in terms of the traces of labels of applied rules during computations. Of course, the topic of accepting and generating control languages deserves to be investigated for other variants of tissue P systems, such as monodirectional tissue P systems with channel states [33] and monodirectional tissue P systems with promoters [34].