Abstract
An impressive hardness theory which can prove compression lower bounds for a large number of FPT problems has been established under the assumption that NP ⊈ coNP/poly. However, there are no problems in FPT for which the existence of polynomial Turing compressions under any widely believed complexity assumptions has been excluded. In this paper, we provide a technique which can be used to prove that some FPT problems have no small Turing compressions under the assumption that there exists a problem in NP which does not have small-sized circuits. These FPT problems, which include edge clique cover parameterized by the number of cliques, integer linear programming and a-choosability parameterized by some structural parameters, etc. have the property that they remain NP-hard under Cook reductions even if their parameter values are small. Moreover, a trade-off between the size of the Turing compression lower bound and the robustness of the complexity assumption is obtained. In particular, we demonstrate that these FPT problems have no polynomial Turing compressions unless every set in NP has quasi-polynomial-sized circuits, and have no 
 Turing compressions unless every set in NP has sub-exponential-sized circuits. Additionally, Turing kernelization lower bounds for these FPT problems are provided under some weaker complexity assumptions. Lastly, compression lower bounds for the above-mentioned FPT problems are proved under some complexity assumptions which are weaker than NP ⊈ coNP/poly, moreover, these results are proved under a method which is different from the previous hardness theory for compression lower bounds.

Previous
Next 
Keywords
Turing compression

Turing kernelization

Parameterized complexity

Compression

Kernelization

1. Introduction
Compression, of which kernelization is a special case, is a well-known and important research direction in parameterized complexity [1], [2]. It is a theoretical formalization of efficient preprocessing for NP-hard languages. A compression for a parameterized language 
⁎
 is a polynomial-time algorithm A that, given any instance , returns an instance 
 of a parameterized language 
, such that  if and only if 
, and 
 is bounded by a function . We say L has a polynomial compression if  is a polynomial function of k. We call A a kernelization for L if 
. Compression is a relaxation of kernelization, moreover, we can obtain polynomial compressions for some problems, but it is unknown whether they have polynomial kernelizations [3], [4]. Turing compression is a natural relaxation of compression. Turing compression for a parameterized language allows the polynomial-time algorithm to query an oracle for a parameterized language more than once for answers to instances of bounded size. As more and more techniques for compression were found, researchers realized that it was hard for some problems to obtain polynomial compressions using existing techniques. As a result, an impressive hardness theory for compression lower bounds has been established under the assumption that NP ⊈ coNP/poly and the polynomial hierarchy does not collapse [5], [6], [7], [8]. However, people subsequently found that some problems, for which can be excluded polynomial compressions by using the hardness theory, have polynomial Turing compressions. For example, Binkele-Raible et al. [9] found the first problem of this kind, namely leaf out-tree(k), which has no polynomial compressions unless NP ⊆ coNP/poly and the polynomial hierarchy collapses but has a polynomial Turing compression. Afterward, people found more problems that have polynomial Turing compressions but do not have polynomial compressions [9], [10], [11], [12], [13], [14], [15], [16], [17]. Thus, it becomes a very interesting problem whether a hardness theory for Turing compression, which is similar to the hardness theory for compression, can also be established. In order to understand which problems probably have no polynomial Turing compressions, Hermelin et al. [18] introduced a new complexity hierarchy for parameterized problems named WK/MK-hierarchy. The classes in the hierarchy are MK[i] and WK[i] for all , moreover, for all , these classes fulfill that MK[i] ⊆ WK[i] and WK[i]⊆ MK[]. They conjectured that all WK[1]-hard problems, which include Min ones d-SAT(k), binary NDTM halting(k), connected vertex cover(k), and clique(), etc. do not admit polynomial Turing compressions. However, the hardness theory for compression lower bounds cannot be applied to prove Turing compression lower bounds directly. Moreover, until now, there have been no methods which can be used to obtain Turing compression lower bounds for any FPT problems. Thus, the problem of refuting the existence of polynomial Turing compressions has received much attention from the parameterized complexity community, and it has been proposed as an open problem on different occasions [5], [9], [19], [20], [21].

In this paper, we demonstrate that the parameterized problems in a specific class  have no small Turing compressions under the assumption that there exists a problem in NP which does not have small-sized circuits. In particular,  is composed of all parameterized problems Q such that the sub-problem of Q, which consists of all the instances  of Q such that the parameter k is a small function, say logarithmic function, of the length of string x, is NP-hard under Cook reductions.1 The idea of the proof by contradiction for this result is as follows. Firstly, we show that if Q has a small Turing compression, then its sub-problem is Cook reducible to a set with low density. Secondly, we use the set with low density as advice to generate small-sized circuits to solve the sub-problem of Q. Thirdly, small-sized circuits are used to simulate the Cook reduction, from any problem in NP to the sub-problem of Q, to prove that any problem in NP has small-sized circuits. Then, we apply our result to eight FPT problems which have been proved to be elements of  in the previous papers. These FPT problems include edge clique cover parameterized by the number of cliques, edge biclique cover parameterized by the number of bicliques, complete width parameterized by the number of independent sets, a-choosability parameterized by treewidth, ILP feasibility parameterized by primal tree-depth, ILP feasibility parameterized by dual tree-depth, a-choosability deletion parameterized by treewidth, and QSAT3 parameterized by treewidth. In particular, for the first six problems, we prove that they have no polynomial Turing compressions unless every set in NP has quasi-polynomial-sized circuits and the exponential hierarchy collapses to its second level, and have no 
 Turing compressions unless every set in NP has sub-exponential-sized circuits and the non-uniform version of ETH fails. For the last two problems, we demonstrate that they have no polynomial Turing compressions unless every set in NP has polynomial-sized circuits and the polynomial hierarchy collapses to its second level. Additionally, Turing kernelization lower bounds for all these FPT problems are provided under some weaker complexity assumptions. Lastly, we modify the technique for proving Turing compression lower bounds slightly. By using the modified technique, compression lower bounds for the above-mentioned FPT problems are proved under some complexity assumptions which are weaker than NP ⊈ coNP/poly, moreover, our technique is different from the hardness theory for compression lower bounds proposed by the previous papers [5], [6], [7], [8]. In addition, a summary of obtained results and their exact positions in this paper can be found in Table 1.


Table 1. Summary of obtained results and their exact positions in this paper. More details for each result can be found in its corresponding corollary, for example, the result of edge clique cover(k) without polynomial Turing compressions under a specific complexity assumption can be found in Corollary 3.4. The first column lists the parameterized problems considered in this paper, where a-choosability del denotes a-choosability deletion, QSAT3 is the quantified Boolean formula problem with three alternating quantifiers ∃∀∃, and the parameters in the brackets include natural parameter k, treewidth , primal tree-depth 
, and dual tree-depth 
. In the fourth column, compression lower bounds for the first three problems have been proved based on the complexity assumption that NP ⊈ coNP/poly [22], [23], [24], [25], but this paper obtains these results based on weaker complexity assumptions.

Problem name	Without polynomial Turing compressions	Without polynomial Turing kernelizations	Without polynomial compressions
edge clique cover(k)	See Corollary 3.4	See Corollary 3.8	See Corollary 4.3
edge biclique cover(k)			
complete width(k)			
a-choosability			
ILP feasibility
ILP feasibility

a-choosability del	See Corollary 3.5	See Corollary 3.9	See Corollary 4.4
QSAT
Organization. Section 2 lays out some definitions and previous results needed in this paper. Section 3 focuses on the lower bounds for Turing compression and its variants. Section 4 focuses on the lower bounds for compression. Section 5 concludes and offers some final remarks.

2. Preliminaries
First, we give the formal definitions of Turing compression and its variants. Let us recall the definitions given by Binkele-Raible et al. [9].

Definition 1

[9]
A t-oracle for a parameterized problem Q is an oracle that takes as input  with , , and decides whether  in constant time.

Definition 2

[9]
A Turing compression for a parameterized problem Q of size  is an algorithm that, given an input  together with a -oracle for a parameterized problem 
, decides whether a given instance  in time polynomial in .

We sometimes abuse terminology by saying that Q has a  Turing compression if there is a Turing compression for Q of size . Let F be a class of functions, we say Q has an F Turing compression if Q has a  Turing compression for some . Note that the oracle problem 
 can be any parameterized problem, even an undecidable one. Let  be a complexity class, we say Q has a Turing compression in  if 
 is in . If we restrict the oracle problem to Q, then Turing compression becomes self-reduced Turing compression, which is called Turing kernelization.

A function  is a logarithmic function if . A function  is a polylogarithmic function if 
. A function  is a quasi-polynomial function if 
, where  is a polylogarithmic function. A function  is a sub-exponential function if 
 for all , i.e., 
. For a function , we use 
 to denote 
. A set 
⁎
 has  density if the cardinality of set  is not greater than  for all but finitely many n. We say S is F density or the density of S is F if S is  density for some . A polynomial density set is called sparse set. An advice function is a function 
⁎
. Let  be a complexity class and F be a class of advice functions. The class  is the collection of all sets A such that for some language  and some , . DTIME[] indicates all the problems which can be solved in  time. The complexity classes EXP, SUBEXP, and QP are the sets of decision problems that can be solved in 
 time, sub-exponential time, and quasi-polynomial time, respectively. Next, recall the definitions of Boolean circuits and circuit families.

Definition 3

Boolean circuits [26]
A Boolean circuit C with n inputs and one output is a directed acyclic graph with n sources and one sink, where the source is a vertex without incoming edges, the sink is a vertex without outgoing edges, and n is a natural number. Every non-source vertex, which is called gate, is assigned one of the three logical operations OR, AND, and NOT. The gates assigned NOT have fan-in, which is the number of incoming edges, equal to one and the gates assigned OR or AND have fan-in equal to two. The size of C, represented by , equals the number of vertices of it.

Let U be the vertex set of C and 
⁎
 be an input of C. We define a function  as follows. If u is the i-th input vertex, then  equals the value of the i-th bit of x. If u is a gate, then we recursively define  by applying the logical operation of u on the function values of the vertices connected to u. Let v be the output vertex of C. Then, the output of C on input x, represented by , equals .

Definition 4

Circuit families [26]
Let  be a function and 
 be a Boolean circuit with n inputs and a single output. If the size of 
 is at most  for every natural number n, then the sequence of Boolean circuits 
 is called an -sized circuit family.

For a language L, we say that L can be solved by an -sized circuit family or L has an -sized circuit family if there exists an -sized circuit family 
 such that for every 
⁎
,  if and only if 
. An -sized circuit family is also called a circuit family of  size or -sized circuits. We say that L has polynomial-sized circuit family if  is a polynomial function of n. Furthermore, that a problem has a quasi-polynomial-sized circuit family or a sub-exponential-sized circuit family is defined in the analogous way. SIZE[] indicates all the problems which can be solved by circuit families of at most  sized. Next, we introduce a theorem for later use.

Theorem 2.1

[26], [27]
For a decision problem L, if L can be solved by a polynomial-time algorithm, then L can be solved by a polynomial-sized circuit family.

The original proof for the theorem can be found in Theorem 4 of [27]. In addition, the proof can also be found in Theorem 6.6 of textbook [26]. Next, recall the definition of Tree decompositions.

Definition 5

Tree decompositions [1]
A tree decomposition of a graph  is a tuple 
, where T is a tree in which every vertex t of T is attached a vertex set 
, which is called a bag, such that the following statements hold:

(1) 
.

(2) For every edge , the tree T contains a vertex t such that 
.

(3) For every vertex , the subgraph induced in T by the vertex set 
 is connected.

The width of  is the size of the largest bag of  minus one. The treewidth of G, represented by , is the minimum width of any  of G. Let F be a Boolean formula in CNF or DNF. The primal graph of F is defined as follows. The vertices of the primal graph are the variables of F, and two vertices of the primal graph are connected if and only if both of them appear in a clause of F. Furthermore, the treewidth (or primal treewidth) of F, represented by , is the treewidth of the primal graph of F. Next, we refer to the paper [28] to give the definitions of primal tree-depth and dual tree-depth of a matrix. Assume that m is a integer and two different integers  are elements of set . Let 
 denote the primal graph of a matrix 
 which is defined as follows. The vertex set of 
 is 
. The edge set of 
 contains all edges 
 such that M contains a row whose entries in the i-th column and the j-th column are simultaneously not zero. The dual graph of M is 
, where matrix 
 is the transpose of M. The height of a rooted forest is the vertex number of the longest root-to-leaf path of the forest. For a graph , suppose  includes all the rooted forests F such that the vertex set of F equals V and for every edge , u is either an ancestor or a descendant of v in F. The tree-depth of G equals the minimum height of any rooted forest in . The primal tree-depth of M, which is denoted by 
, is the tree-depth of 
. The dual tree-depth of M, which is denoted by 
, is the tree-depth of 
.

We define that a problem L is  time Turing reducible to a problem 
 if there is an algorithm, which is allowed to query the oracle for 
 one or more times, that solves problem L in  time. (Note that querying the oracle one time counts only as a single computational step.) We say L is Cook reducible (or polynomial-time Turing reducible) to 
 if  is a polynomial function of n. For a problem Q, we say that Q is 
-hard for NP or NP-hard under Cook reductions if every problem in NP is Cook reducible to Q. We define that a problem L is  time many-one reducible to a problem 
 if there is a  time algorithm A such that for every string x,  if and only if 
, where  is the output of A given x as an input. We say L is Karp reducible (or polynomial-time many-one reducible) to 
 if  is a polynomial function of n. For a problem Q, we say that Q is 
-hard for NP or NP-hard if every problem in NP is Karp reducible to Q. We define that a problem L is  time bounded truth-table reducible to a problem 
 if there is a  time algorithm for transforming an input to problem L into a constant number of inputs to problem 
, such that the output for the original problem can be expressed as a function of the outputs for 
. We say L is polynomial-time bounded truth-table reducible to 
 if  is a polynomial function of n. For a problem Q, we say that Q is 
-hard for NP or NP-hard under polynomial-time bounded truth-table reductions if every problem in NP is polynomial-time bounded truth-table reducible to Q. Clearly, if a problem is 
-hard for NP, then it is 
-hard for NP; if a problem is 
-hard for NP, then it is 
-hard for NP.

Let Q be a parameterized problem and s be a function from  to . Assume that the -th slice of Q is 
, where integer  represents the length of string x. We also say that 
 is a sub-problem of Q. Note that 
 is a parameterized problem. Since the notion of the -th slice of a parameterized problem is important in this paper, we regard clique as an example to explain it. In the clique problem, we are given a graph G and an integer k as input, and the objective is to decide whether G contains a clique with k vertices. If function , then clique
 is the -th slice of clique. In the clique
 problem, the input is a graph G and an integer , and the objective is to decide whether G contains a clique with  vertices.

Next, we propose the problems and some of their properties used in this paper. Let G be a graph.  and  indicate the vertex set and edge set of G, respectively.  denotes the size of the graph G.

In the edge clique cover problem (ECC), we are given a graph G and an integer k as input, and the objective is to decide whether there exist k subgraphs 
 of G such that each 
 is a clique and 
. Theorem 1 in the paper [22] implies that ECC
 problem is 
-hard for NP for some constant c. In this paper, we use edge clique cover(k) to denote edge clique cover parameterized by the number of cliques, which is FPT [29] and NP-complete [30]. In addition, we define that the edge clique cover number of a graph G is the minimum integer k such that  is a yes instance of edge clique cover(k).

In the edge biclique cover problem (EBC), we are given a bipartite graph G and an integer k as input, and the objective is to decide whether there exist k complete bipartite subgraphs 
 of G such that 
. Theorem 6 in the paper [23] implies that EBC
 problem is 
-hard for NP for some constant c. In this paper, we use edge biclique cover(k) to indicate edge biclique cover parameterized by the number of bicliques, which is FPT [31] and NP-complete [30].

In the complete width problem (CW) [24], the input is a graph  and an integer k, and the objective is to decide whether there exist k independent sets 
, , such that the graph 
 obtained from G by adding some new edges between certain vertices inside the set 
, , is a complete graph. In this paper, we use complete width(k) to denote complete width problem parameterized by the number of independent sets k, which is FPT and NP-complete [24]. Moreover, the complete width of G is the minimum integer k such that  is a yes instance of complete width(k). Let 
 
 be the complement graph of G. Since the edge clique cover number of a graph G equals the complete width of 
 
 [24], [32],  is a yes instance of edge clique cover(k) if and only if 
 
 is a yes instance of complete width(k). Combine with Theorem 1 in the paper [22].2 We have that there exists a Karp reduction from 3-SAT to complete width(k) such that the input is an instance of 3-SAT with n variables and m clauses, and the output is an instance  of complete width(k) with  and . Thus, CW
 problem is 
-hard for NP for some constant c.

In the a-choosability problem, the input is a graph G and a list of colors  for every , where every  has a elements, and the objective is to decide whether there exists a color  for every  such that  for every . Marx and Mitsou [33] proved that a-choosability is 
-hard for NP for every constant  even if the treewidth of G is  for some constant c. In this paper, we use a-choosability() to indicate a-choosability parameterized by treewidth, which is FPT [34] and 
-complete [35], where the constant .

In the ILP feasibility problem, the input is a matrix 
, a vector 
, where m and n are integers, and the objective is to decide whether there exists a vector 
 such that . Let 
 and 
 indicate the largest absolute values of entries in A and b, respectively. Let t indicate the length of the instance. Knop et al. [36] proved that ILP feasibility is 
-hard for NP even if 
 as well as 
 are constants, and the dual tree-depth of A is  for some constant c. Eisenbrand et al. [28] proved that ILP feasibility is 
-hard for NP even if 
 as well as 
 are constants, and the primal tree-depth of A is  for some constant c. In this paper, ILP feasibility(
) represents ILP feasibility parameterized by primal tree-depth, where 
 and 
 are constants; and ILP feasibility(
) denotes ILP feasibility parameterized by dual tree-depth, where 
 and 
 are constants. Moreover, both ILP feasibility(
) and ILP feasibility(
) are FPT [28], [37], [38] and NP-complete [39].

In the a-choosability deletion problem, we are given a graph G and an integer k as input, and the objective is to decide whether there exists a set X of at most k vertices of G such that G with the vertices from X deleted is a-choosable. Marx and Mitsou [33] proved that a-choosability deletion is 
-hard for NP for every constant  even if the treewidth of G is  for some constant c. In this paper, we use a-choosability deletion() to indicate a-choosability deletion parameterized by treewidth, which is FPT and 
-complete [33], where the constant .

In the QSAT3 problem, we are given a Boolean formula  in CNF as input, and the objective is to decide whether it is true that . Pan and Vardi [40] proved that QSAT3 is 
-hard for NP even if the treewidth of QSAT3 formula is , where n and c are the input size and a constant, respectively. In this paper, we use QSAT3() to indicate QSAT3 parameterized by treewidth, which is FPT and 
-complete [40].

Note that all these FPT problems have a special property that they remain NP-hard under polynomial-time bounded truth-table reductions when their parameter values are small. Turing kernelization lower bounds for these problems can be obtained directly by using this special property. To see how the property can be helpful for getting Turing kernelization lower bound, let us consider the edge clique cover(k) as an example. Assume that edge clique cover(k) has a polynomial Turing kernelization, this means that the Turing kernelization solves the problem. Now, let us consider an instance  of ECC
, where n is equal to . The Turing kernelization decides whether  is a yes instance of ECC
 in 
 time, moreover, the function of the oracle of the Turing kernelization is to answer whether a generated string, whose length is 
, is a yes instance of edge clique cover(k). Replacing the oracle of the Turing kernelization by a straightforward dynamic programming algorithm [22] for edge clique cover(k) gives us an 
 time algorithm for ECC
. Furthermore, we know ECC
 is NP-hard under polynomial-time bounded truth-table reductions. Thus, we have that every problem in NP can be solved in quasi-polynomial time. Therefore, we prove that edge clique cover(k) has no polynomial Turing kernelizations unless every problem in NP can be solved in quasi-polynomial time. However, this method only fits for proving lower bound for Turing kernelization but does not fit for proving lower bound for Turing compression, because in the Turing compression the oracle can be queried for answers to a problem that is undecidable, and it can not be replaced by any algorithms in this case. Turing compression lower bounds and its variants will be discussed in the following section.

Additionally, the polynomial hierarchy is a subset of EXP, and all the above-mentioned problems are in the polynomial hierarchy, so these problems are also in EXP. In this paper, we assume that the running time  of any algorithm is larger than the input length n. Generally speaking, for a parameterized problem, the parameter k is smaller than the string length . Thus, we suppose that k is smaller than  in this paper.

3. Lower bounds for Turing compression and its variants
In this section, we first prove Turing compression lower bounds for some FPT problems under the assumption that there exists a problem in NP which does not have small circuits. Then, Turing kernelization lower bounds for these problems are obtained under some weaker complexity assumptions.

3.1. Lower bounds for Turing compression
There is a well-known statement which states that if a problem is polynomial-time Turing reducible to a sparse set, then the problem has a polynomial-sized circuit family [41], [42]. Moreover, the Karp–Lipton theorem [42], which proves that the polynomial hierarchy collapses if an NP-complete problem has a polynomial-sized circuit family, implies that any NP-complete problem is unlikely polynomial-time Turing reducible to a sparse set. In this subsection, we firstly generalize this well-known statement, then get a connection between the generalized statement and Turing compression to prove some FPT problems have no small Turing compressions.

Note that NP-complete sets are proved to be not sparse under the assumption that NP ≠ P (Mahaney's theorem) [43] and even to be exponentially dense [44] under the assumption that NP ⊈ coNP/poly, however, the reductions used in these theorems are not Turing reductions that make unrestricted queries. Therefore, it seems not easy to apply them to obtain Turing compression lower bounds for parameterized problems. (In fact, they are employed to obtain many-one compression lower bounds for parameterized problems in section 4 of this paper.)

Lemma 3.1

Let L be a decision problem. If L is  time Turing reducible to a set of  density, then L can be solved by a circuit family 
 of 
 size.

Proof

In the first step, we prove that if L is  time Turing reducible to a set of  density, then L∈ P/F, where F is made up of all advice functions s such that 
. Suppose that L is Turing reducible to a set S such that the running time of the Turing reduction A is bounded by a function  of n, and that for all m, the number of stings of length m in the set S is bounded by a function . For each , let the strings of length m in S be 
, where . Define 
, and let 
. For each m, we have 
, so 
. Now we construct a polynomial-time algorithm 
 to solve the language 
 and . On an input , where  and 
, 
 simulates the Turing reduction A on the input x step by step except that when A produces a string y and calls the subroutine which decides if , 
 instead searches y in the string  to decide if . Since A runs in time  on the input x, the length of the string y cannot be longer than . Moreover,  contains all 
 up to . Thus, we can always correctly decide if  by searching  in time . Thus, a step of A that calls its subroutine can be simulated by 
 in time . Since A runs in time bounded by , the algorithm 
 on input  runs in time  and decides if x is in L. Since , the running time  of 
 is bounded by a polynomial function of the length  of the input , i.e., the language 
 and  is in P. This proves that L∈ P/F.

In the second step, we prove that if L∈ P/F, where F is made up of all advice functions s such that 
, then L can be solved by a circuit family of 
 size. Suppose that L∈ P/F. Then there is an advice function  such that 
 and  can be decided by a polynomial-time algorithm. Based on Theorem 2.1, the language 
 can be solved by a polynomial-sized circuit family. Since the length of 
 and that  is assumed in the preliminaries, the length m of the string  is 
. Therefore, the size of the circuit 
, which solves all the inputs  with length m, is 
. Note that for all instances x of equal length for L, the corresponding instances  of 
 have the same length thus are handled by the same circuit 
. Therefore,  is accepted by the corresponding 
 in 
 if and only if . Additionally,  is a fixed value for all x of length n. Now if we assign the value  to the corresponding inputs in 
, we get a new circuit 
 with input length  such that 
 accepts x if and only if . Therefore, the new circuit family 
 accepts the language L. As we explained above, the size of the circuit 
 is 
, moreover, the size of its corresponding circuit 
 in 
 is obviously bounded by a polynomial function of the size of the circuit 
. Therefore, the size of the circuit 
 is 
. This proves that L is solved by a circuit family of 
 size. □

Recall that 
 is the -th slice of parameterized problem Q, where s be a function from  to , and integer n represents the length of string x. A function  is a sub-exponential function if 
 for all . For later use, the proof of Lemma 3.2 is broken into two parts.

Lemma 3.2

Let Q be a parameterized problem. If there is an algorithm A that, given an input  together with a -oracle for a parameterized problem B, decides whether  in time , then 
 has a circuit family 
 of 
 size, where .

Proof

Part 1: We prove that if the algorithm A exists, then 
 is Turing reducible to a set of 
 density in  time. Without loss of generality, suppose that . (Otherwise, since the density of any problem is bounded by 
, the result is obtained directly.) On the input , each query  made by A on the -oracle for B satisfies that  and . As a consequence, the length  of the queries made by A on the oracle B is bounded by . Now, the algorithm A together with the oracle can be used to solve the problem 
, as follows: on an instance  of 
, where  and , algorithm A decides in  time if  and the length of the queries  made by algorithm A on the oracle B is bounded by . Let 
. Consider the following set: 
. Since 
, we always have that . So the above set is well-defined. Then, we define a new oracle language 
⁎
 as the union of all the 
. That is, 
⁎
.

The density of set 
⁎
 is 
: the set of elements of length m in 
⁎
 is the set 
, which are all of the form 
, where  is a yes instance of B whose length is bounded by 
. Since the total number of yes-instances of length bounded by 
 in B is bounded by 
 for some constant c, the total number of elements of length m in 
⁎
 is bounded by 
, which is in turn bounded by 
.

Now we construct a new algorithm 
 by modifying the algorithm A so that 
 is an  time Turing reduction from problem 
 to set 
⁎
 with 
 density. The algorithm 
 on an input  simulates the algorithm A on input  step by step except that, when A makes a query  to the oracle B, the algorithm 
 then first lets , and decides if the string 
 is in set 
⁎
 by using a subroutine which solves 
⁎
. To justify this step, note that by the above discussion, on the input , every query  made by A on oracle B has its length bounded by 
. Therefore, 
⁎
 if and only if . Thus, the new algorithm 
 correctly accepts the language 
. Since each step of A can be simulated by  steps of 
, and since A runs in  time, we conclude that the Turing reduction 
 runs in  time. This gives an  time Turing reduction from problem 
 to the set 
⁎
 of 
 density.

Part 2: The result of the first part implies that 
 is  time Turing reducible to a set of 
 density, where  is the input length of 
. Thus, 
 is  time Turing reducible to a set of  density, where 
. According to Lemma 3.1, 
 can be solved by a circuit family 
 whose size is 
 = 
. □

This lemma implies that if 
 is a hard problem, say NP-hard, then functions T, g, and s cannot be small at the same time. Otherwise, the hard problem 
 has a small circuit family. If the function T is a polynomial function, then the lemma is related to Turing compression. In particular, we are especially interested in the case that g is also a polynomial function, which is related to polynomial Turing compression.

It is widely believed that NP-complete problems have no small circuit families. Moreover, Karp and Lipton [42] proved that if an NP-complete problem has a polynomial-sized circuit family, then the polynomial hierarchy (PH) collapses to the second level 
. Sengupta enhanced the result that collapses PH to 
 (This result is included in the paper of Cai [45]), and it became a stronger version after Cai [45] proved that 
. Buhrman and Homer [46] proved that if an NP-complete problem has a circuit family of quasi-polynomial size, then the exponential hierarchy (EXPH) collapses to the second level 
. Pavan et al. [47] obtained a stronger collapse by proving that the same assumption leads to that the exponential hierarchy collapses to 
, which is a subset of 
 [47]. Furthermore, the non-uniform version of ETH assumes that CIRCUIT-SAT does not have 
-sized circuits [48], [49].

Theorem 3.3

Let Q be a parameterized problem such that 
 is NP-hard under Cook reductions.

(1) If Q has a polynomial Turing compression and 
, then every set in NP has a quasi-polynomial-sized circuit family and 
.

(2) If Q has a polynomial Turing compression and 
, then every set in NP has a polynomial-sized circuit family and 
.

(3) If Q has a 
 Turing compression and , then every set in NP has a circuit family of sub-exponential size, and the non-uniform version of ETH fails.

Proof

Suppose that Q has a  Turing compression. Then, there is an algorithm A together with a -oracle for a parameterized problem B, such that on an input , A decides if  in  time, where  is a polynomial function of . Thus, problem 
 can be solved by a circuit family 
 of 
 size based on Lemma 3.2.

Consider a problem L in NP, given a string z of length m, to decide if . Since 
 is NP-hard under Cook reductions, there is a polynomial-time Turing machine M together with an oracle for 
 which can decide that if . Furthermore, the length of every query y made by M on the oracle is 
. Therefore, every query on the oracle can also be solved by the circuit 
 of 
 size. Moreover, M can be simulated by a circuit family 
 of 
 size [27]. As a result, Turing machine M together with the oracle for 
, which can solve the problem L, can be simulated by a new circuit family 
 of  size, where  = 
 = 
. (Each circuit 
 is a proper combination of the circuit 
 and every circuit 
 which decides if y is in 
.) This means that every problem in NP has a circuit family of  size.

For statement (1), g is a polynomial function, and s is a polylogarithmic function, thus  is a quasi-polynomial function of m, moreover, the polynomial hierarchy collapses to 
. For statement (2), g is a polynomial function and 
, thus  is a polynomial function of m, furthermore, the polynomial hierarchy collapses to 
. For statement (3), s is a logarithmic function and 
, thus  is bounded by 
 for all . Accordingly, it easily implies that the non-uniform version of ETH fails. □

In fact,  can also be many other functions, such as 
, which may have other applications. For all the problems in the following corollaries, their properties, which are given in the preliminaries, meet the requirements of Theorem 3.3, so the following two corollaries hold. (Recall that if a problem is NP-hard under polynomial-time bounded truth-table reductions, then it is also NP-hard under Cook reductions.)

Corollary 3.4

For problems edge clique cover(k), edge biclique cover(k), complete width(k), a-choosability, ILP feasibility
, and ILP feasibility
, the following statements hold.

(1) Unless every problem in NP has a quasi-polynomial-sized circuit family and EXPH 
, these problems have no polynomial Turing compressions.

(2) Unless every problem in NP has a circuit family of sub-exponential size, and the non-uniform version of ETH fails, these problems have no 
 Turing compressions, where t indicates the parameter of each problem.

We know that edge clique cover(k), edge biclique cover(k), complete width(k) have 
 kernelizations [24], [29], [31], and have no 
 compressions unless NP ⊆ coNP/poly [22], [23], [24]. Before this paper, one might ask if we can obtain smaller Turing compressions for these problems, say 
 or even polynomial size. However, Corollary 3.4 gives a negative answer to this question.

Corollary 3.5

Unless every problem in NP has a polynomial-sized circuit family and PH 
, a-choosability deletion and QSAT
 have no polynomial Turing compressions.

3.2. Lower bounds for Turing compression in EXP
Recall that EXP is the set of all decision problems that are solvable in 
 time. A parameterized problem Q has a Turing compression in EXP if the oracle problem of the Turing compression is in EXP. Note that for a parameterized problem Q∈ EXP, if Q has no  Turing compressions in EXP, then Q has no  Turing kernelizations.

Lemma 3.6

Let Q and L be parameterized problems, where L is in EXP. If there is an algorithm A that, given an input  together with a -oracle for L, decides whether  in time , then 
 can be solved in time 
.

Proof

For the algorithm A, suppose that each of its query on -oracle for L is string  of length at most . Since L is in EXP, there is another algorithm 
 that solves L in time 
, where m is the length of the input to L. In the execution of the algorithm A on input , we replace each query  made by A on the oracle by a call of the algorithm 
 on input , we can obtain the answer of the query  in time 
. Therefore, we get a new algorithm that accepts the parameterized problem Q and runs in time 
. As a result, 
 can be solved in time 
. □

Theorem 3.7

Let Q be a parameterized problem such that 
 is NP-hard under Cook reductions.

(1) If Q has a polynomial Turing compression in EXP and 
, then NP ⊆ QP.

(2) If Q has a polynomial Turing compression in EXP and 
, then NP = P.

(3) If Q has a 
 Turing compression in EXP and , then NP ⊆ SUBEXP.

Proof

Suppose that Q has a  Turing compression in EXP. Then, there is an algorithm A together with a -oracle for a parameterized problem L in EXP, such that on an input , A decides if  in time , where  is a polynomial function of . Thus, 
 can be solved in 
 time based on Lemma 3.6.

Consider a problem 
 in NP. We need to decide whether a string 
, where 
. Since 
 is NP-hard under Cook reductions, there is a Cook reduction from 
 to 
. This means that there is a polynomial-time algorithm 
 together with an oracle for 
 which can decide that whether 
. Moreover, the length of every query made by 
 on the oracle is 
. Replacing the oracle with the 
 time algorithm for 
 gives us an 
 time algorithm for 
. This means that every problem in NP can be solved in  time.

For statement (1), g is a polynomial function and s is a polylogarithmic function, so  is bounded by a quasi-polynomial function. Thus, every problem in NP can be solved in quasi-polynomial time. For statement (2), g is a polynomial function and 
, so  is bounded by a polynomial function. Thus, every problem in NP can be solved in polynomial time. For statement (3), s is a logarithmic function and 
, so  is bounded by a sub-exponential function. Thus, every problem in NP can be solved in sub-exponential time. □

Note that NP ⊆ QP implies that NEXP = EXP based on the Proposition 2 in [46], and NP ⊆ SUBEXP implies the ETH fails [50]. All the problems in the following corollaries, their properties, which are given in the preliminaries, meet the requirements of Theorem 3.7. Furthermore, a parameterized problem Q∈ EXP has no  Turing kernelizations if Q has no  Turing compressions in EXP, so we have the following corollaries.

Corollary 3.8

For problems edge clique cover(k), edge biclique cover(k), complete width(k), a-choosability, ILP feasibility
, and ILP feasibility
, the following statements hold.

(1) Unless NP ⊆ QP, these problems have no polynomial Turing compressions in EXP, and have no polynomial Turing kernelizations.

(2) Unless NP ⊆ SUBEXP, these problems have no 
 Turing compressions in EXP, and have no 
 Turing kernelizations, where t indicates the parameter of each problem.

Corollary 3.9

Unless NP = P, a-choosability deletion and QSAT
 have no polynomial Turing compressions in EXP, and have no polynomial Turing kernelizations.

In fact, for a-choosability deletion and QSAT
, where the constant , there are no 
 Turing compressions unless every problem in NP has sub-exponential-sized circuits, and there are no 
 Turing kernelizations unless every problem in NP can be solved in sub-exponential time. Since the proofs of this fact are similar to the other proofs in this section, we do not give the details here.

4. Lower bounds for compression
Lemma 4.1

Let Q be a parameterized problem. If there is a  time algorithm A that, given any instance , returns an instance  of a parameterized language B, such that  if and only if , , and , then 
 is many-one reducible to a set of 
 density in  time.

Proof

The proof goes in the same logic as the first part of the proof of Lemma 3.2. However, compare to the Turing reduction, many-one reduction only produces one query string at the end of the reduction, and returns yes if and only if the query string is in the target problem. Thus under the assumption of the current lemma, the only difference between algorithm A and 
 is the last step. When A produces  for B, the algorithm 
 then first lets , and produces the string 
 for 
⁎
. Therefore, the time complexity of 
 of the current lemma is . □

Theorem 4.2

Let Q be a parameterized problem such that 
 is 
-hard for NP.

(1) If Q has a polynomial compression and 
, then NP ⊆ QP ∩ coNP/poly.

(2) If Q has a polynomial compression and 
, then NP = P.

(3) If Q has a 
 compression and , then NP ⊆ SUBEXP ∩ coNP/poly.

Proof

Suppose that Q has a  compression, then 
 is Karp reducible to a set B of 
 density based on Lemma 4.1. Since 
 is 
-hard for NP, B is 
-hard for NP. According to Theorem 5 of [51], we have NP ⊆ DTIME[], where 
. (Since B is 
-hard for NP, the function  in Theorem 5 of [51] is a constant here.)

For statement (1), g is a polynomial function and s is a polylogarithmic function, thus both  and  are quasi-polynomial functions of n. First,  being a quasi-polynomial function of n implies NP ⊆ QP. Secondly, the fact that 
 is Karp reducible to a set B of quasi-polynomial density implies that every problem in NP is polynomial-time bounded truth-table reducible to a set B of quasi-polynomial density. Then, we have NP ⊆ coNP/poly based on the Theorem 3.9 of [44]. Consequently, we have NP ⊆ QP ∩ coNP/poly. For statement (2), g is a polynomial function and 
, thus  is a polynomial function of n and NP = P. For statement (3), s is a logarithmic function and 
, then both  and  are sub-exponential functions of n. First, that  is a sub-exponential function of n implies NP ⊆ SUBEXP. Secondly, that 
 is Karp reducible to a set B of sub-exponential density implies that every problem in NP is polynomial-time bounded truth-table reducible to a set B of sub-exponential density. Then, we have NP ⊆ coNP/poly based on the Theorem 3.9 of [44]. Consequently, we have NP ⊆ SUBEXP ∩ coNP/poly. □

For all the problems in the following corollaries, their properties, which are mentioned in the preliminaries, meet the requirements of Theorem 4.2, so the following corollaries hold.

Corollary 4.3

For problems edge clique cover(k), edge biclique cover(k), complete width(k), a-choosability, ILP feasibility
, and ILP feasibility
, the following statements hold.

(1) Unless NP ⊆ QP ∩ coNP/poly, these problems have no polynomial compressions.

(2) Unless NP ⊆ SUBEXP ∩ coNP/poly, these problems have no 
 compressions, where t indicates the parameter of each problem.

Corollary 4.4

Unless NP = P, a-choosability deletion and QSAT
 have no polynomial compressions.

Note that, for edge clique cover(k), edge biclique cover(k), and complete width(k), there have been some results of compression lower bounds for them, however, Corollary 4.3 improves these results as follows. Assume that NP ⊈ coNP/poly, edge clique cover(k) and complete width(k) are proved to have no polynomial compressions [24], [25]. Corollary 4.3 obtains these results under a weaker complexity assumption that NP ⊈ QP ∩ coNP/poly. Assume that NP ⊈ coNP/poly, edge clique cover(k) and edge biclique cover(k) are proved to have no 
 compressions [22], [23]. Corollary 4.3 obtains these results under a weaker assumption that NP ⊈ SUBEXP ∩ coNP/poly. Moreover, this paper applies a new method to obtain the results in the corollaries, which is different from the previous techniques that can be used to prove compression lower bounds. Additionally, as far as we know, the result of Corollary 4.4 is the first time to obtain compression lower bounds based on the assumption that NP ≠ P, which is the gold-standard and applied in many algorithmic contexts. (Only kernelization lower bounds and strict kernelization lower bounds can previously be obtained under the assumption that NP ≠ P [22], [52], [53], [54].)

5. Conclusion and final remarks
This paper has proposed a technique for proving that eight natural problems in FPT, most of which are very important and are widely studied, have no polynomial Turing compressions and no super-polynomial Turing compressions under some reasonable complexity assumptions. Additionally, polynomial Turing kernelization lower bounds and super-polynomial Turing kernelization lower bounds for these problems are also proved under some weaker complexity assumptions. Moreover, the technique provides a new way, which is different from the hardness theory in [5], [6], [7], [8], to obtain compression lower bounds under some complexity assumptions which is more robust than the assumption that NP ⊈ coNP/poly. The reader may feel that the paper just strengthens some results of classic computational complexity in a straightforward way and combines them properly to form the lower bounds technique. However, I think this loses sight of the contribution of the paper. The contribution and new observation of the paper are mainly in the aspect of parameterized complexity, more specifically, the paper provides the first technique to prove that some FPT problems have no polynomial Turing compressions under some widely believed complexity assumptions, which has been elusive for quite a while.

Unfortunately, with our proposed technique, it seems difficult to refute the existence of polynomial Turing compressions for the problems in the WK/MK-hierarchy [18], in which many FPT problems are conjectured without polynomial Turing compressions, such as Min ones d-SAT(k), binary NDTM halting(k), connected vertex cover(k), and clique(), etc. Moreover, people are more interested in knowing whether these problems have polynomial compressions. Additionally, our technique does not provide any direct insight for proving Turing compression lower bounds for the problems in WK/MK-hierarchy, since these parameterized problems are not NP-hard when their parameter values are restricted to polylogarithmic functions of the input lengths. (Otherwise, NP is a subset of QP.) Thus, the following problem is still open.

Open problem: Can we prove that the problems in WK[i]-complete and MK[]-complete, where , have no polynomial Turing compressions under some widely believed complexity assumption?