Abstract
In this paper, we deal with the synthesis problem for Halpern and Shoham's interval temporal logic HS extended with an equivalence relation ∼ over time points (HSImage 1 for short). The definition of the problem is analogous to that for . Given an HSImage 1 formula φ and a finite set  of proposition letters and temporal requests, it consists of determining, whether or not, for all possible valuations of elements in  in every interval structure, there is a valuation of the remaining proposition letters and temporal requests such that the resulting structure is a model for φ.

We focus on the decidability and complexity of the problem for some meaningful fragments of HSImage 1, whose modalities are drawn from the set  interpreted over finite linear orders and . We prove that, over finite linear orders, the problem is decidable (Ackermann-hard) for Image 2 and undecidable for . Moreover, we show that if we replace finite linear orders by , then it becomes undecidable even for . Finally, we study the generalization of Image 2 to Image 3, where k is the number of distinct equivalence relations. Despite the fact that the satisfiability problem for Image 3, with , over finite linear orders, is already undecidable, we prove that, under a natural semantic restriction (refinement condition), the synthesis problem turns out to be decidable.

Keywords
Synthesis
Interval temporal logic
Decidability
Complexity

1. Introduction
Since its original formulation by Church (for the monadic second-order theory of one successor ) [1], the synthesis problem received a lot of attention from the computer science community. A solution to the problem was provided by Büchi and Landweber in [2]. In the following decades, a number of extensions and variants of the problem have been investigated, see, for instance, [3], [4]. In particular, the synthesis problem for (point-based) temporal logic has been addressed in [5], [6], [7].

In this paper, we formally state the synthesis problem for interval temporal logic and give some basic results about it. To begin with, we focus on some meaningful fragments of Halpern and Shoham's modal logic of time intervals HS [8]. Then, we address the same problem for (fragments of) HS extended with 
 distinct equivalence relations 
 over time points, denoted by HS

Image 4
. The considered fragments allow us to define a first raw border between decidable and undecidable HS (resp., HS
Image 4
) fragments with respect to the synthesis problem.
The emerging picture is quite different from the one for the synthesis problem for . In [3], Rabinovich proves that the decidability of the satisfiability problem for  extended with a unary predicate P entails the decidability of its synthesis problem, that is, the synthesis problem for a monadic second-order theory is decidable if and only if its underlying theory is decidable. Here, we show that this is not the case with interval temporal logic.

We focus our attention on two fragments of HS, namely, the logic 
 of Allen's relations meets, begun by, and begins, and the logic 
, that extends 
 with a modality for the Allen relation met by. The satisfiability problem for 
 over finite linear orders and  has been shown to be ExpSpace-complete [9], while the same problem for 
 has been proved to be decidable (Ackermann-hard) over finite linear orders, and undecidable over  [10].

In this paper, we show that there is a significant blow up in computational complexity moving from the satisfiability to the synthesis problem for 
 over finite linear orders: while the former is ExpSpace-complete, the latter is Ackermann-hard. As a matter of fact, such an increase in the complexity is paired with an increase in the expressive power of the logic: one can exploit universally quantified variables, that is, proposition letters under the control of the environment, to constrain the length of intervals in a way which is not allowed by 
. Moreover, we prove that the synthesis problem for 
 over  as well as that for 
 over finite linear orders are both undecidable.

Then, we study the extension of 
 with a special pre-interpreted proposition letter ∼, to be read as an equivalence relation over points, denoted by

Image 2
, and we show that its synthesis problem is decidable.1 Such a result may seem surprising, as, over finite linear orders, the satisfiability problem for 
 and that for
Image 2
, belong to two quite different complexity classes: they are ExpSpace-complete and Ackermann-hard, respectively [9], [10]. However, while for 
 there is drastic blow-up in complexity when moving from satisfiability to synthesis, the complexity of the synthesis problem for
Image 2
is the same as that of the satisfiability one (Ackermann-hard). Finally, we generalize
Image 2
by replacing ∼ by a finite set of equivalence relations 
, and we prove that, under a natural semantic restriction on them, the synthesis problem for the resulting logic
Image 3
, over finite linear orders, remains decidable.
This paper is a considerably revised and extended version of [11]. The main improvements to the original contribution can be summarized as follows:

(i)
all proofs have been fully worked out, and explanations have been added to help the reader in understanding their structure and arguments;

(ii)
the possibility of adding more than one equivalence relation to 
, interpreted over finite linear orders, preserving the decidability of the synthesis problem (under some natural semantic restrictions) is explored.

The rest of the paper is organized as follows. In Section 2, we introduce syntax and semantics of the logic 
 and of its fragments. Moreover, we give some examples of significant properties that can be expressed in 
 and its fragments. In Section 3, we define the synthesis problem for interval temporal logic, focusing on the considered fragments. The problem is systematically addressed in the following two sections. In Section 4, we focus on the HS fragment  and its extensions 
 and 
, and we provide some basic hardness and undecidability results. In particular, we prove that the synthesis problem for 
, over finite linear orders, is Ackermann-hard. Decidability of such a problem is shown in Section 5. To make the proof easier to follow, we provide an intuitive account of its structure, and we illustrate in detail the exploited technical machinery. In Section 6, we investigate the synthesis problem for 
 extended with one or more equivalence relations. It is already known from the literature that the satisfiability problem for 
 extended with one equivalence relation, over finite linear orders, is decidable, while it becomes undecidable if two or more equivalence relations are added. Exactly the same holds for the synthesis problem. We introduce a natural semantic restriction, called refinement condition, that suitably constrains the behavior of the equivalence classes in the presence of more than one equivalence relation, and show that it allows us to make the synthesis problem for
Image 3
(for any ), over finite linear orders, decidable. In Section 7, we provide an assessment of the work done and we outline future research directions. In particular, we summarize the achieved results, and compare them with existing ones for satisfiability checking of the same HS fragments (Table 3). Proofs of a more technical nature are given in Appendix A.
2. The logic 
 and its fragments
In this section, we provide syntax and semantics of the fragments of HS we are interested in. The maximal considered fragment is 
, which features unary modalities , 
, , and 
 for Allen's binary ordering relations meets, met by, begun by, and begins, respectively [12].

Formally, let  be a countable set of proposition letters, formulas of 
 are built up from proposition letters in  by using Boolean connectives ∨ and ¬, and unary modalities from the set 
. Formulas of the fragment 
 are defined in a similar way. We will often make use of shorthands like 
,  (the same for 
),  (the same for 
), , and , for some .

As for the semantics, for any 
, we denote by 
 the set of all closed intervals  over the prefix  of , if 
, or over , if . Formally, 
. Allen's relations meets, met by, begun by, and begins are formally defined as follows (a graphical account of the four relations is given in Fig. 1):

;

;

;

.

We define an interval structure as a pair of the form , where 
 and 
 is a function mapping intervals to sets of proposition letters. Formulas are interpreted over an interval structure  and an initial interval 
 as follows:
•
 if and only if , for any ;

•
 if and only if ;

•
 if and only if 
 or 
;

•
 if and only if there exists  such that ;

•
 if and only if there exists  such that ;

•
 if and only if there exists  such that ;

•
 if and only if there exists  such that .

Fig. 1
Download : Download high-res image (43KB)
Download : Download full-size image
Fig. 1. A graphical representation of the interval relations 
 
, and 
 
.

We say that an 
 formula φ is satisfiable over the class of finite linear orders if and only if there exists 
 and an interval structure  such that . Analogously, an 
 formula φ is satisfiable over  if and only if there exists an interval structure  such that .

We will often make use of the following formulas. The formula  (π for short) holds over all and only the singleton intervals . From now on, we will refer to intervals of the form  as “point intervals” or simply “points”. Similarly, the formula  (
 for short) holds over the unit-length intervals of any discrete linear order, e.g., over the intervals over  of the form . The formula  (abbreviated 
π
) forces ψ to be satisfied over an adjacent-to-the-right non-point interval, that is, an interval , with . Finally, to state that a formula ψ holds universally, that is, over all intervals in 
, it suffices to force the formula  (abbreviated ) to hold over the interval .

Given a formula φ of 
 (or of one of its fragments or variants), we denote by 
 the (finite) set of all and only the proposition letters that occur in φ. We define the closure 
 of a formula φ as the set of all its sub-formulas and all their negations (we identify  with ψ,  with , and so on). Moreover, we denote by 
 (resp., 
) the set 
 
 
 (resp., 
 
 
) of all and only the temporal requests (resp., temporal universal requests) in 
. We denote by 
 the set 
. The following subsets of 
 (resp., 
) will also be considered: 
 (resp., 
 ), 
 
 
 (resp., 
 
 ), 
 (resp., 
 ), and 
 
 
 (resp., 
 
 ).

We conclude the section by providing two more complex examples of temporal conditions that can be expressed in 
. They will turn out to be useful in the encodings of Section 4.

The formula  is true over an interval  if and only if both the following conditions hold: (i) there exists a point  which is the left endpoint of an interval 
 where ψ holds (this is forced by the first conjunct of the formula), and (ii) if a point z strictly inside  begins an interval where ψ holds, then any point 
, with 
, is not the left endpoint of an interval where ψ holds (this is forced by the second conjunct of the formula).

In Fig. 2.(a), we display a model that satisfies , while in Fig. 2.(b) we show how the formula can be exploited to detect a “violation” of the property it expresses. By way of contradiction, let us assume that  holds over the interval  and there exist two distinct points 
 which are the left endpoints of an interval where ψ holds. In Fig. 2.(b), we depict such a scenario and we observe that a contradiction is generated since both  and its negation  are required to hold on the interval 
.

Fig. 2
Download : Download high-res image (117KB)
Download : Download full-size image
Fig. 2. An interval structure that satisfies 〈BA!〉 (a) and one that does not satisfy it (with a graphical account of the violation of the imposed uniqueness condition) (b).

The second example is the formula 
, which holds over an interval  if and only if ψ holds over its maximal proper prefix  and does not hold over any non-maximal one, that is, over any interval 
, with 
. The semantics of the derived modality 
 is illustrated in Fig. 3: the formula 
 holds over the interval  if and only if  is the “smallest” interval starting at x, where ψ holds. It can be easily checked that, among all the intervals with the same left endpoint x, 
 may hold over at most one, that is, it cannot be the case that 
 holds over two intervals  and 
 with 
.

Fig. 3
Download : Download high-res image (51KB)
Download : Download full-size image
Fig. 3. An interval structure showing how the property stated by formula 
 can be fulfilled.

The property imposed by formula  has been already extensively used in previous works on the satisfiability problem for HS fragments to encode counter machines (see, for instance, [10]). The condition expressed by formula 
 will turn out to be useful in Section 4, where, in the context of a game-theoretic characterization of the synthesis problem for , it prevents one of the two players from “cheating” (as a matter of fact, such a condition cannot be forced in the satisfiability setting for ).

3. The synthesis problem for interval temporal logics
We are now ready to define the synthesis problem for the interval temporal logic 
 (the definition immediately transfers to all its fragments) with respect to the class of finite linear orders and to (any linear order isomorphic to) . We adopt the classical formulation of the synthesis problem that represents it as a turn-based game between two players, namely ⋄ and □. As it commonly happens, at every turn, the moves of ⋄ and □ are played on the current point of the given order (see, for instance, [13]). However, here each move involves all the intervals ending at the current point of the linear order. The main difference between the point and the interval setting is that, at each turn, the number of intervals whose labeling must be defined grows by 1 with respect to the previous turn, to say, at the first turn the players act on the interval , at the second turn they play on the pair of intervals  and , at the third turn they play on the three intervals , and , and so on. In the following, we show how to linearize interval structures in order to suitably play on them.

To begin with, we introduce the notion of run.

Definition 1

Let A be a finite alphabet. An 
⁎
-run ρ is a sequence of words 
 (finite) or 
 (infinite) such that, for every , 
.

Notice that ρ is a word on an infinite alphabet, whose elements are words on the finite alphabet A. We separate each word from the next one by means of the separator ||, which does not count as a symbol. We denote the set of all finite (resp., infinite) 
⁎
-runs with 
⁎
⁎
 (resp., 
⁎
). Let . An example of a (finite) 
⁎
-run is 
. By Definition 1, the length of a (component) word in an 
⁎
-run ρ is determined by its position, e.g., at position 3 in an 
⁎
-run we can only have words belonging to the language 
. We define the length of ρ, denoted by , as the number of words in ρ, e.g., 
. The total number of elements of A in ρ is equal to 
 
 (if 
⁎
, we assume ).

To deal with 
⁎
-runs, we introduce two indexing methods. We write  to refer to the word in the y-th position of ρ, e.g., 
, while we write  to refer to the x-th symbol in the y-th word of ρ, e.g., 
. Moreover, we will often make use of the slicing notation  to denote the prefix  of ρ of length . It is immediate to see that each prefix of an 
⁎
-run is still an 
⁎
-run. Given the tight constraint on the length of component words in an 
⁎
-run, for every 
⁎
-run ρ, the set of all admissible indexes of the form  is 
. As a consequence, a run ρ can be naturally used to represent the complete “labeling” of an interval structure, where  can be viewed as the labeling of the interval  and  as the enumeration of the labellings of all the intervals , that is, all the intervals with right endpoint y. Finally, in the case of finite 
⁎
-runs ρ, we write 
 for , that is, we denote by 
 the maximum index to be used for accessing the words in ρ.

The representation of the labeling of interval structures by means of suitable 
⁎
-runs is pictorially illustrated in Fig. 4.(a)-(c). In Fig. 4.(a), we give an example of a labeling of an interval structure, where a subset of 
 is associated with each interval  to be interpreted as the subset of 
 that hold over it. As shown in Fig. 4.(c), any such subset is represented by the symbol  in the 
⁎
-run, the finite alphabet of ρ being the set of all subsets of 
. In Fig. 4.(b), we introduce an alternative (equivalent) representation of an 
⁎
-run/interval structure, called compass structure. Such a representation has been successfully used in the interval temporal logic literature to prove the decidability of the satisfiability problem for a number of HS fragments (see, e.g., [9], [14]). In the compass structure corresponding to an 
⁎
-run ρ, the symbols of ρ become the labels of the points in the second octant of the Euclidean plane . As depicted in Fig. 4.(c), for every 
,  becomes the label of point . Notice that in such a representation word  is mapped into row y of the octant.

Fig. 4
Download : Download high-res image (456KB)
Download : Download full-size image
Fig. 4. Alternative representations of a run (c) as an interval model (a) and a compass structure (b), plus an example of a 
-response strategy (d).

Formally, let φ be an 
 formula and ρ be a 
⁎
-run. The interval structure induced by ρ is 
, where 
 for all 
. The next definition specifies the successful runs for player □, that is, the player whose goal is to satisfy φ.

Definition 2

Let φ be an 
 formula and ρ be a run in 
⁎
⁎
⁎
. We say that ρ is successful iff (i) 
 and (ii) for all 
 and , 
.

Notice that requirement (ii) of Definition 2 is only needed for formulas in 
, as for proposition letters in 
 it is satisfied by definition of 
. The next definition of minimal successful run turns out to be useful to distinguish between the finite and the infinite synthesis problem.

Definition 3

Let φ be an 
 formula and ρ be an infinite successful run in 
⁎
. We say that ρ is limit-successful iff for every , the finite run  is not successful.

Basically, an infinite run is limit-successful if it is successful and any of its finite prefixes is not a finite successful run for φ, that is, φ is satisfied by ρ taken as a whole infinite run.

Let us now introduce the notion of strategy.

Definition 4

Let φ be an 
 formula and 
. A 
-response strategy is a (partial) function 
⁎
⁎
 such that:

-
 is defined on pairs 
⁎
⁎
 where 
 is a finite word on the finite alphabet 
, with ;

-
for every pair  on which  is defined, , that is, the finite word  on the finite alphabet 
 has exactly the same length as w.

Definition 4 can be viewed as the specification of the rules of a possibly infinite game between ⋄ (spoiler) and □ (duplicator), which are responsible of the truth values of proposition letters and temporal requests in 
 and 
, respectively. The game can be informally described as follows. It starts with  as the time point to be initially considered. At each round, being y the time point under consideration, for each , ⋄ chooses the elements of 
 belonging to the labeling of the interval , and □ replies by choosing the elements of 
 belonging to it. At the end of the round, the labeling of all the intervals ending at y is completely defined, and the game proceeds with the definition of the labeling of all the intervals ending at . According to the above-defined linearization, these intervals are represented by the sequence of words in  of the run ρ that the game is building.

An example of how a strategy  works and of a possible round of the game is given in Fig. 4.(c)-(d). Let us assume 
 (and thus 
). At round 4, ⋄ plays 
. Such a move is represented by the bits of the q-labeled intermediate row in Fig. 4.(c). □ replies according to , taking into account both the 
-run , generated by the previous rounds, and the move 
. Such a reply consists of the word 
 belonging to 
. It is pictorially depicted by the rightmost bit matrix in Fig. 4.(d). The component-wise union of words 
 and 
, that is, 
, gives the labeling over 
 of the intervals ending at 4 in the current interval structure. This concludes round 4 of the game.

We are now ready to formalize the above notions. Let 
,  be a 
-response strategy, and 
 be a run in 
⁎
. The induced run 
 is recursively defined as follows
 	
 	
  The induced language of  is defined as 
⁎
. We say that the strategy  is (i) winning if and only if for every 
, ρ is successful, and (ii) finitely winning if and only if for every 
, ρ is successful, but not limit-successful.

Notice that item (i) defines the notion of winning strategy in the case of infinite games, while item (ii) defines such a notion in the case of finite games. As a matter of fact, item (ii) is obtained from item (i) by imposing the limit-successful condition to be false. More precisely, by negating such a condition, item (ii) requires every -generated infinite run in 
 to feature a finite prefix which is a successful run.

We conclude the section by formally stating the interval-based synthesis problem.

Definition 5 The synthesis problem

Let φ be an 
 formula and 
. The (finite) interval-based synthesis problem is the problem of determining whether or not there exists a (finite) winning 
-response strategy 
⁎
⁎
. If the answer is affirmative, we say that the pair 
 is a positive instance of the (finite) interval-based synthesis problem.

For the sake of brevity, from now on we will just say that  is a positive instance when it is clear from the context. Moreover, we will refer to the problem of Definition 5 simply as the (finite) synthesis problem.

As a final remark, we would like to observe that the synthesis problem for 
 is more general than the satisfiability/validity one. It is indeed immediate to see that, given an 
 formula φ, the satisfiability (resp., validity) problem for φ can be reduced to the problem of checking whether or not  (resp., 
) is a positive instance.

4. Undecidability and hardness reductions
In this section, we state some basic (un)decidability and hardness results for the synthesis problem for the fragment , as such results can be naturally transferred to its super-fragments 
, 
 and 
, and their extensions with one or more equivalence relations.

In the following, we first show how to capture both the finite and infinite error-incrementing computations as well as the acceptance condition of a counter automaton  (see [15]) by means of suitable  formulas; then, by exploiting these formulas, we prove three fundamental results:

1.
the language of finite words accepted by error-incrementing finite computations of  is not empty if and only if 
⁎
 is a positive instance of the finite synthesis problem, where 
⁎
 is the  formula 
⁎
, with 
 and 
⁎
 respectively encoding the error-incrementing computations and the acceptance condition;

2.
the language of infinite words accepted by error-incrementing computations of  is not empty if and only if 
 is a positive instance of the synthesis problem, where 
 is the  formula 
, with 
 and 
 respectively encoding the error-incrementing computations and the acceptance condition;

3.
the language of finite words accepted by error-free finite computations of  is not empty if and only if 
⁎
, is a positive instance of the finite synthesis problem, where 
⁎
 is the 
 formula 
⁎
, with 
 (resp., 
⁎
, 
) the  (resp., , 
) formula encoding the error-incrementing computations (resp., the acceptance condition, the surjectivity condition).

By exploiting existing results about error-incrementing and error-free computations of counting automata [15], [16], [17], [18], [19], we have that: (i) the complexity of the finite synthesis problem for  is Ackermann-hard (it follows from item 1); the synthesis problem is undecidable for any HS fragment that has  as its sub-fragment (it follows from item 2); the finite synthesis problem is undecidable for any HS fragment that has 
 as its sub-fragment (it follows from item 3). It is worth pointing out that all the reductions are given in an interval-based synthesis setting where the “power” of ⋄ is minimal (
 means that ⋄ only controls one proposition letter). The only way to further weaken the power of ⋄ is indeed to put 
, which leads us to the satisfiability problem.

We now introduce the notion of counter automata with ϵ-transitions and zero testing.

Definition 6

A counter automaton  is a tuple 
, where 
 is the number of distinct counters, A (the alphabet) and S (the states) are finite sets, , 
, and . Moreover, for each 
, if 
, then .

For the sake of brevity, from now on we will denote the set  with 
. Given a counter automaton 
, a configuration of  is a pair 
, with  and 
. Then, the set of all possible configurations is 
.

Let us now introduce two transition relations between configurations of a counter automaton 
. The first one, called (error-free) transition relation, for each  preserves the values of the counters not affected by the operation of δ. The latter one, called error-incrementing transition relation can introduce arbitrary incrementing errors after any transition  has been fired. Formally speaking, let 
 (resp., 
) 
 be the (error-free) transition relation (resp., error-incrementing transition relation), then for every pair 
 we have 
 (resp., 
) if and only if there exists 
 such that one of the following conditions holds:

•
there exists a tuple 
 such that 
 (resp., 
), for each , with , and 
 (resp., 
);

•
there exists a tuple 
 such that 
 (resp. 
), for each , with , and 
 (resp., 
);

•
there exists a tuple 
 such that 
 (resp., 
), for each , and 
.

Let 
⁎
 (resp., 
⁎
) be the reflexive and transitive closure of 
 (resp., 
). We focus on the following decision problems for counter automata.
Definition 7 Decision problems for counter automata

Let 
 be a counter automaton.

1.
The finite non-emptiness problem (resp., non-emptiness error-incrementing problem) is the problem of deciding whether or not there exists a configuration 
, with , such that 
⁎
 (resp., 
⁎
).

2.
The non-emptiness problem (resp., non-emptiness error-incrementing problem) is the problem of deciding whether or not there exists a state  and an infinite sequence of 
 vectors 
 such that 
⁎
 (resp., 
⁎
), and 
⁎
 (resp., 
⁎
), for every .

The four problems of Definition 7 have been extensively studied from a decidability/complexity perspective. The following theorem summarizes the results that are of our interest.

Theorem 1

[15], [16], [17], [18], [19] Let 
 be a counter automaton. It holds that:

1.
the finite non-emptiness problem for  is undecidable (
-complete);

2.
the finite non-emptiness error-incrementing problem for  is decidable (Ackermann-hard);

3.
the non-emptiness problem for  is undecidable (
-complete);

4.
the non-emptiness error-incrementing problem for  is undecidable (
-complete).

Let us now describe the  formula 
 that captures, by means of the set of its models (both finite and infinite ones), all the possible error-incrementing computations of .

We start with a short account of the key ingredients of the encoding; then, we introduce the formula that forces the encoding to be error-incrementing by exploiting the synthesis setting.

Let 
 be a counter automaton. Every configuration is encoded by means of a sequence of consecutive unit intervals. The first unit interval of any such sequence is labeled with a proposition letter s and a symbol a, where  is the state of the configuration and 
 is the symbol processed in the state. A unary encoding of the values of the counters is then provided by putting the right number of unit intervals labeled with 
, for all , in between such a unit interval and the next unit interval labeled with a state in S, say, 
. More precisely, for all , the value of the counter 
 in the configuration beginning at s is given by the number of 
-labeled unit intervals between s and 
. Moreover, on 
-labeled intervals we allow the presence of two special proposition letter, namely, 
 and 
, to respectively denote an increment and decrement of a counter.

The appropriate behavior of such proposition letters is forced by the following formulas:

-
 constrains all the above-mentioned proposition letters to hold over unit intervals only, and all unit intervals to be labeled with at least one of such proposition letters:
  
 
 

-
 constrains intervals to be labeled with at most one proposition letter  and at most one proposition letter 
; moreover, an s-labeled interval cannot be a 
-labeled interval, for any , but it must be an a-labeled interval, for some 
:
  
 
 
  
 
  
 

-
 constrains intervals to be labeled with at most one proposition letter 
, for ; moreover, an interval cannot be labeled with both 
 and 
, and if an interval is labeled with 
 or 
, then must be also labeled with a proposition letter 
, for some :
 
 
 

An example of how configurations can be expressed as interval structures, which are models of  formulas, is shown in Fig. 5, where configurations  and 
 are represented.

Fig. 5
Download : Download high-res image (47KB)
Download : Download full-size image
Fig. 5. Encoding of an error-incrementing computation in : (s,a,++,1,s′)∈Δ.

Let us now encode the constrains between two consecutive configurations. For the sake of readability, in the following we make use of the symbol S as a shorthand for the formula  
 . To start with, we impose the satisfaction of the relation Δ by means of the following formula:
 Before providing the sub-formulas 
, 
, and 
, we give a short account of the structure of the formula 
. Its outermost operator is a logical implication whose antecedent holds only over intervals that cover two consecutive configurations. In particular, any interval of this kind shares its left-endpoint with an s-labeled interval, strictly contains exactly one s-labeled interval, and its right-endpoint is either the left-endpoint of an s-labeled interval or is the last point of the model. An example of an interval that satisfies 
 is given in Fig. 5.

The antecedent of 
 triggers one of the formulas 
, 
, and 
. These formulas guarantee that any pair of consecutive configurations in our encoding “respects” at least one transition in Δ. Formulas 
, 
, and 
 are defined as follows:
  
 

If  is a 
-satisfying interval and 
 holds over it, a transition of the form , 
 is fired. Such a case is encoded by forcing the following conditions: (i) the beginning unit-interval  satisfies s and a, (ii) the unique s-labeled unit interval 
 strictly contained in , that is, 
, satisfies 
, (iii) there is not a unit interval starting between x and 
, that is, belonging to the source configuration, that satisfies 
 (this basically says that a transition of type −− is not fired from the configuration beginning in x), (iv) there exists a unique unit interval between 
 and y that satisfies 
, and (v) such a unit interval also satisfies 
. An example of a 
 satisfying-interval that triggers 
 is shown in Fig. 5.
  
 

If  is a 
-satisfying interval and 
 holds over it, a transition of the form , 
 is fired. This is encoded by imposing the following conditions: (i) the beginning unit-interval  satisfies s and a; (ii) the unique s-labeled unit interval 
 strictly contained in  (
) satisfies 
; (iii) there exists a unique interval starting strictly between x and 
, that is, belonging to the source configuration, that satisfies 
, and (iv) such a unit interval also satisfies 
. An example of a 
 satisfying-interval that triggers 
 is given in Fig. 6.
 

Fig. 6
Download : Download high-res image (44KB)
Download : Download full-size image
Fig. 6. Encoding of an error-incrementing computation in : (s,a,−−,1,s′)∈Δ.

If  is a 
-satisfying interval and 
 holds over it, a transition of the form , 
 is fired. Such a case is encoded by forcing the following conditions: (i) the beginning unit-interval  satisfies both s and a, (ii) the unique s-labeled unit interval 
 strictly contained in  (
) satisfies 
, and (iii) each interval starting strictly between x and 
, i.e., each interval belonging to the source configuration, satisfies neither 
 nor 
, which amounts to say that the value of the counter 
 in the source configuration is equal to 0. An example of a 
 satisfying-interval that triggers 
 is shown in Fig. 7.

Fig. 7
Download : Download high-res image (44KB)
Download : Download full-size image
Fig. 7. Encoding of an error-incrementing computation in : (s,a,0?,1,s′)∈Δ.

It is worth pointing out that, in the above encoding, the role of 
 and 
 with respect to transitions is symmetric: when an 
 transition is fired, 
 labels the newly introduced 
 unit-interval in the 
-configuration, while when an 
 transition is fired, 
 labels the 
 unit-interval in the s-configuration must not be transferred to the 
-configuration.

Let us now deal with problem of connecting 
-labeled intervals, which are labeled with neither 
 nor 
, belonging to consecutive configurations. Such a connection is established by exploiting a proposition letter p. In every model  (resp., ) of the final formula, the evaluation of p encodes a partial function 
, with 
 for all x, such that if 
 is defined, then 
 (and thus 
), and 
 is defined if and only if (i) 
, (ii) 
, (iii) 
, for some , (iv) x does not belong to (the encoding of) the last configuration (if M is finite), and (v) there exists exactly one point 
, with 
, such that 
.

Intuitively, given two consecutive configurations, 
 maps the left endpoints of 
-labeled unit intervals that do not satisfy 
 to the left endpoints of 
-labeled unit intervals that do not satisfy 
. Examples of such a (required) behavior of p are shown in Fig. 5, Fig. 6, and Fig. 7. The formulas that force the above-specified constraints on p are the following ones:
 
 
 The first formula constrains each p-labeled interval  (i) to begin with a 
-labeled unit interval  and to be followed by a 
-labeled unit interval , that is,  and  are both labeled with 
, (ii) to contain exactly one s-labeled interval 
, with 
, (iii) to exclude 
 from the labeling of its beginning unit interval , (iv) to exclude 
 from the labeling of the following unit interval , and (v) to exclude p from the labeling of any of its (proper) prefix (this allows us to conclude that 
 is well defined).
 

The second formula constrains each 
-labeled unit interval  that does not belong to the last configuration (if M is finite) and does not satisfy 
 to begin a p-labeled interval. Its outermost operator is an implication, whose antecedent is triggered on an interval  if (and only if) (i)  is a 
-labeled interval, (ii)  is an s-labeled interval, (iii) there is exactly one s-labeled interval 
, with 
, and (iv) 
 does not hold on . Observe that if the antecedent is triggered on an interval , then, by conjuncts (ii) and (iii), any p-labeled interval starting at x and ending (strictly) after y would violate the condition, imposed by 
, that p must “cross” one and only one s-labeled interval. Then, p must hold over a proper prefix of , and hence the consequent .

From the definition of 
, it easily follows that 
 and 
. All the proposition letters and temporal requests introduced so far are assumed to be under the control of □, that is, they belong to 
, and thus all the considered conditions can actually be dealt with in the framework of  satisfiability (the additional capabilities of the synthesis setting are not needed). This holds for formulas 
 and 
 as well, which simply guarantee that 
 is a well-defined total function from 
 to 
. However, such a requirement is too weak to correctly encode error-incrementing and error-free computations.

In order to model error-incrementing computations, 
 must be forced to be injective. Such a condition can be expressed by preventing two distinct p-labeled intervals  and 
 from sharing the same right endpoint, that is, if 
, then 
. An example of a possible violation of this requirement is given in Fig. 8 (ρ case). Such a property cannot be encoded in  in the satisfiability setting. However, it can be forced in the synthesis framework by means of the following formula:
 In order to understand how 
 forces 
 to be injective (in the synthesis setting), consider the example given in Fig. 8. Let us assume that 
 and that, at round z of computation/play 
, □ has replied to ⋄ by imposing p to hold over both interval  and interval , with . Then, at the next round  of the computation/play 
, ⋄ may force q to hold on  and ¬q on . This begs the question: if 
 must be satisfied, which truth value can be assigned to r by □ on interval ? If □ puts r in 
, then, from 
, it follows that ¬r must hold over , as both ¬q and 
 hold over , and thus r does not belong to 
 (contradiction). However, if □ forces r not to belong to 
, then from 
, it follows that r must hold over , as both q and 
 hold over , and thus r belongs to 
 (contradiction). This allows us to conclude that if, at a given round z, □ associates p with more than one interval ending in z, he loses the game at the next round.2

Fig. 8
Download : Download high-res image (41KB)
Download : Download full-size image
Fig. 8. Behavior of the formula ψinj.

To have a chance to win under the condition imposed by 
, at every round z of some ρ, □ must associate p with at most one interval . Then, at the next round , if ⋄ puts q in , then □ can safely replay with r in ; otherwise, if ⋄ plays  then □ can still safely replay with .

It is worth pointing out that the formula 
, with 
, only guarantees that 
 is injective. For any given configuration, indeed, it cannot exclude a model where the next configuration features an arbitrary number of additional 
-labeled intervals , with 
, as it happens in Fig. 5, Fig. 6, and Fig. 7. Luckily, this is precisely the expected behavior of an error-incrementing computation.

The only missing element of 
 is the following formula encoding the initial configuration: 
. It can be easily checked that if such a formula holds over the initial interval , then the model begins with configuration 
.

We are now ready to define 
 as:

To complete the encoding, we need to add a formula that captures the accepting condition of finite (resp., infinite) non-emptiness error-incrementing computations. The finite case and the infinite one respectively dealt with the following formulas:
⁎
 

If 
⁎
 holds over the initial interval , then it forces the model to feature an interval labeled with a final state .
 

 forces the model to feature, for each interval , an interval 
, with 
, labeled with a final state .

Pairing 
 with 
⁎
 (resp., 
), one can prove the following fundamental lemma.

Lemma 1

Let 
 be a counter automaton. Then,

 is a positive instance of the finite non-emptiness error-incrementing problem if and only if 
⁎
 is a positive instance of the finite synthesis problem for ;

 is a positive instance of the non-emptiness error-incrementing problem if and only if 
 is a positive instance of the synthesis problem for .

The next theorem directly follows from Lemma 1 and Theorem 1.

Theorem 2

The finite interval-based synthesis problem for  is Ackermann-hard, while the interval-based synthesis problem for  is 
-hard.

Theorem 2 leaves open the question about the decidability of the finite interval-based synthesis problem for 
, which is a super-fragment of . Let us focus on it. To start with, recall that 
 is guaranteed only to be injective. Surjectivity (that is, 
) can be imposed neither in  nor in 
, even in the synthesis setting. Luckily, 
 makes it possible to impose it by means of the following formula:
 
 By exploiting modality 
, 
 constrains each 
-labeled interval , whose label does not include 
, to start at a point where a p-labeled interval 
 ends (a similar formalization of surjectivity can be found in some papers devoted to the satisfiability problem of 
 [10], [14]).

The formula 
 forces 
 to be a bijection between 
 and 
. Thanks to it, the values of the counters can be transferred in a precise way. The following result follows.

Lemma 2

Let 
 be a counter automaton. Then,  is a positive instance of the finite non-emptiness problem if and only if 
⁎
 is a positive instance of the finite synthesis problem for 
.

By pairing Lemma 2 and Theorem 1, we get the following theorem.

Theorem 3

The finite interval-based synthesis problem for 
 is 
-hard, while the interval-based synthesis problem for 
 is 
-hard.

In this section, we show that finite synthesis problem for 
 is decidable. We give a high-level account of the most significant steps of the proof, and move to the appendix the most technical details. First, we show that, in the setting of 
 finite synthesis problem, any winning strategy  for □ can be reduced to a finite one that still allows □ to win the game. As a matter of fact, such a result only allows us to conclude that the problem is semi-decidable. To complete the proof, we provide a sort of “small model theorem” on the set of finite representations of strategies, that restricts the search space to representations that cannot be arbitrarily large. Decidability of the finite synthesis problem for 
 immediately follows. In the next section, we will show that the decidability proof for 
 can actually be extended to deal with the finite synthesis problem for

Image 6
under a suitable semantic restriction.
As a preliminary step towards the description of the finite representation of strategies we provide some useful definitions. Let φ be an 
 formula. A φ-atom F (from now on, simply atom) is a maximal consistent subset 
. Formally, we have that (i) for each 
,  if and only if , and (ii) for each 
,
 if and only if 
 or 
. We denote by 
 the set of all possible atoms for φ. Given a set 
, the Σ-induced atom is the atom  such that  and 
 (proposition letters and temporal requests in  are all and only those in Σ). It is straightforward to prove, by induction on 
, that for every 
, atom  is well defined, that is, it exists, it is an atom, and it is unique. Similarly, given a run 
⁎
, with a slight abuse of notation, we define the induced atom-run 
⁎
⁎
⁎
 as follows:

(i)
 if 
⁎
⁎
;

(ii)
 if 
⁎
.

Once again, it is easy to prove that the extension of  to runs is well defined and, even more important, that it is a bijection between 
⁎
⁎
 (resp., 
⁎
) and 
⁎
⁎
 (resp., 
⁎
). The following definition identifies the class of atom-runs we are interested in.

5. Decidability of the finite synthesis problem for 
Definition 8

Let φ be an 
 formula and 
⁎
⁎
⁎
 be an atom-run. We say that τ is:

•
-consistent if for any 
 and any interval 
, if , then for all 
, with 
, 
;

•
-consistent if for any 
 and any interval 
, if , then for all , ;

•
 
-consistent if for any 
 and any interval 
, if 
, then for all 
, with 
, 
;

•
-fulfilling if for any 
 and any interval 
, if , then there exists 
, such that 
;

•
-fulfilling if for any 
 and any interval 
, if , then there exists 
, with , such that ;

•
 
-fulfilling if for any 
 and any interval 
, if 
, then there exists 
, with 
, such that 
;

•
φ-fulfilling if .

If an atom-run τ satisfies all the conditions of Definition 8, we say that it is consistent and fulfilling. The following result connects consistent and fulfilling atom-runs to successful runs.

Theorem 4

Let φ be an 
 formula. For every run on 
⁎
⁎
⁎
, it holds that ρ is successful if and only if  is consistent and fulfilling.

Theorem 4 allows us to focus on atom-runs instead of on runs. In the following, we will make extensive use of rooted trees, that is, directed, fully connected, and acyclic graphs whose root is the unique node devoid of incoming edges. Standard notions like those of children of a node (the set of all and only the nodes which are reachable from the node with a single edge), level of a node (the distance of the node from the root, that is, the level of the root is 0, the level of any child of the root is 1, and so on), root-to-leaf path (the unique sequence of nodes from the root to a given leaf) will obviously come into play. Moreover, we will usually denote the root the tree by 
.

We are now ready to give a finite representation of a strategy as a strategy tree for φ.

Definition 9

Let 
 be an instance of the finite 
 synthesis problem. A 
-strategy-tree for it is a finite labeled tree 
, where  is a finite tree, 
⁎
, and for every , the following conditions hold:

1.
;

2.
if v is not a leaf, then v has exactly 
 children;

3.
if v is not a leaf, then for every word 
, there exists 
 such that 
, for every 
.

A strategy tree  is winning if and only if for every leaf v of , if 
 the root-to-leaf path in , with 
, then 
 is a consistent and fulfilling atom run.

An example of -strategy-tree for an 
 formula φ, with 
, is shown in Fig. 9. It is worth pointing out that moving from one level to the next one, the out-degree of each node grows by a multiplying factor equal to 
. As an example, in the case of Fig. 9, given that 
, the out-degree doubles from one level to the next one. The degree of node v at level y represents all the possible assignments of subsets of 
 to the sequence of intervals , that is, all the possible words in the language 
. Thus, to each edge 
 outgoing from v, we assign the 
 word that represents the move where ⋄ plays 
 on the interval , for each . Conditions 2-3 of Definition 9 guarantee that there exists exactly one node 
 for each move 
 of ⋄. Such a labeling also represents the reply from □, which is the word 
 belonging to the language 
.

Fig. 9
Download : Download high-res image (736KB)
Download : Download full-size image
Fig. 9. An example of a {q}-strategy-tree.

With each node  in a 
-strategy-tree 
, we can associate a unique run (equivalently, interval structure, atom-run), which is generated by simply traversing the path from the root of the tree to v, that is, 
 where 
 is a path in the strategy-tree. In Fig. 9, we highlighted the paths corresponding to four of the leaves.

Now, it is immediate to turn each 
-strategy tree 
 into a unique 
-response-strategy 
⁎
⁎
⁎
⁎
 as follows:

It is worth noticing that Definition 9 guarantees that 
 is well defined. Moreover, by definition of 
, we have that 
 “actively reacts”, that is, it cannot answer always with a word in 
⁎
, up to a certain round y, which depends on whether or not it is possible to match the current play on a tree path stemming from the root. Since we are dealing with the finite synthesis problem, this amount to say that, even if a winning strategy 
 can, in principle, give rise to infinite plays, a finite number of rounds suffices for □ to win following 
. This is captured by the above-defined notion of winning 
-strategy-tree (Definition 9).

The above considerations lead to the following theorem, which allows us to reduces the finite synthesis problem to the problem of checking the existence of a winning 
-strategy tree. Its formal proof is given in the appendix.

Theorem 5

Let 
 be an instance of the finite 
 synthesis problem. It holds that  is a positive instance if and only if there exists a winning 
-strategy tree for φ.

Unfortunately, Theorem 5 only allows one to exhaustively generate all the finite 
-strategy trees, until a winning one is (possibly) found, thus proving semi-decidability of the problem.

To prove its decidability, we need to introduce a couple of additional notions about atom-runs. First, given an atom-run τ and an interval 
, we define the set of pending A-requests of  in τ as the set 
. Notice that, given an atom-run τ, for every 
, the set of pending A-requests 
 depends on the labellings 
, with 
, and it is monotone, that is, 
. Moreover, 
, that is, the “universal” component of temporal requests does not change if the left endpoint remains the same.

Function 
 is pictorially illustrated in Fig. 10 (see also Table 1) with reference to an atom-run τ. To help the reader in understanding how 
 works, we assumed 
 
. In Table 2, we also show a table representing all the possible values that 
 may take when 
, namely, sets 
. Since both 
 and 
 are drawn from 
, and 
 contains each sub-formula of φ, together with its negation, then 
 automatically determines 
. Moreover, by consistency of atoms, some sets 
 cannot belong to 
. In particular, for each 
, if , then 
, since, by definition of 
, there should exist an interval  such that , which contradicts the definition of atom (in Table 2, we put the subsets of 
 that represent these “inconsistent” values of 
 into a red frame). As an example, in Fig. 10, we have, 
, 
 for interval , and 
, 
 for interval , that is, pending A-requests are the same, but atoms are different. Moreover, we have, 
, 
 for interval , and 
, 
 for interval , that is, atoms are the same, but pending A-requests are different.

Fig. 10
Download : Download high-res image (149KB)
Download : Download full-size image
Fig. 10. An example of how 
 is defined for an atom-run τ.


Table 1. A table reporting all and only the subsets of Σφ that are represented in Fig. 10.




Table 2. A table representing all the possible values that 
 may take when 
 (values surrounded by a red frame are the “inconsistent” ones).



Decidability of the finite synthesis problem for 
 relies on counting the occurrences of pairs 
 in intervals  ending at the same right endpoint y, that is, the pairs associated with the same row y of a compass structure. Such a counting function is formally defined as follows.

Definition 10

Let 
⁎
⁎
 be an atom-run. For each , the counting-state of τ in y 
 is defined as 
, for all 
.

As an example, let us consider the τ depicted in Fig. 10 (for the sake of brevity, we restrict our attention to those for which 
 is greater than 0). We have that:

;

, and 
;

, 
, and 
;

, 
, and 
;

, 
, 
, 
, and 
;

, 
, 
, 
, and 
.

It is easy to see that, since 
 is a finite set, every counting state may be represented as a vector in 
, as 
 by definition of 
. For the sake of clarity, in the following, we will index these vectors with the elements of the sets 
 in place of standard natural indexing.

Then, by means of the standard (well) quasi-order ≤ over vectors 
, given an atom-run τ and two indexes 
, we write 
 if and only if for every 
, 
.

The next notion of sub-optimal atom-run is based on the comparison of vectors 
 and 
 for pairwise distinct positions y and 
 in τ. Such a notion of sub-optimal atom-run will be fundamental in detecting when □ is playing a game “too long”, that is, if □ wins the game according to a certain strategy, then he could have won it by generating a shortest successful atom-run τ.

Definition 11

Let τ be an atom-run. Then, τ is sub-optimal if and only if there exist two indexes 
 such that 
.

If an atom-run τ does not satisfy the condition of Definition 11, we say that τ is optimal. Let  be a 
-strategy tree. We say that  is optimal if and only if for every root-to-leaf path 
 in , 
 is optimal. The following results allow us to focus only on optimal 
-strategy trees in order to solve the finite 
 synthesis problem.

Lemma 3

Let 
 be a finite 
 synthesis problem. If there exists a winning 
-strategy tree  for , then there exists an optimal one 
.

Let us give an intuitive account of the proof of Lemma 3 by making use of the example depicted in Fig. 11 and Fig. 12. Basically, the proof is an adaptation to 
-strategy trees of a technique, called “contraction”, that has been successfully applied to prove the decidability of the satisfiability problem for many HS fragments (see [20] for a comprehensive overview of the results obtained by exploiting such a techniques). The main idea is that if a winning 
-strategy tree  is sub-optimal, then there exists a root-to-leaf path 
 in  such that the induced atom-run 
 is sub-optimal. This means that there exist 
 such that 
. We can get rid of such a “defect” by replacing  by a winning 
-strategy tree 
, where 
 and, among other changes, path 
 has been replaced 
. By iterating such an operation, we can fix one “defect” at a time, until we get an optimal winning 
-strategy tree.

Fig. 11
Download : Download high-res image (606KB)
Download : Download full-size image
Fig. 11. An example of contraction on a non-optimal run τ for a {q}-strategy-tree (before).

Fig. 12
Download : Download high-res image (471KB)
Download : Download full-size image
Fig. 12. An example of contraction on a non-optimal run τ for a {q}-strategy-tree (after).

Let us now dig a little deeper into the contraction operation by means of the example shown in Fig. 11 and Fig. 12. For the sake of simplicity, we consider -strategy trees, where each move of ⋄ at node v can be described by a word of exactly 
 bits, that is, a word belonging to the language 
.

In Fig. 11, we give an example of a root-to-leaf path 
, with the induced atom-run 
, which is not optimal because 
, as shown by the compass structure at bottom left of the figure. In general, condition 
 implies that there exists an injective function 
 such that, for any , 
. For what concerns our example, f is represented by the arrows in the compass structure in Fig. 11: we have that , , and .

Given 
, the contraction operation for a 
-strategy tree consists of pruning the tree at level of 
 and building a smaller sub-tree rooted at 
 by “carefully selecting” portions of the subtrees rooted at the children of 
, as shown in Fig. 12 for the atom-run of Fig. 11. Before explaining what “carefully selecting” means, we would like to clarify the role of function f, whose existence is guaranteed by the counting argument 
. Its distinctive feature is the following: if in the new 
-strategy tree 
 the game reaches node 
 then, for any  and any 
, the reply of □ to Σ on the interval  is the reply of □ on 
 to Σ in the old 
-strategy tree . However, since 
 in , by Definition 9, 
 has fewer children than 
. Then, since we want the new tree 
 to satisfy Definition 9, we have to select a subset of cardinality 
 among the set of all the children of 
 (cardinality 
) in a way that all the possible words in 
 are represented exactly once through the permutation and selection operated by f.

As an example, in Fig. 11 we highlighted the selected sub-trees rooted in the children of 
 that will be used for rewriting the strategy from 
 in 
. Notice that 
 has 64 children, while the children of 
 are 16. We selected 000010, 000101, 001001, 001100, 001111, 010001, 010110, 011010, 100001, 100010, 100101, 101001, 101010, 101101, 101110, and 110110, which, according to f, are used for ⋄ moves 0000, 0011, 1001, 1010, 1011, 0001, 0010, 1000, 0101, 0100, 0111, 1101, 1100, 1111, 1110, and 0110, respectively. For instance (see Fig. 12), if ⋄ plays 0111 at 
, the reply by □ in 
 is the one corresponding to 100101 in , since we have 0 at position 2 (), 1 at position 0 (), and 1 at position 3 (). As for the last element of the word corresponding the labeling of the newly introduced point interval , □ plays 1.

By [21], ≤ is a well quasi order (WQO for short), that is, given 
⁎
, for every infinite chain of indexes 
 in , there exist two indexes 
 such that 
, that is, if τ is infinite, then it is sub-optimal). Moreover, by Definition 11, if τ is finite and sub-optimal, then every atom-run that features τ as its prefix is also sub-optimal, that is, sub-optimality propagates upward with respect to the prefix relation. By also exploiting Lemma 3, this allows us to prove the following lemma.

Lemma 4

Given a finite 
 synthesis problem 
, it is decidable whether or not there exists an optimal winning 
-strategy tree for it.

A simple procedure that can be used to prove Lemma 4 is the following:

1.
initialize a 
-strategy tree 
, with 
, and set ;

2.
if 
 is the current 
-strategy tree, then:

(a)
if 
 is a winning 
-strategy tree, then return ;

(b)
if there exists a root-to-leaf path 
 in 
 such that 
 is sub-optimal, then return ;

(c)
if neither condition (a) nor condition (b) applies, then there exists a root-to-leaf path 
 in 
 such that 
 is not a consistent and fulfilling atom-run. In such a case, guess 
 words 
 in the language 
 such that, for any word 
, there exists  for which 
, for each 
. Then, let 
, where 
 are fresh nodes, 
, for all , and 
, for each . Finally, update i to  and go back to the very beginning of item 2.

It is easy to prove that there exists a computation of the above procedure that terminates with  if and only if there exists an optimal winning 
-strategy tree for φ. By contradiction, let us assume that there is no optimal winning 
-strategy tree for φ and that the above procedure does not terminate. Then, the only possibility is that step  is executed an infinite numbers of times. Since for every , the 
-strategy tree 
 is obtained by finitely extending a root-to-path of 
 from its leaf, by König's Lemma there exists an infinite path 
 in the limit tree 
 generated by an infinite execution of the above procedure. Since 
 is infinite, 
 is sub-optimal, which means that there exist two indexes 
 in 
 such that 
. By item , it follows that the procedure should have stopped the step immediately after the introduction of node 
 (contradiction). Lemma 4, together with Theorem 5, achieve the ultimate goal of the section.
Theorem 6

The finite 
 synthesis problem is decidable.

In conclusion, we would like to point out that the given proof is constructive, that is, we did not only prove that we are able to decide whether an instance 
 of the finite 
 synthesis problem is positive, but, in case it is, by exploiting an optimal 
-strategy tree  for φ, we are able to produce a procedure that effectively implements such a strategy in a system.

6. Decidability of the finite
Image 3
synthesis problem under refinement condition
In this section, we focus on the decidability of the finite synthesis problem for an extension of 
 with a finite number k of equivalence relations over time points. We prove that the problem is decidable if we allow only one equivalence relation, i.e., for the logic

Image 10
. As for , it has been already proved that the less general, finite satisfiability problem is undecidable [22]. We show that, for every , the finite synthesis problem (and thus the finite satisfiability one) turns out to be decidable under a suitable semantic restriction.
Let us begin by illustrating the way in which equivalence relations are represented in an interval structure. Given 
 equivalence relations over a finite prefix  of  (resp., ), we represent each equivalence relation 
, with , by means of a special proposition letter 
. Hence, for every 
, the logic

Image 11
introduces the dedicated proposition letters 
.
For any  and any model  (resp., ) of

Image 3
, the proposition letter 
 must satisfy the following conditions:
(C∼i)
, for all ;

(C∼ii)
for every  (resp., ), if 
, then 
;

(C∼iii)
for every  (resp., ), if 
, then 
;

(C∼iv)
for every  (resp., ), if 
, then 
.

Given an

Image 3
formula φ and a run 
⁎
⁎
⁎
, we say that ρ is ∼-respecting if and only if for each , ρ satisfies conditions (C∼-i), (C∼-ii), (C∼-iii), and (C∼-iv). Moreover, given 
 and a 
-response strategy , we say that  is ∼-winning (resp., finite ∼-winning) if and only if  is winning (resp., finite winning) and for every 
 ρ is ∼-respecting. Let us now define the finite
Image 3
synthesis problem.
Definition 12

Let φ be an

Image 3
formula and a set 
. The finite interval-based synthesis problem consists of determining whether or not there exists a finite ∼-winning 
-response strategy 
⁎
⁎
.
Definition 12 adds to the finite 
 synthesis problem of Definition 5 the requirement that successful runs generated by the 
-response strategy  must satisfy conditions (C∼-i), (C∼-ii), (C∼-iii), and (C∼-iv) for proposition letters 
. Notice that, given a finite

Image 3
synthesis problem 
, it is reasonable to assume 
. If this is not the case,  can be immediately classified as a negative instance, as ⋄ can immediately win the game by playing ∅ on the interval , which implies that 
, for any 
, thus violating the requirement that 
 must satisfy condition (C∼-i).
Now, we know that the less general, finite satisfiability problem for

Image 12
is undecidable, while that for
Image 5
is decidable [22]. Thus, the only still open issue is the one about the decidability of the finite synthesis problem for
Image 5
.
In the following, we prove the decidability of the finite synthesis problem for an extension of

Image 5
, which features additional equivalence relations 
, but under a suitable semantic restriction, called refinement condition.
Definition 13

(Refinement condition) Let  (resp., ) be a model for an

Image 3
formula φ. We say that M is a refined model if and only if for each  and any interval 
 (resp., 
), 
 implies 
.
Such a notion of refinement condition is well known in the literature, and it has been studied in various contexts (see, for instance, [23], where it has been investigated in the framework of Ehrenfeucht-Fraïssé games). An example of a model for an

Image 13
formula that satisfies the refinement condition is given in Fig. 14.(a). In such an example, for instance, 
, and thus, by Definition 13, 
 as well. In Fig. 14.(b), we provide a possible interpretation for an
Image 13
formula that violates the refinement condition. Indeed, we have that 
 which, by Definition 13, would imply 
, but this is not the case; moreover, 
 would imply both 
 and 
, but 
 only holds. In the following, we will restrict ourselves to models  (resp., ) of
Image 3
that satisfy the refinement condition of Definition 13. We will denote such a semantic fragment by
Image 14
. Given an
Image 14
formula φ and a 
-response strategy 
⁎
⁎
 for it, we say that  is refined if and only if for every 
⁎
 
 is a refined model. We are now ready to define the decision problem we are interested in.
Fig. 14
Download : Download high-res image (165KB)
Download : Download full-size image
Fig. 14. An example of fulfilling (a) and violation (b) of the refinement condition in the case of three special proposition letters ∼1,∼1, and ∼3.

Definition 14

Let φ be an

Image 14
formula and a set 
. The finite interval-based synthesis problem consists of determining whether or not there exists a finite refined ∼-winning 
-response strategy 
⁎
⁎
.
In the rest of the section, we prove that the problem stated in Definition 14 is decidable. From such a result, we have the following corollary.

Corollary 1

The finite interval-based synthesis problem for

Image 15
is decidable (Ackermann-hard), moreover the decidability of the satisfiability problem for the logic
Image 16
interpreted over the class of finite linear orders is decidable (Ackermann-hard).
Decidability of the finite interval-based synthesis problem for

Image 5
follows from observing that the restrictions imposed by Definition 13 do not affect the equivalence relation 
, i.e.,
Image 5
and
Image 17
are actually the same logic. Moreover, since decidability of the synthesis problem subsumes decidability of the satisfiability one, the finite satisfiability problem for
Image 14
turns out to be decidable as well.
We can immediately transfer conditions (C∼-i), (C∼-ii), (C∼-iii), and (C∼-iv), as well as the constraints imposed by Definition 13, to atom-runs as follows. Let φ be an

Image 18
formula, 
⁎
⁎
 (resp., 
⁎
) be an atom-run over it, and 
 (resp., 
). We say that τ is refined if and only if 
 is ∼-respecting and 
 is refined.
Given an

Image 18
formula φ and a refined atom-run 
⁎
⁎
 over it, it is easy to prove that, for every , the relation 
 is an equivalence relation on 
. Now, let 
⁎
⁎
 be a refined atom-run on an
Image 18
formula φ. For each  and each 
, we consider the 
 equivalence class 
, that is, 
. Moreover, given an
Image 18
formula φ, it is straightforward to prove that, for each refined atom-run 
⁎
⁎
, each , and each 
, 
. Notice that such a property is precisely the counterpart on interval structures of the constraint imposed by Definition 13. Finally, it is easy to show that, for each refined atom-run τ, any of its prefixes , with 
, is a refined atom-run.
Let us now specialize the notion of strategy tree in order to capture the class of refined finite ∼-winning strategies.

Definition 15

Let 
 be an instance of the finite

Image 18
synthesis problem. A 
-strategy tree  over it is a refined, winning strategy tree if and only if for each root-to-leaf path 
 in , 
 is a refined, consistent, and fulfilling atom-run.
Equivalence classes on a refined atom-run τ can be represented in a tree-like structure as follows.

Definition 16

Let τ be a refined atom-run for an

Image 14
formula φ. A refinement-tree for τ is a finite tree 
, where 
, such that:
1.
the root 
 of 
 is 
;

2.
for every 
, 
 if and only if , that is, the children of the root are all and only the equivalence classes of 
;

3.
for all 
 and all 
, 
 if and only if 
 and 
.

It is easy to prove that, given a refined atom-run τ, its refinement-tree 
 is well defined. Given an

Image 14
formula φ, from Definition. 16 it follows that every refined atom-run τ for φ satisfies the conditions: (i) all the root-to-leaf paths in 
 have length exactly k; (ii) for each  and each 
, 
 that is, the h-th level 
 consists of all and only the 
 classes in τ. Two examples of refinement-trees are given in Fig. 15. Both of them are drawn from two distinct prefixes of the same run τ: the tree above is 
, while the one below is 
.
Fig. 15
Download : Download high-res image (453KB)
Download : Download full-size image
Fig. 15. An example of sub-optimal run.

By Definition 16, every refinement-tree can be represented as a word of balanced parenthesis (Dyck word), where each index appears exactly once, at nesting level . As an example, in the case of the tree at the bottom of Fig. 15, 
.

The next notion we are going to define is that of dominance for atom-runs on

Image 14
formulas. As a preliminary step, we introduce a specialized version of the counting function 
 defined in Section 5. Recall that, given an atom-run τ and an index 
, the function 
 counts the occurrences pairs (atom, pending-A-requests) in τ for all and only the interval indexes , that is, for all . To this end, a specialization of 
, denoted by 
, is introduced, that count the pairs (atom, pending-A-requests) only on a given subset . Function 
 is formally defined as follows.
Definition 17

Let 
⁎
⁎
 be an atom-run. For all  and all , the X-restricted counting-state of τ in y is the function 
 such that 
, for all 
.

As we did in Section 5 for 
, function 
 may still be represented as a vector in the space 
. Moreover, since 
 simply restricts the counting done by 
 to the pairs (atom, pending-A-requests) witnessed by elements in X, as for the vectors belonging to 
, it holds that 
, for all . As an example, if we consider τ as specified in Fig. 15, we have that 
 and 
.

In the following, we will exploit 
 to count the pairs (atom, pending-A-requests) within the same equivalence class 
 for . As an example, in Fig. 15, we have that 
 and 
, as 
. Building on Definition 16 and Definition 17, we define a new binary relation on the class of all refined atom-runs for a given

Image 14
formula φ.
Definition 18

Let τ and 
 be two refined atom-runs for an

Image 14
formula φ and let 
 and 
 be the refinement trees for τ and 
, respectively. We say that 
 dominates τ, written
Image 19
, if and only if there exists an injective function 
 such that:
(subgraph)
for each 
, 
;

  (≤)  
for each , 
.

We will use of

Image 20
to detect sub-optimality of successful refined atom-runs, i.e., atom-runs that can be contracted into successful smaller ones, as we exploit the WQO ≤ over vectors in 
 (see Section 5). To this end, we need to prove a preliminary lemma.
Lemma 5

Let φ be an

Image 16
formula. The relation
Image 21
is a well quasi-order over the set of all the refined atom-runs for φ.
The proof combines a well-known result by Kruskal [24] with the fact that, given the formula φ, every refinement-tree built on an atom-run τ for it has bounded height k.

The next lemma guarantees that, for every atom-run τ, if

Image 22
, for some pair of indexes 
, then it is possible to apply the contraction operation in such a way that the resulting contracted atom-run is a refined one.
Lemma 6

Let τ and 
 be two refined atom-runs for a given

Image 16
formula φ. If
Image 23
, then there exists an injective function 
 such that, for each  and each 
, 
 if and only if 
.
We are now ready to extend the results for 
 proved in Section 5 to

Image 14
. As a matter of fact, all the machinery and the results developed in Section 5 until Theorem 5 included still hold for the finite
Image 14
synthesis problem, provided that the winning 
-strategy tree existence requirement is replaced by the refined winning 
-strategy tree existence requirement.
Theorem 7

Let 
 be an instance of the finite

Image 16
synthesis problem. It holds that  is a positive instance if and only if there exists a refined winning 
-strategy tree for φ.
Moreover, from Theorem 7 the decidability proof for the finite

Image 14
synthesis problem unfolds in a very similar way to the one in Section 5. Indeed, it suffices to replace the WQO ≤ over counting states 
 for an atom-run τ by the WQO
Image 20
and quite the same series of results can be easily proved. In particular, the notion of sub-optimal atom-run may be specialized as follows.
Definition 19

Let τ be a refined atom-run for an

Image 14
formula φ. It holds that τ is ∼-sub-optimal if and only if there exist two indexes 
 such that
Image 24
.
An example of a ∼-sub-optimal refined atom-run τ is given in Fig. 15, where

Image 25
.
Given the specific structural constraints imposed by Definition 16, it is easy to show that every morphism g (if any) witnessing a subgraph relation between two refinement-trees is uniquely determined by an injective function between the leaves of the trees, that is, there is no need to specify g for the internal nodes. As an example, in Fig. 15, we have that , and  (we leave to the reader the easy task of checking that Definition 18.(≤) is satisfied). Moreover, a function  that can be built over g according to Lemma 6 is , and  (see again Fig. 15).

If τ is not ∼-sub-optimal, we say that it is ∼-optimal. Then, given a refined 
-strategy tree  for an

Image 14
formula, we say that  is ∼-optimal if and only if, for each root-to-leaf path 
 in , it holds that 
 is ∼-optimal.
The following results allow us to focus only on optimal refined 
-strategy trees for solving the finite

Image 14
synthesis problem. A result analogous to Lemma 3 may be proved for ∼-optimal refinement-trees.
Lemma 7

Let 
 be a finite

Image 16
synthesis problem. If there exists a refined winning 
-strategy tree  for , then there exists also a ∼-optimal winning refined 
-strategy tree 
 for .
It is easy to see that Lemma 7 allows us to build a decision algorithm for the finite

Image 14
problem analogous to the one for the finite 
 synthesis problem given in Section 5. Then, the next theorem holds.
Theorem 8

The finite

Image 16
synthesis problem is decidable.
7. Conclusions
In this paper, we systematically explored the synthesis problem for meaningful fragments of HS, possibly extended with one or more equivalence relations over points.

We proved that the computational complexity of the synthesis problem is in general worse than that of the corresponding satisfiability problem (from Elementary/Decidable to Ackermann-Hard/Undecidable). There are, however, some quite expressive fragments, such as

Image 2
and
Image 14
, interpreted over finite linear orders, that remain decidable status when moving from the satisfiability problem to the more general synthesis one.
For a better understanding of the emerging picture, we reported in Table 3 both the novel results given in this paper and the related results already present in the literature.


Table 3. Decidability and complexity of the satisfiability and synthesis problems for the considered HS fragments. Results written in bold are given in the present work; results with no explicit reference immediately follow from those given in this paper or in other referred ones.

As for future work, there are at least two natural developments that stem from this work. On the one hand, it is worth investigating the synthesis problem for other small fragments of HS, e.g., safety fragments of the studied interval temporal logics. On the other hand, practical applications of the considered logics can be explored. As an example, we are investigating the possible use of HS

Image 4
to model the execution of processes represented in Business Process Modeling Notation [26]. In such a domain, the synthesis problem can be helpful for tackling fairly non-trivial issues such as, for instance, deciding whether or not scenarios where dynamically spawning instances of processes come into play are controllable.
Appendix A. Proofs for Section 5 and Section 6
Theorem 4

Let φ be an 
 formula. For every run on 
⁎
⁎
⁎
, it holds that ρ is successful if and only if  is consistent and fulfilling.

Proof

This lemma is the direct adaptation to runs/atom-runs of a result that has been proved multiple times for compass structures (see [9], for a complete proof). It is easy to see that the thesis follows from proving that, for every 
 and every interval 
, we have 
 if and only if . Let 
 (resp., 
 if ) the proof is by induction on ψ:

(base case)
if  by definition of  we have 
 and thus  if and only if ;

(¬ - induction step)
if 
 by inductive hypothesis we have 
 if and only if 
, by atom definition we have 
 if and only if 
, and thus, by combining this two conditions, 
 if and only if 
;

(∨ - induction step)
if 
 by inductive hypothesis we have 
 if and only if 
 and 
 if and only if 
 by atom definition we have 
 if and only if 
, and thus, by combining this two conditions, 
 if and only if 
;

( - induction step)
if , since ρ is -consistent and -fulfilling by hypothesis, we have that  if and only if there exists 
 for which 
 and thus, by inductive hypothesis, we have 
 if and only if 
, then we can conclude, by the semantics of 
, that this happens if and only if 
;

( - induction step)
if  since ρ is -consistent and -fulfilling by hypothesis, we have  if and only if there exists 
 with 
 for which 
 and thus, by inductive hypothesis, we have 
 if and only if 
, then we can conclude, by the semantics of 
, that this happens if and only if 
;

(
 - induction step)
if 
 since ρ is 
 
-consistent and 
 
-fulfilling by hypothesis, we have 
 if and only if there exists 
 with 
 for which 
 and thus, by inductive hypothesis, we have 
 if and only if 
, then we can conclude, by the semantics of 
, that this happens if and only if 
.  □

Theorem 5

Let 
 be an instance of the finite 
 synthesis problem. It holds that  is a positive instance if and only if there exists a winning 
-strategy tree for φ.

Proof

For the right-to-left direction the fundamental intuitions behind the proof, including the formal construction of a finite winning strategy from a strategy tree, has been given in the paragraph above the statement of the theorem in Section 5.

Let us now prove the left-to-right direction. Since 
 is a positive instance of the finite 
 synthesis problem there exists a finite winning 
-response strategy 
⁎
⁎
. First, let us define the labeled tree 
 where 
. In the following, starting from 
, we will recursively build a tree 
 as the limit of a series of finite labeled trees 
. Moreover, we will guarantee that at each step  the tree 
 satisfies the following invariant:

for every root-to-leaf path 
 in 
 there exists a run 
 such that 
.

Clearly the invariant condition holds for 
 since ϵ is a prefix of all the words in 
. Let us now define the tree 
 starting from the tree 
. Let 
 be a root-to-leaf path in 
 such that 
 is not a consistent and fulfilling atom-run, if a root-to-leaf path that satisfies such property does not exists we simply put 
 and we stop the procedure. On the other hand, let us suppose that such a path 
 exists. For the invariant condition we have that there exists an ω-word 
 such that 
. By definition of 
, we have that for every 
 the pair  belongs to , i.e.,  has a response for every extension 
 of . Then, let 
, which means 
, we introduce 
 fresh nodes and we define 
 as 
 where 
 for every 
 and 
 for every 
. Now let us prove that 
 is a 
-strategy tree for φ. Since  is finite winning strategy it is straightforward to prove by construction, that conditions 1, 2, and 3 of Definition 9 are satisfied by 
. Moreover, by construction of 
 it is easy to see we expand a finite root-to-leaf 
 path in 
 if and only if 
 is not consistent and fulfilling. If it is not the case, i.e., 
 is consistent and fulfilling, we have that 
 is also a finite root-to-leaf in 
 since, by the procedure defined above, 
 is not extended anymore for any iteration greater than i. Then we can conclude that 
 is consistent and fulfilling for all the finite root-to-leaf paths 
 in 
. It remains to prove that 
 is finite, that is, there exists i for which for every root-to-leaf 
 path in 
 we have that 
 is a consistent and fulfilling atom-run. Let us suppose, by contradiction that there exists an infinite path 
 in 
. By the invariant condition we have that there exists 
 such that 
. Moreover, by construction of 
, we have that for every 
 the run 
 is not a consistent and fulfilling atom-run and thus, by Theorem 4, we have that  is not successful then we can conclude that ρ is limit-successful. However, having a limit-successful run in 
 implies that  is not a finite winning 
-response strategy for φ (contradiction). Finally, we can conclude that 
 is a winning 
-strategy tree for φ. □

Lemma 3

Let 
 be a finite 
 synthesis problem. If there exists a winning 
-strategy tree  for , then there exists an optimal one 
.

Proof

Let 
 a winning 
-strategy tree for φ, if  is optimal we simply set 
 and we have proved the thesis. Let us assume that  is not optimal then there exists a root-to-leaf path 
 such that 
 is not optimal. This means that there exist two indexes 
 such that 
. Now we will build a winning 
-strategy tree 
 for φ which is identical to  for what concerns everything but the sub-tree rooted in 
, while all the paths passing through the sub-tree rooted in 
 are shorter of exactly 
 nodes with respect to their counterparts in . This is done by means of an adaptation to the “contraction” technique introduced for proving the decidability of the satisfiability problem for 
 in [9]. In this case, however, such a contraction technique is applied on a set of models that may be organized as a tree instead of a single model like in [9]. Since 
 there exists an injective function 
 such that for every  we have 
. Let us observe that  is a sequence of elements in 
 devoid of repetitions, i.e., in combinatorical terms, a -simple disposition of the elements in 
. Let 
 be the 
-tree where 
, 
, 
 for every 
, and 
. It is easy to observe that 
 is still a 
-strategy tree for φ but it may not be a winning one. In the following we will build a winning 
-strategy tree for φ by means of a recursive procedure that, starting from 
, generates a sub-tree rooted in 
, which is a leaf in 
, by “mimicking” the sub-tree rooted in 
 in the original 
-strategy tree . Let 
 the current 
-strategy tree, i.e., the 
-strategy tree generated at step i, we will guarantee the following invariant conditions at the end of each step of our procedure:

(1-inv)
at every step  the tree 
 is a 
-strategy tree for φ, i.e., satisfies conditions 1, 2, and 3 of Definition 9;

(2-inv)
 and for every 
 we have that 
 if 
 and 
 otherwise.

Let us suppose that we have build 
, two cases may arise: (i) if all the leaves in 
 are also leaves in , then we put 
 and we return from the procedure; (ii) if there exists a leaf v in 
 which is not a leaf in , then we have to build 
 which extends 
 by introducing a set of children for v selected among the children of v in . Let us assume that we are in case (ii) for 
, by invariant conditions v belongs to the subtree of  rooted in 
. Let us notice that if  we have that 
. Since v is an internal node in , by conditions 1, 2, and 3 of Definition 9, it has 
 children in  such that for each 
-length word 
 there exists exactly one children 
 of v such that 
. we define the function 
 as 
 for every 
 and 
 for every 
. Let us notice now that 
 is a disposition of the indexes for the words of length 
. Since each word 
 is represented by a unique children of v in  we are guaranteed that we may always select a subset of exactly 
 children of 
 in  whose “scrambled” labeling via 
 (i.e., 
) represents the whole finite language of 
-length words 
. Formally speaking, let 
 be a minimal set of children of v in  for which for every 
 there exists 
 for which 
. As we discussed above the existence of such a set 
 is guaranteed by invariant condition (2-inv), moreover, by minimality condition, we have 
. Finally, we define 
 where 
 for each 
, and 
 for each 
. By construction of 
 and by the cardinality considerations above we can easily prove that both invariant conditions (1-inv) and (2-inv) hold for 
.
Let us notice that at each intermediate step i for a selected leaf v in 
 which is not a leaf in  we introduce a subset 
 of children of v in  as children of v in 
 and thus v is no longer a leaf in 
. Since  is finite it is easy to prove that for every step i and every leaf v in 
 which is not a leaf in  there exists a step 
 for which v is an internal node in 
. On the other hand, it is straightforward to see that every leaf v of  which is introduced at some step i in 
 stays a leaf in 
 for every subsequent step 
. Since at each step we have 
 the termination of the above procedure immediately follows from the finiteness of V.

The invariant condition (1-inv) guarantees that the resulting 
 tree is a 
-strategy tree for φ, moreover condition (2-inv) guarantees that 
 and, since we have at least removed 
 we are guaranteed that 
. As we mentioned above it will make things easier to understand if we look at 
 as a version of  where the sub-tree rooted at 
 has been replaced by a “slimmer” version of the sub-tree rooted at 
. We say “slimmer” because only the number of children of nodes for each node in the sub-tree rooted at 
 has decreased in the transformation from  to 
.

It remains to prove that 
 is winning. Then for any root-to-leaf path 
 in 
 we have to prove that 
 is a consistent and fulfilling atom-run. By construction of 
 two cases may arise: (i) if 
 then 
 is also a root-to-leaf path 
, in such a case we can immediately conclude that 
 is a consistent and fulfilling atom-run; (ii) 
, then the suffix 
 of 
 involves the part of  that has been shrunk in 
. The proof that, in the latter case, 
 is still a consistent and fulfilling atom-run follows the same steps of the proofs of Lemma 3.2 in [9], even if in this case the underlying construction is more complex such a result may be used as a reference for better understanding the general idea behind the current proof. First of all, let us take a look at how 
 is build by the procedure that generates 
. By construction, we have that there exists a path 
 in  and thus 
 is a consistent and fulfilling atom run. Let us define a function 
 such that for every 
 

By construction of 
, for every 
 we have that 
. Let us now prove that τ satisfies all the conditions of Definition 8:

(-consistent)
first let us recall that all the intervals sharing the same right-endpoint share the same -requests, as proved in [27], and thus for every 
 and for every 
 with 
 we have to consider just two cases either 
 or 
. If 
 for any interval 
 we have two possible sub-cases either 
 or 
. If 
 we have that 
 and 
 immediately follows from -consistency of τ. It is easy to prove that for every -consistent atom-run τ and for every 
 we have 
 implies . If 
 we have, by definition of function g, 
. Since 
 and 
 we have 
. By definition of 
 we have that 
 and since by definition of f we have 
 we obtain 
. Since τ is -consistent it immediately follows that 
 for every 
. This property holds, in particular, for 
 and thus we have 
. If 
 regardless whether or not 
 we have by construction 
 and, by property of function g, we have 
 and thus by -consistency of τ we have 
 for every 
. By mapping function g we have that 
 for every 
. and thus 
;

(-consistent)
for every 
 and for every 
 with 
 we have that (i) 
, (ii) 
, (iii) or 
. If 
 (i) by definition of g we have 
 and thus, since τ is -consistent for every 
 we have 
 and since 
 we have 
. If 
 (ii), we have that 
 then, by function g, for every 
 we have that either 
 or 
. In the first case we have 
 and since, by functions f and g, it holds 
 we have 
 since 
 begins 
. Then, since τ is -consistent we have 
 for every 
 and thus, for g, 
 for every 
. In the latter case, 
, we have 
. Since 
 begins 
, by -consistency of τ we have 
 for every 
. Then, by definition of g, we have 
 for every 
. If 
 (iii) by definition of g we have 
 and thus, since τ is -consistent for every 
 we have 
 then, by definition of g, 
 for every 
;

(
 
-consistent)
for every 
 and for every 
 with 
 we have that (i) 
, (ii) 
, (iii) or 
. If 
 (i) by definition of g we have 
 for every 
 we have that either 
 or 
. In the first case, since τ is 
 
-consistent for every 
 we have 
 and since 
 we have 
. In the latter case, 
, we have 
. By definition of functions f and g, it holds 
 and thus, since τ is 
 
-consistent. We have that for every 
 it holds that 
 and thus 
 for every 
. If 
 (ii), by definition of g we have 
. Since τ is 
 
-consistent, we have that for every 
 it holds that 
 and thus 
 for every 
. If 
 (iii) by definition of g we have 
 and thus, since τ is 
 
-consistent for every 
 we have 
 then, by definition of g, 
 for every 
;

(-fulfilling)
for every 
 and for every 
 with , for the very same argument of point (-consistent) we have to consider just two cases either 
 or 
. If 
, 
 and since τ is -fulfilling we have that there exists 
 s.t. 
. Let us assume that 
, this implies that 
 for every 
. We have two possible sub-cases either 
 or 
. If 
 we have that 
 and thus 
. If 
, since we have that 
 is the minimum interval starting in 
 for which ψ holds in τ then we have that either 
 or 
. In the first case by definitions of functions f and g we have 
 then 
. In the latter case by definition of function f we have 
 then, since τ is -fulfilling, there exists 
 such that 
 By definition of g we have 
. If 
 we have by construction 
, moreover, since τ is -fulfilling, either 
 or 
. If we are in the first case we can immediately conclude, by function g that, 
. For the first case, since τ is -fulfilling we have that there exists 
 such that 
 and thus by g we have 
.

(-fulfilling)
for every 
 and for every 
 with 
 we have that (i) 
, (ii) 
, (iii) or 
. If 
 (i) then, by function g, we have 
. Since τ is -fulfilling we have that there exists 
 such that 
. Since 
, by function g, we have 
 and thus 
. If 
 (ii) then we have 
. Two cases may arise either 
 or 
. In the first case, by definition of f and g, since 
, we have that, since τ is -fulfilling, there exists 
 such that 
. Since 
 we have 
 with 
. In the latter case, since 
, 
, and τ is -fulfilling we have that there exists 
 such that 
. Then, by definition of g, we have 
 with 
. If 
 (iii), we have that 
, then, since 
 and τ is -fulfilling, we have that there exists 
 such that 
. By definition of g we have 
 with 
.

(
 
-fulfilling)
for every 
 and for every 
 with 
 we have that (i) 
, (ii) 
, (iii) or 
. If 
 (i) then, by function g, we have 
. Two cases may arise either 
 or 
. In the first case, by definition of f and g, since 
, we have that, since τ is 
 
-fulfilling, there exists 
 such that 
. Then, by definition of f and g, we have 
 with 
. In the latter case, since 
, 
, and τ is -fulfilling we have that there exists 
 such that 
. Then, by definition of g, we have 
 with 
. If 
 (ii) then we have 
. Since τ is 
 
-fulfilling, there exists 
 such that 
. Then, by definition of f and g, we have 
 with 
. If 
 (iii), we have that 
, then, since 
 and τ is 
 
-fulfilling, we have that there exists 
 such that 
. By definition of g we have 
 with 
.

(φ-consistent)
two cases may arise either  or . In the latter case since  and 
 and thus 
 is φ-fulfilling. In the first case we have that it holds 
 by definition of g it holds 
 and thus 
 then 
 is φ-fulfilling.

Now we have proved that 
 is a winning 
-strategy tree for φ. Let us observe that, by the above construction, we have guaranteed that 
. Then two cases may arise, if 
 is optimal then we have proved the hypothesis, otherwise if 
 is not optimal we can apply again the very same procedure to 
 thus obtaining a “smaller” winning 
-strategy tree 
 for φ and so on. Since, every tree obtained from iterating the latter case is both finite and strictly smaller than its predecessor we have that the latter case may be executed just a finite number of times and thus at the end we are guaranteed to obtain an optimal winning 
-strategy tree for φ. □

Lemma 5

Let φ be an

Image 16
formula. The relation
Image 21
is a well quasi-order over the set of all the refined atom-runs for φ.
Proof

Let assume by contradiction that

Image 20
is not a WQO then there exists an infinite sequence of refined atom-runs 
 for a given
Image 14
formula φ such that for every pair of indexes  in  we have
Image 28
. From Kruskal's Tree Theorem [24] we have that for each infinite sequence of finite labeled trees 
 whose node labels belongs to a WQO, say 
 for every  and ≤ is the standard WQO among vectors of 
, we have that there exists two indexes  in  for which there exists an injective function 
 such that:
(i)
for every 
 we have that 
 is a descendant of 
 in 
;

(ii)
for every 
 we have 
.

We can instantiate the above properties of Kruskal's Tree Theorem in our case by putting 
 and, for every , 
 for each 
. Then we have that the existence of two indexes  in  and the function 
 that satisfies points (i) and (ii) above is guaranteed. Let us observe that from property (ii) of h we have that condition Definition 18.(≤) is satisfied. However condition (ii), which defined the so-called “embedding relation”, is in general more relaxed than the subgraph relation used in Definition 18. As a matter of fact, over the class of all finite trees the subgraph relation is not a WQO. However, the class of refinement trees over φ is a specific class of finite trees. In particular, from Definition 16, we have that for every refined atom-run τ for φ not only 
 has height bounded by the constant k but each root-to-leaf path in 
 has length exactly k. Since
Image 28
we have that there exists 
 such that 
. Since 
 is a tree then there exists a unique root-to-leaf path in 
 
 and  for which 
 and 
. By property (i) of 
 and by the transitivity of the descendant relation we have that there exists a path 
 
 
 in 
 such that for every 
 there exists 
 
 
 such that 
 
 and 
 
. By construction we have that there exists 
 
 
 for which 
 
 and 
 
 and thus, since 
, we have 
 
 
. This immediately implies that 
 (contradiction). □
Lemma 6

Let τ and 
 be two refined atom-runs for a given

Image 16
formula φ. If
Image 23
, then there exists an injective function 
 such that, for each  and each 
, 
 if and only if 
.
Proof

Let 
 and 
 Since

Image 19
from Definition 18 we have that there exists a function g such that for every 
 we have 
 and for every  we have 
. Let  and 
 be the two sets representing all and only the leaves in 
 and 
, respectively. By Definition 18 and the structure of refinement trees defined in Definition 16 we have that for every  
, moreover, still from Definition 16 we have that 
  and 
 . From condition Definition 18.(≤) it immediately follows that for every  there exists an injective function 
 such that for every  we have 
. Then from the injectivity of g, the injectivity of 
 for every , and the fact that  is a partition of 
, since it represent the collection of all the 
 classes in τ, we have that the function 
 defined as 
 with  is injective and well-defined. Let us now prove that for each 
 and for each  we have that  if and only if , let us call such a property upward-propagation. This may be done by induction on i being  the distance of w from the root of . The induction base () immediately follows from the definition of f for both directions. For the inductive step of the left-to-right-direction let us assume by contradiction that there exists 
 for which  and  where w has distance  from the root of . Since  and by Definition 16, since  
 
, we have that there exists 
 for which 
 thus, by inductive hypothesis, we have that 
. By Definition 18 we have that, since 
, 
 then  (contradiction). For the inductive step of the right-to-left-direction let us assume by contradiction that there exists 
 for which  and  where w has distance  from the root of . Since  and by Definition 16, since  
 
 
 
, we have that there exists 
 
 for which 
 
. Let us notice that since 
 
 we have that 
 
 since either 
 
 is the unique leaf 
 
 that contains  or 
 
 belong to the unique root-to-leaf path in 
 to 
 
. In both cases since 
 
 belongs to  by definition of f, then, by the definition of g, all the nodes in 
 on the root-to-leaf path to 
 
 belongs to . Then, by inductive hypothesis, we have that there exists 
 for which 
 and 
 
. By Definition 18 we have that g guarantees that 
 thus, since 
, then  (contradiction). It remains to prove that for every  and every pair 
 we have 
 if and only if 
. Both directions immediately follows from the upward-propagation property. For the left-to-right let us assume by contradiction that 
 and 
. By Definition 16, 
 and thus for the upward-propagation property we have that if 
 then 
. Again from upward-propagation 
 (trivially) then 
, and thus, since 
 is an equivalence class 
. We can conclude 
 (contradiction). For the right-to-left let us assume by contradiction that 
 and 
. Since 
, and 
 from the upward propagation property we obtain 
. Since 
 is an equivalence class and we have 
 it holds that 
. If 
 by upward-propagation we have 
 and thus 
 since we just shown that 
. However, for the hypothesis, we have 
 (contradiction). □
Theorem 7

Let 
 be an instance of the finite

Image 16
synthesis problem. It holds that  is a positive instance if and only if there exists a refined winning 
-strategy tree for φ.
Proof

Let us observe each refined winning 
-strategy tree is itself a winning 
-strategy tree with additional constraints regarding the behavior of the 
 special proposition letters. In an analogous way, a finite refined ∼-winning 
-response strategy is a specialized version of a finite winning 
-response strategy with the additional constraint that 
 must respect the refinement condition. The proof here is basically the same of Theorem 5 and thus here we will point out the minimal modifications/observations to the proof of Theorem 5 that will make it work also for the current thesis. For the right-to-left direction we have just to slightly modify the definition of the finite winning 
-response strategy 
 extracted from the (refined) winning 
-strategy tree  as follows:

Let us notice that in this newly defined 
 is unchanged w.r.t. definition in Theorem 5 for the part that “traverses” the tree for obtaining the right answer to ⋄ (which is the only part that is really needed since the tree is winning). On the other hand, for the part that “completes” each prefix into an infinite one once the relative path in  finishes in a leaf we just asking the labeling related to the point  to contain 
 in order to guarantee that the resulting infinite run is ∼-consistent. It is easy to observe that, since 
 is built over a refined winning 
-strategy tree  each resulting run is a refined ∼-consistent one.

For the left-to-right direction the construction of  from  is exactly the same of the one reported in the proof of Theorem 5. In order to prove that the resulting winning strategy tree 
 is a refined one it is sufficient to observe that if we build  from refined ∼-winning 
-response strategy  the invariant condition imposed on 
 for each iteration  may be strengthened by adding the following condition:

for each root-to-leaf path 
 in 
 the atom-run 
 is a refined one.

Such an additional invariant condition may be trivially guaranteed for every iteration  by observing that 
 is a prefix  of some word 
. Since  is a refined ∼-winning 
-response strategy, we have that 
 is refined. It is easy to observe that if 
 is refined then for every  
 is refined, that is, if 
 is ∼-respecting any model built on any finite prefix of ρ is refined as well. In particular, we have that 
 is refined and thus 
 is a refined atom-run. Finally, it immediately follows that the resulting winning 
-strategy tree 
 is also a refined one. □
Lemma 7

Let 
 be a finite

Image 16
synthesis problem. If there exists a refined winning 
-strategy tree  for , then there exists also a ∼-optimal winning refined 
-strategy tree 
 for .
Proof

The proof follows the structure of the proof of Lemma 3. Let us assume that there exists a root-to-leaf path 
 for which 
 is not ∼-optimal. Then there exists two indexes 
 such that

Image 30
. Then Lemma 6 guarantees that we can assume the existence of an injective function 
 s.t. for every  and every 
 we have 
 if and only if 
. By means of such function f we can perform the very same contraction operation that outputs the very same “smaller” winning 
-strategy tree 
 obtained in the proof of Lemma 3.
In order to prove that such a tree is also a refined one we have to prove that 
 and is refined and 
 is ∼-respecting, for each run 
 in 
, resulting from a root-to-leaf path 
 that contains 
. Let 
 be the run in  that has been “contracted” into 
, let us observe that, since  is refined we have that ρ is ∼-respecting and 
 is refined. Moreover, let 
 and 
 the atom-runs relative to 
 and ρ, respectively.

Since ρ is ∼-respecting we have that 
 for every  for each . By definition of function g in the proof of Lemma 3 we have 
 for every  and 
 for every , then we obtain 
 for every  and 
 for every . This implies that 
 for every 
 for each  and thus condition (C∼-i) is satisfied by 
. Let us now prove that conditions (C∼-ii), (C∼-iii), and (C∼-iv) are satisfied in 
 for each 
 with  Given three distinct points 
 we have the following cases:

•
, by definition of g in the proof of Lemma 3, we have 
, 
, and 
. Then we have 
, 
, and 
. Since ρ satisfies conditions (C∼-ii), (C∼-iii), and (C∼-iv) we have that the very same conditions are satisfied in 
 for 
, and 
;

•
, by construction of g we have 
, 
, and 
. Then we have 
, 
, and 
. Since 
 we have that 
 if and only if 
. Two cases may arise either 
 (i) or 
 (ii). In the first case (i) we have, by definition of g in the proof of Lemma 3, 
. If 
, by Lemma 6 we have that 
. If 
 and 
 we have that, by Lemma 6, 
. Then the fact that 
 is an equivalence relation in 
 is not violated in such a case since we have 
. If 
 and 
 we have that, by Lemma 6, 
. Then the fact that 
 is an equivalence relation in 
 is not violated in such a case since we have 
. In all the above sub-cases of (i), we have that the antecedents of (C∼-ii), (C∼-iii), and (C∼-iv) are not triggered for 
 in 
, and thus they are trivially satisfied in 
. In the latter case (ii) we have, by definition of g in the proof of Lemma 3, 
. Since 
 we have that 
 if and only if 
. If 
 then by, by Lemma 6, 
, and thus, by definition of g in the proof of Lemma 3, 
. In such a case we have that the antecedents of (C∼-ii), (C∼-iii), and (C∼-iv) are not triggered for 
 in 
 and thus such conditions are trivially satisfied in 
. If 
 then by, by Lemma 6, 
, and thus, by definition of g in the proof of Lemma 3, 
. Then (C∼-ii), (C∼-iii), and (C∼-iv) are satisfied for 
 in 
;

•
, by construction of g we have 
, 
, and 
. Then we have 
, 
, and 
. Since 
, by definition of g in the proof of Lemma 3, we have that 
 if and only if 
. Two cases may arise either 
 (i) or 
 (ii). In the first case (i) we have, by definition of g in the proof of Lemma 3, 
. If 
, by Lemma 6 we have that 
. If 
 and 
 we have that, by Lemma 6, 
. Then the fact that 
 is an equivalence relation in 
 is not violated in such a case since we have 
. If 
 and 
 we have that, by Lemma 6, 
. Then the fact that 
 is an equivalence relation in 
 is not violated in such a case since we have 
. In all the above sub-cases of (i), we have that the antecedents of (C∼-ii), (C∼-iii), and (C∼-iv) are not triggered for 
 in 
, and thus they are trivially satisfied in 
. In the latter case (ii) we have, by definition of g in the proof of Lemma 3 and by Lemma 6, 
. If 
 then by, by definition of g in the proof of Lemma 3, 
. Moreover, by Lemma 6, 
 since 
 then, by definition of g in the proof of Lemma 3, 
. In such a case we have that the antecedents of (C∼-ii), (C∼-iii), and (C∼-iv) are not triggered for 
 in 
 and thus such conditions are trivially satisfied in 
. If 
 then by, by definition of g in the proof of Lemma 3, and thus, 
. Moreover, since 
, we have, by Lemma 6, 
, then, by definition of g in the proof of Lemma 3, we have 
. Then (C∼-ii), (C∼-iii), and (C∼-iv) are satisfied for 
 in 
;

•
, by construction of g we have 
, 
, and 
. Then we have 
, 
, and 
. Since ρ satisfies conditions (C∼-ii), (C∼-iii), and (C∼-iv) for 
, 
, and 
 we have that the very same conditions are satisfied in 
 for their translated versions 
, and 
.

Let us now prove that 
 satisfies the refinement condition. Let 
 and 
 by definition of function g in the proof of Lemma 3 we have that 
. Since ρ satisfies the refinement condition, we have that for each  for every 
 it holds that 
 if and only if 
. Then, since 
, we may conclude that for each  for every 
 it holds that 
 if and only if 
. □