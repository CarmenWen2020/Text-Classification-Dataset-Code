Abstract
Cops and Robbers games have been studied for the last few decades in computer science and mathematics. As in general pursuit evasion games, pursuers (cops) seek to capture evaders (robbers); however, players move in turn and are constrained to move on a discrete structure, usually a graph, and know the exact location of their opponent. In 2017, Bonato and MacGillivray [2] presented a general characterization of Cops and Robbers games in order for them to be globally studied. However, their model doesn't cover cases where stochastic events may occur, such as the robbers moving in a random fashion. In this paper we present a novel model with stochastic elements that we call a Generalized Probabilistic Cops and Robbers game (GPCR). A typical such game is one where the robber moves according to a probabilistic distribution, either because she is rather lost or drunk than evading, or because she is a robot. We present results to solve GPCR games, thus enabling one to study properties relating to the optimal strategies in large classes of Cops and Robbers games. Some classic Cops and Robbers games properties are also extended.

Keywords
Cops and Robbers games
Pursuit games
Optimal strategies
Graph theory
Stochastic games

1. Introduction
Cops and Robbers games have been studied as examples of discrete-time pursuit games on graphs since the publication of Quilliot's doctoral thesis [25] in 1978 and, independently, Nowakowski and Winkler's article [22] in 1983. Both monographs describe a turn-based game in which a lone cop pursues a robber on the vertices of a graph. The game evolves in discrete time and with perfect information. The cop wins if he eventually shares the same vertex as the robber's, otherwise, if the play continues indefinitely, the latter wins. A given graph is copwin if the cop has a winning strategy: for any possible move the robber makes, the cop has an answer that leads him to eventually catch the robber (in finite time). As there is no tie, it is always true that one player has a (deterministic) winning strategy.

Since the first exposition of the game of Cop and Robber, many variants have emerged. Aigner and Fromme [1] notably presented in 1984 the cop number: it is the minimal number of cops required on a graph to capture a robber. Since then, more alternatives have been described, each one modifying one game parameter or more such as the speed of the players, the radius of capture of the cops, etc. We refer to Bonato and Nowakowski's book [4] for a comprehensive description of these different formulations. The survey on guaranteed graph searching problems by Fomin and Thilikos [11] is also a great reference on the subject. In graph searching games, the objective is to capture a fugitive on a graph. The problems in which the object is always found are called guaranteed.

In 2017 Bonato and MacGillivray [2] presented a first generalization of Cops and Robbers games that encompasses the majority of the variants described previously. Indeed, all two-player, turn-based, discrete-time, pursuit games of perfect information on graphs in which both players play optimally are contained in Bonato and MacGillivray's model. As such, this model encompasses all pursuit games deemed combinatorial (we refer to Conway's book On Numbers and Games [9] for an introduction on the subject of combinatorial games). Those games include the set of turn-based, perfect information, games played on a discrete structure without any randomness.

Recently, some researchers such as Prałat and Kehagias [16], Komarov and Winkler [19] and Simard et al. [27] described a game, called the Cop and Drunk Robber game, in which the robber walks in a random fashion: each of her movements is described by a uniform random walk on the vertices of the graph. In general, this strategy is suboptimal. Since this particular game cannot be described by Bonato and MacGillivray's model, it appears natural to seek to extend their framework to integrate games with random events.

There has also been a recent push towards more game theoretic approaches to modeling Cops and Robbers games, notably by Konstantinidis and Kehagias [20]. Our paper can be considered more in line with this way of treating Cops and Robbers games than more traditional approaches.

This paper thus presents a model of Cops and Robbers games that is more general than that of Bonato and MacGillivray. The main objective of this model is to incorporate games such as the Cop and Drunk Robber game. The probabilistic nature of this game leads to define a framework different from the one of Bonato and MacGillivray.

In Cops and Robbers games, one is generally interested in the question of solving a game. This question is universal to game theory where one defines a solution concept such as the Nash Equilibrium. In Cops and Robbers games, often-times the cops' point of view is adopted and one seeks to determine whether it is feasible, and if so how, for them to capture the robbers. In stochastic Cops and Robbers games, one can generalize the question to a quantitative scale of success: what is the (best) probability for the cops to capture the robbers, and which strategy reflects it. One can also ask the dual question of what would be the minimal number of cops required in order to capture the robbers with some probability. In deterministic games, this graph parameter is known as the cop number.

One can note that many solutions of Cops and Robbers games share the same structure, and this is reflected in the fact that they can be solved with a recursive expression. Indeed, Nowakowski and Winkler [22] in 1983 presented a preorder relation on vertices, writing 
 when the cop has a winning strategy in at most n moves if positioned on vertex y, while the robber is on vertex x. An important aspect of this relation 
 is that it can be computed recursively and thus leads to a polynomial time algorithm to compute its values, as well as the strategy of the cop. This relation was extended 20 years later by Hahn and MacGillivray [15] in order to solve games of k cops by letting players move on the graph's strong product. Clarke and MacGillivray [7] have also defined a characterization of k-cop-win graphs through a dismantling strategy and studied the algorithmic complexity of the problem. For a fixed k the problem can be resolved in polynomial time with degree . On a related note, Kinnersley [17] proved that it is EXPTIME-complete to determine whether the cop-number of a graph G is less than some integer k when both G and k are part of the input. This shows that Clarke and MacGillivray's result is somehow optimal.

In games with stochastic components, such order relations can be generalized by considering the probability of capture, as is done in a recent paper about the Optimal Search Path (OSP) problem [27]. A recursion 
 is defined: it represents the probability that a cop standing on vertex y captures the robber, positioned on vertex x, in at most n steps. This relation, defined on the Cop and Drunk Robber game [16], [19], [27], is analogous to Nowakowski and Winkler's 
 and is slightly more general as it enables to model the robber's random movement. One can wonder up to what point the relation 
 can be extended while preserving its polynomial nature. Theorem 2.13 and Proposition 2.24 give an answer to this question.

This paper is divided as follows. Section 2 presents our model of Cops and Robbers games, the 
 recursion along with some complexity results, notably, on 
. Stationarity results on 
 are also included. Since most Cops and Robbers games are played on graphs, another formulation of our model is presented on such a structure in Section 3. We conclude in Section 4.

2. An abstract Cops and Robbers game
We now present a general model of Probabilistic Cops and Robbers games; it is played with perfect information, is turn-based starting with the cops, and takes place on a discrete structure. From each state/configuration of the game, after choosing their actions, the cops and robbers will jump to a state according to their transition matrices, denoted 
 and 
. These matrices may encode probabilistic behaviours: 
1 is interpreted as the probability that the cop, starting in s and playing action a, will arrive in 
.

Definition 2.1

A Generalized Probabilistic Cops and Robbers game (GPCR) is played by two players, the cop team and the robber team. It is given by the following tuple(1)
 satisfying

1.
, the non-empty finite set of states representing the possible configurations of the game. The sets 
 and 
 hold the possible cops and robbers positions while 
 may contain other relevant information (like whose turn it is).

2.
 is the initial state.

3.
 is the set of final (winning) states for the cops.

4.
, with 
 and 
 the non-empty, finite sets of actions of the cops and robbers, respectively.

5.
 is a transition function for the cops, that is,
 When the sum is 1, we say that a is playable in s, and we write 
 for the set of playable actions for the cops at state . Furthermore, 
 also satisfies

•
for all , 

•
if , then 
 for all action 
; hence 
 for all 
.

6.
 is a transition function for the robbers, similar to 
. 
 is the set of playable actions by the robbers in state .

A play of  is an infinite sequence 
 of states and playable actions of  that alternates the moves of cop and rob. It thus satisfies 
 for  and 
 for . The cops win whenever a final state  is encountered, otherwise the robbers win. A turn is a subsequence of two moves, starting from cop. We also consider finite plays and we write 
 for the game where plays are finite with n (complete) turns.
An equivalent formulation for 
, and sometimes more handy, is to rather define 
 as a distribution on S, for an action a playable in s. The correspondence is 
 for . For example, the second condition of the fifth item in the preceding definition could have been stated 
, where 
 is the Dirac distribution on an element s, that is, 
 has value 1 on , and is 0 elsewhere.
A play progresses as follows: from a state s, the cops choose an action 
, which results in a new state 
, randomly chosen according to distribution 
; then the robbers play an action 
, which results in the next state 
, drawn with probability 
. Once a final state is reached, the players are forced to stay in the same state. Notice that one could record whose turn it is in the third component of the states: 
. However, this doubles the state set and complexifies the definition of the transition function. In most games, it is more intuitive to define the rules for movement independently of when this transition will be taken, like in chess.

We sometimes use the notation 
, for  to denote the projection of a state  on the set 
. The set 
 is rarely used in the current section, but will be valuable further on, such as in Example 3.5 on dynamic graphs whose structures vary with time.

In what follows, we write 
 as the set of discrete distributions on a set B and 
 for the discrete uniform distribution on the same set.

Most of the example games we will describe will be between a single cop and a single robber, even if the definition specifies a cop team and a robber team. The usual way of presenting the positions of the cop team is with a single vertex in the strong product of each member's possible territory.

2.1. Encoding of known games and processes, stochastic or not
We now describe a few known games, following the structure of Definition 2.1. The first one is a typical, deterministic example of a Cops and Robbers game. We say a game is deterministic when both distributions defined by 
 and 
 are concentrated on a single point, in other words if 
 and 
 are Dirac for all  and . The reader can safely skip this section.

Example 2.2

Classic Cop and Robber game
Let  be a finite graph. In this game, both players play alone and walk on the vertices of the graph, successively choosing their next moves among their neighbourhoods. The final states are those in which both players share a vertex, in which case the cop wins. The tricky part for encoding this game is that in their first move, the cop and the robber can choose whatever vertices they want, so the rule of moving differs at the first move from the rest of the play. So we let 
 be two elements that will serve as starting points for the cop and the robber. Because the first moves are chosen in turn, the set of states S below must contain states in 
, which can only be reached after the cop's first move, but before the robber's. To simplify S, we include states that will not be reached, and this will be governed by the transition functions. The different sets are:
 Let , , and actions 
 and 
. We define:
 
  Thus, for state 
, the playable action set is 
. Similarly, for the robber we get 
. Because a play starts with the cop, it is not required to specify the condition 
 in function 
. Similarly, is it not necessary to make a special case of state , since the play ends anyway.

The stochasticity of Definition 2.1 is motivated by the following example, called the Cop and Drunk Robber game. It is rather similar to the one just presented except that the robber moves randomly on the vertices of the graph.

Example 2.3

Cop and Drunk Robber game
From the preceding example, only the robber's transition function 
 is modified, the rest stays the same. Let  and 
. The robber's transition function is then:
  The robber, after the first move, moves uniformly randomly on her neighbourhood, which amounts to ignoring her action 
. One could also restrict her actions by 
 when 
.

In the Cop and Drunk Robber game, the robber moves according to a uniform distribution on her neighbourhood. Varying her transition function could represent various scenarios. For example, the robber's probability of ending on a vertex 
 from vertex r could depend on the distance between r and 
.

In addition to the Cop and Drunk Robber game itself, a recent paper by Simard et al. [27] presented a variant of this game in which the robber can evade capture. The main difference between these games is that the cop may not catch the robber even when standing on the same vertex. This game is presented in the next example.

Example 2.4

Cop and Drunk Defending Robber
The game's main structure is again similar to that of Example 2.2, but we need a jail to simulate the catch of the robber, 
⁎
. The initial state is the same, and we have:
⁎
⁎
⁎
⁎
 When players do not meet, they move on G as before. Yet, when the cop steps on the same vertex v as the robber, there is a probability  the robber gets captured, where . For , the robber's transition function is then:
 
 
⁎
⁎
  When the cop steps on the robber's vertex (), at the end of his turn, the next move for the robber follows the distribution 
. The robber is caught by the cop with probability , bringing the play in a final state, otherwise she proceeds as expected: the target state is chosen uniformly randomly in the robber's neighbourhood. Variations of this game could be defined through different distributions for 
 with . Likewise, in 
, the factor 
 
 could be replaced with any distribution on .

We now present the Cop and Fast Robber game with surveillance zone as first formulated in Marcoux [21]. This example is reconsidered further on in Section 3. Chalopin et al. also studied a game of Cop and Fast Robber with the aim of characterizing graph classes [6].

Example 2.5

Cop and Fast Robber
This game is similar to the classic one (Example 2.2) except that the robber is not limited to a single transition. It has been studied by Fomin et al. [10]. We present a variation where the cop can capture the robber when she appears in his watch zone, even in the middle of a path movement. This watch zone can simulate the use of a weapon by the cop. The states will now contain, in addition to both players' positions, the set of vertices watched by the cop. We assume here that the cop's watch zone is his neighbourhood, as in Marcoux [21]; Fomin et al.'s version is retrieved with a watch zone consisting of a single vertex, the cop's position. In the initial state, the cop's watch zone is empty since the robber cannot be captured before her first step. We again use a jail state 
⁎
. When both players find themselves there, the game ends and the robber has lost. Hence, we let:
⁎
⁎
 Let  be the current state and 
 an action of the cop. Here is the cop's transition function, for :
  As in the classic game, the cop can jump to any vertex in his first move; after that he moves in the neighbourhood of his current position. His watch zone then changes to 
. We use C as watch zone in this definition to emphasize the fact that it does not influence the cop's next state. On her turn, on vertex 
, the robber's action consists in choosing a path 
 of finite length , that is, 
 is an edge in E for each . The robber's transition function is:
⁎
⁎
  The robber is thus ensured to reach her destination 
 provided that she never crosses the cop's watch zone on her path π. If this happens, then the robber is taken to the jail state 
⁎
⁎
.

In Section 3, we present this game again, but with the possibility for the robber to evade capture.

Hence, because of Definition 2.1's rather general description, it is possible to encode a great variety of random events resulting from the cops' or the robbers' actions. In the following example, we encode a simple inhomogeneous Markov Chain by forgetting the notions of cop. This makes the example fairly degenerate but it also shows the generality of Definition 2.1.

Example 2.6

Finite Markov chain
A Markov chain is a sequence of random variables 
 on a space E, having the Markov property. So we can assume that the evolution is given by an initial distribution q on E and a family of matrices 
, where 
 is the probability that 
 given that 
. We can encode it as a GPCR game from Definition 2.1. In previous examples, we have ignored the third component of states, 
, but here we can ignore one of the players sets, like 
; equivalently, we can assume a single state for the robber and no effect by 
. We define
 Since the action of the player has no influence on the progress of the game, it is natural to define A as a singleton. Technically, a play alternates between the moves of cops and robbers, so it is a sequence 
; the repetitions reflect the fact that the robber has no effect. If we ignore the useless information of such a play, we obtain a sequence 
, which is just a walk in the Markov chain (and the robber wins). Another way to write down this model would have been to let the two players play similarly, with 
, but the states would then have to be triplets, and the initial state would force a less simple encoding.

Similarly, we can encode a finite state Markov Decision Process (MDP) with reachability objectives [24] with Definition 2.1. The encoding will satisfy that the optimal value of the MDP is 1 if the cops win, otherwise it is 0 and the robber wins.

The probabilistic Zombies and Survivors game on graphs [3] can also be viewed as a GPCR game, one in which only the robbers play optimally. It models a situation in which a single robber (the survivor) tries to escape a set of cops (the zombies). However, the cops have to choose their initial vertices at random and, on each turn, choose randomly among the set of vertices that minimize the distance to the robber.

2.2. Strategies
A deterministic (or pure) strategy is a function that prescribes to a player which action to play on each possible game history. Some strategies are better than others; we will be interested in the probability of winning for the cops, which will be attained by following a strategy. Ultimately, we are interested in memoryless strategies, that is, those that only depend on the present state, and not on the previous moves; nevertheless, we need to define more general strategies as well.

Definition 2.7

Let  be a game. A history on  is an initial fragment of a play on  ending in a state. The set of histories on  is denoted 
.

•
The set of general strategies is 
.

•
The set of memoryless strategies is .

•
The set of finite horizon strategies is 
.

A finite horizon strategy counts the number of turns remaining, and it is otherwise memoryless. A finite horizon strategy is conveniently defined on  but it is actually played on 
, hence the following definition of how such a strategy is followed. At turn 0 of h (histories 
 and 
), there are n turns remaining, so σ is evaluated with n on the second coordinate of its argument; at turn 1 (histories 
 and 
), there are  turns remaining.
Definition 2.8

Let 
 be a (finite or infinite) play of .

•
h follows a general strategy 
 for the cops if for all  we have 
. Similarly for the robbers.

•
h follows a memoryless strategy 
 for the cops if for all  we have 
. Similarly for the robbers.

•
h follows a finite horizon strategy 
 on 
 for the cops if for  we have 
 
.

•
h follows a finite horizon strategy 
 on 
 for the robbers if  we have 
 
.

These strategies are all deterministic, or pure: a single action is chosen. Some papers consider mixed or behavioral strategies, where this choice is randomized. This is unnecessary in our setting because, as is well known in perfect information games, among all optimal strategies, there is always a pure one. We will come back to this when we study optimal strategies later on.

We now present an example where the optimal strategy for the infinite game is memoryless (only depends on the states), but, for any finite horizon game 
, it is a finite horizon strategy.

Example 2.9

This example is in the spirit of the Cop and Drunk Robber game, presented in Example 2.3. As in this example, the cop moves on his neighbourhood and so does the robber, who cannot choose her action, as before, but the difference with Example 2.3 is that the robber's movement is not uniform. The graph is a cycle of length 5. The robber moves clockwise with probability 0.9, and counterclockwise with probability 0.1. If the cop is at distance 1 of the robber at his turn, of course he wins in this turn. Otherwise, the cop is at distance 2, more specifically at clockwise distance 2 or 3. Let us focus on states s where this clockwise distance is 2 (from the cop to the robber). On the long term, the cop's best choice is to move counterclockwise. However, if only one turn remains, the best move for the cop is the clockwise move because then with probability 0.1, the robber will jump to his position, whereas the probability of winning is zero in the counterclockwise direction. So the best strategy σ for 
 satisfies  in such a state s, for , hence it is not memoryless. Indeed, for example,  because the probability of catching the robbers by playing counterclockwise when 2 turns remain is 0.9, and it is 0.19 by playing clockwise (0.1 in one move of the robber plus 0.09 in two moves).

2.3. Winning conditions in GPCR games
In this section we are interested in winning strategies for the cops, their probability of winning in a given number n of turns (that is, in 
) and their probability of winning without any limit on the number of turns (in ).

Given finite horizon strategies 
 and 
, for the cops and for the robbers, we consider the probability that the robbers are captured in n steps or less:
 Since the cops want to maximize this probability, and the robbers want to minimize it, the probability for the cops to win in n turns or less (playing optimally), whatever the robbers strategy, is:(2)
⁎
  
 
 This is in fact the value of 
 in the sense of game theory. In game theory, the value for 
 exists if(3) 
  
 
  
 
 In our setting defining the payoff function of a play as 1 when the robbers are captured and 0 otherwise, we have, by Wal and Wessels [29], that the game 
 has value 
⁎
. That the restriction of 
⁎
 to finite horizon strategies does achieves the value of 
 is given again by Wal and Wessels, who call such strategies Markov strategies. Finally, since 
 is finite and with perfect information, a standard game-theoretical argument [23] justifies that the optimal strategies are deterministic (or pure).

We say that the cops and the robbers play optimally in 
 if they each follow a strategy that yields probability 
⁎
 for the cops to win. We will show later on, but it is also straightforward2 from the definition, that 
⁎
 is increasing in n; since it is moreover bounded by 1, the limit always exists and we will prove that is it equal to the value of .

Indeed, from a known result in Simple Stochastic Games (SSG), one can show that  has a value and that this value is achieved by a pair of optimal strategies that are deterministic (or pure) and memoryless. The argument is well known in the literature on SSGs, but requires a construction, so we leave it to Appendix A. Thus, let us write the value of game  as 
⁎
, that is,(4)
⁎
  
 
 and the equality still holds when the min and max operators are switched. This value is guaranteed by Theorem A.1 [8], [26]. In Proposition 2.16, we will show that the difference in the cop using a finite-horizon strategy in 
 and a memoryless one in  is negligible for a sufficiently large integer n.

Equation (2) returns either 0 or 1 in deterministic games such as the Classic Cop and Robber game. We seek here to study games that can be stochastic, where 
⁎
 can take any value in . Thus, we adapt the usual definition of copwin to our broader model.

Definition 2.10

Let  be a GPCR game. We say  is

•
-win if the cops can ensure a win with probability at least p in at most n turns, that is 
⁎
;

•
p-copwin if it is -win for some ;

•
almost surely copwin if the cops can win when they are allowed to play infinitely, that is 
⁎
;

•
copwin if it is -win for some .

It is easy to see that when  corresponds to the Classic Cop and Robber game, as defined in Example 2.2, this definition of copwin coincides with the classical one. In that sense, it can be considered as a generalization of the classical one, because in any copwin finite graph, the cop wins in at most
Image 1
turns.
Remark 2.11

We will see in Proposition 2.16 that 
⁎
⁎
. Thus, if there exists n such that 
⁎
 and if all states reachable within a finite number of moves of the cop's optimal strategy are in the same strongly connected component, then 
⁎
. Indeed, after n turns, if the play is not over, the cops can go back to the configuration where 
⁎
: the initial position that is proposed by the cops' strategy. In that state, the probability that the robbers have not been caught is at most 
⁎
; the probability that the robbers are not caught after m repetition of this cycle is at most 
⁎
. It is thus zero at the limit. This happens, for example, if 
⁎
 and  is a strongly connected graph. However, we cannot, in general, claim that if 
⁎
 after

Image 2
turns have been played, then 
⁎
.
We define a probabilistic analog to the cop number, , which is the minimal number of cops required on a graph G in order for the cops to capture the robbers. It is an important subject of research in Classic Cops and Robbers games [4], in particular relating to Meyniel's conjecture that

Image 3
. Furthermore, one of the main areas of research on cops and robbers games that involve random events is the expected capture time of the robbers [19], [18], [16]. Thus, we further generalize the expected capture time of the robbers for any game .
Adding cops in a game  is done in the natural way. The set of cops states 
 is the cartesian product of the sets of single cop positions, and the transition function is updated so as to let all cops move in one step.

Definition 2.12

The -cop number 
 of a game  is the minimal number of cops required for the capture of the robbers in at most n turns with probability at least p. In other words, 
 is the minimal number of cops required for a game  to be -win. The p-cop number, 
, is the minimal number of cops necessary for having 
⁎
.

Let 
 be the random variable giving the number of turns required for the robbers to be captured with probability at least p in  under optimal strategies. Then, the p-expected capture time of the robbers is 
. The expected capture time of the robbers is 
.

Since some of the optimal strategies of  are memoryless, we can turn the question of computing 
 into a question of computing an expected hitting time in a Markov chain. Let us write 
⁎
 (
⁎
) for the optimal strategy of the cops (robbers) in  and let  be the Markov chain such that for any state , it has two states 
⁎
 and 
⁎
. Furthermore, let M be its transition matrix, that is governed by the distributions 
⁎
 and 
⁎
. Suppose 
 describes the stochastic process on  beginning at the initial state 
, then 
 
 is the hitting time of F from 
. The expectation of T is 
.

2.4. Solving GPCR games
Similarly as with Bonato and MacGillivray's model, we define a method for solving GPCR games, that is, for computing the probability for the cops to capture the robbers in an optimal play, and the strategy to follow. This method takes the form of a recursion, defining the probability 
 that state s leads to a final state in at most n steps (w is for winning in the following theorem). This recursion gives a strategy for the cops.

Theorem 2.13

Let  be a GPCR game, and let:

(5)
 
  
 
  
 
  Then 
 gives the probability for the robbers to be captured in n turns or less, given that both players play optimally, starting in state s. Thus,
⁎
 This also says that  is -win if and only if 
. For , let 
⁎
 be the argmax in place of max in Equation (5). This defines finite horizon strategies that are optimal in 
.3

The recursive part of 
's definition is as follows: to win, the cops must take the best action a; this leads them to state 
 with probability 
; from this state, the robbers must choose the action 
 that will give them the smallest probability of being caught. Action 
 leads the robbers to state 
 with probability 
 and then we multiply by the probability that the cops catch the robbers from this state, 
. Since the cops want a high probability, a maximum is taken; it is the converse for the robbers. The full equation gives the expected probability of capture of the robbers by the cops when both players move optimally.
Proof

The proof is by induction on n. We prove that 
 gives the probability for the robbers to be captured in n turns or less, given that both players play optimally, starting in state s. Let s be any state.

If , then the cops win if and only if , in which case, by definition we do have 
. Otherwise the robbers win and 
, as wanted.

If , suppose the result holds for  and let s be the current state. If this state is final, then the robbers are caught in n turns or less with probability 1 and 
 as desired. Otherwise, let the cops, playing first, choose an action 
, after what the next state 
 is drawn according to 
. Then, the robbers can choose an action 
, in which case the next state 
 will be drawn with probability 
. By the induction hypothesis, we know a final state will be encountered in  turns or less with probability 
 starting from state 
. Thus, the probability the robbers are caught in n turns or less by playing action 
 after the cops have reached state 
 is given by: 
 
 Note that if 
, this value is exactly 
, since by definition, we must have 
 if 
 and 0 otherwise. The robbers wish to minimize this value among their set of available actions, which is possible since both sets S and 
 are finite. Hence, supposing action 
 has been chosen by the cops, the game stochastically transits to some other state 
 with probability 
. Thus, with probability 
 
  
 
 the robbers are caught in at most n turns from state s, when the cops play action 
. The cops want to maximize this value and, as for the robbers, this is possible because the considered sets are finite. Thus, the cops must play the action 
  
 
  
 
 The claim about 
⁎
 is straightforward from this result. The choices of actions at the initial state thus give the probability 
. Because 
⁎
 is, by definition, the probability of capture of the robbers in n turns or less when both players play optimally, we conclude that 
⁎
. □

This result implies that the 
's are probabilities that increase with n. In other words, we have the following corollary.

Corollary 2.14

For any  we have 
.

Note that there are many optimal strategies for the cops in , that is, strategies that have value 
⁎
, but they are not all as efficient. Consider a game 
 where the robbers can be caught in  turns with probability 1, and let 
 be an optimal strategy for 
. Then the strategy that stays idle for  turns and then behave as prescribed by 
 is optimal, but not efficient, and it respects the argmax of Equation (5). The next proposition shows how to define an efficient one: the action chosen must be the same whenever the probability of winning does not increase (with the horizon).

Proposition 2.15

For each , there exists an optimal strategy 
⁎
 with horizon N that satisfies: for all , if 
 for 
, then 
⁎
⁎
. Similarly for the robbers.

Proof

For any , we denote by  the set of actions that achieve the maximum in Equation (5) for 
. We have proved in Theorem 2.13 that any strategy satisfying  for all  is optimal. Let us prove that if 
 for 
, then 
. By contradiction let k be the smallest integer such that there is a state s and an action . By induction, the cops play action a at time , and then, with horizon k, they choose an optimal action in 
⁎
 that is also in 
⁎
 (possible by minimality of k) and so on until the last turn where they stay in place or whatever possible. This gives us a value of at least 
 and, by definition, at most 
. Since 
, the finite horizon strategy defined above is optimal. This is a contradiction since a should be in . Thus we obtain that if 
 for 
, then 
, for all 
. Hence the wanted strategy exists. The argument is similar for the robbers. □

Although 
 only gives the value of the game 
 with finite-horizon strategies, we can show that this relation, as a function of n, converges to the value of .

Proposition 2.16

The value of  is 
⁎
. Furthermore, the optimal strategies of 
 are ϵ-optimal strategies of  for any  and sufficiently large integer n.

Proof

From a previous argument, we know that some pair 
 of optimal memoryless strategies for the cops and the robbers yields a probability 
⁎
 of winning for the cops. It holds that 
⁎
⁎
, for any integer n, since the value of 
 can only be at most the value of . Recall that since 
⁎
 is non-decreasing in n and bounded above by 
⁎
, we have that 
⁎
⁎
.

Now, let us play strategies 
, chosen above, in the game 
 for any integer n. Consider the probability that the cops win in 
 when both players follow those strategies. These probabilities, for each n, form a sequence 
. This sequence is non-decreasing and bounded above by 
⁎
.

Let 
 be the event “there is a capture in at most n turns under strategies 
 and 
”. Observe that 
 is a non-decreasing sequence. Thus, by the Monotone Convergence Theorem:
⁎
 
 
 Thus, for any  there exists an integer N such that for all , 
⁎
. But, we also have that 
⁎
 for any integer n, since 
 is the value of 
. Hence, it follows that

Image 4
. This completes the proof. □
It is interesting to note that this theorem only applies if there are best strategies for the cops and robbers. In particular, it is not true if  is played on the infinite graph of the following example.

Example 2.17

Consider an infinite star graph with a central vertex, from which paths of lengths n are deployed, for every integer n, and consider the Classic Cops and Robbers game  on this graph with one cop and one robber. The best move for the cop is to start on the (infinitely branching) central vertex. Then whatever state the robber chooses, the cop will catch her in a finite number of turn, so this graph is copwin in the sense of Definition 2.10. However this number of turn is unbounded, so when playing in 
, the robber can simply choose a vertex at distance greater than n; so the value of 
 is 0 for all n. The proof of the theorem fails in that case because, the graph being infinite, there is no optimal strategy for the robber in . Whatever state the robber chooses, there is always a further state that would allow her to be captured in more turns, that is, there is always a better strategy.

Under certain conditions that will be further studied in Subsection 2.6, the 
 sequence becomes constant.

Definition 2.18

We say that 
 is stationary if there exists an integer  such that 
, for all , . We write 
 
 for the stationary part of 
.

Remark 2.19

It follows from the definition of 
 that, if for some N, 
 for all , then 
 is stationary and 
 
 starts at  or less.

From Theorem 2.13, we deduce Theorem 2.20 that is more in line with traditional game theoretical arguments and show that in addition to the equality 
⁎
 we can compute explicitly the optimal strategy of the cops in , from the limit of the 
's.

Theorem 2.20

The (point-wise) limit 
 exists and it satisfies(6)
  
 
  
 
  Moreover, the optimal (memoryless) strategy for the cops in , from any state s, can be retrieved by a cops' action for which the maximum of Equation (6) is achieved.

Proof

Let L be the lattice of functions , ordered point-wise, with the null function as bottom element ⊥. Equation (5) determines the following function . For  and , 
  
 
  
 
  From previous remarks,  is monotone increasing. Thus, we deduce from the Knaster-Tarski fixed point theorem [14] that  has a least fixed point given by 
. Furthermore, we have 
 and 
, so 
, for all integer n, and thus 
 and satisfies Equation (6).

We showed in Theorem 2.13 that 
⁎
, and in Proposition 2.16 that 
⁎
⁎
. Consequently, 
⁎
. Hence, 
 is the probability that the cops capture the robbers when both teams play optimally. Similarly, one can show that 
 is the probability that, starting at s, the cops capture the robbers when both team play optimally. This, together with the fact that 
 satisfies Equation (6), imply that the optimal strategy for the cop is coherent with an action achieving the argmax operator in place of the max operator in Equation (6). One cannot choose any such action because, for example, a temporary bad action, like staying idle, can give the same probability of winning than another action, but you can only choose it a finite number of times, which is incompatible with a memoryless strategy. □

Remark 2.21

Recall that we have 
⁎
 and that, by definition, it holds that 
⁎
. Thus, we could have defined 
 with switched operators min and max. Then, we can deduce the optimal robbers strategies by flipping those operators and replacing the min operator by an argmin operator. This also holds in 
.

Now, with the help of Equation (5) we can generalize the classic theorem of Cops and Robbers games. This is done in the next corollary.

Corollary 2.22

Let  be a GPCR game. Then,  is copwin if and only if the sequence 
 is stationary and
 
 Moreover, the game is p-copwin if and only if the sequence is stationary and
 
 If  is not p-copwin for any p, then the game is almost surely copwin if and only if the sequence is not stationary and

Remark 2.23

If the GPCR game  is deterministic, then 
 is 0 or 1 for any  and . It therefore follows from monotonicity of 
 (see Corollary 2.14) and from Remark 2.19 that the stationary part starts at some

Image 5
. Indeed, if 
 there is at least one s such that 
 and 
. This difference can be observed at most
Image 6
times.
The conditions under which 
 is stationary are presented in Proposition 2.26.

2.5. The computational complexity of the 
 recursion
We show a result on the algorithmic complexity of computing function 
 (Equation (5)). This function is computable with dynamic programming, yet it may require a high number of operations, especially as its complexity is function of the size of the state space. Recall that Equation (5) was devised to be as general and efficient as possible. However, given the context of Definition 2.1, the best one can hope for its polynomial complexity in the size of the state and action spaces.

Proposition 2.24

In the worst case and under a dynamic programming approach, computing 
 requires

Image 7
operations, where 
 is
Image 8
, similarly for 
. The spatial complexity is
Image 9
.
Proof

Let 
 be the number of operations required for computing the recursion of 
. Assume that computing probabilities 
 and 
 require unit cost. Clearly, 
. In the worst case, when , all elements of the sets 
 and 
 must be considered in order to ensure optimality of the actions chosen and thus 
 operations are required. We always have that

Image 10
and similarly for 
. Then, in the worst case, where we assumed that all values of 
 were saved in memory for all . Memorizing those values requires a spatial complexity of
Image 12
at most. The final complexity is thus
Image 13
. □
Consequently, both spatial and temporal algorithmic complexities depend on the three sets S, 
 and 
. This suggests that these complexities may be high if the number of available actions is. One could imagine a game in which actions are paths, resulting in exponential complexity in

Image 6
. Still, whenever
Image 14
and
Image 15
for some polynomials p and q, then Equation (5) is clearly computable in polynomial time in the size of S. Moreover, as we will see in Corollary 2.27, 
 does not have to be computed for all n in order to determine if the cops have a winning strategy or not, essentially,
Image 16
suffices. In many studied cases,  is itself polynomial in the size of the structure on which the game is played, leading each time to polynomial time algorithms for solving the game.
2.6. A stationarity result
In traditional games of Cops and Robbers where a relation 
 is defined (such as the classic game [22] and the game with k cops [7]), it is useful to prove results on the convergence of the recursion 
. One demonstrates the relation becomes stationary, that is, there exists a number  such that for all integers  and all pairs of vertices 
, if 
, then 
. One then writes ⪯ for the stationary part of the sequence, i.e. 
. This result is vital for solving Cops and Robbers games as it ensures the relation ⪯ can be computed in finite time.

Contrary to the relation 
 found in deterministic Cops and Robbers games (such as the classic game in Example 2.2), the relation 
 does not always become stationary. For example, on the triangle 
, with one cop and one robber, although it is copwin in the classical sense, whenever one adds a probability of capture on the vertices, say  for , then after n turns the cop will have captured the robber with probability only 
 
. Thus, after n turns, the cop can only ensure a probability of capture strictly less than 1, although he wins with probability 1 (in the limit). In other words, a game may be almost surely copwin, but not -win for any integer n. In the following proposition, we formulate and prove an upper bound on the minimal number of steps n required to determine 
⁎
, the probability of capture in an infinite game.

Recall that it does not hold in general that in a copwin graph (in the classical sense of one cop against one robber) every optimal strategy of the cop prevents him from visiting any vertex more than once [5]. Were this to be true, we could easily upper bound the capture time of the robber. However, we show in Lemma 2.25 that a milder version of this result holds for states, instead of simple cop position.

To have an intuition of why the following lemma is true, it is important to note that the condition of stationarity is a very strong one. The contraposition of the lemma may be more informative: the only way for 
 to become stationary is that there is no loop possible in any play following the optimal strategies of the players. An example of a graph where it does not happen is a cycle of length 3, where the robbers have equal probability in both directions in every state. There are plays where the robbers are caught after an arbitrary large number of turns. On the other hand, an acyclic graph does induce stationarity for 
 (because of the recursive nature of 
).

Lemma 2.25

Suppose 
 is stationary at  in a game , for a state s and that the cops and robbers follow their optimal strategy, in the sense of Proposition 2.15. Then, in every winning play from s, the cops never start a turn in the same state twice along the play.

Proof

We prove the result for both the cops and robbers, that is, in a winning play where they follow their optimal strategy, none of them visit the same state twice at their turn. Because of stationarity, Proposition 2.16 and Proposition 2.15, the (memoryless) optimal strategy for the cops in  is also optimal in 
,  for the  first turns. Let 
⁎
 and 
⁎
 be these strategies. Suppose the lemma is false. Then there is a winning play 
 (i.e., reaching F in say 
 turns) from state s containing a loop. Let 
 be the first state of π that is reached twice by the same player in the play, and let the second time be at 
,  (with k and l having the same parity). This play follows the optimal strategies of the players in game 
, but also in 
 (and even in ). None of the states of the loop are in F by definition of a play. Consider the set Π of plays 
 that start as π until the first occurrence of 
, follow the fragment 
 from 
 for i times, and then continue as the fragment of π after it exits 
 the last time. These are plays (in particular, they are alternating between the players). All these plays are winning (one of them may be π but may also not be, as π's second loop through 
 may be different from the first one), but infinitely many of them reach F at a turn greater than N. If we prove that these plays follow 
⁎
 and 
⁎
, this contradicts stationarity as the robbers are caught in more than N turns in an infinite number of them, which implies that the value of 
 is strictly greater than the value of 
 for infinitely many i.

We do have that any play of Π follows the optimal strategies. Indeed, since π follows the optimal memoryless strategies, everytime the play reaches state 
, the same action is chosen for the player. In the first occurrence, it leads to enter the loop, in the last one it leads to leave it. This happens when the action leads to a stochastic next state. Indeed, the robber has advantage in staying the longest possible in the loop (even forever), so the exit of the loop is not her choice. Similarly, because the strategy is optimal, entering the loop is not a choice of the cop, as cutting up the loop would be more efficient. The cop is forced by stochasticity to enter the loop, hoping that next time the odds will let him leave it, and this can happen only after i loop turns. □

Note that the lemma is not true if the robbers do not play well. Indeed consider the very simple deterministic game played on a cycle of length greater than 3; a robber can avoid capture indefinitely by traveling away from the cop. Then 
 is stationary for every state s. Consider a play where the robber decides to stop after having traveled 8 times around the cycle. The play is winning for the cop, but even if the cop follows the optimal strategy, the same state is encountered 8 times.

Proposition 2.26

Let  be a GPCR game and . Then, the recursion 
 defined by Equation (5) is such that:

1.
if

Image 17
, then for every ,
Image 18
;
2.
if

Image 19
, then 
 is not stationary.
Proof

For the first claim, assume that

Image 20
. Then there is a path from state s to a final state in F that follows
Image 21
(and that has positive probability). If this path is longer than
Image 6
then it contains a repetition of at least one state 
, at turns, say, 
, and 
. Consider the finite horizon strategy that follows
Image 21
for the first 
 turns, and then follows
Image 22
, which is the strategy followed by
Image 21
when 
 of π was encountered for the second time originally in π. So removing from π the subpath between 
 and 
, we obtain a shorter path that has positive value and that follows this strategy. By continuing this procedure, we obtain a path of length  or less and Claim 1 is proved.
From Lemma 2.25, if 
 is stationary from N, there is no (positive, or winning) plays where the same state is encountered twice in the N first turns of 
 following 
⁎
. Now, suppose . Thus, there is no repetition of states, which implies that for all , all paths that contribute to the value 
 are of length at most , and the result follows. □

It is interesting to note the contrapositive of the second item in Proposition 2.26, that if 
 is stationary for some state s, then

Image 23
. In other words, the stationary part starts at most at turn
Image 6
. This result is by state, so other states may not be stationary. Note however that we cannot deduce stationarity from observing
Image 23
because the sequence may stay stable for a few turns and then be updated with a positive value. Anyway, we can complete the algorithmic complexity presented in Proposition 2.24.
Corollary 2.27

In the worst case, under a dynamic programming approach at most

Image 24
operations are sufficient in order to determine whether 
 is null, stationary equal to a number  or infinitely increasing.
Proof

The result follows from Proposition 2.26, Proposition 2.24 by substituting n for

Image 6
. For stationarity, for example, if 
 is stationary, then 
 is stationary for all , so we can conclude that 
 is stationary at
Image 16
. □
2.7. Bonato and MacGillivray's generalized Cops and Robbers game
This subsection is dedicated to our comparison with Bonato and MacGillivray's generalized Cops and Robbers game [2], which is another attempt at studying Cops and Robbers games in general forms. For the sake of self-containment, their model is transcribed here. This model is completely deterministic and thus is included as a special case of Definition 2.1.

Bonato and MacGillivray's game is presented in the following definition.

Definition 2.28 Bonato and MacGillivray's game

A discrete time process  is a generalized Cops and Robbers game if it satisfies the following rules:

1.
Two players, pursuer and evader compete against each other.

2.
There is perfect information.

3.
There is a set 
 of admissible positions for the pursuer and a set 
 for the evader. The set of admissible positions of the game is the subset 
 of positions that can be reached according to the rules of the game. The set of game states is the subset  such that 
 if, when X is the player next to play, the position 
 can be reached by following the rules of the game.

4.
For each game state and each player, there exists a non-empty set of allowed movements. Each movement leaves the other player's position unchanged. We write 
 the set of allowed movements for the pursuer when the game state is 
 and 
 for the set of movements allowed to the evader when the game state is 
.

5.
The rules of the game specify how the game begins. Thus, there exists a set 
 of admissible starting positions. We define 
 and, for 
, we define the set 
. The game  starts with the pursuer choosing a starting position 
 and then the evader choosing a starting position 
.

6.
After both players have chosen their initial positions, the game unfolds alternatively with the pursuer moving first. Each player, on his turn, must choose an admissible action given the current state.

7.
The rules of the game specify when the pursuer has captured the evader. In other words, there is a subset  of final positions. The pursuer wins  if, at any moment, the current position belongs to . The evader wins if his position never belongs to .

Only Cops and Robbers games in which the set  is finite are considered. Games considered are played on a finite sequence of turns indexed by natural integers including 0.
We also present how the same authors defined an extension of the relation 
 of Nowakowski and Winkler [22] in order to solve the set of games characterized by their model.

Definition 2.29

Bonato and MacGillivray's 
Let  be a Cops and Robbers game given by Definition 2.28. We let:

1.
 if and only if 
.

2.
Suppose that 
 have all been defined for some . Define 
 if 
 or if 
 and for all 
 either 
 or there exists some 
 such that 
 for some .

By definition, 
 contains 
 for all . Since 
 are finite, there exists some t such that 
 for all . We define 
.
Bonato and MacGillivray then use the relation defined in Definition 2.29 to show a necessary and sufficient condition for the existence of a winning strategy for the pursuer that is greatly similar to corresponding theorem of Nowakowski and Winkler [22].

Theorem 2.30

The copwin theorem of Bonato and MacGillivray
The pursuer has a winning strategy in a game of Cops and Robbers characterized by Definition 2.28 if and only if there exists some 
 such that for all 
, either 
 or there exists 
 such that 
.

It should be clear at this point that both Definition 2.1, Definition 2.28 describe alternative pursuit games of perfect information that unfold on discrete structures. Although the notation is different in both cases, it should also be clear that Bonato and MacGillivray's model is embedded in ours. The only difference between our formalism has to do with the initial states. Indeed, we only allow one initial state, 
, which is not the case in Definition 2.28. This does not cause any problem as it suffices to play one more turn in our model, or even to modify the first reachable states. In order to simplify what follows, we assume the set of initial states in both models are equivalent. We conclude that Equation (5) should encode the relation 
 of Definition 2.29. Indeed, other than its deterministic character, the relation 
 is greatly similar to our recursion. Both relations are binary and recursive. Both share the same structure: a single case when  in which both players may not make another move; a second case when , but the current state is final; finally, a last case, again when , when both players must choose an action that is optimal in the subsequent turns. Thus, we formally show how those two equations are related in the coming lines.
We first note that Equation (5) can be simplified when following Bonato and MacGillivray's model. Since the component 
 is not used in what follows, we simply write . Since the game is deterministic, we let players choose their next position directly. The recursion 
 is thus given by:(7)
  
 
 

The following theorem thus makes the connection between the two formalisms that are our model and that of Bonato and MacGillivray. In order to clarify the exposition, the relation 
 is written in our model, that of Definition 2.1. Given the preceding remarks, this should incur no loss of generality.

Theorem 2.31

Let the relation 
 be given by Definition 2.29 and 
 the recursion given by Equation (5). Assume  is a GPCR game given by Definition 2.1, but following the specifications of Definition 2.28. Then, we have:(8)

Proof

First, observe that relation 
 compares the positions of the pursuer and the evader. These positions are encoded in the game states S of our model. Moreover, the set of actions  defined in model 2.28 are in fact restrictions of the set of actions from model 2.1. Indeed, actions in  directly correspond to game positions, whereas we enable, in Definition 2.1, the action sets to be disjoint from the set of states. It is thus possible to define a game  that respects the hypotheses of Definition 2.28 and where Expression (8) is well-defined. A subtle difference between both formalisms has to do with the turn counters: in relation 
 the cops are next to play while the robbers are to make their move in relation 
. This does not change the fact that cops play first in both games. Now, we prove the result by induction, similarly as in the proof of Proposition 3.2.

Base case: . 
 if and only if  and  if and only if 
.

Induction step. Assume the result holds for  and let's show it for . It holds that 
 if and only if , in which case 
 by definition, or there exists an action 
 for the cops such that no matter the response 
 of the robbers, we have 
. By the induction hypothesis, we have 
 if and only if there exists an action 
 such that 
. Thus, if the cops play action 
, they position themselves on a state in which 
. Conversely, assume there exists an action 
 such that 
. Then, by definition, for all response 
 of the robbers there exists an action 
 of the cops such that 
. In this case, by the induction hypothesis, we have 
. The cops play action 
, in which case we have 
. □

3. A concrete model of GPCR games
In this section we present a more concrete model of GPCR games that is closer to the usual definitions in the literature. Thus, we specify that the game is played on a graph, without pointing out its particular shape. The actions of the players will correspond to paths as in the game of Cop and Fast Robber [21]. The game presented in Definition 2.1 is abstract because its sets do not depend on any precise structure and so neither does the algorithmic complexity of computing Equation (5). The point of reformulating Definition 2.1 is to refine some results and formulate them in terms of the graph's structure.

3.1. Definition of concrete Cops and Robbers games
In the game presented below, players walk on paths since it appears, in light of the literature, that such actions are most general. We also grant the cops a watch zone that enables them to capture the robbers whenever they are observed. We write  for the set of finite paths in a graph and 
 for the set of paths that start on vertex . To simplify the notation, we formulate the concrete model in the setting where there are one cop and one robber, and without the auxiliary information set 
. The extension to the general case is straightforward.

Definition 3.1

A GPCR game 
 with one cop and one robber (Definition 2.1) is concrete if there is a graph  satisfying:

1.
 is a finite set of configurations of the game.

2.
, where 
.

3.
 is the set of configurations of the cop. The second coordinate is the cop's watch zone.

4.
 is the set of positions of the robber.

5.
 is the set of available actions for the cop. He can move along a path from his present position c and choose a watch zone. From the initial state, 
.

6.
 is the set of available actions for the robber. She can move along a path from her present position r. From the initial state, 
.

The definition of a play and all previous remarks and details that apply to Definition 2.1 are still applicable in Definition 3.1. A peculiarity here is how we let the cop have his own watch zone consisting in a set of edges. Thus, the cop can only capture the robber on the robber's turn. Indeed, seeing as the robber moves along paths, we can explicitly deduce at what point a robber is susceptible to get caught crossing a cop's watch zone. It's a natural choice of modeling that makes writing the probability of capture easier.

3.2. Example: classic Cop and Robber game
Nowakowski and Winkler's, and Quilliot's, game is now presented in the form of Definition 3.1. In this game, we will consider that the game is over not when the cop reaches the same position as the robber, but exactly after that, during the robber's turn, when she tries to escape. This slightly different interpretation leads to the same game. Our presentation allows to model and solve a more general situation where the robber could have a possibility of escaping, even if the cop reaches the robber's position. Let  be a finite, undirected, reflexive and connected graph and let:
 The watch zone 
 of the next state is the set of adjacent edges of the cop's next position 
. The final states are those in which both players stand on the same vertex, . The initial state is 
 and we let players choose any vertex from it, that is, 
, with . Finally, the probabilities of transition are trivial since the game is deterministic.

Now, in order to show that Equation (5) is well-defined, we demonstrate how it encodes the relation 
 of Nowakowski and Winkler [22]. Since the game is deterministic, Equation (5) reduces to:(9)
  
 
 This equation is also a particular case of Equation (7). The next proposition shows that Equation (9) simulates the relation 
.

Proposition 3.2

It holds that 
 if and only if there exists a vertex 
 such that 
.

Proof

We prove the result by induction. We note that in recursion 
 it is the cop's turn to play, while in relation 
 the robber is next to move.

Base case: . 
 if and only if  and  if and only if 
.

Induction step. Assume the result holds for  and let us show it holds for . Then, 
 if and only if there exists an action 
 for the cop from which, no matter the response 
 of the robber, we have 
. By the induction hypothesis, 
 if and only if there exists a vertex 
 such that 
. Thus, the cop can play action 
 and we have 
. Conversely, if there exists a vertex 
 such that 
, then, by definition, for any action 
 of the robber there exist a response 
 of the cop such that 
. By the induction hypothesis, we thus have 
. In this case, the cop can play action 
 such that, no matter the answer of the robber 
, 
. By definition, we thus have 
. □

3.3. Example: Cop and Fast Defending Robber game
Definition 3.1 is further illustrated on the following example. It describes the game of Cop and Fast Robber with probability of capture, which is a variant of the one presented by Fomin et al. [10], already mentioned in Example 2.5, and a variant of Example 2.4, where the robber could evade from capture. Unsurprisingly, given both games ask of robbers to move along paths, it is easier to write this new game following Definition 3.1.

For a path  on a graph G, we write  for its 
 vertex and ⁎ for its last one. Let  be a finite graph. Assume that the cop guards a watch zone  and that each time the robber crosses an edge e he survives his walk with probability 
 (between 0 and 1). In Example 2.4, a capture probability was used, here we define a survival probability as it is simpler to use in the current context. Contrary to the Defending Robber game 2.4, the probability of survival depends on the cop's watch zone as well as the robber's action. Here, only the cop's watch zone and the transition functions are modified compared to Example 2.5. So we have an element 
⁎
 and the set of final states are 
⁎
⁎
. We write 
 for the set of edges incident to c. Similarly, we write 
 for the set of edges of a path π. Let:
  The robber's transition function is given by:
⁎
⁎
  where 
⁎
 is a function satisfying:
⁎
⁎
⁎
⁎
  Note that to retrieve the game considered in Example 2.5 and Marcoux's thesis [21], we should rather use a watch zone 
 containing all edges on paths of length 2 from c and change the conditions on 
 for 
 and 
, where 
 is the subpath of π starting in .

Since the watch zone is determined by the cop's position, we can use the simplified notation  for a state 
. Thus, the recursion of Equation (5) can be written as follows. For the jail state: 
⁎
⁎
 for all . For 
⁎
⁎
, we have 
 and, for ,
  
 
⁎
⁎
⁎
⁎

Following Proposition 2.24, the algorithmic complexity of the previous recursion is at most

Image 25
, where Δ is the maximal degree of G. Indeed, S corresponds to the set of pairs of vertices, the cop can only move on his neighbourhood and the robber is allowed to choose any path of finite length. Hence, even if we restrict the possible paths that can choose the robber to elementary paths (paths that do not cross twice a same vertex), the size of the possible robber actions, and therefore the size of , is exponential in the size of the graph on which the game is played. However, as shown in the next proposition, 
 can be computed in polynomial time in the size of the graph itself.
Proposition 3.3

Computing 
 in the Cop and Fast Defending Robber game requires at most

Image 26
operations and uses at most
Image 27
space, for any .
Proof

Let 
 be the set of paths beginning in r and ending in 
. Let 
 be a cop position. The robber's transition function can be simplified by assuming 
 if 
. Then, 
 if the robber is not caught on 
. The previous recursion, when state  is not final, can be simplified to:
  
 
  
 
 
  
 
 
 
 If 
 and 
 are fixed, we look for the path π minimizing the expression in parentheses, so maximizing 
. This is the same path that maximizes 
 because log is a monotone increasing function. Because 
, with 
, we can minimize 
.

Observe that the survival probabilities 
 depend only on the vertex 
 and the edge e. Thus, prior to evaluating 
, we can precompute

Image 28
all-pairs shortest paths (one for each possible cop position) by weighting each edge  with 
 for each source . This is done in
Image 29
operations, for example by using the algorithm of Fredman and Tarjan [12]. This takes
Image 30
space (because the path does not have to be stored, it can be recomputed in
Image 31
). Thus, for each 
 we store the values 
 for the shortest path π between r and 
. Finding the next robber position 
 thus requires at most
Image 32
operations.
Now, assume 
 is computed for all 
, 
, in time 
. We look for the vertex 
 maximizing 
 
 
 
 The values 
 are already computed, as well as the 
's. Thus, at most

Image 33
operations are required to evaluate expression 
 when c and r are fixed. To find all maxima, that is for all  and , we need to make at most
Image 34
operations. On turn n, we make a number of operations
Image 35
. The total complexity is thus: The bottlenecks for the spatial complexity are the shortest path algorithms which require at most
Image 30
space. For each 
 we only need 
 so we do not need to store any other 
 for . □
An important aspect of the fast robber game is its ability to model situations of imperfect information in which the cops only gather information on the robber's position at regular intervals. This game, deemed with witness, is shown by Chalopin et al. [6] to correspond to the game of Cop and Fast Robber (without watch zone). In essence, the authors present an equivalence between the classes of copwin graphs in the witness game and in the fast robber one. We can wonder if the same could be said of the stochastic case.

3.4. Example: Cop and Drunk Robber game
Let us revisit the Cop and Drunk Robber game of Example 2.3 with the concrete model and Equation (5).

We can show how it is always easier to capture a robber moving randomly than a robber playing optimally. For the sake of generality, assume the robber can play according to any distribution in 
 when she finds herself on vertex r. Let 
 be a sequence of distributions on the vertices V and 
 its component that is in 
. Then, we write 
 for the recursion in which 
. In other words, we write:(10)
  
 
 if  and 
 if , for all . The classic recursion from Equation (9) is written 
.

Proposition 3.4

It is always easier to capture a robber playing randomly than an adversarial one, that is:

Proof

We write 
 for the set of Dirac distributions defined on . The robber would be harder to capture if she were to minimize her probability of capture only on Dirac distributions because her optimal strategy is deterministic. Thus, we compute
  
 
  
  
 
  
  
 
  
 
 The first line is the definition of Equation (10) with the robber playing according to distribution ϕ. If she could choose this distribution, she could fall on a distribution 
 ensuring her a greater probability of survival, that justifies the second line. Then, we observe that since her optimal strategy is deterministic, it corresponds to a sequence of Dirac distributions and she loses nothing in playing according to 
. The last line is simply the preceding one rewritten without distributions, as in this case 
 is concentrated on a single vertex 
. □

3.5. Example: temporal Cop and Robber game
In graph theory, one can define many random processes to stochastically generate graphs that vary on each time step. One thus obtains a sequence of graphs 
 that represents the evolution of a network over time. Those graphs are called dynamic graphs, link streams, time-varying graphs or temporal networks, depending on the community, and can model, for example, the destruction of a bridge or of a road that makes it impossible for the players to pass through it.

Suppose k cops are chasing l robbers on the sequence 
. In order to take into account the variable nature of the underlying structure of a game from Definition 3.1, we can make use of the component 
 as a turn counter. Let 
 be the graph generated at time t,
 and 
. Hence, on each time step t a new graph 
 is created according to a certain process and the set of states is renewed. The sets of actions can also be redefined. Let 
 be the set of finite paths on 
 that begin on vertex 
. The sets of actions are thus:
 
 
 Since this example is rather general, we let the transition functions be undefined. We require, however, that the transition functions follow the arrow of time: if 
 is a game state at time t and 
 is a cop action, then 
 only if 
. The same holds for the robbers.

4. Conclusion
This paper presented a relatively simple yet very general model in order to describe games of Cops and Robbers that, notably, may include stochastic aspects. The game  was presented along with a method of resolution in the form of a recursion 
 in Theorem 2.13. We show in Proposition 2.16 that we can always retrieve an ϵ-optimal strategy for  from the recursion 
 (for large enough n). Moreover, in Proposition 2.26 we show that if the recursion becomes stationary, stationarity must occur at most at index

Image 6
. This is a first step in the analysis of the rate of convergence of the recursion.
We have exposed how some classic Cops and Robbers games can be written into our model and extended. Many more games could now be studied as GPCR such as the Firefighting game (under certain conditions) in which a team of firefighters seeks to prevent the nodes of a graph from burning. An interesting notion that is captured by our framework, in Definition 3.1, is that of the surveillance zones of the cops that can be chosen at each step. Thus, we claim a wide variety of games of Cops and Robbers can be solved with the concepts developed in this paper. Furthermore, such a broad exposition of games of Cops and Robbers as ours enables one to study the effects of modifying certain rules, for example on the number of cops or on the speed of players, on the games. That is, one can use Equation (5) and probe its values in order to test how modifying these rules affect the ability of the cops to capture the robbers.

We have extended the classic notion of cop number with the p-cop number, although the question is still open about the behaviour of this function. The expected capture time of the robbers is also of great interest. This function can now be studied on large swaths of Cops and Robbers games. In part, this question can be motivated by a paper of Simard et al. [27] on the relation between an Operations Research problem and the resolution of a Cop and Drunk Robber game. Specifically, the authors tackled the problem of upper bounding the probability of detecting a hidden and randomly moving object on a graph with a single optimally moving searcher. This problem, being NP-hard [28], is constrained to be solved in a maximum number of time steps . In particular, it appears that if one could tightly upper bound the expected capture time of a game derived from Definition 2.1, then one could, following the ideas presented in this paper, deduce the optimal number of searchers to send on a mission to rescue the object. Then, if this number were deduced, one could further apply the ideas of this article along with Equation (5) in order to help solve this search problem with multiple searchers. This is only a small justification for studying those questions, albeit, it seems, a valuable one.

Finally, a last avenue of research that is worth mentioning and that is possibly of most interest to researchers in robotics and operations research concerns the extension of model 2.1 to games of imperfect information. Imperfect information describes the lack of information from one or both players. Cops and Robbers games of imperfect information thus contain games in which robbers are invisible, that can model problems of graph search such as the one mentioned above. Game theory seems apt to enable the transition from perfect information to imperfect information games with the use of belief states. Such generalization could be paired with the branch and bound method presented in Simard et al. [27] in order to solve more general search problems.

In light of the literature on Cops and Robbers games it appears this paper distances itself from most studies on the subject. Indeed, we do not claim any results on typical Cops and Robbers questions such as the asymptotic behaviour of 
 or on dismantling schemes to characterize classes of winning graphs. However, we think that modelling such a wide variety of games opens the door to further studies on Cops and Robbers games that can now be tackled in their generality, which was not possible before. Thus, although our model may not enable one to compute analytical solutions on classical questions of Cops and Robbers games, we have good hope that algorithmic ones will be devised in order to solve more general problems on classes, not of graphs, but of games. In short, it appears that new and promising avenues of research have come to light with the objects presented in this paper and we hope researchers will be driven to tackle those open questions that were unearthed.