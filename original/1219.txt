We present an efficient coreset construction algorithm for large scale Support Vector Machine SVM training in Big Data and streaming applications. A coreset is a small representative subset of the original data points such that a model trained on the coreset is provably competitive with that trained on the original data set. Since the size of the coreset is generally much smaller than the original set our preprocess then train scheme has potential to lead to significant speedups when training SVM models. We prove lower and upper bounds on the size of the coreset required to obtain small data summaries for the SVM problem. As a corollary we show that our algorithm can be used to extend the applicability of any off the shelf SVM solver to streaming distributed and dynamic data settings. We evaluate the performance of our algorithm on real world and synthetic data sets. Our experimental results reaffirm the favorable theoretical properties of our algorithm and demonstrate its practical effectiveness in accelerating SVM training.Previous article in issueNext article in issueKeywordsSupport vector machinesCoresetsData reductionLarge scale learningStreamingLoading...Recommended articlesCiting articles ☆This research was supported in part by the U.S. National Science Foundation NSF under Awards Office of Naval Research ONR Grant N Microsoft corporation and JP Morgan Chase.These authors contributed equally.View Abstract© Elsevier B.V. All rights reserved

Abstract
We present an efficient coreset construction algorithm for large-scale Support Vector Machine (SVM) training in Big Data and streaming applications. A coreset is a small, representative subset of the original data points such that a model trained on the coreset is provably competitive with that trained on the original data set. Since the size of the coreset is generally much smaller than the original set, our preprocess-then-train scheme has potential to lead to significant speedups when training SVM models. We prove lower and upper bounds on the size of the coreset required to obtain small data summaries for the SVM problem. As a corollary, we show that our algorithm can be used to extend the applicability of any off-the-shelf SVM solver to streaming, distributed, and dynamic data settings. We evaluate the performance of our algorithm on real-world and synthetic data sets. Our experimental results reaffirm the favorable theoretical properties of our algorithm and demonstrate its practical effectiveness in accelerating SVM training.

Previous
Next 
Keywords
Support vector machines

Coresets

Data reduction

Large-scale learning

Streaming

1. Introduction
Popular machine learning algorithms are computationally expensive, or worse yet, intractable to train on massive data sets, where the input data set is so large that it may not be possible to process all the data at one time. A natural approach to achieve scalability when faced with Big Data is to first conduct a preprocessing step to summarize the input data points by a significantly smaller, representative set. Off-the-shelf training algorithms can then be run efficiently on this compressed set of data points. The premise of this two-step learning procedure is that the model trained on the compressed set will be provably competitive with the model trained on the original set – as long as the data summary, i.e., the coreset, can be generated efficiently and is sufficiently representative.

Coresets are small weighted subsets of the training points such that models trained on the coreset are approximately as good as the ones trained on the original (massive) data set. Coreset constructions were originally introduced in the context of computational geometry [1] and subsequently generalized for applications to other problems, such as logistic regression, neural network compression, and mixture model training [2], [3], [4], [5], [6], [7], [8] (see [9] for a survey).

A popular coreset construction technique – and the one that we leverage in this paper – is to use importance sampling with respect to the points' sensitivities. The sensitivity of each point is defined to be the maximum relative impact of each data point on the objective function across all possible scenarios.2 Points with high sensitivities have a large impact on the objective value (in the worst case) and are sampled with correspondingly high probability, and vice-versa. The main challenge in generating small-sized coresets often lies in evaluating the importance of each point in an accurate and computationally-efficient way.

1.1. Our contributions
In this paper, we propose an efficient coreset construction algorithm to generate compact representations of large data sets to accelerate SVM training. Our approach hinges on bridging the SVM problem with that of k-means clustering. As a corollary to our theoretical analysis, we obtain theoretical justification for the widely reported empirical success of using k-means clustering as a way to generate data summaries for large-scale SVM training. In contrast to prior approaches, our approach is both (i) provably efficient and (ii) naturally extends to streaming or dynamic data settings. Above all, our approach can be used to enable the applicability of any off-the-shelf SVM solver – including gradient-based and/or approximate ones, e.g., Pegasos [10], to streaming and distributed data settings by exploiting the composibility and reducibility properties of coresets [9], [11].

In particular, this paper contributes the following:

1.
A coreset construction algorithm for accelerating SVM training based on an efficient importance sampling scheme.

2.
An analysis proving lower bounds on the number of samples required by any coreset construction algorithm to approximate the input data set.

3.
Theoretical guarantees on the efficiency and accuracy of our coreset construction algorithm.

4.
Evaluations on synthetic and real-world data sets that demonstrate the effectiveness of our algorithm in both streaming and offline settings.

2. Related work
Training SVMs requires 
 time and 
 space in the offline setting where n is the number of training points. Towards the goal of accelerating SVM training in the offline setting, [12], [13] introduced the Core Vector Machine (CVM) and Ball Vector Machine (BVM) algorithms, which are based on reformulating the SVM problem as the Minimum Enclosing Ball (MEB) problem and Enclosing Ball (EB) problem, respectively, and by leveraging existing coreset constructions for each; see [14]. However, CVM's accuracy and convergence properties have been noted to be at times inferior relative to those of existing SVM implementations [15]; moreover, unlike the algorithm presented in this paper, neither the CVM, nor the BVM algorithm extends naturally to streaming or dynamic settings where data points are continuously inserted or deleted. Similar geometric approaches, including extensions of the MEB formulation, those based on convex hulls and extreme points, among others, were investigated by [16], [17], [18], [19], [20], [21]. Another class of related work includes the use of canonical optimization algorithms such as the Frank-Wolfe algorithm [22], Gilbert's algorithm [22], [23], and a primal-dual approach combined with Stochastic Gradient Descent (SGD) [24].

SGD-based approaches, such as Pegasos [10], have been a popular tool of choice in approximately-optimal SVM training. Pegasos is a stochastic sub-gradient algorithm for obtaining a -approximate solution to the SVM problem in 
 time for a linear kernel, where λ is the regularization parameter and d is the dimensionality of the input data points. In contrast to our method, these approaches and their corresponding theoretical guarantees do not feasibly extend to dynamic data sets and/or streaming settings. In particular, gradient-based approaches cannot be trivially extended to streaming settings since the arrival of each input point in the stream results in a change of the gradient.

There has been prior work in streaming algorithms for SVMs, such as those of [16], [18], [25], [21]. However, these works generally suffer from poor practical performance in comparison to that of approximately optimal SVM algorithms in the offline (batch) setting, high difficulty of implementation and application to practical settings, and/or lack of strong theoretical guarantees. Unlike the algorithms of prior work, our method is simultaneously simple-to-implement, exhibits theoretical guarantees, and naturally extends to streaming and dynamic data settings, where the input data set is so large that it may not be possible to store or process all the data at one time.

3. Problem definition
Let 
 denote a set of n input points. Note that for each point , the last entry 
 of x accounts for the bias term embedding into the feature space.3 To present our results with full generality, we consider the setting where the input points P may have weights associated with them. Hence, given P and a weight function 
, we let  denote the weighted set with respect to P and u. The canonical unweighted case can be represented by the weight function that assigns a uniform weight of 1 to each point, i.e.,  for every point . For every , let 
. We consider the scenario where n is much larger than the dimension of the data points, i.e., .

For a normal to a separating hyperplane 
, let 
 denote vector which contains the first d entries of w. The last entry of w (
) encodes the bias term . Under this setting, the hinge loss of any point  with respect to a normal to a separating hyperplane, 
, is defined as 
, where 
. As a prelude to our subsequent analysis of sensitivity-based sampling, we quantify the contribution of each point  to the SVM objective function as(1)
 
 where  is the SVM regularization parameter, and 
 is the hinge loss with respect to the query 
 and point . Putting it all together, we formalize the λ-regularized SVM problem as follows.

Definition 1

λ-regularized SVM Problem
For a given weighted set of points  and a regularization parameter , the λ-regularized SVM problem with respect to  is given by 
 
 where(2)
 

We let 
⁎
 denote the optimal solution to the SVM problem with respect to , i.e., 
⁎
. A solution 
 is an ξ-approximation to the SVM problem if 
⁎
. Next, we formalize the coreset guarantee that we will strive for when constructing our data summaries.

Coresets  A coreset is a compact representation of the full data set that provably approximates the SVM cost function (2) for every query 
 – including that of the optimal solution 
⁎
. We formalize this notion below for the SVM problem with objective function 
 as in (2) below.

Definition 2

ε-coreset
Let  and let  be the weighted set of training points as before. A weighted subset , where  and 
 is an ε-coreset for  if(3)

This strong guarantee implies that the models trained on the coreset  with any off-the-shelf SVM solver will be approximately (and provably) as good as the optimal solution 
⁎
 obtained by training on the entire data set . This is formalized by the simple lemma below and we provide the proof inline to distinctly demonstrate the advantage of ε-coresets as defined above.

Lemma 3

Let  be a weighted set, 
 
 and let  be an ε-coreset with respect to . Let 
⁎
 be the optimal solution on the coreset  and similarly let 
⁎
 be the optimal solution with respect to . Then,
⁎
⁎

Proof

We have
⁎
⁎
 
⁎
 
 
⁎
⁎
 where the first and third inequalities follow from  being an ε-coreset (see Definition 2), the second inequality holds by definition of 
⁎
, and the last inequality follows from  using 
 
. □

This also implies that, if the size of the coreset is provably small, e.g., logarithmic in n (see Sec. 5), then an approximately optimal solution can be obtained much more quickly by training on  rather than , leading to computational gains in practice for both offline and streaming data settings (see Sec. 8). The difficulty in constructing coresets lies in constructing them (i) efficiently, so that the preprocess-then-train pipeline takes less time than training on the full data set and (ii) accurately, so that important data points – i.e., those that are imperative to obtaining accurate models – are not left out of the coreset, and redundant points are eliminated so that the coreset size is small. In the following sections, we introduce and analyze our coreset algorithm for the SVM problem.

4. Method
Our coreset construction scheme is based on the unified framework of [7], [8] and is shown in Algorithm 1. The crux of our algorithm lies in generating the importance sampling distribution via efficiently computable upper bounds (proved in Sec. 5) on the importance of each point (Lines 1–11). Sufficiently many points are then sampled from this distribution and each point is given a weight that is inversely proportional to its sample probability (Lines 12–13) to obtain an unbiased estimator for the objective function across all points. The number of points required to generate an ε-coreset with probability at least  is a function of the desired accuracy ε, failure probability δ, and complexity of the data set (t from Theorem 8). Under mild assumptions on the problem at hand (see Sec. A.4), the required sample size is polylogarithmic in n.

Algorithm 1
Download : Download high-res image (193KB)
Download : Download full-size image
Algorithm 1.

Image 1
.
Our algorithm is an importance sampling procedure that first generates a judicious sampling distribution based on the structure of the input points and samples sufficiently many points from the original data set. The resulting weighted set of points , serves as an unbiased estimator for 
 for any query 
, i.e., 
. Although sampling points uniformly with appropriate weights can also generate such an unbiased estimator, it turns out that the variance of this estimation is minimized if the points are sampled according to the distribution defined by the ratio between each point's sensitivity and the sum of sensitivities, i.e.,  on Line 13 [26].

4.1. Computational complexity
Coresets are intended to provide efficient and provable approximations to the optimal SVM solution. However, the very first line of our algorithm entails computing an (approximately) optimal solution to the SVM problem. This seemingly eerie phenomenon is explained by the merge-and-reduce technique [27] that ensures that our coreset algorithm is only running against small partitions of the original data set [6], [27], [28]. The merge-and-reduce approach leverages the fact that coresets are composable and reduces the coreset construction problem for a (large) set of n points into the problem of computing coresets for 
 
 points, where  is the minimum size of input set that can be reduced to half using Algorithm 1 [6]. Assuming that the sufficient conditions for obtaining polylogarithmic size coresets implied by Theorem 8 hold, the overall time required is approximately linear in n.

5. Analysis
In this section, we analyze the sample-efficiency and computational complexity of our algorithm. The outline of this section is as follows: we first formalize the importance (i.e., sensitivity) of each point and summarize the necessary conditions for the existence of small coresets. We then present the negative result that, in general, sublinear coresets do not exist for every data set (Lemma 5). Despite this, we show that we can obtain accurate approximations for the sensitivity of each point via an approximate k-means clustering (Lemma 6, Lemma 7), and present non-vacuous, data-dependent bounds on the sample complexity (Theorem 8).

5.1. Preliminaries
We will henceforth state all of our results with respect to the weighted set of training points , , and SVM cost function 
 (as in Sec. 3). The definition below rigorously quantifies the relative contribution of each point.

Definition 4

Sensitivity [6]
The sensitivity of each point  is given by(4) 
 
 

Note that in practice, exact computation of the sensitivity is intractable, so we usually settle for (sharp) upper bounds on the sensitivity  (e.g., as in Algorithm 1). Sensitivity-based importance sampling then boils down to normalizing the sensitivities by the normalization constant – to obtain an importance sampling distribution – which in this case is the sum of sensitivities 
. It turns out that the required size of the coreset is at least linear in t [6], which implies that one immediate necessary condition for sublinear coresets is .

5.2. Lower bound for sensitivity
The next lemma shows that a sublinear-sized coreset cannot be constructed for every SVM problem instance. The proof of this result is based on demonstrating a hard point set for which the sum of sensitivities is , ignoring d factors, which implies that sensitivity-based importance sampling roughly boils down to uniform sampling for this data set. This in turn implies that if the regularization parameter is too large, e.g., , and if  (as in Big Data applications) then the required number of samples for property (3) to hold is .

Lemma 5

For an even integer , there exists a set of weighted points  such that
 
 
 

We next provide upper bounds on the sensitivity of each data point with respect to the complexity of the input data. Despite the non-existence results established above, our upper bounds shed light into the class of problems for which small-sized coresets are ensured to exist.

5.3. Sensitivity upper bound
In this subsection we present sharp, data-dependent upper bounds on the sensitivity of each point. Our approach is based on an approximate solution to the k-means clustering problem and to the SVM problem itself (as in Algorithm 1). To this end, we will henceforth let k be a positive integer, 
⁎
 be the error of the (coarse) SVM approximation, and let 
, 
 and 
 for every ,  and  as in Lines 4–9 of Algorithm 1.

Lemma 6

Let k be a positive integer, 
⁎
, and let  be a weighted set. Then for every ,  and 
,
 
 
 
 

Lemma 7

In the context of Lemma 6, the sum of sensitivities is bounded by 
 
 
 
 
 where 
 for all  and .

Theorem 8

For any , let m be an integer satisfying
 
 where t is as in Lemma 7. Invoking

Image 2
with the inputs defined in this context yields a ε-coreset  with probability at least  in  time, where T represents the computational complexity of obtaining an ξ-approximated solution to SVM and applying k-means++ on 
 and 
.
Sufficient conditions and the effect of k-means on our sensitivity  Theorem 8 immediately implies that, for reasonable ε and δ, coresets of poly-logarithmic (in n) size can be obtained if , which is usually the case in our target Big Data applications, and if 
 
 
.

Despite the fact that any k-partitioning of the data can be applied instead of k-means for achieving an upper bound on the sensitivities of the points, it's important to note that k-means actually acts as a trade-off mechanism between the raw contribution and the actual contribution (the weight term and the max term from Lemma 6, respectively). Choosing the best k can be done via binary search over the values of k that minimize the sensitivity.

6. The rationale for clustering
Recall that the bound in Lemma 6 was achieved by using an approximate clustering algorithm (k-means++) as in Algorithm 1. Following our analysis from Sec. A.2, we observe that we can simply use any k-partitioning of the dataset, instead of applying k-means clustering. Moreover, we can also simply choose  which translates to simply taking the mean of the labeled points. Despite all of above, we did choose to use a clustering algorithm as well as having larger values of k, and such decisions were inspired by the following observations:

(i)
k-means clustering aims to optimize the sum of squared distances between the points and their corresponding center, which on some level, helps in lowering the distance between points and their corresponding centers. Such observation leads to having tighter bounds for the sensitivity of each point, consequently leading to lower coreset sizes; See Theorem 8.

(ii)
Having larger k also helps lowering the distance between a point and its corresponding center.

(iii)
k-means clustering acts as a trade-off mechanism between

•
the raw contribution of each point, which is translated into the weight of the point divided by the sum of the weights with respect to the cluster that each point is assigned to,

•
and the actual contribution of each point which is translated to the distance between each point and its corresponding center.

In light of the above, we observe that as k goes larger, the sensitivity of each point gets closer and closer to being simply the raw contribution, which in case of unweighted data set, is simply applying uniform sampling. Thus, for each weighted , we simply chose  where n denotes the total number of points in a P.

This observation helps in understanding how the sensitivities that we are providing for outliers and misclassified points actually quantify the importance of such points. Specifically speaking, as k increases, the outliers would mostly be assigned to the same cluster and since in general there aren't much of these points, we end up giving higher sensitivities for such points (than others) due to the fact that their raw contribution increases as the size of their corresponding cluster decreases. When k isn't large enough to separate these points from the rest of the data points, the actual contribution is used. This results in shifting the mean of the cluster towards the “middle” between the outliers and the rest of the points in same cluster. Such observation means that the actual contribution of the rest of points inside the same cluster is increased; See Fig. 1.

Fig. 1
Download : Download high-res image (329KB)
Download : Download full-size image
Fig. 1. Understanding the effect of k-means on the sensitivities of the points. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)

6.1. Automating the search for the (approximately) optimal k
To find the approximately optimal value of k, observe that the sum of sensitivities (see Lemma 7) contains two distinct expressions in terms of the number of clusters. By Lemma 7, they are (roughly) (1) k and (2) the sum of distances to the nearest cluster. Note that term (1) is increasing with k while (2) is decreasing with k. This means that we can perform a binary search over  to find the best k.

However, a challenge here is that the clusterings are generated by k-means++ in practice, which is only -competitive in expectation. Nevertheless, at the additional multiplicative cost of  time, we can run k-means++ for a clustering size of k  times and pick the best k-clustering to obtain a -competitive solution with probability at least  (amplification). Doing this for every k we pick as we binary search, and accounting for the total  iterations of binary search leads to a multiplicative factor increase of  in the computation time. Using the -competitiveness of all of the k-means instances implies that the binary search (and keeping track of the best k found thus far) generates a 
-clustering such that the sum of sensitivities is within  factor of the minimum (over all ) sum of sensitivities with respect to the best clustering, with probability4 at least .

The additional concern here is the run-time of k-means++, which is  for a clustering of size k [29]. To alleviate the dependence on d in the case of a high dimensional data set, we can use the Johnson-Lindenstrauss transform [30] on the input data points as a pre-processing step to reduce the dimensionality d to 
 at the cost of ε distortion in the pairwise distances between the points, while retaining the -competitiveness of the k-means++ solution [31]. This leads to at most  time per k-means++ invocation. To further reduce the runtime in the case of large values of k, we can restrict the end point of our binary search to e.g.,  (or even ), since any  would yield a sum of sensitivities that is , and in turn, a sampling complexity of at least  – which would imply a higher-than-ideal coreset size in the first place. With these changes in place, the runtime per invocation of k-means++ reduces to , where b ( for instance) is the end-point of the binary search over , without compromising its -competitiveness.

7. Extension to streaming settings
As a corollary to our main method, Algorithm 2 extends the capabilities of any SVM solver, exact or approximate, to the streaming setting, where data points arrive in a stream. Algorithm 2 is inspired by [7], [8] and constructs a binary tree, termed the merge-and-reduce tree, starting from the leaves which represent chunks of the data stream points. For each batch of l points, we construct an ε-coreset using Algorithm 1, which then is added to 
, a bucket which is responsible for storing each of the parent nodes of the leaves (Lines 1–6).

Algorithm 2
Download : Download high-res image (123KB)
Download : Download full-size image
Algorithm 2.

Image 3
.
Note that for every , the bucket 
 will hold every node which is a root of a subtree of height i. For every two successive items in each bucket 
, for every , a parent node is generated by computing a coreset on the union of the two successive items, which is then, added to the bucket 
 (Lines 7–11). This process is applied until all the buckets are emptied other than 
, that will contain only one tuple  which is set to be the root of the merge-and-reduce tree.

In sum, we obtain a binary tree of height  for a stream of n data points. At Lines 4 and 10, we have used error parameter 
 and failure parameter 
 in order to obtain ε-coreset with probability at least .

Coreset construction of size poly-logarithmic in n  In case when the total sensitivity is sub-linear in n where n denotes the number of points in P, which is obtained by Lemma 6, we provide the following theorem which constructs a -coreset of size poly-logarithmic in n.

Lemma 9

Let 
 
 
, 
 
, , a weighted set , 
⁎
 where 
⁎
. Let t denote the total sensitivity from Lemma 6 and suppose that there exists  such that 
. Let 
 
 
 and let  be the output of a call to

Image 4
. Then  is an ε-coreset of size
8. Results
In this section, we present experimental results that demonstrate and compare the effectiveness of our algorithm on a variety of synthetic and real-world data sets in offline and streaming data settings [32]. Our empirical evaluations demonstrate the practicality and wide-spread effectiveness of our approach: our algorithm consistently generated more compact and representative data summaries, and yet incurred a negligible increase in computational complexity when compared to uniform sampling.

Evaluation  We considered 6 real-world data sets of varying size and complexity as depicted in Table 1. For each data set of size n, we selected a set of  geometrically-spaced subsample sizes 
. For each sample size m, we ran each algorithm (Algorithm 1 or uniform sampling) to construct a subset  of size m. We then trained the SVM model as per usual on this subset to obtain an optimal solution with respect to the coreset , i.e., 
⁎
. We then computed the relative error incurred by the solution computed on the coreset (
⁎
) with respect to the ground-truth optimal solution computed on the entire data set (
⁎
): 
⁎
⁎
⁎
. The results were averaged across 100 trials.


Table 1. The number of input points and measurements of the total sensitivity computed empirically for each data set in the offline setting. The sum of sensitivities is significantly less than n for virtually all of the data sets, which, by Theorem 8, ensures the sample-efficiency of our approach on the evaluated scenarios.

Measurements∖Dataset	HTRU	Credit	Pathol.	Skin	Cod	W1
Number of data-points (n)	17,898	30,000	1,000	245,057	488,565	49,749
Sum of Sensitivities using k-means (t)	630.81	1264.68	111.1	380.74	2980.81	18062.98
Sum of Sensitivities using k-medians (t)	585.47	1178.83	104.24	375.13	2902.46	17367.34
t/n when using k-means (Percentage)	3.52%	4.22%	11.11%	0.16%	0.61%	36.31%
t/n when using k-medians (Percentage)	3.27%	3.93%	10.42%	0.15%	0.59%	34.91%
Fig. 2, Fig. 3 depict the results of our comparisons against uniform sampling in the offline setting. In Fig. 2, we see that the coresets generated by our algorithm are much more representative and compact than the ones constructed by uniform sampling: across all data sets and sample sizes, training on our coreset yields significantly better solutions to SVM problem when compared to those generated by training on a uniform sample. For certain data sets, such as HTRU, Pathological, and W1, this relative improvement over uniform sampling is at least an order of magnitude better, especially for small sample sizes. Fig. 2 also shows that, as a consequence of a more informed sampling scheme, the variance of each model's performance trained on our coreset is much lower than that of uniform sampling for all data sets.

Fig. 2
Download : Download high-res image (562KB)
Download : Download full-size image
Fig. 2. The relative error of query evaluations with respect uniform and coreset subsamples for the 6 data sets in the offline setting. Shaded region corresponds to values within one standard deviation of the mean.

Fig. 3
Download : Download high-res image (525KB)
Download : Download full-size image
Fig. 3. The total computational cost of constructing a coreset and training the SVM model on the coreset, plotted as a function of the size of the coreset.

Fig. 3 shows the total computational time required for constructing the sub-sample (i.e., coreset)  and training the SVM on the subset  to obtain 
⁎
. We observe that our approach takes significantly less time than training on the original model when considering non-trivial data sets (i.e., ), and underscores the efficiency of our method: we incur a negligible cost in the overall SVM training time due to a more involved coreset construction procedure, but benefit heavily in terms of the accuracy of the models generated (Fig. 2).

Next, we evaluate our approach in the streaming setting, where data points arrive one-by-one and the entire data set cannot be kept in memory, for the same 6 data sets. The results of the streaming setting are shown in Fig. 4. Fig. 4 portray a similar trend as the one we observed in our offline evaluations: our approach significantly outperforms uniform sampling for all of the evaluated data sets and sample sizes, with negligible computational overhead.

Fig. 4
Download : Download high-res image (574KB)
Download : Download full-size image
Fig. 4. The relative error of query evaluations with respect uniform and coreset subsamples for the 6 data sets in the streaming setting. The figure shows that our method tends to fare even better in the streaming setting (cf. Fig. 2).

Comparison against different clustering models  We have considered using k-medians instead of k-means since it can lead to a lower sum of sensitivities, i.e., since the sum of sensitivities entails a summation on the Euclidean distance (not the squared distance) between each point and it's corresponding center as shown in Lemma 7, where k-medians clustering problems handle such instances. In light of this, the only change we have made is by simply running the k-means++ initialization for the k-medians problem using Theorem 5.1 of [29]. As shown in Table 1, the sum of sensitivities will reduce when using k-median clustering. We refer the reader to Fig. 5 for comparison between the quality of coresets when using the two different clustering techniques for bounding the sensitivities.

Fig. 5
Download : Download high-res image (603KB)
Download : Download full-size image
Fig. 5. The relative error of query evaluations with respect uniform, coreset subsamples when using k-means clustering for bounding the sensitivity, and coreset subsamples when using k-medians clustering for bounding the sensitivity. This is done for 6 data sets.

Why coreset is favorable over uniform sampling  Theoretically speaking, to attain small approximation error, we need a large sample if the sampling method is basically entailing uniform sampling [6]. Although, in practice it is not the case as seen in Fig. 2 and Fig. 4, this behavior is not prominent in very structured data where important points form very small percentage of the data. Such points can alter the separating hyperplane drastically if they are not present in the sample. To ensure that the approximation error that is obtained by using Uniform sampling (from probabilistic point of view), the sample must contain sufficiently large amount of points which will coincide with the theoretical bound on the sample size (with large enough probability).

In sum, our empirical evaluations demonstrate the practical efficiency of our algorithm and reaffirm the favorable theoretical guarantees of our approach: the additional computational complexity of constructing the coreset is negligible relative to that of uniform sampling, and the entire preprocess-then-train pipeline is significantly more efficient than training on the original massive data set.

9. Conclusion
We presented an efficient coreset construction algorithm for generating compact representations of the input data points that are provably competitive with the original data set in training Support Vector Machine models. Unlike prior approaches, our method and its theoretical guarantees naturally extend to streaming settings and scenarios involving dynamic data sets, where points are continuously inserted and deleted. We established instance-dependent bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of input data complexity and established dataset dependent conditions for the existence of compact representations. Our experimental results on real-world data sets validate our theoretical results and demonstrate the practical efficacy of our approach in speeding up SVM training. We conjecture that our coreset construction can be extended to accelerate SVM training for other classes of kernels and can be applied to a variety of Big Data scenarios.