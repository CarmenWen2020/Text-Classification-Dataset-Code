Abstract
We study algorithmic randomness properties for probability measures on Cantor space. We say that a measure μ on the space of infinite bit sequences is Martin-Löf absolutely continuous if the non-Martin-Löf random bit sequences form a null set with respect to μ. We think of this as a weak randomness notion for measures.

We begin with examples, and provide a robustness property related to Solovay tests. The initial segment complexity of a measure μ at a length n is defined as the μ-average over the descriptive complexity of strings of length n, in the sense of either C or K. We relate this weak randomness notion for a measure to the growth of its initial segment complexity. We show that a maximal growth implies the weak randomness property, but also that both implications of the Levin-Schnorr theorem fail. We discuss C-triviality and K-triviality for measures and relate these two notions with each other. Here, triviality means that the initial segment complexity grows as slowly as possible.

We show that every measure that is Martin-Löf random in the sense of Hoyrup and Rojas is Martin-Löf absolutely continuous; the converse fails because only the latter property is compatible with having atoms. In a final section we consider weak randomness relative to a general ergodic computable measure. We seek appropriate effective versions of the Shannon-McMillan-Breiman theorem and the Brudno theorem where the bit sequences are replaced by measures. We conclude with several open questions.

Keywords
Martin-Löf randomness
Measures
Initial segment complexity
K-triviality

1. Introduction
The theory of algorithmic randomness is usually developed for bit sequences, or equivalently, reals in the unit interval. A central randomness notion based on algorithmic tests is the one due to Martin-Löf [17].

Let 
 denote the topological space of infinite bit sequences. A probability measure μ on 
 can be seen as a statistical superposition of bit sequences. A single bit sequence Z forms an extreme case: the corresponding measure μ is the Dirac measure 
, i.e., μ is concentrated on . At the opposite extreme is the uniform measure λ which independently gives each bit value the probability 1/2. The uniform measure represents maximum disorder as no bit sequence is preferred over any other.

Recall that a measure μ on 
 is called absolutely continuous if each λ-null set is a μ-null set. We introduce Martin-Löf absolute continuity, an algorithmic randomness notion for probability measures that is a weakening of absolute continuity: we require that the λ-null set in the hypothesis be effective in the sense of Martin-Löf. Given that there is a universal Martin-Löf test, and hence a largest effective null set, all we have to require is that  where  is the class of bit sequences that are not Martin-Löf random. (By a “measure” we mean a probability measure on Cantor space unless otherwise stated.)

Our research is motivated in part by a recent definition of Martin-Löf randomness for quantum states corresponding to infinitely many qubits, due to the first author and Scholz [26]. Let 
 be the algebra of complex 
 matrices. Using the terminology of [26], measures correspond to the quantum states ρ where the matrix 
 is diagonal for each n. For such diagonal states, Tejas Bhojraj [3, Section 2.3], starting from an early announcement of the conference version of the present work [28], has shown that for measures, the randomness notion defined there is equivalent to the one proposed here. So, measures form a useful testing ground for conjectures in the subtler setting of quantum states. This applies, for instance, to the Shannon-McMillan-Breiman (SMB) theorem discussed below, which has been studied in the setting of quantum states by the first author and Tomamichel (see [25, Section 6]). In Proposition 7.2 we will show that a certain effective form of the SMB theorem holds for measures; the more general case of quantum states remains open. In Proposition 7.3 we give an example based on measures which shows that [25, Conjecture 6.3] cannot hold as stated, because an additional boundedness hypothesis is missing.

Growth of the initial segment complexity.  Given a binary string x, by  we denote its plain descriptive complexity, and by  its prefix-free descriptive complexity. Some of our motivation is derived from the classical theory. Randomness of infinite bit sequences is linked to the growth of the descriptive complexity of their initial segments. For instance, the Levin-Schnorr theorem intuitively says that randomness of Z means incompressibility (up to the same constant b) of all the initial segments of Z. We want to study how much of this is retained in the setting of measures μ. The formal growth condition in the Levin-Schnorr theorem says that  for each initial segment x of Z. The “n-th initial segment” of a measure μ is given by its values  for all strings x of length n, where  denotes the set of infinite sequences extending x. We define the complexities 
 and 
 of such an initial segment as the μ-average of the individual complexities of the strings of length n. With this definition, in Section 4 we show that both implications of the analog of the Levin-Schnorr theorem fail. However by Proposition 7.4 discussed below, for measures that are random in our weak sense, 
, and hence also 
, converge to 1. Thus, such measures have effective dimension 1. (See Downey and Hirschfeldt [9, Section 12.3] for background on effective dimension.)

It turns that the initial complexity of measures can exhibit interesting growth behaviour that is not possible for bit sequences. We will verify in Fact 3.3 that the initial segment complexity of the uniform measure λ has maximal growth in the sense of K, namely 
. This implies that the initial complexity of λ in the sense of C also has maximal growth. For bit sequences, such a growth rate is ruled out by a result of Katseff [13]. To prove this maximal growth rate for K, we use that, in a sense to be made precise, most strings of a given length are incompressible. We also provide an example of such a measure other than λ. On the other hand, we show that a maximal growth rate in the sense of C implies ML absolute continuity.

K-trivial and C-trivial measures.  Opposite to random bit sequences lie the K-trivial sequences A, where the initial segment complexity grows no faster than that of a computable set, in the sense that 
. C-trivial sequences are defined analogously. Chaitin [7] proved that each C-trivial sequence is computable, while Solovay [36] showed that a K-trivial sequence can be noncomputable. For background see e.g. [24, Section 5.3]. In Section 5 we extend these notions to statistical superpositions of bit sequences: we introduce C- and K-trivial measures. We first show that such measures have a countable support. This means that they are countable convex combinations of Dirac measures (based on bit sequences that necessarily satisfy the same notion of triviality). Thereafter, again guided by the case of bit sequences, we provide closure properties of the two notions: downward closure under truth-table reducibility, and closure under products.

We leave the question open whether each C-trivial measure is K-trivial. In Proposition 5.8 we study three conditions relating C-complexity and K-complexity, and show that they are all equivalent. If these conditions hold then each C-trivial measure is K-trivial. (Whether they indeed hold was discussed at length in a working group at the 2020 American Institute of Mathematics workshop on randomness and applications. A weaker version of such a condition was considered in Bruno Bauwen's thesis [1].)

We will also consider slightly stronger forms of the notions of C- and K-triviality, for which this implication C-trivial → K-trivial holds, and is proper.

Martin-Löf random measures.  Measures can be viewed as points in a canonical computable probability space, in the sense of Hoyrup and Rojas [12]. This yields a notion of Martin-Löf randomness for measures. Bienvenu and Culver (see [8, Theorem 2.7.1]) have shown that no measure μ that is Martin-Löf random is absolutely continuous; in fact μ is orthogonal to λ in the sense that some λ-null set is co-null with respect to μ. In contrast, in Section 6 we show that this notion implies our weak notion of randomness, Martin-Löf absolute continuity. The stronger randomness notion forces the measure to be atomless, so the converse implication fails. The different randomness notions for measures we have discussed beg further questions. For instance, does the randomness notion studied by Bienvenu and Culver imply that the initial segment complexity for the measure (in the sense of C or of K) is maximal infinitely often?

Shannon-McMillan-Breiman Theorem.  This theorem from the 1950s says informally that for an ergodic measure ρ on 
, outside of an appropriate ρ-null set, every bit sequence Z knows the entropy of the measure ρ: it is the limit of the normalized information content w.r.t. ρ of initial segments of Z, namely, 
 
. See e.g. [34], where the result is called the Entropy Theorem. In the short Section 7, we replace Z by a measure μ that is Martin-Löf a.c. with respect to ρ, and take the μ-average of the information contents of strings of the same length. We only obtain a partial analog of the theorem. We use as a hypothesis that the normalized information content of strings w.r.t. ρ is bounded, and also provide an example showing that this hypothesis is necessary. However, in a similar vein, in Proposition 7.4 we establish an analog for measures of the effective Brudno's theorem [10], [11], which states that the entropy of ρ is given as the limit of 
, for any Z that is ρ-ML-random. We show that one can take the limit of 
 for any μ that is ML absolutely continuous in ρ.

We end the paper with conclusions and open questions.

For general background on recursion theory and algorithmic randomness we refer the readers to the textbooks of Calude [5], Downey and Hirschfeldt [9], Li and Vitányi [16], Nies [24], Odifreddi [29], [30] and Soare [35]. Lecture notes on recursion theory are also available online, e.g. [37].

A conference version of the paper has appeared in the proceedings of STACS 2020 [28]. The present paper is a much improved and expanded version. This applies in particular to Sections 3 and 5.

2. Measures and randomness
In this section we will formally define one of our main notions, Martin-Löf absolute continuity (Definition 2.3), and collect some basic facts concerning it. In particular, we verify that the well-known equivalence of Martin-Löf test and Solovay tests extends to measures. We begin by briefly discussing algorithmic randomness for bit sequences [9], [24]. We use standard notation: letters  denote elements of the space of infinite bit sequences 
,  denote finite bit strings, and  is the set of infinite bit sequences extending σ. 
 denotes the string consisting of the first n bits of Z. For quantities  depending on the same parameters, one writes 
 for . A subset G of 
 is called effectively open if 
 for a computable sequence 
 of strings. A measure ρ on 
 is computable if the map 
 given by  is computable. That is, each real  is computable, uniformly in σ.

Definition 2.1

Let ρ be a computable measure on 
. A ρ-Martin-Löf test (ρ-ML-test, for short) is a sequence 
 of uniformly effectively open sets such that 
 for each m. A bit sequence Z fails the test if 
; otherwise it passes the test. A bit sequence Z is ρ-Martin-Löf random (ρ-ML-random for short) if Z passes each ρ-Martin-Löf test.

By λ one denotes the uniform measure on 
. So 
 for each string σ. If no measure ρ is provided it will be tacitly assumed that , and we will use the term “Martin-Löf random” instead of “λ-Martin-Löf random” etc. Let  denote the prefix free version of descriptive (i.e., Kolmogorov) complexity of a bit string x.

Theorem 2.2

Levin [15], Schnorr [32]
Z is Martin-Löf random ⇔ 
.

Using the notation of [24, Chapter 3], let 
 denote the set of bit sequences Z such that 
 for some n. It is easy to see that 
 is a Martin-Löf test. The Levin-Schnorr theorem says that this test is universal: Z is Martin-Löf random iff it passes the test.

Recall that unless otherwise stated, all measures are probability measures on Cantor space. We use the letters  for measures (and recall that λ denotes the uniform measure). We now provide the formal definition of our weak randomness notion for measures.

Definition 2.3

Let ρ be a measure. A measure μ is called Martin-Löf absolutely continuous in ρ if 
 = 0 for each ρ-Martin-Löf test 
. We denote this by 
.

If 
 we say that μ passes the test. If 
 where  we say μ fails the test at level δ.

In the case that , we say that μ is ML absolutely continuous, for short.

Martin-Löf absolute continuity is a weakening of the usual notion that μ is absolutely continuous in ρ, usually written . In fact,  holds iff μ is ρ-
 absolutely continuous for each oracle X.

In the definition it suffices to consider ρ-ML tests 
 such that 
 for each m, because we can replace 
 by the ρ-ML test given by 
, and of course 
 implies that 
. So we can change the definition above, replacing the condition 
 by the only apparently stronger condition that 
.

It is well known that there is a universal ρ-ML test. The intersection of all the components of such a test consists of the non-ρ-ML-random sequences. This implies:

Fact 2.4

 iff the sequences that are not ρ-ML-random form a μ-null set.

We have already mentioned the two diametrically opposite types of examples of measure that are ML absolutely continuous in a measure ρ:

Example 2.5

(a) 
 for each measure ρ.

(b) For a Dirac measure 
, we have 
 iff Z is ρ-ML-random.

Next we provide a source of non-examples where .

Example 2.6

A Bernoulli measure on 
 assigns a fixed probability p to a 0, independently of the position. Such a measure is not ML absolutely continuous in the case that . To see this, note that each Martin-Löf random sequence Z satisfies the law of large numbers 
 
 
 see e.g. [24, Proposition 3.2.13]. So if μ is ML absolutely continuous, then μ-almost surely, Z satisfies the law of large numbers. This is not the case when μ is a Bernoulli measure for , because μ-almost surely, Z satisfies the law of large numbers for p instead of 1/2.

Definition 2.7

For a measure ν and string σ with , let 
 denote the localisation to :
 Clearly if ν is ML absolutely continuous then so is 
.

A set S of probability measures is called convex if 
 for  implies that the convex combination 
 is in S, where the 
 are reals in  and 
. The extreme points of S are the ones that can only be written as convex combinations of elements of S where .

Proposition 2.8

The ML absolutely continuous probability measures form a convex set. Its extreme points are the ML absolutely continuous Dirac measures, i.e. the measures 
 where Z is a ML-random bit sequence.

Proof

Let 
 as above where the 
 are ML absolutely continuous measures. Suppose 
 is a Martin-Löf test. Then 
 for each i, and hence 
.

Suppose that μ is Martin-Löf a.c. If μ is a Dirac measure then it is an extreme point of the Martin-Löf a.c. measures. Conversely, if μ is not Dirac, there is a least number t such that the decomposition 
 
 is nontrivial. Hence μ is not an extreme point. □

Recall that a ρ-Solovay-test, for a computable probability measure ρ, is a sequence 
 of uniformly 
 sets such that 
. If  we will simply use the term Solovay test. A bit sequence Z passes such a test if 
 for almost every k. (Each Martin-Löf test is also a Solovay test, but Solovay tests have a stronger passing condition). A basic fact from the theory of algorithmic randomness (e.g. [24, Prop. 3.2.19]) states that Z is Martin-Löf random iff Z passes each Solovay test. This is usually asserted only relative to the uniform measure λ, but in fact carries over to any computable measure ρ.

The following characterises the ML absolutely continuous measures with countable support.

Fact 2.9

Let . Let 
 where 
 for each k, and 
. Then μ is ML absolutely continuous iff all the 
 are Martin-Löf random.

Proof

The implication from left to right is immediate. For the converse implication, given a Martin-Löf test 
, note that the 
 pass this test as a Solovay test. Hence for each r, there is M such that for each  and each  we have 
. This implies that 
 for each . So 
. □

We say that a measure μ passes a Solovay test 
 if 
. The fact that passing all Martin-Löf tests is equivalent to passing all Solovay tests generalises from bit sequences to measures. We note that Tejas Bhojraj [3, Thm. 2.11] has proved such a result in even greater generality in the setting of quantum states, where the proof is considerably more involved.

Proposition 2.10

For probability measures  such that ρ is computable, we have

 iff μ passes each ρ-Solovay-test.

Proof

Relative to ρ, each Martin-Löf test is a Solovay test, and the passing condition 
 works for both types of tests by the remark after Definition 2.3. This yields the implication from right to left.

For the implication from left to right, suppose that 
, and let 
 be a ρ-Solovay-test. By 
 one denotes the set of bit sequences Z such that 
, that is, the sequences that fail the test. By the basic fact (e.g. [24, Prop. 3.2.19]) mentioned above, the set of ρ-Martin-Löf random sequences is disjoint from 
. Hence, by hypothesis on μ, we have 
. By Fatou's Lemma, 
. So μ passes the Solovay test. □

3. A fast growing initial segment complexity implies being ML-a.c.
In this section we formally define our two variants of initial segment complexity for measures, and first their connections with ML absolute continuity.

Recall that in this paper we use  to denote the plain Kolmogorov complexity of a binary string x and  to denote the prefix-free Kolmogorov complexity. Note that other letters are also used in the literature; for example, the textbook by Calude [5] uses K and H, a notation going back to Chaitin.

Recall the upper bounds 
 and 
 for all strings x of length n. We assume for notational convenience the strict lower bounds that  and . This convention reduces the number of constants needed in the statements and proofs of our results. To achieve this, we replace some given universal machine V by a new universal machine U such that whenever  then 
 and  with 
 standing for n, and U is undefined on all other inputs. It is easy to see that if V is universal by adjunction, so is U; if V is prefix-free so is U. The descriptions for U are only by one bit longer than those for V and universality requires only that the length of the shortest descriptions for some x goes up at most by a constant.

Definition 3.1

Let 
 be the μ-average of all the  over all strings x of length n. In a similar way we define 
. We also define the descriptive complexity conditioned on n, namely 
, as expected.

It may help to think of 
 as 
 where 
 and 
 are the respective functions restricted to the set of strings of length n. Note that for each bit sequence Z, we have 
.

In this section we write  for 
. We tacitly rely on standard inequalities such as 
, 
 and 
. We also use that for each , there are at most 
 strings such that . See e.g. [24, Chapter 2].

Note that for each string x of length n, the deviations from the upper bounds are related by the inequality 
. For this, see e.g. [24, 2.5.5] which yields the inequality after some elementary algebraic manipulation. Taking the μ-average, this implies the corresponding inequality for the initial segment complexity of measures on Cantor space:

Fact 3.2

Let μ be a measure. For each n one has

The following says that the uniform measure λ on 
 has the fastest growing initial segment complexity that is possible in the sense of K, and therefore also of C.

Fact 3.3

, and therefore also 
.

Proof

Chaitin [6] showed that there is a constant c (dependent only on the universal prefix-free machine) such that, for all d, among the strings of length n, there are at most 
 strings with ; also see [24, Thm. 2.2.26].

Fix n. For  let 
 be the set of strings of length n such that . Identifying sets of strings of the same length and the clopen sets they generate, by Chaitin's result we have 
.

Clearly 
 whenever . Taking the λ-average over all strings of length n we obtain
  
 
 
 □

We provide an example of a computable probability measure  that satisfies the hypothesis of Fact 3.3. Informally, along a string, μ puts 1/3 on 0 until it hits the first 1; from then on μ behaves like the uniform measure λ.

Recall from 2.7 on the localization 
 of a measure ν to a string σ. For  let  denote the string 
. Note that for a string y of length  one has 
 (where the additive constant is independent of both n and r). So by Fact 3.3, for  one has 
. Trivially, for  one has 
.

Fact 3.4

The computable measure 
 satisfies 
. Moreover, μ is absolutely continuous.

Proof

Keeping in mind that 
, we calculate
 
 
 
 
 The last inequality holds because 
, and 
, and 
 is finite.

Clearly μ is computable;  because when viewed as a measure on the unit interval via the binary expansion, we have  where the function  is given by , 
 
 for x in the interval 
 where  (this interval corresponds to ). We also have , albeit via an unbounded μ-integrable function. □

Li and Vitanyi [16] called a bit sequence 
 Kolmogorov random if there is r such that 
 for infinitely many n. One says that Z is strongly Chaitin random if there is r such that 
 for infinitely many n. For bit sequences these notions are equivalent to 2-randomness (i.e., ML-randomness relative to the halting problem) by [27] and [19], respectively; also see [24, Theorem 8.1.14] or [9].

One can extend these notions to measures. We call μ strongly Chaitin random if 
 for infinitely many n. Similarly we define Kolmogorov randomness of a measure. By Fact 3.2 the following is immediate:

Fact 3.5

If a measure μ is strongly Chaitin random, then μ is Kolmogorov random.

We leave open the question whether the converse implication holds as well. The potentially weaker notion already implies being ML absolutely continuous:

Theorem 3.6

Suppose that μ is a Kolmogorov random measure. Then μ is ML absolutely continuous.

Proof

By hypothesis there is  such that 
 for infinitely many n. Assume for a contradiction that μ is not ML absolutely continuous. So there is a Martin-Löf test 
 and  such that 
 for each d. We view 
 as given by an enumeration of strings, uniformly in d; thus 
 for a sequence 
 that is computable uniformly in d. Let 
 denote the clopen set generated by the strings in this enumeration of length at most n. (Note that this set is not effectively given as a clopen set, but we effectively have a description of it as a 
 set). Let c be a constant such that, for each x of length n, one has .

Lemma 3.7

If x is a string of length n such that 
 then 
.

To verify this, let N be a fixed plain machine that on input y and auxiliary input d prints out the y-th string x of length  such that the enumeration of 
 asserts that 
. (Here we view y as the binary representation of a number, with leading zeros allowed.) Since 
, sufficiently many strings are available to print all such x. This machine shows that 
 for any x such that 
, as required. This verifies the lemma.

Note that (after possibly increasing the constant c) any x as above satisfies . For each , letting x range over strings of length n, we have
  
  
  The first summand is bounded above by 
 via the lemma, the second by 
. We obtain
 Now for each d, for sufficiently large n we have 
. So given r letting, , for large enough n we have 
. This contradicts the hypothesis on μ. □

It would be interesting to find out whether the above-mentioned coincidences of randomness notions for bit sequences lift to measures; for instance, do the conditions in the theorem above actually imply that the measure is ML absolutely continuous relative to the halting problem 
?

4. Both implications of the Levin-Schnorr Theorem fail for measures
We will show that both implications of the analog of the Levin-Schnorr Theorem 2.2 fail for measures. The implication from left to right would say that a ML absolutely continuous measure cannot have an initial segment complexity in the sense of K growing slower than . This can be disproved by a simple example of a measure with countable support.

Note that, by Proposition 7.4 below, we have 
 for each ML absolutely continuous measure μ, which provides a lower bound on the growth.

Example 4.1

There is a ML absolutely continuous measure μ such that for each , one has 
.

Proof

We let 
 where 
 is Martin-Löf random and 
 for a sequence 
 of reals in  that add up to 1, and a sufficiently fast growing computable sequence 
 to be determined below. Then μ is ML absolutely continuous by Fact 2.9.

For n such that 
 we have
 
 
 Hence, to achieve 
 it suffices to ensure that 
 for almost all k. For instance, we can let 
 
 and 
. □

To disprove the implication from right to left in a potential generalisation to measures of the Levin-Schnorr Theorem, we need to provide a measure μ such that μ is not Martin-Löf a.c. and 
. This is immediate from the following fact on the growth of the initial segment complexity for certain bit sequences , letting 
.

Theorem 4.2

There are a Martin-Löf random bit sequence X and a non-Martin-Löf random bit sequence Y such that, for all n,

Proof

Let X be a low Martin-Löf random set (i.e., 
). We claim that there is a strictly increasing function f such that the complement of the range of f is a recursively enumerable set E, and 
 for all . To see this, recall that 
. Since X is low there is a computable function p such that for all n, 
 is the maximal m such that 
.

Define  for  as follows. ; for  let n be least such that  or . If  and  then let  else let .

Note that for each n there are only finitely many  with  and that almost all s satisfy , as otherwise  would be modified either at n or some smaller value. Furthermore,  can only happen if there is an  with  and that happens only finitely often, as all the  converge to a fixed value and every change of an  at some time s leads to a value above s. Furthermore, once an element is outside the range of f, it will never return, and so the complement of the range of f is recursively enumerable. So 
 is a function as required, which verifies the claim. (The complement E of the range of f is called a Dekker deficiency set in the literature [29].)

Now let  (with the convention that ). Since g is unbounded, by a result of Miller and Yu [22, Corollary 3.2] there is a Martin-Löf random Z such that there exist infinitely many n with 
; note that the hypothesis of this result of Miller and Yu does not make any effectivity requirements on g. Let Note that 
, as one can enumerate the set E until there are, up to n, only  many places not enumerated and then one can reconstruct 
 from 
 and  and the last  bits of 
. As Z is Martin-Löf random, 
, so
 The definitions of  give 
. This shows that, for almost all n, 
.

However, the set Y is not Martin-Löf random, because there are infinitely many n such that 
. Now 
 can be computed from 
 and , as one needs only to enumerate E until the  nonelements of E below n are found, and they allow to see where the zeroes have to be inserted into the string 
 in order to obtain 
. We have  for almost all n and thus 
 for infinitely many n. So the set Y is not Martin-Löf random, as required. □

Failing a strong Solovay test implies non-complex initial segments.  In contrast to the negative results above, we show that failing a stronger type of tests leads to considerable dips in the initial segment complexity of a measure in the sense of C (and hence, also in the sense of K by Fact 3.2).

We say that a Solovay test 
 is strong if each 
 is clopen and there is a recursive function g such that  is a strong index for a finite set of strings 
 such that 
. For bit sequences, this means no restriction of the corresponding randomness notion: in that case any Solovay test 
 can be replaced by a strong Solovay test, listing strings that make up the uniformly 
 sets 
 one-by-one. We conjecture that this equivalence of test notions no longer holds for measures.

The following is a weak version for measures of one implication of the Miller-Yu theorem [21, Theorem 7.1]. We show that if a measure μ is far from ML absolutely continuous, then its initial segment complexity 
 defined in Definition 3.1 has infinitely many dips of size  for some positive constant δ and a computable function f that grows somewhat fast in that 
. We use elements of its proof in Bienvenu, Merkle and Shen [4]. We note that the result is a variation on (the contrapositive of) Theorem 3.6, proving a stronger conclusion from a stronger hypothesis. A version of the result for quantum states is [26, Thm. 4.4].

Proposition 4.3

Suppose that μ fails a strong Solovay test 
 at level δ, namely 
. Then there is a computable function f such that 
 and

Proof

Let 
 be as in the definition of a strong Solovay test. We may assume that 
, and all strings in 
 have the same length 
, where 
 for each r. We write 
 for 
. Let f be the computable function such that
 and  for each m that is not of the form 
. There is a constant d such that each bit string x in the set 
 satisfies (where 
)(1) For, r can be computed from 
, and each string 
 is determined by r and its position 
 in the lexicographical listing of 
. We can determine i by 
 bits for some fixed d. In fact we may assume the description has exactly that many bits. Thus, there is a Turing machine L with two inputs such that for each 
, we have 
 for some bit string 
 of length .

Let c be a constant such that  for each string x. Now suppose that 
. Then for 
, where σ ranges over strings of length n,
  
 
 
 There are infinitely many such r by hypothesis. This completes the proof. □

5. K- and C-triviality for measures
Recall that a set A is called K-trivial if 
 for each n; one says that b is a K-triviality constant for A. In a similar way one defines C-trivial sets. This section generalises these definitions to measures.

Definition 5.1

A measure μ is called K-trivial if 
 for each n; μ is called C-trivial if 
 for all n.

Suppose μ is a Dirac measure 
. Then μ is K-trivial iff A is K-trivial in the usual sense, and μ is C-trivial iff A is C trivial in the usual sense, which is equivalent to being recursive by a well-known result of Chaitin [7] (for a more recent proof see e.g. [24, 5.2.20]). More generally, any finite convex combination of K-trivial Dirac measures is K-trivial, and similarly for C. Now if  are any positive real numbers such that , then 
 is a C-trivial and K-trivial measure such that any measure representation in the sense of [24, 1.9.2] computes α. Thus one cannot bound the computational complexity of K-trivial or of C-trivial measures in any way.

For a further example, let 
 be a sequence of uniformly recursive sets, and let 
 be a sequence of uniformly recursive positive real numbers such that 
 and 
 is finite. Let 
. Since 
 and 
, for each n we have
 
 Hence μ is K-trivial.

In the following, recall from the third paragraph of Section 3 that our choice of universal machines implies that  and  for all strings x of length n.

Atoms and Triviality.  We show that both K-trivial and C-trivial measures are discrete in the sense of [33]; namely, they are concentrated on the set of their atoms. Such measures were further studied by Kautz [14], where they were called “trivial measures”, and more recently by Porter [31].

Proposition 5.2

If a measure μ is K-trivial, then μ is supported by its set of atoms.

In fact the weaker hypothesis 
 suffices for this. A similar statement holds for C.

Proof

For a set 
⁎
, by 
 one denotes the open set 
.

Assume for a contradiction that μ gives a measure greater than  to the set of its non-atoms. Fix c arbitrary, with the goal of showing that 
 for large enough n.

There is a constant d (in fact 
) such that for each n there are at most d strings x of length n with  (see e.g. [24, Theorem 2.2.26(ii)]).

Let 
. By hypothesis we have 
 for large enough n. Therefore by choice of d we have
 Now we can give a lower bound for the μ-average of  over all strings x of length n: 
 
 
 
 as required. Notice that we have only used the weaker hypothesis. □

Thus, if μ is K-trivial for constant p, then μ has the form 
 where  and each 
 is positive and 
. The following observation will show that in addition all the 
 are K-trivial. In particular, K-trivial measures are not Martin-Löf absolutely continuous.

Fact 5.3

Suppose 
 is K-trivial for the constant p. Then each 
 is K-trivial for the constant 
. Similarly, if μ is C-trivial for the constant p, then each 
 is C-trivial for the constant 
.

Proof

We have
 
 Therefore, 
, as required.

The proof for C is obtained via replacing K by C everywhere. □

Above we built a computable K-trivial measure with infinitely many atoms. On the other hand, the following example shows that not every infinite convex combination 
 of K-trivial Dirac measures for constants 
 yields a K-trivial measure, even if there is a finite bound on the values 
. Identifying Chaitin's Ω with its binary expansion, let
 All sets 
 are finite and thus K-trivial for constant . Furthermore, the sum of all 
 is 1 and 
. We have
  
 
 for almost all n, and thus the average grows faster than . So the measure is not K-trivial.

In a sense, an atomless measure can be arbitrarily close to being C-trivial and K-trivial.

Proposition 5.4

For each nondecreasing unbounded function f which is recursively approximable from above there is a non-atomic measure μ such that 
 and 
.

Proof

The proof for C and K is essentially the same; we present the one for K first.

There is a recursively enumerable set A such that, for all n,  has up to a constant  non-elements. One lets μ be the measure such that 
 in the case that all ones in x are not in A and  otherwise, where m is the number of non-elements of A below . One can see that when 
 then x can be computed from  and the string 
 which describes the bits at the non-elements of A. Thus
 It follows that 
, as the μ-average of strings 
 with 
 is at most  plus a constant.

Now the bound for C follows along the same lines; note that for C one gives a prefix-free coding of the string 
 above followed by a C-description for  plus a constant-length coding prefix – assuming that the universal machine is universal by adjunction. Again the length of the code is  plus a constant. □

Closure properties of C- and of K-triviality.  In the following suppose Γ is a total Turing reduction, that is, a truth-table reduction, mapping 
 to 
. Given a measure ν, let  denote the image measure, defined as usual by 
, where  is a subset of 
. As Γ is a truth-table reduction, some strictly increasing computable function f bounds the use of Γ.

Proposition 5.5

Let  as above. We have 
 and similarly for C. In particular, if ν is K-trivial then so is μ, and if ν is C-trivial so is μ.

Proof

Let x range over strings of length n and y over strings of length . Since 
 we have
  
  
  
 
  
 
 Using that n and  have, up to a constant, the same descriptive complexity in the sense of K, this completes the proof. The proof for C is similar to the proof for K with only the obvious changes. □

Note that 
 is effectively isomorphic to 
 via the map . So a product of two measures on 
 can itself be viewed as a measure on 
. We show that K- and C-triviality of measures is closed under taking products, generalising well-known facts for bit sequences. The following fact will be needed for the case of C.

Fact 5.6

 for each n and each 
.

Proof

One suffices to show that there is a constant d such that for each b, Following an argument of Chaitin [7], Nies [24, Proof of Lemma 5.2.21] showed that for every plain machine M there are at most 
 descriptions p with  and . Consider the machine M such that 
 where U is the universal plain machine. There are at most 
 many 
 with , because any such x has the form  for some p with  and . Uniformly in b, n and , the set of such x is recursively enumerable. Hence one can provide a prefix-free code of length up to 2b plus constant for any pair  with 
 selecting the a-th such x. This shows that a constant d as above exists. □

Theorem 5.7

Let μ and ν be measures.

(a) If μ and ν are K-trivial then  is K-trivial.

(b) If μ and ν are C-trivial then  is C-trivial.

Proof

We assume in both cases (a) and (b) that μ and ν are trivial with respect to the corresponding descriptive string complexity and then show that  is trivial for the same complexity notion.

For (a), by Proposition 5.2 one may assume that 
 and 
 with 
 and 
, where the 
 and 
 are K-trivial sets. For a string x of length n and a shortest description p of (a code for) n, 
; this follows easily from [16, Theorem 3.9.1]. Note that
 Since μ and ν are K-trivial we have 
 and 
, so 
 and 
 are both bounded by some constant independent of n. Let c be a common upper bound of such constants. Now, for some constant d,
 
 
 
 
 
 
 Since 
, one can conclude that 
. The same can be proven for  with only a slight more notational complexity. Thus  is K-trivial.

For (b), as in (a), assume that 
 and 
 with 
 and 
, where now 
 are C-trivial sets. Let c be the supremum over n of all 
 and 
. Note that  because μ and ν are C-trivial.

One can produce descriptions for 
 and 
 from prefix-free conditional descriptions for 
 and 
, given n and  together with a coding of these two parameters n and . Note that  can be given by a shortest C-description of n in one go, which needs  bits. With the other two prefix-free conditional descriptions appended, one obtains for a suitable constant 
 that
 Now one uses this to show the following as in (a):
 
 
 
 
 
 
 
 The inequality from the first to the second line follows from the definition of 
; the inequality from the second to the third line follows from Fact 5.6.

Thereafter, one uses the distributivity of absolutely converging sums, and the definition of c. The same can be proven for  with only a slight increase in notational complexity. □

The next proposition shows that three conditions involving the descriptive complexities of strings are equivalent. It is unknown whether these equivalent conditions are true. These conditions relate fundamental properties of plain and prefix-free Kolmogorov complexity; properties of similar type have been investigated quite a lot in algorithmic information theory; fairly recent sample references are [1], [2], [20], [16]. The last condition states informally that there are finitely many potential ways to compute  from n and , and for each n one of them succeeds. (In fact at present it is unknown whether there is a single way.)

Proposition 5.8

The following conditions are equivalent:

(a)
There is a constant c such that for all n and all 
,

;

(b)
For each 
 there is 
 such that for all n and all 
,

if 
 then 
;

(c)
There is a constant 
 with 
 for all n.

Proof

The implication (a) to (b) is straightforward: if 
 bounds  then 
 bounds .

The implication (b) to (c) can be seen as follows: Note that for almost all n it holds that . So given n sufficiently large, one considers the string 
, the n-bit string that has a 1 in position  and 0s elsewhere. This string has plain Kolmogorov complexity satisfying 
, so let 
 be such that 
. Thus 
 for all n, where 
 is defined by condition (b). By [24, Theorem 2.2.26] there are at most 
 strings 
 with 
 and one can enumerate the first, the second, …, the k-th until 
 comes up; let k be the corresponding sequence-number. Now 
 for some constant 
 independent of n and therefore 
. As one can compute  from 
, there is a further constant 
 with 
 for all n.

The implication (c) to (a) is based on Fact 5.6 which says that  
  and that the description 
 witnessing it is prefix-free. Now by condition (c) one has that  is bounded by a constant. From a shortest K-description 
 of n one can compute both n and  and therefore one can compute all of  from a suitable prefix-free description 
 with 
. Now one can define a prefix-free machine M such that 
 for each string x of length n where 
 and 
. It follows that 
. This yields condition (a). □

If these conditions are true then the question whether all C-trivial measures are K-trivial has an affirmative answer. To see this note that if 
 is C-trivial then there is a constant b such that 
 for all n. It follows from Condition (a) that
 
 
 

Strong C- and strong K-triviality for measures.

Given the examples above we consider a strengthening of our notions.

Definition 5.9

Call a measure μ strongly C-trivial if it can be written as a convex combination 
 such that the 
 are C-trivial via constants 
 such that 
 is finite. Similarly, one defines strongly K-trivial measures.

Fact 5.10

Every strongly C-trivial measure is C-trivial. Every strongly K-trivial measure is K-trivial.

Proof

By hypothesis 
 where each 
 is C-trivial with constant 
, and 
. For all n we have 
. The proof is analogous for K. □

While we ignore whether every C-trivial measure is K-trivial, we show that this implication holds for the strong versions of the two notions. For this we prove a lemma about sets of interest on its own. Its proof owes to the proof that each C-trivial is recursive (Chaitin [7], also see [24, Proof of Thm 5.2.20(i)]).

Lemma 5.11

There is an absolute constant d such that every set that is C-trivial via a constant b is K-trivial via the constant .

Proof

As mentioned in the proof of Fact 5.6, there are only 
 binary strings x of length n with . Now one considers the following recursively enumerable tree 
 of binary strings: a string x of length n is in 
 iff there is an y extending x of length 
 such that for all 
 it holds that 
.

Note that there is an m with 
 such that . Thus for that length m, there are only 
 strings which qualify and therefore, for length n, there are at most 
 many strings in 
. Furthermore, note that the membership in 
 is closed under prefixes and therefore 
 is a recursively enumerable tree.

For each length n, one can enumerate from given  the 
 strings of that length in 
. So one can describe these strings by any description of n plus prefix-free codes of size  which compute the strings given . Note that these codes just say “the first string of length n enumerated into 
”, “the second string of length n enumerated into 
” and so on. As there are 
 of these strings, one can represent them and b in a prefix-free way with 2b plus constant bits. Thus they can be described in a prefix-free way with a description of size  where d is a constant independent of . Hence 
. □

Proposition 5.12

Every strongly C-trivial measure is strongly K-trivial.

Proof

Suppose that a measure μ is given as a convex combination 
 where each 
 has the C-triviality constant 
, and 
 is finite. By the foregoing lemma every 
 is K-trivial via the constant 
. Clearly 
 is finite. So μ is strongly K-trivial. □

One might ask whether every K-trivial measure is strongly K-trivial, and whether every C-trivial measure is strongly C-trivial. The answer to this question is “no” as the following result shows.

Proposition 5.13

There is a measure μ which is C-trivial and K-trivial but not strongly K-trivial (and hence not strongly C-trivial).

Proof

For each n, let 
 be the description of length up to 
 for the prefix-free universal machine which produces the longest output string of form 
; here recall that 
 and 
 are identified with each other. Note that 
 and 
 are both approximately 
. Now one defines the sets 
 has a 1 at position k}. The sets 
 are uniformly limit recursive and the approximations converge in time 
 to 
; furthermore, 
. Every set 
 is finite and thus K-trivial. Let 
.

We show that μ is K-trivial. One sees that for 
 with 
, 
 of the mass is concentrated on 
 and for , 
 of the mass is concentrated on 
. For , the string 
 can be computed from h and ℓ. Thus it has prefix-free complexity  which contributes to the average measure 
 a term bounded by 
. As the sum 
 converges to a constant c, one can estimate the overall sum as
 and the latter is bounded by , as 
. Thus μ is K-trivial. By a similar argument, μ is C-trivial.

The K-triviality constant 
 for 
 is bounded from below by 
 and that term is at least 
, as 
 and 
. It follows that 
. Thus μ is not strongly K-trivial. □

By the facts and propositions above, in Proposition 5.5 the condition that Γ be a truth-table reduction is necessary; downward closure of the K-trivial measures does not hold for Turing reductions in general.

Proposition 5.14

There are measures  and a Turing reduction Γ such that ν is strongly K-trivial, Γ is defined on all the atoms of ν and  is not K-trivial.

Proof

Consider the set 
 
, that is, the join of Ω with its complement. This set is left-r.e. and initial segments of length 2m have approximately complexity m. Now let
 
 and 
 
 
 
. Furthermore, let 
 be the least integer greater than 
 and let 
 where 
 is the time to follow the recursive left-enumeration of Ω until the first 
 bits are correct (and therefore remain correct from then on). Note that 
 can be computed from 
 and 
 can be computed from 
. Let 
 and 
 and 
.

Note that for 
, 
 can be computed from ℓ and m, as ℓ is an upper bound of 
; for 
, 
. Thus the coding constant of 
 is  plus some constant. As 
 is a convergent sum – almost all terms are bounded by 
 which is a convergent sum – the measure ν is K-trivial by Fact 5.10. Actually it is by definition even strongly K-trivial.

The measure μ is not K-trivial, as all 
 with  have the common prefix 
 
 of length 
 and that prefix has, for almost all m, the Kolmogorov complexity 
 or more. Note that 
 is at least 
 
, so this string contributes to the average 
 at least 
. Thus μ is not K-trivial, as the function 
 cannot be bounded from below by any increasing unbounded recursive function.

Now one defines a Turing functional Γ that translates each 
 to 
: the functional searches in the oracle 
 for the first element x of 
; as long as the oracle is not empty, such an element is found. Then it determines the  with  and computes the corresponding entry in the set 
 
. In the case that the oracle has the set 
, the search gives 
 and this Turing reduction provides the set 
. Thus Γ has the desired properties. Note that Γ is undefined only on ∅, and the measure of  with respect to ν is 0. □

While we know that every strongly C-trivial measure is strongly K-trivial, we ignore whether every C-trivial measure is K-trivial. The converse implication fails even for Dirac measures because each C-trivial sequence is computable by the aforementioned result of Chaitin. We next show that this failure of the converse implication can be witnessed by a strongly K-trivial measure where all the atoms are recursive, and thus C-trivial (as sequences).

Proposition 5.15

There is a strongly K-trivial measure which is not C-trivial and which has only recursive atoms.

Proof

Let A be an r.e. K-trivial set which is not C-trivial, that is, which is not recursive. Let 
 be the least number m such that 
; this is defined because as A is not C-trivial. Furthermore, as A and the function C are both limit-recursive, the numbers 
 can be computed in the limit and have uniformly recursive approximations 
 which converge to 
, though not monotonously.

One next defines a sequence 
 such that 
 and 
 where the value  is taken if
 Note that the definition of these values implies that
 Now let 
. The next claim will show that the 
 are all K-trivial with a small K-triviality constant and that for this, it is not needed to compute 
 uniformly from k; instead one computes the 
 from s and k only for 
.

Claim 5.16

The set 
 has the K-triviality constant  for some constant c.

To see this, it will be shown that 
 can be computed nonuniformly from 
 and k. Suppose that one knows k and 
 and suppose that 
. Then one can compute s from 
 as s is the length of 
. Furthermore, 
 equals 
 and 
 equals 
, thus both can be computed from 
 and k. Below 
, 
 and no computations are needed. As A is K-trivial, it follows that
 Thus each 
 is K-trivial with the K-trivially constant bounded by  for some constant c independent of k. This shows the claim.

The measure 
 is strongly K-trivial by the result of the claim and the fact that 
 is finite. However, μ is not C-trivial, as 
 satisfies for  that 
 and
 note that the values 
 are obviously not bounded by a constant. Furthermore, all atoms of μ are the sets 
 which are finite and thus recursive. □

6. Martin-Löf random measures
Let 
 be the space of probability measures on Cantor space (which is canonically a compact topological space). Probability measures on this space have been defined by Mauldin and Monticino [18]. A special case is the uniform measure  on 
.

To define , first let  be the set of representations of probability measures; namely,  consists of the functions 
⁎
 such that 
 and 
 for each string σ. We note that  is closed in the product topology.  is the unique measure on  such that for each string σ and , we have 
. Intuitively, we choose 
 at random w.r.t. the uniformly distribution on the interval 
, and the choices made at different strings are independent. In the language of [18], the transition kernel τ takes the value λ for each dyadic rational (corresponding to a string σ).

We remark that for  and a string σ of length n, the function 
 on  given by  only depends on . In fact, the 
 are given by the recursion 
 and 
. E.g., 
.

Culver's thesis [8] shows that the measure  is computable in the sense of Hoyrup and Rojas [12]. So the framework provided in [12] yields a notion of Martin-Löf randomness for points in the space 
.

Proposition 6.1

Every probability measure μ that is Martin-Löf random w.r.t.  is Martin-Löf absolutely continuous.

For the duration of this proof let μ range over 
. For an open set 
, let
 Our proof of Proposition 6.1 is based on two facts.

Fact 6.2

.

Proof

Clearly, for each n we have 
 
  Furthermore, 
 whenever  because by the remark above there is a -preserving transformation T of 
 such that . Therefore 
.

If  are incompatible then 
. Now it suffices to write 
 where the strings 
 are incompatible, so that 
. □

Fact 6.3

Let 
 and let 
 be a Martin-Löf test such that there is 
 with 
. Then μ is not Martin-Löf random w.r.t. .

Proof

Observe that by the foregoing fact
 Let 
 which is uniformly effectively open in the space of measures 
. Fix k such that 
. By the inequality above, we have 
. Hence 
 is a Martin-Löf test w.r.t.  that succeeds on μ. □

This argument also works for randomness notions stronger than Martin-Löf's. For instance, if there is a weak-2 test 
 such that 
 for each m, then μ is not weakly 2-random with respect to . The converse of Proposition 6.1 fails. Culver [8] shows that each measure μ that is Martin-Löf random w.r.t.  is non-atomic. So a measure 
 for a Martin-Löf random bit sequences Z is ML absolutely continuous but not Martin-Löf random with respect to .

7. Being ML-a.c. relative to computable ergodic measures
We review some notions from the field of symbolic dynamics, a mathematical area closely related to Shannon information theory. See e.g. [34] for more detail. Thereafter we will consider effective “almost-everywhere theorems” related to that area in the framework of randomness for measures.

In symbolic dynamics it can be useful to admit alphabets other than the binary one. Let 
 denote the topological space of one-sided infinite sequences of symbols in an alphabet . Randomness notions etc. carry over from the case of . A dynamics on 
 is given by the shift operator T, which erases the first symbol of a sequence. A measure ρ on 
 is called shift invariant if 
 for each open (and hence each measurable) set G. The empirical entropy of a measure ρ along 
 is given by the sequence of random variables
 
 A shift invariant measure ρ on 
 is called ergodic if every ρ integrable function f with  is constant ρ-almost surely. The following equivalent condition can be easier to check: for any strings 
⁎
, 
 
 
 
 For ergodic ρ, the entropy  is defined as 
, where
 
  Thus, 
 is the expected value with respect to ρ. By the concavity of the logarithm function the limit exists and equals the infimum of the sequence. This limit is denoted , the entropy of ρ.

A well-known result from the 1950s due to Shannon, McMillan and Breiman states that for an ergodic measure ρ, for ρ-a.e. Z the empirical entropy along Z converges to the entropy of the measure. See e.g. Shields [34, Chapter 1], but note that the result is called the Entropy Theorem there.

Theorem 7.1 SMB theorem

Let ρ be an ergodic measure on the space 
. For ρ-almost every Z we have 
.

Recall from Fact 2.4 that a measure μ is ML absolutely continuous with respect to ρ iff  where  is the class of sequences in 
 that are not Martin-Löf random with respect to ρ.

If a computable measure ρ is shift invariant, then 
 exists for each ρ-Martin-Löf random Z by a result of Hochman [10]. Hoyrup [11, Theorem 4.1] gave an alternative proof for ergodic ρ, and also showed that in that case we have 
 for each ρ-ML-random Z. We extend this result to measures μ that are ML absolutely continuous with respect to ρ, under the additional hypothesis that the 
 are uniformly bounded. This holds e.g. for Bernoulli measures and the measures given by a Markov process.

Proposition 7.2

Let ρ be a computable ergodic measure on the space 
 such that for some constant D, each 
 is bounded above by D. Suppose a measure μ is ML absolutely continuous with respect to ρ. Write . Then 
.

Proof

By Hoyrup's result, 
 for each ρ-ML-random Z. Since the sequences that are not Martin-Löf random w.r.t. ρ form a null set w.r.t. μ, we infer that 
 for μ-a.e. Z. Since the exception set is measurable and the 
 are bounded, the Dominated Convergence Theorem now shows that 
, as required. □

We now given an example showing that the boundedness hypothesis on the 
 is necessary. We provide a computable ergodic measure ρ such that some finite measure  makes the sequence 
 converge to ∞.

Proposition 7.3

There are an ergodic computable measure ρ (associated to a binary renewal process) and a computable measure  such that 
. (We can then normalise μ to become a probability measure, while maintaining the same conclusion.)

Proof

Let k range over positive natural numbers. The real 
 is computable. Let 
 so that 
. Let 
, and note that b is also computable.

Let ρ be the shift invariant measure associated with the binary renewal process corresponding to the sequence 
 and , which is given by the conditions
 Informally, the process has initial value 1 with probability , and after each 1, with probability 
 it takes k many 0s until it reaches the next 1. See again e.g. [34, Section 1.2c] where it is shown that a measure ρ of this kind is ergodic. Write 
. Note that 
.

Define a function f in 
 by 
 and  for any X not extending any 
. It is clear that f is 
-computable, in the usual sense that there is an effective sequence of basic functions 
 converging effectively to f in the 
-norm: let 
 in case 
, , and 
 otherwise. Define the measure μ by , i.e. 
. Thus 
. Since ρ is computable and f is 
-computable, μ is computable. Also note that 
 is finite.

For any , letting , we have
 
 
 
 This completes the proof. □

The next observation shows that the asymptotic initial segment complexity of a ML absolutely continuous measure relative to ρ obeys some lower bound. Note that . So for , this shows that in Example 4.1 we cannot subtract, say,  instead of 
.

Proposition 7.4

Let ρ be a computable ergodic measure, and suppose μ is a ML absolutely continuous measure with respect to ρ. Then 
 
 
 
 

Proof

We can use K and C interchangeably because 
 [24, Proposition 2.4.1]. We settle on K.

Let 
. The argument is very similar to the one in Proposition 7.2 above, replacing the functions 
 by the 
. Note that 
 is bounded above by a constant because 
. Hoyrup's result [11, Theorem 4.1] states that 
 for each ρ-ML-random Z. Now we can apply the Dominated Convergence Theorem as in the proof of Proposition 7.2. □

8. Conclusions and open questions
In the present paper, we studied probability measures on Cantor space with respect to algorithmic randomness and triviality properties.

A main property we studied was that a measure μ is ML absolutely continuous: for every Martin-Löf test the μ-measure of its components converges to 0. We provided several examples and showed a robustness property related to Solovay tests. In Section 4 we introduced strong Solovay tests for measures. We leave the following question open.

Question 8.1

If a measure passes every strong Solovay test, does it pass every Solovay test?

Thereafter, we studied measures μ with respect to the growth behaviour of the μ-averages of the descriptive string complexities  and , taken over all strings x of length n. We considered measures where the growth is as fast as possible, and others where it is as slow as possible. In the second direction, we looked at C- and at K-trivial measures, where the averages are bounded up to a constant by  or , respectively. Such measures are atomic, that is, they are weighted, possibly infinite sums of Dirac measures. The atoms of such measure satisfy the corresponding triviality property; furthermore, the weighted sum of the atoms has to converge fast, though we could not find a precise criterion for this. In this context, we mention two open questions.

Question 8.2

Is every C-trivial measure also K-trivial?

An affirmative answer to the next question would lead to an affirmative answer to Question 8.2; Proposition 5.8 provided more detail.

Question 8.3

Is there a constant bound on the function ?

Measures μ on the upper end of the growth spectrum are ML absolutely continuous. However, we showed that also here there is no exact correspondence between the growth-rate of the μ averages of the string complexity and the property of being ML absolutely continuous; instead, in the border area, there are measures that are smaller with respect to the growth-rate of the averages which are ML absolutely continuous, and measures that are larger but which are not ML absolutely continuous. In particular, an analogue of the Levin-Schnorr Theorem does not hold for measures. We noted that near the upper end of the growth spectrum for C are the Kolmogorov random measures μ, the ones for which there is a constant d with 
 for infinitely many n. Besides the usual uniform measure, there are other examples, such as the Dirac measure of a set that is 2-random. While the classes of C-trivial and K-trivial measures are closed under finite convex combinations, this condition is not known to hold for classes of measures at the upper end of the growth spectrum.

Question 8.4

Assume that measures  are Kolmogorov random. Is every convex combination of μ and ν Kolmogorov random as well?

As mentioned in Section 3, a sequence X is 2-random iff it is Kolmogorov random as defined in [16]. So an affirmative answer to the question above implies an affirmative answer to the following:

Question 8.5

If 
 are both 2-random, does there exist a constant d such that infinitely many n satisfy both 
 and 
?

The following diagram summarizes the implications obtained between (weak) randomness notions for measures. Non-labelled implication arrows are trivial. We ignore at present whether the implications given by the arrows labelled 3.5 and 3.2 are proper. All the other implication arrows are proper. In most cases this can be seen already for Dirac measures. For instance, Martin-Löf randomness of a measure implies that the measure has no atoms, so the converse of the implication labelled Proposition 6.1 fails. We can rule out two further implications. Each ML-random μ is orthogonal to λ by Bienvenu and Culver [8, Section 2.6], and hence not absolutely continuous. We ignore whether ML-randomness implies one of the conditions stronger than ML-a.c. displayed in the lower part of the diagram. In Section 7 we considered ML absolute continuity relative to a general ergodic computable measure. We proved appropriate effective versions of the Shannon-McMillan-Breiman theorem and the Brudno theorem where the bit sequences are replaced by measures; in the former case we needed an additional boundedness hypothesis.

One possible effective version of the Birkhoff ergodic theorem states that for an ergodic computable ρ, if 
 is ρ-integrable and lower semicomputable and Z is ρ-Martin-Löf random, then the limit of the usual ergodic averages 
 
 equals . For background see e.g. [23] which contains references to original work. If in addition the 
 are bounded then an argument similar to the one in the proof of Proposition 7.2 shows that 
 for any measure 
. However, it is unknown whether this additional hypothesis is necessary.