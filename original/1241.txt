Abstract
Graphical representation (representing logical specifications by means of one or several labeled transition systems) is a typical connection between process algebra and temporal logic, the two main paradigms that specify and reason about reactive concurrent systems. In this paper, we encode (represent “graphically”) a fragment of Action-based CTL, proposed by Lüttgen and Vogler, in the process calculus CLLR. In this way, safety properties can be described easily, and usual process operators (parallel, choice, etc.), logical operators (conjunction and disjunction) and temporal operators (always, until, etc.) can be mixed freely in CLLR.

Keywords
Safety
Action-based CTL
Logic labeled transition system
CLLR
Graphical representation

1. Introduction
In order to formally specify and reason about reactive concurrent systems, two popular paradigms are provided: process algebra and temporal logic. In the former, specification and implementation of a system are usually formulated in terms of expressions (terms) built from a number of operators and the underlying semantics is often given operationally. Meanwhile in the latter, a specification is expressed by a set of formulae in some temporal logic without referring operational details. The refinement in process algebra is often based on behavioral relation and the verification amounts to comparing terms. In temporal logic, a natural refinement on logic specifications is the set inclusion of models and refining a specification amounts to enriching original one by adding new formula; verification is often a model checking activity. Since the behavioral relations in process algebra are compatible with (most) process constructors, i.e. they are (pre)congruent w.r.t operators, process algebra often supports compositional construction and reasoning, an advantage in system design that we can develop and verify systems modularly. However, temporal logic often gives little support for modular design and compositional reasoning since ones are often required to consider a system as a whole when formulating and verifying a logic property. For the detailed description of systems, it is often difficult for process algebra to describe abstract properties of systems. On contrary, temporal logic is suitable for describing common abstract properties of concurrent systems, such as safety and liveness etc. We list the comparisons above briefly as follows,

Process algebra	Temporal logic
Refinement	Behavioral relation	Set inclusion of models
Verification	Comparison of terms	Model checking
Modularity	Yes	No
Abstraction	No	Yes
where Yes (or No) indicates that usually the paradigm given on top of the table supports (doesn't support respectively) the one given on the first column.
Much research focuses on the connections between the two paradigms and Pnueli has studied them systematically [23]. Here we give a brief introduction. Let  be a process language and ⊑ its associated behavioral relation (refinement). Let  be a logic language and ⊨ its associated satisfiability relation.

• (Adequacy)  is adequate with respect to , if for any process 1 This is the weakest requirement of compatibility between a process language and a logic [23], and often called modal characterization. A classic result is the characterization theorem of bisimulation equivalence in terms of Hennessy-Milner Logic (HML), which says that, two image-finite processes are bisimilar if and only if they satisfy the same HML formulae. In concurrency theory, there are many modal characterization results in that spirit [25], [9]. Aceto et al. point out in [3] that modal characterization is less useful if one intends to check the behavioral equivalence of processes using model checking.

Stronger compatibility requirement between process algebra and logic system involves translations between them. One is characterizing processes in terms of logic formulae, the other is representing logic formulae by (sets of) processes terms. We recall them one by one.

• (Expressivity I)  is expressive with respect to , if for each process p there exists 
 such that for any process q,

(a)
;

(b)
.

If such formula of a process can be algorithmically constructed, (a) enables reduction of implementation verification to model checking, and verification of predicate  can be transformed to the validity problem in  by (b). To the best of our knowledge, Graf and Sifakis may be the first to develop logics which are expressive for process algebra [11], [12], [13].
Given a process p, a formula 
 is called characteristic formula if it satisfies (a). Graf and Sifakis [13] offered a translation from recursion-free CCS terms to formulae of a modal language which represent the equivalence class with respect to observational congruence. For various behavioral relations, some examples of characteristic formula construction have been reported in [10], [14], [24], and Aceto et al. offer a general method for constructing characteristic formulae [3].

• (Expressivity II)  is expressive with respect to , if for each logic formula ψ there exists 
 such that for any process q,(EII)
 If such process of a formula can be algorithmically constructed, the model checking problem can be transformed to the implementation verification. This connection between process algebra and logic is often called graphical representation, and addressed in [8], [1], [20], [6]. Boudol and Larsen [8] translate every HML formula without negation into a finite set of terms in a process language. In particular, if the given formula is consistent and prime (i.e. irreducible with respect to disjunction), the corresponding translated finite terms could be reduced to a singleton one and (EII) is satisfied.

Recently, Lüttgen and Vogler have provided an interesting graphical representation connection between process algebra and logic in their heterogeneous framework, logic labeled transition system (Logic LTS or LLTS for short), which combines operational and logical styles of specification in one unified framework [18], [19], [20], where we can take advantage of process algebra and temporal logic at the same time. In addition to the usual process algebra operators (e.g., CSP-style parallel composition, hiding, etc.) and logical operators (disjunction and conjunction), some standard temporal logic operators, such as “always” and “unless”, are also integrated into LLTS [20]. Lüttgen and Vogler embed a universal fragment of the temporal logic Action-based CTL (UACTL) [22], which can specify safety properties, into LLTS, and prove that the logic satisfaction relation is compatible with the refinement in LLTS [19], i.e., (1) a process satisfies a property if and only if it refines this property; (2) the refinement is compositional for the temporal operators.

Lüttgen and Vogler's heterogeneous framework is entirely semantic, and their graphical representation is very complex and visualized by means of drawings, but for many purposes it is convenient to have a syntax. Recently, we have proposed an LLTS-oriented process calculus 
2 [26], provided an axiomatic system 
 for 
 finite processes [28], researched equations of the form 
 in 
 and established Unique and Greatest Solution Theorems, where = is the equivalence relation induced by the refinement in LLTS [26], [27]. In this paper, we use recursive terms to represent temporal formulae, and give a syntactic translation from formulae of UACTL to 
. Our translation is straightforward and simple. We also prove that this translation is correct, i.e. (EII) is satisfied. It means that a process p satisfies a temporal logic formula ψ if and only if p refines the process translated from ψ.

In 
, operational and logical operators can be mixed freely without limitation and temporal properties can be expressed easily. We can take advantage of both process calculus and temporal logic, describe behavioral details of systems and abstract temporal properties (safety) easily, construct systems and reason their properties compositionally. To the best of our knowledge, we cannot find other process calculus that has these characteristics at the same framework.

The rest of this paper is organized as follows. The notion of Logic LTS and the calculus 
 are recalled in the next section. A fragment of ACTL is recalled in Section 3, where we also encode this fragment in 
. Section 4 provides some preliminary results concerning a temporal operator and a logical constant and we show the correctness of our encoding in Section 5. The paper is concluded with Section 6.

2. Preliminaries
In this section, we fix our notation and terminology, introduce Logic LTS and its associated refinement relation (i.e. ready simulation), and recall the LLTS-oriented process calculus 
 and its operational semantics.

2.1. Logic LTS and ready simulation
Let Act be a nonempty and finite3 set of visible action names ranged over by a, b, etc., let τ represent invisible actions and let 
 denote  ranged over by α and β. A labeled transition system (LTS for short) is a quadruple 
 which consists of a set P of states, a set 
 of transition labels, a transition relation 
 and a predicate . In the following, for simplicity, we omit 
 and write LTS as .

As usual, we write 
 instead of 
, 
 if 
 for some , otherwise we write

Image 1
. When 
, we say that process p can perform an α-action to 
, and we call 
 an α-derivative of p. When p performs several actions to reach 
, i.e. 
, we call 
 a descendant of p; particularly, if all 
 are τ, we call 
 a τ-descendant of p. Given a state p, its ready set 
 is denoted by , we also call it the initial actions of p. A state p is stable if
Image 2
. Next we list a number of useful decorated transition relations:
 iff 
 and ;

 iff 
⁎
 i.e. p performs some number, possibly zero, of τ-actions to reach q;

 iff 
⁎
 i.e. 
 and all states along this sequence, including p and q, are not in F;

 iff 
 for some ;

 iff 
 for some ;

 iff 
 and q is stable, where 
;

 iff 
 and q is stable, where 
.

Notice that 
 has the same meaning as the notation 
 in [19], [20], and F-predicate is not involved in the notation 
 in this paper.

Definition 2.1

Logic LTS [19]
An LTS  is an LLTS if, for each ,

(LTS1)
 if 
;

(LTS2)
 if 
.

Moreover, an LTS  is τ-pure if, for each , 
 implies

Image 3
for any .
Compared with usual LTSs, as a distinguishing feature of LLTS, the additional predicate F denotes the set of all inconsistent states that represent empty behavior that cannot be implemented [19]. Intuitively, states in F should not be reached at runtime of systems. The main motivation behind the consideration of inconsistencies lies in dealing with them caused by conjunctive composition. In the sequel, the phrase “inconsistency predicate” is used to refer to F. The backward propagation of inconsistencies is formalized by the condition (LTS1), i.e. a process is inconsistent if it has no other choice for some action than entering an inconsistent state, and the intuition that divergence (i.e., infinite sequences of τ-transitions) is viewed as catastrophic is captured by the condition (LTS2). For more intuitions and motivations about these settings, the reader may refer to [18], [19].

As a variant of the usual notion of weak ready simulation, the ready simulation below is adopted to capture the refinement relation [19], [20].

Definition 2.2

Ready simulation on LLTS [19]
Let  be a LLTS. A relation  is a stable ready simulation relation if, for any  and 

(RS1)
both p and q are stable;

(RS2)
 implies ;

(RS3)
 implies 
;

(RS4)
 implies .

If  belongs to a stable ready simulation relation , we say that p is stable ready simulated by q, denoted by  
 
. Further, p is ready simulated by q, denoted by 
, if for every 
 there exists 
 such that 
 and 
 
. The kernels of  
 
 and 
 are defined as usual and denoted by 
 and 
 respectively. It is not difficult to find that  
 
 itself is a stable ready simulation relation and both  
 
 and 
 are pre-orders.
2.2. The calculus 
 and its operational semantics
For LLTS, we provide a process calculus 
 [26] and introduce it in this subsection briefly. Let Var be an infinite set of variables. The terms of 
 can be given by the abstract syntax
 where , 
,  and every recursive specification E is associated with a set  such that E is a set of equations  and the variable  acts as the initial variable. Sometimes, we want to indicate such set V explicitly and use the notation  instead of .

Most of the operators in 
 are usual, they are from CCS and CSP, such as deadlock 0 (cannot execute any action), action prefixing , non-deterministic external choice □ and parallel composition 
 with synchronization action set A. Three special ones are ⊥, ∧ and ∨. The first one represents inconsistent processes with empty behavior and the last two are logical operators, which are intended for describing logical combinations of processes. Semantically, ∨ acts as internal choice and ∧ is a synchronous composition on visible actions and an interleaving product on τ action. For recursion operator  [4], it computes the least transition relation satisfying a system of recursive equations.

For , the variables in V are recursive, they are bound with scope E and the notion of free occurrence of variable, bound (and free) variables and α-equivalence are defined as usual. A process is a closed term (without free variables) and the set of all processes is denoted by 
 ranged over by  etc. In this paper, as usual, the recursive variables in  do not occur free in any term that contains  and recursive variables are distinct from each other; moreover we consider α-equivalent terms indistinguishable and use ≡ for both syntactical identity and α-equivalence. In the sequel, 
 is often denoted by 
 briefly.

Given a recursive specification  and a term t, we define  to be , i.e. simultaneously replace all free occurrences of each  by  in t. In  we replace not only variable  occurring directly in t, but even Y occurring free inside inner recursive specification. For example, consider 
 and 
 then 
. In particular, if  then for any ,  whenever  and  if .

Given a term t, an occurrence of X is strongly guarded if such occurrence is within some subexpression 
 () of t and weakly guarded if such occurrence is within some subexpression 
 or 
. For example, X is weakly guarded in  as ∨ acts as internal choice and X may expose itself after τ-transition of , similar to the one in . Further, if each occurrence of X is strongly (or weakly) guarded in t then X is strongly (weakly respectively) guarded in t. Given a recursive specification , if each  is (weakly or strongly) guarded in 
 such that 
 then  is guarded. As usual, in the remainder of this paper, we only consider guarded recursive specifications.

SOS rules of 
 are listed in Table 1 on Page 5, where , 
 and . They are chosen as simple as possible and reflect semantical settings of operators and we divide them into two parts.


Table 1. SOS rules of CLLR.

The first part contains operational rules specifying the behavior of processes. As a technical constraint in LLTS [18], τ-purity of 
 is guaranteed by Rules 
, 
, 
 and 
 whose negative premises give τ-transition precedence over visible ones. The operational behavior of 
 (see Rules 
 and 
) is same as internal choice in usual process calculus. Rules 
, 
 and 
 reflect that the conjunction operator is a synchronous product on visible transitions and an interleaving product on τ action. The operational rules about the other operators are as usual.

The second part contains predicate rules specifying the inconsistency predicate F. The axiom 
 says that ⊥ is inconsistent. Hence ⊥ is not implementable. While 0 is consistent and can be implemented. It means that ⊥ and 0 represent different processes, clearly, 
 but

Image 4
. Rules 
, 
 and 
 concerning logical operators are straightforward. The first one reflects that a disjunction is inconsistent if its two disjunctive parts are inconsistent and the last two say that if one conjunct is inconsistent so is the conjunction. The typical system design strategy that one inconsistent part brings the inconsistent whole composition is described by Rules 
. In other words, inconsistency is catastrophic. Rules 
 and 
 reveal that a stable conjunction is inconsistent if its two conjuncts have distinct ready sets. Rules 
 and 
 are used to capture (LTS2) in Definition 2.1. Intuitively, these two rules say that if all stable τ-descendants of a process are inconsistent, then the process itself is inconsistent. Note that the transition relation 
 occurring in them does not involve any requirement on consistency. For more intuitions and settings about these rules, the readers could refer to [26].
Some rules in 
 contain negative premises, the existence of the transition model of 
 is not obvious. By the stratification technique, we get the unique stable transition model of 
, denoted by 
, which exactly consists of all positive literals of the form 
 or tF that are provable in 
 [26], i.e., 
. Here 
 is the stripped version [7] of 
 w.r.t 
, i.e.
 
 where  (or, ) is the set of positive (negative respectively) premises of r,  is the conclusion of r and 
 means that for each

Image 8
, 
 for any 
.
Note that all proofs in this paper are based on 
 (a set of ground rules without negative premises) not on the set of rule patterns in Table 1 directly. 
 (or tF) means that 
 (tF respectively) is provable in 
, i.e. there is a well-founded upwardly branching tree with root 
 (tF respectively) whose nodes are labeled by literal 
 or 
, and if H is the set of labels of the nodes directly above a node with label C then 
 
. The formal definition of the proof tree could be referred to [2]. For example, see Case 2 in Lemma 4.4(2) where we use the ground rule 
 
 with a side condition that

Image 9
or 
 
 with a side condition that
Image 10
.
The LTS associated with 
, denoted by 
, is 
, where 
 iff 
, and 
 iff 
. Therefore, for any p, 
 and 
, 
 (or 
) iff 
 (
 respectively). For simplicity, the subscripts in 
 and 
 are omitted in the following.

We quote some results from [26] at the end of this section. Lemma 2.3 lists some facts about F and Theorem 2.4 reflects that 
 is indeed a τ-pure LLTS.

Lemma 2.3

Let p and q be any two processes. Then

(1)
 iff ;

(2)
 iff  for each 
;

(3)
 iff either  or  with 
;

(4)
 or  implies ;

(5)
 and .

Theorem 2.4

 is a τ-pure LLTS. Moreover if  and  then 
.

Lemma 2.5

 and 
 iff 
.

Intuitively, Lemma 2.5 says that p refines  if and only if p refines both q and r. It is the desirable (logical) property that 
 should have. The next theorem says that 
 is precongruent w.r.t all operators of 
.

Theorem 2.6 Precongruence

If 
 then 
, where 
 is any context defined as usual.

3. Encoding of a fragment of ACTL in 
In this section, we will recall a universal fragment of the Action-based CTL (UACTL) and its satisfaction relation given in [20], which can express safety properties, recall Lüttgen and Vogler's graphical representation of UACTL in LLTS briefly, and give an encoding of this fragment in 
. A whole subsection is also provided to compare their graphical representation with our encoding by an example. In the following, the relation 
 is defined as  
 
.

3.1. UACTL and its graphical representation
Definition 3.1

[20]
The formulae of UACTL are defined by the BNF grammar: where . The set of UACTL formulae is denoted by 
. The satisfaction relation 
 is defined recursively as follows


Download : Download high-res image (93KB)
Download : Download full-size image
 and  are atomic formulae, which represent enableness and disableness of action a respectively.  is a kind of next operator with explicit action a. A and W are the usual temporal operators always and weak until. For more intuition of this satisfaction relation, the reader may refer to [20].

Lüttgen and Vogler graphically represent UACTL in LLTS and prove that ⊨ is compatible with 
, i.e.  iff 
 where  is the graphical representation (GR) of ϕ in LLTS and 
 is compositional for the temporal operators. Next we recall their graphical representation concerning all operators except A- and W-ones briefly. In Subsection 3.3, we give an example concerning A-operator. For the detailed definitions of all operators, the reader can refer to [20].

Since ⊨ is compatible with 
 and  for any process p, the  should simulate any process and may be like Fig. 1(a), which can nondeterministically select an arbitrary  via a τ-transition; from there it can return to  by any transition labeled with an action . Formula ff can be represented by the inconsistent process in Fig. 1(b). The  may be the LLTS in Fig. 1(c). By τ-transition, it can move to any process with ready set A containing a, from where, it can engage in a b-transition, , to . Similarly,  could be almost the same except that by τ-transition it moves to any process with ready set that does not contain a. The  may be like Fig. 1(d). Intuitively, it says that every a-derivative should be  and it simulates any process whose a-derivative satisfies ϕ. For 
 and 
, their GRs are straightforward, i.e. the conjunction and disjunction compositions on 
 and 
 respectively.

Fig. 1
Download : Download high-res image (49KB)
Download : Download full-size image
Fig. 1. Graphical representation of some UACTL formulae.

3.2. Encoding of 
 in 
Before encoding 
 in 
, we give some additional notations. Let 
 be a tuple of terms. The general external choice  
 
 is defined as follows: 
 
 
  The general disjunction  
 
 and general conjunction  
 
 are defined similarly, and  
 
 and  
 
 are defined as ⊥ and ⊤ respectively. Additionally, given a finite set S, by the associative and commutative laws of external choice [28], we often write  
  (or □S) instead of  
 
 without giving the fixed tuple induced by S explicitly. Moreover, the general disjunction ⋁S and general conjunction ⋀S are defined similarly.

The encoding of 
 in 
 is conducted along the structure of formulae. The first step is to encode tt, ff and atomic formulae  and . According to  in Fig. 1, the encoding of tt may be the recursive term of 
, denoted as ⊤, in Table 2, and intuitively the τ-transition in  (Fig. 1(a)) is a result of ⋁ in ⊤. Since Act is nonempty and finite as the constraint in this paper, the definition of ⊤ makes sense. The formula ff can be encoded as term ⊥ directly. According to  (Fig. 1(c)), we need to encode every stable τ-descendant of . Clearly, 
 in Fig. 2 is what we need and  can be encoded as  
 
 whose restriction  guarantees that every stable τ-descendant has the ability to perform action a. Similarly,  can be encoded by the process term  
 
.


Table 2. Auxiliary definitions for encoding.

  
 		
 
 		 
 
 		 
 
 

Fig. 2
Download : Download high-res image (128KB)
Download : Download full-size image
Fig. 2. An example for temporal operator on LLTS.

In order to encode , according to  (Fig. 1(d)), we need to encode the stable process with ready set A such that all its derivatives except a-derivative are  and its a-derivative is  if . Clearly, it can be encode by using  in Table 2 for 
 if  and 
 if  and . Intuitively, if the process  (see Table 2) executes visible action a then its a-derivative is p, and it can be used to encode the next operator definitely. Further, we define  (see Table 2) and use it to encode the (general) next operator with all actions in A. In particular,  is used to encode temporal operator W (see Table 3).


Table 3. The encoding function .

 
 
Intuitively, ⊤ is the loosest specification, in other words, as a specification, ⊤ specifies nothing. Formally, it is easy to prove that 
 for each p, i.e., ⊤ can be implemented by any process (see Lemma 4.1(3)). ⊤ is not stable, i.e., 
. 
 if .  if , and 
 whenever .

Inspired by Lüttgen and Vogler's graphical representation of UACTL in LLTS [20] and the standard fixpoint representation of temporal operators [5], we use recursive processes to represent the two temporal operators A and W, and the encoding function 
 is defined in Table 3. It is not difficult to see that it is well defined. Clearly, X is strongly guarded in terms  
  
  and . This means that the encoding of tt and 
 makes sense as it is assumed that all recursive specifications in this paper are guarded.

3.3. An example
In this subsection, we use an example to compare Lüttgen and Vogler's graphical representation with our encoding of 
. This typical example contains two switches, 
 and 
, one scheduler S, and an additional constraint 
, the whole specification with the action set 
 is
 whose component LLTSs are sketched in Fig. 2 and 
 and ∧ are parallel and conjunction composition on LLTSs respectively. As we know, in general, the parallel composition 
 is not associative. But in this example, the two ones have the same synchronization action set and 
 is commutative and associative. So we omit parentheses explicitly.

In Fig. 2, the LLTS 
 specifies a simple switch with two states, OFF and ON; one can toggle between them by using the switching action 
. The 
 self-loop of the state ON indicates that the switch 
 is on. 
 is defined as 
 but replacing all indices 1 with 2. The LLTS S allows at least one switching action at any state and it cannot control the actions 
 and 
. For simplicity, we often use ready set to denote real process in Fig. 2.

The constraint 
 indicates that two switches are not allowed to be on at the same time and it is formulated as 
 in UACTL abstractly. Normally, we specify 
 as 
 directly. Since UACTL does not have negation operator, we use a semantically equivalent one.

The 
 is an LLTS from A-operator on LLTS 
, see Fig. 2 where we also give the LLTSs 
 and 
 explicitly. The definition of  can be referred to the one in Fig. 1(a).

All processes in 
 are finite vectors over the processes of 
. Intuitively, a vector behaves as the general conjunction of all its components, i.e., synchronous composition on visible actions of all components and interleaving product on τ action of some component. Moreover, F predicate associated with A-operator is also similar to the one associated with conjunction, such as if one component is inconsistent then so is the vector, a stable vector is inconsistent if its two distinct components have different ready sets, etc.

Since 
, we have two τ-transitions 
. For simplicity, two τ-transitions are denoted as a 
-labeled transition in Fig. 2.

As a τ-derivative of  and 
, 
 has a 
-transition to , we have 
 where intuitively an additional process 
 is added to the process  after the 
-transition and the same thing is done for every visible action of 
. If the process  is considered as the 
-derivative of 
, it is not difficult to see that the desired constraint is violated since it has a stable consistent τ-descendant process 
 with ready set 
. The addition of 
 can avoid this violation and we see why it is in the following. For every stable τ-descendant p of 
, the stable process 
 is inconsistent due to that 
 and p have different ready set. Hence 
 is inconsistent (tagged by F in Fig. 2) due to that 
 is a LLTS and (LTS2) in Definition 2.1, and it should not be reached at runtime although it is a τ-descendant of 
. Summarily, for every stable consistent process in 
, its ready set does not contain 
 and 
 at the same time.

Clearly, it is not easy to give 
 by constructing its LLTS directly. Next we will see how to express this example in 
. The switch 
 can be expressed as 
 and the switch 
 is almost the same but replacing all indices 1 in 
 with 2. The scheduler S is formulated as 
. By the encoding function , the constraint's specification 
 can be transformed to a term as
 
 
 Finally, we express the example as 
 where 
 and ∧ are operators in 
. Clearly, our encoding is much more straightforward and concise than Lüttgen and Vogler's graphical representation [20].

Although their graphical representation is complex, it does inspire us that we should find a mechanism in an encoding to preserve the property we need along the transitions, especially along the visible transitions. We find that the recursive terms of specific form can solve our problem (see Table 3).

4. Some results about ⊤ and weak until
The previous section gives a method to encode 
 in 
. Next, we shall prove that this encoding is correct, i.e., for every 
 and 
,
 This means that the model checking problem can be transformed to the 
-relation checking problem easily. In order to prove this result, in this section, we give some lemmas.

In the following, for simplicity, we need two notations: where 
. Note that . Obviously, we get(AW) where  is the encoding function and 
.

We first give some simple results about ⊤ which will be used many times in the following proofs.

Lemma 4.1

(1)
 and 
 for any .

(2)
 for any .

(3)
 
 for any stable p; 
 for any p.

(4)
 for any stable p; 
 for any p.

Proof

(1) Since 
, by Lemma 2.3(5) and Theorem 2.4, we get . Further, by Lemma 2.3(2)(3), it is easy to see 
.

(2) Straightforward.

(3) It is not difficult to check that 
 is a stable ready simulation relation. So we have  
 
. Next we prove 
. Assume 
. By Item (2), 
. Moreover, 
 
. Hence 
.

(4) Straightforward from Item (3) and the fact that 
 iff 
 [19], [26]. □

To prove the correctness of our encoding, the most difficult part is to handle the temporal operator W, the remainder of this section is devoted to it and we shall show the main result of this section:(WU)
 whose form is similar to the definition of the satisfaction relation of 
 (see Definition 3.1).

Firstly, we will prove the reverse implication (see Lemma 4.9). To this end, we shall prove
 under the specific condition (the right of (WU)). By the definition of 
, we need to prove that every stable consistent τ-descendant of p has a corresponding stable consistent τ-descendant of  to match. There are two kinds of τ-descendants of  due to its definition, the τ-descendants of r and the τ-descendants of . In the following, we analyze some descendants of .

Assume 
, by the definition of 
, we should prove that there exists 
 such that 
 
. By the definition of , there are two cases depending on whether 
 or not

1.
 and 
;

2.
 and 
.

We generalize the second case and assume 
 with the restriction that
Image 11
for every . It follows that 
 by the right of (WU). Then, we should prove that there exists a corresponding transition sequence(A)
 where 
 
. Next we analyze the form of 
. Since ∧ is a synchronous composition on visible actions and an interleaving product on τ action (i.e. it is a static operator [21]), the left conjunct q is involved in all transitions from  to 
 and even from 
 to 
; we get
 where 
 is a conjunct of the conjunction 
. Moreover, by Lemma 2.3(4), all processes along this transition sequence should be consistent. For the right conjunct , similarly, we have
 where 
 is a conjunct of 
 and 
. Moreover, it is not difficult to see that 
 is of the form  
 
. We must make sure that 
 for  otherwise 
 for 
 and 
 have different ready set. Clearly,  is a subterm of 
 and 
 due to 
. For this new , in the following transitions it is involved, the restriction 
 pushes it to choose the right disjunct of the definition of its recursive equation (i.e. ) and a new q appears, similar to the second case above (we generalize it). Moreover, 
 for 
. For these ⊤'s, there are irrelevant for our analysis here. Summarily, for each 
, as a subterm of 
,  exposes itself after 
 is performed by 
 and only its right disjunct is involved. In other words, every visible action 
 in the sequence (A) brings a new q which engages in the transitions after where it appears. By the analysis above, we have the following diagram
 where 
 are conjuncts of 
. It follows that 
 
 by 
 
 and Lemma 2.5. In order to prove the reverse implication of (WU), we reverse the whole analysis above and prove some results when the diagram above with 
 
 exists (see Lemma 4.5, Lemma 4.8).
As we know the ready simulation on LLTS (see Definition 2.2) focuses on the stable consistent states mainly, so that, one of crucial tasks in our proofs is to prove whether given processes are consistent. Lemma 4.5 is exactly the one that we prove that the processes of the specific form are consistent. We can not prove it directly since the predicate rules in Table 1 specify which terms are inconsistent. Luckily, by coinduction and the fact that proof trees are well-founded, we can prove it. When we use coinduction method, usually, we put terms in the set we construct as possible as we can. By Definition 4.2, Definition 4.3, we give a general form of processes in the sequence (A) and put all processes of this form in the set we need, i.e. the set Ω in Lemma 4.5.

We give a high-level picture as follows to clarify the structure of the remainder of this section


Download : Download high-res image (48KB)
Download : Download full-size image
where an arrow means that its target is dependent on its source directly.
Definition 4.2

Let S be a set of processes and p be a process. The disjunctions, whose disjuncts belong to S, are defined by the BNF grammar: where , we use 
 to denote the set of these processes. Similarly, we use 
 to denote the set of conjunctions whose conjuncts belong to S. Moreover, the disjunctions, whose disjuncts belong to  and particularly do contain p, are defined by the BNF grammar: where 
, we use 
 to denote the set of these disjunctions.

Intuitively, 
 is the set of terms of the form 
 where 
. Notice that 
 if .

Definition 4.3

Let  be two processes and 
 a tuple of processes. A set of processes 
 is defined as follows:
 

Clearly, for any 
, t contains 
 as a disjunct, 
, 
 for some 
 if t is unstable, and 
 if t is stable. Similarly, 
 () has the same properties. The next lemma shows some properties of 
 and the proof trees involved in the inconsistency predicate F.

Lemma 4.4

(1)
For any 
 with 
, if 
 then 
 for some 
.

(2)
If 
 for any , then for any 
, every proof tree of 
 has a subtree with root 
 for some 
.

Proof

(1) By the definition of Γ, t is a conjunction and let C be the set of its conjuncts. Clearly, 
. Since 
, 
 for some . In order to prove that 
 for some 
, it suffices to prove that 
 for some 
. It is a routine procedure to check different form of s according to 
, and here we check two cases, the others are left to the readers. For , 
 for some 
 due to the definition of ⊤. For , clearly 
, since 
 for any , we get 
 due to 
, as desired.

(2) Let 
 and  be a proof tree of 
. We prove the statement by induction on the depth of  and distinguish different cases based on the last rule applied in . Here, we handle the nontrivial case that 
.

Case 1. 
 
 or 
 
.

We treat the former. If 
 for some 
 such that 
 then this case is impossible due to 
, else for the proof tree of 
, by the induction hypothesis (IH),  has a (proper) subtree with root 
 for some 
.

Case 2. 
 
 with

Image 9
or 
 
 with
Image 10
.
It is trivial due to the fact that

Image 12
and 
.
Case 3. 
 
.

Since 
 for , 
 for some 
. Clearly, 
 is a premise of the root of .

Case 4. 
 
.

For , it is similar to Case 2 and trivial. If  then there exists 
 with  such that 
 and 
 is a conjunct of 
. Then, 
 for some 
. So, 
 for some 
 and 
 is a premise of the root of . Further, by IH,  has a (proper) subtree with root 
 for some 
. □

As discussed on Page 11, by coinduction and the fact that proof trees are well-founded, we can prove the next lemma which shows that the processes of a specific form are consistent, this result is needed in the proof of Lemma 4.8.

Lemma 4.5

Let  be processes such that 
. For any 
 and 
 with 
, if

Image 13
and 
 
, then  
 
 
.
Proof

Let Obviously,  
 
 
. Due to the well-foundedness of the proof trees, in order to prove  
 
 
, it is sufficient to show that, for each , if  is a proof tree of 
 then  has a proper subtree with root 
 for some 
. We shall prove this as follows.

Let  and  be a proof tree of tF. Then 
 for some n and 
. By the definition of Γ, we distinguish the following two cases.

Case 1. 
.

By Lemma 4.1(1), it is easy to see that t is one of the following two forms.

Case 1.1. 
 for some .

If t is stable then 
. Next we distinguish two cases based on whether 
 or not. If 
 then 
, this case is impossible. If 
 then it is not difficult to see that  has a proper subtree with root 4 and .

If t is not stable then 
 for some 
 and the last rule applied in  is 
 
. It is easy to know that 
 or 
 belongs to 
. Hence, there exists 
 such that one premise of the root of  is 
.

Case 1.2. .

In this case, the last rule applied in  is 
 
 or 
 
.

For the former, it is easy to see that 
 and  has a proper subtree with root  due to 
 
. For the latter, since 
 
 and 
 
,  has a proper subtree with root 
 for some 
.

Case 2. 
 and 
.

Since 
 
 and 
, we have 
 and(4.5.1)
 Next we distinguish different cases based on the last rule applied in .

Case 2.1. 
 
 or 
 
.

It is trivial.

Case 2.2. 
 
 with

Image 15
or 
 
 with
Image 16
.
Since 
, 
, 
 and 
 are stable, by the definition of  and that ⊤ and  are not stable, it is easy to see that 
. By (4.5.1), we have 
. Hence this case is impossible.

Case 2.3. 
 
.

We distinguish two cases based on α.

Case 2.3.1. .

By Lemma 4.4(1), there exists 
 such that 
 and 
 is one of premises of the root of .

Case 2.3.2. .

Similar to Case 2.2, we get 
 due to the stableness of 
. Then, by (4.5.1), we have 
. Since 
, by Theorem 2.4, 
 for some 
. Further, by 
 
, we get
 
 Moreover, by (4.5.1), it is easy to know that 
 and
 where . Hence, 
 for some 
 such that  has a proper subtree with root 
. Next we prove that 
.

If

Image 17
then it follows from
Image 18
and the assumption of this lemma that 
. So 
 for some 
 such that 
 
. Hence, 
.
In the following, we prove that

Image 17
. Contrarily, assume that 
. Then, 
 for some 
 such that 
 
. Further, it follows from the definition of  and Theorem 2.4 that 
. Moreover, by Lemma 4.1(2)(3), we get 
 and 
 
. Further, by 
 
 and Lemma 2.5, it is easy to see that, for any 
, we have 
 
 and  due to 
. Moreover, since 
, 
 and 
, by Lemma 4.4(2), for any proof tree 
 of 
, there exists 
 such that 
 has a subtree with root 
, a contradiction arises due to 
, as desired.
Case 2.4. 
 
.

Clearly, 
 for any 
, and 
 for any 
. Since 
 and 
 
, there exists 
 such that 
 and  has a proper subtree with root 
. □

In order to prove the main result of this section, we need a notion of up-to  
 
, which depends on an equivalent formulation of 
 provided by van Glabbeek [19]. Let us recall it.

Definition 4.6

[26]
A relation 
 is a ready simulation relation up to  
 
 whenever, for any  and ,

(Upto-1)
 implies 
 and 
 
 
;

(Upto-2)
 and  stable implies 
 and 
 
 
;

(Upto-3)
 and  stable implies .

Lemma 4.7

[26]
If  is a ready simulation relation up to  
 
, then 
.

We want to prove that 
 under the specific condition (the right of (WU) on page 10). As discussed on Pages 10-11, we generalize a case that if p performs several visible actions to reach a consistent stable state then  should perform the same sequence of visible actions to reach a corresponding consistent stable state to match, with the restriction that every consistent stable state that p can reach is not ready simulated by r. Moreover, the corresponding state (process) is of a specific form. By using Lemma 4.5, Lemma 4.7, we will prove the next lemma with the above spirit as a general result which is used in Lemma 4.9 with .

Lemma 4.8

Let  be processes such that 
. For any 
 and 
, if

Image 13
and 
 
 then 
 
 
 
.
Proof

Let It suffices to prove that  is a ready simulation relation up to  
 
. Let 
 
 
. Clearly,  
 
 
 and 
 are stable. We shall prove that (Upto-1), (Upto-2) and (Upto-3) hold one by one.

(Upto-1) Assume 
, we have  
 
 
 
 
 by Lemma 4.5, and
 
 
 
 
 
 
 Hence, (Upto-1) holds.

(Upto-3) Assume 
. Since 
 
, we have(4.8.1)
 Clearly, 
 
 
. Hence (Upto-3) holds.

(Upto-2) Assume 
. Since 
 
,
 
 Let
  where . Further, by (4.8.1), 
 
 
 
 
 We shall prove that there exists s such that 
 
 
 and 
 
 
 
 
 Clearly, by the assumption of this lemma, we have 
 or 
 for some . In the following, we distinguish two cases based on whether 
 or not.

Case 1. There exists  such that 
.

Since

Image 18
, we get 
. Then, since 
, there exists 
 such that 
 and 
 
. Hence 
. By Lemma 4.1(2), we obtain 
. Let
  where . Then, it is easy to see that 
 
 
 
 
 
 
 Next we shall show that all processes along this transition sequence are consistent. Since 
 
 and 
, 
. Then, by Lemma 4.1(3), 
 
. Further, since 
 
 and 
 
, by Lemma 2.5, we get
 
 
 
 Hence, by Lemma 4.5 and Theorem 2.4, 
 
 
 
 
 
 
 Clearly, 
 
 
 
 
 
 
 due to  
 
, as desired.
Case 2. There does not exist  such that 
.

In this case, 
. So there exists 
 such that 
 and 
 
. Thus, 
 
, and 
 by Lemma 4.1(2). Let
 
  where . Moreover, by Theorem 2.6, Lemma 4.1(4) and the associative and commutative laws of ∧ [28], we have 
 
 
 
 
 
 
 
 
 
 
 Next we shall show that all processes along this transition sequence are consistent. By Lemma 4.5, we get  
 
 
. So,  
 
 
. Moreover, by Lemma 4.5 and Theorem 2.4, 
 
 
 
 
 
 
 Clearly, 
 
 
 
 
, as desired. □

Here we prove the right implies the left of (WU) on Page 10.

Lemma 4.9

If 
 then 
.

Proof

Assume 
. Then we have 
 or 
. In order to prove 
, it suffices to prove that there exists t such that 
 and 
 
. Next we distinguish two cases based on whether 
 or not.

If 
 then 
 and 
 
 for some 
. Moreover, by Theorem 2.4, we have 
, as desired.

If

Image 20
then 
 and there exists 
 such that 
 and 
 
. Hence, by Lemma 4.8 with , 
 
 
. Further, by Theorem 2.4, we get 
 
, as desired. □
Finally, we prove the left implies the right of (WU) on Page 10. Note that the second conjunct in Lemma 4.10(1) is only there to prove the lemma by induction.

Lemma 4.10

If 
 and 
 with 
 then

(1)
either 
 and 
 
 for any ;

(2)
or 
 for some .

Proof

We prove the statement by induction on n.

For the inductive base 
, since 
, there exists t such that 
 
 and 
. Then, 
 or 
. For the former, we get 
, hence Item (2) holds. For the latter, since t is stable and , by Lemma 2.3 and Theorem 2.4, there exists 
 such that
 
 where 
 and for each , 
 with 
. Further, by Lemma 2.5, we have 
 
 and 
 
 for any . Hence, 
 for each  due to 
. Clearly, 
 
 for any  and 
. Hence, Item (1) holds.

For the inductive step, suppose 
. For 
, by IH, either 
 and 
 
 for any , or 
 for some . For the latter, Item (2) holds trivially. For the former, since 
 
 and 
, there exists s such that 
 
 and 
. Then, 
 or 
. The remainder argument is similar to the inductive base and omitted. □

Finally, by Lemma 4.9, Lemma 4.10, we get the main result of this section as follows.

Theorem 4.11

For any processes , 
 iff 
.

5. Correctness of the encoding
In the previous section, we presented some preliminary results concerning ⊤ and the weak until, which will help us to prove the main result of this paper: our encoding is correct, i.e., for any 
 and 
,
 We divide the whole proof into two parts. Firstly we prove the following.

Lemma 5.1

For any 
 and 
, if  then 
.

Proof

We proceed by induction on ϕ and distinguish different cases based on the form of ϕ.

Case 1. .

By Lemma 4.1(3), 
.

Case 2. .

Then . Hence 
.

Case 3. .

Let . Assume 
. Then we get 
. Clearly, 
 
 due to Lemma 4.1(3). Moreover, by Theorem 2.4, 
 due to the definition of . Hence 
.

Case 4. .

Similar to Case 3 and omitted.

Case 5. 
.

Let 
. Assume 
. Then, we get 
 or 
. By IH, 
 or 
. Thus 
. Hence 
.

Case 6. 
.

By using Lemma 2.5, the argument is similar to Case 5 and omitted.

Case 7. 
.

Let 
. By IH, we get 
. In the following, we prove 
. Assume 
. If 
 then 
 
 due to Lemma 4.1(3). If 
 then it is not difficult to prove 
 
 by setting 
 
 and proving that  is a stable ready simulation relation. Hence, 
 
. So 
. Therefore, 
, as desired.

Case 8. 
.

Assume 
. By IH, we get 
. Since 
, we get

Image 21
. Hence, by Lemma 4.9 and (AW) on Page 9, we obtain 
.
Case 9. 
.

Let 
. By IH, we have 
. Therefore, by Lemma 4.9, we get 
. □

Secondly we prove the following.

Lemma 5.2

For any 
 and 
, if 
 then .

Proof

We prove the statement by induction on ϕ, and distinguish different cases based on the form of ϕ.

Case 1. .

Clearly, .

Case 2. .

Let 
. Then, we have . Hence .

Case 3. .

Suppose that 
. Assume 
. Then 
 and 
 
 for some t. Thus, by the definition of , there exists  such that 
 and . It follows from 
 that 
. Then 
. Hence .

Case 4. .

Similar to Case 3 and omitted.

Case 5. 
.

Suppose 
. Assume 
. Since 
, we get 
. Then 
 or 
. Further, by IH, 
 or 
. Thus, 
.

Case 6. 
.

Suppose 
. Assume 
. Since 
, we have 
. Thus, by Lemma 2.5, we get 
 and 
. Further, by IH, 
 and 
. Hence, 
.

Case 7. 
.

Suppose 
. Assume 
. In order to prove 
, it suffices to prove that 
. Since 
, there exists 
 such that 
, 
 
 and 
 
. Clearly, by the definition of 
, 
 for some . Since 
 
 and 
, we get 
, 
 and . Then 
. Thus 
 due to 
 
. So, by IH, we have 
, as desired.

Case 8. 
.

Suppose 
. Assume 
. Since 
, by Lemma 4.10 and the fact that

Image 22
if  for any t, we get 
. Hence, by IH, we have 
. Therefore 
.
Case 9. 
.

Suppose 
. Assume 
. Since 
, by Lemma 4.10, 
 or 
 for some . Thus, by IH, 
 or 
 for some . Hence, 
. □

As an immediate consequence, by Lemma 5.1, Lemma 5.2, we obtain the main result of this paper.

Theorem 5.3

For any 
 and 
,  iff 
.

The theorem above shows that our encoding is correct, it is similar to Compatibility Theorem in [20], but in a term-based way. It also shows that  is the loosest one in the set of processes satisfying property ϕ, i.e., it can mimic any process that satisfies the property ϕ.

6. Conclusions and discussion
Lüttgen and Vogler's LLTS and graphical representation of UACTL in LLTS are entirely semantic. For many purposes, it is convenient to have a syntax. We pick up SOS rules as simple as possible to reflect their complex semantic constructions and provide a LLTS-oriented process calculus 
, which contains all properties a LLTS system should have. In this paper, we “graphically” represent UACTL in 
, where usual operational operators (prefix, external choice and parallel operators), logic connectives (conjunction and disjunction) and standard temporal operators (always and weak until) can be mixed freely without any restrictions, meanwhile compositional reasoning is also admitted by Theorem 2.6. In the same framework, it allows us to capture desired operational behavior and describe intended safety properties easily.

We want to mix process algebra and temporal logic in one framework and take advantage of both sides at the same time. On the one hand, we could describe system's behaviors, compositionally reason and refine specifications step by step; on the other hand, we could express system's abstract properties, such as temporal properties, easily. Among them, as we know, two main ones are safety and liveness properties. Since safety properties can be expressed in 
 easily, the open problem is to encode liveness in 
. To this end, it is reasonable to add “eventuality” in Lüttgen and Vogler's UACTL, and the satisfaction relation may be defined as◇
 where ◇ is the eventuality operator. We try to encode formulae with eventuality as◇ but fail to prove Theorem 5.3. Maybe, we need a kind of fairness assumptions in 
.

Boudol and Larsen's graphical representation [8] of HML is based on the Modal Transition Systems (MTSs) [17], [15]. Recently, based on the same foundation, Benes et al. graphically represent HML with greatest fixed point (νHML) [6]. Aceto et al. [1] study the graphical representation of covariant-contravariant modal formulae. Contrast to these works, our work is based on LLTS and safety could be easily be expressed in 
, while it could not be expressed in [8], [1]. Although safety could be expressed in νHML [16], the graphical representation in [6] is directly based on MTSs not syntactic.

As future work, the important one is to encode liveness in 
. Lüttgen and Vogler also discuss this issue in [20]. Hitherto, how to get a general theory of “graphical representation” is still unknown, the aforementioned works including ours are special cases.