Abstract
We identify a tradeoff curve between the number of wheels on a train car, and the amount of track that must be installed in order to ensure that the train car is supported by the track at all times. The goal is to build an elevated track that covers some large distance ℓ, but that consists primarily of gaps, so that the total amount of feet of train track that is actually installed is only a small fraction of ℓ. In order so that the train track can support the train at all points, the requirement is that as the train drives across the track, at least one set of wheels from the rear quarter and at least one set of wheels from the front quarter of the train must be touching the track at all times.

We show that, if a train car has n sets of wheels evenly spaced apart in its rear and n sets of wheels evenly spaced apart in its front, then it is possible to build a train track that supports the train car but uses only Θ(ℓ/n) feet of track. We then consider what happens if the wheels on the train car are not evenly spaced (and may even be configured adversarially). We show that for any configuration of the train car, with n wheels in each of the front and rear quarters of the car, it is possible to build a track that supports the car for distance ℓ and uses only O(ℓlog⁡nn) feet of track. Additionally, we show that there exist configurations of the train car for which this tradeoff curve is asymptotically optimal. Both the upper and lower bounds are achieved via applications of the probabilistic method.

The algorithms and lower bounds in this paper provide simple illustrative examples of many of the core techniques in probabilistic combinatorics and randomized algorithms. These include the probabilistic method with alterations, the use of McDiarmid's inequality within the probabilistic method, the algorithmic Lovász Local Lemma, the min-hash technique, and the method of conditional probabilities.

Keywords
Probabilistic method
Algorithms
TrainsLovász local lemma
McDiarmid's inequality

0. Introduction
A gap in the track. A few years ago, while traveling on a train, and on only a few hours of sleep, I was staring out the window. The train crossed a bridge over a road, and the ground was momentarily replaced by a steep drop. Startled, my sleep-deprived mind briefly wondered whether there was still a track underneath us. Of course there is, I thought to myself. Without a track, the train car would have fallen into the gap.

Ah, no so fast! responded the latent mathematician inside me. If the train car had more than two sets of wheels, then perhaps it could cross the bridgeless gap without falling in. It was true.

Consider, for example, a train with four sets of wheels: one set in the rear, one set in the front, and one set in each of the first and third quartiles.



Download : Download high-res image (32KB)
Download : Download full-size image
As long as the gap in the track is less than the distance between any pair of wheels, then at least three sets of wheels will touch track at all times. Assuming that the center of mass of the train is in the middle half of the train, it follows that the train does not fall into the gap.

In fact, continued the latent mathematician, what if we have n sets of wheels? Maybe we can build a mono-rail using an asymptotically small amount of track.

That's a stupid idea, responded I. Gaps in train track are not something to optimize.

But I was sleep deprived, so I did it anyway.

The basic observation: more wheels means less track. Consider a train car with 2n sets of wheels. Half the sets are evenly dispersed across the first quarter of the train car, and half the sets are evenly dispersed across the final quarter. The train can safely drive down the track as long as at least one set of wheels from each side of the train car is touching track at all times. The train car looks something like this:



Download : Download high-res image (56KB)
Download : Download full-size image
Want to build a monorail, but you're short on track? No problem! You can get away with filling in only an  fraction of the track:



Download : Download high-res image (52KB)
Download : Download full-size image
Every fourth of a train length, we place a piece of track whose length is a 
 
 fraction of the length of the train car. We get asymptotic cost savings!

To see that this is the best we can do, suppose that the fraction of track that is filled in is less than 
 
, and for symmetry sake suppose the track is circular (i.e., the end of the track loops back to the start). If we place the train at a random position in the circular track, then each wheel has a less than 
 
 chance of touching track. By a union bound, it follows that the probability of any wheel in the rear quarter of the train touching the track is less than 1. Thus no matter how the track is placed, if the total fraction of track that is filled in is less than 
 
, then there is some position at which the train falls through.

Paper outline. The rest of the paper considers the situation in which the wheels on the train car are placed unevenly (and possibly even adversarially!) in each of the front and rear quarters of the car. Section 1 describes the problem in more detail, and shows that for any configuration of the train car, with n wheels in each of the front and rear quarters of the car, it is possible to efficiently build a track that supports the car for distance ℓ and uses only 
 
 feet of track. Section 2 then establishes a matching lower bound, showing that there exist configurations of the train car for which 
 
 feet of track are required. Both the upper and lower bounds are achieved via applications of the probabilistic method.

The train-track problem serves as a veritable playground for applying many of the core techniques from probabilistic combinatorics and randomized algorithms to a simple and fun problem. Section 3 give three alternative algorithms for achieving the upper bound of 
 
, each of which builds on a different technique.

Combined, the algorithms and lower bounds in this paper give simple illustrative examples of the algorithmic Lovász Local Lemma, the min-hash technique, the method of conditional probabilities, the probabilistic method with alterations, and the use of McDiarmid's inequality within the probabilistic method.

1. Train cars with arbitrary wheel arrangements
Consider a train car that has n wheels in its rear quarter and n wheels in its front quarter, but suppose that the wheels aren't evenly spaced. For example, maybe the rear of the car looks something like this:



Download : Download high-res image (72KB)
Download : Download full-size image
Can we still fill in an asymptotically small fraction of the track in a way that will allow the train car to drive down the track? In other words, can we place down a small amount of track in a way so that, as the train drives down it there is always at least one wheel from each of the front and rear quarters of the train touching track? It turns out that, via a simple application of the probabilistic method with alterations, we can.

To formalize the situation, let's focus just on the first quarter of the train. (In particular, up to a constant factor in the amount of train track that we install, we can consider the two quarters of the train separately.) Suppose this portion of the train is f feet long, and assume that each of the n sets of wheels resides at a distinct integer offset from the rear of the train. Our goal is to build a train track that is ℓ feet long. We build the train track out of pillars that are each 1 foot long and are each placed at integer positions on the track. We are required to put down the pillars in a way so that, as the train drives down the track, at least one wheel from the rear quarter is always touching the track (i.e., touching some pillar). We want to use as little track as possible, with the best we could hope for being a total of 
 
 pillars.

As a reminder, there are three variables: the number of wheels n (in the quarter of the train car that we're considering), the length f of one quarter of the train car, and the length ℓ of the train track. In general, we have . Note that, although n and f could reasonably be close to one another (e.g., ), we also want to be able to handle cases where . This allows for the train car to be configured in truly strange ways – for example, the positions of the wheels could even form a geometric series:



Download : Download high-res image (63KB)
Download : Download full-size image
1.1. A randomized algorithm for building track
Our algorithm is a simple example of the probabilistic method with alterations. In particular, we begin by randomly constructing a track that uses only a small number of pillars, and then we show that this track can be slightly altered in order to support the rear of the train car at every point.

We begin by installing each pillar randomly with probability 
 
. Even though this strategy has nothing to do with the structure of where the wheels are on the train, it already does remarkably well. In particular, if we place the train at some given position on the track, then there are n different pillar positions that could potentially hold up the rear quarter. Each of these pillars positions has a 
 
 probability of having a pillar installed. It follows that, for a given position on the track, the rear quarter of the train has a
 
 probability of falling through the track. Taking advantage of the identity 
 
 
, which holds for any , it follows that the wheels fall through the track with probability at most,
 
 

In other words, out of all the places we can place the train on the track, only a 
 
-fraction of them will be problematic (in expectation). To fix this, we can just install one additional pillar to remedy each of these problematic positions. The result is a train track that fully supports the rear quarter of our train, and that uses only 
 
 total feet of track, in expectation.

Of course, this isn't quite as good as we did when the wheels were evenly spaced out (we are a roughly  factor worse). But it's still pretty amazing! No matter how the wheels are distributed across each quarter of the train car, we can get away with installing only a 
 
 fraction of the track!

The algorithm and analysis described above can be summarized in the following theorem:

Theorem 1.1

Consider a 4f-foot long train car, and suppose that the rear quarter of a train car contains n sets of wheels, each of which resides at a distinct integer distance from the rear of the car. In time , one can construct an ℓ-foot long train track with the following two properties: (1) as the train car drives down the track at least one set of wheels from the rear of the car is always touching track; and (2) the track consists of 1-foot pillars, with the total expected number of pillars being at most 
 
.

2. A matching lower bound
In this section we show that using 
 
 pillars is optimal for some configurations of the train car. We are again going to use the probabilistic method, but this time in a more sophisticated way.

We continue to focus only on the rear quarter of the train car, which is f feet long. We set , and we construct the rear quarter of the train car by placing n wheels at integer positions in the set . We will then consider a track of length , and show that  pillars are necessary in order to support the rear quarter of the train car at all positions on the track. Recall that each pillar is one foot wide and is placed at an integer offset on the track.

Let C be the set of wheel-positions in the rear quarter of the train car. We choose C by placing a wheel at each position in  independently with probability 
 
. This means that C has n wheels in expectation, but may not actually have exactly n wheels. The important thing to note is that, by symmetry, C has n or more wheels with at least 50% probability.

Now consider a track layout given by a subset T of , and satisfying 
 
. Whereas C is a random variable, T is a fixed set.

Define  to be the event that, for every possible position of the rear quarter of the train on the track, at least one wheel from the rear quarter of the train is supported by track. From the perspective of the train car,  is a good event. Formally,  occurs if for every offset , we have .12

The key to proving the desired lower bound is to show that the probability of  occurring is very small, namely that 
 
 
. Because T is a 
 
-element subset of , there are at most  
  possibilities for T. Taking a union bound over all of these possibilities implies that
 
 On the other hand, we know that the number of wheels  is less than n with probability at most 1/2. By a union bound, the probability that either  has fewer than n wheels or that  holds for some T is less than 1. It follows that there must exist a car C with n or more wheels for which no track T of size smaller than  satisfies . In fact, with slightly more careful bookkeeping, one can show that an even stronger property is true: almost all choices of how to place n wheels in C require a track of size larger than  to support the car.

To complete the lower bound, the challenge becomes to show that  is very small. That is, for a given choice of track T containing  pillars, the probability that T supports the rear-quarter of the train car C is small.

Rather than examine the event  directly, we instead examine a related quantity. Define  to be the number of positions  for which  (i.e., the number of positions in which the rear quarter of the car, given by C, falls through the track T).

The relationship between  and  is that  occurs only if . Our approach to completing the analysis will be to first show that  is relatively large, and then to show that the probability of  deviating substantially from its expected value is small. This, in turn, implies that  is small, completing the analysis. In other words, the problem of proving that there exists a train-car configuration requiring a large amount of track is reduced to the problem of proving a concentration inequality on the random variable .

For each position , the set  consists of  elements. Since each of these elements is contained in C with probability at most 1/2, the probability that C avoids all of the elements in  is at least
 
 
 Summing over the values of k, it follows that the expected number of positions k in which  satisfies
 

The final step of the analysis is to prove a concentration inequality on . Standard Chernoff bounds do not apply here because  is not a sum of independent indicator random variables. Instead, we employ a more powerful tool, namely McDiarmid's Inequality:

Theorem 2.1

McDiarmid '89 [8]
Let  be independent random variables over an arbitrary probability space. Let F be a function mapping  to , and suppose F satisfies, 
 
 
 
 for all . That is, if  are fixed, then the value of  can affect the value of  by at most R; this is known as the Lipschitz Condition. Then for all ,

To apply McDiarmid's Inequality to our situation, recall that  is defined to be the number of positions in the track T that the rear quarter of the car, given by C, falls through. Whereas the track T is fixed, each of the 2n possible wheels in C is picked with probability 1/2. Define the indicator random variables  so that  indicates whether . As required by McDiarmid's Inequality, the 's are independent of one-another, and  is a function of the 's.

Now we show that the Lipschitz condition holds with . Recall that the track T consists of only  pillars. Out of the 2n possible wheels i that C could contain, each of those wheels i is only relevant (to the car's stability) when the car is k feet down the track for some k that places wheel i on top of a pillar. Since there are only  pillars, each wheel i is only relevant to the train car's stability for  positions k on the track. In other words, for a given wheel position , there are only  values of  for which  can possibly contain i. As a result, each  can only affect the value of  by at most , meaning that the Lipschitz condition holds with .

Applying McDiarmid's Inequality with  and , we get that When n is large, this probability is much smaller than 
 
 
. It follows that 
 
 
. Summing over all possible options for the track T, the probability that any of them support the train car C is therefore less than 1/2. It follows that some train car C with n or more wheels fails to be supported by any track T consisting of  or fewer pillars. This completes the lower bound, and establishes the following theorem.

Theorem 2.2

There exists a set of wheel positions  such that , and such that in order for a track  to support the set of wheels at every position (meaning that  for each ) the track T must have size 
 
.

3. Three algorithms for building track
In this section, we revisit the problem of constructing a train track that uses 
 
 feet of track, and present three alternative algorithms for this problem, each of which gives the same guarantees as the algorithm in Section 1.

We continue to assume that the wheels of the train car are at integer positions, and we focus only on the n wheels in the rear quarter of the train car. We use C to denote the set of positions of wheels, meaning that C is an n-element subset of . Our goal is to construct a set of pillars  such that for each , the set  is non-empty. As was the case in Section 1, we want an algorithm that runs in expected time  and produces a set T with expected size 
 
.

Each of the three algorithms applies a different core technique from the overlap of probabilistic combinatorics and randomized algorithms:

•
A Deterministic Algorithm (Section 3.1). The first algorithm uses the method of conditional probabilities to derandomize the algorithm given in Section 1.

•
An Application of the Algorithmic Lovász Local Lemma (Section 3.2). The second algorithm uses the algorithmic version of the Lovász Local Lemma due to Moser and Tardos [9].

•
An Application of the Min-Hash Technique (Section 3.3). The final algorithm uses a variant of the min-hash technique, which has previously found important applications in locality sensitive hashing and string alignment [2], [3], [4], [6], [7], [10].

3.1. A deterministic algorithm
In this section, we use the method of conditional probabilities [1] in order to design a deterministic algorithm for the train-track problem.

The basic idea behind the method of conditional probabilities is as follows. Suppose  are independent real-valued random variables, and that  is an objective function that we wish to minimize. We are given that  for some value R, and we wish to find values of  for which . The method of conditional probabilities takes an iterative approach. Suppose we already have values of  such that Then there must exist some value  such that(1) The key challenge in applying the method of conditional probabilities is to design an objective function F that both captures the problem at hand, but that also allows for one to efficiently determine which value of  satisfies (1). This, in turn, allows for one to iteratively determine values for all of  such that .

In order to apply the method of conditional probabilities to the train-track problem, we define  to be zero-one random variables, each of which takes value 1 with probability 
 
. Given values  for random variables , we can construct a train track T by first setting , and then defining T to be  with one additional pillar for each position k in which the train wheels C fall through the track . Since our goal is to minimize the size of T, we define our objective function to be .

In Section 1, we showed that . Suppose that we have values  such that(2) Moreover, suppose that we maintain values  such that each  denotes the probability that the set of wheels  fall through the track . This means that we can compute  as(3)
 
  The first two terms represent , and the third term represents .

Given values of  such that (2) holds, we wish to find a value of  so that (2) will hold for . If we set , then this has the effect of increasing the expected initial size of  by 
 
, and of zeroing out any 's for which . On the other hand, if we set , then this has the effect of decreasing the expected initial size of  by 
 
 and replacing each  for which  with 
 
. It follows that in time , one can update (3) in order to determine  for each of the two possible values of . By selecting the value that minimizes the expected objective function, we can guarantee that
 
 Continuing like this, we can find values of  such that  in time . Using these  to construct the track T results in a track that uses at most  pillars, as desired.

3.2. An application of the algorithmic Lovász local lemma
Given a large collection of unlikely events , such that each event  is related to only a small number of other events , the Lovász Local Lemma is a technique for showing that there exists a way for all m events to mutually not occur. In one of its most basic forms, the Lovász Local Lemma can be stated as follows:

Theorem 3.1

Lovász and Erdös '73 [5]
Suppose  are independent random variables, possibly over different probability spaces. Let  be events such that each  is determined by some subset of the 's – that is, there exists an index set  such that  is determined by the outcome of the 's for which . Say that two events  and  depend on each other if . Let p be such that  for each i, and let d be such that each  depends on at most d different 's (including ). If , where e is the universal constant, then there is a positive probability that none of the events  occur.

The algorithmic version of the Lovász Local Lemma gives an efficient algorithm for constructing values of  in order to ensure that none of the events  occur.

Theorem 3.2

Moser and Tardos '10 [9]
Suppose that the conditions from Theorem 3.1 hold. Consider the following algorithm for choosing values of : First independently sample each of  from its defining probability distribution; then, as long as there exists at least one event  that holds, pick such an event  and resample the 's for each . Each time that the 's are resampled for some event , we call the resamplings a phase of the algorithm. The algorithm terminates once the 's have been assigned values that result in no events  occurring.

The above algorithm, known as the fix-it algorithm, terminates in finite expected time, and the expected number of phases is at most .

In order to apply the Algorithmic Lovász Local Lemma to our problem, we define  to be independent zero-one random variables, each taking value 1 with probability 
 
. Each  is the indicator variable for whether we include pillar i in the train track. We then define events  so that  is the event that the rear-quarter of the train car falls through the track at position i. That is,  occurs if .

Each event  depends on only n different 's, and each  is relevant to only n different 's. It follows that each event  depends on at most  other 's (including ). This means that we can apply the Algorithmic Lovász Local Lemma with .

In order for a given event  to occur, there are n different pillars that all must fail to appear in the track. The probability of this happening is
 
 
 

Using  and 
 
, we can apply the Lovász Local Lemma in order to conclude that there exists a choice of pillars  so that the wheels in C are supported along the entire track. This alone is not a useful observation since, of course, setting  all to 1 would trivially support the wheels in C at all points. On the other hand, if we apply the algorithmic version of the Lovász Local Lemma, then get an additional fact: that the fix-it algorithm terminates after only  phases in expectation.

Since each phase resamples only n different 's, the resamples contribute at most  pillars in expectation. On the other hand, the initial configuration of the 's contributes at most  pillars in expectation. It follows that, at the end of the fix-it algorithm, the resulting track configuration will use at most  pillars in expectation. A careful implementation of the fix-it algorithm will run in expected time , as desired.

3.3. An application of Min-Hash
Given a collection of sets , Min-Hashing is a technique for randomly sampling one element for each set . The technique works by first hashing each element s of each set S in  to a random real number . For each set , one then samples the element  with minimum hash .

The Min-Hashing technique plays important roles in both Locality Sensitive Hashing [2], [3], [7] and string-alignment algorithms [4], [6], [10]. The key property of Min-Hashing is that if two sets  are similar to one-another, then their min-hash is likely to be the same. And more generally, if an element s is the minimum-hashed element in one set , then s is likely to also be the minimum-hashed element in other sets.

In our application of Min-Hashing, we need not actually use hash functions. Instead, we assign random real numbers  to each of the ℓ possible track pillars. For each possible offset , define the set  to be the positions that the wheels in C take when the train car is k feet down the track. We construct a train track T by adding the pillar  for each set . That is, for each position that the train could sit in the track, we look at all possible pillars that could hold the rear-quarter of the train up, and we include in our track the pillar with the minimum assigned random value . We say that this pillar s is sampled from .

By construction, the set of pillars T is guaranteed to support the wheels C at every position. What is less clear is whether  will be small. Here is where we take advantage of the properties of Min-Hashing, and the fact that many of the sets  sample the same pillars as one another.

The key observation is that almost all of the pillars s that are sampled have small random values . Consider, in particular, the probability that for a given set , we sample a pillar s for which . This means that all n pillars in  were assigned random values larger than , which happens with probability at most,
 
 
 

It follows that, out of the  samplings that occur, the expected number of pillars s for which  that are sampled is at most . On the other hand, even if every pillar s for which  is sampled, the expected number of them is at most . The total number of sampled pillars, and thus the size of T, is therefore at most , in expectation. This completes the analysis of the algorithm.