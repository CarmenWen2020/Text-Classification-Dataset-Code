Abstract
We study problems with stochastic uncertainty information on intervals for which the precise value can be queried by paying a cost. The goal is to devise an adaptive decision tree to find a correct solution to the problem in consideration while minimizing the expected total query cost. We show that, for the sorting problem, such a decision tree can be found in polynomial time. For the problem of finding the data item with minimum value, we have some evidence for hardness. This contradicts intuition, since the minimum problem is easier both in the online setting with adversarial inputs and in the offline verification setting. However, the stochastic assumption can be leveraged to beat both deterministic and randomized approximation lower bounds for the online setting.

Keywords
Stochastic optimization
Query minimization
Sorting
Selection
Online algorithms

1. Introduction
Consider the problem of sorting n data items that are updated concurrently by different processes in a distributed system. Traditionally, one ensures that the data is strictly consistent, e.g., by assigning a master database that is queried by the other processes, or by running a distributed consensus algorithm. However, those operations are expensive, and we wonder if we could somehow avoid them. One different approach has been proposed for the TRAPP distributed database by Olston and Widom [40], and is outlined as follows. Every update is sent to the other processes asynchronously, and each process maintains an interval on which each data item may lie. Whenever the precise value is necessary, a query on the master database can be performed. Some computations (e.g., sorting) can be performed without knowing the precise value of all data items, so one question that arises is how to perform these while minimizing the total query cost. Another setting in which this type of problem arises is when market research is required to estimate the data input: a coarser estimation can be performed for a low cost, and more precise information can be obtained by spending more effort in research. The problem of sorting under such conditions, called the uncertainty sorting problem with query minimization, was recently studied by Halldórsson and de Lima [31].

The study of uncertainty problems with query minimization dates back to the seminal work of Kahan [33] and the TRAPP distributed database system by Olston and Widom [40], which dealt with simple problems such as computing the minimum and the sum of numerical data with uncertainty intervals. These results were later generalized for arbitrary query costs and precision levels by Khanna and Tan [35]. More recently, more sophisticated problems have been studied in this framework, such as geometric problems [12], [14], network discovery [6], shortest paths [21], [45], minimum spanning tree and minimum matroid base [17], [19], [20], [23], [38], [39], [45], linear programming [43], [37], and NP-hard problems such as the knapsack [25], scheduling [3], [16] and traveling salesman problems [45]. See [18] for a survey.

The literature describes two kinds of algorithms for this setting. Though the nomenclature varies, we adopt the following one. An adaptive algorithm may decide which queries to perform based on results from previous queries. An oblivious algorithm, however, must choose the whole set of queries to perform in advance; i.e., it must choose a set of queries that certainly allow the problem to be solved without any knowledge of the actual values.1

Two main approaches have been proposed to analyze both types of algorithms. In the first, an oblivious (adaptive) algorithm is compared to a hypothetical optimal oblivious (adaptive) strategy; this is the approach in [21], [33], [39], [40]. However, for more complex problems, and in particular for adaptive algorithms, it usually becomes more difficult to understand the optimal strategy. A second (more robust) approach is competitive analysis, which is a standardized metric for online optimization [10]. In this setting, both oblivious and adaptive algorithms are compared to an optimum query set, which we define next. For a given realization of the actual values, a set of queries is a feasible query set if, after querying all intervals in that set, one can find a solution of the underlying problem; an optimal query set is a feasible query set of minimum cost. An algorithm (either adaptive or oblivious) is α-query-competitive if its total query cost is at most α times the cost of an optimum query set. This type of analysis is performed in [6], [12], [19], [20], [30], [31], [33], [38], [45]. For NP-hard problems, since we do not expect to find the “correct” solution in polynomial time, there are two approaches in the literature: either we have an objective function which combines query and solution costs (this is how the scheduling problem is addressed in [16]), or we have a fixed query budget and the objective function is based only on the solution cost (as for the knapsack problem in [25]). Another related problem is that of, given a realization of the precise values, how to compute an optimum query set. This is often called the verification [6], [14], [17] or offline [31] version of the problem. This may be interesting both for performing experimental evaluation of the algorithms, as for obtaining insight for solving the corresponding oblivious/adaptive problems (as we discuss in Section 3).

Competitive analysis is, however, rather pessimistic. In particular, many problems such as minimum, sorting and spanning tree have a deterministic lower bound of 2 and a randomized lower bound of 1.5 for adaptive algorithms, and a simple 2-competitive deterministic adaptive algorithm, even if queries are allowed to return intervals [20], [30], [31], [38]. For the sorting problem, e.g., Halldórsson and de Lima [31] showed that there is essentially one structure preventing a deterministic adaptive algorithm from having a competitive ratio better than 2.

One natural alternative to competitive analysis is to assume stochastic inputs, i.e., that the precise value in each interval follows a known probability distribution, and we want to build a decision tree specifying a priority ordering for querying the intervals until the correct solution is found, so that the expected total query cost is minimized.2 In this paper, we study the sorting problem and the problem of identifying the data item with minimum value in this setting.

Some literature is devoted to a similar goal of this paper, but we argue that there are some essential differences. One first line of work consists of the stochastic probing problem [1], [24], [26], [27], [28], [29], [44], which is a general stochastic optimization problem with queries. Even though those works presented results for wide classes of constraints (such as matroid and submodular), they differ in two ways from our work. First, they assume that only elements that are queried can be in a solution of the underlying problem, or that the objective function is based on the expectation of the non-queried elements. Second, the objective function is either a combination of the solution and query costs, or there is a fixed budget for performing queries. Since most of these variants are NP-hard [24], some papers [27], [44] focused on devising approximation algorithms, while others [24], [28] on bounding the ratio between an oblivious algorithm and an optimal adaptive algorithm (the adaptive gap). Another very close work is that of Welz [45, Section 5.3] and Maehara and Yamaguchi [37], who, like us, assume that a solution may contain non-queried items. Welz presented results for the minimum spanning tree and traveling salesman problems, but under strong assumptions on the probability distributions. Maehara and Yamaguchi devised algorithms for a wide class of problems, that also yield improved approximation algorithms for some classical stochastic optimization problems. However, both works focus on obtaining approximate solutions for the underlying problem, while we wish to obtain an exact one, and they only give asymptotic bounds on the number of queries performed, but do not compare this to the expected cost of an optimum query set. To sum up, our work gives a better understanding on how the stochastic assumption differs from competitive analysis, since other assumptions are preserved and we use the same metric to analyze the algorithms: minimizing query cost while finding the correct solution.

Our results. We prove that, for the sorting problem with stochastic uncertainty, we can construct an adaptive decision tree with minimum expected query cost in polynomial time. We devise a dynamic programming algorithm which runs in time 
, where n is the number of uncertainty intervals and d is the clique number of the interval graph induced by the uncertainty intervals. We then discuss why simpler strategies fail, such as greedy algorithms using only local information, or relying on witness sets, which is a standard technique for solving query-minimization problems with adversarial inputs [12], [20]. We also discuss why we believe that the dynamic programming algorithm cannot be improved to 
.

Surprisingly, on the other hand, we present evidence that finding an adaptive decision tree with minimum expected query cost for the problem of finding the data item with minimum value is hard, although the adaptive online version (with adversarial inputs) and the offline (verification) version of the problem are rather simple. We prove that the (locally) optimal decision tree conditioned to first querying the leftmost interval can be computed easily, and that, in a (globally) optimal decision tree, the leftmost interval is queried first or last. This also implies that, for any subtree of an optimal decision tree, one branch can be calculated easily. The hard part of the problem occurs when the global optimum does not query the leftmost interval first, and the question becomes how to find the order in which the other intervals are considered in the “hard branch” of the decision tree. We discuss why various heuristics fail in this case. A simple approximation result with factor 
 for uniform query costs, where 
 is the degree of the leftmost interval in the interval graph, follows from the adaptive online version with adversarial inputs [33]. For arbitrary query costs, we show that the stochastic assumption can be used to beat both deterministic and randomized lower bounds for the adaptive online version with adversarial inputs.

Other related work. One interesting related problem was studied by van der Hoog et al. [32]: how to preprocess a set of intervals so that the actual numbers can be sorted efficiently after their precise values are revealed. Ajtai et al. [2] studied an uncertainty variant of the sorting problem in which a comparison between two values may be imprecise, and the goal is to minimize the number of comparisons to sort the values within a given precision. Braverman and Mossel [11] studied the problem of estimating the most probable ordering of a set of values in two models of uncertainty: (1) when comparisons are not reliable, and (2) when a permutation of the original ordering is sampled within a given probability distribution.

Our work falls into the wide area of stochastic optimization [9], and in particular multi-stage stochastic optimization [41]. Problems with uncertainty data described by intervals have also been studied under the framework of robust optimization [4], [34], [46]. A classical paper on multi-stage robust optimization is [15]. For surveys on robust optimization, see [7], [8].

Organization of the paper. Section 2 is devoted to the sorting problem with stochastic uncertainty, and Section 3 to the problem of finding the minimum data item. We conclude the paper with future research questions in Section 4.

2. Sorting
The problem is to sort n numbers 
 whose actual values are unknown. We are given n open intervals 
 such that 
. We can query interval 
 by paying a cost 
, and after that we know the value of 
. We want to find a permutation  such that 
 if  by performing a minimum-cost set of queries. We focus on adaptive algorithms, i.e., we can make decisions based on previous queries. We are interested in a stochastic variant of this problem in which 
 follows some known probability distribution on 
. The only constraints are that (1) values in different intervals have independent probabilities, (2) for any subinterval 
, we can calculate 
 in constant time,3 and (3) the known information on the probability distribution cannot be used to shorten the endpoints of any uncertainty interval.4 The goal is to devise a strategy (i.e., a decision tree) to query the intervals so that the expected query cost is minimized. More precisely, this decision tree must tell us which interval to query first and, depending on where its value falls, which interval to query second, and so on, until we have enough information to find the permutation π.

Definition 1

Two intervals 
 and 
 such that 
 and 
 are dependent. Two intervals that are not dependent are independent.

The following lemma and proposition are proved in [31]. The lemma tells us that we have to remove all dependencies in order to be able to sort the numbers.

Lemma 2

[31]
The relative order between two intervals can be decided without querying either of them if and only if they are independent.

Proposition 3

[31]
Let 
 and 
 be intervals with actual values 
 and 
. If 
 (and, in particular, when 
), then 
 is queried by every feasible query set.

Note that the dependency relation defines an interval graph, where we have a vertex for each interval, and two vertices are adjacent if the corresponding intervals intersect [36]. Proposition 3 implies that we can immediately query any interval containing another interval, hence we may assume a proper interval graph (that is, without nested interval pairs) [42]. We may also assume the graph is connected, since the problem is independent for each component, and that there are no single-point intervals, as they would give a non-proper or disconnected graph.

2.1. An optimal algorithm
We describe a dynamic programming algorithm to solve the sorting problem with stochastic uncertainty. Since we have a proper interval graph, we assume intervals are in the natural total order, with 
 and 
. We also pre-compute the regions 
 defined by the intervals, where . A region is the interval between two consecutive points in the set 
; we assume that the regions are ordered. We write 
 with 
, and we denote by 
 the indices of the intervals totally contained in 
 that contain 
. For simplicity we assume that, for any interval 
 and any region 
, 
; this is natural for continuous probability distributions, and for discrete distributions we may slightly perturb the distribution support so that this is enforced (we give more detail in Section 2.4). Since the dependency graph is a connected proper interval graph, we can also assume that each interval contains at least two regions.

Before explaining the recurrence, we first examine how Proposition 3 reduces the space of feasible query sets with an example. In Fig. 1(a), suppose we first decide to query 
 and its value falls in region 
. Due to Proposition 3, all intervals that contain 
, namely 
 and 
, have to be queried as well. In Fig. 1(b), we assume that 
 falls in 
 and 
 falls in 
. This forces us to query 
 but also implies that 
 can be left unqueried. Therefore, each time we approach a subproblem by first querying an interval 
 whose value falls in region 
, we are forced to query all other intervals that contain 
, and so on in a cascading fashion, until we end up with subproblems that are independent of current queried values. To find the best strategy, we must pick a first interval to query, and then recursively calculate the cost of the best strategy, depending on the region in which its value falls. Here, the proper interval graph can be leveraged by having the cascading procedure follow the natural order of the intervals.

Fig. 1
Download : Download high-res image (123KB)
Download : Download full-size image
Fig. 1. A simulation of the querying process for a fixed realization of the values. (a) Querying I3 first and assuming v3 ∈ S5. (b) Assuming v2 ∈ S3 and v4 ∈ S6.

We solve the problem by computing three tables. The first table, M, is indexed by two regions , and  is the minimum expected query cost for the subinstance defined by the intervals totally contained in 
. Thus, the expected query cost of an optimum decision tree for the whole problem is . To compute , we suppose the first interval in 
 that is queried by the optimum decision tree is 
. Then, for each region 
, when 
, we are forced to query every interval 
 with 
 and this cascades, forcing other intervals to be queried depending on where 
 falls. So we assume that, for all 
, 
 falls in the area defined by regions 
, with 
, and that this area is minimal (i.e., some point is in 
, and some point is in 
). We call this interval 
 the cascading area of 
 in 
. In Fig. 1(b), we have , , 
 and 
. As the dependency graph is a proper interval graph, the remaining intervals (which do not contain 
) are split in two independent parts, and we compute the resulting expected query cost using two tables, L and R, which we describe next. So the recurrence for  is
 
 
 
 
  
 
 
 
  
 
 
 
  
 
  
 
 
  
 
 
 
 
  where 
 is the probability that 
 is the cascading area of 
 in 
. We omit how to calculate this probability.

The definitions of L and R are symmetric, so we focus on L. For region indices , let 
 be the leftmost interval totally contained in 
. Now, 
 is the minimum expected query cost of solving the subproblem consisting of intervals 
, assuming that a previously queried point lies in the region 
. We ensure that 
 is the leftmost region in 
 that contains a queried point, so that we query all intervals that contain some point. For example, in Fig. 1(b), after querying 
, 
 and 
, the left subproblem has 
 and . It holds that L can be calculated in the following way. If no interval before 
 contains 
, then the cascading is finished and we can refer to table M for regions 
. Otherwise 
 must contain 
, we query it, and either 
 falls to the right of 
 and we proceed to the next interval, or 
 falls in a region 
 with 
, and we proceed to the next interval with the leftmost queried point now being in 
. Thus, we have
 
  We illustrate this in Fig. 2. In Fig. 2(a), the subproblem contains 
, and the leftmost queried point is in 
. Since 
, we query 
 and assume 
 falls in a region 
. In Fig. 2(b), we have that 
, so we recurse on 
; this will recurse on 
 in its turn, since 
. In Fig. 2(c), we have that 
, so we recurse on , which in its turn will have to query 
.

Fig. 2
Download : Download high-res image (128KB)
Download : Download full-size image
Fig. 2. An illustration of the definition of table L. (a) L[y,z′,j]. (b) If k ≥ z′, we recurse on L[y,z′,j − 1]. (c) If k < z′, we recurse on L[y,k,j − 1].

Analogously for table R, let 
 be the rightmost interval totally contained in 
. We want to find the best strategy for intervals 
, assuming that the rightmost queried point is contained in 
. Thus, we have
 
 

Observe that the definition of table L only depends on which is the leftmost region in the cascading area (and, symmetrically, R only depends on the rightmost region). Therefore, we can simplify the recurrence for table M to
  
 
 
 
 
 
  where 
 is the probability that 
 is the leftmost region in the cascading area of 
 in 
, and 
 is the probability that 
 is the rightmost region. We explain how to calculate these probabilities in Section 2.3.

At this point it is not hard to see that the next theorem follows by a standard optimal substructure argument. We present a proof for the sake of completion.

Theorem 4

The recurrence defined above for  correctly defines the minimum expected query cost to solve the stochastic sorting problem with uncertainty.

Proof

We prove that, for any , the value  is the expected query cost of a best decision tree for the subproblem defined by the intervals totally contained in 
. The proof is by induction on the number of intervals totally contained in 
. If it contains less than two intervals, then no query has to be done to solve this subproblem and the claim follows, so let us assume it contains at least two intervals.

Let us define more precisely how the decision tree for a subproblem is structured. Let  be a collection of intervals and queried points, with at least two dependent elements, and let  be a best decision tree for solving the subproblem defined by . The root of the tree indicates which interval 
 to query first. Then, for each region 
 contained in 
, the tree has a branch which is the decision tree for the remaining intervals, conditioned to the fact that 
; we can write this subtree as 
, for some 
. Note that, for any 
, the cost of 
 is the same, since 
 will be dependent to the same intervals, and the dependencies between other intervals do not change. The expected cost of the decision tree encoded by  is then 
. The leaves of the tree will correspond to collections of independent intervals, and will have query cost zero.

If a subtree  contains a queried value 
 and a non-queried interval 
 with 
, then Proposition 3 says that any feasible query set for  must query 
. This implies that 
 is queried in the path between the root and any leaf of the tree. Thus, it is easy to see that there is a decision tree for this subproblem with the same cost in which the first query is 
. If more than one interval contains a queried value 
, then we can query them before other intervals, and in any order, so we can actually query all of them at the same time, and have a root with branches for each combination of regions in which the values fall.

The algorithm starts by querying an interval 
 and, depending on the region 
 in which 
 falls, queries all intervals that contain 
. Since we have a proper interval graph, the remaining intervals are divided into two independent suproblems. Also, if the minimal area containing the regions in which the values fall is the same, then the cost of the subtree is the same, since the same intervals will contain a point queried at this time; this implies that each cascading area is a single disjoint event. Given a cascading area 
, the remaining problem consists of finding the best decision tree for two subproblems: one considering that the intervals to the left of 
 have not been queried and that the leftmost queried point is in 
, and another that the intervals to the right of 
 have not been queried and that the rightmost queried point is in 
. This is precisely the definition of tables L and R; thus, if the recurrences for tables L and R are correct, then the theorem follows by an optimal substructure argument.

So let us prove that the recurrence for table 
 is correct; the proof for table R is analogous. Let us recall the definition: 
 is the minimum expected cost of solving the subinstance of 
 (for some 
 consisting of intervals 
, where 
 is the leftmost interval totally contained in 
, assuming that the leftmost queried point is contained in 
. If , then 
 contains no interval and therefore 
 is zero. If 
, then no interval to the left of 
 is contained in 
, so 
 contains no interval and 
 is zero. If  and 
, but 
, then all intervals in 
 have a value to the right of 
, and thus any feasible query set to the intervals totally contained in 
 is feasible to complement the current decision tree. Thus, by an optimal substructure argument, 
. If , 
, and 
 contains 
, then Proposition 3 implies that 
 must be queried in any feasible query set of the subproblem, and thus can be the first interval queried in this subproblem. When querying 
, we ensure that the leftmost region with a queried point is updated correctly, so the last term in the recurrence for 
 is correct by an optimal substructure argument. □

The recurrences can be implemented in a bottom-up fashion in time 
: if we precompute the value of 
 and 
, then each entry of M is computed in time 
, and each entry of L and R can be computed in linear time. It is possible to precompute 
 and 
 in time 
, which we discuss in Section 2.3. A more careful analysis shows that the time consumption of the whole algorithm is 
, where d is the clique number of the interval graph. First, we show that each entry of M can be computed in time 
. Note that, in a proper interval graph, an interval contains at most  regions, so we have at most  choices of 
 for each 
. It holds that 
 contains at most d intervals, since every such interval contains 
. Moreover, for a given choice of 
, note that 
 cannot go further left than the leftmost region of the leftmost interval that contains 
, and since 
 we have at most  choices of 
; an analogous argument applies to 
. Now we argue that we only need to compute 
 if 
: the leftmost queried point cannot be to the right of 
, since we assume 
 was already queried, and cannot be to the left of 
, since we assume 
 is the leftmost queried interval. Moreover, it is clear that each entry of L can be computed in time . Analogous arguments apply to table R. Finally, in Section 2.3 we also refine the analysis to argue that the probabilities can be precomputed in time 
.

We now argue that an actual optimal decision tree can be constructed using polynomial time and space, if we represent it by a directed acyclic graph (DAG). First, we augment table M with an entry for the interval that is queried first in each subproblem, so we can track what is the best option. Then, we create one node in the DAG for each entry in tables . Each node will then have a polynomial number of children: for table M, we have a child for each choice of 
 and, for tables L and R, we have a child for each choice of k. Note that the same entry of the table is used by overlapping subproblems, so we can do the same for the nodes in the DAG. It is not hard to see that this construction can be done using at most as much time as for computing the original tables.

It seems difficult to improve this dynamic programming algorithm to something better than 
. Note that the main information that the decision tree encodes is which interval should be queried first in a given independent subproblem (and there are 
 such subproblems). We could hope to find an optimal substructure that would not need to test every interval as a first query, and that this information could somehow be inferred from smaller subproblems. However, consider 
, 
, and 
, with uniform query costs and uniform probability distributions. The optimum solution for the first two intervals is to first query 
, but the optimum solution for the whole instance is to start with 
. Thus, even though 
 is a suboptimal first query for the smaller subproblem, it is the optimal first query for the whole instance. This example could be adapted to a larger instance with more than d intervals, so that we need at least a linear pass in n to identify the best first query.

2.2. Simpler strategies that fail
It may seem that our dynamic programming strategy above is overly complex, and that a simpler algorithm may suffice to solve the problem. Below, we show sub-optimality of two such strategies.

We begin by showing that any greedy strategy that only takes into consideration local information (such as degree in the dependency graph or overlap area) fails. Consider a 5-path with intervals 
, where interval 
 has non-empty intersection with intervals 
 (if ) and 
 (if ). Let each interval have query cost 1 and an overlap of 1/3 with each of its neighbors, and the exact value be uniformly distributed in each interval. It can be shown by direct calculation that if we query 
 (or, equivalently, 
) first, then we have an expected query cost of at most 
, while querying 
 first yields an expected query cost of at least 
. However, a greedy strategy that only takes into consideration local information cannot distinguish between 
 and 
.

One technique that has been frequently applied in the literature of uncertainty problems with query minimization is the use of witness sets. A set of intervals W is a witness if a correct solution for the underlying problem cannot be computed unless at least one interval in W is queried, even if all other intervals not in W are queried. Witness sets are broadly adopted because they simplify the design of query-competitive adaptive algorithms. If, at every step, an algorithm queries disjoint witness sets of size at most α, then this algorithm is α-query-competitive. This concept was proposed in [12]. For the sorting problem, by Lemma 2, any pair of dependent intervals constitute a witness set. However, we cannot take advantage of witness sets for the stochastic version of the problem, even for uniform query costs and uniform probability distributions, and even if we take advantage of the proper interval order. Consider the following intervals: . The witness set consisting of the first two intervals may lead us to think that either of them is a good choice as the first query. However, the unique optimum solution first queries the third interval. (The costs are  if we first query the first interval,  if we first query the second interval, and  if we first query the third interval.)

2.3. Computing the probability of a cascading area
Let us discuss how to calculate 
, which is the probability that, given that 
, it holds that 
 for each 
, and some 
 has 
. (The arguments are symmetric for 
, so we omit them.) Note that, considering the definition of table M, we can assume that 
; otherwise we don't need to compute 
. We will also assume that 
, and the remaining case can be computed similarly.

Let us fix the values , and denote 
, 
, and 
. Let us further denote by 
 the probability that each  has 
. Note that 
, and 
 can be computed in linear time. Let
 
 denote the similar probability defined for the set of intervals 
, instead of , with 
.

Let 
 be the event corresponding to 
, and 
 be the event corresponding to 
. Then 
, and 
 because 
. Thus, we have that
 
 
 Note that all divisions in the equation are defined, because we assumed that 
 and 
. Since 
 does not depend on i, for each fixed , it can be computed in linear time, so all 
 can be computed for all  in time 
. Given this precomputation, all values 
 for all 
 can be precomputed in 
 time.

We can further improve the runtime to 
 as follows. Here, we fix 
 and , and compute 
 for various values of x, so let us now denote 
. Recall that 
, and note that each factor in this product is non-zero due to assumption (3) at the beginning of Section 2. We compute 
 sequentially from 
 to 
⁎
, where 
⁎
 is the rightmost region such that there is an interval that contains both 
 and 
⁎
. Given 
 for some x, 
 is computed by removing from the product intervals in 
 and including intervals in 
. The remainder of the product is reused. Observe that, during this computation, each interval that contains a region in 
⁎
 is included in (and removed from) the product exactly once. Therefore, the computation corresponding to fixed values of 
 can be done in time proportional to the number of regions x between 
 and 
⁎
 plus the number of intervals that contain some region in 
⁎
, that is, in  time. Thus, the computation of the whole table takes 
 time.

Finally, the preprocessing time can be further reduced (still having constant time computation of 
 after preprocessing), by decomposing the table for q into two parts, based on the following observation. The first table is indexed by 
, while the second one by 
. Consider the value 
. This is a product of probabilities ranging over intervals 
. The observation is that 
, where 
 is the set of intervals that contain both 
 and 
 and do not end at 
. Thus,
 
 if the denominator is non-zero, and otherwise 
, where 
 is defined similarly as 
, except it ranges over 
. The subtables 
 and 
 can be computed in 
 time using the observations in the previous paragraph.

To refine the analysis, recall that each interval in a proper interval graph contains at most  regions. Since we only consider pairs 
 such that there is an interval that contains both 
 and 
, we have at most  choices of x for each choice of 
. Thus, we only need time  to compute 
 for all values of x and fixed 
, and the whole preprocessing can be done in time 
.

2.4. Allowing arbitrary probabilities on interval endpoints
In this section we discuss how to remove the assumption that 
 for every interval 
 and every region 
. Remember that we assume that all intervals are open. Suppose that 
 for some interval 
 and some region 
 with . (Note that 
 if , and 
 as all intervals are open.) We consider three cases.

1.
If there is some interval 
 with 
 but no interval 
 with 
, then we can simply move the probability 
 to 
, for some 
. Note that we did not need to query 
 if originally 
; we could simply make . On the other hand, all intervals that originally contained 
 will contain 
, so they will be queried if originally 
.

2.
We can do a symmetric operation if there is some interval 
 with 
 but no interval 
 with 
.

3.
If there is an interval 
 with 
 and an interval 
 with 
, then we can “shift” the whole ray 
 to the right by 2ε, for some . More formally, any interval 
 with 
 will have a new left endpoint 
; any interval 
 with 
 will have a new right endpoint 
; for any 
, we will move the probability 
 to 
. Then we can move the probability 
 to 
; we did not need to query 
 or 
 if originally 
 (we could simply make 
), and all intervals that originally contained 
 will also contain 
 (so they will be queried if originally 
).

Note that the last transformation creates new regions, but the total number of regions will be at most twice the original, so the time consumption does not increase asymptotically.
Moreover, the algorithm can be modified to allow closed and half-open intervals, and thus discrete distributions with finite support. We only need to be careful when 
 for some queried interval 
 and some region 
: we do not need to query all intervals that contain 
, but only intervals 
 with 
.

3. Finding the minimum
We also consider the problem of finding the minimum (or, equivalently, the maximum) of n unknown values 
. Note that we do not need to know the precise minimum value, but just the data item that contains it; therefore, in some cases it is not necessary to query the corresponding interval. We assume that all intervals are open,5 and that they are sorted by the left endpoint, i.e., 
. Regarding the probability distributions, we also assume constraints (1)–(3) described at the beginning of Section 2. Let 
.

We begin by discussing some assumptions we can make. First, we can assume that the interval graph is a clique: with two independent intervals, we can remove the one on the right. (However, we cannot assume a proper interval graph, as we did for sorting: the argument that one of the nested intervals has to be queried no longer holds, since here we are only interested in finding the minimum rather than the total order.) The last assumption is that 
 does not contain another interval; it is based on the following proposition and implies that 
. Note that it also implies that 
.

Proposition 5

If 
 contains some 
, then 
 is queried in every feasible query set.

Proof

Suppose by contradiction that there is a feasible query set Q that does not contain 
. Since 
, we must have 
 and 
 for every 
, otherwise we cannot decide if 
 is the minimum. However, assuming both 
 and 
 is a contradiction. □

It is also useful to understand how to find an optimum query set, i.e., to solve the verification problem assuming we know 
.

Lemma 6

The optimum query set either

(a)
queries interval 
 with minimum 
 and each interval 
 with 
; or

(b)
queries all intervals except for 
, if 
 is the minimum, 
 for all , and this is better than option (a).

Option (b) can be better not only due to a particular non-uniform query cost configuration, but also with uniform query costs, when 
. Note also that 
 is always queried in option (a).
Proof

First let us consider the case when 
 is the minimum. If 
 for some , then 
 has to be queried even if all other intervals have already been queried, due to Proposition 5. Thus, the only situation in which 
 may not be queried is when 
 for all , and in this case clearly we have to query all other intervals, since otherwise we cannot decide who is the minimum.

Now we prove that, if 
 is the minimum with , then 
 must be queried. Since 
 is the minimum, all other values fall to the right of 
. In particular, 
, since 
 has minimum 
. Thus, even if all other intervals have already been queried, 
 must be queried due to Proposition 5.

It remains to prove that, for any , if 
 is minimum and 
 is queried, then all intervals with 
 must also be queried; we actually prove that 
 must be queried, by induction on j. The base case is , and if  the claim follows from Proposition 5, since 
 and 
. So assume ; note that 
 thus, by induction hypothesis, 
 must be queried. Since 
 is the minimum, 
, for . Therefore, after 
 are queried, 
 is the leftmost interval, and must be queried due to Proposition 5, since it contains 
. □

We first discuss what happens if the first interval we query is 
. In Fig. 3(a), we suppose that 
. This makes 
 become the leftmost interval, so it must be queried, since it contains 
. At this point we also know that we do not need to query 
, since 
. After querying 
, we have two possibilities. In Fig. 3(b), we suppose that 
, so we already know that 
 is the minimum and no other queries are necessary. In Fig. 3(c), we suppose that 
, so we still need to query 
 to decide if 
 or 
 is the minimum. Note that, once 
 has been queried, we do not have to guess which interval to query next, since any interval that becomes the leftmost interval will either contain 
 or will be to the right of 
. Since this is an easy case of the problem, we formalize how to solve it. The following claim is clear: if we have already queried 
 and 
, then we have to query 
. (This relies on 
 having minimum 
 among 
.) If we decide to first query 
, then we are discarding option (b) in the offline solution, so all intervals containing the minimum value must be queried. The expected query cost is then 
. Given an interval 
, it will not need to be queried if there is some 
 with 
, thus the former probability is the probability that no value lies to the left of 
. Since the probability distribution is independent for each interval, the expected query cost will be 
. This can be computed in 
 time.

Fig. 3
Download : Download high-res image (114KB)
Download : Download full-size image
Fig. 3. A simulation of the querying process when we decide to query I1 first. (a) If v1 ∈ S3, I2 must be queried, but not I4. (b) If v2 ∈ S2, then v2 is the minimum. (c) If v2 ∈ S6, then we still have to query I3.

Now let us consider what happens if an optimum decision tree does not start by querying 
, but by querying some 
 with . When we query 
, we have two cases: (1) if 
 falls in 
, then we have to query 
 and proceed as discussed above, querying 
 if 
, then querying 
 if 
 and so on; (2) if 
, then 
 falls to the right of 
, for all , so essentially the problem consists of finding the optimum decision tree for the remaining intervals, and this value will be independent of 
. Therefore, the cost of querying 
 first is
 
 
 Thus, we can see that a decision tree can be specified simply by a permutation of the intervals, since the last term in the last equation is fixed. More precisely, let  be a permutation of the intervals, where  means that 
 is the k-th interval in the permutation. We have two types of subtrees. Given a subset 
 that contains 1, let 
 be the tree obtained by first querying 
, then querying the next leftmost interval in 
 if it contains 
 and so on. The second type of subtree 
 is defined by a suffix  of the permutation. If , then 
 is a decision tree with a root querying 
 and two branches. One branch, with probability 
, consists of 
; the other branch, with probability 
, consists of 
. If , then 
, unless , in which case 
 will be empty: 
 does not need to be queried, because all other intervals have already been queried and their values fall to the right of 
. We have that 
  Note that in the last case we need to condition 
 to the fact that 
. We have that
 
 
 
 and
 
 
 

It holds that, if 
 is not the last interval in a decision tree permutation, then it is always better to move 
 one step towards the beginning of the permutation. This fact is formalized in the following lemma.

Lemma 7

Given a decision tree permutation 
 of a subset S, with  and 
, it costs at least as much as the cost of 
.

Proof

Let  be the indicator variable of the event A, i.e.,  if A is true, and zero otherwise. The cost of 
 is
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 where the first equality holds since the probability distributions are independent, so 
 unless . The second equality holds since 
 for all i, so 
. The third equality holds since 
 and by Bayes' theorem.

On the other hand, if we swap 
 and 
, then the cost is 
 
 
 
 
 
 
 
 since 
, and the last value is precisely the cost of the former permutation. □

This lemma implies, by induction, that the optimum decision tree either first queries 
, or has 
 at the end of the permutation. If 
 is the last interval in the permutation, then it does not have to be queried if all other values fall to its right. Thus it may be that, in expectation, having 
 as the last interval is optimal.

We do not know, however, how to efficiently find the best permutation ending in 
. Simply considering which interval begins or ends first, or ordering by 
 is not enough. To see this, consider the following two instances with uniform costs and uniform probabilities. In the first, 
, 
 and 
; the best permutation is 
 and has cost 2.594689. If we just extend 
 a bit to the right, making 
, then the best permutation is 
, whose cost is 2.550467.

If there was a way to determine the relative order in the best permutation between two intervals 
, simply by comparing some value not depending on the order of the remaining intervals (for example, by comparing the cost of 
 and 
), then we could find the best permutation easily. Unfortunately, the ordering of the permutations is not always consistent, i.e., given a permutation, consider what happens if we swap 
 and 
: it is not always best to have 
 before 
, or 
 before 
. Consider intervals 
, 
, 
, and 
, with uniform query cost and uniform probability distributions. The best permutation is 
, and the costs of the permutations ending in 
 are as follows. Note that it is sometimes better that 
 comes before 
, and sometimes the opposite. This issue also seems to preclude greedy and dynamic programming algorithms from succeeding. It seems that it is not possible to find an optimal substructure, since the ordering is not always consistent among subproblems and the whole problem. We have implemented various heuristics and performed experiments on random instances, and could always find instances in which the optimum was missed, even for uniform query costs and uniform probabilities.

Another reason to expect hardness is that the following similar problem is NP-hard [24]. Given stochastic uncertainty intervals 
, costs 
, and a query budget C, find a set  with  that minimizes 
.

Note that the decision version of our problem (whether there is a decision tree with expected cost ≤α) is in NP as we can represent a decision tree using linear space (by a permutation of the intervals) and compute its cost in polynomial time.

3.1. Approximation algorithms
Good approximation algorithms have been proposed for the adaptive online version with adversarial inputs [33]. If query costs are uniform, then first querying 
 costs at most , which yields a factor 
, where 
 is the degree of 
 in the interval graph. For arbitrary costs, there is a randomized 1.5-approximation algorithm using weighted probabilities in the two strategies stated in Lemma 6. Those results apply to the stochastic version of the problem simply by linearity of expectation.

Theorem 8

The stochastic minimum problem with uncertainty admits a 
-approximation for uniform query costs, and a randomized 1.5-approximation for arbitrary costs.

Proof

For any path between the root and a leaf in the decision tree, the result holds in the worst case, so it holds in expectation. □

Those results have matching lower bounds for the adaptive online setting with adversarial inputs, and for arbitrary query costs there is a deterministic lower bound of 2. We show that the stochastic assumption can be used to beat those lower bounds for arbitrary costs. First, the randomized 1.5-approximation algorithm can be derandomized, simply by choosing which strategy has smaller expected query cost: either first querying 
, or first querying all other intervals and if necessary querying 
. We know how to calculate both expected query costs in time 
; the latter is 
.

Theorem 9

There is a deterministic 1.5-approximation algorithm for the stochastic minimum problem with uncertainty with arbitrary query costs.

Proof

Let  be the set of realizations of the values, and assume  is finite. (Otherwise finiteness can be attained by grouping realizations into equivalence classes based on the partition into regions.) For each , let 
 be the random variable denoting the cost of first querying 
, then querying 
 if 
, and so on, and let 
. We partition  in sets 
 and 
, where 
 if 
, and 
 if ; note that Lemma 6 guarantees that this is indeed a partition.

Let 
 and 
. If 
⁎
 is the expected query cost of the best permutation for the stochastic problem, then
⁎
  
 
 
 where the inequality holds by bounding 
⁎
 via a “fractional” decision tree.

Now consider the solution for the stochastic problem obtained by the algorithm. Let 
 be the cost of first querying 
, and let 
 be the cost of first querying all other intervals. Then
 
⁎
 
 where the second inequality holds because 
 and 
 for any V. On the other hand,
⁎
 The expected query cost of the algorithm is
⁎
 
⁎
 
⁎
⁎
 
 
⁎
 where the second and third inequalities hold by the properties of the geometric mean. □

Ahead in Theorem 11 we propose a more refined algorithm, that we analyze according to the expected approximation ratio. In the proof above, instead, we bounded the ratio of the expected query cost and the expected optimum query cost (the latter corresponding to the fractional decision tree we use as lower bound). Since there is no fixed relation between  and  for random variables , we cannot compare the results of Theorem 9, Theorem 11. To circumvent that, we prove the following proposition. The example used in the proof was proposed for a similar result in [5], regarding the ratio of the expected query cost and the expected optimum query cost.

Proposition 10

For every , there is an instance for which the algorithm of Theorem 9 has expected approximation ratio .

Proof

Consider three intervals 
, with 
. Let the query costs be 
 and 
, with 
; note that . Let the probability distributions be the following:

Pr[v1 ≤ ℓ2]=ε,	Pr[ℓ2 < v1 ≤ ℓ3]=0,	Pr[v1 > ℓ3]=1 − ε;	
Pr[v2 ≤ ℓ3]=1/2,	Pr[ℓ3 < v2 < r1]=0,	Pr[v2 ≥ r1]=1/2;
Pr[v3 < r1]=ε,	Pr[v3 ≥ r1]=1 − ε.
The algorithm will either query 
 and cascade, with expected query cost
 
 
 
 (the algorithm queries 
, then 
 if 
, then 
 if 
), or query 
, and then 
 if necessary, with expected query cost
 
 
 
 Since the first option is slightly better, the algorithm will choose that strategy. Thus, the expected approximation ratio is
 
 
 
 
 
 
 
 
 
 
 □
Next we describe a more refined deterministic algorithm, which attains expected approximation ratio at most 1.4507; a pseudocode is given in Algorithm 1. The idea is, in some sense, to try to decide between first querying 
 or 
 (as the algorithm above), but in a more cautious way, taking into consideration the relationship between the query costs and the probability distributions. The algorithm has six parameters 
, which are described in the analysis and are functions of the input.

Algorithm 1
Download : Download high-res image (131KB)
Download : Download full-size image
Algorithm 1. Refined approximation algorithm for the stochastic minimum problem with uncertainty.

Let 
. For a set , let 
. We say that a set  hits 
 if, for some interval 
, it holds that 
; note that 
.

First, we try to find a set  such that  and 
, for some . If we manage to find such G, then we use the parameters 
, defined later in the analysis, to decide between two strategies. If 
, then we query 
 and cascade. Otherwise, we first query G, and if it hits 
 then we query 
 and cascade. If G does not hit 
, then we query the remaining of R, and query 
 if necessary. The high-level reason why it is better to query G before querying all of R is the following: if R hits 
, then there is a good probability (at least 1/4) that G also hits 
, but the cost of G is bounded by 3/4 times the cost of R.

Now we describe how to find G. If some interval 
 has cost at least  and the probability that 
 hits 
 is at least 
, then we clearly can take 
. If every interval in R has cost less than , then there is a set 
 with 
 (it can be computed greedily). In this case, let 
; if 
 hits 
 with probability at least 
, then we can take 
 and . Otherwise, the complement 
 has cost  and probability of hitting 
 So we let 
 and . Note that  and , so .

If we do not manage to find the set G, then note that there is an interval 
 with 
, but the probability of 
 hitting 
 is less than 
. We have two strategies, depending on whether 
:

1.
If 
, then we make a deterministic choice between querying 
 and cascading, or querying R and then 
 if necessary. That choice is made with the parameters 
, which are defined later in the analysis. In this case, we show that there is a high chance that 
 is part of every feasible query set, and it comprises most of the cost of the instance, so the expected query cost is not far from the expected optimum.

2.
If 
, then we first query 
. If 
 hits 
, then we query 
, and then 
 if necessary. Otherwise, we use the parameters 
 to make a deterministic choice between querying 
 or 
, then querying the other if necessary. The idea here is that the cost of 
 is not relevant compared to 
 and 
, so we can query 
 immediately, and the main question becomes whether to query 
 or 
.

We now analyze the expected approximation factor of this algorithm.

Theorem 11

Algorithm 1 has expected approximation factor at most 1.4507 for the stochastic minimum problem with uncertainty with arbitrary query costs.

Proof

Throughout the proof, we denote by 
 the random variable consisting of the cost of querying 
 and cascading. Let k be the maximum index such that 
, and let 
 (cf. Proposition 5). (Note that  because 
.) Note that 
: this is because, if we assume that all the values in R are to the right of 
, then every interval 
 with 
 must be queried. We also point out that 
 is independent from the choices of the values of the intervals in R. To simplify notation, let 
.

We divide the analysis in three cases: (1) when , in which case the algorithm runs Lines 11–16, (2) when  and 
, corresponding to Lines 19–20, and (3) when  but 
, in which case we run Lines 22–27.

Case 1, . We have  and 
, with . We divide the analysis in two cases, depending on the behavior of the algorithm.

First, consider the case that the algorithm queries 
 and cascades. If R hits 
, which happens with probability 
, then the optimal strategy is to query 
 and cascade, as discussed in Section 3, so we are optimal. Otherwise, the optimum cost is 
, while the algorithm pays 
. In this case, the approximation ratio is 1 if 
 (which happens with probability 
); otherwise (with probability 
), the ratio is at most 
. Thus, the expected approximation ratio is at most(1)
 
 

Next, suppose we first query G. Recall that, with probability 
, R hits 
 and it is optimal to query 
 and cascade. Let 
. If G hits 
, then an upper bound on the query cost of the algorithm is 
, while the optimum is 
. Since 
, the approximation ratio in this case is 
. Otherwise, in the worst case the algorithm pays 
, while again, the optimum is at least 
, so the approximation ratio is at most 
. Thus, the expected approximation ratio conditioned to R hitting 
 is at most
 
 
 
 
 
 where the first inequality holds because  (hence ), and the last inequality holds because the maximum for 
 in the interval  is 13/16.

If R does not hit 
, which happens with probability 
, the optimum cost is 
, while the algorithm pays . Conditioned to this event, the expected approximation ratio is at most 
, by conditioning whether 
 or not.

Altogether, the expected approximation ratio when the algorithm first queries G is at most(2)
 
 
 
 
 where we denote 
.

By definition, the expected approximation ratio of the algorithm is at most 
. Our goal now is to show that this minimum is upper-bounded by the claimed approximation factor.

First, consider the case when ; then, both 
 and 
 are increasing in 
, so we may let 
. Then the functions to bound are 
 and 
. If 
, then 
, so assume 
. In this case, for every fixed 
, it holds that 
 is a decreasing function of z, while 
 is an increasing function of z. Thus, for every 
, the z that maximizes 
 is given by 
, that is, 
 
. Plugging that value of z back into 
, we obtain 
 
, which is maximized at 
, giving us the bound .

Now assume . Note that we may assume , as otherwise 
. Under such assumption, the coefficient of 
 in (2) is negative, which means that for every fixed 
 and z, 
 is a decreasing function of 
, while 
 is an increasing function of 
. Thus, for each z and 
, 
 is maximized when 
. Solving the last equation for 
 (ignoring the restriction 
), we get
 
 The common value of 
 and 
 for such 
, plugging the expression above in (1), is then
 
 
 where the first inequality holds by letting 
, which we can do since  implies that the left hand side is an increasing function of 
. The second inequality is obtained by minimizing the denominator by setting its derivative to 0: 
, . Concluding, we see that the expected approximation ratio in Case 1 is always bounded by .

Case 2, , 
. In this case we make a deterministic choice between two options: either query 
 and cascade, or query R and then query 
 only when R hits 
. The choice is based on estimates 
 of the expected approximation ratio we get in either option.

First, consider the case when we query 
 and cascade. With probability 
, the optimum is to query 
 and cascade as discussed in Section 3. Assuming R does not hit 
, with probability 
 again the optimum is to query 
 and cascade. In the remaining case, we have approximation ratio at most 
, as the optimum only queries R. Putting these cases together, we get approximation ratio at most(3)
 
 

Next, let us estimate the approximation ratio when we choose to first query R. We divide in several cases to organize the argument.

1.
With probability 
, R does not hit 
. Conditioned on this event, with probability 
, the optimum queries R only, otherwise the optimum costs 
. Thus, we have approximation ratio at most
 

2.
With probability 
, R hits 
, so the optimum is to query 
 and cascade, and the algorithm will query all the intervals. Remember that R contains an element 
 with 
, but
 Thus(4)
 Let 
. Again, note that 
 is independent from the choices in R. Conditioned on the event when R hits 
, we have the following cases.

(a)
With probability 
, we have that 
. Conditioned to this event, we have two subcases.

i.
Since we are conditioned to R hitting 
, by Equation (4) we have probability at least 3/4 that 
 does not hit 
 but 
. Therefore, querying 
 must be part of any feasible query set, since 
 and every interval 
 has 
 (including those with 
). So we have approximation ratio at most
 
 

ii.
With probability at most 1/4, we use the bound of 
.

Since 
 
 
, we have that the ratio conditioned to 
 is at most
 
 
 
 
(b)
With probability 
, we simply bound the ratio by 
.

Thus we have that the ratio conditioned to R hitting 
 is at most
 
 
 
 We then claim that 
. Suppose by contradiction that 
, then we have that . By the definition of k, it holds that
 a contradiction since we assume that 
. Thus, since 
 
 
, the approximation ratio conditioned to R hitting 
 is at most
 
 
 
Summing up, the expected approximation ratio assuming that we first query R is at most(5)
 
 
 
 
 
 
 
 
 
 
 
 where 
.
Thus, our goal is to bound from above the minimum of the last expression and (3). Consider their difference:
 
 
 
 Note that 
, so the difference is non-positive whenever the last three terms have a non-positive sum, that is, 
 
. That is, for all such values of 
, (3) is the smaller one. It is then maximized when 
 and 
 
, in which case it equals
 
 Now, it can be shown using standard univariate optimization tools that this function is always bounded by 1.289 in .

Thus, we can assume that 
 
. Note that (3) is an increasing function of 
, while (5) is a non-increasing function of 
, since the coefficient
 
 
 
 
 of 
 is non-positive, for all  and 
. Hence, the minimum of (3) and (5) is maximized when they are equal. Thus, we equate them and solve for 
, to find
 
 Note that 
, due to our assumption on 
. Plugging back into (3), we get(6)
 
 
(7)
 
 
 where 
, and 
 (for all ). Note also that  holds for all . We can rule out , as follows. Assuming , our bound on 
 implies 
. Let us set 
 in the first parentheses in (7). Then we obtain a decreasing function of 
, since , which is maximized for 
, giving us the following function of z: 
 
. The latter is easily checked to be less than 1.44 for all .

Thus, we assume that . It can be checked that the value of B in this interval is bounded by 0.353. We replace  and 
 in the denominator of (6), to get the function
 
 We optimize this for 
. If , the optimum is 
, giving us the function 
 
, while for , the optimal value is 
, giving us the function 
 
. Both functions can rather easily be checked to be bounded by 1.41, for .

Case 3, , 
. To simplify notation, let us write 
. Remember that 
; this implies 
, so 
 and 
.

First let us consider the case that 
 hits 
, so the optimum strategy is to query 
 and cascade. If 
 is not queried, then the approximation ratio is at most
 
 
 If 
 is queried in the cascading, then it must be part of every feasible query set, so the approximation ratio is at most
 
 

Now let us assume that 
 does not hit 
. Let 
 and 
; note that those are independent events.

First, consider the situation in which the algorithm queries 
 before 
. With probability 
, the algorithm also queries 
; if 
 (which happens with probability 
), then both 
 and 
 need to be queried to decide which is the minimum, otherwise the optimum strategy is to query R only, as 
 but 
. With probability 
, the algorithm does not query 
, and the optimum strategy is to query 
 and cascade if 
 or 
. Thus, the approximation ratio is at most(8)
 
 
 
 
 
 
 
 
 where 
.

Now consider the case when the algorithm queries 
 before 
. With probability 
, the optimum strategy is to query 
 and cascade, and the algorithm queries all intervals; also, if 
, then 
. With probability 
, the algorithm does not query 
; if 
, then the optimum is to query R only, as 
 (as discussed in the previous paragraph), otherwise the optimum is the minimum between querying 
 and cascading or querying R only. Therefore, the approximation ratio is at most(9)
 
 
 
 
 
 

We need an upper bound for 
. We have two cases, depending on whether 
.

Case 3.1, 
. Note that , where 
. Also remember that 
 and 
, so 
. In this case, Equation (8) is at most(10)
 
 
 
 
 
 
 
 
 
 which is a function of 
 that is non-increasing for any fixed value of 
, since  and 
. On its turn, Equation (9) is at most (recall that 
)(11)
 
 
 
 which is a function of 
 that is increasing for any fixed value of 
, since 
. Since one function is non-increasing and the other is increasing, and both are linear functions of 
, their minimum is maximized when they are equal. Equating them, we obtain
 
 and 
 because 
 and . Equation (11) then becomes
 
 whose partial derivative on z is
 
 
 because 
 and . Thus, for any fixed 
, it holds that 
 is a non-decreasing function of z, so it attains its maximum at z tending to 4/3. Therefore,
 
 which is maximized when 
 and the maximum is .

Case 3.2, 
. Note that , where 
. Recalling that 
 and 
, we have 
, 
, and 
. In this case, Equation (8) is at most(12)
 
 
 
 
 
 
 
 
 
 which is a function of z that is non-increasing for any fixed value of 
, since 
. On its turn, Equation (9) is at most (using 
)(13)
 
 

If 
 then this is optimal, and when 
 it can be bounded by 8/7. Otherwise we have a function of z that is increasing for any fixed value of 
, since 
 and 
. Since one function is non-increasing and the other is increasing, their minimum is maximized when they are equal, which happens when
 
 and Equation (13) then becomes
 
 Note that the denominator of the first summand in 
 is minimized w.r.t. z at , hence is at most 
, and the numerator is non-negative for . Taking factors 
 and 
 out of the first and second brackets (resp.) of the numerator and denoting 
 
, 
 
, we have:
 
 For fixed 
, the right-hand side of this inequality is a parabola with negative coefficient of 
, hence it is maximized when , implying
 
 
 
 
 
 
 
 
 The derivative of this last function on 
 is 
 
 
, which is negative for 
. So we have a decreasing function of 
, which is thus maximized at 
, giving us 
. □

The approximation factor can be improved slightly by choosing a different parameter  such that . Note that it does not work for , since in this case we cannot build 
 with 
 simply by assuming that every interval in R has cost less than . Using the analysis of Case 1, this implies that a lower bound on the approximation that can be achieved by this strategy is , even if not restricted by Cases 2 and 3.

4. Further questions
It would be interesting to find out whether it is possible to implement our dynamic programming algorithm for the sorting problem more efficiently, e.g., in 
 time. We could also try to extend our approach for sorting so as to handle a dynamic setting, e.g. as in [13], where some intervals can be inserted/deleted from the initial set; updating the dynamic program should be faster than building it again from scratch.

The main question that we leave open regards the complexity of finding the best decision tree for the minimum problem. It has proved non-trivial to obtain either a negative or positive result. It could also be promising to look for better approximation algorithms for particular cases, e.g. if query costs and probability distributions are uniform.

If it is NP-hard to find the best decision tree for the minimum problem, then the same applies to the stochastic versions of the problem of finding the k-th smallest value (the generalized median problem) [22], [33] and the minimum spanning tree problem with uncertainty on edge weights [17], [20], [38]. This holds because the minimum problem is a particular case of both: for the k-th smallest value problem it is the case with ; for the minimum spaning tree problem, if we have two vertices connected by multiple edges, then we want to identify the edge of minimum weight. It would be interesting to find out whether it is possible to devise polynomial-time algorithms for those problems with better approximation guarantees than the respective best results for the adaptive online problem with adversarial inputs, as we did for the minimum problem. We believe those problems would require different techniques to the ones we developed here, as they are more intricate.