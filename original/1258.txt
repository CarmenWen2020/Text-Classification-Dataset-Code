Abstract
Let G=(V,E) be an undirected graph, LG∈RV×V be the associated Laplacian matrix, and b∈RV be a vector. Solving the Laplacian system LGx=b has numerous applications in theoretical computer science, machine learning, and network analysis. Recently, the notion of the Laplacian operator LF:RV→2RV for a submodular transformation F:2V→R+E was introduced, which can handle undirected graphs, directed graphs, hypergraphs, and joint distributions in a unified manner. In this study, we show that the submodular Laplacian system LF(x)∋b can be solved in polynomial time. Furthermore, we prove that even when the submodular Laplacian system has no solution, we can solve its regression form in polynomial time. Finally, we discuss potential applications of submodular Laplacian systems in machine learning and network analysis.

Keywords
Submodular functions
Laplacian systems

1. Introduction
In spectral graph theory, the Laplacian matrix (or simply Laplacian) 
 associated with an undirected graph  is an important object, where 
 is a diagonal matrix with the -th element equal to the degree of  and 
 is the adjacency matrix of G. Using the Laplacian 
 of an undirected graph G, one can extract various information regarding G, such as commuting time of random walks, maximum cut, and diameter of the graph. Further, a cornerstone result in spectral graph theory is Cheeger's inequality [1], [2], which associates the community structure of G with the second smallest eigenvalue of 
. See Chung [7] for a survey on this area.

An important problem considering Laplacians for undirected graphs is solving the Laplacian system 
 for a graph  and a vector 
; in terms of electrical circuits, this problem can be interpreted as follows: We regard each edge  as a resistance of 1Ω and each vertex  as a joint connecting these resistances. Then, the solution x provides the electric potential at the vertices when a current of A flows through each . Based on this interpretation, it is evident that the Laplacian system has a solution only when 
, that is, the amount of inflow is equal to that of outflow.

Solving Laplacian systems has numerous applications such as in simulating random walks [10], generating spanning trees [20], constructing sparsifiers [32], faster interior point methods [12], semi-supervised learning [18], [40], [42], and network analysis [5], [15], [28], [29]. For more applications of solving Laplacian systems, refer to Vishnoi [35].

Although the concept of Laplacians for undirected graphs was first introduced in the 1980s, it is only recently that the notions of Laplacian operators for directed graphs [37] and hypergraphs [27] have been proposed and corresponding Cheeger's inequalities have been obtained.1 It is important to note that these operators are no longer linear, and therefore, cannot be expressed using matrices. Furthermore, recently, Yoshida [38] showed that these operators can be systematically constructed using the cut function 
 associated with an undirected graph, directed graph, or hypergraph, where 
 is equal to one if the edge  is cut, and zero otherwise. A key property used in the analysis of the constructed operators is the submodularity of the cut function, which is given by
 for every . Indeed, Yoshida [38] showed that the same construction can be applied to any submodular transformation 
, that is, each function 
 is non-negative submodular. More precisely, he introduced the concept of submodular Laplacians, which can be defined as an operator that maps every point 
 to the subdifferential of the energy function 
 
, where 
 is the Lovász extension of 
 [38]. Upon the concept of submodular Laplacians, he obtained a generalization of Cheeger's inequality when  [38]. In this work, we always assume that a submodular transformation 
 satisfies .

Submodular Laplacian systems  Because solving Laplacian systems based on undirected graphs have numerous applications, it is natural to consider solving Laplacian systems for general submodular transformations, which are referred to as submodular Laplacian systems. It should be noted that the Laplacian operator 
 associated with a submodular transformation 
 is set-valued, and therefore, the question here is computing 
 such that 
 for a given vector 
. As the first contribution of our study, we show the tractability of submodular Laplacian systems:

Theorem 1.1

Informal, see Theorem 3.2
Let 
 be a submodular transformation and 
 be a vector. Then, we can compute 
 
 in polynomial time, if it exists.

Because the algorithm used in Theorem 1.1 is based on the ellipsoid method, the time complexity is large albeit polynomial. Later, in Section 3.2, we will discuss more efficient algorithms for special cases including when F is given by a directed graph or hypergraph. For practical use, we also consider an iterative algorithm that shows a linear convergence:
Theorem 1.2

Informal, see Theorem 3.5
Let 
 be a submodular transformation and 
 be a vector. Then, there exists an iterative algorithm for solving 
 with a linear convergence, where each iteration takes polynomial time in the total number of extreme points of the base polytopes associated with 
's.

In some special cases of the abovementioned problem, a direct interpretation is possible, which is discussed in the following lines. Suppose that 
 is constructed from a directed graph . Then, we regard each arc  as an ideal diode of 1Ω, that is, if the electric potential of u is higher than or equal to that of v, then each arc represents a resistance of 1Ω, and otherwise, no current flows through it. Then, the solution x for this directed graph provides the electric potential at the vertices when a current of A flows through each . For hypergraphs, a hyperedge acts as a circuit element in which current flows from a vertex of the highest potential to that of the lowest.

Solving Laplacian systems for undirected graphs has been intensively studied. The first nearly-linear-time algorithm was achieved by Spielman and Teng [33], and the current fastest algorithm runs in 
 time [9] for some . In contrast, to the best of our knowledge, there is no prior study on solving submodular Laplacian systems even for directed graphs and hypergraphs. We note that solving Laplacian systems in terms of Laplacian matrices for directed graphs [8] has been studied, and almost linear-time algorithms were obtained [10], [11].

Submodular Laplacian regression  When the given submodular Laplacian system 
 does not admit a solution, we may want to find 
 close to b such that 
 has a solution; this serves as motivation for the following submodular Laplacian regression problem: 
 
 It should be noted that once we have solved this problem, we can obtain 
 with 
 in polynomial time by applying Theorem 1.1. The second contribution of this study is the following:

Theorem 1.3

We can solve the submodular Laplacian regression problem in polynomial time.

1.1. Applications
In this section, we discuss potential applications of our results in machine learning and network analysis.

1.1.1. Semi-supervised learning
Semi-supervised learning is a framework for predicting unknown labels of unlabeled data based on both labeled and unlabeled instances. In the case of supervised learning, we utilize only labeled instances to predict the labels of unlabeled instances. However, in many realistic scenarios, only a few of labeled instances are available compared with a large number of unlabeled instances; in such cases, the unlabeled instances also need to be effectively utilized to predict the unknown labels. This type of learning is referred to as semi-supervised learning.

Formally, the problem of semi-supervised learning can be stated follows: The given dataset V is partitioned into a set T of labeled instances and set U of unlabeled instances. Each labeled instance  has its label 
. The aim of this problem is to predict the labels  of unlabeled instances .

A standard approach to semi-supervised learning is using Laplacians associated with undirected graphs. In this method, first, the training dataset is transformed into a similarity graph , where  is a weight function; then, the labels of unlabeled instances are predicted using the Laplacian matrix 
 for the constructed similarity graph. In particular, a similarity graph represents pairwise relations among labeled and unlabeled instances.

Zhu et al. [42] proposed a prominent algorithm for this approach, which involves solving the Laplacian system 
 under the constraints that  for all  and 
 for all .2 Since its introduction, many variants of this approach have been proposed based on Laplacians for undirected graphs, which have applied to various applications.

In this study, we propose a generalized approach for semi-supervised learning that utilizes submodular Laplacians. Using submodular Laplacian systems, we can extend previous semi-supervised learning algorithms to more general similarity expressions, such as directed graphs and hypergraphs. An arc represents that the tail vertex is closer to +1 than the head vertex. A hyperedge represents that all incident vertices have similar labels. Furthermore, we can express more complicated relationships with general submodular functions.

To develop a general framework for semi-supervised learning, we extend Theorem 1.1 by imposing constraints 
 and 
, where 
 and 
 are provided as a part of the input:

Theorem 1.4

Let 
 be a submodular transformation and suppose that V is partitioned into disjoint subsets  and . Let 
 and 
 be vectors. Then, we can compute 
 that satisfies 
, 
 for all  and 
 for all  in polynomial time if it exists.

As with semi-supervised learning using undirected similarity graphs, we can apply Theorem 1.4 to semi-supervised learning with general similarity expressions by setting 
 to the given labels and 
 to the zero vector.
There are several studies (implicitly) utilizing hypergraph Laplacians. Hein et al. [16] used hypergraph Laplacians for semi-supervised learning with a formulation slightly different from ours. Zhang et al. [39] dealt with semi-supervised learning with directed hypergraph Laplacians, which is a special case of our setting. They applied the subgradient descent method to this problem, but this method is not guaranteed to be linearly convergent. In contrast, Theorem 1.4 shows that we can obtain a solution in polynomial time. Moreover, we can modify the iterative algorithm given in Theorem 1.2 for this setting to obtain a linearly convergent algorithm. Li and Milenkovic [22] considered hypergraphs with a submodular weight function on each hyperedge, but the problem they studied is the normalized cut problem and k-way cut problem, which are different from ours. Li et al. [23] defined p-Laplacians and proposed algorithms for obtaining eigenvalues of p-Laplacians. Our submodular Laplacians correspond to 2-Laplacians in their terminology. Independently to our work, Li et al. [24] studied a semi-supervised learning problem involving general submodular Laplacian. The main difference from our work is that they have an additional strongly convex proximity term in the objective, which played a crucial role in their convergence analysis. In contrast, our objective function is not strongly convex. Their problem is equivalent to computing the proximal operator for our problem, which is a subroutine of the proximal point algorithm in Section 3.3. While we compute the proximal operator by solving its dual problem, which is a convex quadratic program, they solve it by applying the random coordinate descent algorithm or the alternative projection algorithm to another dual problem with cone constraints related to the base polyhedra.

1.1.2. Network analysis
A typical task in network analysis is measuring the importance, or centrality, of vertices and edges. There are many centrality notions depending on applications, and for undirected graphs, some notions are defined using Laplacians matrices.

Before describing these centrality notions, we require some definitions. Let  be an undirected graph, and for each , let 
 be the vector with 
 and 
 for . Then, for vertices , the quantity 
 is called the effective resistance from u to v, where 
 is the pseudo-inverse of the Laplacian 
. The effective resistance can be considered as the resistance of the circuit associated with G when a current is passed from u to v.

In [15], [28], the effective resistance 
 of an edge  is directly used as the centrality of an edge, and is referred to as the spanning tree centrality, because it is known to be equal to the probability that the edge is used when sampling a spanning tree uniformly at random.

Brandes and Fleischer [5] introduced the current flow closeness centrality of a vertex, which is defined as
 
 This notion implicitly assumes that the effective resistance satisfies the triangle inequality, that is, 
 for every  and hence can be seen as a (quasi-)metric. Then, we regard a vertex as important when it is close to every other vertex; in other words, it is in the center of the network.

It is natural to inquire whether these notions can be extended to directed graphs, hypergraphs, or general submodular transformations. If there exists a vector 
 such that 
, we can define the effective resistance 
 for  in terms of the Laplacian 
 associated with a submodular transformation 
 as
 which generalizes the case of undirected graphs, since 
 in this special case. Though there may be multiple 
 that satisfy 
 in general, the value 
 is identical for all such x (see Lemma 6.2 for more details). When there is no such 
 that satisfies 
, we define 
. The formal definition is deferred to Section 6.

For directed graphs and hypergraphs, as in the case of undirected graphs, the effective resistance 
 can be seen as the resistance of the circuit associated with the graph when a current is passed from u to v. Note that 
 may not be equal to 
 for directed graphs.

Further, we can generalize centrality notions mentioned above to general submodular transformations. As mentioned earlier, the implicit assumption when defining current flow closeness centrality is of the triangle inequality of effective resistance. Here, we show that it also holds for general submodular transformations:

Theorem 1.5

Let 
 be a submodular transformation. Then, effective resistance 
 satisfies triangle inequality.

1.2. Organization
The notions used in this paper are reviewed in Section 2. Then, we prove Theorem 1.1, Theorem 1.2 in Section 3. Section 4 provides our algorithm for submodular system regression and the proof of Theorem 1.3. Section 5 describes an application of our method for semi-supervised learning. The triangle inequality theorem (Theorem 1.5) is proved in Section 6. Finally, we present our experimental results in Section 7.

2. Preliminaries
For a positive integer , the set  is denoted by . For a vector 
 and a set ,  denotes the sum 
.

2.1. Submodular functions
Let 
 be a distributive lattice. A function  is called submodular if for every . For a submodular function , the submodular polyhedron 
 and base polyhedron 
 associated with F are defined as
 respectively. We omit the subscripts  when it is clear from the context.

Let  be a normalized submodular function, that is, . Then, its Lovász extension 
 is defined as 
  where  is the inner product. We note that f is an extension considering 
 for every , where 
 is a vector with 
 if  and 
 otherwise. Moreover, f is convex and positively homogeneous, that is,  for every . It is known that the subdifferential of f at x is 
, and this is denoted by .

2.2. Submodular Laplacians
A transformation 
 is called submodular if each function 
 is submodular. We now formally introduce the Laplacian operator associated with a submodular transformation:

Definition 2.1

Submodular Laplacian [38]
Let 
 be a submodular transformation. Then, the Laplacian 
 of F is defined as
 
 
 where and 
 is the Lovász extension of 
 for each  and 
 
 is the energy function associated with F.

Here, we have 
 for any 
, and hence we write 
 to denote this quantity.
Example 2.2

As a descriptive example, consider an undirected graph  and the associated submodular transformation 
. Then, for every edge , we have 
 and
  We can confirm that 
 for every , where 
 is the standard Laplacian matrix associated with G.

Refer to Yoshida [38] for further examples.

3. Solving submodular Laplacian systems
In this section, we prove Theorem 1.1. Throughout this section, we fix a submodular transformation 
 and a vector 
. Note that we assume that  throughout the paper. Therefore, the image of 
 is contained in the linear subspace 
. Hence, without loss of generality, we assume that , since otherwise the submodular system does not have any solution. We assume submodular transformation F can be accessed through the value oracle, i.e., the value 
 can be obtained in  time for each  and .

We describe a general method based on the ellipsoid method in Section 3.1. Then, in Section 3.2, we discuss a more efficient algorithm when 
 is given by a directed graph or hypergraph. In Section 3.3, we give an alternative iterative method that linearly converges to a solution whose running time is polynomial in the total number of extreme points in the base polytopes associated with 
's.

3.1. General case
Let 
 be the Lovász extension of 
. We consider the following optimization problem:(1) 
 
 
 This problem is equivalent to minimizing 
 
 
, which is convex. Then, we can directly show that the minimizer 
⁎
 of the latter problem satisfies 
 from the first order condition. By considering (1), however, we can exploit the combinatorial structure of the problem as we show below.

Introducing the Lagrange multiplier , we can obtain the corresponding Lagrangian as(2)
 
 
 
 

Thus, 
 
 
  
 
 
 
⁎
 where 
⁎
 is the Fenchel conjugate. Since Lovász extensions are positively homogeneous, we can confirm that 
⁎
 is equal to either infinity or zero. Thus, we obtain the dual problem:(3) 
 
 
 
⁎

The constraint can be checked with submodular function minimization. To see this, we observe the following:

Lemma 3.1

We have 
 
⁎
  
 
 if and only if 
  
 
  
 

Proof

 Trivial.

 Suppose there is 
 such that 
. By the assumption that , we have  for any , where 
 is the all-one vector. This holds because 
, where we used the fact that  holds for any 
 as 
.

Now, by adding  to x for a large , we can assume  for every . Note that since , this does not change the linear term . Moreover,  because 
 for each . Since each Lovász extension 
 is positively homogeneous, for 
, we have 
. Therefore,
 
 
 
 □

By Lemma 3.1, we obtain the following dual problem:(4) 
 
 
 
 A separation oracle for the constraint can be implemented by submodular function minimization. Therefore, we can use the ellipsoid method to solve (4).

Theorem 3.2

Formal version of Theorem 1.1
The following holds:

(1)
(1) is bounded if and only if (4) is feasible.

(2)
A strong duality holds between (1) and (4), that is, the optimal values of (1) and (4) coincide.

(3)
We can solve (1) in polynomial time.

(4)
The optimal solution 
⁎
⁎
 for (1) satisfies 
⁎
.

Proof

(1) Standard.

(2) It is clear that (1) satisfies Slater's condition and hence the claim holds.

(3) Since we can compute the subgradient of 
 at a given point x, we have a separation oracle for (1). Let  be an arbitrary constant. To show that the ellipsoid method solves (1), we need to estimate  such that an optimal solution is contained in a ball with radius R centered at the origin and a ball with radius r is contained in the set 
⁎
, where 
⁎
 is the optimal value. This can be carried out in a standard way and the formal proof can be found in Appendix A.

(4) Let 
⁎
⁎
 be an optimal solution of (1). Based on Rockafellar [31, Corollary 28.3.1], there exists 
⁎
 such that 
⁎
⁎
⁎
 is a saddle point of the Lagrangian P. Since 
 
, we have 
⁎
⁎
 using the saddle condition. By complementary slackness, if 
⁎
, we have 
⁎
⁎
 for any . If 
⁎
, we have 
⁎
, and since 
⁎
, we obtain 
⁎
⁎
 for . Thus, we conclude that 
⁎
⁎
 for . Finally, by the saddle condition for 
⁎
, we must have 
 
⁎
⁎
 
⁎
⁎
 which implies 
⁎
. □

3.2. Flow-like formulation
If the number of the extreme points of the base polytope for each 
 is small, we can use a different formulation. In general, the number of the extreme points of the base polytope for a submodular function can be exponential in , but it is small when each 
 is a cut function associated with an arc or a hyperedge. As we will illustrate in examples of directed graphs and hypergraphs, the constraint of this formulation can be interpreted as a generalization of a flow constraint.

First, we enumerate all extreme points of the base polytope 
 for each 
. Let 
 be the set of extreme points of 
 (). The constraint 
 in (1) is equivalent to  for all extreme points 
. Therefore, (1) is equivalent to(5) 
 
 

For each extreme point 
, we introduce a “flow” variable . By a calculation similar to that in Section 3.1, we can obtain the following dual problem:(6) 
 
 
  
 
  
  The number of the variables is equal to the total number of extreme points, that is, 
. If this number is small, since this is a convex quadratic problem with linear constraints, we can solve it by using the ellipsoid method [21] or the interior-point method [19], [36].

The constraint 
 can be interpreted as a “flow boundary constraint”, as illustrated in the following examples.

Example 3.3 Cut functions of directed graphs

Let  be a directed graph and 
 be the cut function associated with e. The extreme points of 
 for  are 0 and 
. Since the value of  does not interact with the constraint, we can assume that . Then, the constraint is the ordinary flow boundary constraint: 
 (), where we denote 
 by . Now (6) is equivalent to the quadratic cost flow problem, which can be solved in 
 time [34].

Example 3.4 Cut functions of hypergraphs

Let  be a hypergraph and 
 be the cut function associated with e. That is, 
 if X intersects both e and , and 
 otherwise. The extreme points of 
 are in the form of 
 (, ). The number of the extreme points of 
 is 
, and then the total number of the variables is 
. The value of 
 can be interpreted as a “flow” from v to u through a hyperedge e. Indeed, we can construct the equivalent (ordinary) flow network 
 as follows. The vertex set of 
 is V, and for each distinct , an arc uv in 
 is drawn. Then, any flow 
 in 
 with the boundary b, which can be computed via minimizing a quadratic function under a flow constraint, corresponds to the original variable φ in (6). An approach to this problem is to use the Frank-Wolfe algorithm. We can observe that each iteration of the Frank-Wolfe algorithm corresponds to solving a minimum cost flow problem. Due to the convergence rate by Jaggi [17], the number of iterations for obtaining an ϵ-approximate solution is 
 
, where 
 is the curvature. We can bound this curvature by the product of the smoothness parameter of the objective function, which is 
, and the squared radius of the feasible region, which can be bounded by using the fact that an optimal solution exists in the ball centered at the origin with radius 
. Therefore, the number of iterations we need is 
 
.

3.3. Proximal point algorithm
As discussed in Section 3.1, to solve 
, it suffices to solve(7) 
 
 
 
 In this section, we show that the proximal point algorithm, described below, can be used to solve (7) efficiently when the number of extreme points of the base polytopes of 
's is small.

For a vector 
, the proximal operator for (7) is defined as
 
 
 
 
 Then, the proximal point algorithm for solving (7) is an algorithm that iteratively computes
 where 
 is an arbitrary initial vector.

As in Section 3.2, let 
 be the set of extreme points of 
 for each . In general, the number 
 of extreme points can be , but is small in typical cases such as directed graphs or hypergraphs. Then, we show the following:

Theorem 3.5

Formal version of Theorem 1.2
The proximal point algorithm satisfies the following:

(i)
Each step of the proximal point algorithm takes polynomial time in 
.

(ii)
The proximal point algorithm shows linear convergence, that is,  
⁎
⁎
 
⁎
⁎
⁎
 
⁎
, where 
⁎
 is the set of minimizers of (7) and  is a constant depending on 
's.

Proof

(i) We can rephrase 
 as(8) 
 	
 
 
 

As with Section 3.2, we consider the dual problem of (8). Introducing a Lagrange multiplier  for each  and 
, we obtain the following Lagrangian:
 
 
  
  Then, we have(9) 
  
  
  
 
 
 
  
  
 
 
  
 
 
  
 
  
 
  which is a convex quadratic program. We can solve it in time polynomial in the total number of extreme points, i.e., 
. Due to the complementary slackness condition, we can restore a primal solution from the dual optimal solution as follows:
⁎
 
⁎
 
  
 

(ii) A function 
 is called piecewise quadratic if there are finitely many disjoint convex polyhedral subsets 
 of 
 such that 
 and f is quadratic in each 
. Li [25] showed the linear convergence of the proximal point algorithm for convex piecewise quadratic functions. To utilize this result, we show that (7) is convex piecewise quadratic. For any permutation σ of V, the Lovász extension 
 is linear in , and hence the objective function is piecewise quadratic. Since each 
 is convex, the objective function is also convex.

Formally, Li [25] showed 
⁎
⁎
⁎
 
⁎
⁎
⁎
 for k such that 
⁎
⁎
⁎
 and 
⁎
⁎
⁎
⁎
⁎
⁎
⁎
⁎
⁎
 
⁎
⁎
⁎
 for k such that 
⁎
⁎
⁎
, where β is a positive constant depending only on 
's and b. If we set 
, by Lemma A.2, the initial distance 
⁎
⁎
⁎
 can be bounded by 
 
, where 
 
. Therefore, if we define 
 
 
 
 
, we have  
⁎
⁎
 
⁎
⁎
⁎
 
⁎
. □

As the number of extreme points of an arc (in a directed graph) is two and that of a hyperedge of size r is 
, Theorem 3.5 implies that the proximal point algorithm linearly converges to a solution for Laplacian systems associated with directed graphs and hypergraphs.
4. Submodular Laplacian regression
In this section, we prove Theorem 1.3.

First, we explain when a submodular Laplacian system, or equivalently (4), is feasible. We define 
, which is the set of  that minimizes all 
 (). Then we can characterize when a submodular Laplacian system is feasible as follows.

Lemma 4.1

Let 
 be a submodular transformation and 
 be a vector. Then, there exists a vector 
 such that 
 if and only if  for all .

Proof

First we assume there exists a vector 
 such that 
. Then, from the duality between (1) and (4) (Theorem 3.2 (1)), problem (4) is feasible, that is, there exists  such that 
 for all . Therefore, for all , we have .

Next we show the converse. We assume that  for all . Now we consider  whose entries are all very large. If , there exists  such that 
, and 
 for all 
 from the non-negativity of F, hence 
 holds. If , we have  from the assumption, hence 
 holds. Therefore, such  satisfies 
 for all . Finally, the duality between (1) and (4) implies that there exists a vector 
 such that 
. □

We remark that, when each submodular function 
 is symmetric, the above feasibility condition can be simplified to the condition that 
 for each e, where 
 is a “connected component” of 
 (see Li and Milenkovic [22] for the precise definition). In the general case, however, we need  to characterize the feasibility.
Now we consider the regression problem of minimizing 
 subject to 
 for some 
. By using Lemma 4.1, this problem reduces to the following optimization problem.(10) 
 
 Since the minimizers of each 
 form a distributive lattice,  is also a distributive lattice. Hence, (10) can be considered as the minimum norm point problem in 
, where .

To handle (10) efficiently, we use the Birkoff representation of  because it has a polynomial size. To this end, note that  is the lattice of minimizers of 
, since each 
 is nonnegative. Then, the Birkoff representation of  can be efficiently constructed from the minimum norm point of 
. Refer to [13, Section 7.1 (a)] for further details.

The minimum norm point problem (10) is slightly different from the one used for submodular function minimization considering that (i) the target polytope is a submodular polyhedron 
 rather than a base polyhedron 
, and (ii) the lattice  is not a Boolean lattice 
 but a distributive lattice on V.

In the following subsection, we introduce the Birkoff representation theorem as a preliminary. After that, we present two algorithms for solving the problem. The first one is the standard Frank-Wolfe iterative algorithm (Section 4.2), while the other is a combinatorial algorithm (Section 4.3).

4.1. Distributive lattice and the Birkoff representation theorem
In this subsection, we introduce the Birkoff representation theorem for later use.

A subset S in a poset  is called a lower ideal if S is down-closed with respect to the partial order ⪯. The Birkoff representation theorem [4], [13] states that for any distributive lattice 
 with , there uniquely exists a poset P on a partition  of V and a partial order ⪯ such that  if and only if S is a lower ideal in P.

The poset P can be constructed as follows. Let us define an equivalence relation ∼ on V as  if any  contains either both, i and j, or none. Let  be the set of equivalence classes induced by ∼. Then, a partial order ⪯ on  is defined as , where  and  are equivalence classes containing i and j, respectively, if any set  containing j also contains i. It can be confirmed that ⪯ is a well-defined partial order. Then,  is the desired poset.

Example 4.2

Let  and consider a distributive lattice . Then, the partition  consists of  because any element in  containing 1 also contains 2 and vice versa and there is no other such pair. The partial order ⪯ on  is given as  and any other pair is incomparable because any element in  containing  (or 3) also contains 4. Hence, the Hasse diagram of the poset  is as illustrated in Fig. 1a. The lower ideals in P are , and , which are exactly the elements of the original distributive lattice .

Fig. 1
Download : Download high-res image (132KB)
Download : Download full-size image
Fig. 1. This figure illustrates the method used to solve 
 using minimum st-cut for .

Using the Hasse diagram of P, one can maintain the poset P in a digraph with n vertices and m arcs. We refer to such a digraph as the Birkoff representation of . Since  and 
, the Birkoff representation is a compact representation of a distributive lattice, even if  contains exponentially many subsets. Furthermore, if  is the set of minimizers of a submodular function, one can construct the Birkoff representation in strongly polynomial time using submodular function minimization algorithms (see Fujishige [13]).

4.2. Frank-Wolfe algorithm
Given the Birkoff representation of  as a directed graph with n vertices and m arcs, we can optimize linear functions over the polyhedron 
 in  time using the greedy algorithm. This fact suggests to use the Frank-Wolfe algorithm [17] to solve (10). To this end, we restrict ourselves to a compact region 
. Based on the analysis of the Frank-Wolfe algorithm in Jaggi [17], we need to bound the squared Euclidean diameter of . Trivially, 
 is an upper bound. We obtain the following convergence rate:

Theorem 4.3

Let 
 be a sequence generated by the Frank-Wolfe algorithm for (10) () and 
⁎
 be the optimal solution. Then, 
⁎
 
. Each iteration of the Frank-Wolfe algorithm takes  time.

Theorem 4.3 is unsatisfactory because we cannot guarantee 
 has a solution for any k. The algorithm discussed in the next subsection resolves this issue.
4.3. Combinatorial algorithm
In this section, we present a combinatorial algorithm for the minimum norm point problem (10). First, we show that this problem can be reduced to parametrized submodular minimization. Although we only need to consider a modular function 
, that is, , to solve (10), we aim to describe it in the most general case. Formally, we need to solve the following:(11) 
 
 
 Since 
 is down-closed,3 the optimal solution must be a nonpositive vector. Indeed, suppose that the optimal solution 
 has a positive entry, say . Then, the vector 
 defined as 
 if  and 
 is also contained in 
 by the down-closedness and obviously has a larger objective value than w, contradicting the optimality of w.

Let us consider the following dual problem:(12) 
 
 
 where 
 is the Lovász extension of H. This is indeed dual to (11) as shown in the next lemma.

Lemma 4.4

The problems (11) and (12) are strong dual to each other. Furthermore, if 
⁎
 is the optimal solution of (11), then 
⁎
⁎
 is the optimal solution for (12), and vice versa.

Proof

Using the Fenchel strong duality, we have(since 
) 
 
 
  
 
 
  
 
 
(by the Fenchel strong duality) 
  
 
 
 
 
 The second assertion can be checked by direct calculation. □

Therefore we focus on (12). In what follows, we will show that an optimal solution to (12) can be constructed by solving parametrized submodular minimization:(13) 
  where  is a parameter. Let 
 be an optimal solution to (13).

Lemma 4.5

Bach [3, Proposition 8.2]
If , then 
.

For a vector 
 and a scalar , we write  (resp., ) to denote the set  (resp., ).
Lemma 4.6

If , then  for any .

Proof

Here, we prove the contrapositive. Suppose that  for some ; this indicates that there exist  such that  and . Let us take an arbitrary 
⁎
 and consider 
⁎
, where . Then, since , any  containing v must also contain u. Thus 
⁎
. Since , 
⁎
 attains infinity as , that is, . □

Lemma 4.7

Define 
 as 
. Then z is a minimizer of (12).

Proof

We can check that 
 for  using the definition of z and Lemma 4.5. Therefore, 
 almost everywhere. Let 
 be a feasible solution such that . Then for any ,  holds by Lemma 4.6. Furthermore, we have
 
 
 
 
 
 
 
 
 
 where the inequality follows since the integrand is equal to 
 almost everywhere. □

Now, we consider the following modular case: 
. Considering the abovementioned arguments, we only need to solve 
 for all . Using the approach used in Picard and Queyranne [30], we define the following directed graph. Let 
 be the digraph corresponding to the Birkoff representation of . Then, define a directed graph 
, where 
. In addition, define a capacity function c on 
 as(14)
  Then, the minimum st-cuts in G provide the desired minimizers (refer to Fig. 1).

Furthermore, the capacity function c satisfies the so-called GGT structure [14]4; the capacities of arcs from source s are nonincreasing, whereas the capacities of arcs to sink t are nondecreasing, while the others are constant. For such a capacity function, all the values of α at which the value of minimum st-cuts change can be computed in 
 
 time using a parametric flow algorithm [14], where n and m are the number of vertices and arcs in G, respectively.

Theorem 4.8

Assume that the Birkoff representation of  is given as a directed graph with n vertices and m arcs. Then, there exists a strongly polynomial time algorithm for (10) with time complexity 
 
.

Theorem 1.3 is immediate from the above theorem and the fact that we can construct the Birkoff representation of  in polynomial time as previously discussed.
5. Semi-supervised learning
In this section, we prove Theorem 1.4.

We consider the problem of solving 
 under constraints that 
 for all  and 
 for all , where  is a partition of V. To find such x and b, we consider the following optimization problem:(15) 
 	
 
 
  The following theorem shows that (15) can be solved in polynomial time, and an obtained optimal solution x satisfies 
.

Theorem 5.1

There is a polynomial time algorithm that solves (15). In addition, for an optimal solution 
⁎
⁎
 to (15), 
⁎
 holds for each , and there exists some 
⁎
 such that 
 holds for each .

Proof

(15) is equivalent to the following problem:(16) 
 	
 
 
 
  This problem can be viewed as an unconstraint convex programming with variables 
. As we can compute the subdifferential of this objective function, we can solve (15) with the ellipsoid method in polynomial time.

Let 
⁎
⁎
 be an optimal solution. Since 
⁎
 is a solution to (15), 
⁎
 holds for each . From the first-order optimality condition, there exists 
⁎
 for each  such that 
⁎
 for all . It follows that 
⁎
 for some b such that 
. □

To derive more efficient algorithms for special cases such as directed graphs and hypergraphs, we introduce a flow-like formulation, which is an extension of (5) and (6).

Let 
 be the set of extreme points of 
 for each . Then, the original primal problem (15) is equivalent to:(17) 
 	
 
 
  
 
  with regarding 
 as variables. The dual problem is(18) 
 	
 
  
 
  
 
 
 

The first constraint can be interpreted as a flow boundary constraint on . In the cases where the number of the extreme points of each 
 is polynomial in , including directed graphs and hypergraphs, the number of variables 
 in the dual problem is bounded by a polynomial in . As discussed in Section 3.2, if F is given by a directed graph, this dual problem can be reduced to the quadratic cost flow problem, which can be solved in 
 time [34].

6. Triangle inequality of effective resistance
In this section, we prove Theorem 1.5.

First, we provide a formal definition of the effective resistance.

Definition 6.1 Effective resistance

Let  be elements of V. If (4) is feasible for 
, the effective resistance 
 from u to v is defined to be the double of the optimal value of (4), i.e., 
⁎
, where 
⁎
 is an optimal solution for (4) in the case of 
. If (4) is infeasible for 
, we define 
.

Originally, the effective resistance is defined for undirected graphs as 
. For general submodular Laplacians, there may be multiple 
 such that 
 and it is difficult to define the effective resistance in a similar manner. However, as we show in the following lemma, the value of 
 is identical for all 
 such that 
, and coincides with 
 in Definition 6.1. Hence, we can say 
 is a generalization of the effective resistance for submodular Laplacians.

Lemma 6.2

Let 
 be a submodular transformation and  be arbitrary elements of V. Suppose (4) is feasible for 
, and let 
⁎
 be an optimal solution. Let 
⁎
 be a vector that satisfies 
⁎
. Then we have 
⁎
⁎
.

Proof

From the assumption, we have 
⁎
. This is the first-order optimality condition for the primal problem (1) with 
, and hence 
⁎
 is an optimal solution to the primal problem (1) with 
. From the strong duality between (1) and (4), we have
 
 
⁎
⁎
 
⁎
 From the complementary slackness condition, we have 
⁎
⁎
 for all . Substituting it to the above equation and rearranging, we obtain
⁎
⁎
 and the claim holds. □

Now we show the triangle inequality of the effective resistance 
.

Lemma 6.3

(Triangle inequality of effective resistance) Let 
 be a submodular transformation. Suppose that effective resistances 
 and 
 are bounded. Then, it holds that

Proof

Suppose 
, 
 and 
 are optimal solutions for (4) with 
, 
 and 
, respectively. Let 
 be 
. We show 
 is a feasible solution to (4) for 
. It is enough to show for any , the constraint 
 
 holds. If  or , the constraint holds since 
 and 
 is non-negative for each . Hence we consider S such that  and . When , we have 
 
 
 
 The last inequality holds because 
 is a feasible solution to (4) for 
.

Similarly, when , we have 
 
 
 

Since 
 is a feasible solution to (4) for 
, by Lemma 6.2, we can show the triangle inequality as follows:
 and the claim holds. □

7. Experiments about proximal point algorithm
In this section, we show experimental results on the performance of the proximal point algorithm given in Section 3.3 applied to the semi-supervised learning problem on a randomly generated hypergraph. Given a hypergraph , a partition  of V, 
, and 
, we are to solve(19) 
 	
 
 
  where 
 is the hypergraph Laplacian of G. For the detail of this problem and our proposed methods, see Appendix 5.

We generated a hypergraph randomly in the following way. First we fix the number  of vertices and the number  of hyperedges. We partition the set V of n vertices into equal-sized two clusters 
 and 
. Half of the m hyperedges are assigned label +1 and the others are assigned −1. The set of vertices included in each hyperedge is determined randomly as follows: If the hyperedge is assigned label +1, each vertex in 
 is included in the hyperedge with probability  and each vertex in 
 is included with probability , and vice versa. The labeled subset T is set to the union of random subsets of 
 and 
 of size . We set 
 if 
 and 
 if 
. We also set 
 to the zero vector.

We implemented two different versions of the proximal point algorithm as our proposed methods. The first method directly solves the constraint convex quadratic programming problem (9). On the other hand, the second method solves the following optimization problem equivalent to (9): 
 
 
  
 
 
  
 
  
 
 where ∂e is the set of vertices included in hyperedge e. The number of variables used in the second formulation is 
, which is smaller than 
 of the first formulation. We call the first one proximal_quadratic and the second one proximal_quadratic_faster. In both of the methods, we used GUROBI optimizer as a subroutine for solving the constrained convex quadratic programming. We also implemented the subgradient method, which was proposed in Zhang et al. [39], as a baseline.