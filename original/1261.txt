Abstract
We study the decomposition of multivariate polynomials as sums of powers of linear forms. As one of our main results, we give a randomized algorithm for the following problem: given a homogeneous polynomial f(x1,…,xn) of degree 3, decide whether it can be written as a sum of cubes of linearly independent linear forms with complex coefficients. Compared to previous algorithms for the same problem, the two main novel features of this algorithm are:
(i)
It is an algebraic algorithm, i.e., it performs only arithmetic operations and equality tests on the coefficients of the input polynomial f. In particular, it does not make any appeal to polynomial factorization.

(ii)
For f∈Q[x1,…,xn], the algorithm runs in polynomial time when implemented in the bit model of computation.

The algorithm relies on methods from linear and multilinear algebra (symmetric tensor decomposition by simultaneous diagonalization). We also give a version of our algorithm for decomposition over the field of real numbers. In this case, the algorithm performs arithmetic operations and comparisons on the input coefficients.

Finally we give several related derandomization results on black box polynomial identity testing, the minimization of the number of variables in a polynomial, the computation of Lie algebras and factorization into products of linear forms.

Keywords
Arithmetic circuits
Circuit reconstruction
Polynomial identity testing
Tensor decomposition
Linear algebra

1. Introduction
Let 
 be a homogeneous polynomial of degree d, also called a degree d form. In this paper we study decompositions of the type:(1)
 
 where the 
 are linear forms. Such a decomposition is sometimes called a Waring decomposition, or a symmetric tensor decomposition. We focus on the case where the linear forms 
 are linearly independent. This implies that the number r of terms in the decomposition is at most n. When  we have 
 where A is an invertible matrix of size n and(2)
 is the “sum of d-th powers” polynomial. If f can be written in this way, we say that f is equivalent to a sum of n d-th powers. More generally, two polynomials  in n variables are said to be equivalent if they can be obtained from each other by an invertible change of variables, i.e., if  where A is an invertible matrix of size n. As pointed out in [37], the case  (equivalence to a sum of n cubes) can be tackled with the decomposition algorithm for cubic forms in Saxena's thesis [48]. Equivalence to 
 for arbitrary d was studied by Kayal [37]. This paper also begins a study of equivalence to other specific polynomials such as the elementary symmetric polynomials; this study is continued in [20], [23], [24], [38], [39], in particular for the permanent and determinant polynomials. The contributions of the present paper are twofold:

(i)
We give efficient tests for equivalence to a sum of n cubes over the fields of real and complex numbers. In particular, for an input polynomial with rational coefficients we give the first polynomial time algorithms in the standard Turing machine model of computation. As explained below in Section 1.1, this is not in contradiction with the polynomial time bounds from [37], [48] because we do not address exactly the same problem or work in the same computation model as these two papers.

More generally, we can test efficiently whether the input f can be written as in (1) as a sum of cubes of linearly independent linear forms. This follows easily from our equivalence tests and the algorithms from [37], [48] for the minimization of the number of variables in a polynomial.

(ii)
Our first equivalence algorithm for the fields of real and complex numbers is randomized. We derandomize this algorithm in Section 5, and we continue with several related derandomization results on black box polynomial identity testing, the minimization of the number of variables in a polynomial, the computation of Lie algebras and factorization into products of linear forms.

1.1. Equivalence to a sum of cubes
Our algorithm for equivalence to a sum of n cubes over  is algebraic in the sense that the input polynomial may have arbitrary complex coefficients and we manipulate them using only arithmetic operations and equality tests. Over the field of real numbers we also allow inequality tests. We therefore work in the “real RAM” model; an appropriate formalization is provided by the Blum-Shub-Smale model of computation [6], [7]. We can provide algebraic algorithms only because we are considering a decision problem: it is easy to see that if the input 
 is equivalent to a sum of n cubes, the coefficients of the linear forms 
 in the corresponding decomposition need not be computable from those of f by arithmetic operations (see Example 16, Example 17 at the beginning of Section 3).

Polynomial factorization is an important subroutine in many if not most reconstruction algorithms for arithmetic circuits, see e.g. [18], [19], [36], [37], [39], [40], [51]. It may even seem unavoidable for some problems: reconstruction of ΠΣ circuits is nothing but the problem of factorization into products of linear forms, and reconstruction of ΠΣΠ circuits is factorization into products of sparse polynomials. Useful as it is, polynomial factorization is clearly not feasible with arithmetic operations only, even for polynomials of degree 2. We therefore depart from the aforementioned algorithms by avoiding all use of such a subroutine.

For an input polynomial f with rational coefficients, our algebraic algorithms run in polynomial time in the standard bit model of computation, i.e., they are “strongly polynomial” algorithms (this is not automatic due to the issue of coefficient growth during the computation). We emphasize that even for an input 
 we are still considering the problem of equivalence to a sum of n cubes over the real or complex numbers. Consider by contrast Kayal's equivalence algorithm [37], which appeals to a polynomial factorization subroutine. We can choose to factor polynomials over, say, the field of rational numbers. We can then run Kayal's algorithm without any difficulty on a probabilistic polynomial time Turing machine, but the algorithm will then reject the polynomial of Example 17 whereas our algorithm will accept it.1 At first sight this difficulty seems to have a relatively simple solution: for an input with rational coefficients, instead of factoring polynomials in  we will factor in a field extension of  containing the coefficients of the linear forms 
 (for instance in  for Example 17). It is unfortunately not clear that this approach yields a polynomial time algorithm because it might lead to computations in a field extension of exponential degree. We explain this point in more detail in Section 3.1. For the same reason (reliance on a polynomial factorization subroutine) similar issues arise in the analysis of Saxena's decomposition algorithm. A complete analysis of these two algorithms for equivalence to a sum of powers over  in the Turing machine model would entail good control of coefficient growth and good bounds on the degrees of the field extensions involved. This has not been done yet to the best of our knowledge.

1.2. Derandomization
We give a deterministic black box identity testing algorithm for polynomials which can be represented as in (1) as a sum of powers of linearly independent linear forms. As we will see in Section 6.2, the problem is really to decide whether the (unknown) number of terms r in the decomposition is equal to 0. Indeed, for  such a polynomial can never be identically zero (and the PIT problem for this family of polynomials can therefore be solved by a trivial algorithm in the white box model). In contrast to our equivalence algorithms, this black box PIT applies to homogeneous polynomials of arbitrary degree.

There is already a significant amount of work on identity testing for sums of powers of linear forms. In particular, Saxena [47] gave a polynomial time algorithm in the white box model (where we have access to an arithmetic circuit computing the input polynomial). Subsequently, several algorithms were given for the black box model [1], [15], [14], [16] but they do not run in polynomial time. The current state of the art is in [16], with a black box algorithm running in time 
. We obtain here a polynomial running time under the assumption that the 
 are linearly independent. Without this assumption, designing a black box PIT algorithm running in polynomial time remains to the best of our knowledge an open problem.

In Section 7 we build on our black box PIT to derandomize Kayal's algorithm for the minimization of the number of variables in a polynomial [37]. Like our black box PIT, this result applies to polynomials that can be written as sums of powers of linearly independent linear forms. For such a polynomial, the minimal (or “essential”) number of variables is just the number r of terms in the corresponding decomposition (1). We continue with the computation of Lie algebras of products of linear forms. Finally, our deterministic algorithm for this task is applied to the derandomization of a factorization algorithm from [41] and of Kayal's algorithm for equivalence to 
 [37].

1.3. Our approach
We obtain our equivalence algorithms by viewing the coefficients of the input polynomial 
 as the coefficients of a symmetric tensor T of size n and order 3 (since f is of degree 3). Equivalence to a sum of n cubes then amounts to a kind of diagonalizability property of T. This approach is explained in detail in Section 3. It can be viewed as a continuation of previous work on orthogonal tensor decomposition [42] (the present paper is more algorithmic, is not limited to orthogonal decompositions and can be read independently from [42]).

We work on a tensor of size n by cutting it into n “slices”; each slice is a symmetric matrix of size n. We therefore rely on methods from linear algebra. This explains the presence of a section of preliminaries on simultaneous reduction by congruence (which is then applied to the slices of T). Despite these rather long preliminaries, the resulting randomized algorithm is remarkably simple: it is described in just 3 lines at the beginning of Section 4.

Our deterministic algorithms also rely on important insights from Kayal's paper [37]. In particular we rely on the factorization properties of the Hessian determinant of the input f, which we manage to use without appealing explicitly to a factorization subroutine (as explained in Section 1.1, this is ruled out in our approach). Our deterministic algorithm for the minimization of the number of variables is directly inspired by the randomized algorithm for this problem in the same paper.

1.4. Comparison with previous tensor decomposition algorithms
There is a vast literature on tensor decomposition algorithms, most of them numerical (see [4] for a recent paper showing that many of these algorithms are numerically unstable). From this literature, two papers by De Lathauwer et al. [12], [13] are closely related to the present work since they already recognized the importance of simultaneous diagonalization by congruence for tensor decomposition. Jennrich's algorithm is also closely related for the same reason. One can find a presentation of this algorithm in the recent book by Moitra [44] but it goes back much further to Harschman [27], where it is presented as a uniqueness result. There are nevertheless important differences between the settings of [12], [13], [44] and of the present paper. In particular, these three works do not phrase tensor decomposition as a decision problem but as an optimization problem which is solved by numerical means (one suggestion from [12], [13] is to perform the simultaneous diagonalization with the extended QZ iteration from [52]; Jennrich's algorithm as presented in [44] relies on pseudoinverse computations and eigendecompositions). All these numerical algorithms attempt to produce a decomposition of a tensor 
 which is close to the input tensor T. If one tries to adapt them to the setting of the present paper there is a fundamental difficulty: given 
 and its decomposition, it is not clear how we can decide whether or not T admits an exact decomposition. This is the main reason why we need to design a new algorithm. As an alternative, one could attempt to run Jennrich's algorithm in symbolic mode. In particular, eigenvalues and the components of eigenvectors would be represented symbolically as elements of a field extension. This leads exactly to the same difficulty as with Kayal's algorithm: as explained in Section 1.1 and in more detail in Section 3, the resulting algorithm might not run in polynomial time because it might lead to computations in field extensions of exponential degree. Note finally that [12], [13], [44] deal with decompositions of ordinary rather than symmetric tensors. Algorithms for symmetric tensor decomposition can be found in the algebraic literature, see e.g. [8], [5]. Like [12], [13], these two papers do not provide any complexity analysis for their algorithms.

1.5. Future work
In the current literature there is apparently no polynomial time algorithm (deterministic or randomized) in the Turing machine model for the following problem: given a homogeneous polynomial 
 of degree  with rational coefficients, decide whether it is equivalent to 
 over . It will be shown in a forthcoming paper that this can be done by extending the tensor-based approach of the present paper to higher degree. This results in a black box algorithm with running time polynomial in n and d [43]. Alternatively, one could try to modify Kayal's equivalence algorithm [37] or provide a better analysis of the existing algorithm. As we have argued in Section 1.1 this has not been done at present even for degree 3. One could also try an approach based on Harrison's work [26] like in Saxena's thesis [48].

More generally, we suggest to pay more attention to absolute circuit reconstruction, i.e., arithmetic circuit reconstruction over .2 Circuit reconstruction over  or over finite fields has a number-theoretic flavor, whereas circuit reconstruction over  or  is of a more geometric nature.

One goal could be to obtain algebraic decision algorithms; as we have explained, this requires the removal of all polynomial factorization subroutines. In principle, this is always possible since the set of polynomials computable by arithmetic circuits of a given shape and size is definable by polynomial (in)equalities, i.e., it is a constructible set (over ) or a semi-algebraic set (over ).3 Another goal would be to obtain good complexity bounds for the Turing machine model when they are not available in the existing literature.

1.6. Organization of the paper
Section 2 is devoted to preliminaries on simultaneous diagonalization, by similarity transformations and especially by congruence. In Section 3 we review Kayal's equivalence algorithm and explain why it does not yield a polynomial time bound in the bit model of computation. We also present our tensor-based approach to the equivalence problem. In Section 4 we give a polynomial time randomized algorithm for equivalence to a sum of cubes based on this approach. We derandomize this algorithm in Section 5. In Section 6 we give a PIT algorithm for polynomials that can be written as sums of d-th powers of linearly independent linear forms. As mentioned before, our algorithm runs in polynomial time in the black box model. We give a randomized algorithm to decide whether a polynomial can be expressed under that form in Section 7 (Proposition 41). The remainder of Section 7 is devoted to the derandomization of several algorithms from [37], [38] related to sums of powers of linear forms. We begin in Section 7.1 with the computation of linear dependencies between polynomials. Then we give applications to the minimization of the number of variables in sums of powers of linear forms (in Section 7.2), and to the computation of Lie algebras of products of linear forms (in Section 7.3). This leads to the derandomization of a factorization algorithm from [41] and of the equivalence algorithm by Kayal [37] described in Section 3.1. The resulting equivalence algorithm runs in polynomial time for every fixed value of the degree d. For the computation of the Lie algebras of products of linear forms, we give as an intermediate result (see Proposition 47 and Remark 48) an identity testing algorithm for a certain class of rational functions (rather than polynomials as is done usually). In the commutative setting, this is to the best of our knowledge the first result of this type.

2. Preliminaries
This section is devoted to preliminaries from linear algebra, and more specifically to simultaneous diagonalization by congruence. We begin with complex symmetric matrices in Section 2.1 and consider real symmetric matrices in Section 2.3. Section 2.2 is devoted to some refinements that are not strictly necessary for our main algorithms (they lead to an interesting connection with semidefinite programming, though; see Theorem 14 and the remarks following it). Upon first reading, if one wishes to understand only our results for the field of complex numbers it will therefore be sufficient to read Section 2.1 only (or even just the statement of Theorem 4; the corresponding result for the field of real numbers is Theorem 10).

A proof of the following lemma for the case of two matrices can be found in [41]. As shown in [42], the general case then follows easily. Note that this lemma is about the “usual” notion of diagonalization (by similarity) rather than by congruence (where one attempts to diagonalize a matrix A by a transformation of the form 
). That is, we say that an invertible matrix T diagonalizes A if 
 is diagonal.

Lemma 1

Let 
 be a tuple of simultaneously diagonalizable matrices with entries in a field , and let  be a finite set of size . Then there exist 
 in S such that any transition matrix which diagonalizes 
 must also diagonalize all of the matrices 
.

See Proposition 11 in Section 2.3 for an improvement of this lemma. In the next lemma we give an explicit description of a set of suitable choices for the tuple 
 in Lemma 1. This description will be needed for a derandomization result in Section 7.3.
Lemma 2

Let 
 be a tuple of simultaneously diagonalizable matrices with entries in a field . There exists a set of at most  hyperplanes of 
 such that the following properties hold for any point 
 which does not belong to the union of the hyperplanes:

(i)
Any eigenvector of 
 must also be an eigenvector of the k matrices 
.

(ii)
Any transition matrix which diagonalizes 
 must also diagonalize all of the matrices 
.

Proof

Since 
 are simultaneously diagonalizable, in order to establish (i) we may as well work in a basis where these matrices become diagonal. Let us therefore assume without loss of generality that 
. For any 
, the matrix 
 is diagonal and its i-th diagonal entry is 
. It therefore suffices to avoid the (proper) hyperplanes of 
 of the form:
 where 
 ranges over all the pairs such that 
. Indeed, there are at most  hyperplanes of this form, and outside of their union there is no “collision of eigenvalues.” More precisely, the eigenspace of 
 associated to the eigenvalue 
 is equal to 
, where 
 denotes the eigenspace of 
 associated to 
. In particular, any eigenvector of 
 is also an eigenvector of 
.

Finally, we show that any point 
 which satisfies (i) also satisfies (ii). For any invertible matrix T, 
 is diagonal iff all the column vectors of T are eigenvectors of 
. By (i) this implies that each column vector of T is also an eigenvector of 
. As a result, the k matrices 
 are all diagonal. □

We note that Lemma 1 directly follows from Lemma 2.(ii) via e.g. the Schwartz-Zippel lemma.
2.1. Simultaneous diagonalization by congruence
The following result is from Horn and Johnson [28]. The first part is just the statement of Theorem 4.5.17(b), and the additional properties in (ii) are established in the proof of that theorem (see [28] for details).

Theorem 3

Let 
 be two complex symmetric matrices of size n with A nonsingular, and let 
.

(i)
C is diagonalizable if and only if there are complex diagonal matrices D and Δ and a nonsingular 
 such that 
 and 
.

(ii)
Moreover, if 
 where S is nonsingular and Λ diagonal then the matrix R in (i) can be taken of the form 
 where V is unitary and commutes with Λ.

The next result generalizes Theorem 3 and provides a solution to the second part of Problem 4.5.P4 in [28].
Theorem 4 Simultaneous diagonalization by congruence

Let 
 be complex symmetric matrices of size n and assume that 
 is nonsingular. The  matrices 
 () form a commuting family of diagonalizable matrices if and only if there are diagonal matrices 
 and a nonsingular matrix 
 such that 
 for all .

Proof

Suppose that 
 where the 
 are diagonal and R nonsingular. Then the matrices 
 indeed form a commuting family of diagonalizable matrices. For the converse, assume that the matrices 
 form such a family. Then these matrices are simultaneously diagonalizable, and by Lemma 1 there is a tuple 
 such that any transition matrix that diagonalizes the matrix
 diagonalizes all of the  matrices 
. Now we apply Theorem 3 to 
 and 
. Write 
 where S is nonsingular and Λ diagonal. By part (ii) of Theorem 3 we can write 
 and 
 where R is nonsingular, D and Δ are diagonal, 
, V commutes with Λ and is unitary. By choice of the tuple α, we can write 
 where 
 is diagonal. We will show that 
 for , thereby completing the proof of the theorem. First, we note that V commutes with the 
. Indeed, V and Λ are simultaneously diagonalizable since they commute and are diagonalizable (Λ is diagonal and V unitary). But any transition matrix which diagonalizes simultaneously V and Λ will diagonalize simultaneously V and the 
 (this follows from the choice of α and the relations 
, 
). These matrices must therefore commute. We can now complete the proof: for  we have
 Since V commutes with 
, 
 as announced. □

2.2. A refinement of Theorem 4
In this section we give a more “invariant” formulation of Theorem 4. Note indeed that this theorem assigns a special role to 
. In Theorem 8 we give a formulation that depends only on the space spanned by the 
 and not on the choice of a specific spanning family 
. Some of the results in this section apply to  as well as .

The role of 
 in Theorem 4 could of course be played by any other invertible matrix in the tuple. As it turns out, for our  matrices the commutation property alone is also independent of the choice of the invertible matrix in the tuple. More precisely, we have:

Proposition 5

Let 
; assume that 
 and 
 are nonsingular. The  matrices 
 () commute if and only if the same is true of the  matrices 
 ().

The proof will use the following simple fact.
Lemma 6

If A and B commute and A is invertible, then 
 and B commute as well.

Proof of Proposition 5

Suppose that the 
 commute. We can write:
 It follows from our hypothesis and from Lemma 6 that the four factors on the right hand side commute. We can therefore rewrite this equation as:
 and now the right hand side is equal to 
. □

Let  be the space of matrices spanned by 
. The matrices 
 commute if and only if 
 is a commuting subspace of 
. With Proposition 5 in hand, we can characterize this property in a way that is completely independent of the choice of a spanning family 
 for .
Theorem 7

Let  be a nonsingular subspace of matrices of 
 (i.e.,  does not contain singular matrices only). The two following properties are equivalent:

(i)
There exists a nonsingular matrix  such that 
 is a commuting subspace.

(ii)
For all nonsingular matrices , 
 is a commuting subspace.

Proof

Since  is nonsingular, (ii) implies (i). For the converse, assume that 
 is a commuting subspace and that 
 is a spanning family of . Let  be a nonsingular matrix. We can add A to our spanning family and apply Proposition 5 to 
, with A playing the role of 
 in that proposition. □

As a result, we can dissociate the commutativity test from the diagonalizability test in Theorem 4:
Theorem 8

Let 
 be complex symmetric matrices of size n and assume that the subspace  spanned by these matrices is nonsingular. The two following properties are equivalent:

(i)
There are diagonal matrices 
 and a nonsingular matrix 
 such that 
 for all .

(ii)
 satisfies the two equivalent properties of Theorem 7, and there exists an invertible  such that the matrices 
 () are all diagonalizable.

Proof

Let  be nonsingular, and suppose that there are diagonal matrices 
 and a nonsingular matrix 
 such that 
 for all i. Note that we have the same form for A, i.e., 
 with Λ diagonal. As a result, we may assume without loss of generality that A is one of the matrices in the tuple 
 (we add it if necessary), and we may even assume that 
. We may then take  by Theorem 4.

Let us now prove the converse. We therefore assume that  satisfies the two properties of Theorem 7, and that  is an invertible matrix such the matrices 
 () are all diagonalizable. From property (ii) in Theorem 7 it follows that the matrices 
 commute. We conclude by applying Theorem 4 to the tuple 
. □

2.3. Real matrices
Here we study the existence of decompositions similar to those of Theorem 3 and Theorem 4 for real matrices. We begin with a real version of Theorem 3.

Theorem 9

Let 
 be two real symmetric matrices of size n with A nonsingular, and let 
.

(i)
C is diagonalizable and has real eigenvalues if and only if there are real diagonal matrices D and Δ and a nonsingular 
 such that 
 and 
.

(ii)
Moreover, if 
 where S is a real nonsingular matrix and Λ diagonal then the matrix R in (i) can be taken of the form 
 where V is orthogonal and commutes with Λ.

Proof

If a decomposition of the pair  as in (i) exists, it is clear that C must be diagonalizable with real eigenvalues since 
. The converse and part (ii) can be obtained by a straightforward adaptation of the proof of Theorem 4.5.17(b) in [28]. □

The next result generalizes Theorem 9 and provides a real version of Theorem 4.

Theorem 10 Simultaneous diagonalization by congruence

Let 
 be real symmetric matrices of size n and assume that 
 is nonsingular. The  matrices 
 () form a commuting family of diagonalizable matrices with real eigenvalues if and only if there are real diagonal matrices 
 and a nonsingular matrix 
 such that 
 for all .

Proof

The proof of Theorem 4 applies almost verbatim: we just need to work everywhere with real matrices instead of complex matrices (and with real coefficients 
), and appeal to Theorem 9 instead of Theorem 3. There is just one point in the proof where a little care is needed. Namely, in the proof of Theorem 4 we used the fact that Theorem 3.(ii) provides us with a unitary matrix V, and that unitary matrices are diagonalizable. In the real case we get an orthogonal matrix instead (as per Theorem 9.(ii)), and orthogonal matrices are not necessarily diagonalizable over . Nevertheless, real orthogonal matrices are diagonalizable over  since they are unitary. We can therefore conclude like in the proof of Theorem 4 that our real orthogonal matrix V commutes with the 
. The remainder of the proof is unchanged. □

In the proofs of Theorem 4, Theorem 10 we have used the fact that V is a unitary matrix. These arguments can be somewhat simplified at the expense of proving the following improvement to Lemma 1. First we recall that the centralizer of a matrix M, denoted , is the subspace of matrices that commute with M.

Proposition 11

Let 
 and 
 be as in Lemma 1; let 
. Then
 

This result applies to real as well as complex matrices. Before giving the proof, let us explain how it can be used in Theorem 4, Theorem 10. Applying Proposition 11 to the tuple of simultaneously diagonalizable matrices 
, we see that
 
 Since 
 and 
, this implies 
. Since , we conclude that V commutes with the 
. Therefore, we have established this commutation property without using the fact that V can be taken unitary.
Proof of Proposition 11

The inclusion from right to left obviously holds for any choice of the 
. For the converse, let  and assume as a first step that B is diagonalizable. Since A and B commute and both matrices are diagonalizable, there exists a transition matrix T such that 
 and 
 are diagonal. By choice of the 
, all of the matrices 
 are diagonal as well. We conclude that B and 
 commute since they are simultaneously diagonalizable. To complete the proof, we just need to observe that diagonalizable matrices are dense in . This follows from the fact that A itself is diagonalizable (observe indeed that the centralizer of a diagonal matrix takes a block-diagonal from, and diagonalizable matrices are dense in each block). □

Like in the complex case, we can dissociate the commutativity test in Theorem 10 from the diagonalizability test:

Theorem 12

Let 
 be real symmetric matrices of size n and assume that the subspace  spanned by these matrices is nonsingular. The two following properties are equivalent:

(i)
There are diagonal matrices 
 and a nonsingular matrix 
 such that 
 for all .

(ii)
 satisfies the two equivalent properties of Theorem 7, and there exists an invertible  such that the matrices 
 () are all diagonalizable with real eigenvalues.

The proof is identical to the proof of Theorem 8, except that we appeal to Theorem 10 instead of Theorem 4. The criterion in Theorem 12 takes a particularly simple form when  contains a positive definite matrix. Before explaining this, we recall the following lemma.
Lemma 13

Let A and B be two real symmetric matrices with B positive definite. Then 
 is diagonalizable with real eigenvalues.

Proof

Since B is positive definite, we can write 
 where H is a real invertible matrix. Hence 
. Since 
 is real symmetric it is diagonalizable with real eigenvalues. This is true of 
 as well since the two matrices are similar. □

As an immediate consequence of Lemma 13 and Theorem 12 we have:

Theorem 14 Simultaneous diagonalization by congruence, positive definite case

Let 
 be real symmetric matrices of size n and assume that the subspace  spanned by these matrices contains a positive definite matrix. The 3 following properties are equivalent:

(i)
There exists a nonsingular matrix  such that 
 is a commuting subspace.

(ii)
For all nonsingular matrices , 
 is a commuting subspace.

(iii)
There are real diagonal matrices 
 and a nonsingular matrix 
 such that 
 for all .

A related characterization can be found in [33, Theorem 3.3]. As we will see at the end of Section 3, the significance of Theorem 14 is that when a polynomial is equivalent to a sum of n real cubes, the corresponding  always contains a positive definite matrix.
3. The equivalence problem
In this section we review Kayal's equivalence algorithm and present our tensor-based approach. We first recall the following definition from the introduction.

Definition 15

A polynomial 
 is said to be equivalent to a sum of n d-th powers if 
 where 
 and 
 is a nonsingular matrix.

As explained before, our equivalence algorithms in Sections 4 and 5 deal only with the case  (equivalence to a sum of cubes). More generally, one could ask whether two forms of degree 3 given as input are equivalent (by an invertible change of variables as above). This problem is known to be at least as hard as graph isomorphism [2], [37] and is “tensor isomorphism complete” [24]. By contrast, equivalence of quadratic forms is “easy”: it is classically known from linear algebra that two real quadratic forms are equivalent iff they have the same rank and signature (this is “Sylvester's law of inertia”) and two quadratic forms over 
 are equivalent iff they have the same rank.4
Note that when , Definition 15 requires the changes of variables matrix A as well as the input polynomial f to be real. It also makes sense to ask if an input 
 is equivalent to a sum of n cubes as a complex polynomial. The following example shows that these are two distinct notions of equivalence.

Example 16

Consider the real polynomial
 This decomposition shows that as a complex polynomial, f is equivalent to a sum of two cubes. Moreover, there is no decomposition as a sum of 2 cubes of real linear forms since the above decomposition is essentially the unique decomposition of f. This follows from Corollary 20 below: in any other decomposition 
, the linear forms 
 and 
 must be scalar multiples of 
 and 
.5 Note that this is very different from the case of degree 2 forms: any real quadratic form in n variables can be written as a linear combinations of n real squares.

A similar example shows that for a polynomial 
 with rational coefficients, equivalence to a sum of two cubes over  or other  are distinct notions:
Example 17

Consider the rational polynomial
 This polynomial is equivalent to a sum of two cubes over  but not over .

3.1. Review of Kayal's equivalence algorithm
Let 
 be a homogeneous polynomial of degree . Recall that the Hessian matrix of f is the symmetric matrix of size n with entries 
, and that the Hessian determinant of f, denoted 
, is the determinant of this matrix. Kayal's equivalence algorithm is based on the factorization properties of the Hessian determinant. Note that 
 for 
. It is shown in [37] that we still have a factorization as a product of linear forms after an invertible change of variables:

Lemma 18

Suppose that 
 where the 
 are linear forms and 
. The Hessian determinant of f is of the form
 
 for some constant c. Moreover,  iff the 
 are linearly independent.

As a consequence we have the uniqueness result of Corollary 20 below, which generalizes Corollary 5.1 in [37]. First, we need the following lemma:
Lemma 19

Suppose that f can be written as in (1) as a sum of powers of linearly independent linear forms. If  then f is not identically zero.

Proof

We already know this for : Lemma 18 shows that 
 is not identically 0. For a more direct proof of the result in this case, one can simply observe that f is equivalent to 
 but 
 is not equivalent to 0.

The general case can be reduced to the case  by setting  of the variables of f to 0. In this way, we obtain a sum of powers of r linear forms 
 in r variables, Moreover, it is always possible to choose the variables of f that are set to 0 so that the 
 remain linearly independent like the forms 
 in (1). Indeed, this follows from the fact that a  matrix of rank r must contain a  submatrix of rank r. □

Corollary 20

Suppose that 
 where the 
 are linearly independent linear forms. For any other decomposition 
 the linear forms 
 must satisfy 
 where 
 is a d-th root of unity and 
 a permutation.

Proof

Consider first the case . By Lemma 18 and uniqueness of factorization we must have 
 for some constants 
 and some permutation π. Plugging this relation into the two decompositions of f shows that:
 
 
 Moving all terms to the left-hand side we obtain:
 
 and Lemma 19 then implies that 
 for all i. Assume now that . Since the 
 are linearly independent, we can extend this family into a family of n linearly independent linear forms 
. Our two decompositions of f yield two decompositions for the polynomial 
, and we can apply the result for the case  to g. Another way to reduce to this case would be to decrease n by setting  of the variables of f to 0 as in the proof of Lemma 19. □

Kayal's algorithm can be summarized by the 3 following steps. It takes as input a degree d form 
 and determines whether f is equivalent to 
 over , where  are two subfields of . If f is equivalent to 
 it determines linearly independent linear forms 
 such that 
. This presentation generalizes slightly [37], which focuses on the case .

1.
Check that the Hessian determinant 
 is not identically 0 and can be factorized in 
 as 
 where the 
 are linear forms and . If this is not possible, reject.

2.
Try to find constants 
 such that
 
 If this is not possible, reject.

3.
Check that all the 
 have d-th roots in . If this is not the case, reject. Otherwise, declare that f is equivalent to 
 over  and output the linear forms 
 where 
 and 
.

The correctness of the algorithm follows from Lemma 18 and Corollary 20. Note in particular that if the algorithm accepts, the forms 
 must be linearly independent (or else 
 would be identically 0 by Lemma 18, and the algorithm would have rejected at step 1); and the constants 
 at step 2 are unique if they exist.
For , or more generally for small degree, the constants 
 at step 2 can be found efficiently by dense linear algebra assuming an algebraic model of computation (for the Turing machine model, see the comments below). For large d we can instead evaluate f and the powers 
 at random points (see Section 7 on linear dependencies or [37] for details).

At step 1, the Hessian determinant can be factorized by Kaltofen's algorithm [35] for the factorization of arithmetic circuits as suggested in [37], or by the black box factorization algorithm of Kaltofen and Trager [34]. These two algorithms assume access to an algorithm for the factorization of univariate polynomials (one of the algorithms in [41] reduces instead to the closely related task of matrix diagonalization). For  one can just assume the ability to factor univariate polynomials as part of our computation model. This yields a polynomial time algorithm, which is clearly not designed to run on a Turing machine. Another option is to take , and we obtain a polynomial time algorithm for the Turing machine model.

Assume now that ,  and that we wish to design again an algorithm for the Turing machine model. As mentioned in Section 1.1, a natural approach would be to factor 
 symbolically at step 1, i.e., to construct an extension 
 of  of finite degree where we can find the coefficients of the linear forms 
. The linear algebra computations of step 2 would then be carried out symbolically in 
. It is not clear that this approach yields a polynomial time algorithm even for  because these computations could possibly take place in an extension of exponential degree (recall indeed that the splitting field of a univariate polynomial of degree r may be of degree as high as r!). We provide polynomial time algorithms for this problem (and for ) in Sections 4 and 5. In order to stay closer to Kayal's original algorithm, a plausible approach would be to stop his algorithm at step 1. Note indeed that step 3 is not necessary for , and it is not immediately clear whether step 2 is necessary. Namely, it is not obvious whether there are polynomials that pass the factorization test of step 1 but fail at step 2. This led us to the following question:

Question 1

Let 
 be a homogeneous polynomial of degree . If the Hessian determinant of f is equal to 
, must f be of the form 
?

A positive answer would yield a polynomial time decision algorithm for the equivalence problem since the existence of a suitable factorization at step 1 can be decided in polynomial time [41]. Representation of polynomials by Hessian determinants has proved to be a delicate topic: see [22] for a famous mistake by Hesse about his eponymous determinant. Hesse's mistake was about polynomials with vanishing Hessian, a topic that remains of interest to this day [29]. One of the authors of [29] came across Question 1 in an earlier version of the present paper, and managed to obtain a negative answer for many cases of interest [53]:  and  even, or  and , or  and  for some . The approach pursued in our paper, based on simultaneous diagonalization by congruence, therefore remains the only way of testing equivalence to a sum of cubes over  in polynomial time. We now present this approach in detail.
3.2. Equivalence by tensor decomposition
In the remainder of this section we explain our approach to the equivalence problem. Like in most of the paper, we work in a field  which is either the field of real or complex numbers. Recall that we can associate to a symmetric tensor T of order 3 the homogeneous polynomial 
. This correspondence is bijective, and the symmetric tensor associated to a homogeneous polynomial f can be obtained from the relation:(3)
 
 The i-th slice of T is the symmetric matrix 
 with entries 
. By abuse of language, we will also say that 
 is the i-th slice of f. Note that (3) is the analogue of the relation
 
 which connects the entries of a symmetric matrix Q to the partial derivatives of the quadratic from 
. Comparing these two equations shows that the matrix of the quadratic form 
 is equal to 
.

Remark 21

The slices of a polynomial of the form(4)
 are the diagonal matrices 
. Conversely, if all the slices of a degree 3 homogeneous polynomial g are diagonal then g must be of the above form (in particular, such a g is equivalent to a sum of n cubes iff the coefficients 
 are all nonzero; this follows from the fact that for , any element of  has a cube root in ). Indeed, the presence of any other monomial in g would yield an off-diagonal term in some slice: for the monomial 
 with  we have 
 and for 
 with all indices distinct we have 
.

In light of Definition 15, it is important to understand how slices behave under a linear change of variables. This was done for symmetric and ordinary tensors in [42, Section 2.1 and Proposition 48]. In particular, for symmetric tensors the following result can be obtained from (3):

Proposition 22

Let g be a degree 3 form with slices 
 and let . The slices 
 of f are given by the formula: 
 where 
 and the 
 are the entries of A. In particular, if g is as in (4) we have 
.

A similar property appears in the analysis of Jennrich's algorithm [44, Lemma 3.3.3]. The action on slices given by the formula 
 in this proposition seems at least superficially related to the action on tuples of symmetric (and antisymmetric) matrices studied by Ivanyos and Qiao [30]. They consider an action of 
 sending a tuple 
 to the tuple 
 where 
. Two tuples are said to be isometric if there exists an invertible matrix A realizing this transformation. Some of the main differences with our setting are:
(i)
The number of elements in our matrix tuples is the same as the dimension n of the matrices, but in their setting m and n are unrelated.

(ii)
The matrices in our tuples must come from a symmetric tensor but they allow arbitrary tuples of symmetric matrices.

(iii)
They act independently on each component of a matrix tuple, whereas we “mix” components with the transformation 
. In spite of this difference, the actions on the space of matrices spanned by the tuple's components are the same, see Lemma 23 below.

Also we note that their algorithm for isometry testing is not algebraic since it requires the construction of field extensions as explained e.g. in the paragraph on the representation of fields and field extensions in [30].6
Lemma 23

Let 
 and 
 be two forms of degree 3 such that  for some nonsingular matrix A.

(i)
If  and  denote the subspaces of 
 spanned respectively by the slices of f and g, we have 
.

(ii)
In particular, for 
 the subspace  is the space of diagonal matrices and  is a nonsingular subspace, i.e., it is not made of singular matrices only.

Proof

Proposition 22 shows that 
. Since 
, the same argument shows that 
. The inclusion 
 therefore cannot be strict. The second part of the lemma follows immediately from the first and from Remark 21. □

For the next theorem, we recall from the beginning of Section 3.2 that one may take either  or .
Theorem 24

A degree 3 form 
 is equivalent to a sum of n cubes if and only if its slices 
 span a nonsingular matrix space and the slices are simultaneously diagonalizable by congruence, i.e., there exists an invertible matrix 
 such that the n matrices 
 are diagonal.

Proof

Let  be the space spanned by 
. If f is equivalent to a sum of n cubes, Proposition 22 shows that the slices of f are simultaneously diagonalizable by congruence and Lemma 23 shows that  is nonsingular.

Let us show the converse. Since the slices are simultaneously diagonalizable by congruence, there are diagonal matrices 
 and a nonsingular matrix 
 such that 
 for all . Let 
. By Proposition 22 the slices of g are linear combinations of the 
, i.e., they are all diagonal. By Remark 21, g must be as in (4). It therefore remains to show that the coefficients 
 are all nonzero. This must be the case due to the hypothesis on . Indeed, this hypothesis implies that the matrix space  spanned by the slices of g is nonsingular (apply again Lemma 23, this time in the other direction). But if some 
 vanishes,  is included in the space of diagonal matrices with a 0 in the i-th diagonal entry. □

Corollary 25

Let f be a degree 3 form with slices 
 and assume that 
 is nonsingular. Then f is equivalent to a sum of n cubes if and only if the  matrices 
 () commute and are diagonalizable over .

Proof

This follows from Theorem 24 as well as Theorem 4 for  and Theorem 10 for . □

We conclude this section with an alternative characterization of equivalence to a sum of cubes for the field of real numbers.

Theorem 26

Let f be a real form of degree 3 and let  be the subspace of 
 spanned by the slices of f. The 3 following properties are equivalent:

(i)
f is equivalent as a real polynomial to a sum of n cubes.

(ii)
There exist two invertible matrices  such that 
 is a commuting subspace and B is positive definite.

(iii)
 contains a positive definite matrix, and 
 is a commuting subspace for any invertible matrix .

Proof

Suppose that f is equivalent to a sum of n cubes, i.e., 
 where 
 is invertible. We have seen in Lemma 23 that the slices of 
 span the space  of diagonal matrices, and that those of f span 
. The latter span contains the positive definite matrix 
. Moreover, according to Proposition 22 the slices of f are simultaneously diagonalizable by congruence. By Theorem 12, 
 is a commuting subspace for any invertible matrix . Hence we have shown that (i) implies (iii). That (iii) implies (ii) is clear since  is nonsingular (by hypothesis, it contains a positive definite matrix). Finally, let us show that (ii) implies (i). By hypothesis,  contains a positive definite matrix B hence we can apply Theorem 14. It follows that the slices are simultaneously diagonalizable by congruence. By Theorem 24, f must be equivalent to a sum of n cubes. □

Compared to Theorem 24 or Corollary 25, Theorem 26 does not involve any diagonalizability test. One can check that  contains a positive definite matrix using semi-definite programming. Unfortunately, no efficient algebraic algorithm is known for semi-definite programming (famously, this is already an open problem for linear programming). For this reason, the equivalence algorithms of this paper will be based on Corollary 25 rather than Theorem 26.
4. Randomized equivalence algorithm
As a test for equivalence to a sum of n cubes, Corollary 25 is not quite satisfactory due to the hypothesis on 
 (note indeed that this hypothesis is not even satisfied by 
). This restriction can be overcome by performing a random change of variables before applying Corollary 25. This yields the following simple randomized algorithm with one-sided error. The input is a degree 3 form 
. We recall from Section 3.2 that  or  (except in Proposition 27 where any field of characteristic 0 is allowed).

1.
Pick a random matrix 
 and set .

2.
Let 
 be the slices of h. If 
 is singular, reject. Otherwise, compute 
.

3.
If the matrices 
 commute and are all diagonalizable over , accept. Otherwise, reject.

Before proving the correctness of this algorithm, we explain how the diagonalizability test at step 3 can be implemented efficiently with an algebraic algorithm. This can be done thanks to the following classical result from linear algebra (see e.g. [28, Corollary 3.3.8] for the case ).
Proposition 27

Let  be a field of characteristic 0 and let 
 be the characteristic polynomial of a matrix 
. Let 
 be the squarefree part of 
. The matrix M is diagonalizable over 
 
 iff 
. Moreover, in this case M is diagonalizable over  iff all the roots of 
 lie in .

Over the field of complex numbers it therefore suffices to check that 
. Over , we need to check additionally that all the roots of 
 are real. This can be done for instance with the help of Sturm sequences, which can be used to compute the number of roots of a real polynomial on any real (possibly unbounded) interval. Alternatively, the number of real roots of a real polynomial can be obtained through Hurwitz determinants [46, Corollary 10.6.12], and is given by the signature of the Hermite quadratic form [3, Theorem 4.48]. The arithmetic cost of these methods is polynomially bounded, and they can also be implemented to run in polynomial time in the bit model.7
Theorem 28

If an input 
 is accepted by a run of the above randomized algorithm then f must be equivalent to a sum of n cubes.

Conversely, if f is equivalent to a sum of n cubes then f will be accepted with high probability over the choice of the random matrix R at step 1. More precisely, if the entries 
 are chosen independently at random from a finite set S the input will be accepted with probability at least .

Proof

Assume that f is accepted for some choice of 
. Since 
 is invertible, it follows from Proposition 22 that R must be invertible as well. Moreover, h must be equivalent to a sum of n cubes by Corollary 25. The same is true of f since 
.

For the converse, assume that f is equivalent to a sum of n cubes. We can obtain the slices 
 of h from the slices 
 of f by Proposition 22, namely, we have 
 where 
 and the 
 are the entries of R. Therefore 
 is invertible iff R and 
 are invertible. By Lemma 23.(ii) there is a way to choose the entries 
 so that 
 is invertible. In fact, 
 will be invertible for most choices of these entries. This follows from the fact that as a polynomial in the entries 
, 
 is not identically zero. Therefore, by the Schwartz-Zippel lemma 
 will fail to be invertible with probability at most . Likewise, R will fail to be invertible with probability at most  and the result follows from the union bound. □

Remark 29

In Theorem 28 and in the corresponding algorithm, we can reduce the amount of randomness by picking random matrices R of the following special form: R is lower triangular with 1's on the diagonal (except possibly for 
), 
 are drawn independently and uniformly from S, and all the other entries are set to 0. The same analysis as before shows that 
 will fail to be invertible with probability at most . Moreover, R will fail to be invertible with probability at most  since 
. By the union bound, f will be accepted with probability at least .

We will use a similar construction in the deterministic algorithm of the next section.
5. Deterministic equivalence algorithm
In the analysis of our randomized algorithm we have invoked the Schwartz-Zippel lemma to argue that a polynomial of the form
 does not vanish for most of the random choices 
 (recall from the proof of Theorem 28 that 
 denoted the slices of f). In this section we will obtain our deterministic equivalence algorithm by derandomizing this step. Namely, we will use the fact that we are not trying to solve an arbitrary instance of symbolic determinant identity testing: as it turns out, the polynomial H can be factored as a product of linear forms. This fact was already at the heart of Kayal's equivalence algorithm. Indeed, his algorithm is based on the factorization of the Hessian determinant of f [37, Lemma 5.2] and as pointed out in [42], the symbolic matrix 
 is a constant multiple of the Hessian. The point where we depart form Kayal's algorithm is that we do not explicitly factor H as a product of linear forms (recall indeed that this is not an algebraic step). Instead, we will use the existence of such a factorization to find deterministically a point where H does not vanish. We can then conclude as in the previous section.

First, we formally state this property of H as a lemma and for the sake of completeness we show that it follows from Proposition 22 (one can also make this argument in the opposite direction, see Section 2.1 of [42] for details).

Lemma 30

Let f be a degree 3 form with slices 
 and let 
. If f is equivalent to a sum of n cubes then H is not identically 0 and can be factored as a product of n linear forms.

Proof

Let A be the invertible matrix such that 
. By Proposition 22, 
 where
 
 This gives the required factorization. In particular, H is nonzero since A is invertible. □

The non vanishing of H means that the slices span a nonsingular matrix space. We have given in Theorem 24 a slightly different proof of the fact that this space is indeed nonsingular when f is equivalent to a sum of n cubes. By Lemma 30, the zero set of H is a union of n hyperplanes. We can avoid the union of any finite number of hyperplanes by a standard construction involving the moment curve 
.
Lemma 31

Let 
 be a set of  points on the moment curve. For any set of p hyperplanes 
 there is at least one point of M which does not belong to any of the 
.

Proof

Let 
 be the equation of 
. The moment curve has at most  intersections with 
 since 
 is a nonzero polynomial of degree . For the p hyperplanes we therefore have a grand total of  intersection points at most. □

The size of M in this lemma is the smallest that can be achieved in such a blackbox construction. Indeed, for any set of  points one can always find a set of p hyperplanes which covers them all.
We can now describe our deterministic algorithm. As in Section 4 the input is a degree 3 form 
 with slices 
.

1.
Pick an arbitrary set M of  points on the moment curve.

2.
Enumerate the elements of M to find a point 
 such that the matrix 
 is invertible. If there is no such point, reject.

3.
Construct the following matrix 
: R is lower triangular with 1's on the diagonal, 
 and all the other entries are set to 0.

4.
Compute , the slices 
 of h and 
.

5.
If the matrices 
 commute and are all diagonalizable, accept. Otherwise, reject.

Theorem 32

A degree 3 form 
 is accepted by the above algorithm if and only if f is equivalent to a sum of n cubes.

Proof

As a preliminary observation, we note that if the algorithm reaches step 4 the matrix 
 is well-defined since 
 is invertible. Indeed, we have seen in the proof of Theorem 28 that 
; moreover, 
 is invertible since the algorithm has not failed at step 2 and R is clearly invertible as well.

Suppose now that an input 
 is accepted by the algorithm. The same argument as in the proof of Theorem 28 shows that f is equivalent to a sum of n cubes. Namely, h must be equivalent to a sum of n cubes by Corollary 25. The same is true of f since R is an invertible matrix.

For the converse, suppose that an input 
 is equivalent to a sum of n cubes. By Lemma 30 and Lemma 31, there exists a point  where the polynomial 
 does not vanish. As a result, the algorithm will not reject at step 2. Since the matrix R constructed at step 3 is invertible, the polynomial h at step 4 is equivalent to a sum of n cubes and the algorithm will accept at step 5 by Corollary 25. □

Remark 33

Some of the results in the paper by Ivanyos and Qiao [30] mentioned after Proposition 22 are motivated by an application to symbolic determinant identity testing (SDIT). In our setting we only need to consider very simple determinants (as explained at the beginning of this section, they factor as a product of linear forms). As a result we can use the simple black box solution provided by Lemma 31. More connections between group actions and SDIT can be found in [21], [31], [32].

6. Polynomial identity testing
It is a basic fact that black box PIT for a class of polynomials  is equivalent to constructing a hitting set for , i.e., a set of points H such that every polynomial in  which vanishes on all points of H must vanish identically. Indeed, from a hitting set we obtain a black box PIT algorithm by querying the input polynomial f at all points of H. Conversely, for any black box PIT algorithm the set of points queried on the input  must form a hitting set. Note that the validity of this simple argument depends on the hypothesis that  (otherwise we can declare that  without making any query).

In this section we first consider the following scenario. An algorithm is provided with black box access to a polynomial f that is either identically 0 or equivalent to 
, and must decide in which of these two categories its input falls (note that these are indeed two disjoint cases). This is equivalent to constructing a hitting set for the equivalence class of 
, a task that we carry out in Section 6.1. Then in Section 6.2 we generalize this hitting set construction to a larger class of polynomials, namely, those that can be written as sums of d-th powers of linearly independent linear forms.

6.1. A hitting set for the equivalence class of 
Here we construct a polynomial size hitting set for the set of polynomials 
 that are equivalent to 
. Let 
 be a set of  complex numbers with 
. We denote by 
 the set of points 
 with 
 and all other coordinates equal to 0. Pick an arbitrary nonzero point 
, and form the set 
, of size .

Proposition 34

For any  and any nonzero point 
, 
 is a hitting set for the set of polynomials 
 that are equivalent to 
.

Proof

Let 
 where A is an invertible matrix. The proof is based on the fact that the gradient 
 does not vanish anywhere except at . Indeed, this property clearly holds true for 
, and is preserved by an invertible change of variables since(5)
 by the chain rule. In particular,  since . This implies that f does not vanish on all of 
. Indeed, if f vanishes on 
 then f vanishes on the whole line going through 
, hence 
 
. □

Let K be a subfield of . The same construction yields a hitting set for the set of polynomials in 
 that are equivalent to a polynomial of the form 
 where 
. Such polynomials are indeed equivalent to 
 as complex polynomials.

6.2. Fewer powers
We now give a black box PIT algorithm for a bigger class of polynomials, namely, those that can be written as sums of d-th powers of linearly independent linear forms. These polynomials therefore admit decompositions as in (1) where the forms 
 are linearly independent and the number  of forms in the decomposition is unknown. We will generalize the approach from Section 6.1. In particular, we will see that the gradient of f does not vanish outside of a certain (unknown) linear subspace V. We can find deterministically a point outside of V with the help of the moment curve like in Section 5. This leads to the construction of the following hitting set: for an arbitrary set M of n points on the moment curve, we construct the union of the 
's as p ranges over M (recall that 
 is the hitting set of Section 6.1). This set 
 is of size .

Theorem 35

For any , G is a hitting set for the set of polynomials 
 that can be written as sums of d-th powers of linearly independent linear forms.

Proof

Suppose that f can be written as a sum of r d-th powers for some . We have 
 where A is an invertible matrix and
 We will use the fact that the gradient of f vanishes only on a (proper) linear subspace V of dimension . This property clearly holds true for 
, and it is preserved for f since we now have
 instead of (5). By Lemma 31, M contains at least one point p lying outside of V. At this point , and the same argument as in the proof of Proposition 34 shows that f does not vanish on 
. □

7. Linear dependencies, essential variables and Lie algebras
In this section we build on the results from Section 6 to derandomize several algorithms from [37], [38]. We begin in Section 7.1 with the computation of linear dependencies between polynomials. Then we give applications to the minimization of the number of variables in sums of powers of linear forms (in Section 7.2), and to the computation of Lie algebras of products of linear forms (in Section 7.3). This leads to the derandomization of a factorization algorithm from [41] and of the equivalence algorithm by Kayal [37] described in Section 3.1.

7.1. From black box PIT to linear dependencies
We first recall from [37] the notion of linear dependencies among polynomials. It has found applications to the elimination of redundant variables [37], the computation of the Lie algebra of a polynomial [38], the reconstruction of random arithmetic formulas [25], full rank algebraic programs [39] and non-degenerate depth 3 circuits [40].

Definition 36

Let 
 be a tuple of m polynomials of 
. The space of linear dependencies of f, denoted 
, is the space of all vectors 
 such that 
 is identically 0.

As a computational problem, the POLYDEP problem consists of finding a basis of 
 for a tuple f given as input. If the 
 are verbosely given as sum of monomials, this is a simple problem of linear algebra. The problem becomes more interesting if the 
 are given by arithmetic circuits or black boxes. In Section 7.1 we present a simple and general relation between this problem and black box PIT.
A natural approach to POLYDEP consists of evaluating the 
 at certain points 
 of 
 to form a  matrix M with the 
 as entries. Note that 
 for any choice of the evaluation points. We would like this inclusion to be an equality since this will allow to easily compute a basis of 
. This motivates the following definition.

Definition 37

The points 
 form a hitting set for the linear dependencies of f if the above matrix 
 satisfies 
.

Kayal [37] showed (without using explicitly this terminology) that if  and the 
 are chosen at random, a hitting set for the linear dependencies of f will be obtained with high probability.
Here we point out that constructing deterministically a hitting set for the linear dependencies of f is equivalent to solving black box PIT for the family of polynomials in  (the space of linear combinations of 
):

Proposition 38

Let 
 be a tuple of m polynomials of 
. For any tuple 
 of k points of 
, the two following properties are equivalent:

(i)
The points 
 form a hitting set for the linear dependencies of f.

(ii)
They form a hitting set for .

Proof

This is immediate from the definitions. Suppose indeed that (i) holds, and that some polynomial 
 of  vanishes at all of the 
. This means that , hence 
 by (i). We conclude that f is identically 0 and (ii) holds.

To prove the converse we can take the same steps in reverse. Suppose that (ii) holds and that . This means that 
 vanishes at all the 
, hence f is identically 0 by (ii). We have shown that 
, i.e., 
. □

In Section 7.2 we will use this observation and the black box PIT algorithm of Section 6.2 to minimize the number of variables in sums of powers of linearly independent linear forms. In Section 7.3 we give an application to the computation of Lie algebras and factorization into products of linear forms.
7.2. Minimizing variables
We first recall the notion of redundant and essential variables studied by Carlini [9] and Kayal [37].

Definition 39

A variable 
 in a polynomial 
 is redundant if f does not depend on 
, i.e., 
 does not appear in any monomial of f.

We say that f has t essential variables if t is the smallest number for which there is an invertible matrix of size n such that  depends on t variables only.

A randomized algorithm for minimizing the number of variables is given in [37, Theorem 4.1]. More precisely, if the input f has t essential variables the algorithm finds (with high probability) an invertible matrix A such that  depends on its first t variables only. It is based on the observation from [9], [37] that 
 where ∂f denotes the tuple of n partial derivatives 
 (and  denotes the dimension of the spanned subspace). As recalled in Section 7.1, a basis of the space of linear dependencies 
 can be found by a randomized algorithm from [37]. Moreover, a suitable invertible matrix A is easily found from such a basis by completing it into a basis of the whole space 
 (see appendix B of [37] for details).
Example 40

If f can be written as a sum of r powers of linearly independent linear forms then the number of essential variables of f is equal to r. This is clear for 
 since ∂f is spanned by 
. In the general case, f is equivalent to 
 and two equivalent polynomials have the same number of essential variables.

The next proposition is a consequence of the above variable minimization algorithm. The input f to the algorithm of Proposition 41 can be described by an arithmetic circuit like in [37] or more generally by a black box. Here we assume (in contrast with Sections 4 and 5) that we have access to an oracle for the factorization of univariate polynomials. This is a prerequisite for running Kaltofen's factorization algorithms for the arithmetic circuit [35] and black box models [34].
Proposition 41

There is a randomized polynomial time algorithm that decides whether a homogeneous polynomial 
 can be written as in (1) as a sum of powers of linearly independent linear forms.

Proof

First compute the number r of essential variables in f using the randomized algorithm from [37], and make the corresponding change of variables to obtain a polynomial 
. Then test whether g is equivalent to 
 using the equivalence algorithm from [37]. To prove that this algorithm is correct, we show that f can be written as a sum of r powers of linearly independent linear forms if and only if g can be written in such a form. Let 
 be an invertible matrix such that 
 for all 
. Suppose that 
 for some invertible matrix 
. If we denote  
 
 where 
 is the identity matrix, then the matrix AC is invertible and we have 
 for every 
. Conversely, suppose that there exists an invertible matrix 
 such that 
. We have 
 by Example 40. Moreover, for all 
 we have 
. Thus, by setting 
, we get 
 for all 
, where 
 is the submatrix of 
 obtained by taking the first r rows and columns. To show that C is invertible, suppose that this is not the case. Then, there exists an invertible matrix 
 such that 
. Hence, for every 
 we have 
, where 
. In particular, h has less than r essential variables, which gives a contradiction with Example 40. Therefore, the matrix C is invertible. □

In this algorithm it is essential to compute the number of essential variables in f before calling the equivalence algorithm from [37]. Indeed, this algorithm is based on the factorization of the Hessian determinant of f; but 
 is identically 0 for any polynomial with fewer than n essential variables. Hence looking at 
 does not yield any useful information for .
Remark 42

We can minimize the number of variables of a degree 3 form 
 in deterministic polynomial time using dense linear algebra. Indeed, as pointed out in Section 7.1 this is true more generally for the POLYDEP problem with inputs that are verbosely given as sums of monomials.8 Combining this observation with the deterministic equivalence algorithm from Section 5 we obtain, as in Proposition 41, a deterministic algorithm to decide whether a degree 3 form can be written as in (1) as a sum of cubes of linearly independent (real or complex) linear forms.

The second result of this section is the following derandomization of Kayal's algorithm for finding the number of essential variables, under the assumption that the input polynomial is a sum of powers of independent linear forms:

Theorem 43

Let 
 be a homogeneous polynomial of degree d and let 
 be the hitting set of Theorem 35 corresponding to polynomials of degree  in n variables (recall that it is of size 
). Let 
 be the partial derivative 
. We consider like in Section 7.1 the matrix 
.

If f can be written as in (1) as a sum of r powers of linearly independent linear forms then 
. In particular, the number of essential variables of such an f can be computed deterministically from a black box for f by the formula: .

Proof

We recall that a black box for 
 can be easily obtained from a black box for f by polynomial interpolation. It therefore remains to show that 
. By Proposition 38 it suffices to show that 
 is a hitting set for . This is clear from the definition of 
 since the elements of  can be written as linear combinations of at most r -th powers of linearly independent linear forms (namely, the same forms that appear in the decomposition of f). □

7.3. Lie algebras and polynomial factorization
One can associate to a polynomial 
 the group of invertible  matrices A that leave f invariant, i.e., such that . One can in turn associate to this matrix group its Lie algebra. This is a linear subspace of 
, which we call simply “the Lie algebra of f.” It turns out that elements of this Lie algebra correspond to linear dependencies between the 
 polynomials 
 
. A proof can be found in [38, Section 7.2], and we will take this characterization as our definition of the Lie algebra for the purpose of this paper:

Definition 44

The Lie algebra of a polynomial 
 is the subspace of all matrices 
 that satisfy the identity: 
 
 

A randomized algorithm for the computation of the Lie algebra was given in [38], with applications to the reconstruction of affine projections of polynomials. In this section we study the deterministic computation of Lie algebras of polynomials, a topic which has not been studied in the literature as far as we know.
The Lie algebra of a homogeneous polynomial f consists of all matrices of 
 if and only if f is identically 0. This shows that one cannot hope to compute the Lie algebra in deterministic polynomial time without derandomizing Polynomial Identity Testing (and these two problems are in fact equivalent in the black box setting by Proposition 38). Nevertheless, it makes sense to search for deterministic algorithms for specific classes of polynomials. We take a first step in this direction in Theorem 49, for polynomials that factor as products of linear forms. Taking again our cue from Proposition 38, we will do this by constructing a hitting set for a related family of polynomials. As it turns out, it is convenient to first design a hitting set for a certain family of “simple” rational functions. Those are defined as follows:

Definition 45

Let 
 be a collection of 2m vectors in 
. Furthermore, let 
.

We associate to this collection of 2m vectors an oracle which for any 
 returns the value(6)
 
 
 

In the commutative setting, there does not seem to be a lot of literature on rational identity testing (there is however the deep result that rational identity testing can be done in deterministic polynomial time in the non-commutative setting [21]). For (commutative) arithmetic circuits with divisions, deterministic rational identity testing is easily seen to be equivalent to PIT for ordinary (division free) arithmetic circuits. Nevertheless, it makes sense to investigate it for specific families of rational functions such as those in Definition 45.
Remark 46

According to the above definition, the oracle returns NaN when we evaluate f on a point x where 
, and this remains true even if the corresponding vector 
 is equal to 0. This convention is useful for the proof of Proposition 47 below.

For every , let 
 be the set of  points defined as
 Recall from Lemma 31 that these points cannot be all contained in a union of k hyperplanes since they lie on the moment curve. Moreover, for every  let
 The next result shows that the set  is a hitting set for the rational functions of Definition 45.

Proposition 47

The function  in (6) is equal to 0 for every  if and only if  for every .

Proof

The “only if” implication is trivial. To prove the other direction, suppose that  for every . First we observe that it is enough to assume that 
 are pairwise linearly independent. Indeed, if 
 for some  and , then we can put 
, replace 
 by 
 and forget 
 and 
. This does not change the function f (in particular, by Remark 46 the domain of definition of f is unchanged). By repeating this procedure, we can write f in such a way that the denominators are pairwise linearly independent (and their number m does not increase).

From now on, we assume that 
 are pairwise linearly independent. By Lemma 31, there exists  such that 
 for all . For every , denote 
. We will show that 
 for all . To do so, suppose that there exists at least one i such that 
. For every pair 
 such that  let 
. Using Lemma 31 one more time, there exists 
 satisfying the following two conditions:

(i)
 for every  such that 
;

(ii)
 for every  such that . (We note that 
 because the 
 are pairwise linearly independent.)

Let 
 for all  and consider the univariate function  defined as . Note that for every  such that  we have
 
 
 
 
 
 Observe that the function  attains the value NaN for at most m values of λ. Furthermore, since we assumed that 
, the functions 
 have distinct zeros. In particular, if 
, then  approaches +∞ as λ approaches 
. Since we assumed that at least one 
 is nonzero, it follows that the function  attains some values not in . Moreover,  has at most m zeroes, because it can be written in the form  where  are nonzero polynomials of degree at most m. In particular, at least one of the values  does not belong to , contradicting our assumption. Therefore, we have 
 for all . In particular, for every  we have 
. To conclude, we observe that  since . It follows that 
 for all . □
Remark 48

One can derive from the above proof a syntactic characterization of the rational functions in Definition 45 that are identically 0. Namely, assuming that the 
 are pairwise linearly independent, the following condition is necessary and sufficient: there exist constants 
 such that 
 and 
 for all .

Let 
 be a polynomial that factors as a product of linear forms, i.e., 
 for some vectors 
 in 
. From Proposition 47 we can derive the following characterization of the Lie algebra of P.
Theorem 49

Let 
 be a polynomial of degree  that factors as a product of linear forms. Then, a matrix 
 belongs to the Lie algebra of P if and only if(7) 
 
 
 for every . In particular, a basis of the Lie algebra can be computed deterministically in polynomial time with black box access to P.

Proof

The “only if” direction follows immediately from Definition 44. To prove the opposite implication, let us now assume that (7) holds for every . We need to show that C belongs to the Lie algebra of f. Let us write 
 where the 
 are nonzero vectors, and consider the function
 
 
 
  Here  denotes the union of the m hyperplanes 
 as in Definition 45. Note that we have 
 for every . Furthermore, observe that for every  we have
 
 
 
 
 
 
 
 
 Since this rational fraction is of form (6), we can apply Proposition 47 and conclude that 
 is equal to zero for every . This implies that 
 
 for all . By continuity we obtain 
 
 for all 
, which implies that C belongs to the Lie algebra of P.

Let us now turn to the second part of the theorem. It is well known that a black box for 
 can be constructed from a black box for P (by interpolating P on a line). By the first part, the determination of the Lie algebra therefore boils down to the resolution of a system of  linear equations in 
 variables. □

Remark 50

We have stated Proposition 47 and Theorem 49 for the field of complex numbers only because the proof of Proposition 47 uses the absolute value. Nevertheless, it follows from general principles that these two results apply to any field K of characteristic 0. Indeed, K can be embedded in an algebraically closed field 
 
 which must satisfy the same first order formulas as .

For our final derandomization results we will need to perform simultaneous diagonalization in polynomial time over .

Proposition 51

There is a polynomial time deterministic algorithm which takes as input a tuple 
 of matrices of size n with rational entries, and:

(i)
decides whether 
 are simultaneously diagonalizable over ;

(ii)
if they are, constructs an invertible matrix 
 such that the k matrices 
 are all diagonal.

This result is not particularly surprising but we could not find it in the literature.
Proof

For  this is quite standard. We can for instance compute the characteristic polynomial of 
, compute its roots (which should all be rational) and attempt to construct a basis of eigenvectors by solving the corresponding linear systems.

For , we can check that the matrices are simultaneously diagonalizable by checking that each matrix is diagonalizable, and that the 
 pairwise commute (Theorem 1.3.21 in [28]). In this case, in order to construct a transition matrix T which diagonalizes the 
, we will use Lemma 2 to reduce to the case . Namely, we will construct a finite set S of points 
, and for each  we will:

1.
Diagonalize 
 as: 
, where 
 is diagonal and 
 invertible.

2.
Check whether 
 is diagonal for all .

When the 
 are simultaneously diagonalizable, Lemma 2.(ii) guarantees that this test will succeed for at least one  if S is not included in a certain union of  hyperplanes. In order to avoid these hyperplanes we can proceed as in Section 5 and pick any set S of  points on the moment curve as per Lemma 31. □
An alternative to the above algorithm can possibly be extracted from the proof of Theorem 1.3.21 in [28].
Let 
 be a polynomial that can be written as(8)
 where λ is a constant, the 
 are linearly independent linear forms and the exponents 
 are all nonzero. A randomized algorithm which finds such a factorization from black box access to f was proposed in [41, Section 4]. This algorithm appealed to randomization for the computation of the Lie algebra of f and also, following [38], for simultaneous diagonalization. In this paper we have given deterministic algorithms for these two tasks, in Theorem 49 and Proposition 51 respectively. This leads to the following polynomial time deterministic factorization algorithm, which we call the derandomized Lie-algebraic factorization algorithm (or DerandLie for short):

1.
Compute a basis 
 of the Lie algebra of f.

2.
Reject if , i.e., if the Lie algebra is not of dimension .

3.
Check that the matrices 
 commute and are all diagonalizable over . If this is not the case, reject. Otherwise, declare the existence of a factorization 
 where the linear forms 
 are linearly independent and 
 (λ, the 
 and 
 will be determined in the last 3 steps of the algorithm).

4.
Perform a simultaneous diagonalization of the 
's, i.e., find an invertible matrix A such that the  matrices 
 are diagonal.

5.
At the previous step we have found a matrix A such that 
 has a Lie algebra 
 which is an -dimensional subspace of the space of diagonal matrices. Then we compute the orthogonal of 
, i.e., we find a vector 
 such 
 is the space of matrices 
 satisfying 
. We normalize α so that 
.

6.
We must have  where 
⁎
 and m is the monomial 
 (in particular, α must be a vector with integral entries). We therefore have  and we output this factorization.

Theorem 52

Let 
 be a polynomial that can be written as in (8) as a product of powers of linearly independent linear forms, where λ and the coefficients of the linear forms are in . From a black box for f, the above DerandLie algorithm computes this factorization deterministically in polynomial time.

See [41, Section 4] for a correctness proof. As mentioned above, in order to obtain a deterministic algorithm we appeal to Theorem 49 in Step 1 and to Proposition 51 in Step 4. In order to find the scaling factor λ at step 6, we evaluate f at a point x where . From Lemma 31, we can find such a point deterministically by trying at most  on the moment curve since we need to avoid the n hyperplanes 
. Note that DerandLie may fail if f does not factor as a product of linear forms since this is a prerequisite of Theorem 49. The fact that DerandLie fails on some inputs may seem at first sight like a weakness of the algorithm, but this is in fact unavoidable for any deterministic polynomial-time black box algorithm (see [41, Section 1.5] for details).
Recall from Section 3.1 that Kayal's algorithm for equivalence to a sum of powers relies on factorization into products of linear forms. If this factorization is performed with the DerandLie algorithm, we obtain a deterministic version of Kayal's algorithm. Let us call LieEquivalence this deterministic equivalence algorithm. As our final result, we observe that LieEquivalence will work correctly on all inputs due to the presence of the verification step in Kayal's algorithm:

Theorem 53

Let 
 be a homogeneous polynomial of degree d given verbosely as a sum of monomials. The LieEquivalence algorithm determines whether f is equivalent over  to 
, the “sum of d-th powers” polynomial from (2). If this is the case, it outputs an invertible matrix A with rational entries such that 
. Moreover, for any fixed d the algorithm runs in polynomial time in the Turing machine model.

Proof

Since we are interested in equivalence over the field of rational numbers, we will run the 3-step algorithm from Section 3.1 with . First we establish the correctness of LieEquivalence. If the algorithm accepts its input f, it explicitly finds at step 2 and step 3 linearly independent linear forms 
 such that 
. The algorithm's answer must therefore be correct in this case. Conversely, assume that such a decomposition exists. Then the Hessian determinant 
 factors as 
 where c is a nonzero constant. Since the 
 are linearly independent, we are in the situation where DerandLie works correctly. Therefore, by Theorem 52 we will find the 
 (or actually constant multiples 
 of the 
) at step 1 of the algorithm of Section 3.1. Finally, the decomposition 
 is obtained at steps 2 and 3.

We now turn to the algorithm's complexity. Since DerandLie runs in polynomial time, the first step of the algorithm from Section 3.1 will also run in polynomial time. At step 2 we can afford to expand the powers 
 as sums of monomials (this takes polynomial time for constant d), and then we find the constants 
 by dense linear algebra. Finally, the extraction of d-th roots of rational numbers at step 3 also takes polynomial time. □