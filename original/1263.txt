Abstract
Multi-column dependencies in relational databases come associated with two different computational tasks. The detection problem is to decide whether a dependency of a certain type and size holds in a given database, the discovery problem asks to enumerate all valid dependencies of that type. We settle the complexity of both of these problems for unique column combinations (UCCs), functional dependencies (FDs), and inclusion dependencies (INDs).

We show that the detection of UCCs and FDs is W[2]-complete when parameterized by the solution size. The discovery of inclusion-wise minimal UCCs is proven to be equivalent under parsimonious reductions to the transversal hypergraph problem of enumerating the minimal hitting sets of a hypergraph. The discovery of FDs is equivalent to the simultaneous enumeration of the hitting sets of multiple input hypergraphs.

We further identify the detection of INDs as one of the first natural W[3]-complete problems. The discovery of maximal INDs is shown to be equivalent to enumerating the maximal satisfying assignments of antimonotone, 3-normalized Boolean formulas.

Keywords
Data profiling
Enumeration complexity
Functional dependency
Inclusion dependency
Parameterized complexity
Parsimonious reduction
Transversal hypergraph
Unique column combination
W[3]-completeness

1. Introduction
Data profiling is the extraction of metadata from databases. An important class of metadata in the relational model are multi-column dependencies. They describe interconnections between the values stored for different attributes or even across multiple instances. Arguably the most prominent type of such dependencies are the unique column combinations (UCCs), also known as candidate keys. These are collections of attributes such that the value combinations appearing in those attributes identify all rows of the database. In Fig. 1, the Name and Area Code provide a unique fingerprint of the first database as, for example, there is only one Doe, John living in UK-W1K. Note that none of the two columns can be left out due to duplicate values. Small UCCs are natural candidates for primary keys, avoiding the need to introduce surrogate identification. More importantly though, knowledge of the inclusion-wise minimal unique column combinations enable various data cleaning tasks as well as query optimization. For example, value combinations of UCCs are by definition distinct and form groups of size 1, thus SQL queries working on UCCs can skip the grouping phase and the DISTINCT operation, even if requested by the user [12], [47]. Also, the presence of UCCs allows for early returns of SELECT and ORDER BY operations.

Fig. 1
Download : Download high-res image (352KB)
Download : Download full-size image
Fig. 1. Illustration of multi-column dependencies. Name and Area Code together are a minimal unique column combination in the first database. The functional dependency Name,Area Code → City holds, but its left-hand side is not minimal since Area Code → City is also valid. There are two maximal inclusion dependencies of size 1 between the first and the second database, Age is included in ID and Name in Author. They cannot be combined to an inclusion dependency of size 2 since, e.g., the value combination (33, Doe,John) does not appear in column combination ID, Author.

Unfortunately, a dataset only rarely comes annotated with its dependencies. Much more often they need to be computed from raw data. This leads to two different computational tasks. The detection problem, is to decide for a given database whether it admits a UCC with only a few columns. The discovery problem instead asks for a complete list of all minimal UCCs, regardless of their size and number. An equivalent term for the latter, which is probably more common in the algorithms community, is the enumeration of minimal solutions.

Functional dependencies (FDs), another dependency type, model the case in which one is only interested in identifying the values of a specific column, instead of all columns. In the example in Fig. 1, it is enough to know the Area Code to also infer the City since the value in the former determines the latter. Finally, inclusion dependencies (INDs) reveal connections between different databases. A unary IND holds if all values in a column of the first database are also contained in one column of the second database. In Fig. 1, all values of the Name attribute appear again for the Author attribute in the second table. The IND has higher arity if the inclusion also pertains to the tuples of values in multiple columns.2 In contrast to UCCs and FDs, where we want solutions to be small, we ought to find large or maximal inclusion dependencies. Those are much more likely to be caused by the inherent structure of the data than by mere coincidence.

Similar to UCCs, discovering the functional dependencies and inclusion dependencies of a database (respectively pairs thereof) is an important preprocessing step intended to improve the subsequent data access. The knowledge of the valid FDs and INDs is used for example in cardinality estimation in query plan optimizers [35], query rewriting [33], and joins [10]. For a detailed exposition of the applications of data dependencies, we refer the reader to the recent survey by Kossmann, Papenbrock, and Naumann [41].

The detection (decision) problems for all three types of dependencies are NP-complete.3 Notwithstanding, detection algorithms often perform well on practical datasets [1]. One approach to bridge this apparent gap is to analyze whether properties that are usually observed in realistic data benevolently influence the hardness of the problem. Exploiting those properties may even lead to algorithms that guarantee a polynomial running time in case these features are present in the problem instance. This is formalized in the concept of parameterized algorithms [18], [25], [45]. There, one tries to ascribe the exponential complexity entirely to an parameter of the input, other than its mere size. If the parameter is bounded for a given class of instances, we obtain a polynomial running time whose order of growth is independent of the parameter. This, of course, requires one to identify parameters that are both algorithmically exploitable and small in practice.

Consider, for instance, the histograms in Fig. 2, showing the size distribution of minimal unique column combinations and functional dependencies, as well as maximal inclusion dependencies in the MusicBrainz database [52]. The majority of functional dependencies are rather small, same for unique column combinations and inclusion dependencies. Beside surrogate keys, giving rise to multiple functional dependencies of size 1, causalities in the data can also lead to small FDs. For example, the name of an event together with the year in which it started determines the year in which it ends, implying an FD of size 2. Note that the starting year alone is usually not enough to infer this information. The name of the action, however, seems to indicate whether the event ends in the same year or the next. The size of the dependency is thus a natural candidate for an algorithmic parameter. Notwithstanding, we show that it is unlikely to be the sole explanation for the good practical performance. We prove that the detection of unique column combinations and functional dependencies is -complete with respect to the size of the sought solution, detecting inclusion dependencies is even -complete. For all we know, this excludes any algorithm parameterized by the size.

Fig. 2
Download : Download high-res image (121KB)
Download : Download full-size image
Fig. 2. The number of minimal unique column combinations, minimal functional dependencies, and maximal inclusion dependencies for varying solution sizes in the MusicBrainz database.

The hardness of detecting INDs is surprising also from a complexity-theoretic standpoint. Currently, there are only a handful of natural problems known to be complete for the class . The first one was given by Chen and Zhang [14] in the context of supply chain management. We show here that the detection of inclusion dependencies has this property, making it the second such problem. Since this result was first announced, Bläsius et al. [6] have also proven the extension problem for minimal hitting sets to be -complete using different techniques. The latter has subsequently been improved by Casel et al. [11], they have shown that already the special case of extension to minimal dominating sets in bipartite graphs is hard for . Finally, building on the work presented here, Hannula, Song, and Link [34] have very recently identified independence detection in relational databases as another representative of this class.

For enumeration problems, the border of tractability does not run between polynomial and super-polynomial time, at least not when measured in the input only. The number of solutions one wishes to be computed may be exponential in the input size, ruling out any polynomial algorithm. Instead, one could ask for an algorithm that scales polynomially both in the input and the number of solutions. The most important yardstick in enumeration complexity is the transversal hypergraph problem, where one is tasked to compute all minimal hitting sets of a given hypergraph. Currently, the fastest known algorithm runs in time 
, where N is the combined input and output size [30]. It is the major open question in the field of enumeration algorithms whether the transversal hypergraph problem can be solved in output-polynomial time. Besides data profiling, the problem also emerges in many other applications in artificial intelligence [50], machine learning, distributed systems, and monotone logic [28], as well as bioinformatics [49]. There are many algorithms known that work well on practical instances [32].

We use the insights gained on the detection of multi-column dependencies in databases to also investigate their discovery (enumeration) problems. It is known that minimal unique column combinations and functional dependencies can be discovered in output-polynomial time if and only if the transversal hypergraph problem has an output-polynomial solution [26]. However, this was proven via a Turing-style reduction that continuously calls a decision subroutine to check whether the enumeration part has already found all solutions. This construction inherently uses space proportional to the output and is therefore hardly useful in practice. We are able to radically simplify this equivalence and connect unique column combinations, functional dependencies, and hitting sets directly at the enumeration level using so-called parsimonious reductions, running in polynomial time and space. We give similar results also for the discovery of maximal inclusion dependencies.

Parsimonious reductions are the most restrictive form of reduction between enumeration problems, but–therefore–also the most useful in practice. The close connection to the transversal hypergraph problem that we prove in this work explains in parts why dependency discovery works quite well on real-world databases. Moreover, it allows us to transfer ideas from the design of hitting set enumeration algorithms to data profiling, thereby connecting the two research areas. For example, there are very space-efficient algorithms known for the transversal hypergraph problem, while memory consumption still seems to be a major obstacle in dependency discovery [46], [53]. We hope that our results here inspire better data profiling algorithms in the future.

Our contribution. We settle the parameterized complexity of the cardinality-constrained decision problems for unique column combinations, functional dependencies, and inclusion dependencies in relational databases, with the solution size as parameter. We prove the following theorems.

Theorem 1

Detecting a unique column combination of size k in a relational database is -complete when parameterized by k. The same is true for the detection of a valid, non-trivial functional dependency with a left-hand side of size at most k, even if the desired right-hand side is given in the input.

Theorem 2

Detecting an inclusion dependency of size k in a pair of relational databases is -complete when parameterized by k. The result remains true even if both databases are over the same relational schema with the identity mapping between their columns.

We also characterize the complexity of enumerating all multi-column dependencies of a certain type in a given database. We do so by proving parsimonious equivalences with well-known enumeration problems as well as generalizations thereof.

Theorem 3

The following enumeration problems are equivalent under parsimonious reductions:

(i)
discovering all minimal unique column combinations of a relational database;

(ii)
discovering all minimal, valid, and non-trivial functional dependencies with a fixed right-hand side;

(iii)
the transversal hypergraph problem.

The discovery of functional dependencies with arbitrary right-hand sides is equivalent to enumerating the hitting sets of multiple input hypergraphs. The latter two problems are at least as hard as the transversal hypergraph problem.
Theorem 4

The following enumeration problems are equivalent under parsimonious reductions:

(i)
discovering all maximal inclusion dependencies of a pair of relational databases;

(ii)
enumerating all maximal satisfying assignments of an antimonotone, 3-normalized Boolean formula.

This remains true even if the two databases are over the same schema and only inclusions between the same columns are allowed. All those problems are at least as hard as the transversal hypergraph problem.
Finally, we also briefly discuss the consequences of our findings to the approximability of minimum dependencies.

Outline. In the next section, we fix some notation and review basic concepts needed for the later proofs. Section 3 treats unique column combinations and functional dependencies, Section 4 then considers inclusion dependencies. Both of these sections start with a segment that formally defines all decision and enumeration problems discussed in the respective section. The paper is concluded in Section 5.

2. Preliminaries
2.1. Hypergraphs and hitting sets
A hypergraph is a non-empty, finite vertex set  together with a set of subsets , the (hyper-)edges. We identify a hypergraph with its edge set  if this does not create any ambiguities. We do not exclude special cases of this definition like the empty graph (), an empty edge (), or isolated vertices (
). A hypergraph is Sperner if none of its edges is contained in another. The minimization of  is the subset of all inclusion-wise minimal edges, 
. This should not be confused with the notation for a minimum element of a set. The minimization of a hypergraph is always Sperner.

A hitting set, or transversal, of a hypergraph  is a set  of vertices such that T has a non-empty intersection with every edge . A hitting set is (inclusion-wise) minimal if it does not properly contain another hitting set. The minimal hitting sets of  form a Sperner hypergraph on the same vertex set V, the transversal hypergraph . Regarding transversals, it does not make a difference whether the full hypergraph  is considered or its minimization, it holds that .

2.2. Relational databases and dependencies
A (relational) schema R is a non-empty, finite set of attributes or columns. Each attribute comes implicitly associated with a set of admissible values. A row, or record, over schema R is a tuple r whose entries are indexed by R such that, for any attribute , the value  is admissible for a. A (relational) database  over R is a finite set of records. For any row r and subset  of columns,  is the subtuple of r projected onto S. We let  denote the family of all such subtuples of rows in . Note that  is a multiset as the same combination of values may appear in different rows.

We are interested in multi-column dependencies in relational databases, namely, unique column combinations, functional dependencies, and inclusion dependencies. For their definition, we need the notion of difference sets. For any two distinct rows 
, 
, over the same schema R, their difference set 
 is the set of attributes in which the rows disagree. A difference set is (inclusion-wise) minimal if it does not properly contain a difference set for another pair of rows in the database. We denote the hypergraph of minimal difference sets by . The vertex set is R.

A unique column combination (UCC), or simply unique, for some database  over schema R, is a subset  of attributes such that for any two records , , there is an attribute  such that . Equivalently, U is a UCC for  iff the collection  of projections onto U is a mere set, that is, every tuple appears at most once. A UCC is (inclusion-wise) minimal if it does not properly contain another UCC. There is an intimate connection between UCCs and transversals in hypergraphs. It is well-known in the literature, see [1], [19], probably the first explicit mention was by Mannila and Räihä [43].

Observation 5 Folklore

The unique column combinations are the hitting sets of difference sets. In particular, let  be a database and  the hypergraph of its minimal difference sets. Then, any minimal transversal in  is a minimal unique of  and there are no other minimal uniques in .

Functional dependencies (FDs) over a schema R are expressions of the form  for some set  of columns and a single attribute . The set X is the left-hand side (LHS) of the dependency and a the right-hand side (RHS). We say that the FD has size . A functional dependency  is said to hold, or be valid, in an database  (over R) if any pair of records that agree on X also agree on a, that is, if  implies  for any . Otherwise,  is said to fail in , or be invalid. The FD  holds iff all rows agree on a. An FD  is (inclusion-wise) minimal if it holds in  and 
 fails for any proper subset 
. A functional dependency is non-trivial if . Note that trivial functional dependencies hold in any database.

Finally, we turn to multi-column dependencies among multiple databases. Let R and S be two relational schemas and  and  databases over R and S, respectively. For some , let  be an injective map. The pair  is an inclusion dependency (IND) if, for each row , there exists some  such that  for every , that is, iff the inclusion  holds. If the map σ is given in the input, we say that X is the dependency. Such an inclusion dependency then is maximal if the set X is maximal among all INDs for  and . For the general case, we define a partial order on the pairs . We say that 
 holds if 
 and σ is the restriction of 
 to X. An inclusion dependency then is maximal if it is an ≼-maximal element among the inclusion dependencies between  and . Observe that the inclusion dependencies are indeed downward closed with respect to ≼. However, it may happen that  and 
 are both maximal although 
 is a strict subset.

2.3. Parameterized complexity
The central idea of the parameterized complexity of a decision problem is to identify a quantity of the input, other than its mere size, that captures the hardness of the problem. The decision problem associated with a language 
⁎
 is to determine whether some instance 
⁎
 is in Π. A decision problem is parameterized if any instance I is additionally augmented with a parameter 
, we thus have 
⁎
. A parameterized decision problem Π is fixed-parameter tractable (FPT), if there exists a computable function 
 and an algorithm that decides any instance  in time . The complexity class FPT is the collection of all fixed-parameter tractable problems.

Let Π and 
 be two parameterized problems. A parameterized reduction, or FPT-reduction, from Π to 
 is an algorithm running in time  on instances , which outputs some instance 
 such that 
 for some computable function 
, and  holds if and only if 
 does. Due to the time bound, we have 
. If there is also an FPT-reduction from 
 to Π, we say that the problems are FPT-equivalent. A notable special case of FPT-reductions are polynomial many-one reductions that preserve the parameter, meaning 
.

Parameterized reductions give rise to the so-called W-hierarchy of complexity classes. We use one of several equivalent definitions involving (mainly) Boolean formulas. However, first consider the Independent Set problem on graphs parameterized by the size of the sought solution. The class  is the collection of all parameterized problems that admit a parameterized reduction to Independent Set. For some positive integer t, a Boolean formula is t-normalized if it is a conjunction of disjunctions of conjunctions of disjunctions (and so on) of literals with  alternations or, equivalently, t levels in total. The Weighted t-normalized Satisfiability problem is to decide, for a t-normalized formula φ and a positive integer k, whether φ admits a satisfying assignment of Hamming weight k, that is, with (exactly) k variables set to true. Here, k is the parameter. For every ,  is the class of all problems reducible to Weighted t-normalized Satisfiability. The classes  form an ascending hierarchy. All inclusions are conjectured to be strict, which is however still unproven. Chen et al. [13] showed that, assuming the Exponential Time Hypothesis4 (ETH), Independent Set with budget k on n-vertex, m-edge graphs cannot be solved in time 
. Hence, ETH implies .

2.4. Enumeration complexity
Enumeration is the task of compiling and outputting a list of all solutions to a computational problem without repetitions. Note that this is different from a counting problem, which asks for the mere number of solutions. More formally, an enumeration problem is a function 
⁎
⁎
 such that, for all instances 
⁎
, the set of solutions  is finite. An algorithm solving this problem needs to output, on input I, all elements of  exactly once. We do not impose any order on the output. We focus on the enumeration of minimal hitting sets, that is, .

An output-polynomial algorithm runs in time polynomial in the combined input and output size . A (seemingly) stronger requirement is an incremental polynomial algorithm, generating the solutions in such a way that the i-th delay, the time between the -st and i-th output, is bounded by . This includes the preprocessing time until the first solution arrives () as well as the postprocessing time between the last solution and termination (). The strongest form of output-efficiency is that of polynomial delay, where the delay is universally bounded by . There is currently no output-polynomial algorithm known for the transversal hypergraph problem, but its existence would immediately imply also an incremental polynomial algorithm [4].

Arguably the most restrictive way to relate enumeration problems are parsimonious reductions. The concept is closely related but should not be confused with the homonymous class of reductions for counting problems [9]. A parsimonious reduction from enumeration problem Π to 
 is a pair of polynomial time computable functions 
⁎
⁎
 and 
⁎
⁎
 such that, for any instance 
⁎
,  is a bijection from 
 to . The behavior of  on 
⁎
 is irrelevant. Intuitively, any enumeration algorithm for 
 can then be turned into one for Π by first mapping the input I to  and translating the solutions back via g. Observation 5 establishes a parsimonious reduction from the enumeration of minimal UCCs to the transversal hypergraph problem with  and  being the identity over subsets of R.

3. Unique column combinations and functional dependencies
Theoreticians as well as practitioners in data profiling and database design are frequently confronted with the task of finding a small collection of items that has a non-empty intersection with each member of a prescribed family of sets, see [1], [19], [20], [38], [42]. They thus aim to solve instances of the hitting set problem. In this section, we show that this encounter is inevitable in the sense that detecting a small unique column combination or functional dependency in a relational database is the same as finding a hitting set in a hypergraph. Even more so, this equivalence extends to enumeration. We show that the associated discovery problems of finding all UCCs or FDs is indeed the same as enumerating all hitting sets.

We first formally define the respective decision and enumeration problems. The decision versions are always parameterized by the solution size. We then order them in a (seemingly) ascending chain via parameterized reductions. However, the start and end points of this chain will turn out to be FPT-equivalent, settling the complexity of the problems involved as complete for the parameterized complexity class . We then also show the equivalence of the corresponding enumeration problems under parsimonious reductions.

3.1. Problem definitions
Recall the definitions of hitting sets as well as unique column combinations and functional dependencies from Sections 2.1 and 2.2. We are interested in the parameterized complexity of the associated cardinality-constrained decision problems. The constraint always serves as the parameter.

Hitting Set

Instance:
A hypergraph  and a non-negative integer k.

Parameter:
The non-negative integer k.

Decision:
Is there a set  of vertices with cardinality 

such that T is a hitting set for ?

Note that if , then the answer to the decision problem is trivially false; otherwise, there is no difference between deciding the existence of a transversal with at most or exactly k elements since every superset of a hitting set is again a hitting set. We ignore the special case of a too large k since parameterized complexity is primarily concerned with the situation that the parameter is much smaller than the input size. The unparameterized Hitting Set problem is one of Karp's initial 21 NP-complete problems [39]. In fact, its minimization variant–to compute the minimum size of any hitting set–is even NP-hard to approximate within a factor of  for any constant  [21]. The parameterized variant defined above is the prototypical -complete problem [25].
The corresponding enumeration problem broadens the notion of a “small” solution, namely, the task is now to list all inclusion-wise minimal hitting sets, that is, the edges of the transversal hypergraph . All other (non-minimal) hitting sets can be trivially obtained from the minimal ones by arbitrarily adding more vertices. Note that the term enumeration implicitly contains the requirement not to repeat outputs.

Transversal Hypergraph

Instance:
A hypergraph .

Enumeration:
List all edges of .

Let  denote the combined input and output size, Fredman and Kachiyan's algorithm solves Transversal Hypergraph in time 
 [30].
We generalize the problem to the enumeration of minimal hitting sets for multiple input hypergraphs simultaneously. We do not prescribe any order in which the hypergraphs are processed. However, we want to be able to quickly tell to which input a solution belongs. For this, let 
 denote the disjoint union of the transversal hypergraphs of  and  with the additional requirement that the union is encoded in such a way that, for any 
, the containment  is decidable in time , independently of the sizes  and .

Transversal Hypergraph Union

Instance:
A d-tuple of hypergraphs 
.

Enumeration:
List all edges of 
.

We now define the detection and discovery problems of multi-column dependencies in relational databases, again starting with the cardinality-constraint decision problems.

Unique Column Combination

Instance:
A relational database  over schema R

and a non-negative integer k.

Parameter:
The non-negative integer k.

Decision:
Is there a set  of attributes with cardinality 

such that U is a unique column combination in ?

The minimization variant of Unique Column Combination is NP-hard [51].
Enumerate Minimal UCCs

Instance:
A relational database .

Enumeration:
List all minimal unique column combinations of .

For functional dependencies, we define two variants of the decision problem that slightly differ in the given input. The first one fixes the right-hand side of the desired dependency, while the second one asks for an FD with arbitrary RHS holding in the database. While their parameterized complexity will turn out to be the same, there are some differences in their discovery.

Functional Dependency

Instance:
A relational database  over schema R, an attribute ,

and a non-negative integer k.

Parameter:
The non-negative integer k.

Decision:
Is there a set  with  such that

the functional dependency  holds in ?

Functional Dependency
Instance:
A relational database  over schema R

and a non-negative integer k.

Parameter:
The non-negative integer k.

Decision:
Is there a set  with  and an attribute 

such that the functional dependency  holds in ?

The unparameterized variant of Functional Dependencyfixed RHS is NP-complete even if the number of admissible values for each attribute is at most 2 [20]. It is the same to ask for a functional dependency whose left-hand side is of size at most k; unless of course  since then no non-trivial FD adheres to the (exact) size constraint. Again, we ignore this issue.
Recall that we say that a functional dependency  is minimal if its LHS X is inclusion-wise minimal among all 
 such that 
 is valid.

Enumerate Minimal FDs

Instance:
A relational database  over schema R and an attribute .

Enumeration:
List all minimal, valid, non-trivial functional dependencies of 

with right-hand side a.

Enumerate Minimal FDs
Instance:
A relational database .

Enumeration:
List all minimal, valid, non-trivial functional dependencies of .

Enumerate Minimal UCCs and Enumerate Minimal FDs can be solved in output-polynomial time (in incremental polynomial time or with polynomial delay, respectively) if and only if this is possible for the Transversal Hypergraph problem [26].
3.2. Detection
Next, we prove the parameterized complexity of the detection problems for unique column combinations and functional dependencies.

We remark that since the first announcement of our results, we learned that Unique Column Combination and the Functional Dependencyfixed RHS problem have been studied before from a parameterized perspective, under different names and in different contexts. Therefore, some of the following results already appeared in the literature. Namely, Lemma 6 was obtained independently by Froese et al. [31], and Cotta and Moscato [16] showed the -completeness of Functional Dependencyfixed RHS using different techniques. Our work can be seen as a unifying framework that shows that Unique Column Combination is also contained in  and settles the complexity of the more general Functional Dependency problem (with an arbitrary right-hand side) in a way that additionally sheds some light on the enumeration complexity of the related discovery problems.

We start by proving that Unique Column Combination is hard for  under parameterized reductions.

Lemma 6

There is a parameterized reduction from Hitting Set to the Unique Column Combination problem.

Proof

Let  be the hypergraph given in the input to the Hitting Set problem. Without loss of generality, we can assume it to be Sperner; otherwise, we replace it by its minimization  (in quadratic time). Observe that  has a hitting set of size at most k iff  has one. We construct from  in polynomial time a database  over schema V such that the minimal difference sets of  are the edges of . The lemma then follows immediately from Observation 5. In particular, since Observation 5 transfers solutions, the parameter k is preserved by the reduction.

Let 
 be the edges of . We take the integers  as the admissible values for all attributes in V. As rows, we first add the all-zero tuple 
 to . For each , we also add the record 
 defined as
  See Fig. 3a for an illustration. Clearly,  can be computed in linear time.

Fig. 3
Download : Download high-res image (146KB)
Download : Download full-size image
Fig. 3. Illustration of the reductions in Lemma 6, Lemma 7. (a) An instance of Hitting Set and the equivalent instance of Unique Column Combination. (b) An instance  of Functional Dependencyfixed RHS with fixed right-hand side a and the equivalent instance 
 of Functional Dependency. The functional dependencies ab → d hold in , but not in 
.

Any edge 
 is a difference set in , namely, that of the pair 
. Any other difference set must come from a pair 
 with . It is easy to see that those rows disagree in 
, which is not minimal. Since  is Sperner, it contains exactly the minimal difference sets of . □

The next two reductions are straightforward due to the similar structures of uniques and functional dependencies. While a UCC separates any pair of rows, an FD  needs to distinguish only those with .

Lemma 7

There are parameterized reductions

(i)
from Unique Column Combination

to Functional Dependencyfixed RHS;

(ii)
from Functional Dependencyfixed RHS

to Functional Dependency.

Proof

To prove Statement (i), we add a single unique column to the database and fix it as the right-hand side of the sought functional dependency. Let 
 be a database over schema R, and a an attribute not previously in R. We construct 
 over  by adding, for each 
, the row 
 defined by 
 and 
. The reduction maps an instance  of Unique Column Combination to the instance 
 of Functional Dependencyfixed RHS. Since, for any two distinct rows 
, , we have 
, the left-hand sides of the non-trivial, valid FDs  in 
 are in one-to-one correspondence to the UCCs in .

To reduce Functional Dependencyfixed RHS to Functional Dependency, we need to mask all “unwanted” FDs with RHS different from the fixed attribute a. See Fig. 3b for an example. Let again  be the input database over R. To construct the resulting database 
 over the same schema R, we take all rows from  and add  new ones. Fix an arbitrary record 
⁎
 and let × be a new symbol that does not previously appear as a value. For each attribute , we add the row 
 satisfying 
⁎
 and 
. The rows 
⁎
 and 
 now witness that any non-trivial FD  fails in 
.

It is left to prove that  holds in  if and only if it holds in 
. Evidently, any valid FD in 
 is also valid in the subset . Suppose  holds in  and let rows 
 be such that . The only case where the conclusion  may possibly be in doubt is if 
 and  (all new rows in 
 agree on a). Hence, 
 for some . If , the new value × appears in the projection  but not in ; otherwise, we have 
⁎
. Since  holds for the pair 
⁎
, the relation 
⁎
 follows. □

The next lemma proves that every instance of the unrestricted Functional Dependency problem can be expressed as an equivalent Boolean formula in conjunctive normal form. Since CNF formulas are exactly the 2-normalized ones, we obtain a reduction to Weighted 2-normalized Satisfiability. This is the main result of this section.

Lemma 8

There is a parameterized reduction from Functional Dependency to the Weighted 2-normalized Satisfiability problem.

Proof

Given a database  over R, we derive a CNF formula that has a satisfying assignment of Hamming weight  if and only if there is a non-trivial FD with left-hand side of size k that holds in . We use two types of variables distinguished by their semantic purpose, 
 and 
. Some variable 
 from 
 being set to true corresponds to the attribute a appearing on the left-hand side of the sought functional dependency; for 
 from 
, this means the attribute is the right-hand side.

We start with the RHS. It might be tempting to enforce that any satisfying assignment chooses exactly one variable from 
. We prove below that this is not necessary and forgoing the 
 clauses representing this constraint allows for a (slightly) leaner construction. However, we do have to ensure that at least one variable from 
 is set to true and the corresponding one in 
 is false. This is done by the clauses 
 and 
 for each . The subformula 
 is their conjunction.

We now model the LHS. For any attribute  and rows , let
 
 
 Intuitively, the clause 
 represents the fact that if  is a valid, non-trivial FD, then X has to contain at least one attribute b, different from a, such that . From these clauses, we assemble the subformula
 
 
 The output of our reduction is the formula 
. Indeed, φ is in conjunctive normal form and has 
 clauses with at most  literals each. An encoding of φ is computable in time linear in its size.

Regarding the correctness of the reduction, recall that we claimed φ to have a weight  satisfying assignment iff a non-trivial functional dependency  of size k holds in . Suppose that the latter is true. We show that setting the variable 
 as well as all 
 with  to true (and all others to false) satisfies φ. Note that the assignment automatically satisfies 
 and all 
 with . We are left with the subformula 
 containing the clauses 
 for row pairs with . To distinguish these pairs, the LHS X includes, for each of them, some attribute  such that . Clause 
 is then satisfied by the corresponding literal 
.

For the other direction, we identify assignments with the variables they set to true. Let 
 be an assignment of Hamming weight  that satisfies φ. The assignment induces two subsets of the schema R, namely, 
 and 
. Due to the clause 
, Y is non-empty and X contains at most k elements. Moreover, X and Y are disjoint as the 
 are all satisfied. We say that the generalized functional dependency  holds in a database if  holds for every . It is clearly enough to show that  indeed holds in .

Assume  fails for some . This is witnessed by a pair of rows  with  but , whence the clause 
 is present in φ. Since 
 is in the assignment, the literal 
 evaluates to false. Also, as X is disjoint from the difference set , no other literal satisfies 
, which is a contradiction. □

We have established a chain of parameterized reductions between the dependency detection problems of unique column combinations and functional dependencies. The fact that the endpoints Hitting Set and Weighted 2-normalized Satisfiability are both -complete shows that all of the problems are, this completes the proof of Theorem 1.

3.3. Approximation and discovery
Our reductions have implications beyond the scope of parameterized complexity. Observe that the transformations in Lemma 6, Lemma 7 are computable in polynomial time, do not change the size of the vertex set/relational schema (by more than an additive constant), and preserve approximations with respect to the solution size k or , respectively. Also, recall that the minimization version of Hitting Set is NP-hard to approximate within a factor of  for every  [21]. As a consequence, the minimization versions of Unique Column Combination, Functional Dependencyfixed RHS, and Functional Dependency all inherit the same hardness of approximation with respect to the size  of the schema. Previously, an approximation-preserving reduction was known only for the Unique Column Combination problem, starting from Set Cover [2].

Rather than approximating minimum solutions, we are mainly interested in the discovery of minimal dependencies in databases. Traditionally, enumeration has been studied via embedded decision problems that are different from those defined in Section 3.1. Instead, the Transversal Hypergraph problem (enumerating minimal hitting sets) has been associated with the problem of deciding for two hypergraphs  and  whether  or, equivalently, , called the Dual problem. Enumerate Minimal UCCs analogously corresponds to decide for a database  and hypergraph , whether  consists of all minimal uniques of . Intuitively, this formalizes the decision whether an enumeration algorithm has found all solutions.

Both decision problems are in coNP and it was proven by Eiter and Gottlob [26] that they are many-one equivalent. Using a lifting result by Bioch and Ibaraki [4], this shows that minimal hitting sets can be enumerated in output-polynomial time if and only if minimal UCCs can, which is the case if and only if Dual is in P. Such an equivalence is theoretically appealing and has lead to the quasi-output-polynomial upper bound on the running time of hitting set/UCC enumeration that is currently the best known [30]. The connection to enumeration has also inspired the intriguing result that Dual is likely not coNP-hard as it can be solved in polynomial time when given access to 
 suitably guessed nondeterministic bits [3], [27], where N denotes the total input size of the pair . There are classes of hypergraphs for which Dual is indeed in P, see for example [8], [6], [22], [27], [48], or at least in FPT with respect to certain structural parameters [29]. The two most well-known special cases that are polynomial-time solvable are the classes of hypergraphs with constant maximum degree [22] or edge size [27], respectively. Translated to our database setting, this means that attributes can only participate in a bounded number of (minimal) difference sets or that any two rows can differ only in a bounded number of columns. Both are severe restrictions and fairly uncommon in real-world databases. For a much more thorough overview of decision problems associated with enumeration, see the recent work by Creignou et al. [17].

Unfortunately, the approach described above holds only limited value when it comes to designing practical algorithms. Imagine an implementation of the discovery of minimal UCCs of a database  via repeated checks whether the hypergraph  of previously found solutions is already complete. Such an algorithm is bound to use an amount of memory that is exponential in the size of . This is due to the fact that some databases have exponentially many minimal solutions and the decision subroutine at least has to read all of . Note that such a large memory consumption is not at all necessary as there are algorithms known for Transversal Hypergraph whose space complexity is only linear in the input size [29], [44]. In fact, enumeration algorithms are often analyzed not only with respect to their running time, but also in terms of space consumption, see [9], [15]. For data profiling problems like Enumerate Minimal UCCs on the other hand, space-efficient algorithms have only recently started to received some attention [5], [6], [40].

In the following, we simplify and at the same time extend the above equivalences making them usable in practice, namely, we prove Theorem 3. It states the existence of parsimonious reductions, in both directions, that relate the input instances directly on the level of the enumeration problems, without decision problems as intermediaries. This way, we characterize the enumeration complexity of unique column combinations as well as functional dependencies, both with fixed and arbitrary right-hand side. It is worth noting that our insights on enumeration do stem from the study of decision problems while the obtained results are entirely lifted.

The reductions between the decision problems Hitting Set, Unique Column Combination, Functional Dependencyfixed RHS, and Functional Dependency (in that order) for minimum dependencies described in Lemma 6, Lemma 7 are all built on bijective correspondences between the solutions. The running times of the reductions are polynomial and independent of the given budget k. Finally, the mappings of the solution spaces also preserve set inclusions. This means, the same input transformations applied to the discovery of minimal dependencies are in fact parsimonious reductions from Transversal Hypergraph to Enumerate Minimal UCCs and further to Enumerate Minimal FDs/FDsfixed RHS. Regarding the inverse direction, Observation 5 shows that the enumeration of UCCs is at most as hard as that of hitting sets, that is, Enumerate Minimal UCCs and Transversal Hypergraph are equivalent. It is worth noting that the parsimonious reductions increase the size of the instances at most by a polynomial factor (usually a constant one) in the input size only and therefore transfer the space complexity of any enumeration algorithm.

To complete the proof of Theorem 3, we still need the following lemma that characterizes the complexity of functional dependency discovery in terms of the Transversal Hypergraph Union problem.

Lemma 9

The problems Enumerate Minimal FDs and Transversal Hypergraph Union are equivalent under parsimonious reductions. Moreover, there is a parsimonious reduction from Enumerate Minimal FDsfixed RHS to Transversal Hypergraph.

Proof

This proof uses techniques that already helped to establish the previous lemmas of this section. Let  be a database over schema R,  some attribute,  two rows with . Recall that their difference set is . We define their punctured difference set to be . It is implicit in the proof of Lemma 8–and easy to verify from the definition of functional dependencies–that a set  is the left-hand side of a valid, minimal, non-trivial FD  if and only if it is a minimal hitting set for the hypergraph 
 of punctured difference sets.

Transforming the input database  over schema 
 into the  hypergraphs 
 is a parsimonious reduction from the Enumerate Minimal FDs problem to Transversal Hypergraph Union. In the same fashion, fixing the desired right-hand side in the input reduces Enumerate Minimal FDsfixed RHS to Transversal Hypergraph.

The opposite reduction is the main part of the lemma. We are given hypergraphs 
, without loss of generality all on the same vertex set V, and we need to compute some database  such that its valid, non-trivial functional dependencies are in one-to-one correspondence with the hitting sets of the 
. An example can be seen in Fig. 4. As the relational schema, we take the set 
, where the 
 are attributes not previously appearing in V. The construction of  starts similarly to Lemma 6. We first add the all-zeros row 
 (with 
 for every ). Let 
 be the total number of edges and 
 an arbitrary numbering of them. Note that if the same set of vertices is an edge of multiple hypergraphs, it appears in the list with that multiplicity. For every edge 
, we add the following row 
,
  In other words, the subtuple 
 is the characteristic vector of 
 only that its non-zero entries are j instead of 1; the subtuple 
 has exactly one 1 at the position corresponding to the hypergraph containing 
. The remaining construction of database  uses an idea of Lemma 7 (ii). Let × be a new symbol. For every vertex , we add the row 
 with 
 and 
 for all other attributes . The database  can be obtained in time .

Fig. 4
Download : Download high-res image (74KB)
Download : Download full-size image
Fig. 4. Illustration of Lemma 9. The three hypergraphs 
 on the vertex set V are on the left and the equivalent database  is on the right. The hypergraphs 
 and 
 share the edge {b,c}, but this results in two rows r2 and r3. The corresponding transversal hypergraphs are 
, 
, and 
. The functional dependencies b → x1 and b → x2 indeed hold in , and adding the attribute a gives ab → x3. The rows in the last block eliminate all non-trivial functional dependencies X → v with v ∈ V.

We claim that the minimal, valid, non-trivial functional dependencies of  are exactly those having the form 
 with 
. The existence of a parsimonious reduction from Transversal Hypergraph Union to Enumerate Minimal FDs easily follows from that. Let  and  be such that the FD  holds in  and is minimal. For any , the rows 
 and 
 differ only in attribute v, therefore v is not the right-hand side of any non-trivial FD, whence 
 for some . As seen above, the set X must be a minimal transversal of the hypergraph 
. We are left to prove that 
 has the same minimal transversals as 
. Let  be rows that differ in attribute 
, say, 
 and 
. We thus have 
 for some . The rows r and s share only the value 0, if any. Therefore,
  In the second case, note that  since 
. The above implies that 
. Moreover, all edges in 
 are supersets of ones in 
. The respective minimizations 
 are thus equal and, by duality, also their transversal hypergraphs 
 are the same. □

It is known that Enumerate Minimal FDs can be solved in output-polynomial time if and only if Transversal Hypergraph can [26], this has been established along the same lines as discussed in the remarks preceding Lemma 9. Notably, Eiter and Gottlob additionally presented an alternative construction in the extended version of [26] that is almost parsimonious. The only condition they needed to relax is the bijection between the solution spaces. They transform a database over schema R into some hypergraph 
 such that the majority of its minimal hitting sets indeed correspond to the functional dependencies with arbitrary right-hand side. However,  has some 
 excess solutions (that is, polynomially many in the input size only), which do not have an FD counterpart, but are easily recognizable. We leave it as an open problem to give a fully parsimonious reduction between the problems. We have shown above that this is equivalent to encoding the hitting set information of  different hypergraphs into a single one.

4. Inclusion dependencies
We now discuss inclusion dependencies in relational databases. We show that their detection problem, when parameterized by the solution size, is one of the first natural problems to be complete for the class . We do so by proving its FPT-equivalence with the weighted satisfiability problem for a certain fragment of propositional logic. Later in Section 4.4, we transfer our results to the discovery of maximal inclusion dependencies.

4.1. Problem definitions
A Boolean formula is antimonotone if it only contains negative literals. We identify a variable assignment with those variables that are set to true. In the case of antimonotone formulas, this means that the satisfying assignments are closed under arbitrarily turning variables to false, that is, taking subsets. The empty assignment that assigns false to all variables is always satisfying. Recall that a formula is 3-normalized if it is a conjunction of disjunctions of conjunctions of literals or, equivalently, if it is conjunctions of subformulas in disjunctive normal form (DNF). An example of an antimonotone, 3-normalized formula is
 This formula admits satisfying assignments of Hamming weight 0, 1, and 2, but none of larger weight.

The Weighted Antimonotone 3-normalized Satisfiability problem is the special case of Weighted 3-normalized Satisfiability restricted to antimonotone formulas.

Weighted Antimonotone 3-normalized Satisfiability (WA3NS)

Instance:
An antimonotone, 3-normalized Boolean formula φ

and a non-negative integer k.

Parameter:
The non-negative integer k.

Decision:
Does φ admit a satisfying assignment of Hamming weight k?

By the above remark, this is the same as asking for an assignment of weight at least k. The Antimonotone Collapse Theorem of Downey and Fellows [23], [24] implies that the WA3NS special case is -complete on its own.
The inclusion-wise maximal satisfying assignments carry the full information about the collection of all satisfying assignments. It is therefore natural to define the corresponding enumeration problem as follows.

Enumerate Maximal Satisfying WA3NS Assignments

Instance:
An antimonotone, 3-normalized Boolean formula φ.

Enumeration:
List all maximal satisfying assignments of φ.

For inclusion dependencies, the situation is similar. Every subset of a valid IND is also valid. Asking for a dependency of size exactly k is thus the same as asking for one of size at least k. We define two variants of the decision problem, similar as we did with functional dependencies. The more restricted variant requires the two databases to have the same schema with the identity mapping between columns.

Inclusion DependencyIdentity

Instance:
Two relational databases ,  over schema R

and a non-negative integer k.

Parameter:
The non-negative integer k.

Decision:
Is there a set  with  such that

 is an inclusion dependency?

Inclusion Dependency
Instance:
Two relational databases,  over schema R and  over S,

and a non-negative integer k.

Parameter:
The non-negative integer k.

Decision:
Is there a set  with  and an injective mapping

 such that  is an inclusion dependency?

The unparameterized variant of the general Inclusion Dependency problem is NP-complete already for pairs of binary databases [38].
The solutions of Inclusion DependencyIdentity are mere subsets of the underlying schema, therefore it is clear what we mean by a maximal solution. The case of the general Inclusion Dependency problem is slightly more intricate. Recall from Section 2.2 that we say a general inclusion dependency  is maximal if there is no other IND 
 such that 
 is a proper subset and σ is the restriction of 
 to X. Note that the pair 
 with an alternative mapping τ might still be a valid inclusion dependency. This leads to the following enumeration problems.

Enumerate Maximal INDsIdentity

Instance:
Two relational databases ,  over the same schema.

Enumeration:
List all maximal valid inclusion dependencies between  and 

with the identity mapping between the columns.

Enumerate Maximal INDs
Instance:
Two relational databases  and .

Enumeration:
List all maximal valid inclusion dependencies between  and .

4.2. Membership in 
We show that both variants of the Inclusion Dependency decision problem are contained in the class . Recall that the Inclusion DependencyIdentity problem restricts the input to pairs  of databases over the same schema and forbids solutions in which the set of values  of one column are contained in  for some other column . As a first step, we show (not entirely surprisingly) that this variant is at most as hard as the general problem.

Lemma 10

There is a parameterized reduction from Inclusion DependencyIdentity to Inclusion Dependency.

Proof

Let  and  be two databases over the schema R and let 
 be a new row, where the 
 are  different symbols none of which are previously used anywhere in  or . In the restricted setting, an inclusion dependency is a set  of columns such that . It is easy to see that  has such an inclusion dependency of size k, for any k, if and only if 
 has an inclusion dependency of the same size with an arbitrary mapping between the columns since 
 holds iff . The lemma follows from here. □

To demonstrate the membership of the general problem in , we reduce is to WA3NS. Namely, we compute from the two databases an antimonotone, 3-normalized formula which has a weight k satisfying assignment if and only if the databases admit an inclusion dependency of that cardinality. For this, we use a correspondence between pairs of attributes and Boolean variables.

Lemma 11

There is a parameterized reduction from Inclusion Dependency to Weighted Antimonotone 3-normalized Satisfiability.

Proof

Let 
 and 
 be two schemas. We introduce a Boolean variable 
 for each pair of attributes 
 and 
. We let 
 denote the set of variables corresponding to a collection  of such pairs. Consider a subset  together with an injection . From this, we construct a truth assignment including the variable 
 (setting it to true) iff 
 and 
. The resulting assignment has weight  and the collection of all possible configurations  is uniquely described by 
 and the truth assignments obtained this way. Moreover, these assignments all satisfy the following antimonotone Boolean formula 
.
 
 
 
 
 
 
 The first half of 
 expresses that, for every pair of variables 
 and 
 with 
, at most one of them shall be true; the second half is satisfied if the same holds for all pairs 
 and 
 with 
. Conversely, a satisfying assignment A (a subset of 
) for 
 defines a relation  and a set  by setting 
 and 
. By construction, the relation σ is not only a function , but an injection. In summary, 
 is fulfilled exactly by the assignments described above. Observe that 
 is in conjunctive normal form and therefore also 3-normalized as each literal is a conjunctive clause of length 1.

We now formalize the requirement that a configuration  is an inclusion dependency in a given pair of databases  and  over the respective schemas R and S, that is, that  holds. First, assume that each database consists only of a single row 
 and 
, respectively. We say a pair of attributes 
 is forbidden for 
 and 
 if 
. Let 
 be the set of all forbidden pairs. For an configuration  to be an IND, the variables 
 need to be set to false for all 
. In terms of Boolean formulas, this is represented by the conjunctive clause 
. It follows that  is an inclusion dependency if and only if the corresponding variable assignment satisfies both 
 and 
.

Now suppose  has multiple rows, while  is still considered to have only one. The configuration  is an IND for  iff it is one for at least one instance 
 with 
. If also  has more records, then  is an IND for  iff it is one in each instance 
 with 
. Therefore, we obtain an inclusion dependency if and only if 
 and the formula 
  
 
 are simultaneously satisfied by the assignment corresponding to .

The formula 
 is antimonotone and 3-normalized. The (disjunctive) clauses of 
 can be constructed in total time 
 and all sets 
 together are computable in time . An encoding of 
 can thus be obtained from the input databases  and  in polynomial time. Finally, by the above observation that any solution for the sub-formula 
 that corresponds to  has weight , the reduction preserves the parameter. □

4.3. Hardness for 
We now show that detecting inclusion dependencies is also hard for . We argue that the existence of weighted satisfying assignments for 3-normalized, antimonotone formulas can be decided by solving instances of the more restricted Inclusion DependencyIdentity variant. For the reduction, we make use of indicator functions. On the one hand, we interpret propositional formulas φ over n variables as Boolean functions 
 in the obvious way. On the other hand, for a pair of databases  and  over the same schema R, we represent any subset  by its characteristic vector of length . We then define the indicator function 
 by requiring that 
 holds if and only if X is an inclusion dependency (with the identity mapping between the columns).

We claim that for any formula φ that is antimonotone and 3-normalized, there is a pair  of databases computable in polynomial time such that 
. Clearly, this gives a parameterized reduction from WA3NS to the Inclusion DependencyIdentity problem. The remainder of this section is dedicated to proving this claim. Recall that the top level connective of a 3-normalized formula is a conjunction. We start by demonstrating how to model this using databases.

Lemma 12

Let 
 and 
 be two pairs of databases, all over the same schema R, with indicator functions 
 and 
, respectively. There exists a polynomial time computable pair  over R of size 
 and 
, having indicator function 
.

Proof

Without loosing generality, the values appearing in 
 and 
 are disjoint from those in 
 and 
. We straightforwardly construct  as 
 and 
, which matches the requirements on both the computability and size. We still need to show 
.

Equivalently, we prove that a set  is an inclusion dependency in  if and only if it is one in both pairs 
 and 
. Let X be an IND in 
 as well as 
. That means, for every row 
, there exists some 
 with ; same for 
 and 
. As all those rows are also present in , X is an IND there as well. Conversely, suppose X is not an inclusion dependency in, say, 
. Then, 
 has a row r that disagrees with every 
 on some attribute in X. The record r is also in present in  and all rows in  belong either to 
 or have completely disjoint values. This results in  for every record . □

One could hope that there is a similar method treating disjunctions. However, we believe that there is none that is both computable in FPT-time and compatible with a complementing method representing conjunctions (for example, the one above). The reason is as follows. Negative literals are easily expressible by pairs of single-row databases. Together with FPT-time procedures of constructing conjunctions as well as disjunctions, one could encode antimonotone Boolean formulas of arbitrary logical depth. The Antimonotone Collaps Theorem [25], states that the Weighted Antimonotone t-normalized Satisfiability problem is -complete for every odd . This would then render Inclusion DependencyIdentity to be hard for all classes  and, as a consequence of Lemma 10, Lemma 11, the -hierarchy would collapse to its third level. That being said, there is a method specifically tailored to antimonotone DNF formulas.

Lemma 13

Let φ be an antimonotone formula in disjunctive normal form. There are relational databases  and  over the same schema computable in time polynomial in the size of φ such that 
.

Proof

Let 
 be the variables of φ. Define schema 
 by identifying variable 
 with attribute 
. We first describe how to obtain  and subsequently construct a matching database . Let 
 denote the m constituting conjunctive clauses of the DNF formula φ. For each 
, we define the row 
, similarly as in the proof of Lemma 6, as
  See Fig. 5 for an example.

Fig. 5
Download : Download high-res image (51KB)
Download : Download full-size image
Fig. 5. Illustration of Lemma 13. The antimonotone DNF formula φ on the left has the three conjunctive clauses M1,M2,M3. The equivalent instance of Inclusion DependencyIdentity consists of database  in the center and  on the right. There are three maximal inclusion dependencies {a4,a5,a6}, {a1,a3,a6}, and {a2,a5}. Adding any more attributes to either of them would create a hitting set for the conjunctive clauses, corresponding to an unsatisfying assignment.

The second database  is constructed by first creating m copies of . Let × be a new symbol not appearing anywhere in . In the j-th copy of , we set the value for attribute 
 to × whenever 
 occurs in 
. (See Fig. 5 again.) Note that  equals the number of variables of φ and  is linear in the number m of conjunctive clauses, while  is quadratic. The time to compute the pair  is linear in their combined size and polynomial in the size of φ. It is left to show that the indicator function satisfies 
.

First, suppose 
 for some length-n binary vector X or, equivalently, for some subset . We show that 
, meaning that X is an inclusion dependency in . Necessarily, we have 
 for at least one conjunctive clause 
. Since 
 contains only negative literals, all of its variables evaluate to false. This is equivalent to X not containing any attribute that corresponds to a variable in 
. In the j-th copy of  in the database , the values were changed to × for exactly those attributes. Thus, the projection  contains an exact copy of  and X is indeed an IND, resulting in 
.

For the opposite direction, suppose 
. Each conjunctive clause thus contains a variable corresponding to some attribute in X. Consequently, in each row of , there is an attribute in X whose value was replaced by ×. As  does not contain the symbol × at all, X is not an IND and 
. □

Lemma 12, Lemma 13 imply that, given an antimonotone, 3-normalized formula φ, we can build an instance  of Inclusion DependencyIdentity in FPT-time (even polynomial) such that 
. Together with the findings of Section 4.2, this finishes the proof of Theorem 2.

4.4. Discovery
As we did with minimal unique column combinations and functional dependencies, we can lift our results from detecting a single inclusion dependency to discovering all of them. It turns out that there is a parsimonious equivalence with the enumeration of assignments to antimonotone, 3-normalized formulas, as detailed in Theorem 4. The key observations to prove this are once again that the reductions above are polynomial time computable, independently of the parameter, and that they preserve inclusions.

Lemma 12, Lemma 13 describe how to turn the formula φ in polynomial time into a pair of databases over the common schema R, which is effectively the same as 
, such that the inclusion dependencies  are in canonical correspondence with the satisfying assignments 
. Moreover, the parameterized reduction preserves inclusion relations between the solutions such that the maximal dependencies also correspond to the maximal assignments. In other words, Lemma 12, Lemma 13 induce a parsimonious reduction from Enumerate Maximal Satisfying WA3NS Assignments to Enumerate Maximal INDsIdentity. It is also easy to see that Lemma 10 implies a parsimonious reduction from the enumeration of such restricted inclusion dependencies to the general Enumerate Maximal INDs problem. The lemma does nothing else but invalidating all non-identity mappings between the columns. Finally, Lemma 11 shows how to translate general inclusion dependencies back to an antimonotone, 3-normalized formula φ. Observe that the (inclusion-wise) maximal satisfying assignments of the resulting formula correspond exactly to the notion of maximality for general INDs (see Sections 2.2 and 4.1 for details). This shows the equivalence of all the enumeration problems involved. Again, the space complexity is preserved up to polynomial factors by the parsimonious reductions.

We complete the proof of Theorem 4 by showing that the problems are at least as hard as Transversal Hypergraph. This is an easy exercise using the structure of antimonotone CNFs.

Lemma 14

The enumeration of maximal satisfying assignments of antimonotone Boolean formulas in conjunctive normal form is equivalent to Transversal Hypergraph under parsimonious reductions. In particular, Enumerate Maximal Satisfying WA3NS Assignments is at least as hard as the Transversal Hypergraph problem.

Proof

For the reductions, in both directions, we identify the (disjunctive) clauses of an antimonotone CNF formula φ with the sets of variables they contain. To spell it out, let 
 be the set of all variables of φ and 
 the clauses. Since the formula is antimonotone, any 
 is satisfied iff there is a variable 
 that is assigned false. In other words, an assignment 
 (the set of true variables) is satisfying iff its complement 
 
 is a hitting set of the hypergraph 
. Assignment A is maximal in that regard iff 
 
 is a minimal transversal. In the very same fashion, we can construct from any hypergraph  an antimonotone CNF formula on the variable set 
 by setting 
. Complementing the maximal satisfying assignments of φ recovers the minimal hitting sets of .

The second part of the lemma follows from any CNF formula being also 3-normalized by viewing literals as conjunctive clauses of length 1. □

5. Conclusion
We have determined the complexity of the detection problems for various types of multi-column dependencies, parameterized by the solution size. Our results imply that these problems do not admit FPT-algorithms unless the -hierarchy at least partially collapses. In fact, the detection of inclusion dependencies turned out to be surprisingly hard in that it is -complete. Therefore, a small solution size alone is not enough to explain the good performance in practice. This is unfortunate as the choice of parameter appears to be very natural in the sense that the requirement of a small solution size is regularly met in practice. Of course, our results do not preclude FPT-algorithms for other parameters. As an example, Unique Column Combination on databases over schema R is trivially in FPT with respect to the parameter  by checking all subsets. This is of course not very satisfying since assuming the schema to be small is much stronger than assuming the solutions to be small. Similarly, one could consider the maximum number d of attributes on which two rows in the data disagree as parameter. Using the standard bounded search tree of height k with nodes of degree at most d gives an FPT-algorithm with respect to the parameter . Again, the assumption that any two rows in a relation differ only on a few columns seems to be unrealistic for most practical data sets.

More structural research into relational databases is needed to bridge the gap between the worst-case hardness and empirical tractability of dependency detection problems. One needs to identify properties of realistic instances that explain the good performance of practical methods. In turn, such properties can then be exploited, for example, by designing multivariate algorithms with more than one parameter. This may even lead to further improvements in the running time.

On the other hand, our results regarding the discovery of all dependencies of a certain type in a database are indeed able to explain the good run times in practice. We proved that the profiling of relational data at its core is closely related to the transversal hypergraph problem. Although the exact complexity of the latter is still open, there are many empirically efficient algorithms known for it. Most importantly, modern algorithms for the enumeration of hitting sets have the advantage that their space complexity is only linear in the input size, this is a feature many data profiling algorithms are still lacking today [1], [46], [53]. Even more than large run times, prohibitive memory consumption still puts certain databases out of reach for data profiling today. The relations shown here may therefore be a way to improve the current state of the art algorithms in that direction.