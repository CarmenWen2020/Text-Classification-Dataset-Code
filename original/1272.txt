Abstract
As a kind of important soft clustering model, the fuzzy C-means method is widely applied in many fields. In this method, instead of the strict distributive ability in the classical k-means method, all the sample points are endowed with degrees of membership to each center to depict the fuzzy clustering. In this paper, we show that the fuzzy C-means++ algorithm, which introduces the k-means++ algorithm as a seeding strategy, gives a solution for which the approximation guarantee is . A novel seeding algorithm is then designed based on the contribution of the fuzzy potential function, which improves the approximation ratio to . Preliminary numerical experiments are proposed to support the theoretical results of this paper.

Keywords
Fuzzy C-means problem
Seeding algorithm
Approximation algorithm
Approximation ratio

1. Introduction
Clustering analysis is defined as the problem of dividing a group of objects into clusters with similar features [1]. As a classical problem, it is widely applied in data mining [2], image segmentation [3], machine learning [4] and other fields. Those different clustering techniques can be divided into hard or soft clustering types based on whether each sample point clearly belongs to one cluster [5], [6].

The k-means problem is a widely hard clustering model. Given an input set X and a positive integer k, the objective of the k-means problem is to select a k-configuration set that minimizes the sum of distances from each point in X to this set. This problem turns out to be NP-hard [7]. The approximation guarantees of the first constant approximation algorithm and the best algorithm are 108 [8] and 6.357 [9], respectively.

In practice, the Lloyd's algorithm (often called the k-means algorithm) [10] as a heuristic method is most used to solve the k-means problem due to its ease of operation. The algorithm starts with a uniformly selected k initial centers from the input set. Thus all the points can be assigned to their closest centers to form a set of clusters. Then the centers can be updated to the center of mass of the clusters and alternate these two steps until the process stabilizes. On account of its simple structure and low complexity, Lloyd's algorithm has shown its efficiency in many real clustering problems. However, this algorithm still fails in solving many ill-posed problems which cannot obtain an approximation guarantee. In order to provide an accurate guarantee, the k-means++ algorithm [11] is designed by introducing a simple, careful and random seeding strategy to select the initial centers. The underlying idea is that the centers can be selected randomly according to a distribution (often called 
-weighting) which is constructed by the proportions among the contribution of each point to the k-means potential function. The k-means++ algorithm is proved to have an  [11] approximation guarantee and in real applications it improves the performance of Lloyd's algorithm. This seeding strategy has also been used to solve some other generalized problems, see [12], [13], [14], [15].

In the k-means problem, the sample set has the attribute of strict classification. However, in some real applications, this kind of hard clustering ideas may not be suitable since there may not exist strict boundary between different clusters, and an object can be classified into several different clusters. Take the supermarket shopping choice problem as an example. This problem can be described as the k-means problem if the customer prefers going to the closer ones. However, due to various factors that may influence their decisions, a scheme of multiple choices is more appreciated than that of only one. In this sense, the soft clustering models are more reasonable to be applied [16].

At present, the fuzzy C-means problem [5] is a popular soft clustering model. It can emerge a more intuitive and comprehensive relationship between the sample set and center set by introducing the concept of the membership degree. The attributions of sample points are determined to achieve the purpose of fuzzy classification. It is worthy to note that k-means problem is a special case of the fuzzy C-means problem, whose membership degrees can only be taken as 0 or 1. To solve this problem, Bezdek et al. propose the standard fuzzy C-means (FCM) algorithm [17]. This approach requires the calculation of membership degree relative to the size of the data set at each iteration, which is obviously a costly operation. In order to improve the solving time and effectiveness, Stetco et al. propose the fuzzy C-means++ (FCM++) algorithm [1] combining the k-means++ seeding strategy with the standard algorithm. Moreover, Blömer et al. propose a PTAS algorithm for this problem when k is considered as a constant [18]. Some progress has also been made in the comparative study of the k-means problem and the fuzzy C-means problem [19], [20], [21].

The main contribution of this paper consists of two aspects. First, we prove that the FCM++ algorithm with the randomized seeding technique is 
 approximation guarantee. Secondly, a novel seeding algorithm (NFCM) for the fuzzy C-means problem is proposed, where the initial centers are randomly chosen according to a new probability distribution. By applying the NFCM algorithm we can obtain an approximation ratio as . Moreover, some numerical experiments are presented to show the effectiveness of the new algorithm.

The rest of this paper is structured as follows. In Section 2, we introduce the fuzzy C-means problem. In Section 3, the operation mechanism of the standard algorithm and the seeding algorithms is described in detail. The proof of the approximation guarantee is also demonstrated in this section. In Section 4, the main results of this paper are generalized to a more general case. Numerical experiments on the two seeding algorithms are shown in Section 5. In Section 6 we give the conclusions.

2. Preliminaries
The definition of the fuzzy C-means problem can be described as follows. Given a sample set 
 and a positive integer k, the fuzzy C-means problem is to build a k-configuration set 
, which minimizes the potential function
 
 
 where m denotes the fuzzifier parameter and 
 denotes the membership degree from the i-th point to the j-th center, satisfying 
 for . We can also specify the potential function on any subset A of X as 
 
 
 The fuzzy C-means problem can also be regarded as a generalization of the k-means problem. As m approaches 1, the sample points are basically only assigned to the nearest center and the membership relationship with other centers can be ignored. In particular, if , it is obvious that the fuzzy C-means problem simplifies to the k-means. On the contrary, when m is larger, it means that the fuzziness is higher, and the corresponding clustering will become more indistinct. At present, many researchers are devoted to the discussion of the selection of parameter m, and its value usually falls between 1 and 2 [22], [23]. Different choices of m will lead to different optimal solutions of the problem. However, this issue seems not trivial to explain. We construct an example to show that the fuzzy C-means problem will have different optimal solutions for  and 2.

Example 1

Given an input set  and the number of clusterings is . The optimal solution of the k-means problem under this data set is , which are actually two mean values of two pairs of sample points. As for the fuzzy C-means problem, the value of the potential function based on the  is 0.0624. However, there is a better solution  with the corresponding value is 0.0590. Here we list the values for the corresponding membership.

Membership degree	μ1⁎	μ2⁎	μ3⁎	μ4⁎
μ⁎1	0.9624	0.9758	0.0187	0.4341
μ⁎2	0.0376	0.0242	0.9813	0.5659
Therefore, it can be concluded that the optimal solution of these two problems may be different even under the same data set, and in this case, the optimal solution of the fuzzy C-means problem is more competitive.
In the discussion that follows, we assume  for convenience, and the case where m is an arbitrary positive integer is shown in Section 4. The corresponding potential function is defined as
 
 
 It is worth mentioning that each cluster is implicitly defined as a subset of X which is closer to this center than others. We also use 
⁎
 for the optimal potential value, corresponding to the optimal k-clustering 
⁎
 and the optimal membership degree 
⁎
.
Remark 1

Given any clustering set 
, we can get the corresponding membership degree as follows by deriving the first order optimality condition of ,
 
 
 In addition, by substituting the above formula into the original potential function, it can be further obtained in another form as follows, which also plays a crucial role in the later analysis,
 
 
 
 
 
 In particular, when the clustering center is a certain point in X, we define its membership degree to itself to be 1, and to other points to be 0. At this time, the potential value of this point is 0. Conversely, given any 
 with 
, we can obtain the corresponding 
 in the same way,
 

3. The seeding algorithms and main results for 
In this section, we mainly introduce the operating principle of the standard fuzzy C-means (FCM) algorithm and the fuzzy C-means++ (FCM++) algorithm utilizing the k-means++ seeding technique. A novel seeding algorithm (NFCM) is also proposed.

In the FCM algorithm, as shown in Algorithm 1, k initial centers are randomly selected, then the corresponding membership degree 
 and subsequent centers 
 are updated alternatively in each round. This update process continues until the membership degree or clustering set is no longer changed.

Algorithm 1
Download : Download high-res image (83KB)
Download : Download full-size image
Algorithm 1. The standard fuzzy C-means algorithm.

Traditionally, the probability involved in the FCM++ algorithm is called 
-weighting, while the NFCM algorithm is called 
-weighting. To make the distinction easier, we use 
 for the k-means potential function, 
⁎
 and 
⁎
 for the corresponding optimal case.

Theorem 1

Assume that the clustering set C is constructed with the FCM++ algorithm, then there holds,
⁎

Theorem 2

Assume that the clustering set C is constructed with the NFCM algorithm, then there holds,
⁎

Theorem 1, Theorem 2 can be proved together. First of all, Lemma 1 shows that the transition from the k-means problem to the fuzzy C-means problem will result in the generation of a coefficient k, which explains the reason that the approximation guarantee of the fuzzy C-means problem is .

Lemma 1

Assume that A is an arbitrary cluster and 
 is the current clustering set. Then the following two inequalities hold
⁎
⁎

Proof

The first inequality holds obviously, we focus on the second one. Let 
⁎
 be the center of mass of A. Then
⁎
 
⁎
 
⁎
⁎
⁎
⁎
 
 
⁎
⁎
 
 
⁎
⁎
⁎
 □

In fact, the first inequality can be tight if every component of 
 is the same, that is, every point is the same distance from every center.

Lemma 2 to Lemma 5 show that the FCM++ and NFCM algorithms are competitive if we only consider selecting centers from the optimal cluster. The case of selecting the first center is simple and can be proved by triangle inequality.

Lemma 2

Suppose that A is an arbitrary optimal cluster, and C is the current clustering set with just one center, which is selected uniformly at random from A. Then
⁎

Proof

Each point has a 
 
 chance of being selected because a single center is randomly and uniformly selected. Applying the Lemma 1, we can obtain 
 
 
 
 
 
 
⁎
⁎
 
⁎
⁎
 □

The following two lemmas can be deduced simply by induction, which play a crucial role in the proof of the Lemma 5.

Lemma 3

For any  and , we have
 
 
 
 
 
 

Proof

 
 
 
 
 
 
 
 
 □

Lemma 4

For any positive constants 
 and . The following inequality holds
 
 
 
 
 
 

Proof

If , the inequality holds obviously. Now suppose that the inequality is true for , that is
 
 
 
 
 
 
 Then we prove the case for n,
 
 
 
 
 
 
 
 
 
 
 
 
 The last step is given by the Lemma 3. □

If we select one of the remaining centers from the optimal cluster, we can also get another bound on the potential function from the lemma below.

Lemma 5

Suppose that A is an arbitrary optimal cluster, and 
 is the current clustering set. If we add a center to C from A, selected with 
-weighting or 
-weighting, then
⁎
 where 
 is the resulting clustering set.

Proof

The proving process of the two probability is basically the same, and we only analyze the second one. First of all, we state an inequality relationship. Suppose 
 is the next center that we select according to the 
-weighting.(1)
 
 
 
 
 
 
 
 
 
 In particular, when 
, the potential contribution is 0, so it does not affect the proof. Applying the Lemma 1 again, we can obtain
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
  
 
⁎
 
  
 
⁎
⁎
 The second inequality follows from the Lemma 4 and the third follows by (1). □

Finally, we use induction to prove the change of potential function in the general case. An optimal cluster A is uncovered if the center is not selected from A.

Lemma 6

Suppose that C is an arbitrary clustering set,  is the number of “uncovered” optimal clusters, and 
 denotes the set of points in these clusters. Also let 
. Now suppose we add  centers to C and 
 denotes the resulting clustering set.

(i) If the new centers are selected with 
-weighting, then
⁎
 
 (ii) If the new centers are selected with 
-weighting, then
⁎
 
 Here, 
 denotes the t-th harmonic number, and 
.

Proof

The proof procedure is similar for the two probability distributions, and we only analyze the second one. If  and , 
. Then
⁎
 
 If  and , the number of centers selected from  covered clusters or one uncovered cluster is required to be 1. Then the contribution to 
 is at most,
 
 
 
⁎
⁎
 
 Now suppose the result is true for  and , then we prove it holds for the general  as well. Considering two cases.

Case 1. If the first center a is selected from a covered cluster, the number of uncovered clusters remains the same. Then we only need add  centers to . The contribution to 
 in the case  is at most,(2)
 
 
 
 
⁎
 
 
⁎
 
 Case 2. If the first center a is selected from an uncovered cluster A, then A needs removed to the set of covered clusters. The number of uncovered clusters is decreased by one. Note that the Lemma 5 implies 
 
⁎
, we have
 
 
 
 
⁎
 
 
⁎
 
 Summing over all uncovered clusters 
 and we can obtain(3) 
 
 
⁎
 
 
⁎
 
 
 
 
 
⁎
 
 The inequality can be realized by the power-mean inequality 
 
 
 Combining (2) and (3), the contribution 
 is at most,
 
⁎
 
 
⁎
 
⁎
 
 
 
⁎
 
 □

Proof of Theorem 1

Applying the Lemma 2 and Lemma 6 (i) with . Suppose A is the only covered cluster, where the selected center is . Then we have,
⁎
⁎
⁎
⁎
 □

Proof of Theorem 2

Applying the Lemma 2 and Lemma 6 (ii) with . Suppose A is the only covered cluster, where the selected center is , then we have,
⁎
⁎
⁎
⁎
 □

4. Generalization results for an arbitrary constant m
In the previous content, the FCM++ and NFCM algorithms are only applicable to the metric space with the potential 
 with . In this section, we extend our results to the more general fuzzifier parameter 
.

This generalization only involves changing the potential function and the two algorithms themselves. Instead of applying the 
-weighting (cited from Algorithm 3), we switch to the 
-weighting, i.e., we select 
 as the center with probability 
 
 and the 
-weighting (cited from Algorithm 2) stays the same.

Lemma 7

For any 
,  and 
, we have

Proof

Notice that the function 
 of 
 is convex. Dividing both sides by 
, then the LHS is the function value at an average, while the RHS is the corresponding average of the function values. So the inequality can be derived from Jensen's inequality. □

Lemma 8

Suppose that A is an arbitrary cluster and 
 is the current clustering set. Then the following two inequalities hold
⁎
⁎

Lemma 9

Suppose that A is an arbitrary optimal cluster, and C is the current clustering set with just one center, which is selected uniformly at random from A. Then,
⁎

Lemma 10

For any positive constants 
 and 
. The following inequality holds
 
 
 
 
 
 
 
 
 
 

Proof

Let
 
 
 
 
 
 
 
 
 
 
 
 
 By Lemma 4, we can obtain
 
 
 
 
 
 
 Lemma 10 is equivalent to:
 
 
 
 
 From the above equation, we can continue to get
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Now we discuss whether this inequality is true. Suppose 
. Let
 
 
 Then we have
 
 
 Let
 
 Since
 
 
 
 
 
 
 
 
 So 
. According to the monotony property, we can draw the conclusion 
. Thus, the lemma holds. □

Lemma 11

Suppose that A is an arbitrary optimal cluster, and 
 is the current clustering set. If we add a center to C from A, selected with 
-weighting or 
-weighting, then
⁎
 where 
 is the resulting clustering set.

Lemma 12

Suppose that C is an arbitrary clustering set,  is the number of “uncovered” optimal clusters, and 
 denotes the set of points in these clusters. Also let 
. Now suppose we add  centers to C and 
 denotes the resulting clustering set.

(i) If the new centers are selected with 
-weighting, then
⁎
 
 (ii) If the new centers are selected with 
-weighting, then
⁎
 

Theorem 3

Suppose that the clustering set C is constructed with the FCM++ Algorithm, then the corresponding potential function satisfies
⁎

Theorem 4

Suppose that the clustering set C is constructed with the NFCM Algorithm, then the corresponding potential function satisfies
⁎

5. Numerical experiments
In this section, we present three groups of numerical experiments to evaluate the performance of the FCM++ and NFCM algorithms. Due to the randomness of both algorithms, for every fixed instance we run 10 trials and record the means of the potential value (seeding and final) and the number of iterations. The details of the experiments are given in the following three subsections.

5.1. Stability of fuzzy C-means compared to k-means
In this experiment we try to adopt both the k-means and fuzzy C-means model to solve Example 1, which has been defined in Section 2. For the k-means model, we find that there is a 50 percent chance that k-means++ leads to an incorrect solution, such as . By contrast, for the fuzzy C-means model, FCM++ and NFCM are more steady to find the correct solution for this problem (Fig. 1).

Fig. 1
Download : Download high-res image (111KB)
Download : Download full-size image
Fig. 1. The distribution and solution of the 2-means problem computed by the k-means++, FCM++ and NFCM algorithms. Within the k-means++ algorithm, it is conceivable that the optimal solution as shown in the second figure cannot be gotten.

5.2. Similarity between FCM++ and NFCM algorithms
In this experiment, we utilize both the FCM++ and NFCM algorithms to solve two real data instances, as the  and  data sets in the UC Irvine Machine Learning Repository. The , perhaps the most famous database in the area of pattern recognition, consists of 3 classes of 50 instances each, each class referring to a type of iris plant. Each instance records the associated 4-dimensional properties. The complete  contains 4601 instances, each of which corresponds to an e-mail message with a 57-dimensional property marked as either spam or non-spam.

5.3. The advantages of NFCM algorithm
Finally, FCM++ and NFCM algorithms are implemented on the synthetic data set . To produce this data set, we select 16 centers from a 16-dimensional hypercube with side length 1, which are located at the vertices of the cube. Then, around each center we have a normal distribution of 200 random points with variances of 
 
 
. Hence, we get a few well-separated clusters. For this group of self-built data sets, we test the two algorithms by setting  and .

The potential values of all instances after seeding by FCM++ and NFCM are listed in Table 1. In general, NFCM performs better than FCM++, primarily reflected in the lower potential value. To be specific, with the increase of the fuzzier parameter m, the performance gap between the two approaches has also widened. The reason might be that when m is larger, the problem is getting further away from the k-means problem. Therefore, NFCM, the customized seeding strategy for fuzzy C-means problem, can show its more and more obvious advantage.


Table 1. Numerical experiments on the Norm16 data set. For FCM++ and NFCM algorithms, we list the potential value after seeding for different m, k, and variances.

Norm16	k					
FCM++	NFCM	FCM++	NFCM	FCM++	NFCM	FCM++	NFCM	FCM++	NFCM
Var = 1	k = 5	3.74⋅103	3.86⋅103	1.68⋅102	1.72⋅102	7.42	7.47	0.30	0.30	0.01	0.01
k = 10	1.76⋅103	1.76⋅103	21.29	20.95	0.23	0.23	2.26⋅10−3	2.216⋅10−3	2.30⋅10−5	2.17⋅10−5
k = 15	1.17⋅103	1.15⋅103	6.45	6.19	0.03	0.03	1.35⋅10−4	1.27⋅10−4	6.06⋅10−7	5.75⋅10−7
k = 20	0.85⋅103	0.84⋅103	2.67	2.57	7.27⋅10−3	6.88⋅10−3	1.80⋅10−5	1.72⋅10−5	4.52⋅10−8	4.27⋅10−8

Var = 
 
k = 5	1.22⋅107	1.23⋅107	4.81⋅106	3.55⋅106	1.16⋅105	4.98⋅104	4.13⋅103	1.81⋅103	6.00⋅102	1.91⋅102
k = 10	3.31⋅106	3.20⋅106	3.41⋅105	2.23⋅105	2.43⋅103	1.09⋅103	29.45	9.17	0.86	0.18
k = 15	1.47⋅106	1.44⋅106	8.39⋅104	4.12⋅104	2.53⋅102	1.10⋅102	1.28	0.41	0.01	3.42⋅10−3
k = 20	8.37⋅105	8.31⋅105	2.42⋅104	1.28⋅104	46.70	20.16	0.14	0.05	7.37⋅10−4	1.99⋅10−4

Var = 
 
k = 5	6.44⋅108	6.84⋅108	4.12⋅108	3.61⋅108	7.34⋅106	1.44⋅106	3.61⋅105	1.48⋅105	3.35⋅104	1.49⋅104
k = 10	1.41⋅108	1.41⋅108	2.40⋅107	1.52⋅107	1.51⋅105	7.00⋅104	2.07⋅103	5.75⋅102	72.37	14.16
k = 15	5.99⋅107	5.70⋅107	4.33⋅106	2.48⋅106	1.39⋅104	5.11⋅103	85.13	20.84	1.50	0.23
k = 20	3.11⋅107	2.88⋅107	1.15⋅106	6.85⋅105	2.40⋅103	8.09⋅102	8.26	2.09	0.08	0.01
We also report the potentials and the iteration numbers of the problem with more choices of k. There are 15 instances with different fuzzifier parameter m and variances in total and we select 2 of them, as is shown in Fig. 4 and Fig. 5. It is evident that the NFCM could obtain smaller potential values (both final and seeding) while having nearly same scales of iteration numbers with FCM++, which can also be seen in other cases.

Fig. 4
Download : Download high-res image (270KB)
Download : Download full-size image
Fig. 4. Numerical experiments on solving the fuzzy C-means problem (m = 6) applying the FCM++ and NFCM algorithms with the Norm16 data set (variance = 
 
).

Fig. 5
Download : Download high-res image (262KB)
Download : Download full-size image
Fig. 5. Numerical experiments on solving the fuzzy C-means problem (m = 10) applying the FCM++ and NFCM algorithms with the Norm16 data set (variance = 
 
).

6. Conclusions
The contribution of this paper is mainly in two aspects. First, it provides the theoretical analysis and approximation guarantee 
 for the FCM++ algorithm. Secondly, a novel seeding algorithm NFCM is proposed and its approximation ratio  is proved. Numerical experiments also show the competitiveness of these two algorithms in dealing with the fuzzy C-means problem.

In addition, we are working on improving the theoretical analysis, by way of example, trying to change the clustering pattern to make it more suitable for the fuzzy C-means problem; better approximation is guaranteed by lowering the coefficient.