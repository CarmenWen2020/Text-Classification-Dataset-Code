Abstract
The Nearest neighbour search (NNS) is a fundamental problem in many application domains dealing with multidimensional data. In a concurrent setting, where dynamic modifications are allowed, a linearizable implementation of the NNS is highly desirable.

This paper introduces the LockFree-kD-tree (LFkD-tree ): a lock-free concurrent kD-tree, which implements an abstract data type (ADT) that provides the operations Add, Remove, Contains, and NNS. Our implementation is linearizable. The operations in the LFkD-tree use single-word read and compare-and-swap (Image 1 ) atomic primitives, which are readily supported on available multi-core processors.

We experimentally evaluate the LFkD-tree using several benchmarks comprising real-world and synthetic datasets. The experiments show that the presented design is scalable and achieves significant speed-up compared to the implementations of an existing sequential kD-tree and a recently proposed multidimensional indexing structure, PH-tree.

Keywords
Concurrent data structure
kD-tree
Nearest neighbour search
Similarity search
Lock-free
Linearizability

1. Introduction
1.1. Background
Given a dataset of multidimensional points, finding the point in the dataset at the smallest distance from a given target point is typically known as the nearest neighbour search (NNS) problem. This fundamental problem arises in numerous application domains such as data mining, information retrieval, machine learning, robotics, etc.

A variety of data structures available in the literature, which store multidimensional points, solve the NNS in a sequential setting. Samet's book [1] provides an excellent collection of data structures for storing multidimensional data. Several of these have been adapted to perform the parallel NNS over a static data structure. However, both sequential and parallel designs primarily consider the NNS queries without accommodating dynamic addition or removal (modifications) in the data structure. Allowing concurrent dynamic modifications exacerbates the challenge substantially.

The wide availability of multi-core machines, large system memory, and a surge in the popularity of in-memory databases, have led to a significant interest in the index structures that can support the NNS with dynamic concurrent addition and removal of data. However, to our knowledge no complete work exists in the literature on concurrent data structures that support the NNS.

Typically, a hierarchical tree-based multidimensional data structure stores the points following a space partitioning scheme. Such data structures provide an excellent tool to prune the subsets of a dataset that do not contain the target nearest neighbour. Thus, a NNS query iteratively scans the dataset using such a data structure. The iterative scan procedure starts with an initial guess, at every iteration visits a subset of the data structure (e.g. a subtree of a tree) that can potentially contain a better guess, and remained unvisited up until the last iteration, updates the current guess if required, and thereby finally returns the nearest neighbour.

In a concurrent setting, performing an iterative scan along with concurrent modifications, faces an inescapable challenge. Consider the case of an operation op performing a NNS query in a hierarchical multidimensional data structure that stores points from 
 and where Euclidean distance is used. Let 
 be the target point of the NNS. Let us assume that 
⁎
⁎
 is the nearest neighbour of a at the invocation of op. In a sequential setting, where no addition or removal of data-points occurs during the lifetime of op, 
⁎
 remains the nearest neighbour of a at the return of op. However, if a concurrent addition is allowed, a new node with key 
⁎⁎
 may be added to the data structure in a subset that may already have been visited or got pruned by the completion of the latest iteration step. Clearly, op would not visit that subset. Suppose that 
⁎⁎
 was closer to a compared to 
⁎
, if op returns 
⁎
, it is not consistent to an operation which observes that the addition of 
⁎⁎
 completes before op. Similarly, on allowing a concurrent removal operation, a NNS query might return a point as the nearest neighbour wherein the returned point might have got removed before the return of the NNS query itself.

Considering concurrent operations on data structures, linearizability [2] is the most popular framework for consistency. A concurrent data structure is linearizable if each operation in every execution has a linearization point: an atomic point where it appears to take effect instantaneously, between its invocation and response. Thus, forming a sequence of concurrent operations, described by the real-time order of the linearization points, we observe that concurrent operations meet their sequential specifications. In a concurrent setting, we desire linearizability of a NNS query.

Non-blocking progress guarantees are preferred for concurrent operations. In an asynchronous shared-memory system, where an infinite delay or crash failure of a thread is possible, a lock-based concurrent data structure is vulnerable to pitfalls such as deadlock, priority inversion and convoying. On the other hand, in a non-blocking data structure, threads do not hold locks, and at least one non-faulty thread is guaranteed to finish its operation in a finite number of steps (lock-freedom). Wait-freedom is a stronger progress condition that all threads will complete an operation in a finite number of steps.

In recent years, a number of practical lock-free search data structures have been designed: skip-lists [3], binary search trees (BSTs) [4], [5], [6], [7], etc. Despite the growing literature on lock-free data structures, the research community has largely focused on one-dimensional search problems. To our knowledge, no complete design of any lock-free multidimensional data structure exists in the literature.

The challenge appears in two ways: designing a concurrent lock-free multidimensional data structure that supports a NNS and ensuring its linearizability.

One of the most commonly used multidimensional data structures for a NNS is the kD-tree, introduced by Bentley [8]. In principle, a kD-tree is a generalization of the BST to store multidimensional data. Friedmann et al. [9] proved that a kD-tree can process a NNS in expected logarithmic time assuming uniformly distributed data points. Various efforts, including approximate solutions, have contributed to improving the performance of the NNS in kD-trees [10], [11]. Furthermore, several parallel kD-tree implementations have been presented, specifically in the computer graphics community, where the focus is on accelerating the applications, such as the ray tracing in single-instruction-multiple-data (SIMD) programming model [12]. Unfortunately, these designs do not fit a concurrent setting, where linearizability becomes crucial for the correctness of operations. For robotic motion planning, Ichnowski et al. [13] used a kD-tree of 3-dimensional data in which they add nodes concurrently. Their application does not require a removal operation and thus maintaining a kD-tree is only about adding nodes using atomic

Image 1
primitives, which makes it lock-free. They directly transferred the canonical recursive iterative scan used in sequential kD-trees to their concurrent implementation without discussing its consistency. As discussed before, such a canonical implementation of the NNS, using recursive tree-traversal, is not linearizable.
Contributions: We introduce LockFree-kD-tree (LFkD-tree ) - an efficient concurrent lock-free kD-tree (section 2). LFkD-tree implements an abstract data type (ADT) that provides Add, Remove, Contains and NNS operations for a multidimensional dataset. We describe a novel lock-free linearizable implementation of NNS in LFkD-tree (section 4). We provide a Java implementation of the LFkD-tree (code available at [14]). We evaluate our implementation against that of an existing sequential kD-tree and the PATRICIA-hypercube-tree [15]1 (section 6).

1.2. A high-level summary of the work
The main challenge in implementing a linearizable NNS is to ensure that it is not oblivious to the concurrent modifications in the data structure. An NNS requires an iterative scan, which along with pruning collects an atomic snapshot.

In general, concurrent data structures do not trivially support atomic snapshots. Some exceptions are - the lock-based BST by Bronson et al. [16], the lock-free Trie by Prokopec et al. [17] and lock-free k-ary search tree by Brown and Avni [18].

Petrank et al. presented a method to support atomic snapshots in one dimensional lock-free ordered data structures that implement sets [19]. They illustrated their method in lock-free linked-lists and skip-lists. Chatterjee [20] extended the method of [19] to propose lock-free linearizable range search in one dimensional data structures. Winblad [21] presented a lock-based method for concurrent range search. Recently, Fatourou and Ruppert [22] presented a method for wait-free range search in persistent binary search trees.

The main idea in [19], [20] is augmenting the data structure with a pointer to a special object, which provides a platform for an Add / Remove / Contains operation to report modifications to a concurrent operation performing a full or partial snapshot. Nevertheless, collecting an atomic snapshot of a multidimensional data structure to perform an NNS would be naive. We need to adapt the procedure of iterative scan, which benefits from an efficient hierarchical space partitioning structure, to a concurrent setting.

Our work proposes a solution based on augmenting a concurrent data structure with a pointer to a special object called neighbour-collector. A neighbour-collector provides a platform for reporting concurrent modifications that can otherwise invalidate the output of a linearizable NNS.

Essentially, an operation  first searches for an exact match of the target point α in the data structure, and if it succeeds, returns α itself as its nearest neighbour. If an exact match is not found, before starting the iterative scan,  announces itself. The announcement uses a new active neighbour-collector that contains the target point α and the current best guess for the nearest neighbour of α. On completing the iterative scan, it deactivates the neighbour-collector. A concurrent operation, after completing its steps, checks for any active neighbour-collector, and if found, reports its output if it was a better guess than the current best guess available. Finally,  outputs the best guess among the collected and the reported neighbours as the nearest neighbour of α.

Naturally, there can be multiple concurrent NNS operations with different target points, and we must allow each of them to continue its iterative scan, after announcing it as soon as it begins. To handle multiple concurrent announcements, we use a lock-free linked-list of neighbour-collector objects. The data structure stores a pointer to one end of this list, say the head. A new neighbour-collector gets added only at the other end, say the tail.

Consequently, before announcing a new iterative scan, an NNS operation goes through the list and checks whether there is an active neighbour-collector that has the same target point. If an active neighbour-collector is found, it is used for a concurrent coordinated iterative scan (explained in the next paragraph). A neighbour-collector is removed from the lock-free linked-list as soon as the associated iterative scan is completed. Hence, at any point in time, the length of the list is at most the number of active NNS operations.

During an iterative scan, a subset of the dataset is pruned depending on whether the distance of the target point from a bounding box covering the subset is greater than that from the current best guess. Now, if the current best guess at a neighbour-collector is the outcome of multiple already pruned subsets, an NNS that starts its iterative scan at a later time-point, or is slow (or even delayed), will be able to complete much faster. Thus, the coordination among the concurrent NNS, via their iterative scans at the same neighbour-collector, speeds them up in aggregation.

The basic design of the LFkD-tree is based on the lock-free BST of Natarajan et al. [6]. To perform an iterative scan, we implement an efficient fully non-recursive traversal using parent links, which is not available in [6]. Thus, to manage an extra link in each node, our design requires extra effort for the lock-free synchronization. The modification operations – Add and Remove – use single-word-sized atomic

Image 1
primitives. The helping mechanism is based on operation descriptors at the child-links. Consequently, extra object allocations for synchronization are avoided. The linearizable implementation of NNS is not confined to the LFkD-tree, and it can be used in a similar concurrent implementation of any other multidimensional data structure available in [1]. Keeping that in view, we describe the NNS operations independently and comprehensively. We implemented the LFkD-tree algorithm in Java. The code is available at [14].
2. LockFree-kD-tree implementation
2.1. Design overview of the LFkD-tree
The LFkD-tree is a point kD-tree in which each node that stores data is assigned at most one data-point. Typically, to partition 
, we use axis-orthogonal hyperplanes that are given by 
. The structure and consequently the NNS performance of a kD-tree heavily depends on the splitting rule - the procedure to select the partitioning hyperplanes. In a sequential setting, to construct a kD-tree from static data, the partitioning hyperplanes are chosen to coincide with points that belong to the given dataset. In this approach, similar to an internal BST representation [7], each node is used for storing data. However, removing a node from an internal BST is costly, more so in a concurrent setting [5], [7]. With this in mind, we opt for an external BST representation [4], [6] to design the LFkD-tree. In this design, only leaf-nodes contain the data-points and internal-nodes route a traversal, see Fig. 1 (b). More importantly, it gives us the flexibility to compute c and  for a hyperplane 
, which may not coincide with a data-point.

Fig. 1
Download : Download high-res image (236KB)
Download : Download full-size image
Fig. 1. LFkD-tree Structure.

To compute the values of c and i, in the scenarios where the entire dataset is available beforehand, a number of splitting rules exist in the literature [9], [10]. These rules focus on the hierarchical partition of a closed hyperrectangle that covers the entire dataset, thereby not only tries to balance a kD-tree but also optimize its depth. For example, we can see such efforts in the sliding-midpoint rule of Mount and Arya [10], which is also used in the Python kD-Tree library [23]. The median splitting rule, was proposed by Friedmann et al. [9]. They suggested to choose i in a way such that a partitioning hyperplane is orthogonal to the axis of the cell along which its data points have the maximum spread, and c is chosen as the coordinate median of the points along that axis.

In a concurrent setting, where we do not have knowledge of the entire dataset in advance, the partitioning hyperplane needs to be computed dynamically and in a very localized fashion. For the LFkD-tree, we formulate a simple and practical splitting rule, namely the local-midpoint rule, as given in section 2.2.

A leaf-node of a LFkD-tree ϒ, contains a unique data-point as its key, whereas, an internal-node corresponds to a partitioning hyperplane. Without any ambiguity, we denote a leaf-node containing key 
 by

Image 2
(k) (or
Image 2
(
)), and an internal node associated with a hyperplane 
, by
Image 2
(). An internal-node has three links connected to its left-child, right-child and parent. We indicate the link emanating from a node
Image 3
and terminating at a node
Image 4
by
Image 3
↝
Image 4
. Access to ϒ is given by the address of a unique node root. A node
Image 3
is said to be present in ϒ (
Image 5
), if it can be reached following the links starting from the root.
For each internal-node

Image 2
(), ϒ maintains the following invariants (for the nodes in the subtrees rooted at
Image 2
()): (i) a node
Image 2
(
) belongs to the left subtree, if 
, (ii) a node
Image 2
(
) belongs to the right subtree, if 
 and (iii) both subtrees are themselves LFkD-tree. (i) and (ii) together are called the symmetric order of the LFkD-tree. Fig. 1 illustrates the structure of a subtree of a LFkD-tree corresponding to a sample 2-dimensional dataset.
2.2. Sequential behaviour of the ADT operations
LFkD-tree implements an abstract data type kDSet. It provides operations Add, Remove, Contains and NNS. For each operation, we start with a query from the root, traverse down ϒ, at an internal node decide left/right child using the symmetric order until arrived at a leaf-node.

To perform , 
, if the query terminates at a leaf-node

Image 2
(b), 
, and b = a (an element-wise comparison of keys),  returns
Image 6
. However, if b ≠ a, we allocate a new internal-node
Image 2
() with its child links connected to two leaf-nodes
Image 2
(a) and
Image 2
(b). If
Image 7
was the parent of
Image 2
(b) at the termination of query, we connect the parent link of
Image 2
() to
Image 7
. We update the link
Image 8
to point to
Image 2
() and return
Image 9
. To compute i and c, we employ the local-midpoint rule as given below.
Local-midpoint rule: Let i,  be the index of the coordinate axis along which a and b have the maximum coordinate difference; if there is more than one such axis, select the one with the lowest index. Take the hyperplane as 
 
.

To perform , if the key of the leaf-node, where the query terminates, is a i.e.

Image 10
, we modify the link from the grandparent of
Image 2
(a), denoted by
Image 11
, to its parent, to connect the sibling of
Image 2
(a):
Image 12
to
Image 11
. Thereafter we return
Image 13
. If
Image 14
,  returns
Image 15
. To perform , using a similar query we check whether
Image 10
and return
Image 13
or
Image 15
accordingly.
The operation  is non-trivial. On termination of the initial query, if we reach

Image 2
(b) and b = a, clearly the nearest neighbour of a, available in the dataset stored in ϒ, is a itself. However, if b ≠ a, we take b as our current best guess and check whether the other subtree of
Image 7
(the current subtree contains the single node
Image 2
(b)) stores a better guess. Suppose that
Image 16
. Now, any point on the other side of the hyperplane 
 will be at least at a distance 
 from the target point 
. Therefore, if 
 (the Euclidean distance between a and b), we must prune the other subtree i.e. the one rooted at
Image 17
, otherwise we visit it in the next iteration. A subtree is ensured to be visited and not re-visited guaranteeing a traversal back to the root. At the termination of the iterative scan of ϒ, the current best guess is returned as the nearest neighbour of a.
2.3. Lock-free synchronization
As the basic structure of our LFkD-tree is based on an external BST, for the lock-free synchronization in the LFkD-tree, we build upon the lock-free BST algorithm of [6]. The fundamental idea of the design is a lazy remove procedure that is essentially based on a protocol of atomically injecting operation descriptors on the links connected to the node to be removed, and then modifying those links to disconnect the node from the LFkD-tree. If multiple concurrent operations try to modify a link simultaneously, they synchronize by helping one of the pending operations that would have successfully injected its descriptor.

More specifically, to Remove the node

Image 2
(a), as shown in the Fig. 2(b), we use
Image 1
primitives to inject operation descriptors at the links
Image 18
↝
Image 19
,
Image 11
↝
Image 18
and
Image 18
↝
Image 12
, exactly in this order. We call these descriptors
Image 20
,
Image 21
and
Image 22
respectively. An operation descriptor works as an information source about the steps already performed in  and thus a concurrent operation, if obstructed at a link with descriptor, helps by performing the remaining steps. In particular, a
Image 20
at a link indicates that the next step would be to inject a
Image 21
at the link
Image 11
↝
Image 18
, whereas, a
Image 21
indicates that the next step is to inject the descriptor
Image 22
at the link
Image 18
↝
Image 12
. Finally, a
Image 22
indicates the completion of steps of injecting operation descriptors and thereafter the required link updates are done. The helping mechanism ensures that the concurrent Add and Remove operations do not violate any invariant maintained by the LFkD-tree. The steps of a Remove operation are shown in Fig. 2(c). An Add operation uses a single
Image 1
to update the target link only if it is free from any operation descriptor, otherwise it helps the pending Remove. A Contains or NNS operation does not perform help.
Fig. 2
Download : Download high-res image (371KB)
Download : Download full-size image
Fig. 2. LFkD-tree Structure.

We call the

Image 1
step, which injects a
Image 20
at
Image 18
↝
Image 19
, the logical remove of a. After this step, a  that reads
Image 18
↝
Image 19
returns
Image 15
. Accordingly,  helps to complete the pending , if it reads
Image 18
↝
Image 19
with a
Image 20
descriptor, and then reattempts its own steps. The helping mechanism guarantees that a logically removed node will be eventually detached from the LFkD-tree.
To realize the atomic step to inject an operation descriptor, we replace a link using a

Image 1
with a single-word-sized packet of a link and a descriptor. Given a pointer delegates a link, a well-known method in C/C++ to pack extra information with a pointer in a single memory-word is bit-stealing. In a x86/64 machine, where memory allocation is aligned to a 64-bit boundary, three least significant bits in a pointer are unused. The three operation descriptors used in our algorithm fit over these bits.
For ease of exposition, we assume that a memory allocator always allocates a variable at a new address and thus an ABA2 problem does not occur. For lock-free memory reclamation in a C/C++ implementation of the algorithm, a method such as one based on reference counting [24] can be used. Whereas, traditionally a Java implementation uses the JVM garbage collector. Furthermore, to avoid null pointers at the beginning of an application, we use a subtree containing an internal-node and two leaf-nodes which work as sentinel nodes. See Fig. 2(a). The keys in the sentinel nodes maintain 
, , for any data point 
 stored in the LFkD-tree. The sentinel internal-node

Image 2
(
) works as the root of the LFkD-tree and the entire dataset is stored in its left subtree.
3. Linearizable Add, Remove and Contains operation
3.1. Description of the Add, Remove and Contains operations
First, we present the node-structures in the LFkD-tree, which will help in the subsequent discussion. The classes

Image 23
and
Image 24
, which represent an internal- and a leaf-node respectively, are shown in lines 1 and 3 in Algorithm 1. Every
Image 25
, in addition to the fields
Image 26
and
Image 27
that represent the associated hyperplane, has three pointers
Image 28
,
Image 29
and
Image 30
that delegate the left-child, right-child and parent links, respectively. An
Image 31
contains only an array
Image 32
to represent a data-point 
. The node-pointer
Image 33
, line 5, delegates address of the sentinel node
Image 2
(
).
Algorithm 1
Download : Download high-res image (508KB)
Download : Download full-size image
Algorithm 1. The node structure and inlined methods in the LFkD-tree.

Pseudo-code convention. As a convention, if

Image 34
is a field of a class
Image 35
, we use
Image 36
⋅
Image 34
to indicate the field
Image 34
of an instance of
Image 35
pointed by
Image 36
. Note that,
Image 23
and
Image 31
inherit
Image 37
. The methods
Image 38
(),
Image 39
(), etc. denote introducing the corresponding descriptors
Image 20
,
Image 22
, etc. on the pointers. The methods
Image 40
(),
Image 41
(), etc. are used to check if the corresponding descriptors
Image 20
,
Image 22
, etc. are on the pointer. The method
Image 42
() is used to obtain a pointer stripped all the descriptors off it.
The method

Image 43
(), line 13 to 15, which performs a query, returns the pointers to the leaf-node and its parent, where the query terminates. The method
Image 44
() allocates a new
Image 26
.
The operation Add, line 22 to 24, calls

Image 45
() to get the pointer to the node and its parent, either added by itself or already present there, containing its query key, and the result of addition accordingly. Thereafter, Add calls the method
Image 46
(), line 24, and outputs the result. We describe
Image 46
() in Section 4.
The method

Image 45
(), line 25 to 38, attempts to add a new node in the LFkD-tree. It starts with calling
Image 43
(), line 27. If the returned leaf-node-pointer
Image 47
is found containing
Image 20
, it indicates that the node containing the query key is logically removed, and therefore, the method
Image 48
() is called to help the concurrent pending Remove operation, line 37.
Otherwise, the node pointed by

Image 47
is checked whether it contains the query key, line 29, and if found,
Image 15
is returned, line 30.
Image 45
() also outputs the descriptor-free pointers to the leaf-node and its parent where the query terminated. However, if the leaf-node did not contain the query-key, it is checked whether
Image 47
has the descriptor
Image 22
, which indicates a pending Remove of the sibling of the node pointed by
Image 47
; and if
Image 22
is found,
Image 48
() is called, line 31. Only in the case
Image 47
is descriptor-free, the method
Image 44
() is called to allocate a new node, and a
Image 1
executed in the method
Image 49
(), called at line 35, modifies
Image 50
to add the new node. On success of the
Image 1
it returns
Image 13
. The method
Image 48
() is described in Section 3.2.
The Remove operation, line 39 to 49, performs query in a similar way calling

Image 43
(), line 41. At the return of
Image 43
(), if
Image 47
is found to have
Image 20
, it indicates that even if the query key
Image 51
was present in the LFkD-tree, it was already logically removed and therefore Remove returns
Image 15
, line 48. If
Image 47
is free of
Image 20
, we check if the node pointed by
Image 47
contains the query key, and if not, Remove returns
Image 15
, line 43. However, if the pointer
Image 47
is found to have the descriptor
Image 22
, it indicates a pending Remove of the sibling of the node pointed by
Image 50
, and therefore we call the method
Image 48
() to perform helping steps. After return of
Image 48
(), the steps are reattempted. Finally, if
Image 47
was descriptor-free,
Image 20
is injected on it via the method
Image 49
(), line 46, and if it succeeds, the
Image 48
() is called to take further steps and
Image 13
is returned, line 47.
A Contains, line 50 to 55, calling

Image 43
(), returns
Image 13
only if the pointer
Image 47
does not have
Image 20
and the query key matches at the leaf-node pointed by
Image 47
at line 54; else it returns
Image 15
, line 55. Similar to Add, Contains also calls
Image 46
(), which will be explained in Section 4.
3.2. The helping steps
The method

Image 52
(), line 62 to 65, first calls
Image 54
() to fix the
Image 11
, pointed by
Image 55
. And then calls
Image 56
() to complete the remaining steps of Remove. To distinguish between the
Image 21
put by the Remove of left and right child of
Image 18
, we use two types of
Image 21
:
Image 57
and
Image 58
. In the method
Image 54
(), line 75 to 88, if the link was found already tagged, the type of
Image 21
(
Image 57
or
Image 58
) is read using the method
Image 59
. And, if the link was found to be tagged by a Remove of the other child of
Image 18
, first that Remove is helped and then we reattempt, line 81, otherwise we return
Image 55
, line 80. However, if the link
Image 11
↝
Image 18
is found
Image 60
, line 84, it indicates a pending Remove of  and therefore we help it before reattempt. On successfully
Image 61
the link
Image 11
↝
Image 18
, we return the pointer
Image 55
, line 86. Also, if
Image 11
is found not connected with
Image 18
, we return
Image 55
, line 88, and Remove operation terminates because it indicates the completion.
The method

Image 56
(), line 73 to 74, reads the direction of the child whose Remove had tagged the link
Image 11
↝
Image 18
(represented by
Image 62
), line 73,
Image 63
the (sibling) link calling
Image 64
() and finally calls
Image 53
() to perform the remaining steps, see line 74.
In

Image 64
(), line 66 to 72, if the link
Image 18
↝
Image 12
(represented by
Image 65
) was found
Image 66
, line 68, we return this link as it was, because it is guaranteed that the Remove operation that
Image 66
this link, will perform helping before reattempting its
Image 1
to put a
Image 21
in the method
Image 54
(). In that case, the
Image 66
link is further carried to the method
Image 53
() and connected to
Image 18
. If
Image 18
↝
Image 12
is found flagged, we return
Image 12
, represented by the value of
Image 65
without any descriptor i.e.
Image 42
(sa), line 69. On a successful
Image 1
to
Image 22
the link, we return address of
Image 12
represented by
Image 65
, line 72.
Finally, the method

Image 53
(), line 89 to 92, if required, connects the
Image 30
pointer of
Image 12
to
Image 11
, see line 91. And lastly, node a is detached from the LFkD-tree by connecting
Image 12
, represented by
Image 67
, to
Image 11
using a
Image 1
at line 92.
4. Linearizable nearest neighbour search
In this section, we begin with the algorithm that addresses the case where concurrent NNS operations have coinciding target points. We build on it to present the algorithm for general cases without any restriction. However, first we discuss the linearization scenario to present a precise picture of the challenge that our design addresses.

4.1. Linearization argument
As an illustration, consider Fig. 3. In this example, an  operation 
 by thread 
 is concurrent with modification operations by thread 
 in the LFkD-tree. 
 completes operation 
: , to a sub-tree that has already been traversed by 
, then proceeds to complete operation 
:  to a sub-tree that is yet to be traversed by 
. Thus, 
 observes the operation 
 but not 
, even though, to 
, 
 (
 precedes 
). In case 
 returns  as the nearest neighbour, then the operations 
, 
 and 
 can not be linearized as explained in the Section 1.1. Thus, 
 essentially needs to report its modification to 
, after completing its own steps.

Fig. 3
Download : Download high-res image (225KB)
Download : Download full-size image
Fig. 3. Illustration of modification operations concurrent with an NNS(6,4) operation. Light shaded nodes denote nodes currently on the traversal path of NNS and the dark shaded nodes denote roots of sub-trees that have been pruned.

Suppose that 
 got delayed after adding a new node

Image 68
to the LFkD-tree and could not report it to 
. If a concurrent Contains operation, say 
 by thread 
, reads node
Image 68
and later makes modifications to the tree that are observable to 
 and thus linearizable before 
. Similarly, operations 
 and 
 can not be ordered sequentially without violating linearizability.
Therefore, 
 also needs to report its output to 
. Now, given that 
, 
 and 
 are made to report their modifications to 
, we need to change the linearization point of 
. To maintain the order, we put the linearization point of 
 just after reading reports made by concurrent operations before returning the result of the iterative scan.

Certainly, we need to be careful about unnecessary reporting, which may potentially degrade performance. As an illustration, suppose that 
 and 
 both got delayed after their linearization. Now, if invocation of op happened after that, op is guaranteed to read

Image 2
, if
Image 2
contained the nearest neighbour of the target point. But, if in between the linearization of 
 and invocation of op, a concurrent Remove operation removed
Image 2
, op will certainly not read it, and a reporting may render the linearization point of op to be shifted to even before its invocation, which is undesired. To avoid this situation, before every reporting, we first ascertain whether the node to be reported is logically removed by calling the method
Image 40
().
4.2. Concurrent NNS with coinciding target point
When concurrent NNS operations have coinciding target points, they can output same result by adopting a single atomic step, which is performed during the lifetime of one of them, as the linearization point for each of them. The real-time order amongst them can be taken as the order of any pre-decided step, for example, their invocation steps. Thus, essentially they require a single iterative scan. Basically, it is similar to the linearizable snapshot algorithm of [19]. The pseudo-code of the algorithm is given in Algorithm 3.

Algorithm 3
Download : Download high-res image (543KB)
Download : Download full-size image
Algorithm 3. Linearizable NNS operations with single target point in LFkD-tree.

The class

Image 69
, line 94, represents a packet of a data-point, as contained in a leaf-node pointed by the node-pointer
Image 50
, and its distance, given as
Image 70
, from the target point of an NNS. The class
Image 71
, line 95, represents a neighbour-collector: the platform for collecting and reporting the nearest neighbour.
Image 71
contains pointers to two
Image 69
instances:
Image 72
points to one that contains collected data-point during iterative scan by an NNS operation and
Image 73
points to one that contains a data-point reported by a concurrent operation, in addition to the target point
Image 74
. It also contains a boolean
Image 75
, which if set
Image 13
implies an active neighbour-collector; and a neighbour-collector-pointer
Image 76
, which is used in Algorithm 5. The LFkD-tree is augmented with a pointer
Image 77
, line 97, initialized to point to an inactive neighbour-collector.
4.2.1. The coordinated iterative scan
The operation NNS, line 98–103, starts with calling the method

Image 78
(), line 99, to perform the initial query to arrive at a leaf-node. The process of a query down the kD-tree is described in Section 4.3. If the pointer to the leaf-node
Image 47
does not contain the descriptor
Image 20
, which indicates that the node pointed by
Image 47
is not logically removed, and if the query key
Image 51
matches at the leaf-node, which is checked by the distance between
Image 51
and the key at the leaf-node,
Image 51
itself is the nearest neighbour available in the dataset and NNS returns, line 103. Otherwise, NNS calls
Image 79
() to perform further steps and returns the nearest neighbour, line 102. The arrays
Image 80
and
Image 81
are used to support non-recursive traversal, described in Section 4.3.
Image 79
() and called methods thereof are described here.
The method

Image 79
(), line 109–120, starts with checking whether
Image 77
points to an active neighbour-collector, and if it does not, it allocates a new active neighbour-collector and attempts a
Image 1
to modify
Image 77
to point to the new one, line 114. In case
Image 77
was pointing to an active neighbour-collector, the current best guess of nearest-neighbour, as contained in the leaf-node, is attempted to be added to that. On discovering or allocating and adding an active neighbour-collector, the method
Image 82
() is called to perform a coordinated iterative scan, line 119.
Image 82
(), line 104–107, calls the method
Image 83
(), line 105, to perform next iteration that can better the current best guess of the nearest neighbour.
Image 83
() performs a classical non-recursive traversal in the kD-tree. The non-recursive method improves the traversal in a concurrent setting, where the kD-tree can change its structure during the traversal. We describe
Image 83
() in Section 4.3.
Before attempting to add the new guess, contained in a leaf-node, to the neighbour-collector using the method

Image 84
(), it is always checked whether the leaf-node is logically removed by calling the method
Image 85
(). Please note that, given a (possibly stale) pointer to a leaf-node, we can not directly check whether it was logically removed. Therefore, we also supply the pointer to the parent and thus the method
Image 85
(), line 152–156, gets the latest pointer to the leaf-node considering the fact that a new internal-node may get added between the parent of the leaf-node and the leaf-node to be reported.
Image 84
(), line 126–135, is called to add a collected or reported neighbour to an active neighbour-collector. It calls the method
Image 86
(), shown in line 136–139, which returns a new neighbour only if the distance of the “newly guessed neighbour” is less than the distance of the already collected or reported neighbours to the neighbour-collector.
After the iterative scan, the method

Image 87
() is called by
Image 79
() at line 120.
Image 87
(), line 141–142, other than setting the
Image 88
to
Image 15
, also injects a descriptor
Image 89
at both the neighbour-pointers of the neighbour-collector using the method
Image 90
().
Image 90
(), line 146–151, performs a
Image 1
to replace a neighbour-pointer with one that has the descriptor
Image 89
over it, see lines 149 and 150. It ensures that each of the concurrent NNS operations using the same neighbour-collector have the matching view of it after linearization. The method
Image 91
() returns
Image 13
when called on a neighbour-pointer with descriptor
Image 89
. Thus,
Image 84
() can not add a new neighbour in a neighbour-collector if the corresponding pointer is injected with
Image 89
, see line 128.
Finally, the method

Image 92
(), line 143–145, is called by
Image 79
() to select the better candidate between the reported and the collected neighbours of the target point, which is returned to the caller NNS to output. Note that, once a neighbour-collector is deactivated by an NNS, the method
Image 84
() returns 0, line 135. This in turn, immediately terminates the
Image 93
loop in
Image 82
() at line 104. Thus, as mentioned in Section 1.2, we can observe that the coordination among the concurrent iterative scans at the same neighbour-collector helps a delayed NNS to complete faster.
4.3. The non-recursive traversal
The main tool of a non-recursive traversal for the iterative scan is to keep track of an (orthogonal) axis aligned bounding box (AABB) of the points in the subtrees, both visited and pruned. An AABB is described by its two corner points. We use the variables

Image 80
and
Image 81
throughout the algorithms to represent the two corner points. Initially, to begin the query in the operation NNS, the corner points are taken as 
 and 
, see line 98 in Algorithm 3, which cover the entire dataset.
The method

Image 78
(), line 4 to 163, which is called by NNS for the initial query at line 99 in Algorithm 3, starts with the initial AABB as described by the two arrays
Image 80
and
Image 81
with their initial values, and performs a query absolutely similar to the method
Image 43
() to arrive at a leaf-node. At the termination of
Image 78
(), the arrays AABB represent the bounding box that covers every data-point that can be in the sub-tree of the parent of the leaf-node, where it terminates, which has the same direction as the leaf-node with respect to its parent. We follow the convention that an array is always passed by reference and therefore any modification at any element in a method call persists even after the return of the method call. Thus, at the return of
Image 78
(), if the query point did not match at the key of the leaf-node, we go to perform further iterations using the method
Image 83
() with the current bounding box which represents the rectangular region of the Euclidean space that we have covered.
The method

Image 83
(), line 164 to 181, performs an iteration for a better guess of the nearest neighbour given the distance of the current guess from the target point. We input the pointers to the current leaf-node and its parent along with the AABB described by its two corners. The first step is to find the direction of the current sub-tree and then decide whether the other sub-tree of the parent is visited or not, see lines 164, 167 and 168. Basically we check whether the axis-orthogonal hyperplane associated with the parent node is beyond the AABB. Having done that, we check whether the unvisited AABB on the other side of the hyperplane should be visited by checking its distance from the target point and comparing it with the current distance as input, see line 169. Now, if we need to visit the other sub-tree, the method
Image 78
() is called to perform the query and update AABB, line 171, else we traverse back to
Image 33
. When we traverse back to
Image 33
, the AABB is widened to cover both sub-tree rooted at an internal node, see lines 178 and 180.
Thus, the method

Image 82
() repeatedly calls
Image 83
() to perform an iterative scan of the LFkD-tree, see line 105 in Algorithm 3.
4.3.1. The reporting methods
Add or Contains operations after completion, use the method

Image 46
(), line 122–125, to synchronize with concurrent NNS operations.
Image 46
() first checks the status of the neighbour-collector, then calls
Image 86
() to create a neighbour. If the point to be reported is not better than the current best guess available,
Image 86
() returns
Image 94
and then
Image 46
() returns without any change. Otherwise, it checks whether the leaf node with the point to be reported is logically removed by calling the method
Image 85
(), and then calls the method
Image 95
(), which in turn calls
Image 84
() to add the reported neighbour, line 121.
4.4. A general concurrent NNS with multiple target points
To allow multiple concurrent NNS with non-coinciding target points to progress together, we need to have as many active neighbour-collectors as the number of different target points. Essentially, we need to have a dynamic list of neighbour-collectors. In this list, before adding a new neighbour-collector, an NNS must scan through it so that if there was already an active neighbour-collector with a matching target point, coordination among the concurrent iterative scans with coinciding target points can be achieved. For each of the operations in the LFkD-tree to be lock-free, we ensure the lock-freedom of this list as well. Hence, we augment the LFkD-tree with a single-word

Image 1
based lock-free list of neighbour-collectors.
The linearization points remain as before: the concurrent NNS with coinciding target points share an atomic step during the lifetime of one of them as their linearization point with some order among themselves; other operations linearize as described earlier.

The pseudo-code of the algorithm is given in Algorithm 5, in which every method is absolutely same as that in Algorithm 3, except

Image 79
() and
Image 46
(). The list is initialized with two sentinel nodes pointed by
Image 96
and
Image 97
, with
Image 97
⋅
Image 76
set as
Image 96
, as given in lines 182 and 183. A new neighbour-collector is added to this list at one of the ends only, which is just before the node pointed by
Image 96
. The method of maintaining this list is similar to the lock-free linked-list of Harris [25], except the fact that no addition happens anywhere in the middle of the list. Removal of a neighbour-collector, say one pointed by
Image 27
, takes two successful
Image 1
steps: first we inject a
Image 20
descriptor at the
Image 27
⋅
Image 76
using a
Image 1
and then modify the pointer
Image 98
⋅
Image 76
to
Image 99
with a
Image 1
, if
Image 98
and
Image 99
happened to be the pointers to the predecessor and successor, respectively, of the neighbour-collector pointed by
Image 27
. We use the method
Image 38
() to get a word-sized packet of a neighbour-collector-pointer and the descriptor
Image 20
. As before, the method
Image 42
() masks the descriptor off such a packet and does not change a neighbour-collector-pointer. Adding a neighbour-collector takes a single successful
Image 1
similar to [25].
The method

Image 79
(), line 184–201, as called by NNS after the initial query in Algorithm 3, starts with traversing the list. We maintain an
Image 100
variable
Image 101
that indicates the stages of
Image 79
(). Initially, the
Image 101
is
Image 102
. During the traversal, if an active neighbour-collector with matching target point is found, the
Image 101
is changed to
Image 103
and traversal terminates, line 194. Otherwise, the traversal terminates in the
Image 101
Image 102
itself. On the termination of the traversal in the
Image 101
Image 102
, it is checked whether the neighbour-collector, where traversal terminated (in this case
Image 27
), is already logically removed, line 196, and if it is, a
Image 1
is attempted to detach it from the list and the traversal is restarted, line 197.
After that, if the

Image 101
is
Image 102
or
Image 103
, the method
Image 104
() is called.
Image 104
(), line 221–229, if called in the
Image 101
Image 102
, allocates a new neighbour-collector by calling the method
Image 105
(), otherwise uses the input neighbour-collector. If
Image 105
() could not add a new neighbour-collector, it returns
Image 94
and the entire process restarts from scratch with a fresh traversal. After successfully adding a new neighbour-collector to the list or asserting that it needs to use an existing one,
Image 104
() calls the methods
Image 82
() and
Image 87
() similar to those in Algorithm 3. On deactivating the neighbour-collector, the method
Image 106
() is called to remove it from the list and return the value of the nearest neighbour.
Image 106
(), line 206–212, performs the two
Image 1
steps to remove the neighbour-collector and calls the method
Image 92
(), line 211, to compute the nearest neighbour. However, if after injecting
Image 20
, it could not modify the
Image 76
pointer of the predecessor, it returns
Image 94
, which again causes a fresh traversal in the
Image 101
Image 107
in
Image 104
(). A traversal in
Image 101
Image 107
, on discovering the deactivated neighbour-collector, calls the method
Image 106
(), line 191, to redo the remaining steps and return the nearest neighbour. If the traversal terminates in the
Image 101
Image 107
, that implies that a concurrent NNS would have detached the deactivated neighbour-collector and therefore
Image 92
() is called to finish, line 201.
4.4.1. Approximate nearest neighbour search
Practitioners prefer better query latency at the cost of exact solution in various applications that require a nearest neighbour search, which is commonly known as approximate-Nearest Neighbour (ANN) [26], [27], [11], [28]. Consider a target point 
 of the NNS, given , we say that a point 
⁎
 is the -ANN of q if
⁎
 where k is the true nearest neighbour to q.

Generally, in a hierarchical multidimensional data structure like kD-tree, ANN algorithms relax the pruning criterion so that an NNS operation visits lesser number of subsets and thereby it speeds up the performance. Implementing ANN in a concurrent hierarchical multidimensional data structure does not impact the design-complexity as long as we follow the same consistency framework.

5. Correctness and lock-freedom
We present a detail proof of the presented algorithm. The proof structures as defining the key notions, presenting the invariants of the data structure and finally proving the correctness in terms maintenance those invariants and then satisfying the requirements of linearizability and lock-freedom.

Shared Memory System: We consider an asynchronous shared memory system  which comprises a set of word-sized objects  and a finite set of processes  and supports primitives

Image 108
,
Image 109
and
Image 1
(compare-and-swap).  guarantees that the primitives are atomic i.e. they take effect instantaneously at an indivisible time-point [29]. Each object  has a unique address, commonly known as a pointer to v, denoted by v⋅
Image 110
.
Image 1
(
Image 111
⋅
Image 110
,
Image 112
,
Image 113
) compares the value of
Image 111
with
Image 112
and on a match updates it to
Image 113
in a single atomic step and returns
Image 13
; else it returns
Image 15
without any update at
Image 50
. Let . Processes  communicate by accessing the objects  using a primitive. A configuration 
 of  specifies the value of each of  and the state (values of local variables, etc.) of each of  at time t. The initial configuration 
 represents the initial value of each of  and the initial state of each of .
Abstract Data Type: The multidimensional data indicates the set of points 
 and distance refers to Euclidean distance 
 ∀ 
. Let 
 be the set of all countably finite subsets of 
. Let 
 and 
. Let

Image 114
. An abstract data type (ADT) kDSet is specified as a set of mappings  as defined below:
Definition 1 ADT operations

1.
 s.t.

Image 115
if 
Image 116
Image 117
if .
2.
 s.t.

Image 118
if 
Image 116
Image 119
if .
3.
 s.t.

Image 120
if 
Image 116
Image 119
if .
4.
 s.t. 
⁎
 where 
⁎
⁎
.

Data Structure: A LFkD-tree ϒ stores points from a dataset 
. The state of ϒ in configuration 
, denoted ϒ
, stores points from 
. For an unbounded and dynamic design, ϒ is constructed using nodes and links that are assembled of the objects . In Section 2.1, we described the structure of the LFkD-tree in detail. The access of ϒ is availed by

Image 33
- the address of a fixed sentinel node. The left (right)-subtree of a node
Image 3
, denoted by
Image 3
⋅
Image 121
(
Image 3
⋅
Image 122
), is the set of nodes comprising of the left (right)-child and all its descendants.
Operation Descriptors: An operation descriptor is a boolean variable. A link represented by a pointer, which occupies a single object , is called to be injected with a descriptor

Image 123
if a test for
Image 123
on the link returns
Image 13
. A descriptor
Image 123
can be
Image 20
,
Image 22
,
Image 57
or
Image 58
. We call a link
Image 124
if it is not injected with any descriptor. At any time , a node
Image 3
is said to be present in ϒ
, denoted by
Image 125
, if it can be reached following links starting from
Image 33
and the link that connects its parent to itself is not injected with the descriptor
Image 20
. If ϒ
 stores 
 then
Image 126
.
Implementation: An implementation 
 of kDSet is an algorithm, which implements mappings  using operations on ϒ. We call the implementation full if , otherwise it is called partial. We assign the operations same name as its corresponding mapping. Thus, a mapping , where 
, 
 and 
, is implemented by an operation  that outputs

Image 13
or
Image 15
and makes appropriate changes in ϒ storing . An  is implemented by NNS (k), which outputs a point 
⁎
 according to the mapping definition.
Operation Steps: A process  performs an operation op as a set of steps. Often we group a subset of steps in op as a

Image 127
, which is called from inside of op. A step , where g and h are the values of the object v before and after the execution of s, comprises at most one execution of a primitive and can contain some calculations over process-local variables of p. The execution-point of s is the point on a real time-line where its atomic primitive takes effect. We denote the invocation and response steps of op by 
 and 
, respectively. The execution-points of 
 and 
, denoted by 
 and 
, are called the invocation point and response point, respectively. 
 specifies the initial configuration 
.
Execution History: An execution α of 
 is a (finite or infinite) sequence of steps performed by the processes , starting from 
. A history  of α is its subsequence consisting of the invocation and response steps. A subhistory of  is its subsequence. A process subhistory of , denoted by 
 is its subsequence containing steps executed by a . We call histories  and 
 equivalent, denoted 
, if , 
. In , a response step of an operation op matches an invocation step if the two are performed by the same process. A history is called sequential, if the first step is an invocation and every invocation step, except possibly the last one, follows by a matching response step. We assume that every history  is well-formed: , 
 is sequential. An operation in a history is effectively the pair of its invocation and response steps. Let 
 and 
 be two operations in . We call 
 precedes 
 in , denoted 
 
, if 
. We call two operations 
 and 
 concurrent in , if neither precede the other.  is called concurrent if it contains at least one pair of concurrent operations.

Extension and Completion of History: We call an invocation s pending in , if  does not contain a matching response to it. An extension of , denoted , is obtained by appending matching response steps to every pending invocation in . A completion of , denoted by , is obtained by either (a) dropping all the pending invocation steps from , or (b) extending , or (c) dropping some of the pending invocation steps and extending the remaining  (this case will be clear after we define the linearization point below).

Consistent Sequential History: A sequential specification of 
 is a set of sequential histories. Let 
, where  is a sequential history. Let ϒ
 and ϒ
 be the states of ϒ at 
 and 
, which store the datasets 
 and 
, respectively. We call the operation op consistent with respect to the ADT kDSet in  if the output arguments at the response and 
 satisfy the corresponding mapping definition of kDSet. The sequential history  is consistent if each operation in it is consistent.

Definition 2 Linearizability

A history  is linearizable if 
 and a consistent sequential history  s.t. (a) 
 and (b) 
 
 
. We call an implementation 
 linearizable if every execution history of 
 is linearizable.

The most common approach to prove linearizability is: (a) define linearization point of each operation op as the execution-point of a step, called linearization, which should be between the invocation and response point of op then (b) in an arbitrary history  append appropriate response (in any arbitrary order) of all the operations which have performed their linearization, then (c) drop the invocation steps without a matching response to obtain , and (d) construct a sequential history  by arranging the invocation-response pair of operations according to their linearization points. It is easy to argue that . And, finally, show that the constructed sequential history  is consistent.

Below we list out the linearization points of the operations as the following:

Definition 3 Linearization points

1.
For a successful Add operation, it is at line 9 or line 11 in the method

Image 49
(), which is called at line 35 in the method
Image 45
() and which in turn was called by Add.
2.
For a successful Remove operation, it is at line 9 or line 11 in the method

Image 49
(), which is called at line 46 in Remove.
3.
For an unsuccessful Add and a successful Contains operation it is at line 14 in the method

Image 43
() called from these operations.
4.
For an unsuccessful Contains and Remove operation, it can be either just after the linearization point of a concurrent Remove operation or at the invocation point of these operations.

5.
6.
For an NNS operation, if it returns a data-point which was contained in a reported-neighbour, the linearization point is just after the linearization point of either Contains or Add that reported the neighbour.

It is easy to observe in Algorithm 1, Algorithm 3 that these linearization points are in between 
 and 
 for respective .
Now with that, given any concurrent execution history  of an implementation 
, where , we form an equivalent sequential history  by following the steps as described above. And thus it remains to be shown that such a sequential history will be consistent.

To do that, we essentially show that the invariants of the LFkD-tree, as stated in Section 2.1 are maintained, and the sequential specifications as described in Section 2.2 are satisfied by the consistent operations. Because the implementation of the lock-free list of neighbour-collectors is orthogonal to the implementation of the LFkD-tree, we also need to show that the invariants of the list, as stated in section 4.4, are maintained by the NNS operations. Therefore, here we state the invariants and present some observations and lemmas which help us to show that the invariants are maintained.

Given a LFkD-tree ϒ, for every internal-node

Image 2
() and a leaf node
Image 2
(
), ϒ maintains the following invariants:
Invariant 1

A node

Image 128
(
) belongs to the left subtree, if 
.
Invariant 2

A node

Image 128
(
) belongs to the right subtree, if 
.
Invariant 3

A node

Image 128
(
) belongs to the right subtree, if 
.
A LFkD-tree state ϒ
 that satisfies the Invariants 1 to 3 is called a valid state. Now, for the list of the neighbour-collectors, we denote a neighbour-collector by

Image 129
(
) if the target point that it contains is 
. A neighbour-collector list maintains following invariant:
Invariant 4

In the list there can not be two neighbour-collectors

Image 130
(
) and
Image 130
(
) such that 
.
To prove that the above invariants are maintained throughout the algorithms, we provide the following observations, claims, and lemmas.

Observation 1

The fields

Image 131
and
Image 132
are never changed in a
Image 133
.
Observation 2

Any link in a LFkD-tree is updated only using a

Image 134
.
Observation 3

The sentinel nodes are never removed.

Observation 4

The

Image 135
pointer of the node
Image 136
is never dereferenced.
Going through the pseudo-code we can observe that once we allocate a node, we never call any store step on the fields

Image 32
and
Image 26
and any pointer update is done using a
Image 1
. The choice of keys in the sentinel nodes verifies the third observation. The
Image 30
pointer of an internal node is dereferenced only if a Remove operation on any of its children is called. Thus Observation 3, implies Observation 4. We further make the following claims.
Claim 1

In each call of

Image 137
(), line 6, variable
Image 138
represents a pointer which is
Image 139
and points to an internal-node and thus is not
Image 140
.
Claim 2

In each call of

Image 141
(), line 7,
Image 142
is
Image 139
and points to an internal-node and thus is not
Image 140
.
Claim 3

In each call of

Image 143
(), line 8 to 12,
Image 142
is
Image 139
and points to an internal-node, whereas
Image 144
is
Image 139
and points to a leaf-node; thus
Image 142
and
Image 144
are both not
Image 140
.
Claim 4

In each call of

Image 145
(), line 13,
Image 146
is
Image 139
and points to an internal-node, whereas
Image 147
is
Image 139
and points to a node (internal or leaf); thus both are not
Image 140
.
Claim 5

In each call of

Image 145
(), line 13,
Image 146
and
Image 147
satisfy
Image 148
.
Claim 6

In each call of

Image 149
(), line 62,
Image 146
is
Image 139
and points to an internal-node, whereas
Image 147
is
Image 139
and points to a leaf-node; thus both are not
Image 140
.
Claim 7

In each call of

Image 150
(), line 89,
Image 151
and
Image 146
are
Image 139
and point to two different internal-nodes, whereas
Image 152
is eitherpoints to a leaf-node and thus are not
Image 140
.
Claim 8

In each call of

Image 153
(), line 73,
Image 151
is
Image 139
and points to an internal-nodes, whereas
Image 154
is either
Image 155
or
Image 156
and points to an internal-node and thus are not
Image 140
.
Claim 9

In each call of

Image 157
(), line 75,
Image 146
and
Image 147
are
Image 139
.
Image 146
points to an internal-nodes, whereas
Image 147
points to a leaf-node and thus both are not
Image 140
.
Claim 10

In each call of

Image 158
(), line 66,
Image 146
is
Image 139
and points to an internal-node and thus is not
Image 140
.
Claim 11

A pointer once injected with a descriptor

Image 159
,
Image 160
,
Image 155
or
Image 156
is not injected with any descriptor ever after.
Claim 1, Claim 11 provide a base to prove that at no point an implementation of the presented algorithm faces a segmentation fault due to the dereferencing of a

Image 94
pointer during the operations Add, Remove and Contains. Proving these claims require inspecting the pseudo-code in Algorithm 1, Algorithm 2. At each call of a method we find that the inputs to the method follow the requirements to prove these claims. Subsequently, a listing of the lines of the pseudo-code containing call of these methods verifies these claims. The statements of this set of claims are what we need to prove the next set of lemmas which provides the verified base for post-conditions of the LFkD-tree operations. The above claims essentially direct us proving the following Lemma 1.
Lemma 1

At the termination of

Image 145
at line 15,
(a)
Image 146
points to an internal-node and is
Image 139
.
(b)
Image 147
points to a leaf-node and can be either
Image 139
or
Image 159
or
Image 160
.
(c)
Image 146
and
Image 147
satisfy
Image 148
.
(d)
Image 161
.
(e)
Image 162
.
Following from Claim 4, Claim 5, the while loop ensures that the variable
Image 163
always points to one of the child-pointers of the node pointed by
Image 164
; this validates Lemma 1 (a), (b) and (c).
Now, Claim 11 provides that if the

Image 1
steps are orderly in a Remove operation, it does not result in a malformation of the LFkD-tree. Also, for an Add operation, because the single
Image 1
that it requires can not happen over a link with descriptor, an incorrect addition is avoided.
Now, the keys in the sentinel nodes vacuously prove the following Lemma 2, which provides base condition for an induction to prove Theorem 1.

Lemma 2

Initially, the LFkD-tree consisting of the sentinel nodes satisfies the invariants as stated in section 2.1.

Now we are prepared to prove Theorem 1. We use induction to prove it. Using Lemma 2, when no update has happened, the nodes in the LFkD-tree satisfy the invariants. It is straightforward to observe that no Contains or NNS operation involves a write (

Image 1
) step and therefore they do not change the state of the LFkD-tree. From Lemma 1, at the end of every call to
Image 43
, which satisfies the symmetric order of the LFkD-tree, a
Image 1
to Add does not violate the Invariants 1 to 3. For a Remove operation, after the
Image 1
to logically removing the node i.e.
Image 20
Image 1
, the order of
Image 1
do not let any update operation let the node reappear in the LFkD-tree following Claim 11.
Thus if the state of the LFkD-tree was consistent before the application of an update operation, it remains so after its linearization. Using induction, Theorem 1 follows.

Theorem 1

At any time  the LFkD-tree state ϒ
 is a valid state.

Now considering the neighbour-collector-list, its semantics are absolutely same as those of Harris's lock-free linked list [25] and which was further improved by Micheal [30]. A very sophisticated proof of the state change and thus validity of the list algorithm was provided by Micheal [30]. The invariant maintained our list, Invariant 4, can be proved along the same lines and we skip the detail here. Now, we prove the linearizability of the implementation 
 as given below.

Theorem 2

(Correctness) The operations Add, Remove, Contains and NNS are linearizable with respect to the abstract data type kDSet.

Proof

We show that a sequential history  obtained by following the steps: (a) in an arbitrary history  append appropriate responses (in any arbitrary order) of all the operations which have performed their linearization steps as defined in Definition 3, (b) drop the invocation steps without a matching response to obtain , and (c) construct  by arranging the invocation-response pair of operations according to their linearization points, is consistent.

Let 
 be a sub-history of  that contains the first n complete operations. Let 
 be the dataset which was added to the LFkD-tree by the successful Add operations in 
. Let 
 be the dataset which was removed from the LFkD-tree by the successful Remove operations in 
. Let 
. We use (strong) induction on n to show that 
 is consistent ∀ .

Suppose that 
 is consistent . Let the 
 operation in 
 be , where 
. Then for 
 we prove the following:

1.
Let  be an Add operation.

(a)
Let  returns

Image 13
. We show that if 
 is an Add operation such that 
 
  and 
 returns
Image 13
then ∃ a Remove operation 
 such that 
 
 
 
  and 
 returns
Image 13
.
Suppose there does not exist such a Remove operation. Now, following Lemma 1, at the termination of

Image 43
(), line 14 in Algorithm 1,
Image 164
↝
Image 163
is a leaf-node pointer. Now using the construction of 
 and Definition 3-(1), at the linearization of op, it performed a successful
Image 1
at the link
Image 164
↝
Image 163
which must have been
Image 124
. Using the same argument 
 also performed a successful
Image 1
at the link
Image 164
↝
Image 163
which must have been
Image 124
. Now because 
 linearized before op, the set of nodes that the
Image 43
() called from op, terminates at, by the consistency of 
 op must find k being the key at that leaf-node. Now unless the link
Image 164
↝
Image 163
was already injected with the descriptor
Image 20
, op would not have continued beyond the termination of
Image 43
() and reading the descriptor at it and thereby returning
Image 15
. Therefore, there must have been a Remove operation which marked the link
Image 164
↝
Image 163
before op read and thus it had the linearization point before that of op. This is a contradiction.
(b)
Let  returns

Image 15
. We show that ∃ an Add operation 
, which returns
Image 13
, such that 
 
  and ∄ a Remove operation 
, which returns
Image 13
, such that 
 
 
 
 .
Suppose the contrary. Then at the termination of

Image 43
(), line 14 in Algorithm 1, by Definition 3-(3) the link
Image 164
↝
Image 163
is
Image 124
and
Image 163
⋅
Image 32
= k. But, following (a) as above and the consistency of 
, there must exist an 
 in 
 which returns
Image 13
and that does not precede an 
 which returns
Image 13
- which contradicts our assumption.
Now, it is easy to see that after the linearization of an Add operation that returns
Image 13
, the node added by it is reachable from
Image 33
following the links and thus that node belongs to the LFkD-tree which in turn implies that 
. Thus, combining this fact with (a) and (b) together, the mapping definition of Add, Definition 1-(1), is satisfied. Thus, Add is consistent in 
.
2.
Let  be a Remove operation.

(a)
Let  returns

Image 13
. We show that if 
 is a Remove operation, which returns
Image 13
, such that 
 
  then ∃ an Add operation 
, which returns
Image 13
, such that 
 
 
 
 . We use similar argument as given in (1) to prove it.
(b)
Let  returns

Image 15
. We show that one of the following is
Image 13
:
i.
If 
 is a Remove operation, which returns

Image 13
, such that 
 
  then ∄ an Add operation 
, which returns
Image 13
, such that 
 
 
 
 .
Suppose the contrary is true. Then, because 
 return

Image 13
, by the construction of 
 and the definition of the linearization point Definition 3-(2), either a leaf-node does not exist with key k or the link to it is injected with
Image 20
. Now if that is the case and op also returns
Image 13
, then there must have been a link to a leaf-node with key k which was
Image 124
. But that was possible only if an Add existed before op, which added a leaf-node with key k. This contradicts our claim.
ii.
There ∄ an Add operation 
, which returns

Image 13
, and 
 
 .
We can observe that at the linearization of , the link to the leaf-node with key k gets injected with
Image 20
and thus after that 
. Combining this fact with (a) and (b) satisfies the sequential specification of Remove - Definition 1-(2). Thus, Remove is consistent in 
.
3.
Let  be a Contains operation.

(a)
Let  returns

Image 13
. We show that ∃ an Add operation 
 such that 
 
  and ∄ a Remove operation 
 such that 
 
 
 
 .
The arguments are similar to (1)(b) above.

(b)
Let  returns

Image 15
. We show that one of the following is
Image 13
:
i.
If 
 is a Remove operation, which returns

Image 13
, such that 
 
  then ∄ an Add operation 
, which returns
Image 13
, such that 
 
 
 
 .
ii.
There ∄ an Add operation 
, which returns

Image 13
, and 
 
 .
The arguments are similar to (2)(b) above. Combining (3)(a) and (3)(b), Contains is consistent in 
.
4.
Let  be an NNS operation that returns 
⁎
. We show that (a) there ∃ 
⁎
 such that 
⁎
 
  and (b) if there ∃ 
⁎⁎
, which returns

Image 13
, where 
 is either Add or Contains and 
⁎⁎
⁎
 such that 
⁎⁎
 
  then there ∃ a Remove operation 
⁎⁎
, which returns
Image 13
, such that 
⁎⁎
 
 
⁎⁎
 
 .
To prove (a), it is easy to see that if such an Add did not exist preceding op then at the linearization of op it can not read a leaf-node containing 
⁎
. Therefore, (a) is

Image 13
.
Now, for (b), suppose the contrary is

Image 13
. Thus, if there did not exist a Remove operation 
 then at the linearization of op, which is either at the termination of the method
Image 78
() called by itself or at the termination of the method
Image 43
() called by reporting Contains or at the
Image 1
step performed by a reporting Add operation, the leaf-node containing 
⁎⁎
 must have been connected by a
Image 124
link. But then either op would have read the
Image 124
link to the leaf-node with 
⁎⁎
 or the operation reporting to it would have done the same. Thus the method
Image 92
() that is called by NNS before its return, by virtue of 
⁎⁎
⁎
, would have returned 
⁎⁎
 which in turn would have been returned as the nearest neighbour of k by op. Which is a contradiction. Thus, NNS is consistent in 
.
By (1) to (4), 
 is consistent whenever 
 is consistent . Thus, using induction, 
 is consistent for every positive integer n. □
Theorem 3

(Lock-freedom) The LFkD-tree operations Add, Remove, Contains and NNS are lock-free and thus the presented algorithm implements a lock-free LFkD-tree.

Proof

We take the NNS operation separately because it also involves the steps related to the lock-free list. By the description of the algorithm, a non-faulty thread performing a Contains will always return unless its search path keeps on getting longer forever. If that happens, an infinite number of Add operations would have successfully completed adding new nodes making the implementation lock-free. So, in the context of Add, Remove and Contains, it will suffice to prove that the modify operations are lock-free.

Suppose that a process  performs a modify operation op on a valid state of LFkD-tree ϒ
 and takes infinite steps and no other modify operation completes after that. Now, if no modify operation completes then ϒ
 remains unchanged forcing p to retract every time it wants to execute its own modification step on ϒ
. This is possible only if every time p finds the injection point of op with descriptor

Image 20
,
Image 22
,
Image 57
or
Image 58
. This implies that a Remove operation is pending. It is trivial to observe in the method Add that if it gets obstructed by a concurrent Remove, then before retrying after recovery from failure, it helps the pending Remove by executing all the remaining steps of that. We can also observe that whenever two Remove operations obstruct each other, one finishes before the other. It implies that whenever two modify operations obstruct each other one finishes before the other and so ϒ
 changes. It is contrary to our assumption. Hence, by contradiction we show that no non-faulty process shall remain taking infinite steps if no other non-faulty process makes progress where the executed operation is either Add or Remove.
Now we consider an NNS with concurrent Add, Remove or Contains operations. We consider the case where concurrent NNS operations do not necessarily have coinciding target points; this case obviously covers the case when they do have coinciding target points. We can see that a Remove operation does not have to report to a concurrent NNS operation. Moreover, an Add or a Contains operation to perform a reporting, needs to first traverse through the unordered list and then possibly perform a

Image 1
if required to report. Now, unless the number of NNS operations keep on increasing infinitely, the total length of the unordered list will be finite and thus the traversal path for an Add or a Contains operation to report will be finite. Now, at each neighbour-collector, where the reporting is required, if a
Image 1
to report fails, that implies that a concurrent Contains or Add operation succeeds. Similarly, when a
Image 1
by a NNS operation fails, it indicates that a
Image 1
by a concurrent NNS operation succeeded. Finally, a
Image 1
to add a new neighbour-collector only indicates that either a new neighbour-collector by a concurrent NNS has been successfully added or a NNS operation has terminated. In case of a
Image 1
failure to add a new neighbour-collector, a NNS operation always helps a concurrent pending NNS operation before reattempting, in case it finds the link with descriptor
Image 20
. It shows that in all cases at least one non-faulty thread succeeds with respect to execute a NNS operation concurrent to any other LFkD-tree operation. Thus we arrive at Theorem 3. □
This concludes the proof of the presented algorithm.

6. Experimental evaluation
6.1. Experiment setup
We implemented the LFkD-tree algorithm in Java using run-time type identification (RTTI). We used the library objects

Image 165
to perform
Image 1
. The test environment comprised a dual-socket server with a 2.0 GHz Intel (R) Xeon (R) E5-2650 with 8 physical cores each (32 hardware threads in total with hyper-threading enabled). The server has 64 GB of RAM, runs Ubuntu 13.04 Linux (Kernel version: 3.8.0-35-generic x86_64) with Java HotSpot (TM) 64-Bit Server VM (build 25.60-b23), and we compiled all the implementations with javac version 1.8.0_60.
Data Structures: For experimental evaluation, in addition to our designs, we considered two other implementations that support NNS. The implementations in the evaluation are:

1
Levy-Kd : An implementation of kD-tree of [31] by Levy [32] that supports Remove operations (we could not find any other Java implementation of a kD-tree with Remove ). To allow for concurrent access, we augmented the implementation with coarse-grained

Image 166
3 lock.
2
LFKD : Our implementation of the LFkD-tree with NNS.

3
A-LFKD : Our implementation of the LFkD-tree with approximate-NNS ().

4
PH-tree : A multi-dimensional storage and indexing data structure by Zäschke et al. [15] that supports Remove operations. Similar to Levy-Kd , we add coarse-grained

Image 166
lock to allow for concurrency.
Workload and Methodology: We run each test for 5 seconds and measured throughput as the total number of operations per microsecond executed by all threads in this time duration. We run each experiment in a separate instance of the JVM, starting off with a 2-second “warm-up” period to allow the Java HotSpot compiler to initialize and optimize the running code. During this warm-up phase, we performed random Add, Remove and Contains operations, and then flushed the tree. At the start of each execution, the data structure is pre-filled with keys in the selected key-range.

To simulate the variation in contention and tree structure, we chose following combination of workload configurations: i) dataset space dimension ∈ , ii) distribution of (Add -Remove -NNS ) ∈ , , and iii) number of threads ∈ .

We did not include Contains operations in experiment because essentially it would increase the proportion of exact-match NNS. All executions use the same set of randomly generated points for the selected workload characteristics. The graphs present average of throughput over 6 runs of each experiment.

6.2. Datasets
We performed evaluation using a 2D real-world dataset and a set of synthetic benchmarks. For the real-world dataset, we used the United States Census Bureau 2010 TIGER/Line KML [33] dataset that consists of polylines describing map features of the United States of America. TIGER/Line is a standard dataset used for benchmarking spatial databases [15]. For this evaluation, we extracted points representing the mainland, resulting in ⁎
 unique 2-d points, with x-y coordinates that lie between -124.85  -66.89 and 24.40  49.38.

To investigate more variable workloads, two synthetic datasets were utilized. The SKEWED data simulates datasets in which different dimensions may have varying distributions. The SKEWED(α) dataset contains uniformly distributed points which fall within 0.0 and 1.0 in every dimension that have been skewed in the y-dimension. For each point in the dataset, the y value is replaced with the value 
, for example in the 2-dimension case, each point  is replaced with 
. In the Fig. 5(a), we show examples for SKEWED(1) which is intuitively uniform distribution in all dimensions. SKEWED(3) and SKEWED(6) are shown in the Fig. 5(b) and Fig. 5(c), respectively.

The CLUSTER dataset [15] is an extension of a synthetic dataset previously described by Arge et al. [34]. In this evaluation we used clusters of 1000 points evenly spaced on a horizontal line. Each of the clusters is filled with evenly distributed points and stretches 0.00001 in every dimension. Fig. 5(d) depicts an example of the CLUSTER dataset with 49 points per cluster. The line of clusters falls within (0.0, 1.0) along the x-axis and is parallel to every other dimensional axis with a 0.5 offset.

6.3. Observations and discussion
In all of experiments, LFKD and A-LFKD have better throughput performance (in million operations per second) compared to both the PH-tree and the Levy-Kd, even in single thread cases, for all workload distributions. The performance significantly scales up with increasing thread count. This shows that our implementation is both lightweight and scalable. As we increase the key dimension, the performance degrades for workloads dominated by the NNS. This degradation with increasing key dimensions is expected in kD-trees due to the curse of dimensionality [1]. This performance pattern is identical for different key ranges. However, the LFKD still achieve speed-up over the single threaded implementations.

We further observe that, as expected, A-LFKD outperforms LFKD in NNS dominated workload, the performance benefit increases with increasing dimensionality of the data set that brings the increased load of iterative scan. This can be explained by early termination of the iterative scan in the A-NNS in A-LFKD which prunes parts of the tree with are otherwise traversed by the NNS in LFKD.

For the TIGER/Line dataset, in a single thread case, both LFKD and LFKD(SC) perform at least 2.5× better than Levy-Kd, and, it goes up to 19× in the NNS dominated workload. Additionally, the PH-tree outperforms the Levy-Kd only for workloads that do not involve NNS (00% NNS, 50% Add and 50% Remove ).

We observe that for NNS dominated workload (90% NNS, 5% Add and 5% Remove ), the A-LFKD achieves speed-ups up to 66× for SKEWED and up to 150× for CLUSTER datasets over the sequential implementations. These observations can be partially attributed to the local-midpoint rule, which carries the essence of the sliding-midpoint-splitting rule of [10] that targets the extreme cases such as a CLUSTER dataset, to a concurrent setting.

For a mixed workload (50% NNS, 25% Add and 25% Remove ), the performance of LFkD-tree degrades by increasing key dimension. The absolute throughput figures are higher for the NNS dominated workload in lower dimensions than in mixed workloads. This is because the modify operations incur higher synchronization (conflicts, expensive atomic operations, and helping) overhead. However in higher dimensions, the throughput of the NNS is lower as the number of visited nodes increases tremendously with dimension.

7. Conclusion and future work
For a large number of applications, which require a multidimensional data structure supporting dynamic modifications along with nearest neighbour search, research community has largely focused on improving the design of sequential data structures. Parallel implementations of the sequential designs speed up loading of and NNS on a fully loaded data structure. Thus, they do not address the issue of dynamic modifications in the datasets. On the other hand, the concurrent data structure research is primarily confined to one-dimensional problems.

Our work is the first to extend the concurrent data structures to problems covering multidimensional datasets. We introduced LFkD-tree, a lock-free design of kD-tree, which supports linearizable nearest neighbour search operations with concurrent dynamic addition and removal of data. We provided a sample implementation which shows that the LFkD-tree algorithm is highly scalable.

Our method to implement linearizable nearest neighbour search is generic and can be adapted to other multidimensional data structures. We plan to design lock-free data structures which are suitable for nearest neighbour search in high dimensions, for example, the ball-tree [35]. We also plan to extend our work to k-nearest neighbour (kNN) search.