Abstract
Indexing of static and dynamic sets is fundamental to a large set of applications such as information retrieval and caching. Denoting the characteristic vector of the set by B, we consider the problem of encoding sets and multisets to support approximate versions of the operations  (i.e., computing ) and  (i.e., finding ) queries. We study multiple types of approximations (allowing an error in the query or the result) and present lower bounds and succinct data structures for several variants of the problem. We also extend our model to sliding windows, in which we process a stream of elements and compute suffix sums. This is a generalization of the window summation problem that allows the user to specify the window size at query time. Here, we provide an algorithm that supports updates and queries in constant time while requiring just  factor more space than the fixed-window summation algorithms.

Keywords
Streaming
Algorithms
Sliding window
Lower bounds

1. Introduction
Given a bit-string  of size n, one of the fundamental and well-known problems proposed by Jacobson [16], is to construct a space-efficient data structure which can answer  and  queries on B efficiently. For , these queries are defined as follows.

•
: returns the number of b's in .

•
: returns the position of the i-th b in B.

A bit vector supporting a subset of these operations is one of the basic building blocks in the design of various succinct data structures. Supporting these operations in constant time, with close to the optimal amount of space, both theoretically and practically, has received a wide range of attention [17], [19], [20], [21], [24]. Some of these results also explore trade-offs that allow more query time while reducing the space.

We also consider related problems in the streaming model, where a quasi-infinite sequence of integers arrives, and our algorithms need to support the operation of appending a new item to the end of the stream. For , let 
 be the sum of the last i integers. Here, n is the maximal suffix size we support queries for. For streaming, we consider processing a stream of elements, and answering two types of queries, suffix sum () and inverse suffix sum (), defined as:

•
: returns 
 for any .

•
: returns the smallest j, , such that .

In this paper, our goal is to obtain space efficient data structures for supporting a few relaxations of these queries efficiently using an amount of space below the theoretical minimum (for the unrelaxed versions), ideally. To this end, we define approximate versions of rank and select queries, and propose data structures for answering approximate rank and select queries on multisets and bit-strings. We consider the following approximate queries with an additive error .

•
: returns any value r which satisfies 
. If 
, then 
.

•
: returns any value r which satisfies 
.

•
: returns any position p which satisfies 
.

•
: returns any position p which satisfies 
.

•
: returns any value r which satisfies .

•
: returns any value r which satisfies .

We propose data structures for supporting approximate rank and select queries on bit-strings efficiently. Our data structures uses less space than that is required to answer the exact queries, and most of the data structures use optimal space. We also propose a data structure for supporting  and  queries on binary streams while supporting updates efficiently. Finally, we extend some of these results to the case of larger alphabets. For all these results, we assume the standard word-RAM model [18] with word size  if it is not explicitly mentioned.

1.1. Previous work
Rank and Select over bit-strings. Given a bit-string B of size n, it is clear that at least n bits are necessary to support  and  queries on B (since the bit vector can be reconstructed by using the answers to these queries). Jacobson [16] proposed a data structure for answering  queries on B in constant time using  bits. Clark and Munro [8] extended it to support both  and  queries in constant time with  bits. For the case when there are m 1's in B, at least  
  bits1 are necessary to support  and  on B. Raman et al. [24] proposed a data structure that supports both operations in constant time while using  bits. Golynski et al. gave an asymptotically optimal time-space trade-off for supporting  and  queries on B [14]. A slightly related problem of approximate color counting has been considered in El-Zein et al. [10].

Algorithms that Sum over Sliding Windows. A natural generalization of the static case is answering queries with respect to a sliding window over a data stream. The sliding window model was extensively studied for multiple problems including summing [4], [9], heavy hitters [2], [5], Bloom filters [1] and counting distinct elements [11]. Our ss queries for streaming are a generalization of the problem of summing over sliding windows. That is, window summation is a special case of the suffix sum problem where the algorithm is always asked for the sum of the last  elements. Approximating the sum of the last n elements over a stream of integers in , was first introduced by Datar et al. [9]. They proposed a  multiplicative approximation algorithm that uses 
 bits and operates in  amortized time, or  worst case time. In [12], Gibbons and Tirthapura presented a  multiplicative approximation algorithm that operates in constant worst case time while using similar space for 
. Ben-Basat et al. [4] studied the potential memory savings one can get by replacing the  multiplicative guarantee with a δ additive approximation. They showed that  bits are required and sufficient. Recently, [3] showed the potential memory saving of a bi-criteria approximation, which allows error in both the sum and the time axis, for sliding window summation. The data structure of [6] looks at a generalization of the ssA queries to general alphabet, where at query time we also receive an element x and return an estimate for the frequency of x in the last i elements.

It is worth mentioning that these data structures do allow computing the sum of a window whose size is given at the query time. Alas, the query time will be slower as they do not keep aggregates that allow quick computation. Specifically, we can compute a  multiplicative approximation in 
 time using the data structures of [9] and [12]. We can also use the data structure of [4] for an additive approximation of δ in  time.

1.2. Our results
In this paper, we obtain the following results for the approximate , ,  and  queries with additive error. Let B be a bit-string of size n.

1.  and  queries with additive error δ:

•
We first show that  bits are necessary for answering 
 and 
 queries on B and propose a -bit data structure that supports 
 and 
 queries on B in constant time. For the case when there are m 1's in B, we show that  bits are necessary for answering 
 and 
 queries on B, and obtain -bit data structure that supports 
 and 
 queries on B in constant time.

•
We show that  bits are necessary for answering 
 and 
 queries on B, and obtain an -bit data structure that supports 
 queries in  time, and 
 queries in  time. Furthermore, we show that there exists an additive error δ such that any 
-bit data structure requires at least  time to answer 
 queries on B.

•
Using the above data structures, we also obtain data structures for answering approximate  and  queries on a given multiset S from the universe  with additive error δ, where  query returns the value , and  query returns the i-th smallest element in S. We consider two different cases: (i) rankA, drankA selectA, and dselectA queries when , and (ii) drankA and selectA queries when the frequency each element in S is at most ℓ. For case (ii), we first show that at least  bits are necessary for answering  queries, and obtain an optimal space structure that supports  queries in constant time, and an asymptotically optimal space structure that supports both  and  queries in constant time when .

•
We also consider the  and  queries on strings over large alphabets. Given a string A of length n over the alphabet  of size σ, we obtain a -bit data structure that supports drankA and selectA on A in  time. We summarize our results for bit-strings in Table 1.


Table 1. Summary of results of upper and lower bounds for approximate  and  queries on bit-string of size n (m is the number of 1's in B). The function t(n,u) is defined as .

Query	Space (in bits)	Query time	Error
Lower bounds
, 
⌊n/δ⌋		δ, additive
, 
, 

Upper bounds
, 
n/δ + o(n/δ)	O(1)	δ, additive
, 
t(n/δ,n)
2. ss and iss queries with additive error δ:
•
We first consider a data structure for answering  and  queries on binary stream, i.e., all integers in the stream are 0 or 1. For exact  and  queries on the stream, we propose an -bit data structure for answering those queries in constant time while supporting constant time updates whenever a new element arrives from the stream. This data structure is obtained by modifying the data structure of Clark and Munro [8] for answering rank and select queries on bit-strings. Using the above structure, we obtain an -bit structure that supports  and  queries on the stream in constant time while supporting constant time updates. Since at least  bits are necessary for answering 
 (or 
) queries on bit-strings, and  bits are necessary for answering  queries [4], the space usage of our data structure is succinct (i.e., optimal up to lower-order terms) when , and asymptotically optimal otherwise.

•
We then consider the generalization that allows integers in the range , for some . First, we present an algorithm that uses the optimal  bits for exact suffix sums. Then, we provide a second algorithm that uses  bits for solving ssA. Specifically, our data structure is succinct when , and is asymptotically optimal otherwise, and improves the query time of [4] while using the same space. Table 2 presents this comparison.


Table 2. Comparison of data structures for ss queries over stream of integers in {0,…,ℓ}. All works can answer fixed-size window queries (where i ≡ n) in O(1) time. Worst case times are specified.

Guarantee	Space (in bits)	Update time	Query time
DGIM02 [9]	(1 + ε)-multiplicative	
GT02 [12]	(1 + ε)-multiplicative	
O(1)	
BEFK16 [4]	δ-additive, for 		O(1)	O(ℓ ⋅ n/δ)
BEFK16 [4]	δ-additive, for 		O(1)	O(n)
This paper	δ-additive	Same as in [4]	O(1)	O(1)
2. Queries on binary strings and streams
In this section, we first consider data structures for answering approximate rank and select queries on bit-strings and multisets. We also show how to extend our data structures on static bit-strings to sliding windows on binary streams, for answering approximate ss and iss queries.

2.1. Approximate rank and select queries on bit-strings
We consider the approximate rank and select queries on bit-strings with additive error δ. We only show how to support 
, 
, 
, and 
 queries. To support 
, 
, 
, and 
 queries, one can construct the same data structures on the bit-wise complement of the original bit-string. We first introduce a few previous results which will be used in our structures. The following lemmas describe the optimal structures for supporting rank and select queries on bit-strings.

Lemma 2.1

[8]
For a bit-string B of length n, there is a data structure of size  bits that supports 
, 
, 
, and 
 queries in  time.

Lemma 2.2

[24]
For a bit-string B of length n with m 1's, there is a data structure of size

•
(a)  bits that supports 
 queries in  time, and

•
(b)  bits that supports 
, 
, 
, and 
 queries in  time.

We use results from [15] and [23], which describe efficient data structures for supporting the following queries on integer arrays. For the standard word-RAM model with word size  bits, let A be an array of n non-negative integers. For  and any non-negative integer x, (i)  returns the value 
, and (ii)  returns the smallest i such that . In what follows, we use the following function to state the running time of some of the (Searchable Partial Sum) queries. 
 
 
 

Lemma 2.3

[15], [23]
An array of n non-negative integers, each of length at most α bits, can be stored using  bits, to support  queries on A in constant time, and  queries on A in 
 time. Moreover, when , we can answer both queries in  time.

Supporting drankA and selectA queries. We first consider the problem of supporting 
 or 
 queries with additive error δ on a bit-string B of length n, and prove a lower bound on space used by any data structure that supports either of these two queries.

Theorem 2.4

Any data structure that supports 
 or 
 queries with additive error δ on a bit-string of length n requires at least  bits. Also if the bit-string has m 1's in it, then at least  bits are necessary for answering the above queries.

Proof

Consider a bit-string B of length n divided into  blocks 
, 
, …, 
 where for , 
 and 
 (the last block may contain more than δ, but less than 2δ bits). Let S be the set of all possible bit-strings satisfying the condition that all the bits within a block are the same (i.e., each block contains either all zeros, or all ones), and hence 
. We now show that any two distinct bit-strings in S will have different answers for some 
 query (and also some 
 query). Consider two distinct bit-strings B and 
 in S, and let i be the leftmost index with 
. Then by the construction of B and 
, all the possible answers of 
 are different from all the possible answers of 
, for all values of i (note that the answer to a 
 query is not unique). By the same argument, there exists no position which can be an answer of both 
 and 
 queries, where j is the number of 1's in . Thus, any structure that supports either of these queries must distinguish between every element in S, which implies  bits are necessary to answer 
 or 
 queries.

For the case when the number of 1's in the bit-string is fixed to be m, we choose  blocks from each bit-string and make all bits in the chosen blocks to be 1's (and the rest of the bits as 0's). Since there are  
  ways for select such  blocks in a bit-string of length n, it implies that  bits are necessary to answer 
 or 
 queries in this case. □

The following theorem gives a data structure for supporting 
 and 
 queries in constant time, using optimal space.

Theorem 2.5

For a bit-string B of length n, if there is an -bit data structure which supports 
 and 
 queries in  time, then there exists an -bit data structure which supports 
 and 
 queries with additive error δ in  time.

Proof

We divide B into  blocks 
, 
, …, 
 where for , 
 and 
. Now we define another bit-string 
 of length  where for , 
 if 
 contains jδ-th 1 in B for any integer , and 
 otherwise. Note that any block of B has at most one such position in B. Then by constructing the -bit data structure on 
, we can support 
 and 
 queries on 
 in  time.

Now we claim that 
 gives an answer of the 
 query, which can be computed in  time. Let 
, and let d be the position of D-th 1 in B. By the definition of 
, if 
 or , the claim holds since there are less than δ 1's in . Now consider the case when 
 and . Then there are at most  1's in , which is the case when  is the position of the -th 1 in B, and all the values in  are 1. Also there are at least  1's in , which is the case when  is the position of the -th 1 in B and all the values in  are 1. Thus, C gives is a valid answer for 
 in this case. Also by the same argument, one can answer the 
 query in  time by returning 
. □

By combining the data structures of Lemma 2.1, Lemma 2.2 with Theorem 2.5, we obtain the following corollary.

Corollary 2.6

For a bit-string B of length n, there is a data structure that uses  bits and supports 
 and 
 queries with additive error δ, in constant time. If there are m 1's in B, the data structure uses  bits and supports the queries in  time.

Note that the proof of Theorem 2.5 implies that any data structure that supports 
 (or 
) queries on 
 can be used to answer 
 (or 
) queries on B. For example if B is very sparse, i.e., when  (in this case, the space usage of the structure of Corollary 2.6 is sub-optimal), one can use the structure of [21] that uses  bits (asymptotically optimal space), to support 
 queries in  time, and 
 queries in constant time.

Supporting  and  queries. Now we consider the problem of supporting 
 and 
 queries with additive error δ on bit-strings of length n. The following theorem describes a lower bound on space.

Theorem 2.7

Any data structure that supports 
 or 
 queries with additive error δ on a bit-string of length n requires at least  bits.

Proof

We first construct a set V of bit-strings of length n as follows. We divide each bit-string B into  blocks 
, 
, …, 
 such that for , 
 and 
. Now for every , we set all bits in 
 to 0 if i is odd. If i is even, we fill 
 to  1's followed  0's. Thus there's only one choice of blocks 
 (if i is odd), and δ choices for blocks 
 (if i is even). Hence 
. Now consider two distinct bit-strings B and 
 in V, and let i be the smallest even index which satisfies 
, where 
 and 
 has k and 
 1s. Without loss of generality, assume k is less than 
. Then by the construction of B and 
, there exists no value which can be an answer of both 
 and 
 queries. By the same argument, there exists no position which can be an answer of both 
 and 
 queries, where ℓ is number of 1's upto i-th block in 
. Thus, any structure that supports either of these queries must distinguish between every element in S, which implies  bits are necessary to answer 
 or 
 queries. □

We now show that for some values of δ, any data structure that uses up to a 
 factor more than the optimal space cannot support 
 queries in constant time.
Theorem 2.8

Any 
-bit data structure that supports 
 queries with an additive error 
, for some constant  on a bit-string of length n requires  query time.

Proof

We reduce the predecessor search problem to the problem of supporting 
 queries. Given a set , a predecessor query,  returns the largest elements in S that is smaller than i, for . Now suppose we want to support  queries on S with , where 
 with some constant . For this range of parameters, Patrascu and Thorup [22] showed that any data structure that represents S using 
 bits needs  time to support  queries. We now show that any data structure that supports 
 queries can be used to obtain a data structure that supports  queries, using asymptotically the same space and query time. The theorem immediately follows from this reduction.

Let 
. We call the elements in 
 as the dummy elements. Next, let 
 with 
, and let 
 be the set of all elements of 
 in increasing order (note that  since both S and 
 have size ). The dummy elements in 
 ensure that 
 and 
, for . Now consider the bit-string 
, where the block 
 and 
 for  (i.e., B encodes the differences between successive elements of 
 using fixed-length right-justified unary codes of size 2δ). Note that B contains 
 1's, and has length . In addition, we store an array A of length ℓ where 
 using  bits, since 
.

Suppose that there is a data structure X that uses  space, and supports 
 queries on B in  time. To answer the query , we first perform the 
 on X. Let 
 be the block to which this answer belongs. Since each block starts with a sequence of at least δ zeros, and since 
, it follows that 
. Hence we return 
 as the answer of . Thus, from the assumption about the data structure X, we can obtain a structure that uses  bits and supports  queries in  time. The theorem follows from this reduction, and the predecessor lower bound mentioned above. □

The following theorem describes a simple data structure for supporting 
 and 
 queries.

Theorem 2.9

For a bit-string B of length n, there is a data structure of size  bits, which supports 
 queries on B using  time and 
 queries on B using  time.

Proof

We divide the B into  blocks 
, 
, …, 
, as in the proof of Theorem 2.5. Also we define an array  of length  where for ,  is the number of 1's in 
. We store the structure of Lemma 2.3 on C using  bits, to support  and  queries on C. Then since , the answer of  query on C gives the answer of 
, which can be answered in  time by Lemma 2.3. To answer the query 
, we first find the block 
 in B which contains the position 
 by answer the  query on C. Since 
,  gives an answer of the 
 query. □

2.2. Approximate rank and select queries on multisets
In this section, we describe data structures for answering approximate rank and select queries on a multiset with additive error δ. Given a multiset S where each element is from the universe , the  and  queries on S are defined as follows.

•
: returns the number of elements in S whose value is at most i.

•
: returns the i-th smallest element in S.

One can define approximate rank and select queries on multisets (also denoted as rankA, drankA, selectA, dselectA) analogously to the queries on strings [24]. Any multiset S of size m from a universe of size n can be represented as a characteristic vector 
 of size , where 
 when the element k has multiplicity 
 in S, for . Then one can answer  and  queries using  and  queries on 
, by 
 and 
. We now describe our data structures for answering approximate rank and select queries on S, in the following two settings:

(1) rankA, drankA, selectA, and dselectA queries when  is fixed: We construct a bit-string 
 of length  where 
 only keeps every iδ-th 1 in 
, for , and removes all the other 1's in 
. To answer the  query, we first compute 
. Since , we return 
 as the answer. Similarly, we can answer the  query by returning 
. By storing the structure of Lemma 2.2(b) on 
 using  bits, we can support 
, 
, 
 and 
 queries on 
 in constant time, which implies both  and  queries on S also can be supported in constant time.

For answering  and  queries on S, we first construct the data structure of Theorem 2.9 on 
 to support 
 queries. In addition to that, we maintain the data structure of Lemma 2.3 to support  and  queries on the arrays  and  where for ,  and  store the number of 0's and 1's, respectively, in the block 
 (as defined in the proof of Theorem 2.9). Thus, the total space of the data structure is  bits. To answer the  query, we first find the block 
 of 
 which contains i-th 0 by computing  on D, and then return  on E, which gives an answer of the  query by the similar argument in the proof of Theorem 2.9. To answer , we first find the j-th block 
 which contains an answer of the 
 query, and then return  on D. Note that if , we return 0 for both queries. By Theorem 2.9, the total running time is  for both  and  queries. For special case when , we can answer  and  queries on S in constant time.

(2) drankA and selectA queries when the frequency of each element in S is at most ℓ: We describe a data structure for answering  and  queries on S in  time. We then show that at least  bits are necessary for supporting  queries on S. Thus, for supporting only  queries, the data structure uses the optimal space. We consider the following two cases: (a) , and (b) .

•
Case 2a. : In this case, we first observe that . Hence, 
 is a bit-string with n 0's and at most nℓ 1's. Let 
 be a bit-string defined as in the Case (1); 
 has n 0's and at most  1's. To support  on S, we need to support 
 on 
. We represent the bit-wise complement of 
 using the structure of Lemma 2.2(a), which takes at most  bits and supports 
 on 
 in  time. Using this structure, we can achieve optimal space usage, and support  queries on S in  time. Alternatively, we can represent 
 using the structure of Lemma 2.2(b), which takes at most  bits, and supports 
, 
, 
 and 
 queries on 
 in  time. Using this structure, we can support both  and  queries on S in  time, while using asymptotically optimal space when .

•
Case 2b. : In this case, we first set , and define a bit-string 
 of length  where 
 if and only if there exists a 1 between the positions of the -th 0 and the -th 0 in 
. Note that there exists at most a single 1 between those two positions since . Now using Lemma 2.1, we construct an -bit data structure which supports 
 and 
 queries on 
 in constant time. Then one can show that 
 is an answer to the query , using an argument similar to the one in the proof of Theorem 2.5. For  queries, we set  and construct a same structure as above, using  bits. Since 
, we can answer  query in constant time by returning 
. Therefore, our data structure supports  queries in constant time with optimal space, and twice the optimal space for supporting both  and  queries in constant time (note that at least  bits are necessary in this case).

We now show a matching lower bound on space for supporting  queries on S.

Theorem 2.10

Given a multiset S where each element is from the universe  of size n such that the maximum frequency of each element in S is at most ℓ, any data structure that supports  queries on S requires at least  bits.

Proof

Note that S can be represented by a sequence 
 of size n, where 
 denotes a frequency of i in S. Now we first set  and denote I as , and denote 
 as 
. Next, consider all inputs that contains a sequence of  blocks padded by zeros, such that each block is a member of 
; that is, consider 
. It is easy to show that every input of  gives a representation of S. We show that every two distinct inputs in  must lead to distinct answer of a  query, thereby implying a  bits lower bound as required. Let two distinct sets 
 and 
 be represented by the sequences in  such as 
 and 
 respectively such that 
 for any . Also let t be a leftmost index which satisfies 
. Now we consider 
 and 
 queries. If , then  and (due to the definition of I) 
, which implies that there is no answer which satisfies both 
 and 
 queries. On the other hand, if  then  and thus either 
 or 
. In either case, 
. We established that if two inputs in  lead to the same configuration of  queries, the error for one of them would be at least δ while we assumed it is strictly lower. □

2.3. Approximate ss and iss queries on binary streams
In this section, we consider a data structure for answering  and  queries over a stream of binary symbols , when sliding window size is fixed to n. We first describe a data structure for answering exact  and  queries in constant time using  bits, while supporting updates in constant time, which is based on the data structure of Lemma 2.1.

Theorem 2.11

There exists a -bit data structure which supports  and  queries in  time while supporting -time updates whenever a new element arrives from the stream.

Proof

We partition the stream into frames of size n, and maintain the current n elements in the sliding window, which span at most two consecutive frames, in a circular array  using n bits. Clark and Munro [8] showed that one can answer both  and  queries in  time on a static bit-string of size n by storing substructures of total size  bits, which consist of (i) some auxiliary arrays, and (ii) a fixed lookup (precomputed) table. The lookup tables are independent of the frames, and can be shared between the frames. The auxiliary arrays store the answers to the  and  queries for a subset of the positions/values (i.e., a subset of all possible queries). Furthermore, these auxiliary arrays can be constructed in  time per entry by scanning the original bit-string. Thus, the auxiliary arrays can be easily computed incrementally (i.e., by appending elements at the end), using  time per insertion. Thus, by maintaining the substructures for the two frames spanning the current sliding window using  bits in addition to the -bit counter for storing the number of 1's in the current frame, we can support both  and  queries in  time, while supporting updates in  time. □

Next, we consider a data structure for answering  and  queries on a binary stream in constant time, using  bits, while supporting constant time updates. We first divide the stream into consecutive chunks of size δ. For each chunk, we assign a bit as follows: a chunk is assigned the bit 1 if it contains jδ-th 1 of the stream, for some , and 0 otherwise. Next, we consider the values assigned to the chunks as a stream, and maintain the data structure of Theorem 2.11 for supporting ss and  queries in  time over the stream of values of chunks. Also, to support -time update, we maintain two counters 
 and 
 using  bits, where 
 and 
 are the number of elements and the number of 1's in the current from the (original) stream up to position i, respectively. Now we describe how to answer  and  queries.

•
: We return 0 if . Otherwise, we return 
, where κ is an indicator function defined as  if and only if , and the value assigned to the k-th latest chunk is 1. The correctness can be proved by the same argument as in the proof of Theorem 2.5.

•
: We return 0 if . Otherwise, we return 
. Again, the correctness can be proved by the same argument as in the proof of Theorem 2.5.

In conclusion, we obtain the following theorem.
Theorem 2.12

For a binary stream, there exists a data structure that uses  bits and supports  and  queries on the stream with additive error δ, in constant time, while supporting constant time updates.

Compared to the lower bound of Theorem 2.4 for answering  and  queries on bit-strings (which also gives a lower bound for answering  and  queries), the above data structure takes  bits when . However for a sliding window of size n, at least  bits are necessary [4] for answering  queries, even in the case when i is always equal to n. Therefore the data structure of Theorem 2.12 supports  and  queries with optimal space when , and with asymptotically optimal space otherwise.

3. Queries on strings and streams over a large alphabet
In this section, we consider non-binary inputs. First, we look at general alphabet and derive results for approximate rank and select. Then we consider approximate  queries over integer streams.

3.1.  and  queries on strings over general alphabet
Let A be a string of length n over the alphabet  of size σ. Then, for , the query 
 returns the number of j's in , and the query 
 returns the position of the i-th j in A (if it exists). Similarly, the queries 
 and 
 are defined analogous to the queries  and  on bit-strings. By extending the proof of Theorem 2.4 to strings over larger alphabets, one can show the following.

Corollary 3.1

Any data structure that supports  or  queries with additive error δ on a string of length n over an alphabet of size σ requires at least  bits.

Proof

We divide the string B of length n into  blocks 
, 
, …, 
 where for , 
 and 
 (the last block may contain more than δ, but less than 2δ characters). Let S be the set of all possible strings satisfying the condition that all the characters within a block are the same. Then there are 
 possible strings in S, where for any two distinct strings from S, they have different answers for some 
 query (and also some 
 query), by the same argument as the proof of Theorem 2.4. □

In this section, we describe a data structure that supports  and  queries in  time, using twice the optimal space. We make use of the following result from [13] for supporting  and  queries on strings over large alphabets.

Lemma 3.2

[13]
Given a string of length n over the alphabet , one can support 
 queries in  time and 
 queries in  time, using  bits, for any .

The following theorem shows we can construct a simple data structure for supporting 
 and 
 queries on A using the above lemma.

Theorem 3.3

Given a string of length n over the alphabet , one can support 
 and 
 queries for any  on the string in  time, using  bits.

Proof

We first divide the input string, say A, into  blocks 
, 
 …
 where for , 
 and 
 (if n is not multiple of δ). Then we construct a new string 
 of length , where $ is a symbol not in Σ. Now we construct yet another string 
 of length at most , which is a subsequence of 
, obtained by only keeping every iδ-th occurrence of each symbol from Σ, for  in 
, and also all the occurrences of $ in 
, while removing all the other characters in 
. We then represent 
 using the structure of Lemma 3.2, using  bits to support  and  queries in  and  time, respectively.

Now we describe how to support the queries. For answering the 
 query, we first compute the position, 
, of the -th $ in 
, in constant time, using 
. Then by an argument similar to the one in the proof of Theorem 2.5, one can show that 
 gives an answer of the 
 query, where 
, which is either 0 or 1. Thus, 
 query can be answered in  time. Similarly, it is easy to see that we can answer the 
 query in  time by returning 
. □

3.2. Supporting ssA queries over non-binary streams
In this section, we consider the problem of computing suffix sums over a stream of integers in . This generalizes the result of the Theorem 2.12 for ssA. For such streams, one can use ssA binary search to solve issA, while a constant time issA queries are left as future work. Specifically, we show a data structure that requires ; i.e., it requires  times as many bits as the static-case lower bound of Theorem 2.10 when .

We note that this model was studied in [4], [9], [12] for window-sum queries. That is, our work generalizes this model to allow the user to specify the window size  at query time while previous works only considered the sum of the last n elements. In fact, all previous data structure implicitly supports ssA queries but with slower run time. The algorithms in [12], [9] require 
 time to compute a -approximation for the sum of the last n elements while the algorithm of [4] needs  for a δ-additive one. Here, we show how to compute a δ-additive error for the sum of the last  elements in constant time for both updates and queries.

Exact ss queries En route to ssA, we first discuss how to compute an exact answer for suffix sums queries. It is known, even for fixed window sizes, that one must use  bits for tracking the sum of a sliding window [4]. Here, we show how to compute exact ssA using succinct space of  bits. We start by discussing why the approaches used in Theorem 2.11 cannot work for a large ℓ value. If we use sub-blocks of size  as in [8], then the lookup table will require 
 bits, which is not even asymptotically optimal for non-constant ℓ values. While one may think that this is solvable by further breaking the sub-blocks into sub-sub-blocks, sub-sub-sub-blocks, etc., it is not the case. To see this, consider a lookup table for sequences of length 2. Then its space requirement will be 
 bits. If ℓ is large (say, ) then this becomes , which is not even asymptotically optimal.

Theorem 3.4

There exists a data structure that requires  bits and support  time  queries and updates.

Proof

Same as in the proof of Theorem 2.11, we break the stream into n-sized frames, and store auxiliary structures for each frame to support the queries efficiently. We first divide each frame into 
 equal-sized blocks. The blocks are then sub-divided into  sized sub-blocks. For each frame, we keep a 
 sized array  that stores the sum of elements from the beginning of the frame, and a -sized array  with the sums from the beginning of the corresponding blocks. The number of bits required for  is 
. Similarly,  requires 
 
 bits. Next, we consider the two cases (i) 
, and (ii) 
, and give separate data structures to handle each case. In both the cases, we store the arrays  and  for the two frames spanning the current (sliding) window. In case (i), we also maintain all the elements in the current window in a circular array , using  bits. Since the sub-block size is , we can store a look-up table for all sequences of size  as this only requires 
. To handle an update, we simply need to add the value of the newly added element to the last two entries in the  and  arrays (or create new entries if needed). Thus, by maintaining the array  and the lookup table in addition to the  and  arrays for the two frames, we get a solution that requires  bits and supports suffix sums queries and updates in constant time. In case (ii), the lookup-table approach fails. Thus, we do not use such a table, nor do we keep the last n-elements window. Instead, we keep an n-sized circular array  in which every entry is the cumulative sum from the beginning of the sub-block. The space required for keeping the array is then 
 
. Thus by keeping this array  in addition to the  and  arrays for the two frames, we get a solution that takes  bits and supports constant time updates and queries. □

General ssA queries Here, we consider the general problem of computing ssA (i.e., up to an additive error of δ). Intuitively, we apply the exact solution from the previous section on a compressed stream that we construct on the fly. A simple approach would be to divide the streams into consecutive chunks of size  and represent each chunk's sum as an input to an exact suffix sum algorithm, same as in the binary stream case. However, this fails to achieve succinct space. For example, summing  integers requires  bits. However,  bits may be asymptotically larger than the  bits lower bound of Theorem 2.10. We alleviate this problem by rounding the arriving elements. Namely, when adding an input , we first round its value to 
 
 so it will require  bits.2 The rounding allows us to sum elements in a chunk (using a variable denoted by ), but introduces a rounding error. To compensate for the error, we use chunks of size . We also consider 
 that is slightly lower than δ to compensate for the rounding error when .3 We then employ the exact suffix sums construction of Theorem 3.4 for window size of  (the number of chunks that can overlap with the window) over a stream of integers in , where 
 is a upper bound on the resulting items. We use ρ to denote the input that represents the current block.
The query procedure is also a bit tricky. First, we introduce the following variables used in our algorithm for  queries:

•
 - an exact suffix sum algorithm, as described in the previous section. It allows computing suffix sums over the last  elements on a stream of integers in .

•
 - tracks the sum of elements that is not yet recorded in .

•
o - the offset within the chunk.

Intuitively, we can estimate the sum of the last i items by querying  for the sum of the last  inserted values and multiplying the result by 
; but there are a few things to keep in mind which were not considered in the binary case. First,  may not be an integer. Next, the values within the current chunk (that has not ended yet) are not recorded in . Finally, we are not allowed to overestimate, so 's propagation may be an issue. To address the first issue, we weigh the oldest chunk's ρ value by the fraction of that chunk that is still in the window. For the second, we add the value of  to the estimation, where  is the sum of rounded elements. Notice that we do not reset the value of  but rather propagate it between chunks. Finally, to assure that our algorithm never overestimates we subtract 
 from the result. A pseudo code of our method appears in Algorithm 1. The space usage of the algorithm is analyzed in the following lemma.

Lemma 3.5

Algorithm 1 requires 
 bits.

Proof

The algorithm utilizes three variables:  that requires  bits,  that uses  space, and o that is allocated with  bits. Recall that  is the number of blocks that can overlap with the maximal n-sized window and 
 is a bound on ρ. Overall, the number of bits used by our construction is
 Since , we get the desired bound. □

Algorithm 1
Download : Download high-res image (94KB)
Download : Download full-size image
Algorithm 1. Algorithm for ssA and update on stream.

Thus, we conclude that our algorithm is succinct if the (additive) error δ satisfies . We note that a  bits lower bound for Basic-Summing with an additive error was shown in [4], even when only fixed sized windows (where i is always n) are considered. Thus, our algorithm always requires 
 space, even if . Here, 
 is the lower bound for static data shown in Theorem 2.10.

Corollary 3.6

Let 
 such that  satisfies
 then Algorithm 1 is succinct. For other parameters, it uses 
 space.

We now state the correctness of our algorithm.
Theorem 3.7

Algorithm 1 solves ssA while processing elements and answering queries in  time.

Proof

For the proof, we recall a few quantities that we also use in Algorithm 1: 
 
 and . We assume that the index of the most recent element is , such that 
 is the first element in the chunk of 
 and 
 is the offset within the current chunk. We also denote , such that 
 is the last element of the most recently completed chunk. Fig. 1 illustrates the setting. By the correctness of the  exact suffix sum algorithm, and as illustrated in Fig. 1, we have that totalSum is the sum of the last numElems added to , that 
 is the value of the element that represents the last chunk that overlaps with the queried window. Also, notice that out is the number of elements in that chunk that are not a part of the window. For any , we denote by 
 the value of  after the 
 item was added; e.g., 
 is the value of  at the time of the query and 
 is its value before the current chunk. Notice that 
 is also at the end of a chunk (that does not overlap with the queried interval). For other variables, we consider their value at query time.

Fig. 1
Download : Download high-res image (172KB)
Download : Download full-size image
Fig. 1. Theorem 3.7 proof's setting, with all relevant quantities that Algorithm 1 uses illustrated.

When a chunk ends (Line 5), we effectively perform 
 (lines 6 and 7), thus:(1)
 Our goal is to estimate the quantity(2)
 
 
 Recall that our estimation (Line 17) is:(3)
 
 where the last equality follows from the fact that within a chunk we simply sum the rounded values (Line 4). Next, observe that we sum the rounded values in each chunk and that if  is decreased by 
 (for some ) at Line 7, then we set one of the last numElems elements added to  to k. This means that:(4)
 
 Plugging (4) into (3) gives us(5)
 
 
 Joining (5) with (2), we can express the algorithm's error as:(6)
 
 
 
 where ξ is the rounding error, defined as 
.

Since each rounding of an integer  has an error of at most 
 
, and as we round  elements, we have that the rounding error satisfies(7)
 
 where the last inequality is immediate from our choice of the number of bits – . We now split to cases based on the value of μ. We start with the simpler  case, in which  (and consequently, ). This allows us to express the algorithm's error of (6) as(8)
 We now use (1), (7), and the definition of 
 to obtain:
 Similarly, we can bound it from below:
 We established that if  we achieve the desired approximation. Henceforth, we focus on the case where , which means that  and 
. We now consider two cases, based on the value of 
.

1.
 case.

In this case, we know that after the processing of element 
 the value of  was at least 
 (Line 6). This implies that 
 and equivalently
 
 
 Substituting this in (6), and applying (7), we get that:
 
 
 
 In order to bound the error from above we use (1) and (7):
 

2.
 case.

Here, since the value of 
 is 0, we have that 
 and thus
 
 
 We use this for the error expression of (6) to get:
 
 
 We now use (1), (7), and the fact that  to bound the error from below as follows:
 

Finally, we need to cover the case of . In this case, we can return 
 as the estimation. This directly follows from (1) and the fact that within a chunk we simply sum the rounded values (Line 4). We established that in all cases 
. □
4. Conclusion
In this paper, we considered the problem of supporting approximate versions of the well-studied  problems over static sets and suffix sums and inverse suffix sums over sliding windows. We studied the generalization of these problems to multi-sets and examined different error guarantees, such as error in the input vs. error in the result. Most of our results include lower bounds and matching upper bounds that are asymptotically optimal and often succinct. The resulting solutions require considerably less space compared to the space required for supporting exact queries, which makes them more likely to be implemented in practice.