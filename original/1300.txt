Abstract
A biochemical network can be simulated by a set of ordinary differential equations (ODE) under well-stirred reactor conditions, for large numbers of molecules, and frequent reactions. This is no longer a robust representation when some molecular species are in small numbers and reactions changing them are infrequent. In this case, discrete stochastic events trigger changes of the smooth deterministic dynamics of the biochemical network. Piecewise-deterministic Markov processes (PDMP) are well adapted for describing such situations. Although PDMP models are now well established in biology, these models remain computationally challenging. Previously we have introduced the push-forward method to compute how the probability measure is spread by the deterministic ODE flow of PDMPs, through the use of analytic expressions of the corresponding semigroup. In this paper we provide a more general simulation algorithm that works also for non-integrable systems. The method can be used for biochemical simulations with applications in fundamental biology, biotechnology and biocomputing. This work is an extended version of the work presented at the conference CMSB2019.

Keywords
Gene networks
Stochastic gene expression
Piecewise-deterministic processes
Liouville-master equation
Push-forward method

1. Introduction
Stochastic simulation is a powerful tool in biology. Moreover, stochasticity is a general property of biochemical networks, having multiple origins. Noise is generated intrinsically by these molecular systems when neither the law of large numbers nor the averaging theorem can be applied, for instance when some of the constituent molecular species are present in small numbers and trigger relatively slow reactions [5]. There are also extrinsic sources of noise, resulting from a fluctuating cellular environment. The extrinsic noise paradigm also applies to stochasticity resulting from manipulating biochemical networks in an artificial environment such as lab-on-a-chip or simply in a titration device [1].

Stochastic simulation is used in single cell experimental studies, where the amounts of mRNA [25], [19], [4], [24] and protein [9], [10] products of a gene can be determined for each cell. Using double- or multiple-fluorophore fluorescence techniques, products from several genes can be quantified simultaneously and one can have access to multivariate probability distributions of mRNA or proteins. The stochastic dynamics of promoters and gene networks can have important consequences for fundamental biology [8] but also for HIV [20] and cancer research [11]. Predicting the probability distributions of biochemical networks' products is also instrumental for lab-on-a-chip applications, when one wants to optimize and control the functioning of these networks. For these reasons we aim to develop effective methods for computing time-dependent distributions for stochastic models. Our main objective is the reduction of computation time which is prerequisite for parameter estimation and machine learning applications [12].

The traditional stochastic simulation protocol uses the chemical master equation and the Gillespie algorithm. In such a protocol all the chemical reactions are simulated individually as discrete stochastic events. Simulations on relevant experimental times have to cope with 
 such events and generate 
 samples in order to achieve statistically significant estimates of molecular species distributions. Altogether, these simulations are extremely costly in terms of execution time.

An important reduction of the simulation time is obtained by noticing that the sources of noise can be localized in small parts of the system that behave discretely, or in the discrete environmental variables in the case of intrinsic or extrinsic noise, respectively. The remaining, large part of the system consists of molecular species in large numbers that evolve continuously. This leads to piecewise-deterministic Markov process (PDMP) approximations of the biochemical dynamics, coupling discrete state Markov chain dynamics with continuous state ordinary differential equation (ODE)-governed dynamics. The justification of the PDMP approximations of the chemical master equation can be found in [5], [6]. Although simpler than the chemical master equation, direct simulation of the PDMP remains time consuming because the Markov chains can still have a very large number of states [5].

In the CMSB2019 proceedings paper we have introduced new methods for simulating PDMPs for gene networks [15]. A gene network PDMP model can be simulated by numerical integration of ODEs satisfied by the mRNA and the protein species, coupled to the Markov chain describing the successive transitions of the gene promoters [26], [5], [21], [17]. The simulation becomes particularly effective when analytic solutions of the ODEs are available [14].

Probability distributions of PDMP are solutions of the Liouville-master partial differential equations (PDEs). Numerical integration of these PDEs is an interesting alternative to direct simulation, combining precision and speed for small models. Finite difference methods, however, are of limited use in this context as they can not cope with high dimensional models (for instance, extant gene networks applications are restricted to the dimension 2, corresponding to a single promoter, with or without self-regulation see [14], [16]).

Another interesting method for computing time dependent distributions is the push-forward method. For gene networks, this method has been first introduced in [13] and further adapted for continuous mRNA variables in [14]. It is based on the idea to compute the probability distribution of gene products as the push-forward measure of the semigroup defined by the ODEs. This method is an approximation, as one has to consider that the discrete PDMP variables are piecewise constant on a deterministic time partition. The transition rates between promoter states were computed using a mean field approximation in [14]. In the CMSB2019 proceeding paper, the mean field approximation was replaced by the next order approximation taking into account the second moment of the protein distribution [15]. In this paper we extend the method to general PDMP models, thus covering all biochemical networks, with both intrinsic and extrinsic noise simulation protocols.

2. Models
2.1. ODE models of biochemical networks
A biochemical network is defined by a set of chemical reactions and a set of chemical species. The amounts of different chemical species form a vector 
, where 
 is the concentration of the species i. Each reaction is characterized by a stoichiometric vector 
 and a reaction rate function 
. When all chemical species are present in large numbers, the biochemical network is well described by a system of ODEs(1)
 
 

For example, a simple gene transcription model can be obtained when a large number, let's say G, of a specific gene is present in the cell and, moreover, the gene promoter has just two states: active and inactive. In this simple model the gene product is the mRNA concentration. The vector of species concentration is given by: 
 (inactive promoter concentration), 
⁎
 (active promoter concentration) and 
 (mRNA concentration). The concentration of inactive promoters is measured by the ratio between the number of inactive promoters (
) and the cell volume 
, so 
; the same holds for the concentrations of active promoters and mRNA, 
, where 
 is the number of active promoters. The reactions are 
⁎
, 
⁎
, 
⁎
⁎
, . Thus, the stoichiometric vectors are 
, 
, 
, 
 and reaction functions 
, 
⁎
, 
⁎
, 
, where the reaction rate constants 
, have dimension . For large copy numbers of P, 
⁎
 and X the simple transcription model reads:(2)
 
⁎
⁎
 
⁎
 
⁎
 Note that 
 
⁎
 
 and so one of the first two equations can be discarded.

We must emphasize that gene transcription is only one of the many possible examples of our formalism. The formalism defined by (1) covers all biochemical network models and has large applicability in cell physiology.

2.2. PDMP models
When some, but not all, species are present in low numbers and there are also slow reactions, a stochastic representation, such as a piecewise deterministic Markov process (PDMP), is more appropriate than the deterministic equations (1).

For instance, in the simple transcription model (2), assume that there is only one copy of the gene, meaning 
. In this case, it does not make sense to describe the time evolution of promoter variables by differential equations anymore. In this scenario, the promoter variables are discrete numbers. As the copy number of the gene is one, is more suitable to replace the concentration P by the discrete variable 
 assuming values 
 or 
. The same holds for 
⁎
, that is replaced by 
. Moreover, since 
 one of these variables can be discarded.

The switching between the two discrete states of 
 can be described by a continuous time Markov chain with transition rates 
 and 
. That is, if 
, then 
 for 
 where 
 is a random time such that 
 and 
 for 
 where 
 is a random time such that 
.

Moreover, suppose that the switching constants, 
 and 
, are small compared to 
, and also that 
. Thus, the variable X has a switching behavior alternating accumulation and degradation periods when 
 and 
, respectively, each one described by ODEs:(3)
 
  where 
 is the cell volume.

Using methods from [6] it is possible to show that the “hybrid system” 
, with X given by (3), converges to (2), as .

The PDMP formalism can be extended to a rather general class of biochemical network models as follows:

A PDMP biochemical model is a stochastic process 
 having states in a set of the form 
, where  is a finite set encoding discrete states of the biochemical model, and 
, where 
 is a vector in 
 whose components 
 () encode the dynamics of i continuous biochemical species, and 
 describes the jump Markov process between the discrete states. The PDMP 
 is determined by three characteristics:

1)
For all fixed , a vector field 
 determining a unique global flow 
 in 
, such that, for ,(4)
 

The flow 
 represents a one parameter semigroup fulfilling the properties

(i)
,

(ii)
.

2)
A transition rate matrix 
, such that 
 is the  element of the matrix H, 
 is the number of states. If , 
 is the rate of probability to jump to the state r from the state s. Furthermore, 
 for all  and for all 
.

3)
A jump rate 
. The jump rate can be obtained from the transition rate matrix(5) 
 

From these characteristics, right-continuous sample paths 
 starting at 
 can be constructed as follows. Define(6)
 where 
 is a realization of the first jump time of 
, with the distribution(7)
 
 and ω is the element of the probability space for which the particular realization of the process is given. The pre-jump state is 
 and the post-jump state is 
, where s has the distribution(8)
 
 We then restart the process 
 and recursively apply the same procedure at jump times 
, etc..

Note that between each two consecutive jumps 
 follow deterministic ODE dynamics defined by the vector field 
. At the jumps, the values 
 are continuous. More general definitions of PDMPs include jumps in the continuous variables but will not be discussed here.

We define multivariate probability density functions 
. These functions satisfy the Liouville-master equation which is a system of partial differential equations:(9)
 
 

This general formalism covers all biochemical models that have discrete variables. As salient examples we can cite gene networks [14], ion channels dynamics for neuron networks [2], lab on chip biochemical devices [1].

2.3. PDMP models of ON/OFF gene networks
A particular example of PDMP biochemical model is represented by the gene networks. Because these models are extensively used in systems biology, we provide their details in this subsection.

Two state ON/OFF gene networks generalize the simple two state single gene transcription model, by considering more interacting genes and by using separate variables for the two gene products: mRNA and protein.

To each gene we associate a discrete variable 
 with two possible values 
 for a non-productive state (OFF) and 
 for a productive state (ON). Furthermore, a gene i produces proteins and mRNAs in the amounts 
 and 
, respectively. A gene network state is described by the N-dimensional vector 
. The gene products amounts satisfy ODEs:(10)
 
 
 The coupling between genes is modeled at the level of discrete state transitions. The elements of the matrix H are functions of products from various genes. For instance, if a gene i inhibits a gene j the transition rate from ON to OFF of the gene j is an increasing function of the protein concentration 
.

As a first example that we denote as model 
, let us consider a two genes network; the expression of the first gene being constitutive and the expression of the second gene being activated by the first. We consider that the transcription activation rate of the second gene is proportional to the concentration of the first protein 
. All the other rates are constant 
, 
, 
, representing the transcription activation rate of the first gene, and the transcription inactivation rates of gene one and gene two, respectively. For simplicity, we consider that the two genes have identical protein and mRNA parameters 
, 
, 
. We further consider that 
 if the gene i is OFF and 
 if the gene i is ON.

The gene network has four discrete states, in order , , , and . Then, the transition rate matrix for the model 
 is(11)
 
 The Liouville-master equation for the model 
 reads(12)
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 The model 
 differs from the model 
 by the form of the activation function. Instead of a linear transcription rate 
 we use a Michaelis-Menten model 
. This model is more realistic as it takes into account that the protein 
 has to attach to specific promoter sites which become saturated when the concentration of this protein is high.

The transition rate matrix for the model 
 is(13)
 
 
 
 
 
 The Liouville-master equation for the model 
 reads
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

3. Simulation methods
3.1. Monte-Carlo method
The Monte-Carlo method utilizes the direct simulation of the PDMP based on the iteration of the following equations:(14)
 
 
 for 
 with initial conditions 
, , and stopping condition 
, where U is random variable, uniform in the range , followed by the choice of the next discrete state 
 by using the matrix 
.

A large number 
 of sample traces is generated and the values of 
 are stored at selected times. Multivariate time dependent probability distributions are then estimated from this data.

The direct simulation of PDMPs needs the solutions of Eqs. (14) which can be obtained by numerical integration. This is not always computationally easy. Problems may arise for fast switching promoters when the ODEs have to be integrated many times on small intervals between successive jumps. Alternatively, the numerical integration of the ODEs can be replaced by analytic solutions or quadrature. Analytic expressions are always available for the gene network flow (10) and read
 
 
 
 

Let us consider the following general expression of the jump intensity function
 
 
 where 
 are non-linear functions, for instance Michaelis-Menten function
 
 or Hill functions
 
 If 
 for all , the cumulative distribution function of the waiting time 
 can be solved analytically [14], otherwise it can be obtained by quadrature. For example, for the model 
 one has
 
 
 where 
 is Kronecker's delta. In this case, the waiting time 
 is obtained as the unique solution of the equation(15)
 
 
 
 
 where U is a random variable, uniformly distributed in the range . In our implementation of the algorithm we solve (15) numerically, using the bisection method.

3.2. Finite difference Liouville-master equation method
The finite difference Liouville-master equation method for a given number of genes, 
, uses a discrete approximation of the domain to compute the numerical solution across time for a given higher-dimensional system (12), with initial conditions given by 
 and 
 where  is the Dirac delta function. We compute the solution for 
 distributions, since each gene has both an ON and OFF state, respectively.

In order to achieve the simulation of the equations given by system (12), we begin by discretizing each of the domains into 
 intervals, whose centers are given by 
 meeting the condition that 
 and where, likewise, an arbitrary discrete value in 
 would be denoted 
. Each 
 then represents a unique, discrete, jth position in the protein abundance domain spanned by 
. Likewise, mRNA would have an analogous discretization given by all 
. Time is then similarly discretized by a time-step τ into 
 temporal locations given by 
. The task then becomes the computation of the solution at each of these discrete locations in the domain, such that the sink and source terms may be trivially calculated but where the derivative terms warrant further explanation.

The solution to the advection equation under a uniform coefficient, χ, is given by a translation in the relevant domain. To achieve this, whilst also maintaining the stability of the system, as a whole, we implement upwind difference discrete operator scheme. This means that for an arbitrary probability density function, 
, we write the discrete partial derivative operator as(16)
 
 
 
  This may be evaluated as such for each term within the system of equations, given by (12), and guarantees the probability balance of the system, as a whole.

Beyond the computation of the equation's solutions at a single time-step the solutions must be computed robustly across time. We therefore employ a McCormack predictor-corrector scheme [18], given explicitly for any ith time-step by(17)
 
 
 where 
, 
 and 
 is the discrete nomenclature for a prediction of the solution for 
 at time 
.

The method for solving the problem, in totality, is then given by Algorithm 3 where, again, the partial derivative terms on the right-hand side of each equation are evaluated using (16). Algorithm 3 is a direct implementation of (17) as an algorithm with the concurrent calculation of the distributions 
 from the individual distributions 
. For each time-step, we solve the right-hand side of the equation using the predictor corrector scheme, update the value of each distribution, and calculate the total distribution, 
.

Algorithm 3
Download : Download high-res image (136KB)
Download : Download full-size image
Algorithm 3. PDMPLiouvillemaster.

3.3. Push-forward method
3.3.1. General algorithm
This method allows one to compute the multivariate probability distribution of the continuous variable x at a time τ given the probability distribution of  at time 0.

In order to achieve this we consider a deterministic partition 
 of the interval  such that 
 is small. The main approximation of this method is to assume that 
, for , is piecewise constant on this partition, more precisely, that 
. This is rigorously true for intervals 
 completely contained between two successive random jump times of 
. This situation becomes very frequent for a very fine partition (large M). Thus, the error generated by the approximation vanishes in the limit  (the rigorous result is Theorem 1 given in the Results section).

For each path realization 
 of the discrete states, we can compute  as the continuous solution of the following ODE with piecewise-defined r.h.s.:(18)
 
 and with the initial condition 
.

In order to compute the probability 
 of a path realization we can use the fact that, given 
, 
 is a finite state Markov process. Therefore,(19)
 where 
 is the initial distribution of the discrete state, and 
 is the solution, at 
, of(20)
 
 with 
 and 
 is given by (18).

In order to compute the probability distribution of x at time τ one has to sum the contributions of all solutions of (18), obtained for the 
 realizations of promoter state paths with weights given by the probabilities of the paths (Fig. 1).

Fig. 1
Download : Download high-res image (73KB)
Download : Download full-size image
Fig. 1. Deterministic partition of time in the push forward method. Trajectories of the PDMP model (red line) having a jump of the discrete state inside [τ2,τ3) are replaced by a trajectory (black line) having a jump of the discrete state at τ3. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)

Suppose that we want to estimate the distribution of , using a multivariate histogram with bin centers 
 where 
 is the number of bins in each direction 
. The initial distribution of x at time  is given by the bin probabilities 
 and the distribution at time τ is given by the probabilities 
.

Let 
 be the solution of (18) with 
 and let 
 be the histogram bin containing 
. The application 
 is in general many to one. Given the probability 
 of a path 
, the push forward distribution of  is computed as(21)
  
 

Remark 1

The Algorithm 4 can be used for computing the full multivariate probability distribution of the vector , but also for computing marginal distributions. For gene network the full multivariate distribution implies products from all genes, whereas a marginal distribution can select only one, or a small number of genes. For marginals, the dimension N is replaced by 
 where 
 is the number of components of interest of the vector .

Remark 2

For gene networks, the numerical integration of the ODEs (steps 7,9,11 in the Algorithm 4) can be replaced by symbolic solutions. For each path realization 
 of promoter states, we can compute the protein and mRNA levels, 
 and 
, respectively, of all genes , at :(22)
 
 
 
(23)
 
 
 
 
 
 
 for . Here,
 
 
 
 with 
 if promoter i is OFF for 
 and 
 if promoter i is ON for 
.

3.3.2. Complexity issues
The complexity of the push forward algorithm scales as 
, because there are 
 possible paths 
, 
 histogram bins, and 
 time complexity for solving (18), (20) is 
. The complexity of computing marginal distributions of 
 variables is lower and scales as 
.

3.3.3. Mean field push-forward method
A way to mitigate the computational burden of the Algorithm 4 is to use the mean field approximation. In the mean field approximation, the probabilities 
 are computed from averaged equations that are identical for each histogram bin.

More precisely, equation (20) is replaced by(24)
 
 where 
, 
.

Using a Taylor expansion of 
 around the expectation 
, one gets(25)
 
 
 where 
 is the variance/covariance matrix of 
, 
 is the element-wise second derivative matrix of H and : stands for the double dot product.

In (25) the moments 
 and 
 are either available analytically or are solutions of ODEs obtained using moment closures such as in [22].

For gene networks, the mean field push-forward procedure can be applied also to ODE dynamics of the individual genes. This is possible because during ON or OFF periods, the ODE dynamics of one gene is uncoupled from that of another gene. Furthermore, the 
 transition matrix H can be replaced by   transition matrices of one gene with elements averaged over the values of the other genes. This approximation reduces the complexity of the calculations to 
, which is linear in the number of genes. The mean field push-forward algorithm for gene networks is presented in the Algorithm 6.

Algorithm 6
Download : Download high-res image (262KB)
Download : Download full-size image
Algorithm 6. PDMPpushforwardmeanfieldgenenetwork.

This method has already been used for particular models. In [14] we have replaced the regulation term 
 occurring in the transition matrix of the gene network model 
 by its mean 
. This means that the gene 2 switches between its ON and OFF states with rates given by the mean of the regulatory protein 
. In this case both H and Π can be computed analytically, which leads to a drastic reduction in the execution time. This simple mean field approach is suitable for the model 
, which contains only linear regulation terms. For non-linear regulation terms, Π can not generally be computed analytically. Moreover, the naive mean field approach introduces biases. For instance, in the case of the model 
, the approximation 
 is poor. In the CMSB2019 paper we proposed a better approximation [15], in which we replace 
 by its mean value and use(26)
 
 
 
 in order to correct the bias. This approach is generalized by (25).

As in [14] we can use analytic expressions for 
, but also for 
. These expressions can be found in the Appendix A. Although the elements of matrix H have analytic expressions, the elements of the matrix Π contain integrals that must be computed numerically. For the model 
, we have(27)
 
 for the transition rates of the first gene, where 
, 
, and(28)
  for the transitions of the second gene, where 
 and 
 
.

4. Results
4.1. Convergence of the push-forward method
The probability distribution obtained with the push-forward method converges to the exact PDMP distribution in the limit . This is a consequence of the following theorem:

Theorem 1

Let 
 be the flow defined by the formulas (18), such that 
 for . Let 
 be the probability measure defined as 
, where 
 is the probability distribution of x at , 
 is given by (19), and 
 is the σ-algebra of Borel sets on 
. Let 
, the exact distribution of 
 for the PDMP defined in Section 2.2, with initial values 
 distributed according to 
. Assume that the vector fields 
, , and the transition matrix H are 
 functions and that there exists a bounded set of 
 such that all flows 
, , leave B invariant. Assume that 
 for all , where C is a positive constant. Then, for all , 
 converges in distribution to 
, when . More precisely, for all Lipschitz functions φ on 
, there exists a constant κ depending on the data of the PDMP, τ and the Lipschitz constant of φ such that:
 
 
 Also for all Borel set A in 
 such that 
 we have 
 when . Here, ∂A denotes the boundary of A.

The proof of this theorem is given in Appendix B. It is inspired by the classical proof of weak order of convergence for the Euler scheme for a stochastic differential equation (see [23]).

Remark 3

If the flows 
 are not known explicitly, a numerical scheme can be used. This introduces another source of error. Our proof easily extends and provides a similar result of convergence. If one wants to investigate the convergence of 
 given by (21), this follows from the above theorem only under the assumption that the probability that the PDMP is on the boundary of the bins at time t is 0. Note this is not restrictive and happens only in pathological situations. Also, if the initial distribution has a smooth density, we can prove that the error estimate above holds for borelian bounded functions φ, thus we can choose φ to be an indicator function and obtain error bounds for these probabilities without this restriction. More precisely, we have for any Borel set A:
 where now κ depends on the Lipschitz constant of the initial density, see Remark 4 in Appendix B.

4.2. Testing the performance of various methods
4.2.1. Testing accuracy and speed of push-forward method compared to the Monte-Carlo method
In order to test the push-forward method, we compared the resulting probability distributions with the ones obtained by the Monte-Carlo method using the direct simulation of the PDMP. We considered the models 
 and 
 with the following parameters: , 
 
, , , 
, 
 for the two genes. For the parameter 
 
 we used two values:  for slow genes and  for fast genes. We tested the slow-slow and the fast-fast combinations of parameters.

The initial distribution of the promoters state was 
 where the state  means that both promoters are OFF. The initial probability measure 
 was a delta Dirac distribution centered at 
 and 
. This initial condition is obtained by always setting the direct simulation of the PDMP to start from 
, 
, and 
. The simulations were performed between 
 and 
 for fast genes and between 
 and 
 for slow genes. In order to estimate the distributions we have used  samples for the highest sampling. The push-forward method was implemented with  equal length sub-intervals of . The time step τ was chosen to be  for fast genes and  for slow genes. The procedure was iterated 10 times for fast genes (up to 
) and 6 times for slow genes (up to 
).

The execution times are shown in the Table 1. The comparison of the probability distributions is illustrated in the Fig. 2, Fig. 3. In order to quantify the relative difference between methods we use the 
 distance between distributions. More precisely, if  and 
 are probability density functions to be compared, the distance between them is(29)


Table 1. Execution times for different methods. All the methods were implemented in Matlab R2013b running on a single core (multi-threading inactivated) of a Intel i5-700u 2.5 GHz processor. The Monte-Carlo method computed the next jump waiting time (Algorithm 1) using the analytical solution of (15) for M1 and the numerical solution of (15) for M2. The push-forward method used Algorithm 6 and analytic solutions for mRNA and protein trajectories from (23), (18) and numerical computation of the integrals in (28), for both models.

Model	Monte-Carlo high sampling [min]	Push-forward [s]
M1 slow-slow	45	20
M1 fast-fast	74	30
M2 slow-slow	447	20
M2 fast-fast	758	30
Fig. 2
Download : Download high-res image (212KB)
Download : Download full-size image
Fig. 2. Histograms of protein for the second gene, produced by the Monte-Carlo method (green lines) and by the Push-forward method (black lines) for the model M1. The green dotted line results from low sampling Monte-Carlo with similar execution time as the push-forward method, whereas the solid green line results from high sampling Monte-Carlo. The distances, defined by (29), are between low sampling and high sampling Monte-Carlo (d⁎) and between push-forward and high sampling Monte-Carlo (d).

Fig. 3
Download : Download high-res image (195KB)
Download : Download full-size image
Fig. 3. Histograms of protein for the second gene, produced by the Monte-Carlo method (green lines) and by the Push-forward method (black lines) for the model M2. The green dotted line results from low sampling Monte-Carlo with similar execution time as the push-forward method, whereas the solid green line results from high sampling Monte-Carlo. The distances, defined by (29), are between low sampling and high sampling Monte-Carlo (d⁎) and between push-forward and high sampling Monte-Carlo (d).

This distance was computed for distributions resulting from the push-forward method and the Monte-Carlo method with the highest sampling. We have also used a reduced sampling Monte-Carlo scheme whose execution time is similar to the one of the push-forward method. The distributions resulting from low sampling and high sampling Monte-Carlo were compared using the same distance. Fig. 2, Fig. 3 clearly show that, for the same execution time, the push-forward method outperforms the Monte-Carlo method.

4.2.2. Comparing the Monte Carlo, push-forward and Liouville-master methods
Using Python, we have implemented Algorithm 4 for the one gene model, to compute mRNA  and protein  probability densities  and , respectively. For the one gene model, we have considered two switching regimes: slow  and fast . Also in Python, we have implemented Algorithm 5 for the two genes model, to compute the probability densities for mRNA and protein associated to gene one (
 and 
) and for mRNA and protein associated to gene two (
 and 
). For the two genes model, we have considered four different switch configurations: slow-slow, fast-fast, slow-fast and fast-slow. For all implementations each time interval  has been partitioned into four sub-intervals of equal sizes  resulting in the sequence 
, representing the state of the switch inside each sub-interval (
 for 
), leading to 24 path realizations. For all slow genes we have set 
, 
 and evaluated the solution up to 
, using the composition rule between successive time intervals. For all fast genes we have set 
, 
 and evaluated the solution up to 
, using the composition rule between successive time intervals, as well.

We have compared the results obtained by the implementation of the push forward method (PF), as described above, with Monte-Carlo simulation (MC) (Algorithm 1, Algorithm 2) and with numerical solution of the Liouville-master equation (LME) (Algorithm 3). The comparison between execution times for each model and each method can be found in Table 2 (all the execution times are expressed in minutes).


Table 2. Execution times (in minutes for all cases) for different methods to compute the probability distributions for one gene model and two genes model. Methods: PF = Push-forward, LME = Liouville-master equation.

Model	Monte-Carlo (high sampling)	PF	LME
One gene slow	–	2.55	97.39
One gene fast	–	5.12	21.64
Two genes slow-slow	45	6.86	97.39
Two genes fast-fast	74	10.85	21.64
Two genes slow-fast	243	9.22	21.64
Two genes fast-slow	249	8.83	97.39
In order to illustrate our results we have produced Fig. 4, Fig. 5, Fig. 6, Fig. 7, for selected models and for methods: Push forward (PF), Monte-Carlo simulation (MC) and Liouville-master equation (LME).

Fig. 4
Download : Download high-res image (374KB)
Download : Download full-size image
Fig. 4. This set of plots shows the comparison between the computed probability distribution for mRNA (P(r,t)) and protein (P(y,t)), using the push forward method (Algorithm 4), direct Monte-Carlo simulation (Algorithm 1, Algorithm 2) and numerical solution of the Liouville-master Equation (Algorithm 3). The model is for one gene in the slow switch regime (ϵ = 0.5). The execution time of PF is 2.55 minutes in Python and 97.39 minutes for LME in MATLAB. The values dLME and dPF are shown in each plot and measure the distances between the LME equation and MC simulation, PF and MC simulation, respectively, as given in (29). The values of parameters are: p0 = 0.5, k0 = 4, k1 = 40, a = 1/5 and b = 4. For initial conditions we set: r(0)=y(0)=0 and p0(0)=1.

Fig. 5
Download : Download high-res image (348KB)
Download : Download full-size image
Fig. 5. This set of plots shows the comparison between the computed probability distribution for mRNA (P(r,t)) and protein (P(y,t)), using PF (Algorithm 4), MC simulation (Algorithm 1, Algorithm 2) and numerical solution LME (Algorithm 3). The model is for one gene producing in the fast switch regime (ϵ = 5.5). The execution time of the PF is 5.12 minutes in Python, and 21.64 minutes for LME in MATLAB. The values dLME and dPF are shown in each plot and measure the distances between the LME equation and MC simulation, and PF and MC simulation, respectively, as given in (29). The values of parameters are: p0 = 0.5, k0 = 4, k1 = 40, a = 1/5 and b = 4. For initial conditions we set: r(0)=y(0)=0 and p0(0)=1.

Fig. 6
Download : Download high-res image (378KB)
Download : Download full-size image
Fig. 6. This set of plots shows the comparison between the computed probability density for mRNA (P(r2,t)) and protein (P(y2,t)) associated to gene two, using PF (Algorithm 5), MC simulation (Algorithm 1, Algorithm 2) and numerical solution LME (Algorithm 3). The model is M1 in the slow-slow regime (ϵ = 0.5). The execution time of PF is 6.86 minutes in Python, and 97.39 minutes for LME in MATLAB. The values dLME and dPF are shown in each plot and measure the distances between the LME equation and MC simulation, and PF and MC simulation, respectively, as given in (29). The values of parameters, for both genes are: p0 = 0.5, k0 = 4, k1 = 40, a = 1/5 and b = 4. For initial conditions we set: r1(0)=y1(0)=r2(0)=y2(0)=0 and p0(0)=1.

Fig. 7
Download : Download high-res image (382KB)
Download : Download full-size image
Fig. 7. This set of plots shows the comparison between the computed probability density for mRNA (P(r2,t)) and protein (P(y2,t)) associated to gene two, using PF (described in Algorithm 5), MC simulation (Algorithm 1, Algorithm 2) and numerical solution LME (Algorithm 3). The model is M1 in the fast-slow regime (ϵ1 = 5.5 and ϵ2 = 0.5). The execution time of PF is 8.83 minutes in Python and 97.39 minutes for LME in MATLAB. The values dLME and dPF are shown in each plot and measure the distances between the LME equation and MC simulation, and PF and MC simulation, respectively, as given in (29). The values of parameters, for both genes are: p0 = 0.5, k0 = 4, k1 = 40, a = 1/5 and b = 4. For initial conditions we set: r1(0)=y1(0)=r2(0)=y2(0)=0 and p0(0)=1.

The one gene model (Fig. 4, Fig. 5) has as initial conditions 
, where 
 is the probability to find the switch in state 0 at time , and , for both switch regimes: slow and fast. The two genes model (Fig. 6, Fig. 7) has as initial conditions 
 (meaning that at time  both genes are in state 0 with probability one), 
, for both switch configurations: slow-slow and slow-fast. We used the same parameters for all the models and for all the genes: 
 (slow switch), 
 (fast switch), 
, 
, 
, 
, 
 and 
. We have quantified the relative difference between the distributions obtained by two distinct methods using the 
 distance (29). The distance between the distributions obtained by the Push Forward method and the Monte-Carlo method (high sampling) is indicated in the plots by 
, and the distance between the distributions obtained by the Liouville-master equation and Monte-Carlo method (high sampling) is indicated by 
.

5. Discussion and conclusion
Combining direct simulation of PDMP gene network models and analytic formulae for the ODE flow provides an effective, easy-to-implement method for computing time dependent, multivariate probability distributions of these models. However, the precision of the Monte-Carlo estimates of the distributions increases with , where MC is the number of Monte-Carlo samples. For this reason, the execution time of the Monte-Carlo method, although smaller when compared to PDMP simulation methods, which implement numerical resolution of the ODEs, such as reported in [17] (data not shown), is large when compared to the push-forward method. The push-forward method represents an effective alternative to Monte-Carlo and Liouville-master equation methods, ensuring reduced execution time.

With respect to an earlier implementation of this method for gene networks in [13] we used promoter states instead of mRNA copy numbers as discrete variables of the PDMP. As a consequence, the number of discrete states is lower and we can afford to increase the number M of temporal subdivisions. Compared to the similar work done in [14] we used second moments of the protein distribution, which took into account the correlation of the promoter states and lead to increased accuracy in the case of nonlinear regulation. Although the protein moments and the exponential transition rate matrix Π can be computed numerically, the effectiveness of the push-forward method is increased when analytic expressions are available for these quantities. In this paper, these expressions were computed for particular cases.

The push-forward method is an approximate method, and its accuracy relies on the careful choice of the temporal and spatial step densities, namely of the integers M, 
, 
. We have rigorously proved that the convergence of the push-forward method is of order one, with errors that scale with .

We situate our findings in the broader effort of the community to produce new effective simulation algorithms for computational biology. Although in this manuscript we restricted ourselves to examples of modeling gene networks, the push forward method and the algorithms of this paper can be applied to a broader class of phenomena where some continuous quantity is perturbed by a discrete stochastic process. A very interesting and important example is the stochastic representation of ion channel kinetics in neuron models [2]. The system of equations describing the kinetics of an ion channel is mathematically equivalent to the system of equations describing our one gene model. More complex neuron models with multiple types of ion channels (stochastic versions of Hodgkin-Huxley or Morris-Lecar deterministic models, for instance) are also covered by our PDMP formalism and algorithms. Thus, the push forward method as presented in Algorithm 4 can be used to obtain the histograms for the membrane voltage for rather general neuron models. A detailed comparison between the push forward method and the one presented in [2] will be the subject of a future work.

Appendix A. Expectation and variance of the protein
For the sake of completeness, we give explicit formulas for the expectation and the variance of the protein synthesized by a constitutive promoter (gene 1 of models 
 and 
). The details of the calculation can be found in [15].

The expectation is given by(A.1)
 where
 
 
 
 
 
 
 
 
 
 

The variance is given by(A.2)
 
 
 
 
 
 
 where
 
 
 
 
 
 
 
 
 
 
 
 

Appendix B. Proof of the Theorem 1
For simplicity, we consider only the case , the proof generalizes immediately to  at the price of cumbersome notations. We also consider the case when the process x always belongs to . Thus, we may assume that 
,  and H are bounded as well as their derivatives. We first consider the case of a Lipschitz function φ.

We denote by  the PDMP starting with the random initial data distributed along 
 and by 
 the PDMP starting from the deterministic initial data 
.

In this proof, we use the letter κ for a constant which depends on 
, , H and τ.

The push-forward algorithm can be rewritten as follows: starting from 
, we set 
 and then choose 
 randomly, it is equal to  with probability 
, the latter matrix being defined in (20).

In fact, we consider more generally the approximation of , for a function  (
 as defined in Section 2.2), by 
. The result is obtained when we choose φ independent on .

We first assume that φ is 
 with respect to x. Let us denote by L its Lipschitz constant with respect to x:
 for 
, .

The main tool for the proof is the generator of the process:
 
 and the forward Kolmogorov equation:
 
 When  for 
, it is well known that 
 (see [7]). Therefore, 
 
 

It follows from Proposition A.1 in [6] that u satisfies the formula:
 
 
 
 
 Following [6] we apply Gronwall's lemma to show that, when φ is Lipschitz with respect to x, so is u, with a Lipschitz constant less than 
. Here, κ depends on the characteristics of the PDMP and the time horizon τ; 
 is the sum of the supremum of φ with its Lipschitz constant. Moreover, it follows from the forward Kolmogorov equation that its time derivative is also bounded. Hence,(B.1)

Now we rewrite the error as follows:
 
 On each interval 
 we have, by chain rule and the forward Kolmogorov equation, that
 
 
 
 
 
 Hence,
 
 
 
 
 
 By Eq. (20), we have that
 
 where I is the identity matrix.

Since H is bounded, it follows from Gronwall's lemma that Π is bounded and
 for some constant κ, where  is a norm on the space of matrices. For instance we may take the supremum of the modulus of the coefficients of a matrix. Hence,
 
 
 for another constant κ. In particular, for 
:
 
 Moreover,
 since its derivative is 
, which is bounded. We have shown in (B.1) that u is Lipschitz with respect to x and t, thus(B.2)

Now we have the following estimate
 
 
 
 
 
 and so
 
 The final error estimate is obtained by taking φ depending only on x and integrating with respect to 
:
 
 
 

When φ is only Lipschitz, we obtain the same result by choosing a sequence of 
 functions which converges uniformly to φ and whose Lipschitz constants do not exceed L. This finishes the proof of the first statement. It is classical that this implies that the left-handed side converges to zero when 
 
 for any uniformly continuous and bounded function φ. We conclude thanks to the Portmanteau theorem [3].

Remark 4

When φ is only borelian bounded but 
 has a 
 and Lipschitz density 
, the proof can be adapted. We observe that the only point where the Lipschitz property of φ is used is to obtain (B.1) which is used to derive (B.2). To replace (B.2), we can instead write the explicit formula for  and 
. Since the flows 
 are 
 diffeomorphisms that depend smoothly on time, we see that (B.2) still holds, but on the left-handed side, 
 must be replaced by a constant which depends on 
 and the supremum of φ.