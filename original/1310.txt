Abstract
Suppose an oracle knows a string S that is unknown to us and that we want to determine. The oracle can answer queries of the form “Is s a substring of S?”. In 1995, Skiena and Sundaram showed that, in the worst case, any algorithm needs to ask the oracle  queries in order to be able to reconstruct the hidden string, where σ is the size of the alphabet of S and n its length, and gave an algorithm that spends  queries to reconstruct S. The main contribution of our paper is to improve the above upper-bound in the context where the string is compressible. We first present a universal algorithm that, given a (computable) compressor that compresses the string to τ bits, performs  substring queries; this algorithm, however, runs in exponential time. For this reason, the second part of the paper focuses on more time-efficient algorithms whose number of queries is bounded by specific compressibility measures. We first show that any string of length n over an integer alphabet of size σ with  runs can be reconstructed with  substring queries in linear time and space. We then present an algorithm that spends  substring queries and runs in  time using linear space, where g is the size of a smallest straight-line program generating the string.

Keywords
String reconstruction
String learning
Adaptive learning
Kolmogorov complexity
String compression
Lempel-Ziv
Centroid decomposition
Suffix tree

1. Introduction
String reconstruction (or learning) from substrings queries is a well-established problem that has natural applications in many areas, including bioinformatics, data compression, security, etc. (see, for example, [1], [2], [3], [4]).

In a more general setting, one is interested in understanding whether and how it is possible to reconstruct an unknown target string S from some piece of information about S. This information can be, for example, a collection of substrings (e.g., the classical NP-hard Shortest Superstring Problem), or substring compositions ([5]), or subwords ([6]), of S. Furthermore, the problem can be viewed from different angles, e.g., combinatorial, computational, algorithmic, information theoretical.

In this paper, we deal with the problem of reconstructing a string from information about its substrings. Apart from the classical static model for the reconstruction (exact or with uncertainty), many different models have been introduced in the literature for the string reconstruction problem, including the one we consider in this paper, and which has been presented in 1995 by Skiena and Sundaram [3]. In this model, one can ask an oracle, which knows the target string S, queries of the form “Is s a substring of S?” and is interested in designing an adaptive algorithm minimizing the number of such queries. In this setting, adaptive means that the algorithm may reuse the information resulting from previous queries in order to decide which queries to ask next. It is worth mentioning that, with the same model, other query complexities have been investigated very recently by Amir et al. [7].

A trivial information-theoretic argument implies a worst-case lower bound of  queries, where σ is the size of the alphabet of S. Skiena and Sundaram [3] improved this bound and showed that  queries are necessary to reconstruct S in the worst case. This remains true even if the oracle returns for each substring query the number of its occurrences in S. In the same paper, they gave an algorithm for the reconstruction which spends at most  queries, thus asymptotically matching the lower bound. They also gave an algorithm that spends at most  queries if the length n of S is known. Iwama et al. gave an algorithm for binary strings that spends  queries on average [8]. Amir et al. [7] recently proved that if the string has period , then it can be reconstructed using  substring queries, even if both n and p are unknown.

We stress out that these bounds hold in the adaptive case: answers to previous queries can be used to decide the next query. As shown by Skiena et al. [2], the non-adaptive model is much harder: if the algorithm has to reconstruct all substrings (of the unknown string) of length  after a pre-determined batch of queries, then 
 queries are needed to solve the problem. Tsur [4] explored this model more in detail, providing bounds as a function of the fraction  (with ) of substrings of length k that have to be reconstructed: in this case, 
 non-adaptive queries are sufficient and necessary.

1.1. A novel approach to the problem
While the aforementioned papers tackled the problem in the worst-case, the minimum number of queries needed to reconstruct a string may be significantly smaller than the worst-case on particular instances. For example, consider a string of the form 
, where a is a single letter. An algorithm could first try to find out if the string is of this form by issuing  queries and, only if the check fails, proceed with Skiena and Sundaram's algorithm [3]. Observe that the resulting algorithm optimizes for a particular class of highly-compressible strings. In fact, in this paper we show that this reasoning continues to hold for any compressor. Our first result is a universal algorithm that, given as input a computable compressor , performs the reconstruction asking a number of queries that is proportional to the bit-size  of the string compressed by . We complement this result by showing that any deterministic adaptive algorithm for reconstructing a string yields a string compressor. Together, these results imply the equivalence between the string reconstruction and compression problems.

Motivated by the fact that our universal algorithm performs an exponential number of calls to , we then focus on optimizing the running time and the space usage for commonly used compressors, including run-length encoding, Lempel-Ziv factorization and context-free grammars.

In measuring the efficiency of an algorithm, we assume that any query can be submitted to the oracle in constant time and space regardless of the length of the queried substring. The reason for this assumption is that the implementation of the oracle strongly depends on the application. For example, if the application admits a collaborative oracle, there are several possible approaches to achieve constant query time, e.g., using hashing. Moreover, one could also assume that the oracle knows the reconstruction strategy and therefore it could run the reconstruction algorithm itself, that is, we do not even need to transmit the next substring query because the oracle already knows the next query it has to answer.

1.2. Preliminary definitions
Let S be a binary string. A compressor is an injective computable function 
 that converts any 
 into a reversible representation  of size  bits. We require also the inverse function 
 (i.e. the function such that 
) to be computable; this function is the decompressor associated with . Informally speaking, a function  qualifies as a good compressor if  on particular string families (for example, repetitive strings), and  for all other strings outside this family.

A popular compressor is the LZ77 factorization of S. The Lempel-Ziv 1977 (LZ77) algorithm [9] parses a string S into a sequence of z phrases, where each new phrase is either a fresh character or the longest string that also occurs starting from a position strictly smaller than the phrase start position. The bit-size of the LZ77 factorization of S is . For example, the LZ77 factorization of the string  is . In this example, the string is factored into  phrases. A more restricted version of LZ77 does not allow overlaps between a phrase and its source. We denote this version as LZ77 without overlaps and with 
 the number of generated phrases. Between those two measures, it holds 
 [10]. Clearly, also 
 always holds. This version factorizes the above string as , with 
.

Another common measure of compressibility is the number  of equal-letter runs in S, that is, the number of maximal unary substrings of S. This measure is not as strong as z; in fact, it is easy to see that 
.

In this work we also consider the size (number of nonterminals) g of the smallest straight-line program (SLP) producing (only) S. SLPs are particular cases of acyclic context-free grammars composed of rules of the kind , where A is a nonterminal and  are either nonterminals or terminals.

The known relations between g and 
 are 
 and 
. See Navarro's recent survey [10] for more details on these and several other relations between string complexity measures.

2. Universal string reconstruction
In this section we present an algorithm that, given a compressor C, reconstructs any string S with  queries to the oracle. We furthermore prove a dual result: any reconstruction algorithm performing  queries yields a compression algorithm (with associated decompressor) that compresses the string to  bits. These findings show that the string reconstruction problem essentially coincides with the string compression problem. For simplicity, in this section we restrict our attention to binary alphabets only.

We start with a lemma of Skiena and Sundaram [3] stating that any set M of binary strings admits a string that is a substring of a constant fraction of the members of M. Letting M be a set of strings, we let  denote the subset of M whose elements contain S as a substring.

Lemma 1

([3, Lem. 12]) Let 
 be a set of binary strings, each of length n. Then, there exists a string S such that 
 
 
.

Lemma 1 can be turned into a universal algorithm for determining the substring queries to be asked to the oracle as a function of any given compressor.

Lemma 2

Let  be a compressor, and let 
 be an unknown binary string of known length n. Then, there is an algorithm that reconstructs S using  substring queries.

Proof

Let 
 be the set of strings of length n compressed to at most k bits by . Note that 
, since  is injective and there are no more than 
 binary strings (compressed representations) of length at most k. Assuming we know the value of  (later we remove this assumption), it is easy to design an (exponential-time) algorithm that builds 
: simply apply  to all strings of length n, keeping only those such that . By definition of τ, note that 
. Then, by applying recursively Lemma 1 starting from the set 
, we end up selecting S from this set. Each recursive iteration yields a string that we use to perform a substring query on S, thereby reducing the number of candidates by a factor 4/5 in the worst case. After 
 iterations (i.e., substring queries on S), we discover which element of 
 corresponds to S. To conclude, we can remove the assumption that we know τ. To achieve this goal it is sufficient to run an exponential search on the above strategy, i.e., run it on 
 for 
. The last iteration will reveal S, after a total of  substring queries on S. □

We finally prove the following lemma.

Lemma 3

Let A be a deterministic adaptive algorithm that reconstructs any string S by asking  queries to the oracle, for some (computable) function . Then, there exists a compressor  (and an associated decompressor 
) such that .

Proof

It is straightforward to turn A into a compressor : the compressed representation  of S is the binary string of length  formed by the  answers received by the oracle while reconstructing S. The  answers can be computed by any pattern matching algorithm testing membership of the substrings queried by A in the substring closure of S. Similarly, A itself can be turned into a decompressor: by definition of A, the  answers of the oracle (i.e. the compressed file representation) are sufficient to reconstruct A. □

While the above results establish an asymptotic equivalence between the string reconstruction and compression problems, they do not yield time-efficient algorithms for reconstructing a string in time proportional to its compressed size. In the next section we tackle this problem by focusing on particular string compressors.

3. Feasible algorithms for the reconstruction
Let S be a string of length n over an integer alphabet . A trivial algorithm for reconstructing S with  substring queries is the following [3]: We make queries of single character substrings, so that after at most σ queries a new character of S is determined. Let s be a known substring of S. In general, we can increase the length of this known substring by one character by querying on the strings 
, for every character 
. At least one of these queries must be a substring of S, unless s is a suffix of S that has no other occurrences in S. When s can no longer be extended to the right, i.e., s is a suffix of S not appearing elsewhere in S, we can continue the process by prepending characters to the known substring s, until it can no longer be extended to the left, and the string S is then reconstructed.

This algorithm is optimal up to constant factors due to the following lower bound [3].

Theorem 4

([3, Thm. 8]) In the worst case, 
 
 substring queries are necessary to reconstruct a string of length n.

In the rest of this section, we will provide algorithms for reconstructing the string S whose efficiency is measured towards commonly used measures of compression for strings.

Let us first show an easy result for the size  of the run-length encoding of S, i.e.,  is the number of runs (maximal repetitions of the same character) in S. We show that S can be reconstructed with 
 
 queries. The reconstruction is done in  steps. Let 
 be the substring reconstructed so far. In the ith step, we first identify the character c that follows 
 in S. This is done by querying 
 for any . Once we know c, we need to identify the length of the run of c, i.e., the maximal value 
 such that 
 is a substring of S. This can be done with an exponential search on 
, which takes 
 queries. When at some step j, 
 cannot be extended further, we continue the process by prepending (runs of) characters to the known substring.

The overall number of queries is 
. This is in 
 
 queries because the sum of the terms 
 is maximized when every 
 is in 
 
.

Theorem 5

Any string of length n with  runs can be reconstructed with 
 
 substring queries in  time and  space.

Note that, accordingly to Theorem 4, this result is optimal up to a constant factor for sufficiently large σ.

Our next aim is to give algorithms whose complexity grows as a function of the size of the LZ77 parsing of S. We let 
 denote the number of phrases of the LZ77 parsing when the parse does not allow overlapping phrases (both settings are commonly considered in the literature).

We can use Theorem 4 to prove a lower bound on  in terms of 
.

Theorem 6

In the worst case, 
 substring queries are necessary to reconstruct a string of length n.

Proof

It is well known that for any string 
 
. The theorem follows by combining this fact with Theorem 4. □

We are not required to know the length n of S, but we assume to know its alphabet . If instead also the alphabet is unknown, we need  queries to identify the largest character in S. This is done by performing an exponential search to identify the largest character occurring in S. Notice that this is correct only if all the characters in Σ occur in S (in particular, ), which we assume as hypothesis.

Our goal is to prove the following theorem.

Theorem 7

Let S be a string of length n over the alphabet . There exists an algorithm that reconstructs S with 
 substring queries to the oracle. The algorithm runs in  time using linear space.

Note that this result is optimal up to a factor 
 
 by Theorem 6.

In the next subsection we review a technique to solve pattern matching queries on a text which exploits the centroid decomposition of the suffix tree of a string. This will allow us to give an efficient algorithm for reconstructing the suffix tree of S, from which S is therefore determined.

Pattern matching with the centroid decomposition.  This technique has been introduced by Naor [11] and has found applications, for example, in designing cache-oblivious string B-trees [12], [13] or randomized pattern matching [14] on a dictionary of strings.

The centroid decomposition of a tree  (also known as separator decomposition) is a popular and powerful technique to obtain a tree 
 of logarithmic height. The decomposition is based on a theorem proved by Jordan in 1869 [15].

Lemma 8

Any tree  of n nodes has at least a node, called centroid, whose removal leaves connected components of size at most .

The centroid decomposition is defined recursively. Given , we identify a centroid node u, which is chosen to be the root of the new rooted tree 
. Then, we remove u from  and recurse on each connected component to get u's subtrees in 
. The children of u in 
 are the roots of the centroid decompositions of these components. Let us use 
 to denote the set of children of u in 
. As we have a (possibly empty) component for u's parent and children in , the outdegree of u in 
, i.e., 
, is at most the outdegree of u in  plus one. The resulting decomposition is a new tree 
 on the same nodes, whose height is .

A folklore algorithm computes the centroid decomposition in  time as follows. We first observe that a centroid node of  can be easily identified in linear time. Indeed, we can arbitrary choose a root in  and visit the tree to compute the size of each subtree. Then, we start from the root and move to the largest subtree until we reach a node whose subtrees have size at most . This node is a centroid of the tree. The centroid decomposition is computed by repeating the above algorithm recursively in each component. It easily follows that the decomposition of the tree can be computed in  time. However, there exist construction algorithms to compute the decomposition in linear time [16], [17].

In the following, we will use the centroid decomposition 
 of the suffix tree  of a string S. Given a node u in , we use  to denote its locus, i.e., the string obtained by concatenating the sequence of labels encountered along the path from the root to u.

Assume we are given a pattern  and our goal is to find the suffix of S that shares the longest common prefix with P. This problem can be easily solved with the suffix tree  of S with the following two-phase strategy: We first identify the highest node 
⁎
 in  such that 
⁎
 shares the longest common prefix with P. Then, we try to extend the match by comparing the remaining characters of P with the characters on the edge between 
⁎
 and one of its children, i.e., the child where the label starts with the character 
⁎
.

We can perform the same search for 
⁎
 on the centroid decomposition 
 of the suffix tree of S. The search is done by traversing a root-to-node path of  nodes. We start from the root of 
 and we move down to the leaves. For every node u we visit, we compare  with P and decide in which of its children we have to continue the search. As the target node 
⁎
 is guaranteed to be visited, we simply take track of the visited node sharing the longest common prefix with P. Based on the result of comparing  and P, there are the following cases:

•
If  equals P, then u is our target node 
⁎
 and we conclude.

•
If  is not a prefix of P, we continue to search on the child of u which corresponds to the connected component containing the parent of u in the suffix tree. If such a node does not exist, we conclude.

•
If  is a prefix of P, we continue the search on the connected component containing one of the children of u in the suffix tree. The child is the node v such that the first character of the edge between u and v equals the character . This is exactly the node v that a normal search on the suffix tree would visit next, once the search reaches u. Notice that in general v is not a child of u in the centroid decomposition. If such a node does not exist, we finish the visit.

Let us now show how to use the above algorithm to reconstruct an unknown string S.

Solution with prefix queries to the oracle.  We first describe our algorithm for querying the oracle in an easier setting. Instead of answering substring queries, the oracle answers prefix queries: given a string P, the oracle tells us whether P is a prefix of the unknown string S. This model is stronger because it allows us to remain anchored to the beginning of S while reconstructing it.1 A direct consequence is that the algorithm is easier and faster.

We now describe how to reconstruct a string S with 
 prefix queries to the oracle.

Our algorithm works in steps. In the i-th step it reconstructs the i-th LZ77 phrase 
. Once 
 is reconstructed, the algorithm knows the string 
, which is the concatenation of all the phrases reconstructed so far. Observe that 
 is the longest substring of 
 such that the prefix query 
 is answered affirmatively.

The phrase 
 is identified with 
 prefix queries as follows. Assume we have the suffix tree  of 
 and its centroid decomposition 
. Our first goal is to identify the lowest node 
⁎
 in  such that 
⁎
 is a prefix of S. This can be done by performing a search for the unknown pattern 
⁎
 on 
. Even if 
⁎
 is unknown, the search can be performed correctly. Indeed, observe that 
⁎
 and all its ancestors in  are the only nodes u such that 
 is a prefix of S. Thus, we perform the search on the centroid tree by binary searching for 
⁎
 on root-to-
⁎
 path. The cost of the search is 
 prefix queries. Indeed, we need to visit 
 nodes of 
 to identify 
⁎
. For each visited node u, we need a query to check if 
 is a prefix of S. If this is the case, at most σ queries of the form 
, with , are needed to know in which child of u we have to continue our search. Otherwise, we move to the component containing the parent of u, if any.

Once we know 
⁎
, we have to extend 
⁎
 to match 
. Indeed, 
 may end up in the middle of the edge from 
⁎
 to one of its children, say v, in . This step can be easily done with 
 queries. First, we use  queries to identify the child v of u, then we perform an exponential search on the length of the edge label.

We conclude by proving that the reconstruction of S takes  time. A trivial implementation of our algorithm consists in rebuilding at each step i the suffix tree of string 
 and its centroid decomposition from scratch. This takes quadratic time.

A faster algorithm is the following: First, we observe that, as the string is reconstructed from left to right, we can use the Ukkonen's construction of the suffix tree [18]. This construction builds the suffix tree in  time and linear space. It is an online algorithm that processes the string from left to right, hence it allows us to build the suffix tree of the prefix of the string that we have already reconstructed.

The centroid decomposition of the suffix tree is instead kept updated dynamically. Brodal et al. [16] showed how to keep an approximation of the centroid decomposition of a tree subject to insertions of new nodes in  amortized time per insertion. The decomposition is approximated in the sense that each selected centroid node splits the tree in connected components having a fraction 
 
 of the overall tree dimension, for any 
 
. The height of the 
 is still , thus this approximated decomposition suffices for our purposes.

Solution with substring queries to the oracle.  The string S can be reconstructed with substring queries using an easy variant of the above algorithm. The algorithm reconstructs (a portion of) the string exactly as described above, until the string reconstructed so far, say 
, cannot be further extended to the right, hence we know that 
 is a suffix of S. Then, we start extending 
 from its beginning, proceeding backwards (that is, prepending characters to 
). More formally, our strategy first queries forward strings and builds the suffix tree and the LZ77 factorization of some suffix  of the string. Then, we build the suffix tree of 
 
 and proceed backwards, building the LZ77 factorization of the remaining portion 
 
. Since the size g of the smallest grammar is invariant under reversals and upper-bounds the number of Lempel-Ziv phrases, in both phases we generate at most  phrases. The following theorem is therefore immediate.

Theorem 9

Let S be a string of length n over the alphabet . There exists an algorithm that reconstructs S with  substring queries to the oracle. The algorithm runs in  time using linear space.

Finally, Theorem 7 follows from the well-known bound 
 (see also Navarro [10]).

Running example.  Suppose we have already reconstructed the string 
. This is a substring of the unknown string S shown in Fig. 1c. There are two occurrences of 
 in S and the red cells highlight the characters that we still need to learn. The suffix tree  of 
 (see Fig. 1a) has been built online with Ukkonen's algorithm and it will be updated as soon as we learn more characters. For this reason we do not append any special character at the end of 
. Thus, there may exist suffixes of 
 which do not have their leaves in  because they are proper prefixes of some another suffix. In our example this happens to the last three suffixes A, AA and AAA which are proper prefixes of the whole string 
. Nodes of  are numbered (in our example levelwise just for convenience) to map the corresponding node in the centroid decomposition 
 (see Fig. 1b).

Fig. 1
Download : Download high-res image (156KB)
Download : Download full-size image
Fig. 1. A running example. (For interpretation of the colors in the figure, the reader is referred to the web version of this article.)

The label on the edge from node u to its child v reports the interval  of positions on string 
 representing the locus of node v. For example, the label on the edge from node 5 to node 13 is  and, thus, . In the centroid decomposition 
, each node u is labeled with the interval of positions of . The label of node 13 is  because . In the centroid tree, the leftmost child of any node u is the centroid decomposition of the connected component containing the parent of u (if any) while the other children are the centroid decompositions of the subtrees rooted at the children of u in  (if any). Note that for any child v of node u but, possibly, the leftmost one, we have that  is a prefix of .

We start from the root of 
 (node 0) which in our example, by coincidence, corresponds to the root of . We query the oracle for the substring 
. As  is the empty string, the oracle's answer will be positive. The algorithm continues on a child of node 0.

For any child v of u, let be 
 the character such that 
 is a prefix of , i.e., 
. We process the children of node 0 and continue on a node v such that the query for the substring 
 is successful. There may be several such nodes v. For example, we could continue on both nodes 5 and 7. This is because both substrings 
 and 
 occur in S. We can arbitrarily choose any of these nodes but, of course, the length of the substring we reconstruct may vary. Suppose we continue on node 5. Then, we ask for the substring 
. As the answer is positive, we continue with node 12 because 
 and 
 occurs in S. We query for 
. As the answer is negative, we binary search for the longest prefix P of  such that 
 occurs in S. This way, we reconstruct the substring  and we learn 
.

4. Conclusions and future work
We investigated the connection between the string reconstruction and compression problems, establishing that they essentially coincide: the number of substring queries that need to be asked to an oracle in order to reconstruct a string S is proportional to the complexity of S.

We also showed that it is possible to efficiently reconstruct a string of length n over an alphabet of size σ using 
 queries in  time, where 
 is the number of phrases of the LZ77 factorization of S without overlaps and g is the size of the smallest grammar producing S. Immediate improvements over our work would be to replace 
 with the more powerful z (i.e., allowing overlaps), or to shave log factors from the complexities of our algorithms. In particular, we know that the number of queries cannot be improved by more than a factor 
 
 in general.

In our setting, we aim at reconstructing the whole unknown string S. One can also consider the problem of reconstructing the set of all substrings of S of a given length k, see for example [4]. Notice that knowing all the substrings of S of length  allows one to uniquely determine S, where  is the repetition index of S, that is, the length of the longest repeat of S [19], [20].

Another direction of investigation consists in introducing uncertainty into the model. For example, allowing the oracle to answer the queries with a certain probability of returning a wrong result — this could model strings with character ambiguities, e.g., DNA strings arising from a sequencing — or allowing the oracle to return positive answers to queries within a limited Hamming distance from substrings of the target string.