In this paper we study regular expression matching in cases in which the identity of the symbols received is subject to uncertainty. We develop a model of symbol emission and uses a modification of the shortest path algorithm to find optimal matches on the Cartesian Graph of an expression provided that the input is a finite list. In the case of infinite streams, we show that the problem is in general undecidable but, if each symbols is received with probability 0 infinitely often, then with probability 1 the problem is decidable.

Introduction
Regular expressions are a useful and compact formalism to express regular languages, and are frequently used in text-based application such as text retrieval, query languages or computational genetics. Approximate string matching is one of the classical problems in this area [1]. Given a text of length n, a pattern of length m and a number k of errors allowed, we want to find all the sub-strings in the text that match the pattern with at most k errors. If the text is not known in advance (viz., if the algorithm must work on-line, without pre-processing the text), then dynamic programming can provide a solution of complexity O(mn) [18, 26], while improved algorithms can run in O(kn) [10, 31, 32].

Regular expressions can be used as pattern detectors in more general situations, such as activity detection [5]. In this context, the approximation problem takes a new form: the problem is not just matching despite the absence of expected symbols or the presence of spurious ones. The problem is that, in many applications, the identity of the symbols received is uncertain, and known only probabilistically. That is, at each input position, rather than having a symbol drawn from an alphabet Î£, we have a probability distribution on Î£. The problem, in this case, is to find the most likely sequence of symbols that matches the expression.

In this paper, we present algorithms to solve this problem, and we study their properties, both for matching sub-strings of finite strings and of infinite streams.

Our matching model is in some measure related to Markov models used for sequence alignment, a technique quite common in bioinformatics [16]. In particular, our model bears some resemblance to Profile Hidden Markov Models (PHMM: Markov models with states representing symbol insertion and symbol deletion) for multiple alignments of sequences [8, 27]. In both PHMM and our algorithms. matching can be seen as traversing a maximal path with additive logarithmic weights. PHMM have been developed to align sequences with gaps and insertions; it should be in principle possible to extend them to matching regular expressions, but the derivation of a PHMM from an expression appears to be quite complex.

Weighted automata [7] have also been used for problems related to ours. As a matter of fact, the Cartesian graph, which we use in this paper, can be seen as an equivalent formalism and as an implementation of matching using weighted automata. Graphs provide a more direct implementation and a simple instrument for studying the properties of the methods.

Early work on infinite streams has generally focused on the recognition of the whole infinite sequence (ğœ”-word): an ğœ”-word is accepted ih the automaton can read it while going through a sequence of states in which some final state occurs infinitely often (BÃ¼chi acceptance, [28, 29]), an approach that has been extended to infinite trees [21, 22]. The problem that we are considering here is different in that we are trying to match finite sub-words of an infinite word. This problem, without dealing with uncertainty, was considered in [25].

Matching with uncertain symbolsâ€”the problem that we are considering hereâ€”is gaining prominence in fields in which uncertainty in the data is the norm due to the imprecision of detection algorithms. The detection of complex audio or video events is an example. Some attempts at the definition of high-level languages for video events were made in the 1990s using temporal logic [6], Petri Nets [11] or semi-orders [2]; they had little impact at the time due to the relative immaturity of detection techniques and to the paucity of video data sets available.

With the progress of detection techniques and the availability of more data to train sophisticated classifiers, things have begun to change, and researchers "have started working on complex video event detection from videos taken from unconstrained environment (sic), where the video events consist of a long sequence of actions and interactions that lasts tens of seconds to several minutes" [17]. These new possibilities open up opportunities for video event detection but also new semantic problems [12, 19, 20].

In this new scenario, researchers have begun to explore complex event languages. Francois et al. [9] define complex events from simple ones using an event algebra with operations such as sequence, iteration, and alternation. In [15] and [23] stochastic context-free grammars are used, while in [13] event models are defined using case frames. As in other cases, these systems assume that different events are separated (no event is part of another one) and that their length is known, thus eschewing the length bias and the decidability problems that figure prominently in this paper.

In our model, we consider the alphabet symbols as elementary events that the system can recognize (we assume that there are a finite number of them) and whose detection is subject to uncertainty, so that the uncertainty of event detection translates to an uncertainty over which symbol is present in input. We assume that the information that we have can be represented as a stochastic observation process ğœˆ, where ğœˆ[ğ‘˜](ğ‘) is the probability that the alphabet symbol a were the kth symbol of the input sequence.

Within this general framework, we consider the following problems:

Finite estimation:
we consider a finite sequence of uncertain input symbols (that is, a finite stochastic process on Î£), called the observation. Assuming that at least one sub-string of the sequence matches the expression, which is the most likely matching sub-string given the observation?

Finite matching:
given a finite number of observations, what is the probability that at least one sub-string matches the expression?

Infinite estimation:
we show that, in general, estimation is undecidable in infinite stream. However, if for each symbol the probability of observing it is zero infinitely often, then with probability one estimation can be decided in finite time.

The paper is organized as follows. In Sect. 2 we remind a few facts about regular expression in order to establish the language and the basic facts that we shall use in the rest of the paper. In Sect. 3 we present a matching algorithm based on the Cartesian Graph; although this algorithm is equivalent to standard NFA algorithms, it provides a more convenient formalism to discuss the extension to uncertain data. In Sect. 4 we present our model of uncertainty, modeling it as the emission of an unobservable string on a noisy channel. Section 5 presents the algorithm for finite estimation, while in Sect. 6 we present the algorithm for finite matching. Section 7 proves the properties of matching algorithms on infinite streams, while Sect. 8 draws some conclusions.

Some facts about regular expressions
We present here a brief review of some relevant facts about regular expressions, limited to what we shall use in the remainder of the paper. The interested reader may find more detailed information in the many papers and texts on the subject [3, 14].

Let Î£ be a finite set of symbol, which we call the alphabet. We shall denote with Î£âˆ— the set of finite sequences of symbols of Î£, including the empty string ğœ–. A word, or string on Î£ is an element ğ‘0â‹¯ğ‘ğ¿âˆ’1âˆˆÎ£âˆ—. We indicate with |ğœ”| the number of symbols of the string ğœ”. String concatenation will be indicated by juxtaposition of symbols. Ranges of ğœ” will be indicated using pairs of indices in square brackets, that is,

ğœ”[ğ‘–:ğ‘—]=ğ‘ğ‘–â‹¯ğ‘ğ‘—âˆ’1  0â‰¤ğ‘–â‰¤ğ‘—â‰¤|ğœ”|,
(1)
with ğœ”[ğ‘–:ğ‘–]=ğœ€ (the empty string). If a string is composed only of the (ğ‘–+1)th symbols of ğœ” (viz., ğ‘ğ‘–), we shall use the notation ğœ”[ğ‘–] in lieu of ğœ”[ğ‘–:ğ‘–+1], and we shall use the notations ğœ”[:ğ‘—]=ğœ”[0:ğ‘—] and ğœ”[ğ‘—:]=ğœ”[ğ‘—:|ğœ”|]; note that ğœ”[ğ‘–:ğ‘—]ğœ”[ğ‘—:ğ‘˜]=ğœ”[ğ‘–:ğ‘˜]. For the sake of consistency, we shall always use subscript to range over elements of Î£ and square brackets to range over positions of a string: ğœ”[ğ‘˜]=ğ‘ğ‘– means that the (ğ‘˜+1)th element of ğœ” is ğ‘ğ‘–âˆˆÎ£.

Syntactically, the regular expressions that we use in this paper are standard:

ğœ™::=ğ‘|ğœ™ğœ™|ğœ™âˆ—|ğœ™+ğœ™|(ğœ™)|ğœ€|ğœ‚
(2)
with ğ‘âˆˆÎ£. The symbol ğœ– represents the expression that only generates the empty string, while the symbol ğœ‚ is the expression that doesnâ€™t generate any string. Given an expression ğœ™ its length |ğœ™| is the number of symbols it contains.

Our semantics is derived from the standard semantics for ğœ”âŠ¨ğœ™ [14]. The language generated by ğœ™, ğ¿(ğœ™) is defined as ğ¿(ğœ™)={ğœ”|ğœ”âˆˆÎ£âˆ—âˆ§ğœ”âŠ¨ğœ™}. Note that ğ¿(ğœ–)={ğœ€}, and ğ¿(ğœ‚)=âˆ…. Two expressions are equivalent if they generate the same language. The recognition problem for regular expressions can be defined as follows: given an expression ğœ™ on an alphabet Î£ and a string ğœ”âˆˆÎ£âˆ—, is it the case that ğœ”âˆˆğ¿(ğœ™) (or, equivalently, that ğœ”âŠ¨ğœ™)? If the answer is yes, we say that ğœ™ recognizes ğœ”.

One important aspect of regular expressions is their connection with finite state automata.

Definition 1
A (nondeterministic) finite state automaton (NFA) is a 5-tuple îˆ­=(ğ‘„,Î£,ğ‘0,ğ¹,ğ›¿) where Q is a finite set of states, Î£ is the input alphabet, ğ‘0âˆˆğ‘„ is the initial state, ğ¹âŠ†ğ‘„ is the set of final states, and ğ›¿âŠ†ğ‘„Ã—(Î£âˆª{ğœ€})Ã—ğ‘„ is the state transition relation

In the following, we shall mostly restrict our attention to a class of NFA that we call simple. An NFA is simple if it doesnâ€™t have multiple transitions between pairs of states, except possibly for the presence of ğœ–-transitions. That is, we never have a fragment of state diagram such as

	(3)
Formally we have:

Definition 2
A NFA îˆ­=(ğ‘„,Î£,ğ‘0,ğ¹,ğ›¿) is simple if for all ğ‘,ğ‘â€²âˆˆğ‘„ and all ğ‘,ğ‘â€²âˆˆÎ£, ğ‘,ğ‘â€²â‰ ğœ–, if ğ›¿(ğ‘,ğ‘,ğ‘â€²) and ğ›¿(ğ‘,ğ‘â€²,ğ‘â€²) then ğ‘=ğ‘â€².

It is easy to transform an NFA into simple form: for each multiple arc from q to ğ‘â€² and for each symbol a in that arc, one creates a new state ğ‘ğ‘ connected to ğ‘â€² with an ğœ–-transition and connects q to ğ‘ğ‘ with an arc labeled a. That is, if ğ›¿ contains a subset

ğ›¿â€²={ğ›¿(ğ‘,ğ‘1,ğ‘â€²),â€¦,ğ›¿(ğ‘,ğ‘ğ‘˜,ğ‘â€²)}
(4)
which violates the condition, this subset is eliminated from ğ›¿ and replaced with

ğ›¿â€³={ğ›¿(ğ‘,ğ‘1,ğ‘ğ‘1),â€¦,ğ›¿(ğ‘,ğ‘ğ‘˜,ğ‘ğ‘ğ‘˜),ğ›¿(ğ‘ğ‘1,ğœ–,ğ‘â€²),â€¦,ğ›¿(ğ‘ğ‘ğ‘˜,ğœ–,ğ‘â€²)}.
(5)
It is easy to see that the NFA with transitions (ğ›¿âˆ–ğ›¿â€²)âˆªğ›¿â€³ is simple and equivalent to the original one. Graphically, the process can be represented (for ğ‘˜=2) as

	(6)
Note that the most common algorithms for building an NFA given an expression ğœ™, such as Thompsonâ€™s [30] create simple automata.

Matching as path finding
The matching algorithm that we use in this paper is a modification of a method known as the Cartesian graph (also known as the DB-Graph) [24]. Let îˆ­=(ğ‘„,Î£,ğ‘0,ğ¹,ğ›¿) be the (nondeterministic) automaton that recognizes a regular expression ğœ™, and let ğœ”=ğ‘0â‹¯ğ‘ğ¿âˆ’1 be a finite string of length L. We build the Cartesian graph ğ¶(ğœ™,ğœ”)=(ğ‘‰,ğ¸) as follows:

(i)
V is the set of pairs (q, k) with ğ‘âˆˆğ‘„ and ğ‘˜âˆˆ[0,â€¦,ğ¿];

(ii)
(ğ‘¢,ğ‘£)âˆˆğ¸ if either:

(a)
ğ‘¢=(ğ‘,ğ‘˜âˆ’1), ğ‘£=(ğ‘â€²,ğ‘˜) and ğ›¿(ğ‘,ğ‘ğ‘˜âˆ’1,ğ‘â€²), or

(b)
ğ‘¢=(ğ‘,ğ‘˜), ğ‘£=(ğ‘â€²,ğ‘˜) and there is an ğœ€-transition between q and ğ‘â€², that is ğ›¿(ğ‘,ğœ€,ğ‘â€²).

In order to simplify the representation, in the figures we shall indicate the vertex (ğ‘ğ‘–,ğ‘˜) as ğ‘ğ‘˜ğ‘–. Recognition using the graph is based on the following result:

Theorem 1
ğœ”âŠ¨ğœ™ iff ğ¶(ğœ™,ğœ”) has a path (ğ‘0,0)âŸ¶âˆ—(ğ‘,ğ¿) with ğ‘âˆˆğ¹.

Proof
ğœ”âŠ¨ğœ™ iff the automaton has an accepting run, that is, a sequence of states ğ‘0ğ‘1â‹¯ğ‘ğ‘› such that ğ›¿(ğ‘ğ‘–âˆ’1,ğ‘ğ‘–âˆ’1,ğ‘ğ‘–) and ğ‘ğ‘›âˆˆğ¹. It is immediate to see from the definition of the graph that such a run exists iff there is a path

(ğ‘0,0)â†’â‹¯â†’(ğ‘ğ‘›,ğ¿)
(7)
in ğ¶(ğœ™,ğœ”) (ğ¿â‰¤ğ‘›)Footnote1. â—»

In many cases we shall be interested in determining whether there is a sub-string ğœ”[ğ‘–:ğ‘—] of ğœ” that matches ğœ™. To this end, it is easy to verify the following result:

Corollary 1
ğœ”[ğ‘–:ğ‘—]âŠ¨ğœ™ iff ğ¶(ğœ™,ğœ”) has a path (ğ‘0,ğ‘–)âŸ¶âˆ—(ğ‘ ,ğ‘—) with ğ‘ âˆˆğ¹.

Example I
Consider the expression ğœ™â‰¡(ğ‘ğ‘ğ‘)âˆ—ğ‘âˆ—ğ‘ğ‘âˆ—, corresponding to the NFA

	(8)
and the string ğœ”=ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘. The graph ğ¶(ğœ™,ğœ”) is

	(9)
The double edges show a path from ğ‘00 to ğ‘84 corresponding to the accepting run ğ‘0ğ‘3ğ‘2ğ‘0ğ‘3ğ‘2ğ‘0ğ‘1ğ‘4ğ‘4, which shows that the string matches the expression. Note that the sub-string ğœ”[3:7]=ğ‘ğ‘ğ‘ğ‘ also matches the expression, corresponding to the path ğ‘30â†’ğ‘43â†’ğ‘52â†’ğ‘60â†’ğ‘61â†’ğ‘74.

Example II: Consider the same expression and the string ğœ”=ğ‘ğ‘ğ‘; the graph ğ¶(ğœ™,ğœ”) is now

	(10)
The graph has no path from ğ‘00 to ğ‘34, indicating that the string doesnâ€™t match the expression. However, there is a path from ğ‘00 to ğ‘24, indicating that the sub-string ğœ”[:2]=ğ‘ğ‘ does match the expression

The uncertainty model
We consider the probabilistic model of string production and detection shown schematically in Fig. 1.

Fig. 1
figure 1
The model of string production and detection. The string ğœ”, of length L, is generated by a Markov chain over the alphabet Î£. A channel noise corrupts the stream so that at the output we observe the stochastic process ğœˆ, where ğœˆ[ğ‘˜] is the probability distribution of the kth element of ğœ” and ğœˆ[ğ‘˜](ğ‘) is ğ‘ƒ{ğœ”[ğ‘˜]=ğ‘}. This process is fed to the recognition algorithm, which determines the most likely interpretation of ğœˆ that matches the expression

Full size image
The module M emits a string ğœ”=ğ‘0â‹¯ğ‘ğ¿âˆ’1âˆˆÎ£âˆ—. In many cases of practical interest, the elements ğœ”[ğ‘˜] are not emitted independently. Rather, the fact that ğœ”[ğ‘˜]=ğ‘ğ‘˜ skews the probability distribution of ğœ”[ğ‘˜+1]. Correspondingly, we assume that M is a Markov chain with transition probabilities ğœ(ğ‘|ğ‘), ğ‘,ğ‘âˆˆÎ£. In this case, ğœ(ğ‘ğ‘–|ğ‘ğ‘–âˆ’1) is the conditional probability distribution of the ith element of ğœ”. In order to simplify the equations that follow, we formally define ğœ(ğ‘0|ğ‘âˆ’1)=â–³ğœ(ğ‘0), the a priori probability that the first symbol of the chain were ğ‘0

The channel N introduces some noise so that, when the symbol ğœ”[ğ‘˜]=ğ‘ is emitted, we observe a probability distribution ğœˆ[ğ‘˜] over Î£ such that

ğœˆ[ğ‘˜](ğ‘)=ğ‘ƒ{ğœ”[ğ‘˜]=ğ‘}
(11)
the values ğœˆ[ğ‘˜](ğ‘) are the observations on which we base the estimation, and constitute, together with the transition probabilities ğœ(ğ‘|ğ‘), the input of the problem.

Suppose that a string ğœ” is produced by the module M, that the transition probabilities ğœ are known a priori, and that the stochastic process ğœˆ is observed. The string ğœ” is, of course, unobservable. We are interested in two problems:

finite estimation:
assuming that there is at least one substring of ğœ”, ğœ”[ğ‘–:ğ‘—] such that ğœ”[ğ‘–:ğ‘—]âŠ¨ğœ™, which is the most likely matching substring?

finite matching:
can we determine (with a prescribed confidence) whether there is at least one substring ğœ”[ğ‘–:ğ‘—] such that ğœ”[ğ‘–:ğ‘—]âŠ¨ğœ™?

The solution of the second problem can be based on the solution of the first, to which we now turn.

Finite estimation
Given that the module M emits a string ğœ” of length L, in this section we are interested in finding the most likely substring ğœ”[ğ‘–:ğ‘—] that matches ğœ™.

When we match substrings, we are trying to match ğœ™ with strings of different length, and this entails that we must compensate a bias towards shorter strings. The a posteriori probability of a string ğœ” is given by the product of the probabilities of its constituent symbols. These probabilities, in general, will be composed of two terms: a probability that ğœ”[ğ‘˜] were in the string given the observations ğœˆ[ğ‘˜], and the probability ğœ(ğœ”[ğ‘˜]|ğœ”[ğ‘˜âˆ’1]) that the symbol ğœ”[ğ‘˜] were generated. Both these terms have values in [0, 1], and so has their product. This means that the a posteriori probability of ğœ”[ğ‘–:ğ‘—] is the product of (ğ‘—âˆ’ğ‘–) terms smaller than one. That is, cÅ“teris paribus, a shorter string, being the product of a smaller number of terms, will have a higher probability and will therefore be chosen.

We avoid this bias by considering the information carried by a string. If we have no a priori information on the string that is produced, its being revealed to us would carry an information ğœ„(ğœ”)=âˆ’logğ‘ƒ(ğœ”). If we have observed the process ğœˆ, we already possess some information about the string, and its being revealed to us would give us an information ğœ„ğœˆ(ğœ”)=âˆ’logğ‘ƒ(ğœ”|ğœˆ)â‰¤ğœ„(ğœ”). The information that the process ğœˆ gives us about the string ğœ” is the difference of these two values:

ğ¼(ğœ”,ğœˆ)=ğœ„(ğœ”)âˆ’ğœ„ğœˆ(ğœ”)=logğ‘ƒ(ğœ”|ğœˆ)âˆ’logğ‘ƒ(ğœ”)=logğ‘ƒ(ğœ”|ğœˆ)ğ‘ƒ(ğœ”).
(12)
Given ğœˆ, we search the string ğœ” that maximizes ğ¼(ğœ”,ğœˆ). The term ğ‘ƒ(ğœ”) at the denominator (which comes from considering the a priori information ğœ„(ğœ”)) avoids the bias toward shorter strings. Given two strings ğœ”1 and ğœ”2 with the same a posteriori probability and ğœ”2 longer than ğœ”1, ğœ”2 will be selected since ğ‘ƒ(ğœ”2)<ğ‘ƒ(ğœ”1). The rationale here is that longer strings are less likely to be emitted by chance so if we have equal evidence to support the hypothesis that either ğœ”1 and ğœ”2 were emitted, it is reasonable to select ğœ”2.

In order to compute ğ¼(ğœ”,ğœˆ), we begin by computing ğ‘ƒ(ğœ”|ğœˆ). Let ğœ”=ğ‘0â‹¯ğ‘ğ¿âˆ’1. Then

ğ‘ƒ(ğœ”âˆ£âˆ£âˆ£ğœˆ)=ğ‘ƒ(ğœ”[ğ¿âˆ’1],ğœ”[:ğ¿âˆ’2]âˆ£âˆ£âˆ£ğœˆ[ğ¿âˆ’1],ğœˆ[:ğ¿âˆ’2])=ğ‘ƒ(ğœ”[ğ¿âˆ’1]âˆ£âˆ£âˆ£ğœˆ[ğ¿âˆ’1],ğœˆ[:ğ¿âˆ’2],ğœ”[:ğ¿âˆ’2])ğ‘ƒ(ğœ”[:ğ¿âˆ’2]âˆ£âˆ£âˆ£ğœˆ[ğ¿âˆ’1],ğœˆ[:ğ¿âˆ’2])=ğ‘ƒ(ğœ”[ğ¿âˆ’1]âˆ£âˆ£âˆ£ğœˆ[ğ¿âˆ’1],ğœ”[:ğ¿âˆ’2])ğ‘ƒ(ğœ”[:ğ¿âˆ’2]âˆ£âˆ£âˆ£ğœˆ[:ğ¿âˆ’2]).
(13)
The last equality reflects the fact that ğœ”[ğ¿âˆ’1] only depends on observations at time ğ¿âˆ’1. We make the hypothesis that the conditional probability of ğ‘ğ‘˜ occurring in position t conditioned on ğœˆ[ğ‘¡] depends only on the observation on ğ‘ğ‘˜, at step t, that is,

ğ‘ƒ(ğ‘ğ‘˜âˆ£âˆ£âˆ£ğœˆ[ğ‘¡])=ğ‘ƒ(ğ‘ğ‘˜âˆ£âˆ£âˆ£ğœˆ[ğ‘¡](ğ‘ğ‘˜));
(14)
this is tantamount to considering that our observations are complete: the value ğœˆ[ğ‘¡](ğ‘) gives us all the information available on a. With this hypothesis we have

ğ‘ƒ(ğ‘ğ¿âˆ’1âˆ£âˆ£âˆ£ğœˆ[ğ¿âˆ’1],ğœ”[:ğ¿âˆ’2])=ğ‘ƒ(ğ‘ğ¿âˆ’1âˆ£âˆ£âˆ£ğœˆ[ğ¿âˆ’1](ğ‘ğ¿âˆ’1),ğœ”[:ğ¿âˆ’2])=ğ‘ƒ(ğœˆ[ğ¿âˆ’1](ğ‘ğ¿âˆ’1)âˆ£âˆ£âˆ£ğ‘ğ¿âˆ’1,ğœ”[:ğ¿âˆ’2])ğ‘ƒ(ğ‘ğ¿âˆ’1âˆ£âˆ£âˆ£ğœ”[:ğ¿âˆ’2])ğ‘ƒ(ğœˆ[ğ¿âˆ’1](ğ‘ğ¿âˆ’1))=(*) ğ‘ƒ(ğœˆ[ğ¿âˆ’1](ğ‘ğ¿âˆ’1)âˆ£âˆ£âˆ£ğ‘ğ¿âˆ’1)ğ‘ƒ(ğ‘ğ¿âˆ’1âˆ£âˆ£âˆ£ğ‘ğ¿âˆ’2)ğ‘ƒ(ğœˆ[ğ¿âˆ’1](ğ‘ğ¿âˆ’1))=ğœˆ[ğ¿âˆ’1](ğ‘ğ¿âˆ’1)ğœ(ğ‘ğ¿âˆ’1|ğ‘ğ¿âˆ’2)ğ‘ƒ(ğœˆ[ğ¿âˆ’1](ğ‘ğ¿âˆ’1)).
(15)
The equality (*) depends on two properties: first, the measure on ğœ”[ğ¿âˆ’1] does not depend on the previous values of ğœ” and, second, on the Markov property

ğ‘ƒ(ğœ”[ğ¿âˆ’1]âˆ£âˆ£âˆ£ğœ”[:ğ¿âˆ’2])=ğ‘ƒ(ğœ”[ğ¿âˆ’1]âˆ£âˆ£âˆ£ğœ”[ğ¿âˆ’2]).
(16)
Putting this result in (13), we have

ğ‘ƒ(ğœ”âˆ£âˆ£âˆ£ğœˆ)=ğœˆ[ğ¿âˆ’1](ğ‘ğ¿âˆ’1)ğœ(ğ‘ğ¿âˆ’1|ğ‘ğ¿âˆ’2)ğ‘ƒ(ğœˆ[ğ¿âˆ’1](ğ‘ğ¿âˆ’1))ğ‘ƒ(ğœ”[:ğ¿âˆ’2]âˆ£âˆ£âˆ£ğœˆ[:ğ¿âˆ’2]);
(17)
the value

ğ‘ƒğœˆ(ğœ”)=â–³âˆğ‘˜=0ğ¿âˆ’1ğ‘ƒ(ğœˆ[ğ‘˜](ğ‘ğ‘˜))
(18)
is the a priori probability of observing ğœ”. Working out the recursion and using this definition we have:

ğ‘ƒ(ğœ”âˆ£âˆ£âˆ£ğœˆ)=1ğ‘ƒğœˆ(ğœ”)âˆğ‘˜=0ğ¿âˆ’1ğœˆ[ğ‘˜](ğ‘ğ‘˜)ğœ(ğ‘ğ‘˜|ğ‘ğ‘˜âˆ’1).
(19)
Substituting (19) in (12), the criterion that we want to maximize is

îˆ¸(ğœ”)=ğ¼(ğœ”,ğœˆ)=log[1ğ‘ƒğœˆ(ğœ”)ğ‘ƒ(ğœ”)âˆğ‘˜=0ğ¿âˆ’1ğœˆ[ğ‘˜](ğ‘ğ‘˜)ğœ(ğ‘ğ‘˜|ğ‘ğ‘˜âˆ’1)]=1ğ‘ƒğœˆ(ğœ”)ğ‘ƒ(ğœ”)âˆ‘ğ‘˜=0ğ¿âˆ’1logğœˆ[ğ‘˜](ğ‘ğ‘˜)ğœ(ğ‘ğ‘˜|ğ‘ğ‘˜âˆ’1).
(20)
The a priori probabilities ğ‘ƒğœˆ(ğœ”) and ğ‘ƒ(ğœ”) will be estimated assuming that no a priori information is available, that is, assuming a uniform distribution

ğ‘ƒ(ğœ”)=ğ‘ƒğœˆ(ğœ”)=1|Î£|ğ¿,
(21)
leading to

îˆ¸(ğœ”)=âˆ‘ğ‘˜=0ğ¿âˆ’1log(|Î£|2ğœˆ[ğ‘˜](ğ‘ğ‘˜)ğœ(ğ‘ğ‘˜|ğ‘ğ‘˜âˆ’1)).
(22)
We are interested not only in detecting maximum length strings, but in detecting sub-strings ğœ”[ğ‘–:ğ‘—] as well. To this end, we define the partial information difference:

îˆ¸ğ‘–,ğ‘—(ğœ”)=âˆ‘ğ‘˜=ğ‘–ğ‘—âˆ’1log(|Î£|2ğœˆ[ğ‘˜](ğ‘ğ‘˜)ğœ(ğ‘ğ‘˜|ğ‘ğ‘˜âˆ’1))=(ğ‘—âˆ’ğ‘–)log|Î£|2+âˆ‘ğ‘˜=ğ‘–ğ‘—âˆ’1log(ğœˆ[ğ‘˜](ğ‘ğ‘˜)ğœ(ğ‘ğ‘˜|ğ‘ğ‘˜âˆ’1)).
(23)
The second expression highlights the effect of considering prior information: the term (ğ‘—âˆ’ğ‘–)log|Î£|2 is the bias that, all else being equal, favors the detection of longer strings.

Our problem can therefore be expressed as finding the sub-string:

ğœ”Â¯=argmaxğœ”[ğ‘–:ğ‘—]âŠ¨ğœ™îˆ¸ğ‘–,ğ‘—(ğœ”).
(24)
Two simplified cases are of importance in applications. The first is when the generation of the symbols has no temporal dependence, in which case ğœ(ğ‘ğ‘˜|ğ‘ğ‘˜âˆ’1)=ğœ(ğ‘ğ‘˜), and

îˆ¸ğ‘–,ğ‘—(ğœ”)=âˆ‘ğ‘˜=ğ‘–ğ‘—âˆ’1log(|Î£|2ğœˆ[ğ‘˜](ğ‘ğ‘˜)ğœ(ğ‘ğ‘˜)),
(25)
the second is when the symbols are generated with uniform a priori probability, in which case ğœ(ğ‘ğ‘˜)=1/|Î£| and

îˆ¸ğ‘–,ğ‘—(ğœ”)=âˆ‘ğ‘˜=ğ‘–ğ‘—âˆ’1log(|Î£|ğœˆ[ğ‘˜](ğ‘ğ‘˜)).
(26)
Finding the string that maximizes îˆ¸ is the basis on which we define several forms of matching.

Definition 3
Given the string ğœ” and the expression ğœ™, we say that the sub-string ğœ”[ğ‘–:ğ‘—] matches ğœ™ with strength ğ›½ (or ğ›½-matches ğœ™), written ğœ”[ğ‘–:ğ‘—]âŠ¨ğ›½ğœ™, if ğœ”[ğ‘–:ğ‘—]âŠ¨ğœ™, and îˆ¸ğ‘–,ğ‘—(ğœ”)=ğ›½.

Matching is defined as an optimality criterion over ğ›½-matchings. We use two such criteria: the first (weakly optimal) restricts optimality to continuations of a string, while the second (strongly optimal) extends it to all matching sub-strings.

Definition 4
Given the string ğœ” and the expression ğœ™, ğœ” weakly-optimally matches ğœ™ (wo-matches ğœ™, written ğœ”âŠ¨ğ‘¤ğœ™) if:

(i)
ğœ”âŠ¨ğ›½ğœ™;

(ii)
for all ğœ”â€²âˆˆÎ£âˆ—, if ğœ”ğœ”â€²âŠ¨ğ›½â€²ğœ™, then ğ›½â‰¥ğ›½â€².

Definition 5
Given the string ğœ” and the expression ğœ™, ğœ” strongly-optimally matches ğœ™ (so-matches ğœ™, written ğœ”âŠ¨ğ‘ ğœ™) if:

(i)
ğœ”âŠ¨ğ›½ğœ™;

(ii)
for all ğœ”â€²âˆˆÎ£âˆ—, if ğœ”â€²âŠ¨ğ›½â€²ğœ™, then ğ›½>ğ›½â€².

The following property is obvious from the definition

Lemma 1
For all ğœ”, ğœ™, if ğœ”âŠ¨ğ‘ ğœ™, then ğœ”âŠ¨ğ‘¤ğœ™.

Matching method
We match the expression to uncertain data using a modification of the Cartesian graph. Let îˆ­=(ğ‘„,Î£,ğ‘0,ğ¹,ğ›¿) be the NFA that recognizes the expression ğœ™, ğœˆ the observed process of length n and, for ğ‘–=0,â€¦,ğ‘›âˆ’1, ğ‘ğ‘–âˆˆÎ£, let ğœˆ[ğ‘–](ğ‘) be given. We shall assume that îˆ­ is simple. The modified Cartesian graph is a weighted graph ğ¼(ğœ™,ğœˆ)=(ğ‘‰,ğ¸,ğ‘¤), ğ‘¤:ğ¸â†’îˆ¾ defined as follows:

(i)
ğ‘‰={(ğ‘,ğ‘˜)âˆ£âˆ£âˆ£ğ‘âˆˆğ‘„,ğ‘˜âˆˆ{0,â€¦,ğ¿}}

(ii)
(ğ‘¢,ğ‘£)âˆˆğ¸ and ğ‘¤(ğ‘¢,ğ‘£)=ğ‘Ÿ if either

a.
ğ‘¢=(ğ‘,ğ‘˜), ğ‘£=(ğ‘â€²,ğ‘˜+1), there is a such that ğ›¿(ğ‘,ğ‘,ğ‘â€²), ğ‘Ÿ=ğœˆ[ğ‘˜](ğ‘)>0 (note that 0<ğ‘Ÿâ‰¤1);

b.
ğ‘¢=(ğ‘,ğ‘˜), ğ‘£=(ğ‘â€²,ğ‘˜), ğ›¿(ğ‘,ğœ–,ğ‘â€²) and ğ‘Ÿ=1.

For such an edge, we set ğœ[(ğ‘¢,ğ‘£)]=ğ‘âˆˆÎ£ if ii.a applies, and ğœ[(ğ‘¢,ğ‘£)]=ğœ– if ii.b applies; that is, given an edge e, ğœ[ğ‘’] is the symbol that causes e to be crossed.

In order to use the graph to find îˆ¸-matches, we need a way to associate possible strings (viz., strings with non-zero probability) to paths in the graph. Given the path ğœ‹=[ğœ‹0,â€¦,ğœ‹ğ‘›] with ğœ‹ğ‘˜=(ğ‘ ğ‘˜,â„), â„â‰¤ğ‘˜, we build the string ğœ”[ğœ‹] applying the function mstr in Fig. 2.

Fig. 2
figure 2
The function that builds the string ğœ”[ğœ‹] associated to a path ğœ‹ in the Cartesian graph C. If ğœ‹ is not a path in C, the function returns the empty string ğœ–

Full size image
Lemma 2
If the NFA is simple then for each path ğœ‹,  mstr (ğ¶,ğœ‹) is unique.

This lemma is a consequence of the fact that, if the NFA is simple, for each edge there is only one ğ‘âˆˆÎ£ that causes it to be traversed, that is, ğœ[ğ‘’] is a well-defined function.

Lemma 3
Let ğœ‹ be a path and ğœ”= mstr (ğ¶,ğœ‹)=ğ‘0â‹¯ğ‘ğ¿âˆ’1. Then, for all ğ‘˜=0,â€¦,ğ¿âˆ’1, ğœˆ[ğ‘˜](ğ‘ğ‘˜)>0.

Proof
If ğœ”[ğ‘˜]=ğ‘ğ‘˜, then, by step 3, (ğœ‹ğ‘˜,ğœ‹ğ‘˜+1)âˆˆÎ£ and by step 4, ğ‘ğ‘˜=ğœ[(ğœ‹ğ‘˜,ğœ‹ğ‘˜+1)]. By condition ii.a of the definition of the graph C (page 11), this entails that ğœˆ[ğ‘˜](ğ‘ğ‘˜)>0. â—»

Theorem 2
Given the path ğœ‹=[ğœ‹0,â€¦,ğœ‹ğ‘›], if ğœ”= mstr (ğ¶,ğœ‹), ğœ‹0=(ğ‘ 0,ğ‘˜) and ğœ‹ğ‘›=(ğ‘ ,ğ‘˜+â„) with ğ‘ âˆˆğ¹, then ğœ”âŠ¨ğœ™.

Proof
Let ğ¶â€² be the Cartesian graph (without uncertainty) generated by ğœ” on ğœ™. Let ((ğ‘,ğ‘¡),(ğ‘â€²,ğ‘¡+1)) be an edge on ğ¶â€² caused by ğ‘ğ‘¡âˆˆÎ£. By Lemma 3, ğœˆ[ğ‘¡](ğ‘ğ‘¡)>0, so the edge will also be an edge of G, that is, ğºâ€² is a subgraph of G.

By construction, ğºâ€² has a path ğœ‹â€²:(ğ‘0,0)â†’(ğ‘ğ‘“,|ğœ”|), with ğ‘ğ‘“âˆˆğ¹, and so has G. Because of Theorem 1, ğœ”âŠ¨ğœ™. â—»

Finding the optimal match to the expression is akin to finding the shortest path on a weighted graph, with some modifications. In a typical shortest path algorithm, each edge (u, v) has a weight w(u, v) and, given a path ğœ‹=[ğ‘¢0,ğ‘¢2,â€¦,ğ‘¢ğ‘›], the weight of the path is the sum of the weights of its edges, that is ğ‘¤[ğœ‹]=âˆ‘ğ‘›âˆ’1ğ‘–=0ğ‘¤(ğ‘¢ğ‘–,ğ‘¢ğ‘–+1). Moreover, each vertex u has associated a distance value d[u]. When the vertex is analyzed,Footnote2 its in-edges are analyzed. If the situation is the following:


then the distance value of u is updated as

ğ‘‘[ğ‘¢]â†min{ğ‘‘[ğ‘£ğ‘–]+ğ‘¤ğ‘–,ğ‘–=1â€¦,ğ‘›}
(27)
In our case, rather than with a weight w, we mark each edge with a pair (ğ‘,ğœˆ[â‹…](ğ‘)), where ğ‘âˆˆÎ£ is the symbol that causes that edge to be crossed, and ğœˆ is the probability that the symbol emitted at that particular step were a. We look for the most probable path, that is, we are trying to maximize, rather than minimize, a suitable (additive) function of the weights of the graph.

We also have, with respect to the standard algorithm, a complication due to the conditional probabilities of the Markov chain: in the general case, the estimation that we have to minimize for the node u depends not only on the in-neighbors ğ‘£1,â€¦,ğ‘£ğ‘›, but also on the label of the edges of the optimal paths that enter the nodes ğ‘£1,â€¦,ğ‘£ğ‘›. To clarify this point, given a node u, let îˆ¸[ğ‘¢] be the estimate of the criterion that we are maximizing for the paths through u. Suppose we are evaluating a path entering u:


The estimation îˆ¸[ğ‘¢] for this path is given by

îˆ¸[ğ‘¢]=îˆ¸[ğ‘£]+log(|Î£|2ğœˆ[â‹…](ğ‘)ğœ(ğ‘|ğ‘))
(28)
So, in order to update the estimate îˆ¸[ğ‘¢] we need to look at one edge further back than we would for a normal path-finding algorithm. In general, given the edges entering u:


we have

îˆ¸[ğ‘¢]=max{îˆ¸[ğ‘£ğ‘–]+log(|Î£|2ğœˆ[â‹…](ğ‘ğ‘–)ğœ(ğ‘ğ‘–|ğ‘ğ‘–))}
(29)
This complication is not present if the symbols are generated independently, that is, if we are optimizing (25).

It would not be hard to adapt one of the standard shortest path algorithms to work for this case, but it is more efficient to take advantage of the structure of the Cartesian graph. In the graph, we have only two kinds of edges: forward edges (ğ‘,ğ‘˜)â†’(ğ‘â€²,ğ‘˜+1) and ğœ–-edges, corresponding to the ğœ€-transitions, (ğ‘,ğ‘˜)â†’(ğ‘â€²,ğ‘˜); ğœ–-edges can be traversed at any time without changing the value of the objective function.

Table 1 Symbols used in the path finding algorithm
Full size table
The algorithm is composed of two parts: the first is the top level function match that receives the NFA for the expression and the observations ğœˆ, builds the graph ğ¶(ğœ™,ğœˆ) with the edges marked by the probability of having received the corresponding symbol, and manages the traversal of forward edges. The second is a local relaxation function that checks whether the criterion estimation of some nodes can be improved by traversing some ğœ–-edges. This function also checks whether it is convenient to start a new path: if the state ğ‘0 has a negative estimation, then its estimation is set to 0 and a new path is started. Table 1 shows the symbols used in the algorithm. The function relax is shown in Fig. 3.

Fig. 3
figure 3
The relaxation function. The function initializes a new path (lines 2, 3) if the start state has a negative value for îˆ¸ and attempts to traverse all ğœ–-edges to check whether some value can be improved by traversing them

Full size image
The auxiliary function îˆ¸ (Fig. 4) receives a pair of states q and ğ‘â€² and a time t, and determines the value îˆ¸[ğ‘,ğ‘¡] resulting by arriving at (q, t) from (ğ‘â€²,ğ‘¡âˆ’1). If there is no edge between the two states, then the function returns âˆ’âˆ.

Fig. 4
figure 4
Auxiliary function: computes the value of the objective function îˆ¸[ğ‘,ğ‘¡] that one obtains traversing the edge ((ğ‘¢,ğ‘¡âˆ’1),(ğ‘£,ğ‘¡)). If no such edge exists, the function returns âˆ’âˆ

Full size image
The main function of the algorithm is shown in Fig. 5. The algorithm proceeds time-wise from the first symbol received to the last. The main loop adjusts the objective taking into account the edges from the previous time step, and then calls the function relax to take into account ğœ–-edges and possible re-initializations of the path.

The algorithm returns the node that maximizes îˆ¸[ q,k]; the loops of steps 9 and 10 go through the nodes of the graph and for each node (q, k), steps 11 and 13 choose the predecessor (ğ‘â€²,ğ‘˜âˆ’1) that maximizes îˆ¸[ q,k] among all symbols read at step ğ‘˜âˆ’1. If there is a node (ğ‘â€²,ğ‘˜) that provides a better îˆ¸[ q,k], that is, if the objective at step k is maximized by not reading any symbol and doing instead an ğœ€-transition, then that option will be discovered at step 9 of the function relax and the value îˆ¸[ q,k] will be updated at step 10.

Finally, step 17 of match will return the final state with the highest value of the objective function, that is, the state where the optimal accepting path ends.

At the end, ğ‘ğ‘“ contains the final state that represents the end of the most likely path. A simple recursive function (Fig. 6) can then be used to return the optimal path.

Fig. 5
figure 5
The main matching algorithm. The main loop of lines 10â€“14 proceeds time-wise updating at each step the objective estimation for all the states after symbol i. Once the "best" predecessor of a state has been found (line 11), the value of the objective for that cost as well as its predecessor are updated (lines 12â€“13). At the end, w contains the final state that represents the end of the most likely path. The path can be reconstructed following the predecessor pointers until an initial path

Full size image
Fig. 6
figure 6
The function that creates a path ending at a given state. The predicate initial is true if the parameter q is the initial state of the automaton

Full size image
As an implementation note, observe that we have presented here a fairly naÃ¯ve implementation of the algorithm, one that explicitly generates the whole graph. In a more optimized version, one can generate at step k only the states (q, k) with finite cost, and keep track only of the open paths for each node. This implementation is akin to the standard implementation of an NFA, with the additional complication that, if the optimal string is to be reported (as opposed to requiring a simple yes/no answer), one must keep track of the open paths.

The following property is an easy consequence of the maximization of îˆ¸.

Theorem 3
Let ğœ™ be an expression, N the associated NFA, and ğœˆ a series of L observations of a string. If

ğœ”= mstr ( cartesian (ğ‘), path ( match (ğ‘,ğœˆ,ğ¿)))
then ğœ”âŠ¨ğ‘ ğœ™

Example II
Consider, once more, the regular expression of Example I. We detect eight symbols from the alphabet Î£={ğ‘,ğ‘}, with probabilities as in Table 2.

Table 2 The sequence of symbols detected in input for example II, with the probabilities of detection of each one
Full size table
We assume that the symbols are independent and equiprobable, so we can apply (26). Before the first iteration, the first column of the graph has been initialized as:


Note that the state ğ‘1 has value 0 as the ğœ–-edge that joins it to ğ‘0 has been relaxed. During the second iteration, we consider the edges to the second column, with the following weights:


At this point, each node in the second column computes its value adding |Î£|ğœˆ[0](ğœ[ğ‘’]) to the value of the predecessor for each incoming edge, and taking the maximum. If the start state has a negative value or if some value may be increased by traversing an ğœ–-edge, this is done in the function relax. The graph is now:


The state ğ‘0 had a negative value of âˆ’âˆ, so it has been reset to 0, making it possible to start a new path. Continuing until all the symbols have been processed, we arrive to the graph of Fig. 7.

Fig. 7
figure 7
The graph corresponding to the expression of Example I with the input probabilities of Example II. The optimal path ends at the bottom-right state of the figure. The path is highlighted using double arrows, and corresponds to the input ababb

Full size image
The state at the bottom-right of the figure, with a value îˆ¸=3.56 is the final state where the optimal path ends. The optimal path is indicated by double arrows. Note that it does not extend to the whole input, but it begins at step 3, and corresponds to the input ababb. The reason for this is the very low probability of the symbol a in step 3, a symbol that would be necessary to continue the sequence ab that began in the first step. At the third step, the state that precedes ğ‘0 and that would have to transition to ğ‘0 to permit the continuation (state ğ‘2) has a value 0.68; given that a has a probability of only 0.01, this gives:

îˆ¸[ğ‘ 0,3]=îˆ¸[ğ‘ 2,2]+log|Î£|ğœˆ(ğ‘)=0.68+log2â‹…0.01=âˆ’4.96
(30)
This negative value causes (ğ‘0,3) to be reset to 0 in the function relax and a new path to be started.

Probability of misdetection
In this section we are interested in studying some illustrative examples of detection error. With reference to Fig. 1, we assume that the module M emits a string ğœ”=ğ‘0â‹¯ğ‘ğ¿âˆ’1 and that an initial substring ğœ”â€² matches ğœ™. We introduce some error in N, and we are interested in determining under which conditions the algorithm will misclassify, that is, it will estimate a ğœ”â€³â‰ ğœ”â€² as the best match for the expression.

Note that this can be seen as a constrained estimation problem: we estimate ğœ” based not only on the probabilities ğœˆ(ğ‘) but also on the constraint that our estimation must be such that ğœ”âŠ¨ğœ™.

Example III
Consider the following situation: we have an alphabet Î£ with ğ‘,ğ‘âˆˆÎ£ and the expression ğœ™=ğ‘âˆ—. Assume that the module M of Fig. 1 emits the string ğ‘â‹¯ğ‘ğ‘, where the symbol a is repeated n times. The detection probabilities are assumed to be constant, independent of the position:

ğœˆ[â‹…](ğ‘)ğœˆ[â‹…](ğ‘)=ğ‘=1âˆ’ğ‘|Î£|âˆ’1
(31)
We are interested in analyzing the following two scenarios:

(i)
the symbol "b" in ğœ” is correctly detected; consequently, the algorithm will detect that ğœ”âŠ­ğœ™, but that ğœ”[:ğ‘›]âŠ¨ğœ™ (correct classification), or

(ii)
the symbol "b" is misinterpreted as an "a", in which case the algorithm will match the whole ğœ” to ğœ™ (misclassification).

In the first case, the value of the objective function will be

îˆ¸=âˆ‘ğ‘˜=0ğ‘›âˆ’1log|Î£|ğœˆ[ğ‘˜](ğœ”ğ‘˜)=âˆ‘ğ‘˜=0ğ‘›âˆ’1log|Î£|ğ‘=ğ‘›log|Î£|+ğ‘›logğ‘
(32)
The second will give a value

îˆ¸â€²=âˆ‘ğ‘˜=0ğ‘›log|Î£|ğœˆ[ğ‘˜](ğœ”ğ‘˜)=âˆ‘ğ‘˜=0ğ‘›âˆ’1log|Î£|ğœˆ[ğ‘˜](ğ‘)+log|Î£|ğœˆ[ğ‘›](ğ‘)=ğ‘›log|Î£|+ğ‘›logğ‘+log|Î£|1âˆ’ğ‘|Î£|âˆ’1
(33)
The algorithm will produce the solution ii) (viz., it will misclassify) if îˆ¸â€²âˆ’îˆ¸>0, that is, if

log|Î£||Î£|âˆ’1(1âˆ’ğ‘)>0
(34)
or

ğ‘<1|Î£|
(35)
If c is relatively high, then the probability of confusion is small, the algorithm will assume that the last symbol is a "b" and match (correctly) the shorter string. On the other hand, if c is small, then the uncertainty on the symbol that has actually been emitted is higher, and the cost of assuming that the symbol is actually an "a" gives a higher value of the objective function, as it permits the identification of a longer string. Note that in this case the threshold at which misclassification occurs is independent of n, the length of the string.

Example IV
In this example, we consider a case of considerable interest in applications: spike noise (noise on a single symbol). Consider again the expression ğœ™=ğ‘âˆ— and the string ğ‘ğ‘›ğ‘ğ‘ğ‘š. We call central a the symbol that comes between the two sequences ğ‘ğ‘› and ğ‘ğ‘š, and we are interested in determining the effect of spike noise in the central a on the detection of the string. Assume, for the sake of this example, that we are interested only in detecting initial sub-strings of ğœ”. Suppose that the observations are the same for all symbols except the central a, that is

ğœˆ[ğ‘˜](ğ‘¥)=â§â©â¨âªâªâªâªğ‘ğ‘â€²1âˆ’ğ‘|Î£|âˆ’11âˆ’ğ‘â€²|Î£|âˆ’1ğ‘¥=ğ‘,ğ‘˜â‰ ğ‘›ğ‘¥=ğ‘,ğ‘˜=ğ‘›ğ‘¥â‰ ğ‘,ğ‘˜â‰ ğ‘›ğ‘¥â‰ ğ‘,ğ‘˜=ğ‘›
(36)
In our scenario, most of the symbols are detected with low noise, in particular ğ‘>1/|Î£|, while at the central a the noise spikes, that is ğ‘â€²<ğ‘. The scenarios in which we are interested are the following:

(i)
the central a is mistakenly interpreted as a different symbol, and the algorithm chooses ğ‘ğ‘› as the best initial matching string;

(ii)
the central a is correctly interpreted, and the algorithm identifies ğ‘ğ‘›+ğ‘š+1 as the best initial matching string.

The value of the objective function in the first case is

îˆ¸=âˆ‘ğ‘¡=0ğ‘›âˆ’1log|Î£|ğœˆ[ğ‘¡](ğ‘)=ğ‘›log|Î£|ğ‘
(37)
while in the second it is

îˆ¸â€²=(ğ‘›+ğ‘š)log|Î£|ğ‘+log|Î£|ğ‘â€²
(38)
The correct interpretation is chosen if îˆ¸â€²>îˆ¸, that is

|Î£|ğ‘â€²>(|Î£|ğ‘)âˆ’ğ‘š
(39)
Note that the value of ğ‘â€² for which misinterpretation occurs does not depend on n, that is, it does not depend on the part of the string before the noise spike, as this part contributes equally to both scenarios. It does, on the other hand, depend on m, that is, on the length of the portion of string that follows the spike. The relation (39) is illustrated in Fig. 8.

Fig. 8
figure 8
The relation (39), |Î£|ğ‘â€² is represented as a function of |Î£|ğ‘ for ğ‘š=1,2,5,10,20. The portion above each curve corresponds to the area in which the correct decision is made. Note that if the string that follows the spike (of length m) is short, the wrong interpretation will prevail for relatively small errors but as m grows, matching becomes more robust, and the correct interpretation is maintained for larger errors (viz. small ğ‘â€²)

Full size image
The condition ğ‘>1/|Î£| translates to |Î£|ğ‘>1, hence the lower limit of the abscissas.

For constant |Î£|, the limiting value of ğ‘â€² decreases when c increases as well as when m does. In other words, we can tolerate more noise in the central a if we have a smaller error on the other symbols or if the input string is longer: both cases provide more evidence that the whole string matched ğœ™, thus offsetting the effects of uncertainty on the central a.

Also, all else being equal, the threshold value for ğ‘â€² behaves as ğ‘â€²âˆ¼|Î£|âˆ’(ğ‘šâˆ’1), that is, it decreases as |Î£| increases. This is due mostly to the characteristics of our setup: if the probability of observing the correct symbol is held fixed to c so, as |Î£| increases, the probability of the incorrect ones decreases as 1/(|Î£|âˆ’1).

Remark 1
This example, its simplicity notwithstanding, is quite general. Each time we have an expression ğœ™ and strings ğœ”, ğœ”â€² such that ğœ”âŠ¨ğœ™ and ğœ”ğœ”â€²âŠ¨ğœ™, and an error spike on a symbol of ğœ”â€², the considerations of this example apply with ğ‘š=|ğœ”â€²|.

Finite match
We now turn to the second problem introduced in Sect. 4: finite match. Given the expression ğœ™ and an (unknown) string ğœ”=ğ‘0â‹¯ğ‘ğ¿âˆ’1, information about which is only available through the stochastic process ğœˆ, we want to know the probability that ğœ”âŠ¨ğœ™. We begin by considering matching the whole string only; we then extend the method to determine the probability that (at least) a sub-string of ğœ” match ğœ™.

We begin by determining, using the Cartesian graph, the probability that starting from a state (ğ‘ğ‘ ,ğ‘¡ğ‘ ), we arrive at a state (ğ‘â€²,ğ‘¡), ğ‘¡â‰¥ğ‘¡ğ‘ . The structure of the algorithm is similar to that of the algorithm match of Fig. 5 but, in this case, instead of computing the value îˆ¸[ğ‘,ğ‘¡] for each state we compute the probability ğ”­[ğ‘,ğ‘¡] of reaching it. We begin by setting ğ”­[ğ‘ğ‘ ,ğ‘¡ğ‘ ]=1 and ğ”­[ğ‘â€²,ğ‘¡â€²]=0 for (ğ‘â€²,ğ‘¡â€²)â‰ (ğ‘ğ‘ ,ğ‘¡ğ‘ ). We then operate iteratively in two steps: the first is a relaxation function that corrects the probability of reaching (q, t) from another state (ğ‘â€²,ğ‘¡) through an ğœ–-edge that is, the function operates the transformation shown in Fig. 9.

Fig. 9
figure 9
Updating the estimated probability of reaching the state (q, t) through an ğœ–-transation. The structure of the graph fragment that we are considering is shown in a. In b, the value ğ”­[ğ‘,ğ‘¡] is the estiated probability of reaching state (q, t) without considering the ğœ–-transition, and ğ”­[ğ‘â€²,ğ‘¡] is the probability of reaching (ğ‘â€²,ğ‘¡), the source of the transition. In c the updated probabilities are shown

Full size image
We assume that in the previous step we had already estimated the probability of arriving at (q, t) from states of type (ğ‘â€³,ğ‘¡âˆ’1). This step updates the estimate by considering the ğœ–-translation as

ğ”­[ğ‘â€²,ğ‘¡]â†ğ”­[ğ‘,ğ‘¡]+ğ”­[ğ‘â€²,ğ‘¡]
(40)
This entails, coherently with our model, that the probability of executing an ğœ€-transition is 1. The second step is a forward projection step, in which we estimate the probability of reaching states at ğ‘¡+1 based on the probabilities at t. The projection operation is shown in Fig. 10.

Fig. 10
figure 10
Updating the estimated probability of reaching the state (ğ‘,ğ‘¡+1) from the states (ğ‘1,ğ‘¡),â€¦,(ğ‘ğ‘›,ğ‘¡). The structure of the graph fragment that we are considering is shown in a. In b, the values ğ”­[ğ‘ğ‘¢,ğ‘¡] have been estimated at a previous step. In c the probability of reaching (ğ‘,ğ‘¡+1) is estimated (without considering the ğœ–-transitions between states at ğ‘¡+1) as the weighted sum of the probabilities of reaching (ğ‘ğ‘¢,ğ‘¡), weighted by the probability of having observed the input that causes the transition (ğ‘ğ‘¢,ğ‘¡)â†’(ğ‘,ğ‘¡+1)

Full size image
The probability of reaching (ğ‘,ğ‘¡+1) is a weighted sum of the probabilities of reacing the abutting (ğ‘ğ‘¢,ğ‘¡) states, each weighted by the probability of observing in input the symbol that causes the transition (ğ‘ğ‘¢,ğ‘¡)â†’(ğ‘,ğ‘¡), that is

ğ”­[ğ‘,ğ‘¡+1]â†âˆ‘ğ‘¢=1ğ‘›ğ”­[ğ‘ğ‘¢,ğ‘¡]ğœˆ[ğ‘¡](ğ‘ğ‘¢)
(41)
This procedure, alternating forward projections and relaxations, correctly determines, given the start state, the probability of reaching any other state, with an exception. If a portion of the graph has a configuration like

	(42)
then it is easy to see that ğ‘2=ğ‘3=ğ‘1â‹…ğœˆ[ğ‘˜](ğ‘), while our recursion computes ğ‘3=2â‹…ğ‘1â‹…ğœˆ[ğ‘˜](ğ‘). This configuration, however, is never encountered as the automata that we are considering are simple (see Definition 2).

The algorithm takes in input a point t of the input string and the initial state ğ‘0, and produces an array ğ”­ with the probabilities of reaching the other states: that is, ğ”­[ğ‘,ğ‘¡â€²], ğ‘¡â€²>ğ‘¡ is the probability of reaching (ğ‘,ğ‘¡â€²) starting from (ğ‘0,ğ‘¡).

The function prelax, analogous to relax of Fig. 3 works based on a topological ordering of the sub-graph of the NFA induced by the ğœ€-transitions. The ğœ€-transitions are acyclical, so the set of states of the NFA with the edges corresponding to the ğœ€-transitions is a DAG, and the topological ordering is well defined. The function eps_sort (not described here) returns the list of states topologically sorted (Fig. 11).

Fig. 11
figure 11
The relaxation function for the probability determination algorithm. The set of states is topologically sorted using the graph induced by the ğœ€-transition and each node propagates its probability value to its followers in the order

Full size image
The main function, MatchProb takes an initial node of the Cartesian graph and determines the probability of reaching all the other nodes that can be reached from the initial one (Fig. 12).

Fig. 12
figure 12
The main function for determining the probability of matching. The initial node is (ğ‘0,ğ‘¡0), which is reached with probability 1 (set in line 7). The following loop (lines 9â€“16) goes one step at the time updating at each one the probability that a state is reached through a non-ğœ€ symbol (loop of lines 10â€“14) or through an ğœ–-transaction (relax of line 15)

Full size image
The probability that the whole string match the expression is the probability that, starting at the first symbol one reaches a final at time L, being L the length of the string. That is,

ğ”­ğ‘ƒ= MatchProb ( NFA ,ğœˆ,ğ¿,0)=âˆ‘ğ‘âˆˆğ¹ğ”­[ğ‘,ğ¿âˆ’1]
(43)
To determine the probability of one matching substring, let

ğ”­ğ‘˜= MatchProb ( NFA ,ğœˆ,ğ¿,ğ‘˜)
(44)
Then ğ”­ğ‘˜[ğ‘,ğ‘¡]=0 for ğ‘¡<ğ‘˜, and for ğ‘¡â‰¥ğ‘˜, ğ”­ğ‘˜[ğ‘,ğ‘¡] is the probability that, starting from state ğ‘0 at symbol number k, and based on the observations, the unknown substring ğœ”[ğ‘˜:ğ‘¡] will lead to state q. The probability that ğœ”[ğ‘˜:ğ‘¡]âŠ¨ğœ™ is therefore

ğ”“[ğ‘˜,ğ‘¡]=â–³âˆ‘ğ‘âˆˆğ¹ğ”­ğ‘˜[ğ‘,ğ‘¡]
(45)
The probability that at least one of the sub-strings ğœ”[ğ‘˜:ğ‘¡] for ğ‘¡>ğ‘˜ match the expression is

ğ‘„[ğ‘˜]=â–³1âˆ’âˆğ‘¡=ğ‘˜+1ğ¿âˆ’1(1âˆ’ğ”“[ğ‘˜,ğ‘¡])
(46)
Finally, the probability that at least one sub-string match the expression is

ğ‘ƒ=â–³1âˆ’âˆğ‘˜=0ğ¿âˆ’1(1âˆ’ğ‘„[ğ‘˜])=1âˆ’âˆğ‘˜=0ğ¿âˆ’1âˆğ‘¡=ğ‘˜+1ğ¿âˆ’1(1âˆ’ğ”“[ğ‘˜,ğ‘¡])
(47)
Remark 2
The applicability of the probability approach is limited in the case of expressions that can be satisfied by short strings. In this case, even if the probability if seeing the right symbol is relatively low, the sheer number of possible short expressions makes the probability of at least one match quite high.

Fig. 13
figure 13
Two views of the behavior of ğ‘ƒğ‘€ (viz. the probability that at least one substring of ğœ”=ğ‘ğ¿ match ğœ™=ğ‘âˆ—) as a function of c for various values of L (a). Due to the presence of many short subexpressions, the probability approaches 1 very rapidly as c or n increase. To give a better idea of the speed of convergence to 1, in b the value 1âˆ’ğ‘ƒğ‘€ is shown on a logarithmic scale: this value approaches 0 as c or n increase

Full size image
Example V
Consider again the expression ğœ™â‰¡ğ‘âˆ— and the string ğœ”=ğ‘ğ‘›, with ğ‘âˆˆÎ£ and ğœˆ[ğ‘˜](ğ‘)=ğ‘ for ğ‘˜=0,â€¦,ğ¿âˆ’1.

If we take a specific one-symbol substring, say ğœ”ğ‘˜=ğ‘, we have â„™(ğœ”ğ‘˜âŠ¨ğœ™)=ğ‘. There are L such sub-strings, so the probability that at least one of them match ğœ™ is

ğ‘ƒ1=â–³1âˆ’âˆğ‘¢=0ğ¿âˆ’1(1âˆ’ğ‘)=1âˆ’(1âˆ’ğ‘)ğ¿
(48)
For a specific two-symbol substring we have â„™(ğœ”ğ‘˜ğœ”ğ‘˜+1âŠ¨ğœ™)=ğ‘2 and, since there are ğ¿âˆ’1 such string, we have

ğ‘ƒ2=â–³1âˆ’âˆğ‘¢=0ğ¿âˆ’2(1âˆ’ğ‘2)=1âˆ’(1âˆ’ğ‘2)ğ¿âˆ’1
(49)
In general, the probability that at least one k-symbol substring match ğœ™ is

ğ‘ƒğ‘˜=â–³1âˆ’âˆğ‘¢=0ğ¿âˆ’ğ‘˜(1âˆ’ğ‘ğ‘˜)=1âˆ’(1âˆ’ğ‘ğ‘˜)ğ¿âˆ’ğ‘˜+1
(50)
The probability that at least one substring match the expression is

ğ‘ƒğ‘€=1âˆ’âˆğ‘˜=0ğ¿âˆ’1(1âˆ’ğ‘ƒğ‘˜)=1âˆ’âˆğ‘˜=0ğ¿âˆ’1(1âˆ’ğ‘ğ‘˜)ğ¿âˆ’ğ‘˜+1
(51)
Figure 13a shows the behavior of ğ‘ƒğ‘€ as a function of c for various values of L. In order to have a better view of the speed of convergence of the function, in Fig. 13b we show the value log(1âˆ’ğ‘ƒğ‘€), which converges to âˆ’âˆ as ğ‘ƒğ‘€ converges to 1.

The probability of having at least one match is very close to 1 for ğ‘›>4 or ğ‘>0.4; this constantly high probability limits the discriminating power of the probability test.

Remark 3
The problem highlighted in the previous example is present only for expressions that can be matched by short strings. In the example, most of the probability of match is due to the probability of matching one-symbol strings: Fig. 14 shows (ğ‘ƒâˆ’ğ‘ƒ1)/ğ‘ƒ.

Fig. 14
figure 14
The value of (ğ‘ƒâˆ’ğ‘ƒ1)/ğ‘ƒ as a function of c for various values of L

Full size image
In most cases, the error that one would commit by replacing P with ğ‘ƒ1 is less than 10%; this entails that the probability method is viable for expressions that do not match short string as the fast convergence to the probability 1 would not occur in those cases.

If short matching strings are common, a viable solution for practical applications is to find the best matching substring ğœ”[ğ‘–:ğ‘—] and use the value îˆ¸ğ‘–,ğ‘—(ğœ”) as an indicator of the likelihood of matching. We shall not pursue this possibility in this paper.

Infinite streams
Many applications, especially on-line applications, require the detection of certain combinations of symbols in an infinite stream of data. Most of these applications are real-time and use a terminology a bit different from what we use here: what we have called symbols are often elementary events detected in the stream, and our position in the string corresponds to the time of detection (in a discrete time system).

In the case of infinite streams, we are not interested in finding the one sub-string that best matches the expressions: in general there will be infinitely many string in different parts of the stream, possibly partially overlapping, that match the expressions. We are interested in catching them all. This multiplicity causes several problems for the definition of a proper semantics for collecting matching strings (many problems arise out of having to decide what to do when matching strings overlap) which, in turn, may cause decidability issues [25]. We shall not consider those issues here, as they are orthogonal to the problems caused by uncertainty: if we can solve the basic problem of deciding whether ğœ”âŠ¨ğ‘¤ğœ™ under uncertainty, then all the problems related to the definition of a proper semantics in a stream can be worked out using the theory in [25] (in which these problems were considered under the hypothesis of no uncertainty).

In the case of streams, we are not typically interested in strong semantics, which represents too strong a condition for practical applications. Given a (finite) portion of the stream ğœ” such that ğœ”âŠ¨ğ›½ğœ™, it is clearly undecidable in an infinite stream whether there will be, at some future time, a portion ğœ”â€² such that ğœ”â€²âŠ¨ğ›½â€²ğœ™ with ğ›½â€²>ğ›½. Moreover, in streams we are interested in determining a collection of finite strings that match the expression, so the use of an absolute criterion such as the strong semantics (only one string can match the expression in the strong sense) is not very useful.

We shall therefore make use of the weak semantics throughout this section. Since the stream is infinite and we are interested in chunks of it, we shall assume, without loss of generality, that the strings we are testing start at the beginning of the relevant part of the stream, that is, all the strings that we test are sub-strings of type ğœ”[:ğ‘˜].

The problem we are interested in is therefore the following:

STREAM-WEAK: Given a string ğœ”, and expression ğœ™, and an infinite stream of observations ğœˆ, is it the case that ğœ”âŠ¨ğ‘¤ğœ™?

As we mentioned, we assume that if |ğœ”|=ğ¿, the recognition of ğœ” is based on the first L observations of the stream ğœˆ.

Our first result is a simple and negative one.

Theorem 4
STREAM-WEAK is undecidable.

Proof
Suppose the problem is decidable. Then there is an algorithm îˆ­ such that, for each expression ğœ™, observations ğœˆ, and string ğœ”, îˆ­(ğœ™,ğœˆ,ğœ”) stops in finite time with "yes" if ğœ”âŠ¨ğ‘¤ğœ™, and with "no" otherwise.

Consider the expression ğœ™â‰¡ğ‘âˆ—, and an alphabet Î£ with |Î£|>1 and ğ‘âˆˆÎ£. Suppose that the observations are such that îˆ¸(ğ‘ğ¿)=ğ›½ and, for ğ‘˜>ğ¿, ğœˆ[ğ‘˜](ğ‘)=ğ‘<1/|Î£|. Then, for ğ‘>ğ¿:

îˆ¸(ğ‘ğ‘)=îˆ¸(ğ‘ğ¿)+(ğ‘âˆ’ğ¿)log|Î£|ğ‘<ğ›½
(52)
Since the algorithm is correct, it will stop after M steps on "yes". Note that îˆ¸(ğ‘ğ‘€)=ğ›½â€²<ğ›½.

Consider now a new stream of observations ğœˆâ€² with ğœˆâ€²[ğ‘˜]=ğœˆ[ğ‘˜] for ğ‘˜â‰¤ğ‘€ and, for ğ‘˜>ğ‘€, ğœˆâ€²[ğ‘˜](ğ‘)=ğ‘>1/|Î£|, that is, log|Î£|ğ‘>0. Take ğ‘„>ğ‘€, then

îˆ¸(ğ‘ğ‘„)=îˆ¸(ğ‘ğ‘€)+(ğ‘„âˆ’ğ‘€)log|Î£|ğ‘=ğ›½â€²+(ğ‘„âˆ’ğ‘€)log|Î£|ğ‘
(53)
If

ğ‘„>ğ‘€+ğ›½âˆ’ğ›½â€²log|Î£|ğ‘
(54)
then îˆ¸(ğ‘ğ‘„)>îˆ¸(ğ‘ğ¿)=ğ›½, therefore ğ‘ğ¿âŠ­ğ‘¤ğœ™. On the other hand îˆ­ is working as in the previous case on the same data: it will only visit at most M elements on ğœˆâ€², so it will stop on "yes", contradicting the hypothesis that it is correct. â—»

Remark 4
Note that we have proven something stronger than undecidability: undecidability is related to Turing machines, while we have proven that with the available information no finite method can decide the problem, that is, we have problem the unrealizability of the problem [4, 22].

The undecidability result depends on having ğœˆ[ğ‘˜](ğ‘)>0 for all k. If for some m we have ğœˆ[ğ‘š](ğ‘)=0, then for all ğ‘˜â‰¥ğ‘š we have îˆ¸(ğ‘ğ‘˜)=âˆ’âˆ independently of the values ğœˆ[ğ‘˜](ğ‘) for ğ‘˜>ğ‘š. This entails that, in the previous example, in order to ascertain whether ğ‘ğ¿âŠ¨ğ‘¤ğœ™ we only have to check strings ğ‘ğ‘˜ for ğ‘˜<ğ‘š and the problem is therefore decidable.

In terms of the Cartesian graph, ğ‘ğ¿ corresponds to a path ğœ‹ğ¿ and decidability depends on the fact that in order to check matching we only have to extend the path up to m: after that the value of the objective in all paths that extend ğœ‹ğ¿ is âˆ’âˆ.

The presence of zero-valued observations, even an infinite number of them, does not always guarantee decidability.

Example VI
Let ğœ™â‰¡(ğ‘ğ‘)âˆ—, ğ‘,ğ‘âˆˆÎ£, and |Î£|>2. Suppose ğœˆ[ğ‘˜](ğ‘)=0 for k odd and ğœˆ[ğ‘˜](ğ‘)=0 for k even. Then îˆ¸((ğ‘ğ‘)ğ‘›)>âˆ’âˆ for all n, and the same considerations of the theorem apply considering sequences of groups ab (for which ğœˆ[2ğ‘˜](ğ‘)â‹…ğœˆ[2ğ‘˜+1](ğ‘)>0) in lieu of the symbol a of the demonstration.

However, if the values of k and a for which ğœˆ[ğ‘˜](ğ‘)=0 are in sufficient number and randomly distributed, then the problem is almost always decidable.

Theorem 5
Suppose that for each ğ‘âˆˆÎ£, ğœˆ[ğ‘˜](ğ‘)=0 infinitely often. Then with probability 1 STREAM-WEAK can be decided in finite time.

Before we prove the theorem, we need some additional constructions and results. For each ğ‘âˆˆÎ£, define the list

[ğ‘]=[ğ‘˜ğ‘1,ğ‘˜ğ‘2,â€¦]
(55)
with ğ‘˜ğ‘ğ‘–<ğ‘˜ğ‘ğ‘–+1 and for each ğ‘–âˆˆâ„•, ğœˆ[ğ‘˜ğ‘ğ‘–](ğ‘)=0. That is, [a] is the list of indices of the observations in which a has zero probability of occurrence. Because of our hypothesis, each [a] is an infinite list of finite numbers.

Also, set

[ğ‘]|ğ‘›=[ğ‘˜|ğ‘˜âˆˆ[ğ‘],ğ‘˜>ğ‘›];
(56)
[ğ‘]|ğ‘› is the portion of the list [a] with indices greater than n, that is, the list of indices ğ‘˜>ğ‘› such that ğœˆ[ğ‘˜](ğ‘)=0. From these lists, we build a list Î as follows:

figure a
where uniform(Î£) is a function that picks an element of Î£ at random with uniform distribution. The list Î is a list of indices such that, for each ğ‘˜âˆˆÎ, there is an ğ‘âˆˆÎ£ with ğœˆ[ğ‘˜](ğ‘)=0. The particularity of Î is that we pick the indices i such a way that, for each k, the probability that ğœˆ[ğ‘˜](ğ‘)=0 is uniform over Î£. The construction of the list Î is possible due to the hypothesis that each a has zero probability of observation infinitely often. Note that the list

Î=[ğœ‰1,ğœ‰2,â€¦]
(57)
is also infinite, so it can never be built completely and, consequently, the algorithm never stops. However, in the proof of the theorem we shall only use finite parts of Î so one can imagine a lazy evaluation of the algorithm that only computes the portions that we need for the proof. From the construction of Î, it is easy to see that, for each ğœ‰ and a,

â„™[ğœˆ[ğœ‰ğ‘–](ğ‘)=0]=1|Î£|
(58)
and, consequently,

â„™[ğœˆ[ğœ‰ğ‘–](ğ‘)>0]=|Î£|âˆ’1|Î£|
(59)
Lemma 4
Let ğœ‹ be a path in a Cartesian graph. ğœ”[ğœ‹]âˆˆÎ£âˆ— the string that causes it to be followed, and let îˆ¸(ğœ‹)>âˆ’âˆ. Then, with probability 1, ğœ‹ is finite.

Proof
Suppose that the lemma is not true, then there is ğœ–>0 such that

â„™[ğœ‹ finite ]<1âˆ’ğœ–
(60)
That is, â„™[ğœ‹ infinite ]>ğœ–, or

âˆ€ğ‘›âˆˆâ„•  â„™[|ğœ‹|>ğ‘›]>ğœ–
(61)
Consider the list Î, and the element ğœ‰ğ‘˜, with

ğ‘˜>log1/ğœ–log|Î£||Î£|âˆ’1
(62)
If |ğœ‹|>ğœ‰ğ‘˜ then, for each ğ‘–â‰¤ğ‘˜, if ğœ‹[ğœ‰ğ‘–]=ğ‘ğ‘–, it must be ğœˆ[ğœ‰ğ‘–](ğ‘ğ‘–)>0 (since, by hypothesis, îˆ¸(ğœ”[ğœ‹])>âˆ’âˆ). This event has probability (|Î£|âˆ’1)/|Î£|, therefore

â„™[ğœˆ[ğœ‰1](ğ‘1)>0,â€¦,ğœˆ[ğœ‰ğ‘˜](ğ‘ğ‘˜)>0]=(|Î£|âˆ’1|Î£|)ğ‘˜<ğœ–
(63)
which contradicts (61). â—»

Proof of Theorem 5
Let ğœ”âŠ¨ğ›½ğœ™; it is ğœ”âŠ­ğ‘¤ğœ™ if and only if there is ğœ”â€² such that ğœ”ğœ”â€²âŠ¨ğ›½â€²ğœ™ with ğ›½â€²>ğ›½. The string ğœ”ğœ”â€² corresponds to a path ğœ‹ that, because of lemma 4, with probability 1 is finite, so the hypothesis ğœ”âŠ¨ğ‘¤ğœ™ can be checked in finite time with respect to ğœ”â€². With probability 1, there is a finite number of such finite paths, so the hypothesis can be checked, with probability 1, in finite time. â—»

Conclusions
In this paper we have considered the problem of detecting whether a string (or part of it) matches a regular expression when the symbols that we observe are subject to uncertainty. The main contributions of this paper are two: on the one hand, we consider the problem of matching the most likely substring of the input, a problem of considerable interest in applications, as the duration of the event that one want to detect may be unknown, and different events of interest may have overlapping structures. We have seen that considering sub-strings produces a bias towards shorter strings, a bias that can be compensated by minimizing the residual informationâ€”the information carried by the string that matching does not recover. On the other hand, we show that optimal detection in an infinite stream is undecidable, but becomes decidable with probability one under hypotheses often met in practical applications.

The regular expressions that we are presenting here are quite limited. In particular, they do not allow an efficient definition of counting (expressions like ğ‘[ğ‘›,ğ‘š], which is matched if the string contains between n and m symbols a). In principle, regular expressions do allow counting, as the previous expression is equivalent to

ğ‘â‹¯ğ‘âğ‘›(ğœ–+ğ‘+ğ‘ğ‘+â‹¯+ğ‘â‹¯ğ‘âğ‘šâˆ’ğ‘›)
(64)
but the implementation of such an expression is so inefficient as to make it impractical in all but the most trivial cases. One possibility to introduce counting as as part of a more general algebra (e.g., a query algebra) of which matching is part. In the example above, the query would be translated into a query with ğ‘âˆ— as a regular expression plus a condition on the result to ensure that the number of as is the desired. It is not an optimal solution, and the efficient integration of better solutions in the framework presented here is still an open problem.

