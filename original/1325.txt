One hope when using non-elitism in evolutionary computation is that the ability to abandon the current-best solution aids leaving local optima. To improve our understanding of this mechanism, we perform a rigorous runtime analysis of a basic non-elitist evolutionary algorithm (EA), the (𝜇,𝜆) EA, on the most basic benchmark function with a local optimum, the jump function. We prove that for all reasonable values of the parameters and the problem, the expected runtime of the (𝜇,𝜆) EA is, apart from lower order terms, at least as large as the expected runtime of its elitist counterpart, the (𝜇+𝜆) EA (for which we conduct the first runtime analysis on jump functions to allow this comparison). Consequently, the ability of the (𝜇,𝜆) EA to leave local optima to inferior solutions does not lead to a runtime advantage. We complement this lower bound with an upper bound that, for broad ranges of the parameters, is identical to our lower bound apart from lower order terms. This is the first runtime result for a non-elitist algorithm on a multi-modal problem that is tight apart from lower order terms.

Access provided by University of Auckland Library

Introduction
The mathematical runtime analysis of evolutionary algorithms (EAs) and other randomized search heuristics is a young but established subfield of the general research area of heuristic search [2, 23, 44, 58]. This field, naturally, has started with regarding the performance of simple algorithms on simple test problems: The problems usually were unimodal, that is, without local optima different from the global optimum, the algorithms were elitist and often had trivial populations, and the runtime guarantees only estimated the asymptotic order of magnitude, that is, gave 𝑂(⋅) upper bounds or Ω(⋅) lower bounds.

Despite this restricted scope, many fundamental results have been obtained and our understanding of the working principles of EAs has significantly increased with these works. In this work, we go a step further with a tight (apart from lower order terms) analysis of how a non-elitist evolutionary algorithm with both non-trivial parent and offspring populations optimizes a multimodal problem.

In contrast to the practical use of evolutionary algorithms, where non-elitism is often employed, the mathematical analysis of evolutionary algorithms so far could find only little evidence for the use of non-elitism. The few existing works, very roughly speaking (see Sect. 2.1 for more details) indicate that when the selection pressure is large, then the non-elitist algorithm simulates an elitist one, and when the selection pressure is low, then no function with unique global optimum can be optimized efficiently. The gap between these two regimes is typically very small. Consequently, a possible profit from non-elitism would require a careful parameter choice.

One often named advantage of non-elitist algorithms is their ability to leave local optima to inferior solutions, which can reduce the time spent uselessly in local optima. To obtain a rigorous view on this possible advantage, we analyze the performance of the well-known (𝜇,𝜆) EA (see Sect. 3.1 for a precise definition) on the multimodal jump function benchmark [15]. We note that, apart from some sporadic results on custom-tailored example problems and the corollaries from very general results (see Theorems 1 and 2 further below), this is the first in-depth study of how a non-elitist algorithm optimizes a classic benchmark with local optima.

Our main result (see Sect. 5) is that in this setting, the small middle range between a too small and a too high selection pressure, which could be envisaged from previous works, does not exist. Rather, the two undesired regimes overlap significantly. We note that for the (𝜇,𝜆) EA, the selection pressure is reasonably well described by the ratio of the offspring population size 𝜆 to the parent population size 𝜇. The selection pressure is low if 𝜆≤(1−𝜀)𝑒𝜇 for some constant 𝜀>0. In this case, the (𝜇,𝜆) EA needs an exponential time to optimize any function 𝑓:{0,1}𝑛→ℝ with at most a polynomial number of global optima [50]. The selection pressure is high if 𝜆≥(1+𝜀)𝑒𝜇 for some constant 𝜀>0 and if 𝜆 is at least logarithmic in the envisaged runtime. In this case, the (𝜇,𝜆) EA can optimize many classic benchmark functions in a runtime at most a constant factor slower than, say, the (𝜇+𝜆) EA, see [51].

Our main result implies (Corollary 9) that already when 𝜆≥2𝜇, 𝜆 is super-constant, and 𝜆=𝑜(𝑛𝑘−1), the runtime of the (𝜇,𝜆) EA on all jump functions with jump size 𝑘≤𝑛1−𝜀 is at least the runtime of the (𝜇+𝜆) EA (apart from lower order terms); to prove this statement, we also conduct the first so far and sufficiently tight runtime analysis of the (𝜇+𝜆) EA on jump functions (Theorem 3). Consequently, the two regimes of a too low selection pressure and of no advantage over the elitist algorithm overlap in the range 𝜆∈[2𝜇,(1−𝜀)𝑒𝜇], leaving for jump functions no space for a middle regime with runtime advantages from non-elitism. We note that our result, while natural, is not obvious. In particular, as a comparison of the (1, 1) EA and the (1+1) EA on highly deceptive functions shows (see Sect. 2.1.3 for more details), it is not always true that the elitist algorithm is at least as good as its non-elitist counterpart.

Our result does not generally disrecommend to use non-elitism, in particular, it does not say anything about possible other advantages from using non-elitism. Our result, however, does indicate that the ability to leave local optima to inferior solutions is non-trivial to turn into a runtime advantage (whereas at the same time, as observed in previous works, there is a significant risk that the selection pressure is too low to admit any reasonable progress).

We also prove an upper bound for the runtime of the (𝜇,𝜆) EA on jump functions (Theorem 11), which shows that our lower bound for large ranges of the parameters (but, of course, only for 𝜆≥(1+𝜀)𝑒𝜇) is tight including the leading constant. This appears to be the first preciseFootnote1 on runtime result for a non-trivial non-elitist algorithm on a non-trivial problem.

From the technical perspective, it is noteworthy that we obtain precise bounds in settings where the previously used methods (negative drift for lower bounds, level-based analyses of non-elitist population processes for upper bounds) could not give precise analyses, and in the case of negative drift could usually not even determine the right asymptotic order of the runtime. We are optimistic that our methods will be profitable for other runtime analyses as well.

State of the Art and Our Results
This work progresses the state of the art in three directions with active research in the recent past, namely non-elitist evolutionary algorithms, precise runtime analyses, and methods to prove lower bounds in the presence of negative drift and upper bounds for non-elitist population processes. We now describe these previous states of the art and detail what is the particular progress made in this work. We concentrate ourselves on classic evolutionary algorithms (also called genetic algorithms) for the optimization in discrete search spaces. We note that non-elitism has been used in other randomized search heuristics such as the Metropolis algorithm [48, 74], simulated annealing [43, 68], strong-selection-weak-mutation (SSWM) [59, 63], and memetic algorithms [57]. Letting the selection decisions not only depend on the fitness, e.g., in tabu search or when using fitness sharing, also introduces some form of non-elitism. From a broader perspective, also many probabilistic model building algorithms such as ant colony optimizers or estimation-of-distribution algorithms can be seen as non-elitist, since they often allow moves to inferior models. From an even broader point of view, even restart strategies can be seen as a form of non-elitism. While all these research directions are interesting, it seems to us that the results obtained there, to the extent that we understand them, are not too closely related to our results and therefore not really comparable.

Non-Elitist Algorithms
While non-elitist evolutionary algorithms are used a lot in practice, the mathematical theory of EAs so far was not very successful in providing convincing evidences for the usefulness of non-elitism. This might be due to the fact that rigorous research on non-elitist algorithms has started only relatively late, caused among others by the fact that many non-elitist algorithms require non-trivial populations, which form another challenge for mathematical analyses. Another reason, naturally, could be that non-elitism is not as profitable as generally thought. Our work rather points into the latter direction.

The previous works on non-elitist algorithms can roughly be grouped as follows.

Exponential Runtimes When the Selection Pressure is Low
By definition, non-elitist algorithms may lose good solutions. When this happens too frequently (low selection pressure), then the EA finds it hard to converge to good solutions, resulting in a poor performance

The first to make this empirical observation mathematically precise in a very general manner was Lehre in his remarkable work [50]. For a broad class of non-elitist population-based EAs, he gives conditions on the parameters that imply that the EA cannot optimize any pseudo-Boolean function 𝑓:{0,1}𝑛→ℝ with at most a polynomial number of optima in time sub-exponential in n. Due to their general nature, we have to restrict ourselves here to what Lehre’s results imply for the (𝜇,𝜆) EA, but we note that analogous results hold for a much wider class of algorithms. For the (𝜇,𝜆) EA using the usual mutation rate 1𝑛, Lehre shows that when 𝜆≤(1−𝜀)𝑒𝜇, where 𝜀>0 is any positive constant, then the time to find a (global) optimum of any pseudo-Boolean function with at most a polynomial number of optima is exponential in n with high probability.

We note that more specific results showing the danger of a too low selection pressure have appeared earlier. For example, already in 2007 Jägersküpper and Storch [46, Theorem 1] showed that the (1,𝜆) EA with 𝜆≤114ln(𝑛) is inefficient on any pseudo-Boolean function with a unique optimum. The range of 𝜆 for which such a negative performance is observed was later extended to the asymptotically tight value 𝜆≤(1−𝜀)log𝑒𝑒−1𝑛 by Rowe and Sudholt [65]. Happ et al. [39] showed that two simple (1+1)-type hillclimbers using fitness proportionate selection in the choice of the surviving individual are not efficient on any linear function with positive weights. Neumann et al. [56] showed that a mutation-only variant of the Simple Genetic Algorithm with fitness proportionate selection is inefficient on the ONEMAX function when the population size 𝜇 is at most polynomial, and it is inefficient on any pseudo-Boolean function with unique global optimum when 𝜇≤14ln(𝑛). Oliveto and Witt [62] showed that the true Simple Genetic Algorithm (using crossover) cannot optimize ONEMAX efficiently when 𝜇≤𝑛14−𝜀.

We note that the methods in [50] were also used to prove lower bounds for particular objective functions. The following result was given for a variant of jump functions [50, Theorem 5]. To be precise, a similar result was proven for a tournament-selection algorithm and it was stated that an analogous statement, which we believe to be the following, holds for the (𝜇,𝜆) EA as well. As a reviewer pointed out, it is not immediately clear how the proof in [50] extends to the classic definition of jump functions.

Theorem 1
(cf. Lehre [50]). Let 𝑛∈ℤ≥1, 𝜀>0 a constant, 𝑘≤(0.2−𝜀)𝑛, and 𝑘=𝜔(log𝑛). Let 𝑓𝑘:{0,1}𝑛→ℝ be defined by 𝑓𝑘(𝑥)=ONEMAX(𝑥) when ONEMAX(𝑥)∉[𝑛−𝑘+1..𝑛−1] and 𝑓𝑘(𝑥)=0 otherwise. Then the expected runtime of the (𝜇,𝜆) EA with polynomial 𝜆 on JUMP𝑛𝑘 is at least exp(Ω(𝑘)), where all asymptotics is for n tending to infinity.

Pseudo-Elitism When the Selection Pressure is High
When a non-elitist algorithm has the property that, despite the theoretical danger of losing good solutions, it very rarely does so, then its optimization behavior becomes very similar to the one of an elitist algorithm. Again the first to make this effect precise for a broad class of algorithms was Lehre in his first paper on level-based arguments for non-elitist populations [51].

Lehre’s fitness-level theorem for non-elitist population-based algorithms assumes that the search space can be partitioned into levels such that (i) the algorithm has a reasonable chance to sample a solution in some level j or higher once a constant fraction of the population is at least on level 𝑗−1 (“base level”) and (ii) there is an exponential growth of the number of individuals on levels higher than the base level; more precisely (but still simplified), if there are 𝜇0<𝛾0𝜇 individuals above the base level, then in the next generation the number of individuals above the base level follows a binomial distribution with parameters 𝜇 and 𝑝=(1+𝛿)𝜇0𝜇, where 𝛾0 and 𝛿 are suitable parameters. If in addition the population sizes involved are large enough, then (again very roughly speaking) the runtime of the algorithm is at most a constant factor larger than the runtime guarantee which could be obtained for an elitist analogue of the non-elitist EA. From the assumptions made here, it cannot be excluded that the non-elitist EA loses a best-so-far solution; however, due to the exponential growth of condition (ii) and the sufficiently large population size, this can only happen if there are few individuals above the base level. Hence the assumptions of the level-based method, roughly speaking, impose that that the EA behaves like an elitist algorithm except when it just has found a new best solution. In this case, with positive probability (usually at most some constant less than one) the new solution is lost. This constant probability for losing a new best individual (and the resulting need to re-generate one) may lead to a constant-factor loss in the runtime, but not more. Very roughly speaking, one can say that such non-elitist EAs, while formally non-elitist algorithms, do nothing else than a slightly slowed-down emulation of an elitist algorithm. That said, it has to be remarked that both proving level-based theorems (see [9, 17, 18, 51]) and applying them (see also [21]) is technical and much less trivial than what the rule of thumb “high selection pressure imitates elitism” suggests.

For the optimization of jump functions via the (𝜇,𝜆) EA, the work [9] implies the following result. We note that it was shown only to the variant of jump functions regarded in Theorem 1 (where the fitness in the gap region is uniformly zero), but from the proofs it is clear that the result also holds for the standard definition [15] used in this work.

Theorem 2
([9]). Let 𝑘∈[1..𝑛]. Let 𝜀>0 be a constant and let c be a sufficiently large constant (depending on 𝜀). Let 𝜆≥𝑐𝑘ln(𝑛) and 𝜇≤𝜆(1+𝜀)𝑒. Then runtime T of the (𝜇,𝜆) EA on JUMP𝑛𝑘 satisfies

𝐸[𝑇]=𝑂(𝑛𝑘+𝑛𝜆+𝜆log𝜆).
For the particular case of the (1,𝜆) EA and the (1+𝜆) EA, Jägersküpper and Storch in an earlier work also gave fitness-level theorems [46, Lemma 6 and 7]. They also showed that both algorithms essentially behave identical for t iterations when 𝜆 is at least logarithmic in t [46, Theorem 4]. This effect, without quantifying 𝜆 and without proof, was already proposed in [45, p. 415]. Jägersküpper and Storch show in particular that when 𝜆≥3ln𝑛, then the (1,𝜆) EA optimizes ONEMAX in asymptotically the same time as the (1+𝜆) EA. The actual runtimes given for the (1,𝜆) EA in [46] are not tight since at that time a tight analysis of the (1+𝜆) EA on ONEMAX was still missing; however, it is clear that the arguments given in [46] also allow to transfer the tight upper bound of 𝑂(𝑛log𝑛+𝜆𝑛loglog𝑛log𝑛) for the (1+𝜆) EA from [16] to the (1,𝜆) EA.

The minimum value of 𝜆 that ensures an efficient optimization of the (1,𝜆) EA on ONEMAX was lowered to the asymptotically tight value of 𝜆≥log𝑒𝑒−1𝑛≈2.18ln𝑛 in [65]. Again, only the upper bound of 𝑂(𝑛log𝑛+𝑛𝜆) was shown. We would not be surprised if with similar arguments also a bound of 𝑂(𝑛log𝑛+𝜆𝑛loglog𝑛log𝑛) could be shown, but this is less obvious here than for the result of [46].

For the benchmark function LEADINGONES, the threshold between a superpolynomial runtime of the (1,𝜆) EA and a runtime asymptotically equal to the one of the (1+𝜆) EA was shown to be at 𝜆=(1±𝜀)2log𝑒𝑒−1𝑛 [65].

Examples Where Non-Elitism is Helpful
The dichotomy described in the previous two subsections suggests that it is not easy to find examples where non-elitism is useful. This is indeed true apart from two exceptions.

Jägersküpper and Storch [46] constructed an artificial example function that is easier to optimize for the (1,𝜆) EA than for the (1+𝜆) EA. The CLIFF function CLIFF:{0,1}𝑛→ℕ is defined by CLIFF(𝑥)=OM(𝑥) if OM(𝑥)<𝑛−⌊𝑛/3⌋ and CLIFF(𝑥)=OM(𝑥)−⌊𝑛/3⌋ otherwise. Jägersküpper and Storch showed that the (1,𝜆) EA with 𝜆≥5ln𝑛 optimizes CLIFF in an expected number of 𝑂(exp(5𝜆)) fitness evaluations, whereas the (1+𝜆) EA with high probability needs at least 𝑛𝑛/4 fitness evaluations. While this runtime difference is enormous, it has to be noted that even for the best value of 𝜆=5ln𝑛, the runtime guarantee for the (1,𝜆) EA is only 𝑂(𝑛25). Also, we remark that the local optimum of the CLIFF function has a particular structure which helps to leave the local optimum: Each point on the local optimum has ⌊𝑛/3⌋ neighbors from which it is easy to hill-climb to the global optimum (as long as one does not use a steepest ascent strategy). Also, for each point on the local optimum there are Ω(𝑛2) search points in Hamming distance two from which any local search within less than n/3 improvements finds the global optimum. This is a notable difference to the JUMP𝑛𝑘 function, where hill-climbing from any point of the search space that is not the global optimum or one of its n neighbors surely leads to the local optimum. We would suspect that such larger radii of attraction are closer to the structure of difficult real-world problems, but we leave it to the reader to decide which model is most relevant for their applications.

We note that a second, albeit extreme and rather academic, example for an advantage of non-elitism is implicit in the early work [36] by Garnier, Kallel, and Schoenauer. They show that the (1, 1) EA on any function 𝑓:{0,1}𝑛→ℝ with unique global optimum has an expected optimization time of (1+𝑜(1))𝑒𝑒−12𝑛; this follows from Proposition 3.1 in their work. When taking a highly deceptive function like the trap function, this runtime is significantly better than the ones of elitist algorithms, which typically are 𝑛Θ(𝑛). Of course, all this is not overly surprising – the (1, 1) EA uses no form of selection and hence just performs a random walk in the search space (where the one-step distribution is given by the mutation operator). Therefore, this algorithm does not suffer from the deceptiveness of the trap function as do elitist algorithms. Also, a runtime reduction from 𝑛Θ(𝑛) to exp(Θ(𝑛)) clearly is not breathtaking. Nevertheless, this is a second example where a (𝜇,𝜆) EA significantly outperforms the corresponding (𝜇+𝜆) EA.

Since in this work we are only interested in how non-elitism (and more specifically, comma selection) helps to leave local optima and by this improve runtimes, we do not discuss in detail other motivations for employing non-elitist algorithms. We note briefly, though, that comma selection is usually employed in self-adaptive algorithms. Self-adaptation means that some algorithm parameters are stored as part of the genome of the individuals and are subject to variation together with the original individual. The hope is that this constitutes a generic way to adjust algorithm parameters. When using plus selection together with self-adaptation, there would be the risk that the population at some point only contains individuals with unsuitable parameter values. Now variation will only generate inferior offspring. These will not be accepted and, consequently, the parameter values encoded in the genome of the individuals cannot be changed. When using comma selection, it is possible to accept individuals with inferior fitness, and these may have superior parameter values. We are not aware of a rigorous demonstration of this effect, but we note that the two runtime analysis papers [19, 30] on self-adaptation both use comma selection. We further note that comma selection is very common in continuous optimization, in particular, in evolution strategies, but since it is generally difficult to use insights from continuous optimization in discrete optimization and vice-versa we do not discuss results from continuous optimization here.

Our Contribution
In Sect. 5, we show that for all interesting values of the parameters of the problem and the algorithm, the expected runtime of the (𝜇,𝜆) EA on jump functions is, apart from possibly lower order terms, at least the expected runtime of the (𝜇+𝜆) EA. This shows that for this problem, there can be no significant advantage of using comma selection.

Our upper bound in Theorem 11, provided mostly to show that our analysis is tight including the leading constant, improves Theorem 2 by making the leading constant precise and being applicable for all offspring population sizes 𝜆≥𝐶ln(𝑛), C a constant independent of the jump size k. To the best of our knowledge, this is the first time that the runtime of a non-elitist algorithm was proven with this precision.

Precise Runtime Analyses
Traditionally, algorithm analysis aims at gaining a rough understanding how the runtime of an algorithm depends on the problem size. As such, most results only show statements on the asymptotic order of magnitude of the runtime, that is, results in big-Oh notation. For classic algorithmics, this is justified among others by the fact that the predominant performance measure, the number of elementary operations, already ignores constant factor differences in the execution times of the elementary operations.

In evolutionary computation, where the classic performance measure is the number of fitness evaluations, this excuse for ignoring constant factors is not valid, and in fact, in the last few years more and more precise runtime results have appeared, that is, results which determine the runtime asymptotically precise apart from lower order terms. Such results are useful, obviously because constant factors matter in practice, but also because many effects are visible only at constant-factor scales. For example, it was shown in [14] that all Θ(1𝑛) mutation rates lead to a Θ(𝑛log𝑛) runtime of the (1+1) EA on all pseudo-Boolean linear functions, but only Witt’s seminal result [70] that the runtime is (1+𝑜(1))𝑒𝑐𝑐𝑛ln𝑛 for the mutation rate 𝑐𝑛, 𝑐>0 a constant, allows to derive that 1𝑛 is the asymptotically best mutation rate.

Overall, not too many non-trivial precise runtime results are known. In a very early work [36], it was shown that the (1+1) EA with mutation rate 𝑐𝑛 optimizes the ONEMAX function in an expected time of (1+𝑜(1))𝑒𝑐𝑐𝑛ln𝑛 and the NEEDLE function in time (1+𝑜(1))11−𝑒𝑐2𝑛. More than ten years later, in independent works [8, 67] the precise runtime of the (1+1) EA on LEADINGONES was determined; here [8] also regarded general mutation rates and deduced from their result that the optimal mutation rate of approximately 1.59𝑛 is higher than the usual recommendation 1𝑛, and that a fitness dependent mutation rate gives again slightly better results (this was also the first time that a fitness dependent parameter choice was proven to be superior to static rates by at least a constant factor difference in the runtime). Precise runtime results for a broader class of (1+1)-type algorithms on LEADINGONES have recently appeared in [24]. A series of recent works [7, 22, 53] obtained precise runtimes of different hyper-heuristics on LEADINGONES and thus allowed to discriminate them by their runtime. The precise expected runtime of the (1+1) EA with general unbiased mutation operator on the PLATEAU𝑘 function was determined [3] to be (1+𝑜(1))(𝑛𝑘)𝑝−11:𝑘, where 𝑝1:𝑘 is the probability that the mutation operator flips between one and k bits. Apparently, here the details of the mutation operator are not very important – only the probability to flip between one and k bits has an influence on the runtime.

The only precise runtime analysis for an algorithm with a non-trivial population can be found in [37], where the runtime of the (1+𝜆) EA with mutation rate 𝑐𝑛, c a constant, on ONEMAX was shown to be (1+𝑜(1))(𝑒𝑐𝑐𝑛ln𝑛+𝑛𝜆lnln𝜆2ln𝜆). This result has the surprising implication that here the mutation rate is only important when 𝜆 is small.

The only precise runtime analysis for a multi-modal objective function was conducted in [20], where the runtime of the (1+1) EA with arbitrary mutation rate was determined for jump functions; this work led to the development of a heavy-tailed mutation operator that appears to be very successful [1, 32, 33, 35, 72].

In summary, there is only a small number of precise runtime analyses, but many of them could obtain insights that would not have been possible with less precise analyses.

Our result, an analysis of the (𝜇,𝜆) EA on jump functions that is precise for 𝑘≤0.1𝑛, 𝜆=𝑜(𝑛𝑘−1), 𝜆≥(1+𝜀)𝑒𝜇, and 𝜆=Ω(log𝑛) sufficiently large, is the second precise analysis for a population-based algorithm (after [37]), is the second precise analysis for a multimodal fitness function (after [20]), and is the first precise analysis for a non-elitist algorithm (apart from fact that the result [37] could be transfered to the (1,𝜆) EA for large 𝜆 via the argument [46] that in this case the (1+𝜆) EA and the (1,𝜆) EA have essentially identical performances).

Methods: Negative Drift and Level-based Analyses
To obtain our results, we also develop new techniques for two classic topics, namely the analysis of processes showing a drift away from the target (“negative drift”) and the analysis of non-elitist population processes via level-based arguments.

Negative Drift
It is natural that a stochastic process 𝑋0,𝑋1,… finds it hard to reach a certain target when the typical behavior is taking the process away from the target. Negative drift theorems are an established tool for the analysis of such situations. They roughly speaking state the following. Assume that the process starts at some point b or higher, that is, 𝑋0≥𝑏, and that we aim at reaching a target 𝑎<𝑏. Assume that whenever the process is above the target value, that is, 𝑋𝑡>𝑎, we have an expected progress 𝐸[𝑋𝑡+1−𝑋𝑡]≥𝛿, 𝛿 some constant, away from the target, and that this progress satisfies some concentration assumption like two-sided exponential tails. Then the expected time to reach or undershoot the target is at least exponential in the distance 𝑏−𝑎.

The first such result in the context of evolutionary algorithms was shown by Oliveto and Witt [60] (note the corrigendum [61]). Improved versions were subsequently given in [49, 55, 62, 65, 71]. The comprehensive survey [52, Section 2.4.3] gives a complete coverage of this topic. What is important to note for our purposes is that (i) all existing negative drift results are quite technical to use due to the concentration assumptions, that (ii) they all give a lower bound that is only exponential in the length of the interval in which the (constant) negative drift is observed, and that (iii) they all (apart from the technical work of Hajek [38]) do not give tight bounds, but only bounds of type exp(Ω(𝑏−𝑎)) with the implicit constant in the exponent not specified.

Earlier than the general negative drift theorem, Lehre [50] proved a negative drift theorem for population-based processes via multi-type branching processes. Just as the general negative drift theorems described above, it only gives lower bounds exponential in the length of the negative drift regime and the base of the exponential function is not made explicit. Consequently, in [50, Theorem 5] (Theorem 1 in this work), only an exp(Ω(𝑘)) lower bound for the runtime of the (𝜇,𝜆) EA on JUMP𝑛𝑘 was derived

Since we aim at an Ω(𝑛𝑘) lower bound caused by a negative drift in the short gap region (of length k) of the jump function, and since further we aim at results that give the precise leading constant of the runtime, we cannot use these tools. We therefore resort to the additive drift applied to a rescaled process argument first made explicit in [6]. The basic idea is very simple: For a suitable function 𝑔:ℝ→ℝ one regards the process (𝑔(𝑋𝑡))𝑡 instead of the original process (𝑋𝑡)𝑡, shows that it makes at most a slow progress towards the target, say 𝐸[𝑔(𝑋𝑡+1)−𝑔(𝑋𝑡)∣𝑋𝑡>𝑎]≥−𝛿, and concludes from the classic additive drift theorem [42] (Theorem 6 in this work) that the expected time to reach or undershoot a when starting at b is at least 𝑔(𝑏)−𝑔(𝑎)𝛿. While the basic approach is simple and natural, the non-trivial part is finding a rescaling function g which both gives at most a slow progress towards the target and gives a large difference 𝑔(𝑏)−𝑔(𝑎). The rescalings used in [6] and [25] were both of exponential type, that is, g was roughly speaking an exponential function. By construction, they only led to lower bounds exponential in 𝑏−𝑎, and in both cases the lower bound was not tight (apart from being exponential).

Our progress: Hence the technical novelty of this work is that we devise a rescaling for our problem that (i) leads to a lower bound of order 𝑛𝑘 for a process having negative drift only is an interval of length k, and (ii) such that these lower bounds are tight including the leading constant. Clearly, our rescalings (as all rescalings used previously) are specific to our problem. Nevertheless, they demonstrate that the rescaling method, different from the classic negative drift theorems, can give very tight lower bounds and lower bounds that are super-exponential in the length of the interval in which the negative drift is observed. We are optimistic that such rescalings will find other applications in the future.

Note added in proof: For reasons of completeness of this discussion on lower bound methods, we note that between the submission of this work in May 2020 and the first notification in June 2021, a further lower bound method called negative multiplicative drift was proposed [28]. Different from what a reviewer suggests, it is not in any way related to the rescaling method. While we do not want to rule out that it can also be employed to prove our lower bound, it is clear that this would either also need a rescaling of the process (and then our approach appears more direct) or it would need estimates on the change of the maximum ONEMAX-value in the population that are substantially different from ours in the proof of Theorem 8.

Level-Based Analyses
While level-based arguments for the analysis of non-elitist algorithms have been used much earlier, see, e.g., [31], the fitness-level analysis of Lehre [51] might still be the first general method to analyze non-elitist population-based processes. We gave a high-level description of this method in Sect. 2.1.2 and we will give a more detailed discussion in Sect. 6.1 to enable us to prove our upper bound. For this reason, we now explain without further explanations what is our progress over the state of the art of this method.

Similar to the state of the art in negative drift theorems, all existing variants of the level-based methods do not give results that are tight including the leading constant. Also, from the complexity of the proofs of these results, it appears unlikely that such tight results can be obtained in the near future.

Our progress: For our problem of optimizing jump functions, we can exploit the fact that the most difficult, and thus time consuming, step is generating the global optimum from a population that has fully converged into the local optimum. To do so, we use the non-tight level-based methods only up to the point when the population only consists of local optima (we call this an almost perfect population). This can be done via a variation of the existing level-based results (Corollary 13). From that point on, we estimate the remaining runtime by computing the waiting time for generating the optimum from a local optimum. Of course, since we are analyzing a non-elitist process, we are not guaranteed to keep an almost perfect population. For that reason, we also need to analyze the probability of losing an almost perfect population and to set up a restart argument to regain an almost perfect population. Naturally, this has to be done in a way that the total runtime spent here is only a lower-order fraction of the time needed to generate the global optimum from an almost perfect population.

A side effect of this approach is that we only need a logarithmic offspring population size, that is, it suffices to have 𝜆≥𝐶ln(𝑛) for some constant C that is independent of the jump size k. This is different from using the level-based methods for the whole process, as done in the proof of Theorem 2, which would require an offspring population size at least logarithmic in the envisaged runtime, hence here Ω(log𝑛𝑘)=𝑂(𝑘log𝑛), which is super-logarithmic when k is super-constant.

While our arguments exploit some characteristics of the jump functions, we are optimistic that they can be employed for other problems as well, in particular, when the optimization process typically contains one step that is more difficult than the remaining optimization.

Preliminaries
In this section, we define the algorithm and the optimization problem regarded in this paper together with the most relevant works on these.

The (𝜇,𝜆) EA
The (𝜇,𝜆) EA for the maximization of pseudo-Boolean functions 𝑓:{0,1}𝑛→ℝ is made precise in Algorithm 1. It is a simple non-elitist algorithm working with a parent population of size 𝜇 and an offspring population of size 𝜆≥𝜇. Here and in the remainder by a population we mean a multiset of individuals (elements from the search space {0,1}𝑛). Each offspring is generated by selecting a random parent (independently and with replacement) from the parent population and mutating it via standard bit mutation, that is, by flipping each bit independently with probability 1/n.Footnote2 The next parent population consists of those 𝜇 offspring which have the highest fitness (breaking ties arbitrarily).

figure a
The (𝜇+𝜆) EA, to which we compare the (𝜇,𝜆) EA, differs from the (𝜇,𝜆) EA only in the selection of the next generation. Whereas the (𝜇,𝜆) EA selects the next generation only from the offspring population (comma selection), the (𝜇+𝜆) EA selects it from the parent and offspring population (plus selection). In other words, to obtain the (𝜇+𝜆) EA from Algorithm 1, we only have to replace the selection by “select 𝑃𝑡 from the multi-set 𝑃𝑡−1∪{𝑦1,…,𝑦𝜆} by choosing 𝜇 best individuals (breaking ties arbitrarily)”. Often, the tie breaking is done by giving preference to offspring, but for all our purposes there is no difference.

When talking about the performance of the (𝜇,𝜆) EA or the (𝜇+𝜆) EA, as usual in runtime analysis [2, 23, 44, 58], we count the number of fitness evaluations until for the first time an optimal solution is evaluated. We assume that each individual is evaluated immediately after being generated. Consequently, if an optimum is generated in iteration t, then the runtime T satisfies

𝜇+(𝑡−1)𝜆+1≤𝑇≤𝜇+𝑡𝜆.
(1)
Since we described the most important results on the (𝜇,𝜆) EA already in Sect. 2.1, let us briefly mention the most relevant results for the (𝜇+𝜆) EA. Again, due to the difficulties in analyzing population-based algorithms, not too much is known. The runtimes of the (1+𝜆) EA, among others on ONEMAX and LEADINGONES, were first analyzed in [45]. The asymptotically tight runtime on ONEMAX for all polynomial 𝜆 was determined in [16], together with an analysis on general linear functions. In [69], the runtime of the (𝜇+1) EA on ONEMAX and LEADINGONES, among others, was studied. The runtime of the (𝜇+𝜆) EA with both non-trivial parent and offspring population sizes on the ONEMAX function was determined in [4].

The Jump Function Class
To define the jump functions, we first recall that the n-dimensional ONEMAX function is defined by

OM(𝑥)=‖𝑥‖1=∑𝑖=1𝑛𝑥𝑖
for all 𝑥∈{0,1}𝑛

Now the n-dimensional jump function with jump parameter (jump size) 𝑘∈[1..𝑛] is defined by

JUMP𝑛𝑘(𝑥)={‖𝑥‖1+𝑘𝑛−‖𝑥‖1 if ‖𝑥‖1∈[0..𝑛−𝑘]∪{𝑛}, if ‖𝑥‖1∈[𝑛−𝑘+1..𝑛−1].
Hence for 𝑘=1, we have a fitness landscape isomorphic to the one of ONEMAX, but for larger values of k there is a fitness valley (“gap”)

𝐺𝑛𝑘:={𝑥∈{0,1}𝑛∣𝑛−𝑘<‖𝑥‖1<𝑛}
consisting of the 𝑘−1 highest sub-optimal fitness levels of the ONEMAX function. This valley is hard to cross for evolutionary algorithms using standard bit mutation. When using the common mutation rate 1𝑛, the probability to generate the optimum from a parent on the local optimum is only 𝑝𝑘:=(1−1𝑛)𝑛−𝑘𝑛−𝑘<𝑛−𝑘. For this reason, e.g., the classic (𝜇+𝜆) EA has a runtime of at least 𝑛𝑘 when k is not excessively large. This was proven formally for the (1+1) EA in the classic paper [15], but the argument can easily be extended to all (𝜇+𝜆) EAs (as we do now for reasons of completeness). We also prove an upper bound, which will later turn out to agree with our lower bound for the (𝜇,𝜆) EA for large ranges of the parameters.

Theorem 3
Let 𝜇,𝜆∈ℤ≥1. Let 𝑛∈ℤ≥2 and 𝑘∈[2..𝑛]. Let 𝑝𝑘:=(1−1𝑛)𝑛−𝑘𝑛−𝑘. Let T denote the runtime, measured by the number of fitness evaluations until the optimum is found, of the (𝜇+𝜆) EA on the JUMP𝑛𝑘 function.

(i)
Let ℎ(𝑛):=2𝑛log(𝜇𝑛)‾‾‾‾‾‾‾‾‾‾√). If 𝑘≤𝑛2−ℎ(𝑛), then

𝐸[𝑇]≥(1−1𝑛)(𝜇+1𝑝𝑘),
otherwise 𝐸[𝑇]≥(1−1𝑛)(𝜇+1𝑝𝑘′) with 𝑘′:=𝑛2−ℎ(𝑛).

(ii)
𝐸[𝑇]≤1𝑝𝑘+𝑂(𝑛log𝑛+𝑛𝜇+𝑛𝜆log+log+(𝜆/𝜇)log+(𝜆/𝜇)+(𝜇+𝜆)log𝜇), where we write log+𝑥:=max{1,ln𝑥} for all 𝑥>0. If 𝜇≤𝜆, 𝜆=exp(𝑂(𝑛)), and 𝜆=𝑜(1𝑛𝑝𝑘), then 𝐸[𝑇]≤(1+𝑜(1))1𝑝𝑘.

Proof
To cover both cases, let 𝑘′=min{𝑘,𝑛2−ℎ(𝑛)}. Using the additive Chernoff bound (Theorem 5) and a union bound, we see that with probability at least

1−𝜇exp(−(𝑛2−𝑘′)22𝑛)≥1−𝜇exp(−ℎ(𝑛)22𝑛)≥1−1𝑛
all 𝜇 initial individuals x satisfy OM(𝑥)≤𝑛−𝑘′. Conditioning on this, in the remaining run all individuals that are taken into the parent population also satisfy OM(𝑥)≤𝑛−𝑘′ (unless they are the optimum). Consequently, for an offspring to become the first optimum sampled, there is a unique set of ℓ≥𝑘′ bits in the parent that need to be flipped (and the other bits may not be flipped). The probability for this event is (1−1𝑛)𝑛−ℓ(1𝑛)ℓ≤(1−1𝑛)𝑛−𝑘′(1𝑛)𝑘′=𝑝𝑘′. Hence the time until this happens is stochastically dominated (see, e.g., [24]) by a geometric distribution with success probability 𝑝𝑘′, which has an expectation of 1𝑝𝑘′. Together with the 𝜇 initial fitness evaluations, this shows the lower bound.

For the upper bound, we use a recent analysis of the runtime of the (𝜇+𝜆) EA on ONEMAX. In [4], it was shown that the (𝜇+𝜆) EA finds the optimum of ONEMAX in an expected number of

𝑂(𝑛log𝑛+𝑛𝜇+𝑛𝜆log+log+(𝜆/𝜇)log+(𝜆/𝜇))
fitness evaluations (the result is stated in terms of iterations in [4], but with (1) one immediately obtains the form above). It is easy to see from the proof in [4] that this bound also holds for the expected time until the (𝜇+𝜆) EA optimizing any jump function as found an individual on the local optimum (if it has not found the optimum before).

What cannot be taken immediately from the previous work is remainder of the runtime analysis. In particular, since generating the optimum from the local optimum is more difficult than generating an individual on the next ONEMAX fitness level, we need a larger number of individuals on the local optimum before we have a reasonable chance of making progress. Since we mostly aim at a good upper bound in the regime where 𝜇 and 𝜆 are not excessively large, we allow for the time until the whole population is on the local optimum or better. By [66, Lemma 2], this takeover time is 𝑂((𝜇+𝜆)log𝜇) fitness evaluations (or the optimum is found earlier). From this point on, any further individual has a probability of exactly 𝑝𝑘 of being the optimum, giving an additional 1𝑝𝑘 term for the runtime bound. This shows the general upper bound. If 𝜇≤𝜆, 𝜆=exp(𝑂(𝑛)) and 𝜆=𝑜(1𝑛𝑝𝑘), then (𝜇+𝜆)log𝜇=𝑂(𝜆log𝜆)=𝑂(𝑛𝜆)=𝑜(1/𝑝𝑘). For similar reasons, the expressions 𝑛𝜇 and 𝑛𝜆log+log+(𝜆/𝜇)log+(𝜆/𝜇) are of lower order. Since 𝑘≥2, we have 𝑝𝑘=Ω(𝑛2), and thus also the 𝑛log𝑛 expression is of lower order. ◻

By using larger mutation rates or a heavy-tailed mutation operator, the runtime of the (1+1) EA can be improved by a factor of 𝑘Θ(𝑘) [20], but the runtime remains Ω(𝑛𝑘) for k constant.

Asymptotically better runtimes can be achieved when using crossover, though this is not as easy as one might expect. The first work in this direction [47], among other results, showed that a simple (𝜇+1) genetic algorithm using uniform crossover with rate 𝑝𝑐=𝑂(1𝑘𝑛) has an 𝑂(𝜇𝑛2𝑘3+22𝑘𝑝−1𝑐) runtime when the population size is at least 𝜇=Ω(𝑘log𝑛). A shortcoming of this result, as noted by the authors, is that it only applies to uncommonly small crossover rates. Using a different algorithm that first applies crossover and then mutation, a runtime of 𝑂(𝑛𝑘−1log𝑛) was achieved by Dang et al. [13, Theorem 2]. For 𝑘≥3, the logarithmic factor in the runtime can be removed by using a higher mutation rate. With additional diversity mechanisms, the runtime can be further reduced down to 𝑂(𝑛log𝑛+4𝑘), see [12]. The (1+(𝜆,𝜆)) GA can optimize JUMP𝑘 in time 𝑂(𝑛(𝑘+1)/2𝑘−Ω(𝑘)) [5].

With a three-parent majority vote crossover, among other results, a runtime of 𝑂(𝑛log𝑛) could be obtained via a suitable island model for all 𝑘=𝑂(𝑛1/2−𝜀) [34]. A different voting algorithm also giving an 𝑂(𝑛log𝑛) runtime was proposed in [64]. Via a hybrid genetic algorithm using as variation operators only local search and a deterministic voting crossover, an O(n) runtime was shown in [73].

Runtimes of 𝑂(𝑛(𝑛𝑘)) and 𝑂(𝑘log(𝑛)(𝑛𝑘)) were shown for the (1+1) IAhyp and the (1+1) Fast-IA artificial immune systems, respectively [10, 11]. In [54], the runtime of a hyper-heuristic switching between elitist and non-elitist selection was studied. The lower bound of order Ω(𝑛log𝑛)+exp(Ω(𝑘)) and the upper bound of order 𝑂(𝑛2𝑘−1/𝑘), however, are too far apart to indicate an advantage or a disadvantage over most classic algorithms. In this work, it is further stated that the Metropolis algorithm (using the 1-bit neighborhood) has an exp(Ω(𝑛)) runtime on jump functions.

Without diversity mechanisms and non-standard operators, the compact genetic algorithm, a simple estimation-of-distribution algorithm, has a runtime of 𝑂(𝑛log𝑛+2𝑂(𝑘)) [26, 41].

Technical Tools
In this section, we collect a few technical tools that will be used in our proofs. All but the last one, an elementary non-asymptotic lower bound for the probability to generate an offspring with equal ONEMAX fitness, are standard tools in the field.

Let X be a binomially distributed random variable with parameters n and p, that is, 𝑋=∑𝑛𝑖=1𝑋𝑖 with independent 𝑋𝑖 satisfying Pr[𝑋𝑖=1]=𝑝 and Pr[𝑋𝑖=0]=1−𝑝. Since X is a sum of independent binary random variables, Chernoff bounds can be used to bound its deviation from the expectation. However, the following elementary estimate also does a good job. This estimate appears to be well-known (e.g., it was used in [45] without proof or reference). Elementary proofs can be found in [37, Lemma 3] or [29, Lemma 1.10.37].

Lemma 4
Let 𝑋∼Bin(𝑛,𝑝). Let 𝑘∈[0..𝑛]. Then

Pr[𝑋≥𝑘]≤(𝑛𝑘)𝑝𝑘.
The following additive Chernoff bound from Hoeffding [40], also to be found, e.g., in [29, Theorem 1.10.7], provides a different way to estimate the probability that a binomial random variable and, in fact, any sum of bounded independent random variables exceeds its expectation.

Theorem 5
Let 𝑋1,…,𝑋𝑛 be independent random variables taking values in [0, 1]. Let 𝑋=∑𝑛𝑖=1𝑋𝑖. Then for all 𝜆≥0,

Pr[𝑋≥𝐸[𝑋]+𝜆]≤exp(−2𝜆2𝑛).
As part of our additive drift with rescaling lower bound proof strategy, we need the following additive drift theorem of He and Yao [42], see also [52, Theorem 2.3.1], which allows to translate a uniform upper bound on an expected progress into a lower bound on the expected time to reach a target.

Theorem 6
Let 𝑆⊆ℝ≥0 be finite and 0∈𝑆. Let 𝑋0,𝑋1,… be a stochastic process taking values in S. Let 𝛿>0. Let 𝑇=inf{𝑡≥0∣𝑋𝑡=0}. If for all 𝑡≥0 and all 𝑠∈𝑆∖{0} we have 𝐸[𝑋𝑡−𝑋𝑡+1∣𝑋𝑡=𝑠]≤𝛿, then 𝐸[𝑇]≥𝐸[𝑋0]𝛿.

Finally, we shall use occasionally the following lower bound on the probability that standard bit mutation creates from a parent x with OM(𝑥)<𝑛 an offspring with equal ONEMAX-value. The main difference to the usual estimate (1−1𝑛)𝑛=(1−𝑜(1))1𝑒, which is the probability to recreate the parent, is that our lower bound is exactly 1𝑒, which avoids having to deal with asymptotic notation.

Lemma 7
Let 𝑥∈{0,1}𝑛 with 0<OM(𝑥)<𝑛. Let y be obtained from x via standard bit mutation with mutation rate 1𝑛. Then Pr[OM(𝑦)=OM(𝑥)]≥1𝑒.

Proof
Let 𝑘:=OM(𝑥). For y to have this same OM-value, it suffices that either no bit in x is flipped or that exactly one zero-bit and exactly one one-bit are flipped. The probability for this event is (1−1𝑛)𝑛+𝑘(𝑛−𝑘)𝑛2(1−1𝑛)𝑛−2≥(1−1𝑛)𝑛+𝑛−1𝑛2(1−1𝑛)𝑛−2=(1−1𝑛)𝑛−1≥1𝑒. ◻

A Lower Bound for the Runtime of the (𝜇,𝜆) EA on Jump Functions
In this section, we prove our main result, a lower bound for the runtime of the (𝜇,𝜆) EA on jump functions which shows that for a large range of parameter values, the (𝜇,𝜆) EA cannot even gain a constant factor speed-up over the (𝜇+𝜆) EA. With its Ω(𝑛𝑘) order of magnitude, our result improves significantly over the only previous result on this problem, the exp(Ω(𝑘)) lower bound in [50] (Theorem 1 in this work).

Before stating the precise result, we quickly discuss two situations which, in the light of previous results, do not appear overly interesting and for which we therefore did not make an effort to fully cover them by our result.

When 𝜆≤(1−𝜀)𝑒𝜇 for an arbitrarily small constant 𝜀≥0 and 𝜆 is at most polynomial in n, the results of Lehre [50] imply that the (𝜇,𝜆) EA has an exponential runtime on any function with a polynomial number of optima (and consequently, also on jump functions). We guess that the restriction to polynomial-size 𝜆 was made in [50, Corollary 1] only for reasons of mathematical convenience (together with the fact that super-polynomial population sizes raise some doubts on the implementability and practicability of the algorithm). We do not see any reason why Lehre’s result, at least in the case of the (𝜇,𝜆) EA, should not be true for any value of 𝜆 (possibly with a sub-exponential number of iterations, but still an exponential number of fitness evaluations).

Rowe and Sudholt [65, Theorem 10] showed that for all constants 𝜀>0 the (1,𝜆) EA with population size 𝜆≤(1−𝜀)log𝑒𝑒−1𝑛 has an expected optimization time of at least exp(Ω(𝑛𝜀/2)) on any function 𝑓:{0,1}𝑛→ℝ with a unique optimum. From inspecting the proof given in [65], we strongly believe that the same result also holds for the (𝜇,𝜆) EA. Since this is not central to our work, we do not give a rigorous proof. Our main argument would be that the runtime of the (𝜇,𝜆) EA on a jump function is at least (in the strong sense of stochastic domination) its runtime on ONEMAX. This follows from a coupling arguments similar to the one given in [24, Proof of Theorem 23]. More precisely, when comparing how offspring are selected in the JUMP and in the ONEMAX process, we can construct a coupling such that the parent individuals in the JUMP process always are not closer to the optimum than in the ONEMAX process. Now comparing how the (1,𝜆) EA and the (𝜇,𝜆) EA optimize ONEMAX, we see that the (𝜇,𝜆) EA may select worse parent individuals than the (1,𝜆) EA, which generate (in the stochastic domination sense) worse offspring, leading to a larger optimization time. As said, we do not declare this a complete proof, but since the case of small population sizes might generally not be too interesting and since our not fully rigorous analysis indicates that an interesting performance of the (𝜇,𝜆) EA is not to be expected here, we refrain from giving a complete analysis.

Let us declare the parameter settings just discussed as not so interesting since previous works show or strongly indicate that the (𝜇,𝜆) EA is highly inefficient on any objective function with unique optimum. Let us further declare exponential population sizes as not so interesting (mostly for reasons of implementability, but also because Lemma 10 will show that they imply exponential runtimes). With this language, our following result shows that the runtime of the (𝜇,𝜆) EA on jump functions with jump size 𝑘≤0.1𝑛 for all interesting parameter choices is, apart from lower order terms, at least the one of the (𝜇+𝜆) EA. For 𝑘>0.1𝑛, this runtime is at least 𝑛Ω(𝑛).

Theorem 8
Let 𝑐≤0.1 and C be large enough such that (4𝑐)𝐶/2≤𝑒−2. Let 𝑛≥2𝑐. Let 𝐶ln(𝑛)≤𝜆≤23exp(0.16𝑛) and 𝜇≤𝜆2. Let 𝑐′=1𝑒+𝑐 and ℎ(𝑛,𝜆):=exp(−(1−2𝑐′)22𝜆)+2𝑛−1𝑛2−𝑛. Let 𝑘∈[2..𝑛] and 𝑝𝑘:=(1−1𝑛)𝑛−𝑘𝑛−𝑘.

If 𝑘≤𝑐𝑛, then the expected runtime, measured by the number of fitness evaluations until the optimum is evaluated, of the (𝜇,𝜆) EA on jump functions with jump size k is at least

𝑇𝑘:=(1−exp(−0.16𝑛))(𝜇+(1−ℎ(𝑛,𝜆))1𝑝𝑘)=(1−𝑜(1))(𝜇+1𝑝𝑘),
where the asymptotic expression is for 𝑛→∞ and 𝜆=𝜔(1).

For 𝑘>𝑐𝑛, the expected runtime is at least 𝑇⌊𝑐𝑛⌋.

We phrased our result in the above form since we felt that it captures best the most interesting aspect, namely a runtime of essentially 1𝑝𝑘 when 𝑘≤0.1𝑛 and 𝜆=Ω(log𝑛) suitably large. Since our result is non-asymptotic, both c and C do not have to be constants. Hence if we are interested in the smallest possible value for 𝜆 that gives an (1−𝑜(1))1𝑝𝑘 runtime, then by taking 𝑐=𝑘𝑛 and 𝐶=4/ln(𝑛/(4𝑘)), we obtain the following result.

Corollary 9
Let 𝑘≥2. Let 𝑛≥10𝑘 and

4ln(𝑛4𝑘)ln(𝑛)≤𝜆≤23exp(0.16𝑛).
Let 𝜇≤𝜆2. Let 𝑐′=1𝑒+𝑘𝑛. With ℎ(𝑛,𝜆):=exp(−(1−2𝑐′)22𝜆)+2𝑛−1𝑛2−𝑛 and 𝑝𝑘:=(1−1𝑛)𝑛−𝑘𝑛−𝑘, the expected runtime of the (𝜇,𝜆) EA on JUMP𝑛𝑘 is at least

𝑇𝑘:=(1−exp(−0.16𝑛))(𝜇+(1−ℎ(𝑛,𝜆))1𝑝𝑘)=(1−𝑜(1))(𝜇+1𝑝𝑘),
where the asymptotic expression holds for 𝑛→∞ and 𝜆=𝜔(1).

In particular, if 𝑘=𝑂(𝑛1−𝜀) for a constant 𝜀>0, then it suffices to have 𝜆=𝜔(1) for the lower bound (1−𝑜(1))(𝜇+1𝑝𝑘) to hold.

Before giving the precise proof of Theorem 8, let us briefly explain the main ideas. As discussed earlier, this proof is an example for proving lower bounds by applying the additive drift theorem to a suitable rescaling of a natural potential function. As this work shows, this method can give very tight lower bounds, different from, say, negative drift theorems.

The heart, and art, of this method is defining a suitable potential function. The observation that the difficult part of the optimization process is traversing the region {𝑥∈{0,1}𝑛∣OM(𝑥)∈[𝑛−𝑘..𝑛]} together with the fact that the lower bound given by the additive drift theorem depends on the difference in potential of starting point and target suggested to us the following potential function. For a population P, let OM(𝑃) denote the maximum ONEMAX-value in the population. For OM(𝑃)>𝑛−𝑘, the potential of P will essentially be min{𝑛OM(𝑃)−(𝑛−𝑘),1𝜆𝑝𝑘}. All other populations have a potential of zero. This definition gives the desired large potential range of 1𝜆𝑝𝑘 and, after proving that the expected potential gain is at most one and the initial potential is zero with high probability, gives the desired lower bound on the runtime. The proof below gives more details, including the precise definition of the potential, which is minimally different from this simplified description.

Since the classic definition of the runtime of an evolutionary algorithm, the number of fitness evaluations until the optimum is evaluated, implies that we usually lose a number of 𝜆−1 fitness evaluations when translating iterations into fitness evaluations (since we usually cannot rule out that the optimum is sampled as the first individual generated in the iteration which finds the optimum, see (1)), we add a short argument to the proof of Theorem 8 to gain an extra 𝜆 fitness evaluations. The main observation is that both the 𝜇 initial individuals and the 𝜆 offspring of the first generation are uniformly distributed in the search space. This makes them very unlikely to be the optimum, and this argument (with some finetuning) allows us to start the drift argument from the second iteration on.

We now give the complete proof of our lower bound result.

Proof of Theorem 8
For a unified proof for the two cases that k is at most cn or greater than cn, let us denote by 𝑘′ the jump size and recall that this can be a function of n. Let 𝑘:=min{𝑘′,⌊𝑐𝑛⌋}. Assume that n and 𝜆 are large enough so that ℎ(𝑛,𝜆)<1, as otherwise there is nothing to show.

Let 𝑔max=(1−ℎ(𝑛,𝜆))1𝜆𝑝𝑘. Note that by our assumption, 𝑔max>0. Also, we easily see that 𝑔max≤𝑛𝑘: If 𝜆≥3, then 𝑔max≤1𝜆𝑝𝑘≤1𝜆𝑛−𝑘/𝑒≤𝑛𝑘; however, if 𝜆=2, the only other option leaving us with a positive integral 𝜇, then ℎ(𝑛,𝜆)≥exp(−(1−2/𝑒)2)>0.93 and again 𝑔max=(1−ℎ(𝑛,𝜆))1𝜆𝑝𝑘≤0.071𝜆𝑛−𝑘/𝑒≤𝑛𝑘.

For all 𝐿∈[1..𝑘], let 𝑔(𝐿):=min{𝑛𝐿,𝑔max}, and let 𝑔(0):=0.

Let 𝑘∗ be the smallest integer in [1..k] such that 𝑔(𝑘∗)=𝑔max. Note that 𝑘∗ is well defined since 𝑔max≤𝑛𝑘.

For all individuals 𝑥∈{0,1}𝑛, denote by

OM(𝑥)ℓ(𝑥)𝑔(𝑥):=∑𝑖=1𝑛𝑥𝑖∈[0..𝑛] itsONEMAX -value,:=max{0,OM(𝑥)−(𝑛−𝑘)}∈[0..𝑘],:=𝑔(ℓ(𝑥))∈{0,𝑛,𝑛2,…,𝑛𝑘,𝑔max}∩[0,𝑔max].
For a population P, that is, a multiset of individuals, we write

OM(𝑃)ℓ(𝑃)𝑔(𝑃):=max{OM(𝑥)∣𝑥∈𝑃},:=max{ℓ(𝑥)∣𝑥∈𝑃}=max{0,OM(𝑃)−(𝑛−𝑘)},:=max{𝑔(𝑥)∣𝑥∈𝑃}=𝑔(ℓ(𝑃)).
We use g(P) as a measure for the quality of the current population of the algorithm. We shall argue that we typically start with 𝑔(𝑃)=0 and that one iteration increases g(P) in expectation by at most 1. Since we have 𝑔(𝑃)=𝑔max if P contains the optimum, the additive drift theorem yields that it takes an expected number of at least 𝑔(𝑘∗) iterations to find the optimum. Let us make these arguments precise.

We first show that with high probability both the initial population 𝑃0 and the first offspring population (and, consequently, also 𝑃1) contain no individual x with OM(𝑥)>𝑛−𝑘. For this, we first observe that trivially the random initial individuals are uniformly distributed in {0,1}𝑛. We recall that if 𝑥∈{0,1}𝑛 is uniformly distributed, then this is equivalent to saying that the random variables 𝑥1,𝑥2,…,𝑥𝑛 are independent and uniformly distributed in {0,1}. Interestingly, also the individuals of the first offspring population are uniformly distributed, since they are generated from taking a random individual by flipping each bit independently with probability 1𝑛. Consequently, all their bits are independent and uniformly distributed in {0,1} (the different offspring are not independent, but this independence is not needed in the following). By the additive Chernoff bound (Theorem 5), the probability that a random individual x satisfies OM(𝑥)>(1−𝑐)𝑛 is at most exp(−2(0.5−𝑐)2𝑛2/𝑛)=exp(−0.32𝑛) by our choice of c. Since 𝜇≤𝜆2 and 𝜆≤23exp(0.16𝑛), a simple union bound over the 𝜇 initial individuals and the 𝜆 offspring shows that with probability at least 1−(𝜇+𝜆)exp(−0.32𝑛)≥1−exp(−0.16𝑛), none of these individuals x satisfies OM(𝑥)>𝑛−𝑘. In this case, the population 𝑃1 satisfies 𝑔(𝑃1)=0 and up to this point, the algorithm has used 𝜇+𝜆 fitness evaluations without ever sampling the optimum.

We now analyze the effect of one iteration. Let P be a population of 𝜇 elements that does not contain the optimum. Let 𝑃′ be the (random) offspring population generated in one iteration of the (𝜇,𝜆) EA started with P, and let 𝑃″ be the next parent population, that is, the random population generated from selecting 𝜇 best individuals from 𝑃′.

Let 𝑖:=OM(𝑃) and 𝑗>max{𝑖,𝑛−𝑘}. Hence if OM(𝑃″)=𝑗, then ℓ(𝑃″)>ℓ(𝑃), and in the case that 𝑖<𝑛−𝑘+𝑘∗ we have made a true progress with respect to our measure g(P).

For this reason, we now compute the probability that OM(𝑃″)=𝑗.

We consider first the case 𝑗=𝑛. Note that here OM(𝑃′)=𝑗 implies OM(𝑃″)=𝑗. Let x be an element of P and y be an offspring generated from x by mutation. Then Pr[𝑦=(1,…,1)]=(1−1𝑛)OM(𝑥)𝑛−(𝑛−OM(𝑥))≤(1−1𝑛)𝑖𝑛−(𝑛−𝑖), using that 𝑛≥2𝑐≥2. By a union bound over the 𝜆 offspring, we have OM(𝑃′)=OM(𝑃″)=𝑛 with probability at most

Pr[OM(𝑃″)=𝑛]≤𝜆(1−1𝑛)𝑖𝑛−(𝑛−𝑖).
(2)
Let now 𝑗<𝑛. By the definitions of the (𝜇,𝜆) EA and the jump functions, we have OM(𝑃″)=𝑗 only if 𝑃′ contains at least 𝜆−𝜇+1 individuals y with OM(𝑦)∈[𝑗..𝑛−1]. To obtain an upper bound for Pr[OM(𝑃″)=𝑗] we regard the (slightly larger) event  that 𝑃′ contains at least 𝜆−𝜇+1 individuals y with OM(𝑦)∈[𝑗..𝑛].

To analyze this event , let again 𝑥∈𝑃 and y be a mutation-offspring generated from x. By a natural domination argument [70, Lemma 6.1], the probability of the event OM(𝑦)≥𝑗 does not decrease if we increase OM(𝑥). For this reason, let us assume that OM(𝑥)=max{𝑖,𝑛−𝑘}=:ı̃ . Now for OM(𝑦)≥𝑗 to hold, at least 𝑗−ı̃  of the 𝑛−ı̃  zero-bits in x have to be flipped. The number of flipped zero-bits follows a binomial distribution with parameters 𝑛−ı̃  and 1𝑛. By Lemma 4, we have

Pr[OM(𝑦)≥𝑗]≤(𝑛−ı̃ 𝑗−ı̃ )𝑛−(𝑗−ı̃ )≤(𝑛−ı̃ 𝑛)𝑗−ı̃ =:𝑝ı̃ 𝑗.
Since the individuals of the offspring population 𝑃′ are generated independently, the number of offspring y with OM(𝑦)∈[𝑗..𝑛] is binomially distributed with parameters 𝜆 and some number 𝑝′≤𝑝ı̃ 𝑗. Using again Lemma 4, we see that

Pr[OM(𝑃″)=𝑗]≤Pr[]≤(𝜆𝜆−𝜇+1)(𝑝′)𝜆−𝜇+1≤2𝜆((𝑛−ı̃ 𝑛)𝑗−ı̃ )𝜆−𝜇+1≤2𝜆((𝑘𝑛)𝑗−ı̃ )𝜆/2≤(4𝑘𝑛)(𝑗−ı̃ )𝜆/2≤((4𝑐)𝐶/2)ln(𝑛)(𝑗−ı̃ )≤𝑛−2(𝑗−ı̃ ),
(3)
where we used the estimates (𝜆𝜆−𝜇+1)≤2𝜆 and our assumptions 𝜇≤𝜆2 and (4𝑐)𝐶/2≤𝑒−2.

So far we have computed that it is difficult to strictly increase OM(𝑃) (once OM(𝑃) is above 𝑛−𝑘). Using a similar reasoning, we now show that also the probability of the event OM(𝑃)=OM(𝑃″) is small (when 𝑖:=OM(𝑃)>𝑛−𝑘). Again, for this event it is necessary that at least 𝜆−𝜇+1 offspring y satisfy OM(𝑦)∈[𝑖..𝑛]. Let y be an offspring generated from a parent 𝑦∈𝑃. As above, using a domination argument we can assume that OM(𝑥)=OM(𝑃)=𝑖. For such an x, we have OM(𝑦)≥𝑖 only if either no bit at all flips or if at least one zero-bit is flipped. Hence Pr[OM(𝑦)≥𝑖]≤(1−1𝑛)𝑛+𝑝𝑖,𝑖+1≤1𝑒+𝑘𝑛≤1𝑒+𝑐=𝑐′<0.5. Denoting by X the number of offspring y with OM(𝑦)≥𝑖, we have 𝐸[𝑋]≤𝑐′𝜆. Using the additive Chernoff bound (Theorem 5) rather than Lemma 4 and 𝜇≤12𝜆, we compute

Pr[OM(𝑃″)=𝑖]≤Pr[𝑋≥𝜆−𝜇+1]≤Pr[𝑋≥12𝜆]≤Pr[𝑋≥𝐸[𝑋]+(0.5−𝑐′)𝜆]≤exp(−2((0.5−𝑐′)𝜆)2𝜆)=exp(−(1−2𝑐′)22𝜆).
(4)
We are now ready to compute the expected progress of g(P) in one iteration. Let first ℓ(𝑃)=0 and thus 𝑔(𝑃)=0. Since Pr[ℓ(𝑃″)=𝐿]≤𝑛−2𝐿 for all 𝐿∈[1..𝑘−1] by (3) and Pr[ℓ(𝑃″)=𝑘]≤𝜆𝑝𝑘 by (2), we have

𝐸[𝑔(𝑃″)]≤1⋅𝑔(0)+∑𝐿=1𝑘−1𝑛−2𝐿⋅𝑔(𝐿)+𝜆𝑝𝑘⋅𝑔max≤0+∑𝐿=1𝑘−1𝑛−𝐿+1−ℎ(𝑛,𝜆)≤1𝑛−1+1−ℎ(𝑛,𝜆)≤1
by the choice of 𝑔max and h. Consequently, 𝐸[𝑔(𝑃″)−𝑔(𝑃)]≤1.

For ℓ:=ℓ(𝑃)>0, the probability to reach ℓ(𝑃″)=𝑘 is larger, however, we profit from the fact that we can reduce the potential by having ℓ(𝑃″)<ℓ. Since there is nothing to show when g(P) is already at the maximal value 𝑔max, let us assume that ℓ<𝑘∗. Now equations (2), (3), and (4) give

Pr[ℓ(𝑃″)=𝑘]Pr[ℓ(𝑃″)=𝐿]Pr[ℓ(𝑃″)=ℓ]≤𝜆(1−1𝑛)𝑛−(𝑘−ℓ)𝑛−(𝑘−ℓ)≤𝜆(1−1𝑛)𝑛−𝑘𝑛−(𝑘−ℓ),≤𝑛−2(𝐿−ℓ),𝐿∈[ℓ+1..𝑘−1],≤exp(−(1−2𝑐′)22𝜆).
With these estimates, we compute

𝐸[𝑔(𝑃″)]≤Pr[ℓ(𝑃″)=𝑘]𝑔max+∑𝐿=ℓ+1𝑘−1Pr[ℓ(𝑃″)=𝐿]𝑔(𝐿)+Pr[ℓ(𝑃″)=ℓ]𝑔(ℓ)+1⋅𝑔(ℓ−1)≤𝜆(1−1𝑛)𝑛−𝑘𝑛−(𝑘−ℓ)⋅(1−ℎ(𝑛,𝜆))1𝜆(1−1𝑛)−(𝑛−𝑘)𝑛𝑘+∑𝐿=ℓ+1∞𝑛−2(𝐿−ℓ)𝑛𝐿+exp(−(1−2𝑐′)22𝜆)𝑛ℓ+𝑛ℓ−1=𝑔(𝑃)(1−ℎ(𝑛,𝜆)+1𝑛−1+exp(−(1−2𝑐′)22𝜆)+1𝑛)=𝑔(𝑃)
by our choice of 𝑔max and h as well as our assumption that 𝑔(𝑃)<𝑔max. Consequently, again we have 𝐸[𝑔(𝑃″)−𝑔(𝑃)]≤1.

Assuming that the population 𝑃1 satisfies 𝑔(𝑃1)=0, we can now apply the additive drift theorem (Theorem 6) as follows. As before, let 𝑃𝑡 denote the population at the end of iteration t. For all 𝑡≥0, let 𝑋𝑡=𝑔max−𝑔(𝑃𝑡+1). Then 𝑋0=𝑔max and 𝐸[𝑋𝑡−𝑋𝑡+1∣𝑋𝑡>0]≤1 for all 𝑡≥0. Consequently, the additive drift theorem (Theorem 6) gives that 𝑇:=min{𝑡∣𝑋𝑡=0} has an expectation of at least 𝑔max. By definition, 𝑋𝑡=0 is equivalent to saying that 𝑃𝑡+1 contains the optimum. Recall that if optima are generated in some iteration t, then some remain in 𝑃𝑡. Hence 𝑇+1 is indeed the first iteration in which the optimum is generated.

If the optimum is found in some iteration 𝑡≥1, then the total number of fitness evaluations up to this event is at least 𝜇+(𝑡−1)𝜆+1, where the 𝜇 accounts for the initialization and the −1 and +1 for the fact that the optimum could be the first search point sampled in iteration t (so that we cannot count the remaining offspring generated in the last iteration, see also (1)). This gives an expected optimization time of at least 𝜇+(𝐸[𝑇+1]−1)𝜆+1=𝜇+𝑔max𝜆+1 in the case 𝑔(𝑃1)=0.

Since we have 𝑔(𝑃1)=0 with probability 1−exp(−0.16𝑛), the expected runtime is at least (1−exp(−0.16𝑛))(𝜇+𝑔max𝜆+1). Recalling that 𝑔max=(1−ℎ(𝑛,𝜆))1𝜆𝑝𝑘, we have proven the theorem. ◻

Since it might be useful in other applications, we now explicitly formulate our lower bound of, essentially, 𝜇+𝜆, which was observed in the proof above.

Lemma 10
Let 𝑓:{0,1}𝑛→ℝ. Assume that f has at most M global optima. Let 𝜇,𝜆 be positive integers. Consider the optimization of f via the (𝜇,𝜆) EA (and assume 𝜇≤𝜆 in this case) or the (𝜇+𝜆) EA. Let 𝑁≤𝜇+𝜆. Then with probability 1−𝑀𝑁2−𝑛, the optimization time is larger than N. In particular, the expected optimization time is at least (1−𝑀𝑁2−𝑛)(𝑁+1)≥14min{𝜇+𝜆,2𝑛/𝑀}.

Proof
As discussed in the proof of Theorem 8, each of the first 𝜇+𝜆 individuals generated in a run of the (𝜇,𝜆) EA (and the same applies to the (𝜇+𝜆) EA) is uniformly distributed in {0,1}𝑛. Consequently, it is an optimum with probability at most 𝑀2−𝑛. By a union bound over the first N of these 𝜇+𝜆 individuals, the probability that one of them is optimal, is at most 𝑁𝑀2−𝑛. This gives the claims, where the last estimate follows from taking 𝑁=min{𝜇+𝜆,2𝑛/(2𝑀)}. ◻

A Tight Upper Bound
While our main target in this work was showing a lower bound that demonstrates that the (𝜇,𝜆) EA has little advantage in leaving the local optima of the jump functions, we now also present an upper bound on the runtime. It shows that our lower bound for large parts of the parameter space is tight including the leading constant. This might be the first non-trivial upper bound for a non-elitist evolutionary algorithm that is tight including the leading constant. This result also shows that our way to exploit negative drift in the lower bound analysis, namely not via the classic negative drift theorems, but via additive drift applied to an exponential rescaling, can give very precise results, unlike the previously used methods.

We shall show the following result.

Theorem 11
Let K be a sufficiently large constant and 𝜆≥𝐾ln𝑛. Let 0<𝛿<1 be a constant and 𝜇≤1(1+𝛿)𝑒𝜆. Let 𝑘∈[2..𝑛] and 𝑝𝑘=(1−1𝑛)𝑛−𝑘𝑛−𝑘. Then the runtime T of the (𝜇,𝜆) EA on JUMP𝑛𝑘 satisfies

𝐸[𝑇]≤𝜆1−𝑛−1/2(8𝐶𝑛+1+9𝐶𝑛𝑝𝑘𝜆‾‾‾‾√+8𝐶𝑛𝑝𝑘𝜆⌊𝑛3/2⌋+1𝑝𝑘𝜆),
where C is a constant depending on 𝛿 only.Footnote3

Consequently, for 𝜆=𝑜(1/(𝑛𝑝𝑘))=𝑜(𝑛𝑘−1), we have 𝐸[𝑇]≤(1+𝑜(1))1𝑝𝑘, and for 𝜆=Ω(1/(𝑛𝑝𝑘))=Ω(𝑛𝑘−1), we have 𝐸[𝑇]=𝑂(𝜆𝑛).

We note that when 𝜆=𝑜(𝑛𝑘−1), 𝜆≥𝐾ln𝑛 with K a sufficiently large constant, 𝜇≤1(1+𝛿)𝑒𝜆, and 𝑘≤0.1𝑛, our upper bound and our lower bound of Theorem 8 agree including the leading constant. So we have a precise runtime analysis in this regime.

We did not try to find the maximal range of parameters in which the runtime is (1±𝑜(1))1𝑝𝑘. From [65] (see the discussion at the beginning of Sect. 5) it is clear that for 𝜆≤𝑐ln𝑛, c a sufficiently small constant, the runtime is exp(Ω(𝑛𝐶)), where C is a constant that depends on c. For 𝜇≥1(1−𝛿)𝑒𝜆, Lehre [50] gives an exponential lower bound. The restriction to 𝑘≤0.1𝑛 is most likely not necessary, but the range of larger k appears not to be overly interesting given that the super-exponential lower bound from the case 0.1n still applies.

When 𝜆=Ω(𝑛𝑘−1), besides being a possibly unrealistically large population size, our time estimate of O(n) iterations is the same as the best known upper bound for the runtime of the (𝜇,𝜆) EA on the ONEMAX test function [9]. Since this analysis works with the natural partition into Θ(𝑛) fitness levels, the runtime order of O(n) shows that each fitness level is gained in an amortized constant number of iterations. This speed of progress on a difficult problem like jump functions again indicates that the offspring population size 𝜆 here is chosen too large.

The exact order of magnitude of the runtime of the (𝜇,𝜆) EA on ONEMAX is still an open problem. The upper bound proven for the (𝜇+𝜆) EA in [4], which in our setting simplifies to 𝑂(𝑛log𝑛𝜆+𝑛loglog𝜆/𝜇log𝜆/𝜇), indicates that there could be some (but not much) room for improvement. So clearly, the next progress here should rather be for the ONEMAX function than for jump functions.

The result in Theorem 11 above improves over the 𝑂(𝑛𝑘+𝑛𝜆+𝜆log𝜆) upper bound for the runtime of the (𝜇,𝜆) EA on JUMP𝑛𝑘 proven in [9] (see Theorem 2) in three ways. First, as discussed above, we make the leading constant precise (and tight for large ranges of the parameters). Second, we obtain a better, namely at most linear, dependence of the runtime on 𝜆. Third, we reduce the minimum offspring population size required for the result to hold, which is Ω(𝑘log𝑛) in [9] and Ω(log𝑛) in our result.

Level-based Analyses
A central step in our proof is an analysis of how the (𝜇,𝜆) EA progresses to a parent population consisting only of individuals on the local optimum. Since the (𝜇,𝜆) EA is a non-elitist algorithm, this asks for tools like the ones introduced by Lehre [51] and then improved by various authors [9, 17, 18]. Unfortunately, all these results are formulated for the problem of finding one individual of a certain minimum quality. Consequently, they all cannot be directly employed to analyze the time needed to have the full parent population consist of individuals of at least a certain quality. Fortunately, in their proofs all previous level-based analyses proceed by analyzing the time until a certain number of individuals of a certain quality have been obtained and then building on this with an analysis on how better individuals are generated. Among the previous works it appears that [17] is the one that makes this argumentation most explicit, whereas the other works with their intricate potential function arguments give less insight into the working principles of the process.

For this reason, we build now on [17]. To avoid restating an essentially unchanged proof from [17], we instead first state the level-based theorem shown in [17], explain where the different expressions in the runtime estimate stem from, and then state without explicit proof the level-based result we need. With the explanations given beforehand, we feel that the interested reader easily can see from [17] why our level-based theorem is correct.

The general setup of level-based theorems for population processes is as follows. There is a ground set , which will be search space {0,1}𝑛 in our applications. On this ground set, a population-based Markov process (𝑃𝑡) is defined. We consider populations of fixed size 𝜆, which may contain elements several times (multi-sets). We write 𝜆 to denote the set of all such populations. We only consider Markov processes where each element of the next population is sampled independently (with repetition). That is, for each population 𝑃∈𝜆, there is a distribution D(P) on  such that given 𝑃𝑡, the next population 𝑃𝑡+1 consists of 𝜆 elements of , each chosen independently from the distribution 𝐷(𝑃𝑡). We do not make any assumptions on the initial population 𝑃0.

In the level-based setting, we assume that there is a partition of  into levels 𝐴1,…,𝐴𝑚. Based on information in particular on how individuals in different levels are generated, we aim for an upper bound on the first time such that the population contains an element of the highest level 𝐴𝑚. Now the level-based theorem shown in [17] is as follows.

Theorem 12
(Level-based theorem). Consider a population process as described above.

Let (𝐴1,…,𝐴𝑚) be a partition of . Let 𝐴≥𝑗:=⋃𝑚𝑖=𝑗𝐴𝑖 for all 𝑗∈[1..𝑚]. Let 𝑧1,…,𝑧𝑚−1,𝛿∈(0,1], and let 𝛾0∈(0,11+𝛿] with 𝛾0𝜆∈ℤ. Let 𝐷0=min{⌈100/𝛿⌉,𝛾0𝜆} and 𝑐1=56000. Let

𝑡0=7000𝛿⎛⎝⎜⎜𝑚+11−𝛾0∑𝑗=1𝑚−1log02⎛⎝⎜⎜2𝛾0𝜆1+𝑧𝑗𝜆𝐷0⎞⎠⎟⎟+1𝜆∑𝑗=1𝑚−11𝑧𝑗⎞⎠⎟⎟,
where log02(𝑥):=max{0,log2(𝑥)} for all 𝑥∈ℝ. Assume that for any population 𝑃∈𝜆 the following three conditions are satisfied.

(G1):
For each level 𝑗∈[1..𝑚−1], if |𝑃∩𝐴≥𝑗|≥𝛾0𝜆/4, then

Pr𝑦∼𝐷(𝑃)[𝑦∈𝐴≥𝑗+1]≥𝑧𝑗.
(G2):
For each level 𝑗∈[1..𝑚−2] and all 𝛾∈(0,𝛾0], if |𝑃∩𝐴≥𝑗|≥𝛾0𝜆/4 and |𝑃∩𝐴≥𝑗+1|≥𝛾𝜆, then

Pr𝑦∼𝐷(𝑃)[𝑦∈𝐴≥𝑗+1]≥(1+𝛿)𝛾.
(G3):
The population size 𝜆 satisfies

𝜆≥256𝛾0𝛿ln(8𝑡0).
Then 𝑇:=min{𝜆𝑡∣𝑃𝑡∩𝐴𝑚≠∅} satisfies

𝐸[𝑇]≤8𝜆𝑡0=𝑐1𝜆𝛿⎛⎝⎜⎜𝑚+11−𝛾0∑𝑗=1𝑚−2log02⎛⎝⎜⎜2𝛾0𝜆1+𝑧𝑗𝜆𝐷0⎞⎠⎟⎟+1𝜆∑𝑗=1𝑚−11𝑧𝑗⎞⎠⎟⎟.
Let us explain where the time bound stated in this theorem stems from. We argue in terms of iterations now, not in terms of search point evaluations. Then the time bound is 8𝑡0 with 𝑡0 as defined in the theorem. The main argument of the proof given in [17] is as follows. Let us, in the next three paragraphs, say that a population P is well-established on level j if |𝑃∩𝐴≥𝑗|≥𝛾0𝜆/4. Now condition (G2) imposes that if the current population is well-established on level j, then the number of individuals on level 𝑗+1 or higher increases, in expectation, by a factor of 1+𝛿 until at least 𝛾0𝜆 such individuals are in the population. It appears natural (and is true, but not trivial to prove) that it takes roughly log1+𝛿(𝛾0𝜆)≈1𝛿log(𝛾0𝜆) iterations from the first individual on level 𝑗+1 to having at least 𝛾0𝜆 individuals on this level. This explains roughly the middle term in the definition of 𝑡0. Without going into details, we remark that the extra 𝑧𝑗𝜆𝐷0 expression exploits that when generating individuals on a higher level (as described in (G1)) is easy, then we can assume that we do not start with a single individual on level 𝑗+1, but with roughly 𝑧𝑗𝜆 individuals. Consequently, we need the factor-(1+𝛿) growth only to go from 𝑧𝑗𝜆 to 𝛾0𝜆 individuals.

The remaining term 𝑚+1𝜆∑𝑚−1𝑗=11𝑧𝑗 accounts for the time needed to generate the first individuals on a higher level. Given that the population is well-established on some level j, by (G1) the probability that a new individual is on level 𝑗+1 or higher is at least 𝑧𝑗. Since we generate 𝜆 individuals in each step, the time to find an individual on a higher level (tacitly assuming that we stay well-established on level j, which is ensured by (G3) via Martingale concentration arguments) is at most ⌈𝑋/𝜆⌉≤1+𝑋/𝜆, where X is geometrically distributed with success probability 𝑧𝑗 and thus expectation 1𝑧𝑗.

This explanation of the definition of 𝑡0 motivates that we can extend the result of Theorem 12 to statements on how long it takes to have a certain level well-established (or even filled with at least 𝛾0𝜆 individuals). This is what we do now. We omit the formal proof, but invite the reader to consult the proof in [17], which immediately yields our claim.

Corollary 13
(Level-based theorem for filling sub-optimal levels). Let a population process be given as described above.

Let (𝐴1,…,𝐴𝑚) be a partition of . Let 𝐴≥𝑗:=⋃𝑚𝑖=𝑗𝐴𝑖 for all 𝑗∈[1..𝑚]. Let 𝑧1,…,𝑧𝑚−1,𝛿∈(0,1], and let 𝛾0∈(0,11+𝛿] with 𝛾0𝜆∈ℤ. Let 𝐷0=min{⌈100/𝛿⌉,𝛾0𝜆} and 𝑐1=56000.

Let ℓ∈[1..𝑚−1] and

𝑡0(ℓ)=7000𝛿⎛⎝⎜⎜𝑚+11−𝛾0∑𝑗=1ℓ−1log02⎛⎝⎜⎜2𝛾0𝜆1+𝑧𝑗𝜆𝐷0⎞⎠⎟⎟+1𝜆∑𝑗=1ℓ−11𝑧𝑗⎞⎠⎟⎟,
where log02(𝑥):=max{0,log2(𝑥)} for all 𝑥∈ℝ. Assume that for any population 𝑃∈𝜆 the following three conditions are satisfied.

(G1):
For each level 𝑗∈[1..ℓ−1], if |𝑃∩𝐴≥𝑗|≥𝛾0𝜆/4, then

Pr𝑦∼𝐷(𝑃)[𝑦∈𝐴≥𝑗+1]≥𝑧𝑗.
(G2):
For each level 𝑗∈[1..ℓ−2] and all 𝛾∈(0,𝛾0], if |𝑃∩𝐴≥𝑗|≥𝛾0𝜆/4 and |𝑃∩𝐴≥𝑗+1|≥𝛾𝜆, then

Pr𝑦∼𝐷(𝑃)[𝑦∈𝐴≥𝑗+1]≥(1+𝛿)𝛾.
(G3):
The population size 𝜆 satisfies

𝜆≥256𝛾0𝛿ln(8𝑡0(ℓ)).
Then 𝑇:=min{𝜆𝑡∣|𝑃𝑡∩𝐴≥ℓ|≥𝛾0𝜆} satisfies

𝐸[𝑇]≤8𝜆𝑡0(ℓ)=𝑐1𝜆𝛿⎛⎝⎜⎜𝑚+11−𝛾0∑𝑗=1ℓ−1log02⎛⎝⎜⎜2𝛾0𝜆1+𝑧𝑗𝜆𝐷0⎞⎠⎟⎟+1𝜆∑𝑗=1ℓ−11𝑧𝑗⎞⎠⎟⎟.
Proof of the Upper Bound
We are now ready to prove our upper bound result. We start by giving a brief outline of the main arguments. We use our variant of the level-based theorem to argue that from any possible state of the algorithm, it takes an expected number of O(n) iterations to reach a parent population that consists only of individuals in the local optimum (or the global optimum, but since we are done then, we can ignore this case). We call this an almost perfect population. From this point on, we cannot use the level-based method anymore, since the small probability for going from the local to the global optimum would require a large value of 𝜆, a requirement we try to avoid. This requirement is necessary in the level-based method because there one tries to ensure that once a decent number of individuals are on at least a certain level, this state is never lost. When 𝜆 is only logarithmic in n, there is an inverse-polynomial probability to completely lose a level. Since for, say, 𝑘=Θ(𝑛), we expect a runtime of roughly 𝑛𝑘/𝜆, in this time it will regularly happen that we lose a level, including the cases that we lose a level in each of several iterations or that we lose several levels at once.

We overcome this difficulty with a restart argument. Since the probability for such an undesirable event is only inverse-polynomial in n, we see that we keep an almost perfect population for at least 𝑛2 iterations (with high probability). Since it took us only O(n) iterations to reach (or regain) an almost perfect population, we obtain that in all but a lower order fraction of the iterations we have an almost perfect parent population. Hence apart from this lower order performance loss, we can assume that we are always in an almost perfect population. From such a state, we reach the optimum in one iteration with probability 1−(1−𝑝𝑘)𝜆, which quickly leads to the claimed result.

We now state the formal proof, which makes this proof sketch precise and adds a few arguments not discussed so far.

Proof of Theorem 11
Since 𝑛𝑛 is a trivial upper bound for the expected runtime of any evolutionary algorithm creating all individuals as random search points or via standard bit mutation with mutation rate 1𝑛, simply because each of these search points with probability at least 𝑛−𝑛 is the optimumFootnote4, we can assume that 𝑘<𝑛.

Let 𝑚=𝑛+1 and let 𝐴1,…,𝐴𝑚 be the partition of {0,1}𝑛 into the fitness levels of JUMP𝑛𝑘, that is, for all 𝑖∈[1..𝑚−1] we have 𝐴𝑖={𝑥∈{0,1}𝑛∣𝑓(𝑥)=𝑖} and for 𝑖=𝑚 we have 𝐴𝑖={(1,…,1)}. In particular, for all 𝑖∈[1..𝑚−1] and all 𝑥∈𝐴𝑖, 𝑦∈𝐴𝑖+1 we have 𝑓(𝑥)<𝑓(𝑦). Also, 𝐴𝑚 consists of the unique optimum and 𝐴𝑚−1 consists of all local optima.

Consider a run of the (𝜇,𝜆) EA on JUMP𝑛𝑘. As in Algorithm 1, we denote by 𝑃𝑡 the population (of size 𝜇) selected in iteration t, which serves as parent population in generation 𝑡+1. Let 𝑃0 denote the initial population. We denote by 𝑄𝑡 the offspring population (of size 𝜆) generated in iteration t. Hence 𝑃𝑡 consists of 𝜇 best individuals chosen from 𝑄𝑡. For the sake of a smooth presentation, let 𝑄0 be a population obtained from 𝑃0 by adding 𝜆−𝜇 random search points of minimal fitness. Note that we can again assume that 𝑃0 is obtained from 𝑄0 by selecting 𝜇 best individuals.

We say that a parent population 𝑃𝑡 is almost perfect if 𝑃𝑡⊆𝐴≥𝑚−1. Note that this is equivalent to having |𝑄𝑡∩𝐴≥𝑚−1|≥𝜇.

Step 1: We first argue that for any time 𝑠≥0 and regardless of what is 𝑄𝑠, the first time 𝑆≥𝑠 such that 𝑃𝑆 is almost perfect satisfies 𝐸[𝑆−𝑠]≤8𝑡0, where

𝑡0=104𝛿⎛⎝⎜⎜𝑚+11−𝛾0∑𝑗=1𝑚−2log02⎛⎝⎜⎜2𝛾0𝜆1+𝑧𝑗𝜆𝐷0⎞⎠⎟⎟+1𝜆∑𝑗=1𝑚−21𝑧𝑗⎞⎠⎟⎟.
To ease the notation, we assume that 𝑠=0. To estimate S, we apply our variant of the level-based theorem (Corollary 13) to the process (𝑄𝑡)𝑡≥0. Since optimizing jump functions up to the local optimum is very similar to optimizing the ONEMAX function, this analysis is similar to an analogous analysis for ONEMAX (where we note that the work [9] proving the previous-best result for ONEMAX for most details of the proof refers to the not very detailed conference paper [51]).

We choose suitable parameters to use the level-theorem. For 𝑗∈[1..𝑘−1], this corresponds to the fitness levels lying in the gap region of JUMP𝑛𝑘, let 𝑧𝑗=14𝑛−𝑗𝑒𝑛. For 𝑗∈[𝑘..𝑚−2], here 𝐴𝑗 consists of the search points x with OM(𝑥)=𝑗−𝑘, we let 𝑧𝑗=14𝑛−(𝑗−𝑘)𝑒𝑛. Note that for 𝑗∈[1..𝑘−1], we have 𝑧𝑗≥14𝑘+1−𝑗𝑒𝑛, and hence

∑𝑗=1𝑚−21𝑧𝑗≤4𝑒𝑛∑𝑖=2𝑛1𝑖≤4𝑒𝑛ln𝑛,
(5)
recalling that the harmonic number 𝐻𝑛=∑𝑛𝑖=11𝑖 satisfies 𝐻𝑛≤ln(𝑛)+1, see, e.g., [29, (1.4.12)]. Note also, for later, that for any j we have 𝑧𝑗≥14𝑛−𝑗𝑒𝑛.

Let 𝛾0 be such that 𝛾0𝜆=⌊𝜆(1+𝛿)𝑒⌋. Note that by our assumption that 𝜆 is large, 𝛾0𝜆 is an integer greater than one as required in Corollary 13. Also, 𝛾0≤11+𝛿 as required. By our assumption 𝜆≥(1+𝛿)𝑒𝜇, we have 𝛾0𝜆≥𝜇. Trivially, 𝛾0≤1(1+𝛿)𝑒≤1𝑒. Let 𝐷0=min{⌈100/𝛿⌉,𝛾0𝜆}.

We check that the conditions (G1) to (G3) of Corollary 13 are satisfied for ℓ=𝑚−1. To show (G1) and (G2), let 𝑡≥0 be any iteration.

(G1): Let 𝑗∈[1..𝑚−2] such that |𝑄𝑡∩𝐴≥𝑗|≥𝛾0𝜆/4. We need to show that an offspring y generated in iteration 𝑡+1 is in 𝐴≥𝑗+1 with probability at least 𝑧𝑗. Let first 𝑗≥𝑘, that is, 𝐴𝑗 is not a level in the gap. Let y be an offspring generated in iteration 𝑡+1 and let 𝑥∈𝑃𝑡 be its random parent, which we can assume to be not the optimum as otherwise we would be done already. Since 𝛾0𝜆/4≥𝜇/4, there are at least 𝜇/4 individuals in 𝑃𝑡∩𝐴≥𝑗. Hence with probability at least 1/4, we have 𝑥∈𝐴≥𝑗. In this case, we have

Pr[𝑦∈𝐴𝑗+1]≥min{1𝑒,(1−1𝑛)𝑛−1𝑛−(𝑗−𝑘)𝑛}≥𝑛−(𝑗−𝑘)𝑒𝑛,
where the first case refers to x already being in 𝐴≥𝑗+1, that is, 1≤𝑗+1−𝑘≤OM(𝑥)≤𝑛−1, and uses Lemma 7, and where the second case refers to 𝑥∈𝐴𝑗, that is, OM(𝑥)=𝑗−𝑘. In total, we have Pr[𝑦∈𝐴≥𝑗+1]≥14𝑛−(𝑗−𝑘)𝑒𝑛=𝑧𝑗. If 𝑗<𝑘, we proceed analogously with the only exception that, since in the first case we could have OM(𝑥)=0, we now estimate Pr[OM(𝑦)=OM(𝑥)]≥(1−1𝑛)𝑛. We thus obtain Pr[𝑦∈𝐴𝑗+1∣𝑥∈𝐴≥𝑗]≥min{(1−1𝑛)𝑛,(1−1𝑛)𝑛−1𝑛−𝑗𝑛}≥(1−1𝑛)𝑛−1𝑛−𝑗𝑛≥𝑛−𝑗𝑒𝑛. Consequently, now Pr[𝑦∈𝐴≥𝑗+1]≥14𝑛−𝑗𝑒𝑛=𝑧𝑗.

(G2): Let 𝑗∈[1..𝑚−2] such that |𝑄𝑡∩𝐴≥𝑗|≥𝛾0𝜆/4. Let 𝛾∈(0,𝛾0] such that |𝑄𝑡∩𝐴≥𝑗+1|≥𝛾𝜆. We need to show that an offspring y generated in iteration 𝑡+1 is in 𝐴≥𝑗+1 with probability at least (1+𝛿)𝛾. Let x be a parent selected uniformly at random from 𝑃𝑡, where again we assume that 𝑃𝑡 contains no optimal solution. There are at least min{𝛾𝜆,𝜇}≥min{𝛾(1+𝛿)𝑒𝜇,𝜇}=𝛾(1+𝛿)𝑒𝜇 individuals in 𝑃𝑡∩𝐴≥𝑗+1. Hence with probability at least 𝛾(1+𝛿)𝑒, we have 𝑥∈𝐴≥𝑗+1. In this case, Pr[𝑦∈𝐴≥𝑗+1]≥Pr[OM(𝑦)=OM(𝑥)]≥1𝑒 by Lemma 7 when OM(𝑥)≠0. When OM(𝑥)=0, then Pr[𝑦∈𝐴≥𝑗+1]≥Pr[OM(𝑦)∈{0,1}]≥Pr[∀𝑖∈[2..𝑛]:𝑥𝑖=𝑦𝑖]=(1−1𝑛)𝑛−1≥1𝑒, where the first estimate uses our assumption 𝑘<𝑛. Hence without conditioning on 𝑥∈𝐴≥𝑗+1, we have Pr[𝑦∈𝐴≥𝑗+1]≥𝛾(1+𝛿)𝑒⋅1𝑒≥(1+𝛿)𝛾.

(G3): We first estimate 𝑡0. We recall that 𝛾0≤1𝑒 and 𝑧𝑗≥14𝑛−𝑗𝑒𝑛 for all j. Thus, for all 𝑗∈[1..𝑚−2], we have

log02⎛⎝⎜⎜2𝛾0𝜆1+𝑧𝑗𝜆𝐷0⎞⎠⎟⎟≤log02((2/𝑒)𝐷0𝑧𝑗)≤log2(8𝐷0𝑛𝑛−𝑗).
Consequently,

11−𝛾0∑𝑗=1𝑚−2log02⎛⎝⎜⎜2𝛾0𝜆1+𝑧𝑗𝜆𝐷0⎞⎠⎟⎟≤𝑒𝑒−1∑𝑗=1𝑚−2log2(8𝐷0𝑛𝑛−𝑗)=𝑒𝑒−1log2(∏𝑗=1𝑚−28𝐷0𝑛𝑛−𝑗)≤𝑒𝑒−1log2((8𝐷0𝑛)𝑛𝑛!)≤𝑒𝑒−1log2((8𝐷0𝑛)𝑛(𝑛/𝑒)𝑛)=𝑒𝑒−1𝑛log2(8𝑒𝐷0),
where we used the well-known estimate 𝑛!≥(𝑛/𝑒)𝑛, see, e.g., [29, (1.4.13)].

From this and (5), we obtain

𝑡0=104𝛿⎛⎝⎜⎜𝑚+11−𝛾0∑𝑗=1𝑚−2log02⎛⎝⎜⎜2𝛾0𝜆1+𝑧𝑗𝜆𝐷0⎞⎠⎟⎟+1𝜆∑𝑗=1𝑚−21𝑧𝑗⎞⎠⎟⎟≤104𝛿(𝑚+𝑒𝑒−1𝑛log2(8𝑒𝐷0)+1𝜆4𝑒𝑛ln𝑛)=𝑂(𝑛),
(6)
where the asymptotic estimate uses the fact that 𝜆=Ω(log𝑛). This shows (G3).

From (G1) to (G3), Corollary 13 shows that after an expected number of 8𝑡0 iterations, we have reached an offspring population 𝑄𝑡 with |𝑄𝑡∩𝐴≥𝑚−1|≥𝛾0𝜆=𝜇 and thus an almost perfect population 𝑃𝑡.

Step 2: We now show that when 𝑃𝑡 contains only local optima, then with probability at least 1−𝑛−2, the same is true for 𝑃𝑡+1 or the global optimum has been found. Indeed, by our initial assumption 𝑘<𝑛, the search points on the local optimum have a ONEMAX-value between 1 and 𝑛−1. Hence Lemma 7 implies that 𝑋:=|𝑄𝑡+1∩𝐴≥𝑚−1| follows a binomial law with parameters 𝜆 and success probability at least 1𝑒. By the additive Chernoff bound (Theorem 5), we have

Pr[𝑋≤𝜆𝑒(1+𝛿)]=Pr[𝑋≤𝐸[𝑋]−𝛿1+𝛿𝐸[𝑋]]≤exp(−2(𝛿1+𝛿𝐸[𝑋])2𝜆)=exp(−2𝑒2(𝛿1+𝛿)2𝜆)≤𝑛−2
by our assumption that 𝜆≥𝐾ln(𝑛) with a constant K sufficiently large. Since 𝜇≤𝜆𝑒(1+𝛿), we have |𝑃𝑡+1∩𝐴≥𝑚−1|<𝜇 only if 𝑋<𝜇≤𝜆𝑒(1+𝛿). As just computed, this happens with probability at most 𝑛−2.

Step 3: Recall that 𝑝𝑘=(1−1𝑛)𝑛−𝑘𝑛−𝑘 is the probability to generate the optimum from a parent on the local optimum. Let 𝑇0=min{⌈𝑡0/𝜆𝑝𝑘‾‾‾‾‾‾√⌉,⌊𝑛3/2⌋}. We call a phase of a run of the algorithm an interval of (i) first all iterations until we have an almost perfect parent population 𝑃𝑡, and then (ii) another exactly 𝑇0 iterations. We assume here for simplicity that we continue to run the algorithm even when it found the optimum; in such a case, we replace such an optimum immediately with a random search point on the local optimum. Since we are interested in the first time an optimum is found, this modification does not change our results. By definition and step 1 above, the expected length of a phase is at most 8𝑡0+𝑇0 regardless of how this phase starts.

We call a phase regular if after reaching an almost perfect parent population 𝑃𝑡 we never (in the following exactly 𝑇0 iterations) encounter a parent population 𝑃𝑡 that is not almost perfect. By a simple union bound and step 2 above, each phase is regular with probability at least 1−𝑛−2𝑇0≥1−𝑛−1/2, regardless how the phase started and how it reached an almost perfect parent population.

A regular phase is successful if it finds the optimum at least once. Since in a regular phase at least 𝜆𝑇0 times an offspring is generated from a parent on the local optimum (which results in the global optimum with probability 𝑝𝑘), and since these offspring are generated independently, the probability for a regular phase to be not successful is at most (1−𝑝𝑘)𝜆𝑇0, which is at most 11+𝑝𝑘𝜆𝑇0 by an elementary estimate stated as Lemma 8 in [65]. Since thus a regular phase is successful with probability at least 1−11+𝑝𝑘𝜆𝑇0=𝑝𝑘𝜆𝑇01+𝑝𝑘𝜆𝑇0, it takes an expected number of 1+𝑝𝑘𝜆𝑇0𝑝𝑘𝜆𝑇0=1+1𝑝𝑘𝜆𝑇0 regular phases to find the optimum. Since phases are regular with probability at least 1−𝑛−1/2, it takes an expected number of at most 11−𝑛−1/2⋅(1+1𝑝𝑘𝜆𝑇0) phases to find the optimum. By Wald’s equation, these take an expected number of at most 11−𝑛−1/2⋅(1+1𝑝𝑘𝜆𝑇0)⋅(8𝑡0+𝑇0) iterations. We estimate

(1+1𝑝𝑘𝜆𝑇0)⋅(8𝑡0+𝑇0)=8𝑡0+𝑇0+8𝑡0𝑝𝑘𝜆𝑇0+1𝑝𝑘𝜆≤8𝑡0+𝑡0𝑝𝑘𝜆‾‾‾‾√+1+8𝑡0𝑝𝑘𝜆‾‾‾‾√+8𝑡0𝑝𝑘𝜆⌊𝑛3/2⌋+1𝑝𝑘𝜆.
Recalling that 𝑡0=𝑂(𝑛), we note that this expression is O(n) when 𝑝𝑘𝜆=Ω(1/𝑛) and (1+𝑜(1))1𝑝𝑘𝜆 when 𝑝𝑘𝜆=𝑜(1/𝑛). Recalling further that each iteration contains 𝜆 fitness evaluations, see also (1), the claim follows. ◻

Conclusion
In this work, we observed that for all reasonable parameter values, the (𝜇,𝜆) EA cannot optimize jump functions faster than the (𝜇+𝜆) EA. The (𝜇,𝜆) EA thus fails to profit from its ability to leave local optima to inferior solutions. While we prove this absence of advantage formally only for the basic (𝜇,𝜆) EA and jump functions (which constitute, however, a standard algorithm and a classic benchmark), we feel that our proofs do not suggest that this result is caused by very special characteristics of the (𝜇,𝜆) EA or the jump functions, but that it rather follows from the fact that leaving a local optimum having moderate radius of attraction via comma selection is generally difficult because, relatively independent of the population sizes, there is a strong drift towards the local optimum. We do not show such a strong drift when 𝜆<2𝜇, but in this case the selection pressure is known to be so low that no efficient optimization is possible.

Overall, this work suggests that the role of comma selection in evolutionary computation deserves some clarification. Interesting directions for future research could be to try to find convincing examples where comma selection is helpful or a general result going beyond particular examples that shows in which situations comma selection cannot speed up the optimization of multimodal objective functions. From a broader perspective, any result giving a mildly general advice which of the existing approaches to cope with local optima are preferable in which situations, would be highly desirable. The new analysis methods developed in this work, which can yield precise runtime bounds for non-elitist population processes and negative drift situations, could be helpful as they now allow to prove or disprove constant-factor advantages.