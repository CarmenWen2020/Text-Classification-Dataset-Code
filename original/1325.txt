One hope when using non-elitism in evolutionary computation is that the ability to abandon the current-best solution aids leaving local optima. To improve our understanding of this mechanism, we perform a rigorous runtime analysis of a basic non-elitist evolutionary algorithm (EA), the (ğœ‡,ğœ†) EA, on the most basic benchmark function with a local optimum, the jump function. We prove that for all reasonable values of the parameters and the problem, the expected runtime of the (ğœ‡,ğœ†) EA is, apart from lower order terms, at least as large as the expected runtime of its elitist counterpart, the (ğœ‡+ğœ†) EA (for which we conduct the first runtime analysis on jump functions to allow this comparison). Consequently, the ability of the (ğœ‡,ğœ†) EA to leave local optima to inferior solutions does not lead to a runtime advantage. We complement this lower bound with an upper bound that, for broad ranges of the parameters, is identical to our lower bound apart from lower order terms. This is the first runtime result for a non-elitist algorithm on a multi-modal problem that is tight apart from lower order terms.

Access provided by University of Auckland Library

Introduction
The mathematical runtime analysis of evolutionary algorithms (EAs) and other randomized search heuristics is a young but established subfield of the general research area of heuristic search [2, 23, 44, 58]. This field, naturally, has started with regarding the performance of simple algorithms on simple test problems: The problems usually were unimodal, that is, without local optima different from the global optimum, the algorithms were elitist and often had trivial populations, and the runtime guarantees only estimated the asymptotic order of magnitude, that is, gave ğ‘‚(â‹…) upper bounds or Î©(â‹…) lower bounds.

Despite this restricted scope, many fundamental results have been obtained and our understanding of the working principles of EAs has significantly increased with these works. In this work, we go a step further with a tight (apart from lower order terms) analysis of how a non-elitist evolutionary algorithm with both non-trivial parent and offspring populations optimizes a multimodal problem.

In contrast to the practical use of evolutionary algorithms, where non-elitism is often employed, the mathematical analysis of evolutionary algorithms so far could find only little evidence for the use of non-elitism. The few existing works, very roughly speaking (see Sect. 2.1 for more details) indicate that when the selection pressure is large, then the non-elitist algorithm simulates an elitist one, and when the selection pressure is low, then no function with unique global optimum can be optimized efficiently. The gap between these two regimes is typically very small. Consequently, a possible profit from non-elitism would require a careful parameter choice.

One often named advantage of non-elitist algorithms is their ability to leave local optima to inferior solutions, which can reduce the time spent uselessly in local optima. To obtain a rigorous view on this possible advantage, we analyze the performance of the well-known (ğœ‡,ğœ†) EA (see Sect. 3.1 for a precise definition) on the multimodal jump function benchmark [15]. We note that, apart from some sporadic results on custom-tailored example problems and the corollaries from very general results (see Theorems 1 and 2 further below), this is the first in-depth study of how a non-elitist algorithm optimizes a classic benchmark with local optima.

Our main result (see Sect. 5) is that in this setting, the small middle range between a too small and a too high selection pressure, which could be envisaged from previous works, does not exist. Rather, the two undesired regimes overlap significantly. We note that for the (ğœ‡,ğœ†) EA, the selection pressure is reasonably well described by the ratio of the offspring population size ğœ† to the parent population size ğœ‡. The selection pressure is low if ğœ†â‰¤(1âˆ’ğœ€)ğ‘’ğœ‡ for some constant ğœ€>0. In this case, the (ğœ‡,ğœ†) EA needs an exponential time to optimize any function ğ‘“:{0,1}ğ‘›â†’â„ with at most a polynomial number of global optima [50]. The selection pressure is high if ğœ†â‰¥(1+ğœ€)ğ‘’ğœ‡ for some constant ğœ€>0 and if ğœ† is at least logarithmic in the envisaged runtime. In this case, the (ğœ‡,ğœ†) EA can optimize many classic benchmark functions in a runtime at most a constant factor slower than, say, the (ğœ‡+ğœ†) EA, see [51].

Our main result implies (Corollary 9) that already when ğœ†â‰¥2ğœ‡, ğœ† is super-constant, and ğœ†=ğ‘œ(ğ‘›ğ‘˜âˆ’1), the runtime of the (ğœ‡,ğœ†) EA on all jump functions with jump size ğ‘˜â‰¤ğ‘›1âˆ’ğœ€ is at least the runtime of the (ğœ‡+ğœ†) EA (apart from lower order terms); to prove this statement, we also conduct the first so far and sufficiently tight runtime analysis of the (ğœ‡+ğœ†) EA on jump functions (Theorem 3). Consequently, the two regimes of a too low selection pressure and of no advantage over the elitist algorithm overlap in the range ğœ†âˆˆ[2ğœ‡,(1âˆ’ğœ€)ğ‘’ğœ‡], leaving for jump functions no space for a middle regime with runtime advantages from non-elitism. We note that our result, while natural, is not obvious. In particular, as a comparison of the (1, 1) EA and the (1+1) EA on highly deceptive functions shows (see Sect. 2.1.3 for more details), it is not always true that the elitist algorithm is at least as good as its non-elitist counterpart.

Our result does not generally disrecommend to use non-elitism, in particular, it does not say anything about possible other advantages from using non-elitism. Our result, however, does indicate that the ability to leave local optima to inferior solutions is non-trivial to turn into a runtime advantage (whereas at the same time, as observed in previous works, there is a significant risk that the selection pressure is too low to admit any reasonable progress).

We also prove an upper bound for the runtime of the (ğœ‡,ğœ†) EA on jump functions (Theorem 11), which shows that our lower bound for large ranges of the parameters (but, of course, only for ğœ†â‰¥(1+ğœ€)ğ‘’ğœ‡) is tight including the leading constant. This appears to be the first preciseFootnote1 on runtime result for a non-trivial non-elitist algorithm on a non-trivial problem.

From the technical perspective, it is noteworthy that we obtain precise bounds in settings where the previously used methods (negative drift for lower bounds, level-based analyses of non-elitist population processes for upper bounds) could not give precise analyses, and in the case of negative drift could usually not even determine the right asymptotic order of the runtime. We are optimistic that our methods will be profitable for other runtime analyses as well.

State of the Art and Our Results
This work progresses the state of the art in three directions with active research in the recent past, namely non-elitist evolutionary algorithms, precise runtime analyses, and methods to prove lower bounds in the presence of negative drift and upper bounds for non-elitist population processes. We now describe these previous states of the art and detail what is the particular progress made in this work. We concentrate ourselves on classic evolutionary algorithms (also called genetic algorithms) for the optimization in discrete search spaces. We note that non-elitism has been used in other randomized search heuristics such as the Metropolis algorithm [48, 74], simulated annealing [43, 68], strong-selection-weak-mutation (SSWM) [59, 63], and memetic algorithms [57]. Letting the selection decisions not only depend on the fitness, e.g., in tabu search or when using fitness sharing, also introduces some form of non-elitism. From a broader perspective, also many probabilistic model building algorithms such as ant colony optimizers or estimation-of-distribution algorithms can be seen as non-elitist, since they often allow moves to inferior models. From an even broader point of view, even restart strategies can be seen as a form of non-elitism. While all these research directions are interesting, it seems to us that the results obtained there, to the extent that we understand them, are not too closely related to our results and therefore not really comparable.

Non-Elitist Algorithms
While non-elitist evolutionary algorithms are used a lot in practice, the mathematical theory of EAs so far was not very successful in providing convincing evidences for the usefulness of non-elitism. This might be due to the fact that rigorous research on non-elitist algorithms has started only relatively late, caused among others by the fact that many non-elitist algorithms require non-trivial populations, which form another challenge for mathematical analyses. Another reason, naturally, could be that non-elitism is not as profitable as generally thought. Our work rather points into the latter direction.

The previous works on non-elitist algorithms can roughly be grouped as follows.

Exponential Runtimes When the Selection Pressure is Low
By definition, non-elitist algorithms may lose good solutions. When this happens too frequently (low selection pressure), then the EA finds it hard to converge to good solutions, resulting in a poor performance

The first to make this empirical observation mathematically precise in a very general manner was Lehre in his remarkable work [50]. For a broad class of non-elitist population-based EAs, he gives conditions on the parameters that imply that the EA cannot optimize any pseudo-Boolean function ğ‘“:{0,1}ğ‘›â†’â„ with at most a polynomial number of optima in time sub-exponential in n. Due to their general nature, we have to restrict ourselves here to what Lehreâ€™s results imply for the (ğœ‡,ğœ†) EA, but we note that analogous results hold for a much wider class of algorithms. For the (ğœ‡,ğœ†) EA using the usual mutation rate 1ğ‘›, Lehre shows that when ğœ†â‰¤(1âˆ’ğœ€)ğ‘’ğœ‡, where ğœ€>0 is any positive constant, then the time to find a (global) optimum of any pseudo-Boolean function with at most a polynomial number of optima is exponential in n with high probability.

We note that more specific results showing the danger of a too low selection pressure have appeared earlier. For example, already in 2007 JÃ¤gerskÃ¼pper and Storch [46, Theorem 1] showed that the (1,ğœ†) EA with ğœ†â‰¤114ln(ğ‘›) is inefficient on any pseudo-Boolean function with a unique optimum. The range of ğœ† for which such a negative performance is observed was later extended to the asymptotically tight value ğœ†â‰¤(1âˆ’ğœ€)logğ‘’ğ‘’âˆ’1ğ‘› by Rowe and Sudholt [65]. Happ et al. [39] showed that two simple (1+1)-type hillclimbers using fitness proportionate selection in the choice of the surviving individual are not efficient on any linear function with positive weights. Neumann et al. [56] showed that a mutation-only variant of the Simple Genetic Algorithm with fitness proportionate selection is inefficient on the ONEMAX function when the population size ğœ‡ is at most polynomial, and it is inefficient on any pseudo-Boolean function with unique global optimum when ğœ‡â‰¤14ln(ğ‘›). Oliveto and Witt [62] showed that the true Simple Genetic Algorithm (using crossover) cannot optimize ONEMAX efficiently when ğœ‡â‰¤ğ‘›14âˆ’ğœ€.

We note that the methods in [50] were also used to prove lower bounds for particular objective functions. The following result was given for a variant of jump functions [50, Theorem 5]. To be precise, a similar result was proven for a tournament-selection algorithm and it was stated that an analogous statement, which we believe to be the following, holds for the (ğœ‡,ğœ†) EA as well. As a reviewer pointed out, it is not immediately clear how the proof in [50] extends to the classic definition of jump functions.

Theorem 1
(cf. Lehre [50]). Let ğ‘›âˆˆâ„¤â‰¥1, ğœ€>0 a constant, ğ‘˜â‰¤(0.2âˆ’ğœ€)ğ‘›, and ğ‘˜=ğœ”(logğ‘›). Let ğ‘“ğ‘˜:{0,1}ğ‘›â†’â„ be defined by ğ‘“ğ‘˜(ğ‘¥)=ONEMAX(ğ‘¥) when ONEMAX(ğ‘¥)âˆ‰[ğ‘›âˆ’ğ‘˜+1..ğ‘›âˆ’1] and ğ‘“ğ‘˜(ğ‘¥)=0 otherwise. Then the expected runtime of the (ğœ‡,ğœ†) EA with polynomial ğœ† on JUMPğ‘›ğ‘˜ is at least exp(Î©(ğ‘˜)), where all asymptotics is for n tending to infinity.

Pseudo-Elitism When the Selection Pressure is High
When a non-elitist algorithm has the property that, despite the theoretical danger of losing good solutions, it very rarely does so, then its optimization behavior becomes very similar to the one of an elitist algorithm. Again the first to make this effect precise for a broad class of algorithms was Lehre in his first paper on level-based arguments for non-elitist populations [51].

Lehreâ€™s fitness-level theorem for non-elitist population-based algorithms assumes that the search space can be partitioned into levels such that (i) the algorithm has a reasonable chance to sample a solution in some level j or higher once a constant fraction of the population is at least on level ğ‘—âˆ’1 (â€œbase levelâ€) and (ii) there is an exponential growth of the number of individuals on levels higher than the base level; more precisely (but still simplified), if there are ğœ‡0<ğ›¾0ğœ‡ individuals above the base level, then in the next generation the number of individuals above the base level follows a binomial distribution with parameters ğœ‡ and ğ‘=(1+ğ›¿)ğœ‡0ğœ‡, where ğ›¾0 and ğ›¿ are suitable parameters. If in addition the population sizes involved are large enough, then (again very roughly speaking) the runtime of the algorithm is at most a constant factor larger than the runtime guarantee which could be obtained for an elitist analogue of the non-elitist EA. From the assumptions made here, it cannot be excluded that the non-elitist EA loses a best-so-far solution; however, due to the exponential growth of condition (ii) and the sufficiently large population size, this can only happen if there are few individuals above the base level. Hence the assumptions of the level-based method, roughly speaking, impose that that the EA behaves like an elitist algorithm except when it just has found a new best solution. In this case, with positive probability (usually at most some constant less than one) the new solution is lost. This constant probability for losing a new best individual (and the resulting need to re-generate one) may lead to a constant-factor loss in the runtime, but not more. Very roughly speaking, one can say that such non-elitist EAs, while formally non-elitist algorithms, do nothing else than a slightly slowed-down emulation of an elitist algorithm. That said, it has to be remarked that both proving level-based theorems (see [9, 17, 18, 51]) and applying them (see also [21]) is technical and much less trivial than what the rule of thumb â€œhigh selection pressure imitates elitismâ€ suggests.

For the optimization of jump functions via the (ğœ‡,ğœ†) EA, the work [9] implies the following result. We note that it was shown only to the variant of jump functions regarded in Theorem 1 (where the fitness in the gap region is uniformly zero), but from the proofs it is clear that the result also holds for the standard definition [15] used in this work.

Theorem 2
([9]). Let ğ‘˜âˆˆ[1..ğ‘›]. Let ğœ€>0 be a constant and let c be a sufficiently large constant (depending on ğœ€). Let ğœ†â‰¥ğ‘ğ‘˜ln(ğ‘›) and ğœ‡â‰¤ğœ†(1+ğœ€)ğ‘’. Then runtime T of the (ğœ‡,ğœ†) EA on JUMPğ‘›ğ‘˜ satisfies

ğ¸[ğ‘‡]=ğ‘‚(ğ‘›ğ‘˜+ğ‘›ğœ†+ğœ†logğœ†).
For the particular case of the (1,ğœ†) EA and the (1+ğœ†) EA, JÃ¤gerskÃ¼pper and Storch in an earlier work also gave fitness-level theorems [46, Lemma 6 and 7]. They also showed that both algorithms essentially behave identical for t iterations when ğœ† is at least logarithmic in t [46, Theorem 4]. This effect, without quantifying ğœ† and without proof, was already proposed in [45, p. 415]. JÃ¤gerskÃ¼pper and Storch show in particular that when ğœ†â‰¥3lnğ‘›, then the (1,ğœ†) EA optimizes ONEMAX in asymptotically the same time as the (1+ğœ†) EA. The actual runtimes given for the (1,ğœ†) EA in [46] are not tight since at that time a tight analysis of the (1+ğœ†) EA on ONEMAX was still missing; however, it is clear that the arguments given in [46] also allow to transfer the tight upper bound of ğ‘‚(ğ‘›logğ‘›+ğœ†ğ‘›loglogğ‘›logğ‘›) for the (1+ğœ†) EA from [16] to the (1,ğœ†) EA.

The minimum value of ğœ† that ensures an efficient optimization of the (1,ğœ†) EA on ONEMAX was lowered to the asymptotically tight value of ğœ†â‰¥logğ‘’ğ‘’âˆ’1ğ‘›â‰ˆ2.18lnğ‘› in [65]. Again, only the upper bound of ğ‘‚(ğ‘›logğ‘›+ğ‘›ğœ†) was shown. We would not be surprised if with similar arguments also a bound of ğ‘‚(ğ‘›logğ‘›+ğœ†ğ‘›loglogğ‘›logğ‘›) could be shown, but this is less obvious here than for the result of [46].

For the benchmark function LEADINGONES, the threshold between a superpolynomial runtime of the (1,ğœ†) EA and a runtime asymptotically equal to the one of the (1+ğœ†) EA was shown to be at ğœ†=(1Â±ğœ€)2logğ‘’ğ‘’âˆ’1ğ‘› [65].

Examples Where Non-Elitism is Helpful
The dichotomy described in the previous two subsections suggests that it is not easy to find examples where non-elitism is useful. This is indeed true apart from two exceptions.

JÃ¤gerskÃ¼pper and Storch [46] constructed an artificial example function that is easier to optimize for the (1,ğœ†) EA than for the (1+ğœ†) EA. The CLIFF function CLIFF:{0,1}ğ‘›â†’â„• is defined by CLIFF(ğ‘¥)=OM(ğ‘¥) if OM(ğ‘¥)<ğ‘›âˆ’âŒŠğ‘›/3âŒ‹ and CLIFF(ğ‘¥)=OM(ğ‘¥)âˆ’âŒŠğ‘›/3âŒ‹ otherwise. JÃ¤gerskÃ¼pper and Storch showed that the (1,ğœ†) EA with ğœ†â‰¥5lnğ‘› optimizes CLIFF in an expected number of ğ‘‚(exp(5ğœ†)) fitness evaluations, whereas the (1+ğœ†) EA with high probability needs at least ğ‘›ğ‘›/4 fitness evaluations. While this runtime difference is enormous, it has to be noted that even for the best value of ğœ†=5lnğ‘›, the runtime guarantee for the (1,ğœ†) EA is only ğ‘‚(ğ‘›25). Also, we remark that the local optimum of the CLIFF function has a particular structure which helps to leave the local optimum: Each point on the local optimum has âŒŠğ‘›/3âŒ‹ neighbors from which it is easy to hill-climb to the global optimum (as long as one does not use a steepest ascent strategy). Also, for each point on the local optimum there are Î©(ğ‘›2) search points in Hamming distance two from which any local search within less than n/3 improvements finds the global optimum. This is a notable difference to the JUMPğ‘›ğ‘˜ function, where hill-climbing from any point of the search space that is not the global optimum or one of its n neighbors surely leads to the local optimum. We would suspect that such larger radii of attraction are closer to the structure of difficult real-world problems, but we leave it to the reader to decide which model is most relevant for their applications.

We note that a second, albeit extreme and rather academic, example for an advantage of non-elitism is implicit in the early work [36] by Garnier, Kallel, and Schoenauer. They show that the (1, 1) EA on any function ğ‘“:{0,1}ğ‘›â†’â„ with unique global optimum has an expected optimization time of (1+ğ‘œ(1))ğ‘’ğ‘’âˆ’12ğ‘›; this follows from Proposition 3.1 in their work. When taking a highly deceptive function like the trap function, this runtime is significantly better than the ones of elitist algorithms, which typically are ğ‘›Î˜(ğ‘›). Of course, all this is not overly surprising â€“ the (1, 1) EA uses no form of selection and hence just performs a random walk in the search space (where the one-step distribution is given by the mutation operator). Therefore, this algorithm does not suffer from the deceptiveness of the trap function as do elitist algorithms. Also, a runtime reduction from ğ‘›Î˜(ğ‘›) to exp(Î˜(ğ‘›)) clearly is not breathtaking. Nevertheless, this is a second example where a (ğœ‡,ğœ†) EA significantly outperforms the corresponding (ğœ‡+ğœ†) EA.

Since in this work we are only interested in how non-elitism (and more specifically, comma selection) helps to leave local optima and by this improve runtimes, we do not discuss in detail other motivations for employing non-elitist algorithms. We note briefly, though, that comma selection is usually employed in self-adaptive algorithms. Self-adaptation means that some algorithm parameters are stored as part of the genome of the individuals and are subject to variation together with the original individual. The hope is that this constitutes a generic way to adjust algorithm parameters. When using plus selection together with self-adaptation, there would be the risk that the population at some point only contains individuals with unsuitable parameter values. Now variation will only generate inferior offspring. These will not be accepted and, consequently, the parameter values encoded in the genome of the individuals cannot be changed. When using comma selection, it is possible to accept individuals with inferior fitness, and these may have superior parameter values. We are not aware of a rigorous demonstration of this effect, but we note that the two runtime analysis papers [19, 30] on self-adaptation both use comma selection. We further note that comma selection is very common in continuous optimization, in particular, in evolution strategies, but since it is generally difficult to use insights from continuous optimization in discrete optimization and vice-versa we do not discuss results from continuous optimization here.

Our Contribution
In Sect. 5, we show that for all interesting values of the parameters of the problem and the algorithm, the expected runtime of the (ğœ‡,ğœ†) EA on jump functions is, apart from possibly lower order terms, at least the expected runtime of the (ğœ‡+ğœ†) EA. This shows that for this problem, there can be no significant advantage of using comma selection.

Our upper bound in Theorem 11, provided mostly to show that our analysis is tight including the leading constant, improves Theorem 2 by making the leading constant precise and being applicable for all offspring population sizes ğœ†â‰¥ğ¶ln(ğ‘›), C a constant independent of the jump size k. To the best of our knowledge, this is the first time that the runtime of a non-elitist algorithm was proven with this precision.

Precise Runtime Analyses
Traditionally, algorithm analysis aims at gaining a rough understanding how the runtime of an algorithm depends on the problem size. As such, most results only show statements on the asymptotic order of magnitude of the runtime, that is, results in big-Oh notation. For classic algorithmics, this is justified among others by the fact that the predominant performance measure, the number of elementary operations, already ignores constant factor differences in the execution times of the elementary operations.

In evolutionary computation, where the classic performance measure is the number of fitness evaluations, this excuse for ignoring constant factors is not valid, and in fact, in the last few years more and more precise runtime results have appeared, that is, results which determine the runtime asymptotically precise apart from lower order terms. Such results are useful, obviously because constant factors matter in practice, but also because many effects are visible only at constant-factor scales. For example, it was shown in [14] that all Î˜(1ğ‘›) mutation rates lead to a Î˜(ğ‘›logğ‘›) runtime of the (1+1) EA on all pseudo-Boolean linear functions, but only Wittâ€™s seminal result [70] that the runtime is (1+ğ‘œ(1))ğ‘’ğ‘ğ‘ğ‘›lnğ‘› for the mutation rate ğ‘ğ‘›, ğ‘>0 a constant, allows to derive that 1ğ‘› is the asymptotically best mutation rate.

Overall, not too many non-trivial precise runtime results are known. In a very early work [36], it was shown that the (1+1) EA with mutation rate ğ‘ğ‘› optimizes the ONEMAX function in an expected time of (1+ğ‘œ(1))ğ‘’ğ‘ğ‘ğ‘›lnğ‘› and the NEEDLE function in time (1+ğ‘œ(1))11âˆ’ğ‘’ğ‘2ğ‘›. More than ten years later, in independent works [8, 67] the precise runtime of the (1+1) EA on LEADINGONES was determined; here [8] also regarded general mutation rates and deduced from their result that the optimal mutation rate of approximately 1.59ğ‘› is higher than the usual recommendation 1ğ‘›, and that a fitness dependent mutation rate gives again slightly better results (this was also the first time that a fitness dependent parameter choice was proven to be superior to static rates by at least a constant factor difference in the runtime). Precise runtime results for a broader class of (1+1)-type algorithms on LEADINGONES have recently appeared in [24]. A series of recent works [7, 22, 53] obtained precise runtimes of different hyper-heuristics on LEADINGONES and thus allowed to discriminate them by their runtime. The precise expected runtime of the (1+1) EA with general unbiased mutation operator on the PLATEAUğ‘˜ function was determined [3] to be (1+ğ‘œ(1))(ğ‘›ğ‘˜)ğ‘âˆ’11:ğ‘˜, where ğ‘1:ğ‘˜ is the probability that the mutation operator flips between one and k bits. Apparently, here the details of the mutation operator are not very important â€“ only the probability to flip between one and k bits has an influence on the runtime.

The only precise runtime analysis for an algorithm with a non-trivial population can be found in [37], where the runtime of the (1+ğœ†) EA with mutation rate ğ‘ğ‘›, c a constant, on ONEMAX was shown to be (1+ğ‘œ(1))(ğ‘’ğ‘ğ‘ğ‘›lnğ‘›+ğ‘›ğœ†lnlnğœ†2lnğœ†). This result has the surprising implication that here the mutation rate is only important when ğœ† is small.

The only precise runtime analysis for a multi-modal objective function was conducted in [20], where the runtime of the (1+1) EA with arbitrary mutation rate was determined for jump functions; this work led to the development of a heavy-tailed mutation operator that appears to be very successful [1, 32, 33, 35, 72].

In summary, there is only a small number of precise runtime analyses, but many of them could obtain insights that would not have been possible with less precise analyses.

Our result, an analysis of the (ğœ‡,ğœ†) EA on jump functions that is precise for ğ‘˜â‰¤0.1ğ‘›, ğœ†=ğ‘œ(ğ‘›ğ‘˜âˆ’1), ğœ†â‰¥(1+ğœ€)ğ‘’ğœ‡, and ğœ†=Î©(logğ‘›) sufficiently large, is the second precise analysis for a population-based algorithm (after [37]), is the second precise analysis for a multimodal fitness function (after [20]), and is the first precise analysis for a non-elitist algorithm (apart from fact that the result [37] could be transfered to the (1,ğœ†) EA for large ğœ† via the argument [46] that in this case the (1+ğœ†) EA and the (1,ğœ†) EA have essentially identical performances).

Methods: Negative Drift and Level-based Analyses
To obtain our results, we also develop new techniques for two classic topics, namely the analysis of processes showing a drift away from the target (â€œnegative driftâ€) and the analysis of non-elitist population processes via level-based arguments.

Negative Drift
It is natural that a stochastic process ğ‘‹0,ğ‘‹1,â€¦ finds it hard to reach a certain target when the typical behavior is taking the process away from the target. Negative drift theorems are an established tool for the analysis of such situations. They roughly speaking state the following. Assume that the process starts at some point b or higher, that is, ğ‘‹0â‰¥ğ‘, and that we aim at reaching a target ğ‘<ğ‘. Assume that whenever the process is above the target value, that is, ğ‘‹ğ‘¡>ğ‘, we have an expected progress ğ¸[ğ‘‹ğ‘¡+1âˆ’ğ‘‹ğ‘¡]â‰¥ğ›¿, ğ›¿ some constant, away from the target, and that this progress satisfies some concentration assumption like two-sided exponential tails. Then the expected time to reach or undershoot the target is at least exponential in the distance ğ‘âˆ’ğ‘.

The first such result in the context of evolutionary algorithms was shown by Oliveto and Witt [60] (note the corrigendum [61]). Improved versions were subsequently given in [49, 55, 62, 65, 71]. The comprehensive survey [52, Section 2.4.3] gives a complete coverage of this topic. What is important to note for our purposes is that (i) all existing negative drift results are quite technical to use due to the concentration assumptions, that (ii) they all give a lower bound that is only exponential in the length of the interval in which the (constant) negative drift is observed, and that (iii) they all (apart from the technical work of Hajek [38]) do not give tight bounds, but only bounds of type exp(Î©(ğ‘âˆ’ğ‘)) with the implicit constant in the exponent not specified.

Earlier than the general negative drift theorem, Lehre [50] proved a negative drift theorem for population-based processes via multi-type branching processes. Just as the general negative drift theorems described above, it only gives lower bounds exponential in the length of the negative drift regime and the base of the exponential function is not made explicit. Consequently, in [50, Theorem 5] (Theorem 1 in this work), only an exp(Î©(ğ‘˜)) lower bound for the runtime of the (ğœ‡,ğœ†) EA on JUMPğ‘›ğ‘˜ was derived

Since we aim at an Î©(ğ‘›ğ‘˜) lower bound caused by a negative drift in the short gap region (of length k) of the jump function, and since further we aim at results that give the precise leading constant of the runtime, we cannot use these tools. We therefore resort to the additive drift applied to a rescaled process argument first made explicit in [6]. The basic idea is very simple: For a suitable function ğ‘”:â„â†’â„ one regards the process (ğ‘”(ğ‘‹ğ‘¡))ğ‘¡ instead of the original process (ğ‘‹ğ‘¡)ğ‘¡, shows that it makes at most a slow progress towards the target, say ğ¸[ğ‘”(ğ‘‹ğ‘¡+1)âˆ’ğ‘”(ğ‘‹ğ‘¡)âˆ£ğ‘‹ğ‘¡>ğ‘]â‰¥âˆ’ğ›¿, and concludes from the classic additive drift theorem [42] (Theorem 6 in this work) that the expected time to reach or undershoot a when starting at b is at least ğ‘”(ğ‘)âˆ’ğ‘”(ğ‘)ğ›¿. While the basic approach is simple and natural, the non-trivial part is finding a rescaling function g which both gives at most a slow progress towards the target and gives a large difference ğ‘”(ğ‘)âˆ’ğ‘”(ğ‘). The rescalings used in [6] and [25] were both of exponential type, that is, g was roughly speaking an exponential function. By construction, they only led to lower bounds exponential in ğ‘âˆ’ğ‘, and in both cases the lower bound was not tight (apart from being exponential).

Our progress: Hence the technical novelty of this work is that we devise a rescaling for our problem that (i) leads to a lower bound of order ğ‘›ğ‘˜ for a process having negative drift only is an interval of length k, and (ii) such that these lower bounds are tight including the leading constant. Clearly, our rescalings (as all rescalings used previously) are specific to our problem. Nevertheless, they demonstrate that the rescaling method, different from the classic negative drift theorems, can give very tight lower bounds and lower bounds that are super-exponential in the length of the interval in which the negative drift is observed. We are optimistic that such rescalings will find other applications in the future.

Note added in proof: For reasons of completeness of this discussion on lower bound methods, we note that between the submission of this work in May 2020 and the first notification in June 2021, a further lower bound method called negative multiplicative drift was proposed [28]. Different from what a reviewer suggests, it is not in any way related to the rescaling method. While we do not want to rule out that it can also be employed to prove our lower bound, it is clear that this would either also need a rescaling of the process (and then our approach appears more direct) or it would need estimates on the change of the maximum ONEMAX-value in the population that are substantially different from ours in the proof of Theorem 8.

Level-Based Analyses
While level-based arguments for the analysis of non-elitist algorithms have been used much earlier, see, e.g., [31], the fitness-level analysis of Lehre [51] might still be the first general method to analyze non-elitist population-based processes. We gave a high-level description of this method in Sect. 2.1.2 and we will give a more detailed discussion in Sect. 6.1 to enable us to prove our upper bound. For this reason, we now explain without further explanations what is our progress over the state of the art of this method.

Similar to the state of the art in negative drift theorems, all existing variants of the level-based methods do not give results that are tight including the leading constant. Also, from the complexity of the proofs of these results, it appears unlikely that such tight results can be obtained in the near future.

Our progress: For our problem of optimizing jump functions, we can exploit the fact that the most difficult, and thus time consuming, step is generating the global optimum from a population that has fully converged into the local optimum. To do so, we use the non-tight level-based methods only up to the point when the population only consists of local optima (we call this an almost perfect population). This can be done via a variation of the existing level-based results (Corollary 13). From that point on, we estimate the remaining runtime by computing the waiting time for generating the optimum from a local optimum. Of course, since we are analyzing a non-elitist process, we are not guaranteed to keep an almost perfect population. For that reason, we also need to analyze the probability of losing an almost perfect population and to set up a restart argument to regain an almost perfect population. Naturally, this has to be done in a way that the total runtime spent here is only a lower-order fraction of the time needed to generate the global optimum from an almost perfect population.

A side effect of this approach is that we only need a logarithmic offspring population size, that is, it suffices to have ğœ†â‰¥ğ¶ln(ğ‘›) for some constant C that is independent of the jump size k. This is different from using the level-based methods for the whole process, as done in the proof of Theorem 2, which would require an offspring population size at least logarithmic in the envisaged runtime, hence here Î©(logğ‘›ğ‘˜)=ğ‘‚(ğ‘˜logğ‘›), which is super-logarithmic when k is super-constant.

While our arguments exploit some characteristics of the jump functions, we are optimistic that they can be employed for other problems as well, in particular, when the optimization process typically contains one step that is more difficult than the remaining optimization.

Preliminaries
In this section, we define the algorithm and the optimization problem regarded in this paper together with the most relevant works on these.

The (ğœ‡,ğœ†) EA
The (ğœ‡,ğœ†) EA for the maximization of pseudo-Boolean functions ğ‘“:{0,1}ğ‘›â†’â„ is made precise in Algorithm 1. It is a simple non-elitist algorithm working with a parent population of size ğœ‡ and an offspring population of size ğœ†â‰¥ğœ‡. Here and in the remainder by a population we mean a multiset of individuals (elements from the search space {0,1}ğ‘›). Each offspring is generated by selecting a random parent (independently and with replacement) from the parent population and mutating it via standard bit mutation, that is, by flipping each bit independently with probability 1/n.Footnote2 The next parent population consists of those ğœ‡ offspring which have the highest fitness (breaking ties arbitrarily).

figure a
The (ğœ‡+ğœ†) EA, to which we compare the (ğœ‡,ğœ†) EA, differs from the (ğœ‡,ğœ†) EA only in the selection of the next generation. Whereas the (ğœ‡,ğœ†) EA selects the next generation only from the offspring population (comma selection), the (ğœ‡+ğœ†) EA selects it from the parent and offspring population (plus selection). In other words, to obtain the (ğœ‡+ğœ†) EA from Algorithm 1, we only have to replace the selection by â€œselect ğ‘ƒğ‘¡ from the multi-set ğ‘ƒğ‘¡âˆ’1âˆª{ğ‘¦1,â€¦,ğ‘¦ğœ†} by choosing ğœ‡ best individuals (breaking ties arbitrarily)â€. Often, the tie breaking is done by giving preference to offspring, but for all our purposes there is no difference.

When talking about the performance of the (ğœ‡,ğœ†) EA or the (ğœ‡+ğœ†) EA, as usual in runtime analysis [2, 23, 44, 58], we count the number of fitness evaluations until for the first time an optimal solution is evaluated. We assume that each individual is evaluated immediately after being generated. Consequently, if an optimum is generated in iteration t, then the runtime T satisfies

ğœ‡+(ğ‘¡âˆ’1)ğœ†+1â‰¤ğ‘‡â‰¤ğœ‡+ğ‘¡ğœ†.
(1)
Since we described the most important results on the (ğœ‡,ğœ†) EA already in Sect. 2.1, let us briefly mention the most relevant results for the (ğœ‡+ğœ†) EA. Again, due to the difficulties in analyzing population-based algorithms, not too much is known. The runtimes of the (1+ğœ†) EA, among others on ONEMAX and LEADINGONES, were first analyzed in [45]. The asymptotically tight runtime on ONEMAX for all polynomial ğœ† was determined in [16], together with an analysis on general linear functions. In [69], the runtime of the (ğœ‡+1) EA on ONEMAX and LEADINGONES, among others, was studied. The runtime of the (ğœ‡+ğœ†) EA with both non-trivial parent and offspring population sizes on the ONEMAX function was determined in [4].

The Jump Function Class
To define the jump functions, we first recall that the n-dimensional ONEMAX function is defined by

OM(ğ‘¥)=â€–ğ‘¥â€–1=âˆ‘ğ‘–=1ğ‘›ğ‘¥ğ‘–
for all ğ‘¥âˆˆ{0,1}ğ‘›

Now the n-dimensional jump function with jump parameter (jump size) ğ‘˜âˆˆ[1..ğ‘›] is defined by

JUMPğ‘›ğ‘˜(ğ‘¥)={â€–ğ‘¥â€–1+ğ‘˜ğ‘›âˆ’â€–ğ‘¥â€–1 if â€–ğ‘¥â€–1âˆˆ[0..ğ‘›âˆ’ğ‘˜]âˆª{ğ‘›}, if â€–ğ‘¥â€–1âˆˆ[ğ‘›âˆ’ğ‘˜+1..ğ‘›âˆ’1].
Hence for ğ‘˜=1, we have a fitness landscape isomorphic to the one of ONEMAX, but for larger values of k there is a fitness valley (â€œgapâ€)

ğºğ‘›ğ‘˜:={ğ‘¥âˆˆ{0,1}ğ‘›âˆ£ğ‘›âˆ’ğ‘˜<â€–ğ‘¥â€–1<ğ‘›}
consisting of the ğ‘˜âˆ’1 highest sub-optimal fitness levels of the ONEMAX function. This valley is hard to cross for evolutionary algorithms using standard bit mutation. When using the common mutation rate 1ğ‘›, the probability to generate the optimum from a parent on the local optimum is only ğ‘ğ‘˜:=(1âˆ’1ğ‘›)ğ‘›âˆ’ğ‘˜ğ‘›âˆ’ğ‘˜<ğ‘›âˆ’ğ‘˜. For this reason, e.g., the classic (ğœ‡+ğœ†) EA has a runtime of at least ğ‘›ğ‘˜ when k is not excessively large. This was proven formally for the (1+1) EA in the classic paper [15], but the argument can easily be extended to all (ğœ‡+ğœ†) EAs (as we do now for reasons of completeness). We also prove an upper bound, which will later turn out to agree with our lower bound for the (ğœ‡,ğœ†) EA for large ranges of the parameters.

Theorem 3
Let ğœ‡,ğœ†âˆˆâ„¤â‰¥1. Let ğ‘›âˆˆâ„¤â‰¥2 and ğ‘˜âˆˆ[2..ğ‘›]. Let ğ‘ğ‘˜:=(1âˆ’1ğ‘›)ğ‘›âˆ’ğ‘˜ğ‘›âˆ’ğ‘˜. Let T denote the runtime, measured by the number of fitness evaluations until the optimum is found, of the (ğœ‡+ğœ†) EA on the JUMPğ‘›ğ‘˜ function.

(i)
Let â„(ğ‘›):=2ğ‘›log(ğœ‡ğ‘›)â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš). If ğ‘˜â‰¤ğ‘›2âˆ’â„(ğ‘›), then

ğ¸[ğ‘‡]â‰¥(1âˆ’1ğ‘›)(ğœ‡+1ğ‘ğ‘˜),
otherwise ğ¸[ğ‘‡]â‰¥(1âˆ’1ğ‘›)(ğœ‡+1ğ‘ğ‘˜â€²) with ğ‘˜â€²:=ğ‘›2âˆ’â„(ğ‘›).

(ii)
ğ¸[ğ‘‡]â‰¤1ğ‘ğ‘˜+ğ‘‚(ğ‘›logğ‘›+ğ‘›ğœ‡+ğ‘›ğœ†log+log+(ğœ†/ğœ‡)log+(ğœ†/ğœ‡)+(ğœ‡+ğœ†)logğœ‡), where we write log+ğ‘¥:=max{1,lnğ‘¥} for all ğ‘¥>0. If ğœ‡â‰¤ğœ†, ğœ†=exp(ğ‘‚(ğ‘›)), and ğœ†=ğ‘œ(1ğ‘›ğ‘ğ‘˜), then ğ¸[ğ‘‡]â‰¤(1+ğ‘œ(1))1ğ‘ğ‘˜.

Proof
To cover both cases, let ğ‘˜â€²=min{ğ‘˜,ğ‘›2âˆ’â„(ğ‘›)}. Using the additive Chernoff bound (Theorem 5) and a union bound, we see that with probability at least

1âˆ’ğœ‡exp(âˆ’(ğ‘›2âˆ’ğ‘˜â€²)22ğ‘›)â‰¥1âˆ’ğœ‡exp(âˆ’â„(ğ‘›)22ğ‘›)â‰¥1âˆ’1ğ‘›
all ğœ‡ initial individuals x satisfy OM(ğ‘¥)â‰¤ğ‘›âˆ’ğ‘˜â€². Conditioning on this, in the remaining run all individuals that are taken into the parent population also satisfy OM(ğ‘¥)â‰¤ğ‘›âˆ’ğ‘˜â€² (unless they are the optimum). Consequently, for an offspring to become the first optimum sampled, there is a unique set of â„“â‰¥ğ‘˜â€² bits in the parent that need to be flipped (and the other bits may not be flipped). The probability for this event is (1âˆ’1ğ‘›)ğ‘›âˆ’â„“(1ğ‘›)â„“â‰¤(1âˆ’1ğ‘›)ğ‘›âˆ’ğ‘˜â€²(1ğ‘›)ğ‘˜â€²=ğ‘ğ‘˜â€². Hence the time until this happens is stochastically dominated (see, e.g., [24]) by a geometric distribution with success probability ğ‘ğ‘˜â€², which has an expectation of 1ğ‘ğ‘˜â€². Together with the ğœ‡ initial fitness evaluations, this shows the lower bound.

For the upper bound, we use a recent analysis of the runtime of the (ğœ‡+ğœ†) EA on ONEMAX. In [4], it was shown that the (ğœ‡+ğœ†) EA finds the optimum of ONEMAX in an expected number of

ğ‘‚(ğ‘›logğ‘›+ğ‘›ğœ‡+ğ‘›ğœ†log+log+(ğœ†/ğœ‡)log+(ğœ†/ğœ‡))
fitness evaluations (the result is stated in terms of iterations in [4], but with (1) one immediately obtains the form above). It is easy to see from the proof in [4] that this bound also holds for the expected time until the (ğœ‡+ğœ†) EA optimizing any jump function as found an individual on the local optimum (if it has not found the optimum before).

What cannot be taken immediately from the previous work is remainder of the runtime analysis. In particular, since generating the optimum from the local optimum is more difficult than generating an individual on the next ONEMAX fitness level, we need a larger number of individuals on the local optimum before we have a reasonable chance of making progress. Since we mostly aim at a good upper bound in the regime where ğœ‡ and ğœ† are not excessively large, we allow for the time until the whole population is on the local optimum or better. By [66, Lemma 2], this takeover time is ğ‘‚((ğœ‡+ğœ†)logğœ‡) fitness evaluations (or the optimum is found earlier). From this point on, any further individual has a probability of exactly ğ‘ğ‘˜ of being the optimum, giving an additional 1ğ‘ğ‘˜ term for the runtime bound. This shows the general upper bound. If ğœ‡â‰¤ğœ†, ğœ†=exp(ğ‘‚(ğ‘›)) and ğœ†=ğ‘œ(1ğ‘›ğ‘ğ‘˜), then (ğœ‡+ğœ†)logğœ‡=ğ‘‚(ğœ†logğœ†)=ğ‘‚(ğ‘›ğœ†)=ğ‘œ(1/ğ‘ğ‘˜). For similar reasons, the expressions ğ‘›ğœ‡ and ğ‘›ğœ†log+log+(ğœ†/ğœ‡)log+(ğœ†/ğœ‡) are of lower order. Since ğ‘˜â‰¥2, we have ğ‘ğ‘˜=Î©(ğ‘›2), and thus also the ğ‘›logğ‘› expression is of lower order. â—»

By using larger mutation rates or a heavy-tailed mutation operator, the runtime of the (1+1) EA can be improved by a factor of ğ‘˜Î˜(ğ‘˜) [20], but the runtime remains Î©(ğ‘›ğ‘˜) for k constant.

Asymptotically better runtimes can be achieved when using crossover, though this is not as easy as one might expect. The first work in this direction [47], among other results, showed that a simple (ğœ‡+1) genetic algorithm using uniform crossover with rate ğ‘ğ‘=ğ‘‚(1ğ‘˜ğ‘›) has an ğ‘‚(ğœ‡ğ‘›2ğ‘˜3+22ğ‘˜ğ‘âˆ’1ğ‘) runtime when the population size is at least ğœ‡=Î©(ğ‘˜logğ‘›). A shortcoming of this result, as noted by the authors, is that it only applies to uncommonly small crossover rates. Using a different algorithm that first applies crossover and then mutation, a runtime of ğ‘‚(ğ‘›ğ‘˜âˆ’1logğ‘›) was achieved by Dang et al. [13, Theorem 2]. For ğ‘˜â‰¥3, the logarithmic factor in the runtime can be removed by using a higher mutation rate. With additional diversity mechanisms, the runtime can be further reduced down to ğ‘‚(ğ‘›logğ‘›+4ğ‘˜), see [12]. The (1+(ğœ†,ğœ†)) GA can optimize JUMPğ‘˜ in time ğ‘‚(ğ‘›(ğ‘˜+1)/2ğ‘˜âˆ’Î©(ğ‘˜)) [5].

With a three-parent majority vote crossover, among other results, a runtime of ğ‘‚(ğ‘›logğ‘›) could be obtained via a suitable island model for all ğ‘˜=ğ‘‚(ğ‘›1/2âˆ’ğœ€) [34]. A different voting algorithm also giving an ğ‘‚(ğ‘›logğ‘›) runtime was proposed in [64]. Via a hybrid genetic algorithm using as variation operators only local search and a deterministic voting crossover, an O(n) runtime was shown in [73].

Runtimes of ğ‘‚(ğ‘›(ğ‘›ğ‘˜)) and ğ‘‚(ğ‘˜log(ğ‘›)(ğ‘›ğ‘˜)) were shown for the (1+1) IAhyp and the (1+1) Fast-IA artificial immune systems, respectively [10, 11]. In [54], the runtime of a hyper-heuristic switching between elitist and non-elitist selection was studied. The lower bound of order Î©(ğ‘›logğ‘›)+exp(Î©(ğ‘˜)) and the upper bound of order ğ‘‚(ğ‘›2ğ‘˜âˆ’1/ğ‘˜), however, are too far apart to indicate an advantage or a disadvantage over most classic algorithms. In this work, it is further stated that the Metropolis algorithm (using the 1-bit neighborhood) has an exp(Î©(ğ‘›)) runtime on jump functions.

Without diversity mechanisms and non-standard operators, the compact genetic algorithm, a simple estimation-of-distribution algorithm, has a runtime of ğ‘‚(ğ‘›logğ‘›+2ğ‘‚(ğ‘˜)) [26, 41].

Technical Tools
In this section, we collect a few technical tools that will be used in our proofs. All but the last one, an elementary non-asymptotic lower bound for the probability to generate an offspring with equal ONEMAX fitness, are standard tools in the field.

Let X be a binomially distributed random variable with parameters n and p, that is, ğ‘‹=âˆ‘ğ‘›ğ‘–=1ğ‘‹ğ‘– with independent ğ‘‹ğ‘– satisfying Pr[ğ‘‹ğ‘–=1]=ğ‘ and Pr[ğ‘‹ğ‘–=0]=1âˆ’ğ‘. Since X is a sum of independent binary random variables, Chernoff bounds can be used to bound its deviation from the expectation. However, the following elementary estimate also does a good job. This estimate appears to be well-known (e.g., it was used in [45] without proof or reference). Elementary proofs can be found in [37, Lemma 3] or [29, Lemma 1.10.37].

Lemma 4
Let ğ‘‹âˆ¼Bin(ğ‘›,ğ‘). Let ğ‘˜âˆˆ[0..ğ‘›]. Then

Pr[ğ‘‹â‰¥ğ‘˜]â‰¤(ğ‘›ğ‘˜)ğ‘ğ‘˜.
The following additive Chernoff bound from Hoeffding [40], also to be found, e.g., in [29, Theorem 1.10.7], provides a different way to estimate the probability that a binomial random variable and, in fact, any sum of bounded independent random variables exceeds its expectation.

Theorem 5
Let ğ‘‹1,â€¦,ğ‘‹ğ‘› be independent random variables taking values in [0, 1]. Let ğ‘‹=âˆ‘ğ‘›ğ‘–=1ğ‘‹ğ‘–. Then for all ğœ†â‰¥0,

Pr[ğ‘‹â‰¥ğ¸[ğ‘‹]+ğœ†]â‰¤exp(âˆ’2ğœ†2ğ‘›).
As part of our additive drift with rescaling lower bound proof strategy, we need the following additive drift theorem of He and Yao [42], see also [52, Theorem 2.3.1], which allows to translate a uniform upper bound on an expected progress into a lower bound on the expected time to reach a target.

Theorem 6
Let ğ‘†âŠ†â„â‰¥0 be finite and 0âˆˆğ‘†. Let ğ‘‹0,ğ‘‹1,â€¦ be a stochastic process taking values in S. Let ğ›¿>0. Let ğ‘‡=inf{ğ‘¡â‰¥0âˆ£ğ‘‹ğ‘¡=0}. If for all ğ‘¡â‰¥0 and all ğ‘ âˆˆğ‘†âˆ–{0} we have ğ¸[ğ‘‹ğ‘¡âˆ’ğ‘‹ğ‘¡+1âˆ£ğ‘‹ğ‘¡=ğ‘ ]â‰¤ğ›¿, then ğ¸[ğ‘‡]â‰¥ğ¸[ğ‘‹0]ğ›¿.

Finally, we shall use occasionally the following lower bound on the probability that standard bit mutation creates from a parent x with OM(ğ‘¥)<ğ‘› an offspring with equal ONEMAX-value. The main difference to the usual estimate (1âˆ’1ğ‘›)ğ‘›=(1âˆ’ğ‘œ(1))1ğ‘’, which is the probability to recreate the parent, is that our lower bound is exactly 1ğ‘’, which avoids having to deal with asymptotic notation.

Lemma 7
Let ğ‘¥âˆˆ{0,1}ğ‘› with 0<OM(ğ‘¥)<ğ‘›. Let y be obtained from x via standard bit mutation with mutation rate 1ğ‘›. Then Pr[OM(ğ‘¦)=OM(ğ‘¥)]â‰¥1ğ‘’.

Proof
Let ğ‘˜:=OM(ğ‘¥). For y to have this same OM-value, it suffices that either no bit in x is flipped or that exactly one zero-bit and exactly one one-bit are flipped. The probability for this event is (1âˆ’1ğ‘›)ğ‘›+ğ‘˜(ğ‘›âˆ’ğ‘˜)ğ‘›2(1âˆ’1ğ‘›)ğ‘›âˆ’2â‰¥(1âˆ’1ğ‘›)ğ‘›+ğ‘›âˆ’1ğ‘›2(1âˆ’1ğ‘›)ğ‘›âˆ’2=(1âˆ’1ğ‘›)ğ‘›âˆ’1â‰¥1ğ‘’. â—»

A Lower Bound for the Runtime of the (ğœ‡,ğœ†) EA on Jump Functions
In this section, we prove our main result, a lower bound for the runtime of the (ğœ‡,ğœ†) EA on jump functions which shows that for a large range of parameter values, the (ğœ‡,ğœ†) EA cannot even gain a constant factor speed-up over the (ğœ‡+ğœ†) EA. With its Î©(ğ‘›ğ‘˜) order of magnitude, our result improves significantly over the only previous result on this problem, the exp(Î©(ğ‘˜)) lower bound in [50] (Theorem 1 in this work).

Before stating the precise result, we quickly discuss two situations which, in the light of previous results, do not appear overly interesting and for which we therefore did not make an effort to fully cover them by our result.

When ğœ†â‰¤(1âˆ’ğœ€)ğ‘’ğœ‡ for an arbitrarily small constant ğœ€â‰¥0 and ğœ† is at most polynomial in n, the results of Lehre [50] imply that the (ğœ‡,ğœ†) EA has an exponential runtime on any function with a polynomial number of optima (and consequently, also on jump functions). We guess that the restriction to polynomial-size ğœ† was made in [50, Corollary 1] only for reasons of mathematical convenience (together with the fact that super-polynomial population sizes raise some doubts on the implementability and practicability of the algorithm). We do not see any reason why Lehreâ€™s result, at least in the case of the (ğœ‡,ğœ†) EA, should not be true for any value of ğœ† (possibly with a sub-exponential number of iterations, but still an exponential number of fitness evaluations).

Rowe and Sudholt [65, Theorem 10] showed that for all constants ğœ€>0 the (1,ğœ†) EA with population size ğœ†â‰¤(1âˆ’ğœ€)logğ‘’ğ‘’âˆ’1ğ‘› has an expected optimization time of at least exp(Î©(ğ‘›ğœ€/2)) on any function ğ‘“:{0,1}ğ‘›â†’â„ with a unique optimum. From inspecting the proof given in [65], we strongly believe that the same result also holds for the (ğœ‡,ğœ†) EA. Since this is not central to our work, we do not give a rigorous proof. Our main argument would be that the runtime of the (ğœ‡,ğœ†) EA on a jump function is at least (in the strong sense of stochastic domination) its runtime on ONEMAX. This follows from a coupling arguments similar to the one given in [24, Proof of Theorem 23]. More precisely, when comparing how offspring are selected in the JUMP and in the ONEMAX process, we can construct a coupling such that the parent individuals in the JUMP process always are not closer to the optimum than in the ONEMAX process. Now comparing how the (1,ğœ†) EA and the (ğœ‡,ğœ†) EA optimize ONEMAX, we see that the (ğœ‡,ğœ†) EA may select worse parent individuals than the (1,ğœ†) EA, which generate (in the stochastic domination sense) worse offspring, leading to a larger optimization time. As said, we do not declare this a complete proof, but since the case of small population sizes might generally not be too interesting and since our not fully rigorous analysis indicates that an interesting performance of the (ğœ‡,ğœ†) EA is not to be expected here, we refrain from giving a complete analysis.

Let us declare the parameter settings just discussed as not so interesting since previous works show or strongly indicate that the (ğœ‡,ğœ†) EA is highly inefficient on any objective function with unique optimum. Let us further declare exponential population sizes as not so interesting (mostly for reasons of implementability, but also because Lemma 10 will show that they imply exponential runtimes). With this language, our following result shows that the runtime of the (ğœ‡,ğœ†) EA on jump functions with jump size ğ‘˜â‰¤0.1ğ‘› for all interesting parameter choices is, apart from lower order terms, at least the one of the (ğœ‡+ğœ†) EA. For ğ‘˜>0.1ğ‘›, this runtime is at least ğ‘›Î©(ğ‘›).

Theorem 8
Let ğ‘â‰¤0.1 and C be large enough such that (4ğ‘)ğ¶/2â‰¤ğ‘’âˆ’2. Let ğ‘›â‰¥2ğ‘. Let ğ¶ln(ğ‘›)â‰¤ğœ†â‰¤23exp(0.16ğ‘›) and ğœ‡â‰¤ğœ†2. Let ğ‘â€²=1ğ‘’+ğ‘ and â„(ğ‘›,ğœ†):=exp(âˆ’(1âˆ’2ğ‘â€²)22ğœ†)+2ğ‘›âˆ’1ğ‘›2âˆ’ğ‘›. Let ğ‘˜âˆˆ[2..ğ‘›] and ğ‘ğ‘˜:=(1âˆ’1ğ‘›)ğ‘›âˆ’ğ‘˜ğ‘›âˆ’ğ‘˜.

If ğ‘˜â‰¤ğ‘ğ‘›, then the expected runtime, measured by the number of fitness evaluations until the optimum is evaluated, of the (ğœ‡,ğœ†) EA on jump functions with jump size k is at least

ğ‘‡ğ‘˜:=(1âˆ’exp(âˆ’0.16ğ‘›))(ğœ‡+(1âˆ’â„(ğ‘›,ğœ†))1ğ‘ğ‘˜)=(1âˆ’ğ‘œ(1))(ğœ‡+1ğ‘ğ‘˜),
where the asymptotic expression is for ğ‘›â†’âˆ and ğœ†=ğœ”(1).

For ğ‘˜>ğ‘ğ‘›, the expected runtime is at least ğ‘‡âŒŠğ‘ğ‘›âŒ‹.

We phrased our result in the above form since we felt that it captures best the most interesting aspect, namely a runtime of essentially 1ğ‘ğ‘˜ when ğ‘˜â‰¤0.1ğ‘› and ğœ†=Î©(logğ‘›) suitably large. Since our result is non-asymptotic, both c and C do not have to be constants. Hence if we are interested in the smallest possible value for ğœ† that gives an (1âˆ’ğ‘œ(1))1ğ‘ğ‘˜ runtime, then by taking ğ‘=ğ‘˜ğ‘› and ğ¶=4/ln(ğ‘›/(4ğ‘˜)), we obtain the following result.

Corollary 9
Let ğ‘˜â‰¥2. Let ğ‘›â‰¥10ğ‘˜ and

4ln(ğ‘›4ğ‘˜)ln(ğ‘›)â‰¤ğœ†â‰¤23exp(0.16ğ‘›).
Let ğœ‡â‰¤ğœ†2. Let ğ‘â€²=1ğ‘’+ğ‘˜ğ‘›. With â„(ğ‘›,ğœ†):=exp(âˆ’(1âˆ’2ğ‘â€²)22ğœ†)+2ğ‘›âˆ’1ğ‘›2âˆ’ğ‘› and ğ‘ğ‘˜:=(1âˆ’1ğ‘›)ğ‘›âˆ’ğ‘˜ğ‘›âˆ’ğ‘˜, the expected runtime of the (ğœ‡,ğœ†) EA on JUMPğ‘›ğ‘˜ is at least

ğ‘‡ğ‘˜:=(1âˆ’exp(âˆ’0.16ğ‘›))(ğœ‡+(1âˆ’â„(ğ‘›,ğœ†))1ğ‘ğ‘˜)=(1âˆ’ğ‘œ(1))(ğœ‡+1ğ‘ğ‘˜),
where the asymptotic expression holds for ğ‘›â†’âˆ and ğœ†=ğœ”(1).

In particular, if ğ‘˜=ğ‘‚(ğ‘›1âˆ’ğœ€) for a constant ğœ€>0, then it suffices to have ğœ†=ğœ”(1) for the lower bound (1âˆ’ğ‘œ(1))(ğœ‡+1ğ‘ğ‘˜) to hold.

Before giving the precise proof of Theorem 8, let us briefly explain the main ideas. As discussed earlier, this proof is an example for proving lower bounds by applying the additive drift theorem to a suitable rescaling of a natural potential function. As this work shows, this method can give very tight lower bounds, different from, say, negative drift theorems.

The heart, and art, of this method is defining a suitable potential function. The observation that the difficult part of the optimization process is traversing the region {ğ‘¥âˆˆ{0,1}ğ‘›âˆ£OM(ğ‘¥)âˆˆ[ğ‘›âˆ’ğ‘˜..ğ‘›]} together with the fact that the lower bound given by the additive drift theorem depends on the difference in potential of starting point and target suggested to us the following potential function. For a population P, let OM(ğ‘ƒ) denote the maximum ONEMAX-value in the population. For OM(ğ‘ƒ)>ğ‘›âˆ’ğ‘˜, the potential of P will essentially be min{ğ‘›OM(ğ‘ƒ)âˆ’(ğ‘›âˆ’ğ‘˜),1ğœ†ğ‘ğ‘˜}. All other populations have a potential of zero. This definition gives the desired large potential range of 1ğœ†ğ‘ğ‘˜ and, after proving that the expected potential gain is at most one and the initial potential is zero with high probability, gives the desired lower bound on the runtime. The proof below gives more details, including the precise definition of the potential, which is minimally different from this simplified description.

Since the classic definition of the runtime of an evolutionary algorithm, the number of fitness evaluations until the optimum is evaluated, implies that we usually lose a number of ğœ†âˆ’1 fitness evaluations when translating iterations into fitness evaluations (since we usually cannot rule out that the optimum is sampled as the first individual generated in the iteration which finds the optimum, see (1)), we add a short argument to the proof of Theorem 8 to gain an extra ğœ† fitness evaluations. The main observation is that both the ğœ‡ initial individuals and the ğœ† offspring of the first generation are uniformly distributed in the search space. This makes them very unlikely to be the optimum, and this argument (with some finetuning) allows us to start the drift argument from the second iteration on.

We now give the complete proof of our lower bound result.

Proof of Theorem 8
For a unified proof for the two cases that k is at most cn or greater than cn, let us denote by ğ‘˜â€² the jump size and recall that this can be a function of n. Let ğ‘˜:=min{ğ‘˜â€²,âŒŠğ‘ğ‘›âŒ‹}. Assume that n and ğœ† are large enough so that â„(ğ‘›,ğœ†)<1, as otherwise there is nothing to show.

Let ğ‘”max=(1âˆ’â„(ğ‘›,ğœ†))1ğœ†ğ‘ğ‘˜. Note that by our assumption, ğ‘”max>0. Also, we easily see that ğ‘”maxâ‰¤ğ‘›ğ‘˜: If ğœ†â‰¥3, then ğ‘”maxâ‰¤1ğœ†ğ‘ğ‘˜â‰¤1ğœ†ğ‘›âˆ’ğ‘˜/ğ‘’â‰¤ğ‘›ğ‘˜; however, if ğœ†=2, the only other option leaving us with a positive integral ğœ‡, then â„(ğ‘›,ğœ†)â‰¥exp(âˆ’(1âˆ’2/ğ‘’)2)>0.93 and again ğ‘”max=(1âˆ’â„(ğ‘›,ğœ†))1ğœ†ğ‘ğ‘˜â‰¤0.071ğœ†ğ‘›âˆ’ğ‘˜/ğ‘’â‰¤ğ‘›ğ‘˜.

For all ğ¿âˆˆ[1..ğ‘˜], let ğ‘”(ğ¿):=min{ğ‘›ğ¿,ğ‘”max}, and let ğ‘”(0):=0.

Let ğ‘˜âˆ— be the smallest integer in [1..k] such that ğ‘”(ğ‘˜âˆ—)=ğ‘”max. Note that ğ‘˜âˆ— is well defined since ğ‘”maxâ‰¤ğ‘›ğ‘˜.

For all individuals ğ‘¥âˆˆ{0,1}ğ‘›, denote by

OM(ğ‘¥)â„“(ğ‘¥)ğ‘”(ğ‘¥):=âˆ‘ğ‘–=1ğ‘›ğ‘¥ğ‘–âˆˆ[0..ğ‘›] itsONEMAX -value,:=max{0,OM(ğ‘¥)âˆ’(ğ‘›âˆ’ğ‘˜)}âˆˆ[0..ğ‘˜],:=ğ‘”(â„“(ğ‘¥))âˆˆ{0,ğ‘›,ğ‘›2,â€¦,ğ‘›ğ‘˜,ğ‘”max}âˆ©[0,ğ‘”max].
For a population P, that is, a multiset of individuals, we write

OM(ğ‘ƒ)â„“(ğ‘ƒ)ğ‘”(ğ‘ƒ):=max{OM(ğ‘¥)âˆ£ğ‘¥âˆˆğ‘ƒ},:=max{â„“(ğ‘¥)âˆ£ğ‘¥âˆˆğ‘ƒ}=max{0,OM(ğ‘ƒ)âˆ’(ğ‘›âˆ’ğ‘˜)},:=max{ğ‘”(ğ‘¥)âˆ£ğ‘¥âˆˆğ‘ƒ}=ğ‘”(â„“(ğ‘ƒ)).
We use g(P) as a measure for the quality of the current population of the algorithm. We shall argue that we typically start with ğ‘”(ğ‘ƒ)=0 and that one iteration increases g(P) in expectation by at most 1. Since we have ğ‘”(ğ‘ƒ)=ğ‘”max if P contains the optimum, the additive drift theorem yields that it takes an expected number of at least ğ‘”(ğ‘˜âˆ—) iterations to find the optimum. Let us make these arguments precise.

We first show that with high probability both the initial population ğ‘ƒ0 and the first offspring population (and, consequently, also ğ‘ƒ1) contain no individual x with OM(ğ‘¥)>ğ‘›âˆ’ğ‘˜. For this, we first observe that trivially the random initial individuals are uniformly distributed in {0,1}ğ‘›. We recall that if ğ‘¥âˆˆ{0,1}ğ‘› is uniformly distributed, then this is equivalent to saying that the random variables ğ‘¥1,ğ‘¥2,â€¦,ğ‘¥ğ‘› are independent and uniformly distributed in {0,1}. Interestingly, also the individuals of the first offspring population are uniformly distributed, since they are generated from taking a random individual by flipping each bit independently with probability 1ğ‘›. Consequently, all their bits are independent and uniformly distributed in {0,1} (the different offspring are not independent, but this independence is not needed in the following). By the additive Chernoff bound (Theorem 5), the probability that a random individual x satisfies OM(ğ‘¥)>(1âˆ’ğ‘)ğ‘› is at most exp(âˆ’2(0.5âˆ’ğ‘)2ğ‘›2/ğ‘›)=exp(âˆ’0.32ğ‘›) by our choice of c. Since ğœ‡â‰¤ğœ†2 and ğœ†â‰¤23exp(0.16ğ‘›), a simple union bound over the ğœ‡ initial individuals and the ğœ† offspring shows that with probability at least 1âˆ’(ğœ‡+ğœ†)exp(âˆ’0.32ğ‘›)â‰¥1âˆ’exp(âˆ’0.16ğ‘›), none of these individuals x satisfies OM(ğ‘¥)>ğ‘›âˆ’ğ‘˜. In this case, the population ğ‘ƒ1 satisfies ğ‘”(ğ‘ƒ1)=0 and up to this point, the algorithm has used ğœ‡+ğœ† fitness evaluations without ever sampling the optimum.

We now analyze the effect of one iteration. Let P be a population of ğœ‡ elements that does not contain the optimum. Let ğ‘ƒâ€² be the (random) offspring population generated in one iteration of the (ğœ‡,ğœ†) EA started with P, and let ğ‘ƒâ€³ be the next parent population, that is, the random population generated from selecting ğœ‡ best individuals from ğ‘ƒâ€².

Let ğ‘–:=OM(ğ‘ƒ) and ğ‘—>max{ğ‘–,ğ‘›âˆ’ğ‘˜}. Hence if OM(ğ‘ƒâ€³)=ğ‘—, then â„“(ğ‘ƒâ€³)>â„“(ğ‘ƒ), and in the case that ğ‘–<ğ‘›âˆ’ğ‘˜+ğ‘˜âˆ— we have made a true progress with respect to our measure g(P).

For this reason, we now compute the probability that OM(ğ‘ƒâ€³)=ğ‘—.

We consider first the case ğ‘—=ğ‘›. Note that here OM(ğ‘ƒâ€²)=ğ‘— implies OM(ğ‘ƒâ€³)=ğ‘—. Let x be an element of P and y be an offspring generated from x by mutation. Then Pr[ğ‘¦=(1,â€¦,1)]=(1âˆ’1ğ‘›)OM(ğ‘¥)ğ‘›âˆ’(ğ‘›âˆ’OM(ğ‘¥))â‰¤(1âˆ’1ğ‘›)ğ‘–ğ‘›âˆ’(ğ‘›âˆ’ğ‘–), using that ğ‘›â‰¥2ğ‘â‰¥2. By a union bound over the ğœ† offspring, we have OM(ğ‘ƒâ€²)=OM(ğ‘ƒâ€³)=ğ‘› with probability at most

Pr[OM(ğ‘ƒâ€³)=ğ‘›]â‰¤ğœ†(1âˆ’1ğ‘›)ğ‘–ğ‘›âˆ’(ğ‘›âˆ’ğ‘–).
(2)
Let now ğ‘—<ğ‘›. By the definitions of the (ğœ‡,ğœ†) EA and the jump functions, we have OM(ğ‘ƒâ€³)=ğ‘— only if ğ‘ƒâ€² contains at least ğœ†âˆ’ğœ‡+1 individuals y with OM(ğ‘¦)âˆˆ[ğ‘—..ğ‘›âˆ’1]. To obtain an upper bound for Pr[OM(ğ‘ƒâ€³)=ğ‘—] we regard the (slightly larger) event îˆ± that ğ‘ƒâ€² contains at least ğœ†âˆ’ğœ‡+1 individuals y with OM(ğ‘¦)âˆˆ[ğ‘—..ğ‘›].

To analyze this event îˆ±, let again ğ‘¥âˆˆğ‘ƒ and y be a mutation-offspring generated from x. By a natural domination argument [70, Lemma 6.1], the probability of the event OM(ğ‘¦)â‰¥ğ‘— does not decrease if we increase OM(ğ‘¥). For this reason, let us assume that OM(ğ‘¥)=max{ğ‘–,ğ‘›âˆ’ğ‘˜}=:Ä±Ìƒ . Now for OM(ğ‘¦)â‰¥ğ‘— to hold, at least ğ‘—âˆ’Ä±Ìƒ  of the ğ‘›âˆ’Ä±Ìƒ  zero-bits in x have to be flipped. The number of flipped zero-bits follows a binomial distribution with parameters ğ‘›âˆ’Ä±Ìƒ  and 1ğ‘›. By Lemma 4, we have

Pr[OM(ğ‘¦)â‰¥ğ‘—]â‰¤(ğ‘›âˆ’Ä±Ìƒ ğ‘—âˆ’Ä±Ìƒ )ğ‘›âˆ’(ğ‘—âˆ’Ä±Ìƒ )â‰¤(ğ‘›âˆ’Ä±Ìƒ ğ‘›)ğ‘—âˆ’Ä±Ìƒ =:ğ‘Ä±Ìƒ ğ‘—.
Since the individuals of the offspring population ğ‘ƒâ€² are generated independently, the number of offspring y with OM(ğ‘¦)âˆˆ[ğ‘—..ğ‘›] is binomially distributed with parameters ğœ† and some number ğ‘â€²â‰¤ğ‘Ä±Ìƒ ğ‘—. Using again Lemma 4, we see that

Pr[OM(ğ‘ƒâ€³)=ğ‘—]â‰¤Pr[îˆ±]â‰¤(ğœ†ğœ†âˆ’ğœ‡+1)(ğ‘â€²)ğœ†âˆ’ğœ‡+1â‰¤2ğœ†((ğ‘›âˆ’Ä±Ìƒ ğ‘›)ğ‘—âˆ’Ä±Ìƒ )ğœ†âˆ’ğœ‡+1â‰¤2ğœ†((ğ‘˜ğ‘›)ğ‘—âˆ’Ä±Ìƒ )ğœ†/2â‰¤(4ğ‘˜ğ‘›)(ğ‘—âˆ’Ä±Ìƒ )ğœ†/2â‰¤((4ğ‘)ğ¶/2)ln(ğ‘›)(ğ‘—âˆ’Ä±Ìƒ )â‰¤ğ‘›âˆ’2(ğ‘—âˆ’Ä±Ìƒ ),
(3)
where we used the estimates (ğœ†ğœ†âˆ’ğœ‡+1)â‰¤2ğœ† and our assumptions ğœ‡â‰¤ğœ†2 and (4ğ‘)ğ¶/2â‰¤ğ‘’âˆ’2.

So far we have computed that it is difficult to strictly increase OM(ğ‘ƒ) (once OM(ğ‘ƒ) is above ğ‘›âˆ’ğ‘˜). Using a similar reasoning, we now show that also the probability of the event OM(ğ‘ƒ)=OM(ğ‘ƒâ€³) is small (when ğ‘–:=OM(ğ‘ƒ)>ğ‘›âˆ’ğ‘˜). Again, for this event it is necessary that at least ğœ†âˆ’ğœ‡+1 offspring y satisfy OM(ğ‘¦)âˆˆ[ğ‘–..ğ‘›]. Let y be an offspring generated from a parent ğ‘¦âˆˆğ‘ƒ. As above, using a domination argument we can assume that OM(ğ‘¥)=OM(ğ‘ƒ)=ğ‘–. For such an x, we have OM(ğ‘¦)â‰¥ğ‘– only if either no bit at all flips or if at least one zero-bit is flipped. Hence Pr[OM(ğ‘¦)â‰¥ğ‘–]â‰¤(1âˆ’1ğ‘›)ğ‘›+ğ‘ğ‘–,ğ‘–+1â‰¤1ğ‘’+ğ‘˜ğ‘›â‰¤1ğ‘’+ğ‘=ğ‘â€²<0.5. Denoting by X the number of offspring y with OM(ğ‘¦)â‰¥ğ‘–, we have ğ¸[ğ‘‹]â‰¤ğ‘â€²ğœ†. Using the additive Chernoff bound (Theorem 5) rather than Lemma 4 and ğœ‡â‰¤12ğœ†, we compute

Pr[OM(ğ‘ƒâ€³)=ğ‘–]â‰¤Pr[ğ‘‹â‰¥ğœ†âˆ’ğœ‡+1]â‰¤Pr[ğ‘‹â‰¥12ğœ†]â‰¤Pr[ğ‘‹â‰¥ğ¸[ğ‘‹]+(0.5âˆ’ğ‘â€²)ğœ†]â‰¤exp(âˆ’2((0.5âˆ’ğ‘â€²)ğœ†)2ğœ†)=exp(âˆ’(1âˆ’2ğ‘â€²)22ğœ†).
(4)
We are now ready to compute the expected progress of g(P) in one iteration. Let first â„“(ğ‘ƒ)=0 and thus ğ‘”(ğ‘ƒ)=0. Since Pr[â„“(ğ‘ƒâ€³)=ğ¿]â‰¤ğ‘›âˆ’2ğ¿ for all ğ¿âˆˆ[1..ğ‘˜âˆ’1] by (3) and Pr[â„“(ğ‘ƒâ€³)=ğ‘˜]â‰¤ğœ†ğ‘ğ‘˜ by (2), we have

ğ¸[ğ‘”(ğ‘ƒâ€³)]â‰¤1â‹…ğ‘”(0)+âˆ‘ğ¿=1ğ‘˜âˆ’1ğ‘›âˆ’2ğ¿â‹…ğ‘”(ğ¿)+ğœ†ğ‘ğ‘˜â‹…ğ‘”maxâ‰¤0+âˆ‘ğ¿=1ğ‘˜âˆ’1ğ‘›âˆ’ğ¿+1âˆ’â„(ğ‘›,ğœ†)â‰¤1ğ‘›âˆ’1+1âˆ’â„(ğ‘›,ğœ†)â‰¤1
by the choice of ğ‘”max and h. Consequently, ğ¸[ğ‘”(ğ‘ƒâ€³)âˆ’ğ‘”(ğ‘ƒ)]â‰¤1.

For â„“:=â„“(ğ‘ƒ)>0, the probability to reach â„“(ğ‘ƒâ€³)=ğ‘˜ is larger, however, we profit from the fact that we can reduce the potential by having â„“(ğ‘ƒâ€³)<â„“. Since there is nothing to show when g(P) is already at the maximal value ğ‘”max, let us assume that â„“<ğ‘˜âˆ—. Now equations (2), (3), and (4) give

Pr[â„“(ğ‘ƒâ€³)=ğ‘˜]Pr[â„“(ğ‘ƒâ€³)=ğ¿]Pr[â„“(ğ‘ƒâ€³)=â„“]â‰¤ğœ†(1âˆ’1ğ‘›)ğ‘›âˆ’(ğ‘˜âˆ’â„“)ğ‘›âˆ’(ğ‘˜âˆ’â„“)â‰¤ğœ†(1âˆ’1ğ‘›)ğ‘›âˆ’ğ‘˜ğ‘›âˆ’(ğ‘˜âˆ’â„“),â‰¤ğ‘›âˆ’2(ğ¿âˆ’â„“),ğ¿âˆˆ[â„“+1..ğ‘˜âˆ’1],â‰¤exp(âˆ’(1âˆ’2ğ‘â€²)22ğœ†).
With these estimates, we compute

ğ¸[ğ‘”(ğ‘ƒâ€³)]â‰¤Pr[â„“(ğ‘ƒâ€³)=ğ‘˜]ğ‘”max+âˆ‘ğ¿=â„“+1ğ‘˜âˆ’1Pr[â„“(ğ‘ƒâ€³)=ğ¿]ğ‘”(ğ¿)+Pr[â„“(ğ‘ƒâ€³)=â„“]ğ‘”(â„“)+1â‹…ğ‘”(â„“âˆ’1)â‰¤ğœ†(1âˆ’1ğ‘›)ğ‘›âˆ’ğ‘˜ğ‘›âˆ’(ğ‘˜âˆ’â„“)â‹…(1âˆ’â„(ğ‘›,ğœ†))1ğœ†(1âˆ’1ğ‘›)âˆ’(ğ‘›âˆ’ğ‘˜)ğ‘›ğ‘˜+âˆ‘ğ¿=â„“+1âˆğ‘›âˆ’2(ğ¿âˆ’â„“)ğ‘›ğ¿+exp(âˆ’(1âˆ’2ğ‘â€²)22ğœ†)ğ‘›â„“+ğ‘›â„“âˆ’1=ğ‘”(ğ‘ƒ)(1âˆ’â„(ğ‘›,ğœ†)+1ğ‘›âˆ’1+exp(âˆ’(1âˆ’2ğ‘â€²)22ğœ†)+1ğ‘›)=ğ‘”(ğ‘ƒ)
by our choice of ğ‘”max and h as well as our assumption that ğ‘”(ğ‘ƒ)<ğ‘”max. Consequently, again we have ğ¸[ğ‘”(ğ‘ƒâ€³)âˆ’ğ‘”(ğ‘ƒ)]â‰¤1.

Assuming that the population ğ‘ƒ1 satisfies ğ‘”(ğ‘ƒ1)=0, we can now apply the additive drift theorem (Theorem 6) as follows. As before, let ğ‘ƒğ‘¡ denote the population at the end of iteration t. For all ğ‘¡â‰¥0, let ğ‘‹ğ‘¡=ğ‘”maxâˆ’ğ‘”(ğ‘ƒğ‘¡+1). Then ğ‘‹0=ğ‘”max and ğ¸[ğ‘‹ğ‘¡âˆ’ğ‘‹ğ‘¡+1âˆ£ğ‘‹ğ‘¡>0]â‰¤1 for all ğ‘¡â‰¥0. Consequently, the additive drift theorem (Theorem 6) gives that ğ‘‡:=min{ğ‘¡âˆ£ğ‘‹ğ‘¡=0} has an expectation of at least ğ‘”max. By definition, ğ‘‹ğ‘¡=0 is equivalent to saying that ğ‘ƒğ‘¡+1 contains the optimum. Recall that if optima are generated in some iteration t, then some remain in ğ‘ƒğ‘¡. Hence ğ‘‡+1 is indeed the first iteration in which the optimum is generated.

If the optimum is found in some iteration ğ‘¡â‰¥1, then the total number of fitness evaluations up to this event is at least ğœ‡+(ğ‘¡âˆ’1)ğœ†+1, where the ğœ‡ accounts for the initialization and the âˆ’1 and +1 for the fact that the optimum could be the first search point sampled in iteration t (so that we cannot count the remaining offspring generated in the last iteration, see also (1)). This gives an expected optimization time of at least ğœ‡+(ğ¸[ğ‘‡+1]âˆ’1)ğœ†+1=ğœ‡+ğ‘”maxğœ†+1 in the case ğ‘”(ğ‘ƒ1)=0.

Since we have ğ‘”(ğ‘ƒ1)=0 with probability 1âˆ’exp(âˆ’0.16ğ‘›), the expected runtime is at least (1âˆ’exp(âˆ’0.16ğ‘›))(ğœ‡+ğ‘”maxğœ†+1). Recalling that ğ‘”max=(1âˆ’â„(ğ‘›,ğœ†))1ğœ†ğ‘ğ‘˜, we have proven the theorem. â—»

Since it might be useful in other applications, we now explicitly formulate our lower bound of, essentially, ğœ‡+ğœ†, which was observed in the proof above.

Lemma 10
Let ğ‘“:{0,1}ğ‘›â†’â„. Assume that f has at most M global optima. Let ğœ‡,ğœ† be positive integers. Consider the optimization of f via the (ğœ‡,ğœ†) EA (and assume ğœ‡â‰¤ğœ† in this case) or the (ğœ‡+ğœ†) EA. Let ğ‘â‰¤ğœ‡+ğœ†. Then with probability 1âˆ’ğ‘€ğ‘2âˆ’ğ‘›, the optimization time is larger than N. In particular, the expected optimization time is at least (1âˆ’ğ‘€ğ‘2âˆ’ğ‘›)(ğ‘+1)â‰¥14min{ğœ‡+ğœ†,2ğ‘›/ğ‘€}.

Proof
As discussed in the proof of Theorem 8, each of the first ğœ‡+ğœ† individuals generated in a run of the (ğœ‡,ğœ†) EA (and the same applies to the (ğœ‡+ğœ†) EA) is uniformly distributed in {0,1}ğ‘›. Consequently, it is an optimum with probability at most ğ‘€2âˆ’ğ‘›. By a union bound over the first N of these ğœ‡+ğœ† individuals, the probability that one of them is optimal, is at most ğ‘ğ‘€2âˆ’ğ‘›. This gives the claims, where the last estimate follows from taking ğ‘=min{ğœ‡+ğœ†,2ğ‘›/(2ğ‘€)}. â—»

A Tight Upper Bound
While our main target in this work was showing a lower bound that demonstrates that the (ğœ‡,ğœ†) EA has little advantage in leaving the local optima of the jump functions, we now also present an upper bound on the runtime. It shows that our lower bound for large parts of the parameter space is tight including the leading constant. This might be the first non-trivial upper bound for a non-elitist evolutionary algorithm that is tight including the leading constant. This result also shows that our way to exploit negative drift in the lower bound analysis, namely not via the classic negative drift theorems, but via additive drift applied to an exponential rescaling, can give very precise results, unlike the previously used methods.

We shall show the following result.

Theorem 11
Let K be a sufficiently large constant and ğœ†â‰¥ğ¾lnğ‘›. Let 0<ğ›¿<1 be a constant and ğœ‡â‰¤1(1+ğ›¿)ğ‘’ğœ†. Let ğ‘˜âˆˆ[2..ğ‘›] and ğ‘ğ‘˜=(1âˆ’1ğ‘›)ğ‘›âˆ’ğ‘˜ğ‘›âˆ’ğ‘˜. Then the runtime T of the (ğœ‡,ğœ†) EA on JUMPğ‘›ğ‘˜ satisfies

ğ¸[ğ‘‡]â‰¤ğœ†1âˆ’ğ‘›âˆ’1/2(8ğ¶ğ‘›+1+9ğ¶ğ‘›ğ‘ğ‘˜ğœ†â€¾â€¾â€¾â€¾âˆš+8ğ¶ğ‘›ğ‘ğ‘˜ğœ†âŒŠğ‘›3/2âŒ‹+1ğ‘ğ‘˜ğœ†),
where C is a constant depending on ğ›¿ only.Footnote3

Consequently, for ğœ†=ğ‘œ(1/(ğ‘›ğ‘ğ‘˜))=ğ‘œ(ğ‘›ğ‘˜âˆ’1), we have ğ¸[ğ‘‡]â‰¤(1+ğ‘œ(1))1ğ‘ğ‘˜, and for ğœ†=Î©(1/(ğ‘›ğ‘ğ‘˜))=Î©(ğ‘›ğ‘˜âˆ’1), we have ğ¸[ğ‘‡]=ğ‘‚(ğœ†ğ‘›).

We note that when ğœ†=ğ‘œ(ğ‘›ğ‘˜âˆ’1), ğœ†â‰¥ğ¾lnğ‘› with K a sufficiently large constant, ğœ‡â‰¤1(1+ğ›¿)ğ‘’ğœ†, and ğ‘˜â‰¤0.1ğ‘›, our upper bound and our lower bound of Theorem 8 agree including the leading constant. So we have a precise runtime analysis in this regime.

We did not try to find the maximal range of parameters in which the runtime is (1Â±ğ‘œ(1))1ğ‘ğ‘˜. From [65] (see the discussion at the beginning of Sect. 5) it is clear that for ğœ†â‰¤ğ‘lnğ‘›, c a sufficiently small constant, the runtime is exp(Î©(ğ‘›ğ¶)), where C is a constant that depends on c. For ğœ‡â‰¥1(1âˆ’ğ›¿)ğ‘’ğœ†, Lehre [50] gives an exponential lower bound. The restriction to ğ‘˜â‰¤0.1ğ‘› is most likely not necessary, but the range of larger k appears not to be overly interesting given that the super-exponential lower bound from the case 0.1n still applies.

When ğœ†=Î©(ğ‘›ğ‘˜âˆ’1), besides being a possibly unrealistically large population size, our time estimate of O(n) iterations is the same as the best known upper bound for the runtime of the (ğœ‡,ğœ†) EA on the ONEMAX test function [9]. Since this analysis works with the natural partition into Î˜(ğ‘›) fitness levels, the runtime order of O(n) shows that each fitness level is gained in an amortized constant number of iterations. This speed of progress on a difficult problem like jump functions again indicates that the offspring population size ğœ† here is chosen too large.

The exact order of magnitude of the runtime of the (ğœ‡,ğœ†) EA on ONEMAX is still an open problem. The upper bound proven for the (ğœ‡+ğœ†) EA in [4], which in our setting simplifies to ğ‘‚(ğ‘›logğ‘›ğœ†+ğ‘›loglogğœ†/ğœ‡logğœ†/ğœ‡), indicates that there could be some (but not much) room for improvement. So clearly, the next progress here should rather be for the ONEMAX function than for jump functions.

The result in Theorem 11 above improves over the ğ‘‚(ğ‘›ğ‘˜+ğ‘›ğœ†+ğœ†logğœ†) upper bound for the runtime of the (ğœ‡,ğœ†) EA on JUMPğ‘›ğ‘˜ proven in [9] (see Theorem 2) in three ways. First, as discussed above, we make the leading constant precise (and tight for large ranges of the parameters). Second, we obtain a better, namely at most linear, dependence of the runtime on ğœ†. Third, we reduce the minimum offspring population size required for the result to hold, which is Î©(ğ‘˜logğ‘›) in [9] and Î©(logğ‘›) in our result.

Level-based Analyses
A central step in our proof is an analysis of how the (ğœ‡,ğœ†) EA progresses to a parent population consisting only of individuals on the local optimum. Since the (ğœ‡,ğœ†) EA is a non-elitist algorithm, this asks for tools like the ones introduced by Lehre [51] and then improved by various authors [9, 17, 18]. Unfortunately, all these results are formulated for the problem of finding one individual of a certain minimum quality. Consequently, they all cannot be directly employed to analyze the time needed to have the full parent population consist of individuals of at least a certain quality. Fortunately, in their proofs all previous level-based analyses proceed by analyzing the time until a certain number of individuals of a certain quality have been obtained and then building on this with an analysis on how better individuals are generated. Among the previous works it appears that [17] is the one that makes this argumentation most explicit, whereas the other works with their intricate potential function arguments give less insight into the working principles of the process.

For this reason, we build now on [17]. To avoid restating an essentially unchanged proof from [17], we instead first state the level-based theorem shown in [17], explain where the different expressions in the runtime estimate stem from, and then state without explicit proof the level-based result we need. With the explanations given beforehand, we feel that the interested reader easily can see from [17] why our level-based theorem is correct.

The general setup of level-based theorems for population processes is as follows. There is a ground set î‰„, which will be search space {0,1}ğ‘› in our applications. On this ground set, a population-based Markov process (ğ‘ƒğ‘¡) is defined. We consider populations of fixed size ğœ†, which may contain elements several times (multi-sets). We write î‰„ğœ† to denote the set of all such populations. We only consider Markov processes where each element of the next population is sampled independently (with repetition). That is, for each population ğ‘ƒâˆˆî‰„ğœ†, there is a distribution D(P) on î‰„ such that given ğ‘ƒğ‘¡, the next population ğ‘ƒğ‘¡+1 consists of ğœ† elements of î‰„, each chosen independently from the distribution ğ·(ğ‘ƒğ‘¡). We do not make any assumptions on the initial population ğ‘ƒ0.

In the level-based setting, we assume that there is a partition of î‰„ into levels ğ´1,â€¦,ğ´ğ‘š. Based on information in particular on how individuals in different levels are generated, we aim for an upper bound on the first time such that the population contains an element of the highest level ğ´ğ‘š. Now the level-based theorem shown in [17] is as follows.

Theorem 12
(Level-based theorem). Consider a population process as described above.

Let (ğ´1,â€¦,ğ´ğ‘š) be a partition of î‰„. Let ğ´â‰¥ğ‘—:=â‹ƒğ‘šğ‘–=ğ‘—ğ´ğ‘– for all ğ‘—âˆˆ[1..ğ‘š]. Let ğ‘§1,â€¦,ğ‘§ğ‘šâˆ’1,ğ›¿âˆˆ(0,1], and let ğ›¾0âˆˆ(0,11+ğ›¿] with ğ›¾0ğœ†âˆˆâ„¤. Let ğ·0=min{âŒˆ100/ğ›¿âŒ‰,ğ›¾0ğœ†} and ğ‘1=56000. Let

ğ‘¡0=7000ğ›¿â›ââœâœğ‘š+11âˆ’ğ›¾0âˆ‘ğ‘—=1ğ‘šâˆ’1log02â›ââœâœ2ğ›¾0ğœ†1+ğ‘§ğ‘—ğœ†ğ·0ââ âŸâŸ+1ğœ†âˆ‘ğ‘—=1ğ‘šâˆ’11ğ‘§ğ‘—ââ âŸâŸ,
where log02(ğ‘¥):=max{0,log2(ğ‘¥)} for all ğ‘¥âˆˆâ„. Assume that for any population ğ‘ƒâˆˆî‰„ğœ† the following three conditions are satisfied.

(G1):
For each level ğ‘—âˆˆ[1..ğ‘šâˆ’1], if |ğ‘ƒâˆ©ğ´â‰¥ğ‘—|â‰¥ğ›¾0ğœ†/4, then

Prğ‘¦âˆ¼ğ·(ğ‘ƒ)[ğ‘¦âˆˆğ´â‰¥ğ‘—+1]â‰¥ğ‘§ğ‘—.
(G2):
For each level ğ‘—âˆˆ[1..ğ‘šâˆ’2] and all ğ›¾âˆˆ(0,ğ›¾0], if |ğ‘ƒâˆ©ğ´â‰¥ğ‘—|â‰¥ğ›¾0ğœ†/4 and |ğ‘ƒâˆ©ğ´â‰¥ğ‘—+1|â‰¥ğ›¾ğœ†, then

Prğ‘¦âˆ¼ğ·(ğ‘ƒ)[ğ‘¦âˆˆğ´â‰¥ğ‘—+1]â‰¥(1+ğ›¿)ğ›¾.
(G3):
The population size ğœ† satisfies

ğœ†â‰¥256ğ›¾0ğ›¿ln(8ğ‘¡0).
Then ğ‘‡:=min{ğœ†ğ‘¡âˆ£ğ‘ƒğ‘¡âˆ©ğ´ğ‘šâ‰ âˆ…} satisfies

ğ¸[ğ‘‡]â‰¤8ğœ†ğ‘¡0=ğ‘1ğœ†ğ›¿â›ââœâœğ‘š+11âˆ’ğ›¾0âˆ‘ğ‘—=1ğ‘šâˆ’2log02â›ââœâœ2ğ›¾0ğœ†1+ğ‘§ğ‘—ğœ†ğ·0ââ âŸâŸ+1ğœ†âˆ‘ğ‘—=1ğ‘šâˆ’11ğ‘§ğ‘—ââ âŸâŸ.
Let us explain where the time bound stated in this theorem stems from. We argue in terms of iterations now, not in terms of search point evaluations. Then the time bound is 8ğ‘¡0 with ğ‘¡0 as defined in the theorem. The main argument of the proof given in [17] is as follows. Let us, in the next three paragraphs, say that a population P is well-established on level j if |ğ‘ƒâˆ©ğ´â‰¥ğ‘—|â‰¥ğ›¾0ğœ†/4. Now condition (G2) imposes that if the current population is well-established on level j, then the number of individuals on level ğ‘—+1 or higher increases, in expectation, by a factor of 1+ğ›¿ until at least ğ›¾0ğœ† such individuals are in the population. It appears natural (and is true, but not trivial to prove) that it takes roughly log1+ğ›¿(ğ›¾0ğœ†)â‰ˆ1ğ›¿log(ğ›¾0ğœ†) iterations from the first individual on level ğ‘—+1 to having at least ğ›¾0ğœ† individuals on this level. This explains roughly the middle term in the definition of ğ‘¡0. Without going into details, we remark that the extra ğ‘§ğ‘—ğœ†ğ·0 expression exploits that when generating individuals on a higher level (as described in (G1)) is easy, then we can assume that we do not start with a single individual on level ğ‘—+1, but with roughly ğ‘§ğ‘—ğœ† individuals. Consequently, we need the factor-(1+ğ›¿) growth only to go from ğ‘§ğ‘—ğœ† to ğ›¾0ğœ† individuals.

The remaining term ğ‘š+1ğœ†âˆ‘ğ‘šâˆ’1ğ‘—=11ğ‘§ğ‘— accounts for the time needed to generate the first individuals on a higher level. Given that the population is well-established on some level j, by (G1) the probability that a new individual is on level ğ‘—+1 or higher is at least ğ‘§ğ‘—. Since we generate ğœ† individuals in each step, the time to find an individual on a higher level (tacitly assuming that we stay well-established on level j, which is ensured by (G3) via Martingale concentration arguments) is at most âŒˆğ‘‹/ğœ†âŒ‰â‰¤1+ğ‘‹/ğœ†, where X is geometrically distributed with success probability ğ‘§ğ‘— and thus expectation 1ğ‘§ğ‘—.

This explanation of the definition of ğ‘¡0 motivates that we can extend the result of Theorem 12 to statements on how long it takes to have a certain level well-established (or even filled with at least ğ›¾0ğœ† individuals). This is what we do now. We omit the formal proof, but invite the reader to consult the proof in [17], which immediately yields our claim.

Corollary 13
(Level-based theorem for filling sub-optimal levels). Let a population process be given as described above.

Let (ğ´1,â€¦,ğ´ğ‘š) be a partition of î‰„. Let ğ´â‰¥ğ‘—:=â‹ƒğ‘šğ‘–=ğ‘—ğ´ğ‘– for all ğ‘—âˆˆ[1..ğ‘š]. Let ğ‘§1,â€¦,ğ‘§ğ‘šâˆ’1,ğ›¿âˆˆ(0,1], and let ğ›¾0âˆˆ(0,11+ğ›¿] with ğ›¾0ğœ†âˆˆâ„¤. Let ğ·0=min{âŒˆ100/ğ›¿âŒ‰,ğ›¾0ğœ†} and ğ‘1=56000.

Let â„“âˆˆ[1..ğ‘šâˆ’1] and

ğ‘¡0(â„“)=7000ğ›¿â›ââœâœğ‘š+11âˆ’ğ›¾0âˆ‘ğ‘—=1â„“âˆ’1log02â›ââœâœ2ğ›¾0ğœ†1+ğ‘§ğ‘—ğœ†ğ·0ââ âŸâŸ+1ğœ†âˆ‘ğ‘—=1â„“âˆ’11ğ‘§ğ‘—ââ âŸâŸ,
where log02(ğ‘¥):=max{0,log2(ğ‘¥)} for all ğ‘¥âˆˆâ„. Assume that for any population ğ‘ƒâˆˆî‰„ğœ† the following three conditions are satisfied.

(G1):
For each level ğ‘—âˆˆ[1..â„“âˆ’1], if |ğ‘ƒâˆ©ğ´â‰¥ğ‘—|â‰¥ğ›¾0ğœ†/4, then

Prğ‘¦âˆ¼ğ·(ğ‘ƒ)[ğ‘¦âˆˆğ´â‰¥ğ‘—+1]â‰¥ğ‘§ğ‘—.
(G2):
For each level ğ‘—âˆˆ[1..â„“âˆ’2] and all ğ›¾âˆˆ(0,ğ›¾0], if |ğ‘ƒâˆ©ğ´â‰¥ğ‘—|â‰¥ğ›¾0ğœ†/4 and |ğ‘ƒâˆ©ğ´â‰¥ğ‘—+1|â‰¥ğ›¾ğœ†, then

Prğ‘¦âˆ¼ğ·(ğ‘ƒ)[ğ‘¦âˆˆğ´â‰¥ğ‘—+1]â‰¥(1+ğ›¿)ğ›¾.
(G3):
The population size ğœ† satisfies

ğœ†â‰¥256ğ›¾0ğ›¿ln(8ğ‘¡0(â„“)).
Then ğ‘‡:=min{ğœ†ğ‘¡âˆ£|ğ‘ƒğ‘¡âˆ©ğ´â‰¥â„“|â‰¥ğ›¾0ğœ†} satisfies

ğ¸[ğ‘‡]â‰¤8ğœ†ğ‘¡0(â„“)=ğ‘1ğœ†ğ›¿â›ââœâœğ‘š+11âˆ’ğ›¾0âˆ‘ğ‘—=1â„“âˆ’1log02â›ââœâœ2ğ›¾0ğœ†1+ğ‘§ğ‘—ğœ†ğ·0ââ âŸâŸ+1ğœ†âˆ‘ğ‘—=1â„“âˆ’11ğ‘§ğ‘—ââ âŸâŸ.
Proof of the Upper Bound
We are now ready to prove our upper bound result. We start by giving a brief outline of the main arguments. We use our variant of the level-based theorem to argue that from any possible state of the algorithm, it takes an expected number of O(n) iterations to reach a parent population that consists only of individuals in the local optimum (or the global optimum, but since we are done then, we can ignore this case). We call this an almost perfect population. From this point on, we cannot use the level-based method anymore, since the small probability for going from the local to the global optimum would require a large value of ğœ†, a requirement we try to avoid. This requirement is necessary in the level-based method because there one tries to ensure that once a decent number of individuals are on at least a certain level, this state is never lost. When ğœ† is only logarithmic in n, there is an inverse-polynomial probability to completely lose a level. Since for, say, ğ‘˜=Î˜(ğ‘›), we expect a runtime of roughly ğ‘›ğ‘˜/ğœ†, in this time it will regularly happen that we lose a level, including the cases that we lose a level in each of several iterations or that we lose several levels at once.

We overcome this difficulty with a restart argument. Since the probability for such an undesirable event is only inverse-polynomial in n, we see that we keep an almost perfect population for at least ğ‘›2 iterations (with high probability). Since it took us only O(n) iterations to reach (or regain) an almost perfect population, we obtain that in all but a lower order fraction of the iterations we have an almost perfect parent population. Hence apart from this lower order performance loss, we can assume that we are always in an almost perfect population. From such a state, we reach the optimum in one iteration with probability 1âˆ’(1âˆ’ğ‘ğ‘˜)ğœ†, which quickly leads to the claimed result.

We now state the formal proof, which makes this proof sketch precise and adds a few arguments not discussed so far.

Proof of Theorem 11
Since ğ‘›ğ‘› is a trivial upper bound for the expected runtime of any evolutionary algorithm creating all individuals as random search points or via standard bit mutation with mutation rate 1ğ‘›, simply because each of these search points with probability at least ğ‘›âˆ’ğ‘› is the optimumFootnote4, we can assume that ğ‘˜<ğ‘›.

Let ğ‘š=ğ‘›+1 and let ğ´1,â€¦,ğ´ğ‘š be the partition of {0,1}ğ‘› into the fitness levels of JUMPğ‘›ğ‘˜, that is, for all ğ‘–âˆˆ[1..ğ‘šâˆ’1] we have ğ´ğ‘–={ğ‘¥âˆˆ{0,1}ğ‘›âˆ£ğ‘“(ğ‘¥)=ğ‘–} and for ğ‘–=ğ‘š we have ğ´ğ‘–={(1,â€¦,1)}. In particular, for all ğ‘–âˆˆ[1..ğ‘šâˆ’1] and all ğ‘¥âˆˆğ´ğ‘–, ğ‘¦âˆˆğ´ğ‘–+1 we have ğ‘“(ğ‘¥)<ğ‘“(ğ‘¦). Also, ğ´ğ‘š consists of the unique optimum and ğ´ğ‘šâˆ’1 consists of all local optima.

Consider a run of the (ğœ‡,ğœ†) EA on JUMPğ‘›ğ‘˜. As in Algorithm 1, we denote by ğ‘ƒğ‘¡ the population (of size ğœ‡) selected in iteration t, which serves as parent population in generation ğ‘¡+1. Let ğ‘ƒ0 denote the initial population. We denote by ğ‘„ğ‘¡ the offspring population (of size ğœ†) generated in iteration t. Hence ğ‘ƒğ‘¡ consists of ğœ‡ best individuals chosen from ğ‘„ğ‘¡. For the sake of a smooth presentation, let ğ‘„0 be a population obtained from ğ‘ƒ0 by adding ğœ†âˆ’ğœ‡ random search points of minimal fitness. Note that we can again assume that ğ‘ƒ0 is obtained from ğ‘„0 by selecting ğœ‡ best individuals.

We say that a parent population ğ‘ƒğ‘¡ is almost perfect if ğ‘ƒğ‘¡âŠ†ğ´â‰¥ğ‘šâˆ’1. Note that this is equivalent to having |ğ‘„ğ‘¡âˆ©ğ´â‰¥ğ‘šâˆ’1|â‰¥ğœ‡.

Step 1: We first argue that for any time ğ‘ â‰¥0 and regardless of what is ğ‘„ğ‘ , the first time ğ‘†â‰¥ğ‘  such that ğ‘ƒğ‘† is almost perfect satisfies ğ¸[ğ‘†âˆ’ğ‘ ]â‰¤8ğ‘¡0, where

ğ‘¡0=104ğ›¿â›ââœâœğ‘š+11âˆ’ğ›¾0âˆ‘ğ‘—=1ğ‘šâˆ’2log02â›ââœâœ2ğ›¾0ğœ†1+ğ‘§ğ‘—ğœ†ğ·0ââ âŸâŸ+1ğœ†âˆ‘ğ‘—=1ğ‘šâˆ’21ğ‘§ğ‘—ââ âŸâŸ.
To ease the notation, we assume that ğ‘ =0. To estimate S, we apply our variant of the level-based theorem (Corollary 13) to the process (ğ‘„ğ‘¡)ğ‘¡â‰¥0. Since optimizing jump functions up to the local optimum is very similar to optimizing the ONEMAX function, this analysis is similar to an analogous analysis for ONEMAX (where we note that the work [9] proving the previous-best result for ONEMAX for most details of the proof refers to the not very detailed conference paper [51]).

We choose suitable parameters to use the level-theorem. For ğ‘—âˆˆ[1..ğ‘˜âˆ’1], this corresponds to the fitness levels lying in the gap region of JUMPğ‘›ğ‘˜, let ğ‘§ğ‘—=14ğ‘›âˆ’ğ‘—ğ‘’ğ‘›. For ğ‘—âˆˆ[ğ‘˜..ğ‘šâˆ’2], here ğ´ğ‘— consists of the search points x with OM(ğ‘¥)=ğ‘—âˆ’ğ‘˜, we let ğ‘§ğ‘—=14ğ‘›âˆ’(ğ‘—âˆ’ğ‘˜)ğ‘’ğ‘›. Note that for ğ‘—âˆˆ[1..ğ‘˜âˆ’1], we have ğ‘§ğ‘—â‰¥14ğ‘˜+1âˆ’ğ‘—ğ‘’ğ‘›, and hence

âˆ‘ğ‘—=1ğ‘šâˆ’21ğ‘§ğ‘—â‰¤4ğ‘’ğ‘›âˆ‘ğ‘–=2ğ‘›1ğ‘–â‰¤4ğ‘’ğ‘›lnğ‘›,
(5)
recalling that the harmonic number ğ»ğ‘›=âˆ‘ğ‘›ğ‘–=11ğ‘– satisfies ğ»ğ‘›â‰¤ln(ğ‘›)+1, see, e.g., [29, (1.4.12)]. Note also, for later, that for any j we have ğ‘§ğ‘—â‰¥14ğ‘›âˆ’ğ‘—ğ‘’ğ‘›.

Let ğ›¾0 be such that ğ›¾0ğœ†=âŒŠğœ†(1+ğ›¿)ğ‘’âŒ‹. Note that by our assumption that ğœ† is large, ğ›¾0ğœ† is an integer greater than one as required in Corollary 13. Also, ğ›¾0â‰¤11+ğ›¿ as required. By our assumption ğœ†â‰¥(1+ğ›¿)ğ‘’ğœ‡, we have ğ›¾0ğœ†â‰¥ğœ‡. Trivially, ğ›¾0â‰¤1(1+ğ›¿)ğ‘’â‰¤1ğ‘’. Let ğ·0=min{âŒˆ100/ğ›¿âŒ‰,ğ›¾0ğœ†}.

We check that the conditions (G1) to (G3) of Corollary 13 are satisfied for â„“=ğ‘šâˆ’1. To show (G1) and (G2), let ğ‘¡â‰¥0 be any iteration.

(G1): Let ğ‘—âˆˆ[1..ğ‘šâˆ’2] such that |ğ‘„ğ‘¡âˆ©ğ´â‰¥ğ‘—|â‰¥ğ›¾0ğœ†/4. We need to show that an offspring y generated in iteration ğ‘¡+1 is in ğ´â‰¥ğ‘—+1 with probability at least ğ‘§ğ‘—. Let first ğ‘—â‰¥ğ‘˜, that is, ğ´ğ‘— is not a level in the gap. Let y be an offspring generated in iteration ğ‘¡+1 and let ğ‘¥âˆˆğ‘ƒğ‘¡ be its random parent, which we can assume to be not the optimum as otherwise we would be done already. Since ğ›¾0ğœ†/4â‰¥ğœ‡/4, there are at least ğœ‡/4 individuals in ğ‘ƒğ‘¡âˆ©ğ´â‰¥ğ‘—. Hence with probability at least 1/4, we have ğ‘¥âˆˆğ´â‰¥ğ‘—. In this case, we have

Pr[ğ‘¦âˆˆğ´ğ‘—+1]â‰¥min{1ğ‘’,(1âˆ’1ğ‘›)ğ‘›âˆ’1ğ‘›âˆ’(ğ‘—âˆ’ğ‘˜)ğ‘›}â‰¥ğ‘›âˆ’(ğ‘—âˆ’ğ‘˜)ğ‘’ğ‘›,
where the first case refers to x already being in ğ´â‰¥ğ‘—+1, that is, 1â‰¤ğ‘—+1âˆ’ğ‘˜â‰¤OM(ğ‘¥)â‰¤ğ‘›âˆ’1, and uses Lemma 7, and where the second case refers to ğ‘¥âˆˆğ´ğ‘—, that is, OM(ğ‘¥)=ğ‘—âˆ’ğ‘˜. In total, we have Pr[ğ‘¦âˆˆğ´â‰¥ğ‘—+1]â‰¥14ğ‘›âˆ’(ğ‘—âˆ’ğ‘˜)ğ‘’ğ‘›=ğ‘§ğ‘—. If ğ‘—<ğ‘˜, we proceed analogously with the only exception that, since in the first case we could have OM(ğ‘¥)=0, we now estimate Pr[OM(ğ‘¦)=OM(ğ‘¥)]â‰¥(1âˆ’1ğ‘›)ğ‘›. We thus obtain Pr[ğ‘¦âˆˆğ´ğ‘—+1âˆ£ğ‘¥âˆˆğ´â‰¥ğ‘—]â‰¥min{(1âˆ’1ğ‘›)ğ‘›,(1âˆ’1ğ‘›)ğ‘›âˆ’1ğ‘›âˆ’ğ‘—ğ‘›}â‰¥(1âˆ’1ğ‘›)ğ‘›âˆ’1ğ‘›âˆ’ğ‘—ğ‘›â‰¥ğ‘›âˆ’ğ‘—ğ‘’ğ‘›. Consequently, now Pr[ğ‘¦âˆˆğ´â‰¥ğ‘—+1]â‰¥14ğ‘›âˆ’ğ‘—ğ‘’ğ‘›=ğ‘§ğ‘—.

(G2): Let ğ‘—âˆˆ[1..ğ‘šâˆ’2] such that |ğ‘„ğ‘¡âˆ©ğ´â‰¥ğ‘—|â‰¥ğ›¾0ğœ†/4. Let ğ›¾âˆˆ(0,ğ›¾0] such that |ğ‘„ğ‘¡âˆ©ğ´â‰¥ğ‘—+1|â‰¥ğ›¾ğœ†. We need to show that an offspring y generated in iteration ğ‘¡+1 is in ğ´â‰¥ğ‘—+1 with probability at least (1+ğ›¿)ğ›¾. Let x be a parent selected uniformly at random from ğ‘ƒğ‘¡, where again we assume that ğ‘ƒğ‘¡ contains no optimal solution. There are at least min{ğ›¾ğœ†,ğœ‡}â‰¥min{ğ›¾(1+ğ›¿)ğ‘’ğœ‡,ğœ‡}=ğ›¾(1+ğ›¿)ğ‘’ğœ‡ individuals in ğ‘ƒğ‘¡âˆ©ğ´â‰¥ğ‘—+1. Hence with probability at least ğ›¾(1+ğ›¿)ğ‘’, we have ğ‘¥âˆˆğ´â‰¥ğ‘—+1. In this case, Pr[ğ‘¦âˆˆğ´â‰¥ğ‘—+1]â‰¥Pr[OM(ğ‘¦)=OM(ğ‘¥)]â‰¥1ğ‘’ by Lemma 7 when OM(ğ‘¥)â‰ 0. When OM(ğ‘¥)=0, then Pr[ğ‘¦âˆˆğ´â‰¥ğ‘—+1]â‰¥Pr[OM(ğ‘¦)âˆˆ{0,1}]â‰¥Pr[âˆ€ğ‘–âˆˆ[2..ğ‘›]:ğ‘¥ğ‘–=ğ‘¦ğ‘–]=(1âˆ’1ğ‘›)ğ‘›âˆ’1â‰¥1ğ‘’, where the first estimate uses our assumption ğ‘˜<ğ‘›. Hence without conditioning on ğ‘¥âˆˆğ´â‰¥ğ‘—+1, we have Pr[ğ‘¦âˆˆğ´â‰¥ğ‘—+1]â‰¥ğ›¾(1+ğ›¿)ğ‘’â‹…1ğ‘’â‰¥(1+ğ›¿)ğ›¾.

(G3): We first estimate ğ‘¡0. We recall that ğ›¾0â‰¤1ğ‘’ and ğ‘§ğ‘—â‰¥14ğ‘›âˆ’ğ‘—ğ‘’ğ‘› for all j. Thus, for all ğ‘—âˆˆ[1..ğ‘šâˆ’2], we have

log02â›ââœâœ2ğ›¾0ğœ†1+ğ‘§ğ‘—ğœ†ğ·0ââ âŸâŸâ‰¤log02((2/ğ‘’)ğ·0ğ‘§ğ‘—)â‰¤log2(8ğ·0ğ‘›ğ‘›âˆ’ğ‘—).
Consequently,

11âˆ’ğ›¾0âˆ‘ğ‘—=1ğ‘šâˆ’2log02â›ââœâœ2ğ›¾0ğœ†1+ğ‘§ğ‘—ğœ†ğ·0ââ âŸâŸâ‰¤ğ‘’ğ‘’âˆ’1âˆ‘ğ‘—=1ğ‘šâˆ’2log2(8ğ·0ğ‘›ğ‘›âˆ’ğ‘—)=ğ‘’ğ‘’âˆ’1log2(âˆğ‘—=1ğ‘šâˆ’28ğ·0ğ‘›ğ‘›âˆ’ğ‘—)â‰¤ğ‘’ğ‘’âˆ’1log2((8ğ·0ğ‘›)ğ‘›ğ‘›!)â‰¤ğ‘’ğ‘’âˆ’1log2((8ğ·0ğ‘›)ğ‘›(ğ‘›/ğ‘’)ğ‘›)=ğ‘’ğ‘’âˆ’1ğ‘›log2(8ğ‘’ğ·0),
where we used the well-known estimate ğ‘›!â‰¥(ğ‘›/ğ‘’)ğ‘›, see, e.g., [29, (1.4.13)].

From this and (5), we obtain

ğ‘¡0=104ğ›¿â›ââœâœğ‘š+11âˆ’ğ›¾0âˆ‘ğ‘—=1ğ‘šâˆ’2log02â›ââœâœ2ğ›¾0ğœ†1+ğ‘§ğ‘—ğœ†ğ·0ââ âŸâŸ+1ğœ†âˆ‘ğ‘—=1ğ‘šâˆ’21ğ‘§ğ‘—ââ âŸâŸâ‰¤104ğ›¿(ğ‘š+ğ‘’ğ‘’âˆ’1ğ‘›log2(8ğ‘’ğ·0)+1ğœ†4ğ‘’ğ‘›lnğ‘›)=ğ‘‚(ğ‘›),
(6)
where the asymptotic estimate uses the fact that ğœ†=Î©(logğ‘›). This shows (G3).

From (G1) to (G3), Corollary 13 shows that after an expected number of 8ğ‘¡0 iterations, we have reached an offspring population ğ‘„ğ‘¡ with |ğ‘„ğ‘¡âˆ©ğ´â‰¥ğ‘šâˆ’1|â‰¥ğ›¾0ğœ†=ğœ‡ and thus an almost perfect population ğ‘ƒğ‘¡.

Step 2: We now show that when ğ‘ƒğ‘¡ contains only local optima, then with probability at least 1âˆ’ğ‘›âˆ’2, the same is true for ğ‘ƒğ‘¡+1 or the global optimum has been found. Indeed, by our initial assumption ğ‘˜<ğ‘›, the search points on the local optimum have a ONEMAX-value between 1 and ğ‘›âˆ’1. Hence Lemma 7 implies that ğ‘‹:=|ğ‘„ğ‘¡+1âˆ©ğ´â‰¥ğ‘šâˆ’1| follows a binomial law with parameters ğœ† and success probability at least 1ğ‘’. By the additive Chernoff bound (Theorem 5), we have

Pr[ğ‘‹â‰¤ğœ†ğ‘’(1+ğ›¿)]=Pr[ğ‘‹â‰¤ğ¸[ğ‘‹]âˆ’ğ›¿1+ğ›¿ğ¸[ğ‘‹]]â‰¤exp(âˆ’2(ğ›¿1+ğ›¿ğ¸[ğ‘‹])2ğœ†)=exp(âˆ’2ğ‘’2(ğ›¿1+ğ›¿)2ğœ†)â‰¤ğ‘›âˆ’2
by our assumption that ğœ†â‰¥ğ¾ln(ğ‘›) with a constant K sufficiently large. Since ğœ‡â‰¤ğœ†ğ‘’(1+ğ›¿), we have |ğ‘ƒğ‘¡+1âˆ©ğ´â‰¥ğ‘šâˆ’1|<ğœ‡ only if ğ‘‹<ğœ‡â‰¤ğœ†ğ‘’(1+ğ›¿). As just computed, this happens with probability at most ğ‘›âˆ’2.

Step 3: Recall that ğ‘ğ‘˜=(1âˆ’1ğ‘›)ğ‘›âˆ’ğ‘˜ğ‘›âˆ’ğ‘˜ is the probability to generate the optimum from a parent on the local optimum. Let ğ‘‡0=min{âŒˆğ‘¡0/ğœ†ğ‘ğ‘˜â€¾â€¾â€¾â€¾â€¾â€¾âˆšâŒ‰,âŒŠğ‘›3/2âŒ‹}. We call a phase of a run of the algorithm an interval of (i) first all iterations until we have an almost perfect parent population ğ‘ƒğ‘¡, and then (ii) another exactly ğ‘‡0 iterations. We assume here for simplicity that we continue to run the algorithm even when it found the optimum; in such a case, we replace such an optimum immediately with a random search point on the local optimum. Since we are interested in the first time an optimum is found, this modification does not change our results. By definition and step 1 above, the expected length of a phase is at most 8ğ‘¡0+ğ‘‡0 regardless of how this phase starts.

We call a phase regular if after reaching an almost perfect parent population ğ‘ƒğ‘¡ we never (in the following exactly ğ‘‡0 iterations) encounter a parent population ğ‘ƒğ‘¡ that is not almost perfect. By a simple union bound and step 2 above, each phase is regular with probability at least 1âˆ’ğ‘›âˆ’2ğ‘‡0â‰¥1âˆ’ğ‘›âˆ’1/2, regardless how the phase started and how it reached an almost perfect parent population.

A regular phase is successful if it finds the optimum at least once. Since in a regular phase at least ğœ†ğ‘‡0 times an offspring is generated from a parent on the local optimum (which results in the global optimum with probability ğ‘ğ‘˜), and since these offspring are generated independently, the probability for a regular phase to be not successful is at most (1âˆ’ğ‘ğ‘˜)ğœ†ğ‘‡0, which is at most 11+ğ‘ğ‘˜ğœ†ğ‘‡0 by an elementary estimate stated as Lemma 8 in [65]. Since thus a regular phase is successful with probability at least 1âˆ’11+ğ‘ğ‘˜ğœ†ğ‘‡0=ğ‘ğ‘˜ğœ†ğ‘‡01+ğ‘ğ‘˜ğœ†ğ‘‡0, it takes an expected number of 1+ğ‘ğ‘˜ğœ†ğ‘‡0ğ‘ğ‘˜ğœ†ğ‘‡0=1+1ğ‘ğ‘˜ğœ†ğ‘‡0 regular phases to find the optimum. Since phases are regular with probability at least 1âˆ’ğ‘›âˆ’1/2, it takes an expected number of at most 11âˆ’ğ‘›âˆ’1/2â‹…(1+1ğ‘ğ‘˜ğœ†ğ‘‡0) phases to find the optimum. By Waldâ€™s equation, these take an expected number of at most 11âˆ’ğ‘›âˆ’1/2â‹…(1+1ğ‘ğ‘˜ğœ†ğ‘‡0)â‹…(8ğ‘¡0+ğ‘‡0) iterations. We estimate

(1+1ğ‘ğ‘˜ğœ†ğ‘‡0)â‹…(8ğ‘¡0+ğ‘‡0)=8ğ‘¡0+ğ‘‡0+8ğ‘¡0ğ‘ğ‘˜ğœ†ğ‘‡0+1ğ‘ğ‘˜ğœ†â‰¤8ğ‘¡0+ğ‘¡0ğ‘ğ‘˜ğœ†â€¾â€¾â€¾â€¾âˆš+1+8ğ‘¡0ğ‘ğ‘˜ğœ†â€¾â€¾â€¾â€¾âˆš+8ğ‘¡0ğ‘ğ‘˜ğœ†âŒŠğ‘›3/2âŒ‹+1ğ‘ğ‘˜ğœ†.
Recalling that ğ‘¡0=ğ‘‚(ğ‘›), we note that this expression is O(n) when ğ‘ğ‘˜ğœ†=Î©(1/ğ‘›) and (1+ğ‘œ(1))1ğ‘ğ‘˜ğœ† when ğ‘ğ‘˜ğœ†=ğ‘œ(1/ğ‘›). Recalling further that each iteration contains ğœ† fitness evaluations, see also (1), the claim follows. â—»

Conclusion
In this work, we observed that for all reasonable parameter values, the (ğœ‡,ğœ†) EA cannot optimize jump functions faster than the (ğœ‡+ğœ†) EA. The (ğœ‡,ğœ†) EA thus fails to profit from its ability to leave local optima to inferior solutions. While we prove this absence of advantage formally only for the basic (ğœ‡,ğœ†) EA and jump functions (which constitute, however, a standard algorithm and a classic benchmark), we feel that our proofs do not suggest that this result is caused by very special characteristics of the (ğœ‡,ğœ†) EA or the jump functions, but that it rather follows from the fact that leaving a local optimum having moderate radius of attraction via comma selection is generally difficult because, relatively independent of the population sizes, there is a strong drift towards the local optimum. We do not show such a strong drift when ğœ†<2ğœ‡, but in this case the selection pressure is known to be so low that no efficient optimization is possible.

Overall, this work suggests that the role of comma selection in evolutionary computation deserves some clarification. Interesting directions for future research could be to try to find convincing examples where comma selection is helpful or a general result going beyond particular examples that shows in which situations comma selection cannot speed up the optimization of multimodal objective functions. From a broader perspective, any result giving a mildly general advice which of the existing approaches to cope with local optima are preferable in which situations, would be highly desirable. The new analysis methods developed in this work, which can yield precise runtime bounds for non-elitist population processes and negative drift situations, could be helpful as they now allow to prove or disprove constant-factor advantages.