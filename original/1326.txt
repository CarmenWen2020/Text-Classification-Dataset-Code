Abstract
Given a finite set of weighted points in ℝ𝑑 (where there can be negative weights), the maximum box problem asks for an axis-aligned rectangle (i.e., box) such that the sum of the weights of the points that it contains is maximized. We consider that each point of the input has a probability of being present in the final random point set, and these events are mutually independent; then, the total weight of a maximum box is a random variable. We aim to compute both the probability that this variable is at least a given parameter, and its expectation. We show that even in 𝑑=1 these computations are #P-hard, and give pseudo-polynomial time algorithms in the case where the weights are integers in a bounded interval. For 𝑑=2, we consider that each point is colored red or blue, where red points have weight +1 and blue points weight −∞. The random variable is the maximum number of red points that can be covered with a box not containing any blue point. We prove that the above two computations are also #P-hard, and give a polynomial-time algorithm for computing the probability that there is a box containing exactly two red points, no blue point, and a given point of the plane.

Introduction
The maximum box problem receives as input a finite point set in ℝ𝑑, where each point is associated with a positive or negative weight, and outputs an axis-aligned rectangle (i.e., box) such that the sum of the weights of the points that it contains is maximized [3]. We consider this problem on a recent uncertainty model in which each element of the input has assigned a probability. Particularly, each point has assigned its own and independent probability of being present in the final (hence random) point set. Then, one can ask the following questions: What is the probability that for the final point set there exists a box that covers a weight sum greater than or equal to a given parameter? What is the expectation of the maximum weight sum that can be covered with a box? Uncertainty models come from real scenarios in which large amounts of data, arriving from many sources, have inherent uncertainty. In computational geometry, we can find several recent works on uncertain point sets such as: the expected total length of the minimum Euclidean spanning tree [5]; the probability that the distance between the closest pair of points is at least a given parameter [11]; the computation of the most-likely convex hull [16]; the probability that the area or perimeter of the convex hull is at least a given parameter [15]; the smallest enclosing ball [9]; the probability that a 2-colored point set is linearly separable [10]; the area of the minimum enclosing rectangle [17]; and Klee’s measure of random rectangles [20]. We deal with the maximum box problem in the above mentioned random model. The maximum box problem is a geometric combinatorial optimization problem, different from most of the problems considered in this random setting that are computations of some measure or structure of the extent of the geometric objects.

For 𝑑=1, the maximum box problem asks for an interval of the line. If the points are uncertain as described above, then it is equivalent to consider as input a sequence of random numbers, where each number has two possible outcomes: zero if the number is not present and the actual value of the number otherwise. The output is the subsequence of consecutive numbers with maximum sum. We consider the simpler case when the subsequence is a partial sum, that is, it contains the first (or leftmost) number of the sequence. More formally: We say that a random variable X is zero-value if 𝑋=𝑣 with probability 𝜌, and 𝑋=0 with probability 1−𝜌, for an integer number 𝑣=𝑣(𝑋)≠0 and a probability 𝜌. We refer to v as the value of X and to 𝜌 as the probability of X. In any sequence of zero-value variables, all variables are assumed to be mutually independent. Let =𝑋1,𝑋2,…,𝑋𝑛 be a sequence of n mutually independent zero-value variables, whose values are 𝑎1,𝑎2,…,𝑎𝑛, respectively. We study the random variable


𝖲() = max{0,𝑋1,𝑋1+𝑋2,…,𝑋1+⋯+𝑋𝑛},

which is the maximum partial sum of the random sequence . The fact that 𝔼[max{𝑋,𝑌}] is not necessarily max{𝔼[𝑋],𝔼[𝑌]}, even if X and Y are independent random variables, makes hard the computation of the expectation 𝔼[𝖲()].

Kleinberg et al. [13] proved that the problem of computing Pr[𝑋1+⋯+𝑋𝑛>1] is #P-complete, in the case where the values of the variables 𝑋1,𝑋2,…,𝑋𝑛 are all positive. The proof can be straightforwardly adapted to also show that computing Pr[𝑋1+⋯+𝑋𝑛>𝑧] is #P-complete, where the values of 𝑋1,𝑋2,…,𝑋𝑛 are all positive, for any fixed 𝑧>0. This last fact implies that computing Pr[𝖲()≥𝑧] for any fixed 𝑧≥1 is #P-hard. We show hardness results when the probabilities of 𝑋1,𝑋2,…,𝑋𝑛 are the same, and their values are not necessarily positive. Namely, we prove (Sect. 2.1) that computing Pr[𝖲()≥𝑧] for any fixed 𝑧≥1, and computing the expectation 𝔼[𝖲()] are both #P-hard problems, even if all variables of  have the same less-than-one probability. When 𝑎1,𝑎2,…,𝑎𝑛∈[−𝑎..𝑏] for bounded 𝑎,𝑏∈ℕ, we show (Sect. 2.2) that both Pr[𝖲()≥𝑧] and 𝔼[𝖲()] can be computed in time polynomial in n, a, and b. For two integers 𝑢<𝑣, we use [u..v] to denote the set {𝑢,𝑢+1,…,𝑣}.

For 𝑑=2, we consider the maximum box problem in the context of red and blue points, where red points have weight +1 and blue points weight −∞. Let R and B be disjoint finite point sets in the plane with a total of n points, where the elements of R are colored red and the elements of B are colored blue. The maximum box problem asks for a box H such that |𝐻∩𝑅| is maximized subject to |𝐻∩𝐵|=∅. This problem has been well studied, with algorithms whose running times go from 𝑂(𝑛2log𝑛) [6], 𝑂(𝑛2) [3], to 𝑂(𝑛log3𝑛) [2]. Let 𝑆⊆𝑅∪𝐵 be the random point set where every point 𝑝∈𝑅∪𝐵 is included in S independently and uniformly at random with probability 𝜋(𝑝)∈[0,1]. Let 𝖻𝗈𝗑(𝑆) denote the random variable equal to the maximum number of red points in S that can be covered with a box not covering any blue point of S.

We prove (Sect. 3.1) that computing the probability Pr[𝖻𝗈𝗑(𝑆)≥𝑘] for any given 𝑘≥2, and computing the expectation 𝔼[𝖻𝗈𝗑(𝑆)], are both #P-hard problems. We further show (Sect. 3.2) that given a point o of the plane, computing the probability that there exists a box containing exactly two red points of S, no blue point of S, and the point o can be solved in polynomial time. If we remove the restriction of containing o, this problem is also #P-hard. This fact is a direct consequence of the previous #P-hardness proofs.

In all running time upper bounds in this paper, in both algorithms and reductions, we assume a real RAM model of computation where each arithmetic operation on large-precision numbers takes constant time. Otherwise, the running times should be multiplied by a factor proportional to the bit complexity of the numbers, which is polynomial in n and the bit complexity of the input probability values [5, 11].

Weighted Points
Hardness
Theorem 1
For any integer 𝑧≥1 and any 𝜌∈(0,1), the following problem is #P-hard: Given a sequence =𝑋1,𝑋2,…,𝑋𝑛 of n zero-value random variables, each with probability 𝜌, compute Pr[𝖲()≥𝑧].

Proof
Let 𝑧≥1 be an integer, and 𝜌∈(0,1) be a probability. We show a Turing reduction from the #SubsetSum problem, which is known to be #P-complete [8]. Our reduction assumes an unknown algorithm (i.e., oracle) () computing Pr[𝖲()≥𝑧] for any finite sequence  of zero-value random variables, that will be called twice. #SubsetSum receives as input a set {𝑎1,…,𝑎𝑛}⊂ℕ of n numbers and a target 𝑡∈ℕ, and counts the number of subsets 𝐽⊆[1..𝑛] such that ∑𝑗∈𝐽𝑎𝑗=𝑡. It remains #P-hard if the subsets J must also satisfy |𝐽|=𝑘, for given 𝑘∈[1..𝑛]. Let ({𝑎1,…,𝑎𝑛},𝑡,𝑘) be an instance of this #SubsetSum, in which we assume 𝑡≤𝑎1+⋯+𝑎𝑛.

Let 𝑚=max{𝑧,1+𝑎1+⋯+𝑎𝑛}>𝑡, and =𝑋0,𝑋1,𝑋2,…,𝑋𝑛 be a sequence of 𝑛+1 zero-value random variables, each with probability 𝜌, where the value of 𝑋0 is −𝑘𝑚−𝑡+𝑧, and the value of 𝑋𝑖 is 𝑚+𝑎𝑖 for every 𝑖∈[1..𝑛]. Observe that for 𝐽⊆[1..𝑛] we have


∑𝑗∈𝐽(𝑚+𝑎𝑗)=𝑘𝑚+𝑡 ⇔ (∑𝑗∈𝐽𝑎𝑗=𝑡 and |𝐽|=𝑘).

Furthermore, |𝐽|>𝑘 implies ∑𝑗∈𝐽(𝑚+𝑎𝑗)>𝑘𝑚+𝑡.

Let 𝐽={𝑗∈[1..𝑛]:𝑋𝑗≠0}, and for any s, let


𝑁𝑠 = ∣∣∣𝐽⊆[1..𝑛]:|𝐽|=𝑘,∑𝑗∈𝐽𝑎𝑗≥𝑠∣∣∣.

Then, #SubsetSum asks for 𝑁𝑡−𝑁𝑡+1. Call () to compute Pr[𝖲()≥𝑧]. Then:


Pr[𝖲()≥𝑧] = Pr[𝖲()≥𝑧,𝑋0=0]+Pr[𝖲()≥𝑧,𝑋0=−𝑘𝑚−𝑡+𝑧]

where,


Pr[𝖲()≥𝑧,𝑋0=0] = Pr[𝑋0=0]⋅Pr[𝖲()≥𝑧∣𝑋0=0] = (1−𝜌)⋅Pr[|𝐽|≥1] = (1−𝜌)⋅(1−Pr[|𝐽|=0]) = (1−𝜌)⋅(1−(1−𝜌)𝑛),

and


Pr[𝖲()≥𝑧,𝑋0=−𝑘𝑚−𝑡+𝑧]= Pr[𝑋0=−𝑘𝑚−𝑡+𝑧]⋅Pr[𝖲()≥𝑧∣𝑋0=−𝑘𝑚−𝑡+𝑧]= 𝜌⋅Pr[−𝑘𝑚−𝑡+𝑧+∑𝑗∈𝐽(𝑚+𝑎𝑗)≥𝑧]=𝜌⋅Pr[∑𝑗∈𝐽(𝑚+𝑎𝑗)≥𝑘𝑚+𝑡]= 𝜌⋅(Pr[|𝐽|=𝑘,∑𝑗∈𝐽(𝑚+𝑎𝑗)≥𝑘𝑚+𝑡]+∑𝑖=𝑘+1𝑛Pr[|𝐽|=𝑖,∑𝑗∈𝐽(𝑚+𝑎𝑗)≥𝑘𝑚+𝑡])= 𝜌⋅Pr[|𝐽|=𝑘,∑𝑗∈𝐽𝑎𝑗≥𝑡]+𝜌⋅∑𝑖=𝑘+1𝑛Pr[|𝐽|=𝑖]= 𝜌⋅𝑁𝑡⋅𝜌𝑘⋅(1−𝜌)𝑛−𝑘+𝜌⋅∑𝑖=𝑘+1𝑛(𝑛𝑖)⋅𝜌𝑖⋅(1−𝜌)𝑛−𝑖.

Hence, we can compute 𝑁𝑡 in polynomial time from the value of Pr[𝖲()≥𝑧]. Consider now the random sequence ′=𝑋′0,𝑋1,𝑋2,…,𝑋𝑛, where 𝑋′0 has value −𝑘𝑚−(𝑡+1)+𝑧. Using arguments similar to those above, by calling (′) to compute Pr[𝖲(′)≥𝑧], we can compute 𝑁𝑡+1 in polynomial time from this probability. Then, 𝑁𝑡−𝑁𝑡+1 can be computed in polynomial time, plus the time of calling twice the oracle . This implies the theorem. ◻

Theorem 2
For any 𝜌∈(0,1), the following problem is #P-hard: Given a sequence =𝑋1,…,𝑋𝑛 of n zero-value random variables, each with probability 𝜌, compute 𝔼[𝖲()].

Proof
Let =𝑋1,𝑋2,…,𝑋𝑛 be a sequence of zero-value random variables, each with probability 𝜌, and consider the sequence ′=𝑋0,𝑋1,…,𝑋𝑛, where 𝑋0 is a zero-value random variable with value −1 and probability 𝜌. Let w be the sum of the positive values among the values of 𝑋1,…,𝑋𝑛. Then:


𝔼[𝖲()] = ∑𝑖=1𝑤𝑖⋅Pr[𝖲()=𝑖] = ∑𝑖=1𝑤Pr[𝖲()≥𝑖],

and


𝔼[𝖲(′)] = ∑𝑖=1𝑤Pr[𝖲(′)≥𝑖] = ∑𝑖=1𝑤(Pr[𝖲(′)≥𝑖,𝑋0=0]+Pr[𝖲(′)≥𝑖,𝑋0=−1]) = ∑𝑖=1𝑤(Pr[𝑋0=0]⋅Pr[𝖲(′)≥𝑖∣𝑋0=0] +Pr[𝑋0=−1]⋅Pr[𝖲(′)≥𝑖∣𝑋0=−1]) = ∑𝑖=1𝑤((1−𝜌)⋅Pr[𝖲()≥𝑖]+𝜌⋅Pr[𝖲()≥𝑖+1]) = ∑𝑖=1𝑤(1−𝜌)⋅Pr[𝖲()≥𝑖]+∑𝑖=2𝑤+1𝜌⋅Pr[𝖲()≥𝑖] = (1−𝜌)⋅Pr[𝖲()≥1]+∑𝑖=2𝑤Pr[𝖲()≥𝑖].

Then, we have that


𝔼[𝖲()]−𝔼[𝖲(′)] = 𝜌⋅Pr[𝖲()≥1].

Since computing Pr[𝖲()≥1] is #P-hard (Theorem 1), then computing 𝔼[𝖲()] is also #P-hard via a Turing reduction. ◻

Pseudo-Polynomial Time Algorithms
Let =𝑋1,𝑋2,…,𝑋𝑛 be a sequence of n zero-value random variables, with values 𝑎1,𝑎2,…,𝑎𝑛∈[−𝑎..𝑏]⊂ℤ and probabilities 𝜌1,𝜌2,…,𝜌𝑛, respectively, for some 𝑎,𝑏∈ℕ. We show that both Pr[𝖲()≥𝑧] and 𝔼[𝖲()] can be computed in time polynomial in n, a, and b. Let 𝐽={𝑗∈[1..𝑛]:𝑎𝑗<0} and


𝑤0 = ∑𝑗∈𝐽|𝑎𝑗| = 𝑂(𝑛𝑎)  and  𝑤1 =∑𝑗∈[1..𝑛]∖𝐽𝑎𝑗 = 𝑂(𝑛𝑏).

For every 𝑡∈[1..𝑛], let


𝑆𝑡𝐿𝑡 = 𝑋1+⋯+𝑋𝑡,  𝑀𝑡 = max{0,𝑆1,𝑆2,…,𝑆𝑡},  and = {Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠]:𝑘∈[0..𝑤1],𝑠∈[−𝑤0..𝑤1],𝑘≥𝑠}.

Observe that 𝐿𝑡 has size 𝑂(𝑤1(𝑤0+𝑤1))=𝑂(𝑛𝑏(𝑛𝑎+𝑛𝑏))=𝑂(𝑛2𝑏(𝑎+𝑏)) for every t, and that 𝐿1 can be trivially computed. Using the dynamic programming algorithm design paradigm, we next show how to compute the values of 𝐿𝑡, 𝑡≥2, assuming that all values of 𝐿𝑡−1 have been computed. Note that:


Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠]=Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠,𝑋𝑡=0]+Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠,𝑋𝑡=𝑎𝑡],

where


Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠,𝑋𝑡=0] = Pr[𝑋𝑡=0]⋅Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠∣𝑋𝑡=0] = (1−𝜌𝑡)⋅Pr[𝑀𝑡−1=𝑘,𝑆𝑡−1=𝑠]

and


Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠,𝑋𝑡=𝑎𝑡] = Pr[𝑋𝑡=𝑎𝑡]⋅Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠∣𝑋𝑡=𝑎𝑡] = 𝜌𝑡⋅Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠∣𝑋𝑡=𝑎𝑡].

When 𝑘=𝑠, we have for 𝑎𝑡<0 that Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠∣𝑋𝑡=𝑎𝑡]=0, since this event indicates that 𝑆𝑡=𝑋1+⋯+𝑋𝑡 is a maximum partial sum of 𝑋1,…,𝑋𝑡, but this cannot happen because any maximum partial sum ends in a positive element. For 𝑎𝑡>0 we have


Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠∣𝑋𝑡=𝑎𝑡] = Pr[𝑀𝑡−1≤𝑘,𝑆𝑡−1=𝑠−𝑎𝑡] = ∑𝑖=𝑠−𝑎𝑡𝑘Pr[𝑀𝑡−1=𝑖,𝑆𝑡−1=𝑠−𝑎𝑡].

When 𝑘>𝑠, 𝑀𝑡 does not count the element 𝑎𝑡, hence 𝑀𝑡−1=𝑀𝑡. Then


Pr[𝑀𝑡=𝑘,𝑆𝑡=𝑠∣𝑋𝑡=𝑎𝑡] = Pr[𝑀𝑡−1=𝑘,𝑆𝑡−1=𝑠−𝑎𝑡].

Modeling each set 𝐿𝑡 as a 2-dimensional table (or array), note that each value of 𝐿𝑡 can be computed in 𝑂(𝑘−(𝑠−𝑎𝑡))=𝑂(𝑤1) time, and hence all values of 𝐿𝑡 can be computed in 𝑂(𝑤1)⋅𝑂(𝑛2𝑏(𝑎+𝑏))=𝑂(𝑛3𝑏2(𝑎+𝑏)) time. Finally, once all the values of 𝐿𝑛 have been computed in 𝑂(𝑛)⋅𝑂(𝑛3𝑏2(𝑎+𝑏))=𝑂(𝑛4𝑏2(𝑎+𝑏)) time, we can compute Pr[𝖲()≥𝑧] as


Pr[𝖲()≥𝑧] = ∑𝑘=𝑧𝑤1Pr[𝖲()=𝑘] = ∑𝑘=𝑧𝑤1∑𝑠=−𝑤0𝑘Pr[𝑀𝑛=𝑘,𝑆𝑛=𝑠]

in 𝑂(𝑤1(𝑤0+𝑤1))=𝑂(𝑛2𝑏(𝑎+𝑏)) time, and 𝔼[𝖲()] as


𝔼[𝖲()] = ∑𝑧=1𝑤1Pr[𝖲()≥𝑧]

in 𝑂(𝑤1)=𝑂(𝑛𝑏) time. As a consequence, we get the following result.

Theorem 3
Let =𝑋1,𝑋2,…,𝑋𝑛 be a sequence of n zero-value random variables, with values 𝑎1,𝑎2,…,𝑎𝑛∈[−𝑎..𝑏]⊂ℤ and probabilities 𝜌1,𝜌2,…,𝜌𝑛, respectively, for some 𝑎,𝑏∈ℕ. Then, both Pr[𝖲()≥𝑧] and 𝔼[𝖲()] can be computed in time polynomial in n, a, and b.

Red and Blue Points
Hardness
Given a graph 𝐺=(𝑉,𝐸), a subset 𝑉′⊆𝑉 is an independent set of G if no pair of vertices of 𝑉′ define an edge in E. Let N(G) denote the number of independent sets of G. The problem #IndSet of counting the number of independent sets in a graph is #P-complete, even if the graph is planar, bipartite, and with maximum degree 4 [18]. We show in what follows a one-to-many Turing reduction from #IndSet to the problem of computing Pr[𝖻𝗈𝗑(𝑆)≥𝑘], for any given 𝑘≥2. The proof uses techniques similar to that of Kamousi et al. [11] to prove that counting vertex covers in weighted unit-disk graphs is #P-hard, and that of Vadhan [18] to prove that counting weighted matchings in planar bipartite graphs is #P-hard.

Let 𝐺=(𝑉,𝐸) be the input of #IndSet, where G is a planar, bipartite graph with maximum degree 4. Let 𝑛=|𝑉| and 𝑚=|𝐸|=𝑂(𝑛). For an overview of the whole proof, refer to Fig. 1.

Fig. 1
figure 1
Given an instance 𝐺=(𝑉,𝐸) of #IndSet, for every s in the set {ℎ,ℎ+1,…,ℎ+𝑚} of 𝑚+1 integers polynomially bounded in n, we construct the random colored point set 𝑅𝑠∪𝐵𝑠. Let 𝐺𝑠 be the graph obtained from G by subdividing each edge with exactly s new vertices. Assuming that an oracle computes Pr[𝖻𝗈𝗑(𝑆)≥2], where 𝑆⊆𝑅𝑠∪𝐵𝑠 is the random sample, we show that 𝑁(𝐺𝑠) can be computed in constant time from this probability. From all of the values 𝑁(𝐺ℎ),𝑁(𝐺ℎ+1),…,𝑁(𝐺ℎ+𝑚) already computed, we show that N(G) can be computed in polynomial time

Full size image

For any subset 𝑉′⊆𝑉 and any edge 𝑒={𝑢,𝑣}∈𝐸, we say that 𝑉′ 1-covers edge e if exactly one of u and v belongs to 𝑉′. We also say that 𝑉′ 2-covers e if both u and v belong to 𝑉′. Let 𝐶𝑖,𝑗 denote the number of subsets of V that 1-cover exactly i edges and 2-cover exactly j edges. Then,

For 𝑠≥1, let 𝐺𝑠=(𝑉𝑠,𝐸𝑠) be the graph obtained from G by adding exactly s intermediate vertices on each edge of E. Let {𝑓𝑖}∞𝑖=1 be the Fibonacci sequence, with 𝑓1=𝑓2=1 and 𝑓𝑖=𝑓𝑖−1+𝑓𝑖−2 for 𝑖≥3. Let 𝛼𝑖=𝑓𝑖+1/𝑓𝑖+2 for 𝑖≥0. The next lemma relates the number 𝑁(𝐺𝑠) of independent sets of 𝐺𝑠 to the values 𝐶𝑖,𝑗 in G.

Lemma 4
We have


𝑁(𝐺𝑠) = (𝑓𝑠+2)𝑚⋅∑0≤𝑖+𝑗≤𝑚𝐶𝑖,𝑗⋅(𝛼𝑠)𝑖⋅(1−𝛼𝑠)𝑗.

Proof
Any independent set 𝑉′𝑠⊆𝑉𝑠 of 𝐺𝑠 induces the subset 𝑉′𝑠∩𝑉 of V, which is not necessarily an independent set of G because it may 2-cover some edges. Let 𝑉′⊆𝑉 be any subset of V that 1-covers i edges and 2-covers j edges. For any edge 𝑒∈𝐸, let 𝑝𝑒 denote the path induced by the s vertices added to e when constructing 𝐺𝑠 from G. An independent set of 𝐺𝑠 inducing 𝑉′ can be obtained by starting with 𝑉′ and adding vertices in the following way. For every edge 𝑒={𝑢,𝑣}∈𝐸:

(1)
if 𝑉′ neither 1-covers nor 2-covers e, then add any independent set of 𝑝𝑒.

(2)
if 𝑉′ 1-covers e, say 𝑢∈𝑉′, then add any independent set of 𝑝𝑒 not containing the extreme vertex of 𝑝𝑒 adjacent to u in 𝐺𝑠.

(3)
if 𝑉′ 2-covers e, then add any independent set of 𝑝𝑒 with no extreme vertex.

It is well known that the number of independent sets of a path of length ℓ is exactly 𝑓ℓ+3 [18]. Since 𝑝𝑒 has length 𝑠−1 for every e, the number of choices for cases (1), (2), and (3) are 𝑓𝑠+2, 𝑓𝑠+1, and 𝑓𝑠, respectively. Therefore, the number of independent sets of 𝐺𝑠 inducing a subset of V that 1-covers i edges and 2-covers j edges is precisely 𝐶𝑖,𝑗⋅(𝑓𝑠+1)𝑖⋅(𝑓𝑠)𝑗⋅(𝑓𝑠+2)𝑚−𝑖−𝑗. Hence, the number 𝑁(𝐺𝑠) of independent sets of 𝐺𝑠 satisfies


𝑁(𝐺𝑠) = ∑0≤𝑖+𝑗≤𝑚𝐶𝑖,𝑗⋅(𝑓𝑠+1)𝑖⋅(𝑓𝑠)𝑗⋅(𝑓𝑠+2)𝑚−𝑖−𝑗 = (𝑓𝑠+2)𝑚⋅∑0≤𝑖+𝑗≤𝑚𝐶𝑖,𝑗⋅(𝑓𝑠+1𝑓𝑠+2)𝑖⋅(𝑓𝑠𝑓𝑠+2)𝑗 = (𝑓𝑠+2)𝑚⋅∑0≤𝑖+𝑗≤𝑚𝐶𝑖,𝑗⋅(𝑓𝑠+1𝑓𝑠+2)𝑖⋅(1−𝑓𝑠+1𝑓𝑠+2)𝑗 = (𝑓𝑠+2)𝑚⋅∑0≤𝑖+𝑗≤𝑚𝐶𝑖,𝑗⋅(𝛼𝑠)𝑖⋅(1−𝛼𝑠)𝑗,

which completes the proof. ◻

Lemma 5
Let T be a set of 𝑚+1 integers, each bounded by a polynomial in n. If we know the value of 𝑁(𝐺𝑠) for every 𝑠∈𝑇, then the number N(G) can be computed in time polynomial in n.

Proof
For every 𝑠∈𝑇, the value of (𝑓𝑠+2)𝑚 can be computed in 𝑂(log𝑠+log𝑚)=𝑂(log𝑛) time, and the value of 𝛼𝑠 also in 𝑂(log𝑠)=𝑂(log𝑛) time. Let 𝑏𝑠=𝑁(𝐺𝑠)/(𝑓𝑠+2)𝑚 for every 𝑠∈𝑇. Consider the polynomial


𝑃(𝑥) = ∑0≤𝑖+𝑗≤𝑚𝐶𝑖,𝑗⋅𝑥𝑖⋅(1−𝑥)𝑗 = 𝑎0+𝑎1𝑥+𝑎2𝑥2+⋯+𝑎𝑚𝑥𝑚,

of degree m, whose coefficients 𝑎0,𝑎1,…,𝑎𝑚 are linear combinations of the terms 𝐶𝑖,𝑗. By Lemma 4, and using the known values of 𝑏𝑠 and 𝛼𝑠 for every 𝑠∈𝑇, we have 𝑚+1 evaluations of P(x) of the form 𝑏𝑠=𝑃(𝛼𝑠), each corresponding to the linear equation 𝑏𝑠=𝑎0+𝑎1⋅𝛼𝑠+𝑎2⋅𝛼2𝑠+⋯+𝑎𝑚⋅𝛼𝑚𝑠 with variables the coefficients 𝑎0,𝑎1,…,𝑎𝑚. The main matrix of this system of 𝑚+1 linear equations is Vandermonde, with parameters 𝛼𝑠 for every 𝑠∈𝑇. All 𝛼𝑠 are distinct (refer to [18] or Appendix A for completeness), so the determinant of the main matrix is non-zero, and the system has a unique solution 𝑎0,𝑎1,…,𝑎𝑚, which can be computed in time polynomial in n. Finally, observe that for 𝑗=0, the coefficient of the polynomial 𝑃(𝑥)=𝐶𝑖,𝑗⋅𝑥𝑖⋅(1−𝑥)𝑗=𝐶𝑖,0⋅𝑥𝑖 is 𝐶𝑖,0. Furthermore, for 𝑗>0, all the coefficients of the polynomial


𝑃(𝑥) == 𝐶𝑖,𝑗⋅𝑥𝑖⋅(1−𝑥)𝑗  𝐶𝑖,𝑗⋅𝑥𝑖⋅((𝑗0)−(𝑗1)𝑥1+(𝑗2)𝑥2−⋯+(−1)𝑗(𝑗𝑗)𝑥𝑗)

sum up to zero. Indeed, it suffices to note that 𝑃(1)=0. Hence, we obtain


𝑎0+𝑎1+𝑎2+⋯+𝑎𝑚 = ∑𝑖=0𝑚𝐶𝑖,0 = 𝑁(𝐺).

which shows that N(G) can be computed in time polynomial in n. ◻

In polynomial time, the graph 𝐺=(𝑉,𝐸) can be embedded in the plane using 𝑂(𝑛2) area in such a way that its vertices are at integer coordinates, and its edges are drawn so that they are polylines made up of line segments of the form 𝑥=𝑖 or 𝑦=𝑗, for integers i and j [19] (see Fig. 2a). Let ℎ=𝑂(𝑛) be the maximum number of bends of the polylines corresponding to the edges.

Fig. 2
figure 2
a An embedding of G. b The embedding of 𝐺𝑠 for 𝑠=2: two intermediate vertices are added to each edge of G so that all polyline bends are covered

Full size image

For 𝑠=ℎ,ℎ+1,…,ℎ+𝑚, we embed the graph 𝐺𝑠 in the following way. We embed the graph G as above; scale the embedding by factor 2(𝑠+1); and for each edge of G, add s intermediate vertices to the polyline of the edge so that they have even integer coordinates and cover all bends of the polyline (see Fig. 2b). Then, each edge of 𝐺𝑠 is represented in the embedding by a vertical or horizontal segment. Let the point set 𝑅0=𝑅0(𝑠)⊂ℤ2 denote the vertex set of the embedding, and color these points in red. By translation if necessary, we can assume 𝑅0⊂[0..𝑁]2 for some 𝑁=𝑂(𝑛2). Let 𝐵0=𝐵0(𝑠) be the following set of blue points: For each horizontal or vertical line ℓ through a point of 𝑅0, and each two consecutive points 𝑝,𝑞∈𝑅0 in ℓ such that the vertices p and q are not adjacent in 𝐺𝑠, we add a blue point in the segment pq connecting p and q, in order to “block” this segment, so that the blue point has one odd coordinate. In this way, blue points blocking horizontal segments have odd x-coordinates and even y-coordinates; and blue points blocking vertical segments have even x-coordinates and odd y-coordinates. Hence, a blue point cannot block at the same time a horizontal and a vertical segment defined by two red points. Note that |𝐵0|=𝑂(|𝑅0|)=𝑂(𝑛+𝑚⋅𝑠)=𝑂(𝑛2). Now, a horizontal or vertical segment connecting two points p and q of 𝑅0∪𝐵0 represents an edge of 𝐺𝑠 if and only if 𝑝,𝑞∈𝑅0 and the segment does not contain any other point of 𝑅0∪𝐵0 in its interior (see Fig. 4).

We perturb 𝑅0∪𝐵0⊂[0..𝑁]2 to obtain a point set with rational coordinates by applying the function 𝜆:[0..𝑁]2→ℚ2, where


𝜆(𝑝)=(𝑥(𝑝)+𝑥(𝑝)+𝑦(𝑝)4𝑁+1,𝑦(𝑝)+𝑥(𝑝)+𝑦(𝑝)4𝑁+1),

to every 𝑝∈𝑅0∪𝐵0, where x(p) and y(p) denote the x- and y-coordinates of p, respectively. Similar perturbations can be found in [1, 4], and refer to Fig. 3. Since 𝜆 is injective [4], let 𝜆−1 denote the inverse of 𝜆. For 𝐴⊂[0..𝑁]2, let 𝜆(𝐴)={𝜆(𝑝)∣𝑝∈𝐴}, and for 𝐴′⊂𝜆([0..𝑁]2) let 𝜆−1(𝐴′)={𝜆−1(𝑝)∣𝑝∈𝐴′}. Let 𝛿=1/(4𝑁+2), and define the sets


𝑅𝑠 = 𝜆(𝑅0)  and  𝐵𝑠 = 𝜆(𝐵0)∪{𝑝+(1/2,1/2),𝑝+(𝛿,−𝛿)∣𝑝∈𝑅𝑠}.

To simplify the notation, let 𝑅=𝑅𝑠 and 𝐵=𝐵𝑠. Note that |𝑅|=𝑂(𝑛2) and |𝐵|=𝑂(𝑛2). For two points a and b, let D(a, b) be the box with the segment ab as a diagonal. The proof of the next technical lemma is deferred to Appendix B.

Fig. 3
figure 3
The way in which points are perturbed using function 𝜆

Full size image

Fig. 4
figure 4
The point set 𝑅0∪𝐵0 for the embedding 𝐺𝑠, 𝑠=2, depicted in Fig. 2b. The grid lines are equally spaced in two units, then their intersection points have even coordinates. After the perturbation with the function 𝜆 (see Fig. 3), the extra blue points {𝑝+(1/2,1/2),𝑝+(𝛿,−𝛿)∣𝑝∈𝑅𝑠} are included in 𝐵𝑠 to avoid that the box 𝐷(𝜆(𝑝),𝜆(𝑞)) contains no blue point, for any two points 𝑝,𝑞∈𝑅0 which do not belong to the same edge of the embedding. See for example the points p and q, denoted in the figure (Color figure online)

Full size image

Lemma 6
For any different 𝑝,𝑞∈𝑅, the vertices 𝜆−1(𝑝) and 𝜆−1(𝑞) are adjacent in 𝐺𝑠 if and only if the box D(p, q) contains no point of B.

Theorem 7
Given 𝑅∪𝐵, it is #P-hard to compute Pr[𝖻𝗈𝗑(𝑆)≥𝑘] for every integer 𝑘≥2, and it is also #P-hard to compute 𝔼[𝖻𝗈𝗑(𝑆)].

Proof
Let 𝑘=2. Assume that there exists an algorithm (i.e., oracle)  that computes Pr[𝖻𝗈𝗑(𝑆)≥2]. Consider the planar bipartite graph 𝐺=(𝑉,𝐸), with maximum degree 4, the input of #IndSet. Let 𝑇={ℎ,ℎ+1,…,ℎ+𝑚}. For each 𝑠∈𝑇 we create the graph 𝐺𝑠, embed 𝐺𝑠 in the plane, and build the colored point set 𝑅∪𝐵=𝑅𝑠∪𝐵𝑠 from this embedding. For each red point 𝑝∈𝑅 we set its probability 𝜋(𝑝) to 1/2, and for each blue point 𝑞∈𝐵 we set 𝜋(𝑞)=1. Note from Lemma 6 that there does not exist any box containing more than two red points of R and no blue point from B. Then, we have Pr[𝖻𝗈𝗑(𝑆)≥2]=Pr[𝖻𝗈𝗑(𝑆)=2], where 𝑆⊆𝑅∪𝐵 is the random subset of 𝑅∪𝐵. Furthermore,


Pr[𝖻𝗈𝗑(𝑆)=2]===𝑁(𝐺𝑠)=Pr[𝜆−1(𝑆∩𝑅) is not an independent set in 𝐺𝑠]1−Pr[𝜆−1(𝑆∩𝑅) is an independent set in 𝐺𝑠]1−𝑁(𝐺𝑠)2|𝑅|2|𝑅|⋅(1−Pr[𝖻𝗈𝗑(𝑆)≥2]).

Then, for each 𝑠∈𝑇 we can compute 𝑁(𝐺𝑠) by calling  once. By Lemma 5, we can compute N(G) from the 𝑚+1 computed values of 𝑁(𝐺𝑠) for each 𝑠∈𝑇. Hence, it is #P-hard to compute Pr[𝖻𝗈𝗑(𝑆)≥2] via a Turing reduction from #IndSet. To show that computing 𝔼[𝖻𝗈𝗑(𝑆)] is also #P-hard, for each 𝑠∈𝑇 consider the above point set 𝑅∪𝐵 and note that


𝔼[𝖻𝗈𝗑(𝑆)]==𝑁(𝐺𝑠)=1⋅Pr[𝜆−1(𝑆∩𝑅) is an independent set in 𝐺𝑠,𝑆∩𝑅≠∅]+2⋅Pr[𝜆−1(𝑆∩𝑅) is not an independent set in 𝐺𝑠]𝑁(𝐺𝑠)−12|𝑅|+2⋅(1−𝑁(𝐺𝑠)2|𝑅|) = 2−𝑁(𝐺𝑠)+12|𝑅|2|𝑅|⋅(2−𝔼[𝖻𝗈𝗑(𝑆)])−1.

Let now 𝑘≥3. For each 𝑠∈𝑇, the graph 𝐺𝑠 can be colored with two colors, 0 and 1, because it is also a bipartite graph. Each red point in R corresponds to a vertex in 𝐺𝑠. Then, for each red point 𝑝∈𝑅 with color 0 we add new ⌊𝑘2⌋−1 red points close enough to p (say, at distance much smaller than 𝛿), and for each red point 𝑞∈𝑅 with color 1 we add new ⌈𝑘2⌉−1 red points close enough to q. Let 𝑅′=𝑅′(𝑠) be the set of all new red points, and assign 𝜋(𝑢)=1 for every 𝑢∈𝑅′. In this new colored point set 𝑅∪𝑅′∪𝐵, there is no box containing more than k red points and no blue point. Furthermore, every box containing exactly k red points and no blue point contains two points 𝑝,𝑞∈𝑅 such that 𝜆−1(𝑝) and 𝜆−1(𝑞) are adjacent in 𝐺𝑠; and for every 𝑝,𝑞∈𝑅 such that 𝜆−1(𝑝) and 𝜆−1(𝑞) are adjacent in 𝐺𝑠 such a box containing p and q exists. Then, when taking 𝑆⊆𝑅∪𝑅′∪𝐵 at random, we also have


Pr[𝖻𝗈𝗑(𝑆)≥𝑘]===Pr[𝖻𝗈𝗑(𝑆)=𝑘]Pr[𝜆−1(𝑆∩𝑅) is not an independent set in 𝐺𝑠]1−𝑁(𝐺𝑠)2|𝑅|.

Hence, computing Pr[𝖻𝗈𝗑(𝑆)≥𝑘] is also #P-hard for any 𝑘≥3. ◻

Two-Point Boxes
From the proof of Theorem 7, note that it is also #P-hard to compute the probability that in 𝑆⊆𝑅∪𝐵 there exists a box that contains exactly two red points p, q and no blue point; and that this box can be restricted to be the minimum box D(p, q) having p and q as opposed vertices. In this section, we present a polynomial-time algorithm to compute such a probability when the box is further restricted to contain a given point 𝑜∉𝑅∪𝐵 of the plane in the interior. We assume general position, that is, there are no two points of 𝑅∪𝐵∪{𝑜} with the same x- or y-coordinate. We further assume w.l.o.g. that o is the origin of coordinates.

Given a fixed 𝑋⊆𝑅∪𝐵, and 𝑆⊆𝑅∪𝐵 chosen at random, let 𝐸(𝑋)=𝐸(𝑋,𝑆) be the event that there exist two red points 𝑝,𝑞∈𝑆∩𝑋 such that the box D(p, q) contains the origin o, no other red point in 𝑆∩𝑋, and no blue in 𝑆∩𝑋. Then, our goal is Pr[𝐸(𝑅∪𝐵)].

Theorem 8
Given 𝑅∪𝐵, Pr[𝐸(𝑅∪𝐵)] can be computed in polynomial time.

Proof
Let 𝑋⊆𝑅∪𝐵, and define 𝑋+={𝑝∈𝑋∣𝑦(𝑝)>0} and 𝑋−={𝑝∈𝑋∣𝑦(𝑝)<0}. Given points 𝑞∈𝑋+ and 𝑟∈𝑋−, define the events


𝑈𝑞(𝑋,𝑆)=[𝑞=argmin𝑝∈𝑋+∩𝑆{𝑦(𝑝)}]  and  𝑊𝑟(𝑋,𝑆)=[𝑟=argmax𝑝∈𝑋−∩𝑆{𝑦(𝑝)}].

Let 𝑈𝑞(𝑋)=𝑈𝑞(𝑋,𝑆) and 𝑊𝑟(𝑋)=𝑊𝑟(𝑋,𝑆). Using the formula of the total probability, we have:


Pr[𝐸(𝑋)]==∑𝑞∈𝑋+Pr[𝐸(𝑋)∣𝑈𝑞(𝑋)]⋅Pr[𝑈𝑞(𝑋)]∑𝑞∈𝑋+Pr[𝐸(𝑋)∣𝑈𝑞(𝑋)]⋅(𝜋(𝑞)⋅∏𝑝∈𝑋+:𝑦(𝑝)<𝑦(𝑞)(1−𝜋(𝑝))).

To compute Pr[𝐸(𝑋)∣𝑈𝑞(𝑋)], we assume 𝑥(𝑞)>0. The case where 𝑥(𝑞)<0 is symmetric. If 𝑞∈𝐵, then observe that when restricted to the event 𝑈𝑞(𝑋) any box 𝐷(𝑝′,𝑞′) defined by two red points 𝑝′,𝑞′∈𝑆∩𝑋, containing the origin o and no other red point in 𝑆∩𝑋, where one between 𝑝′ and 𝑞′ is to the right of q, will contain q. Hence, we must “discard” all points to the right of q, all points between the horizontal lines through q and o because they are not present, and q itself. Then, we have that:


Pr[𝐸(𝑋)∣𝑈𝑞(𝑋)] = Pr[𝐸(𝑋𝑞)],

where 𝑋𝑞⊂𝑋 contains the points 𝑝∈𝑋 such that 𝑥(𝑝)<𝑥(𝑞) and either 𝑦(𝑝)>𝑦(𝑞) or 𝑦(𝑝)<0. If 𝑞∈𝑅, we expand Pr[𝐸(𝑋)∣𝑈𝑞(𝑋)] as follows:


Pr[𝐸(𝑋)∣𝑈𝑞(𝑋)] = ∑𝑟∈𝑋−Pr[𝐸(𝑋)∣𝑈𝑞(𝑋),𝑊𝑟(𝑋)]⋅Pr[𝑊𝑟(𝑋)]= ∑𝑟∈𝑋−Pr[𝐸(𝑋)∣𝑈𝑞(𝑋),𝑊𝑟(𝑋)]⋅(𝜋(𝑟)⋅∏𝑝∈𝑋−:𝑦(𝑝)>𝑦(𝑟)(1−𝜋(𝑝))).

There are now three cases according to the relative positions of q and r.

Case 1: 𝑥(𝑟)<0<𝑥(𝑞). Let 𝑌𝑞,𝑟⊂𝑋 contain the points 𝑝∈𝑋 (including q) such that 𝑥(𝑟)<𝑥(𝑝)≤𝑥(𝑞) and either 𝑦(𝑝)<𝑦(𝑟) or 𝑦(𝑝)≥𝑦(𝑞). If 𝑟∈𝑅, then Pr[𝐸(𝑋)∣𝑈𝑞(𝑋),𝑊𝑟(𝑋)]=1. Otherwise, if 𝑟∈𝐵, given that 𝑈𝑞(𝑋) and 𝑊𝑟(𝑋) hold, any box 𝐷(𝑝′,𝑞′) defined by two red points 𝑝′,𝑞′ of 𝑆∩𝑋, containing the origin o and no other red point in 𝑆∩𝑋, where one between 𝑝′ and 𝑞′ is not in 𝑌𝑞,𝑟, will contain q or r in the interior. Then


Pr[𝐸(𝑋)∣𝑈𝑞(𝑋),𝑊𝑟(𝑋)] = Pr[𝐸(𝑌𝑞,𝑟)∣𝑈𝑞(𝑌𝑞,𝑟)].

Similar arguments are given in the next two cases.

Case 2: 0<𝑥(𝑞)<𝑥(𝑟). We have


Pr[𝐸(𝑋)∣𝑈𝑞(𝑋),𝑊𝑟(𝑋)] = Pr[𝐸(𝑋𝑞∪{𝑞})∣𝑈𝑞(𝑋𝑞∪{𝑞})].

Case 3: 0<𝑥(𝑟)<𝑥(𝑞). If 𝑟∈𝑅, then


Pr[𝐸(𝑋)∣𝑈𝑞(𝑋),𝑊𝑟(𝑋)] = Pr[𝐸(𝑍𝑞,𝑟∪{𝑟})∣𝑊𝑟(𝑍𝑞,𝑟∪{𝑟})],

where 𝑍𝑞,𝑟⊂𝑋 contains the points 𝑝∈𝑋 such that 𝑥(𝑝)<𝑥(𝑟) and either 𝑦(𝑝)<𝑦(𝑟) or 𝑦(𝑝)>𝑦(𝑞). Note that the event [𝐸(𝑍𝑞,𝑟∪{𝑟})∣𝑊𝑟(𝑍𝑞,𝑟∪{𝑟})] is symmetric to the event [𝐸(𝑋)∣𝑈𝑞(𝑋)], thus its probability can be computed similarly. Otherwise, if 𝑟∈𝐵, we have


Pr[𝐸(𝑋)∣𝑈𝑞(𝑋),𝑊𝑟(𝑋)] = Pr[𝐸(𝑍𝑞,𝑟)].

Note that in the above recursive computation of Pr[𝐸(𝑋)], for 𝑋=𝑅∪𝐵, there is a polynomial number of subsets 𝑋𝑞, 𝑌𝑞,𝑟, and 𝑍𝑞,𝑟; each of such subsets can be encoded in constant space (i.e., by using a constant number of coordinates). Then, we can use dynamic programming, with a polynomial-size table, to compute Pr[𝐸(𝑅∩𝐵)] in time polynomial in n. ◻

Discussion and Open Problems
For fixed 𝑑≥1, the maximum box problem for non-probabilistic points can be solved in 𝑂(𝑛𝑑) time [3]. This fact, combined with the Monte Carlo method and known techniques, can be used to approximate the expectation of the total weight of the maximum box on probabilistic points, in polynomial time and with high probability of success. That is, we provide a FPRAS, which is explained in Appendix C. Approximating the probability that the total weight of a maximum box is at least a given parameter is an open question of this paper. To this end, we give in Appendix D a FPRAS for approximating this probability, but only in the case where the points are colored red or blue, each with probability 1/2, and we look for the box covering the maximum number of red points and no blue point (i.e. red points have weight +1 and blue points weight −∞).

For 𝑑=2 and red and blue points, there are several open problems: For example, to compute Pr[𝖻𝗈𝗑(𝑆)≥𝑘] (even for 𝑘=3) in 𝑑=2 when the box is restricted to contain a fixed point. Other open questions are the cases in which the box is restricted to contain a given point as vertex, or has some side contained in a given axis-parallel line. This two latter variants can be solved in 𝑛𝑂(𝑘) time (see Appendix E), which means that they are polynomial-time solvable for fixed k. This contrasts with the original question that is #P-hard for every 𝑘≥2.

For red and blue points in 𝑑=1, both Pr[𝖻𝗈𝗑(𝑆)≥𝑘] and 𝔼[𝖻𝗈𝗑(𝑆)] can be computed in polynomial time by using standard dynamic programming techniques. This implies that for 𝑑=2 and a fixed direction, computing the probability that there exists a strip (i.e., the space between two parallel lines) perpendicular to that direction and covering at least k red points and no blue point can be done in polynomial time. If the orientation of the strip is not restricted, then such a computation is #P-hard for every 𝑘≥3 (see Appendix F).