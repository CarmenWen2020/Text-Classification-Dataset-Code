We consider the streaming version of the following problem: given an input string s of length n, find the maximum exponent of a substring of s. We prove that any algorithm deciding, w.h.p., whether a string contains a square, uses memory of size 𝛺(𝑛), and thus does not satisfy the limitations of the streaming model. Thus the considered problem has no exact solution in the streaming model. Our main result is a Monte Carlo algorithm which computes the maximum exponent up to an additive error 𝜀<1/2: it outputs a number 𝛼 such that s has a substring of exponent 𝛼 but no substrings of exponent 𝛼+𝜀 or higher. The algorithm uses (log2𝑛𝜀) words of memory and performs (log𝑛) operations, including dictionary operations, per input symbol.

Access provided by University of Auckland Library

Introduction
The exponent of a string, which is the ratio between its length and its minimum period, is a natural measure of global periodicity. At the same time, the maximum exponent of a substring, also known as local or critical exponent of a string, measures local periodicity. The study of exponents and local exponents of strings can be traced back to the seminal papers by Thue [19, 20]. Here we focus on the algorithmic aspects. In the usual RAM model, the exponent of a string can be easily computed online in linear time; for example, the pattern preprocessing in the Knuth–Morris–Pratt string matching algorithm [11] gives the exponent of the pattern for free. Note that this result holds for any alphabet. For local exponents, the things are more complicated. The “easy” case occurs when a string contains periodic substrings (a string is periodic if its minimal period is at least twice smaller than its length). Assuming polynomial integer alphabet (which is a common assumption in stringology), all maximal periodic substrings, or runs, can be found in linear time [13]. Over a general alphabet, the asymptotically best known algorithm, proposed in [6], finds all runs in time (𝑛⋅𝛼(𝑛)), where 𝛼 is the inverse Ackermann function. In the opposite case, where the string contains no periodic substrings, only the case of a constant-size alphabet was analyzed. Here a linear-time algorithm was designed in [3]. This algorithm makes an explicit use of the lower bound on the local exponents of long strings (“repetition threshold”), which depends on the alphabet. We did not find any results for more general alphabets. A related problem of reporting all maximal substrings with exponents greater than a given threshold was considered, also over a constant-size alphabet, in [7, 12]. The solution in optimal (𝑛/𝜀) time for the threshold (1+𝜀) was presented in [7], together with a detailed survey of related results.

We consider the problem of computing the local exponent of a stream. The streaming model of computation became quite popular in stringology in the last decade, after impressive results on streaming string matching [5, 16]. In the streaming model, the input string arrives online symbol by symbol; after reading a symbol, the algorithm must compute the answer for the string it has read. The main restriction is that the string cannot be stored after reading, because the available amount of memory is sublinear. This restriction is quite severe, and so most of the streaming algorithms are approximate, randomized, or both. Often, only Monte Carlo approximation algorithms can reach sublinear memory. This is the case for the computation of the local exponent as well as for other closely related problems: finding a longest repeated substring [14] and computing all runs [15]. The latter problem is closely related to finding the local exponent: it covers the case where the stream contains runs that can be detected by a streaming algorithm.

Our results are as follows. First we show that no streaming algorithm, even a Monte Carlo one, can compute the exact local exponent of a stream. More precisely, we show that it is impossible to correctly compare, using a sublinear amount of memory, the local exponent of a string with 2. Next we formulate the problem of approximating the local exponent with an additive error and then present the main result, which is the following theorem.

Theorem 1
There exists a Monte Carlo streaming algorithm which

given a length-n input stream s over a polynomial integer alphabet and an error parameter 𝜀∈(0;12], returns the number 𝛼 such that s has a substring of exponent 𝛼 but no substrings of exponent 𝛼+𝜀 or higher;

uses (log2𝑛𝜀) words of memory and performs (log𝑛) operations, including dictionary operations, per input symbol.

To prove Theorem 1, both the results and the technique of [15] are heavily used.

Preliminaries
Let s denote a string of length n over an alphabet 𝛴={1,…,𝜎}, where 𝜎 is polynomial in n. We write s[i] for the ith symbol of s and s[i..j] for its substring (or factor) 𝑠[𝑖]𝑠[𝑖+1]⋯𝑠[𝑗]. Thus, 𝑠[1..𝑛]=𝑠. By convention, s[i..j] is an empty string if 𝑗<𝑖. A prefix (resp. suffix) of s is a substring of the form s[1..j] (resp., s[j..n]). If 𝑤=𝑠[𝑖..𝑗], we say that w occurs (or has an occurrence) in s at position i. A period of s is a positive integer p such that 𝑠[1..𝑛−𝑝]=𝑠[𝑝+1..𝑛]; 𝗉𝖾𝗋(𝑠) denotes the minimum period of s. We often refer to the strings having period p as p-periodic. The exponent of s is the ratio exp(𝑠)=|𝑠|/𝗉𝖾𝗋(𝑠), where |s| denotes the length of s. The number 𝗅𝖾𝗑𝗉(𝑠)=max{exp(𝑠[𝑖..𝑗])∣1≤𝑖<𝑗≤𝑛} is the local (or critical) exponent of s.

A repetition of period p is a string s such that 𝑝=𝗉𝖾𝗋(𝑠)≤|𝑠|/2. Thus, s is a repetition iff exp(𝑠)≥2; repetitions of exponent 2 are called squares. A string containing no repetitions (or, equivalently, no squares) is called square-free. A repetition s[i..j] of period p is called a run (in s) if both 𝑠[𝑖−1..𝑗] and 𝑠[𝑖..𝑗+1], whenever exist, have no period p. Following [7], we use the term subrepetition for any string s satisfying 1<exp(𝑠)<2. Such a string can be factorized as 𝑠=𝑢𝑣𝑢, where |𝑢𝑣|=𝗉𝖾𝗋(𝑠); u is its border.

We work in the streaming model of computation: the input string s[1..n] (the stream) is read from left to right, one symbol at a time, and cannot be stored, because the available space is sublinear in n. The space is counted as the number of (log𝑛)-bit machine words (in this paper, log stands for the binary logarithm).

A Monte Carlo algorithm gives a correct answer with high probability (at least 1−1𝑛 on a length n input) and has deterministic working time and space. For the related streaming problems of finding a longest repeat and computing all runs [14, 15], Monte Carlo approximation algorithms were used, and proofs that other types of algorithms cannot work in the streaming model were presented. The present paper follows the same pattern.

The main result of [15] is heavily related to the computation of local exponents, so we give its precise formulation. Let 𝖺𝗉𝗉𝗋𝗈𝗑𝖱𝗎𝗇𝗌 denote the following approximate version of the problem of computing all runs in a stream:

given an input string s and an error parameter 𝜀=𝜀(𝑛)∈(0,12], report a set of substrings of s such that

(i)
for each run of exponent 𝛼≥2+𝜀 in s a single substring of this run is reported, having the same period as the run itself and the exponent at least 𝛼−𝜀;

(ii)
for runs of smaller exponent, zero or one substring of each run is reported; if a substring is reported, it has the same period as the run and the exponent at least 2.

In the algorithm which solved 𝖺𝗉𝗉𝗋𝗈𝗑𝖱𝗎𝗇𝗌 we extended the set of elementary operations with dictionary operations (insert, delete, lookup). The optimal choice of dictionary depends on 𝜀. The following theorem is the main result of [15].

Theorem 2
There is a Monte Carlo streaming algorithm that solves 𝖺𝗉𝗉𝗋𝗈𝗑𝖱𝗎𝗇𝗌 performing (log𝑛) operations per read and using (log2𝑛𝜀) words of memory.

The algorithm from Theorem 2 can be used to approximate 𝗅𝖾𝗑𝗉(𝑠) whenever 𝗅𝖾𝗑𝗉(𝑠)≥2+𝜀 (and sometimes this algorithm will be lucky to approximate 𝗅𝖾𝗑𝗉(𝑠) in the case 2≤𝗅𝖾𝗑𝗉(𝑠)<2+𝜀). The details are given in Sect. 4.

The Square-Freeness Problem
As it is known since Thue [19], there exist arbitrarily long square-free strings over three or more letters. Let 𝖲𝗊𝖥𝗋𝖾𝖾(𝛴,𝑛) be the following decision problem:

given a length-n stream over the alphabet 𝛴 of size 𝜎>3, decide whether the stream is square-free.

The following theorem shows that sublinear memory is insufficient to solve this problem with high probability.

Theorem 3
There is a constant 𝛾 such that every algorithm solving the problem 𝖲𝗊𝖥𝗋𝖾𝖾(𝛴,𝑛) with probability at least 1−1𝑛 uses at least 𝛾𝑛log𝜎 bits of memory.

Proof
The scheme of proof is rather standard for this sort of results. We first use Yao’s minimax principle [21] and then finalize the intermediary result exploiting the so-called amplification trick.

Step 1. Assume that some Monte Carlo streaming algorithm solves 𝖲𝗊𝖥𝗋𝖾𝖾(𝛴,𝑛) exactly using less than ⌊log𝐹⌋ bits of memory, where F is the number of square-free strings of length 𝑛′=𝑛2−1 over 𝜎−1 letters. Let us prove that its error probability is at least 1𝑛𝜎. According to Yao’s minimax principle, it is sufficient to construct a probability distribution  over 𝛴𝑛 such that for any deterministic algorithm 𝖣 using less than ⌊log𝐹⌋ bits of memory, the expected probability of error on a string chosen according to  is at least 1𝑛𝜎.

We define a length-n string w(x, k, c) as follows. We fix $∈𝛴 and let 𝛴1=𝛴−{$}. Next we fix two arbitrary square-free strings 𝑢1 and 𝑢2 of length 𝑛′ over 𝛴1; the only restriction is that their first letters are distinct. Let x be a square-free string of length 𝑛′ over 𝛴1, 𝑐∈𝛴1, and 𝑘∈{1,…,𝑛′}. Then

𝑤(𝑥,𝑘,𝑐)=𝑥[1..𝑛′]$𝑐𝑥[𝑛′−𝑘+2..𝑛′]$𝑢𝑖[1..𝑛′−𝑘],
where 𝑖=1 if 𝑢1[1]≠𝑐 and 𝑖=2 otherwise. Due to the separators $, the string w(x, k, c) contains a unique square (𝑥[𝑛′−𝑘+1..𝑛′]$)2 if 𝑐=𝑥[𝑛′−𝑘+1]; in the case 𝑐≠𝑥[𝑛′−𝑘+1], w(x, k, c) is square free iff 𝑐𝑥[𝑛′−𝑘+2..𝑛′] is square free. Let  be the uniform distribution over all strings w(x, k, c).

Since the available memory is insufficient to distinguish between any two square-free strings from 𝛴𝑛′1, there exists an “indistinguishable” pair (𝑥,𝑥′) of such strings; that is, 𝖣 is in the same state after reading either x or 𝑥′. Let 𝑥=𝑣𝑐𝑠, 𝑥′=𝑣′𝑐′𝑠, where 𝑣,𝑣′,𝑠∈𝛴∗1, 𝑐,𝑐′∈𝛴1, and 𝑐≠𝑐′. Then 𝖣 returns the same answer on 𝑤(𝑥,|𝑠|+1,𝑐) and 𝑤(𝑥′,|𝑠|+1,𝑐), because the right halves of these two strings coincide. However, 𝑤(𝑥,|𝑠|+1,𝑐) contains the square (𝑐𝑠$)2, while 𝑤(𝑥′,|𝑠|+1,𝑐) is square free. Therefore, 𝖣 errs on one of the analysed inputs; similarly, it errs on either 𝑤(𝑥,|𝑠|+1,𝑐′) or 𝑤(𝑥′,|𝑠|+1,𝑐′).

Consider an arbitrary maximal set of disjoint pairs (𝑥,𝑥′) of square-free strings from 𝛴𝑛′1, where the strings in each pair are indistinguishable by 𝖣. The memory of 𝖣 has no more than 2⌊log𝐹⌋−1≤𝐹/2 states. Since at most one string per state is left unpaired, the number of pairs is at least F/4. As was shown above, each pair causes two errors by 𝖣, to the total of at least F/2 errors. The number of strings in the distribution  is 𝐹⋅𝑛′⋅(𝜎−1), which implies that the probability of error is greater than 1𝑛𝜎.

Step 2. To use amplification, we relate the parameter F to n and 𝜎. Namely, we show that there exists a positive constant 𝛿 such that ⌊log𝐹⌋≥𝛿𝑛log𝜎. Let 𝐶𝑘(𝑛) be the number of k-ary square-free strings of length n. Then 𝐶𝑘(𝑛) is an exponentially-growing function of n [4]. As was shown in [17], the base 𝛼𝑘 of this exponential function satisfies, as a function of k, the condition 𝛼𝑘=(𝑘−1)−1/(𝑘−1)−1/(𝑘−1)3+𝑂(1/𝑘5). By definition of F we have 𝐹=𝐶𝜎−1(𝑛′); so we can write 𝐹>𝑑(𝜎−3)𝑛/2−1 for some positive constant d and thus log𝐹>(𝑛2−1)log(𝜎−3)+log𝑑. If 𝜎≥5, this inequality implies the announced lower bound. For the remaining case 𝜎=4 we use a more precise bound 𝛼3>1.3; see [18, Theorem 4] for the method of obtaining lower bounds and [18, Table A.2] for numerical results. Hence in this case 𝐹>𝑑′⋅1.3𝑛/2−1 for some positive constant 𝑑′, and then log𝐹>(𝑛2−1)log1.3+log𝑑′=𝛺(𝑛)=𝛺(𝑛log𝜎). Thus we proved the existence of a positive 𝛿 such that ⌊log𝐹⌋≥𝛿𝑛log𝜎.

Now assume that some Monte Carlo streaming algorithm 𝖠 solves 𝖲𝗊𝖥𝗋𝖾𝖾 exactly with error probability 𝜀≤1𝑛 using s(n) bits of memory. Then we can run its k instances simultaneously and return the most frequent answer. The new algorithm 𝖠𝑘 uses (𝑘⋅𝑠(𝑛)) bits of memory and its error probability 𝜀𝑘 satisfies the inequality 𝜀𝑘≤∑2𝑖<𝑘(𝑘𝑖)(1−𝜀)𝑖𝜀𝑘−𝑖≤2𝑘⋅𝜀𝑘/2=(4𝜀)𝑘/2. Recall that 𝜎=(𝑛𝑝) for some constant p. Let 𝑘=2𝑝+3 and take any positive 𝛾≤𝛿𝑘. If 𝑠(𝑛)<𝛾𝑛log𝜎, then algorithm 𝖠𝑘 uses less than 𝛿𝑛log𝜎≤⌊log𝐹⌋ bits of memory. On the other hand, the error probability of 𝖠𝑘 is 𝜀𝑘≤(4𝜀)𝑘/2≤(4𝑛)𝑝+3/2, which is less than 1𝑛𝜎 for n big enough because 𝜎=(𝑛𝑝). From step 1 we know that this is impossible, so the theorem holds for the chosen value of 𝛾. ◻

Theorem 3 shows that there is no hope to compute local exponents of streams exactly, because we cannot even correctly compare this exponent to 2 without an access to linear-size memory. Hence we come up with a natural approximation version 𝖺𝗉𝗉𝗋𝗈𝗑𝖤𝗑𝗉 of this problem:

given a stream s and an error parameter 𝜀∈(0,12], find a number 𝛼 such that 𝛼≤𝗅𝖾𝗑𝗉(𝑠)<𝛼+𝜀.

The rest of the paper describes the algorithm solving 𝖺𝗉𝗉𝗋𝗈𝗑𝖤𝗑𝗉 within the resource limitations listed in Theorem 1. In fact, the algorithm does more: it is able to output a position of a substring of exponent 𝛼.

Tools
The algorithm solving 𝖺𝗉𝗉𝗋𝗈𝗑𝖱𝗎𝗇𝗌 (Theorem 2) outputs substrings of s as triples (l, p, r) such that s[l..r] is a repetition of period p, so it can keep track of the maximum exponent among these triples without spending additional time or space. This version, returning the maximum exponent found, is below referred to as Algorithm Rep. Note that if Algorithm Rep finds at least one repetition, then it solves 𝖺𝗉𝗉𝗋𝗈𝗑𝖤𝗑𝗉 and, by Theorem 2, satisfies the conditions of Theorem 1. So the problem is how to process the streams in which Algorithm Rep finds nothing.

We explore an obvious idea: design an approximation algorithm (Algorithm Sub) for square-free or “nearly square-free” streams and run it in parallel with Algorithm Rep. Both algorithms update the current maximum exponent. If a repetition is detected, we abort Algorithm Sub and continue running Algorithm Rep. In the description of Algorithm Sub we thus assume that no repetition was detected until the current iteration.

We make use of particular data structures and general organization of data introduced in [15] for Algorithm Rep. All necessary details are reproduced in this section.

Fingerprints, Frames, Checkpoints
We use Karp–Rabin fingerprints [10], which is a hash function ubiquitous in streaming string algorithms. Let p be a fixed prime from the range [𝑛4,𝑛5], and r be a fixed integer randomly chosen from {1,…,𝑝−1}. For a string s, its hash is defined as 𝜙(𝑠)=(∑𝑛𝑖=1𝑠[𝑖]⋅𝑟𝑖)mod𝑝. The probability of hash collision for two strings of length m is at most m/p. Our algorithm compares hashes of strings having equal lengths of the form 2𝑗. The probability that a pair of such strings collide is less than 𝑛3/𝑝 and thus less than the allowed error probability for Monte Carlo algorithms. Hence all further considerations assume that no collisions happen. For a string s, its frame is the tuple (|𝑠|,𝜙(𝑠),𝑟|𝑠|mod𝑝,𝑟−|𝑠|mod𝑝). The crucial property of frames is the following.

Lemma 1
([5]) If the frames of any two of the strings A, B, AB are known, the frame of the third string can be computed in (1) time.

All definitions below refer to the input stream s. For any i, the ith iteration of a streaming algorithm processing s begins with reading s[i] and ends just before reading 𝑠[𝑖+1]. We write I(i) for the frame of 𝑠[1..𝑖−1]. Lemma 1 implies that one can compute 𝐼(𝑖+1) in (1) time from I(i) and s[i].

All information currently stored by our algorithm is associated with checkpoints, which form a subset of all positions. Each position k becomes a checkpoint at the kth iteration and “lives” during 𝗍𝗍𝗅(𝑘) iterations, where the time-to-live function is defined by 𝗍𝗍𝗅(𝑘)=2𝑡𝜀+2+𝛽(𝑘) with 𝑡𝜀=⌈log2𝜀⌉ and 𝛽(𝑘) being the maximum power of 2 dividing k. If 𝑘+𝗍𝗍𝗅(𝑘)=𝑖, then at the start of the ith iteration k “dies” (loses the status of checkpoint) and all associated information is deleted. See the example in Fig. 1.

Lemma 2
([15])

1)
The number of checkpoints at i’th iteration is (log𝑖𝜀).

2)
If 𝑖−𝗍𝗍𝗅(𝑖)>0, then the checkpoint 𝑖−𝗍𝗍𝗅(𝑖) dies at the i’th iteration. Otherwise, no checkpoint dies at this iteration.

Fig. 1
figure 1
The checkpoints (black) after the iteration 𝑖=105 (𝑡𝜀=2). For example, 𝗍𝗍𝗅(52)=22+2+2=64, so the position 52 is a checkpoint until the iteration 116

Full size image
Remark 1
We should say a few words about the functions 𝛽(𝑘) and ⌈log𝑘⌉, both extensively used in our algorithm. Note that ⌈log𝑘⌉=𝗆𝗌𝖻(𝑘−1)+1, where the function 𝗆𝗌𝖻(𝑥) returns the position of the most significant 1 in the binary representation of x. With the aid of fusion trees, 𝗆𝗌𝖻(𝑥) can be computed in constant number of operations with machine words; see [9]. A practical way to compute ⌈log𝑘⌉ is to subtract the value 𝖼𝗅𝗓(𝑘) from the size of the machine word. The 𝖼𝗅𝗓(𝑥) function counts leading zeroes in the binary representation of an unsigned integer x in a machine word and is a standard CPU instruction. Further, 𝛽(𝑘)=𝖼𝗍𝗓(𝑘), where the function 𝖼𝗍𝗓(𝑘) counts trailing zeroes in the binary representation of k. Some CPU architectures contain 𝖼𝗍𝗓(𝑘) as an instruction, but we can operate without it: it was shown in [15, Lemma 10] that all calls to 𝛽() during one iteration can be performed in (log𝑛) total time; this bound fits into Theorem 1.

Blocks, Groups, Navigation
Both Algorithm Rep and Algorithm Sub work with substrings of length 2𝑗, 𝑗=0,…,⌊log𝑛⌋. The notions introduced below are illustrated by Fig. 2. For each iteration i and each j we distinguish a subclass of substrings of length 2𝑗 called j-blocks. A substring u of length 2𝑗 of the stream s is a j-block (at the ith iteration) if it occurs in s[1..i] at some checkpoint; i.e., if 𝑢=𝑠[𝑘..𝑘+2𝑗−1] where k is a checkpoint at the ith iteration. Note that u will lose the status of j-block at some future iteration if no checkpoints corresponding to the occurrences of u will remain alive. Below we say “checkpoint occurrence” instead of “occurrence at a checkpoint”. For each j-block T we maintain a basic structure 𝐵𝑇 called group and consisting of

Frame of T

Doubly-connected list 𝑙𝑖𝑠𝑡 of all checkpoints, in increasing order, that are positions of occurrences of T

Position 𝑓𝑟𝑒𝑠ℎ (described below) and its frame 𝑓𝑓𝑟𝑎𝑚𝑒=𝐼(𝑓𝑟𝑒𝑠ℎ)

Number 𝑒𝑥𝑡 (described below)

Apart from the checkpoint occurrences of j-blocks, we are interested in their fresh occurrences (whether at checkpoints or not). An occurrence at position k is fresh at ith iteration, if 𝑘>𝑖−2𝑗+1+1. This condition means that a fresh occurrence of a j-block was a suffix of the stream less than 2𝑗 iterations below. So we immediately have

Observation 1
Any two fresh occurrences of the same j-block overlap.

By definitions of periods and repetitions, overlapping occurrences of the same string create a repetition. Hence Observation 1 implies

Observation 2
If the suffix of length 2𝑗 of the current stream s[1..i] is a j-block which already has a fresh occurrence at position f, then s[f..i] is a repetition. Since 𝑠[𝑓..𝑓+2𝑗−1]=𝑠[𝑖−2𝑗+1..𝑖], this repetition has the period 𝑖−𝑓+1−2𝑗 by definition. Then exp(𝑠[𝑓..𝑖])=𝑖−𝑓+1𝑖−𝑓+1−2𝑗.

Observation 3
Algorithm Rep needs to memorize, in some compressed form, all fresh occurrences of each j-block. For Algorithm Sub, we simplified the structure of a group and store just one fresh occurrence. If a second fresh occurrence is detected, we use Observation 2, update the answer with the exponent 𝛼>2 of the repetition found, and stop the algorithm. The rest of the stream is then processed solely by Algorithm Rep.

Finally, the extension number 𝑒𝑥𝑡 is used to memorize an additional information about the subrepetition uvu, where the right u is the fresh occurrence of a j-block and the left u is the previous checkpoint occurrence of the same block (depending on whether the fresh occurrence is at a checkpoint or not, the left u is either last or second last element of 𝑙𝑖𝑠𝑡). The number 𝑒𝑥𝑡 shows that the subrepetition uvu can be extended in s by 𝑒𝑥𝑡 symbols to the left, preserving the period.

Fig. 2
figure 2
Illustrating j-blocks and groups. Black positions are checkpoints; the arcs 1, 2, 3, 4 represent all occurrences of some substring T of length 2𝑗 in the stream at the ith iteration. The occurrences 1 and 3 are at checkpoints (so T is a j-block), while the occurrences 2 and 4 are not. The group 𝐵𝑇 contains the information about the occurrences 1, 3, and 4: the positions of 1 and 3 are in 𝐵𝑇.𝑙𝑖𝑠𝑡, the position of 4 is 𝐵𝑇.𝑓𝑟𝑒𝑠ℎ. The occurrence 2 is “forgotten”: no information is stored about it. Finally, the arcs 5 and 6 represent equal substrings of length 𝐵𝑇.𝑒𝑥𝑡

Full size image
Remark 2
A group is a constant-size structure (two frames, links to the beginning and the end of 𝑙𝑖𝑠𝑡, the numbers 𝑓𝑟𝑒𝑠ℎ and 𝑒𝑥𝑡) plus a set of constant-size nodes (position, links to next and previous elements of the list). We store all groups in an array of constant-size cells endowed with a stack of empty cells. This way, creating a new group/node and deleting an existing group/node requires (1) time. The size of the array is proportional to the number of groups plus the number of occurrences of j-blocks; both numbers are (log2𝑛𝜀), as follows from Lemma 2(1).

For navigation we use five dictionaries described in the following table. The values in the first four dictionaries are stored as links.

Id	Key	Value
𝐻1	j, hash F	group of the j-block with hash F
𝐻2	j, checkpoint k	group of the j-block occurring at k
𝐻3	j, position k	group of the j-block having the fresh occurrence at k
𝐻𝐻	j, checkpoint k	node for k in the group of the j-block occurring at k
𝐻𝐶	checkpoint k	frame I(k)
Algorithm Sub
Let 𝑤=𝑠[𝑡..𝑖] be a substring, 𝑝=𝗉𝖾𝗋(𝑤). Let f be the smallest position satisfying 𝑓≥𝑡 and 𝗍𝗍𝗅(𝑓)≥2𝑝, and let g be the largest position such that 𝑔≤𝑖 and 𝗍𝗍𝗅(𝑔−𝑝+1)≥2𝑝. We call the substring s[f..g] of w the core of w.

Lemma 3
Every substring s[t..i] with exp(𝑠[𝑡..𝑖])≥1+𝜀 has a nonempty core. The exponent of the core is greater than exp(𝑠[𝑡..𝑖])−𝜀.

Proof
Let 𝗍𝗍𝗅(𝑥)≥2𝑝 be the minimum time-to-live satisfying this inequality. Then 𝗍𝗍𝗅(𝑥)=2⌈log𝑝⌉+1, so 𝛽(𝑥)=⌈log𝑝⌉−1−𝑡𝜀 by the definition of 𝗍𝗍𝗅. Hence the distance between two consecutive positions with 𝗍𝗍𝗅≥2𝑝 is 2𝛽(𝑥)<2(log𝑝+1)−1−log2𝜀=𝜀𝑝2. Therefore, the numbers f and g from the definition of the core satisfy 𝑓−𝑡,𝑖−𝑔<𝜀𝑝2. Since |𝑠[𝑡..𝑖]|≥(1+𝜀)𝑝, the length of s[f..g] is strictly greater than p. So s[f..g] has period p as a substring of s[t..i], and exp(𝑠[𝑓..𝑔])>exp(𝑠[𝑡..𝑖])−𝜀𝑝𝑝=exp(𝑠[𝑡..𝑖])−𝜀, as required. ◻

Lemma 3 shows that the core of a substring approximates its exponent with the desired precision. The idea of Algorithm Sub is to detect cores using the information about j-blocks, stored in groups. The detection becomes possible because the definition of a core implies that certain positions are checkpoints at the moment when the core is read (see Fig. 3). When a core is detected, its exponent is computed and used to update the answer. Some cores can be missed by the algorithm; the crucial fact, proved in Lemma 4 below, is that a core is missed only if some other substring of bigger exponent was detected before. This fact ensures the correctness of the answer found by Algorithm Sub.

One iteration of Algorithm Sub is presented below as Algorithm 1. As was already said, Algorithm Sub works in parallel with Algorithm Rep, which is responsible for “big” exponents. Moreover, the two algorithms share the auxiliary stages at each iteration, with some details simplified for Algorithm Sub. In line 1, we read a new symbol, compute the frame of the whole string and store it in the dictionary 𝐻𝐶. Then three nontrivial stages follow. These stages are described in [15] as Algorithms 1–3, endowed with the proofs of correctness and time bounds. So here we just recall the performed operations in brief.

figure a
Stage 1 (line 2). Lemma 2 indicates the only checkpoint which can die at the current iteration. We process each of (log𝑛) j-blocks at the checkpoint position, deleting the checkpoints from their groups and from dictionaries; groups without checkpoints are also deleted. The dictionaries 𝐻2 and 𝐻𝐻 are used, an entry in 𝐻𝐶 is deleted.

Stage 2 (line 3). The j-blocks of the form s[k..i], where k is a checkpoint, are processed. For each block we compute its hash, retrieving I(k) from the dictionary 𝐻𝐶 and using Lemma 1. Then we extract the group B of this block from the table 𝐻1; if B does not exist, it is created. A node for k is added to 𝐵.𝑙𝑖𝑠𝑡, and an element is added to 𝐻𝐻. The occurrence at k is fresh, so 𝐵.𝑓𝑟𝑒𝑠ℎ is set to k, 𝐵.𝑓𝑓𝑟𝑎𝑚𝑒 is set to I(k), and a new element is added to the dictionary 𝐻3. If a fresh occurrence existed before, a repetition is detected, which implies the abortion of the algorithm; the rest of the stream will be processed solely by Algorithm Rep. The stage includes one more loop over j: using 𝐻2, the expired fresh checkpoint occurrences are deleted.

Stage 3 (line 5). A trick is used here to find the hash of the suffix 𝑇=𝑠[𝑘..𝑖] of length 2𝑗 in the case if k is not a checkpoint. If T has no checkpoint occurrences, it is useless in the search of (sub)repetitions, and we skip it. But if T has such an occurrence, then its prefix of length 2𝑗−1 is a (𝑗−1)-block occurring at the same checkpoint. We check at 𝐻3 whether there is a (𝑗−1)-block 𝑇′ with the fresh occurrence at k. If yes, we extract the frame I(k) as 𝑓𝑓𝑟𝑎𝑚𝑒 of 𝑇′ and get the hash of T by Lemma 1. The fresh occurrence of 𝑇′ at k is then deleted as expired (it was not deleted at Stage 2 because k is not a checkpoint). After computing the hash of T, the group of T is retrieved from the dictionary 𝐻1. If no such group exists (𝑇′ has checkpoint occurrences but T has not), we skip T (lines 6–7).

In lines 8–9, the algorithm stops according to Observation 3 if two fresh occurrences of T (at 𝐵.𝑓𝑟𝑒𝑠ℎ and at k) are found. Lines 10–22 are related to the main procedure: core detection.

Lemma 4
Suppose that Algorithm Sub processed a stream s and no repetitions were detected. Let s[t..i] be a substring of s such that exp(𝑠[𝑡..𝑖])≥1+𝜀, 𝑝=𝗉𝖾𝗋(𝑠[𝑡..𝑖]) and 𝑠[𝑡−1..𝑖] is not p-periodic. Then Algorithm Sub detected either the core of s[t..i] or some other substring of exponent greater than the exponent of this core.

Fig. 3
figure 3
Finding the core s[f..g] of a p-periodic substring s[t..i] (Lemma 4). Black positions are checkpoints, white positions can have any status. In the picture, 𝑗=𝑗′+2 and 𝑘=6

Full size image
Proof
Let s[f..g] be the core of s[t..i] (see Fig. 3 for an illustration). Since it has period p, 𝑠[𝑓..𝑔−𝑝]=𝑠[𝑓+𝑝..𝑔] by definition. Let us denote this repeated part by u. One has |𝑢|=𝑔−𝑓−𝑝+1. Let 𝑗=⌊log|𝑢|⌋. Then u is strictly shorter than a (𝑗+1)-block and (non-strictly) longer than a j-block. We prove the lemma by showing that Algorithm Sub followed one of two scenarios:

(i)
The core was detected at the gth iteration: for the j-block which is the suffix of s[1..g] (𝑇2 in Fig. 3) the previous occurrence at a checkpoint was at distance p and 𝑒𝑥𝑡=|𝑢|−2𝑗; these two conditions imply 𝑠[𝑓..𝑔−𝑝]=𝑠[𝑓+𝑝..𝑔];

(ii)
A substring of exponent bigger than the exponent of the core was detected no later than at the gth iteration.

Let 𝑗′=max{0,⌈log𝑝⌉−1−𝑡𝜀}. As shown in the proof of Lemma 3, consecutive positions with 𝗍𝗍𝗅≥2𝑝 are at distance 2𝑗′. Since f and 𝑔−𝑝+1 have 𝗍𝗍𝗅≥2𝑝 by the definition of a core, (𝑔−𝑝+1)−𝑓=|𝑢|=ℎ⋅2𝑗′ for some ℎ≥1. Hence each of the positions 𝑓,𝑓+2𝑗′,…,𝑓+(ℎ−1)⋅2𝑗 remains a checkpoint for at least 2⌈log𝑝⌉+1 iterations and 𝑢=𝑠[𝑓..𝑔−𝑝] is a concatenation of h 𝑗′-blocks at these checkpoints. These 𝑗′-blocks can be combined into overlapping j-blocks, denoted by 𝑇0,𝑇1,…,𝑇𝑑 as in Fig. 3. Note that 𝑑=ℎ−2𝑗−𝑗′; in particular, if h is a power of 2, then 𝑠[𝑓..𝑔−𝑝] is a single j-block.

Now we are going to show that Algorithm Sub followed either scenario (i) or scenario (ii) with respect to the core s[f..g]. To do this, we analyze the iterations 𝑖0,…,𝑖𝑑 in which the right (in Fig. 3) occurrences of the j-blocks 𝑇0,𝑇1,…,𝑇𝑑 were suffixes of the stream; that is 𝑖𝑟=𝑓+𝑝+2𝑗+𝑟2𝑗′−1 for 𝑟=0,…,𝑑 (thus 𝑖𝑑=𝑔). First consider the iteration 𝑖0. At this iteration, f was a checkpoint and the stream had the j-block 𝑇0 at position 𝑓+𝑝 as a suffix. Hence, Algorithm Sub found, in line 11 or line 13, the rightmost previous checkpoint occurrence of 𝑇0. The checkpoint is either f or some 𝑓′>𝑓. In the latter case, consider the three occurrences of 𝑇0: at f, 𝑓′, and 𝑓+𝑝 (Fig. 4). These occurrences neither overlap nor touch: otherwise, 𝑇0 had two fresh occurrences at some moment and a repetition was detected, which is impossible by the conditions of the lemma. Thus these occurrences form two subrepetitions 𝑠[𝑓..𝑓′+2𝑗−1] and 𝑠[𝑓′..𝑖0] overlapping by the middle occurrence. One of these subrepetitions was detected at the iteration 𝑖0 and the other one had been found earlier at the iteration 𝑓′+2𝑗−1. The sum of their periods is p. Then one of the periods is at most p/2, and the exponent of the corresponding substring is at least 1+2𝑗+1𝑝. By definition of j, 2𝑗+1>|𝑢|, so this exponent is greater than the exponent of the core, which is 1+|𝑢|𝑝. Therefore, scenario (ii) was realized.

Fig. 4
figure 4
Illustrating the proof of Lemma 4: an additional occurrence of 𝑇0 at position 𝑓′

Full size image
Now consider the case where the rightmost previous checkpoint occurrence of 𝑇0 is at f. The two rightmost occurrences of 𝑇0 do not overlap or touch because no repetition was detected. Then, in particular, 2𝑗<𝑝 (see Fig. 3). The algorithm computed f, p, and 𝑗′ in line 15 and set the extension of 𝑇0 to 0 in line 16. Next, in line 17 the algorithm retrieved the j-block at position 𝑓−2𝑗′. This position was a checkpoint at the considered iteration. Indeed, 𝗍𝗍𝗅(𝑓−2𝑗′)≥2𝑝 (as mentioned above, consecutive positions with 𝗍𝗍𝗅≥2𝑝 are at distance 2𝑗′). Then 𝗍𝗍𝗅(𝑓−2𝑗′)≥2𝑗+2 because 2𝑗<𝑝. It remains to note that 2𝑗+2 is greater than the difference between the current position 𝑖0=𝑓+𝑝+2𝑗−1 and 𝑓−2𝑗′. The retrieved j-block differs from the j-block at the position 𝑓+𝑝−2𝑗′ because 𝑓−2𝑗′<𝑡 by the definition of a core and 𝑠[𝑡−1]≠𝑠[𝑡+𝑝−1] be the conditions of the lemma. Hence the condition in line 18 is false (note that 𝑘=𝑝+𝑓); so the algorithm detected the subrepetition 𝑠[𝑓..𝑖0] and updated the answer with its exponent1+2𝑗𝑝 in line 22.

Next consider the iteration 𝑖1, in which the stream had the suffix 𝑇1. At this iteration, the algorithm found the previous checkpoint occurrence of 𝑇1. If the checkpoint is greater than 𝑓+2𝑗′, then we repeat the above argument about three occurrences of 𝑇0 for the occurrences of 𝑇1 and conclude that scenario (ii) took place. Assume that the occurrence was found at 𝑓+2𝑗′. In line 17, the j-block at the checkpoint f was retrieved; this block is 𝑇0 and its fresh occurrence satisfies the condition in line 18. The extension of 𝑇1 is set in line 19 to 2𝑗. The value 𝐵′.𝑒𝑥𝑡 had been set to 0 during the iteration 𝑖0 we considered above; thus the iteration 𝑖1 ended by updating the answer in line 22 with the exponent 1+2𝑗+2𝑗′𝑝 of the subrepetition 𝑠[𝑓..𝑖1].

Finally we prove by induction the following claim: for any 𝑟≤𝑑 either scenario (ii) was detected no later than at the iteration 𝑖𝑟 or the iteration 𝑖𝑟 ended by updating the answer with the exponent 1+2𝑗+𝑟2𝑗′𝑝 of the subrepetition 𝑠[𝑓..𝑖𝑟]. The base case is proved above; we proceed with the step case.

Consider the iteration 𝑖𝑟 for some 𝑟≥2 and suppose that scenario (ii) was not detected till its end. At the iteration 𝑖𝑟, the algorithm found the previous checkpoint occurrence of 𝑇𝑟. If this checkpoint is greater than 𝑓+𝑟⋅2𝑗′, scenario (ii) is detected repeating the above argument for 𝑇0 (Fig. 4). Since we assumed it was not detected, the previous checkpoint occurrence of 𝑇𝑟 was at 𝑓+𝑟⋅2𝑗′. By the same argument, the previous checkpoint occurrence of 𝑇𝑟−1, found at the iteration 𝑖𝑟−1, was at 𝑓+(𝑟−1)2𝑗′. Further, the iteration 𝑖𝑟−1 ended with updating the answer with the exponent 1+2𝑗+(𝑟−1)2𝑗′𝑝 of the string 𝑠[𝑓..𝑖𝑟−1] by the inductive hypothesis. This means (see line 22) that the extension of the block 𝑇𝑟−1 was set to (𝑟−1)2𝑗′.

Consider the rest of the iteration 𝑖𝑟 after the checkpoint 𝑓+𝑟⋅2𝑗′ was found. In line 17, the block 𝑇𝑟−1 was retrieved. The condition in line 18 is true: it refers to the occurrence of 𝑇𝑟−1 we analysed at the iteration 𝑖𝑟−1. In addition, the condition in line 20 holds: it says that when the algorithm processed the suffix 𝑇𝑟−1 at that iteration, it found the checkpoint occurrence at 𝑓+(𝑟−1)2𝑗′. Hence the algorithm set the extension of 𝑇𝑟 to 2𝑗′ at line 19 and updated it to 2𝑗′+(𝑟−1)2𝑗′=𝑟⋅2𝑗′ in line 21. Then in line 22 the answer is updated with the required exponent 1+2𝑗+𝑟2𝑗′𝑝. The claim is proved.

For 𝑟=𝑑, the claim says that if scenario (ii) was not detected, then at gth iteration the answer was updated with the exponent 1+2𝑗+𝑑2𝑗′𝑝. As 2𝑗+𝑑2𝑗′=ℎ2𝑗′=|𝑢|, this is exactly the exponent of the core s[f..g]. This means that scenario (i) took place. The lemma is proved. ◻

Remark 3
The condition in line 20 prevents Algorithm Sub from an error in a tricky situation illustrated by Fig. 5. At the current iteration i, for a suffix T of length 2𝑗 the rightmost checkpoint occurrence was found at position f. The period p and the number 𝑗′ were computed, the j-block 𝑇′ at the position 𝑓−2𝑗′ was retrieved, and this block appeared to have a fresh occurrence at 𝑘−2𝑗′. Then the extension of T is set to 2𝑗′ (the subrepetition 𝑠[𝑓−2𝑗′..𝑖] is p-periodic). However, it would be an error to further increase the extension of T by the extension of 𝑇′, because they are related to different subrepetitions: the extension of 𝑇′ reflects the equality of two substrings Z shown in the picture. The condition in line 20 fails (if 𝑘−2𝑗′ is a checkpoint, then 𝑏1=𝑘−2𝑗′ and 𝑏2=𝑓′; if not, then 𝑏1=𝑓′), so no error happens.

Fig. 5
figure 5
Illustrating special case of Algorithm 1: the extensions of the j-blocks T and 𝑇′ refer to subrepetitions with different periods. At the ith iteration, the condition in line 20 fails, preventing the algorithm from adding the extension of 𝑇′ to the extension of T

Full size image
Proof of Theorem 1
We run Algorithm Sub and Algorithm Rep in parallel (that is, after reading s[i] the ith iteration of each algorithm is performed; the order does not matter). If s contains a run of exponent at least 2+𝜀, Algorithm Rep finds 𝗅𝖾𝗑𝗉(𝑠) with the error less than 𝜀. If any of the algorithms detects a repetition x (Algorithm Sub detects repetitions with the condition in line 8), then 𝗅𝖾𝗑𝗉(𝑠)≥exp(𝑥)≥2. So either exp(𝑥) is a valid answer to 𝖺𝗉𝗉𝗋𝗈𝗑𝖤𝗑𝗉, or 𝗅𝖾𝗑𝗉(𝑠)≥2+𝜀 and the answer can be found solely by Algorithm Rep. Hence, on detecting a repetition we stop Algorithm Sub and run Algorithm Rep for the rest of the stream.

Now suppose that no repetitions were found. Let x be a substring of s satisfying 𝗅𝖾𝗑𝗉(𝑠)=exp(𝑥)≥1+𝜀. By Lemma 4, the answer obtained by Algorithm Sub cannot be smaller than exp(𝑧), where z is the core of x. By Lemma 3, exp(𝑧)>exp(𝑥)−𝜀. Hence Algorithm Sub solved 𝖺𝗉𝗉𝗋𝗈𝗑𝖤𝗑𝗉 correctly.

Algorithm Rep obliges the required space and time limitations by Theorem 2. It remains to estimate space and time consumed by Algorithm Sub. The space usage of Algorithm Sub is dominated by groups, which occupy (log2𝑛𝜀) words of space by Remark 2. The logarithmic time bounds for lines 2, 3, and 5 were proved in [15]; the remaining part of the algorithm uses (1) operations per value of j (for ⌈log𝑝⌉ see Remark 1), which is (log𝑛) per iteration. ◻

Conclusion and Open Questions
In this paper we presented the first streaming algorithm to compute the exponent of the input string. As often happens for streaming problems, the solution belongs to the class of Monte Carlo approximation algorithms because neither deterministic algorithms nor exact Monte Carlo algorithms can solve the problem in sublinear memory. Our algorithm has competitive parameters: its space usage is polynomial in log𝑛 and linear in 1/𝜀, where n is the length of the input and 𝜀 is the additive error parameter. The (log𝑛) update time is also close to the minimum possible. In addition, our algorithm uses only practical data structures and the -bounds hide no big constants.

Thus the natural question is “can one do better?” More specifically,

Are there any space or time lower bounds for the 𝖺𝗉𝗉𝗋𝗈𝗑𝖤𝗑𝗉 problem?

Is it possible to improve space usage?

Is it possible to improve the update time to “pure” (log𝑛), without dictionary operations?

Concerning the last question, we reproduce the remark from [15] on the choice of dictionaries, which shows what is the current “pure” time bound.

Remark 4
If 𝜀 is small (inverse polynomial), it makes sense to use dynamic perfect hash tables [2, 8] as dictionaries. Both cited versions guarantee that with probability 1−𝑚−𝑐, where m is the dictionary size and c is an arbitrary constant, all dictionary operations will take (1) time. Thus the total probability of a failed run of an algorithm can still be kept below 1/n with (log𝑛) elementary operations between reads. However, this is not the case for big (such as constant or inverse polylog) values of 𝜀. So in this case we suggest to use deterministic dictionaries by Anderson and Thorup [1] which give us (loglog𝑛logloglog𝑛‾‾‾‾‾‾‾‾‾√⋅log𝑛) elementary operations between reads.

