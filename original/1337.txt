This paper is concerned with the 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem, the problem of minimizing the total processing time of tardy jobs on a single machine. This is not only a fundamental scheduling problem, but also an important problem from a theoretical point of view as it generalizes the Subset Sum problem and is closely related to the 0/1-Knapsack problem. The problem is well-known to be NP-hard, but only in a weak sense, meaning it admits pseudo-polynomial time algorithms. The best known running time follows from the famous Lawler and Moore algorithm that solves a more general weighted version in ğ‘‚(ğ‘ƒâ‹…ğ‘›) time, where P is the total processing time of all n jobs in the input. This algorithm has been developed in the late 60s, and has yet to be improved to date. In this paper we develop two new algorithms for problem, each improving on Lawler and Mooreâ€™s algorithm in a different scenario.

Our first algorithm runs in ğ‘‚Ìƒ (ğ‘ƒ7/4) time, and outperforms Lawler and Mooreâ€™s algorithm in instances where ğ‘›=ğœ”Ìƒ (ğ‘ƒ3/4).

Our second algorithm runs in ğ‘‚Ìƒ (min{ğ‘ƒâ‹…ğ·#,ğ‘ƒ+ğ·}) time, where ğ·# is the number of different due dates in the instance, and D is the sum of all different due dates. This algorithm improves on Lawler and Mooreâ€™s algorithm when ğ‘›=ğœ”Ìƒ (ğ·#) or ğ‘›=ğœ”Ìƒ (ğ·/ğ‘ƒ). Further, it extends the known ğ‘‚Ìƒ (ğ‘ƒ) algorithm for the single due date special case of 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— in a natural way.

Both algorithms rely on basic primitive operations between sets of integers and vectors of integers for the speedup in their running times. The second algorithm relies on fast polynomial multiplication as its main engine, and can be easily extended to the case of a fixed number of machines. For the first algorithm we define a new â€œskewedâ€ version of (max,min)-Convolution which is interesting in its own right.

Access provided by University of Auckland Library

Introduction
In this paper we consider the problem of minimizing the total processing times of tardy jobs on a single machine. In this problem we are given a set of n jobs ğ½={1,â€¦,ğ‘›}, where each job j has a processing time ğ‘ğ‘—âˆˆâ„• and a due date ğ‘‘ğ‘—âˆˆâ„•. A schedule ğœ for J is a permutation ğœ:{1,â€¦,ğ‘›}â†’{1,â€¦,ğ‘›}. In a given schedule ğœ, the completion time ğ¶ğ‘— of a job j under ğœ is given by ğ¶ğ‘—=âˆ‘ğœ(ğ‘–)â‰¤ğœ(ğ‘—)ğ‘ğ‘–, that is, the total processing time of jobs preceding j in ğœ (including j itself). Job j is tardy in ğœ if ğ¶ğ‘—>ğ‘‘ğ‘—, and early otherwise. Our goal is find a schedule with minimum total processing time of tardy jobs. If we assign a binary indicator variable ğ‘ˆğ‘— to each job j, where ğ‘ˆğ‘—=1 if j is tardy and otherwise ğ‘ˆğ‘—=0, our objective function can be written as âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—. In the standard three field notation for scheduling problems of Graham [5], this problem is denoted as the 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem (the 1 in the first field indicates a single machine model, and the empty second field indicates there are no additional constraints).

The 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem is a natural scheduling problem, which models a basic scheduling scenario. As it includes Subset Sum as a special case (see below), the 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem is NP-hard. However, it is only hard in the weak sense, meaning it admits pseudo-polynomial time algorithms. The focus of this paper is on developing fast pseudo-polynomial time algorithms for 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—, improving in several settings on the best previously known solution from the late 60s. Before we describe our results, we discuss the previously known state of the art of the problem, and describe how our results fit into this line of research.

State of the Art
1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— is a special case of the famous 1||âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘— problem. Here, each job j also has a weight ğ‘¤ğ‘— in addition to its processing time ğ‘ğ‘— and due date ğ‘‘ğ‘—, and the goal is to minimize the total weight (as opposed to total processing times) of tardy jobs. This problem has already been studied in the 60s, and even appeared in Karpâ€™s fundamental paper from 1972 [6]. The classical algorithm of Lawler and Moore [10] for the problem is one of the earliest and most prominent examples of pseudo-polynomial algorithms, and it is to date the fastest known algorithm even for the special case of 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—. Letting ğ‘ƒ=âˆ‘ğ‘—âˆˆğ½ğ‘ğ‘—, their result can be stated as follows:

Theorem 1
[10] 1||âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘— (and hence also 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—) can be solved in ğ‘‚(ğ‘ƒâ‹…ğ‘›) time.

Note that as we assume that all processing times are integers, we have ğ‘›â‰¤ğ‘ƒ, and so the running time of the algorithm in Theorem 1 can be bounded by ğ‘‚(ğ‘ƒ2). In fact, it makes perfect sense to analyze the time complexity of a pseudo-polynomial time algorithm for either problems in terms of P, as P directly corresponds to the total input length when integers are encoded in unary. Observe that while the case of ğ‘›=ğ‘ƒ (all jobs have unit processing times) essentially reduces to sorting, there are several non-trivial cases where n is smaller than P yet still quite significant in the ğ‘‚(ğ‘ƒâ‹…ğ‘›) term of Theorem 1. The question this paper addresses is:

â€œCan we obtain an ğ‘‚(ğ‘ƒ2âˆ’ğœ€) time algorithm for 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—, for any fixed ğœ€>0 ?â€

For 1||âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘— there is some evidence that the answer to the analogous question should be no. Karp [6] observed that the special case of the 1||âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘— problem where all jobs have the same due date d, the 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘— problem, is essentially equivalent to the classical 0/1-Knapsack problem. Cygan et al. [4] and KÃ¼nnemann et al. [9] studied the (min,+)-Convolution problem (see Sect. 2), and conjectured that the (min,+)-convolution between two vectors of length n cannot be computed in ğ‘‚Ìƒ (ğ‘›2âˆ’ğœ€) time, for any ğœ€>0. Under this (min,+)-Convolution Conjecture, they obtained lower bounds for several Knapsack related problems. In our terms, their result can be stated as follows:

Theorem 2
[4, 9] There is no ğ‘‚Ìƒ (ğ‘ƒ2âˆ’ğœ€) time algorithm for the 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘— problem, for any ğœ€>0, unless the (min,+)-Convolution Conjecture is false. In particular, 1||âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘— has no such algorithm under this conjecture.

Analogous to the situation with 1||âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘—, the special case of 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— where all jobs have the same due date d (the 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem) is equivalent to the classical Subset Sum problem. Recently, there has been significant improvements for Subset Sum resulting in algorithms with ğ‘‚Ìƒ (ğ‘‡+ğ‘›) running times [2, 7], where n is number of integers in the instance and T is the target. Due to the equivalence between the two problems, this yields the following result for the 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem:

Theorem 3
[2, 7] 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— can be solved in ğ‘‚Ìƒ (ğ‘ƒ) time.

On the other hand, due to the equivalence of 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— and Subset Sum, we also know that Theorem 3 above cannot be significantly improved unless the Strong Exponential Time Hypothesis (SETH) fails. Specifically, combining a recent reduction from k-SAT to Subset Sum [1] with the equivalence of Subset Sum and 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—, yields the following:

Theorem 4
[1] There is no ğ‘‚Ìƒ (ğ‘ƒ1âˆ’ğœ€) time algorithm for the 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem, for any ğœ€>0, unless SETH fails.

Nevertheless, Theorem 4 still leaves quite a big gap for the true time complexity of 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—, as it can potentially be anywhere between the ğ‘‚Ìƒ (ğ‘ƒ) time known already for the special case of 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— (Theorem 3), and the ğ‘‚(ğ‘ƒğ‘›)=ğ‘‚(ğ‘ƒ2) time of Lawler and Mooreâ€™s algorithm (Theorem 1) for the 1||âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘— problem. In particular, the 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— and 1||âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘— problems have not been distinguished from an algorithmic perspective so far. This is the starting point of our paper.

Our Results
The main contribution of this paper is two new pseudo-polynomial time algorithms for 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—, each improving on Lawler and Mooreâ€™s algorithm in a different sense. Our algorithms take a different approach to that of Lawler and Moore in that they rely on fast operators between sets and vectors of numbers.

Our first algorithm improves Theorem 1 in case there are sufficiently many jobs in the instance compared to the total processing time. More precisely, our algorithm has a running time of ğ‘‚Ìƒ (ğ‘ƒ7/4), and so it is faster than Lawler and Mooreâ€™s algorithm in case ğ‘›=ğœ”Ìƒ (ğ‘ƒ3/4).

Theorem 5
1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— can be solved in ğ‘‚Ìƒ (ğ‘ƒ7/4) time.

The algorithm in Theorem 5 uses a new kind of convolution which we coined â€œSkewed Convolutionâ€ and is interesting in its own right. In fact, one of the main technical contributions of this paper is a fast algorithm for the (max,min)-Skewed-Convolution problem (see definition in Sect. 2).

Our second algorithm for 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— improves Theorem 1 in case there are not too many different due dates in the problem instance; that is, ğ·#=|{ğ‘‘ğ‘—:ğ‘—âˆˆğ½}| is relatively small when compared to n. This is actually a very natural assumption, for instance in cases where delivery costs are high and products are batched to only few shipments. Let D denote the sum of the different due dates in our instance. Then our second result can be stated as follows:

Theorem 6
1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— can be solved in ğ‘‚Ìƒ (min{ğ‘ƒâ‹…ğ·#,ğ‘ƒ+ğ·}) time.

The algorithm in Theorem 6 uses basic operations between sets of numbers, such as the sumset operation (see Sect. 2) as basic primitives for its computation, and ultimately relies on fast polynomial multiplication for its speedup. It should be noted that Theorem 6 includes the ğ‘‚Ìƒ (ğ‘ƒ) result of Theorem 3 for 1|ğ‘‘ğ‘—=ğ‘‘|âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— as a special case where ğ·#=1 or ğ·=ğ‘‘. However, when measuring only in terms of n and D, the running times of the algorithms in [2, 7] for the single due date case are ğ‘‚Ìƒ (ğ·+ğ‘›), which can be significantly faster than ğ‘‚Ìƒ (ğ‘ƒ).

As a final result we show that the algorithm used in Theorem 6 can be easily extended to the case where we have a fixed number ğ‘šâ‰¥1 of parallel machines at our disposal. This problem is known as ğ‘ƒğ‘š||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— in the literature. We show that the complexity of Theorem 6 scales naturally in m for this generalization. This should be compared with the ğ‘‚(ğ‘ƒğ‘šğ‘›) running time of Lawler and Mooreâ€™s algorithm.

Theorem 7
ğ‘ƒğ‘š||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— can be solved in ğ‘‚Ìƒ (min{ğ‘ƒğ‘šâ‹…ğ·#,ğ‘ƒğ‘š+ğ·ğ‘š}) time.

Roadmap
The paper is organized as follows. In Sect. 2 we discuss all the basic primitives that are used by our algorithms, including some basic properties that are essential for the algorithms. We then present our second algorithm in Sect. 3, followed by our first algorithm in Sect. 4. Section 5 describes our fast algorithm for the skewed version of (max,min)-convolution, and is the main technical part of the paper. In Sect. 6 we consider the case of multiple machines and prove Theorem 7, and we conclude with some remarks and open problems in Sect. 7.

Preliminaries
In the following we discuss the basic primitives and binary operators between sets/vectors of integers that will be used in our algorithms. In general, we will use the letters X and Y to denote sets of non-negative integers (where order is irrelevant), and the letters A and B to denote vectors of non-negative integers.

Sumsets: The most basic operation used in our algorithms is computing the sumset of two sets of non-negative integers:

Definition 1
(Sumsets) Given two sets of non-negative integers ğ‘‹1 and ğ‘‹2, the sumset of ğ‘‹1 and ğ‘‹2, denoted ğ‘‹1âŠ•ğ‘‹2, is defined by

ğ‘‹1âŠ•ğ‘‹2={ğ‘¥1+ğ‘¥2:ğ‘¥1âˆˆğ‘‹1,ğ‘¥2âˆˆğ‘‹2}.
Clearly, the sumset ğ‘‹1âŠ•ğ‘‹2 can be computed in ğ‘‚(|ğ‘‹1|â‹…|ğ‘‹2|) time. However, in certain cases we can do better using fast polynomial multiplication. Consider the two polynomials ğ‘1[ğ›¼]=âˆ‘ğ‘¥âˆˆğ‘‹1ğ›¼ğ‘¥ and ğ‘2[ğ›½]=âˆ‘ğ‘¥âˆˆğ‘‹2ğ›½ğ‘¥. Then the exponents of all terms in ğ‘1â‹…ğ‘2 with non-zero coefficients correspond to elements in the sumset ğ‘‹1âŠ•ğ‘‹2. Since multiplying two polynomials of maximum degree d can be done in ğ‘‚(ğ‘‘logğ‘‘) time [3], we have the following:

Lemma 1
Given two sets of non-negative integers ğ‘‹1,ğ‘‹2âŠ†{0,â€¦,ğ‘ƒ}, one can compute the sumset ğ‘‹1âŠ•ğ‘‹2 in ğ‘‚(ğ‘ƒlogğ‘ƒ) time.

Set of all Subset Sums: Given set of non-negative integers X, we will frequently be using the set of all sums generated by subsets of X:

Definition 2
(Subset Sums) For a given set of non-negative integers X, define the set of all subset sums îˆ¿(ğ‘‹) as the set of integers given by

îˆ¿(ğ‘‹)={âˆ‘ğ‘¥âˆˆğ‘Œğ‘¥:ğ‘ŒâŠ†ğ‘‹}.
Here, we always assume that 0âˆˆîˆ¿(ğ‘‹) (as it is the sum of the empty set).

We can use Lemma 1 above to compute îˆ¿(ğ‘‹) from X rather efficiently: First, split X into two sets ğ‘‹1 and ğ‘‹2 of roughly equal size. Then recursively compute îˆ¿(ğ‘‹1) and îˆ¿(ğ‘‹2). Finally, compute îˆ¿(ğ‘‹)=îˆ¿(ğ‘‹1)âŠ•îˆ¿(ğ‘‹2) via Lemma 1. The entire algorithm runs in ğ‘‚Ìƒ (âˆ‘ğ‘¥âˆˆğ‘‹ğ‘¥) time.

Lemma 2
([7]) Given a set of non-negative integers X, with ğ‘ƒ=âˆ‘ğ‘¥âˆˆğ‘‹ğ‘¥, one can compute îˆ¿(ğ‘‹) in ğ‘‚Ìƒ (ğ‘ƒ) time.

Convolutions: Given two vectors ğ´=(ğ´[ğ‘–])ğ‘›ğ‘–=0, ğµ=(ğµ[ğ‘—])ğ‘›ğ‘—=0, the (âˆ˜,âˆ™)-Convolution problem for binary operators âˆ˜ and âˆ™ is to compute a vector ğ¶=(ğ¶[ğ‘˜])2ğ‘›ğ‘˜=0 with

ğ¶[ğ‘˜]=â—¯ğ‘–+ğ‘—=ğ‘˜ğ´[ğ‘–]âˆ™ğµ[ğ‘—].
Throughout this paper we assume that A, B (and C) are integer vectors with entries bounded by ğ‘›ğ‘‚(1), and with the exceptional values +âˆ and âˆ’âˆ. A prominent example of a convolution problem is (min,+)-Convolution discussed above; another similarly prominent example is (max,min)-Convolution which can be solved in ğ‘‚Ìƒ (ğ‘›3/2) time [8]. For our purposes, it is convenient to look at a skewed variant of this problem:

Definition 3
(Skewed Convolution) Given two vectors ğ´=(ğ´[ğ‘–])ğ‘›ğ‘–=0, ğµ=(ğµ[ğ‘—])ğ‘›ğ‘—=0, we define the (max,min)-Skewed-Convolution problem to be the problem of computing the vector ğ¶=(ğ¶[ğ‘˜])2ğ‘›ğ‘˜=0 where the kth entry in C equals

ğ¶[ğ‘˜]=maxğ‘–+ğ‘—=ğ‘˜min{ğ´[ğ‘–],ğµ[ğ‘—]+ğ‘˜}
for each ğ‘˜âˆˆ{0,â€¦,2ğ‘›}.

The main technical result of this paper is an algorithm for (max,min)-Skewed-Convolution that is significantly faster than the naive ğ‘‚(ğ‘›2) time algorithm.

Theorem 8
The (max,min)-Skewed-Convolution problem can be solved in ğ‘‚Ìƒ (ğ‘›7/4) time.

Algorithm via Sumsets and Subset Sums
In the following section, we provide a proof of Theorem 6 by presenting an algorithm for 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— running in ğ‘‚Ìƒ (min{ğ‘ƒâ‹…ğ·#,ğ‘ƒ+ğ·}) time. Recall that ğ½={1,â€¦,ğ‘›} denotes our input set of jobs, and ğ‘ğ‘— and ğ‘‘ğ‘— respectively denote the processing time and due date of job ğ‘—âˆˆ{1,â€¦,ğ‘›}. Our goal is to determine the minimum total processing time of tardy jobs in any schedule for J. Throughout the section we let ğ‘‘(1)<â‹¯<ğ‘‘(ğ·#) denote the ğ·#â‰¤ğ‘› different due dates of the jobs in J.

A key observation for the 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem, used already by Lawler and Moore, is that any instance of the problem always has an optimal schedule of a specific type, namely an Earliest Due Date schedule. An Earliest Due Date (EDD) schedule is a schedule ğœ‹:ğ½â†’{1,â€¦,ğ‘›} such that

any early job precedes all late jobs in ğœ‹, and

any early job precedes all early jobs with later due dates.

In other words, in an EDD schedule all early jobs are scheduled before all tardy jobs, and all early jobs are scheduled in non-decreasing order of due dates.

Lemma 3
([10]) Any 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— instance has an optimal schedule which is EDD.

The ğ·#-many due dates in our instance partition the input set of job J in a natural manner: Define ğ½ğ‘–={ğ‘—:ğ‘‘ğ‘—=ğ‘‘(ğ‘–)} for each ğ‘–âˆˆ{1,â€¦,ğ·#}. Furthermore, let ğ‘‹ğ‘–={ğ‘ğ‘—:ğ‘—âˆˆğ½ğ‘–} the processing-times of job in ğ½ğ‘–. According to Lemma 3 above, we can restrict our attention to EDD schedules. Constructing such a schedule corresponds to choosing a subset ğ¸ğ‘–âŠ†ğ½ğ‘– for each due date ğ‘‘(ğ‘–) such that âˆ‘ğ‘—âˆˆğ¸â„“,â„“â‰¤ğ‘–ğ‘ğ‘—â‰¤ğ‘‘(ğ‘–) holds for each ğ‘–âˆˆ{1,â€¦,ğ·#}. Moreover, the optimal EDD schedule maximizes the total sum of processing times in all selected ğ¸ğ‘–â€™s.

Our algorithm is given in Algorithm 1. It successively computes sets ğ‘†1,â€¦,ğ‘†ğ·#, where set ğ‘†ğ‘– corresponds to the set of jobs ğ½1âˆªâ‹¯âˆªğ½ğ‘–. In particular, ğ‘†ğ‘– includes the total processing-time of any possible set-family of early jobs {ğ¸1,â€¦,ğ¸ğ‘–}. Thus, each ğ‘¥âˆˆğ‘†ğ‘– corresponds to the total processing time of early jobs in a subset of ğ½1âˆªâ‹¯âˆªğ½ğ‘–. The maximum value ğ‘¥âˆˆğ‘†ğ·# therefore corresponds to the maximum total processing time of early jobs in any schedule for J. Thus, the algorithm terminates by returning the optimal total weight of tardy jobs ğ‘ƒâˆ’ğ‘¥.

figure a
Correctness of our algorithm follows immediately from the definitions of sumsets and subset sums, and from the fact that we prune out elements ğ‘¥âˆˆğ‘†ğ‘– with ğ‘¥>ğ‘‘(ğ‘–) at each step of the algorithm. This is stated more formally in the lemma below.

Lemma 4
Let ğ‘–âˆˆ{1,â€¦,ğ·#}, and let ğ‘†ğ‘– be the set of integers at the end of the second step of 5(i). Then ğ‘¥âˆˆğ‘†ğ‘– if and only if there are sets of jobs ğ¸1âŠ†ğ½1,â€¦,ğ¸ğ‘–âŠ†ğ½ğ‘– such that

âˆ‘ğ‘—âˆˆâ‹ƒğ‘–â„“=1ğ¸â„“ğ‘ğ‘—=ğ‘¥, and

âˆ‘ğ‘—âˆˆâ‹ƒğ‘–0â„“=1ğ¸â„“ğ‘ğ‘—â‰¤ğ‘‘(ğ‘–0) holds for each ğ‘–0âˆˆ{1,â€¦,ğ‘–}.

Proof
The proof is by induction on i. For ğ‘–=1, note that ğ‘†1=îˆ¿(ğ‘‹1)âˆ–{ğ‘¥:ğ‘¥>ğ‘‘(1)} at the end of step 5(1). Since îˆ¿(ğ‘‹1) includes the total processing time of any subset of jobs in ğ½1, the first condition of the lemma holds. Since {ğ‘¥:ğ‘¥>ğ‘‘(1)} includes all integers violating the second condition of the lemma, the second condition holds.

Let ğ‘–>1, and assume the lemma holds for ğ‘–âˆ’1. Consider some ğ‘¥âˆˆğ‘†ğ‘– at the end of the second step of 5(i). Then by Definition 1, we have ğ‘¥=ğ‘¥1+ğ‘¥2 for some ğ‘¥1âˆˆğ‘†ğ‘–âˆ’1 and ğ‘¥2âˆˆîˆ¿(ğ‘‹ğ‘–) due the first step of 5(i). By definition of îˆ¿(ğ‘‹ğ‘–), there is some ğ¸ğ‘–âŠ†ğ½ğ‘– with total processing time ğ‘¥2. By our inductive hypothesis there is ğ¸1âŠ†ğ½1,â€¦,ğ¸ğ‘–âˆ’1âŠ†ğ½ğ‘–âˆ’1 such that âˆ‘ğ‘—âˆˆâ‹ƒğ‘–â„“=1ğ¸â„“ğ‘ğ‘—=ğ‘¥1, and âˆ‘ğ‘—âˆˆğ¸â„“,â„“â‰¤ğ‘–0ğ‘ğ‘—â‰¤ğ‘‘(ğ‘–0) holds for each ğ‘–0âˆˆ{1,â€¦,ğ‘–âˆ’1}. Furthermore, by the second step of 5(i), we know that âˆ‘ğ‘—âˆˆğ¸â„“,â„“â‰¤ğ‘–ğ‘ğ‘—=ğ‘¥â‰¤ğ‘‘(ğ‘–). Thus, ğ¸1,â€¦,ğ¸ğ‘– satisfy both conditions of the lemma. â—»

Let us next analyze the time complexity of the SUMSETSCHEDULER algorithm. Steps 1 and 2 can be both performed in ğ‘‚Ìƒ (ğ‘›)=ğ‘‚Ìƒ (ğ‘ƒ) time. Next observe that step 3 can be done in total ğ‘‚Ìƒ (ğ‘ƒ) time using Lemma 2, as ğ‘‹2,â€¦,ğ‘‹ğ·# is a partition of the set of all processing times of J, and these all sum up to P. Next, according to Lemma 1, each sumset operation at step 5 can be done in time proportional to the largest element in the two sets, which is always at most P. Thus, since we perform at most ğ·# sumset operations, the merging step requires ğ‘‚Ìƒ (ğ·#â‹…ğ‘ƒ) time, which gives us the total running time of the algorithm above.

Another way to analyze the running time of SUMSETSCHEDULER is to observe that the maximum element participating in the ith sumset is bounded by ğ‘‘(ğ‘–+1). It follows that we can write the running time of the merging step as ğ‘‚Ìƒ (ğ·), where ğ·=âˆ‘ğ·#ğ‘–=1ğ‘‘(ğ‘–). Thus, we have just shown that 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— can be solved in ğ‘‚Ìƒ (min{ğ·#â‹…ğ‘ƒ,ğ·+ğ‘ƒ}) time, completing the proof of Theorem 6.

Algorithm via Fast Skewed Convolutions
We next present our ğ‘‚Ìƒ (ğ‘ƒ7/4) time algorithm for 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—, providing a proof of Theorem 5. As in the previous section, we let ğ‘‘(1)<â‹¯<ğ‘‘(ğ·#) denote the ğ·#â‰¤ğ‘› different due dates of the input jobs J, and ğ½ğ‘–={ğ‘—:ğ‘‘ğ‘—=ğ‘‘(ğ‘–)} and ğ‘‹ğ‘–={ğ‘ğ‘—:ğ‘—âˆˆğ½ğ‘–} as in Sect. 3 for each ğ‘–âˆˆ{1,â€¦,ğ·#}.

For a consecutive subset of indices ğ¼={ğ‘–0,ğ‘–0+1,â€¦,ğ‘–1}, with ğ‘–0,â€¦,ğ‘–1âˆˆ{1,â€¦,ğ·#}, we define a vector M(I), where M(I)[x] equals the latest (that is, maximum) time point ğ‘¥0 for which there is a subset of the jobs in â‹ƒğ‘–âˆˆğ¼ğ½ğ‘– with total processing time equal to x that can all be scheduled early in an EDD schedule starting at ğ‘¥0. If no such subset of jobs exists, we define ğ‘€(ğ¼)[ğ‘¥]=âˆ’âˆ.

For a singleton set ğ¼={ğ‘–}, the vector M(I) is easy to compute once we have computed the set îˆ¿(ğ‘‹ğ‘–):

ğ‘€({ğ‘–})[ğ‘¥]={ğ‘‘(ğ‘–)âˆ’ğ‘¥âˆ’âˆ if ğ‘¥âˆˆîˆ¿(ğ‘‹ğ‘–) and ğ‘¥â‰¤ğ‘‘(ğ‘–), otherwise .
(1)
For larger sets of indices, we have the following lemma.

Lemma 5
Let ğ¼1={ğ‘–0,ğ‘–0+1,â€¦,ğ‘–1} and ğ¼2={ğ‘–1+1,ğ‘–1+2,â€¦,ğ‘–2} be any two sets of consecutive indices with ğ‘–0,â€¦,ğ‘–1,â€¦,ğ‘–2âˆˆ{1,â€¦,ğ·#}. Then for any value x we have:

ğ‘€(ğ¼1âˆªğ¼2)[ğ‘¥]=maxğ‘¥1+ğ‘¥2=ğ‘¥min{ğ‘€(ğ¼1)[ğ‘¥1],ğ‘€(ğ¼2)[ğ‘¥2]âˆ’ğ‘¥1}.
Proof
Let ğ¼=ğ¼1âˆªğ¼2. Then M(I)[x] is the latest time point after which a subset of jobs ğ½âˆ—âŠ†â‹ƒğ‘–âˆˆğ¼ğ½ğ‘– of total processing time x can be scheduled early in an EDD schedule. Let ğ‘¥1 and ğ‘¥2 be the total processing times of jobs in ğ½âˆ—1=ğ½âˆ—âˆ©(â‹ƒğ‘–âˆˆğ¼1ğ½ğ‘–) and ğ½âˆ—2=ğ½âˆ—âˆ©(â‹ƒğ‘–âˆˆğ¼2ğ½ğ‘–), respectively. Then ğ‘¥=ğ‘¥1+ğ‘¥2. Clearly, ğ‘€(ğ¼)[ğ‘¥]â‰¤ğ‘€(ğ¼1)[ğ‘¥1], since we have to start scheduling the jobs in ğ½âˆ—1 at time ğ‘€(ğ¼1)[ğ‘¥1] by latest. Similarly, it holds that ğ‘€(ğ¼)[ğ‘¥]â‰¤ğ‘€(ğ¼2)[ğ‘¥2]âˆ’ğ‘¥1 since the jobs in ğ½âˆ—2 are scheduled at latest at ğ‘€(ğ¼2)[ğ‘¥2] and the jobs in ğ½âˆ—1 have to be processed before that time point in an EDD schedule. In combination, we have shown that LHS â‰¤ RHS in the equation of the lemma.

To prove that LHS â‰¥ RHS, we construct a feasible schedule for jobs in â‹ƒğ‘–âˆˆğ¼ğ½ğ‘– starting at RHS. Let ğ‘¥1 and ğ‘¥2 be the two values with ğ‘¥1+ğ‘¥2=ğ‘¥ that maximize RHS. Then there is a schedule which schedules some jobs ğ½âˆ—1âŠ†â‹ƒğ‘–âˆˆğ¼1ğ½ğ‘– of total processing time ğ‘¥1 beginning at time min{ğ‘€(ğ¼1)[ğ‘¥1],ğ‘€(ğ¼2)[ğ‘¥2]âˆ’ğ‘¥1}â‰¤ğ‘€(ğ¼1)[ğ‘¥1], followed by a another subset of jobs ğ½âˆ—2âŠ†â‹ƒğ‘–âˆˆğ¼2ğ½ğ‘– of total processing time ğ‘¥2 starting at time min{ğ‘€(ğ¼1)[ğ‘¥1],ğ‘€(ğ¼2)[ğ‘¥2]âˆ’ğ‘¥1}+ğ‘¥1â‰¤ğ‘€(ğ¼2)[ğ‘¥2]. This is a feasible schedule starting at time RHS for a subset of jobs in â‹ƒğ‘–âˆˆğ¼ğ½ğ‘– which has total processing time x. â—»

Note that the equation given in Lemma 5 is close but not precisely the equation defined in Definition 3 for the (min,max)-Skewed-Convolution problem. Nevertheless, the next lemma shows that we can easily translate between these two concepts.

Lemma 6
Let A and B be two integer vectors of P entries each. Given an algorithm for computing the (max,min)-Skewed-Convolution of A and B in T(P) time, we can compute in ğ‘‡(ğ‘ƒ)+ğ‘‚(ğ‘ƒ) time the vector ğ¶=ğ´âŠ—ğµ defined by

ğ¶[ğ‘¥]=maxğ‘¥1+ğ‘¥2=ğ‘¥min{ğ´[ğ‘¥1],ğµ[ğ‘¥2]âˆ’ğ‘¥1}.
Proof
Given A and B, construct two auxiliary vectors ğ´0 and ğµ0 defined by ğ´0[ğ‘¥]=ğµ[ğ‘¥]+ğ‘¥ and ğµ0[ğ‘¥]=ğ´[ğ‘¥] for each entry x. Compute the (max,min)-Skewed-Convolution of ğ´0 and ğµ0, and let ğ¶0 denote the resulting vector. We claim that the vector C defined by ğ¶[ğ‘¥]=ğ¶0[ğ‘¥]âˆ’ğ‘¥ equals ğ´âŠ—ğµ. Indeed, we have

ğ¶0[ğ‘¥]âˆ’ğ‘¥=====maxğ‘¥1+ğ‘¥2=ğ‘¥min{ğ´0[ğ‘¥1],ğµ0[ğ‘¥2]+ğ‘¥}âˆ’ğ‘¥maxğ‘¥1+ğ‘¥2=ğ‘¥min{ğ´0[ğ‘¥1]âˆ’ğ‘¥,ğµ0[ğ‘¥2]}maxğ‘¥1+ğ‘¥2=ğ‘¥min{ğµ[ğ‘¥1]+ğ‘¥1âˆ’ğ‘¥,ğ´[ğ‘¥2]}maxğ‘¥1+ğ‘¥2=ğ‘¥min{ğµ[ğ‘¥1]âˆ’ğ‘¥2,ğ´[ğ‘¥2]}maxğ‘¥1+ğ‘¥2=ğ‘¥min{ğ´[ğ‘¥1],ğµ[ğ‘¥2]âˆ’ğ‘¥1},
where in the third step we expanded the definition of ğ´0 and ğµ0, and in the last step we used the symmetry of ğ‘¥1 and ğ‘¥2. â—»

We are now in position to describe our algorithm called CONVSCHEDULER which is depicted in Algorithm 2. The algorithm first computes the subset sums îˆ¿(ğ‘‹1),â€¦,îˆ¿(ğ‘‹ğ·#), and the set of vectors îˆ¹={ğ‘€1,â€¦,ğ‘€ğ·#}. Following this, it iteratively combines every two consecutive vectors in îˆ¹ by using the âŠ— operation. The algorithm terminates when îˆ¹={ğ‘€1}, where at this stage ğ‘€1 corresponds to the entire set of input jobs J. It then returns ğ‘ƒâˆ’ğ‘¥, where x is the maximum value with ğ‘€1[ğ‘¥]>âˆ’âˆ; by definition, this corresponds to a schedule for J with ğ‘ƒâˆ’ğ‘¥ total processing time of tardy jobs. For convenience of presentation, we assume that ğ·# is a power of 2.

figure b
Correctness of this algorithm follows directly from Lemma 5. To analyze its time complexity, observe that steps 1â€“4 can be done in ğ‘‚Ìƒ (ğ‘ƒ) time (using Lemma 2). Step 5 is performed ğ‘‚(logğ·#)=ğ‘‚(logğ‘ƒ) times, and each step requires a total of ğ‘‚Ìƒ (ğ‘ƒ7/4) time according to Theorem 8, as the total sizes of all vectors at each step is O(P). Finally, step 6 requires O(P) time. Summing up, this gives us a total running time of ğ‘‚Ìƒ (ğ‘ƒ7/4), and completes the proof of Theorem 5 (apart from the proof of Theorem 8).

Fast Skewed Convolutions
In the following section we present our algorithm for (max,min)-Skewed-Convolution, and provide a proof for Theorem 8. Let ğ´=(ğ´[ğ‘–])ğ‘›ğ‘–=0 and ğµ=(ğµ[ğ‘—])ğ‘›ğ‘—=0 denote the input vectors for the problem throughout the section. Recall we wish the compute the vector ğ¶â„“=(ğ¶[ğ‘˜])2ğ‘›ğ‘˜=0 where

ğ¶[ğ‘˜]=maxğ‘–+ğ‘—=ğ‘˜min{ğ´[ğ‘–],ğµ[ğ‘—]+ğ‘˜}
for each ğ‘˜âˆˆ{0,â€¦,2ğ‘›}.

We begin by first defining the problem slightly more generally, in order to facilitate our recursive strategy later on. For this, for each integer â„“âˆˆ{0,â€¦,log2ğ‘›}, let ğ´â„“=âŒŠğ´/2â„“âŒ‹ and ğµâ„“=âŒŠğµ/2â„“âŒ‹, where rounding is done component-wise. We will compute vectors ğ¶â„“=(ğ¶â„“[ğ‘˜])2ğ‘›ğ‘˜=0 defined by:

ğ¶â„“[ğ‘˜]=maxğ‘–+ğ‘—=ğ‘˜min{ğ´â„“[ğ‘–],ğµâ„“[ğ‘—]+âŒŠğ‘˜/2â„“âŒ‹}.
Observe that a solution for â„“=0 yields a solution to the original (max,min)-Skewed-Convolution problem, and for â„“â‰¥log(2ğ‘›) the problem degenerates to (max,min)-Convolution.

We next define a particular kind of additive approximation of vectors ğ¶â„“. We say that a vector ğ·â„“ is a good approximation of ğ¶â„“ if ğ¶â„“[ğ‘˜]âˆ’2â‰¤ğ·â„“[ğ‘˜]â‰¤ğ¶â„“[ğ‘˜] for each ğ‘˜âˆˆ{0,â€¦,2ğ‘›}. Now, the main technical part of our algorithm is encapsulated in the following lemma.

Lemma 7
There is an algorithm that computes ğ¶â„“ in ğ‘‚Ìƒ (ğ‘›7/4) time, given ğ´â„“, ğµâ„“, and a good approximation ğ·â„“ of ğ¶â„“.

We postpone the proof of Lemma 7 for now, and instead show that it directly yields our desired algorithm for (max,min)-Skewed-Convolution:

Proof of Theorem 8
In order to compute ğ¶=ğ¶0, we perform an (inverse) induction on â„“: As mentioned before, if â„“â‰¥log(2ğ‘›), then we can neglect the â€œ+ âŒŠğ‘˜/2â„“âŒ‹â€ term and compute ğ¶â„“ in ğ‘‚Ìƒ (ğ‘›3/2)=ğ‘‚Ìƒ (ğ‘›7/4) time using a single (max,min)-Convolution computation [8].

For the inductive step, let â„“<log(2ğ‘›) and assume that we have already computed ğ¶â„“+1. We construct the vector ğ·â„“=2ğ¶â„“+1, and argue that it is a good approximation of ğ¶â„“. Indeed, for each entry k, on the one hand, we have:

ğ·â„“[ğ‘˜]=â‰¤2ğ¶â„“+1[ğ‘˜]=2â‹…maxğ‘–+ğ‘—=ğ‘˜min{âŒŠğ´â„“[ğ‘–]/2âŒ‹,âŒŠğµâ„“[ğ‘—]/2âŒ‹+âŒŠğ‘˜/2â„“+1âŒ‹}maxğ‘–+ğ‘—=ğ‘˜min{ğ´â„“[ğ‘–],ğµâ„“[ğ‘—]+âŒŠğ‘˜/2â„“âŒ‹}=ğ¶â„“[ğ‘˜];
and on the other hand, we have:

ğ·â„“[ğ‘˜]=â‰¥2ğ¶â„“+1[ğ‘˜]=2â‹…maxğ‘–+ğ‘—=ğ‘˜min{âŒŠğ´â„“[ğ‘–]/2âŒ‹,âŒŠğµâ„“[ğ‘—]/2âŒ‹+âŒŠğ‘˜/2â„“+1âŒ‹}maxğ‘–+ğ‘—=ğ‘˜min{ğ´â„“[ğ‘–]âˆ’1,ğµâ„“[ğ‘—]+âŒŠğ‘˜/2â„“âŒ‹âˆ’2}â‰¥ğ¶â„“[ğ‘˜]âˆ’2.
Thus, using ğ·â„“ we can apply Lemma 7 above to obtain ğ¶â„“ in ğ‘‚Ìƒ (ğ‘›7/4) time. Since there are ğ‘‚(logğ‘›) inductive steps overall, this is also the overall time complexity of the algorithm. â—»

It remains to prove Lemma 7. Recall that we are given ğ´â„“, ğµâ„“, and ğ·â„“, and our goal is to compute the vector ğ¶â„“ in ğ‘‚Ìƒ (ğ‘›7/4) time. We construct two vectors ğ¿â„“ and ğ‘…â„“ with 2n entries each, defined by

ğ¿â„“[ğ‘˜]=max{ğ´â„“[ğ‘–0]:ğ´â„“[ğ‘–0]â‰¤ğµâ„“[ğ‘˜âˆ’ğ‘–0]+âŒŠğ‘˜/2â„“âŒ‹ andğ·â„“[ğ‘˜]â‰¤ğ´â„“[ğ‘–0]â‰¤ğ·â„“[ğ‘˜]+2},
and

ğ‘…â„“[ğ‘˜]=max{ğµâ„“[ğ‘—0]+âŒŠğ‘˜/2â„“âŒ‹:ğµâ„“[ğ‘—0]+âŒŠğ‘˜/2â„“âŒ‹â‰¤ğ´â„“[ğ‘˜âˆ’ğ‘—0] andğ·â„“[ğ‘˜]â‰¤ğµâ„“[ğ‘—0]+âŒŠğ‘˜/2â„“âŒ‹â‰¤ğ·â„“[ğ‘˜]+2}
for ğ‘˜âˆˆ{0,â€¦,2ğ‘›}. That is, ğ¿â„“[ğ‘˜] and ğ‘…â„“[ğ‘˜] respectively capture the largest value attained as the left-hand side or right-hand side of the inner min-operation in ğ¶â„“[ğ‘˜], as long as that value lies in the feasible region approximated by ğ·â„“[ğ‘˜]. Since ğ·â„“ is a good approximation, the following lemma is immediate from the definitions:

Lemma 8
ğ¶â„“[ğ‘˜]=max{ğ¿â„“[ğ‘˜],ğ‘…â„“[ğ‘˜]} for each ğ‘˜âˆˆ{0,â€¦,2ğ‘›}.

According to Lemma 8, it suffices to compute ğ¿â„“ and ğ‘…â„“. We first focus on computing ğ¿â„“. The computation of ğ‘…â„“ is very similar and we will later point out the necessary changes.

Let 0<ğ›¿<1 be a fixed constant to be determined later. We say that an index ğ‘˜âˆˆ{0,â€¦,2ğ‘›} is light if

|{ğ‘–:ğ·â„“[ğ‘˜]â‰¤ğ´â„“[ğ‘–]â‰¤ğ·â„“[ğ‘˜]+2}|â‰¤ğ‘›ğ›¿.
Informally, k is light if the number of candidate entries ğ´â„“[ğ‘–] which can equal ğ¶â„“[ğ‘˜] is relatively small (recall that ğ·â„“[ğ‘˜]â‰¤ğ¶â„“[ğ‘˜]â‰¤ğ·â„“[ğ‘˜]+2, as ğ·â„“ is a good approximation of ğ¶â„“). If k is not light then we say that it is heavy.

Our algorithm for computing ğ¿â„“ proceeds in three main steps: In the first step it handles all light indices, in the second step it sparsifies the input vector, and in the third step it handles all heavy indices:

Light indices: We begin by iterating over all light indices ğ‘˜âˆˆ{0,â€¦,2ğ‘›}. For each light index k, we iterate over all entries ğ´â„“[ğ‘–] satisfying ğ·â„“[ğ‘˜]â‰¤ğ´â„“[ğ‘–]â‰¤ğ·â„“[ğ‘˜]+2, and set ğ¿â„“[ğ‘˜] to be the maximum ğ´â„“[ğ‘–] among those entries with ğ´â„“[ğ‘–]â‰¤ğµâ„“[ğ‘˜âˆ’ğ‘–]+âŒŠğ‘˜/2â„“âŒ‹. Note that after this step, we have

ğ¿â„“[ğ‘˜]=max{ğ´â„“[ğ‘–0]:ğ´â„“[ğ‘–0]â‰¤ğµâ„“[ğ‘˜âˆ’ğ‘–0]+âŒŠğ‘˜/2â„“âŒ‹ and ğ·â„“[ğ‘˜]â‰¤ğ´â„“[ğ‘–0]â‰¤ğ·â„“[ğ‘˜]+2},
for each light index k.

Sparsification step: After dealing with the light indices, several entries of ğ´â„“ become redundant. Consider an entry ğ´â„“[ğ‘–] for which |{ğ‘–0:ğ´â„“[ğ‘–]âˆ’2â‰¤ğ´â„“[ğ‘–0]â‰¤ğ´â„“[ğ‘–]+2}|â‰¤ğ‘›ğ›¿. Then all indices k for which ğ¿â„“[ğ‘˜] might equal ğ´â„“[ğ‘–] must be light, and are therefore already dealt with in the previous step. Consequently, it is safe to replace ğ´â„“[ğ‘–] by âˆ’âˆ so that ğ´â„“[ğ‘–] no longer plays a role in the remaining computation.

Heavy indices: After the sparsification step ğ´â„“ contains few distinct values. Thus, our approach is to fix any such value v and detect whether ğ¿â„“[ğ‘˜]â‰¥ğ‘£. To that end, we translate the problem into an instance of (max,min)-Convolution: Let (ğ´â„“ğ‘£[ğ‘–])ğ‘›ğ‘–=0 be an be an indicator-like vector defined by ğ´â„“ğ‘£[ğ‘–]=+âˆ if ğ´â„“[ğ‘–]=ğ‘£, and otherwise ğ´â„“ğ‘£[ğ‘–]=âˆ’âˆ. We next compute the vector ğ¿â„“ğ‘£ defined by ğ¿â„“ğ‘£[ğ‘˜]=âŒŠğ‘˜/2â„“âŒ‹+maxğ‘–+ğ‘—=ğ‘˜min{ğ´â„“ğ‘£[ğ‘–],ğµâ„“[ğ‘—]} using a single computation of (max,min)-Convolution. We choose

ğ¿â„“[ğ‘˜]=max{ğ‘£:ğ¿â„“ğ‘£[ğ‘˜]â‰¥ğ‘£ and ğ·â„“[ğ‘˜]â‰¤ğ‘£â‰¤ğ·â„“[ğ‘˜]+2}
for any heavy index k, and claim that ğ¿â„“[ğ‘˜] equals max{ğ´â„“[ğ‘–0]:ğ´â„“[ğ‘–0]â‰¤ğµâ„“[ğ‘˜âˆ’ğ‘–0]+âŒŠğ‘˜/2â„“âŒ‹}.

On the one hand, if ğ¿â„“ğ‘£[ğ‘˜]â‰¥ğ‘£ then there are indices i and j with ğ‘–+ğ‘—=ğ‘˜ for which ğ´â„“[ğ‘–]=ğ‘£ and ğµâ„“[ğ‘—]+âŒŠğ‘˜/2â„“âŒ‹â‰¥ğ´â„“[ğ‘–]=ğ‘£. Thus, the computed value ğ¿â„“[ğ‘˜] is not greater than

ğ¿â„“[ğ‘˜]â‰¤max{ğ´â„“[ğ‘–0]:ğ´â„“[ğ‘–0]â‰¤ğµâ„“[ğ‘˜âˆ’ğ‘–0]+âŒŠğ‘˜/2â„“âŒ‹ and ğ·â„“[ğ‘˜]â‰¤ğ´â„“[ğ‘–0]â‰¤ğ·â„“[ğ‘˜]+2}.
On the other hand, for all values v for which ğ´â„“[ğ‘–]=ğ‘£ for some ğ‘–âˆˆ{0,â€¦,ğ‘›}, we have if ğ‘£=ğ´â„“[ğ‘–]â‰¤ğµâ„“[ğ‘˜âˆ’ğ‘–]+âŒŠğ‘˜/2â„“âŒ‹ then ğ´â„“ğ‘£[ğ‘–]=âˆ’âˆ, which in turn implies that ğ´â„“ğ‘£[ğ‘–]â‰¥ğµâ„“[ğ‘˜âˆ’ğ‘–]+âŒŠğ‘˜/2â„“âŒ‹â‰¥ğ´â„“[ğ‘–]=ğ‘£. Thus, our selection of ğ¿â„“[ğ‘˜] is also at least as large as

ğ¿â„“[ğ‘˜]â‰¥max{ğ´â„“[ğ‘–0]:ğ´â„“[ğ‘–0]â‰¤ğµâ„“[ğ‘˜âˆ’ğ‘–0]+âŒŠğ‘˜/2â„“âŒ‹ and ğ·â„“[ğ‘˜]â‰¤ğ´â„“[ğ‘–0]â‰¤ğ·â„“[ğ‘˜]+2},
and hence, these two values must be equal.

We finally argue how to adapt the approach to compute ğ‘…â„“. In the first step, we instead classify an index k as light if |{ğ‘–:ğ·â„“[ğ‘˜]â‰¤ğµâ„“[ğ‘—]+âŒŠğ‘˜/2â„“âŒ‹â‰¤ğ·â„“[ğ‘˜]+2}|â‰¤ğ‘›ğ›¿. In the same way as before we can compute ğ‘…â„“[ğ‘˜] for all light indices k, as well as apply the sparsification step to replace all entries ğµâ„“[ğ‘—] which satisfy |{ğ‘—0:ğµâ„“[ğ‘—]âˆ’2â‰¤ğµâ„“[ğ‘—0]â‰¤ğµâ„“[ğ‘—]+2}|â‰¤ğ‘›ğ›¿ by âˆ’âˆ. After the sparsification, the vector ğµâ„“ contains only few distinct values, and for any such value v we proceed similar to before. Defining ğµâ„“ğ‘£ analogously, we compute ğ‘…â„“ğ‘£[ğ‘˜]=maxğ‘–+ğ‘—=ğ‘˜min{ğ´â„“[ğ‘–],ğµâ„“ğ‘£[ğ‘—]} and return

ğ‘…â„“[ğ‘˜]=max{ğ‘£+âŒŠğ‘˜/2â„“âŒ‹:ğ‘…â„“ğ‘£[ğ‘˜]â‰¥ğ‘£+âŒŠğ‘˜/2â„“âŒ‹ and ğ·â„“[ğ‘˜]â‰¤ğ‘£+âŒŠğ‘˜/2â„“âŒ‹â‰¤ğ·â„“[ğ‘˜]+2}
for all heavy indices k. One can verify that this choice of ğ‘…â„“[ğ‘˜] is correct with exactly the same proof as before.

This completes the description of our algorithm. As we argued its correctness above, what remains is to analyze its time complexity. Note that we can determine in ğ‘‚(logğ‘›) time whether an index k is light or heavy, by first sorting the values in ğ´â„“. For each light index k, determining ğ¿â„“[ğ‘˜] can be done in ğ‘‚(ğ‘›ğ›¿) time (on the sorted ğ´â„“), giving us a total of ğ‘‚Ìƒ (ğ‘›1+ğ›¿) time for the first step. For the second step, we can determine whether a given entry ğ´â„“[ğ‘–] can be replaced with âˆ’âˆ in ğ‘‚(logğ‘›) time, giving us a total of ğ‘‚Ìƒ (ğ‘›) time for this step.

Consider then the final step of the algorithm. Observe that after exhausting the sparsification step, ğ´â„“ contains at most ğ‘‚(ğ‘›1âˆ’ğ›¿) many distinct values: For any surviving value v, there is another (perhaps different) value ğ‘£â€² of difference at most 2 from v that occurs at least 1/5â‹…ğ‘›ğ›¿ times in ğ´â„“, and so there can only be at most ğ‘‚(ğ‘›1âˆ’ğ›¿) such distinct values. Thus, the running time of this step is dominated by the running time of ğ‘‚(ğ‘›1âˆ’ğ›¿) (max,min)-Convolution computations, each requiring ğ‘‚Ìƒ (ğ‘›3/2) time using the algorithm of [8], giving us a total of ğ‘‚Ìƒ (ğ‘›5/2âˆ’ğ›¿) time for this step.

Thus, the running time of our algorithm is dominated by the ğ‘‚Ìƒ (ğ‘›1+ğ›¿) running time of its first step, and the ğ‘‚Ìƒ (ğ‘›5/2âˆ’ğ›¿) running time of its last step. Choosing ğ›¿=3/4 gives us ğ‘‚Ìƒ (ğ‘›7/4) time for both steps, which is the time promised by Lemma 7. Thus, Lemma 7 holds.

Multiple Machines
In the following we show how to extend the SUMSETSCHEDULER algorithm of Sect. 3 to the case of a fixed number m multiple machines, the ğ‘ƒğ‘š||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem. In this variant, we have m machines at our disposal, and so a schedule ğœ for the set of jobs J is now a function ğœ:{1,â€¦,ğ‘›}â†’{1,â€¦,ğ‘š}Ã—{1,â€¦,ğ‘›}. The first component of ğœ(ğ‘—) specifies the machine one which j is scheduled, and the second component specifies its order on the machine. The completion time ğ¶ğ‘— of j is then the sum of all jobs preceding j (including j itself) on the same machine that j is scheduled. Our goal remains to minimize the total processing time of tardy jobs âˆ‘ğ‘ğ‘—ğ‘ˆğ‘—.

For extending algorithm SUMSETSCHEDULER, we need to first extend Definition 2 to the case of multiple machines.

Definition 4
For a given set of non-negative integers X, define the set îˆ¿ğ‘š(ğ‘‹) as the set of m-tuples given by

îˆ¿ğ‘š(ğ‘‹)={(âˆ‘ğ‘¥âˆˆğ‘Œ1ğ‘¥,â€¦,âˆ‘ğ‘¥âˆˆğ‘Œğ‘šğ‘¥):ğ‘Œ1,â€¦,ğ‘Œğ‘šâŠ†ğ‘‹ and ğ‘Œğ‘–âˆ©ğ‘Œğ‘—=âˆ… for  each ğ‘–,ğ‘—âˆˆ{1,â€¦,ğ‘š},ğ‘–â‰ ğ‘—}.
Thus, every element in îˆ¿ğ‘š(ğ‘‹) is an m-tuple ğ±=(ğ‘¥1,â€¦,ğ‘¥ğ‘š) of non-negative integers, where we interpret ğ‘¥ğ‘– as the total processing time on machine ğ‘–âˆˆ{1,â€¦,ğ‘š}. We consider component-wise addition between two m-tuples, and define the sumset ğ‘‹1âŠ•ğ‘‹2 of two sets of m-tuples as in Definition 1; i.e., ğ‘‹1âŠ•ğ‘‹2={ğ±1+ğ±2:ğ±1âˆˆğ‘‹1,ğ±2âˆˆğ‘‹2}.

To efficiently compute the sumset ğ‘‹1âŠ•ğ‘‹2 when ğ‘‹1 and ğ‘‹2 are sets of m-tuples we use multivariate polynomial multiplication. Let ğ‘1[ğ›¼1,â€¦,ğ›¼ğ‘š]=âˆ‘(ğ‘¥1,â€¦,ğ‘¥ğ‘š)âˆˆğ‘‹1ğ›±ğ‘šğ‘–=1ğ›¼ğ‘¥ğ‘–ğ‘– and ğ‘2[ğ›½1,â€¦,ğ›½ğ‘š]=âˆ‘(ğ‘¥1,â€¦,ğ‘¥ğ‘š)âˆˆğ‘‹2ğ›±ğ‘šğ‘–=1ğ›½ğ‘¥ğ‘–ğ‘–. Then the exponents of all terms in ğ‘1â‹…ğ‘2 with non-zero coefficients correspond to elements in the sumset ğ‘‹1âŠ•ğ‘‹2. Since multiplying two m-variate polynomials of maximum degree d on each variable can be reduced to multiplying two univariate polynomials of maximum degree ğ‘‚(ğ‘‘ğ‘š) using Kroneckerâ€™s map (see e.g. [11]), we obtain the following:

Lemma 9
Given two sets of m-tuples of non-negative integers ğ‘‹1,ğ‘‹2âŠ†{0,â€¦,ğ‘ƒ}ğ‘š, one can compute the sumset ğ‘‹1âŠ•ğ‘‹2 in ğ‘‚(ğ‘ƒğ‘šlogğ‘ƒ) time.

Using the same divide and conquer approach used for Lemma 2, we can use Lemma 9 above to compute îˆ¿ğ‘š(ğ‘‹) from X. The same analysis used for Lemma 2 will give us a running time of ğ‘‚(ğ‘ƒğ‘šlogğ‘ƒ) instead of ğ‘‚(ğ‘ƒlogğ‘ƒ).

Lemma 10
Given a set of non-negative integers X, with ğ‘ƒ=âˆ‘ğ‘¥âˆˆğ‘‹ğ‘¥, one can compute îˆ¿ğ‘š(ğ‘‹) in ğ‘‚Ìƒ (ğ‘ƒğ‘š) time.

The algorithm now proceeds in an analogous manner to the single machine case. First we partition the set of jobs J into ğ½1,â€¦,ğ½ğ·# according to the ğ·# different due dates ğ‘‘(1),â€¦,ğ‘‘(ğ·#), and we let ğ‘‹ğ‘–={ğ‘ğ‘—:ğ‘—âˆˆğ½ğ‘–} for each ğ‘–âˆˆ{1,â€¦,ğ·#}. This can be done in ğ‘‚(ğ‘›)=ğ‘‚(ğ‘ƒ) time. We then compute îˆ¿ğ‘š(ğ‘‹1),â€¦,îˆ¿ğ‘š(ğ‘‹ğ·#) in ğ‘‚Ìƒ (ğ‘ƒğ‘š) time using Lemma 10. Finally, we compute ğ‘†0,ğ‘†1,â€¦,ğ‘†ğ·#âŠ†{0,â€¦,ğ‘ƒ}ğ‘š starting from ğ‘†0=âˆ…, and then iteratively computing ğ‘†ğ‘– by ğ‘†ğ‘–=ğ‘†ğ‘–âˆ’1âŠ•îˆ¿ğ‘š(ğ‘‹), where elements in ğ‘†ğ‘– with a component strictly larger than ğ‘‘(ğ‘–) are discarded from future computations. The time complexity of this last final step is ğ‘‚Ìƒ (min{ğ‘ƒğ‘šâ‹…ğ·#,ğ·ğ‘š}), using a similar analysis to the one done in Sect. 3. This completes the proof of Theorem 7.

Discussion and Open Problems
In this paper we presented two algorithms for the 1||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— problem; the first running in ğ‘‚Ìƒ (ğ‘ƒ7/4) time, and the second running in ğ‘‚Ìƒ (min{ğ‘ƒâ‹…ğ·#,ğ‘ƒ+ğ·}) time. Both algorithms provide the first improvements over the classical Lawler and Moore algorithm in 50 years (which can also solve the more general 1||âˆ‘ğ‘¤ğ‘—ğ‘ˆğ‘—), and use more sophisticated tools such as polynomial multiplication and fast convolutions. Moreover, both algorithms are very easy to implement given a standard ready made FFT implementation for fast polynomial multiplication. Nevertheless, there are still a few ways which our results can be improved or extended:

Multiple machines: As we showed in Sect. 6, the SUMSETSCHEDULER algorithm can easily be extended to the multiple parallel machine case, giving us a total running time of ğ‘‚Ìƒ (min{ğ‘ƒğ‘šâ‹…ğ·#,ğ‘ƒğ‘š+ğ·ğ‘š}). We do not know how to obtain a similar extension for algorithm CONVSCHEDULER. In particular, there is no reason to believe that ğ‘ƒğ‘š||âˆ‘ğ‘ğ‘—ğ‘ˆğ‘— cannot be solved in ğ‘‚Ìƒ (ğ‘ƒğ‘š) time, or even better, either by extending algorithm CONVSCHEDULER or by a completely different approach.

Even faster skewed convolutions: We have no indication that our algorithm for (max,min)-Skewed-Convolution is the fastest possible. It would interesting to see whether one can improve its time complexity, say to ğ‘‚Ìƒ (ğ‘ƒ3/2). Naturally, any such improvement would directly improve Theorem 5. Conversely, one could try to obtain some sort of lower bound for the problem, possibly in the same vein as Theorem 2. Improving the time complexity beyond ğ‘‚Ìƒ (ğ‘ƒ3/2) seems difficult as this would directly imply an improvement to the (max,min)-Convolution problem. Indeed, let A, B be a given (max,min)-Convolution instance and construct vectors ğ´0, ğµ0 with ğ´0[ğ‘–]=ğ‘â‹…ğ´[ğ‘–] and ğµ0[ğ‘—]=ğ‘â‹…ğµ[ğ‘—] for ğ‘=2ğ‘›+1. If ğ¶0 is the (max,min)-Skewed-Convolution of ğ´0 and ğµ0 (that is, ğ¶0[ğ‘˜]=maxğ‘–+ğ‘—=ğ‘˜min{ğ´0[ğ‘–],ğµ0[ğ‘—]+ğ‘˜}), then the vector C with ğ¶[ğ‘˜]=âŒŠğ¶0[ğ‘˜]/ğ‘âŒ‹ is the (max,min)-Convolution of A and B.

Other scheduling problems: Can the techniques in this paper be applied to any other interesting scheduling problems? A good place to start might be to look at other problems which directly generalize Subset Sum.