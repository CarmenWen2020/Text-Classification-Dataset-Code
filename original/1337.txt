This paper is concerned with the 1||∑𝑝𝑗𝑈𝑗 problem, the problem of minimizing the total processing time of tardy jobs on a single machine. This is not only a fundamental scheduling problem, but also an important problem from a theoretical point of view as it generalizes the Subset Sum problem and is closely related to the 0/1-Knapsack problem. The problem is well-known to be NP-hard, but only in a weak sense, meaning it admits pseudo-polynomial time algorithms. The best known running time follows from the famous Lawler and Moore algorithm that solves a more general weighted version in 𝑂(𝑃⋅𝑛) time, where P is the total processing time of all n jobs in the input. This algorithm has been developed in the late 60s, and has yet to be improved to date. In this paper we develop two new algorithms for problem, each improving on Lawler and Moore’s algorithm in a different scenario.

Our first algorithm runs in 𝑂̃ (𝑃7/4) time, and outperforms Lawler and Moore’s algorithm in instances where 𝑛=𝜔̃ (𝑃3/4).

Our second algorithm runs in 𝑂̃ (min{𝑃⋅𝐷#,𝑃+𝐷}) time, where 𝐷# is the number of different due dates in the instance, and D is the sum of all different due dates. This algorithm improves on Lawler and Moore’s algorithm when 𝑛=𝜔̃ (𝐷#) or 𝑛=𝜔̃ (𝐷/𝑃). Further, it extends the known 𝑂̃ (𝑃) algorithm for the single due date special case of 1||∑𝑝𝑗𝑈𝑗 in a natural way.

Both algorithms rely on basic primitive operations between sets of integers and vectors of integers for the speedup in their running times. The second algorithm relies on fast polynomial multiplication as its main engine, and can be easily extended to the case of a fixed number of machines. For the first algorithm we define a new “skewed” version of (max,min)-Convolution which is interesting in its own right.

Access provided by University of Auckland Library

Introduction
In this paper we consider the problem of minimizing the total processing times of tardy jobs on a single machine. In this problem we are given a set of n jobs 𝐽={1,…,𝑛}, where each job j has a processing time 𝑝𝑗∈ℕ and a due date 𝑑𝑗∈ℕ. A schedule 𝜎 for J is a permutation 𝜎:{1,…,𝑛}→{1,…,𝑛}. In a given schedule 𝜎, the completion time 𝐶𝑗 of a job j under 𝜎 is given by 𝐶𝑗=∑𝜎(𝑖)≤𝜎(𝑗)𝑝𝑖, that is, the total processing time of jobs preceding j in 𝜎 (including j itself). Job j is tardy in 𝜎 if 𝐶𝑗>𝑑𝑗, and early otherwise. Our goal is find a schedule with minimum total processing time of tardy jobs. If we assign a binary indicator variable 𝑈𝑗 to each job j, where 𝑈𝑗=1 if j is tardy and otherwise 𝑈𝑗=0, our objective function can be written as ∑𝑝𝑗𝑈𝑗. In the standard three field notation for scheduling problems of Graham [5], this problem is denoted as the 1||∑𝑝𝑗𝑈𝑗 problem (the 1 in the first field indicates a single machine model, and the empty second field indicates there are no additional constraints).

The 1||∑𝑝𝑗𝑈𝑗 problem is a natural scheduling problem, which models a basic scheduling scenario. As it includes Subset Sum as a special case (see below), the 1||∑𝑝𝑗𝑈𝑗 problem is NP-hard. However, it is only hard in the weak sense, meaning it admits pseudo-polynomial time algorithms. The focus of this paper is on developing fast pseudo-polynomial time algorithms for 1||∑𝑝𝑗𝑈𝑗, improving in several settings on the best previously known solution from the late 60s. Before we describe our results, we discuss the previously known state of the art of the problem, and describe how our results fit into this line of research.

State of the Art
1||∑𝑝𝑗𝑈𝑗 is a special case of the famous 1||∑𝑤𝑗𝑈𝑗 problem. Here, each job j also has a weight 𝑤𝑗 in addition to its processing time 𝑝𝑗 and due date 𝑑𝑗, and the goal is to minimize the total weight (as opposed to total processing times) of tardy jobs. This problem has already been studied in the 60s, and even appeared in Karp’s fundamental paper from 1972 [6]. The classical algorithm of Lawler and Moore [10] for the problem is one of the earliest and most prominent examples of pseudo-polynomial algorithms, and it is to date the fastest known algorithm even for the special case of 1||∑𝑝𝑗𝑈𝑗. Letting 𝑃=∑𝑗∈𝐽𝑝𝑗, their result can be stated as follows:

Theorem 1
[10] 1||∑𝑤𝑗𝑈𝑗 (and hence also 1||∑𝑝𝑗𝑈𝑗) can be solved in 𝑂(𝑃⋅𝑛) time.

Note that as we assume that all processing times are integers, we have 𝑛≤𝑃, and so the running time of the algorithm in Theorem 1 can be bounded by 𝑂(𝑃2). In fact, it makes perfect sense to analyze the time complexity of a pseudo-polynomial time algorithm for either problems in terms of P, as P directly corresponds to the total input length when integers are encoded in unary. Observe that while the case of 𝑛=𝑃 (all jobs have unit processing times) essentially reduces to sorting, there are several non-trivial cases where n is smaller than P yet still quite significant in the 𝑂(𝑃⋅𝑛) term of Theorem 1. The question this paper addresses is:

“Can we obtain an 𝑂(𝑃2−𝜀) time algorithm for 1||∑𝑝𝑗𝑈𝑗, for any fixed 𝜀>0 ?”

For 1||∑𝑤𝑗𝑈𝑗 there is some evidence that the answer to the analogous question should be no. Karp [6] observed that the special case of the 1||∑𝑤𝑗𝑈𝑗 problem where all jobs have the same due date d, the 1|𝑑𝑗=𝑑|∑𝑤𝑗𝑈𝑗 problem, is essentially equivalent to the classical 0/1-Knapsack problem. Cygan et al. [4] and Künnemann et al. [9] studied the (min,+)-Convolution problem (see Sect. 2), and conjectured that the (min,+)-convolution between two vectors of length n cannot be computed in 𝑂̃ (𝑛2−𝜀) time, for any 𝜀>0. Under this (min,+)-Convolution Conjecture, they obtained lower bounds for several Knapsack related problems. In our terms, their result can be stated as follows:

Theorem 2
[4, 9] There is no 𝑂̃ (𝑃2−𝜀) time algorithm for the 1|𝑑𝑗=𝑑|∑𝑤𝑗𝑈𝑗 problem, for any 𝜀>0, unless the (min,+)-Convolution Conjecture is false. In particular, 1||∑𝑤𝑗𝑈𝑗 has no such algorithm under this conjecture.

Analogous to the situation with 1||∑𝑤𝑗𝑈𝑗, the special case of 1||∑𝑝𝑗𝑈𝑗 where all jobs have the same due date d (the 1|𝑑𝑗=𝑑|∑𝑝𝑗𝑈𝑗 problem) is equivalent to the classical Subset Sum problem. Recently, there has been significant improvements for Subset Sum resulting in algorithms with 𝑂̃ (𝑇+𝑛) running times [2, 7], where n is number of integers in the instance and T is the target. Due to the equivalence between the two problems, this yields the following result for the 1|𝑑𝑗=𝑑|∑𝑝𝑗𝑈𝑗 problem:

Theorem 3
[2, 7] 1|𝑑𝑗=𝑑|∑𝑝𝑗𝑈𝑗 can be solved in 𝑂̃ (𝑃) time.

On the other hand, due to the equivalence of 1|𝑑𝑗=𝑑|∑𝑝𝑗𝑈𝑗 and Subset Sum, we also know that Theorem 3 above cannot be significantly improved unless the Strong Exponential Time Hypothesis (SETH) fails. Specifically, combining a recent reduction from k-SAT to Subset Sum [1] with the equivalence of Subset Sum and 1|𝑑𝑗=𝑑|∑𝑝𝑗𝑈𝑗, yields the following:

Theorem 4
[1] There is no 𝑂̃ (𝑃1−𝜀) time algorithm for the 1|𝑑𝑗=𝑑|∑𝑝𝑗𝑈𝑗 problem, for any 𝜀>0, unless SETH fails.

Nevertheless, Theorem 4 still leaves quite a big gap for the true time complexity of 1||∑𝑝𝑗𝑈𝑗, as it can potentially be anywhere between the 𝑂̃ (𝑃) time known already for the special case of 1|𝑑𝑗=𝑑|∑𝑝𝑗𝑈𝑗 (Theorem 3), and the 𝑂(𝑃𝑛)=𝑂(𝑃2) time of Lawler and Moore’s algorithm (Theorem 1) for the 1||∑𝑤𝑗𝑈𝑗 problem. In particular, the 1||∑𝑝𝑗𝑈𝑗 and 1||∑𝑤𝑗𝑈𝑗 problems have not been distinguished from an algorithmic perspective so far. This is the starting point of our paper.

Our Results
The main contribution of this paper is two new pseudo-polynomial time algorithms for 1||∑𝑝𝑗𝑈𝑗, each improving on Lawler and Moore’s algorithm in a different sense. Our algorithms take a different approach to that of Lawler and Moore in that they rely on fast operators between sets and vectors of numbers.

Our first algorithm improves Theorem 1 in case there are sufficiently many jobs in the instance compared to the total processing time. More precisely, our algorithm has a running time of 𝑂̃ (𝑃7/4), and so it is faster than Lawler and Moore’s algorithm in case 𝑛=𝜔̃ (𝑃3/4).

Theorem 5
1||∑𝑝𝑗𝑈𝑗 can be solved in 𝑂̃ (𝑃7/4) time.

The algorithm in Theorem 5 uses a new kind of convolution which we coined “Skewed Convolution” and is interesting in its own right. In fact, one of the main technical contributions of this paper is a fast algorithm for the (max,min)-Skewed-Convolution problem (see definition in Sect. 2).

Our second algorithm for 1||∑𝑝𝑗𝑈𝑗 improves Theorem 1 in case there are not too many different due dates in the problem instance; that is, 𝐷#=|{𝑑𝑗:𝑗∈𝐽}| is relatively small when compared to n. This is actually a very natural assumption, for instance in cases where delivery costs are high and products are batched to only few shipments. Let D denote the sum of the different due dates in our instance. Then our second result can be stated as follows:

Theorem 6
1||∑𝑝𝑗𝑈𝑗 can be solved in 𝑂̃ (min{𝑃⋅𝐷#,𝑃+𝐷}) time.

The algorithm in Theorem 6 uses basic operations between sets of numbers, such as the sumset operation (see Sect. 2) as basic primitives for its computation, and ultimately relies on fast polynomial multiplication for its speedup. It should be noted that Theorem 6 includes the 𝑂̃ (𝑃) result of Theorem 3 for 1|𝑑𝑗=𝑑|∑𝑝𝑗𝑈𝑗 as a special case where 𝐷#=1 or 𝐷=𝑑. However, when measuring only in terms of n and D, the running times of the algorithms in [2, 7] for the single due date case are 𝑂̃ (𝐷+𝑛), which can be significantly faster than 𝑂̃ (𝑃).

As a final result we show that the algorithm used in Theorem 6 can be easily extended to the case where we have a fixed number 𝑚≥1 of parallel machines at our disposal. This problem is known as 𝑃𝑚||∑𝑝𝑗𝑈𝑗 in the literature. We show that the complexity of Theorem 6 scales naturally in m for this generalization. This should be compared with the 𝑂(𝑃𝑚𝑛) running time of Lawler and Moore’s algorithm.

Theorem 7
𝑃𝑚||∑𝑝𝑗𝑈𝑗 can be solved in 𝑂̃ (min{𝑃𝑚⋅𝐷#,𝑃𝑚+𝐷𝑚}) time.

Roadmap
The paper is organized as follows. In Sect. 2 we discuss all the basic primitives that are used by our algorithms, including some basic properties that are essential for the algorithms. We then present our second algorithm in Sect. 3, followed by our first algorithm in Sect. 4. Section 5 describes our fast algorithm for the skewed version of (max,min)-convolution, and is the main technical part of the paper. In Sect. 6 we consider the case of multiple machines and prove Theorem 7, and we conclude with some remarks and open problems in Sect. 7.

Preliminaries
In the following we discuss the basic primitives and binary operators between sets/vectors of integers that will be used in our algorithms. In general, we will use the letters X and Y to denote sets of non-negative integers (where order is irrelevant), and the letters A and B to denote vectors of non-negative integers.

Sumsets: The most basic operation used in our algorithms is computing the sumset of two sets of non-negative integers:

Definition 1
(Sumsets) Given two sets of non-negative integers 𝑋1 and 𝑋2, the sumset of 𝑋1 and 𝑋2, denoted 𝑋1⊕𝑋2, is defined by

𝑋1⊕𝑋2={𝑥1+𝑥2:𝑥1∈𝑋1,𝑥2∈𝑋2}.
Clearly, the sumset 𝑋1⊕𝑋2 can be computed in 𝑂(|𝑋1|⋅|𝑋2|) time. However, in certain cases we can do better using fast polynomial multiplication. Consider the two polynomials 𝑝1[𝛼]=∑𝑥∈𝑋1𝛼𝑥 and 𝑝2[𝛽]=∑𝑥∈𝑋2𝛽𝑥. Then the exponents of all terms in 𝑝1⋅𝑝2 with non-zero coefficients correspond to elements in the sumset 𝑋1⊕𝑋2. Since multiplying two polynomials of maximum degree d can be done in 𝑂(𝑑log𝑑) time [3], we have the following:

Lemma 1
Given two sets of non-negative integers 𝑋1,𝑋2⊆{0,…,𝑃}, one can compute the sumset 𝑋1⊕𝑋2 in 𝑂(𝑃log𝑃) time.

Set of all Subset Sums: Given set of non-negative integers X, we will frequently be using the set of all sums generated by subsets of X:

Definition 2
(Subset Sums) For a given set of non-negative integers X, define the set of all subset sums (𝑋) as the set of integers given by

(𝑋)={∑𝑥∈𝑌𝑥:𝑌⊆𝑋}.
Here, we always assume that 0∈(𝑋) (as it is the sum of the empty set).

We can use Lemma 1 above to compute (𝑋) from X rather efficiently: First, split X into two sets 𝑋1 and 𝑋2 of roughly equal size. Then recursively compute (𝑋1) and (𝑋2). Finally, compute (𝑋)=(𝑋1)⊕(𝑋2) via Lemma 1. The entire algorithm runs in 𝑂̃ (∑𝑥∈𝑋𝑥) time.

Lemma 2
([7]) Given a set of non-negative integers X, with 𝑃=∑𝑥∈𝑋𝑥, one can compute (𝑋) in 𝑂̃ (𝑃) time.

Convolutions: Given two vectors 𝐴=(𝐴[𝑖])𝑛𝑖=0, 𝐵=(𝐵[𝑗])𝑛𝑗=0, the (∘,∙)-Convolution problem for binary operators ∘ and ∙ is to compute a vector 𝐶=(𝐶[𝑘])2𝑛𝑘=0 with

𝐶[𝑘]=◯𝑖+𝑗=𝑘𝐴[𝑖]∙𝐵[𝑗].
Throughout this paper we assume that A, B (and C) are integer vectors with entries bounded by 𝑛𝑂(1), and with the exceptional values +∞ and −∞. A prominent example of a convolution problem is (min,+)-Convolution discussed above; another similarly prominent example is (max,min)-Convolution which can be solved in 𝑂̃ (𝑛3/2) time [8]. For our purposes, it is convenient to look at a skewed variant of this problem:

Definition 3
(Skewed Convolution) Given two vectors 𝐴=(𝐴[𝑖])𝑛𝑖=0, 𝐵=(𝐵[𝑗])𝑛𝑗=0, we define the (max,min)-Skewed-Convolution problem to be the problem of computing the vector 𝐶=(𝐶[𝑘])2𝑛𝑘=0 where the kth entry in C equals

𝐶[𝑘]=max𝑖+𝑗=𝑘min{𝐴[𝑖],𝐵[𝑗]+𝑘}
for each 𝑘∈{0,…,2𝑛}.

The main technical result of this paper is an algorithm for (max,min)-Skewed-Convolution that is significantly faster than the naive 𝑂(𝑛2) time algorithm.

Theorem 8
The (max,min)-Skewed-Convolution problem can be solved in 𝑂̃ (𝑛7/4) time.

Algorithm via Sumsets and Subset Sums
In the following section, we provide a proof of Theorem 6 by presenting an algorithm for 1||∑𝑝𝑗𝑈𝑗 running in 𝑂̃ (min{𝑃⋅𝐷#,𝑃+𝐷}) time. Recall that 𝐽={1,…,𝑛} denotes our input set of jobs, and 𝑝𝑗 and 𝑑𝑗 respectively denote the processing time and due date of job 𝑗∈{1,…,𝑛}. Our goal is to determine the minimum total processing time of tardy jobs in any schedule for J. Throughout the section we let 𝑑(1)<⋯<𝑑(𝐷#) denote the 𝐷#≤𝑛 different due dates of the jobs in J.

A key observation for the 1||∑𝑝𝑗𝑈𝑗 problem, used already by Lawler and Moore, is that any instance of the problem always has an optimal schedule of a specific type, namely an Earliest Due Date schedule. An Earliest Due Date (EDD) schedule is a schedule 𝜋:𝐽→{1,…,𝑛} such that

any early job precedes all late jobs in 𝜋, and

any early job precedes all early jobs with later due dates.

In other words, in an EDD schedule all early jobs are scheduled before all tardy jobs, and all early jobs are scheduled in non-decreasing order of due dates.

Lemma 3
([10]) Any 1||∑𝑝𝑗𝑈𝑗 instance has an optimal schedule which is EDD.

The 𝐷#-many due dates in our instance partition the input set of job J in a natural manner: Define 𝐽𝑖={𝑗:𝑑𝑗=𝑑(𝑖)} for each 𝑖∈{1,…,𝐷#}. Furthermore, let 𝑋𝑖={𝑝𝑗:𝑗∈𝐽𝑖} the processing-times of job in 𝐽𝑖. According to Lemma 3 above, we can restrict our attention to EDD schedules. Constructing such a schedule corresponds to choosing a subset 𝐸𝑖⊆𝐽𝑖 for each due date 𝑑(𝑖) such that ∑𝑗∈𝐸ℓ,ℓ≤𝑖𝑝𝑗≤𝑑(𝑖) holds for each 𝑖∈{1,…,𝐷#}. Moreover, the optimal EDD schedule maximizes the total sum of processing times in all selected 𝐸𝑖’s.

Our algorithm is given in Algorithm 1. It successively computes sets 𝑆1,…,𝑆𝐷#, where set 𝑆𝑖 corresponds to the set of jobs 𝐽1∪⋯∪𝐽𝑖. In particular, 𝑆𝑖 includes the total processing-time of any possible set-family of early jobs {𝐸1,…,𝐸𝑖}. Thus, each 𝑥∈𝑆𝑖 corresponds to the total processing time of early jobs in a subset of 𝐽1∪⋯∪𝐽𝑖. The maximum value 𝑥∈𝑆𝐷# therefore corresponds to the maximum total processing time of early jobs in any schedule for J. Thus, the algorithm terminates by returning the optimal total weight of tardy jobs 𝑃−𝑥.

figure a
Correctness of our algorithm follows immediately from the definitions of sumsets and subset sums, and from the fact that we prune out elements 𝑥∈𝑆𝑖 with 𝑥>𝑑(𝑖) at each step of the algorithm. This is stated more formally in the lemma below.

Lemma 4
Let 𝑖∈{1,…,𝐷#}, and let 𝑆𝑖 be the set of integers at the end of the second step of 5(i). Then 𝑥∈𝑆𝑖 if and only if there are sets of jobs 𝐸1⊆𝐽1,…,𝐸𝑖⊆𝐽𝑖 such that

∑𝑗∈⋃𝑖ℓ=1𝐸ℓ𝑝𝑗=𝑥, and

∑𝑗∈⋃𝑖0ℓ=1𝐸ℓ𝑝𝑗≤𝑑(𝑖0) holds for each 𝑖0∈{1,…,𝑖}.

Proof
The proof is by induction on i. For 𝑖=1, note that 𝑆1=(𝑋1)∖{𝑥:𝑥>𝑑(1)} at the end of step 5(1). Since (𝑋1) includes the total processing time of any subset of jobs in 𝐽1, the first condition of the lemma holds. Since {𝑥:𝑥>𝑑(1)} includes all integers violating the second condition of the lemma, the second condition holds.

Let 𝑖>1, and assume the lemma holds for 𝑖−1. Consider some 𝑥∈𝑆𝑖 at the end of the second step of 5(i). Then by Definition 1, we have 𝑥=𝑥1+𝑥2 for some 𝑥1∈𝑆𝑖−1 and 𝑥2∈(𝑋𝑖) due the first step of 5(i). By definition of (𝑋𝑖), there is some 𝐸𝑖⊆𝐽𝑖 with total processing time 𝑥2. By our inductive hypothesis there is 𝐸1⊆𝐽1,…,𝐸𝑖−1⊆𝐽𝑖−1 such that ∑𝑗∈⋃𝑖ℓ=1𝐸ℓ𝑝𝑗=𝑥1, and ∑𝑗∈𝐸ℓ,ℓ≤𝑖0𝑝𝑗≤𝑑(𝑖0) holds for each 𝑖0∈{1,…,𝑖−1}. Furthermore, by the second step of 5(i), we know that ∑𝑗∈𝐸ℓ,ℓ≤𝑖𝑝𝑗=𝑥≤𝑑(𝑖). Thus, 𝐸1,…,𝐸𝑖 satisfy both conditions of the lemma. ◻

Let us next analyze the time complexity of the SUMSETSCHEDULER algorithm. Steps 1 and 2 can be both performed in 𝑂̃ (𝑛)=𝑂̃ (𝑃) time. Next observe that step 3 can be done in total 𝑂̃ (𝑃) time using Lemma 2, as 𝑋2,…,𝑋𝐷# is a partition of the set of all processing times of J, and these all sum up to P. Next, according to Lemma 1, each sumset operation at step 5 can be done in time proportional to the largest element in the two sets, which is always at most P. Thus, since we perform at most 𝐷# sumset operations, the merging step requires 𝑂̃ (𝐷#⋅𝑃) time, which gives us the total running time of the algorithm above.

Another way to analyze the running time of SUMSETSCHEDULER is to observe that the maximum element participating in the ith sumset is bounded by 𝑑(𝑖+1). It follows that we can write the running time of the merging step as 𝑂̃ (𝐷), where 𝐷=∑𝐷#𝑖=1𝑑(𝑖). Thus, we have just shown that 1||∑𝑝𝑗𝑈𝑗 can be solved in 𝑂̃ (min{𝐷#⋅𝑃,𝐷+𝑃}) time, completing the proof of Theorem 6.

Algorithm via Fast Skewed Convolutions
We next present our 𝑂̃ (𝑃7/4) time algorithm for 1||∑𝑝𝑗𝑈𝑗, providing a proof of Theorem 5. As in the previous section, we let 𝑑(1)<⋯<𝑑(𝐷#) denote the 𝐷#≤𝑛 different due dates of the input jobs J, and 𝐽𝑖={𝑗:𝑑𝑗=𝑑(𝑖)} and 𝑋𝑖={𝑝𝑗:𝑗∈𝐽𝑖} as in Sect. 3 for each 𝑖∈{1,…,𝐷#}.

For a consecutive subset of indices 𝐼={𝑖0,𝑖0+1,…,𝑖1}, with 𝑖0,…,𝑖1∈{1,…,𝐷#}, we define a vector M(I), where M(I)[x] equals the latest (that is, maximum) time point 𝑥0 for which there is a subset of the jobs in ⋃𝑖∈𝐼𝐽𝑖 with total processing time equal to x that can all be scheduled early in an EDD schedule starting at 𝑥0. If no such subset of jobs exists, we define 𝑀(𝐼)[𝑥]=−∞.

For a singleton set 𝐼={𝑖}, the vector M(I) is easy to compute once we have computed the set (𝑋𝑖):

𝑀({𝑖})[𝑥]={𝑑(𝑖)−𝑥−∞ if 𝑥∈(𝑋𝑖) and 𝑥≤𝑑(𝑖), otherwise .
(1)
For larger sets of indices, we have the following lemma.

Lemma 5
Let 𝐼1={𝑖0,𝑖0+1,…,𝑖1} and 𝐼2={𝑖1+1,𝑖1+2,…,𝑖2} be any two sets of consecutive indices with 𝑖0,…,𝑖1,…,𝑖2∈{1,…,𝐷#}. Then for any value x we have:

𝑀(𝐼1∪𝐼2)[𝑥]=max𝑥1+𝑥2=𝑥min{𝑀(𝐼1)[𝑥1],𝑀(𝐼2)[𝑥2]−𝑥1}.
Proof
Let 𝐼=𝐼1∪𝐼2. Then M(I)[x] is the latest time point after which a subset of jobs 𝐽∗⊆⋃𝑖∈𝐼𝐽𝑖 of total processing time x can be scheduled early in an EDD schedule. Let 𝑥1 and 𝑥2 be the total processing times of jobs in 𝐽∗1=𝐽∗∩(⋃𝑖∈𝐼1𝐽𝑖) and 𝐽∗2=𝐽∗∩(⋃𝑖∈𝐼2𝐽𝑖), respectively. Then 𝑥=𝑥1+𝑥2. Clearly, 𝑀(𝐼)[𝑥]≤𝑀(𝐼1)[𝑥1], since we have to start scheduling the jobs in 𝐽∗1 at time 𝑀(𝐼1)[𝑥1] by latest. Similarly, it holds that 𝑀(𝐼)[𝑥]≤𝑀(𝐼2)[𝑥2]−𝑥1 since the jobs in 𝐽∗2 are scheduled at latest at 𝑀(𝐼2)[𝑥2] and the jobs in 𝐽∗1 have to be processed before that time point in an EDD schedule. In combination, we have shown that LHS ≤ RHS in the equation of the lemma.

To prove that LHS ≥ RHS, we construct a feasible schedule for jobs in ⋃𝑖∈𝐼𝐽𝑖 starting at RHS. Let 𝑥1 and 𝑥2 be the two values with 𝑥1+𝑥2=𝑥 that maximize RHS. Then there is a schedule which schedules some jobs 𝐽∗1⊆⋃𝑖∈𝐼1𝐽𝑖 of total processing time 𝑥1 beginning at time min{𝑀(𝐼1)[𝑥1],𝑀(𝐼2)[𝑥2]−𝑥1}≤𝑀(𝐼1)[𝑥1], followed by a another subset of jobs 𝐽∗2⊆⋃𝑖∈𝐼2𝐽𝑖 of total processing time 𝑥2 starting at time min{𝑀(𝐼1)[𝑥1],𝑀(𝐼2)[𝑥2]−𝑥1}+𝑥1≤𝑀(𝐼2)[𝑥2]. This is a feasible schedule starting at time RHS for a subset of jobs in ⋃𝑖∈𝐼𝐽𝑖 which has total processing time x. ◻

Note that the equation given in Lemma 5 is close but not precisely the equation defined in Definition 3 for the (min,max)-Skewed-Convolution problem. Nevertheless, the next lemma shows that we can easily translate between these two concepts.

Lemma 6
Let A and B be two integer vectors of P entries each. Given an algorithm for computing the (max,min)-Skewed-Convolution of A and B in T(P) time, we can compute in 𝑇(𝑃)+𝑂(𝑃) time the vector 𝐶=𝐴⊗𝐵 defined by

𝐶[𝑥]=max𝑥1+𝑥2=𝑥min{𝐴[𝑥1],𝐵[𝑥2]−𝑥1}.
Proof
Given A and B, construct two auxiliary vectors 𝐴0 and 𝐵0 defined by 𝐴0[𝑥]=𝐵[𝑥]+𝑥 and 𝐵0[𝑥]=𝐴[𝑥] for each entry x. Compute the (max,min)-Skewed-Convolution of 𝐴0 and 𝐵0, and let 𝐶0 denote the resulting vector. We claim that the vector C defined by 𝐶[𝑥]=𝐶0[𝑥]−𝑥 equals 𝐴⊗𝐵. Indeed, we have

𝐶0[𝑥]−𝑥=====max𝑥1+𝑥2=𝑥min{𝐴0[𝑥1],𝐵0[𝑥2]+𝑥}−𝑥max𝑥1+𝑥2=𝑥min{𝐴0[𝑥1]−𝑥,𝐵0[𝑥2]}max𝑥1+𝑥2=𝑥min{𝐵[𝑥1]+𝑥1−𝑥,𝐴[𝑥2]}max𝑥1+𝑥2=𝑥min{𝐵[𝑥1]−𝑥2,𝐴[𝑥2]}max𝑥1+𝑥2=𝑥min{𝐴[𝑥1],𝐵[𝑥2]−𝑥1},
where in the third step we expanded the definition of 𝐴0 and 𝐵0, and in the last step we used the symmetry of 𝑥1 and 𝑥2. ◻

We are now in position to describe our algorithm called CONVSCHEDULER which is depicted in Algorithm 2. The algorithm first computes the subset sums (𝑋1),…,(𝑋𝐷#), and the set of vectors ={𝑀1,…,𝑀𝐷#}. Following this, it iteratively combines every two consecutive vectors in  by using the ⊗ operation. The algorithm terminates when ={𝑀1}, where at this stage 𝑀1 corresponds to the entire set of input jobs J. It then returns 𝑃−𝑥, where x is the maximum value with 𝑀1[𝑥]>−∞; by definition, this corresponds to a schedule for J with 𝑃−𝑥 total processing time of tardy jobs. For convenience of presentation, we assume that 𝐷# is a power of 2.

figure b
Correctness of this algorithm follows directly from Lemma 5. To analyze its time complexity, observe that steps 1–4 can be done in 𝑂̃ (𝑃) time (using Lemma 2). Step 5 is performed 𝑂(log𝐷#)=𝑂(log𝑃) times, and each step requires a total of 𝑂̃ (𝑃7/4) time according to Theorem 8, as the total sizes of all vectors at each step is O(P). Finally, step 6 requires O(P) time. Summing up, this gives us a total running time of 𝑂̃ (𝑃7/4), and completes the proof of Theorem 5 (apart from the proof of Theorem 8).

Fast Skewed Convolutions
In the following section we present our algorithm for (max,min)-Skewed-Convolution, and provide a proof for Theorem 8. Let 𝐴=(𝐴[𝑖])𝑛𝑖=0 and 𝐵=(𝐵[𝑗])𝑛𝑗=0 denote the input vectors for the problem throughout the section. Recall we wish the compute the vector 𝐶ℓ=(𝐶[𝑘])2𝑛𝑘=0 where

𝐶[𝑘]=max𝑖+𝑗=𝑘min{𝐴[𝑖],𝐵[𝑗]+𝑘}
for each 𝑘∈{0,…,2𝑛}.

We begin by first defining the problem slightly more generally, in order to facilitate our recursive strategy later on. For this, for each integer ℓ∈{0,…,log2𝑛}, let 𝐴ℓ=⌊𝐴/2ℓ⌋ and 𝐵ℓ=⌊𝐵/2ℓ⌋, where rounding is done component-wise. We will compute vectors 𝐶ℓ=(𝐶ℓ[𝑘])2𝑛𝑘=0 defined by:

𝐶ℓ[𝑘]=max𝑖+𝑗=𝑘min{𝐴ℓ[𝑖],𝐵ℓ[𝑗]+⌊𝑘/2ℓ⌋}.
Observe that a solution for ℓ=0 yields a solution to the original (max,min)-Skewed-Convolution problem, and for ℓ≥log(2𝑛) the problem degenerates to (max,min)-Convolution.

We next define a particular kind of additive approximation of vectors 𝐶ℓ. We say that a vector 𝐷ℓ is a good approximation of 𝐶ℓ if 𝐶ℓ[𝑘]−2≤𝐷ℓ[𝑘]≤𝐶ℓ[𝑘] for each 𝑘∈{0,…,2𝑛}. Now, the main technical part of our algorithm is encapsulated in the following lemma.

Lemma 7
There is an algorithm that computes 𝐶ℓ in 𝑂̃ (𝑛7/4) time, given 𝐴ℓ, 𝐵ℓ, and a good approximation 𝐷ℓ of 𝐶ℓ.

We postpone the proof of Lemma 7 for now, and instead show that it directly yields our desired algorithm for (max,min)-Skewed-Convolution:

Proof of Theorem 8
In order to compute 𝐶=𝐶0, we perform an (inverse) induction on ℓ: As mentioned before, if ℓ≥log(2𝑛), then we can neglect the “+ ⌊𝑘/2ℓ⌋” term and compute 𝐶ℓ in 𝑂̃ (𝑛3/2)=𝑂̃ (𝑛7/4) time using a single (max,min)-Convolution computation [8].

For the inductive step, let ℓ<log(2𝑛) and assume that we have already computed 𝐶ℓ+1. We construct the vector 𝐷ℓ=2𝐶ℓ+1, and argue that it is a good approximation of 𝐶ℓ. Indeed, for each entry k, on the one hand, we have:

𝐷ℓ[𝑘]=≤2𝐶ℓ+1[𝑘]=2⋅max𝑖+𝑗=𝑘min{⌊𝐴ℓ[𝑖]/2⌋,⌊𝐵ℓ[𝑗]/2⌋+⌊𝑘/2ℓ+1⌋}max𝑖+𝑗=𝑘min{𝐴ℓ[𝑖],𝐵ℓ[𝑗]+⌊𝑘/2ℓ⌋}=𝐶ℓ[𝑘];
and on the other hand, we have:

𝐷ℓ[𝑘]=≥2𝐶ℓ+1[𝑘]=2⋅max𝑖+𝑗=𝑘min{⌊𝐴ℓ[𝑖]/2⌋,⌊𝐵ℓ[𝑗]/2⌋+⌊𝑘/2ℓ+1⌋}max𝑖+𝑗=𝑘min{𝐴ℓ[𝑖]−1,𝐵ℓ[𝑗]+⌊𝑘/2ℓ⌋−2}≥𝐶ℓ[𝑘]−2.
Thus, using 𝐷ℓ we can apply Lemma 7 above to obtain 𝐶ℓ in 𝑂̃ (𝑛7/4) time. Since there are 𝑂(log𝑛) inductive steps overall, this is also the overall time complexity of the algorithm. ◻

It remains to prove Lemma 7. Recall that we are given 𝐴ℓ, 𝐵ℓ, and 𝐷ℓ, and our goal is to compute the vector 𝐶ℓ in 𝑂̃ (𝑛7/4) time. We construct two vectors 𝐿ℓ and 𝑅ℓ with 2n entries each, defined by

𝐿ℓ[𝑘]=max{𝐴ℓ[𝑖0]:𝐴ℓ[𝑖0]≤𝐵ℓ[𝑘−𝑖0]+⌊𝑘/2ℓ⌋ and𝐷ℓ[𝑘]≤𝐴ℓ[𝑖0]≤𝐷ℓ[𝑘]+2},
and

𝑅ℓ[𝑘]=max{𝐵ℓ[𝑗0]+⌊𝑘/2ℓ⌋:𝐵ℓ[𝑗0]+⌊𝑘/2ℓ⌋≤𝐴ℓ[𝑘−𝑗0] and𝐷ℓ[𝑘]≤𝐵ℓ[𝑗0]+⌊𝑘/2ℓ⌋≤𝐷ℓ[𝑘]+2}
for 𝑘∈{0,…,2𝑛}. That is, 𝐿ℓ[𝑘] and 𝑅ℓ[𝑘] respectively capture the largest value attained as the left-hand side or right-hand side of the inner min-operation in 𝐶ℓ[𝑘], as long as that value lies in the feasible region approximated by 𝐷ℓ[𝑘]. Since 𝐷ℓ is a good approximation, the following lemma is immediate from the definitions:

Lemma 8
𝐶ℓ[𝑘]=max{𝐿ℓ[𝑘],𝑅ℓ[𝑘]} for each 𝑘∈{0,…,2𝑛}.

According to Lemma 8, it suffices to compute 𝐿ℓ and 𝑅ℓ. We first focus on computing 𝐿ℓ. The computation of 𝑅ℓ is very similar and we will later point out the necessary changes.

Let 0<𝛿<1 be a fixed constant to be determined later. We say that an index 𝑘∈{0,…,2𝑛} is light if

|{𝑖:𝐷ℓ[𝑘]≤𝐴ℓ[𝑖]≤𝐷ℓ[𝑘]+2}|≤𝑛𝛿.
Informally, k is light if the number of candidate entries 𝐴ℓ[𝑖] which can equal 𝐶ℓ[𝑘] is relatively small (recall that 𝐷ℓ[𝑘]≤𝐶ℓ[𝑘]≤𝐷ℓ[𝑘]+2, as 𝐷ℓ is a good approximation of 𝐶ℓ). If k is not light then we say that it is heavy.

Our algorithm for computing 𝐿ℓ proceeds in three main steps: In the first step it handles all light indices, in the second step it sparsifies the input vector, and in the third step it handles all heavy indices:

Light indices: We begin by iterating over all light indices 𝑘∈{0,…,2𝑛}. For each light index k, we iterate over all entries 𝐴ℓ[𝑖] satisfying 𝐷ℓ[𝑘]≤𝐴ℓ[𝑖]≤𝐷ℓ[𝑘]+2, and set 𝐿ℓ[𝑘] to be the maximum 𝐴ℓ[𝑖] among those entries with 𝐴ℓ[𝑖]≤𝐵ℓ[𝑘−𝑖]+⌊𝑘/2ℓ⌋. Note that after this step, we have

𝐿ℓ[𝑘]=max{𝐴ℓ[𝑖0]:𝐴ℓ[𝑖0]≤𝐵ℓ[𝑘−𝑖0]+⌊𝑘/2ℓ⌋ and 𝐷ℓ[𝑘]≤𝐴ℓ[𝑖0]≤𝐷ℓ[𝑘]+2},
for each light index k.

Sparsification step: After dealing with the light indices, several entries of 𝐴ℓ become redundant. Consider an entry 𝐴ℓ[𝑖] for which |{𝑖0:𝐴ℓ[𝑖]−2≤𝐴ℓ[𝑖0]≤𝐴ℓ[𝑖]+2}|≤𝑛𝛿. Then all indices k for which 𝐿ℓ[𝑘] might equal 𝐴ℓ[𝑖] must be light, and are therefore already dealt with in the previous step. Consequently, it is safe to replace 𝐴ℓ[𝑖] by −∞ so that 𝐴ℓ[𝑖] no longer plays a role in the remaining computation.

Heavy indices: After the sparsification step 𝐴ℓ contains few distinct values. Thus, our approach is to fix any such value v and detect whether 𝐿ℓ[𝑘]≥𝑣. To that end, we translate the problem into an instance of (max,min)-Convolution: Let (𝐴ℓ𝑣[𝑖])𝑛𝑖=0 be an be an indicator-like vector defined by 𝐴ℓ𝑣[𝑖]=+∞ if 𝐴ℓ[𝑖]=𝑣, and otherwise 𝐴ℓ𝑣[𝑖]=−∞. We next compute the vector 𝐿ℓ𝑣 defined by 𝐿ℓ𝑣[𝑘]=⌊𝑘/2ℓ⌋+max𝑖+𝑗=𝑘min{𝐴ℓ𝑣[𝑖],𝐵ℓ[𝑗]} using a single computation of (max,min)-Convolution. We choose

𝐿ℓ[𝑘]=max{𝑣:𝐿ℓ𝑣[𝑘]≥𝑣 and 𝐷ℓ[𝑘]≤𝑣≤𝐷ℓ[𝑘]+2}
for any heavy index k, and claim that 𝐿ℓ[𝑘] equals max{𝐴ℓ[𝑖0]:𝐴ℓ[𝑖0]≤𝐵ℓ[𝑘−𝑖0]+⌊𝑘/2ℓ⌋}.

On the one hand, if 𝐿ℓ𝑣[𝑘]≥𝑣 then there are indices i and j with 𝑖+𝑗=𝑘 for which 𝐴ℓ[𝑖]=𝑣 and 𝐵ℓ[𝑗]+⌊𝑘/2ℓ⌋≥𝐴ℓ[𝑖]=𝑣. Thus, the computed value 𝐿ℓ[𝑘] is not greater than

𝐿ℓ[𝑘]≤max{𝐴ℓ[𝑖0]:𝐴ℓ[𝑖0]≤𝐵ℓ[𝑘−𝑖0]+⌊𝑘/2ℓ⌋ and 𝐷ℓ[𝑘]≤𝐴ℓ[𝑖0]≤𝐷ℓ[𝑘]+2}.
On the other hand, for all values v for which 𝐴ℓ[𝑖]=𝑣 for some 𝑖∈{0,…,𝑛}, we have if 𝑣=𝐴ℓ[𝑖]≤𝐵ℓ[𝑘−𝑖]+⌊𝑘/2ℓ⌋ then 𝐴ℓ𝑣[𝑖]=−∞, which in turn implies that 𝐴ℓ𝑣[𝑖]≥𝐵ℓ[𝑘−𝑖]+⌊𝑘/2ℓ⌋≥𝐴ℓ[𝑖]=𝑣. Thus, our selection of 𝐿ℓ[𝑘] is also at least as large as

𝐿ℓ[𝑘]≥max{𝐴ℓ[𝑖0]:𝐴ℓ[𝑖0]≤𝐵ℓ[𝑘−𝑖0]+⌊𝑘/2ℓ⌋ and 𝐷ℓ[𝑘]≤𝐴ℓ[𝑖0]≤𝐷ℓ[𝑘]+2},
and hence, these two values must be equal.

We finally argue how to adapt the approach to compute 𝑅ℓ. In the first step, we instead classify an index k as light if |{𝑖:𝐷ℓ[𝑘]≤𝐵ℓ[𝑗]+⌊𝑘/2ℓ⌋≤𝐷ℓ[𝑘]+2}|≤𝑛𝛿. In the same way as before we can compute 𝑅ℓ[𝑘] for all light indices k, as well as apply the sparsification step to replace all entries 𝐵ℓ[𝑗] which satisfy |{𝑗0:𝐵ℓ[𝑗]−2≤𝐵ℓ[𝑗0]≤𝐵ℓ[𝑗]+2}|≤𝑛𝛿 by −∞. After the sparsification, the vector 𝐵ℓ contains only few distinct values, and for any such value v we proceed similar to before. Defining 𝐵ℓ𝑣 analogously, we compute 𝑅ℓ𝑣[𝑘]=max𝑖+𝑗=𝑘min{𝐴ℓ[𝑖],𝐵ℓ𝑣[𝑗]} and return

𝑅ℓ[𝑘]=max{𝑣+⌊𝑘/2ℓ⌋:𝑅ℓ𝑣[𝑘]≥𝑣+⌊𝑘/2ℓ⌋ and 𝐷ℓ[𝑘]≤𝑣+⌊𝑘/2ℓ⌋≤𝐷ℓ[𝑘]+2}
for all heavy indices k. One can verify that this choice of 𝑅ℓ[𝑘] is correct with exactly the same proof as before.

This completes the description of our algorithm. As we argued its correctness above, what remains is to analyze its time complexity. Note that we can determine in 𝑂(log𝑛) time whether an index k is light or heavy, by first sorting the values in 𝐴ℓ. For each light index k, determining 𝐿ℓ[𝑘] can be done in 𝑂(𝑛𝛿) time (on the sorted 𝐴ℓ), giving us a total of 𝑂̃ (𝑛1+𝛿) time for the first step. For the second step, we can determine whether a given entry 𝐴ℓ[𝑖] can be replaced with −∞ in 𝑂(log𝑛) time, giving us a total of 𝑂̃ (𝑛) time for this step.

Consider then the final step of the algorithm. Observe that after exhausting the sparsification step, 𝐴ℓ contains at most 𝑂(𝑛1−𝛿) many distinct values: For any surviving value v, there is another (perhaps different) value 𝑣′ of difference at most 2 from v that occurs at least 1/5⋅𝑛𝛿 times in 𝐴ℓ, and so there can only be at most 𝑂(𝑛1−𝛿) such distinct values. Thus, the running time of this step is dominated by the running time of 𝑂(𝑛1−𝛿) (max,min)-Convolution computations, each requiring 𝑂̃ (𝑛3/2) time using the algorithm of [8], giving us a total of 𝑂̃ (𝑛5/2−𝛿) time for this step.

Thus, the running time of our algorithm is dominated by the 𝑂̃ (𝑛1+𝛿) running time of its first step, and the 𝑂̃ (𝑛5/2−𝛿) running time of its last step. Choosing 𝛿=3/4 gives us 𝑂̃ (𝑛7/4) time for both steps, which is the time promised by Lemma 7. Thus, Lemma 7 holds.

Multiple Machines
In the following we show how to extend the SUMSETSCHEDULER algorithm of Sect. 3 to the case of a fixed number m multiple machines, the 𝑃𝑚||∑𝑝𝑗𝑈𝑗 problem. In this variant, we have m machines at our disposal, and so a schedule 𝜎 for the set of jobs J is now a function 𝜎:{1,…,𝑛}→{1,…,𝑚}×{1,…,𝑛}. The first component of 𝜎(𝑗) specifies the machine one which j is scheduled, and the second component specifies its order on the machine. The completion time 𝐶𝑗 of j is then the sum of all jobs preceding j (including j itself) on the same machine that j is scheduled. Our goal remains to minimize the total processing time of tardy jobs ∑𝑝𝑗𝑈𝑗.

For extending algorithm SUMSETSCHEDULER, we need to first extend Definition 2 to the case of multiple machines.

Definition 4
For a given set of non-negative integers X, define the set 𝑚(𝑋) as the set of m-tuples given by

𝑚(𝑋)={(∑𝑥∈𝑌1𝑥,…,∑𝑥∈𝑌𝑚𝑥):𝑌1,…,𝑌𝑚⊆𝑋 and 𝑌𝑖∩𝑌𝑗=∅ for  each 𝑖,𝑗∈{1,…,𝑚},𝑖≠𝑗}.
Thus, every element in 𝑚(𝑋) is an m-tuple 𝐱=(𝑥1,…,𝑥𝑚) of non-negative integers, where we interpret 𝑥𝑖 as the total processing time on machine 𝑖∈{1,…,𝑚}. We consider component-wise addition between two m-tuples, and define the sumset 𝑋1⊕𝑋2 of two sets of m-tuples as in Definition 1; i.e., 𝑋1⊕𝑋2={𝐱1+𝐱2:𝐱1∈𝑋1,𝐱2∈𝑋2}.

To efficiently compute the sumset 𝑋1⊕𝑋2 when 𝑋1 and 𝑋2 are sets of m-tuples we use multivariate polynomial multiplication. Let 𝑝1[𝛼1,…,𝛼𝑚]=∑(𝑥1,…,𝑥𝑚)∈𝑋1𝛱𝑚𝑖=1𝛼𝑥𝑖𝑖 and 𝑝2[𝛽1,…,𝛽𝑚]=∑(𝑥1,…,𝑥𝑚)∈𝑋2𝛱𝑚𝑖=1𝛽𝑥𝑖𝑖. Then the exponents of all terms in 𝑝1⋅𝑝2 with non-zero coefficients correspond to elements in the sumset 𝑋1⊕𝑋2. Since multiplying two m-variate polynomials of maximum degree d on each variable can be reduced to multiplying two univariate polynomials of maximum degree 𝑂(𝑑𝑚) using Kronecker’s map (see e.g. [11]), we obtain the following:

Lemma 9
Given two sets of m-tuples of non-negative integers 𝑋1,𝑋2⊆{0,…,𝑃}𝑚, one can compute the sumset 𝑋1⊕𝑋2 in 𝑂(𝑃𝑚log𝑃) time.

Using the same divide and conquer approach used for Lemma 2, we can use Lemma 9 above to compute 𝑚(𝑋) from X. The same analysis used for Lemma 2 will give us a running time of 𝑂(𝑃𝑚log𝑃) instead of 𝑂(𝑃log𝑃).

Lemma 10
Given a set of non-negative integers X, with 𝑃=∑𝑥∈𝑋𝑥, one can compute 𝑚(𝑋) in 𝑂̃ (𝑃𝑚) time.

The algorithm now proceeds in an analogous manner to the single machine case. First we partition the set of jobs J into 𝐽1,…,𝐽𝐷# according to the 𝐷# different due dates 𝑑(1),…,𝑑(𝐷#), and we let 𝑋𝑖={𝑝𝑗:𝑗∈𝐽𝑖} for each 𝑖∈{1,…,𝐷#}. This can be done in 𝑂(𝑛)=𝑂(𝑃) time. We then compute 𝑚(𝑋1),…,𝑚(𝑋𝐷#) in 𝑂̃ (𝑃𝑚) time using Lemma 10. Finally, we compute 𝑆0,𝑆1,…,𝑆𝐷#⊆{0,…,𝑃}𝑚 starting from 𝑆0=∅, and then iteratively computing 𝑆𝑖 by 𝑆𝑖=𝑆𝑖−1⊕𝑚(𝑋), where elements in 𝑆𝑖 with a component strictly larger than 𝑑(𝑖) are discarded from future computations. The time complexity of this last final step is 𝑂̃ (min{𝑃𝑚⋅𝐷#,𝐷𝑚}), using a similar analysis to the one done in Sect. 3. This completes the proof of Theorem 7.

Discussion and Open Problems
In this paper we presented two algorithms for the 1||∑𝑝𝑗𝑈𝑗 problem; the first running in 𝑂̃ (𝑃7/4) time, and the second running in 𝑂̃ (min{𝑃⋅𝐷#,𝑃+𝐷}) time. Both algorithms provide the first improvements over the classical Lawler and Moore algorithm in 50 years (which can also solve the more general 1||∑𝑤𝑗𝑈𝑗), and use more sophisticated tools such as polynomial multiplication and fast convolutions. Moreover, both algorithms are very easy to implement given a standard ready made FFT implementation for fast polynomial multiplication. Nevertheless, there are still a few ways which our results can be improved or extended:

Multiple machines: As we showed in Sect. 6, the SUMSETSCHEDULER algorithm can easily be extended to the multiple parallel machine case, giving us a total running time of 𝑂̃ (min{𝑃𝑚⋅𝐷#,𝑃𝑚+𝐷𝑚}). We do not know how to obtain a similar extension for algorithm CONVSCHEDULER. In particular, there is no reason to believe that 𝑃𝑚||∑𝑝𝑗𝑈𝑗 cannot be solved in 𝑂̃ (𝑃𝑚) time, or even better, either by extending algorithm CONVSCHEDULER or by a completely different approach.

Even faster skewed convolutions: We have no indication that our algorithm for (max,min)-Skewed-Convolution is the fastest possible. It would interesting to see whether one can improve its time complexity, say to 𝑂̃ (𝑃3/2). Naturally, any such improvement would directly improve Theorem 5. Conversely, one could try to obtain some sort of lower bound for the problem, possibly in the same vein as Theorem 2. Improving the time complexity beyond 𝑂̃ (𝑃3/2) seems difficult as this would directly imply an improvement to the (max,min)-Convolution problem. Indeed, let A, B be a given (max,min)-Convolution instance and construct vectors 𝐴0, 𝐵0 with 𝐴0[𝑖]=𝑁⋅𝐴[𝑖] and 𝐵0[𝑗]=𝑁⋅𝐵[𝑗] for 𝑁=2𝑛+1. If 𝐶0 is the (max,min)-Skewed-Convolution of 𝐴0 and 𝐵0 (that is, 𝐶0[𝑘]=max𝑖+𝑗=𝑘min{𝐴0[𝑖],𝐵0[𝑗]+𝑘}), then the vector C with 𝐶[𝑘]=⌊𝐶0[𝑘]/𝑁⌋ is the (max,min)-Convolution of A and B.

Other scheduling problems: Can the techniques in this paper be applied to any other interesting scheduling problems? A good place to start might be to look at other problems which directly generalize Subset Sum.