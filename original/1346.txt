Recent progress in the runtime analysis of evolutionary algorithms (EAs) has allowed the derivation of upper bounds on the expected runtime of standard steady-state genetic algorithms (GAs). These upper bounds have shown speed-ups of the GAs using crossover and mutation over the same algorithms that only use mutation operators (i.e., steady-state EAs) both for standard unimodal (i.e., ONEMAX) and multimodal (i.e., JUMP) benchmark functions. The bounds suggest that populations are beneficial to the GA as well as higher mutation rates than the default 1/n rate. However, making rigorous claims was not possible because matching lower bounds were not available. Proving lower bounds on crossover-based EAs is a notoriously difficult task as it is hard to capture the progress that a diverse population can make. We use a potential function approach to prove a tight lower bound on the expected runtime of the (2+1) GA for ONEMAX for all mutation rates c/n with 𝑐<1.422. This provides the last piece of the puzzle that completes the proof that larger population sizes improve the performance of the standard steady-state GA for ONEMAX for various mutation rates, and it proves that the optimal mutation rate for the (2+1) GA on ONEMAX is (97‾‾‾√−5)/(4𝑛)≈1.2122/𝑛.

Introduction
The runtime analysis of randomized search heuristics like evolutionary algorithms (EAs), simulated annealing, ant colony optimization and estimation-of-distribution algorithms is a young and active subfield in algorithm research that has produced remarkable results in the last 20 years [2, 11, 15, 23]. Its main goal is to understand the working principles of the algorithms in different scenarios by deriving runtime bounds depending on problem characteristics, choice of algorithms and parameters. This line of research started with simple evolutionary algorithms using mutation only. Still today, the role of crossover, also called recombination, is less well understood than the one of mutation. In fact, explaining when recombination and mutation based genetic algorithms (GAs)Footnote1 perform better than more traditional general purpose search heuristics that use mutation alone is regarded as one of the fundamental problems in evolution-inspired computation.

Traditionally proofs showing that crossover is a useful operator relied either on excessively low crossover rates [16, 18] or on some diversity-enforcing mechanism to make recombination effective by increasing the probability that members of the population are different [8, 10, 20, 22, 27]. However, it was never shown whether this enforced diversity was necessary or whether it was an additional requirement for the proofs to hold. Recently some results have appeared proving the superiority of standard steady state GAsFootnote2 over mutation-only algorithms, without the need of any additional diversity enforcing mechanisms. Dang et al. [7] proved that for sufficiently large population sizes, the (𝜇+1) GA is at least a linear factor faster than the best algorithm using only standard bit mutation for the JUMP benchmark function. Hence, they showed that crossover may help algorithms to escape more quickly from local optima. Sutton [28] even proved that for the NP-hard Closest String problem from computational biology, the (𝜇+1) GA with sufficiently large population size and restarts is a fixed parameter tractable (FPT) algorithm while if only standard bit mutation is used (i.e., (𝜇+1) EA) it is not.

Strikingly, recombination has also been proven to be useful on unimodal functions. Lengler [21] has shown that there exist monotone functions for which the (𝜇+1) EA with not too low standard bit mutation rate c/n (i.e., 𝑐>2.13) requires exponential runtime with high probability while the (𝜇+1) GA with sufficiently large population sizes can solve them in 𝑂(𝑛log𝑛) expected runtime for arbitrary mutation rates i.e., 𝛩(1)/𝑛. Analyses have revealed that the (𝜇+1) GA is faster than the (𝜇+1) EA using any standard bit mutation rate and population size, even on unimodal functions where the latter is particularly efficient i.e., ONEMAX [5, 6]. Furthermore, if the fitness of offspring that are identical to their parents is not unnecessarily re-evaluated, then the algorithm is faster than any unary unbiased black box algorithm for the problem [19], albeit slower than if the diversity is enforced [3, 27]. To prove these results, precise analyses up to the leading constants are required since for ONEMAX the algorithms have the same asymptotic expected runtime 𝑂(𝑛log𝑛) for moderate population sizes.

An important insight from these analyses is that if diversity is enforced as in Sudholt’s work [27], then inevitably there are no advantages of using population sizes greater than 𝜇=2 for ONEMAX. On the other hand, the analysis of Corus and Oliveto [6] provides upper bounds that decrease with the population size (up to some sub-logarithmic limit). For large enough population sizes the best derived upper bound is roughly 1.64𝑛ln𝑛 while, for 𝜇=2, Corus and Oliveto only provide a larger upper bound of 4𝑒𝑐𝑛ln𝑛𝑐(𝑐+4)+𝑂(𝑛) [5]. Due to a mistake in one probability calculation this turns out to actually be 9𝑒𝑐𝑛ln𝑛𝑐(2𝑐+9)+𝑂(𝑛).

Indeed, all the positive results summarised above regarding the plain (𝜇+1) GA required sufficiently large population sizes. While the comparative statements with the mutation-based algorithms were possible because of the availability of lower bounds on their expected runtime, rigorously showing whether the suggested population sizes are actually necessary requires lower bounds on the expected runtime of the (𝜇+1) GA. Proving lower bounds for GAs with crossover is a notoriously hard task. The only available analysis concerning a standard GA is the proof that the simple genetic algorithm (SGA [12]) cannot solve ONEMAX in polynomial time with overwhelming probability due to the ineffectiveness of the fitness proportional selection operator [25, 26]. There have been recent attempts to generalize proof methods like the family tree technique to crossover-based algorithms; however, these only apply in a specific setting without mutation.

Providing lower bounds on the expected runtime of the (𝜇+1) GA for ONEMAX has turned out to be surprisingly difficult. Sudholt simplified the analysis by considering a “greedy” (2+1) GA that always selects amongst the fittest individuals in the population and is sped-up by automatically achieving the best possible crossover operation between different parents [27]. A less greedy (2+1) GA was considered by Corus and Oliveto where individuals are only immediately crossed over optimally if the Hamming distance between the parents is larger than 2 [5]. These simplified algorithms allow the analysis to ignore the improvements which may occur in standard GAs when one parent is crossed over with another one of different fitness. However, it was never proven that the algorithms are indeed faster than the standard (2+1) GA, hence that the bounds are also valid for the latter algorithm. In this paper we provide a lower bound for the (2+1) GA with no simplifications that matches its upper bound up to the leading constant, hence providing a rigorous proof that larger populations are beneficial to the GA for ONEMAX. The preciseness of the results also allows us to derive that the value of 𝑐∈(0,1.422] that yields the optimal mutation rate c/n is approximately 𝑐=1.21221445.

A major difficulty in proving rigorous lower bounds for populations with crossover is to find a way to aggregate the state of the algorithm such that it accurately captures the current distance from the optimum, but also the potential improvements of the crossover operator. These advancements could be very big if the parents have a large Hamming distance, and our aim is to show that this rarely happens. We solve the aggregation problem for the (2+1) GA by defining a potential function that captures the current fitness and opportunities for easy improvements through crossover. By showing bounds on the expected increase in the potential, we are able to quantify how the distance to the optimum decreases in one generation. The challenge lies in proving this for every possible population, from those with identical individuals to those with a good amount of diversity. Once the potential is appropriately bounded, we can use standard drift analysis arguments to bound the expected time from below.

Main Contributions
The expected optimisation time of the (2+1) GA is bounded from above as follows.

Theorem 1
The expected optimisation time of the (2+1) GA with mutation rate 𝑐/𝑛, 𝑐>0 a constant, on ONEMAX is at most

9𝑒𝑐𝑐(2𝑐+9)⋅𝑛ln(𝑛)+𝑂(𝑛).
For 𝑐=1 this is 911⋅𝑒𝑛ln(𝑛)+𝑂(𝑛)≈2.224𝑛ln(𝑛)+𝑂(𝑛). The upper bound follows from applying the analytical framework in [5] with a corrected transition probability for 𝑝𝑟, using the value 1/(4e) instead of 5/(24e) (we shall give more details in Sect. 3). It can also be proven with mild adaptations of the proof of [27, Theorem 4]. We provide a self-contained proof in Sect. 3.

Our main contribution is the following lower bound that matches the upper bound proven in Theorem 1 up to small-order terms.

Theorem 2
The expected optimisation time of the (2+1) GA with mutation rate c/n, and 0<𝑐≤1.422 a constant, on ONEMAX is at least

9𝑒𝑐𝑐(2𝑐+9)⋅𝑛ln(𝑛)−𝑂(𝑛loglog𝑛).
Since the bounds from Theorems 1 and 2 have the same leading constant 9𝑒𝑐𝑐(2𝑐+9), which is minimised for

𝑐=97‾‾‾√−54≈1.21221445,
we identify this as the optimal mutation rate for the (2+1) GA (up to small-order terms) within the range of rates covered by Theorem 2.

Theorem 3
Amongst all mutation rates c/n with 𝑐∈(0,1.422], the choice 𝑐=97√−54 is the optimal mutation rate of the (2+1) GA on ONEMAX, up to small-order terms. Then the expected optimisation time is ≈2.18417𝑛ln(𝑛)+𝑂(𝑛).

The best identified mutation rate for the (2+1) GA is lower than the one minimising the upper bound for larger population sizes 𝜇≥5 (it is at least 1.425/n and increases with 𝜇) always providing upper bounds below 1.7𝑛ln𝑛 and decreasing with 𝜇 [6]. This implies that the (2+1) GA with mutation rate 97√−54𝑛 is at least 28% slower than any (𝜇+1) GA with 𝜇≥5 and appropriate mutation rates.

Structure of the paper. Section 2 formally defines the (2+1) GA and lists important tools for the analysis, including the drift theorem used for our main result. Section 3 presents the above-mentioned, corrected upper bound from Theorem 1 and its self-contained proof. In Sect. 4, we introduce the potential function that captures the state of the (2+1) GA and is crucial for the drift analysis proving the lower bound in Theorem 2. As determining the drift requires a careful case analysis, we give a roadmap of this analysis in Sect. 5, followed by the technical Sects. 6–8 that analyze the drift in different scenarios. In Sect. 9 we then put all pieces of our analysis together to complete the proof of Theorem 2. In addition, Sect. 10 gives empirical results and results of regression analyses. The latter confirm that our theoretical results are remarkably precise and provide further insight into small-order terms. We finish with some conclusions. To streamline the presentation of the most important cases in the drift analysis, some less insightful proofs have been moved to an appendix.

This paper extends a previous conference paper [24] where most proofs had to be omitted because of space constraints. The experimental results have not been published before either.

Preliminaries
The (2+1) GA is defined in Algorithm 1. The algorithm initialises the population with two randomly chosen individuals. At each generation it selects two random parents with replacement to be mated via uniform crossover. The operator assigns each bit to the offspring by selecting the corresponding bit from one parent with probability 1/2 and from the other with the same probability. Standard bit mutation is then applied to the offspring by flipping each of its bits independently with probability c/n. Finally, the worst individual amongst the parents and the offspring is removed to select the new population. Ties are broken uniformly at random.

figure a
We will analyse the expected runtime of the algorithm to optimise the function 𝑓(𝑥)=OneMax(𝑥)=∑𝑛𝑖=1𝑥𝑖, which counts the number of 1-bits in a bitstring. Formally, the runtime is defined as the number of fitness function evaluations, which equals the smallest t such that the current population contains an optimum, up to an additive term of at most 2 stemming from evaluating the initial population.

Throughout this article, we will use state-of-the-art analysis techniques for randomized search heuristics, including drift analysis and concentration inequalities. In the following, we list three important tools, starting with the drift theorem that is crucial for the lower bound on the runtime.

Theorem 4
(Multiplicative drift, lower bound, e. g., [29])

Let 𝑋𝑡, 𝑡≥0, be a stochastic process, adapted to a filtration 𝑡, over a state space 𝑆⊆ℝ≥𝑥min, where 𝑥min>0. Assume that 𝑋𝑡 is non-increasing, i. e., 𝑋𝑡+1≤𝑋𝑡 for all 𝑡≥0. Let T be the smallest 𝑡≥0 such that 𝑋𝑡≤𝑥min. If there exist positive real numbers 𝛽,𝛿>0 such that for all 𝑡<𝑇 it holds that

1.
E(𝑋𝑡−𝑋𝑡+1∣𝑡)≤𝛿𝑋𝑡

2.
P(𝑋𝑡−𝑋𝑡+1≥𝛽𝑋𝑡)≤𝛽𝛿/ln(𝑋𝑡)

then

E(𝑇∣0)≥1−𝛽1+𝛽ln(𝑋0/𝑥min)𝛿
The following Chernoff bound goes back to [4] and is formulated in the style of [9, Theorem 1.10.1].

Theorem 5
Let 𝑋1,…,𝑋𝑛 be independent random variables taking values in [0, 1]. Let 𝑋:=∑𝑛𝑖=1𝑋𝑖. Let 𝛿≥0. Then

P(𝑋≥(1+𝛿)E(𝑋))≤𝑒−min{𝛿2,𝛿}E(𝑋)/3.
The following lemma bounds a sum by an integral.

Lemma 6
For any integrable function 𝑓:ℝ→ℝ, the following statements hold.

1.
If f is non-increasing in [a, b], ∑𝑏𝑖=𝑎𝑓(𝑖)≤𝑓(𝑎)+∫𝑏𝑎𝑓(𝑖) d𝑖.

2.
If f is non-decreasing in [a, b], ∑𝑏𝑖=𝑎𝑓(𝑖)≤𝑓(𝑏)+∫𝑏𝑎𝑓(𝑖) d𝑖.

3.
If there is an 𝛼∈[𝑎,𝑏] such that f is non-increasing in [𝑎,𝛼] and non-decreasing in [𝛼,𝑏] then ∑𝑏𝑖=𝑎𝑓(𝑖)≤𝑓(𝑎)+𝑓(𝑏)+∫𝑏𝑎𝑓(𝑖) d𝑖.

Proof
The first statement follows from splitting off the term f(a) and using ∫𝑗𝑗−1𝑓(𝑖) d𝑖≥𝑓(𝑗) for all 𝑗∈[𝑎+1,𝑏], which follows from f being non-increasing. This implies ∑𝑏𝑖=𝑎+1𝑓(𝑖)≤∫𝑏𝑎𝑓(𝑖) d𝑖. The second statement is proved similarly. For the last statement, we note ∑𝑏𝑖=𝑎𝑓(𝑖)≤∑⌊𝛼⌋𝑖=𝑎𝑓(𝑖)+∑𝑏𝑖=⌈𝛼⌉𝑓(𝑖). Then we apply the first statement to ∑⌊𝛼⌋𝑖=𝑎𝑓(𝑖) and the second statement to ∑𝑏𝑖=⌈𝛼⌉𝑓(𝑖), yielding a bound of 𝑓(𝑎)+𝑓(𝑏)+∫⌊𝛼⌋𝑎𝑓(𝑖) d𝑖+∫𝑏⌈𝛼⌉𝑓(𝑖) d𝑖≤∑𝑏𝑖=𝑎𝑓(𝑖)≤𝑓(𝑎)+𝑓(𝑏)+∫𝑏𝑎𝑓(𝑖) d𝑖. ◻

Finally, we frequently need the following estimate of the largest binomial coefficient, which can be found in [9, Inequality 1.4.18].

Lemma 7
For all 𝑛∈ℕ and 𝑘∈{1,…,𝑛} it holds that

(𝑛𝑘)≤(𝑛⌈𝑛/2⌉)≤2𝑛2𝜋𝑛‾‾‾‾√.
Upper Bounds for the (2+1) GA
In this section we aim to convince the reader in two different ways that the upper bound for the (2+1) GA is as stated in Theorem 1, with a leading constant of 9𝑒𝑐𝑐(2𝑐+9) instead of the leading constant 4𝑒𝑐𝑐(𝑐+4) claimed in [5].

Sudholt [27] provided an upper bound for (𝜇+𝜆) GAs with a diversity mechanism in the tie-breaking rule for the replacement selection. The analysis is based on a fitness-level argument and a simple Markov chain analysis made on each fitness level. Corus and Oliveto [5] analysed the standard (𝜇+1) GA, for which the choice 𝜇=2 yields the (2+1) GA as defined in Algorithm 1. They observed that the (𝜇+1) GA can lose diversity after creating it, which required a more complex Markov chain analysis of each fitness level. We first explain their framework and argue why the leading constant is 9𝑒𝑐𝑐(2𝑐+9). In addition, we give a self-contained bound based on the analysis in [27] with an ad-hoc Markov chain framework similar to the one used in [5], simplified for the fixed value of 𝜇=2 (and 𝜆=1). This shows that both previous works [5, 27], with appropriate modifications, yield the same upper bound for the (2+1) GA.

The Framework by Corus and Oliveto
Corus and Oliveto coupled the standard artificial fitness levels method [15] with a Markov chain framework to bound the expected runtime of the (𝜇+1) GA for ONEMAX and any population size 𝜇 [5]. More precisely, they divided the search space into the canonical 𝑛+1 fitness levels, each containing all search points with i 1-bits, and assumed that the algorithm is in level 𝐿𝑖 if all the individuals of the population have exactly i 1-bits. Since crossover may only speed up the optimisation process if diversity is present in the population, a Markov chain was used on each fitness level to distinguish between populations with and without diversity. The Markov chain at fitness level i consists of two transient states 𝑆1,𝑖 and 𝑆2,𝑖 (i.e., resp. with/without diversity) and an absorbing state 𝑆3,𝑖, that is reached when a solution with better fitness is identified. For each level the analysis was performed by pessimistically assuming that initially on each level the population has no diversity (i.e., all individuals are identical). For this assumption to hold, and a valid upper bound on the expected runtime achieved, once the absorbing state is reached for level 𝐿𝑖, the expected time for the improved individual to take over the population has to be taken into account before the analysis of the absorption time of the Markov chain for level 𝐿𝑖+1 may be carried out (i.e., the population is initially in state 𝑆1,𝑖+1 for each level 𝐿𝑖). When the algorithm is on the current level 𝐿𝑖 with no diversity (𝑆1,𝑖) only two changes of states may occur, both due to mutation: either the absorption state 𝑆3,𝑖 is reached by increasing the number of 1-bits in an individual, or a state with some diversity is reached by switching the order of 1-bits and 0-bits in an offspring, hence reaching state 𝑆2,𝑖. From this state, 𝑆3,𝑖 may be reached more quickly than from state 𝑆1,𝑖 if diverse parents are selected for reproduction and at least an extra 1-bit obtained in the offspring via crossover with constant probability if the diversity is not lost before (eg., by selecting two identical parents, creating a copy by not flipping any bits, and removing the diverse individual in the selection for replacement step). Overall, the proof strategy requires summing up the bounds on the absorption times of the 𝑛+1 Markov chains and subsequent takeover times over the 𝑛+1 levels according to the artificial fitness levels methodology.

The described technique allows to calculate the transition probabilities of the Markov chain, hence to provide upper bounds on the expected runtime of the (𝜇+1) GA, as a function of any population size 𝜇. In particular, it provides bounds on the expected runtime of the (𝜇+1) GA up to the leading constant for population sizes of 𝜇=𝑜(log𝑛/loglog𝑛) that are smaller than those of any steady-state EA using only standard bit mutation (i.e., no crossover) (i.e., for larger population sizes the take over time becomes asymptotically larger than the 𝑂(𝑛log𝑛) expected runtime of the (1+1) EA). An important insight from the Markov chain analysis is that the probability of losing diversity in state 𝑆2,𝑖 is higher for population size 𝜇=2 than for larger populations: in the former case the diversity may be completely lost in every generation by either of the two individuals taking over, which is not the case for larger populations. However, in the analysis of this transition probability for the special case 𝜇=2, the bound provided by the authors of [5] is by a factor of 2 smaller than the correct one due to a mistake in the calculation of the probability that different parents are selected for reproduction and the variation operators produce an offspring identical to either parent (and the differing parent is removed by selection of replacement). This miscalculation led to an upper bound on the expected runtime of the (2+1) GA of 4𝑒𝑐𝑛ln𝑛𝑐(𝑐+4)+𝑂(𝑛) instead of the correct bound of 9𝑒𝑐𝑛ln𝑛𝑐(2𝑐+9)+𝑂(𝑛) provided in Theorem 1. In the following subsection we provide a self-contained proof of the result and point out exactly where the miscalculation occurred in [5].

A Self-Contained Upper Bound Proof for the (2+1) GA
Here we give a self-contained proof of the upper bound for the (2+1) GA, Theorem 1.

Proof of Theorem 1
We use straightforward adaptations of the proof of [27, Theorem 4], using the same notation as in [27]. As in said proof, we distinguish between the following cases that are labelled according to the current best fitness in the population, called i. There are cases i.1, i.2, and i.3 explained in the following. We estimate the expected time spent in all these cases, summed up over all possible values of i, to obtain an upper bound on the total expected optimisation time.

Case i.1: the population contains a search point with fitness i and one search point of lower fitness.

This case is left for good if another search point of fitness at least i is created as then both search points will have fitness at least i. A sufficient condition for this is that the parent with fitness i is selected twice as parent (probability 1/4) and no 1-bit is flipped during mutation (probability at least (1−𝑐/𝑛)𝑛−1=𝛺(1)). Hence the expected waiting time in case i.1 is O(1).

The other two cases are:

Case i.2: the population contains two copies of the same individual, with fitness i.

Case i.3: the population contains two different individuals, both having fitness i.

The algorithm can switch between the two cases as diversity can be gained or lost.

We first bound the expected time spent in Case i.3. This case can be left for good if the two different search point are chosen as parents (probability 1/2) if crossover generates a surplus of 1-bits (estimated from below by 1/4 in [27, page 427]) and if mutation does not flip any bits (probability at least (1−𝑐/𝑛)𝑛). Together, we obtain a probability of at least (1−𝑐/𝑛)𝑛/8. Hence, the expected time spent in Case i.3 is at most 8/(1−𝑐/𝑛)𝑛=𝑂(1).

Let 𝑇𝑖,2 be the random time spent in Case i.2. We leave this case for good if mutation only flips a single 0-bit and no 1-bit. This event has probability at least 𝑝+:=(𝑛−𝑖)⋅𝑐/𝑛⋅(1−𝑐/𝑛)𝑛−1. We further make a transition to Case i.3 if mutation flips exactly one 0-bit and one 1-bit, leading to a different search point with fitness i, and then choosing one of the identical parents for removal (probability 2/3). The probability of said events is at least 𝑝2→3:=𝑖(𝑛−𝑖)⋅(𝑐/𝑛)2⋅(1−𝑐/𝑛)𝑛−2⋅2/3.

From Case i.3 the algorithm can move back to Case i.2, so that we may have to consider multiple visits to Case i.2. A necessary condition for going back to Case i.2 is that the created offspring is identical to one of the search points in the population, and the other search point is being selected for removal (probability 1/3). The probability for creating an identical offspring is maximised when the two search points have Hamming distance 2. Either the same parent is selected twice (probability 1/2) and mutation does not flip any bit (probability (1−𝑐/𝑛)𝑛), or the same parent is selected twice and mutation creates the other parent (probability 𝑂(1/𝑛2)), or different parents are selected (probability 1/2), crossover and mutation set the two differing bits identical to one of the parents (probability 1/2 i.e., this is the probability that was wrongly estimated to be 1/4 in [5] leading to an upper bound of 5/(24e) for going back to Case i.2 rather than the correct 1/(4e) bound which we derive now) and mutation does not flip any of the common bits (probability (1−𝑐/𝑛)𝑛−2). Together, the probability of going back to Case i.2 is at most

13⋅((1−𝑐/𝑛)𝑛2+𝑂(𝑛−2)+(1−𝑐/𝑛)𝑛−24)=(1−𝑐/𝑛)𝑛4+𝑂(𝑛−1).
Hence the conditional probability that, when Case i.3 is left towards Case i.2 or higher fitness, it is left towards Case i.2 is at most

𝑝3→2:=(1−𝑐/𝑛)𝑛/4+𝑂(𝑛−1)(1−𝑐/𝑛)𝑛/4+𝑂(𝑛−1)+(1−𝑐/𝑛)𝑛/8=23+𝑂(𝑛−1).
With these pessimistic estimations for transition probabilities, we obtain the following recurrence:

𝑇𝑖,2≤1+(1−𝑝+−𝑝2→3)𝑇𝑖,2+𝑝2→3𝑝3→2𝑇𝑖,2
which is equivalent to

𝑇𝑖,2≤1𝑝++𝑝2→3(1−𝑝3→2)
and plugging in the above values yields the following upper bound for 𝑇𝑖,2:

1(𝑛−𝑖)⋅𝑐/𝑛⋅(1−𝑐/𝑛)𝑛−1+𝑖(𝑛−𝑖)⋅(𝑐/𝑛)2⋅(1−𝑐/𝑛)𝑛−2⋅2/3⋅(1−23−𝑂(𝑛−1))≤1(𝑛−𝑖)⋅𝑐/𝑛+𝑖(𝑛−𝑖)⋅(𝑐/𝑛)2⋅2/9⋅(𝑒𝑐+𝑂(𝑛−1)).
Summing up these upper bounds for all i yields a total time bound of

≤(𝑒𝑐+𝑂(𝑛−1))∑𝑖=0𝑛−11(𝑛−𝑖)⋅𝑐/𝑛+𝑖(𝑛−𝑖)⋅(𝑐/𝑛)2⋅2/9=(𝑒𝑐+𝑂(𝑛−1))𝑛𝑐∑𝑖=0𝑛−11(𝑛−𝑖)(1+𝑖⋅𝑐/𝑛⋅2/9).
A closer inspection of the function 𝑓(𝑖):=1(𝑛−𝑖)(1+𝑖⋅𝑐/𝑛⋅2/9) reveals that it is non-decreasing in [0,𝑛−1] if 𝑐≤9/2. For 𝑐>9/2 it is non-increasing in [0,𝑛(2𝑐−9)/(4𝑐)] and non-decreasing in [𝑛(2𝑐−9)/(4𝑐),𝑛−1]. In either case, we obtain an upper bound from Lemma 6:

∑𝑖=0𝑛−11(𝑛−𝑖)+𝑖(𝑛−𝑖)⋅𝑐/𝑛⋅2/9≤1𝑛+11+(𝑛−1)⋅𝑐/𝑛⋅2/9+∫𝑛−1𝑖=01(𝑛−𝑖)(1+𝑖⋅𝑐/𝑛⋅2/9) d𝑖.
The first two summands are in O(1). The integral simplifies as follows, using e. g. equation 3.3.20 in [1]:

∫𝑛−1𝑖=01(𝑛−𝑖)(1+𝑖⋅𝑐/𝑛⋅2/9) d𝑖=[92𝑐+9⋅ln(1+𝑖⋅𝑐/𝑛⋅2/9𝑛−𝑖)]𝑛−10=92𝑐+9(ln(1+(𝑛−1)⋅𝑐/𝑛⋅2/91)−ln(1𝑛))=92𝑐+9⋅ln(𝑛+(𝑛−1)⋅𝑐⋅2/9).
Plugging everything together, the total time bound is

(𝑒𝑐+𝑂(𝑛−1))𝑛𝑐(𝑂(1)+92𝑐+9⋅ln(𝑛+(𝑛−1)⋅𝑐⋅2/9))=9𝑒𝑐𝑐(2𝑐+9)⋅𝑛ln(𝑛+(𝑛−1)⋅𝑐⋅2/9)+𝑂(𝑛)≤9𝑒𝑐𝑐(2𝑐+9)⋅𝑛ln(𝑛)+𝑂(𝑛).
Noting that the times in all Cases i.1 and i.3, summed up for all i, are O(n) proves the claim. ◻

A Potential Function Approach
We now turn to the main contribution of this paper, the tight lower bound for the (2+1) GA. Before going into detail, this section describes the main idea behind our approach and clarifies some further notation and fundamental observations that will be used in the remainder.

We write a population {𝑥1,𝑥2} in order of monotonically decreasing fitness, that is, 𝑓(𝑥1)≥𝑓(𝑥2). Let 𝑛11 be the number of bit positions where both parents have ones and likewise for 𝑛00 and the number of zeros. Let 𝑛10 be the number of positions where 𝑥1 has a 1 and 𝑥2 has a 0 and likewise for 𝑛01. Then we have 𝑓(𝑥1)=𝑛11+𝑛10 and 𝑓(𝑥2)=𝑛11+𝑛01. Since by assumption, 𝑓(𝑥1)≥𝑓(𝑥2), we have 𝑛10≥𝑛01 and 𝑛10=𝑛01 is equivalent to the two individuals having equal fitness. In case 𝑛10=0, both individuals are identical. Such a population is called monomorphic in population genetics, and we use this term here.

Note that the (2+1) GA is an unbiased algorithm in the sense of Lehre and Witt [19]. In brief, this means that the algorithm treats all bit positions and all bit values symmetrically when generating new search points. Owing to this symmetry of bit positions and the fact that the fitness function is symmetrical itself, i. e., it only depends on the number but not the positions of the one-bits, it suffices to know 𝑛11,𝑛10 and 𝑛01 to fully characterise the state of the algorithm. Note that 𝑛00 can be derived as 𝑛−𝑛11−𝑛10−𝑛01.

The following lemma characterises probabilities of setting a bit to 1 in the offspring after a crossover of two different parents and a mutation of the result.

Lemma 8
Consider a crossover of two parents x, y followed by mutation with mutation rate 𝑝𝑚, resulting in an offspring z. For all i,

P(𝑧𝑖=1)=⎧⎩⎨⎪⎪1−𝑝𝑚1/2𝑝𝑚if 𝑥𝑖=𝑦𝑖=1if 𝑥𝑖≠𝑦𝑖if 𝑥𝑖=𝑦𝑖=0.
Proof
If 𝑥𝑖=𝑦𝑖 then crossover will create an offspring with the same bit value. The statement for 𝑥𝑖≠𝑦𝑖 holds because of symmetry, or using the following, alternative argument. The offspring has a 1 if crossover creates a 1 and mutation does not flip bit i, or if crossover creates a 0 and mutation does flip bit i. The probability of the former event is 1/2⋅(1−𝑝𝑚) and the probability of the latter event is 1/2⋅𝑝𝑚. Together, this gives 1/2. ◻

Note that differing bits 𝑥𝑖≠𝑦𝑖 are set to 1 with probability 1/2, irrespective of the mutation rate. Hence, when two parents are selected, we only need to consider the effect of mutation on the bits where the parents agree. We frequently and tacitly use this fact.

Our lower bound applies when only considering populations where the number of zeros in the fitter parent is at most 𝑛/polylog(𝑛) and at least polylog(𝑛). This implies that all probabilities that involve flipping a 0 to 1 are polylogarithmically small.

The main tool for our lower bound is going to be drift analysis, applied to a potential function that captures the current state and potential easy fitness improvements.

Definition 1
For a population P with values 𝑛11,𝑛10,𝑛01,𝑛00 we define the potential of P as

𝜑(𝑃)=𝑛11+𝑛10+𝑛013.
The intuition is that 𝑛11+𝑛10 describes the current best fitness in the population. The term 𝑛01/3 adds potential to the best fitness as the population has the potential to exploit the diversity given by the 𝑛01 1-bits that only exist in the less fit individual during a successful crossover operation.

The choice of the factor 1/3 is motivated as follows. We know from previous work [5, 27] that the most helpful populations for improvements are those where two search points have the same number of ones and Hamming distance 2, that is, 𝑛10=𝑛01=1. (Larger Hamming distances have the potential for larger fitness improvements, but such populations are rarely reached when the number of zeros becomes reasonably small.)

Assume the current state has 𝑛10=𝑛01=1, corresponding to a potential of 𝑛11+𝑛10+1/3. The most likely transitions (and, when only 𝑂(𝑛/polylog(𝑛)) zeros are left, the only transitions with probability 𝛺(1)) are (1) collapsing the population to copies of one parent (and potential 𝑛11+𝑛10) and (2) creating a surplus of one 1-bit by crossover and not flipping anything else (potential 𝑛11+𝑛10+1). The probability of the former event is roughlyFootnote3(1−𝑝𝑚)𝑛/4, which is the probability of selecting the same parent twice, not flipping any bits and then selecting the other population member for removal plus the probability of selecting different parents, creating one parent by crossover and not flipping any bits. The probability of the latter event is roughly (1−𝑝𝑚)𝑛/8, which is the probability of selecting different parents, setting both differing bits to 1 and not flipping any bits in the subsequent mutation.

Comparing these terms, the conditional probability of an improvement via crossover is roughly 1/3. In case a monomorphic population is reached, the potential reduces by 1/3 and this happens with conditional probability 1−1/3. In the latter event, the potential increases by 1−1/3 and this happens with conditional probability 1/3. The net effect of these transitions in the expected change of the potential is (1−1/3)1/3−1/3(1−1/3)=0. So the potential balances out the effects of “volatile” states left quickly.

Obviously, our analysis still needs to account for other, less likely transitions. For populations with 𝑛10=𝑛01>1 the conditional transition probabilities change as the probability of creating one of the parents by crossover depends on the Hamming distance 𝑛10+𝑛01 between parents. For 𝑛10=𝑛01>1 the likely progress in a successful crossover may be smaller than 𝑛01/3. Hence the term +𝑛01/3 in Definition 1 is a precise estimate for the likely progress when 𝑛01=1 and for larger 𝑛01 it is an overestimation.

It suffices to restrict our considerations to moderate values of 𝑛10 and 𝑛01. The reason is that the (2+1) GA always has a constant probability of creating a monomorphic population in one generation, regardless of the current population. This means that large values of 𝑛10 and 𝑛01 are very unlikely.

Lemma 9
Let 𝑡≥log2𝑛 and 𝑡=𝑛𝑂(1). With probability 1−𝑛−𝛺(log𝑛), all populations within the time interval [log2𝑛,𝑡] have Hamming distance at most log2𝑛 between their two individuals.

Proof
We call a generation that creates a population of two identical individuals a monomorphic generation. The crucial idea is to show that monomorphic generations are very frequent so that large Hamming distances are unlikely to occur.

The probability of a monomorphic generation happening is at least (1/4)(1−1/𝑛)𝑛(1/3)=𝛺(1) since it is sufficient to select a fittest parent twice, to clone it and to remove the other parent (which has probability at least 1/3). For a number 𝑡≥0 of generations after a monomorphic one, let 𝐷𝑡 denote the maximum number of bits in which the two parents ever have differed during these t generations. The crucial idea is that only mutations can increase this D-value. The total number of bits flipped in t generations is the sum of tn Poisson trials with success probability c/n each. Hence, within t generations following a monomorphic one, the D-value is bounded from above by 2ct with probability 1−2−𝛺(𝑡) according to Chernoff bounds (Theorem 5), and clearly the Hamming distance is no larger than the D-value. We set 𝑡:=(log2𝑛)/(2𝑐) to bound the D-value by log2𝑛.

The proof is completed by noting that the probability of not observing a monomorphic generation within log2𝑛 generations is (1−𝛺(1))log2𝑛=𝑛−𝛺(log𝑛). Together with the failure bound 2−𝛺(𝑡), which is 𝑛−𝛺(log𝑛) for 𝑡=(log2𝑛)/(2𝑐), and a union bound, this means that in any polynomial number of generations following the first monomorphic one the Hamming distance never exceeds log2𝑛 with probability 1−𝑛𝑂(1)𝑛−𝛺(log𝑛)=1−𝑛−𝛺(log𝑛). ◻

Roadmap for the Analysis
We give a deliberately informal, high-level view of our analysis, where 𝛥:=𝜑(𝑃𝑡+1)−𝜑(𝑃𝑡) denotes the change in potential in one generation. By the law of total probability, 𝛥 can be split up according to the number of zeros flipped by mutation:

E(𝛥)=E(𝛥∣no zeros flip)⋅P(no zeros flip)+E(𝛥∣one zero flips)⋅P(one zero flips)+E(𝛥∣at least 2 zeros flip)⋅P(at least 2 zeros flip).
The case that no zeros flip does not increase the potential, hence we aim to bound the first line from above by 0. The second line captures the most important case: one zero flips and subsequent progress is made. The second line will be bounded by the dominant term in our claimed lower bound. The third line involves the probability of flipping at least two zeros. If the number of zeros is small, this is unlikely and thus the third line only contributes a small order term.

The above high-level view is not particularly accurate. Firstly, the above estimations need to account for error terms. Secondly, the notion of “i zeros flip” used above is not well-defined. This is because the number of zeros that can flip during mutation depends on the parent selection. The same parents may be selected twice, and then the number of zeros depends on the fitness of the parent. If two different parents are used, we only consider mutations of bits that agree in both parents, as per Lemma 8.

Hence, we need to distinguish between different events from the parent selection. To formalise this, let 𝑃11,𝑃22, and 𝑃12 denote the events that parent selection chooses the first parent twice, the second parent twice and both parents, respectively. We further denote by 𝐹00 the number of flipping bits amongst the 𝑛00 bits and likewise for 𝐹11 and 𝑛11 bits. We use asterisks to indicate the union of sets: 𝐹0∗ is the number of flipping bits among 𝑛01+𝑛00 bits and 𝐹∗0 is the number of flipping bits among 𝑛10+𝑛00 bits. Variables 𝐹1∗ and 𝐹∗1 are defined analogously. Armed with this notation, we express the third line rigorously with a combination of events.

Lemma 10
For all populations with 𝑛11≥𝑛−𝑛/log3𝑛 and 𝑛10+𝑛01≤log2𝑛,

E(𝛥∣𝑃11,𝐹0∗≥2)P(𝐹0∗≥2)+E(𝛥∣𝑃22,𝐹∗0≥2)P(𝐹∗0≥2)+E(𝛥∣𝑃12,𝐹00≥2)P(𝐹00≥2)=𝑂(𝑛0∗/(𝑛log𝑛))
Proof
We give one common way of bounding the three lines from the statement. For all 𝐹∈{𝐹0∗,𝐹∗0,𝐹00}, the drift can only increase by at most 𝑛01+𝐹 as every flipping 0-bit can only increase the potential by at most 1 and crossover can increase the potential on all 𝑛01 by at most 1. Also note that P(𝐹∗0≥2) has the largest probability amongst all variables F (as the underlying number of zeros is maximal for 𝐹∗0). Thus, all 3 lines are bounded as

E(𝛥∣𝐹≥2)P(𝐹∗0≥2)=∑𝑖=2∞E(𝛥∣𝐹=𝑖)P(𝐹∗0=𝑖)≤∑𝑖=2∞(𝑛01+𝑖)⋅P(𝐹∗0=𝑖)≤∑𝑖=2∞(𝑛01+𝑖)(𝑐𝑛0∗𝑛)𝑖=(𝑐𝑛0∗𝑛)2∑𝑖=0∞(𝑛01+𝑖+2)(𝑐𝑛0∗𝑛)𝑖
Since 𝑛0∗≤𝑛−𝑛11≤𝑛/log3𝑛, we get (𝑐𝑛0∗𝑛)2=𝑂(𝑛0∗/(𝑛log3𝑛)). The sum is bounded by 𝑛01+2+∑∞𝑖=0𝑖(𝑐𝑛0∗𝑛)𝑖, which is 𝑛01+𝑂(1)=𝑂(log2𝑛). Together, this implies the claim. ◻

The cases of no zeros flipping and one zero flipping are more difficult to handle. Corresponding drift estimates will be derived in the following sections. More precisely, we derive closed formulas for the drift of the potential function depending on the choice of parents in the following Sect. 6 and then distinguish between generations that flip no zeros (Sect. 7) and one zero (Sect. 8). As mentioned in the introduction, the drift bounds obtained in these major technical sections are finally put together in Sect. 9.

Closed Formulas for the Potential Drift
Before proceeding to bound the drift of the potential, we derive closed bounds for the potential under the condition 𝑃12 that different parents are being selected. These bounds hold for arbitrary given values of 𝐹00, 𝐹11, the numbers of flipping common zeros and ones, respectively. These closed formulas will then be used in the subsequent sections to bound the drift for the cases 𝐹00=0 and 𝐹00=1, respectively.

For the derivation of closed formulas it is important to distinguish between two cases: the parents having equal fitness, 𝑛10=𝑛01, or the parents having unequal fitness, 𝑛10>𝑛01. This is because, depending on the fitness of the offspring, different cases may occur and the probabilities used during replacement selection may differ. For instance, when both parents have an equal fitness and the offspring happens to have the same fitness as well, there is a 3-way tie that is resolved uniform randomly by selection. When both parents have different fitness, 3-way ties are not possible (there can only be ties between the offspring and one parent). However, the offspring might be strictly worse than the better parent and strictly better than the worse parent. Such a case cannot happen when 𝑛10=𝑛01. It therefore makes sense to split the analysis into these two cases and to derive separate closed bounds on the potential drift.

A Closed Formula for Selecting Different Parents of Equal Fitness (𝑛10=𝑛01)
In the following lemma we shall first derive a closed formula for the potential drift when two different parents are chosen that have the same fitness, 𝑛10=𝑛01. The lemma defines a threshold ℓ that reflects the number of bits crossover needs to set to 1 to achieve the same fitness as 𝑥1 and 𝑥2.

Lemma 11
Let 𝑆∼Bin(𝑛10+𝑛01,1/2) and ℓ:=𝑛10−𝐹00+𝐹11. Then for all populations with 𝑛10=𝑛01,

E(𝛥∣𝑃12,𝐹00,𝐹11)=P(𝑆≥ℓ)⋅(56⋅E(𝑆∣𝑆≥ℓ)+𝐹00−2𝐹113−𝑛10)+P(𝑆=ℓ)⋅𝑛10−𝐹00−𝐹1118.
Proof
Consider a step where both parents are selected and 𝐹00,𝐹11 are known. The number of bits set to 1 by crossover is given by 𝑆∼Bin(𝑛10+𝑛01,1/2). By the law of total probability,

E(𝛥∣𝑃12,𝐹00,𝐹11)=∑𝑠∈ℤE(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠)⋅P(𝑆=𝑠).
Since 𝑛10=𝑛01, the fitness of both parents is 𝑛11+𝑛10 and the fitness of the offspring is 𝑛11+𝑠+𝐹00−𝐹11. If 𝑠<ℓ, the latter is less than 𝑛11+𝑛10 and the offspring will be rejected. If 𝑠>ℓ, the offspring is fitter than the parent, one of the parents will be chosen uniformly at random for removal. If the offspring is as fit as both parents, the offspring is removed with probability 1/3. Thus,

E(𝛥∣𝑃12,𝐹00,𝐹11)=∑𝑠∈ℤE(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠)⋅P(𝑆=𝑠)=∑𝑠>ℓE(𝛥∣𝑆=𝑠)⋅P(𝑆=𝑠)+23⋅E(𝛥∣𝑆=ℓ)⋅P(𝑆=ℓ)=∑𝑠≥ℓE(𝛥∣𝑆=𝑠)⋅P(𝑆=𝑠)−13⋅E(𝛥∣𝑆=ℓ)⋅P(𝑆=ℓ).
Now we estimate E(𝛥∣𝑆=𝑠)P(𝑆=𝑠) for 𝑠≥ℓ. Let 𝑆10 be the number of bits among the 𝑛10 bits that are set to 1 in the offspring and define 𝑆01 analogously for the 𝑛01 bits. Note that 𝑆:=𝑆10+𝑆01 where 𝑆10∼Bin(𝑛10,1/2) and 𝑆01∼Bin(𝑛01,1/2) according to Lemma 8.

Given 𝑆10=𝑠10 and 𝑆01=𝑠01, the potential difference 𝛥 is derived as follows. Among the 𝑛11 bits, 𝐹11 bits flip to 0, reducing their contribution from 1 to 1/3 each, leading to a contribution of −2𝐹11/3 to the potential difference. All the 𝑛10 bits contribute 1 to the potential 𝜑(𝑃𝑡). In 𝑃𝑡+1, 𝑠10 bits contribute 1 and the remaining 𝑛10−𝑠10 bits contribute 1/3 each. Hence the contribution to the potential difference is −2(𝑛10−𝑠10)/3. The 𝑛01 bits contribute 1/3 each in 𝜑(𝑃𝑡) and in 𝑃𝑡+1 we have 𝑠01 bits contributing 1 each and the other 𝑛01−𝑠01 bits contributing 0. Hence the contribution to the potential difference is 𝑠01−𝑛01/3. Finally, the contribution of the 𝑛00 bits to the potential difference is 𝐹00.

In the following we denote by (𝑋∣𝑌) the conditional variable X conditional on Y, where Y is a sequence of events and/or random variables. Omitting the implicit conditions 𝑃12,𝐹00,𝐹11 for brevity, we obtain a conditional drift 𝛥 of

(𝛥∣𝑆10=𝑠10,𝑆01=𝑠01)=𝐹00−2𝐹113−2(𝑛10−𝑠10)3+𝑠01−𝑛013=𝐹00−2𝐹113−𝑛10+𝑠−13⋅𝑠10.
(1)
By the law of total probability,

E(𝛥∣𝑆=𝑠)=∑𝑠10=0𝑠E(𝛥∣𝑆=𝑠,𝑆10=𝑠10)P(𝑆10=𝑠10∣𝑆=𝑠)=∑𝑠10=0𝑠(𝐹00−2𝐹113−𝑛10+𝑠−13⋅𝑠10)P(𝑆10=𝑠10∣𝑆=𝑠)=𝐹00−2𝐹113−𝑛10+𝑠−13∑𝑠10=0𝑠𝑠10⋅P(𝑆10=𝑠10∣𝑆=𝑠)=𝐹00−2𝐹113−𝑛10+𝑠−13⋅E(𝑆10∣𝑆=𝑠).
Now, (𝑆10∣𝑆=𝑠) follows a hypergeometric distribution with parameters 𝑛10 (number of red balls), 𝑛10+𝑛01 (number of balls) and s (number of draws). The expectation is thus 𝑠⋅𝑛10/(𝑛10+𝑛01)=𝑠/2. Plugging this in yields

E(𝛥∣𝑆=𝑠)=𝐹00−2𝐹113−𝑛10+56⋅𝑠.
(2)
Together, this gives

E(𝛥∣𝑃12,𝐹00,𝐹11)=∑𝑠≥ℓE(𝛥∣𝑆=𝑠)⋅P(𝑆=𝑠)−13⋅E(𝛥∣𝑆=ℓ)⋅P(𝑆=ℓ)=∑𝑠≥ℓ(𝐹00−2𝐹113−𝑛10+56⋅𝑠)⋅P(𝑆=𝑠)−13⋅(𝐹00−2𝐹113−𝑛10+56⋅ℓ)⋅P(𝑆=ℓ).
Using

∑𝑠≥ℓ𝑠⋅P(𝑆=𝑠)=∑𝑠≥ℓ𝑠⋅P(𝑆=𝑠∧𝑆≥ℓ)=∑𝑠≥ℓ𝑠⋅P(𝑆=𝑠∣𝑆≥ℓ)P(𝑆≥ℓ)=E(𝑆∣𝑆≥ℓ)P(𝑆≥ℓ),
the first terms simplify as

∑𝑠≥ℓ(𝐹00−2𝐹113−𝑛10+56⋅𝑠)⋅P(𝑆=𝑠)=(𝐹00−2𝐹113−𝑛10)∑𝑠≥ℓP(𝑆=𝑠)+56∑𝑠≥ℓ𝑠⋅P(𝑆=𝑠)=(𝐹00−2𝐹113−𝑛10)P(𝑆≥ℓ)+56⋅E(𝑆∣𝑆≥ℓ)P(𝑆≥ℓ).
The last term simplifies as

−13⋅(𝐹00−2𝐹113−𝑛10+56⋅ℓ)⋅P(𝑆=ℓ)=−13⋅(𝐹00−2𝐹113−𝑛10+56⋅(𝑛10−𝐹00+𝐹11))⋅P(𝑆=ℓ)=−13⋅(𝐹006+𝐹116−𝑛106)⋅P(𝑆=ℓ)=−𝐹00+𝐹11−𝑛1018⋅P(𝑆=ℓ)=𝑛10−𝐹00−𝐹1118⋅P(𝑆=ℓ).
Together, this proves the claim. ◻

The bound from Lemma 11 depends on the expected surplus E(𝑆∣𝑆≥ℓ) generated by a crossover on the bits that differ between the two parents, where ℓ reflects the fitness threshold above which offspring are accepted. We use the following formula to simplify such expressions. The proof goes back to work by Gruder [13] that is highlighted in a paper by Johnson [17]. It is given in the appendix for the sake of completion.

Lemma 12
Let 𝑆∼Bin(𝑛,1/2), then for all ℓ∈ℕ,

E(𝑆∣𝑆≥ℓ)P(𝑆≥ℓ)=ℓ2⋅P(𝑆=ℓ)+𝑛2⋅P(𝑆≥ℓ).
Using Lemma 12, we obtain the following simplified formula.

Lemma 13
Let 𝑆∼Bin(𝑛10+𝑛01,1/2). Then for all populations with 𝑛10=𝑛01,

E(𝛥∣𝑃12,𝐹00,𝐹11)=P(𝑆≥𝑛10−𝐹00+𝐹11)⋅(𝐹00−2𝐹113−𝑛106)+P(𝑆=𝑛10−𝐹00+𝐹11)⋅17𝑛10−17𝐹00+13𝐹1136.
Proof
Recall ℓ:=𝑛10−𝐹00+𝐹11. By Lemma 11 and Lemma 12,

E(𝛥∣𝑃12,𝐹00,𝐹11)=P(𝑆≥ℓ)⋅(56⋅E(𝑆∣𝑆≥ℓ)+𝐹00−2𝐹113−𝑛10)+P(𝑆=ℓ)⋅𝑛10−𝐹00−𝐹1118=56(ℓ2⋅P(𝑆=ℓ)+𝑛10⋅P(𝑆≥ℓ))+P(𝑆≥ℓ)⋅(𝐹00−2𝐹113−𝑛10)+P(𝑆=ℓ)⋅𝑛10−𝐹00−𝐹1118=P(𝑆≥ℓ)⋅(𝐹00−2𝐹113−𝑛106)+P(𝑆=ℓ)⋅(512⋅ℓ+𝑛10−𝐹00−𝐹1118)=P(𝑆≥ℓ)⋅(𝐹00−2𝐹113−𝑛106)+P(𝑆=ℓ)⋅15ℓ+2𝑛10−2𝐹00−2𝐹1136=P(𝑆≥ℓ)⋅(𝐹00−2𝐹113−𝑛106)+P(𝑆=ℓ)⋅17𝑛10−17𝐹00+13𝐹1136.
◻

A Closed Formula for Selecting Different Parents of Unequal Fitness (𝑛10>𝑛01)
For the case of unequal fitness (𝑛10>𝑛01), we use similar arguments as before. This scenario has more involved calculations as we need to distinguish different cases: the offspring may be at least as good as the fitter parent, and then the calculations are similar to the equal-fitness scenario. The offspring may also be worse than the fitter parent and better than the worse parent. In this case, the potential is derived from a different formula as the values for 𝑛10 and 𝑛01 in the next generation are still determined according to the fitter parent. In case the offspring’s fitness is equal to that of the worse parent, there is a tie and the offspring is only accepted with probability 1/2. The lemma defines two thresholds ℓ1 and ℓ2 that reflect the number of bits crossover needs to set to 1 to achieve the fitness of 𝑥1 and 𝑥2, respectively.

Lemma 14
Let 𝑆∼Bin(𝑛10+𝑛01,1/2), ℓ1:=𝑛10−𝐹00+𝐹11 and ℓ2:=𝑛01−𝐹00+𝐹11. Then for all populations with 𝑛10>𝑛01,

E(𝛥∣𝑃12,𝐹00,𝐹11)=P(𝑆>ℓ1)⋅2𝐹00−2𝐹11−𝑛10+𝑛013+P(𝑆=ℓ1)⋅𝐹00−𝐹11+𝑛013+P(𝑆>ℓ2)⋅2𝐹00−𝑛016+P(𝑆=ℓ2)⋅𝐹006.
Proof
Consider a step where both parents are selected and 𝐹00,𝐹11 are known. The number of bits set to 1 by crossover is given by 𝑆∼Bin(𝑛10+𝑛01,1/2). By the law of total probability,

E(𝛥∣𝑃12,𝐹00,𝐹11)=∑𝑠∈ℤE(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠)⋅P(𝑆=𝑠).
Note that the fitness of 𝑥1 is 𝑛11+𝑛10, that of 𝑥2 is 𝑛11+𝑛01 and the fitness of the offspring is 𝑛11+𝑠+𝐹00−𝐹11.

We distinguish the following cases.

1.
If 𝑠≥ℓ1, the offspring is at least as fit as the fitter parent, and 𝑥2 will be removed. The new values of 𝑛10,𝑛01 will be determined by the offspring.

2.
If ℓ2<𝑠<ℓ1, the offspring is fitter than the worse parent 𝑥2, and 𝑥2 will be removed. Parent 𝑥1 will remain the fitter parent.

3.
If 𝑠=ℓ2, the offspring is as fit as 𝑥2 and 𝑥2 will be removed with probability 1/2. Then the potential is computed as in the case ℓ2<𝑠<ℓ1.

4.
If 𝑠<ℓ2, the offspring is worse than 𝑥2 and will be rejected. Hence the potential does not change.

Let 𝛥 be the change of potential from the current population to the new population 𝑃𝑡+1 as described above. Then,

E(𝛥∣𝑃12,𝐹00,𝐹11)=∑𝑠=ℓ1𝑛10+𝑛01E(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠)⋅P(𝑆=𝑠)+∑𝑠=ℓ2ℓ1−1E(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠)⋅P(𝑆=𝑠)−12⋅E(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=ℓ2)⋅P(𝑆=ℓ2),
where the last line accounts for the fact that with probability 1/2 the offspring is removed if 𝑠=ℓ2.

Now we estimate E(𝛥∣𝑆=𝑠)P(𝑆=𝑠), starting with the case for 𝑠≥ℓ1. Let 𝑆10 be the number of bits among the 𝑛10 bits that are set to 1 in the offspring and define 𝑆01 analogously for the 𝑛01 bits. Note that 𝑆:=𝑆10+𝑆01 where 𝑆10∼Bin(𝑛10,1/2) and 𝑆01∼Bin(𝑛01,1/2).

Given 𝑆10=𝑠10 and 𝑆01=𝑠01, the potential difference 𝛥 is derived as follows. Among the 𝑛11 bits, 𝐹11 bits flip to 0, reducing their contribution from 1 to 1/3 each, leading to a contribution of −2𝐹11/3 to the potential difference. All the 𝑛10 bits contribute 1 to the potential 𝜑(𝑃𝑡). In 𝑃𝑡+1, 𝑠10 bits contribute 1 and the remaining 𝑛10−𝑠10 bits contribute 1/3 each. Hence the contribution to the potential difference is −2(𝑛10−𝑠10)/3. The 𝑛01 bits contribute 1/3 each in 𝜑(𝑃𝑡) and in 𝑃𝑡+1 we have 𝑠01 bits contributing 1 each and the other 𝑛01−𝑠01 bits contributing 0. Hence the contribution to the potential difference is 𝑠01−𝑛01/3. Finally, the contribution of the 𝑛00 bits to the potential difference is 𝐹00. Together,

(𝛥∣𝑆10=𝑠10,𝑆01=𝑠01)=𝐹00−2𝐹113−2(𝑛10−𝑠10)3+𝑠01−𝑛013=𝐹00−2𝐹113−2𝑛103−𝑛013+23⋅𝑠10+𝑠01=𝐹00−2𝐹113−2𝑛10+𝑛013+𝑠−13⋅𝑠10.
(3)
By the law of total probability, for all 𝑠≥ℓ1,

E(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠)=∑𝑠10=0𝑠E(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠,𝑆10=𝑠10)P(𝑆10=𝑠10∣𝑆=𝑠)=∑𝑠10=0𝑠(𝐹00−2𝐹113−2𝑛10+𝑛013+𝑠−13⋅𝑠10)P(𝑆10=𝑠10∣𝑆=𝑠)=𝐹00−2𝐹113−2𝑛10+𝑛013+𝑠−13∑𝑠10=0𝑠𝑠10⋅P(𝑆10=𝑠10∣𝑆=𝑠)=𝐹00−2𝐹113−2𝑛10+𝑛013+𝑠−13⋅E(𝑆10∣𝑆=𝑠).
Now, (𝑆10∣𝑆=𝑠) follows a hypergeometric distribution with parameters 𝑛10 (number of red balls), 𝑛10+𝑛01 (number of balls) and s (number of draws). The expectation is thus 𝑠⋅𝑛10/(𝑛10+𝑛01). Plugging this in yields

E(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠)=𝐹00−2𝐹113−2𝑛10+𝑛013+𝑠(1−𝑛103𝑛10+3𝑛01).
(4)
Now we consider the case ℓ2≤𝑆<ℓ1. Here 𝑥1 remains the fitter parent and the potential difference 𝛥 is derived as follows. Flipping bits among the 𝑛11 and 𝑛10 bits do not change the potential. We have 𝑆01+𝐹00 bits that each contribute 1/3 to the potential, compared to 𝑛01 bits in the previous population. Hence the change in potential is

(𝛥∣𝑃12,𝐹00,𝐹11,𝑆01=𝑠01)=𝑠01−𝑛01+𝐹003.
(5)
By the law of total probability, for all ℓ2≤𝑠<ℓ1,

E(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠)=∑𝑠01=0𝑠E(𝛥∣𝑃12,𝐹00,𝐹11,𝑆=𝑠,𝑆01=𝑠01)P(𝑆01=𝑠01∣𝑆=𝑠)=E(𝑆01∣𝑆=𝑠)−𝑛01+𝐹003=𝑠⋅𝑛013𝑛10+3𝑛01+−𝑛01+𝐹003
using the same calculations as before.

Plugging everything together,

E(𝛥∣𝑃12,𝐹00,𝐹11)=∑𝑠=ℓ1𝑛10+𝑛01(𝐹00−2𝐹113−2𝑛10+𝑛013+𝑠⋅2𝑛10+3𝑛013𝑛10+3𝑛01)P(𝑆=𝑠)+∑𝑠=ℓ2ℓ1−1(𝑠⋅𝑛013𝑛10+3𝑛01+−𝑛01+𝐹003)P(𝑆=𝑠)−12⋅(ℓ2⋅𝑛013𝑛10+3𝑛01+−𝑛01+𝐹003)P(𝑆=ℓ2). 
By adding terms for ℓ1≤𝑠≤𝑛10+𝑛01 to the second line and subtracting them from the first line, we get

=E(𝛥∣𝑃12,𝐹00,𝐹11)∑𝑠=ℓ1𝑛10+𝑛01(2𝐹003−2𝐹113−2𝑛103+2𝑠3)P(𝑆=𝑠)+∑𝑠=ℓ2𝑛10+𝑛01(𝑠⋅𝑛013𝑛10+3𝑛01+−𝑛01+𝐹003)P(𝑆=𝑠)−12⋅(ℓ2⋅𝑛013𝑛10+3𝑛01+−𝑛01+𝐹003)P(𝑆=ℓ2)=P(𝑆≥ℓ1)⋅2𝐹00−2𝐹11−2𝑛103+23∑𝑠=ℓ1𝑛10+𝑛01𝑠⋅P(𝑆=𝑠)+P(𝑆≥ℓ2)(−𝑛01+𝐹003)+𝑛013𝑛10+3𝑛01∑𝑠=ℓ2𝑛10+𝑛01𝑠⋅P(𝑆=𝑠)−12⋅(ℓ2⋅𝑛013𝑛10+3𝑛01+−𝑛01+𝐹003)P(𝑆=ℓ2).
Applying Lemma 12 twice, to the first and second line, yields the following two equations:

∑𝑠=ℓ1𝑛10+𝑛01𝑠⋅P(𝑆=𝑠)∑𝑠=ℓ2𝑛10+𝑛01𝑠⋅P(𝑆=𝑠)=ℓ12P(𝑆=ℓ1)+𝑛10+𝑛012P(𝑆≥ℓ1)=ℓ22P(𝑆=ℓ2)+𝑛10+𝑛012P(𝑆≥ℓ2).
Plugging this in yields

E(𝛥∣𝑃12,𝐹00,𝐹11)=P(𝑆≥ℓ1)⋅2𝐹00−2𝐹11−𝑛10+𝑛013+P(𝑆=ℓ1)⋅ℓ13+P(𝑆≥ℓ2)⋅2𝐹00−𝑛016+P(𝑆=ℓ2)⋅𝑛01−𝐹006.
◻

Potential Drift When No Zeros Flip
We now consider the potential drift when no zeros flip. When the distance to the optimum is o(n), this case is by far the most frequent case. This also means that our drift bounds have to be precise, as even a small error term may have a big impact and spoil the analysis.

Selecting the Same Parent Twice
We start by considering the drift conditional on selecting the same parent twice.

Lemma 15
For all populations with 𝑛10=𝑛01,

E(𝛥∣𝑃11,𝐹0∗=0)P(𝐹0∗=0)=−𝑛019(1−𝑐𝑛)𝑛E(𝛥∣𝑃22,𝐹∗0=0)P(𝐹∗0=0)=−𝑛019(1−𝑐𝑛)𝑛.
For all populations with 𝑛10>𝑛01,

E(𝛥∣𝑃11,𝐹0∗=0)P(𝐹0∗=0)=−𝑛013(1−𝑐𝑛)𝑛E(𝛥∣𝑃22,𝐹∗0=0)P(𝐹∗0=0)=0.
Proof
First assume 𝑛10=𝑛01 and consider the event 𝑃11. Given 𝐹0∗=0, that is, if no 0-bit is flipped, the offspring can only be accepted if no 1-bit is flipped, i. e., 𝐹1∗=0. These events happen with probability P(𝐹0∗=0)P(𝐹1∗=0)=(1−𝑐/𝑛)𝑛 and they lead to an offspring that is identical to 𝑥1. Since all search points have equal fitness, 𝑥2 is removed with probability 1/3. This leads to a monomorphic population and the potential decreases by 𝑛01/3. Multiplying the above terms proves the claimed equality. The case of 𝑃22 follows analogously, considering 𝐹∗0 and 𝐹∗1 instead.

For 𝑛10>𝑛01, if the fitter parent 𝑥1 is selected twice, a copy of it is created with probability (1−𝑐/𝑛)𝑛 and then 𝑥2 is removed. This decreases the potential by 𝑛01/3. Multiplying the above terms yields an expectation of −𝑛01/3⋅(1−𝑐/𝑛)𝑛. Note that other operations cannot increase the potential since no 0-bit is being flipped and flipping 1-bits in 𝑥1 does not decrease the potential.

If 𝑥2 is selected twice as parent, the potential cannot increase since no 0-bits are flipped, and it cannot decrease as any 1-bit being flipped will lead to the offspring being rejected. ◻

Now we consider the event that two different parents are selected. The remainder of this section is split into drift bounds when the parents have equal fitness, 𝑛10=𝑛01, and the case where parents have unequal fitness, 𝑛10>𝑛01.

Selecting Different Parents of Equal Fitness (𝑛10=𝑛01)
We use the closed formula from Lemma 13 to show that, to get an upper bound on the drift when 𝐹00=0, we only need to consider 𝐹11∈{0,1} as larger values lead to a non-positive drift. This is not obvious, but follows from a lengthy and trite calculation. The proof is placed in the appendix to keep the main part streamlined.

Lemma 16
For all 𝑛10=𝑛01 and all 𝑖≥2,

E(𝛥∣𝑃12,𝐹00=0,𝐹11=𝑖)≤0.
Now we are able to give an upper bound on the potential drift, assuming that different parents are selected (𝑃12) and no common zero-bits flip (𝐹00=0).

Lemma 17
For every population with 𝑛10=𝑛01,

E(𝛥∣𝑃12,𝐹00=0)≤⎧⎩⎨⎪⎪019⋅P(𝐹11=0)18⋅P(𝐹11=0)+164⋅P(𝐹11=1)if 𝑛10=0if 𝑛10=1if 𝑛10≥2.
Proof
By the law of total probability and Lemma 13,

E(𝛥∣𝑃12,𝐹00)=∑𝑖=0∞E(𝛥∣𝑃12,𝐹00,𝐹11=𝑖)⋅P(𝐹11=𝑖)=∑𝑖=0∞P(𝐹11=𝑖)(P(𝑆=𝑛10+𝑖)⋅17𝑛10+13𝑖36−P(𝑆≥𝑛10+𝑖)⋅6𝑛10+24𝑖36)=∑𝑖=0𝑛10−1P(𝐹11=𝑖)(P(𝑆=𝑛10+𝑖)⋅17𝑛10+13𝑖36−P(𝑆≥𝑛10+𝑖)⋅6𝑛10+24𝑖36)
as the term in brackets is 0 for all values 𝑖≥𝑛10.

For 𝑛10=0, the above is 0 and for 𝑛10=1, is it P(𝐹11=0)/9 as claimed. For 𝑛10≥2, by Lemma 16 all terms for 𝑖≥2 are non-positive and can be dropped. Thus, we get

E(𝛥∣𝑃12,𝐹00)≤∑𝑖=01E(𝛥∣𝑃12,𝐹00,𝐹11=𝑖)⋅P(𝐹11=𝑖)=∑𝑖=01P(𝐹11=𝑖)(P(𝑆=𝑛10+𝑖)⋅17𝑛10+13𝑖36−P(𝑆≥𝑛10+𝑖)⋅6𝑛10+24𝑖36)=P(𝐹11=0)(P(𝑆=𝑛10)⋅17𝑛1036−P(𝑆≥𝑛10)⋅6𝑛1036)+P(𝐹11=1)(P(𝑆=𝑛10+1)⋅17𝑛10+1336−P(𝑆≥𝑛10+1)⋅6𝑛10+2436).
Writing the above as

P(𝐹11=0)𝑎0+P(𝐹11=1)𝑎1,
we have 𝑎0=0 for 𝑛10=0 and 𝑎0=1/9 for 𝑛10=1. The maximum value is 𝑎0=1/8 for 𝑛10=2. This can be seen from evaluating 𝑎0 for 0≤𝑛10≤10 (shown in Table 1) and from the following analytical arguments for 𝑛10≥11. We use the bound on the largest binomial coefficient (2𝑛10𝑘)≤(2𝑛10𝑛10)≤22𝑛10/(𝜋𝑛10‾‾‾‾√) for all k according to Lemma 7 and the fact that P(𝑆≥𝑛10)≥1/2 by symmetry of the binomial distribution. Then 𝑎0 is at most

P(𝑆=𝑛10)⋅17𝑛1036−P(𝑆≥𝑛10)⋅6𝑛1036≤P(𝑆=𝑛10)⋅17𝑛1036−𝑛1012≤1𝜋𝑛10‾‾‾‾√⋅17𝑛1036−𝑛1012=1𝜋‾‾√⋅17𝑛10‾‾‾√36−𝑛1012.
This upper bound becomes negative for 𝑛10≥11 (the real value 𝑎0 that is being bounded from above already becomes negative for 𝑛10≥7). The term 𝑎1 is 0 for 𝑛10≤1 and at most 1/64 for 𝑛10≥2. This can be seen by evaluating the above formula for 2≤𝑛10≤10 (see Table 1) and bounding it above as follows for 𝑛10≥3, using that P(𝑆≥𝑛10+1) is minimised for 𝑛10=3, where it is 5/16.

P(𝑆=𝑛10+1)⋅17𝑛10+1336−P(𝑆≥𝑛10+1)⋅6𝑛10+2436≤1𝜋𝑛10‾‾‾‾√⋅17𝑛10+1336−516⋅6𝑛10+2436=1𝜋𝑛10‾‾‾‾√⋅17𝑛10+1336−516⋅6𝑛10+2436,
which is at most 𝑛10/64 for 𝑛10≥11. ◻

Table 1 Numerical values for coefficients 𝑎0 and 𝑎1 defined in the proof of Lemma 17, for different values of 𝑛10=𝑛01
Full size table
Selecting Different Parents of Unequal Fitness (n_{10}>n_{01})
The case of P_{12}, different parents being selected, no common 0-bits flipping ({F_{00}=0}) and parents having unequal fitness is easy to handle as in this scenario the potential drift is always non-positive. This is shown in the following lemma. It also states a closed formula for the potential drift as this will be used later on, in the proof of Lemma 22.

Lemma 18
For all n_{10} > n_{01} and all F_{11} \in {\mathbb {N}}_0,

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=0, F_{11}\right) }\\&\quad =\; \mathord {{\mathrm {P}}}\mathord {\left( S> \ell _1\right) } \frac{-2F_{11}-n_{10}+n_{01}}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1\right) }\frac{-F_{11}+n_{01}}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S > \ell _2\right) } \frac{- n_{01}}{6} \le 0. \end{aligned}
The proof is not very insightful, hence it is placed in the appendix.

Combined Drift Bound When No Zero Flips
Assembling the previous drift bounds under various conditions, we get the following drift bounds.

Lemma 19
Assume c \le 56/9, n_{10}=n_{01} and n_{11} \le n-c.

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{11}, F_{0*} = 0\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{11}, F_{0*} = 0\right) }\\&\qquad +\; \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{22}, F_{*0} = 0\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{22}, F_{*0} = 0\right) }\\&\qquad +\; \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 0\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{12}, F_{00} = 0\right) }\\&\quad \le {\left\{ \begin{array}{ll} \frac{cn_{10}^2}{9n} &{} \text {if }n_{10}=n_{01}\\ -\frac{n_{01}}{12} \left( 1 - \frac{c}{n}\right) ^n &{} \text {if }n_{10} > n_{01}. \end{array}\right. } \end{aligned}
Proof
For n_{10}=n_{01}, we argue that Lemma 17 implies

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 0\right) } \le \frac{n_{01}}{9} \left( 1 - \frac{c}{n}\right) ^{n_{11}}. \end{aligned}
This is obvious for n_{10} \le 1; for n_{10} \ge 2 it follows from \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=1\right) } = cn_{11}/n \cdot \left( 1 - c/n\right) ^{n-1} = cn_{11}/(n-c) \cdot \left( 1-c/n\right) ^n \le c\left( 1-c/n\right) ^n and 1/8 + c/64 \le 2/9 \le n_{01}/9 using c \le 56/9.

By Lemmas 15 and 17, along with \mathord {{\mathrm {P}}}\mathord {\left( P_{12}\right) } = \mathord {{\mathrm {P}}}\mathord {\left( P_{11} \cup P_{22}\right) } = 1/2 and \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=0\right) }=(1-c/n)^{n_{00}}, the left-hand side is at most

\begin{aligned}&\frac{n_{10}}{18} \left( 1 - \frac{c}{n}\right) ^{n_{11}+n_{00}} - \frac{n_{10}}{18} \left( 1 - \frac{c}{n}\right) ^n\\&\quad =\;\frac{n_{10}}{18} \left( 1 - \frac{c}{n}\right) ^{n_{11}+n_{00}} \left( 1 - \left( 1 - \frac{c}{n}\right) ^{n_{10}+n_{01}}\right) \\&\quad \le \;\frac{n_{10}}{18} \left( 1 - \frac{c}{n}\right) ^{n_{11}+n_{00}} \left( 1 - \left( 1 - \frac{c(n_{10}+n_{01})}{n}\right) \right) \\&\quad =\;\frac{n_{10}}{18} \left( 1 - \frac{c}{n}\right) ^{n_{11}+n_{00}} \frac{c(n_{10}+n_{01})}{n} \le \frac{cn_{10}^2}{9n}. \end{aligned}
The bound for the case n_{10}>n_{01} follows immediately from Lemmas 15 and 18, along with \mathord {{\mathrm {P}}}\mathord {\left( P_{11}\right) } = 1/4. \square

Potential Drift When One Zero Flips
In this section we show that the drift is bounded by a term that yields the leading constant we are aiming for in our main result. Note that here we can afford to include error terms of lower order.

We first consider the case that two equal parents are selected.

Lemma 20
For all populations with n_{10}=n_{01},

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{11}, F_{0*} = 1\right) } \le \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) } \left( 1 - \frac{n_{01}}{6}\right) + \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=1\right) } \left( \frac{2}{9} - \frac{n_{01}}{9} \right) \\&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{22}, F_{*0} = 1\right) } \le \mathord {{\mathrm {P}}}\mathord {\left( F_{*1}=0\right) } \left( 1 - \frac{n_{01}}{6}\right) + \mathord {{\mathrm {P}}}\mathord {\left( F_{*1}=1\right) } \left( \frac{2}{9} - \frac{n_{01}}{9} \right) . \end{aligned}
For all populations with n_{10} > n_{01},

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{11}, F_{0*} = 1\right) } \le \;&{\left\{ \begin{array}{ll} \frac{1}{3} + \frac{2}{3} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) } &{} \text {if }n_{01}=0\\ \left( 1 - \frac{n_{01}}{3}\right) \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) } + \left( \frac{1}{3} - \frac{n_{01}}{3}\right) \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=1\right) } &{} \text {otherwise}. \end{array}\right. }\\ \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{22}, F_{*0} = 1\right) } \le \;&\frac{1}{3} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{*1} = 0\right) } + \frac{1}{6} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{*1}=1\right) }. \end{aligned}
Proof
First assume n_{10} = n_{01} and consider the event P_{11}. Given F_{0*} = 1, the offspring is accepted with certainty if no 1-bit is flipped. With probability 1/2 the parent survives and then the new potential is at most n_{11}+n_{10}+1. Consequently, the potential changes by at most 1-n_{01}/3 due to the loss of diversity. With the remaining probability 1/2, the potential increases by at most 1. The overall expected change in potential in this case is thus at most 1-n_{01}/6. If a single 1-bit also flips together with the 0-bit, then the offspring has the same fitness as both parents and the individual to be removed is selected uniformly at random. If the offspring is removed, the potential does not change. If the parent is removed, the potential increases by at most 1/3. If the other population member is removed, the potential increases by at most 1/3-n_{10}/3 due to the loss of diversity. The expected change in potential in this case is thus at most 2/9 - n_{10}/9. If more than one 1-bits flip, then the offspring will have lower fitness than both other members and it will be rejected. Summing up the terms proves the first claim and the case of P_{22} follows analogously, considering F_{*0} and F_{*1} instead.

For n_{10} > n_{01}, given F_{0*} = 1 and that the fitter parent x^1 is selected twice, we consider separate cases according to the size of n_{01}. If n_{01}=0, then no diversity can be lost. Thus, if no 1-bits flip, the potential increases by 1 because the new offspring has higher fitness than x^1 and x^2 is rejected. If at least one 1-bit flips, then the best fitness does not change and the potential increases by at most 1/3. Overall, for the case when n_{01}=0, the potential changes by \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) } + 1/3 \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}>0\right) } = 1/3 + 2/3 \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) }. If n_{01}>0 and no 1-bits flip, then the potential changes by at most 1 - n_{01}/3 since the diversity is lost because x^2 is removed. If instead at least one 1-bit flips, then the potential changes by at most 1/3 - n_{01}/3 since the best fitness does not change and the diversity may be lost if x^2 is removed. Since for n_{01}>0 these terms are negative, and the offspring is accepted with probability 1 if F_{1*} = 1 the claim follows by summing up the two terms.

If x^2 is selected twice and no 1-bits are flipped, then the potential increases by at most 1/3 (i.e., if an n_{00} bit is flipped) since the parent is removed and the diversity is kept. If a single 1-bit is flipped then the potential increases again by at most 1/3. However, since the offspring has the same fitness as its parent, it is necessary that the parent is removed which happens with probability 1/2. If more than one 1-bits are flipped, then the offspring is rejected. Summing up the terms completes the proof. \square

The proof of Lemma 20 has revealed a counterintuitive effect. A population of individuals with very different fitness values f(x^1) \gg f(x^2) can have an advantage over a population where both members have the same fitness f(x^1). This is because, conditioning on a 0-bit flipping, if the fitter parent is chosen twice, a near-arbitrary number of 1-bits can flip at the same time and the outcome may still be accepted. This increases the potential and explains why Lemma 20 contains an unexpectedly large potential drift in the case n_{10} > n_{01} = 0.

Now we consider the case P_{12} that two different parents are selected.

When F_{11} \ge 2 and the parents have equal fitness, the drift under P_{12} is non-positive for n_{10} \ge 10, except for F_{11}=n_{10-1}, where it is exponentially small in n_{10}. The proof of the following lemma is given in the appendix.

Lemma 21
For all n_{10}=n_{01}, n_{10} \ge 10 and all 2 \le i \le n_{10}-1,

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=1, F_{11}=i\right) } \le {\left\{ \begin{array}{ll} 5n_{10}^32^{-2n_{10}}/3 &{} \text {if }i = n_{10}-1\\ 0 &{} \text {otherwise.} \end{array}\right. } \end{aligned}
When the two parents have unequal fitness, the following uniform upper bound on the potential drift applies.

Lemma 22
For all n_{10}>n_{01} and all F_{11} \in {\mathbb {N}}_0,

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=1, F_{11}\right) } \le \frac{2}{3}. \end{aligned}
Proof
We start again with the drift formula from Lemma 14 and plug in F_{00}=1. Then

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=1, F_{11}\right) } =\;&\mathord {{\mathrm {P}}}\mathord {\left( S> \ell _1\right) } \cdot \frac{2-2F_{11}-n_{10}+n_{01}}{3}\\&+ \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _1\right) } \cdot \frac{1-F_{11}+n_{01}}{3}\\&+ \mathord {{\mathrm {P}}}\mathord {\left( S > \ell _2\right) } \cdot \frac{2 - n_{01}}{6}\\&+ \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _2\right) } \cdot \frac{1}{6}. \end{aligned}
For n_{10}<12 the claim is verified numerically for all pairs n_{01}<n_{10}, see Table 4. We now turn to an analytical argument for n_{10}\ge 12.

From Lemma 18 we already know that

\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( S> \ell _1\right) } \frac{-2F_{11}-n_{10}+n_{01}}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1\right) }\frac{-F_{11}+n_{01}}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S > \ell _2\right) } \frac{- n_{01}}{6} \le 0 \end{aligned}
regardless of F_{11}. Hence,

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=1, F_{11}\right) }\\&\quad \le \; \mathord {{\mathrm {P}}}\mathord {\left( S> \ell _1\right) } \cdot \frac{2}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _1\right) } \cdot \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S> \ell _2\right) } \cdot \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _2\right) } \cdot \frac{1}{6} \\&\quad \le \; \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S > \ell _2\right) } \cdot \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _2\right) } \cdot \frac{1}{6} \end{aligned}
since \mathord {{\mathrm {P}}}\mathord {\left( S\ge \ell _1\right) }\le 1/2 (using that \ell _1 is strictly greater than the mean of S). Altogether,

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=1, F_{11}\right) }&\le \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S\ge \ell _2\right) }\frac{1}{3} \le \frac{2}{3} \end{aligned}
no matter how \ell _1 and \ell _2 turn out. \square

To obtain a combined drift formula, we need to consider probabilities for flipping a certain number of 1-bits. The following simple lemma gives bounds for these.

Lemma 23
For every mutation rate c/n and every i,

\begin{aligned} \left( \frac{n_{11}-i+1}{n-c}\right) ^i \cdot \frac{c^i}{i!}\left( 1 - \frac{c}{n}\right) ^{n_{11}} \le \mathord {{\mathrm {P}}}\mathord {\left( F_{11} = i\right) } \le \left( \frac{n_{11}}{n-c}\right) ^i \cdot \frac{c^i}{i!} \left( 1 - \frac{c}{n}\right) ^{n_{11}}. \end{aligned}
Proof
\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( F_{11} = i\right) }&=\; \left( {\begin{array}{c}n_{11}\\ i\end{array}}\right) \left( \frac{c}{n}\right) ^i \left( 1 - \frac{c}{n}\right) ^{n_{11}-i}\\&=\; \left( {\begin{array}{c}n_{11}\\ i\end{array}}\right) \left( \frac{c}{n-c}\right) ^i \left( 1 - \frac{c}{n}\right) ^{n_{11}}\\&=\; \left( \frac{n_{11}}{n-c} \cdot \frac{n_{11}-1}{n-c} \cdot \dots \cdot \frac{n_{11}-i+1}{n-c}\right) \cdot \frac{c^i}{i!} \cdot \left( 1 - \frac{c}{n}\right) ^{n_{11}}. \end{aligned}
The first bracket is bounded as

\begin{aligned} \left( \frac{n_{11}-i+1}{n}\right) ^i \le \left( \frac{n_{11}}{n-c} \cdot \frac{n_{11}-1}{n-c} \cdot \dots \cdot \frac{n_{11}-i+1}{n-c}\right) \le \left( \frac{n_{11}}{n-c}\right) ^i. \end{aligned}
\square

Using Lemmas 20 and 21 and considering the drift under P_{12} separately for F_{11} \in \{0,1\} and for all F_{11} when n_{10} \le 10, we get:

Lemma 24
Assume c \le 2.71, n_{10}+n_{01} \le \log ^2 n, n_{11} \ge n - n/\log ^3 n and n_{00} \ge n_{10}\log n.

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{11}, F_{0*} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{11}, F_{0*} = 1\right) }\\&\qquad +\; \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{22}, F_{*0} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{22}, F_{*0} = 1\right) }\\&\qquad +\; \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{12}, F_{00} = 1\right) }\\&\quad \le {\left\{ \begin{array}{ll} \frac{c(2c+9)}{9e^c} \cdot \frac{n_{00}}{n} \cdot (1+O(1/\log n)) &{} \text {if }n_{10}=n_{01}\\ \frac{c}{e^c} \left( \frac{1}{4} + \frac{c}{48} + \frac{e^c}{4} + O(1/\log n)\right) \frac{n_{00}}{n} &{} \text {if }n_{10}> n_{01} = 0\\ \frac{c}{e^c} \left( \frac{1}{4} + \frac{c}{24} + \frac{e^c}{3} + O(1/n)\right) \frac{n_{00}}{n} &{} \text {if }n_{10}> n_{01} > 0. \end{array}\right. } \end{aligned}
Proof
We consider different cases.

Case {{\varvec{n}}}_{{{\varvec{10}}}}={{\varvec{n}}}_{{{\varvec{01}}}}: In case both parents have equal fitness,

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{11}, F_{0*} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{11}, F_{0*} = 1\right) }&=\; \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{22}, F_{*0} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{22}, F_{*0} = 1\right) } \end{aligned}
and we bound the sum of both terms, using \mathord {{\mathrm {P}}}\mathord {\left( P_{11}\right) } = \mathord {{\mathrm {P}}}\mathord {\left( P_{22}\right) } = 1/4 and Lemma 20 (noting that the formulas for P_{11} and P_{22} are identical) as

\begin{aligned}&\frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{0*}=1\right) }}{2} \left( \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) } \left( 1 - \frac{n_{01}}{6}\right) + \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=1\right) } \left( \frac{2}{9} - \frac{n_{01}}{9} \right) \right) . \end{aligned}	(6)
For P_{12} we consider probabilities \mathord {{\mathrm {P}}}\mathord {\left( F_{11} = \cdot \right) }, hence we aim to relate this with probabilities \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=\cdot \right) } as follows. (Note that, for n_{10}=0, F_{1*}=F_{11}.)

\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) } = \left( 1 - \frac{c}{n}\right) ^{n_{11}+n_{10}} = \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=0\right) } \left( 1 - \frac{c}{n}\right) ^{n_{10}} \end{aligned}
and the term \left( 1 - \frac{c}{n}\right) ^{n_{10}} is at least 1 - \frac{cn_{10}}{n} and at most 1. Similarly,

\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=1\right) } = \frac{c(n_{11}+n_{10})}{n}\left( 1 - \frac{c}{n}\right) ^{n_{11}+n_{10}-1} = \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=1\right) } \frac{n_{11}+n_{10}}{n_{11}} \left( 1 - \frac{c}{n}\right) ^{n_{10}} \end{aligned}
and the term \frac{n_{11}+n_{10}}{n_{11}} \left( 1 - \frac{c}{n}\right) ^{n_{10}} is at least 1 - \frac{cn_{10}}{n} and at most 1+O(n_{10}/n). Likewise,

\begin{aligned} \frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{0*}=1\right) }}{\mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) }} = \frac{n_{00}+n_{01}}{n_{00}} \left( 1 - \frac{c}{n}\right) ^{n_{01}}, \end{aligned}
which is at least 1-cn_{01}/n and at most 1+O(1/\log n) owing to the assumption n_{00} \ge n_{10}\log n \ge n_{01}\log n.

Thus, (6) can be written as

\begin{aligned}&\frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) }}{2} \left( \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=0\right) } \left( 1 - \frac{n_{01}}{6}\right) + \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=1\right) } \left( \frac{2}{9} - \frac{n_{01}}{9} \right) \right) + \xi \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) } \end{aligned}	(7)
for an error term \xi \in [-O(1/\log n), +O(1/\log n)], regardless of the signs of the factors the terms \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) } and \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=1\right) } are multiplied with. Here we used that these factors are in [-O(n_{10}), O(n_{10})] and the absolute value of the error from replacing each term \mathord {{\mathrm {P}}}\mathord {\left( F_{1*} = \cdot \right) } with \mathord {{\mathrm {P}}}\mathord {\left( F_{11} = \cdot \right) } is at most O(n_{10}^2/n) = O(1/\log n).

The third term from the statement is

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{12}, F_{00} = 1\right) }\nonumber \\&\quad =\;\frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) }}{2} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1\right) }\nonumber \\&\quad =\;\frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) }}{2} \sum _{i=0}^{n_{10}+1} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1, F_{11}=i\right) } \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=i\right) }. \end{aligned}	(8)
We simplify the expectations in the above terms for i \in \{0, 1\} using Lemma 13 and add them to the terms in (7). This yields an upper bound of

\begin{aligned}&\mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1, F_{11}=0\right) } \cdot a_0 + \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1, F_{11}=1\right) } \cdot a_1 \end{aligned}
for coefficients a_0, a_1 defined as follows:

\begin{aligned} a_0&:=\; \frac{1}{2} \left( 1 - \frac{n_{10}}{6} + \mathord {{\mathrm {P}}}\mathord {\left( S \ge n_{10}-1\right) }\left( 1 - \frac{n_{10}}{6}\right) + \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10}-1\right) } \cdot \frac{17n_{10}-17}{36}\right) \\ a_1&:=\; \frac{1}{2} \left( \frac{2}{9} - \frac{n_{10}}{9} + \mathord {{\mathrm {P}}}\mathord {\left( S \ge n_{10}\right) }\left( \frac{1}{3} - \frac{n_{10}}{6}\right) + \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10}\right) } \cdot \frac{17n_{10}-4}{36}\right) . \end{aligned}
It is easy to verify that a_0 = 1 for n_{10}=0 and a_0 \le 1 for 1 \le n_{10} \le 9. For n_{10} \ge 10, a_0 becomes negative since the term \mathord {{\mathrm {P}}}\mathord {\left( S > n_{10}-1\right) }\left( 1 - \frac{n_{10}}{6}\right) is non-positive and using \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10}-1\right) } \le 1/(\sqrt{\pi n_{10}}) according to Lemma 7, we get

\begin{aligned} a_0 \le \;&\frac{1}{2} \left( 1 - \frac{n_{10}}{6} + \frac{1}{\sqrt{\pi n_{10}}} \cdot \frac{11n_{10}+19}{36}\right) < 0. \end{aligned}
Similarly, it is easy to check that a_1 = 2/9 for n_{10}=0 and a_1 \le 2/9 for 1 \le n_{10} \le 6. For n_{10} \ge 7, a_1 becomes negative as

\begin{aligned} a_1&=\; \frac{1}{2} \left( \frac{2}{9} - \frac{n_{10}}{9} + \mathord {{\mathrm {P}}}\mathord {\left( S > n_{10}\right) }\left( \frac{1}{3} - \frac{n_{10}}{6}\right) + \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10}\right) } \cdot \frac{11n_{10}+8}{36}\right) \\&\le \; \frac{1}{2} \left( \frac{2}{9} - \frac{n_{10}}{9} + \frac{1}{\sqrt{\pi n_{10}}} \cdot \frac{11n_{10}+8}{36}\right) < 0. \end{aligned}
By (8), for larger values of F_{11}, coefficients a_i for \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1, F_{11}=i\right) }, with 2 \le i \le n_{10}+1, can be bounded from considering only

\begin{aligned} a_i =\;&\frac{\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1, F_{11}=i\right) }}{2}. \end{aligned}
Then we can bound the sought expression as

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{11}, F_{0*} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{11}, F_{0*} = 1\right) }\\&\qquad +\; \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{22}, F_{*0} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{22}, F_{*0} = 1\right) }\\&\qquad +\; \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{12}, F_{00} = 1\right) }\\&\quad \le \xi \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) } + \sum _{i=0}^{n_{10}+1} a_i \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1,F_{11}=i\right) }\\&\quad = \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) } \left( \xi + \sum _{i=0}^{n_{10}+1} a_i \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=i\right) }\right) . \end{aligned}
We bound a_i by \max \{0, a_i\} to ensure non-negative terms, which enables us to use the upper bound on \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=i\right) } provided by Lemma 23:

\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=i\right) } \le \left( \frac{n_{11}}{n-c}\right) ^i \cdot \frac{c^i}{i!} \left( 1 - \frac{c}{n}\right) ^{n_{11}} \le \frac{c^i}{i!} \left( 1 - \frac{c}{n}\right) ^{n_{11}} \end{aligned}
where the last inequality follows from n_{11} \le n-c.

Thus,

\begin{aligned}&\mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) } \left( \xi + \sum _{i=0}^{n_{10}+1} a_i \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=i\right) }\right) \\&\quad \le \; \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) } \left( \xi + \sum _{i=0}^{n_{10}+1} \max \{a_i, 0\} \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=i\right) }\right) \\&\quad \le \; \frac{cn_{00}}{n} \left( 1 - \frac{c}{n}\right) ^{n_{00}-1} \left( \xi + \sum _{i=0}^{n_{10}+1} \max \{a_i, 0\} \cdot \frac{c^i}{i!} \cdot \left( 1 - \frac{c}{n}\right) ^{n_{11}}\right) \\&\quad =\; \frac{cn_{00}}{n} \left( 1 - \frac{c}{n}\right) ^{n_{00}+n_{11}-1} \left( \xi \left( 1 - \frac{c}{n}\right) ^{-n_{11}} + \sum _{i=0}^{n_{10}+1} \max \{a_i, 0\} \cdot \frac{c^i}{i!}\right) . \end{aligned}
Using

\begin{aligned}&\left( 1 - \frac{c}{n}\right) ^{n_{00}+n_{11}} = \left( 1 - \frac{c}{n}\right) ^{n-n_{10}-n_{01}} \le \; e^{-c} \left( 1+\frac{c}{n-c}\right) ^{n_{10}+n_{01}}\\&\quad \le \; e^{-c} \left( 1+\frac{2cn_{10}}{n-c}\right) , \end{aligned}
and recalling \xi \le O(1/\log n) we can write this as

\begin{aligned} \;&\frac{cn_{00}}{e^c n} \left( 1 + O(1/\log n)\right) \sum _{i=0}^{n_{10}+1} \max \{a_i, 0\} \cdot \frac{c^i}{i!} \end{aligned}
assuming that \max \{a_i, 0\} \cdot \frac{c^i}{i!} = \varOmega (1) (if it is 0 or o(1), we have proved the claim).

Now we obtain the claimed bound if we can show that

\begin{aligned} \sum _{i=0}^{n_{10}+1} \max \{a_i, 0\} \cdot \frac{c^i}{i!} \le 1 + \frac{2}{9} \cdot c. \end{aligned}	(9)
For n_{10}=0 we have a_0 = 1, a_1 = 2/9, hence (9) holds with equality for n_{10}=0. For 0 \le n_{10} \le 10 we evaluate the coefficients a_0, \dots , a_{n_{10}+1} with the closed formulas given above and verify (9) for c \le 2.71. The coefficients and the ranges of c for which (9) holds are shown in Table 2. Since we assume c \le 2.71, the inequality holds for all considered n_{10}.

For n_{10} > 10, we already established above that a_0 \le 0 and a_1 \le 0. By Lemma 21, a_i \le 0 for all 2 \le i \le n_{10}+1, except for i=n_{10}-1, where it is at most 5n_{10}^3 2^{-2n_{10}}/3. For n_{10} \ge 11, this is at most 0.00053 and thus we get an upper drift bound of

\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) } \left( \xi + 0.00053\right) = \frac{cn_{00}}{n} \cdot \left( \xi + 0.00053\right) \le \frac{c(2c+9)}{9e^c} \cdot \frac{n_{00}}{n} \cdot (1+O(1/\log n)) \end{aligned}
where the last inequality holds for c \le 6.09. This completes the proof for the case n_{10}=n_{01}.

Table 2 Values for coefficients a_i from the proof of Lemma 24 under P_{12}, F_{00}=1 and n_{10}=n_{01}, rounded to 4 digits
Full size table
Case {{\varvec{n}}}_{{{\varvec{10}}}}>{{\varvec{n}}}_{{{\varvec{01}}}}={{\varvec{0}}}: Now we consider unequal fitness, n_{10}>n_{01}, in the special case n_{01}=0 (no diversity). By Lemma 20

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{11}, F_{0*} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{11}, F_{0*} = 1\right) }\\&\quad =\; \frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{0*}=1\right) }}{4} \cdot \left( \frac{1}{3} + \frac{2}{3} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) }\right) \\&\quad =\; \frac{1}{12} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{0*}=1\right) } + \frac{1}{6} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{0*}=1,F_{1*}=0\right) } \end{aligned}
and

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{22}, F_{*0} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{22}, F_{*0} = 1\right) }\\&\quad =\; \frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{*0}=1\right) }}{4} \cdot \left( \frac{1}{3} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{*1} = 0\right) } + \frac{1}{6} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{*1}=1\right) }\right) \\&\quad =\; \frac{1}{12} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{*0}=1,F_{*1} = 0\right) } + \frac{1}{24} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{*0}=1,F_{*1}=1\right) }. \end{aligned}
Moving from events F_{0*}/F_{*0} and F_{1*}/F_{*1} to events F_{00} and F_{11} at the expense of an additive error term \xi ' \in [-O(1/\log n), +O(1/\log n)] as before and adding these two expressions shows that the first two lines of the formula from the statement are bounded by

\begin{aligned} \left( \frac{1}{12} + \xi '\right) \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) } + \frac{1}{4} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1, F_{11}=0\right) } + \frac{1}{24} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1, F_{11}=1\right) }. \end{aligned}
(10)
Note that the first of these summands is multiplied by \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) }, which does not include a statement on F_{11}.

Reusing the above calculations, the third term from the statement is

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{12}, F_{00} = 1\right) }\nonumber \\&\quad =\;\frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) }}{2} \sum _{i=0}^{n_{10}+1} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1, F_{11}=i\right) } \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=i\right) }. \end{aligned}
Using Lemma 14, the expectation simplifies to

\begin{aligned}&\;\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1, F_{11}=i\right) }\\&\quad =\; \mathord {{\mathrm {P}}}\mathord {\left( S> n_{10}+F_{11}\right) } \cdot \frac{2-2F_{11}-n_{10}}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10}+F_{11}\right) } \cdot \frac{1-F_{11}}{3}\\&\qquad + \mathord {{\mathrm {P}}}\mathord {\left( S > F_{11}\right) } \cdot \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = F_{11}\right) } \cdot \frac{1}{6}. \end{aligned}
Since n_{10}+n_{01}=n_{10} \le n_{10}+F_{11}, the event S > n_{10}+F_{11} is impossible. The event S = n_{10}+F_{11} is only possible for F_{11}=0, where \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10} + F_{11}\right) } = \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10}\right) } = \mathord {{\mathrm {P}}}\mathord {\left( S = 0\right) } by symmetry of the binomial distribution. For F_{11}=0 we thus get

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1, F_{11}=0\right) }&=\; \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10}\right) } \cdot \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S > 0\right) } \cdot \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = 0\right) } \cdot \frac{1}{6}\\&=\; \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = 0\right) } \cdot \frac{1}{6}. \end{aligned}
For all i > 0, we get

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1, F_{11}=i\right) } =\;&\mathord {{\mathrm {P}}}\mathord {\left( S > F_{11}\right) } \cdot \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = F_{11}\right) } \cdot \frac{1}{6}. \end{aligned}
Now we can bound the sought expression as

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{11}, F_{0*} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{11}, F_{0*} = 1\right) }\\&\qquad +\; \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{22}, F_{*0} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{22}, F_{*0} = 1\right) }\\&\qquad +\; \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{12}, F_{00} = 1\right) }\\&\quad \le \mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) } \left( \frac{1}{12} + \xi ' + \frac{1}{2}\sum _{i=0}^{n_{10}+1} a_i \mathord {{\mathrm {P}}}\mathord {\left( F_{11}=i\right) }\right) \end{aligned}
where, putting together (10) and the above bounds on \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1, F_{11}=i\right) },

\begin{aligned} a_0&=\; \frac{7}{12} + \mathord {{\mathrm {P}}}\mathord {\left( S = 0\right) } \cdot \frac{1}{6}\\ a_1&=\; \frac{1}{24} + \mathord {{\mathrm {P}}}\mathord {\left( S> 1\right) } \cdot \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = 1\right) } \cdot \frac{1}{6}\\ \forall i&\ge 2 :a_i =\; \mathord {{\mathrm {P}}}\mathord {\left( S > i\right) } \cdot \frac{1}{3} + \mathord {{\mathrm {P}}}\mathord {\left( S = i\right) } \cdot \frac{1}{6}. \end{aligned}
Since all coefficients are positive, we can bound \mathord {{\mathrm {P}}}\mathord {\left( F_{11} = i\right) } as before and obtain a drift bound of

\begin{aligned}&\frac{cn_{00}}{e^c n} \left( 1 + O(1/\log n)\right) \left( \frac{1}{12} + \xi ' + \frac{1}{2}\sum _{i=0}^{n_{10}+1} a_i \cdot \frac{c^i}{i!}\right) . \end{aligned}
Bounding a_0 \le 2/3 (since \mathord {{\mathrm {P}}}\mathord {\left( S = 0\right) } \le 1/2), a_1 \le 1/24 + 1/3 = 3/8 and a_i \le 1/3 for i \ge 2, we have

\begin{aligned} \sum _{i=0}^{n_{10}+1} a_i \cdot \frac{c^i}{i!}&\le \; \frac{2}{3} + \frac{3}{8} \cdot c + \frac{1}{3} \sum _{i=2}^{\infty } \frac{c^i}{i!}\\&=\; \frac{2}{3} + \frac{3}{8} \cdot c + \frac{1}{3} \cdot (e^c - c - 1)\\&=\; \frac{1}{3} + \frac{1}{24} \cdot c + \frac{1}{3} \cdot e^c. \end{aligned}
Plugging this in yields an upper drift bound of

\begin{aligned}&\frac{cn_{00}}{e^c n} \left( 1 + O(1/\log n)\right) \left( \frac{1}{12} + \xi ' + \frac{1}{6} + \frac{c}{48} + \frac{e^c}{6}\right) \\&\quad =\; \frac{cn_{00}}{e^c n} \left( \frac{1}{4} + \frac{c}{48} + \frac{e^c}{4} + O(1/\log n)\right) . \end{aligned}
Case {\varvec{n}}_{{\varvec{10}}}>{\varvec{n}}_{{\varvec{01}}}>{\varvec{0}}: Finally, we deal with the case n_{10}> n_{01} > 0. By Lemma 20

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{11}, F_{0*} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{11}, F_{0*} = 1\right) }\\&\quad =\; \frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{0*}=1\right) }}{4} \left( \left( 1 - \frac{n_{01}}{3}\right) \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) } + \left( \frac{1}{3} - \frac{n_{01}}{3}\right) \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=1\right) }\right) \\&\quad \le \; \frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{0*}=1\right) }}{4} \left( 1 - \frac{n_{01}}{3}\right) \mathord {{\mathrm {P}}}\mathord {\left( F_{1*}=0\right) }\\&\quad \le \; \frac{cn_{00}}{n} \left( 1 - \frac{c}{n}\right) ^{n-1} \left( \frac{1}{4} - \frac{n_{01}}{12}\right) . \end{aligned}
and

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{22}, F_{*0} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{22}, F_{*0} = 1\right) }\\&\quad =\; \frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{*0}=1\right) }}{4} \cdot \left( \frac{1}{3} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{*1} = 0\right) } + \frac{1}{6} \cdot \mathord {{\mathrm {P}}}\mathord {\left( F_{*1}=1\right) }\right) \\&\quad =\; \frac{cn_{00}}{n} \left( \left( 1 - \frac{c}{n}\right) ^{n-1} \frac{1}{12} + \frac{c(n_{11}+n_{01})}{n} \left( 1 - \frac{c}{n}\right) ^{n-1} \frac{1}{24}\right) \\&\quad \le \; \frac{cn_{00}}{n} \left( \left( 1 - \frac{c}{n}\right) ^{n-1} \frac{1}{12} + \left( 1 - \frac{c}{n}\right) ^{n} \frac{c}{24}\right) \\&\quad =\; \frac{cn_{00}}{n} \left( 1 - \frac{c}{n}\right) ^{n-1} \left( \frac{1}{12} + \frac{c}{24}\right) . \end{aligned}
We use Lemma 22 to bound the third term in the statement as

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1\right) } \cdot \mathord {{\mathrm {P}}}\mathord {\left( P_{12}, F_{00} = 1\right) }\\&\quad =\; \frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) }}{2} \cdot \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 1\right) }\\&\quad \le \; \frac{\mathord {{\mathrm {P}}}\mathord {\left( F_{00}=1\right) }}{3} \le \frac{cn_{00}}{n} \cdot \frac{1}{3}. \end{aligned}
Together, we get a drift bound of

\begin{aligned}&\left( \frac{1}{3} + \left( 1 - \frac{c}{n}\right) ^{n-1}\left( \frac{1}{3} - \frac{n_{10}}{12} + \frac{c}{24}\right) \right) \frac{cn_{00}}{n}\\&\quad \le \; \left( \frac{e^c}{3} + \frac{1}{4} + \frac{c}{24} + O(1/n)\right) \frac{cn_{00}}{e^c n}. \end{aligned}
\square

Putting Everything Together
Combining results from previous sections, for different numbers of flipping zeros, yields the following unconditional drift bound.

Lemma 25
If c \le 1.422, n_{10}+n_{01} \le \log ^2 n, n_{11} \ge n-n/\log ^3 n and n_{00} \ge \log ^5 n, then

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \right) } \le \frac{c(2c+9)}{9e^c} \cdot \frac{n_{00}}{n} \cdot (1+O(1/\log n)). \end{aligned}
Proof
This follows from adding drift bounds from Lemmas 10, 19 and 24. For n_{10}=n_{01}, Lemma 24 gives the stated bound. The terms O(n_{10}^2/n) = O((\log ^4 n)/n) from Lemma 19 and O(n_{0*}/(n\log n)) = O(n_{00}/(n \log n)) can both be absorbed in the O(1/\log n) term since n_{00}/n \ge \log ^5 n/n.

For n_{10}>n_{01}=0, the bound from Lemma 24 is at most the bound from the same lemma for n_{10}=n_{01} since

\begin{aligned} \frac{c}{e^c} \left( \frac{1}{4} + \frac{c}{48} + \frac{e^c}{4}\right) \le \frac{c(2c+9)}{9e^c} \end{aligned}
for c \le 1.422. Then the claim follows as above.

For n_{10}>n_{01}>0 note that Lemma 19 yields a negative upper bound of -n_{01}/8 \cdot (1-c/n)^n = -\varOmega (1). Since n_{00} \le n/\log ^3 n, we obtain a negative drift bound if n is large enough. \square

To translate the upper bound from Lemma 25 into a lower bound on the expected runtime, we use the lower-bound version of the multiplicative drift theorem from Theorem 4. This theorem requires an upper bound on the drift of the potential function and a sufficiently small probability for large jumps of this value. Such large jumps can occur if the two individuals of the (2+1) GA have a large Hamming distance. Recall that Lemma 9 shows this to be unlikely.

The following lemma shows that a drift of the potential can be translated into a lower bound on the expected optimisation time. This is not immediate since the potential function is a weighted combination of two quantities.

Lemma 26
Let N_t denote the number of n_{00}-bits at time t of the (2+1) GA and T the first point in time where n_{00}\le \log ^5 n. Assume that n_{01}+n_{10}\le \log ^2 n for all points in time before T, and N_0\ge \log ^5 n. If \mathord {{\mathrm {E}}}\mathord {\left( \varphi _{t+1}-\varphi _t\mid \varphi _t\right) } \le \delta N_t for some n^{-O(1)} \le \delta <1 and all t<T, then

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( T\mid \varphi _0\right) } \ge (1-O(1/\log n)) \frac{\ln (n)-O(\ln \ln n)}{\delta }. \end{aligned}
Proof
We introduce a distance function {\overline{\varphi }}_t :=n - \varphi _t = n_{00} + \frac{2}{3} n_{01} as the mirror image of our potential to obtain a function to be minimised, as required in Theorem 4. Moreover, we write \varDelta _t={\overline{\varphi }}_t-{\overline{\varphi }}_{t+1}. The key idea is to show that the statement on \mathord {{\mathrm {E}}}\mathord {\left( T\mid \varphi _0\right) } holds under the slightly different drift condition

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta _t\mid {\overline{\varphi }}_t\right) } \le \delta {\overline{\varphi }}_t \end{aligned}
(11)
and then to prove that the actual drift condition \mathord {{\mathrm {E}}}\mathord {\left( \varphi _{t+1}-\varphi _t\mid \varphi _t\right) } \le \delta N_t leads to the same result, up to lower-order terms.

We consider the process from the first point in time where {\overline{\varphi }}_t\le n/\log ^3 n, assume (11) to hold for all t<T and estimate the remaining expected time to minimise the distance {\overline{\varphi }}_t. Lemma 9 and the facts that at most \log n bits flip per generation and that the distance does not drop below n/\log ^3 n within the first \log ^2 n generations (each happening with probability 1-n^{-\varOmega (\log n)}), imply that {\overline{\varphi }}_{0} \ge n/(2\log ^3 n) with respect to our time count. We assume this to happen. By Lemma 9, with probability 1-n^{-\varOmega (\log n)} it holds that n_{10}+n_{01}\le \log ^2 n for any polynomial number of steps. Assuming this for a sufficiently long period obtained from applying Markov’s inequality on \mathord {{\mathrm {E}}}\mathord {\left( T\mid {\overline{\varphi }}_0\right) }, our assumptions only change the bound on the expected value by a 1-n^{-\varOmega (\log n)} factor.

Clearly, since n_{10}+n_{01}\le \log ^2 n, crossover followed by a neutral mutation can change the {\overline{\varphi }}-value by at most \log ^2 n. Moreover, each mutation flips k or more bits with probability at most

\begin{aligned} \left( {\begin{array}{c}n\\ k\end{array}}\right) \left( \frac{c}{n}\right) ^k\le \frac{c^k}{k!}; \end{aligned}
(12)
in particular it flips at most \log ^2 n bits with probability 1-n^{-\varOmega (\log n)}. Adding up these effects, we arrive at \mathord {{\mathrm {P}}}\mathord {\left( {\overline{\varphi }}_t-{\overline{\varphi }}_{t+1} \ge 2\log ^2 n\right) } = n^{-\varOmega (\log n)}. The time to minimise the distance function is no smaller than the time to reach a distance of at most x_{\min }:=\log ^5 n (we stop at this point to fulfill the condition on n_{00} in Lemma 25). Along with \beta :=2/\log n and using X_t:={\overline{\varphi }}_t, we verify the second condition of Theorem 4 by estimating

\begin{aligned}&\mathord {{\mathrm {P}}}\mathord {\left( X_t-X_{t+1}\ge \beta X_t\right) } \le \\&\quad \ \ \mathord {{\mathrm {P}}}\mathord {\left( X_t-X_{t+1}\ge \beta x_{\min }\right) } = \mathord {{\mathrm {P}}}\mathord {\left( X_t-X_{t+1}\ge 2\log ^2 n\right) } = n^{-\varOmega (\log n)}. \end{aligned}
Finally, we estimate this bound as n^{-\varOmega (n)} \le \beta \delta /\log n\le \beta \delta /\!\log (X_t) since both \beta and \delta are at least inversely polynomial in n. Hence, \mathord {{\mathrm {P}}}\mathord {\left( X_t-X_{t+1}\ge \beta X_t\right) }\le \beta \delta /\!\log (X_t), which satisfies the condition. Applying the theorem and recalling our assumption {\overline{\varphi }}_{0} \ge n/(2\log n),

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( T\mid X_0\right) }\ge \frac{1-\beta }{1+\beta } \cdot \frac{\ln ({\overline{\varphi }}_0/x_{\min })}{\delta } =\;&(1-O(1/\log n))\frac{\ln (n/\log ^4 n)}{\delta } \end{aligned}
which is (1-O(1/\log n))\frac{\ln (n)-O(\ln \ln n)}{\delta }. Recall that the unconditional expected time is only by a factor 1-n^{-\varOmega (\log n)} smaller.

Finally, we relate N_t to the distance function {\overline{\varphi }}_t and note first that {\overline{\varphi }}_t\ge N_t. Since, for t<T, our assumptions imply {\overline{\varphi }}_t=n_{00}+(2/3)n_{01} \le (1+1/\log n)N_t, the prerequisite

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varphi _{t+1}-\varphi _{t}\mid \varphi _t\right) } \le \delta N_t \end{aligned}
along with the fact \varDelta _t=\varphi _{t+1}-\varphi _{t} imply

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta _t \mid {\overline{\varphi }}_t\right) } \le \delta (1-1/\log n) {\overline{\varphi }}_t. \end{aligned}
Hence, (11) has been established with parameter \delta (1-1/\log n) so that

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( T\mid \varphi _0\right) }\ge (1-O(1/\log n))\frac{\ln (n/\log ^4 n)}{(1-1/\log n)\delta }, \end{aligned}
which is again (1-O(1/\log n))\frac{\ln (n/\log ^4 n)}{\delta } as claimed. \square

We are now ready to prove our main result.

Proof of Theorem 2
We apply drift analysis to a “typical” run, that is, a run where events mentioned in the following that happen with overwhelming probability do occur. The contrary is called a failure event and if a failure occurs, we pessimistically assume that the runtime is 0. By the law of total probability, the expected runtime is bounded from below by the expected runtime in a typical run, multiplied by the probability that no failure occurs.

With probability 1-2^{-\varOmega (n)} the initial population has a maximum number of (2/3)n one-bits. The random number of one-bits that crossover creates among the relevant n_{10} and n_{01} bits follows a binomial distribution with parameters n_{10}+n_{01} and 1/2. The expected value of this number equals (n_{10}+n_{01})/2\le n/2, and using Chernoff bounds (Theorem 5) with \delta =2n^{-1/3} and the upper bound n/2 on the expectation, the number of one-bits of every fixed offspring is at most U:=(n_{10}+n_{01})/2 + n^{2/3} with probability 1-2^{-\varOmega (n^{1/3})}. By a union bound, the probability that at least one offspring has more than U one-bits is still 2^{-\varOmega (n^{1/3})}. Since the fittest parent has at least (n_{10}+n_{01})/2 one-bits, this means that the number of one-bits at the relevant positions, and thereby the number of one-bits of the fitter offspring, grows by at most n^{2/3} with probability 1-2^{-\varOmega (n^{1/3})}. The same holds for each mutation with constant rate c as seen in (12). Hence, the subsequent 2\log ^2 n generations do not increase the ONEMAX-value to more than (3/4)n with probability 1-2^{-\varOmega (n^{1/3})}.

We consider the first point in time when n_{11} \ge n - n/\log ^3 n. By the same arguments as before, we then have n_{11} \le n-n/(2\log ^3 n) with overwhelming probability. Now Lemma 9 is in force, implying that we can apply Lemma 26 with

\begin{aligned} \delta \le \frac{c(2c+9)}{9e^c n} \cdot (1+O(1/\log n)), \end{aligned}
according to the drift bound from Lemma 25. Hence, if none of the above-mentioned failure events occur, the expected optimization time is bounded from below by

\begin{aligned} \left( 1-O\!\left( \frac{1}{\log n}\right) \right) \frac{9e^c n\ln (n)-O(n\ln \ln n)}{c(2c+9)}, \end{aligned}
which is \frac{9e^c}{c(2c+9)} \cdot n\ln (n) -O(n \ln \ln n) as claimed. This still holds when multiplying the above with the probability of no failure event occurring, as the union of all failure events is superpolynomially small.

Experiments
Our lower bound is tight up to lower-order terms. We ran experiments to see how close the results are to the dominant term of \frac{9e^c}{c(2c+9)} \cdot n \ln n, when varying c and n.

We first fixed n=1000 and varied c in steps of 0.1. For each value of c, we ran the (2+1) GA 10000 times to obtain smooth curves. For the range of c-values, c \in (0, 1.422], covered by Theorem 2, Fig. 1 shows that the empirical averages are very close to the dominant term. The vertical line indicates the c-value of \frac{\sqrt{97}-5}{4} \approx 1.2122 that was determined to be optimal (up to small-order terms) in Theorem 3.

Fig. 1
figure 1
Average optimisation times over 10,000 runs of the (2+1) GA on OneMax with n=1000 bits. The mutation rate is set to c/n with c \in \{0.05, 0.1, \ldots , 1.4\}. The thin blue lines show mean ± standard deviation (Color figure online)

Full size image
An obvious question arises: what if values of c are chosen that are larger than the limit 1.422 in Theorem 2? Fig. 2 shows results of the previous experiment for a larger range of c-values, c \in \{0.1, 0.2, 0.3, \dots , 5.0\}. The first vertical line in Fig. 2 again indicates the c-value of \frac{\sqrt{97}-5}{4} \approx 1.2122; the second vertical line indicates the limit of 1.422 from Theorem 2.

Fig. 2
figure 2
Average optimisation times over 10,000 runs of the (2+1) GA on OneMax with n=1000 bits. The mutation rate is set to c/n with c \in \{0.1, 0.2, \dots , 5.0\}. The thin blue lines show mean ± standard deviation (Color figure online)

Full size image
We see that as c increases beyond 1.422, a gap starts to appear between the dominant term and the average runtime. The average runtime seems to be smaller than \frac{9e^c}{c(2c+9)} \cdot n \ln n. This could indicate that the lower bound does not hold for c > 1.422, or that there are small-order terms that affect the plots for n=1000.

To see how quickly the runtime approaches the dominant term of \frac{9e^c}{c(2c+9)} \cdot n \ln n as n grows, we ran experiments with n increasing exponentially in powers of 2 from n=2^3=8 to n=2^{13}=8192. Figure 3 shows the normalised average runtimes, which is the runtime divided by n \ln n. One can see that, for both the default and the optimal mutation rates, c=1 and c=\frac{\sqrt{97}-5}{4}, respectively, the curves approach their respective dominant terms. However, the approach is quite slow. Even for n=8192 there is still a significant gap to the leading constant of the dominant term. This might indicate that the average runtime includes a significant, negative small-order term.

Recall that our lower bound (Theorem 2) includes a negative term of -O(n \log \log n), while the upper bound (Theorem 1) contains an additive term +O(n). The term -O(n \log \log n) is an artefact of the fact that we only considered around n/\log ^3 n fitness levels and we excluded the most difficult \log ^5 n ones. It is easy to show that optimising these fitness levels takes expected time O(n \log \log n) for both (1+1) EA and (2+1) GA. We conjecture that the expected running time is close to \frac{9e^{c}}{c(2c+9)} \cdot n \ln (n) + bn for some negative constant b. This is based on related work on the (1+1) EA whose expected runtime on ONEMAX is known to be en\ln (n) -1.8925\dots n + O(\log n) (cf. [14], who even determine the expected runtime up to additive errors of O(\log n/n)). Intuitively, the negative linear term appears in the bound for the (1+1) EA since the algorithm does not start with the largest possible value of the underlying potential function, which is the number of one-bits. More precisely, the initial number of one-bits X_0 has an expected value of n/2 and the multiplicative drift theorems both for lower and upper bounds involve the term \ln (X_0/x_{\min }). For X_0\approx n/2 this becomes roughly \ln (n) - \ln 2, so that the crucial term \ln (X_0/x_{\min })/\delta , where \delta =\varTheta (1/n), becomes at most (\ln n)/\delta - (\ln 2)/\delta = \varTheta (n\ln n)-O(n). The expected initial value of our potential function for the (2+1) GA equals (1/4+1/4+1/12)n = (7/12)n, so we believe there is a non-negligible negative term in the expression for the expected running as well.

Fig. 3
figure 3
Average optimisation times over 10,000 runs of the (2+1) GA and (1+1) EA on OneMax for different values of n and different mutation rates, normalised by dividing by n \ln n. The solid lines show the best fit obtained by a non-linear regression fit to a model of a n \ln (n) + bn, using the non-normalised running times. The dashed lines show the leading constants provided by theoretical results

Full size image
To see how closely the empirical data matches a conjectured bound with a negative linear term, we performed a non-linear regression to fit the average runtimes to a model of a n \ln (n) + bn, for parameters a, b. We used the software R, version 4.0.3, calling the nls command with starting values of a=1 and b=0. The results of the regression were values of a, b that minimised the residual sum of squares.

For the (2+1) GA with c=1, this resulted in a fitted function of 2.2381 n \ln (n) -0.7995n. The leading constant 2.2381 is close to the value 2.224 from our theoretical analysis. Likewise, for the optimal value c=\frac{\sqrt{97}-5}{4} \approx 1.2122, the best fit is for 2.1532 n \ln (n) -0.5883n and the leading constant is close to 2.18417 from our analysis and smaller than the leading constant for c=1. In both cases, the linear term has a negative sign. For comparison, the same regression for the (1+1) EA returned the best fit for the function 2.745 n \ln (n) -2.116n. The leading constant is close to the theoretically proven one of e=2.71828 and the coefficient of the linear term is also close to the theoretical one of -1.8925\dots [14].

Conclusions
Proving lower bounds for crossover-based GAs is a notoriously hard problem. We have provided such a lower bound for the (2+1) GA on ONEMAX through a careful analysis of a potential function that captures both the current best fitness and the potential for finding improvements through crossover combining different “building blocks” of good solutions. Our lower bound is tight up to small-order terms. This for the first time proves rigorously that populations are beneficial for standard steady-state genetic algorithms. We also identified the optimal mutation rate for the (2+1) GA as (\sqrt{97}-5)/(4n) for the considered range of mutation rates c/n.

Our lower bound applies for c \le 1.422 and an obvious open question is whether the leading constant in the expected runtime remains at 9e^c/(c(2c+9)) when this threshold is exceeded. Our empirical results suggest that the expected runtime is smaller than the stated bound, albeit it is not clear whether these results are affected by negative small-order terms. We conjecture that the runtime is very close to \frac{9e^c}{c(2c+9)} \cdot n \ln (n) + bn, where bn is a linear term with a negative constant b. Our drift estimates subsumed errors that were by a factor of O(1/\log n) smaller than the dominant term in asymptotic notation, hence tighter analyses would be required to obtain rigorous bounds on the conjectured linear term. Another avenue for future work would be to simplify our approach, possibly by exploiting that states with n_{10} > 1 are rarely reached in the late stages of a run, or to generalise our analysis to larger parent population sizes.

Notes
The term “evolutionary algorithms” is commonly used as an umbrella term for evolutionary algorithms with and without crossover. We use the term “Genetic Algorithm (GA)” to emphasize the use of crossover in an evolutionary algorithm.

Steady state GAs are those that replace at most a proper subset of the population in each generation (usually one new individual is created). Typically they use standard bit mutation which flips each bit independently with probability c/n.

In this informal discussion we ignore events of smaller probability (e. g. picking the same parent twice and creating the other population member by a lucky mutation).

References
Abramowitz, M., Stegun, I.A.: Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, tenth GPO printing edition Dover, Ninth Dover Printing, New York (1964)

MATH
 
Google Scholar
 

Auger, A., Doerr, B. (eds.): Theory of Randomized Search Heuristics. World Scientific Publishing, Springer (2011)

MATH
 
Google Scholar
 

Carvalho Pinto, E., Doerr, C.: A simple proof for the usefulness of crossover in black-box optimization. In: Auger, A., Fonseca, C.M., Lourenço, N., Machado, P., Paquete, L., Whitley, D. (eds) Parallel Problem Solving From Nature - PPSN XV, volume 11102 of LNCS, pp. 29–41. Springer (2018)

Chernoff, H.: A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. Ann. Math. Stat. 23, 493–507 (1952)

MathSciNet
 
Article
 
Google Scholar
 

Corus, D., Oliveto, P.S.: Standard steady state genetic algorithms can hillclimb faster than mutation-only evolutionary algorithms. IEEE Trans. Evol. Comput. 22(5), 720–732 (2018)

Article
 
Google Scholar
 

Corus, D., Oliveto, P.S.: On the benefits of populations for the exploitation speed of standard steady-state genetic algorithms. Algorithmica 82(12), 3676–3706 (2020)

MathSciNet
 
Article
 
Google Scholar
 

Dang, D.-C., Friedrich, T., Kötzing, T., Krejca, M.S., Lehre, P.K., Oliveto, P.S., Sudholt, D., Sutton, A.M.: Escaping local optima using crossover with emergent diversity. IEEE Trans. Evol. Comput. 22(3), 484–497 (2018)

Article
 
Google Scholar
 

Dang, D.-C., Friedrich, T., Krejca, M.S., Kötzing, T., Lehre, P.K., Oliveto, P.S., Sudholt, D., Sutton, A.M.: Escaping local optima with diversity-mechanisms and crossover. In: Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2016), pp. 645–652. ACM Press (2016)

Doerr, B.: Probabilistic tools for the analysis of randomized optimization heuristics. In: Doerr, B., Neumann, F. (eds) Theory of Evolutionary Computation–Recent Developments in Discrete Optimization, Natural Computing Series, pp. 1–87. Springer (2020)

Doerr, B., Happ, E., Klein, C.: Crossover can provably be useful in evolutionary computation. Theoret. Comput. Sci. 425, 17–33 (2012)

MathSciNet
 
Article
 
Google Scholar
 

Doerr, B., Neumann, F. (eds.): Theory of Evolutionary Computation-Recent Developments in Discrete Optimization. Springer, Natural Computing Series (2020)

Goldberg, D.E.: Genetic Algorithms in Search. Optimization and Machine Learning, Addison-Wesley Longman, LOndon (1989)

MATH
 
Google Scholar
 

Gruder, O.: The theory of risk. In: 9th International Congress of Actuaries, vol 2, pp. 222 (1930)

Hwang, H.-K., Panholzer, A., Rolin, N., Tsai, T.-H., Chen, W.-M.: Probabilistic analysis of the (1+1)-evolutionary algorithm. Evolut. Comput. 26(2) (2018)

Jansen, T.: Analyzing Evolutionary Algorithms—The Computer Science Perspective. Springer, Berlin (2013)

Book
 
Google Scholar
 

Jansen, T., Wegener, I.: On the analysis of evolutionary algorithms—a proof that crossover really can help. Algorithmica 34(1), 47–66 (2002)

MathSciNet
 
Article
 
Google Scholar
 

Johnson, N.L.: A note on the mean deviation of the binomial distribution. Biometrika 44(3–4), 532–533 (1957)

MathSciNet
 
Article
 
Google Scholar
 

Kötzing, T., Sudholt, D., Theile, M.: How crossover helps in pseudo-Boolean optimization. In: Proceedings of the 13th Annual Genetic and Evolutionary Computation Conference (GECCO 2011), pp. 989–996. ACM Press (2011)

Lehre, P.K., Witt, C.: Black-box search by unbiased variation. Algorithmica 64(4), 623–642 (2012)

MathSciNet
 
Article
 
Google Scholar
 

Lehre, P.K., Yao, X.: Crossover can be constructive when computing unique input–output sequences. Soft. Comput. 15(9), 1675–1687 (2011)

Article
 
Google Scholar
 

Lengler, J.: A general dichotomy of evolutionary algorithms on monotone functions. IEEE Trans. Evol. Comput. 24(6), 995–1009 (2020)

Article
 
Google Scholar
 

Neumann, F., Oliveto, P.S., Rudolph, G., Sudholt, D.: On the effectiveness of crossover for migration in parallel evolutionary algorithms. In: Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2011), pp. 1587–1594. ACM Press (2011)

Neumann, F., Witt, C.: Bioinspired Computation in Combinatorial Optimization—Algorithms and Their Computational Complexity. Springer, Berlin (2010)

MATH
 
Google Scholar
 

Oliveto, P.S., Sudholt, D., Witt, C.: A tight lower bound on the expected runtime of standard steady state genetic algorithms. In: Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2020), pp. 1323–1331. ACM (2020)

Oliveto, P.S., Witt, C.: On the runtime analysis of the simple genetic algorithm. Theoret. Comput. Sci. 545, 2–19 (2014)

MathSciNet
 
Article
 
Google Scholar
 

Oliveto, P.S., Witt, C.: Improved time complexity analysis of the simple genetic algorithm. Theoret. Comput. Sci. 605, 21–41 (2015)

MathSciNet
 
Article
 
Google Scholar
 

Sudholt, D.: How crossover speeds up building-block assembly in genetic algorithms. Evol. Comput. 25(2), 237–274 (2017)

MathSciNet
 
Article
 
Google Scholar
 

Sutton, A.M.: Fixed-parameter tractability of crossover: steady-state GAs on the closest string problem. Algorithmica 83(4), 1138–1163 (2021)

MathSciNet
 
Article
 
Google Scholar
 

Witt, C.: Tight bounds on the optimization time of a randomized search heuristic on linear functions. Comb. Probab. Comput. 22, 294–318 (2013)

MathSciNet
 
Article
 
Google Scholar
 

Download references

Acknowledgements
This work was initiated at the Dagstuhl Seminar 19431 and supported by the EPSRC under Grant EP/M004252/1.

Open Access
This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.

Author information
Affiliations
University of Sheffield, Sheffield, S1 4DP, UK

Pietro S. Oliveto & Dirk Sudholt

University of Passau, Passau, Germany

Dirk Sudholt

DTU Compute, Technical University of Denmark, Kongens Lyngby, Denmark

Carsten Witt

Corresponding author
Correspondence to Pietro S. Oliveto.

Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

Omitted Proofs
Omitted Proofs
In this appendix we present proofs omitted from the main part.

Proof of Lemma 12
Recall that Lemma 12 simplified the expected surplus of a binomially distributed random variable. To prove Lemma 12, we use an equation that is attributed to Gruder [13]. We therefore refer to it as Gruder’s equation.

Lemma 27
(Gruder’s equation [13]) For every 0 \le p \le 1 and all integers a, b,

\begin{aligned} \sum _{i=a}^b \left( {\begin{array}{c}b\\ i\end{array}}\right) p^i (1-p)^{b-i} (i-bp) = a\left( {\begin{array}{c}b\\ a\end{array}}\right) p^a (1-p)^{b-a+1}. \end{aligned}
Proof
The claim is trivial for a > b, hence we assume a \le b. The following proof is stated in [17], attributed to Gruder [13].

\begin{aligned} \sum _{i=a}^b \left( {\begin{array}{c}b\\ i\end{array}}\right) p^i (1-p)^{b-i} (i-bp)&=\; \sum _{i=a}^b \left( {\begin{array}{c}b\\ i\end{array}}\right) p^i (1-p)^{b-i} (i(1-p) - (b-i)p)\\&=\; \sum _{i=a}^b \left( b\left( {\begin{array}{c}b-a\\ i-1\end{array}}\right) p^i (1-p)^{b-i+1} \right. \\&\quad \left. - b\left( {\begin{array}{c}b-1\\ i\end{array}}\right) p^{i+1} (1-p)^{b-i}\right) \\&=\; b \left( {\begin{array}{c}b-1\\ a-1\end{array}}\right) p^{a} (1-p)^{b-a+1}\\&=\; a\left( {\begin{array}{c}b\\ a\end{array}}\right) p^a (1-p)^{b-a+1}. \end{aligned}
\square

We use Gruder’s equation to prove Lemma 12.

Proof of Lemma 12
\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( S \mid S \ge \ell \right) } \mathord {{\mathrm {P}}}\mathord {\left( S \ge \ell \right) }&=\; \sum _{s = \ell }^{n} s \cdot \mathord {{\mathrm {P}}}\mathord {\left( S = s\right) }\\&=\; \sum _{s = \ell }^{n} \left( s-\frac{n}{2}\right) \cdot \mathord {{\mathrm {P}}}\mathord {\left( S = s\right) } + \frac{n}{2} \cdot \mathord {{\mathrm {P}}}\mathord {\left( S \ge \ell \right) }\\&=\; \sum _{s = \ell }^{n} \left( {\begin{array}{c}n\\ s\end{array}}\right) 2^{-n_{10}-n_{01}} \cdot \left( s-\frac{n}{2}\right) + \frac{n}{2} \cdot \mathord {{\mathrm {P}}}\mathord {\left( S \ge \ell \right) }. \end{aligned}
Applying Gruder’s equation (Lemma 27) with p = 1/2, a=\ell , b=n yields

\begin{aligned}&=\; \ell \left( {\begin{array}{c}n\\ \ell \end{array}}\right) 2^{-n-1} + \frac{n}{2} \cdot \mathord {{\mathrm {P}}}\mathord {\left( S \ge \ell \right) }\\&=\; \frac{\ell }{2} \cdot \mathord {{\mathrm {P}}}\mathord {\left( S = \ell \right) } + \frac{n}{2} \cdot \mathord {{\mathrm {P}}}\mathord {\left( S \ge \ell \right) }. \end{aligned}
Proof of Lemma 16
Recall that Lemma 16 claims that if F_{00}=0 and F_{11} \ge 2 then under P_{12} the potential drift is non-positive.

Proof of Lemma 16
By Lemma 13,

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00} = 0, F_{11}=i\right) }\\&\quad =\; \left( \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10} + i\right) } \cdot \frac{17n_{10}+13i}{36} - \mathord {{\mathrm {P}}}\mathord {\left( S \ge n_{10} + i\right) } \cdot \frac{6n_{10} + 24i}{36}\right) . \end{aligned}
The above is 0 for i \ge n_{10}, hence the claim is trivial for n_{10} \le 2. We assume n_{10} \ge 3 in the following.

The claim is equivalent to

\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10} + i\right) } \cdot \frac{17n_{10}+13i}{36} \le \mathord {{\mathrm {P}}}\mathord {\left( S \ge n_{10} + i\right) } \cdot \frac{6n_{10} + 24i}{36}. \end{aligned}
Multiplying by 36 \cdot 2^{2n_{10}} and spelling out the binomial distributions involving S yields

\begin{aligned} \left( {\begin{array}{c}2n_{10}\\ n_{10}+i\end{array}}\right) \cdot (17n_{10}+13i) \le \sum _{j=i}^{n_{10}} \left( {\begin{array}{c}2n_{10}\\ n_{10}+j\end{array}}\right) \cdot (6n_{10} + 24i). \end{aligned}
Dividing both sides by \left( {\begin{array}{c}2n_{10}\\ n_{10}+i\end{array}}\right) yields

\begin{aligned} (17n_{10}+13i) \le \sum _{j=i}^{n_{10}} \frac{(n_{10}+i)!(n_{10}-i)!}{(n_{10}+j)!(n_{10}-j)!} \cdot (6n_{10} + 24i). \end{aligned}
Note that this is equivalent to

\begin{aligned} (11n_{10}-11i) \le \sum _{j=i+1}^{n_{10}} \frac{(n_{10}+i)!(n_{10}-i)!}{(n_{10}+j)!(n_{10}-j)!} \cdot (6n_{10} + 24i) \end{aligned}
which holds trivially for i=n_{10}. For i=n_{10}-1, the above simplifies to

\begin{aligned} 11 \le \frac{(2n_{10}-1)!}{(2n_{10})!} \cdot (6n_{10} + 24n_{10}-24) \Leftrightarrow 11 \le 15 - \frac{12}{n_{10}} \end{aligned}
which is true for n_{10} \ge 3. For i \le n_{10}-2, we have

\begin{aligned}&\sum _{j=i+1}^{n_{10}} \frac{(n_{10}+i)!(n_{10}-i)!}{(n_{10}+j)!(n_{10}-j)!} \cdot (6n_{10} + 24i)\\&\quad \ge \; \sum _{j=i+1}^{i+2} \frac{(n_{10}+i)!(n_{10}-i)!}{(n_{10}+j)!(n_{10}-j)!} \cdot (6n_{10} + 24i)\\&\quad =\; \left( \frac{(n_{10}+i)!(n_{10}-i)!}{(n_{10}+i+1)!(n_{10}-i-1)!} + \frac{(n_{10}+i)!(n_{10}-i)!}{(n_{10}+i+2)!(n_{10}-i-2)!}\right) \cdot (6n_{10} + 24i)\\&\quad =\; \left( \frac{n_{10}-i}{n_{10}+i+1} + \frac{(n_{10}-i)(n_{10}-i-1)}{(n_{10}+i+1)(n_{10}+i+2)}\right) \cdot (6n_{10} + 24i)\\&\quad =\; (n_{10}-i)\left( \frac{1}{n_{10}+i+1} + \frac{n_{10}-i-1}{(n_{10}+i+1)(n_{10}+i+2)}\right) \cdot (6n_{10} + 24i)\\&\quad =\; (n_{10}-i)\left( \frac{2n_{10}+1}{(n_{10}+i+1)(n_{10}+i+2)}\right) \cdot (6n_{10} + 24i)\\&\quad =\; 6(n_{10}-i) \cdot \frac{(2n_{10}+1)(n_{10} + 4i)}{(n_{10}+i+1)(n_{10}+i+2)}. \end{aligned}
Now the claim follows if we can show that the above fraction is at least 11/6.

\begin{aligned}&\frac{(2n_{10}+1)(n_{10} + 4i)}{(n_{10}+i+1)(n_{10}+i+2)}\\&\quad =\; \frac{(n_{10}+i+1)(n_{10}+i+2)+(n_{10}-i)(n_{10}+i+2)+(3i-2)(2n_{10}+1)}{(n_{10}+i+1)(n_{10}+i+2)}\\&\quad =\; 1 + \frac{(n_{10}-i)(n_{10}+i+2)+(3i-2)(2n_{10}+1)}{(n_{10}+i+1)(n_{10}+i+2)}\\&\quad \ge \; 1 + \frac{(n_{10}-i)(n_{10}+i+2)+(3i-2)(n_{10}+i+2)}{(n_{10}+i+1)(n_{10}+i+2)}\\&\quad =\; 1 + \frac{n_{10}+2i-2}{n_{10}+i+1}\\&\quad \ge \; 1 + \frac{n_{10}+2}{n_{10}+3}\\&\quad \ge \; 1 + \frac{5}{6} = \frac{11}{6}, \end{aligned}
where in the last two inequalities we have used i \ge 2 and n_{10} \ge 3.

Proof of Lemma 18
Table 3 Numerical values for the potential drift given P_{12}, n_{10} > n_{01} and F_{00}=F_{11}=0, rounded to 4 digits
Full size table
Table 4 Numerical values for the potential drift given P_{12}, n_{10} > n_{01} and F_{00}=1, F_{11}=0, rounded to 4 digits
Full size table
Proof of Lemma 18
We investigate the formula

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=0, F_{11}\right) }&= \mathord {{\mathrm {P}}}\mathord {\left( S> \ell _1\right) } \cdot \frac{-2F_{11}-n_{10}+n_{01}}{3}\nonumber \\&\quad + \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _1\right) } \cdot \frac{-F_{11}+n_{01}}{3}\nonumber \\&\quad + \mathord {{\mathrm {P}}}\mathord {\left( S > \ell _2\right) } \cdot \frac{ - n_{01}}{6}. \end{aligned}
(13)
that follows from Lemma 14 for F_{00}=0. For n_{10}<12 the claim is verified numerically for all pairs n_{01}<n_{10}, see Table 3. We now turn to an analytical argument for n_{10}\ge 12.

Since the first term of the rhs. in (13) is clearly negative, we omit it in the following. We use our assumption n_{10}\ge 12. The aim is to compare different probabilities occurring in (13) to each other. More precisely, we want to show the claim that \mathord {{\mathrm {P}}}\mathord {\left( S > \ell _2\right) } \ge 2\mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1\right) }. Along with the fact that the third term is clearly negative, we then have

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=0, F_{11}\right) }&\le \; \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _1\right) } \left( \frac{-F_{11} + n_{01}}{3} \right) \\&\quad + 2\mathord {{\mathrm {P}}}\mathord {\left( S = \ell _1\right) } \left( \frac{-n_{01}}{6}\right) \\&\le \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _1\right) } \left( \frac{-F_{11}}{3} + \frac{n_{01}}{3} - \frac{n_{01}}{3}\right) \le 0 \end{aligned}
since F_{11}\ge 0.

Hence, we are left with the claim. We first assume n_{10}\ge n_{01}+2 and treat the case n_{10}=n_{01}-1 separately below. Note that nothing is to show if \ell _1 > n_{10}+n_{01} since then only negative terms have non-zero probability. Hence, we assume \ell _1\le n_{10}+n_{01} in the following. Since n_{01}\le n_{10}-2 and F_{00}=0, we have \ell _1 = n_{10} - F_{00} + F_{11} \ge (n_{10}+n_{01})/2 + 1 and \ell _2 = n_{10} - F_{00} + F_{11} \le \ell _1 -2. Since S follows a binomial distribution with parameters n_{10}+n_{01} and 1/2, we have shown \ell _1\ge \mathord {{\mathrm {E}}}\mathord {\left( S\right) }+1. Using the well-known monotonicity of the binomial distribution, we have \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1-1\right) } \ge \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1\right) }, and, since \mathord {{\mathrm {P}}}\mathord {\left( S>\ell _2\right) } \ge \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _1\right) } + \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1-1\right) }, we obtain \mathord {{\mathrm {P}}}\mathord {\left( S>\ell _2\right) }\ge 2\mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1\right) } as claimed in the case n_{10}\ge n_{01}+2.

If n_{10}=n_{01}+1 we have to argue more carefully. A relatively straightforward subcase (for n_{10}\ge 12) is F_{11}=0. Then \ell _2=n_{01} and therefore

\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( S > \ell _2\right) } = \mathord {{\mathrm {P}}}\mathord {\left( S \ge n_{01}+1\right) } \ge \mathord {{\mathrm {P}}}\mathord {\left( S\ge (n_{10}+n_{01})/2\right) } \ge \frac{1}{2} \end{aligned}
by symmetry of the binomial distribution \mathrm {Bin}(n_{10}+n_{01}, 1/2). Also, for any k\in \{0,\dots ,n_{10}+n_{01}\}, we have

\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( S=k\right) } \le \frac{1}{\sqrt{\pi (n_{10}+n_{01})/2}} \le \frac{1}{\sqrt{\pi n_{10}/2}} \le 0.231 \end{aligned}
according to Lemma 7 for n_{10}\ge 12, which proves \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1\right) }\le 1/4 and, therefore, again \mathord {{\mathrm {P}}}\mathord {\left( S>\ell _2\right) }\ge 2\mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1\right) } as above.

We are left with n_{10}=n_{01}+1 and F_{11}>0. Here we take into account the first line of (13) and obtain due to \ell _1=\ell _2+1 that

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=0, F_{11}\right) }&=\; \mathord {{\mathrm {P}}}\mathord {\left( S > \ell _1\right) } \cdot \frac{-2F_{11}-n_{10}+n_{01}}{3}\nonumber \\&\quad + \mathord {{\mathrm {P}}}\mathord {\left( S = \ell _1\right) } \cdot \frac{-F_{11}+n_{01}}{3}\nonumber \\&\quad + \mathord {{\mathrm {P}}}\mathord {\left( S \ge \ell _1\right) } \cdot \frac{ - n_{01}}{6} \nonumber \\&\le \; \Pr (S=\ell _1+1) \left( -\frac{2F11}{3}-\frac{n_{01}}{6}\right) \nonumber \\&\quad + \Pr (S=\ell _1) \left( -\frac{F_{11}}{3}+\frac{n_{01}}{6}\right) . \end{aligned}
(14)
We will show that \Pr (S=\ell _1+1)/\Pr (S=\ell _1) = \alpha for some sufficiently large \alpha and obtain

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=0, F_{11}\right) } \le \;\Pr (S=\ell _1)\left( -\frac{2\alpha F_{11}}{3}-\frac{\alpha n_{01}}{6} -\frac{F_{11}}{3}+\frac{n_{01}}{6} \right) . \end{aligned}
(15)
The ratio \alpha will reflect the following trade-off: if F_{11} is small then \ell _1=n_{10}+F_{11} is close to the middle value of the binomial distribution with parameters N:=n_{10}+n_{01} and \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1+1\right) } is not much smaller than \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1\right) }. Then the positive contribution of n_{01}/6 can almost be compensated by the negative -\alpha n_{01}/6. However, if F_{11} is big then \alpha must be relatively small due to the tail of the binomial distribution. Then we exploit that the negative terms involving F_{11} make the drift bound small.

We now consider several ranges for F_{11} to relate the drift and \alpha . The first range is F_{11}\ge n_{01}/2. Then have (even without bounding \alpha ) (Table 4)

\begin{aligned} \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=0, F_{11}\right) }&\le \Pr (S=\ell _1)\left( -\frac{n_{01}/2}{3}+\frac{n_{01}}{6} \right) \le 0. \end{aligned}
If F_{11}<n_{01}/2 and F_{11}\ge 1 then we have

\begin{aligned} \frac{n_{01}}{k^*+1} \le F_{11} < \frac{n_{01}}{k^*} \end{aligned}
for some k^*\ge 2. Plugging the lower bound into (15), we have

\begin{aligned} \nonumber \mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=0, F_{11}\right) } \le \;\Pr (S=\ell _1)\left( -\frac{2\alpha n_{01}}{3(k^*+1)}-\frac{\alpha n_{01}}{6} -\frac{n_{01}}{3(k^*+1)}+\frac{n_{01}}{6} \right) . \end{aligned}
The bracket equals

\begin{aligned} \frac{-4\alpha -\alpha (k^*+1)-2+(k^*+1)}{6(k^*+1)} \cdot n_{01}, \end{aligned}
which becomes non-positive (and along with it also our drift bound) for

\begin{aligned} \alpha \ge \frac{k^*-1}{k^*+5}. \end{aligned}
Hence, we are left with establishing this bound on \alpha . Note that, since n_{01}=n_{10}-1, the sum N:=n_{10}+n_{01} is odd, and it holds that

\begin{aligned} \ell _1\le n_{10}+\frac{n_{01}}{k^*} = \left( \frac{N}{2}+\frac{1}{2}\right) + \frac{\left( \frac{N}{2}-\frac{1}{2}\right) }{k^*} \le \left( \frac{N}{2}+\frac{1}{2}\right) + \frac{N}{2k^*} = \left( \frac{1}{2}+\frac{1}{2k^*}\right) N + \frac{1}{2}. \end{aligned}
Using the representation \mathord {{\mathrm {P}}}\mathord {\left( S=k\right) }=\left( {\begin{array}{c}N\\ k\end{array}}\right) 2^{-N}, and the identity

\begin{aligned} \left( {\begin{array}{c}a\\ b+1\end{array}}\right) \biggm / \left( {\begin{array}{c}a\\ b\end{array}}\right) = \frac{a-b}{b+1}, \end{aligned}
we have \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1+1\right) } \ge \mathord {{\mathrm {P}}}\mathord {\left( S=\ell _1\right) } \frac{N/2-N/(2k^*)-1/2}{N/2+N/(2k^*)+1/2+1}, hence

\begin{aligned} \alpha = \frac{N/2-N/(2k^*)-1/2}{N/2+N/(2k^*)+3/2} = \frac{k^*-1-k^*/N}{k^*+1+3k^*/N} \ge \frac{k^*-1-k^*/12}{k^*+1+k^*/4}, \end{aligned}
where we used N\ge 12. Now,

\begin{aligned} \alpha - \frac{k^*-1}{k^*+5} \ge \frac{(11/12)k^*-1}{(5/4)k^*+1} - \frac{k^*-1}{k^*+5} = \frac{11(k^*)^2 - 23k^* + 12}{15(k^*)^2 + 87k^* + 60} \ge 0 \end{aligned}
for k^*\ge 1, which proves \alpha \ge \frac{k^*-1}{k^*+5} as required. \square

Proof of Lemma 21
Lemma 21 is similar to Lemma 16, saying that the drift is non-positive if at least 2 1-bits flip. Strangely enough, this holds for all i \ge 2, except for i=n_{10}-1 where the drift is exponentially small in n_{10}.

Proof of Lemma 21
Using Lemma 13,

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=1, F_{11}=i\right) }\nonumber \\&\quad = \mathord {{\mathrm {P}}}\mathord {\left( S \ge n_{10} - 1 + i\right) } \cdot \left( 1 - \frac{2i}{3} - \frac{n_{10}}{6} \right) + \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10} - 1 + i\right) } \cdot \frac{17n_{10}-17+13i}{36} \nonumber \\&\quad = \mathord {{\mathrm {P}}}\mathord {\left( S \ge n_{10} - 1 + i\right) } \cdot \frac{36-24i-6n_{10}}{36} + \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10} - 1 + i\right) } \cdot \frac{17n_{10}-17+13i}{36}. \end{aligned}
(16)
Noting that the first fraction is negative and the second one positive because of i\ge 2, the whole expression is non-positive if

\begin{aligned} \alpha :=\frac{\mathord {{\mathrm {P}}}\mathord {\left( S \ge n_{10} - 1 + i\right) }}{\mathord {{\mathrm {P}}}\mathord {\left( S = n_{10} - 1 + i\right) }} \ge \frac{17-13i-17n_{10}}{36-24i-6n_{10}}. \end{aligned}
(17)
We will now bound \alpha depending on i. It is easy to see that \frac{17-13i-17n_{10}}{36-24i-6n_{10}} < 1 for i\ge n_{10} while trivially \alpha \ge 1. This proves the statement for i\ge n_{10}. Hence, we now focus on 2\le i \le n_{10} - 2 and write i=n_{10}/r for some r\in [n_{10}/(n_{10}-2),n_{10}/2]. Then

\begin{aligned} \frac{17-13i-17n_{10}}{36-24i-6n_{10}} = \frac{17-13n_{10}/r-17n_{10}}{36-24n_{10}/r-6n_{10}} = \frac{13+17r-17r/n_{10}}{24+6r-36r/n_{10}}. \end{aligned}
We now use a similar argument as in the proof of Lemma 18. Since n_{10}+i = (1+1/r)n_{10} and S is based on a binomial distribution with 2n_{10} trials and success probability 1/2, we obtain that

\begin{aligned} \mathord {{\mathrm {P}}}\mathord {\left( S = n_{10} + i\right) }/\mathord {{\mathrm {P}}}\mathord {\left( S=n_{10}+i-1\right) }= & {} \left( {\begin{array}{c}2n_{10}\\ (1+1/r)n_{10}\end{array}}\right) \biggm /\left( {\begin{array}{c}2n_{10}\\ (1+1/r)n_{10}-1\end{array}}\right) \\= & {} \frac{r-1+r/n_{10}}{r+1} \end{aligned}
using the identity

\begin{aligned} \left( {\begin{array}{c}a\\ b+1\end{array}}\right) \biggm / \left( {\begin{array}{c}a\\ b\end{array}}\right) = \frac{a-b}{b+1} \end{aligned}
and, similarly,

\begin{aligned} \frac{\mathord {{\mathrm {P}}}\mathord {\left( S = n_{10} + i + 1\right) }}{\mathord {{\mathrm {P}}}\mathord {\left( S=n_{10}+i\right) }} = \frac{r-1}{r+1+r/n_{10}} \end{aligned}
as well as

\begin{aligned} \frac{\mathord {{\mathrm {P}}}\mathord {\left( S = n_{10} + i + 2\right) }}{\mathord {{\mathrm {P}}}\mathord {\left( S=n_{10}+i+1\right) }} = \frac{r-1-r/n_{10}}{r+1+2r/n_{10}}. \end{aligned}
Hence, since i\le n_{10}-2

\begin{aligned}&\frac{\mathord {{\mathrm {P}}}\mathord {\left( S\ge n_{10} + i - 1\right) }}{ \mathord {{\mathrm {P}}}\mathord {\left( S=n_{10} + i - 1\right) }}\\&\quad \ge \left( 1+\frac{r-1+r/n_{10}}{r+1}+\frac{r-1+r/n_{10}}{r+1}\cdot \frac{r-1}{r+1+r/n_{10}}\right. \\&\qquad \left. \mathrel {+}\frac{r-1+r/n_{10}}{r+1}\cdot \frac{r-1}{r+1+r/n_{10}} \cdot \frac{r-1-r/n_{10}}{r+1+2r/n_{10}} \right) =:q, \end{aligned}
in other words \alpha \ge q, with equality holding for i=n_{10}-2.

We now show q - \frac{13+17r-17r/N}{24+6r-36r/N}\ge 0 for the desired ranges of N:=n_{10} and i, which implies (17). Simple algebraic manipulations (supported by a computer algebra system) yield that


Clearly, the denominator of the last fraction is positive for N\ge 6. We now prove that its numerator, hereinafter called \nu (r,N), is positive for N\ge 12 and N/(N-2)\le r\le N/2. To see this, we will compute its first and second partial derivative with respect to r:

\begin{aligned} \frac{\partial }{\partial r}\nu (r,N)&=\; (28r^3 + 96r^2 - 132r + 40)N^4 + (-472r^3 + 522r^2 - 228r + 26)N^3\\&\quad \mathrel {+} (-1180r^3 + 306r^2 + 2r)N^2 + (-968r^3 + 30r^2)N - 288r^3 \end{aligned}
and

\begin{aligned} \frac{\partial ^2}{\partial ^2 r}\nu (r,N)&=\; (84r^2 + 192r - 132)N^4 + (-1416r^2 + 1044r - 228)N^3\\&\quad \mathrel {+} (-3540r^2 + 612r + 2)N^2 + (-2904r^2 + 60r)N - 864r^2. \end{aligned}
If we evaluate \nu (r,N) and its first and second partial derivative at the minimum viable r=N/(N-2), we have

\begin{aligned} \nu (N/(N-2),N)= & {} \frac{8N^4(4N^3 - 42N^2 - 61N - 63)}{(N - 2)^4} \\ \frac{\partial }{\partial r}\nu (r,N)\biggm |_{r=N/(N-2)}= & {} \frac{2N^3(16N^4 - 28N^3 - 604N^2 - 1239N - 274)}{(N - 2)^3} \end{aligned}
and

\begin{aligned} \frac{\partial ^2}{\partial ^2r}\nu (r,N)\biggm |_{r=N/(N-2)} = \frac{2N^2(72N^4 - 228N^3 - 2315N^2 - 2494N - 488)}{(N - 2)^2}, \end{aligned}
all of which are clearly non-decreasing for N\ge 2 (since the respective lead coefficients are positive and all other coefficients negative) and verified as positive (with values 1144.6272, 626482.9440 and 2117468.1601, respectively) for all N\ge 12, without claiming this to be tight in all three cases. Hence, if N\ge 12, \nu (r,N) is positive at r=N/(N-2) and increases with r at this point. If the second partial derivative is positive for all r\in [N/(N-2),N/2] then \nu (r,N) is monotone increasing on that interval. Otherwise the second derivative, a quadratic function, will be monotone increasing until its maximum at some r\in [N/(N-2),N/2] and then monotone decreasing. In particular this means that \nu (r,N) has at most one root right of r=N/(N-2). We evaluate \nu (N/2,N), i. e., at the maximum r-value considered and obtain the polynomial

\begin{aligned} \frac{7}{16} N^8 - \frac{27}{8} N^7 - \frac{211}{16} N^6 - \frac{87}{8} N^5 - 3 N^4, \end{aligned}
which is non-decreasing in N. Now, \nu (N/2,N) = 1144.6272 > 0 for N=12. Due to the monotonicity of \nu (N/2,N), this holds for all greater N as well, and together with the observations regarding monotonicity and roots in the interval [N/(N-2),N/2], this concludes the proof that \nu (r,N)\ge 0 for N\ge 12 and N/(N-2)\le r\le N/2, implying (17).

Finally, for i=n_{10}-1 and n_{10} \ge 10 the formula simplifies to

\begin{aligned}&\mathord {{\mathrm {E}}}\mathord {\left( \varDelta \mid P_{12}, F_{00}=1, F_{11}=n_{10}-1\right) }\\&\quad =\; \mathord {{\mathrm {P}}}\mathord {\left( S \ge 2n_{10} - 2\right) } \cdot \left( 1 - \frac{2(n_{10}-1)}{3} - \frac{n_{10}}{6} \right) \\&\qquad + \mathord {{\mathrm {P}}}\mathord {\left( S = 2n_{10} - 2\right) } \cdot \frac{17n_{10}-17+13(n_{10}-1)}{36}\\&\quad \le \; \mathord {{\mathrm {P}}}\mathord {\left( S = 2n_{10} - 2\right) } \cdot \frac{30n_{10}-30}{36}\\&\quad \le \; \mathord {{\mathrm {P}}}\mathord {\left( S = 2n_{10} - 2\right) } \cdot \frac{5n_{10}}{6}\\&\quad =\; \left( {\begin{array}{c}2n_{10}\\ 2\end{array}}\right) \cdot 2^{-2n_{10}} \cdot \frac{5n_{10}}{6}\\&\quad \le \; \frac{(2n_{10})^2}{2} \cdot 2^{-2n_{10}} \cdot \frac{5n_{10}}{6}\\&\quad =\; \frac{5n_{10}^3}{3} \cdot 2^{-2n_{10}}. \end{aligned}
\square