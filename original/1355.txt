Runtime analysis aims at contributing to our understanding of evolutionary algorithms through mathematical analyses of their runtimes. In the context of discrete optimization problems, runtime analysis classically studies the time needed to find an optimal solution. However, both from a practical and from a theoretical viewpoint, more fine-grained performance measures are needed to gain a more detailed understanding of the main working principles and their resulting performance implications. Two complementary approaches have been suggested: fixed-budget analyses and fixed-target analyses. In this work, we conduct an in-depth study on the advantages and the limitations of fixed-target analyses. We show that, different from fixed-budget analyses, many classical methods from the runtime analysis of discrete evolutionary algorithms yield fixed-target results without greater effort. We use this to conduct a number of new fixed-target analyses. However, we also point out examples where an extension of existing runtime results to fixed-target results is highly non-trivial.

Introduction
The classic performance measure in the theory of evolutionary computation [21, 28] is optimization time, that is, the number of fitness evaluations that an algorithm uses to find an optimal solution for a given optimization problem. Often only expected optimization times are analyzed and reported, either for reasons of simplicity or because some analysis methods like certain drift theorems [38] only yield such bounds.

Some works give more detailed information, e.g., the expectation together with a tail estimate [15, 33, 50]. In some situations, only runtime bounds that hold with some or with high probability are given, either because these are easier to prove or more meaningful (see, e.g., the discussion in [8] on such statements for estimation-of-distribution algorithms), or because the expectation is unknown [22] or infinite. The use of the notion of stochastic domination [7] is another way to give more detailed information on runtimes of algorithms.

Nevertheless, all these approaches reduce the whole optimization process to a single point in time: the moment in which an optimal solution is found. For various reasons, more detailed information on the whole process is also desirable, including the following:

(1)
Evolutionary algorithms, different from most classic algorithms, are so-called anytime algorithms. This means that they can be interrupted at essentially any point of time and they still provide some valid solution (possibly of a low quality). The optimization time as the only performance measure gives no information on how good an algorithm is as an anytime algorithm. Such information, however, is of great interest in practice. It can be used, for instance, if one does not know in advance how much time can be allocated to the execution of an algorithm, or when it is important to report whenever a certain milestone (e.g., quality threshold) has been reached.

(2)
When several iterative optimization heuristics are available to solve a problem, one can try to start the optimization with one heuristic and then, at a suitable time, switch to another one which becomes more powerful at that time. To decide which heuristic to use up to a certain point of time or solution quality, more detailed information than the optimization time is needed.

We note that the importance of reporting runtime profiles instead of only optimization times has for a long time been recognized in algorithm benchmarking [23, 25]. These fine-grained performance analyses have helped to advance our understanding of evolutionary computation methods, and have contributed significantly to algorithm development. It is therefore surprising that such more fine-grained notions play only a marginal role in the runtime analysis literature. The following two notions have been used in the runtime analysis community.

Fixed-budget analyses For a fixed number (“budget”) of fitness evaluations, one studies the (usually expected) quality of the best solution found within this budget.

Fixed-target analyses For a fixed quality threshold, one studies the (usually expected) time (often measured in terms of function evaluations) needed to find a solution of at least this quality.

The main goal of this work is a systematic fixed-target runtime analysis. We provide, in particular, a comparison of different more fine-grained performance measures (Sect. 2), a survey of the existing results (Sect. 4), an analysis how the existing methods to prove runtime analysis results can be used to also give fixed-target results (Sects. 5 and 6 ) together with several applications of these methods, some to reprove existing results, others to prove new fixed-target results. The main insight here is that fixed-target results often come almost for free when one can prove a result on the optimization time, so it is a waste to not report them explicitly. However, in Sect. 7 we also point out situations in which the runtime is well understood, but the derivation of fixed-target results appears very difficult.

The preliminary version of this work has been published in proceedings of the GECCO conference [4]. In this present version, apart for implementing properly the pieces that have been shortened or omitted during to the conference page limit and extending some of the results to evolutionary algorithms with fast mutation operators, we have introduced new versions of variable and multiplicative drift theorems that explicitly use the expected potential at the moment of stopping and hence yield better bounds in the fixed-target settings.

Fine-Grained Runtime Analysis: Fixed-Budget and Fixed-Target Analyses
Among the notions other than the time required to find an optimum, the first notion to become the object of rigorous mathematical analysis is fixed-budget analysis [30]. Fixed-budget analysis asks, given a computational budget , for the expected fitness of the best solution seen within b fitness evaluations. In the first paper devoted to this topic (extending results presented at GECCO 2012), Jansen and Zarges [30] investigated the fixed-budget behavior of two simple algorithms, randomized local search (RLS) and the  evolutionary algorithm (the  EA), on a range of frequently analyzed example problems. For these two elitist algorithms, fixed budget analysis amounts to computing or estimating 
, where f is the objective function and 
 is the b-th search point generated by the algorithm. Jansen and Zarges considered small budgets, that is, budgets b below the expected optimization time, and argued that instead of larger budgets, one should rather regard the probability to find the optimum within the budget.

Jansen and Zarges [30] obtained rather simple expressions for the fixed-budget fitness obtained by RLS, but those for the  EA were quite complicated. In [31], the same authors evaluated artificial immune systems (AIS). In terms of the classic optimization time, AIS are typically worse than evolutionary algorithms. Interestingly, in the fixed-budget perspective with relatively small budgets, AIS were proven to outperform evolutionary algorithms, confirming our claim that fine-grained runtime results can lead to insights that cannot be (easily) obtained from studying optimization times only.

These first results were achieved using proof techniques highly tailored to the considered algorithms and problems. Given the innocent-looking definition of fixed-budget fitness, the proofs were quite technical even for simple settings like RLS optimizing LeadingOnes. The analyses were even more complicated for the  EA and many analyses could not cover the whole range of budgets (e.g., for LeadingOnes, only budgets below 
 were covered, whereas the (strongly concentrated) optimization time is around 
, see [2]).

In [16], a first general approach to proving fixed-budget results was presented. Interestingly, it works by estimating fixed-target runtimes and then using strong concentration results to translate the fixed-target result into a fixed-budget result. This might be the first work that explicitly defines the fixed-target runtime, without however using this name. The paper [39] also uses fixed-target runtimes (called approximation times, see [39, Corollary 3]) for a fixed-budget analysis, but most of the results in the paper are achieved by employing drift arguments. An explicit collection of drift theorems designed for fixed-budget analyses, along with an application to derive fixed-budget results for the well-studied LeadingOnes problem, can be found in [35].

The first fixed-budget analysis for a combinatorial optimization problem was conducted in [42]. Subsequently, several papers more concerned with classical optimization time also formulated their results in the fixed-budget perspective, among them [11,12,13].

A similar notion of fine-grained runtime analysis, called the unlimited budget analysis [26], was recently proposed. It can be seen as either a complement to the fixed-budget analysis (as its primary goal is to measure how close an algorithm gets to the optimum of the problem in a rather large number of steps) or as an extension of fixed-budget analysis which goes beyond using small budgets only.

For the second main fine-grained runtime notion, fixed-target analysis, due to it being a direct extension of the optimization time, it is harder to attribute a birthplace. As we argue also in Sect. 5, the fitness level method is intimately related to the fixed-target view. As such, many classic papers can be seen as fixed-target works, which is particularly true for papers where the fitness level method is not used as a black box, but one explicitly splits a runtime analysis into the time to reach a particular fitness and the another time to find the optimum, as done, e.g., in [48]. The first, to the best of our knowledge, explicit definition of the fixed-target runtime in a runtime analysis paper can be found in the above-mentioned fixed-budget work [16, Section 3]. There, for a given algorithm A, a given objective function f (to be maximized), and a given target , the fixed-target runtime 
 is defined as the number of fitness evaluations after which a search point of fitness at least k is found. Since this notion was merely used as a tool in a proof, the name fixed-target runtime was not used yet. The paper [5] argued that fixed-target results, coined runtime profiles in [5], should be made explicit, to provide more information to practitioners. The name fixed-target analysis was, in the context of runtime analysis, first used in the GECCO 2019 student workshop paper [46], the only other work putting fixed-target analysis into its center. It is also worth noting that certain papers combine theoretical results for optimization time and experimental fixed-target results, [24] being a good example, which suggests that there is a demand for a method for an easy translation of the results between these two areas.

In summary, we see that there are generally not too many runtime results that give additional information on how the process progresses over time. Since fixed-budget analysis, as a topic on its own, was introduced earlier into the runtime analysis community, there are more results on fixed-budget analysis. At the same time, by looking over all fixed-budget and fixed-target results, it appears that the fixed-budget results tend to be harder to obtain.

From the viewpoint of designing dynamic algorithms, that is, algorithms that change parameter values or sub-heuristics during the runtime, it appears to us that fixed-budget results are more useful for time-dependent dynamic choices, whereas fixed-target results aid the design of fitness-dependent schemes. If algorithm A with a budget of b computes better solutions than algorithm B with the same budget, then in a time-dependent scheme one would rather run algorithm A for the first b iterations than B. However, if the runtime to the fitness target x of algorithm A is lower than that of B, then a fitness-dependent scheme would use rather A than B to reach a fitness of at least x.

figure a
Since we do not see that the increased difficulty of obtaining fixed-budget results is compensated by being significantly more informative or easier to interpret and since we currently see more fitness-dependent algorithm designs (e.g., [2, 10, 12, 36] than time-dependent ones (where, in fact, we are only aware of a very rough proof-of-concept evolutionary algorithm in the early work [29]), we advocate in this work to rather focus on fixed-target results. We support this view by further elaborating how the existing analyses methods for the optimization time, in particular, the fitness-level methods and drift, can easily be adapted to also give fixed-target results.

Preliminaries
Throughout the paper we use the notation [a..b] to denote a set of integer numbers , and we denote the set [1..n] as [n]. We write 
 for the n-th harmonic number, that is, 
, and 
 for convenience. Finally, we use the shorthand  to denote the function that returns 1 if event  holds true, and which returns 0 otherwise.

We consider simple algorithms, such as the  EA, the  EA, and the  EA, which solve optimization problems on bit strings of length n. Due to the increased interest in mutation operators that do not produce offspring identical to the parent [5], and to mutation operators different to standard bit mutation, such as the fast mutation operators sampling from heavy-tailed distributions [20], we consider them in a generalized form, which samples the number of bits to flip from some distribution . We also use a distribution over search points  during initialization. A default choice for  is to sample every bit string with equal probability, however, we consider also initialization with the search point having smallest possible fitness value. These algorithms are presented in the most general form in Algorithm 1 as a  EA parameterized by  and .

We consider the following distributions of  for the  EA:

randomized local search, or RLS: ;

the  EA with standard bit mutation: , where B(n, p) is the binomial distribution;

the  
 using the shift mutation strategy: ;

the  
 using the resampling mutation strategy:

;

the fast  EA with parameter : 
, and 
 is a distribution defined as follows:


 

where 
 is the normalization constant.

Note that, in fact, we could have also considered the fast  EA with the shift or replacement mutation strategy, as well as the version of the fast  EA that directly samples the number of bits to flip from 
 without using the binomial distribution as a proxy. However, since this paper would not benefit from repetitive analyses of very similar algorithms, we restrict ourselves only to the canonical fast  EA.

Sometimes we are only interested in the probability q of flipping a particular bit while not flipping any other bit. For problem size n and mutation strength p, the values of q for the algorithms above are

RLS: ;

 EA: 
;

 
: 
 
;

 
: 
 
;

fast 
: 
 
 
, as the factor 
 
 is between 
 and 
 
 by [20, Lemma 5].

We consider the following classical problems on bit strings:


 

We also consider the minimum spanning tree (MST) problem. Given a connected undirected graph G with positive weights on each edge, the MST problem asks to find a minimum spanning tree of it, that is, a subgraph that connects all vertices of G and that has the minimum possible weight. This problem was adapted to bit strings as in [43] as follows: each bit corresponds to one edge of G, and the bit value of 1 means that the edge is included in the subgraph.

Overview of Known Fixed-Target Results
In this section, we comment on the known fixed-target results available in recent papers. We cover both the results and the techniques which have been used and possibly modified to achieve these results. Where necessary, we estimate the precision of these results by comparing them with the actual expected hitting times of the corresponding algorithms. In the whole section, n is the problem size and k is the target fitness.

LeadingOnes
LeadingOnes was the first problem for which certain fixed-target results were derived. The paper [48] studied upper and lower bounds on the runtime of the  EA on several benchmark problems. For LeadingOnes, the runtime was proven in [48, Theorem 1] using a technique similar to fitness levels, where the state space also involved the number of the best individuals in the population. Then, [48, Corollary 1] bounded the expectation of the time needed to reach a state of at least k leading ones from above by , which is a fixed-target result. In the framework of that paper, this result appeared to be useful in a subsequent part, where the necessity of having population size  was discussed by constructing and analyzing an artificial problem involving both OneMax and LeadingOnes.

Böttcher et al. [2] proved exact expected runtimes for the  EA on LeadingOnes, which was an important cornerstone in the studies of LeadingOnes. In this paper it is proved for the  EA that the expected time to leave a state with the fitness of i and the mutation probability p is 
. However, the next fitness value may be greater than . The problem-dependent observation, which simplifies the analysis, is that the bits following the i-th bit have a probability of exactly 1/2 to equal 1. This yields the expected optimization time to be exactly 
 for a fixed p.

This result was reused in [16, Section 4] to prove that the expected time that the  EA with mutation probability  needs to hit target  equals

Technically, the result in [16] is given only for , partially because they used this mutation probability throughout the whole paper. However, their proof does not depend on the value of p and hence extends to the general case.

Theorem 11 in [5] extends this result to similar algorithms: the  
, the  
, and RLS. While only upper bounds are claimed in that work, it is clear from the above that these bounds are exact. As the results of our techniques are identical, we direct the reader to our Theorem 7.

Finally, in [7, Corollary 4] an even stronger result is proven about the exact distribution of the time the  EA needs to hit a certain target. The result is expressed in terms of random variables 
 for initial bit values and the probabilities 
 to generate a point with a strictly better fitness from a point with fitness i, and reads as 
, where Geom is the geometric distribution.

The summary of the results for LeadingOnes is given in Table 1.

Table 1 Summary of the fixed-target results available in literature for LeadingOnes
Full size table

OneMax
To the best of our knowledge, explicitly formulated fixed-target results regarding OneMax exist so far only for the  EA and similar algorithms. What is more, due to the fact that the  EA shows different expected progress for different fitness values on the same problem, for which it is hard to find sharp bounds, the available upper and lower fixed-target bounds for OneMax are generally less precise than those for LeadingOnes. For this reason, we augment the existing theoretical results with the actual runtime profiles of the  EA, which are given in Fig. 1 for three problem sizes 
.

Fig. 1
figure 1
The runtime profile of the  EA on OneMax and the applicable bounds. Plots in the left part of the figure show the general picture for three problem sizes 
, whereas plots in the right part highlight the regions close to the optimum. The plotted quantity is the expected number of fitness evaluations required to reach the target fitness value against that (relative) fitness value. The runtime profiles for both cases regarding  EA were computed exactly using dynamic programming following the ideas of [3]. For the upper and lower bounds, the respective expressions were evaluated and plotted, disregarding the  factors. The notation in the legend is as follows: “ EA zero” is the  EA starting at the all-zero bit string, “ EA rand” is the  EA starting at a randomly generated bit string, “Upper bound, CD” is the bound from [5, Theorem 10], “Upper bound, VBBDD” is the bound from [46, Theorem 3.1] applicable to “ EA rand”, and finally “Lower bound, LS” is the bound from [39, Corollary 3]

Full size image

The first result for OneMax appeared in [39, Corollary 3], again in a fixed-budget setting. It is a lower bound, which was proven using fitness levels for lower bounds [45]. This result says that, for the  EA with mutation probability  optimizing OneMax, the expected time until hitting a target k is at least


 

(2)

which was used in [39] to prove fixed-budget results.

This bound is aimed at estimating the time needed to reach targets which are very close to the optimum. Since the  EA spends most of its time in these regions, and because [39] is dedicated to fixed-budget estimations, this choice seems reasonable. However, from the fixed-target perspective, (2) results in a satisfactory lower bound only for a small fraction of possible conditions. In particular, it turns negative when the target is smaller than 
, so, for instance, it cannot be used to estimate the time needed to reach targets of the form cn, where  is a constant factor. The combination of the constant factors also renders it usable only for quite large values of n: for instance, at 
 only five values 
 produce nonzero values. For this reason, Fig. 1 contains special subfigures which highlight this particular bound at targets close to the optimum.

In [5, Theorem 10], the first fixed-target upper bounds were given for the  EA, the  
, the  
, and for RLS on OneMax. They cover the whole range of targets, as well as arbitrary mutation probabilities. They are based on simple fitness level arguments, and for simplicity they assume the worst case regarding initialization, that is, that the algorithm starts at the all-zero bit string. As a result, all these bounds have a form of of 
 
, where 
 is the i-th harmonic number and q is the lower bound on the probability of flipping exactly one bit. This bound is exact for RLS and also captures the behavior of other  EA flavors quite well (see Fig. 1 for the example of the standard  EA).

To quantify how much the upper bound for the  EA on OneMax from [5, Theorem 10] differs from the actual times needed for the  EA to hit various targets when starting at the all-zeros bit string, we reuse the data from Fig. 1 to compute the ratios of the upper bound to the corresponding average target-hitting time. We present these insights in Table 2, from which one can infer, for instance, that the upper bound from [5, Theorem 10] overestimates the true hitting times at most by a factor that tends to some constant value.

Table 2 Insights about the ratio of the upper bound on the fixed-target runtime of the  EA on OneMax from [5, Th. 10] and the actual expected hitting times computed using dynamic programming following the ideas of [3]. We present the maximum of the ratio across all targets , as well as the ranges of targets where the ratio is at least some fixed value (values of 1.5 and 2.5 are used). Note how the left endpoints of these ranges remain the same as the problem size n grows, whereas the right endpoints scale linearly with remarkable precision. The maximum ratio slowly grows and apparently approaches a constant, which resembles  quite closely
Full size table

While being moderately precise for estimating the fixed-target runtimes of the  EA, and similar algorithms, on OneMax starting from the all-zeros bit string, these bounds overestimate the fixed-target results much more when these algorithms start from a randomly generated bit string, which is a more realistic condition. For random initialization, [46, Theorem 3.1] proves a similar upper bound of 
 
 for  using the same technique and additional arguments to use 
. Technically, this result was proven only for  
, however, the proof holds also for arbitrary probabilities q of flipping exactly one bit. Such a bound gives a moderately good approximation, as illustrated in Fig. 1 for the classic  EA. The evaluation of the approximation quality, similar to the one shown in Table 2, shows that the constant factors in the approximation are better: the maximum ratio for 
 is less than 1.8, and the ratio gets smaller than 1.5 at smaller .

In [49, Theorem 6.5], lower bounds for the runtimes of the  EA on OneMax are proven, assuming arbitrary mutation probabilities p satisfying 
. For this, an interval 
 of distances to the optimum is considered, where 
. Multiplicative drift for lower bounds is applied to this interval to yield the lower bound of


 

This result was used in [46] to obtain the fixed-target results for the  
 on OneMax. The small difference between the  EA and the  
, from the point of view of the above-mentioned proof, is only in one of the factors, which depends on p in a slightly different way. The obtained fixed-target results are piecewise. Indeed, [49, Theorem 6.5] uses the time that the  EA needs to go through an interval of fitness values S. For this reason, for all targets k that do not belong to this interval, 
, the fixed-target runtime bound does not depend on k. What is more, the bounds contain a factor  which is hard to estimate well, and whose value appears to be large enough for practical values of n. For this reason we do not show the plots for these bounds in Fig. 1.

The summary of the results for OneMax is given in Table 3.

Table 3 Summary of the fixed-target results available in literature for OneMax
Full size table

BinVal
The fixed-target bounds for BinVal were proven in [46] for the  
. The methods for proving optimization times for linear functions, such as the ones in [49], were found to be insufficient, so a problem-dependent observation was used. To achieve a target value k such that 
, one requires to optimize the t heaviest bits, and it is enough to optimize the  heaviest bits. As a result, reaching the target k is equivalent to solving BinVal of size  to optimality using a n/t times smaller mutation rate. The quite complicated bounds from [49] were adapted to the case of BinVal in [46, Theorem 4.1]. The summary of these results is given in Table 4.

Table 4 Summary of the fixed-target results for BinVal
Full size table

Fitness Levels
In this section we consider the fitness level theorems in the fixed-target context. A key take-away, implicitly known in the community for years, is that the most important theorems of this sort are already suitable to produce fixed-target results.

Fitness Level Theorems
In the fitness level method (also known as artificial fitness levels and the method of f-based partitions [47]), the state of the algorithm is typically described by the best fitness of an individual in the population. It is transformed into a fitness level, which may aggregate one or more consecutive fitness values. For the fitness function f (to be maximized) and two search space subsets A and B one writes 
 if  for all  and . A partition 
 of the search space, such that 
 for all  and 
 contains only the optima of the problem, is called an f-based partition. If the best individual in a population of an algorithm  has a fitness 
, then the algorithm is said to be in 
.

Fitness level theorems work best for the algorithms that typically visit the fitness levels one by one in the increasing order. We summarize below several popular fitness level theorems. We note that a simpler version of the fitness level theorems in [45] was recently given in [19]. Since this work appeared significantly after the original preparation of this manuscript, we do not discuss it any further, but note nevertheless that for future fixed-target analyses it might be easier to use than the theorems from [45].

Theorem 1
(Fitness levels for upper bounds; Lemma 1 from [47]) Let 
 be an f-based partition, and let 
 be a lower bound for the probability that the elitist algorithm  samples a search point belonging to 
 provided it currently is in 
. Then the expected hitting time of 
 is at most


 
 
 
 
 
 

(3)

Theorem 2
(Fitness levels for lower bounds; Theorem 3 from [45]) Let 
 be a partition of the search space. Let the probability for the elitist algorithm  to transfer in one step from 
 to 
, , to be at most 
, and let 
. Assume there is some  such that 
 for all . Then the expected hitting time of 
 is at least


 
 
 
 
 

(4)

We also use an upper-bound theorem similar to Theorem 2.

Theorem 3
(Refined fitness levels for upper bounds; Theorem 4 from [45]) Let 
 be a partition of the search space. Let the probability for the elitist algorithm  to transfer in one step from 
 to 
, , to be at least 
, and let 
. Assume there is some  such that 
 for all . Further, assume that 
 for all . Then the expected hitting time of 
 is at most


 
 
 
 
 

(5)

Theorems 1–3 are applicable only to elitist algorithms. However, a fitness level theorem was proposed in [6] that can be applied to non-elitist algorithms as well (see [18] for a sharper version of this result).

Observation 4
If one of Theorems 1–3 is proven for a certain algorithm on a certain problem, it also holds if a new target fitness level 
 is chosen, such that 
, and all fitness levels 
 are merged.

Proof
This is essentially the same argument as in [39].

This modification does not alter the estimations of probabilities to leave fitness levels preceding 
: 
, 
 and 
 for Theorems 1, 2, and 3 , respectively. The only affected locations are the constraints on 
. Their affected occurrences on the right-hand sides effectively merge by summing up, e.g., 
. Note that only those right-hand sides, which contain the complete sums from 
 to m, survive after the transformation, and not just their parts. For the left-hand sides, only those 
 survive where 
, as all others are either unchanged or cease to exist. However, these occurrences are trivial, since they are limited only by identity inequalities 
 in Theorem 2 and are not limited by anything in Theorem 3 as their limits are conditioned on 
. 

It follows from Observation 4 that it is very easy to obtain fixed-target results from the existing optimization time results whose proofs use the above-mentioned fitness level theorems.

Note that, technically, Theorems 2, 3 and the theorem from [6] do not require the employed partition of the search space to be an f-based partition. Formally speaking, this enables using them in the fixed-target context as is, contrary to the original formulation of fitness levels for upper bounds, where the last partition must contain optima and nothing else. However, this alone does not yet allow to reuse the existing optimization time results in order to obtain the fixed-target ones. Observation 4 fills this gap by specifying the sufficient conditions for this to be possible.

We also note that Theorems 2 and 3 only require the transition graph over the employed partition to be acyclic with regard to the analyzed algorithm, but does not require that a fitness level 
 is reachable from another fitness level 
 for every . In fact, Observation 4 may be extended without much effort to allow merging of fitness levels whose indices do not form a contiguous sequence, provided that one cannot reach a non-merged fitness level from any of the merged levels. This feature might have applications in analysis of certain local search algorithms.

Applications
Hill Climbers on LeadingOnes
We re-prove here the statements about the fixed-target performance of the algorithms from the  EA family, which were proven in [16, Section 4] and stated, but not formally proven, in [5, Theorem 11]. For this we use Theorems 2 and 3 , similarly to their use in [45] to prove the results from [2] with fitness levels alone.

Lemma 5
In the context of Theorems 2 and 3 for LeadingOnes, assuming the target fitness is k, the values 
 when , 
, and  satisfy their requirements.

Proof
Recall that for LeadingOnes, if i is the fitness of an individual, its first  bits are known, and the bits at indices  are uniformly distributed. Thus, conditioned on mutation being applied and the fitness being improved, the probability of transferring to fitness level j is 
. Due to [45, Theorem 5], the constant  to be used in the theorems above is equal to 1/2.

In the fixed-target context, the transition probabilities 
 where  remain the same, as the underlying process is not changed, whereas since fitness levels k, , ..., n merge, the probability 
 results in telescoping the inverse powers of two. Since fitness level merging effectively reduces the problem size from n to k without affecting anything else, the constant value  also remains valid. 

Lemma 6
Assume that 
 is the probability for algorithm  to flip a given bit while not flipping any of some other i given bits. The expected time for  to reach a target of at least k on LeadingOnes of size n is:


 
 
 
 
 
 
 
 
 

(6)

Proof
The left-hand side of the lemma statement follows from Lemma 5, Theorems 2 and 3 , and Observation 4. The right-hand side follows by recalling that in LeadingOnes, it holds for all considered algorithms that 
, and by reordering the sums as in [2].

We can also derive this result from [7, Corollary 4]. 

Theorem 7
The expected fixed-target time to reach a target of at least k when optimizing LeadingOnes of size n is exactly

 
 for RLS;

 
 for the  EA with mutation probability p;

 
 for the  
 with mutation probability p;

 
 
 
 for the  
 with mutation probability p;

 
 
 
 
 for the fast 
.

Proof
We use Lemma 6 and note that, for a fitness of i:

for RLS, 
;

for the  EA, 
;

for the  
, 
;

for the  
, 
 
;

for the fast 
, 
 
 
.

The sums for the  EA and the  
 are simplified as in [2]. 

Note that the presented expression for the fast 
 is complicated, and little can be understood from it about its behavior, and in particular about the dependency on . The simplified upper bound can be obtained by considering only , in which case it is greater by a factor of 
 than the corresponding fixed-target time for the  EA, and this factor is a constant for . Lower bounds, however, require much more work, as, for instance, 
 for  is 
, so the fast 
 performs the first improvements much faster than more conventional versions of the  EA. It is hence quite hard to obtain closed-form sharp bounds, which we leave for possible future work.

Hill Climbers on OneMax, Upper Bounds
We now re-prove the existing results for the  EA variants on OneMax. Our results for the case of random initialization of an algorithm are sharper than in [46], because we use the following argument about the weighed sums of harmonic numbers.

Lemma 8
The following equality holds:


 
 
 
 
 
 
 

Proof
Proven in [9, Sec. 2.5] with [44, Identity 14]. 

Our results are formalized as follows.

Theorem 9
The expected fixed-target time for Algorithm 1 with , whose mutation distribution  selects a single bit to flip with probability q, to reach a target of at least k on OneMax of size n is at most:

 
 when initializing with the worst solution;

 
 
 when initializing randomly, assuming .

Proof
Let 
 be the probability for the algorithm to be initialized at fitness i. Assuming pessimistically that the fitness does not improve when two and more bits are flipped, we apply Theorem 1 to get the following upper bound:


 
 
 
 
 
 
 

The pessimistic bound above proves the theorem for the algorithms initialized with the worst solution. For the random initialization, we note that the initial search point has a fitness i with the probability of  
 
. From the equality above we derive:


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

where the last transformation uses Lemma 8. The second addend is o(1/(qn)), because 
 when , which is further multiplied by 
, since for  it holds that


 
 
 
 
 
 
 

by the Chernoff bound. As a result, the fixed-target upper bound for the random initialization is 
 
 
. 

Hill Climbers on OneMax, Lower Bounds
We also apply fitness-levels to the lower bounds to improve the result from [39].

Theorem 10
(Lower fixed-target bounds on  EA for OneMax) For mutation probability , and assuming that 
, the expected time to find a solution with fitness at least  is at least


 
 
 

Proof
We use the proof of [45, Theorem 9] with Observation 4. The source of this formula is located at [45, journal page 428, bottom of left column]. We use this particular formula, as it gives a good enough precision for a wide range of possible targets and matches the shape of Theorem 9 quite well. 

We conjecture that a similar result can be derived for other versions of the  EA as well, however, finding the exact constant factors would require additional investigation into [45, Theorem 9].

The  EA on OneMax, Upper Bounds
We now introduce the fixed-target bounds for the  EA using fitness levels. For this we adapt [48, Theorem 2] to support fixed targets different from the optimum, which makes it no longer possible to use certain transitions from the original proof.

Theorem 11
Let , and assume . The expected time to reach an individual with fitness at least  on OneMax is:


 
 
 
 
 
 
 
 

Proof
Similarly to [48, Theorem 2], we pessimistically assume that on every fitness level L the  EA creates  replicas of the best individual, and then it waits for the fitness improvement. We also assume that the  EA never improves the fitness by more than one.

If there are  best individuals, the probability of creating a replica is 
, so the expected time until creating R replicas is at most


 
 
 
 
 
 
 
 
 
 

and the total time the  EA spends in creating replicas is


 
 
 
 
 
 
 
 
 
 

where we use the condition  and that, for all ,


 
 
 
 
 
 
 

Concerning the fitness gain, if there are R replicas of the best individual with fitness L, the probability of creating new offspring with fitness  is at least


 
 
 

therefore we can apply Theorem 1 to estimate this part of the fixed-target time:


 
 
 
 
 
 

Unlike [48], we consider what the  clauses can be. Depending on how the target k relates to the boundary , we write


 
 
 
 
 

which completes the proof, noting that the fixed-target time is 
. 

Drift Analysis
In this section we consider the drift theorems in the fixed-target context. These theorems are generally more powerful, but it appears that one should use them for proving fixed-target results with slightly more care than in the case of fitness levels. We also propose variations of variable and multiplicative drift theorems that are specifically aimed at gaining precision in the fixed-target context.

Drift Theorems
Drift theorems translate bounds on step-wise expected progress into bounds on expected first-hitting runtimes. They are usually formulated in terms of a random process that needs to hit a certain minimum value. To this end, the search process of the algorithm is mapped to real numbers via so-called potential functions. Some of these theorems prohibit the process from falling below the target, or from visiting an interval between a target and the next greater value. For this reason, some optimization time results cannot be converted into the fixed-target results without additional work, as targets different from the optimum violate the requirements above.

The paper [34] contains a discussion of processes which may fall below the target, and the implications for drift theorems. For instance, [34, Example 6] gives an example of a process with the target 0 and the expected progress of 1 at 
, which is given by 
 with probability of 1/n and 
 otherwise. By mistakenly applying a well-known additive drift theorem from [27] to this process, one can get an overly optimistic upper bound of 1 on the expected runtime, which is, in fact, n.

We start the discussion with the additive drift theorems. We provide their versions from [34] which appear to be preadapted to the fixed-target conditions. For the first of these theorems we explicitly note that its upper bound is not 
, but a (generally) larger value. Indeed, if we define 
 to be the value of the process at the hitting time T, it is only known that 
, and the latter may be far from being an equality. Proving the bounds for 
 seems to be the essential additional work in order to prove fixed-target results.

Theorem 12
(Additive drift, fixed-target upper bounds; Theorem 7 from [34], original version in [27]) Let k be the target value, let 
 be a sequence of random variables over , and let 
. Suppose that there is some value  such that, for all , it holds that 
. Then 
.

Theorem 13
(Additive drift, fixed-target lower bounds; Theorem 8 from [34]) Let k be the target value, let 
 be a sequence of random variables over , and let 
. Suppose that:

there is some value  such that, for all , it holds that 
;

there is some value  such that, for all , it holds that 
.

Then 
.

For the lower bound, the simplification to 
 is possible (contrary to the upper bound), but having a better bound may be desirable. By recalling again [34, Example 6], we can see that taking 
 into account can improve the bound asymptotically. In fixed-target runtime analysis, such an improvement may occur with very easy targets.

More advanced drift theorems such as the multiplicative drift theorems [17] and the variable drift theorems [14, 32, 37, 41] make it easier to prove sharp bounds on hitting times for processes with drift that depends on the current value, a situation that occurs rather frequently in evolutionary computation. Most of the popular drift theorems of this sort can be classified using the following properties: they estimate the time for a process to either reach a certain target value k or to surpass a certain threshold value 
, and they also may or may not require the process to never fall below the target or to never visit a region between the threshold and the ultimate termination state (usually zero).

The case analysis of four variants of drift theorems presented in [34] revealed that only two of these four variants are suitable to be used for fixed-target research: theorems for upper bounds which require to surpass a threshold 
, and theorems for lower bounds which require to reach a target k. The former contain an extra addend in their statement (such as “1+” or “
”), while the latter do not. This seems to be closely related to the 
 issue in additive drift theorems, which the “good” theorems pessimize to the right direction.

We now present or reformulate certain drift theorems for upper bounds, which can be used out of the box for proving the fixed-target results.

Theorem 14
(Multiplicative drift, upper bounds [17] adapted to fixed-target settings) Let 
 be the threshold value, let 
 be a sequence of random variables over , and let 
. Furthermore, suppose that:

, and for all , it holds that 
;

there is some value  such that, for all , it holds that 
.

Then 
.

Proof
Apply [17] to the new potential function 
. 

Theorem 15
(Variable drift, fixed-target upper bounds; Theorem 10 from [34]) Let 
 be the threshold value, let 
 be a sequence of random variables over , and let 
. Furthermore, suppose that:

;

for all , it holds that 
;

there is a monotonically increasing function 
 such that, for all , it holds that 
.

Then 
 
 
.

In all these theorems, the conditions are the same as for their corresponding optimization-time variants. We now present a drop-in replacement version of [14, Theorem 7] that enables obtaining lower bounds on fixed-target hitting times by using the same functions and conditions that were used for proving the lower bounds on the optimization time using that theorem. Note that, for technical reasons, this version does not support setting the target to the optimum value.

Theorem 16
(Variable drift, fixed-target lower bounds; adapted from Theorem 7 from [14]) Let  be the target value, let 
 be a sequence of monotonically decreasing random variables over 
, and let 
. Suppose that there are two continuous monotonically increasing functions 
, and that for all  it holds that


.

Then 
 
.

Proof
We proceed similarly to how it was done in the proof of [14, Theorem 7], however, with a different additive drift theorem in mind.

Let  be the function defined by


 
 
 
 

This function is strictly monotone increasing and continuous on . Moreover, it is differentiable on  with


 
 
 
 

Using 
 and 
, the monotonicity of the sequence 
 and the condition 
, we get that . According to the mean-value theorem, for all  there exists  such that

which implies


 
 
 

since 
 decreases monotonically.

In terms of the function g, the hitting time T describes the smallest t with 
. As g is monotone and invertible, it holds for all  that


 
 

where the last inequality comes from the theorem condition 
. We apply Theorem 13 for 
 and the zero target to complete the proof. 

Overshoot-Aware Drift Theorems
We have already shown, using additive drift as an example, that in order to obtain good fixed-target bounds, an expected overshoot value 
 needs to be considered. However, more advanced theorems cannot easily make an advantage out of this idea, although it may be desired (especially when analyzing easy targets, or in the case when the hardest part of the optimization process is not near the optimum, as it may happen for the  genetic algorithm on certain monotone functions [40]). Here, we present variations of the variable drift and multiplicative drift theorems for upper bounds which explicitly contain the overshoot term 
 in the expression for the hitting time, for which reason we call them overshoot-aware.

Theorem 17
(Overshoot-aware variable drift, upper bounds) Let 
 be the threshold value, let 
 be a sequence of random variables over , and let 
. Furthermore, suppose that

;

there is a non-decreasing function 
 such that for all  it holds that 
.

Then 
 
 
.

Proof
Let 
. We define the potential function  by setting

For any two points , such that  and 
, the following holds:


 
 
 
 
 
 
 
 
 

As a result, the expected potential decrease is:


 
 

We also note that g(x) is a linear function for 
, hence for any random variable Z taking values from 
 it holds that .

Now we apply Theorem 12 and prove this theorem:


 
 
 

where we use the fact that the random variable 
 takes values from 
 together with the observation just above, and g is a bijection, so conditioning on 
 is equivalent to conditioning on 
. 

Corollary 18
(Overshoot-aware multiplicative drift, upper bounds) Let 
 be the set of possible values of the random process that is defined by the sequence of random variables 
. Let  be the target value of the process, let 
 be the hitting time, and let 
 be the lower bound on the values from D that are larger than k. Suppose that

;

there is some constant value  such that, for all , it holds that 
.

Then 
 
 
 

Proof
Follows directly from Theorem 17 by choosing . 

Note that a proper estimation of the expected overshoot 
 may in fact be tricky, especially given that it is a conditional expectation. Contrary to Markovian processes, where 
 and understanding 
 may be rather easy, for more involved processes, such as optimization of multimodal functions or algorithms with self-adjusting parameters, this value may depend on the whole sequence of values.

Applications
Minimum Spanning Trees
We begin with fixed-target bounds for minimum spanning trees solved by the  EA and its variations. In the context of evolutionary computation, the function to optimize can be defined in different ways. We follow [43] and use a function which consists of two parts: the number of connected components with a large weight (to facilitate connecting all the graph vertices), and the weight of the chosen edges. This function is to be minimized. It is known [17, 43] that the  EA optimizes this function in expected time 
, where m is the number of edges, n is the number of vertices, and 
 is the maximum edge weight.

Theorem 19
Starting from a randomly initialized graph, the expected time for Algorithm 1 with , whose mutation distribution  selects a single bit to flip with probability q, to find a graph with at most k connected components is at most 
 
 
.

Proof
Consider the potential function , where s is the number of connected components in the subgraph which consists of the edges included in the genotype x. If there are s such components, there are at least  edges, which can be added to decrease the number of components. To do that, it is enough to flip at least one bit corresponding to these edges. To apply Theorem 14, we estimate the drift as 

The target of k connected components maps to the target potential of  and hence to the threshold value k. By applying Theorem 14 we get the desired bound. 

Theorem 20
Starting from some spanning tree, the expected time for Algorithm 1 with , whose mutation distribution  selects exactly two bits to flip with probability q, to find spanning tree with the weight at most k larger than the minimum possible weight is at most 
 
 
.

Proof
We again reuse the corresponding result from [17]. The process is defined as 
, and [17] gives the drift bound of 
. The application of Theorem 14 yields the desired upper bound on the fixed-target runtime, as 
. 

We give the two-bit probabilities for the common algorithms.

RLS which flips pairs of bits (“2-opt mutation operator”): 
 
;

 EA and  
: 
;

 
: 
 
;

 
: 
 
;

fast 
: 
 
.

Note that RLS which flips pairs of bits is a rather a mind experiment than an algorithm to use, however, one may use RLS that tosses a coin and flips either single bits or pairs of bits, which just halves the probability above.

The  EA on OneMax, Lower Bounds
We prove the lower fixed-target bounds using variable drift.

Theorem 21
(The lower fixed-target bound on  EA with  for OneMax) The expected time to find an individual with fitness at least k, , when n is large enough, is at least


 
 
 
 
 
 
 

Proof
The basis of this proof is [14, Theorem 5]. Our aim is to apply Theorem 16, which allows jumps below the target. This allows us to use the original the potential function 
 and the existing bound on the expected drift [14, Lemma 6]: 
 
 
.

Following [14], we bound the step size with . We denote as a bad step the event of increasing the fitness by more than . To condition on that, we estimate the probability of making a bad step for , which was shown in [14] to be 
 for . For a good fixed-target result, we need to cover the rest. For , the probability of a bad step is at most  
 
 
. For , a similar calculation yields the probability of 
. Since the latter bound corresponds to only  fitness values, in which the algorithm spends at most O(n) iterations in expectation, the union bound over all bad steps during  iterations is at most 
. This is reflected by having the 
 quotient in the result.

We also reuse the function h(x) from [14, Theorem 5], which is 
 
 
, and apply Theorem 16 to get


 
 
 
 
 

We simplify the expression above by dropping the addend containing the arctangent, since it increases with x and its value is asymptotically smaller than other addends. Finally we choose 
 and bound 
 from below by n/3. The latter choice is due to Chernoff bounds, which show that the algorithm starts with an individual with a Hamming distance to the optimum of at least n/3 with probability 
, which hides in the leading factor of 
. 

Figure 2 illustrates that the bound proven in Theorem 21 is significantly better than the one from [39] and captures the essense of the algorithm’s behaviour.

Fig. 2
figure 2
The runtime profile of the  EA on OneMax and the lower bounds, the existing and the new ones. Plots in the left part of the figure show the general picture for three problem sizes 
, whereas plots on the right zoom into the regions close to the optimum. The plotted quantity is the expected number of fitness evaluations required to reach the target fitness value against that (relative) fitness value. The runtime profiles of the  EA were computed exactly using dynamic programming following the ideas of [3]. For the lower bounds, the respective expressions were evaluated and plotted, disregarding the  factors. The notation in the legend is as follows: “ EA rand” is the  EA starting at a randomly generated bit string, “Our result” is the one proven in Theorem 21, and “Lower bound, LS” is the bound from [39, Corollary 3]

Full size image

Summary of Difficulties of Fixed-Target Analysis
In this work, we have often seen that fixed-target analyses are not much more difficult than classic runtime analyses. However, from the section dedicated to drift theorems we also learned that, in order to obtain fixed-target results from existing optimization time results, more powerful drift theorems sometimes require additional statements to yield bounds that are good enough. More extreme examples are known, such as our earlier attempt with the BinVal function in [46], for which a radically different approach had to be developed. What is more, sometimes even the easy proofs employing fitness levels required us to dig deeper into the original optimization time result and refer not to the main theorem formulation, but to the details of its proof. Below we present an attempt to classify the possible difficulties that may arise when one wants to prove fixed-target results based on the existing results about optimization times.

(1)
Intermediate results can directly be reused, but final theorems are not enough. Since many of the existing papers aim at proving optimization times, they may not expose the intermediate results, which can be used in fixed-target proofs, as separate citable units, such as theorems or lemmas. A simple example is the proof of Theorem 10, where we could not even point to the equation we used due to it being unnumbered.

It may even happen that the proof nearly gives the fixed-target result, but it cannot be derived from the theorem statement. The typical reason is that the result gets simplified in a way that does not change the optimization time much but affects the possible fixed-target result. Our proof of Theorem 11 diverged from its source at the point of simplification of one of the  clauses: if we followed this simplification too, the final result would be much less precise.

Presenting the results as a set of smaller lemmas and theorems may make it easier to build fixed-target results (and further extensions) atop of them. We admit, however, that it requires more work, and some proofs may be too difficult to split into multiple lemmas, which would then have large and cumbersome statements.

(2)
Results can be directly reused, but they are not applicable or sharp enough for the full range of targets. For various reasons the existing optimization time results may be quite sharp on their own, but appear to be too loose in fixed-target contexts. A good example of such a situation is a bound which is sharp for hard regions of the search space, but is loose for the remaining part. This is the case for all simple upper bounds for OneMax, which assume that only one-bit flips are beneficial. A similar example for lower bounds is [39, Corollary 3], which implicitly assumes that optimization in easy parts is performed instantly. Such existing results need to be augmented by more work that refines them in parts that are not too meaningful for finding optimization times, but essential for good fixed-target results.

It follows that, surprisingly at first sight, more complicated proofs of optimization times that yield sharper bounds should be easier to adapt to the fixed-target context than easier proofs. We admit, however, that such proofs are harder to produce and verify, and the main messages important for understanding optimization times may get obscured.

(3)
Some results can be directly reused, but additional statements need to be proven. One of such cases observed in this paper are drift theorems, which can borrow all the conditions from the existing optimization time results, but require to prove the bounds on the overshoot term 
 in order to yield the best possible precision. This is not actually required at times, however, on some occasions not doing that results in a ridiculously imprecise bound, such as an upper bound that does not take the target into account.

This can be seen as a fair price to pay, since drift theorems are applicable for a wider range of processes than fitness level theorems, and harder problems may demand more complicated solutions. Based on some of our preliminary work, however, we conjecture that if one has both upper and lower bounds that are based on drift theorems and are good enough, the matching fixed-target results may be obtained with much less effort.

(4)
Existing results require more work in order to be usable. In the most complicated cases, the existing optimization time proofs may be too overfitted to the optimum being the target. For instance, it may be the case that the distance between the optimum and the target, which used to be zero, appears in so many locations of the proof, that the whole proof needs to be significantly reworked and extended to produce the fixed-target results. This is what seems be the case for linear functions: some of our work in [46] for BinVal, especially the failed attempts that were not included into that paper, suggests that the principles of designing the potential function in [49] shall be significantly changed to work well in the fixed-target context.

While two latter points seem to be fundamental, two former appear to follow from the current habits of performing and presenting the research, for which reason they may be called the social ones: the use of best practices, possibly assisted by automated theorem provers, has the potential of resolving a large fraction of these difficulties just as a side effect.

Conclusion
In this first work focussed on fixed-target runtime analysis, most of our results indicate that deriving fixed-target results for the whole set of reasonable targets is in several cases not more complicated than just analyzing the classical optimization time (which is the special case where the target is set to the fitness of an optimal solution). Since fixed-target results are much more informative than the optimization time alone, in such situations we can only advocate to conduct runtime analyses in the more general fixed-target perspective. As discussed, this often does not need different proofs, all that is required is to formulate the information present in the proof also in the result. That said, there are also problems which require additional facts to be proven in order to obtain sharp enough fixed-target results. Extending our current toolbox to analyze these situations should be a fruitful direction for further research.

Together with a complementary fine-grained notion that appeared recently, optimization times starting from a good solution [1], which might also be called fixed-start runtime analysis to unify terms, fixed-target runtime analysis can help to prove runtimes for algorithms to cross fitness intervals. This has direct applications in designing and understanding hyper-heuristics, as well as in parameter tuning and parameter control. Since these research areas tend to require especially sharp bounds, it may be the case that the difficulties in applying even the hardest tools that take care of fine effects, such as target overshooting, may actually pay off well