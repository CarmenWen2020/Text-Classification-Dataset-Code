We consider the vertex cover problem with multiple coverage constraints in hypergraphs. In this problem, we are given a hypergraph  with a maximum edge size f, a cost function 
, and edge subsets 
 of E along with covering requirements 
 for each subset. The objective is to find a minimum cost subset S of V such that, for each edge subset 
, at least 
 edges of it are covered by S. This problem is a basic yet general form of classical vertex cover problem and the edge-partitioned vertex cover problem considered by Bera et al. We present a primal-dual algorithm yielding an 
-approximation for this problem, where 
 is the 
 harmonic number. This improves over the previous ratio of , where c is a large constant used to ensure a low failure probability for Monte-Carlo randomized algorithms. Compared to the previous result, our algorithm is deterministic and pure combinatorial, meaning that no Ellipsoid solver is required for this basic problem. Our result can be seen as a novel reinterpretation of a few classical tight results using the language of LP primal-duality.

Introduction
The vertex cover problem is one of the most well-known and fundamental problem in graph theory and approximation algorithms. Given an undirected hypergraph  and a cost function 
, the objective is to find a minimum cost subset  such that any edge in E is incident to some vertex in S.

This problem is known to be NP-hard, and f-approximation algorithms based on simple LP rounding and LP primal-duality are known for this problem [10], where f is the maximum size of the hyperedges. Assuming the unique game conjecture, approximating this problem to a ratio better than  is NP-hard for any  [8].

The partial vertex cover problem is a natural generalization of the vertex cover problem. In this problem, we are given an additional parameter k which is called the covering requirement. The objective of this problem is to find a minimum cost subset of V which covers at least k edges in E, i.e., at least k edges of E are incident to at least one vertex in S.

Various methods have been developed to obtain tight approximations for this problem. Bshouty and Burroughs [3], who first proposed this problem, provided a 2-approximation algorithm for graphs, i.e., the case for which , using LP-rounding. Their algorithm generates |V| candidate covers each of which is constructed by guessing the most expensive vertex used in the optimal solution. Gandhi et a. [6] used the same technique to develop a primal-dual method which yields an f-approximation for hypergraphs. Mestre [9] used a more clever primal-dual approach to guess the most expensive vertex and improved the time complexity of the above algorithms.

Fujito [5] developed an f-approximation for hypergraphs, based on a primal-dual method exploiting the property of minimal solutions. Bar-Yehuda [1] used the same property to obtain the same result using a local-ratio method. Hochbaum [7] adopted Lagrangian relaxation to get a 2-approximation on graphs.

Bera et al. [2] considered a generalization of the partial vertex cover problem for which they called the partition vertex cover problem. In this problem, we are given a partition 
 of the edges along with covering requirements 
. The objective is to find a minimum cost vertex subset that covers at least 
 edges of 
 for each .

They obtained a -approximation for normal graphs, where c is a large constant used for Monte-Carlo randomized algorithms to ensure low error probability. They used randomized iterative rounding on a strong LP which is derived by knapsack inequalities on the natural LP. This approach generalizes to hypergraphs with an approximation guarantee of . They also showed that, even for normal graph for which , it is NP-hard to approximate this problem to a ratio better than 
, which means O(f)-approximation for this problem is unlikely to exist.

Wolsey [11] proposed the submodular set cover problem, which is a general formulation to the above covering problems, and presented an 
-approximation, where g is the input submodular function and S is the ground set. Chuzhoy et al. [4] presented a simpler analysis to obtain a similar result. Fujito [5] presented a primal-dual algorithm for this problem which is useful for some special cases such as the partial vertex cover problem.

Our Focus and Contributions In this paper, we consider the vertex cover problem with multiple covering constraints (VC-MCC) in hypergraphs. In this problem, we are given a hypergraph , a cost function 
, and a number of covering constraints 
, where each 
 is a subset of E and 
 is the covering requirement for 
. The objective is to find a minimum cost subset  such that, for each , at least 
 edges of 
 are covered by S.

This problem is a basic yet general form of classical vertex cover and the edge-partitioned vertex cover problem considered in [2].

In this paper, we present a primal-dual algorithm that yields an 
-approximation for this problem, improving over the previous ratio of  due to [2], where c is a large constant used to ensure a low failure probability for the Monte-Carlo randomized algorithm. Our main result is the following theorem.

Theorem 1
There is a deterministic 
-approximation algorithm for VC-MCC which runs in polynomial time, where 
 is the 
 harmonic number.

Compared to the previous result of , which is a Monte-Carlo type randomized algorithm, our algorithm is deterministic and pure combinatorial, which means that our algorithm does not rely on heavy Ellipsoid LP solvers for this basic problem. Furthermore, our algorithm does not compromise between the feasibility and the cost of the solutions. Considering the lower-bound of 
 on the approximation ratio due to [2] and the lower-bound of f known for vertex cover, our result narrows down the gap for the approximability of this problem.

The novelty of this work lies in the way how the dual variables are handled in a non-standard way. In contrast to previously known primal-dual approaches for covering problems, our algorithm manages a series of dual solutions simultaneously and carefully so that the following two criteria are met. First, during the dual-fitting process, the cost of any vertex to be opened in the future must be paid merely by the current unfulfilled covering constraints. Second, the overall dual value possessed by any unfulfilled covering constraint remains the same all the time. This makes the approximation guarantee of  possible.

Our result can be seen as a novel combination of the classical tight approximations with guarantees f and 
 for the covering problem using the language of LP primal-duality. Our side ingredient includes the strong LP relaxation due to [2], which is derived by applying Knapsack-cover inequalities to the natural LP.

We would like to remark, however, that the usage of strong LP relaxation in our result is not a necessity but rather a better and more intuitive way to illustrate our ideas on how the dual variables can be managed, and obtaining the same result using natural LP is possible.

Organization of this paper The rest of this paper is organized as follows. In Sect. 2, we define the notations we will be using throughout this paper and introduce the strong LP formulations. We present our approximation algorithm in Sect. 3 and conclude with future directions in Sect. 4.

Preliminaries
We use  to denote a hypergraph G with a vertex set V and an edge set 
. Note that, under this notion, any edge  is a subset of V that consists the incident vertices of the edge e. We use 
 to denote the maximum cardinality of the edges in G, i.e., 
. The subscript G is omitted when no ambiguity is there in the context.

For any edge subset  and any vertex , we use M(v) to denote the set of edges in M that are incident to v, i.e., . For any subset , we use M(A) to denote the set of edges in M that are incident to the vertices in A, i.e., 
.

The Partial Vertex Cover Problem with Multiple Coverage Constraints
In this problem, we are given a hypergraph , a cost function 
, and a number of covering constraints 
, where for each , 
 is a subset of E and 
 is the covering requirement for 
 to be fulfilled.Footnote 1

The objective of this problem is to find a vertex subset  of minimum cost such that 
 for each .

Intuitively, each of the 
 pair describes a partial covering constraint that requires that, among the edges in 
, at least 
 of them must be covered. The goal of this problem is to compute a minimum cost subset that simultaneously fulfills all of the covering constraints. A natural LP relaxation for this problem is given in Fig. 1.

Fig. 1
figure 1
A natural LP relaxation for VC-MCC

Full size image

We have two sets of indicator variables in this LP formulation: For each , 
 denotes the inclusion of v into the cover and 
 for each  indicates the coverage of e by the vertices chosen in the cover. The first inequality models the coverage of each edge  and the second inequality models the covering requirement for each 
, .

However, the integrality gap of the natural LP can be arbitrarily large. This is illustrated by the following simple example. Consider a star with  vertices. Suppose that the cost of every vertex is 1 and we only have one constraint consisting of all edges with covering requirement 1. The optimal integral cost for this example is 1 while its optimal fractional cost is 1/d, resulting a gap of d which can be arbitrarily large.

A Strong LP Relaxation Instead of using the natural LP relaxation, we use a strong LP relaxation due to [2], which is derived by applying Knapsack-cover inequalities to the natural LP given above.

For any vertex subset  and any , define


 

Intuitively, 
 denotes the residue covering requirement to be fulfilled for 
, if the vertex set A were already chosen as part of the cover.

For any vertex , define


 

Intuitively, 
 is the amount of covering requirement v can be fulfilled for 
 if A is already chosen as part of the cover. Clearly, 
 will be either 
 or the number of incident edges of v in 
, which is 
.

The strong LP relaxation we consider is as follows:

figure a
To see that LP-(S) gives a valid relaxation for VC-MCC, consider any feasible integral solution 
. It suffices to show that 
 is also contained in the feasible region of LP-(S).

Consider an arbitrary subset A of V and any constraint . Clearly, 
 must remain feasible even if the vertices of A were already chosen as part of the cover in advance for free. Hence, the number of edges 
 covers for 
 is at least 
, and the inequality

must hold. To see that LP-(S) is indeed a stronger relaxation than LP-(N), let us consider the simple star example and the inequality with respect to . Clearly, 
 for all vertices v in this star. As a result, we have a constraint 
, and the optimal fractional solution will also be 1.

The Dual LP for LP-(S) In this paper we will be working around the dual LP of LP-(S), which is given as follows.

figure b
Our Approximation Algorithm for VC-MCC
In this section, we present our approximation algorithm for VC-MCC. Let


 

be an instance of this problem. Our algorithm will compute a series of feasible LP solutions of  to LP-Dual-(S). During this process, a feasible cover for  will gradually be formed. The approximation guarantee is then established by comparing the cost of the cover to the values of the dual solutions the algorithm computes.

We describe the algorithm in ยง3.1 and establish the approximation guarantee in ยง3.2.

The Algorithm
The algorithm takes as input an instance  of VC-MCC and outputs a feasible cover S for .

The algorithm maintains a feasible dual solution 
 which is initially zero and a set K which contains the set of covering constraints that have not been satisfied yet. Initially, S is set to be an empty set and K is set to be . We make a note that, the vector 
 maintained during the process will be sparse, and the algorithm only keeps track of its nonzero components.

In the following, we first describe our primal-dual process. Then we describe how this primal-dual process can be turned into a polynomial-time algorithm. Our primal-dual process, denoted PD-VC-MCC, starts with a trivial dual solution for which 
 for all  and all . In each iteration, it proceeds as follows:

1.
Raises 
 at the rate of 
 for all  until the Inequality (*) of some vertex in LP-Dual-(S), say, , becomes tight. Then the algorithm declares v open and adds it to the set S.

2.
For each constraint  with 
 becoming zero after v is added to S, the algorithm sets 
 to be zero for all  and removes i from K.

This process repeats until the set K becomes empty. Then S is returned as the approximate solution. Figure 2 summarizes this primal-dual process for further reference.

We remark that, the step 2.(a) above of resetting 
 to zero for all  when i is to be removed from K is crucial in obtaining a guarantee of 
. The reason is that it allows the overall contribution of the remaining covering constraints to remain balanced.

Fig. 2
figure 2
A high-level description of our primal-dual process PD-VC-MCC

Full size image

Making PD-VC-MCC a Polynomial-Time Algorithm Given the dual-fitting process as described above, it is straightforward to derive a corresponding polynomial-time algorithm. For completeness, in the remaining of this section we describe the corresponding polynomial-time algorithm Approx-VC-MCC.

The algorithm Approx-VC-MCC mimics the operations of the above primal-dual process. Instead of maintaining the dual variables explicitly, it keeps track of the slack of the Inequality (*) in LP-Dual-(S) for each vertex, i.e., the residue before it becomes tight.

For each , let 
 denote the slack of the vertex constraint v before it becomes tight. Initially, 
 is set to be 
. We need a notion that reflects the raising process of the dual variables. For each  and any particular , define


 
 
 

Intuitively, s(v, A) denotes the speed for which 
 will decrease if we raised the dual variables 
 at the speed of 
 for all . Furthermore, in order for the update of 
 for each  to proceed, we use 
 to denote the contribution of dual variables 
 towards 
, for all nontrivial A. 
 is initialized to be zero for all  and .

Now we formally describe the algorithm Approx-VC-MCC. In each iteration, the algorithm proceeds as follows.

1.
First, the algorithm finds the among the vertices in  the one with the smallest ratio of 
. Formally speaking, it computes


 
 
 
 

Intuitively, v is the first vertex constraint to become tight in this iteration in the primal-dual process and 
 is the corresponding amount of time it takes.

2.
For each , the algorithm updates 
 by setting 
. For each , the algorithm updates 
 by setting 
 
.

3.
The algorithm adds v to the set S.

4.
For each  such that 
 is zero, the algorithm updates 
 for all  by setting 
 and remove i from K.

Then the process repeats until the set K becomes empty, and the set S is returned as the approximate solution. To see that algorithm Approx-VC-MCC simulates the execution of PD-VC-MCC, it suffices to observe that, for each iteration the followings hold.

 keeps track of the contribution of  towards v, namely, 
, so that the operation of resetting 
 to zero for i is reflected correctly.

For each vertex , 
 correctly records the current slack of the constraint (*) for vertex v, i.e., 
. Therefore, in Step 1, the algorithm correctly chooses the next vertex to become tight.

From the description and the above two conditions, it is clear that algorithm Approx-VC-MCC simulates the execution of the primal-dual process PD-VC-MCC. In the remaining of this section, we will focus on the procedure PD-VC-MCC.

Analysis
In this section, we provide the analysis for the procedure PD-VC-MCC and prove Theorem 1. First we show in the following lemma that the procedure always terminates and returns a feasible cover for the input instance . Then we establish the approximation guarantee.

Lemma 2
Procedure PD-VC-MCC terminates in polynomial number of iterations and returns a feasible cover.

Proof
Since PD-VC-MCC only terminates when the set K becomes empty and a feasible cover is found, it suffices to argue that PD-VC-MCC always terminates, provided that there is a feasible cover for the input instance.

Assume for contradiction that the input instance has a feasible solution, but PD-VC-MCC does not terminate. Consider the set S the process currently has. The process runs eternally since no vertex  can become tight and 
 is constantly raising for all . This implies that 
 for all  and all , which is a contradiction since all the edges have already been covered by S. 

Approximation Guarantee To establish the approximation guarantee, we compare the cost of the solution our algorithm returns to the values of the dual solutions our primal-dual process maintains, which will be valid lower-bounds for the cost of optimal solutions by the weak LP duality.

Let 
 denote the cover returned by the algorithm, where the indices of the vertices denote the order for which they are added to the set S. For any , we use 
 to denote the set of the first j vertices that are added to S, i.e., 
.

Without loss of generality, we also assume that the covering constraints 
 are fulfilled by the algorithm in this order.

For any , let  denote the index for which the inclusion of 
 into S fulfills 
. Consider the moment when 
 is just fulfilled, i.e., when i was removed from K by the algorithm, and 
 has not yet been reset. Let 
 denote the dual solution the algorithm maintains at this moment, and 
 denote the objective value of 
. It follows that


 
  
 
 

(1)

where the second equality holds since our algorithm resets 
 to zero for all  and all .

In the above equality we write 
 as the sum of dual values each unfulfilled covering requirement possesses. The following lemma says that the dual value possessed by each unfulfilled constraint is the same.

Lemma 3
For any  and any 
 with 
, we have


 
 
 

Proof
This lemma follows directly from the way our primal-dual approach handles the dual variables. Since 
 only changes when a new vertex becomes tight and since we always raise 
 for each  at the rate of 
, the total dual value possessed by any 
 with  will be the same. 

In the following we analyze the cost of S and relate it to the dual values of 
 the algorithm maintains for all .

Consider a vertex  and the moment when v just becomes tight. Suppose that at that time, the algorithm has already fulfilled t covering constraints. Then, from the Inequality (*) of LP-Dual-(S), it follows that


 
 
 

where the second equality holds since, by design, our algorithm has already reset 
 to zero for all  and all  before v becomes tight.

For each , define 
. Then we have

Intuitively, 
 is the share for which the covering constraint i contributes towards the cost of vertex v. We will charge the cost of v to the covering constraints 
 for all , each of which gets a charge of 
.

For each , let 
 denote the total charge 
 receives from the vertices in S. We have the following lemma, which bounds 
 using the dual value it possesses in 
.

Lemma 4
For any , we have


 
 

Proof
From the definition of 
, only 
 will charge 
, and it follows that


 
  
 
  
 
  
 
 

Hence, to prove this lemma, it suffices to show that


  
 
 
 

(2)

To prove Ineq. (2), we first prove the following inequality:


  
 
 
 

(3)

Compare the l.h.s. and the r.h.s. of (3), it suffices to argue that


 
 

Consider any fixed j with  and any t with . Since 
 is not the vertex whose inclusion into S fulfills 
, it follows that 
, and hence 
. Furthermore, under the condition that 
 has already been chosen, the inclusion of 
 into S does not fulfill 
. This implies that 
. Therefore, we have


 
 
 
 

where the second last inequality holds since the size of each hyperedge is at most f. This proves Ineq. (3).

Second, observe that the following inequality holds


 
 
 

(4)

since 
 by the definition of 
.

From Ineq. (3) and Ineq. (4), the Inequality (2) is proved and this lemma holds. 

In the following we establish the approximation guarantee of our algorithm.

Lemma 5
where 
 is the 
 harmonic number and OPT is the cost of any optimal solution.

Proof
By Lemma 4 and the definition of 
, we have


 
  
 
 

By Lemma 3, for each , we have


 
 
 

Since each 
 is a feasible dual solution for LP-Dual-(S), we have 
 for all , and


 
 
 

as claimed. 

Conclusion
We conclude with future directions and open problems. First, considering the lower-bounds of 
 and f for this problem, our 
-approximation ratio has an extra 
 factor in it. However, it seems unclear how this excess 
 factor can be dropped.

Although the approaches of [5, 6, 9] can be used to obtain tight f-approximation for the partial vertex cover problem, it seems difficult to adopt their techniques to our problem. The reason is that, when multiple covering constraints exist, it seems intricate how the key properties of their approaches can be ensured simultaneously for each covering constraint. We believe that this would be an interesting direction to explore.

Second, clarifying the exact lower bound of approximation ratio for this problem is also interesting. For now, 
 is what we only know.