The  genetic algorithm is a younger evolutionary algorithm trying to profit also from inferior solutions. Rigorous runtime analyses on unimodal fitness functions showed that it can indeed be faster than classical evolutionary algorithms, though on these simple problems the gains were only moderate. In this work, we conduct the first runtime analysis of this algorithm on a multimodal problem class, the jump functions benchmark. We show that with the right parameters, the  GA optimizes any jump function with jump size  in expected time 
, which significantly and already for constant k outperforms standard mutation-based algorithms with their 
 runtime and standard crossover-based algorithms with their 
 runtime guarantee. For the isolated problem of leaving the local optimum of jump functions, we determine provably optimal parameters that lead to a runtime of 
. This suggests some general advice on how to set the parameters of the  GA, which might ease the further use of this algorithm.

Introduction
The  genetic algorithm,  GA for short, is a relatively new genetic algorithm, first proposed at GECCO 2013 [15], that tries to increase the rate of exploration by a combination of mutation with a high mutation rate, an intermediate selection, and crossover as mechanism to repair the possible negative effects of the aggressive mutation. For this algorithm, moderate runtime advantages over classic algorithms have been proven for unimodal [13, 16] or close-to-unimodal [10] problems; also some positive experimental results exist [35, 41].

In this work, we conduct the first mathematical runtime analysis for the  GA optimizing a multimodal optimization problem, namely the classic jump functions benchmark. We observe that the combination of aggressive mutation with crossover as repair mechanism works even better here: The  GA can optimize jump functions with gap size  in expected time at most

which is almost the square root of the 
 runtime many classic mutation-based algorithms have. To obtain this performance, however, the parameters of the algorithm have to be set differently from what previous works recommend.

The  GA
Noting that many classic evolutionary algorithms do not profit a lot from inferior solution, whereas algorithms witnessing the black-box complexity [21] (see also [14] for a recent survey), massively do, Doerr, Doerr, and Ebel [15] proposed an algorithm which tries to gain some insight also from solutions inferior to the current-best solution.

The main working principle of their algorithm, which was called  GA, is as follows. From a unique parent individual x, first  offspring are created using standard bit mutation with a relatively high mutation rate p, but in a way that all offspring have equal Hamming distance to x (this can be realized, for example, by first sampling a number  from a binomial distribution with parameters n and p and then generating all offspring by flipping exactly  random bits in x). When the parent is already close to the optimum, these most likely are all worse than the parent. The hope set into the  GA is that nevertheless some mutation offspring, besides all destruction from the aggressive mutation, has also made some progress. To distill such progress the  GA selects a mutation offspring 
 with maximal fitness and creates from it,  times independently, an offspring via a biased uniform crossover with the parent x. This biased crossover inherits bits from 
 only with some small probability c, so that, hopefully, all the destruction caused by the aggressive mutation is repaired. The best of these crossover offspring in an elitist selection competes with x for becoming the parent of the next iteration. The recommendation in previous works was to use a crossover bias of 
 
. With this parameterization, a single application of mutation and crossover with the parent, without intermediate selection, would create an offspring distributed as if generated via standard bit mutation with mutation rate 
 
. Note that 
 
 is a common recommendation for the mutation rate in standard bit mutation [7, 42, 52].

Via a rigorous runtime analysis on the OneMax benchmark function it was shown [16] that the basic idea of the  GA indeed can work. When the crossover biased is set to 
 
 as recommended, then the expected runtime (number of fitness evaluations) of the  GA with any mutation rate 
 
 and offspring population size  is


 
 
 

Hence any choice of 
 
 
 and  yields a runtime asymptotically faster than the runtime  observed by many classic evolutionary algorithms, e.g., by the  EA  [42], the  EA  [38], the  EA  [51], the  EA  [4], and in fact any unary unbiased black-box algorithm [40]. The choice 
 
 and  minimizes the runtime guarantee above and shows an expected runtime of . With a fitness-dependent [16], self-adjusting [13], or heavy-tailed random parameter choice [1], the runtime further improves to O(n). Clearly, these are not a drastic improvement over, say, the  runtime of the  EA, but one has to admit that the room for improvement is limited: The unrestricted black-box complexity of the OneMax function class is 
 
 [21, 32], hence no black-box optimizer can optimize all functions isomorphic to OneMax in a time better than 
 
.

A runtime analysis [10] of the  GA on the random satisfiability instances regarded in [48] showed a similar performance as on OneMax. This is caused by the structure of these random instances, which renders them similar to OneMax to the extent that also the  EA has an  performance [24]. At the same time, these instances do not have the perfect fitness-distance correlation of the OneMax function, and this indeed needed to be taken into account when setting the parameters of the  GA in [10]. A runtime analysis of the  GA on LeadingOnes  [5] showed that for this problem, the  GA with any 
 
 has asymptotically the same runtime of 
 as many other algorithms.

Empirical studies showed that the  GA works well (compared to classic EAs) on linear functions and RoyalRoad functions [16], on the MAX-3SAT problem [35], and on the problem of hard test generation [41].

Multimodal Problems
Clearly, the usual application of evolutionary algorithms are problems with multimodal landscapes, that is, with non-trivial local optima, and these local optima often present a difficulty for the evolutionary algorithm. In the runtime analysis perspective multimodal problems have displayed very different optimization behaviors. For example, on multimodal landscapes it has been observed that crossover can recombine solutions into significantly better ones [39, 49, 50], that mutation rates significantly larger than 
 
 can be preferable [23], and that probabilistic model-building algorithms such as estimation-of-distribution algorithms and ant-colony optimizers can significantly outperform classic algorithms [9, 22, 28, 36].

In this light, and given that all previous runtime analyses for the  GA consider unimodal or almost unimodal problems, we feel that it is the right time to now investigate how the  GA optimizes multimodal problems. Being the most studied multimodal benchmark in runtime analysis, we regard jump functions. These have a fitness landscape isomorphic to the one of OneMax except that there is a valley of low fitness around the optimum. Consequently, a typical hillclimber and also most evolutionary algorithms quickly run into the local optimum consisting of all points on the edge of the fitness valley, but then find it hard to cross the fitness valley.

More precisely, the jump function class comes with a difficulty parameter k, which is the width of the valley of low fitness. The fitness is essentially the fitness of OneMax except for all search points with Hamming distance between one and  from the optimum. Consequently, the only way to leave the local optimum to a strictly better search point is to flip exactly the right k bits and go to the optimum.Footnote 1 For this reason, it comes as no surprise that many mutation-based evolutionary algorithms need 
 time to optimize such a jump function [20, 26]. Using a higher or a heavy-tailed random mutation rate [23] or stagnation detection mechanisms [45,46,47] the runtime can be reduced, but not below 
 
. Crossover can be helpful, but the maybe most convincing work [18] in this direction also only obtains a runtime of 
 with the standard mutation rate and 
 with a higher mutation rate. With additional techniques, runtimes up to O(n) were obtained [17, 33, 43, 54], but the lower the runtimes become, the more these algorithms appear custom-tailored to jump functions (see, e.g., [53]). The extreme end is marked by an 
 
 time algorithm [11] designed to witness the black-box complexity of jump functions.

Our Results
Our main result is a runtime analysis of the  GA on jump functions for all jump sizes 
 
. Since we could not be sure that the parameter suggestions from previous works are still valid for our problem, we consider arbitrary values for the mutation rate p, the crossover bias c, and the offspring population size . This turned out to be the right decision as we observed much better runtimes with novel parameter values.Footnote 2 We also allowed different offspring population sizes 
 and 
 for the mutation and crossover phase, which however did not lead to stronger runtime guarantees.

For all 
 
 and for arbitrary values of these four parameters (except for the only constraint 
 
), we prove that the  GA when started in the local optimum of 
 crosses the fitness valley in expected time (number of fitness evaluations) at most


 
 
 

where 
 is a constant in [0.1, 1]. When ignoring the hidden constants in the 
 factor, this bound is optimized for 
 
 and 
 and then gives a runtime of

This time bound is asymptotically optimal, that is, no other parameter values can obtain a faster expected runtime (apart from the unspecified 
 factor).

When not starting in the local optimum, but with an arbitrary initial solution or the usual random initialization, the  GA reaches the local optimum in an expected time of 
 iterations, if 
 
 and 
 and 
 are at least 
 
. Therefore, large population sizes are not beneficial in this first easy part of the optimization. With slightly smaller values for the population sizes as above, namely 
, the expected runtime is

Similar as in the previous results on OneMax, a speed-up over classic algorithms is observed for larger ranges of parameters, though these are harder to describe in a compact fashion (see Corollary 10 for the details).

The result above shows that the power of the  GA becomes much more visible for jump functions than for the problems regarded in previous works. Concerning the optimal parameter values, we observe that they differ significantly from those that were optimal in the previous works. In particular, the relation of mutation rate and crossover bias is different. Whereas in previous works  was a good choice, we now have . A moment’s thought, however, shows that this is quite natural, or, being more cautious, at least fits to the previous results. We recall that pcn is the expected Hamming distance of the parent from an individual generated from one isolated application of mutation and crossover. The previous works suggested that this number should be one, since one is also the expected distance of an offspring generated the classic way, that is, via standard bit mutation with mutation rate 
 
.

Now for the optimization of jump functions, where a non-trivial local optimum has to be left, it makes sense to put more weight on larger moves in the search space. More specifically, the work [23] has shown that the optimal mutation rate for the  EA optimizing jump functions is 
 
. Hence for the classic  EA, the best way of generating offspring is such that they have an expected Hamming distance of k from the parent. Clearly, this remains an intuitive argument, but it shows that also when optimizing multimodal problems, the intuitive approach of previous works, which might help an algorithm designer, gave the right intuition.

Our recommendation when using the  GA for multimodal optimization problems would therefore be to choose p and c larger than in previous works, and more specifically, in a way that pcn is equal to an estimate for the number of bits the algorithm typically should flip. Here “typically” does not mean that there are actually many moves of this size, but that this is the number of bits the algorithm has to flip most often. For example, when the  EA optimizes a jump function, it will maybe only once move to a search point in distance k, however, it will nevertheless need many offspring in distance k until it finds the right move of this distance.

From our rigorous analysis, we conclude that the  GA is even better suited for the optimization of multimodal objective functions, and we hope that the just sketched intuitive considerations help algorithm designers to successfully apply this algorithm to their problems.

Research conducted after ours In [3], it was shown that the non-trivial choice of the parameters of the  GA when optimizing multimodal problems can partially be overcome by using heavy-tailed random parameter values. If we choose  from a power-law distribution with exponent 
 and set 
 
, where s follows another power-law distribution with exponent 
, then for all  the runtime of the  GA on 
 is 
 for any small constant , which is only by a 
 factor larger (and for some k and  even smaller) than the upper bound for the optimal static parameters (apart from the unspecified 
 factors).

In [2] it was further shown that if all three parameters of the  GA are chosen independently, then the runtime stays the same, namely 
. The empirical analysis in [2] also shows that the  GA with the heavy-tailed choice of parameters significantly outperforms the  EA on small jump sizes ( and  were considered).

Preliminaries and Notation
Notation
By  we understand the set of positive integers. We write [a..b] to denote an integer interval including its borders and (a..b) to denote an integer interval excluding its borders. For  the notion [a..b] means . For the real-valued intervals we write [a, b] and (a, b) respectively. For any probability distribution  and random variable X, we write  to indicate that X follows the law . We denote the binomial law with parameters  and  by .

The  GA
The main idea of the  GA discussed in Sect. 1.1 is realized as follows. The  GA stores a bit string x that is initialized with a random bit string. After the initialization it performs iterations which consist of a mutation phase and a crossover phase until some stopping criterion is met.

In the mutation phase the algorithm first chooses the mutation strength  from the binomial distribution with parameters n and p. Then it creates 
 mutants 
, each of them is a copy of x with exactly  bits flipped. The positions of the flipped bits are chosen uniformly at random, independently for each mutant. The goal of this design of the mutation phase is to generate each of 
 offspring via standard bit mutation, but conditional on that all offspring have the same distance to their parent x. The mutant 
 with the best fitness is chosen as a winner of the mutation phase.

In the crossover phase the algorithm creates 
 offspring 
 
 by applying a biased crossover to x and 
. The crossover operator for each position takes a bit value from x with probability  and it takes a bit value from 
 with probability c (independently for each position and each offspring). If the best offspring y is not worse than x then it replaces x. The pseudocode of the  GA optimizing a pseudo-Boolean function f is shown in Algorithm 1.

figure a
We intentionally do not specify a stopping criterion, which is a common practice in theoretical studies. The goal of our analysis is to determine the expected runtime of the  GA until it finds an optimal solution. By the runtime we understand the number of iterations or fitness evaluations which the algorithm performs. Since each iteration of the algorithm uses exactly 
 fitness evaluations, the transition between these two measures of runtime is trivial.

Jump Functions
The class of jump functions is defined through the classic OneMax function, which is defined on the space of bit strings of length n and returns the number of one-bits in its argument. In formal words,

This function despite its simplicity has given a birth to many fundamental results, e.g. [12, 19, 29,30,31, 38, 44, 52]. In particular, the analysis of the black-box complexity of OneMax  led to the development of the  GA  [16].

The 
 function with parameter  is then defined as follows.


 
 

Fig. 1
figure 1
Plot of the 
 function. As a function of unitation, the function value of a search point x depends only on the number of one-bits in x

Full size image

A plot of 
 is shown in Fig. 1.

Useful Tools
In this section we provide some useful tools which we use in our proofs. We start with the following inequality which we use for multiple times in our proofs.

Lemma 1
Assume  and . Then

Proof
By [44, Lemma 8] we have 
 
. Hence,


 
 
 
 
 


We also make a use of Chernoff bounds (see Theorem 1.10.1 and 10.10.5 in [27]) to show the concentration of some random variables involved in our analysis. We use the following lemma, which is a particular case of these bounds for the random variables following a binomial distribution.

Lemma 2
(Chernoff Bounds) Let X be a random variable following a binomial distribution . Then for all  the probability that  is at most 
 
 and the probability that  is at most 
 
. Also for all  the probability that  is at most 
 
.

The following lemma shows the concentration of the number of the bit flips in the mutation phase by the Chernoff bounds.

Lemma 3
Let 
 
. Then the number  of the bits flipped by the mutation operator of the  GA is in [pn, 2pn] with at least constant probability 
, if n is at least some sufficiently large constant.

Proof
Recall that the number  of the flipped bits is chosen according to the binomial distribution . We first consider the case when p is small. Assume . Then


 
 
 

Note that


 
 
 
 

To estimate 
 
 we consider function 
 
 in the interval [1, 9] (since we now consider only these values of pn). First we note that 
 and further we find its infimum in (1, 9]. Note that f(x) is increasing in each interval , hence we can bound it from below by 
 
. Note also that g(x) is a non-increasing function in (1, 9], thus  for all . Consequently, for n which is large enough we have


 

Now we consider the case when . Since  follows the binomial distribution with parameters n and p, we have . By the Chernoff bounds we have


 
 
 

By Theorem 10 in [25] we have the following bound on the probability that the binomial distribution exceeds its expectation.

Hence,

Therefore, by the union bound the probability 
 that  is at least


 
 
 

Since we assume that , we obtain

and hence,

Therefore


We also state a similar lemma for the larger mutation rates (which are of a greater interest when we aim at escaping the local optimum).

Lemma 4
Assume 
 
. Then the number  of the bits flipped by the mutation operator is in 
 
 with probability 
 
.

Proof
By the Chernoff bounds and by Theorem 10 in [25] we have


 
 
 
 
 
 


We also encounter random variables with hypergeometric distribution. A particular example of such random variable is the number 
 of zero-bits which are flipped by the mutation operator after the total number  of the bits to flip is already chosen. This random variable follows a hypergeometric distribution with parameters n,  and . For this random variable the Chernoff bounds are also applicable [27, Theorem 1.10.25]. We use the following special case of this bound.

Lemma 5
Let x be a bit string with exactly d zero-bits. Let 
 be the number of zero-bits of x which are flipped by the mutation operator of the  GA after  is chosen. Then the probability that 
 
 is at most 
 
.

Upper Bounds
In this section we analyse the  GA with general parameters on 
 and show upper bounds on its runtime. We recede from the standard parameter setting of the  GA, since the intuition behind these parameters values (that is, the intent to have only a single bit flipped if we consequently apply mutation and crossover operators) suggests that they are not efficient to escape local optima.

We observe that the working principles of the  GA, which were shown to be efficient on OneMax, are also successful on Jump. That is, we show that in the mutation phase the algorithm detects a beneficial mutation (which flips all missing zero-bits of the current individual to ones) by inspecting the fitness of the offspring and then crossover is capable to repair all wrong bit flips made in the mutation phase. We also show that the optimal mutation rate for performing a jump is 
 
, which is very different from the optimal one for optimizing OneMax (
 
 when we are in distance d from the optimum). At the same time the optimal crossover bias, which is also 
 
 is very similar to 
 
, which is optimal for OneMax.

We split our analysis into two parts. First we find the expected time the  GA needs to perform a jump to the global optimum when it is already in the local optimum. Then we complete the story by considering the runtime until the  GA gets to the local optimum starting in a random bit string.

We do not consider the case when , since 
 coincides with OneMax, which is already well-studied in the context of the  GA (see [13] for the full picture). We also omit considering too large values of k (namely, 
 
) since they do not give much new insight about the  GA, while they require more complicated arguments for our results to hold.

We also constrain ourselves to the case 
 
 so that once we get to the local optimum we have a decent probability to flip at least 2k bits. Since all mutation offspring have the same Hamming distance from the parent, this implies that an individual with k zero-bits flipped will have a better fitness than any other offspring and therefore selected as the winner of the mutation phase 
. Without this assumption an individual with all zero-bits flipped to one might occur in the fitness valley, thus it is not detected as the mutation phase winner. Hence, the jump to the global optimum becomes more challenging for the algorithm, which makes this parameter setting not really promising to be effective on multimodal functions.

Escaping the Local Optimum
In this section we analyse how the  GA leaves the local optimum. Although by runtime we understand the time until the optimum is sampled, it is fair to consider the time until x becomes the optimum for at least two reasons. (i) By disregarding the event that the optimum is sampled in the mutation phase, we still get an upper bound on the runtime. (ii) Since the probability to sample the optimum in the mutation phase is small compared to the probability to sample the optimum in the crossover phase, we expect to lose only a little. Due to the elitist selection the only chance to leave the local optimum is to find the global optimum in one iteration. For this it is sufficient that the following two consecutive events happen.

1.
The mutation phase winner 
 has all k bits which are zero in the current individual x flipped to one.

2.
The crossover winner y takes all k bits which are zero in x from 
 and all bits which are zero in 
 from x.

We first estimate the probability of the first event and then estimate the probability of the second event conditional on the first one.

We call the mutation phase successful if all k zero-bits of x are flipped to one in 
 (and possibly some one-bits are flipped to zero) and the number  of the flipped bits is at most 2pn. We estimate the probability 
 of having a successful mutation phase in the following lemma.

Lemma 6
Let 
 
. If 
 
, then we have

where 
 is as defined in Lemma 3, which is .

Proof
If  then we flip at least k one-bits in each mutant, hence the fitness of each mutant is at most . Therefore, if there is at least one individual with all k zero-bits flipped, then this individual has a greater value of 
 than any other individual which does not have all zero-bits flipped. Hence, such an individual is chosen as the mutation winner 
. Therefore, for a successful mutation phase it suffices that the following two events occur (in this order).

The number of flipped bits  is in [pn, 2pn].

The k zero-bits of x are among the  chosen bits in at least one of the 
 offspring. We call such offspring good in this proof.

By Lemma 3 the probability of the first event is 
. We condition on this event in the remainder. The probability 
 that one particular offspring is good is 
 
 
 
. By the assumption that 
 
 we have


 
 
 
 
 
 
 
 
 
 

The probability that at least one offspring is good is 
. By Lemma 1, we estimate


 
 
 
 

Therefore, we conclude


 
 
 
 
 


Now we proceed with the crossover phase. We call the crossover phase successful (conditional on a successful mutation phase) if the winner y takes all bits which are zero in 
 from x (where they are ones) and all k bits which are zero in x from 
 (where they are ones). We denote the probability of a successful crossover phase by 
.

Lemma 7
Assume that 
 
 and the mutation phase was successful. Then


 
 

Proof
To generate an optimal solution in one application of the crossover operator we need to take k particular bits from 
 and  particular bits from x. The probability 
 to generate such a crossover offspring is


 

since a successful mutation implies that . The probability to generate at least one such offspring is


 
 

where the last inequality follows from Lemma 1. 

With Lemmas 6 and 7 we are capable of proving the upper bounds on the expected runtime until the  GA escapes the local optimum. We estimate the runtime both in terms of the number fitness evaluations and the number of iterations, denoted by 
 and 
 respectively.

Theorem 8
Let 
 
. Assume that 
 
 and 
 is as defined in Lemma 3. Then the expected runtime of  GA on 
 is


 
 
 

iterations and


 
 
 

fitness evaluations if the algorithm starts in the local optimum.

Proof
When the algorithm is in the local optimum it stays there until it moves to the optimum. During this time in each iteration it has the same probability P to move into the global optimum, which is the probability that a successful mutation phase is followed by a successful crossover phase:


 
 
 
 

Hence we obtain an expected optimization time in terms of iterations of


 
 
 
 

In each iteration the  GA performs exactly 
 fitness evaluations, which gives us an expected number of


 
 
 

fitness evaluations in total. 

With help of Theorem 8 we deliver good values for the parameters, namely 
 
 and 
 
. We omit the proof that these parameters yield the lowest upper bound (apart from optimizing the 
 factor), since it is just a routine work with complicated derivatives, but we state the runtime bounds resulting from these settings in the following corollary. In order to use this result in Sect. 3.2 we also formulate this theorem for general population sizes.

Corollary 9
Let 
 
. Assume that 
 
 and 
 
. Then the expected runtime of  GA on 
 is 
 fitness evaluations and 
 iterations, if it starts in a local optimum of 
. For 
 
 these bounds are 
 and 
 
.

Proof
With 
 
 we have 
 
 and 
. Consequently, by Theorem 8 we have


 
 
 
 
 
 
 
 
 
 
 
 
 
 

where 
 is a constant defined in Lemma 3. By the estimate 
 
 which holds for all 
 
 and by 
 
 
 
 we have


 
 
 
 
 
 
 
 
 
 

(1)

The expected number of fitness evaluations is 
 times greater, hence we have

Putting 
 
 into (1) and (2) we have 
 and 
 
. 

In the following corollary we show a wide range of the parameters, which yield a better upper bound than the mutation-based algorithm (apart from the 
 factor) for the sub-linear jump sizes. We do not show it for , since in this case the upper bound given by Corollary 9 is 
, which is not better than the runtime of best mutation-based EAs.

Corollary 10
Let  and . Assume that 
 
, 
 
 and 
 
. Define 
 
 and 
. If  and  are at most one and 
 
 and 
 
, then the expected number of fitness evaluations until the  GA reaches the global optimum starting from the local optimum of 
 is

Before we prove the corollary we shortly discuss how one can choose the parameters that give us 
 
 runtime with Corollary 10. First we should choose p. It can be any value which is 
 
 and which is o(1). Then with the chosen value of p we can choose any c which is on the one hand 
 
, but on the other hand 
 
. Note that the closer p is to , the smaller the range for c (thus we could not choose , since in this case we cannot simultaneously satisfy 
 
 and 
 
).

After we determine p and c, we can choose 
 and 
. For 
 the upper bound for the possible range is 
 
, which follows from condition 
 
. The lower bound for 
 is 
 
, which follows from the condition 
 
. For 
 we have similarly obtained bounds, which are 
 and 
 
.

Generally, the choice of the 
 and 
 should be made in such way that they were as close as possible to the inverse probabilities of creating a good offsprings in the mutation and crossover phases respectively. By Lemma 1 this choice yields a  probability of a successful iteration. Any smaller population size reduces this probability (usually greater than it reduces the cost of one iteration), while any greater population size only increases the cost of each iteration without significantly increasing the success probability.

Proof of Corollary 10
Since  and  are at most one, the runtime given by Theorem 8 is simplified to


 
 
 
 
 
 

We want both terms to be 
 
. For the first term it is sufficient if the three following conditions hold. (i) 
 
, which holds if 
 
. (ii) 
. For this it is sufficient to have 
 
, since then we have


 
 
 
 

(iii) 
 
, since this implies


 
 
 
 
 

For the second term it is enough that the following two conditions hold. (i) 
 
 
, for which it is sufficient to have 
 
. (ii)  should not be too small, namely, 
 
, since it implies


 
 
 
 
 
 


We find it interesting to show that the standard parameter setting does not give us such a good upper bound. Note, however, that our lower bound given in Theorem 16 allows the actual runtime of the  GA with standard parameters to be better.

Theorem 11
Let 
 
. Assume that 
 
, 
 
 and 
 for some .

If 
 
, then the expected runtime of  GA on 
 is 
 iterations and 
 fitness evaluations.

If 
 
, then the expected runtime of  GA on 
 is 
 iterations and 
 fitness evaluations.

Proof
Since the standard parameter setting with  satisfies the conditions of Theorem 8, we obtain


 
 
 
 
 

Note that the second minimum in the denominator is always equal to its second argument. To understand the first minimum we consider two cases.

Case 1 When 
 
, the first minimum is equal to its second argument. Therefore, we have


 
 
 
 
 
 
 

In each iteration the  GA performs  fitness evaluations, thus we have

Case 2 When 
 
, first minimum is equal to its first argument. Therefore, we have


 
 
 
 
 
 

In each iteration the  GA performs  fitness evaluations, thus we have

Note that this upper bound is minimized when 
 
 (rounded up or down), in this case the expected runtime is


 
 
 


Reaching the Local Optimum
In the previous section we showed that the  GA with non-standard parameters setting can find the global optimum of 
 much faster than any standard mutation-based algorithms if the algorithms are started in the local optimum. However, the non-standard parameter setting includes an unnaturally large population size, which makes each iteration costly. At the same time, there is no guarantee that we increase the fitness by much in one iteration, which makes us pay with many fitness evaluations before we reach the local optimum. Hence we question how much the runtime with this parameter setting increases when we start at a random bit string. In this section we show that slightly changing the parameters we can obtain the runtime which is only by a  factor greater than the runtime when we start in the local optimum. The main result of this section is the following theorem.

Theorem 12
Let 
 
. If 
 
 
 and 
 
, then the expected runtime of the  GA with any initialization on the 
 function is at most 
 
 fitness evaluations.

To prove Theorem 12 we first analyse the runtime until the  GA reaches the local optimum of 
.

Theorem 13
Let 
 
 and 
 
. Then the expected time until the  GA reaches the local optimum of 
 with 
 
 is at most 
 iterations.

The main challenge in the proof of this theorem is that an offspring close to the optimum in the mutation phase can lie in the fitness valley and thus it is not selected as 
. In this section we call the mutation phase successful if the winner has at least one zero-bit of x flipped to one. If such a bit exists, we call it critical and we call a mutation phase offspring which has such a bit and does not lie in the fitness valley good. We write 
 to denote the probability of a successful mutation phase. We call the crossover phase successful if the winner of the crossover phase has inherited the critical bit from 
 and all other bits are not changed compared to x (hence, the name “critical”, since we need such bit to be taken from 
 for a successful iteration). We write 
 to denote the probability of this event.

To prove Theorem 13 we show two auxiliary lemmas for the mutation and crossover phases respectively.

Lemma 14
Let 
 
. If 
 
 and 
 
, then 
.

Proof
We denote the current distance to the global optimum by d. In order to create a good mutant we need to flip at least one zero-bit, but we also need not to flip too many zero-bits (namely, not more than 
 
) not to reach the fitness valley. Let 
 be the number of the flipped zero-bits in a fixed mutant. Then the probability 
 that the mutant is good is


 
 

We estimate the probability that we have not flipped a single zero-bit using Lemma 1.


 
 
 
 
 
 
 
 
 

To estimate the probability that we end up in the fitness valley, we use a Chernoff bound for the hyper-geometric distribution (Lemma 5). Note that the expected value of 
 is 
 
. Hence, we have


 
 
 
 
 
 
 
 
 

Considering the argument of the exponential as a function of d and computing its derivative (we omit tedious details), one can see that its value is maximized either at 
 
, if this value lies in , or at the bounds of this interval.

For 
 
 we have


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Since  and , we have 
 
. Hence, if 
 
, we have


 
 
 
 
 
 
 

Otherwise, if 
 
, then 
 
 only if . Therefore,


 
 
 
 
 
 
 

For  we have


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

if n is large enough.

For  we have


 
 
 
 
 
 
 
 
 
 
 
 

Therefore, we estimate the probability to create a good mutant as


 
 
 
 
 
 
 

where the last inequality holds when n is at least some sufficiently large constant. If 
 
, this probability is already  and hence 
.

Otherwise, by Lemma 1 we compute


 
 

Since 
 
, we have 
 and therefore, 
.

Finally, we note that for constant n the probability 
 is still positive, and hence . 

We proceed with a lemma for the crossover phase.

Lemma 15
Let 
 
. Assume that 
 
, 
 
 and 
 
, and there is at least one critical bit in 
. Then 
.

Proof
To have a successful crossover offspring it is sufficient to take one critical bit from 
 and all other different bits from x. Thus the probability 
 of generating one superior crossover offspring is


 
 
 
 
 
 
 
 
 

Since we need only one of the 
 
 offspring to be superior, by Lemma 1 we have


 
 
 


Now we are in position to prove Theorem 13.

Proof of Theorem 13
We denote the probability to increase fitness in one iteration by P and we estimate this probability as follows.

By Lemmas 4, 14, and 15 we have


 
 

Therefore the expected runtime (in terms of iterations) until the  GA reaches the local optimum of 
 is


Finally, we prove the main result of this section, Theorem 12.

Proof of Theorem 12
By Theorems 9 and 13 the upper bound on the total number of fitness evaluations of the  GA with random initialization is

With 
 
 
, we have


 
 
 
 
 
 
 
 
 
 


We note that 
 
 
 is the value which minimizes our upper bound apart from the 
 factor. We omit the proof of this fact, since it trivially follows from the minimization of a function 
 
 via analysis of its derivative.

Lower Bounds
In this section, we show that the parameters we chose in the previous section for the optimization starting in the local optimum are asymptotically optimal (apart from 
 factors), that is, that no choice of the parameters 
, 
, p, and c leads to an asymptotically better runtime (apart from 
 factors). This in particular shows that our analysis in the previous section is tight, that is, that the runtime proven in Corollary 9 is of the right asymptotic order of magnitude (apart from 
 factors).

Theorem 16
The probability P that the  GA finds the optimum of 
 with 
 
 in one iteration if the current individual is in the local optimum is at most 
 
.

Before we prove Theorem 16 we introduce and prove the following auxiliary lemma. A similar (but less precise) result can be distilled from the proofs of Theorem 4.1 and Corollary 4.2 in [23], but we give a short proof which also delivers a more precise estimate.

Lemma 17
For all , all  and all  we have 
 
 
. If , then we also have 
 
.

Proof
Consider the function 
. It is smooth in [0, 1], therefore its maxima are in the boundaries of the interval or in the roots of its derivative. Since  and  for all , the boundary values cannot be maxima. We compute the derivative as follows.


 

The roots of the derivative are in ,  and 
 
. Hence the maximum can be reached only in 
 
, and the value of f at this point is 
 
 
 
. If we also have , then furthermore


 
 
 
 
 
 
 
 
 

Now we are in position to prove Theorem 16.

Proof of Theorem 16
We use the precise expression of the probability P to go from the local to the global optimum in one iteration, which is

where 
 is the probability to choose  bits to flip, 
 is the probability of a successful mutation phase conditional on the chosen , and 
 is the probability of a successful crossover phase conditional on the chosen  and on the mutation phase being successful. In contrast to the upper bounds, where we have shown a lower bound on the sum of the terms for , now we aim at giving the upper bound on the whole sum.

Since , we have 
 
. The probability of a successful mutation phase depends on the chosen . If , then it is impossible to flip all k zero-bits, hence 
. For larger  the probability to create a good offspring in a single application of the mutation operator is 
  
 . If , then any good offspring occurs in the fitness valley and has worse fitness than any other offspring that is not good. Hence, in order to have a successful mutation phase we need all 
 offspring to be good. Therefore, the probability of a successful mutation phase is 
. For  and  we are guaranteed to choose a good offspring as the winner of the mutation phase if there is at least one. Therefore, the mutation phase is successful with probability 
.

If  and the mutation phase is successful, it implies that the optimum is already found and hence we assume 
. Otherwise, we can create a good offspring in the crossover phase only if . For this we need to take all k bits which are zero in x from 
, and then take all  one-bits which were flipped from x. The probability to do so in the creation of one offspring is 
. Since we create 
 offspring and at least one of them must have a fitness better than the fitness of x, the probability of a successful crossover phase is 
.

Putting these probabilities into (3) we obtain


 
 
 
 
 
 
 
 
 
 
 
 
 
 

(4)

Using Bernoulli’s inequality 
 valid for all  and , we estimate the two terms of type 
 by 
. Note that an equivalent estimate could have been obtained by applying a union bound in the probabilistic setting that gave rise to these two expressions.

We then note that, trivially, we have


 
 
 
 
 
 
 

which allows to uniformly estimate the terms for  and .

With these two estimates, we obtain


 
 
 
 
 
 
 
 

First, we consider the first term which corresponds to . By Lemma 17 we have


 
 
 
 

(5)

For the rest of the expression we argue as follows.


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

(6)

Since  and , by Lemma 17 this expression is at most 
 
. Therefore, we conclude that


 
 
 
 


Theorem 16 lets us show in the following corollary that the parameters stated in Corollary 9 (
 
 and 
 
) which minimize the upper bound give us an optimal runtime (apart from an 
 factor).

Corollary 18
The expected runtime of the  GA with any parameters on 
 is at least 
 
, if it starts in the local optimum of 
. This is of the same asymptotic order (apart from an 
 factor) as the upper bound shown in Corollary 9 for the parameters 
 
 and 
 

Proof
Since the probability P to find the global optimum in one iteration is at most 1, by Theorem 16 we have


 
 
 
 

(7)

Consider the arguments of the maximum as functions of 
 (and fix all other parameters). Then 
 is strictly increasing in 
, while


 
 
 
 
 
 
 

is strictly decreasing. Therefore, the maximum in (7) is minimized when both its arguments are equal. This condition is satisfied only when 
 
, which yields the following lower bound on the runtime.


 
 
 

Considering this as a function of 
 and studying its derivative one can see that this lower bound is minimized when 
 
. This implies that the minimal lower bound on the runtime is reached with 
 
 
 and is equal to


 
 
 
 
 

This is of the same asymptotical order (apart from an 
 factor) as the upper bound 
 
 for parameters 
 
 and 
 
 shown in Corollary 9. 

In this section, we have shown that we determined in Corollary 9 an asymptotically optimal parameter setting for leaving the local optimum of jump functions. We leave open the question if we also used the best parameters in Theorem 12, that is, when starting with a random solution. This is also an interesting question, but it is harder to answer due to the trade-off between optimizing the OneMax-type part of the optimization process and the part leaving the local optimum. In addition, we believe that the question we did answer, how to optimally leave the local optimum, is more interesting from the application point of view. We could imagine that when solving several similar instances of a difficult optimization problem, just by analyzing the optimization processes, one can obtain a rough estimate on the number of bits that need to be flipped to leave the hardest local optima. With this information, the parameters determined in Corollary 9 would be a reasonable starting point for optimizing the parameters of the algorithm. In constrast to this, we doubt that in a practical problem one is able to understand both the structure of the local optima and the easy parts of the fitness landscape sufficiently well that then a trade-off as done in Sect. 3.2 could reasonably be obtained.

Conclusion
In this first runtime analysis of the  GA on a multimodal problem, we observed that this algorithm also has a runtime advantage over classic algorithms on multimodal objective functions, and a much more pronounced one. Whereas the advantage in the previous results on unimodal problems was a gain of a logarithmic factor, we have shown here a runtime that is almost the square root of the runtime of classic algorithms.

For the  GA to show such a good performance, its parameters have to be chosen differently from what was suggested in previous works, in particular, the mutation rate and crossover bias have to be larger. We developed some general suggestions (at the end of Sect. 3.1) that might ease the future use of this algorithm.

Being the first runtime analysis on a multimodal problem, this work leaves a number of questions unanswered. To highlight one of them, we note that we have not proven a matching lower bound for our runtime result. Lower bounds for algorithms with several parameters can be technically demanding as the corresponding analysis [13, Section 5] of the  GA on OneMax shows. Hence such a result, despite desirable and possibly also indicating better upper bounds, is beyond the scope of this paper.