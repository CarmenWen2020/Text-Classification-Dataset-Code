A maximal common subsequence (MCS) between two strings X and Y is an inclusion-maximal subsequence of both X and Y. MCSs are a natural generalization of the classical concept of longest common subsequence (LCS), which can be seen as a longest MCS. We study the problem of efficiently listing all the distinct MCSs between two strings. As discussed in the paper, this problem is algorithmically challenging as the same MCS cannot be listed multiple times: for example, dynamic programming [Fraser et al., CPM 1998] incurs in an exponential waste of time, and a recent algorithm for finding an MCS [Sakai, CPM 2018] does not seem to immediately extend to listing. We follow an alternative and novel graph-based approach, proposing the first output-sensitive algorithm for this problem: it takes polynomial time in n per MCS found, where , with polynomial preprocessing time and space.

Introduction
The widely known longest common subsequence (LCS) is a special case of the general notion of (inclusion-)maximal common subsequence (MCS) between two strings X and Y. Defined formally below, the MCS is a subsequence S of both X and Y such that inserting any character at any position of S no longer yields a common subsequence. We believe that the enumeration of the distinct MCSs is an intriguing problem from the point of view of string algorithms, for which we offer a novel graph-theoretic approach in this paper.

Problem Definition
Let  be an alphabet of size . A string S over  is a concatenation of any number of its characters. A string S is a subsequence of a string X, denoted , if there exist indices 
 such that 
 for all .

Definition 1
Given two strings X, Y, a string S is a maximal common subsequence of X and Y, denoted , if

1.
 and ; that is, S is a common subsequence;

2.
there is no other string W satisfying the above condition 1 such that , namely, S is inclusion-maximal as a common subsequence.

Example 1
Consider  and , where . A greedy left-to-right common sequence is not necessarily an MCS: reading X from left to right and keeping the left-most matching character in Y when possible gives , which is not in MCS(X, Y) as .

The focus of this paper is on the enumeration of MCS(X, Y) between two strings X and Y, stated formally below.

Problem 1
(MCS enumeration) Given two strings X, Y such that  over an alphabet  of size , list all maximal common subsequences .

In enumeration algorithms, the aim is to list all objects of a given set. The time complexity of these type of algorithms depends on the cardinality of the set, which is often exponential in the size of the input. This motivates the need to define a different complexity class, based on the time required to output one solution.

Definition 2
An enumeration algorithm is polynomial delay if it generates the solutions one after the other, in such a way that the delay between the output of any two consecutive solutions is bounded by a polynomial in the input size.

Our aim will be to provide a polynomial delay MCS enumeration algorithm, more specifically we will prove the following result.

Theorem 1
There is a -delay enumeration algorithm for Problem 1, with 
 preprocessing time and 
 space.

Motivation and Relation to Previous Work
Maximal common subsequences were first introduced in the mid-90s by Fraser et al. [9]. Here, the concept of MCS was a stepping stone for one of the main problems addressed by the authors: the computation of the length of the shortest maximal common subsequence (SMCS) (i.e. the shortest string length in MCS(X, Y)), introduced in the context of LCS approximation. For this, a dynamic programming algorithm was given to find the length of a SMCS of two strings in cubic time.

While LCSs [4, 10, 18, 22] have thoroughly been studied in a plethora of string comparison application domains, like spelling error correction, molecular biology, and plagiarism detection, to name a few, little is known for MCSs. In general, LCSs only provide us with information about the longest possible alignment between two strings, while MCSs offer a different range of information, possibly revealing slightly smaller but alternative alignments. Thus, MCS could in principle provide helpful information in all the applications where LCS are used.

Fig. 1
figure 1
On the top, a simplified representation of two genomes for whom 6 potential anchors have been identified. On the bottom, a possible colinear alignment of the anchors. A different colinear alignment would be given for instance by 2, 3, 4, 6. (Picture inspired by Fig. 3 from Delcher et al. [8].)

Full size image

Furthermore, a direct application for the different alignments highlighted by MCSs can be found in the field of comparative genomics. Because of the sheer size of the input, comparative genomics requires more specific tools than general string comparisons, thus branching into a field of its own. Some of the tools employed for genome comparison solve problems akin to the MCS problem, relying on anchor-based methods [5, 8, 16]. Given two genomes, these methods first look for potential anchors, which are long meaningful chunks of the genomes that are considered to be matching (see Fig. 1). Final anchors are extracted from these by computing a colinear (i.e. occurring in the same order in both genomes) sequence of non-overlapping potential anchors; eventual small gaps between the anchors are then filled separately. If we interpret the two sequences of potential anchors as strings, finding MCS between these helps find the optimal colinear alignment to later extract the anchors, and run the final alignment methods. An efficient algorithm to find MCS could therefore be employed to speed up these time-consuming comparison tools.

A drawback of LCS is that there is a quadratic conditional lower bound for their computation, based on the Strong Exponential Time Hypothesis [1].Footnote 1 On the other hand, no quadratic bound exists for MCS: in fact, a recent paper by Sakai [20] presents an algorithm that deterministically finds one MCS between two strings of length O(n) in  time, which he later improved to  time in [21]. These algorithms can also be used to extend a given sequence to a maximal one in the same time. Furthermore, O(n)-time algorithms to check whether a given subsequence is maximal are described in the papers. To this end, in [20] the author gives a neat characterization of MCSs, which will be useful later, as stated in Lemma 1 and illustrated in Fig. 2.

Lemma 1
(MCS Characterization [20]) Given a common subsequence W of X and Y, we define 
 (resp. 
) as the remaining substring obtained from X (resp. Y) by deleting both the shortest prefix containing W[0, k), and the shortest suffix containing W[k, |W|). Substrings 
 are called the k-th interspaces. With this notion, W is maximal if and only if for all , 
 and 
 are disjoint (that is, they share no common characters).

In what follows, we will showcase some pitfalls that arise when trying to trivially extend the aforementioned results to MCS enumeration. It is worth noting that even though there are a few more different approaches in the literature to find LCS or common subsequences with some kind of constraints (e.g. common subsequence trees [11], common subsequence automata [7]), the approaches described in this section are the only ones which directly deal with MCS. Thus, for space and clarity reasons, we will only focus on these latter approaches, and the pitfalls arising from their extensions. Still, to the best of our knowledge, other approaches in the literature do not immediately extend to efficient MCS enumeration either.

Strategic Pitfalls
Fig. 2
figure 2
Visual representation of Sakai’s characterization

Full size image

The aforementioned results by Fraser et al. and Sakai seem to be of little help in our case, as neither of the two can be directly employed to obtain a polynomial-delay enumeration algorithm to solve Problem 1, which poses a quite natural question.

Consider the dynamic programming approach in [9]: even if the dynamic programming table can be modified to list the lengths of all MCS in polynomial time, this result cannot be easily generalized to Problem 1. Indeed we show below that any incremental approach, including dynamic programming, leads to an exponential-delay enumeration algorithm.

Example 2
Consider ,  and . Since , the only string in MCS(X, Y) is the whole X. But if we were to proceed incrementally over Y, at halfway we would compute MCS(X, Z), which can be shown to have size . This means that it would require time exponential in the size of the input to provide just a single solution as output.

As for the approach in [20], the algorithm cannot be easily generalized to solve Problem 1, since the specific choices it makes are crucial to ensure maximality of the output, and the direct iterated application of Lemma 1 does not lead to an efficient algorithm for Problem 1, as shown next.

Example 3
For a given common subsequence W to start with, first find all values of  such that 
 and 
 are not disjoint, that is, they have some characters in common. Then, for these values, compute all distinct characters c which occur in both 
 and 
, and for each of these recur on the extended sequences 
. For instance, given the strings  and  with starting sequences  and , this algorithm would recur on almost every subsequence of X, just to end up outputting the single  an exponential (in the size of X) number of times.

Lastly, the refined approach given in [21] also seems to not have an immediate extension to the enumeration of MCS. This latter approach builds a string W by going from left to right in the input strings, and adding common characters to W in a greedy way, like in Example 1. Once it gets to the end of the strings, it can identify whether a character is surely the last one for some MCS. Fixing this character as the last for W, it recursively looks for an MCS in the prefixes of the strings up to the last occurrences of the selected character, adding more letters to W whenever possible, to finally yield an MCS.

Example 4
Consider strings  and , with MCS given by . First note that, to adapt the algorithm to enumeration, we need to consider multiple starting characters, as otherwise we could miss some MCS in which the chosen starting letter does not appear. Reasonably, we choose all letters that do not have an insertion before them: in our example we would start with A and G. Further note that choosing different starting characters is not enough, as otherwise the algorithm would deterministically only find at most  MCS. Thus, to perform enumeration based on this algorithm we need to choose all possible insertions, at every step, when going left to right. This is equivalent to a greedy left-to-right approach, which may yield the same sequence multiple times: in our example, string GATAGA is output 6 times, and string GATAC 3 times.

Getting polynomial-delay enumeration is therefore an intriguing question. The fact that one maximal solution can be found in polynomial time does not directly imply an enumeration algorithm: there are cases where this is possible, but the existence of an output-sensitive enumeration algorithm is an open problem [14], or would imply  [17]. As we will see, solving Problem 1 can lead to further pitfalls that we circumvent in this paper.

Our Approach. The approach that will finally lead us to our result is a prefix-incremental one: given P a prefix of an MCS, we will be able to identify all characters c such that  is still a prefix of some MCS. As we will see in the next section, this task is highly non-trivial, and leads to several more pitfalls of its own.

Overview
In the rest of the paper, we start by showing how to interpret the MCS problem as a graph problem, in Sect. 2. This change of perspective will lead us to our central combinatorial result, Theorem 2, which will be the basis of our prefix-incremental algorithm; its involved proof will be detailed in Sect. 3. The theorem will lead us naively into our first polynomial-delay enumeration algorithm, introduced in Sect. 4. Lastly, Sect. 5 will present a refinement of the initial algorithm, allowing us to finally reach the bounds described in Theorem 1.

MCS as a Graph Problem
As a starting point, we reduce Problem 1 to a graph problem in order to give a theoretic characterization and to get some insight on how to combine MCS. Afterward, this characterization will be reformulated in an operative way, leading to an algorithm for MCS enumeration.

String Bipartite Graph
Definition 3
Given two strings X, Y, their string bipartite graph G(X, Y) has vertex set 
, where 
 represent respectively the i-th position of X and the j-th position of Y, and edge set 
. Each edge, called pairwise occurrence, connects positions with the same character in different strings. Wherever it is clear from the context, we will denote edge 
 simply as (i, j).

Definition 4
A mapping of G(X, Y) is a subset  of its edges such that for any two edges  we have  iff . That is, a mapping is a non-crossing matching of the string graph. A mapping that is inclusion-maximal is called a maximal mapping.

Each mapping of the string graph spells a common subsequence. Vice versa, each common subsequence has at least one corresponding mapping. Thus one might incorrectly think that MCS correspond to inclusion-maximal mappings; as a counterexample consider  and , with . G(X, Y) has an inclusion-maximal mapping corresponding to .

For a string S, let 
 be the smallest  with  (if any), and 
 otherwise; we use the shorthand 
.

Definition 5
A mapping of G(X, Y) is called rightmost if for each edge (i, j) of the mapping, corresponding to character , the next edge 
 of the mapping is such that 
 and 
. That is, there are no occurrences of c in 
 and 
, the portions between edges (i, j) and 
. We can symmetrically define a leftmost mapping.

Remark 1
Given any common subsequence 
 of X, Y, we can always build its unique rightmost mapping as follows. Start at 
, and let 
 and 
; consider then for every , 
 and 
. Edges 
 form a rightmost mapping spelling S. Symmetrically, we can always build the unique leftmost mapping of S.

In order to design an efficient and correct enumeration algorithm that uses also Definition 5, we first need to study how MCS(X, Y) and 
 relate to 
 for any two pairs of strings X, Y and 
. This will help us develop a prefix-expanding strategy for 
, as we can consider  as a prefix, and the first characters of strings in 
 as its extensions.

Remark 2
A simple concatenation of the pairwise MCS fails: consider for example , 
,  and 
, with 
. We have  and 
. Combining the latter two sets we find the sequence , which is in fact maximal, but also , which is not maximal as .

The correct condition for combining MCS is a bit more sophisticated, as stated in Theorem 2. Here, for a position i of a string S, we denote by 
 the prefix of S up to position , and by 
 the suffix of S from position .

Fig. 3
figure 3
For their concatenation to be an MCS, P has to be maximal in the red dashed part and C in the orange dotted one (color figure online)

Full size image

Theorem 2
(MCS Combination) Let P and C be common subsequences of X, Y. Let (l, m) be the last edge of the leftmost mapping 
 of P, and (i, j) be the first edge of the rightmost mapping 
 of C (see Fig. 3). Then


 

Proof
To ensure the equivalence, it is sufficient to show that Sakai’s interspaces for string  over X, Y are the same as the ones for either P over 
, or for C over 
. Let 
, 
, and let us study the k-th interspace for the subsequence .

Case : the shortest suffixes of X, Y containing 
 as a subsequence are unchanged from the shortest suffixes of 
 containing 
, since C is already in rightmost form starting exactly at (i, j). Let 
 be the shortest prefixes of X, Y containing 
 as a subsequence. Then, by definition of leftmost mapping, (u, v) must be the k-th edge of the leftmost mapping of P, which occurs before edge (i, j) by hypothesis. The shortest prefixes of X, Y containing 
 are thus also unchanged from the ones for 
. Therefore, the interspaces for the whole strings are unchanged from the interspaces for P over 
.

Case : this case is symmetrical to the previous one: the interspaces for the whole strings are unchanged from the interspaces for C over 
.

Case : The last interspaces for P and the first for C coincide, and they are  and . Since P is in leftmost form ending at (l, m) and C is in rightmost form beginning at (i, j), these two strings also coincide with the k-th interspaces for . 

Theorem 2 gives a precise characterization on how to combine maximal subsequences, but it cannot be blindly employed to design an enumeration algorithm for a number of reasons.

Let a string P be called a valid prefix if there exists  such that P is a prefix of W. Suppose that the leftmost mapping for P ends with the edge (l, m), and that we want to expand P by appending characters to it so that it remains valid. These characters correspond to the edges (i, j) related to (l, m) as stated by Theorem 2, for some maximal sequence C. The rest of the paper describes how to perform the following task without explicitly knowing C: given a valid prefix P with leftmost mapping ending at (l, m), find the edges (i, j) whose corresponding characters yield a valid prefix when used to extend P.

Fig. 4
figure 4
The possibility for insertion is not sufficient to discard a character as a valid extension: edge (i, j) corresponds to valid character C even though both its endpoints can be re-mapped (dashed) to allow for insertions (see Remark 3)

Full size image

Remark 3
Note that, when looking for edge (i, j), the occurrence of an insertion when moving its endpoints is not enough to discard the candidate. Consider as an example  and , with valid prefix  ending at . Consider edge ; clearly 
. Also, (i, j) can be moved in both strings to allow for insertions of characters  and . Nonetheless, its corresponding character  still generates the valid prefix . This is illustrated in Fig. 4. Along the same lines, Sakai’s algorithm cannot help here. It generates an MCS that contains P as a subsequence, but not necessarily as a prefix. Therefore, it cannot be easily employed to identify the edges (i, j).

We need a more in-depth study of the properties of graph G(X, Y) to characterize the relationship between (l, m) and (i, j). First, we give the notion of unshiftable edges, and show that edge (i, j) needs to be unshiftable. Second, as being unshiftable is only a necessary condition, we discuss how to single out the (i, j)’s suitable for our given (l, m).

Unshiftable Edges
Definition 6
An edge (i, j) of the bipartite graph G(X, Y) is called unshiftable if it belongs to at least one maximal rightmost mapping of G(X, Y). The set of unshiftable edges is denoted . An edge is called shiftable if it is not unshiftable.

Example 5
Consider  and  with . The unshiftable edges for these two strings are the following ones:

figure a
A symmetric definition of left-unshiftable edges could be given by considering maximal leftmost mappings, and these are related to k-dominant edges, a concept introduced in 1985 by Apostolico [3]: k-dominant edges turn out to be a subset of left-unshiftable edges.Footnote 2

Unshiftable edges can also be characterized more directly, as stated in Proposition 1.

Proposition 1
An edge (i, j) is unshiftable if and only if either (1) it corresponds to the rightmost pairwise occurrence of  in the strings, or (2) there is at least one unshiftable edge in the subgraph 
.

Proof
() Let , then by definition there exists a maximal rightmost mapping 
 such that 
 for some . If , by definition of rightmost mapping, 
 corresponds to the last pairwise occurrence of some character, thus it satisfies (1). Consider now , and let 
 correspond to some character c. By definition of unshiftability, 
. By definition of rightmost maximal mapping, there can be no occurrences of c between 
 and 
; therefore the unshiftable edge 
 belongs to the subgraph 
.

() Let (i, j) satisfy one of the two conditions. If it satisfies the base case, then (i, j) is in rightmost form, and we can extend it to the left to a rightmost maximal mapping. On the other hand, let (i, j) satisfy the second condition. Then, there is an edge  that belongs to the subgraph 
. Consider the rightmost maximal mapping  that contains (h, k); if it also contains (i, j) we are done. Otherwise, let 
 be the restriction that only contains (h, k) and subsequent edges. Consider the rightmost mapping 
; we can extend it to the left until it is rightmost maximal. In any case, we have obtained a rightmost maximal mapping containing (i, j), which is then unshiftable. 

An immediate consequence of Proposition 1 is the following fact, which will give us an operative way of finding the set of unshiftable edges:

Fact 1
Let  and . If 
 are the rightmost occurrences of c respectively in 
 and 
, then edge 
 is also unshiftable.

Remark 4
Although every MCS has a corresponding rightmost maximal mapping, and the edges in the latter are unshiftable by Definition 6, it is incorrect to conclude that the opposite holds too. Not all rightmost maximal mappings give MCS: consider for example  and . In G(X, Y) we have a maximal rightmost mapping for , but .

Remark 5
Unshiftable edges can be dense in G(X, Y). For example, consider 
 and 
: every  of Y has out-degree of unshiftable edges equal to the number of s in X, that is O(n). The total number of unshiftable edges is therefore 
.

Candidate Extensions
We finalize the characterization of the relationship between edges (l, m) and (i, j) of Theorem 2, where (l, m) is the last edge of the leftmost mapping in G(X, Y) of a valid prefix P. We would like to single out a priori the corresponding possible (i, j)’s, without explicitly knowing their Cs. This in turn will lead to the incremental discovery of such C’s one character c at a time.

Specifically, we look for edges (i, j) corresponding to the characters  such that  is still a valid prefix.

Definition 7
Given an edge (l, m), its cross 
 (see Fig. 5) is given by (at most) two unshiftable edges 
 such that


 

Fig. 5
figure 5
Graphical representation of the cross  for edge (l, m), drawn in purple: 
 are the first unshiftable edges soon after (l, m) (color figure online)

Full size image

Definition 8
Given an edge (l, m), let 
 be its cross. We define the set of its mikado edges as the unshiftable edges of 
,


 

and the subset of candidate extensions for (l, m) as


 

It follows immediately from the definition that no two edges in 
 have a common endpoint, and thus 
.

Definitions 7 and 8 find their application in identifying a valid prefix extension, as shown in Fig. 6 and discussed next.

Fig. 6
figure 6
Extraction of 
 from the set 
, pictured on the left. The edges belonging to 
 are dashed

Full size image

Valid Prefix Extensions
Let P be a valid prefix with leftmost mapping 
 ending with the edge (l, m). We use shorthands for 
 and 
. The candidates in 
 are the unshiftable edges soon after 
 such that no other unshiftable edge lies completely delimited between 
 and any of them, as illustrated in Fig. 6.

We thus are ready to give our algorithmic characterization of valid extensions of prefixes to relate edges (l, m) and (i, j) from Theorem 2.

Theorem 3
Let P be a valid prefix of some , with leftmost mapping 
 ending with the edge (l, m). Then  is a valid prefix if and only if the following two conditions hold.

(1)
There exists 
 corresponding to character c, and

(2)
.

The proof of Theorem 3 is quite involved, and thus postponed to Sect. 3. This result is crucial for our polynomial-delay binary partition algorithm, as the latter recursively enumerates MCS(X, Y) by building increasingly long valid prefixes and avoiding unfruitful recursive calls.

Proof of Theorem 3
In this section, we finalize the proof of Theorem 3, at the heart of our results. We introduce the concept of certificate edges, and use it to show sufficiency and necessity of the two conditions (1) and (2) in Theorem 3.

Certificate Edges
Given an edge, we call its certificate edges the unshiftable edges that occur right after the edge in question and before the next pairwise occurrences of the corresponding character, with no other unshiftable in between.

Recalling that 
, certificate edges are defined as follows, and illustrated in Fig. 7.

Definition 9
An edge 
 is a certificate for another edge (i, j) if 
 and no 
 has 
, 
.

In this case, we say that 
 certifies (i, j). We denote with 
 the set of certificates of edge (i, j). An edge  is called a root iff 
.

Certificates are given this name as they certify the unshiftability of the edge, as detailed in the following:

Remark 6
Note that if 
, then . In fact, let 
; then, by definition of certificate, (i, j) are the rightmost occurrences of its corresponding character c in 
 and 
. Thus, by Fact 1, .

Fig. 7
figure 7
The only certificate for the green bold edge corresponding to character c is drawn in solid blue (all dashed edges are unshiftable)

Full size image

Definition 10
A certificate mapping is a mapping in which the rightmost edge is a root, and each edge except the leftmost is a certificate for the one to its left.

Using certificate edges, we can prove the following sufficient condition that will help us identify MCSs.

Lemma 2
Let 
 be a maximal certificate mapping in 
 of a common subsequence 
 between 
 and 
, where 
. Then:

1.
M is a rightmost maximal mapping of unshiftable edges in 
, and

2.
if 
, then 
.

Proof
1.
By contradiction, assume that M is not rightmost. This means that there exists at least one edge 
, corresponding to some , such that c occurs between 
 and 
 in either one of 
 or 
. Therefore, 
 is not a certificate of 
: contradiction.

2.
Assume that 
. By contradiction, let M not be maximal: let k be the minimum index such that we can insert a character c between 
 and 
. If  (insertion at beginning), then there is a pairwise occurrence of c before 
; let (i, j) be the rightmost such pairwise occurrence. Then, 
 is a certificate for (i, j), and therefore by Remark 6, 
: contradiction. We can therefore suppose . If we can insert c between 
 and 
, it means that the k-th interspace is non-empty and contains c. This is equivalent to saying that we can find an edge corresponding to character c in the substrings between the leftmost mapping of 
 and the rightmost mapping of 
. Since the latter is already in rightmost form, we will perform a re-mapping of just 
. First, note that when remapping 
 in any way, we can only shift one endpoint per edge. This is because shifting both endpoints would mean that we have a character match, and therefore an insertion , which is impossible before the k-th edge by hypothesis. Let 
 be the minimal leftward re-map of the edges 
 that allows for the insertion of c between 
 and 
. That is, the endpoints of 
 are the rightmost that allow for insertion. Note that in such a minimal re-map, the edges’ endpoints that get re-mapped always belong to the same string, as otherwise we would either have crossing edges, and thus not a valid mapping, or have a superfluous re-map, contradicting minimality. Such a re-map is illustrated in Fig. 8.

Fig. 8
figure 8
The red edge corresponding to character c is the insertion that takes place between edges 
 and 
. Solid orange edges 
 are the minimal leftward re-map of edges 
, shown dashed in blue (color figure online)

Full size image

Let (i, j) corresponding to some character c be the rightmost insertion that can occur between edge 
 and edge 
. Then, (i, j) is an unshiftable edge, since it has edge 
 as a certificate. It is possible that we can insert a string of characters ending with c between 
 and 
 , as illustrated in Fig. 9. In this case, let us choose for each of these characters the edge corresponding to its rightmost pairwise occurrence before the edge for the next character. All of these edges are unshiftable since each one is a certificate for the previous, starting with the one for c. The following proof can proceed considering without loss of generality the leftmost of these insertions as edge (i, j).

Fig. 9
figure 9
Without loss of generality, we can consider the leftmost insertion given by edge (i, j)

Full size image

By choosing the edge in this fashion, we guarantee that there aren’t any more unshiftable edges ( or even regular edges) between 
 and (i, j). Furthermore, since 
 was the minimal leftward re-map, there are no occurrences of its corresponding character between itself and (i, j), and so we find that 
 is certified by (i, j). Edge 
 is then also unshiftable. This now propagates backward for every h: let 
 correspond to character a, and assume that 
 is unshiftable. Then, there can be no occurrence of a between 
 and 
: (1) there are no occurrences of a between 
 and 
 because the re-map was minimal; (2) there can be no occurrence of a between 
 and 
 because they belonged to a rightmost mapping (see Fig. 10).

Fig. 10
figure 10
Propagation of unshiftability for 
. Edges 
 are shown in solid orange, while their rightmost counterparts 
 are dashed blue. Edge 
 corresponds to character a, which cannot appear in either (1) or (2): the former because of the minimality of the re-mapping, the latter because 
 is rightmost in the original mapping (color figure online)

Full size image

Thus, 
 is unshiftable. Once we get to 
, we obtain an unshiftable edge different from 
 in the subgraph 
: contradiction. 

We now define the find
 procedure, used to generate certificate mappings. This procedure implicitly finds the C from Theorem 2. Given an unshiftable edge, find
 chooses one of its certificates and recurs until it gets to a root edge.


 

Proposition 2
Let (l, m) be any edge of the graph, and 
 in the set of extensions of (l, m). Then find
 returns a certificate mapping having as first edge (i, j), such that the corresponding subsequence is 
.

Proof
The procedure find
 generates a certificate mapping starting with edge (i, j) by definition. Since 
, there cannot be any unshiftable edges in the subgraph , except for (i, j) itself. By setting 
 and 
 in Lemma 2, 
 and is rightmost. 

Necessary and Sufficient Conditions
Necessity. First of all, we will prove that conditions (1) and (2) of Theorem 3 are necessary. Let  be a valid prefix of some .

First, we show that condition (1) holds, namely, there exists 
 corresponding to character c. We do so by supposing that none of the edges in 
 correspond to c, and showing that this leads to a contradiction. By Sakai’s characterization of maximality, for all indices  we have that 
 and 
 are disjoint, that is, they share no common characters. Let 
, and thus 
 and 
 starts with c because  is a valid prefix of W. By definition, 
 and 
 are disjoint, where 
 and 
 are given by the parts of the strings between the leftmost mapping 
 of P and the rightmost mapping of 
. The first edge of the latter mapping is  corresponding to character c as 
 starts with c. By contradiction, suppose 
. We now have two cases:

Case 
: this implies that 
 or 
, where 
 and 
 are those given in Definition 8. Therefore 
 and 
 cannot be disjoint, as there would be at least the character corresponding respectively to f or e. This is a contradiction.

Case 
: this implies that 
 such that  and . Then 
 and 
 are not disjoint, as we would have the edge (h, k) in 
, giving a contradiction.

Second, we prove the necessity of condition (2), namely, 
. To this end, we need a brief remark on the restriction of maximals: let  and 
 any mapping spelling W in the two strings. Given any , we have 
.

Let  be a valid prefix of some , and 
. In the first part of the proof we have shown that the first edge of the rightmost mapping of 
 is some 
 corresponding to c. Therefore, let us consider the mapping for W consisting of P in leftmost form, and 
 in rightmost form. Applying the above remark for 
 we get 
.

Sufficiency. Suppose that conditions (1) and (2) of Theorem 3 hold. By Proposition 2, find
. Since 
 by hypothesis, we have  by Theorem 2. The latter string starts with , which is therefore a good prefix.

figure b
Baseline Algorithm for Polynomial-Delay MCS Enumeration
The characterization given in Theorem 3 immediately gives the “prefix-expanding” enumeration Algorithm 1, which progressively augments prefixes with characters that keep them valid, until whole MCSs are recursively generated. In this section, we will describe this baseline algorithm, and prove that it has polynomial-delay. It is worth noting that Theorem 3 guarantees that each recursive call yields at least one MCS; moreover, all the MCSs are listed once as different recursive calls originating from the same prefix P are performed by appending distinct characters.

Algorithm overview. Algorithm 1 employs a binary partition scheme.Footnote 3 First, it builds the necessary data structures (Sect. 4.1), and it finds the set of unshiftable edges in a polynomial preprocessing phase, using FindUnshiftables as described in detail in Sect. 4.2. Then, it begins a recursive computation BinaryPartition where, at each step, it considers the enumeration of the MCSs that start with some valid prefix P. The partition is made over characters  such that  is valid, which is ensured by checking the two conditions of Theorem 3.

For convenience, we add a dummy character  at the beginning of both strings; i.e. at positions . The recursive computation then starts with , and leftmost mapping 
. At each step, the procedure finds the valid extensions 
 for the given prefix P using the unshiftable edges from . If 
 is empty, then P is an MCS, and is returned. Otherwise, for each character  the procedure loops over the edges of 
 corresponding to c (i.e. condition (1) of Theorem 3), to check whether any of these edges satisfy condition (2) of Theorem 3. If the check has positive answer for one of the edges, it means that  is a valid prefix: given the last edge (l, m) of 
, the algorithm finds the leftmost mapping 
 for character c in 
, as to update 
. Then, it partitions the MCSs to enumerate into the ones that have  as a prefix, and recursively proceeds on  and 
. After the recursive call, the next character needs to be considered, and thus the algorithms breaks from the loop of line 10. The correctness of Algorithm 1 immediately follows from Theorem 3.

Polynomial-delay complexity. Recall that unshiftable edges are polynomial (quadratic) in size (Remark 5). We will show that their extraction in the preprocessing phase, as well as the computation of the 
 set at line 6 are polynomial-time. Then, the algorithm loops over each character of the alphabet, and checks whether the edges in Ext corresponding to it satisfy condition (2) of Theorem 3 (line 11). For a given edge, this check is also polynomial-time, as we could e.g. employ Sakai’s algorithm [21]. Before recurring, the algorithm then updates the leftmost mapping by adding the new edge, which is once again a polynomial-time operation. Since every operation is polynomial-time, and every loop is over polynomial-size sets, each recursive call requires polynomial time. The working space is also polynomial. As for the final delay complexity, note that the height of the recursive binary partition tree is at most the length of the longest MCS, which is bounded by n, and that each leaf corresponds to a distinct solution. The delay of BinaryPartition is the cost of a root-to-leaf path in the recursion tree; since every step is polynomial-time, and the height of the tree is bounded by n, we obtain the following for Algorithm 1.

Theorem 4
There is a polynomial-delay algorithm for Problem 1, which has polynomial-time preprocessing and uses polynomial space.

In the rest of the section, we will describe the preprocessing phase and present the data structures in detail, in order to analyse the complexity of the baseline Algorithm 1. Specifically, we will prove.

Theorem 5
There is a 
-delay algorithm for Problem 1, with 
 preprocessing time and 
 space.

Data Structures
We start by describing the data structures that will be employed by Algorithm 1, allowing for fast queries over the strings X, Y and over the unshiftable edges set .

String data structures. The main operation that we perform multiple times during the execution of the algorithm is the computation of previous and next occurrences of a given character in the two strings X and Y. To be able to perform these operations in constant time, for each string we employ  bitarrays of the same length as the string. These arrays 
 (analogously for Y) are such that


 

On these bitarrays we can implement rank and select directories in a succinct way: we need only  bits per array to do so, where n is the length of the array. With these directories, rank and select operations can be performed in O(1) time [19]. That is, we can retrieve next and previous occurrences of any characters with respect to any positions of the strings in O(1). The total time and space required to build these structures is .

Unshiftable data structures. For subsequent operations, we need to also store the unshiftable edges in different data structures, specifically in two arrays of arrays uX and uY. Each of these arrays gives priority to one of the two strings. Entry uX[i] stores unshiftable edges (wedges) 
 as a sorted array of Y values 
 as shown in Fig. 11. uY is built symmetrically with respect to Y.

Fig. 11
figure 11
For each node of X, wedges are sorted arrays of positions of Y

Full size image

To build these arrays we will consider the following strict orders


 

Given these arrays, we can check whether an edge (i, j) is unshiftable in  time by performing a binary search for j (or i) in the sorted array uX[i] (or uY[j]), since . With the same procedure, we can also find, given i and 
, the first unshiftable (i, j) such that 
. These arrays are constructed and filled during the FindUnshiftables procedure, which we will describe next.

Finding Unshiftable Edges
During the preprocessing phase of the algorithm, we initialize the data structures and we compute the set of unshiftable edges with FindUnshiftables. We compute unshiftable edges by going backwards in the strings X and Y, exploiting Fact 1, which states that if 
 corresponds to the rightmost occurrences of a character c right before an unshiftable edge, then 
 is also unshiftable. We can thus start from the last pairwise occurrences of every character, then for every unshiftable edge (i, j) already found and for every character c, we can compute the rightmost occurrences of c in 
 and 
, and mark the corresponding edge as unshiftable. Once we get to the beginning of the strings, we have correctly identified all unshiftable edges.

Analogously as we do for BinaryPartition, let us add a special character  at the end of both strings, as to obtain an unshiftable edge at the last positions (|X|, |Y|). Starting from this edge, we have a natural recursive visiting procedure that finds unshiftable edges based on Fact 1. For each character , candidate unshiftable edges are found by taking the rightmost occurrences of c before the current edge in both strings. Then, we recur in these new edges, unless already visited. This originates our FindUnshiftables procedure, whose pseudocode is shown in Algorithm 1.

All unshiftable edges are found in this fashion. In fact, the last pairwise occurrences of every character are visited from edge (|X|, |Y|). If an unshiftable edge (i, j) is not the last pairwise occurrence, then by Proposition 1 there is at least one unshiftable edge in 
. Edge (i, j) will then surely be visited from the leftmost of these edges, and therefore it will be correctly marked as unshiftable.

Auxiliary data structures and complexity. During the FindUnshiftables procedure, we will need some intermediate support data structures to quickly check for membership and perform insertions. Since  is static after its creation, these data structures are only needed during the preprocessing phase: at the end of the procedure, they will be used to build the final arrays uX and uY, and then they will be deleted. First of all, in the FindUnshiftables procedure we need to ensure that no edge is visited twice. To this end, we employ two self-balancing binary search trees 
 (for example, AVL or red-black trees), respectively storing the unshiftable edges with respect to orders 
. Such trees allow for membership testing and insertion in  time, and require  space [15, Section 6.2.3]. The procedure considers each unshiftable edge exactly once. For each edge it goes through every character and finds its previous pairwise occurrences; then it checks for membership in the trees, and performs insertion if the edge is not found (see Line 26). Note that edge insertions are always performed for both trees. With the described data structures, looking for pairwise occurrences of the character requires constant time (using the string data structure), and membership tests and insertion for edges are performed in logarithmic time, leading to  total time and  space. Once the whole strings have been processed, each tree contains a sorted copy of the unshiftable edges, with respect to the corresponding order. Arrays uX, uY are created, and they can be filled with one more pass of the already sorted trees.

Recalling Remark 5, we have 
, and thus the FindUnshiftable procedure can be performed in 
 time, and 
 space in the worst case.

Complexity Analysis
We now show that Algorithm 1 satisfies the complexity bounds of Theorem 5.

As detailed in Sects. 4.1–4.2, the preprocessing phase, which encompasses building the data structures and finding the set of unshiftable edges, can be performed in 
 time and quadratic space.

Let us now study the complexity of a recursive call of the BinaryPartition procedure. The first operation at each step consists in computing the set 
: by scanning the unshiftable edges we can trivially find the cross in 
 time, and by another scan we find the mikado and 
 set. When it is nonempty, we loop over every character, finding its corresponding edges of 
; for every such edge we perform the maximality check for P, which takes O(n) time by employing Sakai’s maximality test [20]. If the test is positive, we only need to perform a leftward re-map of the new edge, which can be done in constant time using the string data structure, and we move on to the next character. Thus, recalling that 
, the total time for one recursive call amounts to 
 time.

Overall, the delay of BinaryPartition is given by the cost of a recursive call, times the height of the recursion tree. This leads to a polynomial-delay algorithm with delay 
 and a polynomial-time preprocessing cost of 
. The space required is 
, as we need to store the set 
 for all recursive calls in a root-to-leaf path, plus the set of unshiftable edges . Theorem 5 is thus proved.

While this is sufficient to prove the main result of the paper, i.e., that there exists a polynomial-delay and polynomial-space algorithm for the MCS enumeration problem, in the next section we focus on how to further improve its performance.

Improving the Delay in the Baseline Algorithm
In this section, we will describe a refinement of Algorithm 1, allowing us to achieve the bounds given in Theorem 1:

An ideal method would yield each distinct MCS in time proportional to its length: as the latter can be , this would take time linear in n. In our refined algorithm we only spend a further logarithmic time factor per solution when the alphabet has constant size, so it is quite close to the ideal method in that instance. As for space and preprocessing time, the quadratic factor is unavoidable when employing the possibly quadratic unshiftable edges in .

There are two main refinements to the original algorithm: first, we will show how to quickly extract the Ext set at line 6 in time , instead of the original 
; then, we will provide a way to perform the maximality check of line 11 in constant time, without needing to employ the linear algorithm by Sakai.

Computing the Candidates for Extensions
We start by presenting a faster way to find characters corresponding to edges in 
. This refinement will allow us to extract the 
 set in logarithmic time, speeding up line 6 of Algorithm 1. In what follows, let 
 and 
 be, respectively, the indices corresponding to the first and last occurrences of character c in the string S.

Remark 7
Let (l, m) be the last edge of the leftmost mapping for P. Then, for each edge 
 corresponding to some character c, either 
 or 
. That is, at least one of the endpoints of (h, k) is the first occurrence of c after the leftmost mapping. This follows directly from the definition of the 
 set.

Given a leftmost mapping for P ending with the edge (l, m), for every  consider the two edges


 
 


 
 

These two edges, represented in Fig. 12, are the first unshiftable edges completely after (l, m) that stem from the first occurrences of c respectively in X and Y after edge (l, m). Note that we can have 
.

Fig. 12
figure 12
Graphical representation of the edges 
, shown respectively in bold purple and blue. The red dashed edges are other unshiftable edges stemming from ix, jy that are discarded since their other endpoint is out of bounds (color figure online)

Full size image

At this point, let 
 be the set of all such edges, and consider its subset given by


 

As in the extraction of Ext from Mk, we have removed from  the edges that have another unshiftable completely before them. Note that , by definition.

Proposition 3
Proof
We have seen in Remark 7 that 
. By definition of 
, in the reduction from  to  none of its edges will be removed. Therefore, 
. Specifically, edges 
 are in . Let us now consider the other inclusion. By contradiction, let 
. That is, either 
, or there is an unshiftable edge completely between the leftmost mapping for P and (i, j). The latter condition is impossible by definition of , therefore 
. This means that either 
 or 
. Both of these immediately lead to a contradiction: one of the edges  or  is completely before edge . 

From this characterization, we have a simple procedure FindExtensions (Algorithm 2) that finds all candidates for extension. We restrict ourselves to 
, and for every character we find its first occurrences in both strings, which correspond to ix, jy. For every pair of occurrences, we compute the corresponding jx, iy by looking at the leftmost unshiftable edges in 
 which stem respectively from ix, jy. We then refine the set of edges we found by removing the ones that are preceded by another element of the set. The edges we obtain at the end of these steps are the Ext set.

figure c
Complexity analysis. In Algorithm 2, we first compute 
 for every character c, inserting them into a list . To compute ix and jy we need to find the first occurrences of c in 
 and 
, respectively, which can be done in constant time using the string data structure. Computing jx (resp. iy) is a bit more involved, as we need to find the leftmost unshiftable edge stemming from ix (resp. jy) and falling to the right of m (resp. l). To this end, we employ the unshiftable edges’ arrays uX, uY: we look for (ix, j) with j greater than m (and the symmetric for jy), which can be done in . The worst-case total time for these steps is therefore . The only thing left to do to have our candidate edges is to extract  from . We first sort the list  according to 
, which takes  time, e.g., using merge sort. At this point, we only need to go through the sorted elements once: for each 
 we discard (h, k) if . In this way, we remove the unwanted edges in  time.

Since , we have a time complexity of  for the extraction of . Thus, when using procedure FindExtensions at line 6 instead of the naive computation, the time required for finding candidates for extension is improved from 
 to


 

Maximality Check with Swings
The other major refinement of the algorithm concerns the maximality check 
 at line 11 of Algorithm 1. Instead of using Sakai’s method every time, we want to carry on information that allows us to decide immediately whether an extension is valid. To this end we now introduce the notion of swing.

Definition 11
Let L be a leftmost mapping for some string P, ending with edge (l, m). The swing of mapping L is a pair of integers 
 called respectively top and bottom swings, given by:


 

Equivalently, 
, 
. Simply put, swings indicate how much we are allowed to move our edge while guaranteeing no insertions in the previous parts of the strings. Figure 13 gives a visual representation of the concept of swings.

Fig. 13
figure 13
The swings for the blue mapping are drawn in green; the thick red edge is the insertion resulting from the top swing; more specifically, it is the insertion possible if the character corresponding to edge (l, m) is re-mapped into the green top swing and the previous blue edge is re-mapped into the dashed edge (color figure online)

Full size image

With this notion, the maximality check becomes immediate:

Lemma 3
Let P be a valid prefix, 
 its leftmost mapping, ending with the edge (l, m), and let 
 be its swings. Furthermore, let 
 be a candidate edge. Then:


 

(1)

Proof
If 
, then in particular 
 as , and thus the swing 
 must occur at a position greater than j by Definition 11; symmetrically 
 as , and the swing 
 must occur at positions greater than or equal to i. On the other hand, assume that (i, j) satisfies the right hand side, and let by contradiction 
. By definition of swings, 
 and 
, which means that we cannot re-map and perform insertions in these two pairs of strings. Thus, the only way for P to not be in 
 is inserting an edge (p, q) between edges (l, m) and (i, j), i.e., with  and , but this directly contradicts the fact that 
. 

Thus, if we can quickly update the swings of the growing prefixes (specifically, of their leftmost mappings), then we only only require constant additional time to perform our maximality check. In practice, the swing of a leftmost mapping can be computed inductively in the following fashion, leading to procedure ComputeSwings (Algorithm 3):

(Base case) Let  be a single-edged mapping corresponding to some character d; we want to find . For each character c in 
, we compute 
; that is, 
 is the next occurrence of c after i in X: this corresponds to a possible insertion of c before d if we move i after 
. Consider thus 
. At this point, the first occurrence of d in X after r (i.e. 
) is the top swing for the mapping. Computation of the bottom swing is symmetrical.

(Inductive case) Given a leftmost mapping 
 with its swings , we want to compute the swings for a leftmost mapping of the kind 
. We first need to compute the personal swing of the new edge (i, j) with respect to L, and compare it with the cumulative swing  of the previous mapping. Let 
; the personal swing of edge (i, j) with respect to L, denoted with 
, is the swing of mapping  performed over the strings 
, instead of over the whole strings X, Y. This swing tells us the change in the endpoints necessary to insert something between 
 and (i, j), and is analogous to the base case. The cumulative swing is simply the swing . At this point, the swing of the new mapping is given by the minimums of the two swings:


 

figure d
Procedure ComputeSwings performs the described operations, updating the swings of a mapping when an edge is added. We will adopt the convention that all leftmost mappings start with edge , which by itself has swing (|X|, |Y|). With this convention, the base case occurs when : in this case we have , which ensures that 
 are computed over the whole strings 
 (lines 3, 6), and , in turn ensuring that at line 14 the personal swing of (i, j) is returned.

Complexity analysis Every step of the procedure requires either  (when we iterate over the alphabet), or constant time (looking for the first occurrences of a character in a string). Thus, the total complexity for updating the swings is of .

Refined Algorithm for MCS Enumeration
We are now ready to present our refined enumeration algorithm (Algorithm 4). The algorithm implements the two improvements described in Sects. 5.1–5.2, employing the two procedures FindExtensions, ComputeSwings. Furthermore, one more refinement is given by lines 8–13, where the set of characters which correspond to at least one edge of 
 is computed. The recursive calls will be performed looping over this set, instead of the Ext set, ensuring in a more efficient way that no two recursive calls are performed with respect to the same character.

figure e
Just like its unrefined counterpart, Algorithm 4 employs a binary partition scheme over the characters that extend the current prefix in a valid way. The preprocessing phase is identical to the original one, as both the data structures and the FindUnshiftables procedure are unchanged. The recursive computation is performed by the procedure RefinedBinaryPartition, this time also keeping track of the swing of the mapping 
 of the current prefix P. Once again, we add the character  at positions ; the first recursive call is then performed with , and leftmost mapping 
, with swing given by the last positions of the strings (|X|, |Y|). This is considered the starting swing because trivially 
.

At each step, the procedure finds the set of valid extensions  by calling procedure FindExtensions (Algorithm 2). Then, at lines 8–13 it directly extracts the set  of valid characters that satisfy both conditions of Theorem 3. To this end, it loops over all edges of  checking for maximality through the swing property of Eq. (1). If the maximality condition is satisfied, the corresponding character is added to . If  is empty, then P has no valid extensions, and it is returned as it must be an MCS; otherwise, every character it contains produces a valid extension of P, and will thus produce a recursive call: for every character in , the procedure computes its first pairwise occurrences (i, j) after the mapping, updates the swings of 
 by calling procedure ComputeSwings (Algorithm 3), and performs the recursive call corresponding to prefix  by passing on the new mapping  and its updated swing. Being a refinement of Algorithm 1, the correctness of RefinedMCS is immediate.

Complexity analysis. The procedure starts by computing the extension set , by calling the -time procedure FindExtensions. Then, it computes set  by going through each of the  elements of , and performing the constant-time maximality check of Eq. (1) (line 19). Thus, computing  requires  time overall. Afterwards, for every element of , its first pairwise occurrences in 
 are computed. Once again, this can be performed in O(1) time, using the string data structure. Lastly, before recurring, the -time procedure ComputeSwings is called to update the swings of the new mapping. Thus, the time required to perform one recursive call of RefinedBinaryPartition is 
 However, we can further improve this with the following observation: every time line 19 is executed, a new recursive call is immediately generated. We can thus simply attribute the cost of ComputeSwings to this call (as if the computation was performed in it), meaning that each recursive call will only have to pay the cost of a single ComputeSwings procedure, i.e.,  time instead of 
 time, for a total time cost of

As before, the delay of the final algorithm RefinedMCS is given by the cost of one recursive call times the height of the partition tree, which in our case is O(n). Thus, algorithm RefinedMCS has a delay of

and requires 
 time for preprocessing. Regarding space complexity, in addition to the aforementioned data structures, we need to store other data for each recursive call in a root-to-leaf path in the partition tree. Namely, we store the sets  and , the last edge of the current leftmost mapping, and the values of the swings, leading to an additional  total space. The total space employed by the algorithm is therefore 
, and Theorem 1 is proved.

Specifically, with a constant-sized alphabet, our algorithm enumerates all MCS with delay , preprocessing time 
, and quadratic space complexity.

Conclusions and Acknowledgements
In this paper we have studied the Maximal Common Subsequences (MCSs), and investigated their combinatorial nature by familiarizing with some of their properties. Circumventing various pitfalls, we ultimately provided an efficient, binary partition-based, polynomial-delay algorithm for listing all MCSs on an equivalent bipartite graph problem.

The work done in this paper was partially funded by NII, Japan, JST CREST, Grant Number JPMJCR1401, and the Italian Ministry of University and Research (MIUR) under PRIN Project No. 20174LF3T8 AHeAD. The work was partially done at author TU’s laboratory when author AC was a postdoc at NII, author RG was on leave from the University of Pisa to visit NII, and author GP was visiting NII. A preliminary version of the work in this paper appeared in [6].

We would like to thank Professor Shin-Ichi Minato (Kyoto University) for the useful discussions on combinatorial properties of MCS held during the WEPA 2018 workshop in Pisa, including those illustrated in Example 2.

Notes
The Strong Exponential Time Hypothesis (SETH) [13] states that 
, where 
. It is widely believed to be true, and it has been used to prove conditional lower bounds for a variety of problems (see [2] for some examples).

The author employs these edges to improve the Hunt–Szymansky algorithm [12], which extracts one LCS of two strings of length n in , where r is the total number of ordered pairs of positions at which the two sequences match, that is, the number of edges in the string bipartite graph.

To be precise, each recursive node has up to  child nodes; the binary partition is seen by the fact that each recursive call corresponds to “using the edge 
” and the continuation after backtracking corresponds to “not using the edge 
”.

