We consider the 𝑘-PRIZE-COLLECTING STEINER TREE PROBLEM. An instance is composed of an integer k and a graph G with costs on edges and penalties on vertices. The objective is to find a tree spanning at least k vertices which minimizes the cost of the edges in the tree plus the penalties of vertices not in the tree. This is one of the most fundamental network design problems and is a common generalization of the PRIZE-COLLECTING STEINER TREE PROBLEM and the 𝑘-MINIMUM SPANNING TREE PROBLEM. Our main result is a 2-approximation algorithm, which improves on the currently best known approximation factor of 3.96 and has a faster running time. The algorithm builds on a modification of the primal-dual framework of Goemans and Williamson, and reveals interesting properties that can be applied to other similar problems.

Access provided by University of Auckland Library

Introduction
In many network design problems, the input consists of an edge-weighted graph, and the output is a minimum-cost tree connecting a certain subset of vertices. Two of the most fundamental NP -hard variants are the PRIZE-COLLECTING STEINER TREE PROBLEM (PCST) and the 𝑘-MINIMUM SPANNING TREE PROBLEM (𝑘-MST). For PCST, a solution may contain any subset of vertices, but any not spanned vertex incurs a penalty which is added to the objective-function. For 𝑘-MST, the output tree is required to contain at least k vertices.

In this paper we consider the 𝑘-PRIZE-COLLECTING STEINER TREE PROBLEM (𝑘-PCST), which is a common generalization of PCST and 𝑘-MST. An instance consists of a connected undirected graph 𝐺=(𝑉,𝐸), a special vertex r, called the root, and a non-negative integer 𝑘≤|𝑉|. Each edge 𝑒∈𝐸 has a non-negative cost 𝑐𝑒, and each vertex 𝑣∈𝑉 has a non-negative penalty 𝜋𝑣. A solution is a tree T spanning at least k vertices, including the root r, minimizing the cost of edges of the tree plus the penalties of vertices not spanned by the tree, i.e., minimizing ∑𝑒∈𝐸(𝑇)𝑐𝑒+∑𝑣∈𝑉∖𝑉(𝑇)𝜋𝑣. Without loss generality, we assume that 𝜋𝑟=∞.

Related Works
PCST is the special case of 𝑘-PCST for which 𝑘=0. For this problem, Bienstock et al. presented a 3-approximation based on an LP rounding algorithm [5], and Goemans showed that this factor could be improved to 2.54, by randomizing the rounding algorithm [18]. Goemans and Williamson presented a 2-approximation [12] based on a new general primal-dual scheme for PCST as well as many other constrained forest problems. To bound the value of an optimal solution, they used weak duality, and, as a consequence, their analysis implies that the integrality gap of the usual LP formulation is asymptotically tight. Currently, the best approximation for PCST is due to Archer et al. [1] and it has factor 2−(2−𝜌2+𝜌)2, where 𝜌 is an approximation factor for the STEINER TREE PROBLEM. Using the best-known value for 𝜌, which is ln(4)+𝜖 [7], this yields a factor of 1.9672+𝜖 for PCST.

𝑘-MST is the special case of 𝑘-PCST for which 𝜋𝑣=0 for each vertex v. One may assume that a solution spans exactly k vertices, since a tree with more than k vertices can be pruned without increasing its cost. Several approximation algorithms were devised for 𝑘-MST [4, 8, 17], and Blum et al. [6] gave the first constant-factor approximation, with factor 17, by using the primal-dual scheme. Garg improved this factor to 5 and 3, subsequently [10], and Arya and Ramesh showed how Garg’s algorithm could be used to obtain a factor of 2.5 [3]. Later, Arora and Karakostas gave a (2+𝜖)-approximation by modifying Garg’s algorithm [2]. Currently, the best approximation for 𝑘-MST is a 2-approximation due to Garg [11, 16] and is based on a sophisticated use of the primal-dual scheme.

To our knowledge, the first constant-factor approximation for 𝑘-PCST was given by Han et al. [13] and has factor 5. They presented a primal-dual scheme based on the Lagrangian relaxation of a linear program. Later, Matsuda and Takahashi [15] derived a 4-approximation by combining the solutions for the underlying instances of PCST and 𝑘-MST. The algorithm’s running time is (|𝑉|4|𝐸|log|𝑉|) and is bottlenecked by Garg’s 2-approximation, which is used to solve 𝑘-MST. By using the (1.9672+𝜖)-approximation for PCST, the approximation factor for 𝑘-PCST can be improved to 3.9672+𝜖, with a significant increase in the running time.

Our Results
Our main contribution is a 2-approximation for 𝑘-PCST. More precisely, we present an algorithm with running time (|𝑉|2|𝐸|2+|𝑉|4log2|𝑉|) that finds a tree T such that ∑𝑒∈𝐸(𝑇)𝑐𝑒+2⋅∑𝑣∈𝑉∖𝑉(𝑇)𝜋𝑣≤2⋅opt, where opt is the optimal value. This improves on both the approximation factor and the time complexity of the previously best-known algorithms. Our 2-approximation is based on a modified version of the Goemans and Williamson’s algorithm, and our analysis reveals many interesting properties of the primal-dual scheme, which might give insights to other problems with similar constraints. We note that the inequalities considered by our algorithm do not correspond to a dual LP formulation for 𝑘-PCST, and our analysis does not rely on weak duality. Moreover, a small modification of the algorithm results in a 2-approximation for the quota variant [14] of 𝑘-PCST, for which an instance includes vertex weights, and a solution is any tree whose weight is at least the given quota.

Algorithm’s Overview
Our algorithm successively executes a modified version of the 2-approximation primal-dual scheme for PCST due to Goemans and Williamson [12]. Their algorithm is divided into a growth-phase and a pruning-phase. In the growth-phase, it computes a feasible dual solution y such that for each subset of vertices S, 𝑦𝑆 is a non-negative value. It also outputs a tree T and a collection  of subsets of V, whose edges and subsets correspond to tight dual inequalities of an LP formulation. In the pruning-phase, the algorithm deletes from T the subsets in  which do not disconnect the graph, resulting in a pruned tree . To derive a 2-approximation, they bound the value of  by a factor of the dual objective-function; in our algorithm, we compare the output with an optimal solution directly.

In our modification, the growth-phase receives two new arguments, a potential 𝜆 and a tie-breaking list 𝜏. The potential is a uniform increase on the penalties of each vertex, such that for larger values of 𝜆, the output tree  spans more vertices. During the growth-phase, there might be concurrent events, thus there are multiple choices for the execution path. Usually, these choices are determined by some fixed lexicographic order. Our algorithm, on the contrary, relies on a tie-breaking list 𝜏 to control the priority among concurrent events. The i-th element in this list dictates which event gets the highest priority in the i-th iteration of the algorithm. This allows us to control the execution path of the algorithm.

The use of potential 𝜆 is built on Garg’s arguments for the 2-approximation for 𝑘-MST [11], which can be described as follows. If, for some 𝜆, the pruned tree  spans exactly k vertices, this leads to a 2-approximation by using the Lagrangian relaxation strategy (see, e.g., [8]). However, it might be the case that no such 𝜆 exists; thus, the idea is to find a particular value of 𝜆 such that, for sufficiently small 𝜖, using potential 𝜆−𝜖 leads to a pruned tree  spanning less than k vertices, and using potential 𝜆+𝜖 leads to a pruned tree  spanning at least than k vertices. The tree  is constructed by pruning a tree  using a collection . Similarly,  is constructed by pruning a tree  using a collection . On the one hand,  is a feasible solution, but its cost cannot be bounded in terms of vector y. On the other hand, the value of  can be bounded, but it is not a feasible solution.

In Garg’s algorithm, the trees  and  and the collections  and  might be very different. Thus, to obtain a tree with k vertices and whose cost can be bounded, his algorithm iteratively transforms  into  and  into  by replacing one edge of  or one subset in  at a time. At some iteration, pruning the current tree using the current collection must result in a tree spanning at least k vertices. And, before this step, instead of performing the operation, one augments the current pruned tree by adding a sequence of edges whose corresponding dual restrictions are tight, picking up to k vertices.

Our algorithm also considers similar trees  and  and corresponding collections  and . However, both trees are constructed by executing the growth-phase using a single potential 𝜆. To differentiate between the cases, we take into account a tie-braking list 𝜏 and its maximal proper prefix 𝜏˜. We show how to compute a special tuple (𝜆,𝜏), called the threshold-tuple, such that executing the growth-phase using 𝜏 results in a pruned tree with less than k vertices, while using 𝜏˜ results in a pruned tree with at least k vertices (or vice-versa).

The trees and collections output by the two executions of the growth-phase are only slightly different. Indeed, a key ingredient of our analysis (presented in Lemma 10) shows that one of the two following scenarios hold:

(i)
 and  are equal, and trees  and  differ in one edge; or

(ii)
 and  are equal, and collections  and  differ in one subset.

Moreover, we show that the vector y output in both executions of the growth-phase are the same. This leads to a straightforward way of augmenting the pruned tree , by picking a sequence of edges of  or  whose corresponding inequalities are tight, without the need for a step-by-step transformation.

Although the computed vector y satisfies a set of inequalities, these inequalities do not correspond to an LP dual formulation for 𝑘-PCST, hence we cannot use weak duality to bound the value of an optimal solution. Instead, we show (in Lemma 23) that either our algorithm returns a 2-approximate solution, or it identifies a non-empty subset of the vertices which are not spanned by any optimal solution. Thus, we either find the desired solution, or can safely reduce the size of the instance. Therefore, by running the algorithm at most |𝑉|−𝑘 times, we find a 2-approximate solution.

Text Organization
The remaining sections are organized as follows. In Sect. 2, we give a series of definitions and introduce the terminology used in the text. In Sect. 3, we describe our modification of the primal-dual scheme, which accounts for the potential 𝜆 and the tie-breaking list 𝜏. In Sect. 4, we formally define the threshold-tuple, and show how it can be computed. In Sect. 5, we show that, given a threshold-tuple, one can construct a tree spanning exactly k vertices. In Sect. 6, we bound the cost of the computed tree and give a 2-approximation for 𝑘-PCST. In Sect. 7, we give some concluding remarks.

Definitions and Preliminaries
We say that a nonempty collection ⊆2𝑉 is laminar if, for any two subsets 𝐿1,𝐿2∈, either 𝐿1∩𝐿2=∅, or 𝐿1⊆𝐿2, or 𝐿2⊆𝐿1. A laminar collection  is binary if for every 𝐿∈ with |𝐿|≥2, there are distinct non-empty subsets 𝐿1,𝐿2∈ such that 𝐿=𝐿1∪𝐿2. We denote the collection of inclusion-wise maximal subsets of a collection  by ∗. Observe that, if  is laminar, then the subsets in ∗ are disjoint.

Let  be a partition of V, and consider an edge e with extremes on V. If a set in  contains an extreme of e, then we call this set an endpart of e. We say that an edge e is internal in  if e has only one endpart, and we say that e is external in  if e has two distinct endparts. Also, two external edges are said to be parallel in  if they have the same pair of endparts.

Given a graph H and a subset 𝐿⊆𝑉, we say that H is L-connected if 𝑉(𝐻)∩𝐿=∅ or if the induced subgraph 𝐻[𝑉(𝐻)∩𝐿] is connected. For a collection  of subsets of vertices, we say that H is -connected if H is L-connected for every 𝐿∈.

Let 𝐿⊆𝑉 and 𝐻⊆𝐺. Then 𝛿𝐻(𝐿) denotes the set of edges of H with exactly one extreme in L. We say that L has degree |𝛿𝐻(𝐿)| on H. In the case where 𝐻=𝐺, we drop the subscript and write just 𝛿(𝐿).

For a subset 𝐿⊆𝑉, we define its new penalty as 𝜋𝜆𝐿=∑𝑣∈𝐿𝜋𝑣+𝜆|𝐿|, where 𝜆 is a non-negative value which we call potential. Note that, for any subset L containing the root r, we have 𝜋𝜆𝐿=∞.

Consider a vector y such that, for each 𝐿⊆𝑉, the entry 𝑦𝐿 is a non-negative variable. We say that y respects edge cost 𝑐 if

∑𝐿:𝑒∈𝛿(𝐿)𝑦𝐿≤𝑐𝑒for every edge 𝑒∈𝐸,
(1)
and we say that y respects vertex penalty 𝜋𝜆 if

∑𝑆:𝑆⊆𝐿𝑦𝑆≤𝜋𝜆𝐿for every subset 𝐿⊆𝑉.
(2)
We say that an edge e is tight for (𝑦,𝜆) if the inequality corresponding to e in (1) holds with equality. Analogously, a subset 𝐿⊆𝑉 is tight for (𝑦,𝜆) if the inequality corresponding to L in (2) is satisfied with equality. If the pair (𝑦,𝜆) is clear from the context, we simply say that e and L are tight.

Inequalities (1) and (2) are similar to the inequalities in the dual formulation for PCST [12], with the difference that we include inequalities for subsets containing r. However, these inequalities do not correspond to the dual of an LP formulation for 𝑘-PCST.

For a collection of subsets , denote by [𝑆] the collection of subsets in  which are subsets of S. Also, denote by (𝑆) the collection of subsets in  which contain some, but not all, vertices of S. Moreover, let 𝑐𝐸′=∑𝑒∈𝐸′𝑐𝑒 for 𝐸′⊆𝐸, and 𝜋𝐿=∑𝑣∈𝐿𝜋𝑣 for 𝐿⊆𝑉. To bound the value of an optimal solution, we use the following lemma.

Lemma 1
Let 𝑇∗ be an optimal solution. Suppose that  is a laminar collection and that 𝐿∗ is the minimal subset in  containing 𝑉(𝑇∗). If y respects 𝑐 and 𝜋𝜆, then

∑𝐿∈(𝐿∗)𝑦𝐿−𝜆|𝐿∗∖𝑉(𝑇∗)|≤𝑐𝐸(𝑇∗)+𝜋𝐿∗∖𝑉(𝑇∗).
Proof
Since  is laminar, each subset in (𝐿∗) contains some, but not all, vertices of 𝑉(𝑇∗), or it is a subset of 𝐿∗∖𝑉(𝑇∗). Also, one subset in (𝐿∗) cannot be 𝑉(𝑇∗), because otherwise 𝐿∗ would not be minimal. Thus,

∑𝐿∈(𝐿∗)𝑦𝐿=∑𝐿∈(𝑉(𝑇∗))𝑦𝐿+∑𝐿∈[𝐿∗∖𝑉(𝑇∗)]𝑦𝐿≤∑𝑒∈𝐸(𝑇∗)∑𝐿:𝑒∈𝛿(𝐿)𝑦𝐿+∑𝐿∈[𝐿∗∖𝑉(𝑇∗)]𝑦𝐿≤∑𝑒∈𝐸(𝑇∗)𝑐𝑒+𝜋𝜆𝐿∗∖𝑉(𝑇∗)=𝑐𝐸(𝑇∗)+𝜋𝐿∗∖𝑉(𝑇∗)+𝜆|𝐿∗∖𝑉(𝑇∗)|.
The first inequality holds as each subset in (𝑉(𝑇∗)) is crossed by at least one edge of 𝑇∗, and the second inequality holds because y respects 𝑐 and 𝜋𝜆. ◻

Modified Growth and Pruning Phases
In the following, we detail our modification of the primal-dual scheme due to Goemans and Williamson for PCST [12]. The algorithm is composed of two main routines: a clustering algorithm, also known as the growth-phase, and a cleanup algorithm, known as the pruning-phase.

Modified Clustering Algorithm
The modified growth-phase is described in the following and is denoted by 𝙶𝙿(𝜆,𝜏). A listing of all steps is given afterwards, in Algorithm 1. The algorithm maintains a binary laminar collection ⊆2𝑉, such that ∗ partitions the set V of vertices, and a vector y which respects 𝑐 and 𝜋𝜆. It iteratively constructs a forest 𝐹⊆𝐺 and a subcollection of processed subsets ⊆, such that the edges of F and the subsets of  are tight for (𝑦,𝜆). In each iteration, either a new edge is added to F, or a new subset is included in .

The algorithm begins by defining ={{𝑣}:𝑣∈𝑉} and 𝑦𝑆=0, for each 𝑆⊆𝑉 (implicitly), and by letting 𝐹=(𝑉,∅), and =∅. Once initialized, it starts the iteration process. At a given moment, a maximal subset 𝐿∈∗ is said to be active if it has not been processed yet, i.e., if 𝐿∈∗∖. In each iteration, we increase the value 𝑦𝐿 of every active subset L uniformly until one of the following events occurs:

▹:
an external edge e with endparts 𝐿1,𝐿2∈∗ becomes tight, in which case e is added to F, and the union 𝐿1∪𝐿2 is included in ; or

▹:
an active subset L becomes tight, in which case L is included in .

We note that multiple edges and subsets might become tight simultaneously. In our modified algorithm, we use the tie-breaking list 𝜏 to decide the order in which the events are processed. A tie-breaking list 𝜏 with size |𝜏| is a (possibly empty) sequence of edges and subsets. For each 𝑖=1,2,…,|𝜏|, the i-th element of the list is denoted by 𝜏𝑖. In iteration i, the event to be processed is determined according to the following order:

(i)
if 𝑖≤|𝜏|, then the event corresponding to 𝜏𝑖 has the highest priority;

(ii)
followed by events corresponding to edges;

(iii)
and finally by events corresponding to subsets.

The priority order between events of the same type is determined by a fixed lexicographic order.

The algorithm stops when V is the only active subset in ∗, at which point F is a tree. The algorithm defines 𝑇=𝐹, and outputs the pair (𝑇,).

figure a
Next lemma collects basic invariants of the growth-phase.

Lemma 2
At the beginning of any iteration of 𝙶𝙿(𝜆,𝜏), the following holds:

(gp1)
 is a binary laminar collection, ⋃𝐿∈∗𝐿=𝑉, and ∅∉;

(gp2)
y respects 𝑐 and 𝜋𝜆;

(gp3)
F is an -connected forest and every edge 𝑒∈𝐸(𝐹) is tight for (𝑦,𝜆);

(gp4)
⊆ is laminar and every subset in  is tight for (𝑦,𝜆);

(gp5)
no subset in  contains the root r.

Proof
Before the first iteration, ={{𝑣}:𝑣∈𝑉}, hence (gp1) is valid. Assume that the invariant is valid at the beginning of an iteration and notice that a new subset L is included into  only if 𝐿=𝐿1∪𝐿2, where 𝐿1,𝐿2∈∗. Therefore, (gp1) is also valid at the end of the iteration.

Before the first iteration, we have 𝑦=0, thus (gp2) is valid since 𝑐𝑒 and 𝜋𝜆𝑣 are non-negative for every edge e and vertex v. Assume that the invariant is valid at the beginning of an iteration. In this iteration, each variable 𝑦𝐿 corresponding to an active subset L is increased by the minimum value of 𝛥 such that an edge external in ∗ or an active subset becomes tight. Observe that the only edges that can become tight must be external. Moreover, if a subset 𝐿⊆𝑉 becomes tight, so does some subset 𝐿∈∗, because  is laminar. Therefore, after modifying y, no inequality is violated, and the invariant remains valid.

Before the first iteration, (gp3) is valid because  is composed of singletons, and F contains no edges. Assume that the invariant is valid at the beginning of an iteration, and notice that an edge e is included into F only if it is tight and the union 𝐿1∪𝐿2 of its endparts 𝐿1,𝐿2∈∗ is included into . Since F is 𝐿1-connected and 𝐿2-connected, it follows that F is also (𝐿1∪𝐿2)-connected because e was added to F. Therefore, (gp3) remains valid at the end of the iteration.

Invariant (gp3) holds because  is laminar, and only tight active subsets are included in . For invariant (gp5), observe that in any iteration, if 𝐿∈ is the active component containing the root r, then 𝜋𝜆𝐿=∞. It follows that L is not processed, and thus it is not included in . ◻

Observe that, since G is connected and the algorithm only ends when V is the only active component, by invariant (gp3) T spans V.

Corollary 1
Let (𝑇,) be the pair output by 𝙶𝙿(𝜆,𝜏). Then 𝑉(𝑇)=𝑉.

Lemma 3
The number of iterations executed by 𝙶𝙿(𝜆,𝜏) is at most 3|𝑉|−3.

Proof
Since F is a forest (invariant (gp3)), the number of processed edges is at most |𝑉|−1. Observe that the size of  is initially |V| and increases by 1 for each processed edge. Thus, the final size of  is at most 2|𝑉|−1. Each subset of  which does not contain the root r can be processed. Therefore, the total number of processed events is at most 3|𝑉|−3. ◻

A consequence is that the growth-phase executes in polynomial time. Also, this implies that the size of a tie-breaking list need not exceed 3|𝑉|−3.

Modified Pruning Algorithm
The modified pruning-phase is denoted by 𝙿𝙿(𝐻,). A listing is given in Algorithm 2. The algorithm receives a graph H and iteratively deletes from it any processed subset 𝐵∈ such that the degree of B in H is one. We assume that the input H is a connected subgraph of G containing r, and that  is a laminar collection of subsets of V which do not contain r.

figure b
In the following, we might say that we prune H using  to mean that we execute algorithm 𝙿𝙿(𝐻,), and say that a graph H is pruned with  if |𝛿𝐻(𝐵)|≠1 for every 𝐵∈.

Note that we allow input graphs H with cycles, whereas the standard pruning-phase only considers trees. This will be useful in Sect. 5.1, when, to find a tree with k vertices from distinct trees  and , we will first prune , which might contain a cycle.

Next lemma collects invariants of the pruning-phase.

Lemma 4
At the beginning of any iteration of 𝙿𝙿(𝐻,), the following holds:

(pp1)
H is connected and 𝑟∈𝑉(𝐻);

(pp2)
𝐻[𝑉(𝐻)∩𝐵] is connected.

Proof
Observe that H is connected and 𝑟∈𝑉(𝐻) when the algorithm starts. Consider the subset B chosen at the beginning of an iteration, and assume that (pp1) is valid. Since |𝛿𝐻(𝐵)|=1 and 𝑟∉𝐵, deleting B does not disconnect H nor removes the root r from H. Thus, (pp1) holds at the end of the iteration. For (pp2), observe that, since H is connected and |𝛿𝐻(𝐵)|=1, 𝑉(𝐻)∩𝐵 must induce a connected subgraph. ◻

Corollary 2
Let 𝐻′ be the graph output by 𝙿𝙿(𝐻,). Then, 𝐻′⊆𝐻, 𝐻′ is pruned with , and 𝑟∈𝑉(𝐻).

Proof
By construction, 𝐻′⊆𝐻 and 𝐻′ is pruned with . Invariant (pp1) implies 𝑟∈𝐻′. ◻

Next, we give a structural result about the pruning-phase. It is an auxiliary lemma which implies a series of monotonic properties of the pruning algorithm.

Lemma 5
Consider connected graphs D and H both containing r. Assume that D is pruned with  and let 𝐻′ be the graph output by 𝙿𝙿(𝐻,). If 𝐷⊆𝐻, then 𝐷⊆𝐻′.

Proof
Assume that in the execution that output 𝐻′, the algorithm executed ℓ iterations, and let 𝐵𝑖 and 𝐻𝑖 be the values assigned to variables B and H at the beginning of iteration i. Also, let 𝐻ℓ+1=𝐻′.

We show that 𝐷⊆𝐻𝑖 by induction on i. Clearly 𝐷⊆𝐻=𝐻1. Then, assume that 𝐷⊆𝐻𝑖 for some 𝑖≥1. It follows that |𝛿𝐷(𝐵𝑖)|≤|𝛿𝐻𝑖(𝐵𝑖)|. But |𝛿𝐻𝑖(𝐵𝑖)|=1 by the choice of 𝐵𝑖, and so |𝛿𝐷(𝐵𝑖)|≤1. Since D is pruned with , we know that |𝛿𝐷(𝐵𝑖)|≠1, and thus |𝛿𝐷(𝐵𝑖)|=0. This implies that 𝐵𝑖∩𝑉(𝐷)=∅, because D is connected containing r and 𝑟∉𝐵𝑖 (invariant (gp5). To complete the induction, observe that 𝐷=𝐷−𝐵𝑖⊆𝐻𝑖−𝐵𝑖=𝐻𝑖+1. ◻

An implication of this lemma is that the output of 𝙿𝙿(𝐻,) is invariant to the order in which subsets of  are considered.

Corollary 3
Let 𝐻1 and 𝐻2 be the graphs output by two executions of 𝙿𝙿(𝐻,). Then 𝐻1=𝐻2.

Proof
Observe that 𝐻1 is connected and pruned with , and that 𝐻2 is the output of 𝙿𝙿(𝐻,). Since 𝐻1⊆𝐻, by Lemma 5, 𝐻1⊆𝐻2. Symmetrically, we have 𝐻2⊆𝐻1, and thus 𝐻1=𝐻2. ◻

Next corollary states that pruning a fixed graph H using a subcollection  produces a supergraph of the graph obtained from pruning H with .

Corollary 4
Let 𝐻1 and 𝐻2 be the graphs output by 𝙿𝙿(𝐻,1) and 𝙿𝙿(𝐻,2), respectively. If 1⊆2, then 𝐻2⊆𝐻1.

Proof
Observe that 𝐻2 is connected and pruned with 2, and thus it is pruned with 1 as well. Since 𝐻2⊆𝐻, by Lemma 5, 𝐻2⊆𝐻1. ◻

The last corollary considers graphs D which are “sandwiched” between a graph H and the pruned subgraph 𝐻′. Intuitively, one may interpret this corollary as stating that pruning a partially pruned graph D leads to the fully pruned graph 𝐻′.

Corollary 5
Consider connected graphs D and H. Let 𝐷′ and 𝐻′ be the graphs output by 𝙿𝙿(𝐷,) and 𝙿𝙿(𝐻,), respectively. If 𝐻′⊆𝐷⊆𝐻, then 𝐻′=𝐷′.

Proof
Observe that 𝐻′ is connected and pruned with . Since 𝐻′⊆𝐷, by Lemma 5, 𝐻′⊆𝐷′. Analogously, observe that 𝐷′ is connected and pruned with . Since 𝐷′⊆𝐷⊆𝐻, by Lemma 5, 𝐷′⊆𝐻′. Therefore, 𝐻′=𝐷′. ◻

Modified Goemans–Williamson Algorithm
The modified Goemans-Williamson algorithm wraps up the growth and the pruning-phases and is denoted by 𝙶𝚆(𝜆,𝜏). A listing is given in Algorithm 3. First, the algorithm executes 𝙶𝙿(𝜆,𝜏) to obtain a pair (𝑇,). Then, it executes 𝙿𝙿(𝑇,) and returns the pruned tree .

figure c
One interesting property of 𝙶𝚆(𝜆,𝜏) is that if 𝜆 is too large, then no subset is ever processed, and the returned tree spans the whole set of vertices. Recall that 𝑐𝐸=∑𝑒∈𝐸𝑐𝑒. Next lemma states that 𝑐𝐸 is sufficiently large.

Lemma 6
Let  be the output of 𝙶𝚆(𝜆,𝜏). If 𝜆>𝑐𝐸, then .

Proof
We claim that =∅. If this is the case, by Corollary 1, we have that 𝙶𝙿(𝜆,𝜏) returns the tuple (𝑇,∅), thus no subset is deleted in the pruning-phase, and we get  and .

Now, we show that =∅. Suppose, for a contradiction, that the first processed subset is L and it is processed in iteration ℓ. For a vertex v, denote by 𝛤𝑣 the value of ∑𝑆:𝑣∈𝑆𝑦𝑆 at the end of iteration ℓ. Since no subset was processed before iteration ℓ, each vertex is contained in exactly one active subset in each iteration 𝑖≤ℓ. It follows that, for any two vertices u and v, we have 𝛤𝑢=𝛤𝑣.

Since no subset containing the root r is ever processed (invariant (gp5)), 𝑟∉𝐿, and since G is connected, there must exist an external edge e with extremes 𝑢∈𝐿 and 𝑤∉𝐿. Since y respects 𝑐 (invariant (gp2)), and no 𝑆∈ contains both u and w at the end of iteration ℓ, we have

𝛤𝑢+𝛤𝑤=∑𝑆:𝑒∈𝛿(𝑆)𝑦𝑆≤𝑐𝑒.
Then, since L becomes tight at iteration ℓ,

𝜋𝜆𝐿=∑𝑆:𝑆⊆𝐿𝑦𝑆≤∑𝑣∈𝐿𝛤𝑣=|𝐿|𝛤𝑢≤|𝐿|𝑐𝑒≤𝑐𝐸,
which is a contradiction because 𝜋𝜆𝐿≥𝜆>𝑐𝐸. ◻

The Threshold-Tuple
We execute the modified Goemans-Williamson algorithm using potential zero and passing an empty tie-breaking list, i.e., we execute 𝙶𝚆(0,∅). If the returned tree spans at least k vertices, then this tree is a 2-approximate solution, as stated in Lemma 7. The proof is adapted from Feofiloff et al. [9] and is given in Appendix A.

Lemma 7
Let  be the tree returned by 𝙶𝚆(0,∅), and 𝑇∗ be an optimal solution. Then,


In the remainder of this section, we assume that executing 𝙶𝚆(0,∅) returns a tree spanning less than k vertices. Observe that Lemma 6 implies that using potential greater than 𝑐𝐸 leads to a tree with at least k vertices for any tie-breaking list 𝜏. We would like to find 𝜆 and associated 𝜏 such that the returned tree spans exactly k vertices, but it might be the case that no such pair exists. Instead, our goal will be finding a special tuple (𝜆,𝜏), called the threshold-tuple, which will be defined below.

First, we need to introduce some notation. Note that, in the i-th iteration of 𝙶𝙿(𝜆,𝜏), the edge or subset corresponding to 𝜏𝑖 is not necessarily tight. We say that a tie-breaking list 𝜏 is respected by potential 𝜆 if the sequence of edges and subsets of V processed in the first |𝜏| iterations of 𝙶𝙿(𝜆,𝜏) corresponds to 𝜏. Also, we denote by 𝜏˜ the prefix of 𝜏 with size |𝜏|−1.

Definition 1
Let  be the tree returned by 𝙶𝚆(𝜆,𝜏), and  be the tree returned by 𝙶𝚆(𝜆,𝜏˜). We say that (𝜆,𝜏) is a threshold-tuple if

(i)
𝜏 is respected by 𝜆; and

(ii)
 or .

Given a threshold-tuple, one may obtain a pair of trees, one with less than k vertices, and the other with at least k vertices. These trees share many of their structures, and this will be used in Sect. 5 to construct a tree which spans exactly k vertices and is a 2-approximation. In Sect. 4.1, we study the properties of a threshold-tuple and, in Sect. 4.2, we show how it can be computed in polynomial time.

Properties of a Threshold-Tuple
We start by noticing that the executions of 𝙶𝙿(𝜆,𝜏) and 𝙶𝙿(𝜆,𝜏˜) are identical up to the beginning of iteration |𝜏|.

Lemma 8
If 𝜏 is respected by 𝜆, then 𝜏˜ is respected by 𝜆. Also, at the beginning of the iteration |𝜏| of 𝙶𝙿(𝜆,𝜏) and 𝙶𝙿(𝜆,𝜏˜), the variables F, ,  and y are identical.

Proof
The first statement is clear by definition. For the second statement, observe that up to the beginning of the iteration |𝜏|, the execution depends only on the |𝜏|−1 items of the tie-breaking list. ◻

The event processed in the iteration |𝜏| of the growth-phase plays an important role in distinguishing the outputs returned by executing 𝙶𝚆(𝜆,𝜏) and 𝙶𝚆(𝜆,𝜏˜). Each such an event corresponds to an edge or a subset. The next auxiliary result lists the possibilities when (𝜆,𝜏) is a threshold-tuple.

Lemma 9
Assume that (𝜆,𝜏) is a threshold-tuple. Let 𝜎 be the edge or subset processed in the |𝜏|-th iteration of 𝙶𝙿(𝜆,𝜏), and 𝜎′ be the edge or subset processed in |𝜏|-th iteration of 𝙶𝙿(𝜆,𝜏˜). Then 𝜎≠𝜎′, and 𝜎′ is an edge.

Proof
If we had 𝜎=𝜎′, then the output of 𝙶𝙿(𝜆,𝜏˜) would be identical to the output of 𝙶𝙿(𝜆,𝜏), and 𝙶𝚆(𝜆,𝜏˜) and 𝙶𝚆(𝜆,𝜏) would return identical trees. This is not possible, as (𝜆,𝜏) is a threshold-tuple, thus indeed 𝜎≠𝜎′.

First, assume that 𝜎 is an edge. Since 𝜏 is respected by 𝜆, 𝜎=𝜏|𝜏|. Thus, at the beginning of the |𝜏|-th iteration of 𝙶𝙿(𝜆,𝜏), 𝜎 is a tight edge external in ∗. By Lemma 8, at the beginning of the |𝜏|-th iteration of 𝙶𝙿(𝜆,𝜏˜), 𝜎 is also a tight edge external in ∗. Observe that in this iteration of 𝙶𝙿(𝜆,𝜏˜), edges have the highest priorities, since the size of the considered tie-breaking list is |𝜏˜|<|𝜏|. Because there is at least one tight edge to be processed, this iteration processes an edge, thus 𝜎′ is an edge.

Now, assume that 𝜎 is a subset. Suppose, for a contradiction, that 𝜎′ is a subset. Since a subset is processed in the |𝜏|-iteration of 𝙶𝙿(𝜆,𝜏˜), there are no tight edges external in ∗ in the beginning of this iteration. Let 𝐿0,…,𝐿𝑚 be the collection of tight active subsets in the beginning of the |𝜏|-th iteration of 𝙶𝙿(𝜆,𝜏˜), which is the same for 𝙶𝙿(𝜆,𝜏) by Lemma 8. Notice that processing a subset does not modify variables F,  and y, thus each subset 𝐿𝑖 must be processed in both executions (although in different order) before any other edge or subset becomes tight. It follows that, in the beginning of the (|𝜏|+𝑚)-th iteration of both 𝙶𝙿(𝜆,𝜏) and 𝙶𝙿(𝜆,𝜏˜), variables F, ,  and y are identical. This implies that both executions have the same output. But, again, this is a contradiction because (𝜆,𝜏) is a threshold-tuple. ◻

In the following, we use the notion of rounds of iterations. Recall that during the execution of 𝙶𝙿(𝜆,𝜏), there might be iterations for which the increment 𝛥 to variables of y is set to zero, and thus vector y remains unaltered. We say that iterations i and j are in the same round if the value of vector y at the end of iteration i equals the value of vector y at the end of iteration j.

The output of the growth-phase corresponds to a tree and a collection of processed subsets. Suppose that executing 𝙶𝙿(𝜆,𝜏) returns pair (𝑇,), and executing 𝙶𝙿(𝜆,𝜏˜) returns pair (𝑇′,′). While, for a threshold-tuple, these pairs must be different, we show that the difference is restricted to adding and removing an edge, or adding a subset. The proof’s arguments rely on the fact that both executions of the growth-phase are almost identical, and differ only in the round of iteration |𝜏|.

Whether the trees or the collections will be different depends on the event processed at iteration |𝜏|. If 𝙶𝙿(𝜆,𝜏) processes an edge 𝜎, then 𝙶𝙿(𝜆,𝜏˜) processes an edge 𝜎′, and 𝑇=𝑇′+𝜎−𝜎′. We give an illustration in Fig. 1. The state at the beginning of iteration |𝜏| is depicted in Fig. 1a, which is the same for both executions. Each maximal subset is represented by a circle, and processed subsets correspond to filled circles. Only edges which are tight and external are drawn, and only maximal subsets which are tight are labelled. The priority order of each edge and subset is given by the corresponding label index. Figure 1b represents the end of the round for 𝙶𝙿(𝜆,𝜏˜), where the contours surrounding circles are the subsets added to . In this execution, 𝜎′=𝑒1 is the first edge, and the processing order is 𝑒1,𝑒2,𝑒3,𝑒4,𝑒5. Figure 1c represents the end of the round for 𝙶𝙿(𝜆,𝜏), for which 𝜎=𝑒6 is the last item of 𝜏, and the processing order is 𝑒6,𝑒1,𝑒2,𝑒3,𝑒4.

Fig. 1
figure 1
Executions of 𝙶𝙿(𝜆,𝜏˜) and 𝙶𝙿(𝜆,𝜏) in the edge case

Full size image
Fig. 2
figure 2
Executions of 𝙶𝙿(𝜆,𝜏˜) and 𝙶𝙿(𝜆,𝜏) in the subset case

Full size image
When 𝙶𝙿(𝜆,𝜏) processes a subset 𝜎 at iteration |𝜏|, we have =′∪{𝜎}. We give an illustration in Fig. 2, where we use the same convention as before. Figure 2a depicts the state at the beginning of iteration |𝜏|. Figure 2b represents the end of the round for 𝙶𝙿(𝜆,𝜏˜), for which 𝜎′=𝑒1 is the first edge, and the processing order is 𝑒1,𝑒2,𝑒3,𝑒4,𝐿2. Figure 2c represents the end of the round for 𝙶𝙿(𝜆,𝜏), for which 𝜎=𝐿3 is the last item of 𝜏, and the processing order is 𝐿3,𝑒1,𝑒2,𝑒3,𝑒4,𝐿2. Notice that 𝐿1 was not processed in either execution because, after 𝑒2 had been processed, 𝐿1 stopped being maximal. The subset 𝐿3 was processed by 𝙶𝙿(𝜆,𝜏), but not by 𝙶𝙿(𝜆,𝜏˜), because 𝐿3 is processed first in the execution of 𝙶𝙿(𝜆,𝜏), whereas it stopped being maximal in the execution of 𝙶𝙿(𝜆,𝜏˜) after 𝑒4 had been processed.

The following lemma summarizes the main properties of threshold-tuples.

Lemma 10
Assume that (𝜆,𝜏) is a threshold-tuple, and let 𝜎=𝜏|𝜏|. Also, let T,  and y be the output computed by 𝙶𝙿(𝜆,𝜏), and let 𝑇′, ′ and 𝑦′ be the output computed by 𝙶𝙿(𝜆,𝜏˜). Then, 𝑦=𝑦′ and

(i)
if 𝜎 is an edge, then =′, 𝜎∉𝐸(𝑇′), and 𝑇⊆𝑇′+𝜎;

(ii)
if 𝜎 is a subset, then 𝑇=𝑇′, 𝜎∉′, and =′∪{𝜎}.

Proof
Note that 𝜎 is the edge or subset processed in the |𝜏|-th iteration of 𝙶𝙿(𝜆,𝜏), because 𝜏 is respected by 𝜆. Let 𝜎′ be the edge or subset processed in the |𝜏|-th iteration of 𝙶𝙿(𝜆,𝜏˜). By Lemma 9, 𝜎≠𝜎′, and 𝜎′ is an edge. We consider two cases, depending on the type of 𝜎.

Case 1: Both 𝜎 and 𝜎′ are edges.

Let  be the set of tight external edges considered to be processed in the |𝜏|-th iteration of 𝙶𝙿(𝜆,𝜏). Similarly, define ′ as the analogous set corresponding to 𝙶𝙿(𝜆,𝜏˜). Since 𝜎 and 𝜎′ are processed edges, 𝜎∈, and 𝜎′∈′. By Lemma 8, we have =′, and thus 𝜎,𝜎′∈. Because edges are processed with priority higher than subsets, each edge of  is processed or becomes internal in the same round of iteration |𝜏|, and before any active tight subset is processed.

Since 𝜎∈, at some iteration |𝜏|+ℓ, for some ℓ≥0, the execution 𝙶𝙿(𝜆,𝜏˜) must process edge 𝜎 or some other edge of  which is parallel to 𝜎. For some 𝑖≥0, let 𝑒′𝑖 be the edge of  processed by 𝙶𝙿(𝜆,𝜏˜) at iteration |𝜏|+𝑖. Therefore, 𝑒′0=𝜎′, and 𝑒′ℓ is parallel to 𝜎 at iteration |𝜏|+ℓ. Similarly, let 𝑒𝑖 be the edge of  processed by 𝙶𝙿(𝜆,𝜏) at iteration |𝜏|+𝑖, and observe that 𝑒0=𝜎.

We claim that for each 1≤𝑖≤ℓ, 𝑒𝑖=𝑒′𝑖−1. To see this, note that at the beginning of iteration |𝜏|+𝑖 of 𝙶𝙿(𝜆,𝜏), edge 𝑒′𝑖−1 is tight and external. Since 𝑒′𝑖−1 was processed in iteration |𝜏|+𝑖−1 of 𝙶𝙿(𝜆,𝜏˜), it has priority higher than any other tight edge of  which is still external. Thus, 𝑒′𝑖−1 is selected to be processed, and indeed 𝑒𝑖=𝑒′𝑖−1.

At the end of iteration |𝜏|+ℓ, the forest F computed by 𝙶𝙿(𝜆,𝜏) and the forest 𝐹′ computed by 𝙶𝙿(𝜆,𝜏˜) are such that 𝐹−𝜎′=𝐹′−𝜎. Moreover, 𝜎 and 𝜎′ are in the same connected components of both F and 𝐹′. Thus, the value of y, the set of external edges, and the collection of active subsets at the end of this iteration are the same in both executions. This implies that the sequence of processed edges and subsets in succeeding iterations are the same. Therefore, the output of 𝙶𝙿(𝜆,𝜏) and 𝙶𝙿(𝜆,𝜏˜) are such that 𝑦=𝑦′, =′, and 𝑇=𝑇′+𝜎−𝜎′. This shows the lemma when 𝜎 is an edge.

Case 2: 𝜎 is a subset and 𝜎′ is an edge.

Let  be the collection of active tight subsets considered to be processed in the |𝜏|-th iteration of 𝙶𝙿(𝜆,𝜏). Similarly, define ′ as the analogous collection corresponding to 𝙶𝙿(𝜆,𝜏˜). Since 𝜎 is a processed subset, 𝜎∈. By Lemma 8, we have =′.

Suppose that, after processing edge 𝜎′, 𝙶𝙿(𝜆,𝜏˜) processes other ℓ edges in the same round of iterations. Since edges have higher priority, no subset is processed before every tight external edge is processed or becomes internal. Let 𝑒′0,𝑒′1,…,𝑒′ℓ be the edges processed in iteration |𝜏|,|𝜏|+1,…,|𝜏|+ℓ of 𝙶𝙿(𝜆,𝜏˜), respectively. Note that 𝑒′0=𝜎′.

By Lemma 8, each 𝑒′𝑖 is also a tight external edge in iteration |𝜏| of 𝙶𝙿(𝜆,𝜏). Processing a subset does not change the set of tight external edges, thus, after processing subset 𝜎, 𝙶𝙿(𝜆,𝜏) must process the sequence of edges 𝑒′0,𝑒′1,…,𝑒′ℓ, such that 𝑒′𝑖 is processed at iteration |𝜏|+𝑖+1 of 𝙶𝙿(𝜆,𝜏). This implies that the set of edges processed by 𝙶𝙿(𝜆,𝜏˜) up to iteration |𝜏|+ℓ equals the set of edges processed by 𝙶𝙿(𝜆,𝜏) at up to iteration |𝜏|+ℓ+1.

As a consequence, any tight subset of  that remains maximal at the end of iteration |𝜏|+ℓ of 𝙶𝙿(𝜆,𝜏˜) is also tight and maximal at the end of iteration |𝜏|+ℓ+1 of 𝙶𝙿(𝜆,𝜏), and vice-versa. After processing edges, any such subset which is still unprocessed is processed. Therefore, at the end of the rounds corresponding to 𝙶𝙿(𝜆,𝜏˜) and 𝙶𝙿(𝜆,𝜏), the value of y, the set of external edges, and the collection of active subsets are the same in both executions. As in the previous case, this implies that the sequence of processed edges and subsets in succeeding iterations are the same.

Now observe that both executions process the same set of edges, thus 𝑇=𝑇′. Also, any subset processed by 𝙶𝙿(𝜆,𝜏˜) is processed by 𝙶𝙿(𝜆,𝜏). Conversely, if a subset processed by 𝙶𝙿(𝜆,𝜏) is not processed by 𝙶𝙿(𝜆,𝜏˜), then this subset must be 𝜎. Thus, =′∪{𝜎}. To show that 𝜎∉′, note that if =′, then 𝙶𝚆(𝜆,𝜏) and 𝙶𝚆(𝜆,𝜏˜) would return identical trees, which is not possible since (𝜆,𝜏) is a threshold-tuple. This completes the lemma. ◻

Computing a Threshold-Tuple
Before describing the algorithm to compute a threshold-tuple (𝜆,𝜏), we need some definitions and corresponding auxiliary lemmas.

The Increase-Function
In the following, suppose that we are given a fixed tie-breaking list 𝜏 and a real interval [a, b] such that, for every 𝜆∈[𝑎,𝑏], 𝜏 is respected by 𝜆. Then, there is a fixed pair (,) such that, for every 𝜆∈[𝑎,𝑏], the laminar collection and the collection of processed subsets computed by 𝙶𝙿(𝜆,𝜏) at the end of iteration |𝜏| correspond to  and , respectively. Let =∗∖, and note that  is the collection of active subsets at the beginning of iteration |𝜏|+1. Define the object-collection  corresponding to 𝜏 and [a, b] as the collection that contains the active subsets, the external edges with at least one active endpart, and the tight external edges with no active endparts at the beginning of iteration |𝜏|+1. Observe that, for 𝜆∈[𝑎,𝑏], if 𝙶𝙿(𝜆,𝜏) processes an edge or a subset 𝜎 at iteration |𝜏|+1, then 𝜎∈. Thus,  contains all subsets and edges that might be processed at iteration |𝜏|+1 for some 𝜆∈[𝑎,𝑏].

At the beginning of iteration |𝜏|+1 of 𝙶𝙿(𝜆,𝜏), the variable 𝑦𝑆 of every active subset 𝑆∈ is increased by the largest value 𝛥≥0, for which no inequality corresponding to an edge or a subset is violated. To compute 𝛥, one first finds, for each edge or subset 𝜎∈, the value 𝛥𝜎 to increase each variable 𝑦𝑆 such that 𝜎 becomes tight. Since the considered tie-breaking list 𝜏 and interval [a, b] are fixed, the value 𝛥𝜎 depends only on 𝜆. The increase-function of 𝜎 corresponding to 𝜏 and [a, b] is the function 𝜖𝜎:[𝑎,𝑏]→ℚ≥0 such that 𝜖𝜎(𝜆)=𝛥𝜎.

Lemma 11
For every 𝜎∈, the increase-function of 𝜎 is linear in 𝜆.

Proof
For some integer ℓ, with 0≤ℓ≤|𝜏|, let 𝑇ℓ be the sublist of 𝜏 defined as 𝑇ℓ=(𝜏1,𝜏2,…,𝜏ℓ). Observe that the tie-breaking list 𝑇ℓ is respected by 𝜆 for every 𝜆∈[𝑎,𝑏], then there is an object-collection ℓ corresponding to 𝑇ℓ and [a, b]. Also, for each index 1≤ℓ≤|𝜏|, the entry 𝜏ℓ of the tie-breaking list is processed at iteration ℓ of 𝙶𝙿(𝜆,𝜏), and thus 𝜏ℓ∈ℓ−1. Note that |𝜏|=, thus, by defining 𝜏|𝜏|+1=𝜎, we have 𝜏ℓ∈ℓ−1 for index ℓ=|𝜏|+1 as well.

Let 𝜖ℓ(𝜆) be the increase-function of 𝜏ℓ corresponding to 𝑇ℓ−1 and [a, b]. We will show by induction that, for each 1≤ℓ≤|𝜏|+1, function 𝜖ℓ(𝜆) is linear in 𝜆. The lemma will follow by taking ℓ=|𝜏|+1.

First, consider the case ℓ=1. If 𝜏1 is a subset, then it must be a singleton {𝑣} for some vertex v, because each maximal subset in  is a singleton in the first iteration of the growth-phase. Since at this moment, 𝑦=0, the increase to turn 𝜏1 tight is 𝜖1(𝜆)=𝜋𝜆𝑣=𝜋𝑣+𝜆. Similarly, if 𝜏1 is an edge e, it must have two active endparts at the first iteration, and thus 𝜖1(𝜆)=𝑐𝑒/2.

Now, assume that 𝜖𝑖(𝜆) is linear in 𝜆 for each 𝑖<ℓ. Consider some subset S, and let 𝛬𝑆 denote the set of indices i such that 𝑖<ℓ and S was an active subset at the beginning of iteration i. It follows that, at the beginning of iteration ℓ, 𝑦𝑆=∑𝑖∈𝛬𝑆𝜖𝑖(𝜆), which is linear in 𝜆, since each 𝜖𝑖 is linear in 𝜆, and 𝛬𝑆 does not depend on 𝜆.

Let  be the collection of active subsets at the beginning of iteration ℓ. If 𝜏ℓ is an external edge e with no active endparts, then it is tight, thus 𝜖ℓ(𝜆)=0. In the case that 𝜏ℓ is an edge e with at least one active endpart, the left-hand side of inequality (2) corresponding to e is ∑𝑆:𝑒∈𝛿(𝑆)𝑦𝑆, that is a linear function, say 𝑓(𝜆). It follows that the increase of 𝑦𝑆 for active subsets S which is necessary to turn e tight is

𝜖ℓ(𝜆)=𝑐𝑒−𝑓(𝜆)|{𝑆∈:𝑒∈𝛿(𝑆)|.
In both cases, 𝜖ℓ(𝜆) is linear in 𝜆.

Similarly, if 𝜏ℓ is a subset L, then the left-hand side of inequality (1) corresponding to L is ∑𝑆:𝑆⊆𝐿𝑦𝑆, that is a function which is linear in 𝜆. Let this function be 𝑓(𝜆), then,

𝜖ℓ(𝜆)=𝜋𝜆𝐿−𝑓(𝜆)=𝜋𝐿+𝜆|𝐿|−𝑓(𝜆),
which is linear in 𝜆. ◻

The Diverging Potential
For each potential 𝜆∈[𝑎,𝑏], the element of the object-collection  corresponding to 𝜏 and [a, b] that is processed at iteration |𝜏|+1 of the growth-phase is determined by the increase-function which evaluates to the smallest value. Suppose that executing 𝙶𝙿(𝑎,𝜏) processes some edge or subset 𝜎1 at iteration |𝜏|+1, while executing 𝙶𝙿(𝑏,𝜏) processes a distinct edge or subset 𝜎2 at iteration |𝜏|+1. In this situation, 𝜎1 is selected in the former execution because 𝜖𝜎1(𝑎)≤𝜖𝜎′(𝑎) for any 𝜎′∈, while 𝜎2 is selected in the latter because 𝜖𝜎2(𝑏)≤𝜖𝜎′(𝑏) for any 𝜎′∈. Since increase-functions are linear, this implies that there must be some potential 𝑝∈(𝑎,𝑏) such that, at iteration |𝜏|+1, 𝙶𝙿(𝜆,𝜏) processes 𝜎1 for 𝜆<𝑝, and processes a distinct element in  for 𝜆>𝑝.

Formally, we say that a value 𝑝∈(𝑎,𝑏) is a diverging potential if there are edges or subsets 𝜎,𝜎′∈ with distinct increase-functions 𝜖𝜎,𝜖𝜎′ such that: (i) 𝜖𝜎(𝑝)=𝜖𝜎′(𝑝); and (ii) execution 𝙶𝙿(𝑝,𝜏) processes 𝜎 at iteration |𝜏|+1. Observe that, since increase-functions are linear, the number of diverging potentials is finite and they can be computed in polynomial time using standard line-intersection algorithms.

The definition implies that, for each pair of consecutive diverging potentials, there is an edge or a subset which is tight at iteration |𝜏|+1 when the growth-phase is executed with either diverging potential. This is formalized next.

Lemma 12
Let 𝑝1<𝑝2<⋯<𝑝𝑚 be the sequence of diverging potentials in (a, b), and let 𝑝0=𝑎 and 𝑝𝑚+1=𝑏. Also, let 𝜆0∈(𝑝𝑗,𝑝𝑗+1) for some 0≤𝑗≤𝑚. If 𝜎∈ is processed at iteration |𝜏|+1 of 𝙶𝙿(𝜆0,𝜏), then 𝜎 is tight at the end of iteration |𝜏|+1 of 𝙶𝙿(𝜆,𝜏) for every 𝜆∈[𝑝𝑗,𝑝𝑗+1].

Proof
Because 𝜎 has been processed, 𝜖𝜎(𝜆0)≤𝜖𝜎′(𝜆0) for any 𝜎′∈ with distinct increase-function. Since 𝑝𝑗 and 𝑝𝑗+1 are consecutive potentials, functions 𝜖𝜎 and 𝜖𝜎′ do not intersect for any 𝜆∈(𝑝𝑗,𝑝𝑗+1). Thus, by the continuity of the increase-functions, 𝜖𝜎(𝜆)≤𝜖𝜎′(𝜆) for each 𝜆∈[𝑝𝑗,𝑝𝑗+1]. It follows that, at iteration |𝜏|+1 of 𝙶𝙿(𝜆,𝜏), the increase of the variables is 𝜖𝜎(𝜆), and 𝜎 is tight at the end of the iteration. ◻

The Threshold-Tuple Search Algorithm
We describe the threshold-tuple search algorithm, denoted by 𝚃𝚂. A listing is given in Algorithm 4. To find a threshold-tuple, the algorithm constructs a tie-breaking list 𝜏 iteratively. It maintains the following invariants: (i) 𝜏 is respected by 𝜆 for any 𝜆∈[𝑎,𝑏]; (ii) 𝙶𝚆(𝑎,𝜏) returns a tree spanning less than k vertices, and 𝙶𝚆(𝑏,𝜏) returns a tree spanning at least k vertices.

Initialize the variables by making 𝜏=∅, 𝑎=0 and 𝑏=𝑐𝐸+1, and start the iteration process. At each iteration i, compute the sequence of diverging potentials 𝑝1<⋯<𝑝𝑚 in the range (a, b), and let 𝑝0=𝑎 and 𝑝𝑚+1=𝑏. Since 𝙶𝚆(𝑎,𝜏) returns less than k vertices and 𝙶𝚆(𝑏,𝜏) returns at least k vertices, there must be some index j for which 𝙶𝚆(𝑝𝑗,𝜏) returns less than k vertices, and 𝙶𝚆(𝑝𝑗+1,𝜏) returns at least k vertices.

Discover the edge or subset 𝜎 which is processed at iteration i of 𝙶𝙿(𝜆0,𝜏) for some arbitrary 𝜆0∈(𝑝𝑗,𝑝𝑗+1). Then, extend the tie-breaking list by appending 𝜎 at the end of 𝜏. Note that, by Lemma 12, the extended tie-breaking list 𝜏 is respected by 𝜆 for every 𝜆∈[𝑝𝑗,𝑝𝑗+1], then the first invariant is maintained by making 𝑎=𝑝𝑗 and 𝑏=𝑝𝑗+1.

Next, check if executing 𝙶𝚆(𝑝𝑗,𝜏) using the updated tie-breaking list returns at least k vertices. If this is so, since 𝙶𝚆(𝑝𝑗,𝜏˜) returns less than k vertices, (𝑝𝑗,𝜏) is a threshold-tuple, and the algorithm stops. Analogously, if 𝙶𝚆(𝑝𝑗+1,𝜏) returns less than k vertices, (𝑝𝑗+1,𝜏) is a threshold-tuple. If neither is the case, then the second invariant is maintained, and the process is repeated.

Next, we show that 𝚃𝚂 indeed finds a threshold-tuple.

figure d
Lemma 13
If executing 𝙶𝚆(0,∅) returns a tree spanning less than k vertices, then 𝚃𝚂 returns a threshold-tuple in polynomial time.

Proof
We argue that the algorithm indeed maintains the invariants: (i) 𝜏 is respected by 𝜆 for any 𝜆∈[𝑎,𝑏]; (ii) 𝙶𝚆(𝑎,𝜏) returns a tree spanning less than k vertices, and 𝙶𝚆(𝑏,𝜏) returns a tree spanning at least k vertices. Observe that an empty tie-breaking list is respected by 𝜆 for every potential 𝜆. Also, 𝙶𝚆(0,∅) returns a tree spanning less than k vertices by assumption, and 𝙶𝚆(𝑐𝐸+1,∅) returns a tree spanning at least k vertices by Lemma 6. Thus, at the beginning of the iteration process, both invariants hold. By the description of the algorithm, if the invariants hold at the beginning of an iteration, then they hold at the end of the iteration as well.

Observe that, if the algorithm stops, then it returns a threshold-tuple. We will show that 𝚃𝚂 stops after at most 3|𝑉|−3 iterations. Suppose it does not. Then, at the beginning of iteration 3|𝑉|−2, the tie-breaking list has size |𝜏|=3|𝑉|−3. By Lemma 3, the growth-phase executes at most 3|𝑉|−3 iterations. It follows that the sequence of edges and subsets processed by 𝙶𝙿(𝑎,𝜏) and 𝙶𝙿(𝑏,𝜏) are the same, because we know that 𝜏 is respected by 𝜆∈[𝑎,𝑏] from invariant (i). This implies that 𝙶𝙿(𝑎,𝜏) and 𝙶𝙿(𝑏,𝜏) have the same output, and thus 𝙶𝚆(𝑎,𝜏) and 𝙶𝚆(𝑏,𝜏) return identical trees, which contradicts invariant (ii). ◻

Finding a Solution with a Threshold-Tuple
Assume that we are given a threshold-tuple (𝜆,𝜏). Then, by executing 𝙶𝚆(𝜆,𝜏) and 𝙶𝚆(𝜆,𝜏˜), we obtain two trees, one which spans less than k vertices, and the other, at least k vertices. Let  and  be the trees with less than k vertices, and at least k vertices, respectively, and let  and  denote the corresponding pairs computed in the growth-phase.

Denote by  and  the laminar collections computed in the growth-phase. While these laminar collections might be (slightly) different, Lemma 10 states that 𝙶𝙿(𝜆,𝜏) and 𝙶𝙿(𝜆,𝜏˜) compute identical vectors y. Moreover each edge of  and each subset of  is tight. The objective of this section is to find a tree T from  spanning at least k vertices.

Recall that that  is the collection of subsets which contain some but not all vertices of S, and let 𝑇∗ be an optimal solution. Also, assume that V is the minimal subset in  containing 𝑉(𝑇∗); we will relax this assumption later. To bound the cost of this optimal solution, one can use Lemma 1, which gives the lower bound


To bound the cost of T, we need to prove an inequality analogous to the one given by Goemans and Williamson’s analysis [12], i.e., we want that


Therefore, to obtain a 2-approximation, it is sufficient to find a tree such that |𝑉(𝑇)|≤|𝑉(𝑇∗)|. But, since any feasible solution must span at least k vertices, this means that our goal is to compute a tree T that spans exactly k vertices. Such a tree is constructed by the picking-vertices algorithm. This algorithm follows some of the ideas due to Garg [11].

We now explain our assumption that V is the minimal subset containing 𝑉(𝑇∗). Recall that , thus there are two subsets  such that 𝑉=𝐿1∪𝐿2, because  is binary laminar. Without loss of generality, let 𝐿1 be the one containing r. If V is not the minimal subset in  containing 𝑉(𝑇∗), then either 𝐿1 or 𝐿2 contains 𝑉(𝑇∗). But, because 𝑇∗ contains the root r, 𝐿1 must be the subset containing 𝑉(𝑇∗), as it also contains r. Therefore, either our assumption holds, or we can safely reduce the instance’s size by discarding the vertices in 𝐿2.

Picking k Vertices
Let 𝜎 be the edge or subset processed at iteration |𝜏| of 𝙶𝙿(𝜆,𝜏). Assume for the moment that 𝜎 is a subset. Then, Lemma 10 implies that  and collections  and  differ in exactly one subset. Since  spans fewer vertices than , collection  contains more subsets than collection , by the monotonicity of the pruning operation. Therefore, in this case, .

We would like to obtain a tree from  spanning exactly k vertices. To use Goemans and Williamson’s analysis, this tree needs to be pruned with a collection of tight subsets. If we prune  using , the resulting tree  would have too many vertices, but if we add subset 𝜎 to the pruning collection, then the pruning algorithm would delete a sequence of tight subsets, until finding the tree , as illustrated in Fig. 3. We will soon show that these subsets form a path in .

Fig. 3
figure 3
Sequence of deleted subsets when 𝜎 is a subset

Full size image
Definition 2
Consider a laminar collection of subsets , and let H be a connected graph. A sequence of subsets 𝐷1,𝐷2,…,𝐷ℓ+1 which partition V(H) is called a subset path of H processed with  if 𝐻[𝐷ℓ+1] is pruned with , and, for each 1≤𝑖≤ℓ,

(i)
H has an edge connecting a vertex 𝑣𝑖∈𝐷𝑖 to 𝐷𝑖+1;

(ii)
there is 𝐵𝑖∈ such that 𝐷𝑖⊆𝐵𝑖 and 𝐵𝑖∩𝐷𝑗=∅, for every 𝑗>𝑖;

(iii)
𝐻[𝐷𝑖∪⋯∪𝐷ℓ+1] is connected and pruned with {𝐵∈:𝑣𝑖∉𝐵}.

Figure 4 repeats Figure 3, but representing the corresponding processed subsets and vertices from the definition of a subset path.

Fig. 4
figure 4
A subset path 𝐷1,…,𝐷𝑙+1 when 𝜎 is a subset

Full size image
Next lemma states that, under certain conditions, there exists a subset path, which can be computed in polynomial time.

Lemma 14
If H is -connected and pruned with {𝐵∈:𝑣∉𝐵} for some 𝑣∈𝑉(𝐻), then a subset path of H processed with  can be computed in polynomial time.

Proof
If H is already pruned with , then V(H) is a subset path of H processed with . Thus, we can assume that executing 𝙿𝙿(𝐻,) processes ℓ≥1 iterations. Let 𝐻𝑖 be the graph being pruned at the beginning of the i-th iteration, such that 𝐻1=𝐻 and 𝐻ℓ+1 is the graph returned by the algorithm. Also, let 𝐵𝑖∈ be the subset chosen to be deleted at this iteration. In each iteration of this execution, we choose some subset 𝐵𝑖∈ with |𝛿𝐻𝑖(𝐵𝑖)|=1 which is inclusion-wise minimal. Let 𝐷ℓ+1=𝑉(𝐻ℓ+1) and 𝐷𝑖=𝑉(𝐻𝑖)∩𝐵𝑖 for each 1≤𝑖≤ℓ. Note that each 𝐷𝑖 is connected by invariant (pp2). We will show that the sequence 𝐷1,𝐷2,…,𝐷ℓ+1 is a subset path of H processed with .

For each 1≤𝑖≤ℓ, by the choice of 𝐵𝑖, there is an edge connecting a vertex 𝑣𝑖∈𝐷𝑖 to some vertex in 𝑉(𝐻𝑖+1). We claim that, if |𝛿𝐻𝑖(𝐵)|=1 for some 𝐵∈, then 𝑣𝑖∈𝐵. For 𝑖=1, since 𝐻1 is pruned with {𝐵′∈:𝑣∉𝐵′}, any subset 𝐵′∈ with |𝛿𝐻1(𝐵′)|=1 contains v. Then 𝐵1⊆𝐵, because 𝐵1 is an inclusion-wise minimal subset with |𝛿𝐻1(𝐵1)|=1. Thus, indeed, 𝑣1∈𝐵.

Now suppose that the claim is false, and let 𝑖≥2 be minimum such that |𝛿𝐻𝑖(𝐵)|=1 for some 𝐵∈ with 𝑣𝑖∉𝐵. If 𝐵∩𝐵𝑖≠∅, then either 𝐵⊆𝐵𝑖 or 𝐵𝑖⊆𝐵. But 𝐵𝑖 is an inclusion-wise minimal subset with degree one in 𝐻𝑖, thus 𝐵𝑖⊆𝐵, which contradicts 𝑣𝑖∉𝐵. Thus, we may restrict our attention to the case where B and 𝐵𝑖 are disjoint.

Let 𝐷=𝐵∩𝑉(𝐻𝑖) and 𝑅=𝑉(𝐻𝑖)∖(𝐵∪𝐵𝑖), and observe that the three sets 𝐷𝑖,𝐷,𝑅 partition the vertices of 𝐻𝑖 and 𝑅≠∅. Because both 𝐷𝑖 and D have degree one in 𝐻𝑖, there is no edge between them, as otherwise 𝐻𝑖 would be disconnected because 𝑅≠∅. Thus, there is one edge from 𝐷𝑖 to R, and one edge from D to R.

If the unique edge leaving 𝐷𝑖−1 in 𝐻𝑖−1 connects 𝐷𝑖−1 to R, then 𝐷𝑖 and D would have degree one in 𝐻𝑖−1. Because 𝐵𝑖 and B are disjoint, and  is laminar, 𝑣𝑖−1 can be in only one of them. This implies that the other subset does not have 𝑣𝑖−1 and has degree one in 𝐻𝑖−1. This is a contradiction, since the claim holds for 𝑖−1. Thus, we may restrict our attention to the case where there is no edge between 𝐷𝑖−1 and R.

Then, the unique edge leaving 𝐷𝑖−1 in 𝐻𝑖−1 has an extreme in either 𝐷𝑖 or D. Assume that this edge connects 𝐷𝑖−1 to 𝐷𝑖, as the other case is symmetrical. In this case, there is no edge between D and 𝐷𝑖−1, and thus the degree of B on 𝐻𝑖−1 is one. This implies that 𝑣𝑖−1∈𝐵, and it follows that 𝐵𝑖−1∩𝐵≠∅. Therefore, 𝐵𝑖−1⊆𝐵.

Notice that H is B-connected. Thus, we have two distinct edges between two connected graphs, H[B] and 𝐻[𝐷𝑖∪𝑅], forming a cycle in H, say C. Since C has vertices in both 𝐷𝑖−1 and 𝐻𝑖, at some iteration 𝑗<𝑖, some, but not all vertices of C were removed for the first time. But this implies that |𝛿𝐻𝑗(𝐵𝑗)|≥2, which is a contradiction to the choice of 𝐵𝑗. Hence, the claim holds.

We conclude that, for each {1\le i \le \ell }, H_i is pruned with {\{B \in {\mathcal {B}}: v_i \notin B\}}, and there is an edge from v_i \in D_i to D_{i+1}. Then, D_1, D_2, \dots , D_{\ell +1} is indeed a subset path of H processed with {\mathcal {B}}. This completes the proof. \square

Finding a Subset Path
In the remaining of this section, we consider , i.e., we let H be the graph with vertex set  and edge set , and let  be the graph output by . By the monotonicity of the pruning operation,  contains , then  spans at least k vertices. The goal is to find a subset path of  leading to .

Observe that, when \sigma is a subset, , then  is pruned with . If v is a vertex in \sigma , then  is pruned with . Therefore, Lemma 14 directly implies the following.

Lemma 15
If \sigma is a subset, then a subset path of  processed with  can be computed in polynomial time.

Now, assume that \sigma is an edge. For this case, Lemma 10 implies that . Moreover,  and  span the same set of vertices, and the difference between them is exactly one edge, thus there exists a unique edge  such that .

Unlike before, we cannot obtain a subset path from  that leads to , because removing  from  disconnects the graph. Instead, we use the fact that H has exactly one cycle, because . Let C be this cycle, and notice that C contains . We begin by showing that  also contains cycle C.

Lemma 16
If \sigma is an edge, then  contains cycle C.

Proof
First observe that  contains . If not, then we would have  and, since  is pruned with , this would imply  by Lemma 5. This is not possible because . Thus, indeed  contains .

Because , we have  by Lemma 5. It follows that  contains , and thus  contains at least some vertices of C. Suppose that  contains some, but not all vertices of C. Then, the pruning algorithm removed some vertices of C, and in the first time that this happened, the selected subset would have degree at least 2, because the corresponding cut-set would have at least two edges from C. This is not possible, because only subsets with degree one are selected. Then, no vertex of C was removed, and indeed  contains C. \square

To construct a subset path of  when \sigma is an edge, we show that there is an edge e of the cycle, such that pruning  leads to  by a sequence of deleted subsets that form a path.

Lemma 17
If \sigma is an edge, then there is an edge e \in E(C) such that a subset path D_1, D_2, \dots , D_{\ell +1} of  processed with  can be computed in polynomial time. Moreover,  connects subsets D_i and D_{i+1} for some i\ge 1, and e connects D_1 and D_j for some j \ge i+1.

Proof
Define , which is connected because  contains C, and  is in C. Since  is pruned with , the first processed subset must contain exactly one extreme of . Let v be such an extreme, and v' be the other extreme of .

Now, define , and obtain a graph  by executing \mathtt {PP}(H_1, {\mathcal {B}}_1). In Fig. 5, the grey circles denote the sets deleted when pruning H_1. Because no subset in {\mathcal {B}}_1 contains v', and  is connected,  contains a path K_{v'} starting at the root r and ending at v'. Consider the unique path K_v in H_1 starting at the root r and ending and v, and let u be the last vertex in this path which is in , i.e., u is the last vertex of K_v which was not deleted by the pruning algorithm. Note that u must be a vertex of the cycle, since K_v contains at least one vertex of K_{v'} which is also in C.

Fig. 5
figure 5
Pruning H_1 using {\mathcal {B}}_1

Full size image
Let u' be the vertex that follows u in K_{v}, and observe that C has an edge e connecting u to u'. Define , and . We claim that H_2 is pruned with {\mathcal {B}}_2. Suppose not, and let B be a subset in {\mathcal {B}}_2 with {|\delta _{H_2}(B)| = 1}. Since  is pruned with {\mathcal {B}}, we know that B contains exactly one extreme of e, thus u \in B, and . Since  does not contain u', it does not contain e either, thus . This implies , and thus . Because  is pruned with {\mathcal {B}}_1, this implies that B \notin {\mathcal {B}}_1, then v' \in B. Since H_2 has two edge-disjoint paths, one from v' to the root r, and other from v' to u', it follows that |\delta _{H_2}(B)|\ge 2, which is a contradiction. Therefore, H_2 is pruned with {\mathcal {B}}_2.

We will obtain a subset path of H_2 processed with  in two steps.  First, since H_2 is pruned with , we can use Lemma 14, and obtain a subset path D_1, D_2, \dots , D_{i+1} of H_2 such that u' \in D_1, and H_2[D_{i+1}] is pruned with .  Second, using Lemma 14 again, we obtain a subset path D'_1, D'_2, \dots , D'_{m+1} of H_2[D_{i+1}] such that v' \in D'_1, and H_2[D'_{m+1}] is pruned with . Then D_1, \dots , D_{i}, D'_1, \dots ,D'_m, D'_{m+1} is a subset path of  processed with .

Note that H_2 has exactly one edge between D_i and D_{i+1}, thus this edge must be , and since v' \in D'_1,  must connect D_i to D'_1. Also, observe that e connects D_1 to some D'_j for j \ge 1, forming a cycle with vertices in D_1, \dots , D_i, D'_1, \dots , D'_j. See Fig. 6.

\square

Fig. 6
figure 6
A subset path D_1, \dots , D_{i}, D'_1, \dots ,D'_m, D'_{m+1}

Full size image
Augmenting the Path
Assume that we already computed a subset path D_1, D_2, \dots , D_{\ell +1} of  processed with . Recall that  contains  as a subgraph, and thus spans at least k vertices. Also, observe that D_{\ell +1} corresponds to the vertices of , and thus spans less than k vertices. Then, there exists an index t such that the subsets in D_t, D_{t+1}, \dots , D_{\ell +1} cover at least k vertices, but D_{t+1}, \dots , D_{\ell +1} cover less than k vertices.

If D_{t+1}, \dots , D_{\ell +1} cover exactly {k - m} vertices, then we would like to augment this sequence by iteratively picking subsets from D_t which add up to m vertices. The goal is to find a sequence of subsets P_1, P_2, \dots , P_s such that: each subset P_i induces a connected subgraph in ; P_1 is connected to D_{t+1}; adjacent subsets are connected by an edge of ; and |P_s| = 1.

This can be done as follows. Suppose that we already have computed a sequence P_0, P_1, \dots , P_{i-1} for some i \ge 1, and want to pick m vertices in S \cap D_t for some subset  containing at least m vertices. Also, suppose there is an edge connecting P_{i-1} to some vertex {v \in S \cap D_t}. To initialize the process, let P_0 = D_{t+1} and S = B_t, where B_t is the subset in  corresponding to D_t in the subset path. Note that there is an edge connecting D_{t+1} to some v \in D_t.

If m = 1, then define P_i = \{v\}, and we are done. Otherwise, we have m \ge 2, thus S contains at least two vertices. Since  is binary laminar, this implies that there are disjoint subsets S_1 and S_2 with S = S_1 \cup S_2, and such that v \in S_1.

If |S_1 \cap D_t| \ge m, then just make S = S_1, and repeat the process. This does not change the assumptions, except that it makes S smaller. Otherwise, {|S_1 \cap D_t| < m}, but this implies {|S_2 \cap D_t| \ge 1} because we have {|S \cap D_t| \ge m}. It follows that  spans vertices in both S_1 and S_2, and, since  is S-connected, there must be an edge connecting a vertex v_1 \in S_1 to a vertex v_2 \in S_2. In this case, we define P_i = S_1 \cap D_t, update the variables by making m = m - |P_i|, S = S_2 and v = v_2, and repeat the process for i + 1. Note that P_i induces a connected subgraph in  because  is S_1-connected.

Figure 7 exemplifies this process. In this figure, solid contours denote the subsets  considered in the process.

Fig. 7
figure 7
A sequence \dots , D_{t+1}, P_1, P_2, P_3, P_4

Full size image
The whole process is summarized as the picking-vertices algorithm, denoted by \mathtt {PV}({\lambda },{\tau }). A listing is given in Algorithm 5. First, obtain a subset path D_1, D_2, \dots , D_{\ell +1} using Lemmas 15 or 17, depending on whether \sigma is a subset, or an edge. Then, find the largest t such that D_t, D_{t+1}, \dots , D_{\ell +1} cover at least k vertices, and find a sequence P_1, P_2, \dots , P_s from D_t using the picking process.

Now, the sequence P_s, \dots , P_1, D_{t+1}, \dots , D_{\ell +1} covers exactly k vertices, and is such that each subset induces a connected subgraph in , and adjacent subsets are connected by an edge. Thus, it induces a tree T spanning exactly k vertices and containing the root r, which is the output of the algorithm.

figure e
In what follows, denote by W the subset  corresponding to D_t in the subset path such that D_t \subseteq W and W \cap D_j = \emptyset for every j > t. Recall that {\mathcal {L}}[W] is the collection of subsets in {\mathcal {L}} which are subsets of W. Also, let w be the last vertex selected by the picking process, i.e., P_s = \{w\}. Next lemmas present some properties of vertex w and laminar collections {\mathcal {L}} and {\mathcal {B}}.

Lemma 18
Let  and suppose L contains a vertex in V(T) and a vertex v \in W \setminus V(T). If no subset in  contains v, then {w \in L}.

Proof
Let D_1, \dots , D_{\ell +1} be the computed subset path and recall that T is a subgraph of . Since no subset in  contains v, the pruning algorithm has not deleted v, and thus v \in V(H_t). Because L\subseteq W, we have v \in D_t. This implies that L was considered by the picking process, playing the role of S in some iteration, and because not all vertices in L \cap D_t were selected, all the remaining vertices of T were picked from L. Therefore, w \in L. \square

Next lemma shows that the only processed subsets in  with degree one in T which are not pruned from T are those containing w.

Lemma 19
Let . If |\delta _T(B)| = 1, then w \in B.

Proof
Remember that the algorithm computes a subset path D_1, \dots , D_{\ell +1} of a tree  processed with , where  if \sigma is a subset, and  for some edge e if \sigma is an edge. Define . Let K be the path in T starting at the root r and ending at w, and observe that there is a vertex v_t \in V(K) such that H_t is pruned with .

Let  with |\delta _T(B)| = 1, and suppose for a contradiction that w \notin B. If |\delta _T(B)| = |\delta _{H_t}(B)|, then v_t \in B, and there are two edge-disjoint paths crossing B, one from w to v_t and other from v_t to the root r. This is not possible because |\delta _T(B)| = 1. Therefore, |\delta _T(B)| \ne |\delta _{H_t}(B)|, and thus |\delta _{H_t}(B)|\ge 2.

Observe that K contains all edges connecting adjacent subsets D_i and D_{i+1}, for i \ge 1. It follows that B \subseteq D_j for some j \ge t, because B does not contain a vertex of K. If j \ge t + 1, then we would have {|\delta _{H_t}(B)| = |\delta _T(B)|}, a contradiction. Thus, assume that j = t and B \subseteq D_t.

Because T spans vertices in B and B \subseteq D_t, these vertices were selected by the picking process. Let m be the number of vertices still needed at the iteration which considered subset B. If m \le |B\cap D_{t}|, then all the m remaining vertices of T were picked from B\cap D_{t}, and thus w \in B. Otherwise, all the vertices of B \cap D_{t} were picked, and T has two edges leaving B, which is a contradiction since |\delta _T(B)| = 1.

\square

Next lemma extracts the critical property of the tree output by the algorithm, which is used to bound the solution cost. For some collection of subsets {\mathcal {L}}, denote by {\mathcal {L}}_w the collection of subsets in {\mathcal {L}} which contain vertex w. Thus, {\mathcal {L}}_w[W] contains all subsets in {\mathcal {L}} which are subsets of W and contain w.

Lemma 20
Consider the execution of \mathtt {GP} which returned , and let {\mathcal {A}} be the collection of active subsets that contain some vertex of T at the beginning of an iteration. Then,

\begin{aligned} \sum _{A \in {\mathcal {A}}}|\delta _T(A)| \le 2 (|{\mathcal {A}}| - |{\mathcal {A}}_w[W]|). \end{aligned}
Proof
Let D_1, D_2, \dots , D_{\ell +1} be the subset path computed by the algorithm, and define  for each 1 \le i \le \ell . Observe that T is a subgraph of H_t, and remember that W is the subset in  associated with D_t, such that H_{t+1} = H_t - W and |\delta _{H_t}(W)| = 1. Also, let P_1, P_2, \dots , P_s be the sequence of subsets picked from , such that P_s = \{w\}. By Lemma 19, any subset in  with degree one in T contains w.

Consider the laminar collection  at the beginning of an iteration of \mathtt {GP}, and let {\mathcal {A}} be the collection of maximal subsets in  which are active and contain some vertex of T, and {\mathcal {I}} be the collection of maximal subsets in  which are not active and contain some vertex of T. Note that {\mathcal {A}}\cup {\mathcal {I}} partitions V(T). If we contract on T each subset in {\mathcal {A}}\cup {\mathcal {I}}, obtaining a graph T', then each vertex of T' corresponding to a subset S will have degree |\delta _T(S)|.

Observe that an active subset is a maximal subset of the laminar collection at the beginning of the iteration, thus the subsets in {\mathcal {A}} are disjoint, and then {|{\mathcal {A}}_w[W]| \le 1}. We are also interested in counting the vertices of T' with degree one which correspond to non-active subsets, thus let {\mathcal {I}}_1 be the collection of subsets {S\in {\mathcal {I}}} with {|\delta _T(S)| = 1}. Because each subset in {\mathcal {I}}_1 is in  and has degree one in T, we know that w \in S for every S \in {\mathcal {I}}_1, by Lemma 19.

We break the proof into two cases, depending on whether T' is a tree.

Case 1: The contracted graph T' is a tree.

We claim that |{\mathcal {I}}_1| + |{\mathcal {A}}_w[W]| \le 1. This means that there is at most one non-active subset with degree one, and, if there is one, then there are no active subsets of W containing w. Indeed, if {\mathcal {I}}_1 contains some subset S, then w \in S, and thus no active subset contains w. Otherwise, |{\mathcal {I}}_1|=0, and the claim holds because |{\mathcal {A}}_w[W]| \le 1.

Since T' is a tree, it follows that

\begin{aligned} \sum _{A \in {\mathcal {A}}}|\delta _T(A)| + 2|{\mathcal {I}}| - |{\mathcal {I}}_1|&\le \sum _{A \in {\mathcal {A}}}|\delta _T(A)| + \sum _{I \in {\mathcal {I}}}|\delta _T(I)| = 2(|{\mathcal {A}}| + |{\mathcal {I}}| - 1) \\&\le 2(|{\mathcal {A}}| + |{\mathcal {I}}| - |{\mathcal {I}}_1| -|{\mathcal {A}}_w[W]|), \end{aligned}
which implies \sum _{A \in {\mathcal {A}}}|\delta _T(A)| \le 2(|{\mathcal {A}}| -|{\mathcal {A}}_w[W]|), thus the lemma holds in this case.

Case 2: The contracted graph T' has a cycle.

Because T' has a cycle, there exists some maximal subset L in {\mathcal {A}}\cup {\mathcal {I}} which induces a disconnected subgraph T[L]. Since  is L-connected by invariant (gp3), we know that  is connected, thus H[V(T)\cup L] has a cycle C whose vertices intersect L, and then  contains C as well. Observe that this case can occur only when \sigma is an edge, and  is in C. Because T contains at most one edge that is not in , the contracted tree T' contains at most one cycle. Also, note that T[L] has only two components, since otherwise H would have two cycles.

Recall that C has the edge . Suppose, for a contradiction, that both extremes of  are contained in the same maximal subset S \in {\mathcal {A}}\cup {\mathcal {I}}. Then, because  is S-connected, H[S] would contain a cycle which is not C. Since there is only one cycle, this is not possible, and we conclude that  connects consecutive maximal subsets in the cycle. Therefore,  is an edge of T', and at least one extreme of  is not in L.

We claim that W contains some but not all vertices of C. Observe that D_t spans vertices of C, since otherwise C would be contained in D_1, D_2, \dots , D_{t-1}, by Lemma 17, and thus T would not span vertices of C. Since D_t is a subset of W, it follows that W spans vertices of C. Now, also by Lemma 17,  connects consecutive subsets D_i to D_{i+1} for some i \ge 1. Then T contains vertices of both D_i and D_{i+1}, and it follows that t \le i, since D_t is the first subset of the subset path containing vertices of T. Therefore, W cannot contain all vertices of C, because D_{i+1} \subseteq V(H_{i+1}) and H_{i+1} does not contain vertices of W.

It follows that the cut \delta _{C}(W) contains two edges of C. Since {|\delta _T(W)| = 1}, both edges cannot be edges of T, thus W contains some vertex v_1 in L. Let L_1 and L_2 be the two connected components of T[L]. Suppose without loss of generality that v_1 \in L_1 and let v_2 \in L_2. Because there is no path from v_1 to v_2 in , it follows that v_2 \in D_j for some j > t. Thus, v_2 \notin B_t = W. Now v_1 \in L \cap W, and v_2 \in L \setminus W. Since  is laminar, W \subseteq L.

Figure 8 illustrates T with maximal subsets in dashed lines whose contraction resulted in a graph T' with a cycle.

Fig. 8
figure 8
A graph T' with a cycle

Full size image
This implies |{\mathcal {A}}_w[W]| = 0, since {\mathcal {A}}_w[W] contains only subsets of W. Also, |{\mathcal {I}}_1| = 0, because the maximal subset containing w is L, which has degree at least 2. Using the fact that in this case T' has exactly one cycle, we get

\begin{aligned} \sum _{A \in {\mathcal {A}}}|\delta _T(A)| + 2|{\mathcal {I}}|&\le \sum _{A \in {\mathcal {A}}}|\delta _T(A)| + \sum _{I \in {\mathcal {I}}}|\delta _T(I)| = 2(|{\mathcal {A}}| + |{\mathcal {I}}|) \\&= 2(|{\mathcal {A}}| + |{\mathcal {I}}| -|{\mathcal {A}}_w[W]|), \end{aligned}
which implies \sum _{A \in {\mathcal {A}}}|\delta _T(A)| \le 2(|{\mathcal {A}}| -|{\mathcal {A}}_w[W]|), completing the lemma. \square

The 2-Approximation
This section wraps up the whole algorithm. First, we bound the cost of the tree output by \mathtt {PV}. Then, we show how to use this bound to find a tree whose cost is within factor 2 of an optimal solution for k-PCST.

Bounding the Cost of the Tree
We adopt the same definitions introduced in Sect. 5, except that, to simplify the notation, for the remainder of this section, we let . Recall that {\mathcal {L}}(S) denotes the collection of subsets in {\mathcal {L}} that contains some, but not all vertices of S, that {\mathcal {L}}[S] denotes the collection of subsets in {\mathcal {L}} which are subsets of S, and that {\mathcal {L}}_w is the collection of subsets in {\mathcal {L}} that contain w. Next two lemmas bound the cost of edges and vertex penalties of the tree output by \mathtt {PV}.

Lemma 21
Let T be the tree returned by \mathtt {PV}({\lambda },{\tau }), then


Proof
By Lemma 10, we have that the vectors y computed in the growth-phases \mathtt {GP}({\lambda },{\tau }) and \mathtt {GP}({\lambda },{\widetilde{\tau }}) are the same. Since , each edge of T is tight, thus


We prove by induction that, at the beginning of each iteration of \mathtt {GP}({\lambda },{\tau }),


At the beginning of the growth-phase, y = 0, then the inequality holds. Thus, assume that the inequality holds at the beginning of an iteration, and let {\mathcal {A}} be the collection of active subsets that contain some vertex of T.

Notice that, if some maximal subset S contains V(T), then S \notin {\mathcal {L}}(V(T)). Also, any such subset contains both r and w, thus S \notin {\mathcal {L}}_w[W], because {\mathcal {L}} is laminar. Therefore, if V(T) is contained in some maximal subset, then neither the left nor the right side of the inequality changes. Hence, we can assume that no active subset contains V(T). It follows that {{\mathcal {A}}\subseteq {\mathcal {L}}(V(T))} and that {{\mathcal {A}}_w[W] \subseteq {\mathcal {L}}_w[W]}. Suppose that, in this iteration, the variable y_L of each active subset L is increased by \varDelta . Then, the left side of the inequality is increased by {\sum _{A \in {\mathcal {A}}}|\delta _T(A)|\varDelta }, while the right side is increased by {2 (|{\mathcal {A}}| - |{\mathcal {A}}_w[W]|) \varDelta }. By Lemma 20, the increase on the left side is smaller, thus the inequality is maintained at the end of the iteration. \square

Lemma 22
Let T be the tree returned by \mathtt {PV}({\lambda },{\tau }), then


Proof
We partition W into three parts, P, I, and Q, corresponding to the set of vertices whose penalties can be paid by some processed component which is a proper subset of W \setminus V(T), the intersection with T, and the vertices which are not paid. More precisely, define


First, we consider the vertices of W which are not paid. Since W is tight for (y,{\lambda }) and y respects \pi ^{\lambda }, it follows that

\begin{aligned} \pi ^{\lambda }_{Q} = \pi ^{\lambda }_W - \pi ^{\lambda }_{P \cup I}&\le \sum _{L \in {\mathcal {L}}[W]} y_L - \sum _{L \in {\mathcal {L}}[P\cup I]} y_L. \end{aligned}
(3)
Denote by {\mathcal {P}} the collection of subsets in {\mathcal {L}}[P\cup I] that contain a vertex in P and a vertex in I. Then {\{{\mathcal {L}}[P], {\mathcal {L}}[I], {\mathcal {P}}\}} partitions the collection {{\mathcal {L}}[P\cup I]}. Similarly, denote by {\mathcal {W}} the collection of subsets in {\mathcal {L}}[W] that contain a vertex in {P \cup Q} and a vertex in I. Then {\{{\mathcal {L}}[P \cup Q], {\mathcal {L}}[I],{\mathcal {W}}\}} partitions the collection {\mathcal {L}}[W].

We claim that {\mathcal {W}}\setminus {\mathcal {P}}\subseteq {\mathcal {L}}_w[W]. To see this, consider a subset {L \in {\mathcal {W}}\setminus {\mathcal {P}}}. Since L contains a vertex in I, but is not in {\mathcal {P}}, it does not contain a vertex in P. It follows that L contains a vertex of v \in Q. Because v \in W \setminus V(T) and no subset in  contains v, Lemma 18 implies that {w \in L}. Therefore, L \in {\mathcal {L}}_w[W], and then {{\mathcal {W}}\setminus {\mathcal {P}}\subseteq {\mathcal {L}}_w[W]}.

We can simplify the indices in the summation (3) as

\begin{aligned} {\mathcal {L}}[W] \setminus {\mathcal {L}}[P \cup I]&= ({\mathcal {L}}[P \cup Q] \cup {\mathcal {L}}[I] \cup {\mathcal {W}}) \setminus ({\mathcal {L}}[P] \cup {\mathcal {L}}[I] \cup {\mathcal {P}})\\&\subseteq ({\mathcal {L}}[P \cup Q] \cup {\mathcal {L}}_w[W]) \setminus {\mathcal {L}}[P]. \end{aligned}
Now, we consider the vertices which are paid, i.e., vertices not spanned by T which are contained in some subset . In addition to vertices P, the vertices in {R = V \setminus (V(T) \cup W)} are also paid. Note that any vertex in R was deleted by the pruning algorithm, thus every vertex in R is indeed contained in some subset in  which contains no vertex in V(T) \cup W. Hence, there is a collection \{B_1,B_2,\dots ,B_m\} of processed subsets in  that partitions the vertices in R. By similar arguments, there is also a collection \{B_{m+1},B_{m+2},\dots ,B_\ell \} of subsets in  that partitions the vertices in P. Thus, since each such subset is tight for (y,{\lambda }),

\begin{aligned} \pi ^{\lambda }_{R \cup P}&= \sum _{i=1}^\ell \pi ^{\lambda }_{B_i} = \sum _{i=1}^\ell \sum _{L \in {\mathcal {L}}[B_i]} y_L =\sum _{L \in {\mathcal {L}}[R]\cup {\mathcal {L}}[P]} y_L. \end{aligned}
(4)
Since the collections of summations (3) and (4) are disjoint,

\begin{aligned} ({\mathcal {L}}[W] \setminus {\mathcal {L}}[P \cup I]) \cup ({\mathcal {L}}[R]\cup {\mathcal {L}}[P])&\subseteq {\mathcal {L}}[P \cup Q] \cup {\mathcal {L}}_w[W] \cup {\mathcal {L}}[R]\\&\subseteq {\mathcal {L}}[V \setminus V(T)] \cup {\mathcal {L}}_w[W]. \end{aligned}
By combining (3) and (4), we conclude

\begin{aligned} \pi ^{\lambda }_{V \setminus V(T)}&=\pi ^{\lambda }_{Q} + \pi ^{\lambda }_{R \cup P} \le \sum _{L \in {\mathcal {L}}[V \setminus V(T)]} y_L + \sum _{L \in {\mathcal {L}}_w[W]} y_L. \end{aligned}
\square

Combining the bounds from the previous two lemmas, one obtains the following corollary.

Corollary 6
Let T be the tree returned by \mathtt {PV}({\lambda },{\tau }), then


The Approximation Algorithm
Next lemma is the final ingredient of our 2-approximation.

Lemma 23
Let T be the tree returned by \mathtt {PV}({\lambda },{\tau }), and T^* be an optimal solution. Also, suppose that L^* is the minimal subset in {\mathcal {L}} containing V(T^*). If L^* = V, then

\begin{aligned} c_{E(T)} + 2 \pi _{V \setminus V(T)} \le 2 \left( c_{E(T^*)} + \pi _{V \setminus V(T^*)} \right) . \end{aligned}
Proof
Assume V = L^*. Since T spans exactly k vertices and T^* spans at least k vertices, we have {|V \setminus V(T)| \ge |V \setminus V(T^*)|}. Therefore,

\begin{aligned} c_{E(T)} + 2 \pi _{V \setminus V(T)}&= c_{E(T)} + 2 \pi ^{\lambda }_{V \setminus V(T)} - 2 {\lambda }|V \setminus V(T)|\\&\le c_{E(T)} + 2 \pi ^{\lambda }_{V \setminus V(T)} - 2 {\lambda }|V \setminus V(T^*)|\\&\le \textstyle 2 \sum _{L \in {\mathcal {L}}(V)} y_L - 2 {\lambda }|V \setminus V(T^*)|\\&\le 2 \left( c_{E(T^*)} + \pi _{V \setminus V(T^*)} \right) , \end{aligned}
where the penultimate inequality follows from Corollary 6, and the last inequality follows from Lemma 1, because y respects c and \pi ^{\lambda }. \square

We finally present our 2-approximation, which is denoted by \mathtt {2\text{- }APPROX}. A listing is given in Algorithm 6.

The algorithm will compute a series of trees, the best of which will be the output. In each iteration, start by computing a tree executing \mathtt {GW}(0,\emptyset ), and, if it already spans at least k vertices, then store this tree and stop the iteration process. Otherwise, there is a threshold-tuple ({\lambda },{\tau }), by Lemma 13, which can be computed by \mathtt {TS}. Now, store the tree returned by \mathtt {PV}({\lambda },{\tau }) and let {\mathcal {L}} be the laminar collection used by \mathtt {PV}({\lambda },{\tau }) when computing this tree. Find the subset L_r \in {\mathcal {L}} which is the inclusion-wise maximal proper subset of V containing the root r. If |L_r| < k, then stop; otherwise, remove from G any vertex not in L_r, i.e., repeat the iteration process with G[L_r].

Observe that at least one vertex is deleted in each iteration, and the reduced graph is connected because it is {\mathcal {L}}-connected. Thus, the algorithm stops, and the output is the computed tree T which minimizes the cost with respect to the original graph G. We argue why T is a 2-approximation. Let T^* be an optimal solution with respect to G and consider the last iteration which processed some subgraph G' containing T^*. If the algorithm stopped in this iteration after computing a tree using \mathtt {GW}(0,\emptyset ), then Lemma 7 implies that this tree is a 2-approximation with respect to G'. Otherwise, the inclusion-wise minimal subset containing T^* is V(G'), and Lemma 23 implies that the tree computed by \mathtt {PV}({\lambda },{\tau }) is a 2-approximation with respect to G'. Now, note that a 2-approximation with respect to G' is also a 2-approximation with respect to G.

Theorem 1
If T is the tree returned by \mathtt {2\text{- }APPROX}, and T^* is an optimal solution, then

\begin{aligned} c_{E(T)} + 2 \pi _{V \setminus V(T)} \le 2 \left( c_{E({T^*})} + \pi _{V \setminus V(T^*)} \right) . \end{aligned}
figure f
Final Remarks
In this paper, we presented a 2-approximation for the k-PRIZE-COLLECTING STEINER TREE PROBLEM. This improves over the previous best known approximation factor [15], and has a smaller running time. Observe that \mathtt {2\text{- }APPROX} iterates for {\mathcal {O}}(|V|) times, and the time complexity of each iteration is dominated by the subroutine \mathtt {TS}, whose running time is {\mathcal {O}}(|V||E|^2 + |V|^3 \log ^2|V|). Thus, our algorithm runs in time {{\mathcal {O}}(|V|^2|E|^2 + |V|^4 \log ^2|V|)}.

Variants of the PRIZE-COLLECTING STEINER TREE PROBLEM are among the most classical network design problems, and have long been studied from both practical and theoretical perspectives. In this paper we considered only the version of PCST with a cardinality constraint, but similar techniques can be applied to other connectivity problems, particularly those for which the primal-dual framework has been used [12]. Although the algorithm is based on the primal-dual framework, it does not use an \mathrm {LP} formulation. We note that the standard integer linear program for k-PCST [13] has integrality gap of at least 4.

Johnson et al. [14] also considered the quota version of k-MST, in which each vertex v has an associated non-negative integer weight w_v and a solution is a minimum-cost tree with any number of vertices, such that the total weight of the vertices in the tree is at least some given quota. Note that k-MST is the special case in which the quota is k and every vertex has weight one. A small modification of our algorithm also leads to a 2-approximation for the quota variant of k-PCST. To do this, build an equivalent instance of k-PCST by replacing each vertex v with w_v copies at the same location, and observe that, although the size of this instance is not necessarily polynomial, the growth-phase and picking-vertices can be simulated in polynomial time.

References
Archer, A., Bateni, M., Hajiaghayi, M., Karloff, H.: Improved approximation algorithms for prize-collecting steiner tree and tsp. SIAM J. Comput. 40(2), 309–332 (2011). https://doi-org.ezproxy.auckland.ac.nz/10.1137/090771429

MathSciNet
 
Article
 
MATH
 
Google Scholar
 

Arora, S., Karakostas, G.: A 2 + \varepsilon approximation algorithm for the k-mst problem. Math. Program. 107(3), 491–504 (2006). https://doi-org.ezproxy.auckland.ac.nz/10.1007/s10107-005-0693-1

MathSciNet
 
Article
 
MATH
 
Google Scholar
 

Arya, S., Ramesh, H.: A 2.5-factor approximation algorithm for the k-mst problem. Inf. Process. Lett. 65(3), 117–118 (1998). https://doi-org.ezproxy.auckland.ac.nz/10.1016/S0020-0190(98)00010-6

Awerbuch, B., Azar, Y., Blum, A., Vempala, S.: New approximation guarantees for minimum-weight k-trees and prize-collecting salesmen. SIAM J. Comput. 28(1), 254–262 (1998)

MathSciNet
 
Article
 
Google Scholar
 

Bienstock, D., Goemans, M.X., Simchi-Levi, D., Williamson, D.: A note on the prize collecting traveling salesman problem. Math. Program. 59(1), 413–420 (1993). https://doi-org.ezproxy.auckland.ac.nz/10.1007/BF01581256

MathSciNet
 
Article
 
MATH
 
Google Scholar
 

Blum, A., Ravi, R., Vempala, S.: A constant-factor approximation algorithm for the k-mst problem. J. Comput. Syst. Sci. 58(1), 101–108 (1999). https://doi-org.ezproxy.auckland.ac.nz/10.1006/jcss.1997.1542

MathSciNet
 
Article
 
MATH
 
Google Scholar
 

Byrka, J., Grandoni, F., Rothvoss, T., Sanità, L.: Steiner tree approximation via iterative randomized rounding. J. ACM 60(1), 6:1–6:33 (2013). https://doi-org.ezproxy.auckland.ac.nz/10.1145/2432622.2432628.

Chudak, F.A., Roughgarden, T., Williamson, D.P.: Approximate k-msts and k-Steiner trees via the primal-dual method and Lagrangean relaxation. Math. Program. 100(2), 411–421 (2004). https://doi-org.ezproxy.auckland.ac.nz/10.1007/s10107-003-0479-2

MathSciNet
 
Article
 
MATH
 
Google Scholar
 

Feofiloff, P., Fernandes, C.G., Ferreira, C.E., de Pina, J.C.: A note on Johnson, Minkoff and Phillips’ algorithm for the prize-collecting steiner tree problem. CoRR abs/1004.1437 (2010). arXiv:1004.1437

Garg, N.: A 3-approximation for the minimum tree spanning k vertices. In: Proceedings of the 37th Annual Symposium on Foundations of Computer Science, FOCS ’96, p. 302. IEEE Computer Society, Washington, DC, USA (1996). http://dl.acm.org.ezproxy.auckland.ac.nz/citation.cfm?id=874062.875522

Garg, N.: Saving an epsilon: A 2-approximation for the k-mst problem in graphs. In: Proceedings of the Thirty-seventh Annual ACM Symposium on Theory of Computing, STOC ’05, pp. 396–402. ACM, New York, NY, USA (2005). https://doi-org.ezproxy.auckland.ac.nz/10.1145/1060590.1060650. http://doi.acm.org/10.1145/1060590.1060650

Goemans, M., Williamson, D.: A general approximation technique for constrained forest problems. SIAM J. Comput. 24(2), 296–317 (1995). https://doi-org.ezproxy.auckland.ac.nz/10.1137/S0097539793242618

MathSciNet
 
Article
 
MATH
 
Google Scholar
 

Han, L., Xu, D., Du, D., Wu, C.: A 5-approximation algorithm for the k-prize-collecting Steiner tree problem. Optim. Lett. (2017). https://doi-org.ezproxy.auckland.ac.nz/10.1007/s11590-017-1135-8

Johnson, D.S., Minkoff, M., Phillips, S.: The prize collecting Steiner tree problem: theory and practice. In: Proceedings of the Eleventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’00, pp. 760–769. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA (2000). http://dl.acm.org.ezproxy.auckland.ac.nz/citation.cfm?id=338219.338637

Matsuda, Y., Takahashi, S.: A 4-approximation algorithm for k-prize collecting Steiner tree problems. Optim. Lett. 13(2), 341–348 (2019). https://doi-org.ezproxy.auckland.ac.nz/10.1007/s11590-018-1367-2

MathSciNet
 
Article
 
MATH
 
Google Scholar
 

Oshiro, M.T.I.: k-Árvores de custo mínimo. Master’s thesis, Universidade de São Paulo, Instituto de Matemática e Estatística (2010)

Ravi, R., Sundaram, R., Marathe, M.V., Rosenkrantz, D.J., Ravi, S.S.: Spanning trees–short or small. SIAM J. Discret. Math. 9(2), 178–200 (1996)

MathSciNet
 
Article
 
Google Scholar
 

Williamson, D.P., Shmoys, D.B.: The Design of Approximation Algorithms, 1st edn. Cambridge University Press, New York (2011)

Book
 
Google Scholar
 

Download references

Author information
Affiliations
Institute of Computing, University of Campinas, Campinas, Brazil

Lehilton Lelis Chaves Pedrosa & Hugo Kooki Kasuya Rosado

Corresponding author
Correspondence to Hugo Kooki Kasuya Rosado.

Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

Supported by São Paulo Research Foundation (FAPESP) grant #2015/11937-9 and National Council for Scientific and Technological Development (CNPq) grants #425340/2016-3, #422829/2018-8, #140552/2019-7, and #312186/2020-7.

Proof of Lemma 7
Proof of Lemma 7
In this appendix, we prove Lemma 7. We show that, if \mathtt {GW}(0,\emptyset ) returns a tree T with at least k vertices, then T is a 2-approximation. The proof is adapted from [9].

Let (T,{\mathcal {B}}) be the tuple returned by \mathtt {GP}(0,\emptyset ), and remember that the growth-phase computes a corresponding laminar collection {\mathcal {L}} associated with vector y. Also, let  be the output of \mathtt {GW}(0,\emptyset ), and observe that  is the tree returned by \mathtt {PP}(T,{\mathcal {B}}).

Recall that {\mathcal {L}}(S) denotes the collection of subsets in {\mathcal {L}} which contain some but not all vertices of a subset S, and that {{\mathcal {L}}_v} denote the collection of subsets in {\mathcal {L}} which contain a vertex v. Thus, {\mathcal {L}}_r(S) is the collection of subsets in {\mathcal {L}}(S) which contain the root r.

We bound the cost of  into two steps. Lemma 1 bounds the cost of edges of , and Lemma 2 bounds the penalty of vertices not in .

Lemma 1
Let  be the tree returned by \mathtt {GW}(0,\emptyset ), then


Proof
Since every edge in  is tight for (y, 0), we have


We prove by induction that, at the beginning of each iteration of \mathtt {GP}(0,\emptyset ),


At the beginning of the growth-phase, y = 0, then the inequality holds. Thus, assume that the inequality holds at the beginning of an iteration. Consider the laminar collection {\mathcal {L}} at the beginning of this iteration, let {\mathcal {A}} be the collection of active maximal subsets in {\mathcal {L}}^* which contain some vertex of , and let {\mathcal {I}} be the collection of non-active maximal subsets in {\mathcal {L}}^* which contain some vertex of .

If some maximal subset in {\mathcal {L}}^* contains , then neither side of the inequality changes, thus assume that no maximal subset contains . Suppose that the variable y_L of each active subset L is increased by \varDelta in this iteration. Hence, the left side of the inequality increases by , and the right side increases by 2 |{\mathcal {A}}\setminus {\mathcal {A}}_r|\varDelta . We claim that


and thus the inequality is maintained at the end of the iteration.

Note that {\mathcal {A}}\cup {\mathcal {I}} partition . Thus, we can create a graph T' from  by contracting each subset in {\mathcal {A}}\cup {\mathcal {I}}. As  is a tree and a subgraph of T, it follows that the graph T' is a tree because T is {\mathcal {L}}-connected by invariant (gp3). Let S be a non-active subset in {\mathcal {I}}, and observe that S is in {\mathcal {B}}. Since  is pruned with {\mathcal {B}}, Corollary 2 implies that the degree of S on  is not one. Therefore, the degree of each vertex of T' corresponding a subset in {\mathcal {I}} is at least 2. It follows that


Since the subset containing the root r is always active, exactly one subset in {\mathcal {A}} contains r, hence {|{\mathcal {A}}\setminus {\mathcal {A}}_r| = |{\mathcal {A}}|-1}, and thus , which shows the claim. \square

Lemma 2
Let  be the tree returned by \mathtt {GW}(0,\emptyset ), then


Proof
By Corollary 1, every vertex not spanned by T is contained in a subset in {\mathcal {B}}, and since \mathtt {PP}(T,{\mathcal {B}}) only deletes from T subsets in {\mathcal {B}}, we have that every vertex not spanned by  is contained in a subset in {\mathcal {B}}. Since {\mathcal {B}} is laminar, there is a collection \{B_1,\cdots ,B_m\} of subsets in {\mathcal {B}} which partitions the vertices in . Because every subset in {\mathcal {B}} is tight for (y, 0),


\square

Now, we can prove Lemma 7.

Lemma 7 Let  be the tree returned by \mathtt {GW}(0,\emptyset ), and T^* be an optimal solution. Then,


Proof
Let L^* be the minimal subset in {\mathcal {L}} containing all the vertices of T^*. Also, let {\mathcal {P}} be the collection of subsets in {\mathcal {L}} which contain all the vertices of L^*. Observe that \{{\mathcal {L}}(L^*),{\mathcal {L}}[V \setminus L^*],{\mathcal {P}}\} partitions {\mathcal {L}}. Also, since L^* contains r, it follows that {{\mathcal {P}}\subseteq {\mathcal {L}}_r}.

Combining Lemmas 1 and 2 we have


Observe that the subsets considered in the terms are disjoint. Thus, we can simplify the indices of the summation as


Now, since y respects c and \pi ^0, using Lemma 1 for {\lambda }= 0,


\square

