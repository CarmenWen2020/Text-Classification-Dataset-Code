Recently, practitioners and researchers met to discuss the role of requirements, and AI and SE. We offer here notes on that fascinating discussion. Also, have you considered writing for this column? This “SE for AI” column publishes commentaries on the growing field of SE for AI. Submissions are welcomed and encouraged (1,000–2,400 words, each figure and table counts as 250 words, try to use fewer than 12 references, and keep the discussion practitioner focused). Please submit your ideas to me at timm@ieee.org.—Tim Menzies
From the Editor
Recently, practitioners and researchers met to discuss the role of requirements, and AI and SE. We offer here notes on that fascinating discussion. Also, have you considered writing for this column? This “SE for AI” column publishes commentaries on the growing field of SE for AI. Submissions are welcomed and encouraged (1,000–2,400 words, each figure and table counts as 250 words, try to use fewer than 12 references, and keep the discussion practitioner focused). Please submit your ideas to me at timm@ieee.org.—Tim Menzies

Much has been written about the algorithmic role that artificial intelligence (AI) plays for automation in software engineering (SE). But what about the role of AI, augmented by human knowledge? Can we make a profound advance by combining human intelligence and AI? Researchers in requirements engineering think so, arguing that

requirement engineering is the secret weapon for better AI and better software.

This article is based on the panel “Artificial Intelligence and Requirement Engineering: Challenges and Opportunities,” which took place at the Eighth International Workshop on Artificial Intelligence and Requirements Engineering.

To begin, we first need a definition. What is requirements engineering (RE)? RE used to be viewed as an early lifecycle activity that proceeded analysis, design, coding, and testing. For safety critical applications, there is certainly a pressing need to create those requirements before the coding starts (we will return to this point, later in the article). However, in this age of DevOps and autonomous and self-adaptive systems, requirements can happen at many other times in a software project.1,2 We say that

requirements engineering is any discussion about what to build and how to trade-off competing cost/benefits. It can happen before, during, or after runtime.

As shown in Tables 1 and 2, there are many ways AI can help RE, across a broad range of SE activities. But, what about the other way around? If we add more requirements into AI, and use RE methods to get truly desired requirements, can we make better software by combining human and artificial intelligence?

Table 1. Some AI tools useful for RE.
Table 1.- Some AI tools useful for RE.
Table 2. Some examples of AI for RE across the lifecycle.
Table 2.- Some examples of AI for RE across the lifecycle.
In our view, when integrating AI into software engineering is a codesign problem between humans, the AI model, the data required to train and validate the desired behavior, and the hardware running the AI model, in addition to the classical software components. This means that when integrating AI, you need to know and understand the context of the system in which you want to apply your AI model to derive the necessary model requirements.3

For example, in the arena of safety critical systems, model construction must be guided by safety requirements. One challenge for AI in RE is safety standards that are based on the EN-IEC 61508 standard (Functional Safety of Electrical/Electronic/Programmable Electronic Safety-Related Systems; for example, ISO 26262 for the automotive sector or IEC 61511 for the process industry). These safety standards assume that for software only systematic faults exists. Therefore, they emphasize correct processes and the creation of lifecycle artifacts to minimize systematic mistakes during both the concept and the design phase of the system. Applying AI for RE in the concept phase would require a safety qualified AI performing the RE tasks. Such “tool qualifications,” for example for ISO 26262, would require that the AI tool is developed according to a suitable standard, or that evidence is proved “that the assessed tool errors either do not occur or will be detected.”4 That is, for such systems, humans are still needed to ensure we produce ethical AI.5

This is boon and bane at the same time. On the one hand, it might be difficult and not feasible to consider every contextual attribute and their possible changes at design time for the entire operational environment of your system; see, e.g., Ramirez et al.1 On the other hand, a clear understanding of context and constraints offers opportunities to simplify the learning problem, such that the data required to train the desired behavior can be found, or created, more efficiently. For example, assume that you want a simple classifier for handwritten numbers from 0 to 9. Traditionally, you would have to collect thousands of examples for each number to train a (deep learning) classifier, which will use properties such as curvature, shape, and so on to discriminate between the digits. Now assume you have the context that each number is written in a different, digit-specific color. In this context, the classifier only needs to learn the correlation between color and digit, which will require significant fewer data samples (specifically 10).

Safety critical systems must be understandable and auditable by humans (this is especially true if those systems ever fail and we must fix their problem). The more an AI tool understands human requirements, and human reasoning, the more they can produce simpler and more explicable models.

One concern with this “keep it simple” approach is that some fear that such simpler models will perform worse than more complex options. In some domains, this is certainly true; e.g., when processing 10,000 wavelets from a signal processing model that, in turn, is controlling an autonomous car. But in many domains, there is much evidence that simpler need not be more stupid; e.g., see the examples listed by Rudin20 or recent results in software defect prediction21. In addition, high safety integrity demands systems that are as simple as possible; trying to achieve a high safety integrity level on a complex system will most likely cost a fortune or is simply impossible.

The Road Ahead
Table 3 lists some of the technologies discussed earlier and their challenges. For example, until just recently, formal methods needed humans to manually craft the axioms needed for their operation. But now we can apply formal methods to requirements engineering by automatically extracting those axioms from configuration files.15 As to text mining (and, indeed, any AI inference tool), these came with so many configuration options that industrial practitioners routinely shipped to industry suboptimal products.22 But using multiobjective optimizers, we can autoconfigure and improve these complex tools.22

Table 3. Some challenges and opportunities for the technology discussed.

Note that we mentioned configuration in both of the last two paragraphs. There is so much recent success in automatic configuration to satisfy numerous requirements. This is partly due to the advent of the alignment of machine learning (ML) and DevOps in MLOps, which helps to increase automation.23 So why are these tools not used more widely? The answer, we think, is to improve AI education. Too many software engineers seem to view AI as an incomprehensible black box. We need to show software engineers that AI software is just software. That is, up to a point, software is something that can be refactored and improved by software engineers.

Therefore, additional effort on educating AI experts on SE knowledge is also critical to bridge the communication gap and to set reasonable expectations.
The last three rows of Table 3 deserve special attention. All of these tasks must explore complex spaces to return, perhaps, very complex models. Here, we assert there is much benefit in injecting requirements to reduce the complexity and increase the explainability of these tools. The standard workhorse of industrial AI is classification and regression that forms models from independent variables to predict for a single class variable. Viewed from the lens of RE, that may seem a very strange process (at least, to non-RE people). The lesson from decades of requirements research is that users come from different stakeholder groups and the requirements of each group are expressed as different sets of multiple goals. That is, the landscape of the problem is not one hill with a single top. Rather, our stakeholders live in different high mountain valleys, surrounded by an intricate multidimensional landscape. An AI tool that tries to build one model with one goal across that landscape will end up satisfying very few of the stakeholders (if, indeed, any at all). So what we propose is to reverse the formulation of the problem and consider what happens if we add requirements into AI.

When an AI tool is aware of different requirements, they dramatically reduce their search space.

Specifically, if we sort candidate solutions into different goals, and if we discard the worst half, then N-goals wipes out 2 N parts of the problem (caveat: and different stakeholder goals wipe out different parts of the space).

Such requirements-aware tools can terminate in logarithmic time (orders of magnitude faster than alternate tools that do not exploit the landscape of the solution).21

An Interdisciplinary Approach
Interdisciplinary collaboration is critical. Unfortunately, our subcommunities are divided by the languages we use. Unless we agree on common terminology, misunderstandings will persist. We think RE education (and SE in general) needs to improve. AI experts need to understand the importance of SE. A common vocabulary and understanding are needed for these communities to be able to exploit their strengths.24 Therefore, additional effort on educating AI experts on SE knowledge is also critical to bridge the communication gap and to set reasonable expectations.

Final Thoughts
In our view, intelligence means navigating a landscape of (often competing) constraints. And user requirements is how we define that landscape. That is, mathematically speaking, user requirements are the forces that change the shape of the ideas explored by AI. (For more on the mathematics of user requirements, see the formal analysis of Jureta et al.25 or the empirical analysis of Mathew et al.26) Hence we say that AI and RE are inexorably linked. We cannot get the most out of AI/RE without working more on RE/AI.

AI techniques can offer useful comments on many parts of that landscape.

RE can be used to quickly rule out the less-than-useful parts of that space.

To say that another way, ignoring either can be detrimental to the other. And, to get the most out of both means working harder on each.