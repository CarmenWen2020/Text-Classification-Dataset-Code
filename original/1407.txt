Today, continuous integration (CI) is state of the art for agile software practices. Teams can choose from a range of tools, such as Jenkins, Atlassian Bamboo, and Microsoft Azure DevOps, that facilitate CI, from build servers to deployment platforms. For pure software solutions and where hardware can be virtualized, CI is relatively easy to achieve thanks to fully automated testing. It becomes significantly harder for systems involving hardware that have to be interacted with when executing test cases.
Today, continuous integration (CI) is state of the art for agile software practices. Teams can choose from a range of tools, such as Jenkins, Atlassian Bamboo, and Microsoft Azure DevOps, that facilitate CI, from build servers to deployment platforms. For pure software solutions and where hardware can be virtualized, CI is relatively easy to achieve thanks to fully automated testing. It becomes significantly harder for systems involving hardware that have to be interacted with when executing test cases.

From the Editors
Payment systems are among those we expect to always function correctly, reliably, and securely. They include hardware parts that refuse to be mocked and virtualized, making quality assurance very challenging. Three development leads from a payment software vendor take us on a continuous journey toward fully automated testing, during which we will encounter 3D printers turned into robots, an extensible test command application programming interface serving as tunnel to the future, and a credit card toaster—Cesare Pautasso and Olaf Zimmermann

We were facing this very problem at Abrantix with our payment software engineering practice. We write a number of payment terminal applications for large financial institutions. A payment terminal has a keypad for PIN entry, a display, and a number of different card readers for contact chip cards, contactless chip cards, and magstripes. Some of these components, particularly secure pin entry and chip and contactless card interaction, pose significant hurdles for virtualization and therefore for CI. Still, quality is a key factor for success, especially in the payments industry. No one wants to accidentally pay twice for their coffee, right? As a result, we embarked on a journey to overcome these hurdles and establish automated CI for payment terminals.

Shift Left or Die—or, Why Are We Doing This?
Lots of effort, work hours in particular, is required to manually test payment applications on terminals. However, functional correctness and regulatory compliance are a must in our industry. With manual testing, human errors can become quite common, especially since the process is very repetitive. Imagine having to execute hundreds of transactions per day and, for each transaction, checking individual fields of messages in a trace file. This is an ideal task for a machine since we humans are less equipped for such work.

Payment applications get ever more comptlex, having to support more and more payment means, terminal models, operating systems, and global standards.1 For example, our software supports many payment protocols, add-on features, and older terminal models with native operating systems, as well as newer Android-based terminals. Hence, there is even more to test. In addition, it is crucial that bugs are discovered as early as possible in the development process. In the past, payment terminal software was developed using waterfall processes. In contrast to other applications, the development of payment terminal software has typically not moved to a more agile model. While developers test their own features in an ad hoc manner, mostly manually, there still is a development phase followed by a testing phase. If bugs are found, the process is repeated. This is not very agile because testing is done after developing, not in parallel. It can lead to a very long time to market, especially because manual testing becomes a bottleneck with growing complexity.

Therefore, we wanted to “shift left”: run regression, integration, and even end-to-end tests earlier in the cycle. Our goal was to discover any kind of issue, even stability or performance problems, as early as possible. CI facilitates this and enables testing to be driven by developers. To achieve this, it was clear that end-to-end testing had to be automated. We see this as the only way to continuously deliver high-quality payment terminal software.

To Virtualize or Not to Virtualize
Figure 1 shows a simplified, high-level overview of the architecture inside a payment terminal, as appearing in grocery stores, restaurants, ticket offices, and so on. The payment terminal application is the core function orchestrating the main use cases, such as payments and configuration. Input and output actions are delegated to hardware components, including the display, card readers, and the PIN entry device (PED). One special aspect is the entry of a PIN on the PED. This is not directly governed by the payment terminal application but is handled by the secure module, which also has access to the PED. (A secure module safeguards and manages digital keys and performs encryption and decryption functions for digital signatures, strong authentication, and other cryptographic functions.) From the viewpoint of the payment terminal application, each of these components sits behind a hardware abstraction layer to simplify integration.

Figure 1. - A high-level input/output overview of payment terminal software architecture.
Figure 1.
A high-level input/output overview of payment terminal software architecture.

Show All

Figure 2. - The test robot in action.
Figure 2.
The test robot in action.

Show All

To automate testing, all input to the terminal has to be automated, while output from the terminal has to be verified. An obvious idea to automate the tests of the components shown in Figure 1 is to simply run the payment application in a virtual setup and implement a mock for each component behind the hardware abstraction layer. The upside is that everything is software, easy to change and simple to deploy. However, we identified significant downsides, as in the following:

Card readers are hard to mock: While magstripe cards are very easy to simulate, both contact and contactless chip-based payment cards contain significant logic on their chips and, depending on the card issuer, can behave differently. The Eurocard, MasterCard, Visa (EMV) standard (https://www.emvco.com/) is applied to these cards. The EMV standard describes numerous security options and features. These features would have to be reimplemented in a virtual setup.

Secure modules are hard to mock: A secure module is a black box that handles all aspects of security, from the EMV to PIN encryption and the functionality required by various protocols used by the terminal to communicate to host systems. Virtualizing this component would mean writing the required cryptographic functionality, sometimes proprietary, which would require a lot of effort. In addition, the necessary secret keys, also just for test cards, are governed by various card schemes, and access to this material would have to be provided. These are symmetric and asymmetric private keys, necessary to create valid EMV payment transactions and to correctly encrypt a PIN for an online transaction.

No real end-to-end testing is achieved: A virtual setup would be easier to deploy, but one key aspect would be left out: the varying behavior of different hardware. While we still see value in a virtual setup for many tests, one of the reasons not to virtualize is the fact that a virtualized setup is not a real end-to-end test. Important aspects such as memory management, file locking, and related functionality would be “mocked away” in a virtual setup. Our software runs on various hardware platforms, and in the past, we experienced hardware platforms behaving differently. Therefore, using actual hardware is relevant in the testing processes. It would later turn out that our decision not to use a virtual setup led us to discover a number of issues that would not have been detected in a virtual setup. These considerations let us pursue another route: robotics.

A Journey of Trial and Error
Once we decided to use robotics, we started to rapidly prototype a solution. It seemed obvious: we just needed to find the right robot, integrate it in our existing test automation scripts, and we should be done. Our starting point was a lower-cost single-arm robot. Most of these robots, like the one we chose, have a simple gripper at the end of the arm to pick up items. The idea was to place the payment terminal in front of the robot and have a tray from which the robot grabbed cards to insert. Pressing the relatively small buttons of the terminal could be achieved either by angling the arm in a way that only a corner of the gripper would touch a button or by using a pen.2

At first, we identified the simplest of all transactions to automate: a contactless transaction without a PIN entry. This required no keypad interaction and could be started with an existing point-of-sale (POS) simulator controlled by the test script. All that was required of the robot was to move a contactless card into the range of the reader’s near-field communication antenna and then move it away. This was easy enough, and we achieved automation relatively quickly.

As a next step, we wanted to extend the setup to include inserting a contact-based card. This meant the robot had to pick up cards from a tray and insert them into the reader. Unfortunately, that was when issues started to develop. After a few trials, it became clear that the task was challenging because it was hard to exactly control the position to insert the card. In addition, after some repetitions, the arm went slightly out of calibration. Our robot was not precise enough. Therefore, we decided to create a separate component that was firmly attached to the terminal and simply inserted and ejected the cards.

All of a sudden, we were able to support simple contactless and contact-based transactions. But we were not yet able to press buttons. Since the gripper on our robot could not be angled, we would have needed to use a pen to press buttons. Additionally, the pressing itself would have to be done within the correct range of force and duration. After a lot of trials, we realized we could not move forward with that robot. The components were just not precise enough to repeatedly reach a key properly.

Our first impulse to solve this was to get a higher-quality robot. There are many different robots in the market, and surely there would be many that did not have this problem. The problem with this approach was on one side the cost: we did not know how many robots we had to buy and try until we could find the right one. On the other hand, we also saw a conceptual issue around grabbing cards. How could we recover if the robot dropped a card? Even though we were not sure about the likelihood of this happening with a better robot, it seemed to be a very hard problem to solve if it were to occur.

Because of these considerations, we started contemplating creating our own robot. A single-arm robot is a great and versatile tool, but we thought we could create something less versatile and more reliable, specifically geared to our use case. We set off with some research about similar machines for inspiration and landed on computer numerical control machines and, later, 3D printer kits. 3D printers are based on a three-axis (or more) coordinate system, and the printhead can drive to coordinates in the system. This seemed ideal because the keys on a terminal can simply be seen as points in a cartesian coordinate system. In addition to the finger pressing buttons, we added one more axis for a card arm. However, it was difficult to find an option to grab cards, and we had to fix a single card in this arm. This led us to the next problem: How could we test with multiple cards?

Scaling With Cards
Different brands have their own applications and profiles on their card chips. As a consequence, cards can behave slightly differently in certain cases. Therefore, testing with multiple cards is important. After all, you want to be able to buy your coffee with whatever brand of card is in your wallet. Since the robot itself could not grab different ones, we needed some mechanism to switch cards. Thus, we created another piece of hardware that looked a lot like a toaster since it had multiple slots in which cards could be placed. It could be considered a multiple-input, single-output switch, similar to the multiplexers that are used in electronics. Its role is to accommodate a number of slots, where cards are placed as the input on one side, and a single card probe is situated as the output on the other. An electronic mechanism, controlled via an application programming interface (API), determines which card will be electrically routed to the output line. Due to the correlation to its electronic cousin, we call it a card multiplexer. Of course, these days, many people prefer to use contactless cards. Therefore, the same device had to be created for those as well.

Coping With Calibration
One requirement was that our setup supports different terminal form factors. We needed that because we use many terminal models with different physical shapes. For all these, the key pad is in a different position. Therefore, when pressing the button “1,” the robot finger had to drive to different positions for each terminal model. This meant that when writing our automated test cases, we had to write test cases for each terminal. Quickly, it became clear that this was a very bad design: a simple test case, such as a purchase transaction, had to be written over and over again, just with different coordinates. It would be much more efficient to write one test case that worked with all terminals. To solve the problem, we needed a calibration mechanism that was transparent to the robots’ user. The solution was straightforward: we kept a layout for each terminal model that contained the button coordinates and mapped this to canonical names that could be used in the test script.

APIs and Abstraction Layers
Having solved all these problems, we finally had a solution that allowed us to fully automate tests for certain use cases with a very simple design for the software components of the test system, as shown in Figure 3. To increase automation levels and coverage for more use cases, we started to add more components to the mix. For example, we needed drivers simulating POS systems. They were required to test the integration of terminals with POS systems. Furthermore, we created a magstripe probe device to simulate magstripe cards, and we added terminal display validation. It became clear that our software architecture was not good enough since we had to implement the integration of all these items directly in the test orchestration. This meant a very tight coupling between the extensions and the test execution.

Figure 3. - A high-level overview of the test setup.
Figure 3.
A high-level overview of the test setup.

Show All

At the same time, we started to work with a partner company that uses our robots and other hardware to test terminals for large financial entities. It already had multiple test orchestration solutions for different clients, and using our own orchestration component was not an option. Therefore, we redesigned our architecture, added more abstraction layers, and, together with our partner, added a simple API to call our system from third-party orchestration solutions (Figure 4). The final solution was a very simple generic representational state transfer (REST) HTTP API that could be used by any orchestration tool, and it was employed to distribute calls to the connected extensions.

Figure 4. - The high-level architecture of the extension API in the context of the test target. REST: representational state transfer; OCR: optical character recognition.
Figure 4.
The high-level architecture of the extension API in the context of the test target. REST: representational state transfer; OCR: optical character recognition.

Show All

Generic, but Not!
Once the API was in place, we realized that extensibility would become a major requirement in the future. We wanted to be in a position to simply add new components behind the API as needed, for example, POS simulators and new hardware items, such as simple actuators. The API architecture (Figure 4) supported extensibility well: each new “thing” we wanted to attach simply had to implement a certain set of functions, including the following:

initialize

before execute

execute

after execute

dispose.

The problem with making this generic was that each extension had a proprietary set of commands. The robot could press a button and insert a card, whereas the multiplexers could only select a slot and so forth. Instead of defining extension-specific fully typed commands at the API level, we decided to leave part of the API payload arbitrary. The benefit was that the API entry point implementation itself stayed generic, while each extension could define its own custom command set, as illustrated in Figure 5. A drawback was the loss of type safety for that part of the payload at the entry point. We solved that by implementing command validation within each extension, where the proprietary commands are understood.

Figure 5. - The proprietary commands per extension. JSON: JavaScript Object Notation. BTT: Brand Testing Tool.
Figure 5.
The proprietary commands per extension. JSON: JavaScript Object Notation. BTT: Brand Testing Tool.

Show All

The robot has a simple comma-separated value (CSV) command format, while for the optical character recognition (OCR) extension, the caller can send a JavaScript Object Notation object. The instruction to the robot to press buttons 1, 2, 3, 4, and OK were as follows:

[{

“target”: “Robot-1”,

“AutomationCommands”: [

 {

   “Version”: “1.0”,

   “Command”: “1;2;3;4;OK”

 }

]

}]

The instruction to the OCR extension to grab the lines currently shown on the terminal screen was as follows:

[{

“target”: “Ocr-1”,

“AutomationCommands”: [

 {

   “Command”:

   {

    “action”: “getLines”

   }

 }]

}]

Habemus Automation!
Finally, we had a solution that was extensible and easy to use from a tester’s perspective. At this stage, we started to convert most of our manual test cases to automated ones. The first thing we did was very simple: run 5,000 test transactions in a row on each terminal. What we discovered was surprising: none of our terminals passed the test, and on average, we found more than 70 bugs in each model. No complex test script was required, nor did we think about test coverage at all. Just a simple test case enabled us to discover many low-level issues (memory leaks, hardware driver crashes, file locking issues, and so on) we had not been able to spot and solve before. Rectifying these issues led to a much more stable application. This showed us an important aspect: besides the well-established benefits of automation, such as speed, accuracy, repeatability, and scalability, the ability to stress a terminal through a long time without interruption proved very valuable.

A Continuous Journey
We had stable test sets running automatically. But as with most things, the end was just the beginning. The next problem we faced was the need to automatically deploy a new payment terminal application release to a terminal. In contrast to a server or PC, a payment terminal has strict limitations on how software can be installed. A lot of this is around security and managed by a host system. For example, when resetting a terminal, a host system needs to provision the device with specific keys in a secure way. This meant that the host system had be involved in our process as well. The same, or even a different, host system then contains further configuration settings, and the terminal needs to be instructed to download these. All in all, the setup process can be quite complex, and it is hard to automate. The problems and solutions we found during that part of the journey could fill an article on their own.

There is no final point we will reach one day, and we will never finish this process. Instead, we are on a continuous journey to CI. Every day we improve our setup, add more test cases, add a new extension, and improve our system so that we can improve the overall quality of our solution. While it took longer than expected to achieve automation, we can say that the benefits far outweigh the effort and costs. We would even venture to say that automation is now a must. These days, we test more and faster than ever and are able to improve quality. This makes our clients happy and lets us all buy coffee and be confident that we are charged the correct amount. If you are curious and would like to embark on this journey, you will need robotic and test automation engineers and a good dose of patience.