In this article, we demonstrate that lightweight high-level runtimes, such as WebAssembly, could offer performance and scaling advantages over existing solutions and could enable fine-grained, pay-as-you-use business models.
Serverless computing is an approach to build and deploy cloud services without having to manage servers. Compared with infrastructure as a service (IaaS), serverless computing offers a higher level of abstraction and hence promises to deliver higher developer productivity and lower operation costs. The key characteristics of serverless computing include resource elasticity, zero ops, and pay as you use.1

The original cloud service, Amazon Web Service’s Simple Storage Service (AWS S3), is serverless, but it is only for storage. However, it proves difficult to scale code execution. IaaS became the dominant cloud service. Developers provision virtual machines (VMs, servers) to deploy applications. The developer is responsible for planning, provisioning, deploying, and maintaining the servers. Even with more advanced platform as a service (PaaS), developers get virtual servers preinstalled with a software stack, and they must pay based on allocated resources (for example, CPU and memory) as opposed to actual usage.

In recent years, a new generation of cloud services, pioneered by AWS Lambda, enables developers to create web services based directly on code functions. Developers do not need to provision or manage servers. The cloud provider automatically deploys an execution environment when requests come in and scales the environment based on demand.2 That approach is known as function as a service (FaaS).3 Industry reports indicate that FaaS is already popular among cloud users and could grow into a dominant form of future cloud computing.4

The rise of FaaS is enabled by technology advances in virtualization and containerization, which make it possible to frequently start and stop the execution environment with minimal overhead. Leading public cloud providers use a combination of system VMs and application containers as the infrastructure for FaaS. The system VMs provide isolation at the system or hardware level, such as hypervisor-based VMs. They are in contrast with high-level software-based language VMs, such as the Java VM, JavaScript runtime, and WebAssembly VM. Compared with system VMs, application containers like Docker are faster and lighter5 but less secure.6

The AWS Lambda is based on a lightweight virtualization technology called Firecracker.7 Also known as microVM, Firecracker is lighter and faster than traditional system VMs and hence is suitable for the FaaS workload.

IBM Cloud Functions utilizes Docker containers to provide isolation.

Microsoft Azure Functions is compatible with Docker but with added VM protection when deployed on the public cloud.8

Google Cloud Functions took a compromise approach. Its gVisor engine is designed to safely run Docker in the public cloud. However, gVisor is 2× slower than the plain Docker.9

Even with those performance-optimized system VMs and application containers, FaaS still suffers from performance and scalability issues.

First, cold start is slow.10 To provision and start a microVM or container could take seconds. This is a key issue for FaaS, where each function execution could require a new microVM or container. This problem also leads to an inconsistent and unpredictable performance, since the function could run faster when an execution environment is “warmed up.”

Second, the VM or container must set up a runtime software stack, including operating system-specific standard libraries, for every function call. The result is a large footprint.

Finally, the current generation of VM or container-based FaaS solutions bill users for coarse-grained resource consumption, such as allocated memory and execution time. They do not support fine-grained usage billing, such as CPU cycles for each function execution. In this article, we will discuss an emerging FaaS approach that does not depend on VMs or containers.

A Lightweight Approach
Serverless FaaS must provide a secure and isolated execution environment for user-submitted and untrusted code. It has been suggested that WebAssembly is a lighter and faster alternative for FaaS.11,12 Unlike system VMs and containers, WebAssembly is a high-level language VM runtime.

WebAssembly started as a collaboration to improve the JavaScript performance inside web browsers. Since it is designed to execute code downloaded from the web, the WebAssembly runtime provides a software-based fault isolation13 sandbox for untrusted code.

WebAssembly supports multiple programming languages, such as C/C++ and Rust. The WebAssembly System Interface (WASI) standard14 allows WebAssembly runtimes to interface with operating system resources, which makes it suitable for server-side applications.

WebAssembly functions can run securely and in isolation. Those functions can be started and stopped on demand across different underlying platforms without any code change. Since WebAssembly provides abstractions at the opcode level, it can precisely measure finely grained resource consumptions at runtime.

The most important advantages WebAssembly promises over system VMs, microVMs, and containers are the performance and footprint. Smaller cloud providers, such as Fastly and Cloudflare, are already offering WebAssembly-based FaaS on their edge networks. In this study, we evaluate the performance of leading WebAssembly runtimes against Docker containers. Docker is chosen as the benchmark comparison because it is the most widely used application container and delivers state-of-the-art performance for non-WebAssembly FaaS.

WebAssembly’s performance advantages over Docker in FaaS had been documented in the literature.11,12 But our study has several noticeable differences. Instead of timing an integrated use case, our study evaluates the standard performance benchmarks to eliminate the complexity of the host environment, such as the multithread efficiency or the disk I/O, which are not directly related to WebAssembly versus Docker runtimes. Furthermore, we evaluate several leading WebAssembly runtimes with different optimization strategies. When possible, we execute WebAssembly functions in the standalone WASI mode as opposed to relying on host runtimes, such as Node.js to bootstrap from slow JavaScript.

Performance Evaluation
We conducted experiments on two large cloud platforms, AWS and Microsoft Azure. Both systems run Ubuntu Linux 20.04 long-term support. The hardware configurations are as follows.

AWS: m5.2´large instance; Intel CPU 2.30 GHz with four CPU cores and 32 GB of random-access memory (RAM)

Azure: Standard_D4_v3 instance; Intel CPU 2.50 GHz with four CPU cores and 16 GB of RAM.

The following runtimes are evaluated.

In the latest stable Docker release, v19.03.8, a Docker image for Ubuntu Linux 18.04 was used to start the Docker container.

V8, v8.1.30715 is the WebAssembly runtime developed by Google. It is preinstalled in many web browsers.

Lucet, v0.6.1,16 from the Byte Alliance, is a collaboration between Fastly and Mozilla to build a WebAssembly compiler and runtime.

The Second State Virtual Machine (SSVM), v0.6.0,17 is a WebAssembly runtime optimized for server-side workloads.

The WebAssembly virtual machine (WAVM), nightly/2020-05-28,18 is a collaborative effort led by Andrew Scheidecker. It aims to support WebAssembly outside of the browser.

For each testing machine and runtime combination, we ran the following benchmark tests. We executed each test 50 times and calculated the average and standard deviations for completion time. The source code and instructions are publicly available on GitHub.19

The nop test starts the runtime and exits. It evaluates the cold start performance.

The cat-sync test opens a local file to read and exits. It evaluates the performance in making operating system calls.

The following four benchmarks are from the Computer Languages Benchmarks Game,20 which provides crowdsourced benchmark programs for more than 25 programming languages. The nbody, repeated 50 million times, is an n-body simulation. The fannkuch-redux, repeated 12 times, measures the indexed access to an integer sequence. The Mandelbrot, repeated 15,000 times, is to generate a Mandelbrot set portable bitmap file. The binary tree, repeated 21 times, allocates and deallocates large numbers of binary trees.

All benchmark tests are written in C. For Docker tests, the test source code is compiled to a native binary by the LLVM v10.0.0 toolchain. For WebAssembly tests, the source code is compiled into WebAssembly bytecode using the Emscripten SDK v1.39.17.

Figure 1 shows the execution time results from the nop test. WebAssembly cold starts are at least 10× faster than Docker. Among WebAssembly runtimes, SSVM and Lucet are more than 10× faster than the rest. SSVM and Lucet start in less than 1/500 of the time it took Docker to start.

Figure 1. - The cold start time (the shorter time, the better). The y-axis is in log10 scale. SSVM: Second State Virtual Machine.
Figure 1.
The cold start time (the shorter time, the better). The y-axis is in log10 scale. SSVM: Second State Virtual Machine.

Show All

Figure 2 shows the execution time results from the cat-sync test from the cold start. With the exception of Lucet, which failed this test, WebAssembly runtimes are at least 10× faster than Docker. Among WebAssembly VMs, SSVM showed the best performance. Docker takes 50× to 150× longer than SSVM to open a file.

Figure 2. - The time to open and read a file (the shorter time, the better). The y-axis is in log10 scale.
Figure 2.
The time to open and read a file (the shorter time, the better). The y-axis is in log10 scale.

Show All

Table 1 shows the execution time results from the four computational benchmarks. Since each benchmark test is executed repeatedly in a loop during each test run, those numbers indicate a warm start performance. WebAssembly runtimes are about 10 to 50% faster than Docker with the exception of Lucet, which crashed in several tests and generally performed poorly.

Table 1. The execution time for computational tests (the shorter time, the better). All numbers are in seconds.
Table 1.- The execution time for computational tests (the shorter time, the better). All numbers are in seconds.
Besides the execution time performance advantages, WebAssembly also has a much smaller footprint than Docker. The Docker image for Ubuntu 18.04 is 188 MB, while the SSVM is less than 6 MB. In general, WebAssembly runtimes are 1/10 of the size of the full Docker images for FaaS.

Challenges and Discussions
While the WebAssembly runtimes outperform Docker in many computing benchmarks, there are also many challenges that could hinder their adoption. First, programming language support on WebAssembly is limited. While the LLVM toolchain supports generating WebAssembly bytecode from 20+ languages, only a handful of languages are well supported in practice. Currently, C/C++, Rust, and AssemblyScript are the best-supported programming languages for WebAssembly. That means developers need to write their functions in those languages to take advantage of WebAssembly-based FaaS.

Second, compared with containers and microVMs, WebAssembly has a different model to access the operating system. System features, such as networking and multithread management, are just becoming available to WebAssembly. However, WebAssembly’s access to those resources is governed by high-level security policies (as in, capability-based security) and could be both performant and secure. In this study, we did not evaluate the I/O performance of WebAssembly runtimes beyond simple file operations. Multithreaded I/O is an important aspect of real-world applications, and it is a topic of future research as related WebAssembly specifications mature.

Finally, there is currently no industry standard management and orchestration tools for WebAssembly runtimes. Most use home-grown solutions to start and stop WebAssembly runtimes on demand. We anticipate such tools will become available as WebAssembly adoption increases.

We evaluated the use of WebAssembly runtimes in serverless FaaS. Compared with containers or microVMs, WebAssembly could enable faster and more scalable services with fine-grained, pay-as-you-use billing.

WebAssembly tooling and runtimes optimized for cloud servers are the main hurdles the industry must overcome to see wide adoption of this technology. Developers probably need to rewrite part of their applications in C/C++ or Rust to take advantage of WebAssembly FaaS today.