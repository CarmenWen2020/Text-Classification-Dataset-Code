Software fault localization, the act of identifying the locations of faults in a program, is widely recognized to be one of the most tedious, time consuming, and expensive - yet equally critical - activities in program debugging. Due to the increasing scale and complexity of software today, manually locating faults when failures occur is rapidly becoming infeasible, and consequently, there is a strong demand for techniques that can guide software developers to the locations of faults in a program with minimal human intervention. This demand in turn has fueled the proposal and development of a broad spectrum of fault localization techniques, each of which aims to streamline the fault localization process and make it more effective by attacking the problem in a unique way. In this article, we catalog and provide a comprehensive overview of such techniques and discuss key issues and concerns that are pertinent to software fault localization as a whole.
SECTION 1Introduction
Software is fundamental to our lives today, and with its ever-increasing usage and adoption, its influence is practically ubiquitous. In fact, at present, software is not just employed in, but is critical to, many security and safety-critical systems in industries such as medicine, aeronautics, and nuclear energy. Not surprisingly, this trend has been accompanied by a drastic increase in the scale and complexity of software. Unfortunately, this has also resulted in more software bugs, which often lead to execution failures with huge losses [260], [275], [365]. Furthermore, software faults in safety-critical systems have significant ramifications, including not only financial loss, but also potential loss of life, which is an alarming prospect [368]. A 2002 report from the National Institute of Standards and Technology (NIST) [304] indicated that software errors are estimated to cost the U.S. economy $59.5 billion annually (0.6 percent of the GDP); the cost has undoubtedly grown since then. Over half the cost of fixing or responding to these bugs is passed on to software users, while software developers and vendors absorb the rest.

Even when faults in software are discovered due to erroneous behavior or some other manifestation of the fault(s), 1 finding and fixing them is an entirely different matter. Fault localization, which focuses on the former, i.e., identifying the locations of faults, has historically been a manual task that has been recognized to be time consuming and tedious as well as prohibitively expensive [347], given the size and complexity of large-scale software systems today. Furthermore, manual fault localization relies heavily on the software developer's experience, judgment, and intuition to identify and prioritize code that is likely to be faulty. These limitations have led to a surge of interest in developing techniques that can partially or fully automate the localization of faults in software while reducing human input. Though some techniques are similar and some very different (in terms of the type of data consumed, the program components focused on, comparative effectiveness and efficiency, etc.), they each try to attack the problem of fault localization from a unique perspective, and typically offer both advantages and disadvantages relative to one another. With many techniques already in existence and others continually being proposed, as well as with advances being made both from a theoretical and practical perspective, it is important to catalog and overview current techniques in fault localization in order to offer a comprehensive resource for those already in the area as well as those interested in making contributions to it.

In order to provide a complete survey covering most of the publications related to software fault localization since the late 1970s, we created a publication repository that includes 331 papers published from 1977 to November 2014. We also searched for Masters’ and Ph.D. theses closely related to software fault localization, which are listed in Table 1.

TABLE 1 A List of Recent Ph.D. and Master's Theses on Software Fault Localization

All papers in our repository2 are sorted by year, and the result is displayed in Fig. 1. As shown in the figure, the number of publications grew rapidly after 2001, indicating that more and more researchers began to devote themselves to the area of software fault localization over the last 10 years.


Fig. 1.
Papers on software fault localization from 1977 to November 2014.

Show All

Also, as per our repository, Fig. 2 gives the number of publications related to software fault localization that have appeared in top quality and leading journals and conferences that focus on Software Engineering – IEEE Transactions on Software Engineering, ACM Transactions on Software Engineering and Methodology, International Conference on Software Engineering, ACM International Symposium on Foundations of Software Engineering, and ACM International Conference on Automated Software Engineering – from 2001 to November 2014. This trend again supports the claim that software fault localization is not just an important but also a popular research topic and has been discussed very heavily in top quality software engineering journals and conferences over the last ten years.

Fig. 2. - 
Publication of software fault localization in top venues from 2001 to November 2014.
Fig. 2.
Publication of software fault localization in top venues from 2001 to November 2014.

Show All

There is thus a rich collection of literature on various techniques that aim to facilitate fault localization and make it more effective.3 Despite the fact that these techniques share similar goals, they can be quite different from one another and often stem from ideas that originate from several different disciplines. While we aim to comprehensively cover as many fault localization techniques as possible, no article, regardless of breadth or depth, can cover all of them. In this survey, our primary focus is on the techniques for locating Bohrbugs [139]. Those for diagnosing Mandelbugs [139] such as performance bugs, memory leaks, software bloats, and security vulnerabilities are not included in the scope. Also, due to space limitations, we group techniques into appropriate categories for collective discussion with an emphasis on the most important features and leave other details of these techniques to their respectively published papers. This is especially the case for techniques targeting a specific application domain, such as fault localization for concurrency bugs and spreadsheets. For these, we provide a review that helps readers with general understanding.

The following terms appear repeatedly throughout this article, and thus for convenience, we provide definitions for them here per the taxonomy provided in [37]:

A failure is when a service deviates from its correct behavior.

An error is a condition in a system that may lead to a failure.

A fault is the underlying cause of an error, also known as a bug.

The remainder of this article is organized in the following manner: we begin by describing traditional and intuitive fault localization techniques in Section 2, moving on to more advanced and complex techniques in Section 3. In Section 4 , we list some of the popular subject programs that have been used in different case studies and discuss how these programs have evolved through the years. Different evaluation metrics to assess the effectiveness of fault localization techniques are described in Section 5, followed by a discussion of fault localization tools in Section 6. Finally, critical aspects and conclusions are presented in Sections 7 and 8 , respectively.

SECTION 2Traditional Fault Localization Techniques
This section describes traditional and intuitive fault localization techniques, including program logging, assertions, breakpoints, and profiling.

2.1 Program Logging
Statements (such as print) used to produce program logging are commonly inserted into the code in an ad-hoc fashion to monitor variable values and other program state information [105]. When abnormal program behavior is detected, developers examine the program log in terms of saved log files or printed run-time information to diagnose the underlying cause of failure.

2.2 Assertions
Assertions are constraints added to a program that have to be true during the correct operation of a program. Developers specify these assertions in the program code as conditional statements that terminate execution if they evaluate to false. Thus, they can be used to detect erroneous program behavior at runtime. More details of using assertions for program debugging can be found in [309], [310].

2.3 Breakpoints
Breakpoints are used to pause the program when execution reaches a specified point and allow the user to examine the current state. After a breakpoint is triggered, the user can modify the value of variables or continue the execution to observe the progression of a bug. Data breakpoints can be configured to trigger when the value changes for a specified expression, such as a combination of variable values. Conditional breakpoints pause execution only upon the satisfaction of a predicate specified by the user. Early studies (e.g., [80], [155]) use this approach to help developers locate bugs while a program is executed under the control of a symbolic debugger. The same approach is also adopted by more advanced debugging tools such as GNU GDB [121] and Microsoft Visual Studio Debugger [255].

2.4 Profiling
Profiling is the runtime analysis of metrics such as execution speed and memory usage, which is typically aimed at program optimization. However, it can also be leveraged for debugging activities, such as the following:

Detecting unexpected execution frequencies of different functions (e.g., [43] );

Identifying memory leaks or code that performs unexpectedly poorly (e.g., [150] );

Examining the side effects of lazy evaluation (e.g., [313]).

Tools that use profiling for program debugging include GNU's gprof [120] and the Eclipse plugin TPTP [108].

SECTION 3Advanced Fault Localization Techniques
With the massive size and scale of software systems today, traditional fault localization techniques are not effective in isolating the root causes of failures. As a result, many advanced fault localization techniques have surfaced recently using the idea of causality [215], [288], which is related to philosophical theories with an objective to characterize the relationship between events/causes (program bugs in our case) and a phenomenon/effect (execution failures in our case). There are different causality models [288] such as counterfactual-based, probabilistic- or statistical-based, and causal calculus models. Among these, probabilistic causality models are the most widely used in fault localization to identify suspicious code that is responsible for execution failures.

In this survey, we classify fault localization techniques into eight categories, including slice-based, spectrum-based, statistics-based, program state-based, machine learning-based, data mining-based, model-based and miscellaneous techniques. Many studies that evaluate the effectiveness of specific fault localization techniques have been reported [8], [10], [11], [12], [25], [31], [36], [49], [53], [91], [93], [94], [102], [124], [178], [185], [191], [207], [209], [253], [266], [267], [296], [299], [335], [366], [390], [391], [393], [420], [406], [410], [424]. However, none of them offer a comprehensive discussion on all these techniques.

3.1 Slice-Based Techniques
Program slicing is a technique to abstract a program into a reduced form by deleting irrelevant parts such that the resulting slice will still behave the same as the original program with respect to certain specifications. Hundreds of papers on this topic have been published [52], [344], [394] since Weiser first proposed static slicing in 1979 [361].

One of the important applications of static slicing [360] is to reduce the search domain while programmers locate bugs in their programs. This is based on the idea that if a test case fails due to an incorrect variable value at a statement, then the defect should be found in the static slice associated with that variable-statement pair, allowing us to confine our search to the slice rather than looking at the entire program. Lyle and Weiser extend the above approach by constructing a program dice (as the set difference of two groups of static slices) to further reduce the search domain for possible locations of a fault [235]. Although static slice-based techniques have been experimentally evaluated and confirmed to be useful in fault localization [207], one problem is that handling pointer variables can make data-flow analysis inefficient because large sets of data facts that are introduced by dereferences of pointer variables need to be stored. Equivalence analysis, which identifies equivalence relationships among the various memory locations accessed by a procedure, is used to improve the efficiency of data-flow analyses in the presence of pointer variables [220]. Two equivalent memory locations share identical sets of data facts in a procedure. As a result, data-flow analysis only needs to compute information for a representative memory location, and data-flow for other equivalent locations can be garnered from the representative location. Static slicing is also applied for fault localization in binary executables [192], and type-checkers [343].

A disadvantage of static slicing is that the slice for a given variable at a given statement contains all the executable statements that could possibly affect the value of this variable at the statement. As a result, it might generate a dice with certain statements that should not be included. This is because we cannot predict some run-time values via a static analysis. To exclude such extra statements from a dice (as well as a slice), we need to use dynamic slicing [20], [202] instead of static slicing, as the former can identify the statements that do affect a particular value observed at a particular location, rather than possibly affecting such a value as with the latter. Studies such as [18], [24], [28], [96], [104], [188], [192], [201], [219], [227], [237], [257], [277], [297], [334], [356], [378], [379], [406], [407], [410], which use the dynamic slicing concept in program debugging, have been reported. In [379], Wotawa combines dynamic slicing with model-based diagnosis to achieve more effective fault localization. Using a given test suite against a program, dynamic slices for erroneous variables discovered are collected. Hitting-sets are constructed, which contain at least one statement from each dynamic slice. The probability that a statement is faulty is calculated based on the number of hitting-sets that cover that statement. Zhang et al. [407] propose the multiple-points dynamic slicing technique, which intersects slices of three techniques: Backward Dynamic Slice (BwS), Forward Dynamic Slice (FwS), and Bidirectional Dynamic Slice (BiS). The BwS captures any executed statements that affect the output value of a faulty variable, while the FwS is computed based on the minimal input difference between a failed and a successful test case, isolating the parts of the input that trigger a failure. The BiS flips the values of certain predicates in the execution of a failed test case so that the program generates a correct output. Qian and Xu [297] propose a scenario-oriented program slicing technique. A user-specified scenario is identified as the extra slicing parameter, and all program parts related to a special computation are located under the given execution scenario. There are three key steps to implementing the scenario-oriented slicing technique: scenario input, identification of scenario relevant codes, and, finally, gathering of scenario-oriented slices.

One limitation of dynamic slicing-based techniques is that they cannot capture execution omission errors, which may cause the execution of certain critical statements in a program to be omitted and thus result in failures [411]. Gyimothy et al. [142] propose the use of relevant slicing to locate faulty statements responsible for execution omission errors. Given a failed execution, the relevant slicing first constructs a dynamic dependence graph in the same way that classic dynamic slicing does. It then augments the dynamic dependence graph with potential dependence edges, and a relevant slice is computed by taking the transitive closure of the incorrect output on the augmented dynamic dependence graph. However, incorrect dependencies between program statements may be included to produce oversized relevant slices. To address this problem, Zhang et al. [411] introduce the concept of implicit dependencies, in which dependencies can be obtained by predicate switching. A similar idea has been used by Weeratunge et al. [358] to identify root causes of omission errors in concurrent programs, in which dual slicing, a combination of dynamic slicing and trace differencing, is used.

An alternative approach to static and dynamic slicing is the use of execution slicing based on data-flow tests to locate program bugs [21] in which an execution slice with respect to a given test case contains the set of statements executed by this test. The reason for choosing execution slicing over static slicing is that a static slice focuses on finding statements that could possibly have an impact on the variables of interest for any inputs, versus statements that are executed by a specific input. This implies that a static slice does not make any use of the input values that reveal the fault and violates a very important concept in debugging that suggests programmers analyze the program behavior under the test case that fails and not under a generic test case. Collecting dynamic slices may consume excessive time and file space, even though different algorithms [51], [204], [408], [409] have been proposed to address these issues. Conversely, it is relatively easy to construct the execution slice for a given test case if we collect code coverage data from the execution of the test. Different execution slice-based debugging tools have been developed and used in practice such as χSuds at Telcordia (formerly Bellcore) [22], [427] and eXVantage at Avaya [372]. Agrawal et al. [21] apply the execution slice to fault localization by examining the execution dice of one failed and one successful test to locate program bugs. Jones et al. [186], [187] and Wong et al. [375] extend that study by using multiple successful and failed tests based on the following observations:

The more successful tests that execute a piece of code, the less likely it is for the code to contain a bug.

The more failed tests with respect to a given bug that execute a piece of code, the more likely that it contains this bug.

We use the following example to demonstrate the differences among static, dynamic, and execution slicing. Use the code in column 2 of Table 2 as the reference. Assume it has one bug at s7. The static slice for the output variable, product, contains all statements that could possibly affect the value of product , s1, s2, s4, s5, s7, s8, s10, and s13, as shown in the third column. The dynamic slicing for product only contains the statements that do affect the value of product with respect to a given test case, which includes s1, s2, s5, s 7, and s13 (as shown in the fourth column) when a=2. The execution slice with respect to a given test case contains all statements executed by this test. Therefore, the execution slice for a test case, a = 2, consists of s1, s2, s3, s4, s5, s6, s7, s12, s13 as shown in the fifth column of Table 2.

TABLE 2 An Example Showing the Differences among Static, Dynamic, and Execution Slicing

One problem with the aforementioned slice-based techniques is that the bug may not be in the dice. Even if a bug is in the dice, there may still be too much code that needs to be examined. To overcome this problem, an inter-block data dependency-based augmentation and a refining method is proposed in [373]. The former includes additional code in the search domain for inspection based on its inter-block data dependency with the code which is currently being examined, whereas the latter excludes less suspicious code from the search domain using the execution slices of additional successful tests. Additionally, slices are problematic because they are always lengthy and hard to understand. In [205], the notion of using barriers is proposed to provide a filtering approach for smaller program slices and better comprehensibility. Authors of [330] propose thin slicing in order to find only producer statements that help compute and copy a value to a particular variable. Statements that explain why producer statements affect the value of a particular variable are excluded from a thin slice.

3.2 Program Spectrum-Based Techniques
Following the discussion in the beginning of Section 3, we would like to emphasize that many spectrum-based techniques are inspired by the probabilistic- and statistical-based causality models. With this understanding, we now explain the details of these techniques.

A program spectrum details the execution information of a program from certain perspectives, such as execution information for conditional branches or loop-free intra-procedural paths [149] . It can be used to track program behavior [305]. An early study by Collofello and Cousins [79] suggests that such spectra can be used for software fault localization. When the execution fails, such information can be used to identify suspicious code that is responsible for the failure. Code coverage, or Executable Statement Hit Spectrum (ESHS), indicates which parts of the program under testing have been covered during an execution. With this information, it is possible to identify which components were involved in a failure, narrowing the search for the faulty component that made the execution fail.

3.2.1 Notation
P a program

NCF number of failed test cases that cover a statement

NUF number of failed test cases that do not cover a statement

NCS number of successful test cases that cover a statement

NUS number of successful test cases that do not cover a statement

NC total number of test cases that cover a statement

NU total number of test cases that do not cover a statement

NS total number of successful test cases

NF total number of failed test cases

ti the ith test case

3.2.2 Techniques
Early studies [19], [201], [203], [341] only use failed test cases for spectrum-based fault localization, though this approach has subsequently been deemed ineffective [21], [185], [366]. Later studies achieve better results using both the successful and failed test cases and emphasizing the contrast between them. Set union and set intersection are proposed in [303]. The set union focuses on the source code that is executed by the failed test but not by any of the successful tests. Such code is more suspicious than others. The set intersection excludes the code that is executed by all the successful tests but not by the failed test. Renieris and Reiss [303] propose another ESHS-based technique, nearest neighbor, which contrasts a failed test with a successful test that is most similar to the failed one in terms of the distance between them. If a bug is in the difference set, it is located. For a bug that is not contained in the difference set, the process continues by first constructing a program dependence graph (PDG) and then including and checking adjacent un-checked nodes in the graph step by step until all the nodes in the graph are examined. The idea of nearest neighbor is similar to Lewis’ counterfactual reasoning [216], which claims that, for two events A and B, A causes B (in world w) if and only if, in all possible worlds that are maximally similar to w, A does not take place and B also does not happen. The theory of counterfactual reasoning is also found in other studies such as [137], [179], [400].

Intuitively, the closer the execution pattern of a statement is to the failure pattern of all test cases, the more likely the statement is to be faulty, and consequently the more suspicious the statement seems. By the same token, the farther the execution pattern of a statement is to the failure pattern, the less suspicious the statement appears to be. Similarity coefficient-based measures can be used to quantify this closeness, and the degree of closeness can be interpreted as the suspiciousness of the statements.

A popular ESHS-based similarity coefficient-based technique is Tarantula [186] , which uses the coverage and execution results (success or failure) to compute the suspiciousness of each statement as (NCF/NF)/(NCF/NF+NCS/NS). A study on the Siemens suite [185] shows that Tarantula inspects less code before the first faulty statement is identified, making it a more effective fault localization technique when compared to others such as set union, set intersection, nearest neighbor and cause transition [77]. Based on the suspiciousness computed by Tarantula, studies like [186] , [187] use different colors (from red to yellow to green) to provide a visual mapping of the participation of each program statement in the execution of a test suite. The more failed test cases that execute a statement, the brighter (redder) the color assigned to the statement will be. In [94], Debroy et al. further revise the Tarantula technique. Statements executed by the same number of failed test cases are grouped together, and then groups are ranked in descending order by the number of failed test cases. Using Tarantula, statements are ranked by suspiciousness within each group.

For discussion purposes, let's use the code in Table 2 again. Assume that we have two successful test cases (a=0 and a=1) and one failed test case (a=2 ). The suspiciousness value of each statement can be computed, for example, using the Tarantula technique discussed above. The results are as shown in Table 3.

TABLE 3 An Example Showing the Suspiciousness Value Computed Using the Tarantula Technique

The third to fifth columns in Table 3 represent the statement coverage of the three test cases. An entry with a “•” means the statement is covered by the corresponding test case, while an empty entry means the statement is not. The values of NCF and NCS for each statement are given in the sixth and seventh columns. Based on the definition of Tarantula, the suspiciousness value of each statement is computed and displayed in the eighth column. The ranking of each statement is given in the rightmost column. As we can observe, the faulty statement s7 has the highest ranking.

In recent years, other techniques have also been proposed that perform at the same level with, or even surpass, Tarantula in terms of their effectiveness at fault localization. The Ochiai similarity coefficient-based technique [11] is generally considered more effective than Tarantula, and its formula is as follows:
Suspiciousness(Ochiai)=NCFNF×(NCF+NCS)−−−−−−−−−−−−−−−√.(1)
View Source

There are two major differences between Ochiai and the nearest neighbor model: 1) The nearest neighbor model utilizes a single failed test case, while Ochiai uses multiple failed test cases, and 2) The nearest neighbor model only selects the successful test case that most closely resembles the failed test case, while Ochiai includes all successful test cases. Ochiai2 [267] is an extension of Ochiai, and its formula is as follows:
Suspiciousness(Ochiai2)=NCF×NUS(NCF+NCS)×(NUS+NUF)×(NCF+NUF)×(NCP+NUS)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−√.(2)
View Source

In [267], Naish et al. propose two techniques, O and OP (defined as follows). The technique O is designed for programs with a single bug, while OP is better applied to programs with multiple bugs. Data from their experiments suggest that O and OP are more effective than Tarantula, Ochiai, and Ochiai2 for single-bug programs. On the other hand, Le et al. [210] present a different view by showing that Ochiai can be more effective than O and OP for programs with single bugs
Suspiciousness(O)=⟨−1,ifNUF>0NUS,otherwise.(3)
View Source

Table 4 lists 31 similarity coefficient-based techniques, along with their algebraic forms, which have been used in different studies such as [75] , [364], [371]. A few additional techniques using similar approaches can be found in [230]. Tools like Zoltar [170] and DEPUTO [9] are available to compute the suspiciousness with respect to selected techniques.

TABLE 4 Similarity Coefficient-Based Techniques

Empirical studies have also shown that techniques proposed in [366], [367], [369], [370], [371] are, in general, more effective than Tarantula. Especially in the case of DStar [371], results from empirical evaluations against all 31 similarity coefficient-based techniques listed in Table 4 – as well as Tarantula, Ochiai, Ochiai2, Crosstab [369], H3b and H3c [366], and RBF [367] – suggest that DStar outperforms all compared techniques in most cases.

Comparisons among different spectrum-based fault localization techniques are frequently discussed in recent studies [12], [210], [267], [371]. However, there is no technique claiming that it can outperform all others under every scenario. In other words, an optimum spectrum-based technique does not exist, which is supported by Yoo et al.'s study [397] .

A few additional examples of program spectrum-based fault localization techniques are listed below.

Program Invariants Hit Spectrum (PIHS)-based: This spectrum records the coverage of program invariants [107], which are the program properties that remain unchanged across program executions. PIHS-based techniques try to find violations of program properties in failed program executions to locate bugs. Potential invariants [294], also called likely invariants [317], are program properties that are observed to hold in some sets of successful executions but, unlike invariants, may not necessarily hold for all possible executions. The major obstacle in applying such techniques is how to automatically identify the necessary program properties required for the fault localization. To address this problem, existing PIHS-based techniques often take the invariant spectrum of successful executions as the program properties. In study [27], Alipour and Groce propose extended invariants by adding execution features such as the execution count of blocks to the invariants. They claim that extended invariants are helpful in fault localization.

Predicate Count Spectrum (PRCS)-based: PRCS records how predicates are executed and can be used to track program behaviors that are likely to be erroneous. These techniques are often labeled as statistical debugging techniques because the PRCS information is analyzed using statistical methods. Fault localization techniques in this category include Liblit05 [222], SOBER [223] etc. See Section 3.3 for more details. Authors of [266] suggest that using PRCS could achieve a better fault localization effectiveness than that using ESHS.

Method Calls Sequence Hit Spectrum (MCSHS)-based: Information regarding the sequences of method calls covered during program execution is collected. In one study, Dallmeier et al. [84] collect execution data from Java programs and demonstrate fault localization through the identification and analysis of method call sequences. Both incoming method calls (how an object is used) and outgoing calls (how it is implemented) are considered. In another study, Liu et al. [225] construct software behavior graphs from collected program execution data, including the calling and transition relationships between functions. They define a framework to mine closed frequent graphs based on behavior graphs and use them to train classifiers that help identify suspicious functions.

Time Spectrum-based: A time spectrum [396] records the execution time of every method in successful or failed executions. Observed behavior models are created using time spectra collected from successful executions. Deviations from these models in failed executions are identified and ranked as potential causes of failures.

Other program spectra such as those in Table 5 [149] can also be applied to identify suspicious code in a program.

TABLE 5 Additional Program Spectra Relevant to Fault Localization

3.2.3 Issues and Concerns
A variety of issues and concerns about spectrum-based fault localization has also been identified and studied in depth. One problem is that most spectrum-based techniques do not calibrate the contribution of failed and successful tests. In [385], all statements are divided into suspicious and unsuspicious groups. The suspicious group contains statements that have been executed by at least one failed test case, while the unsuspicious group contains the remaining statements. Risk is only calculated for suspicious statements, and unsuspicious statements are simply assigned the lowest value. It is possible, however, that successful test cases may also contain bugs. In [366], Wong et al. focus on the question of how each additional failed or successful test case can aid in locating program bugs. They describe that with respect to a piece of code, the contribution of the first failed test case that executes it in computing its suspiciousness is larger than or equal to that of the second failed test case that executes it, which in turn is larger than or equal to that of the third failed test case that executes it, and so on. This principle is also applied to the contribution provided by successful test cases. In addition, the total contribution from all the successful test cases that execute a statement should be less than the total contribution from all the failed tests that execute it. Recognizing that fault localization often proceeds by comparing information associated with a failed test case to that with a successful test case, Wong and Qi [373] and Guo et al. [140] attempt to answer the question of which successful test case should be selected for comparison, in the interests of more effective fault localization. Choosing the successful test case whose execution sequence is most similar to that of a failed test case, according to a control flow-based difference metric, can minimize the search domain of the fault.

For most spectrum-based techniques, if statements exhibit the same execution pattern, there is a high likelihood that the suspiciousness score assigned to these statements will be exactly the same. Statements with the same suspiciousness will result in ties in the ranking. To break these ties, the information related to statement execution frequency in addition to statement coverage can also be utilized [7], [213]. In [393], Xu et al. evaluate different tie-breaking strategies, including statement order-based strategy, confidence-based strategy, and data dependency-based strategy. Tie-breaking methods will be further discussed in Section 7.6. Another problem is that almost all spectrum-based techniques have assumed that a test oracle exists, which restricts their practical applicability. Thus, Xie et al. [387] propose a fault localization technique based on the integration of metamorphic relations and slices, in which a program execution slice is replaced by a metamorphic slice; an individual test case is replaced by a metamorphic test group; and the success/failure result of a test case is replaced by the violation/non-violation result of a metamorphic test group. Authors of [71] also use metamorphic relations with symbolic testing for program debugging. However, all these techniques rely strongly on the metamorphic relations derived from program specifications. Proper identification of such relations can be not only difficult but also time consuming in practice.

Zhao et al. [421], [422] posit that using only individual coverage information may not reveal the execution paths. Therefore, they first use the program control-flow graph to analyze the program execution and then map the distribution of failed executions to different control flows. They use bug proneness to qualify how each block contributes to the failure and bug free confidence to quantify the likelihood of each block being bug-free by comparing the distributions of blocks on the same failed execution path.

Instrumentation overhead is another issue, which introduces a considerable cost in the fault localization process, especially in a resource-constrained environment. In order to mitigate this problem, Perez et al. [291] propose coined dynamic code coverage by using coarser instrumentation to reduce such overhead. This technique starts by analyzing coverage traces for large components of the program (e.g., package or class) and then progressively increases the instrumentation granularity for possible faulty components until the statement level is reached.

3.3.4 Statistics-Based Techniques
A statistical debugging technique (Liblit05) that can isolate bugs in programs with instrumented predicates at particular points is presented in [222]. For each predicate P , Liblit05 first computes the probability that P being true implies failure, Failure (P), and the probability that the execution of P implies failure, Context(P). Predicates that have Failure(P) – Context(P) ≤ 0 are discarded. The remaining predicates are prioritized based on their importance scores, which give an indication of the relationship between predicates and program bugs. Predicates with a higher score should be examined first. Chilimbi et al. [74] propose that replacing predicates with path profiles may improve the effectiveness of Liblit05. Path profiles are collected during execution and are aggregated across the execution of multiple test cases through feedback reports. The importance score is calculated for each path and the top results are selected and presented as potential root causes.

In [223], Liu et al. propose the SOBER technique to rank suspicious predicates. A predicate P can be evaluated as true more than once in the execution of one test case. Compute π(P)=n(t)n(t)+n(f), the probability that P is evaluated as true in each execution of a test case, where n( t) is the number of times P is evaluated as true and n(f ) is the number of times P is evaluated as false. If the distribution of π(P ) in failed executions is significantly different from that in successful executions, then P is related to a fault. Hu et al. [168] use a similar heuristic to rank all predicates. In addition, they apply non-parametric hypothesis testing to determine the degree of difference between the spectra of predicates for successful and failed test cases. This new enhancement has been empirically evaluated to be effective [416], [420] .

The study in [369] presents a cross tabulation (a.k.a. Crosstab) analysis-based technique to compute the suspiciousness of statements. A crosstab is constructed for each statement with two vertical categories (covered/not covered) and two horizontal categories (successful execution/failed execution). A hypothesis test is used to provide a reference of dependency/independency between the execution results and the coverage of each statement. The exact suspiciousness of each statement depends on the degree of association between its coverage and the execution results.

The primary difference between Crosstab, SOBER, and Liblit05 is that Crosstab can be generally applied to rank suspicious program elements (i.e., statement, predicate, function/method, etc.), whereas the last two only rank suspicious predicates for fault localization. For Liblit05 and SOBER, the corresponding statements of the top k predicates are taken as the initial set to be examined for locating the bug. As suggested by Jones and Harrold in [185], Liblit05 provides no way to quantify the ranking for all statements. An ordering of the predicates is defined, but the approach does not detail how to order statements related to any bug that lies outside a predicate. For SOBER, if the bug is not in the initial set of statements, additional statements have to be included by performing a breadth-first search on the corresponding program dependence graph, which can potentially be time consuming. However, such a search is not required for Crosstab, as all the statements of the program are ranked based on their suspiciousness. Results reported in [369] suggest that Crosstab is almost always more effective in locating bugs in the Siemens suite than Liblit05 and SOBER.

In program execution, short-circuit evaluation may occur frequently, which means, for a predicate with more than one condition, if the first condition suffices to determine the results of the predicate, the following conditions will not be evaluated (executed). Zhang et al. [414], [415] identify the short-circuit evaluations of an individual predicate and produce one set of evaluation sequences for each predicate. Using such information, their proposed Debugging through Evaluation Sequences (DES) approach is compared to existing predicated-based techniques such as SOBER and Liblit05. You et al. [398] propose a statistical approach employing the behavior of two sequentially connected predicates in the execution. They construct a weighted execution graph for each execution of a test case with predicates as vertices and the transition of two sequential predicates as edges. For each edge, a suspiciousness value is calculated to quantify its fault-relevant likelihood. Authors of [38] apply causal-inference techniques to the problem of fault localization. A linear model is built on program control-flow graphs to estimate the causal effect of covering a given statement on the occurrence of failures. This model is able to reduce confounding bias and thereby help generate better fault localization rankings. In [39], they further enhance the linear model toward better fault localization effectiveness by including information on data-flow dependence. In [256], Modi et al. explore the usage of execution phase information such as cache miss rates, CPU and memory usages in statistical program debugging. They suggest coupling execution phases with predicates results in higher bug localization accuracy as opposed to when phase information is not used.

3.3 Program State-Based Techniques
A program state consists of variables and their values at a particular point during program execution, which can be a good indicator for locating program bugs. One way to use program states in software fault localization is by relative debugging [4], in which faults in the development version can be located via a runtime comparison of the internal states to a “reference” version of the program. Another approach is to modify the values of some variables to determine which one causes erroneous program execution. Zeller and Hildebrandt propose a technique, delta debugging [400], [401], by contrasting program states between executions of a successful test and a failed test via their memory graphs which are described in [426]. Variables are tested for suspiciousness by replacing their values from a successful test with their corresponding values from the same point in a failed test, and repeating the program execution. Unless the identical failure is observed, the variable is no longer considered suspicious. Note that the idea of simplifying failure-inducing inputs discussed in [400], [401] is orthogonal to other techniques, as it significantly reduces the original execution length. The delta tool [86] has been widely used in industry for automated debugging. In [77], Cleve and Zeller extend delta debugging to the cause transition technique to identify the locations and times where the cause of a failure changes from one variable to another. An algorithm named cts is proposed to quickly locate cause transitions in a program execution. Similar studies [273], [274] based on combinatorial testing are reported, which separate input parameters into faulty-possible and healthy-possible and identify minimal failure-inducing combinations of parameters.

However, the cause transition technique is a relatively high-cost approach; there may exist thousands of states in a program execution, and delta debugging at each matching point requires additional test executions to narrow down the causes. Another problem is that the identified locations may not be where the bugs reside. Gupta et al. [141] introduce the concept of a failure-inducing chop as an extension to the cause transition technique to overcome this issue. First, delta debugging is used to identify input and output variables that are causes of failure. Dynamic slices are then constructed for these variables. The code at the intersection of the forward slicing of the input variables and the backward slicing of the output variables is considered suspicious.

Sumner et al. further improve the robustness, precision, and efficiency of delta debugging by combining it with more precise execution alignment techniques [338], [339], [389]. However, there are still three limitations to delta debugging: it fails to handle confounding of partial state replacement, it cannot locate execution omission errors, and it suffers from poor efficiency. To address these limitations, Sumner and Zhang [340] propose a cause inference model, comparative causality , to provide a systematic technique explaining the difference between a failed execution and a successful execution.

Predicate switching [405], proposed by Zhang et al., is another program state-based fault localization technique where program states are changed to forcefully alter the executed branches in a failed execution. A predicate which, if switched, can make the program execute successfully is labeled as a critical predicate. The technique starts by finding the first erroneous value in variables. Different searching strategies, such as Last Executed First Switched (LEFS) ordering and Prioritization-based (PRIOR) ordering, can help determine the next candidates for critical predicates. Wang and Roychoudhury [352] present a similar technique that analyzes the execution path of a failed test and alters the outcome of branches in that path to produce a successful execution. The branch statements with outcomes that have been changed are recorded as bugs. A deficiency of predicate switching is that the alternation of program states is never guided by program dependence analysis, even though faults are intrinsically propagated through the chain of program dependences. The study in [217] extends the predicate switching technique and reduces the search space of program states by selecting a subset of trace points in a failed execution based on dependence analysis.

Jeffrey et al. [172] present a value profile-based technique for fault localization to assist developers in software debugging. The approach involves computing Interesting Value Mapping Pairs (IVMPs) that show how values used in particular program statements can be altered so that failed test cases will produce the correct output instead. Alternate sets of values are selected from profiling information taken from the executions of all test cases in an available test suite. Different alternate value sets are used to perform value replacements in each statement instance for every failed test case. Using these IVMPs, each statement can then be ranked according to the number of failed executions in which at least one IVMP is identified for that statement. In [417], Zhang et al. claim that a bug within a statement may propagate a series of infected program states before it manifests the failure. Also, even if every failed execution executes a particular statement, this statement is not necessarily the root cause of the failure. Thus, they use edge profiles to represent program executions and assess the suspiciousness of the infected program states propagated through each edge. By associating basic blocks with edges, a suspiciousness ranking is generated to locate program bugs.

3.4 Machine Learning-Based Techniques
Machine learning is the study of computer algorithms that improve through experience. Machine learning techniques are adaptive and robust and can produce models based on data, with limited human interaction. This has led to their employment in many disciplines such as bioinformatics, natural language processing, cryptography, computer vision, etc. In the context of fault localization, the problem at hand can be identified as trying to learn or deduce the location of a fault based on input data such as statement coverage and the execution result (success or failure) of each test case.

Wong and Qi [374] propose a fault localization technique based on a back-propagation (BP) neural network, one of the most popular neural network models in practice [112]. A BP neural network has a simple structure, which makes it easy to implement using computer programs. Also, BP neural networks have the ability to approximate complicated nonlinear functions [154]. The coverage data of each test case and the corresponding execution result are collected, and they are used together to train a BP neural network so that the network can learn the relationship between them. Then, the coverage of a suite of virtual test cases that each covers only one statement in the program is input to the trained BP network, and the outputs can be regarded as the likelihood of each statement containing the bug. Ascari et al. [36] extend the BP-based technique [374] to object-oriented programs. As BP neural networks are known to suffer from issues such as paralysis and local minima, Wong et al. [367] propose another approach based on radial basis function (RBF) networks, which are less susceptible to these problems and have a faster learning rate [211], [357]. The RBF network is trained using an approach similar to the BP network. Once the training is completed, the output with respect to the coverage of each virtual test case is considered to be the suspiciousness of the corresponding statement. There are three novelties of this approach: 1) a method for representing test cases, coverage information, and execution results within a modified RBF neural network formalism, 2) an innovative algorithm to simultaneously estimate the number of hidden neurons and their receptive field centers, 3) a weighted bit-comparison based distance (instead of the Euclidean distance) to measure the distance between the coverage of two test cases.

In [57] Briand et al. use the C4.5 decision tree algorithm to construct rules that classify test cases into various partitions such that failed test cases in the same partition most likely fail due to the same causative fault. The underlying premise is that distinct failure conditions for test cases can be identified depending on the inputs and outputs of the test case (category partitioning). Each path in the decision tree represents a rule modeling distinct failure conditions, possibly originating from different faults, and leads to a distinct failure probability prediction. The statement coverage of both the failed and successful test cases in each partition is used to rank the statements using a heuristic similar to Tarantula [185] to form a ranking. These individual rankings are then consolidated to form a final statement ranking which can be examined to locate the faults.

3.5 Data Mining-Based Techniques
Along the lines of machine learning, data mining also seeks to produce a model using pertinent information extracted from data. Data mining can uncover hidden patterns in samples of data that may not be discovered by manual analysis alone, especially due to the sheer volume of information. Efficient data mining techniques transcend such problems and do so in reasonable amounts of time with high degrees of accuracy. The software fault localization problem can be abstracted to a data mining problem – for example, we wish to identify the pattern of statement execution that leads to a failure. In addition, although the complete execution trace of a program is a valuable resource for fault localization, the huge volume of data makes it unwieldy for usage in practice. Therefore, some studies have creatively applied data mining techniques to execution traces.

Nessa et al. [269] generate statement subsequences of length N, referred to as N-grams, from the trace data. The failed execution traces are then examined to find the N-grams with a rate of occurrence that is higher than a certain threshold. A statistical analysis is conducted to determine the conditional probability that a certain N-gram appears in a given failed execution trace – this probability is known as the confidence for that N -gram. N-grams are sorted in descending order of confidence and the corresponding statements in the program are displayed based on their first appearance in the list. Case studies on the Siemens suite as well as the space and grep programs have shown that this technique is more effective at locating faults than Tarantula.

Cellier et al. [65], [66] discuss a combination of association rules and Formal Concept Analysis to assist in fault localization. The proposed technique tries to identify rules regarding the association between statement coverage and corresponding execution failures. The frequency of each rule is measured. A threshold is decided upon to indicate the minimum number of failed executions that should be covered by a selected rule. A large number of rules so generated are partially ranked using a rule lattice. The ranking is then examined to locate the fault.

In [403], the authors propose a technique taking advantage of the recent progress in multi-relational data mining for fault localization. More specifically, this technique is based on Markov logic, combining first-order logic and Markov random fields with weighted satisfiability testing for efficient inference and a voted perceptron algorithm for criminative learning. When applied to fault localization, Markov logic combines different information sources such as statement coverage, static program structure information, and prior bug knowledge into a solution to improve the effectiveness of fault localization. Their technique is empirically shown to be more effective than Tarantula on some programs of the Siemens suite.

Denmat et al. [99] propose a technique that re-interprets Tarantula as a data-mining problem. In this technique, association rules that indicate the relationship between a single statement and a program failure are mined based on the coverage information and execution results of a test suite. The relevance values of these rules are evaluated based on two metrics, conf and lift, which are commonly used by classical data mining problems. Such values can be interpreted as the suspiciousness of a statement that may contain bugs.

3.6 Model-Based Techniques
With respect to each model-based technique, a critical concern is the model's expressive capability, which has a significant impact on the effectiveness of that technique.

While using model-based diagnosis [301], it is assumed that a correct model of each program being diagnosed is available. That is, these models can be served as the oracles of the corresponding programs. Differences between the behaviors of a model and the actual observed behaviors of the program are used to help find bugs in the program [249], [250]. On the other hand, for model-based software fault localization [6], [40], [97], [117], [194], [242], [243], [246], [248], [378], [380], [381], [382], models are generated directly from the actual programs, which may contain bugs. Differences between the observed program executions and the expected results (provided by programmers or testers) are used to identify model elements that are responsible for such observed misbehaviors. As demonstrated by the Java Diagnosis Experiments (JADE) in [241], [252], model-based software fault localization can be viewed as an application of model-based diagnosis [14].

Dependency-based models are derived from dependencies between statements in a program, by means of either static or dynamic analysis. Mateis et al. [242] present a functional dependency model for Java programs that can handle a subset of features for the Java language, such as classes, methods, conditionals, assignments, and while-loops. In their model, the structure of a program is described with dependency-based models, while logic-based languages, such as first order logic, are applied to model the behaviors of the target program. This dependency-based model is then extended to handle unstructured control flows in Java programs [244], [245], such as exceptions, recursive method calls, return and jump statements. The notion of a dependence graph has also been extended to model behaviors of a program over a test suite. Baah et al. [40] use a probabilistic program dependence graph to model the internal behaviors of a program, facilitating probabilistic analysis and reasoning about uncertain program behaviors, especially those that are likely associated with faults.

Wotawa et al. [380] use first order logic to construct dependency-based models based on source code analysis of target programs to represent program structures and behaviors. Test cases with expected outputs are also transformed into observations in terms of first order logic. If the execution of a target program on a test case fails, conflicts between the test case and the models (which can be shown as equivalent to either static or dynamic slices [378]) are used to identify suspicious statements responsible for the failure. For each statement, a default assumption is made to suggest whether the statement is correct or incorrect. These assumptions are to be revised during fault localization until the failure can be explained. The limitation is that their study only focuses on loop-free programs. To fix this problem, Mayer and Stumptner [246] propose an abstraction-based model in which abstract interpretation [55], [78] is applied to handle loops, recursive procedures, and heap data structures. Additionally, abstract interpretation is used to improve the effectiveness of slice-based and other model-based fault localization techniques [247].

In addition to dependency-based and abstraction-based models, value-based models [196], [251] that represent data-flow information in programs are also applied to locate components that contain bugs. However, value-based models are more computationally intensive than dependency-based and are only practical for small programs [250].

We now discuss model checking-based fault localization techniques that rely on the use of model checkers to locate bugs [44], [67], [133], [134], [136], [137], [138], [200]. If a model does not satisfy the corresponding program specifications (implying that the model contains at least one bug), a model checker can be used to provide counter-examples showing how the specifications will be violated. A counter-example does not directly specify which parts of a model are associated with a given bug; however, it can be viewed as a failed test case to help identify the causality of the bug [135].

Ball et al. [44] propose to use a model checker to explore all program paths except that of the counter-example. Successful execution paths (those that do not cause a failure) are recorded. An algorithm is used to identify the transitions that appear in the execution path of the counter-example but not in any successful execution paths. Program components related to these transitions are those that are likely to contain the causes of bugs. This technique suffers from two weaknesses. First, as suggested by Groce and Visser [136], generating all successful execution paths can be very expensive. Second, only one counter-example is used to locate bugs, even though the same bug may be triggered by multiple counter-examples. If this occurs, using only one example can introduce possible bias. To overcome these problems, Groce and Visser [136] generate a small number of executions by exploring backwards from the original counter-example using a model checker. Additional executions so generated may or may not cause a failure. They then analyze the differences (in terms of transitions, invariants, and transformations) between failed and successful executions to identify possible locations of bugs.

Inspired by Lewis’ counterfactual reasoning [216], Groce et al. [135], [137] represent program executions as sets of assignments to variables. They then define a distance metric to measure the distance between two program executions. Based on this metric, a model checker is used to generate one successful execution which is closest to the counter-example. The differences between the successful execution and the counter-example provide the possible explanations and locations of bugs. A tool, explain [138], is used to implement their technique. Chaki et al. further extend the technique of Groce et al. by combining it with predicate abstraction [67].

Techniques such as [44], [67], [135], [136], [137], [138] require at least one successful execution. Griesmayer et al. [133], [134] argue that a successful execution path can be very different from the path of the counter-example and cannot be easily identified using the above techniques. Instead of searching for successful execution paths with small changes from that of the original counter-example, they make minimal changes to the program so that the counter-example will not fail in the revised program. Assuming there is only one bug in one program component, Griesmayer et al. propose a technique with two steps: 1) revising the program specification in such a way that if any one component in the original program is changed, then the original specification cannot be satisfied, and 2) creating variants of the original program such that each variant has exactly one component replaced by a different component with an alternative behavior. For each variant, if a model checker can find a counter-example violating the revised specification, then the replaced component is potentially responsible for the failure. Since more than one component may be responsible for the failure, programmers have to manually inspect these components to identify the one containing the bug. Experiments in [133] use the model Checker CBMC, whereas extended studies using an additional model checker SATABS are reported in [134].

Based on a similar idea described in [133], [134], Könighofer and Bloem [200] use symbolic execution to locate bugs for imperative programs. An important point stated by Griesmayer [134] is that the extensive use of a model checker makes their techniques less efficient (in terms of time) than those in [44], [67], [136], [137], [138]; however, fault localization using model checkers can be used to refine results from less precise techniques.

Last but not least, the idea of modifying a program so that test cases that fail on the original program can be executed successfully on the modified program [133], [134], [200] is also used in other studies for automatic bug fixing [95], [152], [189], [270].

Additional model-based fault localization techniques also exist. They can be applied to functional programs [336], hardware description languages like VHDL [290], [376], and spreadsheets [163], [169]. Studies such as [272], [377] make use of constraint solving, in which programs are automatically compiled into a set of constraints. In [97], DeMillo et al. propose a model for analyzing software failures and faults for debugging purposes. Failure modes and failure types are defined to identify the existence of program failures and to analyze the nature of program failures, respectively. Failure modes are used to answer the question “How do we know the execution of a program fails?” and failure types are used to answer the question “What is the failure?” When abnormal behavior is observed during program execution, the failure is classified by its corresponding failure mode. Referring to some pre-established relationships between failure modes and failure types, certain failure types can be identified as possible causes for the failure. Heuristics based on dynamic instrumentation (such as dynamic slice) and testing information are then used to reduce the search domain for locating the fault by predicting possible faulty statements. A significant drawback of using this model is that it is extremely difficult, if not impossible, to obtain an exhaustive list of failure modes because different programs can have very different abnormal behaviors and symptoms when they fail. As a result, we do not have a complete relationship between all possible failure modes and failure types, and we might not be able to identify possible failure types responsible for the failure being analyzed.

3.7 Additional Techniques
In addition to those discussed above, there are other techniques for software fault localization. Many of them focus on specific program languages or testing scenarios. Listed below are a few examples.

Development of software systems, while enhancing functionality, will inevitably lead to the introduction of new bugs, which may not be detected immediately. Tracing the behavior changes to code changes can be highly time-consuming. Bohnet et al. [54] propose a technique to identify recently introduced changes. Dynamic, static, and code change information is combined to reduce the large number of changes that may have impact on faulty executions of the system. In this way, root cause changes can be semi-automatically located.

In spite of using garbage collection, Java programs may still suffer from memory leaks due to unwanted references. Chen and Chen [69] develop an aspect-based tool, FindLeak, utilizing an aspect to gather memory consumption statistics and object references created during a program execution. Collected information is then analyzed to help detect memory leaks.

An implicit social network model is presented in [70] to predict possible locations of faults using fault locations cited by similar historical bug reports retrieved from BRMS (bug report managing systems).

In [88], de Souza and Chaim propose a technique using integration coverage data to locate bugs. By ranking the most suspicious pairs of method invocations, roadmaps, which are sorted lists of methods to be investigated, are created.

Gong et al. [123] propose an interactive fault localization technique, TALK, which incorporates programmers’ feedback into spectrum-based fault localization techniques. Each time a programmer inspects a suspicious program element in the ranking generated by a fault localization technique, he or she can judge the correctness of the element and provide this information as feedback to re-order the ranking of elements that are not yet inspected. The authors demonstrate that using programmers’ feedback can help increase the effectiveness of existing fault localization techniques.

To better understand a program's behavior, software developers must translate their questions into code-related queries, speculating about the causes of faults. Whyline [195] is a debugging tool that avoids such speculation by enabling developers to select from a set of “why did” and “why didn't” questions derived from source code. Using a combination of static and dynamic slicing, and precise call graphs, the tool can find possible explanations of failures.

Authors of [72] propose a software fault localization technique that mines bug signatures within a program. A bug signature is a set of program elements that are executed by most failed tests but not by successful tests in general. Bug signatures are ranked in descending order by a discriminative significance score indicating how likely it is to be related to the bug. This ranking is used to help identify the location of the bug.

Maruyama et al. [238] indicate that the culprit of an overwritten variable is always the last write-access to the memory location where the bug first appeared. Removing such bugs begins with finding the last write, followed by moving the control point of execution back to the time when the last write was executed. Generally, the statement that makes the last write will be faulty.

Recently, some studies [85], [234] , [298], [315], [425] have applied information retrieval techniques to software fault localization. These studies use an initial bug report to rank the source code files in descending order based on their relevance to the bug report. The developers can then examine the ranking and identify the files that contain bugs. Unlike spectrum-based fault localization techniques, information retrieval-based techniques do not require program coverage information, but their generated ranking is based solely on source code files rather than on program elements with finer granularity such as statements, blocks, or predicates.

Algorithmic debugging (also called declarative debugging), first discussed in Shapiro's dissertation [325] with more details in [328], [402], decomposes a complex computation into a series of sub-computations to help locate program bugs. The outcome of each sub-computation is checked for its correctness with respect to given input values. Based on this, an algorithmic debugger is used to identify a portion of code that may contain bugs. One issue of applying this technique in practice is that testing oracles may not available for sub-computations.

Formula-based fault localization techniques [76], [109], [180], [181] rely on an encoding of failed execution traces into error trace formulae. By proving the unsatisfiability of an error trace formula using certain tools or algorithms, the programmer may capture the relevant statements causing the failure. Jose and Majumdar [180], [181] propose a technique, BugAssist, which uses a MAX-SAT solver to compute the maximal set of statements that may cause the failure from a failed execution trace. In [109], Ermis et al. introduce error invariants, which provide a semantic argument as to why certain statements of a failed execution trace are irrelevant to the root cause of the failure. By removing such statements, the bug can be located with less manual effort. A common weakness of these techniques [109], [180], [181] is that they only report a set of statements that may be responsible for the failure without providing the exact input values that make the executions go to those statements. Christ et al. [76] address this problem by reporting an extended study based on error invariants [109] that encodes a failed execution trace into a flow-sensitive error trace formula. In addition to providing a set of statements that are relevant to the failure, they also specify how these statements can be executed using different input values.

During program maintenance, source code may be modified to fix bugs or enhanced to support new functionalities. Regression testing is also conducted to prevent invalidation of previously tested functionality. If an execution fails, the programmer needs to find the failure-inducing changes. Crisp [302] is a tool to build a compliant intermediate version of the program by adding a partial edit (i.e., a subset of recent changes) to the code before the maintenance is performed. This tool helps programmers focus on a specific portion of changes in the code during the debugging.

Concurrent programs are becoming more prevalent in applications that affect our everyday lives. However, due to their non-determinism, it is very difficult to debug these programs. It is proposed that injecting random timing noise into many points within a program can assist in eliciting bugs. Once the bug is triggered, the objective is to identify a small set of points that indicate the source of the bug. In [398] , the authors propose an algorithm that iteratively samples a lower dimensional projection of the program space and identifies candidate relevant points. Refer to Section 7.7 for more discussion.

3.8 Distribution of Papers in Our Repository
Fig. 3 shows the distribution of papers in our repository across all categories. Spectrum-based is the most dominant category with 35 percent of all the papers 4 followed by slice-based, which contains 20 percent, and model-based, which contains 19 percent. The number of papers in each of the statistics-based, program state-based, and others categories is between 7 and 9 percent. The data-mining and machine learning-based categories have the fewest number of papers with only 1 and 2 percent.


Fig. 3.
Distribution of papers in our repository.

Show All

Below we present the distribution using a different classification: static and dynamic slice-based, execution slice and program spectrum-based, and other techniques (see Footnote 4 for the rationale). Fig. 4 gives the number of papers published each year with respect to this new classification. The first (leftmost) bar gives the total number of papers from 1977 to 1995, the last (rightmost) only counts papers between January and November 2014, and those in between give the number in the corresponding year. Fig. 5 displays the information from a cumulative point of view. Each data point gives the cumulative number of papers published up to the corresponding year. From these two figures, we make the following observations:


Fig. 4.
Number of papers published each year with respect to three different categories.

Show All


Fig. 5.
Cumulative number of papers with respect to three different categories.

Show All

Static and dynamic slice-based techniques were popular between 2002 and 2007. However, the number of papers each year in this category has decreased since then.

The number of papers on execution slice and program spectrum-based techniques has increased dramatically since 2008, indicating that more studies are focused on these techniques rather than static or dynamic slice-based techniques in the recent years.

SECTION 4Subject Programs
Table 6 presents a list of popular subject programs used to study the effectiveness of different fault localization techniques. This table gives the name, the size (lines of code), a brief description of the functionality, the programming language, and the number of papers that use this program.

TABLE 6 Summary of Popular Subject Programs Used in the Fault Localization Studies

We notice that the Siemens suite is the most frequently used. However, every program in the suite is very small-sized with less than 600 lines of code (not including blank lines). Another important point worth noting is that most of the bugs used in the experiments are mutation-based artificially injected bugs. Although mutation has been shown to be an effective approach to simulate realistic faults [29], [103], [223], [268], some real-life bugs are very delicate and cannot be modeled by simple first-order mutants.

With the introduction of advanced techniques in software fault localization, more accurate cross comparisons of their effectiveness are in demand. Furthermore, the feasibility of a technique and the benefits of using it should be demonstrated in an industry-like environment, in contrast to an academic laboratory-oriented controlled environment. In response to these challenges, more and more studies use larger and complex programs in their experiments. Another trend is to use bugs actually introduced at the development phase such as those from Bugzilla for the gcc program and the bugs for Mozilla firefox.

SECTION 5Evaluation Metrics
Since a program bug may span multiple lines of code, which are not necessarily contiguous or in the same module, the examination of suspicious code stops as long as one faulty location is identified. This is because the focus is to help programmers find a good starting point to initiate the bug-fixing process rather than to provide the complete set of code that must be modified, deleted, or added with respect to each bug. With this in mind, the effectiveness of a software fault localization technique is defined as the percentage of code5 that needs to be examined before the first faulty location for a given bug is identified.

The T-score [223], [303] estimates the percentage of code a programmer need not examine before the first faulty location is found. A program dependence graph is constructed, and the nodes are marked as faulty if they are reported by differencing the correct and the faulty versions of the program, and blamed if they are reported by the localizer. For a node n, the corresponding k-dependency sphere set ( DSk) is the set of nodes for which there is a directed path of length no more than k that joins n and them. For example, DS0 contains the node n itself. DS1 includes not only n but also all the nodes such that there is an edge from them to n, or from n to them. For a report R (i.e., a set of nodes the localizer indicates as possible locations of the bug), let DS*(R) be the smallest dependency sphere that includes a faulty node. The T-score of a given R is computed using the ratio of the number of nodes in its smallest dependency sphere to the number of nodes in the entire PDG:
T-score=1−|DS∗(R)||PDG|.(4)
View Source

The use of T-score requires that programmers are able to distinguish defects from non-defects at each location and can do so at the same cost for each location considered [77]. Furthermore, it assumes that programmers can follow the control- and/or data-dependency relations among statements while searching for faults.

The EXAM [188], [366], [367], [369], [374] or Expense [185] score is the percentage of statements in a program that has to be examined until the first faulty statement is reached:
EXAMscore=Number of statements examinedTotal number of statements in the program×100%.(5)
View Source

In [185], the authors use the executable statements instead of the total number of statements. For techniques such as [224] that generate a ranking of predicates (instead of statements) sorted in descending order of their fault relevance, the EXAM score can also be computed in terms of percentage of predicates that need to be examined. The P-score [420], defined as follows, uses the same approach:
P-score=1−basedindexofPinLnumberofpredicatesinL×100%,(6)
View Sourcewhere L is a list of sorted predicates as described above, P is the most fault-relevant predicate to a fault, and the notation of 1-based index means the first predicate of L is indexed by 1 (rather than 0). Studies in [366], [367], [369], [371], [374] also provide figures that report the percentage of all the faulty versions of a given program in which faults can be located by the examination of an amount of code less than or equal to a given EXAM score. A similar idea is subsequently used by Gong et al. to define the N-score [124]:
N-score=NdetectedNstatistic×100%.(7)
View Source

When compared to T-score, EXAM is easier to understand, as it is directly proportional to the amount of code to be examined rather than to an indirect measurement in terms of the amount of code that does not need to be examined (as what T-score does). In summary, the lower the EXAM score (or Expense or P-score), the more effective the technique, whereas it is the opposite for the T-score (i.e., the lower the T-score, the less effective the technique).

The Wilcoxon signed-rank test (an alternative to the paired Student's t-test when a normal distribution of the population cannot be assumed) can also be used as a metric to present an evaluation from a statistical point of view [370], [371]. If we assume a technique α is more effective than another technique β, we examine the one-tailed alternative hypothesis that β requires the examination of an equal or greater number of statements than α. The confidence with which the alternative hypothesis can be accepted helps us determine whether α is statistically more effective than β. Another metric is the total (cumulative) number of statements that need to be examined to locate all bugs of a given scenario [366], [367], [369], [371]. This metric gives a global view in contrast to the Wilcoxon test, which focuses more on individual pairwise comparisons.

An effective fault localization technique should assign a unique suspiciousness value to each statement; in practice, however, the same suspiciousness may be assigned to different statements. If this happens, two different levels of effectiveness result: the best and the worst. The best effectiveness assumes that the faulty statement is the first to be examined among all the statements of the same suspiciousness. The worst effectiveness occurs if the faulty statement is the last to be examined. Reporting only the worst case (such as [28], [165]) or only the best case (such as the P-score in [420]) may not give the complete picture because it is very unlikely that programmers will face the worst or the best case scenario in practice. In most cases, they will see something between the best and the worst. It is straightforward to compute the average effectiveness from the best and worst effectiveness. However, the converse is not true. Providing the average effectiveness offers no insights on where the best and worst effectiveness may lie, and, more seriously, can be ambiguous and misleading. For example, two techniques can have the same average effectiveness, but one has a smaller range between the best and the worse cases while the other has a much wider range. As a result, these two techniques should not be viewed as equally effective as suggested by their average effectiveness. Thus, a better approach is to report the effectiveness for both the best and the worst cases such as [366], [367] , [369], [374] and perform the cross evaluation under each scenario.

All the evaluation metrics discussed above are based on an assumption of perfect bug detection, which is the same as having an ideal user [303] to examine suspicious code to determine whether it contains bugs. That is, a bug in a statement will be detected if the statement is examined. However, a recent study [285] indicates that such an assumption does not always hold in practice. If so, then the number of statements that need to be examined to find the bug may increase.

There are other factors that may affect the effectiveness of a software fault localization technique. Bo et al. [53] present a metric, Relative Expense, to study the impact of test set size on the Expense score. More discussion regarding the impact of test cases on fault localization appears in Section 7.2. Monperrus [258] suggests that effectiveness should be evaluated with respect to different classes of faults. It is possible that one technique is more effective than another for bugs that can be triggered consistently under some well-defined conditions (namely, Bohrbugs in [139]), but less effective for bugs whose failures cannot be systematically reproduced (namely, Mandelbugs). Instrumentation overhead, interference within multiple bugs, and programming language also have an impact on effectiveness of fault localization [90], [333].

Last but not least, it is important to realize that software fault localization techniques should not be evaluated only in terms of effectiveness as described above [285]. Other factors such as computational overhead, time and space for data collection, amount of human effort, and tool support need also be considered. In addition, we also need to emphasize user-centered aims such as how programmers actually debug, how they reveal the cause-effect chains of failures, and how they decide upon solutions beyond a suspiciousness ranking of code. Unfortunately, none of the published studies has reported a comprehensive evaluation covering all these aspects.

SECTION 6Software Fault Localization Tools
One challenge for many empirical studies on software fault localization is that they require appropriate tool support for automatic or semi-automatic data collection and suspiciousness computation. Table 7 gives a list of commonly used tools, including name, a brief description, availability, and which papers use the tool. Of the 63 tools, two are commercial, 16 are open source, 10 are openly accessible but the source code is not available, and the rest may be acquired by contacting their authors.

TABLE 7 Summary of Tools Used in the Fault Localization Studies

SECTION 7Critical Aspects
In this section, we explore some critical aspects of software fault localization.

7.1 Fault Localization with Multiple Bugs
The majority of published papers in software fault localization focus on programs with a single bug (i.e., each faulty program has exactly one bug). However, this is not the case for real-life software, which in general contains multiple bugs. Results of a study [143] based on an analysis of fault and failure data from two large, real-world projects show that individual failures are often triggered by multiple bugs spread throughout the system. Another study [231] also reports a similar finding. This observation raises doubts concerning the validity of some heuristics and assumptions based on the single-bug scenario. In response, studies have been conducted using programs with multiple bugs [87], [90], [101], [102], [125], [172], [173], [184], [224], [293], [331], [332], [359], [395], [423].

A popular assumption is that multiple bugs in the same program perform independently. Debroy and Wong [90] examine possible interactions that may take place between different bugs, and they find that such interferences may manifest themselves to either trigger or mask some execution failures. Results based on their experiments indicate that destructive interference (when execution fails due to a bug but no longer fails when another bug is added to the same program) is more common than constructive interference (when execution fails in the presence of two bugs in the same program but does not in the presence of either bug alone) because failures are masked more often than triggered by additional bugs. It is also possible that a program with multiple bugs suffers from both destructive and constructive interferences. DiGiuseppe and Jones [102] also report that multiple bugs have an adverse impact on the effectiveness of spectrum-based techniques.

One way to debug a multiple-bug program is to follow the one-bug-at-a-time approach. If a program experiences some failures while it is executed against test cases of a given test suite, this approach helps programmers find and fix a bug. Then, the modified program is tested again using all the test cases in the given test suite. If any of the executions fail, additional debugging is required to find and fix the next bug. This process continues until no failure is observed. At this point, even though the program may still contain other bugs, they cannot be detected by the current suite of test cases. This approach has been adopted in studies using the DStar technique [371] and a reasoning fault localization technique based on a Bayesian reasoning framework [14]. A potential weakness of most techniques based on Bayesian reasoning (e.g., [14], [64], [193]) is that they all assume program components fail independently; in other words, interferences between multiple bugs are ignored, which is not necessarily the case in practice.

In [184], Jones et al. suggest that multiple bugs in a program can be located in parallel. The first step is to group failed test cases into different fault-focusing clusters such that those in the same cluster are related to the same bug. Then, the Tarantula fault localization technique [185], failed tests in each cluster, and all the successful tests are used to identify the suspicious code for the corresponding bug.

There are different ways to cluster failed test cases. One approach is to use execution profiles. Podgurski et al. [293] apply supervised and unsupervised pattern classifications as well as multivariate visualization to execution profiles of failed test cases in order to group them into fault-focusing clusters. Steimann and Frenkel [332] use the Weil-Kettler algorithm, a technique widely used in integer linear programming, to cluster failed test cases.

However, clustering based on the similarity between execution profiles may not reflect an accurate causation relationship between certain faults and the corresponding failed executions. For example, two failed tests, even associated with the same bug, may have very different execution profiles. It is possible for clustering techniques based on execution profiles to separate these two failed tests into different clusters.

To overcome this problem, Liu and Han [173], [226] further investigate the due-to relationship between failed tests and underlying bugs. They apply SOBER [172] to each failed test case and all the successful tests to generate a corresponding predicate ranking. The weighted Kendall tau distance is computed between these rankings. The distance between two rankings is small if they identify similar suspicious predicates. It also implies the rank-proximity (R-proximity) between them is high. Failed test cases with high R-proximity are clustered together, as they are likely to have the same due-to relationship.

Other variations include the use of more effective fault localization techniques (such as Crosstab [369], RBF [367], and DStar [371]) instead of Tarantula or SOBER, or using only a subset (refer to Section 7.2), rather than all, of the successful tests. These variations are yet to be explored.

7.2 Inputs, Outputs and Impact of Test Cases
In addition to failed and successful test cases, many (although not all) techniques discussed in Section 3 also need information about how the underlying program/model is executed with respect to each test case. Such details can be provided via different execution profiles (e.g., coverage in terms of statement, predicate, etc.).

The output of many spectrum-based (Section 3.2) fault localization techniques (such as Tarantula) is a suspiciousness ranking with statements ranked in descending order of their suspiciousness values (such as the rightmost column of Table 3). To locate a bug, programmers will examine statements at higher positions of a ranking before statements at lower positions because the former, with higher suspiciousness values, are more likely to contain bugs than the latter. On the other hand, many slice-based techniques (Section 3.1) only return a set of statements without specific ranking. Referring to Table 2, the static slice for the variable product is a set of eight statements, including s1, s2, s4, s5, s7, s8, s10, and s13. However, it does not tell programmers which statements are more likely to contain bugs and should therefore be examined first for possible bug locations.

Techniques discussed in Sections 3.3 (statistics-based), 3.5 (machine learning-based) and 3.6 (data mining-based) are likely6 to generate outputs in terms of suspiciousness rankings similar to those generated by the spectrum-based techniques, whereas program state-based (Section 3.4) and model-based ( Section 3.7) techniques are more likely to output a set of program/model components that will possibly contain bugs but do not explicitly specify the ranking of each component. Although both types of outputs provide suspicious components (statements, predicates, etc.) to help locate bugs, the former further prioritizes these components based on their suspiciousness values, but the latter does not.

The suite of test cases used in the program debugging is another important factor that may affect the effectiveness of a fault localization technique. Some fault localization techniques (e.g., [21] , [77], [133], [134], [140], [303], [400]) focus on locating program bugs using either a single failed test case or a single failed test case with a few successful test cases. Others (e.g., [185], [222], [223], [366], [367], [369], [373], [374], [375]) use multiple failed and successful test cases. These latter techniques take advantage of more test cases than the former, so it is likely that the latter are more effective in locating program bugs. For example, Tarantula [185] which uses multiple failed and multiple successful tests, has been shown to be more effective than nearest neighbor [303], a technique that only uses one failed and one successful test. However, it is important to note that by considering only one successful and one failed test, it may be possible to align the two test cases and arrive at a more detailed root-cause explanation of the failure [77] when compared to the techniques that take into account multiple successful and failed test cases simultaneously.

Although techniques using multiple failed and multiple successful test cases may have better fault localization effectiveness, an underlying assumption is that a large set of such tests is available. This may also lead to the assumption of existence of an oracle that can be used to automatically determine whether an execution is successful or failed. Unfortunately, this may not be true in the real world, as a test oracle can be incomplete, out-of-date, or ambiguous. Studies such as [160], [161] have reported that for many systems and for much of testing as currently practiced in industry, testers do not have formal specifications, assertions, or automated oracles. As a result, they face the potentially daunting task of manually checking the system's behavior for all test cases executed. In response to this challenge, researchers have presented various solutions [16], [147], [148]. Nevertheless, how to generate an automated test oracle still remains an issue that needs to be further explored. Hence, we cannot take it for granted that there are multiple tests with all execution results (success or failure) known.

Using a test suite that does not achieve high coverage of the target program may have an adverse impact on the fault localization results. During test generation, different criteria (e.g., requirements-based boundary value analysis, or white-box-based statement or decision coverage) can be used as guidance. Diaz et al. [100] use a meta-heuristic technique (a so-called Tabu Search approach) to automatically generate a test suite to obtain maximum branch coverage. In [33] , [34], [35], Artzi et al. present a tool called Apollo to generate test cases automatically based on combined concrete and symbolic executions. Apollo first executes a program on an empty input and records a path constraint that reflects the program's executed control-flow predicates. New inputs are then generated by changing predicates in the path constraint and solving the resulting constraints. Executing the program on these inputs produces additional control-flow paths. Failures observed during executions are recorded. This process is repeated until a pre-defined threshold of statements coverage is reached, a sufficient number of faults are detected, or the time budget is exhausted. Authors of [176] suggest that test suites satisfying branch coverage are better than those satisfying statement coverage in effectively supporting fault localization, whereas authors of [177] claim that test suites satisfying MC/DC coverage are better than those satisfying branch coverage.

Furthermore, in [320], Santelices et al. study the fault localization effectiveness of Tarantula using three types of program coverage—statements, branches, and define-use pair. They conclude that Tarantula using define-use pair coverage is more effective and stable than that using branch coverage, which is more effective than that using statement coverage. Based on this, the authors further propose to use a combination of the three types of coverage to achieve better fault localization effectiveness.

Some researchers argue that it is not efficient to use all the test cases in a given test suite to locate program bugs. Instead, they use either test case reduction by selecting only a subset of test cases or test case prioritization by assigning different priorities to different cases to improve the efficiency of fault localization techniques [45], [48], [53], [62], [63], [122], [126], [127], [128], [146], [175], [176], [348], [362], [399]. One approach of test prioritization is to give higher priority to failed test cases that execute fewer statements, as they provide more information and minimize the search domain [263]. In [119], the authors propose an approach to generate balanced test suites in order to improve fault localization effectiveness by cloning failed test cases a suitable number of times to match the number of successful test cases. Rößler et al. [307] propose a technique, BUGEX, which applies dynamic symbolic execution to generate test cases with a minimal difference from the execution path of a single failed test case. Based on the generated test cases, the branches that are executed by more failed test cases but fewer successful test cases are more likely to cause the failure. The study in [179] applies a similar test case generation approach, but the generated test cases are instead used with a spectrum-based fault localization technique to rank basic blocks in descending order according to their suspiciousness values.

Baudry et al. [50] use a bacteriological approach (which is an adaptation of genetic algorithms) to bridge the gap between testing and diagnosis (fault localization) based on a test-for-diagnosis criterion. Test cases are generated to satisfy this criterion so that diagnosis algorithms can be used more efficiently. Their objective is to achieve a better diagnosis (a more efficient fault localization) using a minimal number of test cases. Studies such as [127], [253] focus on a cross evaluation of the impact of different test reduction and prioritization techniques on the efficiency of software fault localization.

Test execution sequence also has an impact on program debugging. For example, it is possible that a program execution fails not because of the current test but because of a previous test that does not set up an appropriate execution environment for the current test. If a failure cannot be observed unless a group of test cases are executed in a specific sequence, then these test cases should be bundled together as one single failed test.

7.3 Coincidental Correctness
The concept of coincidental correctness, introduced by Budd and Angluin in [59], discusses the circumstances under which a test case produces one or more errors in the program state but the output of the program is still correct. This phenomenon can occur for many reasons. For example, given a faulty statement in which a variable is assigned with an incorrect value, in one test execution, this value may affect the output of the program and result in a failure. However, in another test execution, the value of this variable is later overwritten. Thus, the output of the program is not affected and failure is not triggered. Studies discussing coincidental correctness have been reported in recent years [44], [46], [159], [218], [239], [254], [355], [419].

Coincidental correctness can negatively impact the effectiveness of fault localization techniques. Ball et al. [44] claim that this is the reason why their technique fails to locate bugs in three out of 15 single-bug programs. Wang et al. [355] conclude that the effectiveness of Tarantula decreases when the frequency of coincidental correctness is high and increases when the frequency is low.

To overcome this problem, Masri and Assi [239] propose a technique to clean test suites by removing test cases that may introduce possible coincidental correctness for better fault localization effectiveness. Their technique is further enhanced by using fuzzy test suites and clustering analysis [240]. Bandyopadhyay and Ghosh [46] suggest a different approach by first measuring the likelihood of coincidental correctness of a successful test case based on the average proximity of its execution profile with that of all failed test cases. Such likelihood is assigned as the weight of the corresponding successful test case and used for subsequent suspiciousness computation. Zhang et al. [419] present FOnly, a technique that relies only on failed test cases to locate bugs statistically, even though fault localization commonly relies on both successful and failed tests. Authors of [418] propose a fault localization technique, BlockRank, to calculate, contrast, and propagate the mean edge profiles between successful and failed executions to alleviate the impact of coincidental correctness.

7.4 Faults Introduced by Missing Code
One claim that can generally be made against fault localization techniques discussed in this survey is that they are incapable of locating bugs resulting from missing code. For example, slice-based techniques will never be able to locate such bugs – since the faulty code is not even in the program. Therefore, this code will not appear in any of the slices. Based on this, one might conclude that most fault localization techniques are inappropriate for locating such bugs. Although this argument seems to be reasonable, it overlooks some important details. Admittedly, the missing code cannot be found in any of the slices. However, the omission of the code may trigger some adverse effects elsewhere in the program execution, such as the traversal of an incorrect branch in a decision statement. An abnormal program execution path (and, thus, the appearance of unexpected code in the corresponding slice) with respect to a given test case should hint to programmers that some omitted statements may be leading to control-flow anomalies. This implies that we are still able to identify suspicious code related to the omission error, such as the affected decision branch using slice-based techniques. A similar argument can also be made for other techniques, including but not limited to program spectrum-based (Section 3.2), statistics-based (Section 3.3), and program state-based techniques (Section 3.4). Thus, even though software fault localization techniques may not be able to pinpoint the exact locations of missing code, they can still provide a good starting point for the search.

7.5 Combination of Multiple Fault Localization Techniques
The effectiveness of a fault localization technique is very much scenario dependent, affected by successful and failed test cases, program structures and semantics, nature of the bugs, etc. There is no single technique superior to all others in every scenario. Thus, it makes sense to combine multiple techniques and retain the good qualities of individual techniques while mitigating the drawbacks of each. In [91], [92], Debroy et al. propose a way to do so by combining the rankings of statements generated by multiple techniques. The advantage of this approach (i.e., combining the rankings) over a design-based integration approach (in which the actual techniques would somehow be incorporated to form a new technique) is that it is more cost-effective to realize and is always extensible. Based on a similar idea, Lucia and Xia [232] use two normalization methods to combine results of different fault localization techniques.

In [9], Abreu et al. address the inherent limitations of spectrum-based fault localization techniques, stating that component semantics of the program are not considered. They propose a way to enhance the diagnostic quality of a spectrum-based fault localization technique by combining it with a model-based debugging approach using the abstraction interpretation generated by a framework called DEPUTO. More precisely, a model-based approach is used to refine the ranking via filtering to exclude those components that do not explain the observed failures when the program's semantics are considered.

In [350], Wang et al. use two different search algorithms, simulated annealing and genetic algorithm, to find approximate optimal compositions from 22 existing spectrum-based fault localization techniques. However, a search-based approach lacks flexibility and efficiency. For flexibility, the search must be re-performed to update the optimal composition whenever a new fault localization technique is included. Also, an optimal composition for one program may not be the optimal for another program, which means the search process needs to be re-performed when the subject program changes. For efficiency, the potential large size of search space makes the search process very time consuming.

Spectrum-based and slice-based techniques are both widely used. Combinations between techniques from these two categories have been reported [28], [165] , [214], [363]. For example, in [28], Alves et al. combine Tarantula and dynamic slicing to improve fault localization effectiveness. First, all the statements in a program are ranked based on their suspiciousness calculated by using the Tarantula technique. Then, a dynamic slice with respect to a failure-indicating variable at the failure point is generated. Statements not in this slice will be removed from the ranking to further reduce the search domain. In [188], Ju et al. propose a hybrid slice-based fault localization technique combining dynamic and execution slices. A prototype tool, hybrid slice spectrum fault locator ( HSFal), is implemented to support this technique.

Hofer and Wotawa [165] emphasize that spectrum-based fault localization techniques (e.g., Ochiai [12]) operated at a basic block level do not provide fine-grained results, whereas techniques based on slicing-hitting-set-computation (e.g., the HS-Slice algorithm [379]) sometimes produce an undesirable ranking with statements (such as constructors), which are executed by many test cases, at the top. To eliminate these drawbacks, techniques of these two types should be combined.

Other combinations have also been explored. In [33], Artzi et al. combine Tarantula and a technique for output mapping to reduce the number of statements that need to be examined. A similar approach is repeated in which Tarantula is replaced by Ochiai and Jaccard [34]. In [129], Gopinath et al. apply spectrum-based localization in synergy with specification-based analysis to more accurately locate bugs. The key idea is that unsatisfiability analysis of violated specifications, enabled by SAT technology, can be used to compute unsatisfiable cores, including statements that are likely to contain bugs. In [61], Burger and Zeller propose a technique, JINSI, which combines delta debugging and dynamic slicing for effective fault localization. JINSI takes a single failed execution and treats it as a series of object interactions (e.g., method calls and returns) that eventually produce the failure. The number of interactions will be reduced to the minimum number required to reproduce the failure, which will reduce the search space needed to locate the corresponding bug.

7.6 Ties within Fault Localization Rankings
As discussed earlier (referring to Section 3.2), statements with the same suspiciousness are tied for the same position in a ranking. Results of a study by Xu et al. [393], using three fault localization techniques on four sets of programs, show that the symptom of assigning the same suspiciousness to multiple statements (i.e., the existence of ties in a produced ranking) appears everywhere and is not limited to any particular technique or program. Under such a scenario, the total number of statements that a programmer needs to examine in order to find the bugs may vary considerably. In response, two levels of effectiveness, the best and the worst, are computed (see Section 5: Evaluation Metrics). In practice, the more the ties, the bigger the difference between the best and the worst effectiveness. Ties also make the exact effectiveness of a fault localization technique more uncertain.

In voting scenarios when voters are unable to select between two or more alternatives, the candidates are ranked based on some key or natural ordering, such as an alphabetical ordering, to break ties. Similarly, when two statements are tied for the same ranking, the line numbers assigned to them in a text editor can serve as the key. Other techniques such as confidence-based strategy and data dependency-based strategy are also used to break ties [369], [366], [386], [393].

7.7 Fault Localization for Concurrency Bugs
Concurrent programs suffer most from three kinds of access anomalies: data race [32], [321], atomicity violation [110], [113], [115], and atomic-set serializability violations [24], [44].

Among the approaches that have mushroomed in recent years, predictive analysis-based techniques haven drawn significant attention [111], [113] , [115], [116], [322], [349]. Generally speaking, these techniques record a trace of program execution, statically generate other permutations of these events, and expose unexercised concurrency bugs. One potential problem of these techniques is that they may sometimes report a large number of false positives. For example, only six of 97 reported atomicity violations in a study using Atomizer (a dynamic atomicity checker) are real [116]. On the contrary, a study in [329] using a different tool, Penelope, for atomicity violations detection reports no false positive.

Tools such as Chord [262] and RacerX [106] can statically analyze a program to find concurrency bugs. However, since all paths need to be explored, it is impractical to apply these tools to large, complicated programs. A runtime analysis (such as [144], [321], [392]), on the other hand, is less powerful than a static analysis but also produces fewer false alarms. The drawback is that only faults manifested in some specific executions can be detected.

Another approach for bug localization in concurrent programs is to use model checking [60], [190], [261], [324]. For instance, Shacham et al. [324] use a model checker to construct the evidence for data race reported by the lockset algorithm. However, due to the possible exponential size of the search space, it is difficult to adopt this approach for large-sized programs without compromising its detection capability.

There are other techniques for detecting concurrency bugs. For example, Flanagan and Freund use a prototype tool JUMBLE to explore the non-determinism of relaxed memory models and to detect destructive races in the program [114]. Park et al. apply a CTrigger testing framework [282] to detect real atomicity violations by controlling the program execution to exercise low-probability thread inter-leavings. Park also presents a study to debug non-deadlock concurrency bugs [283]. Wang et al. [353] propose a technique to locate buggy shared memory accesses that are responsible for triggering concurrency bugs. Authors of [345] propose a tool, MEMSAT, to help in debugging memory models. Koca et al. [198] locate faults in concurrency programs using an idea similar to spectrum-based fault localization techniques.

7.8 Spreadsheet Fault Localization
Spreadsheet systems represent a landmark in the history of generic software products. It is estimated that 95 percent of all U.S. firms use spreadsheets for financial reporting [280], 90 percent of all analysts in the industry perform calculations in spreadsheets [280] and 50 percent of all spreadsheets are the basis for decisions [156]. Such wide usage, however, has not been accompanied by effective mechanisms for bug prevention and detection, as shown by studies such as [278], [281]. As a result, bugs in spreadsheets are to be blamed for a long list of real problems compiled and available at the European Spreadsheet Risk Interest Group's (EuSpRIG) web site (http://www.eusprig.org/). A recent study by Reinhart and Rogoff [300] also gives a similar conclusion. In response to this, many studies regarding spreadsheet fault localization have been reported [1], [3], [23], [56], [157], [158], [162], [164], [169], [311], [314].

A model-based spreadsheet fault localization technique is presented in [169] , using an extended hitting-set algorithm and user-specified or historical test cases and assertions to identify possible error causes. Hofer et al. [164], apply a constraint-based representation of spreadsheets and a general constraint solver to locate bugs in spreadsheets. Another constraint-based approach for debugging faulty spreadsheets (CONBUG) is presented by Abreu et al. [15], taking a spreadsheet and one test case as input to compute a set of faulty candidates. Abraham and Erwig [2] describe a tool, GoalDebug, for debugging spreadsheets, using a constraint-based approach similar to that in [164] . Whenever the computed output of a cell is incorrect, users can provide an expected value, which is employed to produce a list of possible changes to the corresponding formulae that, when applied, will generate the user-specified output. This involves mutating the spreadsheet based on a set of pre-defined change (repair) rules and ascertaining whether user expectations are met. A similar approach also appears in other studies such as [95] and [152]. Authors of [95] propose a strategy for automatically fixing bugs in both Java and C programs by combining mutation testing and software fault localization. An approach of using path-based weakest preconditions is discussed in [152] to generate program modifications for bug fixing.

Abraham and Erwig also present a system, UCheck, which infers header information in spreadsheets, performs a unit analysis, and notifies users when bugs are detected [3]. Hermans et al. [157] suggest a way to locate spreadsheet smells (possible weak points in the spreadsheet design) and display them to users in data-flow diagrams. An approach to detect and visualize data clones (caused by copying the value computed by a formula in one cell as plain text to a different cell) in spreadsheets is reported in [158].

Other techniques aimed at reducing the occurrence of errors in spreadsheets include code inspection [279], refactoring [42], and adoption of better spreadsheet design practices [82], [83].

7.9 Theoretical Studies
Instead of being evaluated empirically, the effectiveness of software fault localization techniques can also be analyzed from theoretical perspectives.

Briand et al. [57] report that the formula used to compute the suspiciousness of a given statement by Tarantula can be re-expressed so that the suspiciousness only depends on the ratio of the number of failed tests (NCF) to the number of successful tests (NCS) that execute the statement. Lee et al. [182], [212] prove that Tarantula always produces a ranking identical to that of a technique where the suspiciousness function is formulated as NCFNCF+NCS. A study by Naish et al. [267] examines over 30 formulae and divides them into groups such that those in the same group are equivalent for ranking. Independently, Debroy and Wong [93] also report a similar study showing that some similarity coefficient-based fault localization techniques are equivalent to one another.

Xie et al. [384] perform a theoretical study on the effectiveness of some spectrum-based fault localization techniques. Based on the risk values (which is the same as suspiciousness discussed in this survey), program statements are assigned to one of the three sets, SRB, SRF, and SRA, based on whether their risk values are higher than, the same as, or lower than the value of the statement containing the bug. The authors make three assumptions: i) a faulty program has exactly one fault; ii) for any given single-fault program, there is exactly one faulty statement; and iii) this faulty statement must be executed by all failed tests. They also assume that the underlying test suite must have 100 percent statement coverage. Unfortunately, many of these assumptions are over-simplified and do not hold for real-life programs. With respect to some selected techniques (many of which are similarity coefficient-based), they examine the subset relation between SRB and SRA generated by the corresponding ranking formulae and conclude that for two techniques, R1 and R2 , if SR1B⊆SR2B and SR2A⊆SR1A then R1 is better (more effective) than R2 such that the number of statements examined by R1 is less than that examined by R2 to find the first faulty statement. One problem of this proof as reported in [371] is that it does not consider statements in SRF. As a result, for some special cases, even though the proof indicates that one technique is more effective than another, the former has to examine more statements than or the same number of statements as the latter − contradicting the result of the proof. Another weakness is that some advanced and more effective techniques (e.g., [14] , [223], [367], [371]) are excluded, even though they use exactly the same input data as those included in [384]. Authors of [210] also question the validity of [384]. They compare the effectiveness of the five best fault localization techniques based on the theoretical study in [384] with the effectiveness of Tarantula and Ochiai, and they find that the latter are significantly more effective than the former. This directly contradicts the conclusion of [384]. Xie et al. [388] also apply their theoretical analysis framework to 30 genetic programming-evolved formulae and show that some of them can be used for fault localization. However, they make the same over-simplified assumptions as those in [384].

There are other theoretical studies for single-bug programs. For example, Lee et al. [265] identify a class of strictly rational fault localization techniques in which the suspicious value of a statement strictly increases if this statement is executed by more failed test cases and strictly decreases if this statement is executed by more successful test cases. The authors claim that strictly rational techniques do not necessarily outperform those that are not. Therefore, limited attention should be given to these strictly rational techniques. In [264], Lee et al. further identify a class of optimal fault localization techniques for locating deterministic bugs (similar to Bohrbugs defined in [139]) that will always cause test cases to fail whenever they are executed.

SECTION 8Conclusion
As today's software has become larger and more complex than ever before, software fault localization accordingly requires a greater investment of time and resources. Consequently, locating program bugs is no longer an easily-automated mechanical process. In practice, locations based on intelligent guesses of experienced programmers with expert knowledge of the software being debugged should be examined first. However, if this fails, an appropriate fallback would be to use a systematic technique (such as those discussed in this survey) based on solid reasoning and supported by case studies, rather than to use an unsubstantiated ad hoc approach. This is why techniques that can help programmers effectively locate bugs are highly in demand, which also stimulates the proposal of many fault localization techniques from a widespread perspective. It is imperative that software engineers involved with developing reliable and dependable systems have a good understanding of existing techniques, as well as an awareness of emerging trends and developments in the area. To facilitate this, we conduct a detailed survey and present the results so that software engineers at all program debugging experience levels can quickly gain necessary background knowledge and the ability to apply cost-effective software fault localization techniques tailored to their specific environments.

In this survey, a publication repository has been created, including 331 papers and 54 Ph.D. and Masters’ theses on software fault localization from 1977 to November 2014. These techniques are classified into eight categories: slice-based, spectrum-based, statistics-based, program state-based, machine learning-based, data mining-based, model-based, and miscellaneous. The figures and tables presented in the previous sections strongly indicate that software fault localization has become an important research topic on the front burner and suggest the trend of ongoing research directions.

Our analysis shows that the numbers of published papers in each category differ from each other and that the research interest shifts from one category to another as time moves on. For example, static and dynamic slice-based techniques were popular between 2004 and 2007, whereas execution slice and program spectrum-based techniques have dominated since 2008.

Different metrics to evaluate the effectiveness of software fault localization techniques (in terms of how much code needs to be examined before the first faulty location is identified) are reviewed, including T-score, EXAM score/ Expense, P-score, N-score, and Wilcoxon signed-rank test. Subject programs and debugging tools used in various empirical evaluations are summarized. Results of different empirical studies using these metrics, programs, and tools suggest that no one category is completely superior to another. In fact, techniques in each category have their own advantages and disadvantages.

Additionally, effectiveness of these techniques can also be analyzed from theoretical perspectives. However, such analyses very often make over-simplified and non-realistic assumptions that do not hold for real-life programs. Hence, their conclusions in general are only applicable within limited scopes. This implies that a theoretical analysis alone is not enough. It is advisable to apply both empirical evaluations and theoretical analyses to provide a more complete assessment.

We emphasize that effectiveness is not the only attribute of a software fault localization technique that should be considered. Other factors, including overhead for computing the suspiciousness of each program component, time and space for data collection, human effort, and tool support, should be included as well. We also discuss aspects that are critical to software fault localization, such as fault localization on programs with multiple bugs, concurrent programs, and spreadsheets, as well as impacts of test cases, coincidental correctness, and faults introduced by missing code.

To conclude, our objective is to use this survey to provide the software engineering community with a better understanding of state-of-the-art research in software fault localization and to identify potential drawbacks and deficiencies of existing techniques so that additional studies can be conducted to improve their practicality and robustness.