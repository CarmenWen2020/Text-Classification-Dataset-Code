Monolithic systems increasingly suffer from maintainability and scalability issues as they grow in functionality, size, and complexity. It is widely believed that (micro)service-based architectures can alleviate these problems as each service is supposed to have the following characteristics: clearly defined functionality, sufficient modularity, and the ability to evolve independently. Industrial practices show that service extraction from a legacy monolithic system is labor-intensive and complex. Existing work on service candidate identification aims to group entities of a monolithic system into potential service candidates, but this process has two major challenges: first, it is difficult to extract service candidates with consistent quality; second, it is hard to evaluate the identified service candidates regarding the above three characteristics. To address these challenges, this paper proposes the Functionality-oriented Service Candidate Identification (FoSCI) framework to identify service candidates from a monolithic system. Our approach is to record the monolith's execution traces, and extract services candidates using a search-based functional atom grouping algorithm. We also contribute a comprehensive service candidate evaluation suite that uses interface information, structural/conceptual dependency, and commit history. This evaluation system consists of 8 metrics, measuring functionality, modularity, and evolvability respectively of identified service candidates. We compare FoSCI with three existing methods, using 6 widely-used open-source projects as our evaluation subjects. Our results show that FoSCI outperforms existing methods in most measures.
SECTION 1Introduction
Monolithic software systems increasingly suffer from maintainability and scalability issues as they grow in functionality, size, and complexity. It is widely believed that service-based (including microservice-based) architectures can help alleviate these problems as each service is supposed to exhibit well-defined functionality and high modularity. Most importantly, each service should be able to evolve independently1 [2], [3]. To migrate a monolithic system to a services-based system, there are two major options: rewriting from scratch, or extracting services from the legacy source code. If the legacy code has high value, the latter is recommended2 [3]. Large enterprises, such as Netflix and Amazon, have adopted the second option, which includes two phases, splitting design and code implementation. Splitting design is used to identify the boundaries to split a monolith into multiple functional units, where each separable function becomes a service candidate [2]. Code implementation then realizes service candidates as physical services. Our work focuses on splitting design, which is complex and laboring-intensive [4], [5].

To alleviate the burden of splitting design, researchers have proposed various methods to automate Service Candidate Identification, that is splitting entities (e.g., methods, classes) of a monolithic system into multiple function groups, each of which corresponds to a potential service candidate [5], [6]. Service Candidate Identification is similar to traditional software decomposition or software clustering techniques, such as Bunch [7] and ACDC [8], with the assumption that the clustered entity groups should follow the principles of high cohesion and low coupling [9], [10], [11]. Existing work for service candidate identification faces two major challenges: (1) It is difficult to produce high quality service candidates in terms of well-defined functionality, sufficient modularity, and independent evolvability; (2) There is no systematic way to measure these qualities quantitatively and consistently.

This paper introduces our approaches to address the above challenges. We first propose a Functionality-oriented Service Candidate Identification (FoSCI) framework to identify service candidates, through extracting and processing execution traces. This framework consists of three major steps: extracting representative execution traces, identifying entities using a search-based functional atom grouping algorithm, and identifying interfaces for service candidates. We also present a comprehensive measurement system to quantitatively evaluate service candidate quality in terms of functionality, modularity, and evolvability.

FoSCI Framework. There is prior research ([12], [13], [14], [15], [16]) that aims to split a monolithic system based on source code structure. These methods usually model the structure of the monolith as a graph, in which a node represents a class, and an edge represents either a class-to-class structural relation [12], [13], [14], or semantic similarity [15], [16]. To achieve high cohesion and low coupling [9], [10], [11], edges with weaker relations were removed to split into subgraphs. These methods only consider graph structure, without considering functionality. Consequently, as we will show, the resulting service candidates may not have well-defined functionality. In addition, these methods cannot be applied when the source code is not available.

FoSCI addresses this challenge by utilizing execution traces collected from logs to guide service candidate identification. The rationale is that execution traces can precisely reveal program functionality [17], [18], [19]. To evaluate FoSCI, we first collected 3,349,253,852 method call records from execution logs of widely-used projects. After that, we re-constructed, reduced, and extracted representative execution traces. Using these execution traces, FoSCI first generates Functional Atoms, each being a coherent and minimal functional unit. The assumption is that a set of classes that frequently appear in the same set of execution traces are dedicated to related functionality. Next, FoSCI assigns functional atoms to service candidates by optimizing four objectives from execution traces. Finally, FoSCI identifies the interface classes with operations for each candidate. Using execution traces, FoSCI can thus identify service candidates even when the source code of the monolithic application is not available.

Evaluation Framework. There are multiple ways to extract service candidates, but there currently exists no systematic evaluation framework to assess the functionality, modularity and evolvability of the resulting services candidates [2], [3]. Functionality describes visible functions provided by a service, which should be a business capability accessible by external clients. Modularity [3] measures if internal entities within a service behave coherently, while entities across services are loosely coupled. Evolvability [2] measures a service's ability to evolve independently: a system may have tens or even hundreds of services [20]; if changes to one service frequently affect other services, it would be challenging for services to evolve exclusively. There is prior research on quantifying some of these qualities. For example, Bogner et al. [21] proposes cohesion and coupling measures for services. But measuring the evolvability of service candidates has not been explored.

We have constructed a comprehensive evaluation suite to systematically measure service candidates from the three aspects, using 8 measures: 1) Independence of Functionality is measured by integrating Interface Number (IFN), Cohesion at Message Level (CHM), and Cohesion at Domain Level (CHD). 2) Modularity is measured by extending the Modularity Quality measure proposed by Mancoridis [22]. These metrics measure both structural and conceptual modularity of service candidates. 3) Independence of Evolvability is quantified using 3 measures we designed: Internal Co-change Frequency (ICF), External Co-change Frequency (ECF), and Ration of ECF to ICF (REI). These measures can be derived from the revision history of the original system. A project's revision history, stored in its version-control system, provides a unique view of the actual evolution path of a software system [23], [24]. The revision history holds a wealth of software evolution information including the changes and metadata about changes, such as who made each change, what is the purpose of the change, and when the change was made. As we will show, these 8 metrics comprehensively quantify the three critical quality aspects of service candidates.

In summary, our contributions are as follows:

The FoSCI framework to identify service candidates, including entity and interface identification. FoSCI employs execution traces because they accurately reveal functional groupings in software systems.

An evaluation suite for service candidates, which consists of 8 metrics to quantify the three quality criteria of service candidates: Independence of Functionality, Modularity and Independence of Evolvability.

The rest of this paper is as follows. Section 2 describes our proposed functionality-oriented service candidate identification framework. Section 3 illustrates the measures for quantifying Independence of Functionality, Modularity and Independence of Evolvability. Sections 4 and 5 present experiment setup and evaluation results. Section 6 discusses limitations and threats to validity. Section 7 surveys the related works. Section 8 provides conclusions and discusses future work.

SECTION 2Methodology
In this section, we first introduce basic definitions related to service candidates. After that, we illustrate the FoSCI framework using JPetstore3 (Monolithic version 6.0.2) as a running example. The intermediate results and tables of this example can be found in our repository.4 The symbol notations we use are listed in Table 1.

TABLE 1 The Basic Symbols Globally Used

2.1 Definitions
A Service Candidate is an intermediate product in the splitting design phase during a migration from a monolithic system into a (micro)service-based architecture. It has the potential and is subject to further refinement to be actually implemented as a physical Service. We formally define Service Candidate as follows:
SC=(Eser,I,O),(1)
View Sourcewhere, Eser denotes a set of entities which compose SC. I is a set of Interface Classes of SC. O represents a set of fine-grained Operations of I. In this paper, an entity is a class of the original monolith. An interface class is a class entity that has the potential to be publicly published. Through the published interface class, a service candidate provides functionality visible to external clients. An operation is a method publicly provided by an interface class.

2.2 FoSCI Framework Overview
Fig. 1 depicts our Functionality-oriented Service Candidate Identification (FoSCI) framework with three steps:


Fig. 1.
The FoSCI method.

Show All

Step 1: Representative Execution Trace Extraction. We apply a tracing tool, Kieker 1.13 [25], to collect execution traces of a monolithic executable software, using a set of pre-determined Functional test suite. These executions are recorded in Log files, from which we extract a Representative Execution Trace Set, Rtr.

Step 2: Entity Identification. We first identify Functional Atoms based on execution traces, and then modify a Non-dominated Sorting Genetic Algorithm-II, a multi-objective optimization technique, to group Functional Atoms as the class entities Eser for each service candidate.

Step 3: Interface Class Identification. For each service candidate, its interface classes I with operations O are identified. Combining them with Eser, a service candidate SC=(Eser,I,O) is produced.

2.3 Step 1: Representative Execution Trace Extraction
This step has the following parts:

Execution Monitoring. We use a tracing tool, Kieker 1.13 [25], to insert probes into the target software. The probes monitor method executions, and the execution paths are recorded in log files.

To obtain execution traces that accurately capture the functionality of the target system, functional test suites and the executable software (executable instance built from the targeted monolithic software) are required. Executing functional test cases under Functional Testing will allow us to identify independent functions to be split into service candidates. Functional Testing5 is a black-box testing against the functionality of the system, as opposed to Unit Testing that aims to test a class (or class set) or a method (or method set).

Algorithm 1. Execution Trace Reduction Algorithm
Input: Otr={tr1,tr2,…,trn}

Output: Rtr={tr1,tr2,…,trm}, Rtr⊆Otr

tr1∈Otr,Rtr←tr1; // Initialize Rtr.

for each tri in Otr do

if ∃ trj∈Rtr ∧ set(trj)⊂set(tri) then

// trj is replaced by tri.

Rtr.delete(trj);

Rtr.add(tri);

end

else if ∃ trj∈Rtr ∧ set(tri)⊆set(trj) then

continue; // Keep trj and ignore tri.

end

else

Rtr.add(tri); // Keep both tri and trj.

end

end

returnRtr;

Execution Trace Extraction. In an execution log file, each record contains several items, including ThreadID—a globally unique ID to identify a thread trace; Eoi and Ess—the calling order and the depth of the calling stack of the invoked method. From a record set with same ThreadID, we can extract an Execution Trace tr:⟨e1,e2,…,en⟩, a sequence of Method Calls that correspond to the executions of a slice of function [1]. By processing all records, we can re-construct an Execution Trace Set: Otr={tr}.

Execution Trace Reduction. The original execution trace set, Otr, typically contains a large number of redundant execution traces. We created an algorithm, as shown in Algorithm 1, to reduce Otr and form a Representative Execution Trace Set, Rtr. Rtr⊆Otr.

As shown in Algorithm 1, we use set(trk) to denote a set of method calls involved in trk. Assume tri∈Otr, and trj∈Rtr. If set(tri)⊆set(trj), tri will not be added into Rtr. Conversely, if set(trj)⊂set(tri), trj will be replaced by tri. If set(tri)⊈set(trj) and set(tri)⊉set(trj), then keep trj in Rtr and add tri in Rtr. Finally, we obtain the Representative Execution Trace Set, Rtr.

In the JPetstore example, we collected 47 original execution traces. After reduction, we finally obtained 15 representative execution traces.

2.4 Step 2: Entity Identification
To identify class entities, Eser, for a service candidate, we first generate functional atoms based on the reduced execution traces, and then derive four optimization objectives for functional atom grouping. After that, we modify a Non-dominated Sorting Genetic Algorithm-II, a search-based technique, to conduct functional atom grouping. Class entities in each group will be the class entities within the service candidate.

2.4.1 Functional Atom Generation
We define Functional Atom, fa, as a minimal coherent unit, in which all the entities are responsible for the same functional logic. Since an execution trace reflects a slice of software logic, a set of classes that often appear together in the same traces can correspond to a fa.

We employ a classic hierarchical clustering algorithm based on execution traces to generate functional atoms. There are two inputs in the clustering algorithm: diff—the threshold condition when the clustering should stop, and {Ctr}: for ∀Ctri∈{Ctr}, Ctri={c1,c2,…,cl} is a set of Classes whose methods are involved in the tri.

Initially, each functional atom contains just one class: fai={ci}. During the clustering process we use a Jaccard Coefficient fjaccard based on Ti of fai and Tj of faj to compute the similarity between fai and faj:
fjaccard(fai,faj)=|Ti ∩ Tj||Ti ∪ Tj|,(2)
View Sourcewhere, Ti is a set of Ctr that contains ci.

The fai and faj with the maximum of fjaccard are merged as a new functional atom, fa′i. At the same time, T′i of the new fa′i is updated: T′i=Ti∪Tj.

The above clustering is repeated until newDiff>diff:
newDiff=min(|Ti∪Tj|−|Ti∩Tj|), ∀i,j,i<j.(3)
View Source

According to the observations described in Section 4, we suggest setting diff=3. After the clustering, FA={fa1,fa2,…,fam} is obtained, where ∀i, fai={ci,1,ci,2,…,ci,ik}.

In the example of JPetstore, we initialize functional atom with classes: fai={ci}. After employing the above algorithm, we finally generate 8 function atoms:
fa1={c3}, fa2={c4}, fa5={c9}fa0={c0,c1,c2}, fa6={c10,c11,c13,c14}fa7={c12}, fa3={c5,c6}, fa4={c7,c8,c15}.
View SourceRight-click on figure for MathML and additional features.

2.4.2 The Objectives of Functional Atom Grouping
Functional Atom Grouping. This step aims to combine multiple functional atoms into one group, forming Eser for a service candidate. Related functional atoms should be put together while non-related ones should be separated. That is, the grouping process requires maximizing the intra-connectivity inside service candidates while minimizing the inter-connectivity across candidate boundaries. We formalize the grouping objectives based on the information revealed in execution traces.

Structural and Conceptual Intra-Connectivity. In terms of structural and conceptual information recorded in execution traces, we define two objectives based on the intra-connectivity formula in work of Mancoridis [22]. More specifically,

structural intra-connectivity, formulated as 1K∑Ki=1uiN2i. where, Ni is the number of functional atoms inside SCi. ui is the number of edges between the functional atoms inside SCi. In execution traces, a call relationship between a class in fai and that in faj will indicate an edge between fai and faj. K is the number of functional atom clusters.

conceptual intra-connectivity. It is similar to the above formula. The only difference is that an edge between fai and faj exists when the intersection between the term set (a set of textual terms presented in class identifiers) of fai and that of faj is not empty.

Structural and Conceptual Inter-Connectivity. Similarly, we define other two objectives for functional atom clusters based on the inter-connectivity defined by work of Mancoridis [22]. More specifically,

structural inter-connectivity, formalized as 1K(K−1)/2∑Ki≠jσi,j2(Ni×Nj), where Ni or Nj is the number of elements inside atom cluster i or cluster j. σi,j denotes the number of edges between cluster i and cluster j.

conceptual inter-connectivity. Its formula is similar with the above, except for the edge difference as illustrated in the definition of conceptual intra-connectivity.

Optimization Objectives. In summary, the functional atom grouping has four optimization objectives:

Maximizing structural intra-connectivity.

Maximizing −(structural inter-connectivity).

Maximizing conceptual intra-connectivity.

Maximizing −(conceptual inter-connectivity).

2.4.3 Search-Based Functional Atom Grouping
Functional Atom Grouping aims to produce service candidates through different combinations of functional atoms, by optimizing the above four objectives.

To address this multi-objective problem, we tailored the Non-dominated Sorting Genetic Algorithm-II (NSGA-II) [26] to the context of functional atom grouping. NSGA-II has been widely exploited in software remodularization [11]. This efficient genetic technique can search for well-distributed Pareto fronts where approximately optimal solutions are located. The tailoring of NSGA-II to our context of functional atom grouping is described next.

Initial Population. Given the set of functional atoms FA={fa1,fa2,…,fam} obtained in Section 2.4.1, an individual or a chromosome is a partition P of the set FA:
P={q1,q2,…,qk},
View Sourcewhere, ∀qi≠∅, ∪i=1,2,…,k qi=FA and ∀i,∀j,qi∩qj=∅.

A partition P of FA into N non-empty groups is called a N-partition of FA. A N-partition of FA corresponds to producing N service candidates. We randomly generate a set of P from FA as initial population.

Fitness Functions. We define F as a vector of objective functions, composed of four fitness functions:
F=(f1,f2,f3,f4)
View Sourcewhere, f1,f2,f3 and f4 correspond to the previous four optimization objectives, respectively. Functional atom grouping searches for near-optimal solution P to maximize F.

Crossover. We employ single-parent crossover to generate offspring, each of which is a neighbor partition of an individual P. A neighbor partition (NP) [22] is still a partition of FA, and is generated by moving a functional atom fa from an element of P to a different element of P. The moving fa between elements of P manifests two strategies of monolith splitting: pull up and move strategies [1]. The move strategy merges one functional atom into a service candidate. The pull up strategy separates out one functional atom to be a new service candidate.

Mutation. With a mutation probability, we randomly select an offspring generated in Crossover. Then, we randomly choose one NP of this offspring as the mutation result.

Post-Processing. The NSGA-II to Functional Atom Grouping outputs a Pareto front, consisting of a set of near-optimal solutions (i.e., partitions). We consider one partition P as the best solution if P is the knee point. As a compromise of all optimization objectives, the knee point Pknee [27] presents the smallest euclidean distance from its fitness values to the fitness values (Fideal) of Pideal. Pideal is an ideal solution with best fitness values Fideal in the Pareto front: Fideal=(max(f1),max(f2),max(f3),max(f4)). The classes in each element of the Pknee will compose a Eser for a service candidate.

In the example of JPetstore, when initializing populations, we randomly generate N-partitions (Assuming N=4) of FA as individuals. An individual is a partition (P) of FA:
FA={fa1,fa2,fa3,fa4,fa5,fa6,fa7}.P={fa0,fa5},{fa1},{fa3,fa6},{fa2,fa4,fa7}}.
View SourceAfter processing the populations through the tailored NSGA-II above, we choose the knee point from the Pareto front as the best solution. By extending the elements of Pknee with classes of fa, the class entities in each element will form one identified Eser:
Eser3={c5,c6,c9}Eser2={c7,c8,c15}Eser1={c10,c11,c13,c14}Eser0={c0,c1,c2,c3,c4,c12}.
View Source

2.5 Step 3: Interface Class Identification
According to Rtr and the identified Eseri for service candidate SCi, we further recognize the potential interface classes Ii and operations Oi that can be published.

We first detect entry methods from execution traces. We consider an entry method always the first m in tr∈Rtr to process functional requests from external clients. Methods related with common logic such as configuration and context are excluded from the entry methods, since they are irrelevant to the business capabilities and appear in almost all execution traces.

After that, for a service candidate SCi, we regard the entry methods located in classes of Eseri as Operations (Oi) to be published.

We identify interface classes of service candidates in a direct way. We intuitively group Operations in terms of their owner classes [28], each of which is then regarded as an Interface Class. All interface classes compose Ii provided by a service candidate SCi.

For JPetstore, the identified operations and interface classes are method members and classes with a prefix of org.mybatis.jpetstore.web.actions. Table 2 illustrates the identified service candidates from JPetstore. We can observe that the generated service candidates are separated into independent functions: “Catalog service”, “Order service”, “Account service” and “Cart service”. Obviously, even though some classes (e.g., domain.Account, domain.Product) are organized in the same code package of the original monolith, these classes are divided into different service candidates, each having a clearly-defined functionality.

TABLE 2 The Service Candidates Extracted from JPetstore

SECTION 3Evaluation Model
In this section, we introduce an evaluation system to assess the quality of service candidates from three aspects:

External: Independence of Functionality. A service should provide well-defined, independent, and coherent functionality to its external clients, following the Single Responsibility Principle (SRP).

Internal: Modularity. If a service is well-modularized, its internal entities should be cohesive, and entities across service boundaries should be loosely coupled.

Evolvability: Independence of Evolvability. Being able to evolve independently, without changing other services, is the most desirable property of services, meaning that they can flexibly accommodate future changes.

3.1 Independence of Functionality
To quantitatively and objectively assess functional independence, we leverage information from published interfaces that expose the functionality of a module (e.g., a service) [29]. From the interface classes I of identified service candidates, we use three measures to objectively quantify Independence of Functionality [1] as follows.

ifn (interface number), measures the number of published interfaces of a service. The smaller the ifn, the more likely the service assumes a single responsibility. IFN is the average of all ifn. The formal definitions are as follows:
IFN=1N∑j=1Nifnj(4)
View Source
ifnj=|Ij|,(5)
View Sourcewhere, Ij is the published interfaces (or interface classes) of service j. N is the number of services that provide published interfaces in the service-based system.

chm (cohesion at message level), measures the cohesiveness of interfaces published by a service at the message level. The higher the chm of a service, the more cohesive the service is, from an external perspective. CHM is the average functional cohesiveness. We define chm as a variation of LoCmsg (Lack of Message-level Cohesion), proposed by Athanasopoulos et al. [30]. chm+LoCmsg=1.
CHM=1N∑j=1Nchmj(6)
View Source
chmj=⎧⎩⎨⎪⎪∑(k,m)fmsg(oprk,oprm)12|Oj|×(|Oj|−1),1,if |Oj|≠1if |Oj|=1(7)
View Source
fmsg(oprk,oprm)=(|retk∩retm||retk∪retm|+|park∩parm||park∪parm|)2,(8)
View Sourcewhere, oprk,oprm∈Oj are union operations on Ij, and k<m. retm and parm are sets of return values and input parameters of oprm. fmsg computes the similarity between two operations at message level, denoting the average of similarity of input messages (parameters) and output messages (return values). N is the same with that of IFN.

chd (cohesion at domain level), measures the cohesiveness of interfaces provided by a service at the domain level. The higher the chd, the more functionally cohesive this service is. Similarly, CHD is the average of all chd within the system. We define chd as a variation of LoCdom (Lack of Domain-level Cohesion) defined by Athanasopoulos et al. [30]. chd+LoCdom=1.
CHD=1N∑j=1Nchdj(9)
View Source
chdj=⎧⎩⎨⎪⎪∑(k,m)fdom(oprk,oprm)12|Oj|×(|Oj|−1),1,if |Oj|≠1if |Oj|=1(10)
View Source
fdom(oprk,oprm)=|fterm(oprk)∩fterm(oprm)||fterm(oprk)∪fterm(oprm)|,(11)
View Sourcewhere, opr and O are same symbols as those defined in chm. fdom computes the similarity between operations. fterm(opri) describes the set of domain terms contained in the signature of opri.

3.2 Modularity
Modularity of a component or service can be measured from multiple perspectives, such as structural, conceptual, history, and dynamic dimensions [11]. Here we extend the Modularity Quality (MQ) defined by Mancoridis [22] with structural and conceptual dependencies, using Structural Modularity Quality and Conceptual Modularity Quality to assess the modularity of services candidates.

SMQ (Structural Modularity Quality), measures modularity quality from a structural perspective. The higher the SMQ, the better modularized the service is.
SMQ=1N∑i=1Nscohi−1N(N−1)/2∑i≠jNscopi,jscohi=uiN2i,   scopi,j=σi,j2(Ni×Nj).(12)
View Source

Consistent with intra-connectivity and inter-connectivity [22], scoh measures the structural cohesiveness of a service, while scopi,j measures coupling between services. ui is the number of edges inside a service i. σi,j is the number of edges between service i and service j. Ni or Nj is the number of entities inside service i or j. If there is a structural call dependency between two entities, an edge exists. The bigger scoh, and the smaller scop, the better.

CMQ (Conceptual Modularity Quality), similarly measures modularity quality from a conceptual perspective. The higher the CMQ, the better.
CMQ=1N∑i=1Nccohi−1N(N−1)/2∑i≠jNccopi,jccohi=uiN2i,    ccopi,j=σi,j2(Ni×Nj).(13)
View SourceThe formalisms of CMQ, ccoh, and ccop are similar to SMQ, scoh, and scop. The only difference is that in the CMQ definition an edge between two entities exists if the intersection between the textual term set of the entities is not empty.

3.3 Independence of Evolvability
Ideally, we should evaluate Independence of Evolvability of services by examining and tracking the revision history of the implemented services, but this is impossible at the service design phase. We thus propose to measure a service's (or candidate's) Independence of Evolvability by studying the revision history of its original monolithic software. The rationale is: if classes that change together frequently are grouped into one service, and other classes (that do not change together with these chosen classes) are grouped into different services, we assume that the designed services will evolve relatively independently. Based on this rationale, we propose three measures as follows.

icf (internal co-change frequency), measures how often entities inside a service change together as recorded in the revision history. Higher icf means that the entities inside this service will be more likely to evolve together. ICF is the average of all icf within the system.
ICF=1N∑j=1Nicfj(14)
View Source
icfj=1|Eserj|∑m=1|Eserj|1|Eserj|∑n=1|Eserj|fcmt(cm,cn)(15)
View Sourcewhere, fcmt(cm,cn) is the number of commits in which entity cm and cn changed together. cm,cn∈Eserk. cm,cn∈Eserj. fcmt(cm,cn)=0 if m=n.

ecf (external co-change frequency), measures how often entities assigned to different services change together, according to the revision history. A lower ecf score means that entity pairs located in different services are expected to evolve more independently. Similarly, ECF is the average ecf of all services within the system.
ECF=1N∑j=1Necfj(16)
View Source
ecfj=1|Eserj|∑m=1|Eserj|1|Ecserj|∑n=1|Ecserj|fcmt(cm,cn),(17)
View Sourcewhere, ecfj computes the co-change frequency between entities in Eserj of service j and entities in Ecserj. Ecserj is a set of entities not located in Eserj. Ecserj=∪k=1,2,…,NEserk, k≠j. fcmt(cm,cn) is same as that defined in icf. cm∈Eserj, cn∈Ecserj. ecfj=1 if |Eserj|=1.

REI (Ratio of ECF to ICF), measures the ratio of co-change frequency across services versus the co-change frequency within services. The ratio is expected to be less than 1.0, if co-changes happen more often inside a service than across different services. The smaller the ratio is, the less likely co-changes happen across services, and the extracted services tend to evolve independently. Ideally, all co-changes should happen inside services.
REI=ECF/ICF(18)
View SourceRight-click on figure for MathML and additional features.where, ECF and ICF are notions defined above.

An example with JPetstore. We will use the JPetstore example, introduced in Section 2, to intuitively explain ICF, ECF, and REI. Fig. 2 shows that this example includes 6 classes and 3 commits, and the classes will be grouped into 3 service candidates. Fig. 2 shows three splitting scenarios. In Fig. 2a, each commit revision happens within a service, which means each service can change independently. At the other extreme, in Fig. 2c, each commit revision crosses a service boundary, which means changes to each service will always influence another service. Fig. 2b is an intermediate case: one service can change by itself while the others cannot. From Case a to Case b to Case c, the value of ICF becomes smaller while the values of ECF and REI become larger. It can be seen that the measures (ICF, ECF, REI) are able to reflect the change of Independence of Evolvability in different splittings.


Fig. 2.
Independence of Evolvability measures of different splitting for JPetstore example.

Show All

SECTION 4Experimental Setup
In this section, we first introduce the investigated subjects, and then present how we obtained test cases and collected execution traces. After that, we introduce the parameter configurations used in our method, illustrate the baseline methods, and present the evolution history data we collected. Finally, we show the evaluation configuration in the experiments.

4.1 Subjects
We collected six web applications for our experiments. Project_Introduction of Table 3 illustrates these projects. Column Version shows the version of the project that we analyzed. Column Start date and End date denote the time range under examination. LOC and #Class are the lines of code and the number of classes implemented in Java. Springblog6 and Solo7 are blogging systems. JForum8 is a discussion board. Apache Roller9 is a full-featured, multi-user and group-blog server. Agilefant10 is an open-source agile project management tool. Xwiki-platform11 is a generic wiki platform offering runtime services. Xwiki-platform project contains more than 100 modules and supports extensions. These projects are heterogeneous in their sizes and business domains. Most of these systems—such as JForum, Apache Roller, Agilefant, and Xwiki-platform—are popular and widely used in practice.

TABLE 3 Summary of Projects and Collected Execution Traces

These web applications follow classical multi-layered architectures, such as three-layer (Presentation-Business-Persistence) and four-layer (Presentation-Application-Business-Persistence) [31], [32]. We chose web applications as subjects because the back end (server side) of a web application is typically packaged into a single unit, such as a WAR or EAR file. Such monolithic web applications usually suffer from maintainability and scalability issues because of their rapid growth.

Our experiments decompose the back end of these subjects into service candidates, excluding the front end (Presentation Layer) and databases. Various patterns [33] can be adopted for designing the front end decomposition, which is beyond the scope of our current research. Object-Relational Mapping12 is employed in the investigated systems to interact with the databases, and the schema of the database in each case corresponds to Entity [32] classes. Thus, when we decompose software systems at the class level, the database tables would also be naturally re-organized, forming a new schema for each service candidate.

4.2 Test Case and Execution Trace Collection
As mentioned earlier, Functional Testing is required in our method. Functional Testing is black-box testing of the entire application. Different from Unit Testing, Functional Testing requires that the tested application is physically executable. In the experiment, we use both automatic and manual approaches to conduct the testing:

Automation Functional Testing for Xwiki-platform. The repository of the Xwiki-platform project includes functional test cases in 38 modules, which we have confirmed with their developer community. It has an automation test infrastructure, so these test cases can be carried out automatically.13 In total, we executed 4020 functional test cases in the experiment.

Manual Functional Testing for Other Projects. Springblog, Solo, JForum, Apache Roller and Agilefant projects contain unit testing suites, but do not provide test suites for functional testing. We thus designed and manually conducted functional testing for each subject project, following four steps :

Determine the functionality that needs to be tested by checking the project specification documentation;

For each function to be tested, design the test scenarios to cover the maximum amount of application functionality;

Build the project to generate an executable WAR package, and deploy it in Apache Tomcat;

Manually execute the test scenarios by manipulating the application through a web browser. We used Apache JMeter14 to record the operations while executing each test scenario. JMeter supports a fully featured test IDE that allows test plan recording from browsers. We configured JMeter and selected “Functional Testing” for “Test Plan Object”, and set the proxy server. As a user explores the GUI through the browser, JMeter intercepts the HTTP(S) requests and records transactions in test scripts. Each recorded transaction corresponds to a test case.

These recorded test cases can be repeatedly executed using JMeter to automate the testing. In total, we manually executed 250 test scenarios and recorded 1,886 test cases by JMeter in the experiment.

For all subjects, Collected_Execution_Trace in Table 3 presents the execution data extracted from the monitoring log. #TC(#TS) is the number of functional test cases (Test Scenarios). |Otr| is the number of extracted Execution Traces (Otr). |Ocall| is the number of method calls invoked in Otr. |Rtr| and |Rcall| count the Representative Execution Trace (Rtr) and method calls in Rtr. In total, 3,139 representative execution traces were obtained from 28,770 original execution traces.

To ensure reproducibility, all projects, specification documents, the description of test scenarios, the test plan files containing test cases recorded by JMeter, and the collected execution traces can be found at https://github.com/wj86/FoSCI.

4.3 Parameter Setting in Our Method
The FoSCI method requires several parameters: diff, population size, crossover probability, mutation probability, maximum generations.

diff is the stopping condition of Functional Atom Generation. We have observed that the minimum and coherent functional atoms are formed when diff is equal to 3. Fig. 3 illustrates the change in the number of functional atoms (as a percentage of the initial number of classes) as diff is set from 1, 2, …, to 10 in all investigated subjects. It can be seen that the number of functional atoms flattens when diff = 3. Thus, we recommended and set diff to be 3.


Fig. 3.
The percentage of functional atoms change with diff increases.

Show All

Search-based Functional Atom Grouping involves parameters due to the NSGA-II technique. We calibrated these parameters based on existing work on NSGA-II. We set the population size equal to 20 individuals, since this setting achieves a balance between effectiveness and efficiency according to our experiments. We used 0.8 as crossover probability and 0.04×log2(n) as the mutation probability as suggested by Candela et al. [11]. We configured maximum generations to be 200. The search process is repeated 30 times to reduce the bias caused by the randomness of the genetic algorithm. As a result, we got 30 sets of the nearest optimal solutions after completing our method execution for each subject. We merged the solutions and chose the non-dominated individuals. Among them, we selected the knee point (as explained in Section 2.4.3) as the solution of service candidate identification.

4.4 Baseline Methods
We compare FoSCI with three baseline methods: LIMBO [34], WCA [12], and MEM (Microservice Extraction Model) [5]. WCA is a hierarchical clustering method that leverages two measures to determine the similarity between classes: Unbiased Ellenberg (UE) and Unbiased Ellenberg-NM (UENM). We employed UENM in our work since it outperforms UE [13]. LIMBO uses information loss to measure the distance between classes for software clustering. MEM cuts the class graph of the original monolith, and the relations on the edges are extracted using three strategies: logical, semantic, and contributor couplings. In this paper, the semantic coupling-based approach was selected in MEM because the other two strategies cover a very small number of the classes in our target projects.

Table 4 shows the class percentage covered by all four methods: WCA, LIMBO, MEM and FoSCI. Class percentage refers to the proportion of classes that are analyzed by various methods. The class percentage rarely equals 100 percent. The class percentage in LIMBO and WCA is always the same, as both methods are based on structural dependencies parsed from source code. The percentages in MEM and FoSCI are less than those of LIMBO and WCA. We will discuss the uncovered classes in FoSCI in Section 5.

TABLE 4 The Class Percentage by Different Methods

To ensure that all methods identify service candidates from the same set of classes and the comparisons are unbiased, we use the intersection of the classes of all methods as our data standard. In the experiments, we first run these methods using all classes, and then filter out the results based on the classes shared by all methods to evaluate their performance.

4.5 Revision History Collection
We collected commit information from the revision history to evaluate the Independence of Evolvability. As shown in Table 5, #Total is the number of total commits during the evolution period. Many commits are irrelevant to class evolution, such as adding new files, modifying front-end files (.css, .html), modifying configuration files (.xml), and updating licenses.15 By removing these irrelevant commits, 14601/46787=31.21% of all commits are selected. Because of the incomplete revision history of JForum, the number of co-change commits and involved classes is 0.

TABLE 5 Analysis of Commit History

4.6 Evaluation Configuration
The baseline methods require configuring the number N of service candidates to be identified. Similar to the software modularization, it is not a trivial task to define N, since it is highly related to the targeted subjects [11]. Modularization work in [11] uses M2 (M is the number of elements or classes to be clustered) as the maximum number of generated modules. For identifying services from a monolithic software, the domain experts of the investigated project should ideally recommend N in terms of their knowledge of their system's business logic [20]. We set N based on the functionality features (as illustrated in our data repository) for the subjects in study. To address the comparison more rigorously, we supplemented four extra settings by using N−2, N−1, N+1 and N+2 as inputs. As a result, Springblog, Solo, JForum, Agilefant, Apache Roller and Xwiki-platform were split into 6-10, 7-11, 7-11, 9-13, 14-18, and 50-54 service candidates respectively.

Thus we conducted 5×6=30 group experiments in our evaluation. For each group, WCA, LIMBO, MEM and our method are applied. In each group, our method is repeated 30 times, with the configurations described in Section 4.3.

SECTION 5Evaluation
The objective of our evaluation is to assess whether FoSCI can produce effective service candidates, in terms of Independence of Evolvability, Independence of Functionality, and Modualarity. We compare FoSCI with three baseline methods, LIMBO, WCA, and MEM, and explore how the performance of FoSCI changes when the coverage of execution traces differs.

As described in Section 4.6, we conducted 5 test groups for each subject. Section 5.1 illustrates all 5 groups for each subject. Due to the consistent performance of methods in different N for a subject, Sections 5.2 and 5.3 just illustrate 3 groups (N−2,N,N+2). The complete tables with all groups are available in our repository.

5.1 The Evaluation of Evolvability
Table 6 presents the ICF,ECF and REI measures for service candidates generated by all four methods. Each row corresponds to the measures of one case (group). For example, the first row of Springblog shows the evaluation results when identifying N=6 service candidates. To rigorously compare FoSCI with other methods, we employ Wilcoxon's signed-rank test, a non-parametric statistical hypothesis to test whether the sample of one metric measurement from FoSCI is significantly better than the sample of that from each of the other methods over all cases. The P-values are illustrated in the last row of the table.

TABLE 6 Measurement Results of ICF, ECF and REI

5.1.1 Analysis of Results
Recall that REI measures the evolvability feature by integrating ICF and ECF. The smaller the REI, the better. From Table 6, we observe that only FoSCI has all REI scores less than 1.0 except for one outlier (in gray color). Most scores in LIMBO, WCA and MEM are much larger than 1.0, indicating poor evolvability of the service candidates they generated. The P-value is less than 0.001 (denoted as ***) respectively when FoSCI is compared with LIMBO, WCA and MEM with regard to REI, suggesting that FoSCI significantly outperforms the other methods.

According to Table 6, service candidates produced by FoSCI perform by far the best in terms of REI among all methods. Moreover, FoSCI can ensure that co-changes are better constrained within services instead of crossing service boundaries. In addition, Table 6 shows that, even though for different values of N of one subject, the performance of each method is quite consistent.

ICF, ECF and REI measures the overall level of evolvability. It is interesting to investigate the performance of each individual service candidate (measured by icf, ecf) in each case. We selected 4 large subjects to present their distribution of icf and ecf: Solo, Apache Roller, Agilefant, and Xwiki-platform. Since the performance of each method is consistent under different target number of services, N, as shown in Table 6, we present the results using a randomly picked N, as shown in Fig. 4.


Fig. 4.
The distribution of icf, ecf measures for service candidates.

Show All

Fig. 4 shows that the co-changes (indicated by icf) inside service candidates are more frequent than those across services (indicated by ecf) in FoSCI. Its positive difference from icf to ecf by FoSCI is significantly higher than others. Moreover, WCA and MEM may generate service candidates with negative difference from icf to ecf, indicating that these services can hardly evolve independently.

The box plots reveal that the difference from icf to ecf by FoSCI is bigger (with expected positive value) than that by the baseline methods. This observation is consistent with that from Table 6.

5.1.2 Summary of the Evaluation of Evolvability
As indicated by ICF (and icf), ECF (and ecf) and REI, FoSCI can aggregate frequently co-changed entities within a monolith into one service candidate, while placing infrequently co-changed entities into different candidates. In contrast, for service candidates generated by LIMBO, WCA and MEM, change across service boundaries are more common than those within services. Consequently, these services are unable to evolve independently. In conclusion, FoSCI can generate service candidates with significantly greater Independence of Evolvability than the other three baseline methods.

5.2 The Evaluation of Functionality
The analysis in this section is similar to that in Section 5.1. Table 7 shows IFN, CHM and CHD measures for service candidates identified by the four methods. To further observe individual service candidates, Fig. 5 shows the distribution of ifn,chm and chd measures of service candidates in four cases, the same as we reported in Section 5.1.


Fig. 5.
The distribution of ifn, chm, chd measures for service candidates.

Show All

TABLE 7 Measurement Results of IFN, CHM and CHD

5.2.1 Analysis of Results
Recall that the smaller the IFN, the better a split will be. Column IFN of Table 7 shows that, compared with WCA and MEM, FoSCI and LIMBO have the better performance considering IFN. The IFN value of FoSCI and LIMBO can be even about 10 times smaller than that of WCA and MEM in Agilefant and Apache Roller cases. The P-values indicate that, in all statistical tests, except for the comparison with LIMBO, FoSCI significantly outperforms WCA and MEM in terms of IFN.

Similarly, Fig. 5a s shows ifn distribution for each service candidate in the same four cases as reported in Section 5.1. Consider a large-size Xwiki-platform case: WCA assigns most published functionality into a singe service, so that this service candidate has to provide more than 700 interfaces (ifn>700). The same phenomenon can be seen in other cases too. Both FoSCI and LIMBO, however, identify services with less interfaces. We can see these results from ifn box plots are consistent with those from IFN analysis.

In terms of Column CHM and CHD, Table 7 shows that the three baseline methods perform the worst in several cases, but FoSCI never ranks the worst except for one case highlighted in gray. The P-values indicate that FoSCI significantly surpasses LIMBO. But there is no statistical difference between the results of FoSCI and MEM (and WCA).

To understand these observations, we investigate the distribution of chd and chm shown in Figs. 5(b) s and 5(c) s. In particular, for Xwiki-platform, the result of MEM is much higher than that of FoSCI, because MEM produced one large service (providing more than 700 interfaces) and a lot of tiny services (providing 1 interface), as indicated in Fig. 5a. These tiny interfaces have the best chm=1 and chd=1, shown in Figs. 5b and 5c. Consequently, the general results of MEM have exceptionally higher CHM and CHD than those of FoSCI.

5.2.2 Summary of the Evaluation of Functionality
As indicated by IFN (and ifn), FoSCI and LIMBO are more capable of splitting the business responsibilities within a monolith into reasonable services candidates, while MEM and WCA tend to heavily mix the functions together into fewer services, and even one very large service. Furthermore, in terms of CHM, CHD (and chm, chd), FoSCI performs better than LIMBO. For WCA and MEM, their CHM and CHD may be better than those of FoSCI when they decompose a monolith into one super larger service (i.e., still a “monolith”) and lots of tiny services. This phenomenon will be more obvious when decomposing a large-scale software system (such as the Xwiki-platform).

5.3 The Evaluation of Modularity
Table 8 presents the measures of SMQ and CMQ for all cases. Concretely, Fig. 6a illustrates the distribution of scoh and scop of individual service candidates in four cases, same as those in Section 5.1. Likewise, the distribution of ccoh and ccop are in Fig. 6b.


Fig. 6.
The distribution of (scoh, scop) and (ccoh, ccop) measures for service candidates.

Show All

TABLE 8 Measurement Results of SMQ and CMQ

5.3.1 Analysis of Results
In the column SMQ of Table 8, FoSCI always has the best value among all cases. For CMQ, FoSCI outperforms others except for two outliers in gray color. In particular, the SMQ and CMQ of LIMBO, WCA and MEM even have some measures less than 0, indicating poor modularity. Statistically, all the P-values in the last row indicate statistical significance of the comparison between FoSCI and others.

Fig. 6a shows the structural cohesion (scoh) and coupling (scop) for individual service candidates. We observe that service candidates produced by FoSCI exhibit distinctively positive differences from scoh to scop. The other methods are unable to achieve high cohesion and low coupling. In terms of conceptual cohesion (ccoh) and coupling (ccop), the observation is similar, as shown in Fig. 6b. It can bee seen that the observations from Fig. 6 and Table 8 are consistent.

5.3.2 Summary of the Evaluation of Modularity
Both structural and conceptual modularity measurements suggest that FoSCI can split a monolith into service candidates with considerably better modularity. Entities inside service candidates by FoSCI tend to function more coherently than those generated by the other three methods.

5.4 The Influence of Coverage on FoSCI
Since FoSCI depends on functional test suites, we investigated how the test coverage level may influence the results. First, through manually inspecting the source code, we categorized the entities not covered by the collected execution traces. After that, we designed experiments to investigate the service candidates generated by FoSCI and how their performance differ under different level of coverage.

5.4.1 Category of Non-Covered Entities
To figure out why some classes in our experiments were not covered by execution traces as shown in Table 4, we manually inspected the source code of four16 investigated projects: Springblog, Solo, JForum, and Apache Roller. We categorize these uncovered classes into 8 types, as shown in Table 9. The examples of these types can be found in our data repository mentioned before.

TABLE 9 Categories of Uncovered Classes

Based on the categories, Fig. 7 illustrates the distribution of classes in the four projects. Taking Springblog as an example, the execution traces collected in experiment cover 72.94 percent of all classes. Among the uncovered classes, 3rdPartyService classes account for 9.41 percent of all classes. Other classes account for 2.35, 10.88, 11.49 and 3.75 percent in Springblog, JForum, Solo and Apache Roller. We can see that it is difficult for execution traces to cover all code entities.


Fig. 7.
The distribution of classes in four subjects.

Show All

5.4.2 The Coverage Influence on FoSCI
It is interesting to observe how FoSCI will perform when the coverage of execution traces changes. First, we introduce two variables, Coveragetr and Coveragec:

Coveragetr denotes the coverage of execution traces, the ratio of the number of used execution traces to the entire execution traces collected.

Coveragec denotes the coverage of classes, the ratio of the number of covered classes to the number of classes covered by the entire execution traces.

We use all 6 projects as investigation subjects, and conduct experiments with different Coveragetr: 20, 40, 60, 80, 100 percent, that is, 5×6=30 experimental groups in total.

For each experimental group, we observe how Coveragec will change along with the increase of Coveragetr. For each group, we repeated 30 times to randomly select the execution traces by a specified Coveragetr. Fig. 9 presents the statistics labeled with the median of Coveragec. We observe that, individually, the Coveragec in the lower Coveragetr sometimes may be larger than that in the higher Coveragetr. But, statistically, the median of Coveragec tends to increase with the increase of Coveragetr. Therefore, we selected the execution trace set of a specified Coveragetr in experiment if the corresponding Coveragec is equal to the median labeled.

To rigorously conduct the evaluation, we set the number of service candidates being identified (N) for different groups based on Coveragec. As shown in Fig. 9, assume N=k in Solo project is set when Coveragetr=100%. When Coveragetr=20%, N=int(58.33%×k) will be set. k is the same value with the N configured as introduced in Section 4.6. Other parameters in FoSCI are the same with those in Section 4.3.

How many entities in a service candidate remain the in same service candidate when coverage changes? To answer this question, we define Hi,p for service candidate SCi generated under Coveragetr=p:
Hi,p=MAX|SCi|,
View Sourcewhere, |SCi| is the size of service candidate SCi; MAX is the maximum number of classes (∀c∈SCi) that remain in the same candidate under Coveragetr=100%. For example, in Fig. 8, H1,80% of service candidate SC1 is 66.67 percent.


Fig. 8.
An example for Hi,p definition.

Show All


Fig. 9.
The statistic of Coveragec under different Coveragetr.

Show All

In Fig. 10, the distribution of Hi,p indicates that over 50 percent (the value of Hi,p) classes of a service candidate remain in the same candidate when Coveragetr differs. In the largest project (Xwiki-platform), Hi,p is a little lower with the median equal to 40 percent when Coveragetr=20%. In general, the average of Hi,p for Springblog, Solo, JForum, Apache Roller, Agilefant, and Xwiki-platform is 71.36, 64.72, 59.48, 55.75, 74.15, and 49.21 percent, respectively.


Fig. 10.
Hi,p distribution under different Coveragetr.

Show All

In terms of the three quality criteria, how does our method perform differently when the coverage differs? For each investigated subject, Table 10 illustrates the mean and standard deviation (σ) of the sample that is composed of measures under different coverages. We observe that (45−6)/45=86.67% of the σ values is “<=0.2”.

TABLE 10 Measures in Different Coverage

Summary. These data show that the service candidates and the quality evaluation results are influenced when the coverage differs. However, the results suggest that our method offers acceptable stability, as indicated by Hi,p ((71.36%+64.72%+59.48%+55.75%+74.15%+49.21%)/6=62.45%) and σ (<=0.2). In addition, when test coverage varies from 20 to 80 percent for a subject, its impact on FoSCI is not linear, nor following an obvious pattern. One possible reason is that, when the execution traces differ, other control variables change too, such as the set of classes being covered and being changed. One observation is that, coverage rate has most impact on the largest project (Xwiki-platform). We will further investigate the coverage influence in our future work.

5.5 Summary
To sum up, it is evident that our FoSCI can produce efficient service candidates that can consistently exhibit reasonable functionality, modularity and evolution characteristics. Furthermore, FoSCI shows acceptable stability in terms of split results and quality evaluation when the coverage of execution traces differs.

SECTION 6Limitations and Threats to Validity
The FoSCI and service candidate evaluation framework proposed in this paper only focus on functionality, modularity, and evolvability, without considering other quality attributes, such as performance, security, or reliability. The FoSCI framework is open to incorporate other quality attribute assessments in the future.

Our method relies on (black-box) execution traces instead of (white-box) source code. This black-box method is appropriate in the following scenarios: a) The source code of the monolithic application is not available. b) Only part of the functionality within a monolith needs to be extracted into service candidates, such as core business capabilities, commonly-used functions, or frequently-updated features [4], [35]. In practice, the transition process is usually incremental. c) Both the executable monolith and its sufficient functional test suite are available.

Even though migration from monolithic application to service-based architecture brings benefits such as better maintainability and scalability, the process is complex and costly. Most importantly, microservice architecture is not a silver bullet, nor is the only way to improve maintainability. Not all applications should be designed as microservices. If the objective is to improve the quality of a poorly-designed system, the architect should first consider refactoring rather than migrating to microservices.

In this paper, we selected widely-used web applications as evaluation subjects. Web applications often suffer from maintainability and scalability issues because of their rapid evolution and growth in size and complexity. However, it is hard to guarantee that the evaluation results can be generalized to other types of systems, such as embedded systems. To mitigate this threat, we selected web projects of different sizes, architectural structures, and technology stacks. We plan to conduct experiments using more types of systems in the future, to further address this threat to validity.

Due to the lack of comprehensive and reliable quality evaluation methods to assess and compare service candidates [36], [37], we constructed a systematic measurement suite to quantitatively and consistently assess service candidates against three quality criteria (i.e., functionality, modularity, evolvability) using 8 metrics. These metrics are derived from the service interface information, structural/conceptual dependency, and revision history. The evaluation indicated that our method outperforms other three baseline methods with respect to these metrics, but it still deserves further evaluation if more reliable metrics become available.

SECTION 7Related Work
7.1 Software Decomposition
Decomposition Methods. The goal of software decomposition is to split a large system into small, manageable modules or components (e.g., Parnas et al. [38], Bavota et al. [39]). WCA and LIMBO are classic methods that take static structural relations extracted from source code as input [12], [13], [14]. They both employ hierarchical clustering but use different distance measurements. Other studies employed information retrieval techniques [15], [16]. These techniques considered source files as their text corpora, ignoring the structure of their programming languages. Using the natural language approach, each source code file is represented as a vector of keywords or a distribution of topics, extracted from source code and comments. Lutellier et al. [40] and Garcia et al. [13] performed a comparative analysis of six clustering techniques.

Search-based approaches also have been explored. Mancoridis et al. treated module clustering as a single-objective optimization problem using modular quality as the optimization objective [7]. Praditwong et al. defined software modularization as a multi-objective optimization problem [41]. The objectives included cohesion, coupling and others.

These existing methods rarely consider software functionality. Most of them are motivated by the assumption that developers pursue modules with high cohesion and low coupling [9], [10], [11]. However, Candela et al. [11] pointed out that factors other than cohesion and coupling might need to be taken into account. In contrast, our method is functionality-oriented, generates functional atoms before further clustering, and models multiple optimization objectives based on execution traces.

Evaluation of Methods. Bavota et al. [16] and Praditwong et al. [41] evaluated their methods by measuring the improvement of cohesion and the reduction of coupling. Garcia et al. [13] and Lutellier et al. [40] proposed a suite of metrics to assess the accuracy of architecture recovery by comparing with benchmark architectures.

As mentioned above, in addition to cohesion and coupling, other quality criteria for evaluation should be considered. In addition, oracle architectures normally do not exist. By contrast, our work employs measures extracted from service candidate interfaces and revision history, which is novel and more objective.

7.2 Service Candidate Identification
Identification Methods. Service candidate identification is a form of software decomposition in the realm of service-based architecture, a counterpart of traditional software decomposition. We classify existing methods into two categories: data-oriented and structure-oriented.

Data-oriented methods begin with splitting a data source or database. Levcovitz et al. [6] partitioned database tables into several groups. Then they gathered classes, which access the same group of tables, to compose a service candidate. Chen et al. [42] presented a data-flow based method. The data flow diagrams of business logic need to be provided by the users. Structure-oriented methods employ structural relations from source code. Gysel et al. [43] and Mazlami et al. [5] designed graph-cutting algorithms to generate service candidates.

In general, data-oriented methods start with database partitions or the analysis of data flow graphs of business logic, which cannot be automated. Structure-based methods can relieve manual operation, but these may come at the expense of ignoring high-level business functionality.

Evaluation of Methods. Since the research in service extraction especially microservice extraction is still in its infancy, only a few works have conducted evaluations to validate their methods. Mazlami et al. [5] concluded that their methods could produce microservices with the benefits of team size reduction and less domain redundancy. They have not compared their proposed method with others yet. Chen et al. [42] conducted experiments on two use cases.

None of the prior work conducted comparative and comprehensive evaluations. In this paper, we have conducted experiments using 6 widely-used open-source projects, and extracted execution traces from 3,349,253,852 records in execution logs. More importantly, our method has been evaluated against 8 metrics, assessing Independence of Functionality, Modularity and Independence of Evolvability respectively.

7.3 Cloud Service Extraction
Another branch of related work is cloud service extraction. It aims to transitioning an application to use cloud-based services, taking the advantage of cloud resources. Kwon et al. [44] described two mechanisms to recommend which class should be transformed into cloud-based services by taking into factors such as application performance penalty and business functionality. Moreover, they developed and implemented a set of refactoring techniques and reduced the manual efforts involved in the code transformation. Gholami et al. [45] surveyed the research of migrating legacy applications to the cloud, and discussed relevant issues, approaches and concerns. Tilevich et al. [46] addressed the problem of cloud offloading, i.e., executing the energy-intensive portion of a mobile application in a remote cloud server. The work of Zhang et al. [47] is similar, with the purpose of mobile performance improvement or energy optimization.

7.4 Dynamic Analysis Based on Execution Traces
Dynamic analysis based on execution traces has a rich history in program comprehension [48]. It has been recognized that execution data can not only accurately expose actual software behavior [49], but can also reveal the specific functionality of programs [17], [18].

Many works have utilized execution traces for feature localization [48]. Rohatgi et al. [50] combined static analysis and dynamic analysis for feature location. Safyallah et al. [51] analyzed patterns from execution traces. Li et al. [17] relied on analysis of executions of test cases to accurately recognize entities that contribute to a function. Some research employs execution traces to help form a high-level view of a program. Hamou-Lhadj et al. [52] summarized the content of execution traces, generating UML sequence diagrams. Alimadadi et al. [18] inferred hierarchical motifs from execution traces, which can be used to discover specific functions of the program. Based on the same rationale, our work leverages execution traces to extract service candidates.

SECTION 8Conclusion and Future Work
In this paper, we proposed the FoSCI framework for service candidate identification from monolithic software systems. Moreover, we created a comprehensive measurement system to evaluate service candidates. Compared with three baseline methods, our evaluation results indicate that FoSCI can identify service candidates with better Independence of Functionality, Modularity, and Independence of Evolvability. Identifying service boundaries from monolithic software is a complicated task. Our research will focus on two areas to improve our method in the future:

Guided and interactive service candidate identification. In practice, service identification from monoliths is an iterative and incremental process. To better assist architects or developers, it is important to consider expert knowledge during the design of services. We intend to improve our methods by allowing the user to integrate feedback and guidance.

Further refining service candidates to create executable services. Service candidates are just intermediate products that have the potential to be further implemented as services. The transition from service candidates to deployable services is another goal of our future work.