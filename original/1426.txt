Spectrum-based fault localization (SBFL) techniques are widely studied and have been evaluated to be effective in locating faults. Recent studies also showed that developers from industry value automated SBFL techniques. However, their effectiveness is still limited by two main reasons. First, the test coverage information leveraged to construct the spectrum does not reflect the root cause directly. Second, SBFL suffers from the tie issue so that the buggy code entities can not be well differentiated from non-buggy ones. To address these challenges, we propose to leverage the information of version histories in fault localization based on the following two intuitions. First, version histories record how bugs are introduced to software projects and this information reflects the root cause of bugs directly. Second, the evolution histories of code can help differentiate those suspicious code entities ranked in tie by SBFL. Our intuitions are also inspired by the observations on debugging practices from large open source projects and industry. Based on the intuitions, we propose a novel technique HSFL (historical spectrum based fault localization). Specifically, HSFL identifies bug-inducing commits from the version history in the first step. It then constructs historical spectrum (denoted as Histrum) based on bug-inducing commits, which is another dimension of spectrum orthogonal to the coverage based spectrum used in SBFL. HSFL finally ranks the suspicious code elements based on our proposed Histrum and the conventional spectrum. HSFL outperforms the state-of-the-art SBFL techniques significantly on the Defects4J benchmark. Specifically, it locates and ranks the buggy statement at Top-1 for 77.8 percent more bugs as compared with SBFL, and 33.9 percent more bugs at Top-5. Besides, for the metrics MAP and MRR, HSFL achieves an average improvement of 28.3 and 40.8 percent over all bugs, respectively. Moreover, HSFL can also outperform other six families of fault localization techniques, and our proposed Histrum model can be integrated with different families of techniques and boost their performance.
SECTION 1Introduction
Software debugging is time-consuming and labor-intensive. According to a recent study [1], this process costs nearly 50 percent of developers’ time and efforts. To mitigate the problem, automated debugging attracts much attention, where fault localization (FL) has been recognized as an important step [2], [3], [4]. Xia et al. [5] recently conducted an empirical study and found that FL can actually help developers save debugging time in practice. Another recent study also revealed that developers from industry value automated FL techniques [6]. Specifically, more than 97 percent of the developers consider it essential or worthwhile to leverage automated FL techniques. Besides, FL techniques are essential for automated program repair (APR) techniques (e.g., [7], [8], [9], [10]), which rely mostly on FL to generate a fault space at statement granularity. The effectiveness of FL greatly affects the performance of APR [7], [10]. Therefore, there are strong demands for better FL to improve APR’s performance. As a result, various recent efforts (e.g., [11], [12], [13]) have been made to advance FL.

Spectrum-based fault localization (SBFL) is a major category of FL techniques (e.g., [11], [12], [14], [15], [16]). It constructs a coverage based spectrum by running the passing and failing tests, and then uses the spectrum to compute the suspicious score for each code entity (e.g., statement or method). It assumes that the code entities covered by more failing tests but fewer passing tests are more likely to be buggy. Due to its effectiveness, SBFL has been used by developers for debugging in practice [15], [17], [18].

Even though successes in locating faults by SBFL have been demonstrated, the effectiveness of SBFL is still compromised due to two main reasons [19], [20], [21], [22]. First, SBFL is based only on test coverage information. Although test coverage has been leveraged to approximate a bug’s root cause, it does not pinpoint the root cause of a bug directly [19], [20]. Second, SBFL widely suffers from the tie issue [21], [22]. One typical tie example is that the statements in the same program block have the same suspicious score, since they are equally covered by tests. In such cases, the buggy code entities cannot be differentiated from the non-buggy ones in the same program block.

We propose to overcome these limitations by taking a novel perspective from project version histories. First, a bug’s root cause can be directly reflected in the version history. A bug was introduced into a software project by either the initial code commit or subsequent code commits when the software evolves [23]. In particular, the commit introducing a bug is called a bug-inducing commit [23], [24], and the associated bug-revealing tests start to fail after the bug-inducing commit is adopted [25]. Intuitively, identifying the bug-inducing commit will help locate the root cause (i.e., those buggy statements). Second, the version histories of code entities can help differentiate those suspicious code entities, since different code entities (even in the same block) could have different evolution histories (i.e., modified by different commits). Therefore, it greatly increases the chances to break the tie issue in SBFL.

Our intuition is also inspired by the observations from the debugging practices of popular projects. For example, we observed that developers in project GCC often try to locate the bug-inducing commits first when they work on a reported bug. Comments such as “Confirmed, started with r239357” [26] are often left in bug reports. Similar practices are also observed among other projects. For instance, when debugging SOLR-2606 [27], a developer located the corresponding bug-inducing commit and left a message “I’m fairly certain this is caused by the enhancements made in SOLR-1297 to add sorting functions”. Such message reveals that the bug was caused by the code committed to implement enhancements requested by issue SOLR-1297. After obtaining such knowledge, developers located and resolved this bug quickly. Our observations are also confirmed by the feedbacks from industry (see Section 2.1).

Based on the above intuition and observations, we propose Historical Spectrum-based Fault Localization (HSFL) in this study, which leverages the information of version histories in fault localization. HSFL first identifies the bug-inducing commit in the version history for each bug-revealing test. In other words, it finds the first commit in the version history from which the bug-revealing test cases start to fail. However, code commits are usually tangled [28]. They are often large in size, but only a small part of the code elements introduced in these commits are related to the fault. Therefore, it is very challenging to distill the root causes from the bug-inducing commits. Besides, the time gap between when the bug-inducing commit is checked in and the target version (i.e., the version subject to fault localization) might be large, and lots of commits can be adopted during the period. Therefore, it brings the challenge to trace their evolutions to the target version for fault localization.

To address these challenges, HSFL builds a Historical Spectrum (denoted as Histrum) for each suspicious code entity introduced in the bug-inducing commits. The Histrum traces the evolutions for each suspicious code element from the inducing version (i.e., the version after the bug-inducing commit is adopted) to the target version via history slicing [29]. Specifically, it leverages the information of non-inducing commits (i.e., those commits do not introduce the bug) in the version histories to filter out those noises in the bug-inducing commits. HSFL then computes the suspicious score for each code entity based on the Histrum via leveraging those techniques proposed for SBFL (e.g., Ochiai [30]), where a bug-inducing commit and a non-inducing commit are analogous to a failing test and a passing test, respectively. The key insight of our approach is that those code entities modified by more bug-inducing commits but fewer non-inducing commits are more likely to be the root cause of the bug. HSFL further examines whether those suspicious code entities evolved from bug-inducing commits have been executed by bug-revealing tests in the target version to filter out potential noises for better fault localization.

We evaluated HSFL on 357 real bugs from the Defects4J [31] benchmark. Specifically, we applied HSFL to each of the bugs and located the faulty code entities at the statement level, which is the granularity widely adopted by existing SBFL techniques (i.e., [11], [12], [14], [15], [16]), and required by automated program repair techniques to generate the fault space [7], [8]. We compared the results generated by HSFL with the state-of-the-art SBFL techniques [11]. Our evaluation results show that HSFL can significantly improve SBFL’s performance. For example, HSFL locates and ranks the buggy statement at Top-1 for 77.8 percent more bugs compared with SBFL, and 33.9 percent more bugs for Top-5. HSFL also performs significantly better than SBFL for the evaluation metrics MAP and MRR, with an improvement of 28.3 and 40.8 percent respectively. We also applied other SBFL techniques [32], [33], [34], [35] in the Histrum model, and found that HSFL also achieves significant better performances using other techniques such as Tarantula [32], Op2 [34], Barinel [35] and DStar [33]. We also compared HSFL with other six families of fault localization techniques, including mutation-based, slicing-based, stack trace-based, predicate switching-based, hybrid-based and learning-to-rank-based techniques. Our extensive evaluations show that our proposed approach can not only outperform existing baselines from different families, but also it can boost the performance of existing techniques. Moreover, the results generated by HSFL can significantly improve the performance of the state-of-the-art search-based APR techniques. Specifically, the first correct patch can be searched 3.02 times faster via leveraging the fault space generated by HSFL compared with that generated by SBFL.

In summary, our major contributions are as follows.

Observation: We made observations from both open source communities and industry that version histories contain useful debugging information and bug-inducing commits are helpful to understand and locate software bugs.

Originality: We are the first to leverage bug-inducing commits in facilitating fault localization. Specifically, we propose a novel model called historical spectrum, which builds a spectrum along the version histories in orthogonal to the conventional coverage based spectrum.

Implementation: We implement the proposed idea as a fault localization technique, HSFL, which leverages existing techniques (e.g., Ochiai) to rank all suspicious code entities based on the historical spectrum.

Evaluation: We evaluate HSFL on the Defects4J benchmark and compare it extensively with the state-of-the-art FL techniques from seven different families. The results show that our proposed approach can not only outperform existing baselines from different families, but also it can boost the performance of existing techniques. More importantly, it can also significantly boost the performance of the state-of-the-art automated program repair techniques to find the correct patches.

The rest of the paper is structured as follows. Section 2 presents the motivation and challenges of this work. In Section 3, we present our approach in detail. Experimental setup is introduced in Section 4, and Section 5 presents the experimental results which demonstrate the usefulness of HSFL. In Section 6, we discuss several points related to the performance of our proposed tool. Section 7 discusses the related works and Section 8 concludes this work.

SECTION 2Motivation and Challenges
In this section, we present our observations and the motivation of this study together with the potential challenges.

2.1 Debugging Practice
Version control systems are widely used to manage software evolution. The version histories record how faults are introduced into the software. Such information is important and usually leveraged by developers in debugging. We observe that developers of open source projects often discuss about the information of version histories, especially the bug-inducing commits, in bug reports. A bug-inducing commit is the one that introduces a bug [23]. It causes some tests, called bug-revealing tests, start to fail until the bug is fixed. After the bug-inducing commit is submitted, the bug-revealing test cases start to fail. Specifically, We found that substantial bug reports, 821 and 1,733 bug reports from GCC and Apache projects respectively, contain discussions about bug-inducing commits by searching the keywords of “started with”, “caused by” and “introduced by” among the bug reports tracked in the associated bug-tracking system. We selected these three keywords since by sampling a small set of bug reports randomly, we observed that developers in our selected projects mostly used these keywords to deliver the information of bug-inducing commits. Examples of these bug reports [26], [27] are shown in Section 1. We also observed that the root cause of a bug is frequently correlated with its bug-inducing commits. For instance, we found that for 78.9 percent of those bugs, at least one statement in their bug-fixing commits have been modified by the associated bug-inducing commits. Inspired by this, we further surveyed developers from industry to understand the role of the information of version histories and bug-inducing commits in general practices of debugging and fault localization.

To understand current debugging practices in industry, we designed an online survey following the methodology of an existing work [36] and distributed it to the developers at Microsoft. Before distributing our survey, we conducted pilot interviews with 2 experienced engineers at Microsoft to discuss whether our designed questions and answers are appropriate. Based on the collected comments and feedback, we refined our survey questions in order to ensure that our designed questions are relevant and clear.1 For instance, we used “traces” and “running log” in two options in the first question at the first beginning. However, the involved engineers suggested that these two terms are hard to differentiate in practice and thus might not be good answers. We modified these answers accordingly, and then distributed our survey through the discussion groups at Microsoft, which cover nearly 1,500 developers from multiple products. The survey was posted for a week. We set the time to one week since we observe that there is no increasing number of feedback received after one week. Finally, 109 valid responses were received, and we kept the 103 responses submitted from those developers who have at least 2 years of industrial software development experiences in our analysis. We consider these developers as experienced ones in terms of debugging. The response rate is hard to measure since this survey was posted on discussion groups, which is not mandatory. Besides, it is hard to measure exactly how many developers have viewed the post during the one week.

We are first curious about what information is useful and has been leveraged by developers in debugging in practice. Five options are provided, which are log information, test coverage, stack traces, version histories and others. The design of these five options is motivated by the findings of existing studies [23], [37], [38], [39], [40] and refined after the pilot interviews. Fig. 1 shows the results, and we can see that among the 103 responses analyzed, 75.7 percent (78/103) of the developers have ever leveraged version histories for debugging, and the ratio is comparable with the ratios of stack traces and log information. This demonstrates the usefulness of version histories in debugging. We are then curious to know what specific information of version histories that these 78 developers think is useful for debugging, and Fig. 2 shows the results. Specifically, 93.6 percent (73/78) of them think bug-inducing commits are useful for debugging, and 74.4 percent of them (58/78) find that regression range (i.e., the range of commits between the last known good version to the first known bad version of a bug) is useful. These results show that the majority of developers (73/103) find bug-inducing commits providing useful debugging information. For those 73 developers, we further asked them in which ways have they leveraged such information for debugging in practice. Fig. 3 shows the statistical results of the usages of bug-inducing commits by these developers. 95.9 percent of them (70/73) have leveraged bug-inducing commits to understand the root causes of the bugs and further locate the faults. However, we find that 74.3 percent (52/73) of these developers conduct the process of fault localization manually due to the lack of automated tool support. We also observe that a substantial of developers mention that they leveraged the built-in tool “git bisect” to search among version histories.


Fig. 1.
What information have you ever used for debugging?.

Show All


Fig. 2.
What information of version histories is useful for debugging?.

Show All


Fig. 3.
In which ways have you ever leveraged bug-inducing commits?.

Show All

Since the conducted survey is not the major contributions of this study, we only disccused partial results in this section. Detailed survey results are available online.2 Nevertheless, the above discussed results confirm our intuition and reveal the following three points. First, the information of version histories, especially the bug-inducing commits, is useful for developers to debug in practice. Second, bug-inducing commits contain rich information of the root causes of software bugs, which is helpful for fault localization. Third, the majority of developers lack automated tool supports to leverage such information.

As revealed by our survey, it is a common practice for developers to leverage “git bisect” to search for the information of bug-inducing commits when debugging for large-scale projects like LLVM and Lucene. Actually, we also observe that such practice can also be generalized to many open-source project communities. Specifically, we selected three large-scale open source projects: Lucene, LLVM and Accumulo. And then we searched their bug reports using the keyword “bisect”. We observe that many bug reports (i.e., in total nearly 100 for the three projects) directly contain such information and deliver the message that developers actually adopted such a heuristic in practice to identify the inducing commits when debugging. Be noted that not all developers who adopted “git bisect” to identify bug-inducing commits will leave such messages on the associated bug reports. Several examples are selected as follows for each project:

For Lucene:

“git bisect blames commit 26d2ed7c4ddd2 on SOLR-10989” 3

“According to git bisect, this was broken by SOLR-8728” 4

For LLVM:

“bisect indicates that r146856 is the first bad commit (constexpr handling improvements.)” 5

“A bisect points to r115374” 6

“My bisect also pointed to r149641 is the first bad commit.” 7

For Accumulo:

“Using git bisect, found the breaking commit to be 659a33e8 as a part of ACCUMULO-4596” 8

“git bisect revealed f599b46 to have introduced this problem (ACCUMULO-3929).” 9

The above examples demonstrate that the practice of searching bisectly among version histories to look for bug-inducing commits is common and feasible, even for large-scale open-source projects. These examples also shed lights on the design of our approach in this study.

2.2 Motivating Example
The previous subsection presents our observations from both open source communities and industry, which motivates us to leverage the information of bug-inducing commits for automated fault localization. However, commits are often large in size and tangled with code modifications for multiple purposes [28]. For example, we investigated the identified bug-inducing commits for the Chart project from Defects4J [31], and found their average size (i.e., number of modified statements) is 436.2 (with a median value of 165). However, the average size of the fixing patches of the corresponding bugs is 3.92 (with a median size of 2). Therefore, locating the buggy code entities in a bug-inducing commit is challenging.

We propose to build a historical spectrum along the version histories to address the challenge. Specifically, we leverage those commits, which are made after the bug-inducing commit but neither introduce nor fix the bug, to help pinpoint the buggy code entities. Those commits are referred as non-inducing commits. Fig. 5 shows the concept of historical spectrum. Suppose vt is the target version for fault localization, and ci is the bug-inducing commit since the bug-revealing tests start to fail since version vi after ci is committed. Those commits made after ci but neither introduce nor fix the bug are non-inducing commits (e.g., ci+1). We build a historical spectrum by analyzing those code entities modified in the bug-inducing commits and non-inducing commits (i.e., those commits displayed in shadow as shown in Fig. 5). Our key insight is that those code entities modified by more bug-inducing commits but fewer non-inducing commits are more likely to be the root cause of the bug.

Let us illustrate our insight using a concrete example shown in Fig. 4, which is adapted from the bug Lang 6 in the Defects4J benchmark [31]. In this example, the target version for bug localization is #0b5c5d1, and the buggy statement is line 95. However, the suspicious value of the buggy statement reported by the state-of-the-art technique [11] using formula Ochiai [30] is 0.180. It is only ranked at 98th in the suspicious statement list, and there are many ties (e.g., lines 94, 95, 96). These indicate that, conventional SBFL cannot effectively locate the fault. The bug-inducing commit of this bug is #b4255e6, and the bug-revealing tests start to fail after this commit is adopted in the Lang project. Intuitively, those statements introduced by this commit (i.e., statements 88 and 89 in Fig. 4 a) are more likely to be the root cause of this bug. Therefore, we should increase the suspiciousness of statements 94 and 95 in the target version correspondingly (since statement 94 and 95 in #0b5c5d1 are evolved from statement 89 and 90 in #b4255e6, respectively). Meanwhile, we also observe another commit #0cb2ca8 as shown in Fig. 4b, which was made after the bug-inducing commit #b4255e6 but before the target version (commit #0b5c5d1). It changed statement 89 (i.e., evolved to statement 94 in the target version). However, this commit did not change the status of the bug-revealing tests. This indicates that statement 94 in the target version is less likely to be the root cause as compared with statement 95. Therefore, we can decrease the suspiciousness of statement 94. As a result, we can break the tie of lines 94, 95, and 96, which further confirms our intuition that the version histories can help relieve the tie issue. In this way, we can better locate the buggy statement (i.e., statement 95 in Fig. 4c). The priority of other statements that are irrelevant to the bug in the bug-inducing commit #b4255e6 can be similarly lowered.


Fig. 4.
An adapted example from bug Lang 6.

Show All


Fig. 5.
Concept of historical spectrum.

Show All

2.3 Challenges
Three major challenges hinder the process of leveraging version histories in fault localization.

Identifying bug-inducing commits precisely is difficult. Whether we can identify the bug-inducing commits for all bug-revealing tests is unknown since some tests might be complex in their designs and cannot be successfully executed on previous versions. To address this challenge, we minimize the testing logic for each bug-revealing test before executing it to make it runnable on more previous versions (see Section 3.1).

Precisely tracking code evolution is challenging. Precisely mapping code entities from the inducing version (i.e., the version after the bug-inducing commit is made) to the target version is challenging since the gap between these two versions might be large. As shown in the example in Fig. 4, the buggy statement is line 89 at the inducing version while it evolves to line 95 at the target version. To resolve this challenge, we leverage history slicing [29] to track the evolutions of code entities from the inducing version to the target version (see Section 3.2).

Handling the noises of tangled commits is non-trivial. commits are usually tangled with other irrelevant code modifications [28] and large in their sizes, and thus it is challenging to differentiate relevant statements from them. For example, we investigated the identified bug-inducing commits for the Chart project from Defects4J, and found their average size (i.e., the number of modified statements) is 436.2 (with a median of 165). However, the average size of the fixing patches for the corresponding bugs is 3.92 (with a median of 2). Those irrelevant statements might bring noises and thus decrease the performance of fault localization. To tackle this challenge, we apply those techniques designed for conventional SBFL (i.e., Ochiai [14] and Tarantula [32]) on the historical spectrum, to differentiate those buggy statements from other irrelevant ones that are modified in the bug-inducing commits. We also examine whether those suspicious code entities evolved from bug-inducing commits have been executed by bug-revealing tests in the target version in order to further reduce noises in the historical spectrum (see Section 3.3).

SECTION 3Approach
We propose Historical Spectrum based Fault Localization in this paper. HSFL takes the source code, the version history and the associated test suite of a project as inputs. It works at the statement level and contains three steps. The overview of HSFL is shown in Fig. 6.


Fig. 6.
Overview of HSFL.

Show All

First, it identifies the bug-inducing commit from the version histories for each bug-revealing test case to identify a set of suspicious code entities. Second, HSFL constructs a historical spectrum (i.e., denoted as Histrum) to trace the evolutions of each suspicious code entity from the bug-inducing version (i.e., the version after the bug-inducing commit is adopted) to the target version via history slicing [29]. Third, HSFL computes the suspicious score for each code entity based on Histrum. In particular, it works like SBFL, where a bug-inducing commit and a non-inducing commit are analogous to a failing test and a passing test in SBFL, respectively. As such, the ranking formulae designed for SBFL (e.g., Ochiai [30] and Tarantula [32]) can be deployed to compute the suspicious score based on the Histrum. HSFL further leverages the conventional coverage based spectrum used in SBFL to further differentiate buggy code entities from non-buggy ones in Histrum to generate the final rankings.

3.1 Identifying Bug-Inducing Commits
As observed from the open source community, developers identify bug-inducing commit by finding the first commit on which the bug-revealing test starts to fail. For instance, debugging activities such as “Confirmed, the test passes before this commit (LUCENE-6758) and fails after” [41] can be frequently observed in bug reports. Based on such observations and a recent study [25], we formally define bug-inducing commits as follows in this study:

Definition 1.
Given a bug manifested by a bug-revealing test tf, the associated bug-inducing commit is the commit before which tf passes and after which tf fails.

To identify the bug-inducing commits, HSFL conducts binary search on the complete version history (automated by git bisect) following the heuristic used by existing approaches [25], [42], [43]. Such a heuristic is also adopted by developers from open source community as observed in the debugging practices discussed in Section 2.1. Specifically, we extract the bug-revealing test tf from the target version and then execute it on older versions of the program. However, identifying the bug-inducing commit precisely for a bug-revealing test tf can be non-trivial due to the following reasons.

First, a unit test case might involve the testing logics of several bugs. To ease the explanation, we refer to the manifestation of a bug revealed by a bug-revealing test as the “failing signature”, which includes the information about the point of the failure and the error message generated in the failing test run. For example, the test method testCreateNumber() in project Lang tests the functionalities of multiple bugs (i.e., issues Lang-521 and Lang-693) as shown in Fig. 7. Suppose our target bug for fault localization is Lang-521 here. However, running such a test case on previous versions might fail due to different bugs, manifested by different failing signatures thrown by the test (e.g., “createNumber LANG-521 failed, expected..., but ...” or “createdNumber LANG-693 failed, expected..., but ...”). This is because some bug fixes (e.g., fix for Lang-693) might be reverted if we roll back to previous versions. As a result, the test case will fail if it is executed on these versions. This will hinder us to identify the bug-inducing commit for the target bug since the bug-revealing test fails due to another bug (i.e., Lang-693). Our approach takes the following steps to address this challenge. It first analyzes the failing signature of the bug-revealing test executed on the target version to obtain the failure point triggering the target bug (e.g., the assertion statement line at 21 in Fig. 7). It then comments out other assertion statements (e.g., line 24) within the method to remove those testing logics for other bugs. Those statements constructing the data structures for assertion statements (e.g., line 20) will be kept to make the code of the bug-revealing assertions runnable.

Fig. 7. - 
A test case of project lang.
Fig. 7.
A test case of project lang.

Show All

Second, some test cases might require extra self-defined features to construct complex objects for testing. These test cases might not be able to be executed successfully on previous versions if the required features have not been implemented on that version. For example, some test cases of project Lang require an extra class FormatFactory to construct objects to test the functionalities in class ExtendedMessageFormat. However, class FormatFactory is introduced in version #695289c. Therefore, those tests requiring this class cannot be run successfully on those versions prior to #695289c. For such cases, identifying the bug-inducing commit precisely is difficult. To handle these cases, we introduce the concept of likely-inducing commits. Likely-inducing commits include the first commit on which the bug-revealing test fails with the targeted failing signature and those commits on which the bug-revealing test is unable to run successfully.

For each bug-revealing test tf, we can identify a bug-inducing commit or a range of likely-inducing commits. The identified bug-inducing commit can be either an initial code commit or a subsequent code commit during software evolution [23]. If the bug-inducing commit is an initial code commit, it indicates that the first version of the source file contains the bug. The bug is introduced by subsequent code modifications otherwise. The initial code commit is usually larger in its size compared with subsequent commits [23]. Different types of inducing commits would have the impacts on the performance of fault localization, which is discussed in Section 6.2. For a target bug, we can identify a set of bug-inducing commits CI or a set of likely-inducing commits CL, since there might be multiple bug-revealing tests for the target bug. Those commits submitted prior to the target version and do not belong to either CI or CL are denoted as non-inducing commits CN with respect to the target bug since they do not change the status of the bug-revealing tests. For each commit c∈CI∨CL, we denote the statements introduced (i.e., modified or originally added) in it as suspicious code entities (i.e., denoted as SH).

3.2 Constructing Historical Spectrum
To leverage the suspicious code entities SH obtained from bug-inducing commits to locate faults at the target version vt, HSFL constructs Histrums for SH so as to map the statements in SH to the ones in the target version vt. It also tracks the evoluations of SH to see if they have been further modified by other commits subsequent to the bug-inducing commit.

Fig. 8 shows an example of a constructed historical spectrum. Suppose c1 is a bug-inducing commit, and statements 5 and 6 are modified by c1. In order to leverage such information to locate faults in the target version v5, Histrum tracks the evolution of each statement to see whether it has been further modified by other commits. For example, statement 6 has been further modified by two non-inducing commit (i.e., c2 and c3) and evolves to statement 8 at v5. Statement 5 has been further modified by one bug-inducing commit (i.e., c4) and evolves to statement 7 at v5. Here, c4 is another bug-inducing commit identified by other bug-revealing test for the same bug.


Fig. 8.
An example of historical spectrum.

Show All

To construct historical spectrum, we use history slicing [29] to track the modification of SH from the bug-inducing version to the target version. Without loss of the generality, we suppose the version history is ⟨v1,…,vj,vj+1,…,vn⟩, where v1 is the bug-inducing version and vn is the target version. For each pair of two consecutive versions ⟨vj,vj+1⟩, we use the function Mj↦j+1(s) to represent the statement in vj+1 that is mapped from the statement s in vj. We leverage Gumtree [44] to analyze the changes between two versions and remove those non-semantic changes (e.g., formatting or modification of comments). There are three different types of changes made between any two versions, which are deletion, insertion, and update. Fig. 9 shows the change hunks for these three types of changes, where A and C denote the contextual part and B denotes the changed part. Since the statements in hunks Aj and Cj in version vj are unchanged, we can directly map them to those in hunks Aj+1 and Cj+1 in the next version vj+1. There are three cases of the changed hunks. The mappings for statements in a deleted or inserted hunk can be readily built as follows. In the case of deletion, Mj↦j+1(s)=null,s∈Bj, since the statements in hunk Bj are deleted and thus the mappings are null. In the case of insertion, there are no statements in vj that can be mapped to the statements in hunk Bj+1 at version vj+1. The case for update is more complicated. A continuous set of statements Bj are modified to Bj+1 as shown in Fig. 9c. To find the optimum mappings from Bj to Bj+1, we follow the work of history slicing [29] and approach it as the problem of finding the minimum matching of a weighted bipartite graph. The weight between any two lines as shown in Fig. 9c is computed as their Levenshtein Edit Distance [45].


Fig. 9.
Line mappings between two consecutive versions of deleted, added and updated change hunks.

Show All

A bipartite graph is a graph whose vertices can be divided into two disjoint and independent sets such that every edge connects a vertex in one set to another. In our setting, we regard each statement as a vertex, and thus we have two disjoint sets of vertices Bj and Bj+1. We connect each statement in Bj to each of the statements in Bj+1 with the weight of Levenshtein Edit Distance [45] between these two lines of statements. For example, as shown in Fig. 9c, we connect line 2 in Bj to each of the statements in Bj+1 (i.e., line 2 to 4). To obtain the Levenshtein Edit Distance, we first tokenize each line of statement to a vector of words following existing heuristics [23]. We then calculate the minimum number of single-word edits (insertions, deletions or substitutions) required to change one vector of words into the other. The smaller the number, the higher similarity between these two statements. We finally find the minimum weight bipartite matching using the Kuhn-Munkres algorithm [46], and record the identified best mapping between these two hunks in function Mj↦j+1. In our example shown in Fig. 9c, Mj↦j+1(5)=4 and Mj↦j+1(4)=null.

Our goal is to obtain M1↦N(s), which finds the statement in vn that is mapped from the statement s in v1. Using the function Mj↦j+1(s) for each two consecutive versions, we can gradually calculate M1↦N=MN−1↦N∘MN−2↦N−1∘…∘M1↦2(s). Note that not all statements in SH can be mapped to the target version since some statements might be deleted during evolution and the mapping function will return null for such cases. Using the function M1↦N, we can successfully map the statements in SH in the bug-inducing version to the statements in the target version. Similar procedures are conducted for those likely-inducing commits in CL.

3.3 Ranking Suspicious Statements
After mapping SH to the target version based on the Histrum, we can obtain a set of suspicious statements SC at the target version vt. Specifically, SC = {M1↦N(s),∀s∈SH}. HSFL then ranks the statements in SC to locate faults. The main challenge is to differentiate the buggy statements from those irrelevant ones since SC might contain noises (i.e., statements irrelevant to the bugs).

To address this challenge, HSFL first leverages the historical spectrum built in the second step. Specifically, we leverage the history spectra information to compute their suspiciousness of being the root cause of the targeted bug. The intuition is that those code entities modified by more bug-inducing commits but fewer non-inducing commits are more likely to be the root cause of the bug. It works like SBFL, where a bug-inducing commit and a non-inducing commit are analogous to a failing test and a passing test in SBFL, respectively. As such, the techniques designed for SBFL (e.g., Ochiai [30] and Tarantula [32]) can be deployed to compute the suspicious score based on the historical spectrum. Specifically, we use Ochiai [30] by default to compute the suspicious score for each statement s∈SC in HSFL since it has been reported to be the optimum formula for SBFL [2]. In particular, we investigate the impact of different SBFL formulae on HSFL in Section 5.2. Suppose that ci is the bug-inducing commit of s and it has been further modified by a list of commits C=⟨ci,…,cj,cj+1,…,cn⟩, we can calculate its suspicious as
Histrum(s,ci)=induce(s)NI∗(induce(s)+noninduce(s))−−−−−−−−−−−−−−−−−−−−−−−−−−√,(1)
View Sourcein which induce(s) denotes the number of inducing commits that modified statement s, specifically, induce(s)=|{c:c∈CI∧c∈C}|; noninduce(s) is the number of non-inducing commits that modified statement s, specifically, noninduce(s)=|{c:c∈C∧c∉CI}|; NI denotes the total number of bug-inducing commits which is |CI|. Let us further illustrate this using our example shown in Fig. 8. The suspicious score for statement 7 at the target version is 1, which is calculated as 2/2∗(2+0)−−−−−−−−√, while the suspicious score for statement 8 is 0.408 (1/2∗(1+2)−−−−−−−−√). Therefore, statement 7 is more likely to be the root cause of the bug compared with 8.

However, a statement s at the target version might have multiple values obtained from the Histrum model since it is possible that s is affected by multiple bug-inducing commits. For the example shown in Fig. 8, HSFL will also build another historical spectrum starting from v4 after the bug-inducing commit c4 is adopted. Therefore, statement 7 in the target version will have another suspicious value. We use the maximum value of Histrum(s,ci) as the final score for statement s. Specifically, Histrum(s)=max{Histrum(s,ci),ci∈CI}.

To further help differentiate buggy statements from irrelevant ones in SC, HSFL leverages the conventional coverage based spectrum used in SBFL. This intuition follows that of existing FL techniques [2], [11], where buggy statements are more likely to be executed by failing tests than passing tests in the target version vt. By integrating this with Histrum, HSFL produces the final results
HSFL(s)=⎧⎩⎨⎪⎪(1−α)∗SBFL(s)(1−α)∗SBFL(s)+α∗Histrum(s)0s∈A∧s∉SCs∈A∧s∈SCotherwise,(2)
View Sourcein which A denotes the set of suspicious statements executed by the bug-revealing tests at vt, and α is the weight of combining Histrum and SBFL. By default, we set α to 0.5. We investigate the effect of α in the overall performance of HSFL in Section 5.3. In Equation (2), we set the final scores as 0 for those statements that have not been executed by the bug-revealing tests on vt. The intuition is that a statement is unlikely to be the root cause if it has not been executed by any of the bug-revealing tests on the targeted version following existing studies [2], [11], [33]. In this way, HSFL can further eliminate the nosies in SC caused by the potential tangled statements in the bug-inducing commits.

For likely-inducing commits in CL and the corresponding suspicious buggy statements SH, similar procedures are conducted. However, since those commits do not definitely introduce the bug, we decrease the effects of the Histrum model by adding a weight to the value obtained from Equation (1). Specifically, HistrumL(s)=Histrum(s)/|CL|. The larger range of the likely-inducing commits, the smaller weight it gets. The intuition is that the likelihood of each likely-inducing commit in set CL to be the bug-inducing commit is decreasing with the increase of size of CL.

Using the final scores obtained by HSFL(s), we then rank all the suspicious statements at the targeted version vt.

SECTION 4Experiment Setup
4.1 Subjects
We evaluate the effectiveness of HSFL on the benchmark dataset Defects4j [31]. This benchmark contains substantial real bugs extracted from large open source projects, and it was built to facilitate controlled experiments in software debugging and testing research [31]. Defects4J has been widely adopted by recent studies on fault localization and program repair (e.g., [11], [47], [48], [49]). Following existing studies [12], [13], we use all the five projects in Defects4j of version 1.0.1 with a total of 357 real bugs as subjects for our experiments. Their demography is shown in Table 1.

TABLE 1 Subjects for Evaluation

4.2 Measurements
To measure the effectiveness of HSFL, we adopt the following three widely-used metrics in our study [12], [13], [23].

Top-N. This metric reports the number of bugs, whose buggy entities (i.e., statements in our evaluation setting) can be discovered by examining the top N(N=1,2,3,…) of the returned suspicious list of code entities. The higher the value, the less efforts required for developers to locate the bug, and thus the better performance.

MRR. Mean Reciprocal Rank [50] is the average of the reciprocal ranks of a set of queries. This is a statistic for evaluating a process that produces a list of possible responses to a query [51]. The reciprocal rank of a query is the multiplicative inverse of the rank of the first relevant answer found. This metric is widely used to evaluate the ability to locate the first buggy statement for a bug [13], [23]. The larger the MRR value, the better the performance.

MAP. Mean Average Precision [52] is by far the most commonly used traditional information retrieval metric. It provides a single value measuring the quality of information retrieval performance [51]. It takes all the relevant answers into consideration with their ranks for a single query. This metric is also widely used to evaluate the ability of approaches to locate all the buggy statements of a bug [13], [23]. The larger the MAP value, the better the performance.

When multiple statements have the same suspicious score, we use the average rank to present their final rankings, following the strategy to handle the tie issues widely adopted by existing fault localization techniques [11], [12], [53], [54].

4.3 Research Questions
This study aims to answer the following research questions.

•• RQ1: How does HSFL perform in locating real bugs?

To answer this question, we apply HSFL to the 357 real bugs from Defects4J as shown in Table 1, and then evaluate the results using the metrics described in Section 4.2. We also compare our results with those obtained by conventional SBFL reported by the state-of-the-art technique [11]. We select SBFL techniques as the baseline in this RQ, since SBFL is the most widely investigated technique and has been reported to achieve good performance compared with the others [55]. We compare HSFL with other baselines systematically in the subsequent research questions.

•• RQ2: How do different formulae affect the performance of HSFL?

We use Ochiai [30] by default in HSFL since it has been reported to be the best formula for SBFL [2], [11]. However, there are multiple formulae proposed and it is yet unknown whether Ochiai is the optimum for HSFL. In this research question, we choose the five best-studied SBFL formulae [2], [11] and investigate how these different formulae affect the performance of HSFL. The five adapted SBFL formulae for HSFL are shown in Table 2. In these formulae, induce(s), noninduce(s) and NI are defined in the same way as described in Equation (1), and NN denotes the total number of non-inducing commits for the target bug.

TABLE 2 The Five Adapted SBFL Formulae for HSFL

•• RQ3: How does the weight α affect HSFL’s performance?

HSFL sets the weight to 0.5 by default to let the coverage based spectrum has the same weight as Histrum. However, it is yet unknown whether the default value is the optimum. In this research question, we investigate the effect of this weight α on the overall performance of HSFL. Specifically, we change the weight from 0.0 to 1.0 with a step size of 0.1, and then examine the performance of HSFL based on Ochiai.

•• RQ4: Can HSFL improve the performance of search-based automated program repair?

Automated program repair techniques extensively rely on SBFL to generate the fault space [7], [9], [47], [48], which affects the search space of search-based APR techniques [7]. Existing search-based APR techniques are known to suffer from the search space explosion problem [8]. Therefore, better fault space is always demanded to improve the efficiency for searching the correct patches [56]. This research question investigates the practical usefulness of HSFL in improving the performance of the state-of-the-art search-based APR techniques.

•• RQ5: Can HSFL outperform other families of fault localization techniques?

Besides spectrum-based fault localization techniques, there are many other families of techniques proposed over the years with the aim to locate suspicious code elements at the statement level. Based on recent studies and a systematic survey [11], [55], we summarize existing techniques to the following nine families:

Different techniques are proposed via leveraging divergent sources of information, such as the test coverage, test results from mutating the program, dynamic program dependencies, crash reports and so on. Moreover, recent studies proposed the hybrid techniques, which leverage multiple sources of information. For instance, Pearson et al. proposed to combine mutation-based and spectrum-based techniques together [11]. Learning-based techniques also combine multiple sources of information. Specifically, they treat the results of each technique as individual features, and then leverage machine learning techniques to learn the optimum way to combine them automatically. Learning-based techniques differ themselves from hybrid ones in that they need a separate set of data for model training.

In this RQ, we compare HSFL with these techniques with the aim to further investigate the effectiveness of HSFL. IR-based and History-based are excluded in our comparison since these two families of techniques have been reported to achieve extremely poor performance at the statement level [55]. We compare and integrate HSFL with the state-of-the-art learning-based technique in a separate research question (i.e., RQ7) since it requires to divide the data into a training and a testing part and then involves a training process. As a result, 10 baselines from six different families have been selected for comparison in this RQ in total. We compare their performances with HSFL based on the Defects4J dataset in terms of metrics MAP, MRR and Top-N.

•• RQ6: Can our Histrum model boost the performance of other families of fault localization techniques?

Our proposed Histrum model provides a novel perspective to locate faults in terms of evolution history, and it can actually be combined with any families of fault localization techniques besides SBFL. In this RQ, we investigate whether Histrum can boost the performance of other types of fault localization as displayed in Table 3. Specifically, similar to Equation (2), we integrate Histrum with a FL technique as follows:
Boost(s)=⎧⎩⎨⎪⎪(1−α)∗FL(s)(1−α)∗FL(s)+α∗Histrum(s)0s∈A∧s∉SCs∈A∧s∈SCotherwise,(3)
View Sourcein which FL denotes an existing FL technique (e.g., MUSE [57], MCBFL [11]), A denotes the set of suspicious statements identified by the FL technique, and α is the combining weight. We then compare the boosted results after integrating Histrum with the results of the original FL technique on the Defects4J benchmark.

TABLE 3 Popular Families of FL Techniques
Table 3- 
Popular Families of FL Techniques
••null RQ7: Can our approach boost the performance of learning-to-rank techniques?

Learning-to-rank techniques were proposed to combine multiple FL techniques together [55], [63]. The basic idea is to treat the suspicious score produced by each technique as a unique feature and then use machine learning techniques to find the model that ranks the faulty statement as high as possible. In this RQ, we investigate whether our proposed technique can be leveraged as a feature to boost the performance of existing learning-to-rank techniques. Specifically, we choose the state-of-the-art learning-to-rank technique [55] for investigation.

Pearson et al. have evaluated multiple SBFL techniques on the Defects4J recently [11]. They provided the oracle (i.e., the buggy statements) and the conventional coverage-based spectrum for each bug. Zou et al. recently have conducted a systematic empirical study to investigate different families of FL techniques and their combinations. They provided their experimental data and the results of different families of FL techniques as well as the newly proposed learning-to-rank technique. To facilitate the reproduction of our evaluation results, we leverage those publicly available dataset to generate the results of SBFL and other baseline approaches instead of instrumenting and computing by ourselves. We implemented HSFL in Java. Our experiments are run on a CentOS server with 2x Intel Xeon E5-2450 CPU@2.1 GHz and 192 GB physical memory. All the experimental data are publicly available at: https://github.com/justinwm/HSFL

SECTION 5Evaluation Results
In this section, we answer the four designed research questions.

5.1 RQ1: Effectiveness of HSFL
To answer RQ1, we present the results of HSFL evaluated on the five projects shown in Table 1. Specifically, we use Ochiai [30] in Histrum and the default combining weight α=0.5 in HSFL. The results of SBFL are also generated using Ochiai. Fig. 10 displays the results of HSFL and SBFL in terms of MAP and MRR, which show that HSFL outperforms SBFL for all the five subjects in terms of both of the two metrics. The improvement of MAP ranges from 16.0 to 57.2 percent, and the improvement of MRR ranges from 22.8 to 94.0 percent. On average, HSFL is able to achieve an improvement of 28.3 and 40.8 percent for MAP and MRR respectively.


Fig. 10.
Results of MAP and MRR of HSFL and SBFL.

Show All

Fig. 11 shows the results of HSFL and SBFL evaluated by metric Top-N. HSFL ranks the buggy statement at Top-1 for 64 bugs, which is 28 more than SBFL, achieving an significant improvement of 77.8 percent. HSFL ranks the buggy statements for 146 bugs within top 5, 177 within top 10, and 205 within top 20, achieving an improvement of 33.9, 18.0 and 10.8 percent respectively. The rankings of buggy statements is crucial to measure the usefulness of fault localization techniques. Developers usually only spend the efforts to inspect the top-ranked suspicious statements. e.g., over 70 percent developers only check Top-5 ranked elements [6]. These results shown in Figs. 10 and 11 indicate that HSFL is more effective in locating bugs compared with SBFL and is more useful for developers in practice.


Fig. 11.
Results of Top-N of HSFL and SBFL.

Show All

5.2 RQ2: Effect of Different Formulae in HSFL
To answer RQ2, we present the results of HSFL using different formulae as shown in Table 2 in the Histrum model. For the combining weight α, we still use the default value 0.5. Note that when we use a formula (e.g., DStar [33]) in Histrum, we also adopt the same formula for the conventional SBFL in Equation (2). Tables 4 and 5 show the results of MAP/MRR and Top-N respectively. In these two tables, different columns represent different metrics evaluated while different rows represent the projects with different techniques used. The bottom portions of the two tables show the summary of the results (i.e., the weighted average results for MAP and MRR, and the sum numbers for Top-N). Fig. 12 displayed the weighted average results over the five subjects of SBFL and HSFL in terms of MAP and MRR using the five different formulae.


Fig. 12.
Average results of SBFL and HSFL in terms of MAP and MRR using different formulae.

Show All

TABLE 4 Performance of HSFL and SBFL Using Different Formulae Evaluated by MAP and MRR

TABLE 5 Performance of HSFL and SBFL using Different Formulae Evaluated by Top-N

In terms of MAP and MRR, HSFL achieves better performance than SBFL for all subjects using different formulae. On average, adopting Ochiai in HSFL achieves the optimum performance (i.e., with an average MAP of 0.246 and MRR of 0.288), and also achieves the optimum improvement compared with SBFL (i.e., with an average improvement of 28.3 percent for MAP and 40.8 percent for MRR). The formula Barinel achieves the second best performance with an average MAP of 0.238 and MRR of 0.272, and it achieves an average improvement of 26.0 and 38.3 percent for MAP and MRR respectively. Adopting the formulae Tarantula, DStar and Op2 in HSFL also achieves better results compared with SBFL. Specifically, the improvements for MAP are 25.5, 17.8 and 16.1 percent while the improvements for MRR are 34.1, 23.2 and 21.2 percent.

Similar results are observed using the metric Top-N. Adopting the formula Ochiai in HSFL achieves the optimum performance (e.g., locating 64 bugs at Top-1 and 146 at Top-5), followed by the formula Barinel (e.g., locating 59 bugs at Top-1 and 140 at Top-5). HSFL also outperforms SBFL by adopting the other three formulae in the Histrum model.

Adopting Ochiai [30] in HSFL achieves the best performance on average, we then use the one-sided Mann-Whitney U-Test [64] to see whether it is significantly better than the other four techniques. The results show that the performance of adopting Ochiai [30] in HSFL is only significantly better than that of adopting Op2 (p<0.05) while the difference is not significant for the other three techniques. These indicate that techniques Ochiai [30], Tarantula [32], DStar [33] and Barinel [35] are suggested to be applied in HSFL. However, all these techniques are designed for the spectrum built from conventional testing coverage, whether our proposed historical spectrum requires specific designed techniques to achieve the optimum performance remains unknown since these two types spectrum are different intrinsically. We leave the design of specific techniques for Histrum as our future work.

5.3 RQ3: Effects of the Combining Weight α
To answer RQ3, we present the results of HSFL using the formula Ochiai [30] while using a series of different weights α (i.e., from 0.0 to 1.0 with a step size of 0.1) to combine Histrum and SBFL in HSFL. Fig. 13 plots the results of HSFL with respect to MAP and MRR for all the subjects. Specifically, the x-axis represents the weight α used in Equation (2) and the y-axis represents the values of MAP and MRR. From the results, we can see that the performance HSFL share similar patterns for the five different subjects. The performance of HSFL is increasing when α is small (≤0.5). It reaches its peak when α is around 0.5 and 0.7, and then starts to decrease when α continues increasing. When averaged over all the five subjects, HSFL achieves its optimum performance when α is 0.5. These results indicate that HSFL obtain its best performance when the Histrum model and SBFL model are considered to be similarly important.


Fig. 13.
The performance of HSFL with different combination weight α of histrum.

Show All

When α=0, HSFL is equivalent to the conventional SBFL. When α=1, HSFL only includes Histrum model. As we can see, only using one of the two models in HSFL cannot achieve the optimum performance, while combining both models with similar weights performs the best. We found that using only the SBFL model or the Histrum model will result in serious tie issues via further investigation, which is caused by the limited number of bug-revealing tests. Due to the limited number of bug-revealing tests, SBFL is known to suffer from the tie issue problem [21], [22]. The Histrum model also suffers from this problem since the number of bug-inducing commits is limited as a result of the limited number of bug-revealing tests (note that HSFL only identifies one bug-inducing commit for one bug-revealing test). These two models complement well to each other, and thus the combination relieves the tie issue. The combination of Histrum and SBFL model is expected to relieves the tie issue, since they use two different spectrum from two divergent dimensions. This is confirmed by the results displayed in Fig. 14, which shows the number of non-buggy statements that are ranked in tie with the buggy statement from the results returned by only the Histrum model, SBFL and the combining of them, HSFL. The number of ties returned by HSFL is significantly smaller compared with that returned by SBFL (p=1.3e-06) using the one-sided Mann-Whitney U-Test [64]. The number is also significantly smaller compared with that returned by Histrum (p=2.6e-14). These results show that our proposed Histrum model can significantly mitigate the tie issue of conventional SBFL.


Fig. 14.
The number of ties of the FL results.

Show All

5.4 RQ4: Usefulness of HSFL in Automated Program Repair
We evaluate the usefulness of HSFL based on the state-of-the-art search-based APR technique CapGen [7] to answer this RQ. It is well-known that the search-space explosion and overfitting are the two long-standing open challenges for search-based APR [7], [8]. In this RQ, we focus on investigating whether the improvements made by HSFL over the fault space can alleviate the search-space explosion problem. Specifically, we leverage HSFL to generate the fault space for CapGen instead of using SBFL. We then investigate the number of candidate patches required to be validated for CapGen in order to find the first correct patch, and compare it with that of CapGen using SBFL. To avoid the side effects caused by the overfitting problem, we only selected those bugs that can be correctly repaired by CapGen for comparison (i.e., those patches that are semantically equivalent to developers-provided ones via manual checking). In particular, two authors were involved in the process of manual checking, and we also measured the inter-rater reliability score (i.e., mean pairwise Cohens kappa [65]) following the latest work to measure the reliability of this process [66]. Among all the 190 generated plausible patches [7], the authors are asked to label them as “correct” and “incorrect”, and the obtained score of the mean pairwise Cohen’s kappa is 0.957. Such a high score demonstrated the reliability of our manual checking process [65].

This setup for this experiment follows the existing study which evaluates the effectiveness of fault localization using automated program repair [10], and Fig. 15 shows the results for all the bugs that are correctly repaired by CapGen. The results indicate that the searching efficiency of CapGen can be significantly improved by leveraging HSFL. For example, for bug Lang 57, due to the improvement of the fault space generated by HSFL, the correct patch can be searched 30 times faster. Only for three bugs (Math 57, Math 70 and Math 63), the searching efficiency of CapGen cannot be improved since the fault spaces have not been improved by HSFL. We further investigated the reasons behind and found that the bug-inducing commits identified for these three bugs are “initial” or “likely”, and such commits are large in their sizes and contain lots of noises (i.e., irrelevant non-buggy statements). As a result, it makes it extremely challenging for HSFL to differentiate buggy statements from non-buggy statements. Such negative cases motivate us to investigate the effects of different types of identified bug-inducing commits on the performance of HSFL (see Section 6.2 for more discussions). However, the first correct patch is ranked only slightly lower (e.g., ranging from 0.04 to 0.09 percent) using the fault space generated by HSFL than that by SBFL for these three cases, and the first correct patch can be searched 3.02 times faster averaged over all bugs. These results still demonstrate the usefulness of HSFL in improving the searching efficiency of automated program techniques, which is significant since search space explosion is a long-haunting challenge in the domain of program repair.


Fig. 15.
Efficiency improved to find the first correct patch.

Show All

HSFL is also expected to improve the CapGen’s effectiveness (i.e., in terms of the number of correctly repaired bugs) via prioritizing correct patches before incorrect plausible ones. However, since CapGen already ranks the correct patches before the incorrect plausible ones for 95.5 percent of the patched bugs, the improvement that can be made by HSFL is marginal.

5.5 RQ5: Comparing with Other Families of FL
To answer RQ5, we present the results of HSFL and other families of FL techniques (i.e., as shown in Table 3) using the Defects4J dataset. Similar to RQ1, Ochiai [30] is used in Histrum and the combining weight is set to α=0.5 by default in HSFL. The results of all baselines are reproduced based on the dataset of existing studies [11], [55]. Fig. 16 displays the results in terms of MAP and MRR, which shows that HSFL outperforms all baselines in terms of both of the two metrics. The best family among all the baselines is the hybrid technique MCBFL, which combines spectrum-based and mutation-based techniques together [11]. Compared with MCBFL, HSFL is able to achieve an average improvement of 8.5 and 8.4 percent for MAP and MRR respectively. The families SBFL (i.e., Ochiai [30] and Dstar [33]) and MBFL (i.e., Metallaxis [58] and MUSE [57]) achieve poorer performance than MCBFL. The average improvement achieved by HSFL over these two families of the techniques is at least 28.3 and 34.0 percent for MAP and MRR respectively. The performance of other families of techniques are poorer as displayed in Fig. 16, and this is consistent with a recent survey [55].


Fig. 16.
The performance of different families of fault localization techniques in terms of MAP and MRR.

Show All

Similar results have been observed in terms of the metric Top-N as displayed in Table 6. Among all the techniques, HSFL achieves the optimum performance. Second to HSFL, the hybrid technique MCBFL achieves the second optimum performance. Specifically, MCBFL ranks the buggy statements for 59 bugs within top 1, 130 bugs within top 5 and 164 within top 10. The improvement of HSFL over MCBFL is 8.5, 12.3, 7.9 and 4.1 percent for Top-1, Top-5, Top-10 and Top-20 respectively.

TABLE 6 The Performance of Different Families of Fault Localization Techniques in Terms of Top-N

The results displayed in Fig. 16 and Table 6 demonstrate the effectiveness of HSFL in locating bugs compared with different families of FL techniques.

5.6 RQ6: Combining with Other Families of FL
Our proposed model, Histrum, constructs spectrum along the version histories, which provides information in terms of a novel perspective to locate faults. As previously investigated in RQ1 and RQ2, it complements well to existing SBFL techniques. In this RQ, we investigate whether the Histrum model can also boost the performance of other families of FL techniques. The Histrum model can be easily integrated into other FL techniques as specified in Equation (3). To answer this RQ, we compare the result of each FL technique with that after integrating Histrum using Equation (3) on the Defects4J benchmark.

Table 7 displays the results in terms of MAP, MRR and Top-N. Specifically, column “FL” denotes the original performance of a FL technique without integrating Histrum, while column “Boost” denotes the results after combining Histrum. As revealed by the results, the Histrum model is able to boost the performance of any family of FL techniques. For the hybrid techniques, the performance can be improved by 24.6 and 25.2 percent for MAP and MRR respectively after integrating Histrum; For SBFL techniques, the results have been well discussed in previous research questions; For mutation-based techniques, the performance can be improved by at least 29.0 and 33.5 percent for MAP and MRR respectively after combining Histrum; For slicing-based techniques, the Histrum model can boost the performance by at least 61.4 and 89.3 percent for MAP and MRR respectively; The improvements for stack trace-based techniques are 65.4 and 75.0 percent in terms of MAP and MRR respectively after incorporating Histrum, and these ratios are 120.6 and 109.3 percent for predicate switching-based techniques.

TABLE 7 Performance of Different Families of Fault Localization Techniques and Their Combinations With our Proposed Histrum Model

One interesting point as revealed by the results is that the hybrid techniques are able to achieve the optimum performance after combining it with Histrum (as displayed with the  background in Table 3). Specifically, it achieves an average MAP of 0.283 and MRR of 0.333, which outperforms HSFL by 14.9 and 15.6 percent respectively.

These results indicate that our proposed Histrum model complements well to existing techniques. Not only can it boost the performance of SBFL, but also the other families of FL techniques. Specifically, combining Histrum with hybrid techniques is able to achieve the optimum performance.

5.7 RQ7: Integrating with Learning-to-Rank Techniques
Recently, Zou et al. proposed to combine all families of FL techniques via learning-to-rank techniques [55]. Specifically, it treats the suspicious score generated by each FL technique as an individual feature, and then leverages rankSVM [67] to learn the optimum model based on a separated training dataset. In total, the results of the following 10 different FL techniques have been selected as features in their experiments:

Techniques Selected as Individual Features. Ochiai, DStar, Metallaxis, MUSE, Union, Intersection, Frequency, StackTrace, PredicateSwitching, BugSpots.

The details of these techniques have been well explained in Table 3. Ten-fold validation has then been adopted to evaluate the performance of the learned model.

To answer this RQ, we compare the original results using the above 10 techniques and that after integrating HSFL, in which case, the results of HSFL have been considered as the 11th feature. The experimental results show that the MAP and MRR can be improved by 3.7 and 5.4 percent respectively, and the results in terms of Top-N are displayed in Fig. 17. In order for comparison, we also display the results of HSFL and MCBFL since it achieves the optimum performance among all the baselines as displayed in Table 7. In summary, the performance of the learning-to-rank technique has been improved after incorporating our proposed approach as an individual feature.


Fig. 17.
Integrating with learning to rank technique. ‘with’ denotes the results obtained with HSFL as a feature while ‘without’ represents the results without considering HSFL.

Show All

SECTION 6Discussions
6.1 Effectiveness of Identifying Bug-Inducing Commits
Our strategy to identify the bug-inducing commits is adopted by existing works [25] and also those developers from open source projects (see Section 3.1). Actually, the detected bug-inducing commits identified by this strategy can be classified into the three types as mentioned in Section 3.1: 1) the bug-inducing commit is precisely identified and it is the initial commit of the buggy source file (i.e., denoted as initial commit); 2) the bug-inducing commit is precisely identified and it is one of the subsequent commits made to the buggy source file (i.e., denoted as subsequent commit); or 3) a set of likely-commits are identified as the approximation of bug-inducing commits since it is hard to identify the precise one. Fig. 18 shows the distribution ratios of these three different types of inducing commits for the five subjects. The majority of the bug-inducing commits identified are the initial commits, which account for 59.7 percent of the bugs. 26.6 percent of the bug-inducing commits identified are the subsequent commits. For a small part of bugs (i.e., 13.7 percent), HSFL can only identify a set of likely-inducing commits. For project Time, the ratio of likely-inducing commits is extremely higher than the other projects. It is because that most of Time’s test cases require extra classes (e.g., DateTime) to create time objects for testing. Those tests can not be successfully run on old versions since the required extra classes have not been checked in to the system. In this case, we cannot precisely identify the bug-inducing commit and thus use likely-inducing commits instead.


Fig. 18.
Ratios of different types of inducing commits.

Show All

We further examined those bugs whose bug-inducing commits are identified as Type 1 or Type 2, we found that 95.8 percent of those bugs’ root causes (i.e., fixing statements) have overlap with the statements modified by the bug-inducing commits identified by this strategy. This demonstrates that the precision of such strategy is high. Such strategy might not be able to retrieve all the bug-inducing commits for a bug if there are multiple since the bug-revealing tests might not be complete. However, HSFL’s goal is not to retrieve all of them. As long as the identified bug-inducing commits are precise, HSFL is able to improve the performance of fault localization. The improvements shown in our evaluation confirmed this point.

6.2 Effects of Different Kinds of Bug-Inducing Commits
Different types of bug-inducing commits will affect the performance of HSFL. Fig. 19 shows HSFL’s performance based on different types of the identified bug-inducing commits. For subsequent commits, HSFL achieves the optimum performance with an average of MAP of 0.316 and MRR of 0.377. Compared with SBFL, it achieves the improvements of 88.8 and 117.4 percent for MAP and MRR respectively. For the type of initial commits, HSFL is able to achieve an average improvements of 11.5 percent and 21.0 percent for MAP and MRR respectively. However, there are no significant improvements for the type of likely inducing commits. Such differences are caused by the different sizes (i.e., modified number of statements) of the bug-inducing commits. Histrum is built on those modified statements of the bug-inducing commits. Therefore, the larger size of the bug-inducing commit, the more noises (i.e., those non-buggy statements irrelevant to the bug) it contains, and thus more difficult for HSFL to eliminate the noises and distill the root causes. Fig. 20 shows the sizes of these three types of bug-inducing commits in terms of the number of modified statements. The median number of modified statements is 102 for subsequent commits of the five subjects, and the number for initial commits is 1,575. This number is even larger for likely-inducing commits, which is around 105. As a result, HSFL performs the best for subsequent commits and worst for likely-inducing commits. Note that, If the identified bug-inducing commits are a set of likely-inducing commits, we aggregate all the modified statements over all these likely-inducing commits. It is common that the likely-inducing commits include the initial commit of the code base with lots of source files checked in to the project. Therefore, it is not surprising that the likely-inducing commits contain the largest number of modified statements.


Fig. 19.
Performance of HSFL evaluated on different types of inducing commits. The green bar  denotes HSFL and the white bar denotes SBFL.

Show All

Fig. 20. - 
The sizes of different inducing commits. The size is measured by the number of modified statements.
Fig. 20.
The sizes of different inducing commits. The size is measured by the number of modified statements.

Show All

Actually, delta-debugging [68] can be leveraged to help us identify the minimum set of changes that cause the bug in those large initial commits. However, it is extremely time-consuming. We leave the exploration of leveraging delta-debugging in HSFL as our future work.

These results indicate that HSFL is suggested to be applied to locate buggy statements for regression bugs whose bug-inducing commits can be precisely identified as “subsequent“ commits. However, it is not suggested to be applied when only a range of likely-inducing commits can be identified.

6.3 Effects of Multiple Faults
Large real-world programs, such as those subjects in the Defects4J dataset, always contain multiple faults at the same time, some of which might influence with each other. Although it is often the case, a recent study [11] indicate that there is no need to correct for this when performing fault localization, as long as the failing tests only reveal one defect at a time. Despite that, we are still curious about whether the performance of HSFL will be affected, when multiple faults coexist in the same program. Therefore, in this section, we investigate the effects of multiple faults.

We have extended the subject programs in the Defects4J dataset to extract programs with multiple faults following the practice of an existing study [35]. Be noted that we only investigate the effects of two faults at the same time in this study, and the investigation on a larger cardinality of faults is left as our future work. Actually, different faulty programs in Defects4J of a same project are different versions of the project. Therefore, to create a subject with two faults, we check whether any of two faults in the Defects4J dataset coexist in the same version of a same project (both of the faults have been introduced but have not been fixed). If so, we include it in our experimental subject. For instance, in the version before commit #a92450e of project Time, both bug Time-19 and Time-20 coexist in the system. This process was conducted manually. In total, we extracted 309 versions that contain two observable distinct faults. To evaluate the performance of HSFL under such scenarios, we combined the bug-revealing tests of both of these two bugs (suppose we cannot distinguish between the bug-revealing tests of these bugs). Then, we run our tool HSFL to see how well can it rank all the buggy statements. Finally, we compare the performance of HSFL under such scenarios with that obtained via running HSFL for each of the bug individually, in terms of MAP and MRR.

Fig. 21 shows the results of the extracted 309 versions. When locating a single fault at a time, HSFL achieves an average MAP of 0.244 and MRR of 0.281. When locating multiple faults (two faults in our experiment) at a time, HSFL achieves an average MAP of 0.239 and MRR of 0.268. On average, the MAP has been decreased by 1.19 percent and that value is 4.40 percent for MRR when working on a scenario containing multiple faults. However, no significant differences have been observed (p-value ≥0.05) for the performance of HSFL under these two experimental settings as shown in Fig. 21. Such results indicate that our proposed technique is able to handle multiple faults effectively and it does not lead to significant deterioration in its performance.

Fig. 21. - 
The performance of HSFL in terms of MAP and MRR in the presence of multiple faults.
Fig. 21.
The performance of HSFL in terms of MAP and MRR in the presence of multiple faults.

Show All

6.4 Overhead of HSFL
Table 8 shows the overheads of HSFL in processing a bug. In the table, each column represents different subjects while each row represents the different steps in HSFL. Specifically, step 1 refers to identifying bug-inducing commits; step 2 refers to constructing historical spectrum and step 3 refers to ranking suspicious elements. On average, it takes HSFL less than three minutes in total to obtain the final rankings of all the suspicious statements. This indicates that HSFL is practical in locating faults for real-world projects. The major overhead comes from the first step to identify bug-inducing commits. Recompiling the project’s previous versions and rerunning the bug-revealing tests on those versions contribute to overall costs of this step. Therefore, the size of the project (i.e., number of source files) and the number of historical commits (since we need to search among the whole history using binary search) affect the time cost of this step significantly. The five projects from the Defects4J benchmark are not large in their scales. A version of these projects can be compiled successfully in around 10 seconds. Therefore, the overhead of HSFL is not high for these projects. It will take HSFL longer time to identify the bug-inducing commits for projects with larger sizes. However, identifying bug-inducing commits is a common procedure for debugging in practice as confirmed by our observations in Section 2. Therefore, bug-inducing commits can be directly leveraged by HSFL once it has been identified by developers. Actually, similar to static analysis, which usually takes quite a long time, a natural fit of our approach to the development cycles is the nightly build cycle [69], [70]. A typical nightly build and test cycle takes 5-10 hours. Therefore, it is applicable to deploy our approach.

TABLE 8 HSFL Overheads in Seconds

6.5 Threats to Validity
One potential threat to validity is the generality of the projects used in our evaluation, which means that our experimental results may not be generalized to other dataset. Specifically, the distributions of different types of bug-inducing commits for different projects might be divergent as shown in Fig. 18, and this will affect the final performance of HSFL. However, real-world open-source projects with different characteristics used in our evaluation, provided by benchmark Defects4J, may at least partially mitigate the risk of over-generalization. Besides, for those projects with different distributions of inducing commits (e.g., likely-inducing commits ranging from 0.0 to 51.9 percent), HSFL can achieve significantly better results compared with SBFL on average for all of them (as shown in Sections 5.1 and 6.2). This reflects the generality of HSFL in improving the effectiveness of existing FL techniques. Evaluating HSFL on more subjects (e.g., projects from industry) and other languages is left for our future work.

Another threat is that our survey presented in Section 2.1 was only conducted at Microsoft, the findings of which might not be generalized to other companies or the open source communities. However, the empirical survey is not the major contribution of this study. The results of the empirical survey demonstrated the usefulness of version histories, especially the bug-inducing commits, in debugging activities. This inspired us to leverage the version history in fault localization. Such findings have also been echoed by the observations from the open source communities as discussion in Section 2.1. Specifically, developers from the open source community also often look for the information of bug-inducing commits when debugging. Such observations reflect the generality of our empirical survey conducted at Microsoft.

SECTION 7Related Work
7.1 Automated Fault Localization
Various automated fault localization techniques have been proposed [2], such as spectrum-based techniques (e.g., [12], [14], [15], [71]), mutation-based techniques (e.g., [3], [57], [72]), slice-based techniques (e.g., [73], [74]), machine-learning based techniques (e.g., [75]), program-state based techniques (e.g., [60]), data-augmented (e.g., [76]), feedback-based (e.g., [77]), and qualitative reasoning-based techniques (e.g., [78]), which are the most related techniques with our proposed technique. Spectrum-based techniques leverage the test coverage information obtained via executing the associated test suite to rank suspicious code elements. Substantial existing SBFL techniques focus on refining fault locating formulae to improve their effectiveness [30], [32], [33], [34], [35]. Mutation-based techniques (e.g., [3], [57], [72]) utilize the test results obtained via mutating the program to improve the effectiveness of fault localization. Slice-based techniques (e.g., [73], [74]) leverage static or dynamic program dependencies to rank suspicious code elements. Machine-learning based techniques combine different FL techniques with machine learning techniques (e.g., [75]). Data-augmented techniques utilize defect prediction, which is built based on version histories, to point out those code elements that are more likely to be buggy in fault localization [76]. Feedback-based approach is able to leverage past diagnosis experience with the aim to improve fault localization performance [77]. Qualitative reasoning-based techniques leverage qualitative reasoning to augment the spectrum information obtained in SBFL to boost the final performance [78]. Specifically, it qualitatively partitions system components, and treats each qualitative state as a new spectrum component to be used in locating faults [78]. We have compared HSFL with most of the above-mentioned techniques as presented in Section 5.5. Data-augmented technique, feedback-based techniques and qualitative reasoning-based technique are not included in our evaluation in Section 5.5, since they are not applicable to our experimental setting, which targets at locating bugs automatically at the fine granularity of the statement level. Data-augmented techniques work at the source file level [76]. This is because, those features, which are extracted (e.g., number of public methods, number of other classes referenced) to augment the spectrum information, are not applicable to measure a single statement in our setting. The feedback-back technique needs to select modeling variables, which is an essential step in the their approach but requires to be done manually [76]. Besides, the authors evaluated the feedback-based technique on artificial faults (i.e., synthesized or manually injected) [76], while our evaluations were performed on real faults extracted from real large-scale projects. Qualitative reasoning-based technique works at the method level [78], and it leverages the parameter and return values of a method to partition spectrum components. Unfortunately, such an approach (i.e., leveraging parameters and return values) cannot be applied to components such as statements. Although these approaches cannot be directly applied to compare with HSFL, we can still incorporate the insights of these approaches to advance the performance of HSFL. Therefore, we left it as our important future work.

There are also other work focused on preprocessing test cases to improve the effectiveness of SBFL [12], [21], [79], [80], [81], [82]. For instance, Xuan et al. [79] proposed test case purification to reduce failing test cases for better performance. It first produced a set of single-assertion failing test cases, and then removed the irrelevant statements through dynamic slicing in them. Finally, it applied traditional SBFL to rank suspicious statements. Other researches are proposed with the aim to improve the performance of fault localization in terms of other aspects. For instance, Zhang et al. proposed to differentiate the contributions of different test cases using the PageRank algorithm [12], which is used to recompute the spectrum information to improving the effectiveness of SBFL. Furthermore, other works integrated more information to SBFL besides test coverage to improve its effectiveness [13], [80], [83], [84], [85], [86], [87], [88]. Sohn and Yoo [13] introduced code and change metrics (e.g., size and age) that have been widely-used in defect prediction to SBFL to improve its effectiveness. More specifically, based on the suspiciousness values from traditional SBFL and these source-code metrics, they utilized learning-to-rank techniques to rank suspicious source methods. Lucia et al. also proposed Fusion Localizer, which leverage the diversity of existing different SBFL techniques to better localize bugs using data fusion techniques [71]. However, all these techniques focused on the target buggy version to improve FL effectiveness (i.e., building coverage based spectrum on this single version). Different from them, HSFL is the first one to leverage the information of version histories for better fault localization. In particular, HSFL builds spectrum along the version histories, which is orthogonal to the traditional execution-based spectrum on the buggy version. Our experimental results demonstrated that the Histrum and the coverage based spectrum complement well to each other.

7.2 Mining Version Histories
Identifying Bug-Inducing Commits. SZZ-algorithm [89], [90] identifies bug-inducing commits by blaming the changed lines in the bug-fixing commits. Essentially, it assumes that the lines changed the bug-fixing commits contain the fault statements, and leverages git blame to identify which commits changed these lines previously. However, the SZZ algorithm is not applicable in our application scenario since it assumes the information of bug-fixing commits (i.e., the buggy statements) is available. On the contrary, the goal of our approach HSFL is to identify those buggy statements. If the bug-fixing commits are available, in which case, the buggy locations of the bug are known, then there is no need to launch our approach for fault localization. As a result, we resort to software testing to identify the bug-inducing commits. Besides, it has been reported that the bug-inducing commits identified by SZZ is not precise recently [25], [91], [92]. Specifically, it is reported that the SZZ algorithm can only achieve an average recall of 68.7 percent. Locus is later proposed to identify the bug-inducing commits based on bug-reports using information retrieval techniques [23]. ChangeLocator identifies bug-inducing commits based a bucket of crash reports [24]. These techniques work in the case where the bug-revealing tests are not available or the efforts of running test suite is extremely high. Delta debugging is another related work that aims at identifying the minimum set of changes inducing a failure [68]. HSFL identifies bug-inducing commits precisely via running the bug-revealing tests on the complete version history using binary search. This strategy is also adopted by a recent work to identify regression bugs [25].

Tracing Source Code Evolution. Understanding the evolution of source code is important for developers. Girba et al.proposed a meta-model to represent the history of code entities at different levels of granularity (e.g., class, method) [93]. Zimmermann et al. proposed annotation graph to identify line changes across several versions. However, code differences between two versions are maintained by version control systems at the granularity of hunks (i.e., a block of code elements) instead of lines. Therefore, precisely tracking the mappings at the granularity of statement level is challenging. History slicing techniques are proposed to tackle this issue [29], [94], [95]. Specifically, it approaches the line mapping problem as the problem of finding the minimum matching of a weighted bipartite graph, and then leverages the Kuhn-Munkres algorithm [46] to find the optimum matching. The historical evolution of source code has also been leveraged to enhance the performance of automated program repair techniques [96]. HSFL leverages history slicing to construct the historical spectrum based on the identified bug-inducing commits. To the best of our knowledge, historical spectrum is novel to FL. It is another dimension information of spectrum, which complements well to different families of fault localization techniques.

SECTION 8Conclusion
We present a novel FL technique, HSFL, in this paper, which leverages the information of version histories in fault localization. The key novelty of HSFL, which allows it to locate more bugs compared with SBFL, is the historical spectrum (i.e., Histrum). Histrum is constructed by tracing the evolution of bug-inducing commits along version histories and is another dimension of spectrum orthogonal to the conventional coverage based spectrum used in SBFL. It reflects the root causes of bugs directly and breaks the tie issues of conventional SBFL significantly. We evaluate HSFL on the benchmark Defects4J, and the results show that HSFL outperforms SBFL significantly. Specifically, it locates and ranks the buggy statement at Top-1 for 77.8 percent more bugs compared with SBFL, and 33.9 percent more bugs for Top-5. Besides SBFL, our evaluation results also show that our proposed approach can outperform and boost the performance of other six families of FL techniques. In the future, we plan to design better techniques specific for our proposed novel model, Histrum, to further improve the effectiveness of HSFL.