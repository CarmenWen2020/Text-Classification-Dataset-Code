Cross-project defect prediction (CPDP) refers to predicting defects in the target project lacking of defect data by using prediction models trained on the historical defect data of other projects (i.e., source data). However, CPDP requires the source and target projects have common metric set (CPDP-CM). Recently, heterogeneous defect prediction (HDP) has drawn the increasing attention, which predicts defects across projects having heterogeneous metric sets. However, building high-performance HDP methods remains a challenge owing to several serious challenges including class imbalance problem, nonlinear, and the distribution differences between source and target datasets. In this paper, we propose a novel kernel spectral embedding transfer ensemble (KSETE) approach for HDP. KSETE first addresses the class-imbalance problem of the source data and then tries to find the latent common feature space for the source and target datasets by combining kernel spectral embedding, transfer learning, and ensemble learning. Experiments are performed on 22 public projects in both HDP and CPDP-CM scenarios in terms of multiple well-known performance measures such as, AUC, G-Measure, and MCC. The experimental results show that (1) KSETE improves the performance over previous HDP methods by at least 22.7, 138.9, and 494.4 percent in terms of AUC, G-Measure, and MCC, respectively. (2) KSETE improves the performance over previous CPDP-CM methods by at least 4.5, 30.2, and 17.9 percent in AUC, G-Measure, and MCC, respectively. It can be concluded that the proposed KSETE is very effective in both the HDP scenario and the CPDP-CM scenario.
SECTION 1Introduction
Software defect prediction (SDP) is one of the most active research areas in software engineering and has drawn increasing attention of both academic and industrial communities [1], [2], [3], [4], [5], [6], [7], [8], [9]. SDP plays a crucial role in helping to allocate the limited test resources reasonably and improve test efficiency, which aims to predict the defect status of software modules before software testing [10]. Most SDP approaches build the prediction models based on the collected historical defect dataset of a project and apply the trained models to predict the defects of new software modules from the same project, which are called as within-project defect prediction (WPDP) [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21]. The historical defect dataset consists of the information of software metrics and the defects. Software metrics mainly include static code metrics (such as Halstead metrics [22], McCabe metrics [23], CK metrics [24]) and process metrics ([25], [26], [27]). However, it is difficult to perform WPDP for new projects or projects without sufficient historical defect data [28], [29].

Cross-project defect prediction (CPDP) has been proposed by researchers to address above issue [29], [30], [31], [32], [33], [34], [35], [36]. CPDP methods aim to predict the defects of a project lacking of historical defect data by using the prediction model trained on other similar projectsâ€™ defect data. Fortunately, there are many publicly available defect datasets in PROMISE repositories [37], which makes it possible to use these public datasets to predict defects of projects lacking historical defect data. These methods can be divided into two categories: instance-based method [29], [30], [31], [32], [35] and feature-based method [33], [34]. Instance-based CPDP method tries to reduce the distribution differences between source and target datasets by selecting similar instances to the target data from the source data or weighting the source instances. Feature-based CPDP method attempts to maximize the distribution similarity between the source and target data by finding the latent common feature space. Existing CPDP methods assume that the source project dataset and the target project dataset use the common software metrics. However, in practice, the source and target datasets may use different software metrics, i.e., different type or the number of metrics. In this scenario, CPDP is specially called as HDP [38].

To address the problem of defect prediction of projects with the heterogeneous metric sets, HDP methods have been proposed by researchers [38], [39], [40], [41]. Jing et al. [39] proposed an HDP method CCA+, which uses the canonical correlation analysis (CCA) technique and the unified metric representation (UMR) to find the latent common feature space. Nam and Kim [38] presented an HDP method, which uses feature selection and KolmogorovSmirnov test based matching method to reduce the distribution differences between source and target data. Ma et al. [40] proposed KCCA+ for HDP, which combines kernel method and CCA technique. Li et al. [41] presented a new HDP method called CTKCCA by combining cost-sensitive learning, kernel method, and CCA technique.

However, the building of high-performance HDP models is still a serious challenge owing to some critical problems including the class-imbalance distribution, the complex nonlinear relationship between the source and target datasets, the distribution differences between the source and target datasets and so on. The class-imbalance distribution is a main factor accounting for the unsatisfactory prediction performance [5], [42]. It can cause poor performance on the minority class samples for the prediction models. However, existing HDP methods (such as CCA+, HDP-KS, and KCCA+) except CTKCCA, do not consider the class-imbalance problem. Although CTKCCA uses cost-sensitive method for addressing this problem, an appropriate cost matrix is difficult to determine. According to [40], [41], the nonlinear relationship between the source and target datasets seems to have an impact on the prediction performance. The distribution difference between the source and target datasets, especially in the scenario of HDP, makes the building of high-performance defect prediction models a serious challenge. The class-imbalance distribution and nonlinear correlation further increase the learning difficulty of HDP.

In this paper, we propose a novel kernel spectral embedding transfer ensemble method for HDP. The main idea of our HDP method is to first balance the source dataset, and then find a series of the latent common kernel feature subspace that each subspace not only maximizes the similarity between the projected source and target datasets but preserves the intrinsic characteristic of both source and target datasets. Finally, we built classifiers on each common feature space and combine them for predicting the labels of the target data. The experimental results show that our KSETE significantly performs better than previous state-of-the-art HDP and CPDP-CM methods.

The contributions of this paper are as follows:

Considering the problems including imbalanced data, nonlinear correlation, and distribution differences existing in the heterogeneous defect prediction, a novel HDP method named KSETE based on class-imbalance learning, multi-kernel learning, transfer learning, and ensemble learning is proposed in this paper.

To evaluate the performance of the proposed KSETE, extensive experiments are performed on 22 publicly available datasets from three communities including AEEEM [33], JIRA [43], and PROMISE [44] in both HDP and CPDP-CM scenarios. The experimental results show that our KSETE is very effectiveness compared with the baselines.

A replication package of KSETE is shared on GitHub1 and Zenodo,2 which is publicly available.

The rest of this paper is organized as follows: Previous works about CPDP-CM and HDP in the SDP field are briefly reviewed in Section 2. We formulate the problem of HDP in Section 3. The details of our proposed KSETE are presented in Section 4. The experimental setup is described in Section 5. Section 6 shows the experimental results. This study is discussed in Section 7. Some potential threats to our study are shown in Section 8. Finally, we make a conclusion about our research in Section 9.

SECTION 2Related Works
In this section, we briefly introduce the previous cross-project defect prediction methods using common metric set and the heterogeneous defect prediction methods in the SDP field.

2.1 Cross-Project Defect Prediction Using Common Metrics
Many CPDP methods have been proposed in previous SDP studies [29], [30], [31], [31], [32], [33], [35], [35], [45].

Some CPDP methods attempt to select similar training data with target data from the source data [29], [30], [31], [35], [45]. Turhan et al. [29] proposed a CPDP method named Burak filter, which uses the k-nearest neighbor (NN) method to select (aka. filter) training instances from the source data. Specifically, for each instance in target data, they selected some nearest source instances based on Euclidean distance as part of the training data. Ryu et al. [30] also developed a CPDP method called hybrid instance selection using the nearest neighbor (HISNN), which is made of two main phases: target data instance selection and source data instance filtering.

Some CPDP methods attempt to reduce the distribution difference between the source and target data by weighting source instances according to their similarity with the target data [31], [32], [35]. Ma et al. [31] proposed a CPDP approach named transfer naive Bayes (TNB). For each instance in source data, its similarity weight is calculated based on the distributional characteristics of target data and then constructed a classifier based on naive Bayes. Xia et al. [32] developed a CPDP approach named HYDRA, which can adjust the weights of source instances during the training of their proposed extended AdaBoost algorithm. Ryu et al. [35] proposed a value-cognitive boosting with support vector machine (VCB-SVM) approach considering class imbalance learning for CPDP. Specifically, the similarity weights of source data instances are used in a mechanism to balance the training data and in the process to train the defect prediction model.

Some CPDP approaches aim to find latent common feature space which can minimize the distribution differences between source and target datasets [33][36]. Nam et al. [33] proposed a new CPDP method called TCA+, which extends transfer component analysis (TCA) by introducing a set of rules for selecting an appropriate normalization method to obtain better CPDP performance. Krishna and Menzies [36] introduced a baseline method named Bellwether for cross-project defect prediction based on existing CPDP methods (such as TCA+ and TNB).

Above CPDP approaches assume that there are common metric sets in both source and target projects. However, the number of common metrics may be very small, even zero. In this scenario, these approaches cannot achieve good-enough prediction performance.

2.2 Heterogenous Defect Prediction
Recently, heterogenous defect prediction methods [39], [40], [41], [46] have been proposed to predict defects across projects which have heterogeneous metric sets.

Jing et al. [39] proposed an HDP method named CCA+, which uses the canonical correlation analysis technique [47] and the unified metric representation to find the latent common feature space between the source and target projects. Specifically, the UMR is made of three kinds of metrics, including the common metrics of the source and target data, source-specific metrics, and target-specific metrics. Based on UMR, the transfer learning method based on CCA is introduced to find common metrics by maximizing the canonical correlation coefficient between source and target data.

Nam and Kim [46] also proposed an HDP method. They first used a feature selection method on the source to select a subset of metrics by removing redundant and irrelevant metrics. Then, they matched the source and target metrics based on metric similarity (e.g., KolmogorovSmirnov test based matching, KSAnalyzer). With the matched source and target metric sets, they built a defect prediction model for HDP.

Considering the complex nonlinear relationships between software metrics and software defects, Ma et al. [40] proposed an HDP method called kernel canonical correlation analysis (KCCA+) by combining kernel method and CCA technique.

Li et al. [41] argued that previous HDP methods do not consider the class-imbalance problem, which increases the difficulty of building a good-performance HDP model. Therefore, they proposed a new HDP method named CTKCCA by combining cost-sensitive learning, kernel method, and CCA technique. The experimental results show that CTKCCA performs better than the related CPDP methods and state-of-the-art HDP methods.

However, the performance of existing HDP methods are less than satisfactory owing to several serious challenges including class imbalance problem, complex nonlinear relationship between source and target datasets and between software metrics and defects, and making two distribution similar without losing too much intrinsic characteristic of original datasets.

2.3 Transfer Learning
A major assumption in traditional machine learning approaches is that the training and future data must have the same feature space and the same distribution [48]. Differing from traditional machine learning approaches, transfer learning does not require this assumption and allows transfer domain knowledge between different domains. According to [48], the definition of transfer learning is: Given a source domain DS and learning task TS, a target domain DT and learning task TT, transfer learning aims to help improve the learning of the target predictive function fT(â‹…) in DT using the knowledge in DS and DT, where DSâ‰ DT, or TDâ‰ TT.

Transfer learning techniques have been applied in many fields, such as image classification [49], [50], speech recognition [51], face recognition [52], robots modeling [53], and recommended systems [54]. Actually, CPDP is also a transfer learning problem in the SDP field [31].

SECTION 3Problem Formulation
Denote a labeled source project dataset as {S=(x(1)S,â€¦,x(nS)S)T, Y=(y(1)S,â€¦,y(nS)S)T} and an unlabeled dataset from the target project as {T=(x(1)T,â€¦,x(nT)T)T}, where nS and nT respectively denote the number of modules (such as functions/classes/files depending on the granularity considered) in the source and target projects, x(i)SâˆˆR1âˆ—dS represents the dS metricsâ€™ value of ith module in the source project, x(j)TâˆˆR1âˆ—dT denotes the value of dT metrics of jth module in the target project, and y(i)Sâˆˆ{0,1} corresponds the defect information of ith source project module (y(i)S=0 denotes non-defective and y(i)S=1 means defective). Note that source and target datasets have heterogeneous metric sets. A module is defective if it has one or more defects. With respect to the unlabeled target project T, the objective of KSETE is to predict their labels by using the labeled source data.

We aim to find a common feature space for the source and target datasets. The optimal projected space is defined as follows:

3Definition 1.
Given the source dataset S and the target dataset T, let Î¦(â‹…) be a mapping function, Î¦(S)âˆˆRnSâˆ—p and Î¦(T)âˆˆRnTâˆ—q denote respectively the projected source and target datasets, then the optimal further projection of the projected source dataset BÎ¦(S), and that of the projected target dataset BÎ¦(T) are given by the following optimization objective:
minBÎ¦(S),BÎ¦(T)=L(BÎ¦(S),Î¦(S))+L(BÎ¦(T),Î¦(T))+Î²â‹…D(BÎ¦(S),BÎ¦(T)),(1)
View Sourcewhere L(âˆ—,âˆ—) denotes the difference between the projected dataset by using Î¦(â‹…) and its further projected dataset (e.g., Î¦(S) and BÎ¦(S)), the difference between the two further projected datasets are represented as D(BÎ¦(S),BÎ¦(T)), Î² is a hyper-parameter to control how desirable the two datasets are similar. We further define D(BÎ¦(S),BÎ¦(T)) with L(âˆ—,âˆ—) as follows:
D(BÎ¦(S),BÎ¦(T))=12(L(BÎ¦(S),Î¦(T))+L(BÎ¦(T),Î¦(S)))(2)
View Sourcewhich is the average of the difference between the projected source dataset Î¦(S) and the further projected target dataset BÎ¦(T), and that between the projected target dataset Î¦(T) and the further projected source dataset BÎ¦(S).

SECTION 4Proposed Method
Fig. 1 presents the overall framework of the proposed KSETE for HDP. In the following paragraphs, we present the details of KSETE from three aspects: (1) preprocessing, (2) kernel spectral embedding, and (3) transfer ensemble learning.


Fig. 1.
Framework of our proposed KSETE for heterogeneous defect prediction.

Show All

4.1 Preprocessing
The preprocessing includes following four processes: (1) remove the duplicated instances in S and T; (2) remove the instances having missing value; (3) alleviate the class imbalance problem by performing synthetic minority over-sampling technique (SMOTE) [55] on S; (4) normalize S and T with Z-score [56].

SMOTE is one of the most commonly used class-imbalance learning methods in the SDP field. Dealing with the class imbalance problem has been demonstrated to improve the prediction performance [5][36].

Considering the magnitude differences of different metrics in defect datasets, we use Z-score normalization to scale each metric to have mean 0 and standard deviation 1.

4.2 Kernel Spectral Embedding
For the sake of computation, the source and target datasets must be preprocessed to have the same number of instances (supposing it to be n). In this paper, random sampling is used to increase the number of instances of the smaller dataset. Note that, if the size of target data is increased, the added instances must be removed before predicting the labels of target instances.

Denote the mapping matrices of projected source and target datasets as PÎ¦(S)âˆˆRkâˆ—p and PÎ¦(T)âˆˆRkâˆ—q, respectively. The differences between the projected dataset and the original dataset (distortion function L(âˆ—,âˆ—) in Eq. (1)) are defined as follows:
L(BÎ¦(S),Î¦(S))=âˆ¥âˆ¥BÎ¦(S)PÎ¦(S)âˆ’Î¦(S)âˆ¥âˆ¥2FL(BÎ¦(T),Î¦(T))=âˆ¥âˆ¥BÎ¦(T)PÎ¦(T)âˆ’Î¦(T)âˆ¥âˆ¥2FL(BÎ¦(S),Î¦(T))=âˆ¥âˆ¥BÎ¦(S)PÎ¦(T)âˆ’Î¦(T)âˆ¥âˆ¥2FL(BÎ¦(T),Î¦(S))=âˆ¥âˆ¥BÎ¦(T)PÎ¦(S)âˆ’Î¦(S)âˆ¥âˆ¥2F,(3)
View Sourcewhere âˆ¥Xâˆ¥F=âˆ‘ijX2ijâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆš represents the Frobenius norm.

With Eqs. (1) and (3), the optimization objective can be rewritten as:
minBTÎ¦(S)BÎ¦(S)=I,BTÎ¦(T)BÎ¦(T)=IG(BÎ¦(S),BÎ¦(T),PÎ¦(S),PÎ¦(T))=minBTÎ¦(S)BÎ¦(S)=I,BTÎ¦(T)BÎ¦(T)=Iâˆ¥âˆ¥Î¦(S)âˆ’BÎ¦(S)PÎ¦(S)âˆ¥âˆ¥2F+âˆ¥âˆ¥Î¦(T)âˆ’BÎ¦(T)PÎ¦(T)âˆ¥âˆ¥2F+Î²2âˆ—âˆ¥âˆ¥Î¦(S)âˆ’BÎ¦(T)PÎ¦(S)âˆ¥âˆ¥2F+Î²2âˆ—âˆ¥âˆ¥Î¦(T)âˆ’BÎ¦(S)PÎ¦(T)âˆ¥âˆ¥2F,(4)
View Sourcewhere BÎ¦(S)âˆˆRnâˆ—k and BÎ¦(T)âˆˆRnâˆ—k are assumed orthogonal, and Î² is a hyper-parameter to adjust how desirable the projected datasets (i.e., BÎ¦(S) and BÎ¦(T)) are similar.

Lemma 1.
The BÎ¦(S),BÎ¦(T),PÎ¦(S),PÎ¦(T) in Eq. (4) have the following properties:
PÎ¦(S)=12+Î²(2BTÎ¦(S)Î¦(S)+Î²BTÎ¦(T)Î¦(S))PÎ¦(T)=12+Î²(2BTÎ¦(T)Î¦(T)+Î²BTÎ¦(S)Î¦(T))(5)
View Source

Proof.
Based on the relationship of between Frobenius norm and matrix trace, the properties of orthogonal matrix and cyclic permutation property of trace:
âˆ¥Xâˆ¥2F=tr(XTX),BTÎ¦(S)BÎ¦(S)=I,tr(PTÎ¦(S)BTÎ¦(S)Î¦(S))=tr(BTÎ¦(S)Î¦(S)PTÎ¦(S))(6)
View Sourcethe following deduction can be obtained:
âˆ¥âˆ¥Î¦(S)âˆ’BÎ¦(S)PÎ¦(S)âˆ¥âˆ¥2F=tr(Î¦(S)TÎ¦(S))âˆ’2tr(BTÎ¦(S)Î¦(S)PTÎ¦(S))+tr(PTÎ¦(S)PÎ¦(S)).(7)
View Source

Let âˆ¥âˆ¥Î¦(T)âˆ’BÎ¦(T)PÎ¦(T)âˆ¥âˆ¥2F, âˆ¥âˆ¥Î¦(S)âˆ’BÎ¦(T)PÎ¦(S)âˆ¥âˆ¥2F, and âˆ¥âˆ¥Î¦(T)âˆ’BÎ¦(S)PÎ¦(T)âˆ¥âˆ¥2F be expressed in the same fashion. The optimization problem in Eq. (4) can be rewritten as:
minBTÎ¦(S)BÎ¦(S)=I,BTÎ¦(T)BÎ¦(T)=IG(BÎ¦(S),BÎ¦(T),PÎ¦(S),PÎ¦(T))
View Source
=minBTÎ¦(S)BÎ¦(S)=I,BTÎ¦(T)BÎ¦(T)=I(1+Î²2)tr(Î¦(S)TÎ¦(S))
View Source
+(1+Î²2)tr(Î¦(T)TÎ¦(T))+(1+Î²2)tr(PTÎ¦(S)PÎ¦(S))
View Source
+(1+Î²2)tr(PTÎ¦(T)PÎ¦(T))âˆ’2tr(BTÎ¦(S)Î¦(S)PTÎ¦(S))
View Source
âˆ’2tr(BTÎ¦(T)Î¦(T)PTÎ¦(T))âˆ’Î²â‹…tr(BTÎ¦(S)Î¦(T)PTÎ¦(T))âˆ’Î²â‹…tr(BTÎ¦(T)Î¦(S)PTÎ¦(S)).(8)
View Source

The derivation of G with respect to PÎ¦(S) and PÎ¦(T) can be obtained as follows:
âˆ‡G(PÎ¦(S))=12+Î²(Î²â‹…BTÎ¦(T)Î¦(S)+2BTÎ¦(S)Î¦(S))âˆ‡G(PÎ¦(T))=12+Î²(Î²â‹…BTÎ¦(S)Î¦(T)+2BTÎ¦(T)Î¦(T)).(9)
View Source

According to the Karush-Kuhn-Tucker conditions, let âˆ‡G(PÎ¦(S))=0 and âˆ‡G(PÎ¦(T))=0, and then we can obtain Eq. (5), namely
PÎ¦(S)=12+Î²(2BTÎ¦(S)Î¦(S)+Î²BTÎ¦(T)Î¦(S))PÎ¦(T)=12+Î²(2BTÎ¦(T)Î¦(T)+Î²BTÎ¦(S)Î¦(T)).
View Source

Based on Eqs. (4) and (5), the closed form of BÎ¦(T) and BÎ¦(S) can be derived according to the following theorem.

Theorem 1.
The minimization problem in Eq. (4) equals to the following maximization problem:
minBTÎ¦(S)BÎ¦(S)=I,BTÎ¦(T)BÎ¦(T)=IG=maxBTB=Itr(BTAB),(10)
View Source

where
B=[BÎ¦(T)BÎ¦(S)]A=[A1A3A2A4],(11)
View Source

and
A1=2Î¦(T)Î¦(T)T+Î²22Î¦(S)Î¦(S)T=2K(T,T)+Î²22K(S,S),A4=2Î¦(S)Î¦(S)T+Î²22Î¦(T)Î¦(T)T=2K(S,S)+Î²22K(T,T),A2=A3=Î²(Î¦(S)Î¦(S)T+Î¦(T)Î¦(T)T)=Î²(K(S,S)+K(T,T)).(12)
View Source

Proof:
Based on Eq. (5), the optimization problem in Eq. (4) can be rewritten as:
minBTÎ¦(S)BÎ¦(S)=I,BTÎ¦(T)BÎ¦(T)=IG(BÎ¦(S),BÎ¦(T),PÎ¦(S),PÎ¦(T))=(1+Î²2)â‹…tr(Î¦(S)TÎ¦(S))+(1+Î²2)â‹…tr(Î¦(T)TÎ¦(T))âˆ’12+Î²BTAB.(13)
View SourceSince tr(Î¦(S)TÎ¦(S)) and tr(Î¦(T)TÎ¦(T)) are constants, above optimization problem equals to:
minBTÎ¦(S)BÎ¦(S)=I,BTÎ¦(T)BÎ¦(T)=IG=maxBTB=Itr(BTAB).
View Source

Since Î¦(S)Î¦(S)T and Î¦(T)Î¦(T)T are all symmetric, A1, A2, A3, and A4 in Eq. (12) are also symmetric, and then A in Eq. (11) is symmetric. Therefore, the optimization problem in Eq. (10) has a closed-form solution under the condition BTB=I.

Theorem 2.
(Ky-Fan theorem [57]) Denote A as a symmetric matrix with eigenvalues Î»1â‰¥Î»2â‰¥â‹¯â‰¥Î»k and eigenvectors V=[v1,v2,â‹¯,vk]. Then,
âˆ‘i=1kÎ»i=maxXTX=Iktr(XTMX)
View Sourceand X=[v1,v2,â‹¯,vk]Q where Q is an arbitrary orthogonal matrix.

According to Theorem 2, the optimal B in Eq. (10) is the top-k eigenvectors of A. The first half rows of B and the second half rows of B are the best projected datasets BÎ¦(T) and BÎ¦(S), respectively. The algorithm is presented in Algorithm 1. Here, the Lanczos method [58] is used to calculate the eigenvectors.

4.3 Transfer Ensemble Learning
The objective of transfer ensemble learning is to build a transfer-learning based ensemble classifier for HDP by performing KSE multiple times on the target data and multiple feature subspace of source data. To this end, the framework of random forest (RF) [59] is adopted in this paper. RF is a very famous ensemble learning algorithm, which takes the decision tree as the base learner. RF is characterized by the random sampling of training instances with replacement, the random selection of features of training data, and the combining rule (i.e., voting for classification, mean value for regression). Specifically, given the training and test datasets, the number of base learners M, and the dimension of feature subspace k, RF first constructs a series of training subsets by performing the bootstrap method and random selection of feature subspace on the original training dataset. And then, the base learner is trained on each above training subsets. Finally, using the voting rule to combine the predicted labels of all base learners on the test instances. However, there are some main differences between our transfer ensemble learning and RF: (1) RF is not for transfer learning, which is designed for the traditional supervised machine learning task. (2) To adapt RF for transfer learning, the base learner cannot be trained directly on each training subset as RF. We first feed each training subset and the target dataset into our proposed KSE to obtain the corresponding projected datasets and then train the base learner on the projected source dataset.

Algorithm 1. Kernel Spectral Embedding
Input: Source data S; Target data T; Similarity confidence parameter Î²; Dimensionality of the projected feature space k; Kernel function K(x,x)

Output: Projected source data BÎ¦(S); Projected target data BÎ¦(T)

Remove the duplicated instances and the instances having missing value for both S and T.

Perform SMOTE algorithm [55] on S, and denote the sampled source data as S again.

Increase the size of smaller dataset by random sampling to make the source dataset and the target dataset have the same number of instances. Note that, if the number of instances in target data is increased, these added instances must be removed after the projected target data is obtained.

Use z-score to scale S and T.

Construct matrix A according to Eqs. (11) and (12).

Calculate top k eigenvalues and the corresponding eigenvectors V=[v1,v2,â‹¯,vk].

BÎ¦(T) and BÎ¦(S) are respectively the first half rows and the second half rows of V: BÎ¦(T)=âŽ¡âŽ£âŽ¢âŽ¢âŽ¢V(1,1)â‹®V(l2,1)â‹¯â‹±â‹¯V(1,k)â‹®V(l2,k)âŽ¤âŽ¦âŽ¥âŽ¥âŽ¥,BÎ¦(S)=âŽ¡âŽ£âŽ¢âŽ¢âŽ¢V(l2+1,1)â‹®V(l,1)â‹¯â‹±â‹¯V(l2+1,1)â‹®V(l,k)âŽ¤âŽ¦âŽ¥âŽ¥âŽ¥ where l is the number rows of V.

Return BÎ¦(T) and BÎ¦(S).

The details of transfer ensemble learning are presented as follows. Before iteration, we first preprocess the source data as Section 4.1 (Step1-2) and then ensure the source data and target data have the same number of instances (Step3). For each iteration, we first make a subset of the source data, which has the same number of instances as the source data and partial or the same features as the source data (Step5-6). And then use z-score to scale the source and target datasets and calculate the projected datasets BÎ¦(S) and BÎ¦(T) by using KSE algorithm (Step7). If the number of the target data is increased in Step3, then the added instances must be removed from BÎ¦(T) (Step8). We next train a classifier using the projected source data and then make the probability prediction on the projected target dataset (Step9-11). And assign a weight to this classifier (Step12). When the stop criterion of iteration is met, we combine the prediction results of all classifiers and output the predicted labels of the target data (Step13-15). Algorithm 2 presents the details of our proposed transfer ensemble learning method.

SECTION 5Experimental Setup
This section shows the details of our experiments including the research questions, benchmark datasets, performance measures, statistical testing methods, and the experimental settings.

SECTION Algorithm 2.Transfer Ensemble Learning
Input: Source data S and the labels Y; Target data T; Similarity confidence parameter Î²; the number of selected features K; Dimensionality of the projected feature space k; Kernel function K(x,x), base learner learner, the number of base learners M

Output: Predicted labels of target data T.

Remove the duplicated instances and the instances having missing value for both (S,Y) and T.

Perform SMOTE algorithm [55] on (S,Y) and denote the sampled source dataset as (S,Y).

Increase the size of smaller dataset by random sampling to make (S,Y) and T have the same number of instances. Note that, if the size of T is increased, the added instances must be removed from the projected target data is obtained.

for i=1:M do

Take a bootstrap sample (Si,Yi) from (S,Y), which has the same size as (S,Y).

Select k features (columns) from Si to replace Si.

Use z-score to scale Si abd T

Perform KSE (Algorithm 1) on Si and T and denote the obtained optimal projected datasets as BÎ¦(Si) and BiÎ¦(T). Note that, here there is no need to perform the Steps 1-4 of Algorithm (1).

Remove the added instances in BiÎ¦(T) according to step 2, if any.

Using (BÎ¦(Si),Yi) as the training data to train learner and then obtain a hypothesis hi(x):xâ†’[0,1].

Apply hi on BiÎ¦(T), and denote the probabilistic prediction as proi.

Assign a weight wi to hi, by default wi=1/M.

end for

The final hypothesis is obtained: hf(x)=argmaxyâˆˆ{0,1}âˆ‘Mi=1wiâ‹…proi[hi(x)=y]

Suppose BiÎ¦(T)[j] is jth instance of BiÎ¦(T), then the label of jth instance of T is predicted as follows:

argmaxyâˆˆ{0,1}âˆ‘Mi=1wiâ‹…proi[hi(BiÎ¦(T)[j])=y]

Return the predicted labels of the target data.

5.1 Research Questions
We investigate two questions in this paper as follows:

RQ1: Does KSETE outperform previous HDP methods?

Motivation. The main objective of this study is to propose a novel HDP method. Therefore, we must first raise and answer the most important research question that whether our method can obtain a better performance than previous HDP methods.

Approach. To answer RQ1, we compare KSETE with three state-of-the-art HDP approaches including CCA+ [39], HDP-KS [46], and CTKCCA [41] by using hypothesis testing (see Section 5.4 for details). All the baselines use their default settings according to corresponding original studies. Based on aforementioned statistical testing methods, we propose the following hypotheses:

Null hypothesis: KSETE has no statistically significant difference from other HDP methods.

Alternative hypothesis: KSETE outperforms other HDP methods with statistical significance.

RQ2: Does KSETE outperform previous CPDP-CM methods?

Motivation. Although CPDP-CM methods cannot be used in the HDP scenario, HDP methods can be used in the CPDP-CM scenario. We then want to investigate whether our method can outperform existing CPDP-CM methods or not.

Approach. To answer RQ2, we compare KSETE with five well-known CPDP-CM methods including Burak filter [29], TNB [31], TCA+ [33], HISNN [30], and Bellwether (use TCA+ as the transfer learner) [36] by hypothesis testing (see Section 5.4 for details). All the baselines use their default settings according to corresponding original studies. And then we propose the following hypotheses:

Null hypothesis: Null hypothesis: KSETE has no statistically significant difference from other CPDP-CM methods.

Alternative hypothesis: KSETE outperforms other CPDP-CM methods with statistical significance.

5.2 Benchmark Datasets
A total of 22 publicly available datasets from three different communities including AEEEM [33], JIRA [43], and PROMISE [44] are used in our experiments. Datasets in the same community have the same metric sets, but datasets in different communities have heterogeneous metric sets. The NASA datasets used in previous studies [41], [46] are not utilized in this paper owing to the data quality problems [60]. Since we do not consider the inter-release defect prediction [61], for the multiple-version projects, only one version is used. Table 1 shows the details of these datasets. A brief description of each community is presented as follows:

TABLE 1 An Overview of 22 Studied Projects
Table 1- 
An Overview of 22 Studied Projects
AEEEM was collected by DAmbros et al. [62]. Each dataset in AEEEM consists of 61 software metrics: 17 source code metrics (i.e., CK metrics [24] and other 11 object-oriented metrics), 17 entropy-of-source-code metrics, 17 churn-of-source-code metrics, 5 entropy-of-change metrics, and some other metrics.

PROMISE was prepared by Jureczko and Madeyski [44]. It contains 20 class-level metrics, such as CK metrics [24] and QMOOD metrics [63].

JIRA is a new repository of highly-curated defect datasets collected by Yatish et al. [43]. JIRA contains 65 software metrics: 54 code metrics, 5 process metrics, and 6 ownership metrics. Each module in JIRA refers to a java file.

5.3 Performance Evaluation Measures
Seven well-known measures are applied to evaluate the prediction performance, including PD, PF, AUC, G-Measure, MCC, Popt(20%), and IFA. Other measures such as Accuracy, Precision, and F-Measure are not used, because they are poor performance indicator for class-imbalanced datasets according to previous SDP studies [64][65]. PD and PF have been widely used in previous SDP studies [29], [35], [41], [45]. As Menzies et al. [10] stated, an ideal prediction model should have a high PD but a low PF. AUC, G-measure, and MCC are three overall performance measures, which take both PD and PF into consideration and have been widely used in previous SDP studies [45], [46], [65], [66], [67], [68], [69], especially in the case of imbalanced defect datasets. Popt and IFA comes from effort-aware just-in-time (JIT) defect prediction [70], [71], [72], [73]. Apart from PF and IFA, for other measures, the larger the value is, the better the prediction performance is. Most of these measures can be defined by confusion matrix [74] as shown in Table 2. By convention, the defective modules are regarded as positive class samples and the non-defective ones as negative class ones [75].

TABLE 2 Confusion Matrix

PD (aka.recall, true positive rate, or sensitivity) and PF (aka. false positive rate) are defined as follows:
PD=TPTP+FN;PF=FPFP+TN.(14)
View Source

AUC [76] is the area under of receiver operating characteristic (ROC) curve. This curve is plotted in a 2-D space with PF as the x-axis and PD as the y-axis. The AUC is a widely used measure because it is rarely affected by class imbalance [77]. Its range is [0,1], and AUC=0.5 denotes the performance of a random prediction model.

G-Measure [65] is the harmonic mean of PD and (1-PF), which has been widely used in previous SDP studies [45], [65], [67], [68]. G-Measure is a good indicator of performance for imbalanced datasets. The bigger the value of G-measure is, the better the prediction performance is. G-Measure is defined as:
Gâˆ’Measure=(2âˆ—PDâˆ—(1âˆ’PF))PD+(1âˆ’PF).(15)
View Source

MCC [78] is an overall performance measure by taking TP, TN, FP, and FN into consideration. MCC has been widely used in previous SDP studies [39], [45], [68], [69] since it can be utilized even the data is unbalanced [74]. The definition of MCC is:
MCC=TPâˆ—TNâˆ’FPâˆ—FN(TP+FP)(TP+FN)(TN+FP)(TN+FN)âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆš.(16)
View Source

Popt(20%) refers to the percentage of detected defective changes to all defective changes when using 20 percent of effort (i.e., the number of changed code lines) [70]. It is based on the concept of the code-churn-based Alberg diagram.

IFA [73] is the number of initial false alarm encountered until the first true defect is found. According to [73], first sort the predicted defective changes in ascending order based on their change size (i.e., the sum of lines of code added and deleted by the current change), if k changes have been inspected when the first actual defective change is found, then IFA=k. Obviously, IFA does not care how many defective changes can be correctly predicted for a prediction model.

Note that, since the benchmark datasets used in this study have no metrics related to change size, we just use LOC (i.e., lines of code in modules) instead of change size to compute Popt(20%) and IFA. Furthermore, when all positive instances are misclassified (TP=0) and all negative instances are correctly classified (FP=0), the value of MCC is NAN because of TP+FP=0. To avoid this situation, we just let FP=1, TN=TN-1, and remain TP and FN unchanged. Thus, the value of MCC is a very small negative number for the imbalanced datasets. From Table 1, we can see that the benchmark datasets are highly imbalanced. Therefore, we can directly let MCC=0 when MCC is NAN.

5.4 Statistical Testing
Three statistical testing methods including Wilcoxon signed-rank test (Î±=0.05) [79], Cliffâ€™s Î´ effect size test (Î±=0.05) [80], and Scott-Knott effect size difference (ESD) test (Î±=0.05) [81] are adopted in this study. We use Wilcoxon signed-rank test to clarify whether our KSETE is statistically outperform each baseline on each dataset. Cliffâ€™s Î´ is used to quantify the magnitude of difference between KSETE and each baseline. Scott-Knott ESD test is used to demonstrate that whether our KSETE statistically performs better than the baselines on all datasets.

Specifically, the Wilcoxon signed-rank test at the significance level 0.05 is used to detect whether there is a significant difference in the predictive performance between KSETE and each baseline on each dataset. Wilcoxon signed-rank test is a non-parametric alternative to the paired t-test, but it does not make the assumption that the data must be normally distributed.

Cliffâ€™s Î´ [80] is non-parametric effect size measure with the suggestion of Romano et al. [82]: negligible (|Î´|<0.147), small (0.147â‰¤|Î´|<0.330), medium (0.330â‰¤|Î´|<0.474), and large (|Î´|â‰¥0.474). Cliffâ€™s Î´ has been widely used in previous SDP studies [27], [68], [83]. In this paper, Cliffs delta is computed according to the method proposed by Macbeth et al. [84].

Scott-Knott ESD test [81] uses hierarchical cluster analysis to partition different treatment means into statistically distinct groups, which has no the overlap problem of other multiple comparison test methods, e.g., Nemenyiâ€™s post-hoc test [85] suggested by DemÅ¡ar [86]. Scott-Knott ESD test is a variant of Scott-Knott test [87] with two corrections: (1) it does not assume that the data is normally distributed; (2) it merges any two statistically different groups that have a negligible effect size into one group according to the Cohenâ€™s delta [88]. Both Scott-Knott test and Scott-Knott ESD test have been used in previous SDP studies [8], [89], [90].

5.5 Experimental Settings
5.5.1 Validation Method
A total of 22 datasets from three communities including AEEEM (5 projects), JIRA (7 projects), and PROMISE (10 projects) are used as the benchmark. For WPDP, k-fold cross-validation [91] is one of the most commonly used model validation techniques. However, it is unsuitable for CPDP scenario because the source data (training data) and target data (test data) come from different projects. Model validation technique used in this study is presented as follows:

For HDP scenario, if one dataset is selected as the source data from a community, the target data must be selected from the reminding communities. In this way, there are a total of 310 (5*17+7*15+10*12) possible combinations for these 22 projects from three communities. Given a combination, e.g., EQâ‡’ant-1.7, the simplest way to validate a given model is that train this model on EQ and then test on ant-1.7 for only a single run. However, this is very possible to make an excessively pessimistic or optimistic evaluation of the performance of the given model on ant-1.7. To alleviate this potential problem and to overcome the inherent randomness in our KSETE (e.g., SMOTE), the settings of our validation method are: (1) as done in previous HDP study [41], we first randomly select 90 percent instances of the source data as training data to train the model and then test on the target data, since 90 percent instances of the source dataset can not only ensure the training data is not always the same, but also 90 percent instances can represent the source data well to some extent; (2) we repeat this procedure 30 times (30 is the minimum needed samples to satisfy the central limit theorem [36]) and report the average value on each target dataset.

For CPDP-CM scenario, if one dataset is selected as the source data from a community, then the target data must be selected from the same community. In this way, we can obtain 152 (5*4+7*6+10*9) possible combinations. And then we take the same validation procedure as done in HDP scenario to report the average results for each model on each target data.

5.5.2 Parameter Settings
In experiments, for the parameters of Algorithms 1 and 2, we set k=1, Î²=1, M=5, and K=size(source, 2)-1 where size(source, 2) denotes the number of metrics in the original source dataset, respectively. SMOTE is used on the source data to alleviate the class-imbalance problem. For the processed source data, the ratio of the defective modules to the non-defective ones is about 0.85. Moreover, in Eq (12), we need to calculate some kernel matrices. To take advantage of multiple kernel learning [92], [93], we construct a hybrid kernel function based on multiple Gaussian kernel functions, i.e., K(xi,xj)=exp(âˆ’âˆ¥xiâˆ’xjâˆ¥22Ïƒ2). Given a set of positive number w={wi}i=1,â‹¯,m and a dataset XâˆˆRnâˆ—d where m is the number of Gaussian kernel functions, n denotes the number of instances, and d represents the number of features, then the kernel matrix of X, i.e. K(X,X), is calculated as follows:
K(X,X)=1mâˆ‘i=1mKi(X,X)=1mâˆ‘i=1mexp(âˆ’dist(XT)w(i)â‹…sum(var(X))),(17)
View Sourcewhere dist(XT) returns an n-by-n of matrix A (Aij denotes the Euclidean distance between ith instance and jth instance of X), var(X) returns a 1-by-d vector in which jth element is the variance of jth column (feature) of X, and sum(â‹…) is a sum function. In this paper, we let m=3 and w={0.5,1,1.5}.

Logistic regression (LR) [94] is chosen as the base learner in this paper because it has been widely used in previous SDP studies [1], [38], [83], [95], [96]. Furthermore, LR is not only very simple, but it performs well compared with the complex modeling techniques for SDP [5]. Given an unlabeled instance, a trained LR can give its probability of being positive class. In this study, we set the threshold as 0.5, i.e., if the probability of being positive class is not smaller than the threshold, we label this instance as the positive class.

SECTION 6Experimental Results
6.1 RQ1: Does KSETE Outperform Previous HDP Methods?
Table 3 shows the mean results of PD and PF for each method on each project. From this table, we can see that the PD of our KSETE varies from 0.366 to 0.70, the PF varies from 0.183 to 0.431. On average, our KSETE obtains the PD and PF as 0.540 and 0.286, respectively. Although CTKCCA achieves the best average PD (0.838), it makes this at a cost of extremely high PF (0.844). Compared with HDP-KS and CCA+, KSETE greatly improves PD as the cost of an acceptable average PF (0.286). Fig. 2 further graphically visualize the average results of PD and PF for our method and all baseline methods on overall 22 projects.


Fig. 2.
Barplot of PD and PF on overall 22 projects.

Show All

TABLE 3 Comparison Results in PD and PF for the Proposed KSETE and Existing HDP Methods

Tables 4 and 5 respectively present the comparison results of three overall measures (i.e., AUC, G-Measure, and MCC) and two effort-aware measures (i.e., Popt(20%) and IFA) for our KSETE and all the baselines (three existing HDP methods) on each project. The best result is in boldface. The results of a method which has no significant difference (by Wilcoxon signed-rank test at significance level 0.05) compared with the best method having the best result is marked by âœ“. The row â€™Averageâ€™ reports the average results of each method across overall 22 projects, and the row â€™Medianâ€™ the median results. The row â€™Sig. Win/Tie/Loseâ€™ reports the number of projects for which our method achieves a better, equal, and worse performance than the corresponding baseline method with statistical significance according to the Wilcoxon signed-rank test. In particular, a table cell in deep gray background denotes that our method makes a large significant improvement compared with the corresponding baseline method and a table cell in the light gray background indicates a moderate significant improvement in terms of Cliffâ€™s Î´ as described in Section 5.4.

TABLE 4 Comparison Results in AUC, G-Measure, and MCC for the Proposed KSETE and Existing HDP Methods

TABLE 5 Comparison Results in Popt(20%) and IFA for the Proposed KSETE and Existing HDP Methods
Table 5- 
Comparison Results in Popt(20%) and IFA for the Proposed KSETE and Existing HDP Methods
From Tables 4 and 5, we can see that our KSETE achieves high performance on each project in terms of AUC, G-Measure, MCC, Popt(20%), and IFA. The AUC of KSETE varies from 0.511 to 0.780, the G-Measure varies from 0.416 to 0.640, MCC varies from 0.062 to 0.364, Popt(20%) ranges from 0.143 to 0.438, and IFA ranges from 1.2 to 57.8. Especially, KSETE nearly always significantly outperforms all the baselines in terms of AUC, G-Measure, and MCC. Meanwhile, in most cases, KSETE performs significantly better than all the baselines in term of Popt(20%) and IFA except that KSETE ties with CTKCCA in terms of Popt(20%).

On average, KSETE obtains the AUC as 0.691, the G-Measure as 0.554, the MCC as 0.215, the Popt(20%) as 0.279, and the IFA as 23. The improvement of KSETE over the baselines on average is at least by 22.7, 138.9, 494.4, and 19.7 percent in terms of AUC, G-Measure, MCC, and IFA, respectively. In terms of average Popt(20%), CTKCCA (0.33) closely followed by KSETE (0.279) obtains the best performance.

On the median, KSETE obtains AUC as 0.698, the G-Measure as 0.562, the MCC as 0.208, the Popt(20%) as 0.274, and the IFA as 22.5. For AUC, G-Measure, and MCC, the improvement of KSETE over the baselines on the median is at least by 23.2, 128.3, and 414.8 percent, respectively.

In terms of Cliffâ€™s Î´, KSETE nearly always largely improves the performance over the baselines in terms of AUC, G-Measure, and MCC. For most cases, KSETE makes a large improvement with statistical significance over HDP-KS and CCA+. KSETE loses on 14 projects compared with CTKCCA and wins on 5 projects with statistical significance in Popt(20%). In most cases, KSETE largely improve the performance over the baselines in terms of IFA.

Fig. 3 further shows the boxplots of AUC, G-Measure, MCC, Popt(20%), and IFA for KSETE and all baselines across overall 22 projects. From the figure, we can see that KSETE obviously outperforms or is comparable with the baselines in terms of AUC, G-Measure, MCC, Popt(20%), and IFA.


Fig. 3.
Boxplots of AUC, G-Measure, MCC, Popt(20%), and IFA across all datasets for KSETE and other HDP methods. The horizontal line in the box denotes the median, while the asterisk indicates the mean.

Show All

Fig. 4a, 4b, 4c, 4d, 4e present the results of Scott-Knott ESD test for the proposed KSETE and previous HDP methods on all the datasets in AUC, G-Meassure, MCC, Popt(20%), and IFA, respectively. The x-axis shows the HDP methods and the y-axis represents the ranking value. Each method corresponds to a vertical line which denotes the range of ranking of this method on all datasets. The dot in the vertical line is the average ranking. Different colors denotes distinct groups with statistical significance. The smaller ranking, the better performance except IFA. From these figures, we can see that (1) For AUC (Fig. 4a), G-Measure (Fig. 4b), and MCC (Fig. 4c), KSETE has the smallest average ranking and KSETE is divided into a different group without including any baseline. It means that KSETE significantly outperforms the baselines. (2) For Popt(20%) (Fig. 4d), four methods are divided into four different groups and CTKCCA obtains the smallest ranking followed by our KSETE. (3) For IFA (Fig. 4e), KSETE obtains the biggest average ranking and is divided into a group which has no any baseline.


Fig. 4.
The results of Scott-Knott ESD test in AUC, G-Measure, MCC, Popt(20 percent), and IFA across all datasets for KSETE and other HDP methods. The smaller ranking, the better performance except IFA.

Show All

Based on the above results, we can draw the following conclusion:

The proposed KSETE significantly outperforms the previous heterogeneous defect prediction methods and improves the performance over the baselines on average by at least 22.7, 138.9, 494.4, and 19.7 percent in terms of AUC, G-Measure, MCC, and IFA, respectively. KSETE significantly outperforms the baselines except CTKCCA in terms of Popt(20%).

6.2 RQ2: Does KSETE Outperform Previous CPDP-CM Methods?
Table 6 shows the comparison results of PD and PF for each method on each project. Form the table, we notice that the PD of KSETE varies from 0.26.4 to 0.691 and the PF ranges from 0.135 to 0.395. On average, KSETE achieves the PD as 0.50 and the PF as 0.239. KSETE largely improves the PD compared with the baselines at a cost of tolerable performance loss on PF. Fig. 5 further graphically visualizes the average results of PD and PF for all methods by using bar-plot.


Fig. 5.
Barplots of average PD and PF on overall 22 projects.

Show All


Fig. 6.
Boxplots of AUC, G-Measure, MCC, Popt(20%), and IFA across all datasets for KSETE and other CPDP-CM methods. The horizontal line in the box denotes the median, while the asterisk indicates the mean.

Show All

TABLE 6 Comparison Results in PD and PF for the Proposed KSETE and Existing CPDP-CM Methods

Tables 7 and 8 respectively present the mean results of three overall measures (i.e., AUC, G-Measure, and MCC) and two effort-aware measures (i.e., Popt(20%) and IFA) for our KSETE and all baselines (i.e., five existing CPDP-CM methods) on each project. The best result is in boldface. The row â€™Averageâ€™ reports the average results of overall 22 projects, and the row â€™Medianâ€™ the median results. The row â€™W/T/Lâ€™ reports the number of projects for which our method achieves a better, equal, and worse performance than the corresponding baseline method with statistical significance (p-value<0.05 by the Wilcoxon signed-rank test).

TABLE 7 Comparison Results in AUC, G-Measure, and MCC for the Proposed KSETE and Existing CPDP-CM Methods

From Tables 7 and 8, we can see that KSETE achieves high performance in terms of AUC, G-Measure, MCC, and Popt(20%). The AUC of our KSETE varies from 0.513 to 0.807, the G-Measure varies from 0.372 to 0.699, the MCC varies from 0.102 to 0.368, the Popt(20%) varies from 0.123 to 0.410, and the IFA varies from 1.0 to 56.3.

TABLE 8 Comparison Results in Popt(20%) and IFA for the Proposed KSETE and Existing CPDP-CM Methods

On average, KSETE obtains the AUC as 0.691, the G-Measure as 0.543, the MCC as 0.224, the Popt(20%) as 0.259, and the IFA as 19.4. KSETE improves the AUC, G-Measure, and MCC over the baselines by at least 4.5, 30.2, and 17.9 percent, respectively. Except for TNB, KSETE improves the Popt(20%) over other baselines by at least 32.1 percent. KSETE improves the IFA over the baselines by at least 22.1 percent except Burak Filter.

On median, KSETE achieves the AUC as 0.686, the G-Measure as 0.532, the MCC as 0.211, the Popt(20%) as 0.25, and the IFA as 13.3. KSETE improves the AUC, G-Measure, and MCC over the baselines by at least 1.9, 13.2, and 12.3 percent, respectively. In terms of Popt(20%), except for TNB, KSETE outperforms other baselines by at least 29.6 percent. In terms of IFA, KSETE improves the baselines by at least 28.1 percent except Burak Filter and Bellwether.

Fig. 6 further shows the boxplots of AUC, G-Measure, MCC, Popt(20%), and IFA for our method and all the baselines across overall 22 projects. From this figure, we can see that KSETE outperforms all the baselines in terms of AUC, G-Measure, and MCC on average or median. In terms of Popt(20%), KSETE outperforms all baselines except TNB on average or median. In terms of IFA, KSETE performs worse than Burak Filter but outperforms other baselines on average.

Fig. 7a, 7b, 7c, 7d, 7e present the results of Scott-Knott ESD test for the proposed KSETE and previous HDP methods on all the datasets in AUC, G-Meassure, MCC, Popt(20%), and IFA, respectively. The x-axis shows the HDP methods and the y-axis represents the ranking value. Each method corresponds to a vertical line which denotes the range of ranking of this method on all datasets. The dot in the vertical line is the average ranking. Different colors denotes distinct groups with statistical significance. The smaller ranking, the better performance except IFA. From these figures, we can see that (1) For AUC (Fig. 7a), G-Measure (Fig. 7b), and MCC (Fig. 7c) KSETE obtains the smallest average ranking and is categorized into a group without including any baseline. (2) For Popt(20%) (Fig. 7d), KSETE is categorized into a group without including any baseline and obtains the second smallest average ranking. (3) For IFA (Fig. 7e), Bellwether obtains the biggest average ranking closely followed by KSETE which achieves the second largest average ranking.


Fig. 7.
The results of Scott-Knott ESD test in AUC, G-Measure, MCC, Popt(20%), and IFA across all datasets for KSETE and other CPDP-CM methods. The smaller ranking, the better performance except IFA.

Show All

Based on the above results, we can draw the following conclusion:

The proposed KSETE is also suitable for cross-project defect prediction using common metrics (CPDP-CM) scenario. KSETE significantly outperforms the previous CPDP-CM methods and improves the performance over the baselines on average by at least 4.5, 30.2, and 17.9 percent, in terms of AUC, G-Measure, and MCC across all the 22 projects. In terms of Popt(20%) and IFA, KSETE outperforms the baselines except TNB and Bellwether, respectively.

SECTION 7Discussion
7.1 Why KSETE Works?
This section answers this question from two perspectives: architecture and empirical analysis.

Architecture Perspective. The good performance of our proposed KSETE can be attributed to three main aspects: (1) class imbalance learning, (2) (multiple) kernel spectral embedding, (3) transfer ensemble learning.

Software defect dataset is usually imbalanced, where the number of non-defective (majority class) instances is much more than that of defective (minority class) ones [97]. Class imbalance is a major reason accounting for the poor predicting performance on the minority class instances (defective modules) [75] [98]. The class-imbalance problem has been widely studied in the WPDP scenario [45], [99], [99], [100]. Class imbalance learning methods can be divided into two categories: data-level (such as random oversampling, random under-sampling, and SMOTE) and algorithm-level methods (such as cost-sensitive learning and ensemble learning). Actually, it is also a key problem needed to be considered in the scenario of HDP [41]. However, most previous HDP methods [38], [39], [40] did not take it into consideration, except for CTKCCA [41] which uses cost-sensitive learning for addressing the class-imbalance problem. Differing from CTKCCA, we use SMOTE to preprocess the imbalanced source data. We find that if we perform SMOTE on the projected source data, the prediction performance of KSETE obviously decreases compared with the case that performing SMOTE directly on the source data.

We take the kernel method into consideration for considering the complex nonlinear relationship of the metrics between the source and target datasets and the complex nonlinear relationship of metrics and defects. To take advantage of multiple kernel learning, a hybrid kernel function based on Gaussian kernel function is constructed and used in this paper. To find the optimal projected feature space, we maximize the distribution similarity between the kernel projected source and target datasets and simultaneously preserve the intrinsic characteristics of the source and target datasets maximally. However, previous HDP methods either do not apply kernel method such as [38], [39] or just use the simple kernel function such as [40], [41]. Moreover, previous HDP methods just directly try to maximize the distribution similarity without considering preserving the intrinsic characteristics of the original datasets. If too much useful information of original datasets is lost during finding the common feature space, then the final common feature space cannot perform very well.

Previous HDP methods [38], [39], [40], [41] are all the single transfer learning method, which try to find the latent common feature space by utilizing one time of feature space transformation on the original datasets. However, the proposed KSETE is an ensemble-based transfer learning method, which attempts to construct a series of latent common feature space on multiple kernel subspace of original datasets, and then combines these latent common feature space. It is very challenging to make two different distribution sufficiently similar. It is more possible to find the optimal common feature space from multiple feature space compared with the case that from only one feature space. The results of Section 7.3 can help to illustrate this viewpoint intuitively.

Empirical Perspective. KSETE includes three key components: SMOTE, (multiple) kernel learning, and transfer ensemble learning (TE). Now we plan to investigate what is the effect of each component on the performance of KSETE. To this end, we need first to determine the benchmark and the baselines. We just select three typical datasets (i.e., Lucene, hbase-0.94.0, and poi-2.0) as benchmark, since (1) they come from three different communities; (2) their size N (the number of modules) include big (Nâ‰¥1000 for hbase-0.94.0), medium (Nâ‰¥500 for Lucene), and small (N<500 for poi-2.0); (3) their defective rate (DR) vary from high (DRâ‰¥0.2 for hbase-0.94.0) to medium (0.1â‰¤DR<0.2 for poi-2.0) and small (DR<0.1 for Lucene). Thus we can obtain six combinations, e.g., Luceneâ‡’poi-2.0, according to Section 5.5.1. We then construct six baselines as shown in Table 9. For each combination, we also select 90 percent instances from the source data as training data to train the model and then use it on the target data. In this way, we run each method 30 times and take the average results as the performance in this combination. Finally, we report the average results of KSETE and six baselines on the above six combinations in terms of AUC, G-Measure, MCC, and Popt(20%).

TABLE 9 Baselines Used to Investigate Why KSETE Works

Fig. 8 presents the boxplots of AUC, G-Measure, MCC, and Popt(20%) for KSETE and all the baselines. From the figure, we can see that (1) KSE-TE has similar performance to KSETE in terms of AUC and MCC, but performs worse than KSETE in terms of G-measure and Popt(20%). This indicates that SMOTE has a positive effect on the performance of KSETE. (2) KSETE outperforms KSE-SMOTE-TE in terms of G-measure and Popt(20%), has similar performance to it in terms of AUC and MCC. This indicates using SMOTE before feature space transformation is better than using SMOTE after that. (3) Methods using kernel function always obviously outperforms the methods without using that. (4) SE almost always performs worst, especially in terms of G-measure and Popt(20%). (5) KSETE outperforms Single_KSETE in terms of AUC, G-Measure, and Popt(20%), and has similar performance in MCC. This indicates that multiple kernels method has a positive effect on the performance. (6) KETSE seems to always outperform KSE in terms of AUC, G-Measure, MCC, and Popt(20%). This indicates the TE has a positive effect on the performance of KSETE.


Fig. 8.
Boxplots of AUC, G-Measure, MCC, and Popt(20%) values for our KSETE and all the baselines (see Table 9). The horizontal line in the box denotes the median, while the asterisk indicates the mean.

Show All

7.2 The Effect of Î² on the Performance of KSETE?
By default, Î² (i.e, the similarity confidence parameter) is set to be 1. To study the effect of different values of Î² on the performance of the proposed KSETE, we take one prediction combination Luceneâ‡’ant-1.7 (i.e., Lucene is the source data, ant-1.7 is the target data) as an example. Specifically, we let Î² take different values 0.5, 1, 1.5, 2, 2.5, and 3. For each k value, we run KSETE 30 times. For each time, 90 percent instances of the source dataset are randomly selected as the training data, then the prediction performance of each HDP method is evaluated on the target data. Finally, we report the average PD, PF, G-measure, MCC of KSETE for different Î² values.

Fig. 9 shows the error-bars of PD, PF, G-measure, and MCC of KSETE with different Ïƒ values. The x-axis represents the value of Î². The y-axis denotes the value of each performance. The dot in the middle of the vertical line denotes the average performance of KSETE across 30 runs on the prediction combination Luceneâ‡’ant-1.7.


Fig. 9.
The error-bars of performance of KSETE with different values of M on the prediction combination Luceneâ‡’ant-1.7. The error-bar represents one standard deviation above and below the mean value.

Show All

From the figure, we notice that (1) if we pay less attention to the distribution similarity between the projected source and target datasets (D(BÎ¦(S),BÎ¦(T)) in Eq.(1)), i.e., Î²=0.5, PF,G-measure, and MCC are very poor. (2) if we pay less attention to the loss of intrinsic characteristic of the source and target datasets (L(BÎ¦(S),Î¦(S)) and L(BÎ¦(T),Î¦(T)) in Eq.(1)). e.g., the case that Î²=2.5, PD, G-measure, and MCC are also poor, especially PD and G-measure. Therefore, in this paper, Î² is set to 1 for naturally balancing the two objectives.

7.3 The Effect of Different Values of M on the Performance of KSETE?
By default, we let the number of base learners in our transfer ensemble algorithm be M=5. To study the effect of different M values on the performance of the proposed KSETE, we take one prediction combination Luceneâ‡’ant-1.7 (i.e., Lucene is the source data, ant-1.7 is the target data) as an example. Specifically, let M take different values 1, 3, 5, 7, 9, and 11. Then, we take the same experiments as that when we study the effect of different Î² values on the performance of KSETE.

Fig. 10 shows the error-bars of PD, PF, G-measure, and MCC of KSETE with different M values. The x-axis represents the value of M. The y-axis denotes the value of each performance. The dot in the middle of the vertical line denotes the average performance of KSETE across 30 runs on the prediction combination Luceneâ‡’ant-1.7.


Fig. 10.
The error-bars of KSETE with different values of M on the prediction combination Luceneâ‡’ant-1.7. The error-bar represents one standard deviation above and below the mean value.

Show All

From the figure, we notice that (1) if we do not utilize the ensemble strategy (i.e., when M=1), PF, G-measure, and MCC obviously are the worst compared with other cases utilizing the ensemble strategy. (2) to some extent (M ranges from 1 to 9), PF, G-measure, and MCC generally increase along with the increasing of M, although PD is relatively stable. (3) When M increases to a certain extent, the performance of KSETE might not further improve even will decrease. We also notice that when M ranges from 5 to 9, KSETE is not very sensitive to M, especially in terms of PD and G-measure. Therefore, M=5 can be utilized as the default setting.

SECTION 8Threats to Validity
In this section, some potential threats to the validity of our research are presented.

8.1 Internal Validity
Threats to the internal validity of our study mainly relate to the re-implementation of baselines, the lack of causal effect analysis, and the model interpretability bias. Since the source code of all the baselines except for CTKCCA [41] and Bellwether [36] is not publicly opened, we implement the reminding baselines carefully according to the description in corresponding studies. Although we have checked our source code carefully, there still may be errors that we did not notice. Since the causal effect between independent and dependent variables is not the objective of this study, we do not analyze it and related works can be seen in [1], [101]. KSETE achieves good prediction performance in both HDP and CPDP-CM scenarios by feature space transformation. However, it is difficult to illustrate the meanings of new features. All the methods which use feature space transformation such as CCA+ [39], and CTKCCA [41], TCA+ [33], and the deep-learning based methods [83][34], suffer to this threat. Building prediction models having both high performance and good interpretability is our future work.

8.2 External Validity
External validity indicates the degree to generalize the research results to other situations [32], [102]. To evaluate the performance of KSETE, experiments are performed on 22 public projects from three communities including AEEEM [33], JIRA [43], and PROMISE [44]. We compared KSETE with the five CPDP-CM methods and three HDP methods in terms of seven performance measures (see Section 5.3). The Wilcoxon signed-rank test, Cliffâ€™s delta effect size, and Scott-Knott ESD test are also used. However, we still cannot claim that the findings will be completely suitable for other defect datasets. More defect datasets and the baseline methods should be applied to reduce this threat.

8.3 Construct Validity
Threats to construct validity for our study mainly refer to the biases of performance evaluation measures. In this study, seven well-known performance measures including PD, PF, AUC, G-Measure, MCC, Popt(20%), and IFA are used where Popt(20%) and IFA are two effort-aware measures widely used in JIT defect prediction [26], [70]. However, some other measures, such as Balance [10] and H-measure [103], are not applied owing to the space limitation, which also have been utilized for the imbalanced defect datasets in previous studies [14] [75]. Meanwhile, since our method is not for JIT defect prediction and the benchmark does not include the change size metrics, we just use LOC instead of change size when computing Popt(20%) and IFA. Therefore, this study has these threats.

SECTION 9Conclusion
Differing from CPDP-CM, HDP aims to predict defects across projects with heterogeneous metric sets.

In this paper, we propose a novel KSETE method for HDP. KSETE first uses SMOTE to alleviate the class-imbalance problem of the source data. And then KSETE tries to find a latent common feature space by our proposed multiple kernel spectral embedding transfer method, which can maximize the distribution similarity between the source and target datasets and can preserve the intrinsic properties of them. Finally, we build an ensemble classifier based on multiple common feature space by our transfer ensemble algorithm and use it to predict the label of unlabeled target data.

Experiments are performed on 22 projects from three community including AEEEM [33], JIRA [43], and PROMISE [44]. The performance of KSETE is evaluated in terms of seven well-known measures including PD, PF, AUC, G-Measure, MCC, Popt(20%), and IFA. We compare KSETE with the existing HDP methods and CPDP-CM methods by using the Wilcoxon signed-rank test, Cliffâ€™s Î´, and Scott-Knott ESD test in HDP scenario and CPDP-CM scenario, respectively. The experimental results show that KSETE is effective in the both HDP and CPDP-CM scenarios.

As future work, we plan to improve our method by using deep learning owing to its powerful feature learning capability.