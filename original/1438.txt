Software defect prediction has always been a hot research topic in the field of software engineering owing to its capability of allocating limited resources reasonably. Compared with cross-project defect prediction (CPDP), heterogeneous defect prediction (HDP) further relaxes the limitation of defect data used for prediction, permitting different metric sets to be contained in the source and target projects. However, there is still a lack of a holistic understanding of existing HDP studies due to different evaluation strategies and experimental settings. In this paper, we provide an empirical study on HDP approaches. We review the research status systematically and compare the HDP approaches proposed from 2014 to June 2018. Furthermore, we also investigate the feasibility of HDP approaches in CPDP. Through extensive experiments on 30 projects from five datasets, we have the following findings: (1) metric transformation-based HDP approaches usually result in better prediction effects, while metric selection-based approaches have better interpretability. Overall, the HDP approach proposed by Li et al. (CTKCCA) currently has the best performance. (2) Handling class imbalance problems can boost the prediction effects, but the improvements are usually limited. In addition, utilizing mixed project data cannot improve the performance of HDP approaches consistently since the label information in the target project is not used effectively. (3) HDP approaches are feasible for cross-project defect prediction in which the source and target projects have the same metric set.
SECTION 1Introduction
Software defect prediction (SDP) as a means of software quality assurance has been a widely studied research topic for a long time, with early work being conducted in the nineties. At first, SDP was usually performed within the same software project, which is regarded as within-project defect prediction (WPDP) [1], [2], [3], [4], [5]. In this way, researchers use the model trained by sufficient labeled modules (i.e., source code files, classes or functions contained in software projects) to predict unlabeled ones. In recent years, many machine learning-based approaches [6], [7], [8], [9], [10], [11] and various software metrics [12], [13], [14], [15], [16] have been presented and applied to SDP. Since labeled data are usually unavailable and limited for some projects [17], [18], [19], [20], researchers train the prediction model with defect data from other projects, that is, cross-project defect prediction (CPDP). CPDP refers to building prediction models for the target project by using historical data collected from other existing projects. In the last decade, CPDP has gradually grown into a quite important sub-topic of SDP. It has been found that the prediction model built from cross-project data can be just as effective as that built from within-project data [21], [22], [23], [24], [25], [26], [27].

However, existing CPDP approaches require that the modules of source and target projects have the same metric set (i.e., both of the projects are characterized by the same features), while there could be very few common metrics between source and target projects in many situations. The metrics usually depend on the project itself, extraction tools, requirements of maintainers, etc. Consequently, these uncertainties increase the possibility that the source and target projects have different metric sets. When the metrics of source and target projects are totally different, conventional CPDP approaches cannot be applied to this scenario directly. To solve this problem, researchers set about studying a solution for CPDP with different metric sets, that is, heterogeneous defect prediction (HDP) [28], [29]. They drew lessons from transfer learning techniques to eliminate the heterogeneity between source and target projects. With in-depth research, more factors are considered for boosting the performance of HDP approaches. Additionally, corresponding technologies used in CPDP have also been scientifically transplanted into HDP and further improved.

In CPDP and HDP, each prediction can be normally expressed as a prediction combination of the source and target projects [29], [30], [31], that is, source A⇒ target B. Source A is treated as the training set to predict the defective modules in target B that is the test set. In Fig. 1, different shaped modules are characterized by heterogeneous metric sets. In other words, the modules for prediction combination in CPDP have the same shape, whereas those in HDP have different shapes. To distinguish the two types of prediction modes in this paper, we define the typical CPDP and HDP contexts based on the prediction combination as follows.


Fig. 1.
The CPDP and HDP prediction combinations.

Show All

Typical CPDP context, in which the source and target projects from a prediction combination have the same metric set.

Typical HDP context, in which the source and target projects from a prediction combination have different metric sets.

Table 1 shows the number of metrics of five datasets, including NASA [32], SOFTLAB [19], ReLink [33], AEEEM [34] and PROMISE [35]. Table 2 shows the number of common metrics of each pair of datasets. From Table 2, we can see that there is only one common metric between NASA and AEEEM, which is line of code (LOC) metric. In this scenario, conventional CPDP approaches [19], [22], [25], [26] cannot obtain desirable prediction results because some informative metrics necessary for building a good prediction model may not exist across the projects [28], [29]. Therefore, HDP is attracting more attention from researchers, while relevant studies have achieved notable results because of its reduced restriction on defect data.

TABLE 1 Number of Metrics in Projects of Five Datasets

TABLE 2 Number of Common Metrics Between Two Datasets

1.1 Motivation
For current HDP studies, the major contributions merely propose an HDP approach and evaluate its effectiveness under a relatively specific experimental setup. Specifically, researchers focus on evaluating HDP approaches under a typical HDP context while overlooking their relations with CPDP approaches. Based on this knowledge, we find the following deficiencies exist in current HDP studies.

According to our statistics, HDP has been a focus of researchers only for the past five years, and some studies were proposed in the same year, which makes it difficult to comprehensively compare proposed HDP approaches with state-of-the-art approaches. This leads to the current state where there is a lack of a holistic understanding of the progress on HDP studies, resulting in confusion about choosing the proper HDP approach for practical tasks [34], [36], [37], [38]. Therefore, it is significant and valuable for understanding the progress of HDP studies to compare existing HDP approaches under the same experimental context. To the best of our knowledge, there is still no study comparing existing HDP approaches comprehensively.

Although HDP approaches are specifically designed for prediction combination with different metric sets, they actually have few restrictions on the metrics of source and target projects. Therefore, HDP approaches can be naturally applied to cross-project defect prediction in which the source and target projects have the same metric set. However, we still have no idea about their performance under a typical CPDP context. In current HDP studies, researchers usually only evaluate the proposed approaches under a typical HDP context and overlook their feasibility in CPDP. Therefore, we believe that it is worth trying to explore the feasibility and performance of HDP approaches under a typical CPDP context. This will help to provide a holistic understanding of existing HDP approaches for researchers and practitioners.

Motivated by the above analyses, we intend to conduct an empirical study on HDP approaches through extensive experiments and discussions under the typical HDP and CPDP contexts in this paper. Furthermore, we provide guidelines for the application of HDP approaches and share lessons learned for future development and evaluation of HDP approaches to researchers and practitioners.

1.2 Contribution
The main contributions of this empirical study can be summarized as follows.

Systematic Review and Analysis. We review and analyze existing HDP studies comprehensively and systematically. We first collect the literature on HDP up to June 2018. According to the method of processing the metrics, we then categorize existing HDP approaches and summarize some major improvements. Finally, we further clarify the relationship between HDP and CPDP.

Comparison Among HDP Approaches. We conduct a comprehensive comparison among existing HDP approaches under a typical HDP context and determine the approaches that currently have the best performance. In addition, the effects of handling class imbalance problems and utilizing mixed project data on prediction performance are also investigated and discussed.

Comparison With CPDP Approaches. We study the feasibility of HDP approaches under a typical CPDP context by comparing them with a range of CPDP approaches for the first time. The experimental results indicate that HDP approaches can be feasible for cross-project prediction in which the source and target projects have the same metric set.

1.3 Organization
The remaining of this paper is arranged as follows. Section 2 reviews and analyzes the current status of HDP studies. Section 3 introduces the research methodology in detail, and the experiment settings are presented in Section 4. The experimental results are reported in Section 5. In Section 6, we provide some discussions about this empirical study. Section 7 supplements some threats to validity. The conclusion and future work are given in Section 8.

SECTION 2Heterogeneous Defect Prediction
In this section, we first describe the major challenges confronted by HDP. Then, we introduce and analyze the research status of HDP by reviewing existing HDP studies. Based on the literature review, we classify HDP approaches according to their methods of processing metrics and summarize their improvements. Finally, we present the relationship between CPDP and HDP.

2.1 Major Challenges
The major challenges confronted by HDP (i.e., the heterogeneity between source and target projects) are reflected in the following two aspects:

Different Metric Sets. The first challenge of HDP is how to overcome different metric sets contained in source and target projects. Existing CPDP approaches require that the modules in source and target projects are characterized by the same metric set. However, since the metrics are often extracted by organizations that utilize different extraction tools and have diverse requirements, it is likely that the source and target projects will have different metric sets. Therefore, unifying the structure of the metrics is a priority for HDP.

Different Data Distributions. The second challenge is the difference in the data distributions of source and target projects, which exists in HDP and CPDP simultaneously. Although the source and target projects have the same metric set, the essential difference between different projects is also reflected in the diversity of data distributions. Only when the data distributions of source and target projects are as similar as possible, can the prediction model trained by the source project data better adapt to the target project data.

2.2 Review of Research Status
To provide a comprehensive understanding of the research status on HDP, we follow the strategy used in the references [49], [50], [51] to perform the literature review. To collect HDP-related studies, we first define the inclusion criteria as follows.

The paper is published before June 2018.

The paper is written in English.

The paper presents an approach for HDP and evaluates it under a typical HDP context.

The exclusion criteria are also defined as follows.

The study focuses on proposing an approach for CPDP and evaluates it under a typical HDP context by using the common metrics of source and target projects.

If a study has both conference and extended journal versions, its conference version is excluded.

Nam et al. [29] and Jing et al. [28] regarded cross-project prediction with different metric sets as heterogeneous defect prediction to conduct a study for the first time in 2015. Therefore, we search the digital libraries (i.e., Google Scholar, ACM Digital Library, and IEEE Xplore) for relevant papers by using the keyword combinations of “heterogeneous” or “cross project” or “cross company” and “defect prediction” or “fault prediction” from 2015 to 2018. First, based on Google Scholar, we obtain seven papers [28], [40], [41], [42], [43], [44], [46] from the search results with the filtering criteria. Note that the paper from Nam et al. [40] is the extension and journal version of their previous work [29]. Then, we search on IEEE Xplore and obtain another paper [48] published online that meets the criteria. Next, we search the ACM Digital Library without finding any additional relevant papers. Finally, by tracking the citations of the eight collected papers, we also find that He et al. [30] proposed an approach for cross-project defect prediction with imbalanced feature sets (i.e., HDP) in 2014. Based on the above steps, we collect nine papers relevant to HDP. These studies are listed in Table 3. The 3rd to 6th columns list the components of the HDP approaches that are used to address the subproblems in the heterogeneous defect prediction, and “N/A” means that an approach does not contain a solution for this component. The 7th to 11th columns list the experiment settings corresponding to each HDP approach. Specifically, the 7th and 8th columns display whether an approach utilizes mixed project data or multiple source data during prediction; the 9th column reports the numbers of used projects and how many datasets these projects come from; and the 10th column shows the evaluation indicators used in the experiments. Within the last column, we list the classifiers used for predictions that usually have the best performance according to the corresponding papers.

TABLE 3 Literature Overview of Heterogeneous Defect Prediction

From Table 3, we have the following observations: (1) the study of He et al. [30] presented an HDP approach for the first time in 2014. This approach uses distribution characteristics[17] to describe heterogeneous projects so that they have the same metric set. (2) Current HDP studies mainly focus on designing the component to deal with heterogeneity between source and target data instead of proposing an improved predictor (i.e., classifier). (3) Except for the concern of heterogeneity, the class imbalance problem has also attracted the attention of several HDP studies [41], [43], [44], [46]. Specifically, cost-sensitive learning and ensemble learning are primary techniques employed by these HDP approaches for this problem. (4) In CLSUP [46] and MSMDA [48], researchers considered combining source data with a small number of labeled modules in the target project (i.e., training target data) as the mixed project data to be utilized. (5) MSMDA is the only approach that uses multi-source project data to construct the prediction model. (6) Existing HDP approaches are usually evaluated under different data contexts (i.e., the dataset and its contained projects), which hinders researchers from obtaining full knowledge of the approach’ performance. (7) Logistic regression is the most widely used classifier in existing HDP studies and can obtain the best prediction results in most cases.

Except for the studies listed in Table 3, there are several SDP approaches that are unaffected by heterogeneous metric sets, because they actively extract the same metrics for source and target projects. Zhang et al. [52] proposed to apply the connectivity-based unsupervised classifier that does not require any training data in defect prediction. Moreover, Wang et al. [53] leveraged Deep Belief Network (DBN) to automatically learn semantic features based on token vectors extracted from programs’ Abstract Syntax Trees for source and target projects, which ensures that both projects have the same metric set. However, the source and target projects need to be developed by using the same language in order to guarantee the applicability of AST.

In conclusion, a total of nine HDP studies were published up to June 2018, some of which were accepted by top conferences and journals in software engineering. This also illustrates the necessity of proposing targeted solutions for HDP. The empirical study in this paper will also be conducted around these published HDP studies.

2.3 Analysis on Research Status
Based on the above observations of Table 3, we make the following analyses on the current status of HDP.

2.3.1 Categorization of HDP Approaches
According to the method of processing heterogeneous metrics, we classify existing HDP approaches into two categories: metric selection and metric transformation, following the criterion of whether the processed metrics maintain their original meaning. Fig. 2 shows the processes of handling heterogeneous metrics in the two categories of HDP approaches, and the module of Project i has a different metric set from that of Project j. From Fig. 2, we can see that the metric transformation-based HDP approach constructs a common metric space for both projects or transforms the source data into the metric space of the target data to eliminate the heterogeneity between source and target projects. Since transformed metrics are essentially synthesized by the original ones, it is difficult to determine their actual meanings. In addition, the metric selection-based HDP approach selects metrics from the source project and matches them with those in the target project for unifying the metric structure, which simultaneously keeps the original meanings of the metrics.


Fig. 2.
The processes of metric transformation-based and metric selection-based HDP approaches.

Show All

Transfer learning is adept at transferring knowledge across domains, which corresponds to the main idea of addressing the heterogeneity in HDP. Weiss et al. [54] deemed that most heterogeneous transfer learning techniques fall into two categories when transforming the feature spaces: symmetric and asymmetric transformation [55]. Symmetric transformation finds a common latent feature space to represent both source and target domains, whereas asymmetric transformation maps the source domain to the target. Based on the above knowledge, we further divide the process of metric transformation into symmetric and asymmetric transformations, as shown in the blue box of Fig. 2. Symmetric transformation maps both source and target data into a common metric space, and asymmetric transformation maps the source data into the metric space of the target data.

Based on the above analysis, we summarize existing HDP approaches and their corresponding categories in Table 4. From Table 4, we can see that (1) CPDP-IFS, CCA+, CCT-SVM, CTKCCA and MSMDA are based on symmetric metric transformation; (2) EMKCA and CLSUP are based on asymmetric metric transformation; and (3) HDP-KS and FMT are based on metric selection. Metric transformation-based HDP approaches aim to improve the correlation and similarity between source and target data so that the prediction model can be well fitted with the target data, but the metrics in the transformed source or target data have no actual meaning. Metric selection-based HDP approaches usually first remove redundant metrics of the source project and then match selected metrics with those of the target project one-for-one, which allows the selected and matched metrics to maintain their original meaning. However, metric selection always involves the loss of metric information that may be unfavorable for building a good prediction model. Moreover, Nam et al. noted that metric selection and matching in HDP-KS can make the prediction model simpler and easier to explain, whereas CCA+ cannot due to its transformed metrics without actual meanings [40]. We will further investigate the performance of two categories of HDP approaches by detailed comparison in Section 5.1.

TABLE 4 The Categories of HDP Approaches

2.3.2 Improvements of HDP Approaches
In addition to the heterogeneity between source and target projects, we also find that researchers always improve the performance of HDP approaches from three aspects as follows.

Class Imbalance Problem. The distributions of defective and non-defective modules in software projects are often highly imbalanced. This implies that the number of non-defective modules in a project is far more than the number of defective ones. The class imbalanced distribution always causes the misclassification of modules from the minority class and is the major factor leading to unsatisfied predictions. From Table 3, we can observe that EMKCA, CTKCCA, and CLSUP have the component of handling class imbalance problems by different strategies. Specifically, CTKCCA and CLSUP employ cost-sensitive learning to relieve the influence caused by imbalanced classes, while EMKCA makes use of ensemble learning. We will further discuss the effect of handling class imbalance problems on prediction performance in Section 5.2.

Mixed Project Data [26]. This means making use of a small number of labeled modules in the target project that are known as the training target data [27], [46] together with the source project data to construct the prediction model. The unlabeled modules in the target project are known as the test target data. Since training target data presents similar data distribution with test target data, it can provide available label information of modules in the target project to the training data. Based on the observations of Table 3, we know that only CLSUP and MSMDA utilize mixed project data to build the prediction model. We will further discuss the impact of using mixed project data on prediction performance in Section 5.3.

Multi-Source Data. This indicates that more than one source project is selected to train the prediction model. Based on the fact that multiple source projects can generally provide more information than a single project, researchers intuitively realize that there may exist a positive correlation between the quantity of training data and the performance of prediction model. Based on the observations of Table 3, MSMDA is the only HDP approach that uses multi-source data to train the prediction model. Specifically, it constructs the optimal set of source projects by the strategy of multi-source selection to improve prediction results.

In this paper, we mainly discuss the first two improvements (i.e., class imbalance and mixed project data) for the reason that multi-source data are utilized only in MSMDA and are not conducive to controlling the source project accurately.

2.3.3 Relationship Between HDP and CPDP
In this section, we describe the relationship between HDP and CPDP. In Fig. 3, the upper and lower parts separately display the relations from the perspectives of the scope of SDP problems and the design of SDP approaches. According to the upper part, the CPDP problem means that the source and target data are from different projects and has no restriction that they must be characterized by the same metric set. Therefore, the HDP problem in which the source and target projects have different metric sets can be regarded as a special case or subset of the CPDP problem. From the lower part, we can see that existing CPDP approaches mainly focus on eliminating the differences between the data distributions of projects and require that the source and target projects have the same metric set. Hence, CPDP approaches usually cannot be applied to HDP directly. However, HDP approaches need to additionally consider unifying the heterogeneous metric structures of source and target projects before eliminating the differences in data distributions.


Fig. 3.
Relationship between HDP and CPDP.

Show All

In summary, on the one hand, the scope of CPDP problem is broader that of HDP problem since it has no limitation on metric sets. On the other hand, although HDP approaches are designed for prediction combination with different metric sets, they actually do not have any restriction on the metrics of source and target projects. Therefore, HDP approaches can be directly applied to cross-project defect prediction in which the source and target projects have the same metric set. We will further discuss whether HDP approaches are feasible for CPDP in Section 5.3.

SECTION 3Research Methodology
In this section, we introduce the research methodology of this empirical study based on Goal Question Metric (GQM)[56] that has been summarized in Table 5.

TABLE 5 The summary of GQM
Table 5- 
The summary of GQM
3.1 Research Goal
This study focuses on investigating the current status of HDP approaches in terms of the empirical evidence based on prediction results. We plan to evaluate these approaches under the typical HDP and CPDP contexts. Therefore, we define the research goal in a structured way as below.

Goal. Understanding the progress of HDP approaches from the viewpoint of prediction performance within typical HDP and CPDP contexts.

From Table 5, we can see that the research goal is described from five aspects (i.e., purpose, issue, object, viewpoint, and context).

3.2 Research Questions
In order to achieve the research goal, we design the following research questions (RQs) based on the systematic review of HDP studies.

RQ1: Which of the metric selection-based and metric transformation-based HDP approaches have the best prediction performance?

RQ2: How about the effects of such improvements (i.e., handling class imbalance problems and utilizing mixed project data) on HDP approaches’ performance?

RQ3: Are the HDP approaches feasible for cross-project defect prediction in which the source and target projects have the same metric set?

RQ1 is proposed to provide a more holistic and deep understanding of HDP approaches’ performance by comparing them against each other under a typical HDP context and determining which approaches perform best. RQ2 focuses on investigating whether handling class imbalance problems and utilizing mixed project data can improve HDP approaches’ performance. Furthermore, we design RQ3 to examine the feasibility of HDP approaches in cross-project defect prediction by comparing them with a range of representative CPDP approaches under a typical CPDP context.

3.3 Evaluation Metrics
To conduct a comprehensive evaluation, we aim to investigate not only how the prediction effect of each approach is but also whether the differences between their performance are significant. We select the evaluation metrics from two aspects: overall performance and statistical significance.

3.3.1 Evaluation for Prediction Performance
We totally employ four widely used indicators to evaluate the performance of approaches, including F-measure, G-measure, AUC and MCC. These indicators can be defined by using true positive (tp), false negative (fn), false positive (fp) and true negative (tn) in Table 6. tp, fn, fp and tn are the number of defective modules that are predicted as defective, the number of defective modules that are predicted as non-defective, the number of non-defective modules that are predicted as defective, and the number of non-defective modules that are predicted as non-defective, respectively.

TABLE 6 Four Kinds of Defect Prediction Results

recall and pf are two widely used indicators for SDP [9], [19], [21], [22], [25], [26]. Probability of detection (pd or recall) is defined as tp/(tp+fn), which denotes the ratio of the number of defective modules that are correctly classified as defective to the total number of defective modules. Probability of false alarm (pf) is defined as fp/(fp+tn), which denotes the ratio of the number of non-defective modules that are wrongly classified as defective to the total number of non-defective modules. Moreover, the precision of a model denotes the ratio of the number of defective modules that are correctly classified as defective to the number of modules that are classified as defective and it is defined as tp/(tp+fp). The used indicators can be further defined as follows.
F−measure=2∗recall∗precisionrecall+precision(1)
View Source
G−measure=2∗recall∗(1−pf)recall+(1−pf)(2)
View Source
MCC=tp∗tn−fn∗fp(tp+fp)(tp+fn)(fp+tn)(fn+tn)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−√,(3)
View Sourcewhere F-measure [28], [30] is the harmonic mean between recall and precision, G-measure [24], [52], [57], [58] is the harmonic mean between recall and pf, and MCC represents Matthews Correlation Coefficient [21], [28] that is in essence a correlation coefficient between the observed and predicted binary classifications with values in [-1,1].

AUC is the area under the receiver operating characteristic curve, which is plotted in a two-dimensional space with pf as x-coordinate and pd as y-coordinate. The AUC is known as a useful measure for comparing different models and is widely used [25], [29], [37], [38], [59], [60] because it is unaffected by class imbalance as well as being independent from the prediction threshold. The higher AUC represents better prediction performance and the AUC of 0.5 means the performance of a random predictor [61]. Lessmann et al. [37] and Ghotra et al. [38] suggest using AUC for better cross-dataset comparability. Hence, we also select AUC as one of used indicators.

All above evaluation indicators range from 0 to 1 except for MCC with values in [-1,1]. Obviously, an ideal defect prediction model should possess high values of F-measure, G-measure, MCC, and AUC.

3.3.2 Evaluation for Statistical Significance
We also conduct the statistical significance test for determining whether the difference among approaches’ performance is significant in terms of the results on each indicator. Following the suggestion of Herbold [62], we choose the non-parametric Friedman test [63] with the Nemenyi’s post-hoc test at a confidence level of 95 percent as proposed by Demšar [64], which has been broadly used to evaluate the statistical significance of difference among multiple models. Previous studies [38], [59], [65] pointed out its drawback of producing overlapping groups, which implies that there may be models located in more than one group. In this situation, a model has multiple group rankings and we use its average group ranking as the unique result on this indicator.

In this paper, we use the mean value of an approach’s group rankings on all indicators as the final evaluation basis for its performance based on the results of the statistical significance test. However, the Nemenyi’s post-hoc test usually clusters compared approaches into a different number of groups, so that the group ranking on each indicator has different scales (i.e., maximum). In order to eliminate this impact on mean values, we employ the min-max normalization to make the group ranking on each indicator within [0,1]. Based on the normalized group ranking, we define the Rankscore for an SDP approach as below, which makes the approach with better performance has a higher Rankscore rather than the smaller average group ranking.
Rankscore=1−∑|indicators|i=1min_max(rankingi)|indicators|,(4)
View Sourcewhere the |indicators| is the number of used evaluation indicators, min_max() represents the process of normalization, and rankingi means the group ranking of an approach on ith indicator. The value of Rankscore is within (0,1] where 1 indicates that an approach is always located in and only in the first group across all indicators.

SECTION 4Experiments
In this section, we first present details of the datasets used in experiments. Then the SDP approaches for comparisons are introduced. The detailed experimental design for each RQ will be finally explained.

4.1 Datasets
In experiments, 30 publicly available and commonly used projects are chosen from five different datasets including NASA1 [32], SOFTLAB1 [19], ReLink2 [33], AEEEM3 [34] and PROMISE1 [35] as the experimental data. Table 7 shows the details related to these datasets. Prediction granularity in the last column indicates what an module (i.e., prediction object) refers to in corresponding software project.

TABLE 7 Details of Software Project Used in Experiments

NASA dataset is publicly available and commonly used for defect prediction [9], [32]. Each project in NASA represents a NASA software system or sub-system, which contains the corresponding defect-marking data and various static code metrics (e.g., size, readability, complexity, and etc.). We select five projects that share the same metrics and have been preprocessed by Shepperd et al. [32] Therefore, the identical and inconsistent modules have been removed from corresponding projects.

Turkish software dataset (SOFTLAB) consists of AR1, AR3, AR4, AR5 and AR6 projects. There exist 28 common metrics between SOFTLAB and NASA, which are both obtained from PROMISE repository. Although the defect data of these two datasets are from the same repository, they are very different from each other.

ReLink was collected by Wu et al. [33] and the defect information in ReLink has been manually verified and corrected. ReLink has 26 complexity metrics, which are widely used in defect prediction. Among the three used projects, the number of modules ranges from 56 to 399.

The AEEEM dataset was collected by D’Ambros et al. [34] It consists of 61 metrics: 17 source code metrics, 5 previous-defect metrics, 5 entropy-of-change metrics, 17 entropy-of-source-code metrics, and 17 churn-of-source code metrics.

The last group of datasets is originally collected by Jureczko and Madeyski [35] from the online PROMISE data repository, which consists of several open-source projects. The PROMISE datasets have 20 metrics in total, which contains McCabe’s cyclomatic metrics, CK metrics and other OO metrics.

4.2 SDP Approaches for Comparisons
In this section, we introduce the SDP approaches for comparisons under the typical HDP and CPDP contexts respectively.

4.2.1 Approaches for Comparison in Typical HDP Context
For RQ1, all existing HDP approaches listed in Table 3 are used for the comparison under a typical HDP context. For RQ2, on the one hand, we select the HDP approaches that consider the class imbalance problem (i.e., EMKCA, CTKCCA, and CLSUP) as the objects to explore whether the improvement of handling class imbalance can boost their prediction effects. On the other hand, we take the HDP approaches without using mixed project data (i.e., CPDP-IFS, CCA+, HDP-KS, CTKCCA, EMKCA, and FMT) as the objects to investigate whether the combination with training target data can improve their performance. It is noted that we do not compare CCT-SVM in experiments. This is because the only difference between CCT-SVM and CCA+ is the classifier. Moreover, CCT-SVM is just tailored for the SVM classifier and cannot be applied to other classifiers directly (e.g., the LR classifier used in this paper).

4.2.2 Approaches for Comparison in Typical CPDP Context
For RQ3, the approaches used for comparison contain two parts: HDP and CPDP approaches. Regarding HDP approaches, we select some of them that have considerable performance based on the evaluation results in RQ1 to compare with CPDP approaches. Besides, we select a range of representative CPCP approaches, including LT [66], NN-filter [19], VCB [25] and SSTCA+ISDA [67]. In the study of Herbold et al. [50], they conducted a comparative study on CPDP approaches proposed from 2008 to 2015 and provided a distinct ranking in descending order of overall performance where LT and NN-filter ranked first and second respectively. LT is the abbreviation of “log transformation” that is used to standardize source and target data. VCB and SSTCA+ISDA are proposed after 2015 and designed for addressing the class imbalance problem.

4.2.3 Implementations for SDP Approaches
We implement these approaches based on MATLAB programming following the original settings of corresponding papers, except for FMT. In order to ensure the fairness of comparisons, we choose logistic regression classifier [68], [69] for all compared approaches. Moreover, we apply z-score normalization to all of the training and test data before running these approaches. For FMT, according to the original setting in its study, 200 modules should be sampled from source and target projects respectively during the phase of metric matching. Considering the possibility of projects containing less than 200 modules, we set the 90 percent number of the smaller one between source and target projects as the sampling amount so that FMT can be executed on all prediction combinations.

4.3 Experimental Design
The experiments are designed to address the following research questions.

RQ1: Which of the metric selection-based and metric transformation-based HDP approaches have the best prediction performance?

To address RQ1, we compare HDP approaches on 30 projects under a typical HDP context. Specifically, one project is chosen from 30 projects as the target, and each project from other datasets that the target project does not belong to is used as the source in turn. For example, if the target project is from NASA, the source project will be chosen from SOFTLAB, ReLink, AEEEM or PROMISE. In this way, we construct 672 possible prediction combinations (i.e., source⇒target) from these 30 projects of five datasets. Considering the requirement of training target data in MSMDA and CLSUP, we randomly select 10 percent of modules from target project as training target data and the remaining 90 percent as test target data according to the default setting in original papers. Test target data are used for testing across all compared approaches. We repeat the above study 20 times for mitigating the bias introduced by random splitting for the target project. Note that MSMDA constructs a unique set of source projects as the training data for a target project through its multi-source selection framework. Therefore, there are actually 30 prediction combinations for MSMDA4.

RQ2: How about the effects of such improvements (i.e., handling class imbalance problems and using mixed project data) on HDP approaches’ performance?

In order to address RQ2, we conduct the experiments from two aspects of improvement: handling class imbalance problems and utilizing mixed project data. Accordingly, we split RQ2 into the following two sub-questions.

RQ2.1: Does handling class imbalance problems improve the HDP approaches’ performance?

Among compared HDP approaches, CTKCCA, EMKCA and CLSUP employ different strategies, including cost-sensitive learning and ensemble learning, to address class imbalance problem. To answer RQ2.1, we first select these three HDP approaches as research objects. Then, we construct their modified versions that take no account of the class imbalance problem. Specifically, for CTKCCA and CLSUP with the cost-sensitive learning technique, we set the ratio of two types of misclassification costs as 1:1 to get their modified versions. For EMKCA with the ensemble learning technique, we choose the linear kernel and Gaussian kernel that perform well to separately construct its modified version for a single kernel, instead of the ensemble of multiple kernels. Finally, we compare these HDP approaches with their modified versions and report the median results of all evaluation indicators across 30 target projects.

RQ2.2: Does utilizing mixed project data improve the HDP approaches’ performance?

In HDP approaches, only CLSUP and MSMDA utilize mixed project data by combining the 10 percent training target data with the source data to train the prediction model. To address RQ3, we select those HDP approaches without using mixed project data as the research objects. Referring to the settings in the study of Li et al. [48], we incorporate 10 percent labeled modules of target project into the training data and follow the same experimental process designed for RQ1 to evaluate the effect of using mixed project data on HDP approaches’ prediction performance.

RQ3: Are the HDP approaches feasible for cross-project defect prediction in which the source and target projects have the same metric set?

To address RQ3, we compare the selected HDP approaches with a range of representative CPDP approaches on 30 projects under a typical CPDP context. Differing from the setting in RQ1, we select one project from 30 projects as the target, and each project from the same dataset will be chosen as the source in turn except for the target project. For example, if CM1 from NASA is chosen as the target project, MW1, PC1, PC3 and PC4 from the same dataset will be selected as the source project in turn. We can totally construct 198 possible prediction combinations from these 30 projects of five datasets. Considering that the HDP approaches using mixed project data may be included in the comparison, we also follow the same strategy of splitting the target project and repeat the above process 20 times.

SECTION 5Experimental Results
In this section, we report the experiment results and provide the corresponding answer to each research question. To distinguish the two categories of HDP approaches, we use the subscripts T and S to indicate the metric transformation-based and metric selection-based approaches in tables and figures respectively.

5.1 Results for RQ1
The median results of AUC, F-measure, G-measure and MCC for each HDP approach are reported in Table 8. The best value of each indicator on a target dataset is marked in bold. The ”All” shows the median indicator values across 30 target projects. From Table 8, we can observe that CTKCCA obtains the highest AUC, F-measure, G-measure and MCC values in more than half of the cases. To be specific, it achieves the best performance on 12 out of 20 results.5 According to the results in ”All”, CTKCCA performs best on three indicators except for AUC while EMKCA obtains the best AUC value of 0.806. The main reason is that CTKCCA employs the kernel learning and cost-sensitive learning techniques during metric transformation. On the one hand, the kernel learning technique has been theoretically and empirically proven to be capable of handling the linearly inseparable problem by transforming data from its original space into a high dimensional nonlinear space [70], [71]. On the other hand, CTKCCA utilizes the kernel canonical correlation analysis to maximize the correlation between source and target projects, which can make their data distributions similar in the nonlinear space. Furthermore, CTKCCA also incorporates different types of misclassification costs to address the class imbalance problem. However, we also find that there exist some shortcomings for CTKCCA based on the observation of its unsatisfactory performance on SOFTLAB. Possible reasons are two folds: (1) according to prior studies [11], [72], the parameters of Gaussian kernel function used in CTKCCA affect the prediction effect significantly, and it is difficult to determine its optimal value for each prediction combination. Therefore, the pre-set parameters of the Gaussian kernel function may not be suitable for all prediction combinations. (2) The incomplete Cholesky decomposition (ICD) employed by CTKCCA is to find the low-rank approximation of the kernel matrix of which the rank is typically far less than the number of instances [44], [73]. However, the default rank is set to 70 and the projects in SOFTLAB usually contain fewer modules with the lower defective rate (e.g., AR3 has 63 modules with 8 defective ones), which may weaken the effect of kernel mapping.

TABLE 8 Comparison Results on Five Datasets Under a Typical HDP Context

To visualize the prediction results across multiple runs, we use boxplot [31] to display the details of indicator values (20 random runnings) for all target projects, as shown in Fig. 4. The indicator values of CTKCCA and EMKCA are more scattered than those of other approaches, which indicates that their performance presents large fluctuation when predicting for different target projects. This is mainly because both of them employ the kernel learning technique and are sensitive to kernel parameters so that the default settings may not be suitable for all prediction combinations.


Fig. 4.
Boxplot of AUC, F-measure, G-measure and MCC values over all targets in typical HDP context.

Show All

The results of statistical significance test on each indicator are shown in Fig. 5. We can observe that the first groups on AUC, F-measure, G-measure and MCC are {CTKCCA, EMKCA, MSMDA, CLSUP}, {CTKCCA}, {MSMDA, CLSUP} and {CTKCCA} respectively. There is no overlap between the first group and other ones on each indicator, which indicates that the HDP approaches included in the top-ranked group have significant superiority over those in other groups. In addition, according to the results in the last row of Table 8, CTKCCA obtains the best Rankscore of 0.917 followed by CLSUP with 0.875 and MSMDA with 0.825. Therefore, the top three HDP approaches are based on metric transformation. For metric selection-based approaches, HDP-KS with Rankscore of 0.350 has a better overall performance than FMT with 0.083. In summary, we can conclude the answer to RQ1 as follow.


Fig. 5.
Comparison of HDP approaches against each other with Nemenyi’s post-hoc test on AUC, F-measure, G-measure and MCC. Groups of approaches that are not significantly different (at p=0.05) are connected.

Show All

Answer to RQ1. In terms of the Rankscore, most of metric transformation-based HDP approaches generally perform better than metric selection-based approaches. Specifically, CTKCCA, which is based on symmetric metric transformation, performs best among existing HDP approaches, followed by CLSUP and MSMDA. Besides, HDP-KS is the better metric selection-based HDP approach compared with FMT.

5.2 Results for RQ2
5.2.1 Results for RQ2.1
Fig. 6 shows the median results across 30 target projects on four indicators. For the approach marked by asterisks (*), it represents the variant of the original approach and does not handle the class imbalance problem. Note that EMKCA∗ and EMKCA∗∗ stand for the modified EMKCA which only uses the Gaussian kernel and linear kernel respectively. In general, the original approaches (i.e., CTKCCA, CLSUP, and EMKCA) obtain better evaluation results on all indicators than their variants that do not handle the class imbalance problem. However, the improvements in overall performance are usually limited. The reasons are as follows. For CTKCCA and CLSUP using cost-sensitive learning, they introduce different types of misclassification costs and make the prediction model more inclined to predict unlabeled modules as defective ones, which may increase the pf and decrease the precision such that the overall performance boost is restrained. For EMKCA using ensemble learning, the weights for each base classifier are not optimal, that is, it cannot guarantee that the combination of weights is optimal for target projects [43]. In this situation, there may exist some base classifiers that do not produce positive effects on the final ensemble output. Therefore, the boost in performance of EMKCA is also limited on some of the indicators, compared with EMKCA∗ employing the Gaussian kernel. The only exception is that EMKCA obviously performs better than EMKCA∗∗ on AUC, F-measure and MCC. This is mainly because EMKCA∗∗ with the linear kernel is not able to deal with the class imbalance problem, while EMKCA integrates multiple classifiers to relieve this problem. Specifically, EMKCA makes full use of the diversity of individual classifiers and thus the misclassification of the modules in the minority (i.e., defective) class can be decreased [74]. In general, the above observations indicate that the cost-sensitive learning and ensemble learning techniques can eliminate the impact of class imbalance problem and improve the overall prediction effect to a certain degree. The answer to RQ2.1 can be concluded as follow.


Fig. 6.
Histogram of the median values on four indicators for HDP approaches without (∗) and with handling class imbalance problems.

Show All

Answer to RQ2.1. Handling class imbalance problems (i.e., cost-sensitive learning and ensemble learning) can improve the overall performance of HDP approaches, but the improvement is usually limited.

5.2.2 Results for RQ2.2
Fig. 7 shows the comparison results of HDP approaches with and without using mixed project data on four indicators. The approach with a symbol (&) represents the modified version that makes use of the mixed project data to train the prediction model. Based on the comparison results, it is hard to get a consistent conclusion between each approach and its variant. On the one hand, we observe that CPDP-IFS&, HDP-KS& and FMT& obtain better results than their original versions on all indicators. The main reason is that the training target data can provide useful label information from the target project to the prediction model, so the model can be more adaptable to the test target data. On the other hand, EMKCA& performs worse than EMKCA on all indicators. Moreover, the performance of CTKCCA& are inferior to those of CTKCCA on AUC and G-measure. This is contrary to our intuition that using the labeled target modules can be in favor of prediction. One possible reason is that we directly incorporate the training target data with the transformed source data in the testing process. This may disrupt the well-constructed correlation and similarity of the data distribution between source and target projects. In addition, the default parameters are still retained in the modified versions and may not be well-fitted with mixed project data. Among existing HDP approaches, CLSUP and MSMDA are the only two approaches with using mixed project data. They make full use of the label information of training target data in the process of model building, which makes the constructed prediction model with good discriminant ability. Therefore, their performance can obtain large improvement by using mixed project data. Through the above analysis, it is very important to effectively use mixed project data for training the prediction model, rather than use it simply and directly. To sum up, the answer to RQ2.2 can be concluded as follow.


Fig. 7.
Histogram of the median values on four indicators for HDP approaches with (&) and without using training target data.

Show All

Answer to RQ2.2. Utilizing mixed project data (i.e., combining training target data with source data to train the prediction model directly) cannot consistently improve the prediction performance of HDP approaches.

5.3 Results for RQ3
According to the answer to RQ1, CTKCCA, CLSUP and MSMDA rank the top three among existing HDP approaches in terms of RankScore. Thus, we select them to compare with CPDP approaches. Since these three HDP approaches are based on metric transformation, we additionally compare HDP-KS, which is the best metric selection-based HDP approach. This is done to ensure the two categories of HDP approaches are involved in this comparison.

The structure of Table 9 is similar to that of Table 8. It also reports the median results of each indicator, and the best value of each row is marked in bold. For an HDP approach, its evaluation results that outperform those of all CPDP approaches on a target dataset are marked by underline. From Table 9, we can observe that CTKCCA performs best on 14 out of 20 results in terms of four indicators. At the same time, it obtains lower values than those of other compared approaches on SOFTLAB. This is basically consistent with our observations of Table 8. According to the underlined values, CLSUP and MSMDA perform better than all CPDP approaches on 12 out of 20 results and 13 out of 20 results respectively, which demonstrates that these two HDP approaches can achieve better performance in more than half cases. This is probably because both of them utilize the mixed project data effectively to incorporate the label information of training target data into the training process. MSMDA also constructs multi-source training data to further improve the generalization of the prediction model. With respect to HDP-KS, it performs better on 5 out of 20 results and thus does not achieve the comparable performance with CPDP approaches. Although the source and target projects have the same metric set under a typical CPDP context, HDP-KS may still lose part of potentially useful features by using metric selection and matching. Thus, it cannot obtain the desirable prediction effect. Based on the results in ”All”, CTKCCA obtains the best overall performance on all indicators when compared with other approaches.

TABLE 9 Comparison Results on Five Datasets Under a Typical CPDP Context

Fig. 8 shows the details of indicator values for all target projects. We can observe that CTKCCA performs best overall while its prediction values on each indicator are always centralized in larger intervals, which is similar to our observation of Fig. 4. The root of instability lies in the default settings of CTKCCA’s parameters, which may be not suitable for all prediction combinations.


Fig. 8.
Boxplot of AUC, F-measure, G-measure and MCC values over all targets in typical CPDP context.

Show All

From Fig. 9, we can see that the first groups on AUC, F-measure, G-measure and MCC are separately {CTKCCA}, {CTKCCA}, {CTKCCA, CLSUP, MSMDA} and {CTKCCA}. It is noted that the first group on each indicator has no overlap with other groups. This indicates that the approaches in first group outperform those in other groups significantly. Based on the results in the last row of Table 9, we can observe that CTKCCA obtains the highest Rankscore of 1.000 followed by CLSUP with 0.804 and MSMDA with 0.748. This means that CTKCCA is always located in the first group on each indicator and not shared by other groups. HDP-KS also shows promising prediction ability and yields better Rankscore than NN-filter and VCB, whereas it still cannot achieve the same prediction level as SSTCA and LT. In summary, we can conclude the answer to RQ3 as follow.


Fig. 9.
Comparison of HDP and CPDP approaches against each other with Nemenyi’s post-hoc test on AUC, F-measure, G-measure and MCC. Groups of approaches that are not significantly different (at p=0.05) are connected.

Show All

Answer to RQ3. By comparison with CPDP approaches, the selected HDP approaches are feasible for cross-project defect prediction in which the source and target projects have the same metric set. Specifically, CTKCCA, CLSUP and MSMDA perform better than the compared CPDP approaches in terms of Rankscore, whereas HDP-KS still cannot achieve comparable performance with LT and SSTCA+ISDA.

SECTION 6Discussion
6.1 Effect of Percentage of Metric Selection
Metric selection-based HDP approaches always first filter source metrics to remove less informative ones that are considered to be redundant, and then match the selected source metrics with target metrics one-to-one. Specifically, FMT employs the metric subset generation technique [75] without the need to consider how many metrics should be selected from the source project. However, HDP-KS applies the strategy of metric ranking that requires to set an appropriate percentage of metric selection, which may cause varying degrees of metric loss. The evaluation results of HDP-KS under a typical HDP context have been reported in Section 5, and the default percentage of metric selection is set to 15 percent.

To investigate the impact of different selection ratios on its prediction performance, we assign different percentages of metric selection for HDP-KS and keep the same experimental setup with RQ1. Fig. 10 shows the median indicator values of HDP-KS under different selection percentages from 5 to 100 percent. The dashed line parallel to x-axis denotes the result under the percentage of 15 percent. From Fig. 10, we can observe that HDP-KS with the default setting of 15 percent obtains the best performance only on G-measure. Besides, it is noted that the deviation between maximum and minimum values for each indicator is within 0.03, which indicates that the fluctuation of prediction performance is slight as the percentage changes. Therefore, the changing of metric selection percentage has few impacts on the comparisons and conclusions in Section 5. According to Table 8, those metric transformation-based HDP approaches (i.e., CTKCCA, EMKCA, MSMDA, and CLSUP) still perform better than HDP-KS when its percentage of metric selection changes from 5 to 100 percent.


Fig. 10.
The prediction results of HDP-KS with different percentages of metric selection.

Show All

Based on the above observations, we can know that when the percentage of metric selection is small HDP-KS actually uses a minority of source and target metrics to build the prediction model and can obtain considerable performance. This effectively avoids over-fitting of the prediction model and makes it have better generalization ability. However, it is not conducive to achieving the desirable prediction effect since HDP-KS filters out most of the metrics both in source and target projects.

6.2 Effect of Metric Transformation
According to the comparison in Section 5.1, we find that CTKCCA performs best among existing HDP approaches in terms of Rankscore. To further explain its transformation ability, we graphically visualize the feature distribution of training data. Specifically, we take CM1 from NASA as the source project to predict ar1 from SOFTLAB and PC1 from NASA (denoted as CM1⇒ar1 and CM1⇒PC1) respectively by using CTKCCA. We use the principal component analysis (PCA) [76] to extract two principle PCA features for showing their distribution in a two-dimensional space.

Fig. 11a shows the feature distribution of training data on original CM1, where the first two principle PCA features (corresponding to x-axis and y-axis) of each module are used to illustrate this distribution. We can see that there is no clear class boundary between defective and non-defective modules, which indicates that they are linear inseparable. For CM1⇒ar1, after being transformed by CTKCCA, the defective modules gather together and can be clearly separated from the non-defective ones as shown in Fig. 11b. The similar observation can be obtained in Fig. 11c for CM1⇒PC1. This is mainly because CTKCCA employs the kernel learning technique to map the linear inseparable feature data into high-dimensional nonlinear space in which they can be linearly separable. Besides, the kernel canonical correlation analysis used in CTKCCA can only maximize the correlation between source and target data without the capability of reducing the intra-class distance, which may lead to the scattered non-defective modules in Figs. 11b and 11c.


Fig. 11.
Feature distribution of training data on the source CM1, where the first two principle PCA features of each projected features are used to illustrate these distributions.

Show All

In summary, CTKCCA employs the kernel learning technique to make linear inseparable training data become linearly separable in high-dimensional nonlinear space. This is conducive for CTKCCA to obtain the desirable prediction performance whether in typical HDP or CPDP context.

6.3 New Findings
Based on the empirical study, we summarize the new findings that have not appeared in previous studies as follows.

Progress of Existing HDP Approaches. To evaluate HDP approaches, the traditional strategy always selects WPDP (training by part of target data and testing by the rest of target data), CPDP (using the common metrics between source and target projects) and a couple of HDP approaches as the baselines. This is mainly because most of HDP approaches are proposed in the past five years and do not usually provide the source code for replication. Until now, there is still no study on comparing existing HDP approaches. To fill this gap, we collect the HDP approaches that have been proposed up to June 2018 and compare them under a common experimental setting for the first time. Based on the conclusion of RQ1, we have the new finding that metric transformation-based HDP approaches usually have better prediction performance due to their capability of transforming data across projects, whereas the transformed metrics do not have practical meanings. Besides, metric selection-based HDP approaches have better interpretability because they keep the original metrics and build models with the small subset of selected and matched metrics between source and target projects [40]. Overall, CTKCCA performs best among existing HDP approaches in terms of Rankscore, followed by CLSUP and MSMDA.

Feasibility of HDP Approaches in CPDP. Although HDP approaches are designed for the projects with different metrics, they actually have few limitations on the metrics of source and target projects. Therefore, HDP approaches can work in cross-project defect prediction where the source and target projects have the same metric set. Inspired by this observation, we compare HDP approaches with CPDP approaches under a typical CPDP context for the first time in order to verify their feasibility in cross-project defect prediction. According to the conclusion of RQ3, we have the new finding that the selected HDP approaches can obtain comparable performance with a range of CPDP approaches under a typical CPDP context. This also confirms the feasibility of HDP approaches in cross-project defect prediction where the source and target projects have the same metric set. Specifically, the top three approaches on Rankscore are still CTKCCA, CLSUP and MSMDA. In addition, we take CTKCCA as an example and discuss the effect of metric transformation in Section 6.2. We find that its considerable performance under a typical CPDP context is not by chance. This is because CTKCCA still has a good capability of metric transformation in the prediction combination of CPDP.

Effects of Improvements on HDP Approaches’ Performance. Different from the above two findings, the effects of handling class imbalance problems and using mixed project data on HDP approaches’ performance have been discussed by several previous studies [43], [44], [46], [48]. In this empirical study, we investigate the effects of improvements by using different indicators and more HDP approaches for providing a comprehensive evaluation. Through the experiment for RQ2.1, we find that the cost-sensitive learning and ensemble learning techniques can improve HDP approaches’ prediction effects but the performance boosts are usually limited, which is consistent with the observations of previous studies[43], [44], [46]. Besides, according to the conclusion of RQ2.2, we find that using mixed project data (i.e., incorporating training target data into training data directly) cannot improve HDP approaches’ performance consistently, which is somewhat inconsistent with the finding of Li et al. [48]. This is mainly because the compared approaches and datasets used in our study are different from those in the study of Li et al. However, CCA+ and HDP-KS compared in both our and Li et al.’s studies obtain consistent comparison results.

6.4 Practical Guidelines
Previous researches have proven that the prediction model can work well as long as sufficient defect data are available for training [19], [20]. Therefore, WPDP approach is always the first choice for practitioners to perform defect prediction when there is sufficient historical data of target projects. However, in reality, it is difficult for start-up companies and new software projects to produce adequate defect data that has been well preprocessed and labeled. In this scenario, practitioners require to consider introducing external data into the training process. Provided that the metrics of external (source) and target projects are the same, one can conduct defect prediction by employing the recently proposed CPDP approaches [19], [21], [22], [23], [25], [27]. If the source and target projects have different metrics, HDP approaches can be used to conduct defect prediction in this case. We provide the guidelines for using HDP approaches based on two application scenarios as follows.

All of the Modules in the Target Project are Unlabeled. When the target project has no labeled modules, practitioners can consider choosing CTKCCA that performs best currently to build the prediction model. However, it is worthy noted that CTKCCA shows poor performance on the target projects containing a small number of modules with lower defective rates according to its prediction results on SOFTLAB. In this situation, we recommend using HDP-KS that is based on metric selection to make predictions. This is because HDP-KS is less sensitive to the parameter (i.e., the percentage of metric selection), and moreover it builds the prediction model by using a small number of metrics from source and target projects. More specifically, it is adequate for HDP-KS to use 50 to 200 modules transferring the defect knowledge based on the study of Nam et al.[40]. Therefore, HDP-KS has a better generalization ability so that it can be well adaptable for different sizes of projects.

A Small Amount of Modules in the Target Project are Labeled. In the early phases of software development, there usually exists a limited amount of historical data in projects. Owing to a small number of labeled modules in the target project, practitioners can choose CLSUP, which uses the mixed project data, to build the prediction model for the target project. Since a small number of labeled modules tend to better reflect the global distribution in the small-scale target data, CLSUP also has good applicability for the target projects with fewer modules. Besides, CLSUP considers the intra-class and inter-class relations simultaneously for preserving the local structure of each project and improving the discrimination capability, while introducing the misclassification costs to relieve the influence of class imbalance problem. Compared with MSMDA, CLSUP performs more efficiently because of no need for choosing the appropriate multiple source projects.

6.5 Lessons Learned
Based on this empirical study, we have learned several following lessons for the future development and evaluation of HDP approaches.

Combining the Two Categories of HDP Approaches. Metric transformation-based HDP approaches can make the source and target projects have the maximum correlation, similar data distributions, and better separability. However, metric transformation-based HDP approaches usually generate transformed metrics based on all of original metrics in which the redundant ones may impact prediction performance. Regarding metric selection-based HDP approaches, they first remove the less informative and redundant metrics in the source project and then match the remaining metrics with those in the target project. This strategy eliminates the useless metric information of the source and target projects effectively. Therefore, we believe that combining these two categories of HDP approaches can make full use of their complementarity and may become an available way to further improve the prediction effect.

Strategies of Parameter Adjustment are Needed. Among existing HDP approaches, almost all of them require to empirically determine their parameters, few of which can be set in a self-adaptive way (e.g., the misclassification costs). For metric transformation-based HDP approaches, their parameter settings (e.g., the parameter of Gaussian kernel) affect prediction performance to some extents. Besides, the default parameters usually cannot be suitable for all cases. For example, CTKCCA performs worse on SOFTLAB of which the project has fewer modules. Therefore, in addition to giving the global optimal parameter values, we suggest researchers further investigate the parameter setting in the cases that the prediction model shows poor performance. In this way, the strategies of parameter adjustments should be considered and provided in studies, which will promote the practical application of HDP approaches.

Evaluation by Comparing With State-of-the-Art Approaches. Through extensive comparisons, we find that the newly proposed approach does not necessarily perform better than the older approaches. Specifically, according to Table 8, CCA+ proposed in 2015 performs better than FMT proposed in 2017 on four indicators. Besides, CTKCCA proposed in 2017 obtains the higher Rankscore than MSMDA and CLSUP, both of which are proposed in 2018. The above observations indicate that there is a lack of comparison with state-of-the-art approaches in current HDP studies. In order to present persuasive evidence, we suggest researchers try to replicate state-of-the-art approaches and compare with them. If there are some difficulties in comparing with all the state-of-the-art, we suggest taking CTKCCA and HDP-KS as part of baselines at least to evaluate your approach, both of which have the best overall performance in two categories of HDP approaches respectively. When mixed project data are used in your study, researchers should consider comparing it with CLSUP and MSMDA in experiments.

SECTION 7Threats to Validity
We analyze potential threats to the validity of our empirical study from four aspects, which are introduced as follows.

7.1 Construct Validity
The first threat is our method of splitting target project data. We randomly select 10 percent of modules from the target project as training target data and the remaining 90 percent as test target data following the setting of references [46], [48]. To eliminate the bias introduced by random splitting, we also repeat 20 times for each predicting combination. However, the varying ratios of splitting will lead to different prediction results. Moreover, since HDP approaches aim to conquer the obstacle of different metric sets, we conduct the empirical study based on all metrics contained in source and target projects, regardless of metric types. The impact of different types of metrics on HDP will be left for future research. Another threat is relevant with the used evaluation indicators, we select four frequently-used comprehensive measures in this paper, including AUC, F-measure, G-measure and MCC, to reflect the real performance of each approach as much as possible. Although other indicators, such as GM [77] and balance (which balances pd and pf) [9] may lead to different evaluation results, the impact of this threat is limited.

7.2 Internal Validity
In order to provide a comprehensive empirical study on HDP approaches, we collect the HDP-related papers by searching literature from 2015 to 2018 and tracking citations, which ensures that almost all of HDP researches by June 2018 have been included in our study and minimizes the threat from literature collection. However, we cannot exclude the possibility that fresh HDP studies may be presented during our finishing this paper. Another threat is from the implementations of these HDP and CPDP approaches compared in this study. To replicate the real performance of these SDP approaches, we carefully implement and check them so that the implementations correspond to the descriptions in the publications. However, we still have to adjust the parameters for some of the approaches in order to make them can be tested on the datasets used in this paper. Therefore, our implementations are not totally the same as those in original papers, which may lead to a possible bias in performance. For example, we have adjusted FMT so that it can conduct predictions on all target projects without the limitation of sampling size. Besides, we employ the LR classifier as the predictor for each approach, which may cause bias in prediction results. Although the LR classifier has been proven an effective predictor in SDP which always generates the best prediction results, it might not be suitable for all HDP approaches.

7.3 External Validity
An extensive selection of datasets and metrics can better lead to a general conclusion [78]. In this study, we conduct experiments on 30 projects from five datasets, which means that our experimental results may not be generalized to other data contexts. However, the five datasets have been broadly used in previous studies [28], [40], [48], and the projects are developed in both proprietary and open-source paradigms. This alleviates potential bias to some extent. In future work, we plan to reduce this threat by conducting further experiments on more defect data from open-source and commercial software projects.

7.4 Conclusion Validity
In this paper, the conclusions are derived mainly based on the defined Rankscore that is calculated based on the results of statistical significance test. We select the non-parametric Friedman test [63] with the Nemenyi’s post-hoc test at a confidence level of 95 percent as proposed by Demšar [64] to assess the statistical significance of differences between multiple models’ performance. This test is usually applied to evaluate whether multiple groups have significant differences and does not need to make an assumption on the data distribution. Therefore, the impact of this threat is limited. In addition, we define the Rankscore based on the normalized group ranking so that the HDP approach with better performance can obtain higher Rankscore. This is not the unique method of calculation, and different definitions may result in the bias of conclusions. Therefore, we not only report the Rankscore but also present the results of AUC, F-measure, G-measure, and MCC to alleviate the impact of potential bias in this study.

SECTION 8Conclusion
In this paper, we conduct an empirical study on HDP approaches in order to understand their current status of progress. We totally collect nine HDP studies published up to June 2018 by searching literature from 2015 to 2018 and tracking citations (2014), and then perform analyses on them. According to the method of processing metrics, we first classify HDP approaches into two categories (i.e., metric selection and metric transformation). Afterward, we summarize the improvements of HDP approaches from three aspects (i.e., class imbalance, mixed project data, and multi-source). Finally, we illustrate the relationship between HDP and CPDP from the views of the scope of SDP problems and the design of SDP approaches. Through extensive experiments, CTKCCA is determined as the HDP approach with the best overall performance, followed by CLSUP and MSMDA. In general, metric transformation-based HDP approaches usually have better prediction effects while metric selection-based approaches have better interpretability. As for the improvements of HDP approaches, we find that handling class imbalance problems can improve the overall performance but the boost is usually limited. Besides, using mixed project data cannot promote the prediction effect consistently. Based on the comparison under a typical CPDP context, we also find that the selected HDP approaches show comparable prediction performance with CPDP approaches, which verifies their feasibility in cross-project defect prediction. In order to facilitate further study, we share the replication package of the empirical study in our online appendix, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TSE.2020.2968520,6.

In the future, we plan to generalize this study to more extensive contexts of datasets and indicators. Additional HDP approaches proposed after June 2018 will be collected and included in the comparison. Besides, since the metric types of source and target are the basement of prediction and are usually different in HDP, summarizing the common metric types and investigating their impacts on prediction performance are valuable and complicated. This will be also considered as a part of our future work.