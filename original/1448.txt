The test case generation is intrinsically a multi-objective problem, since the goal is covering multiple test targets (e.g., branches). Existing search-based approaches either consider one target at a time or aggregate all targets into a single fitness function (whole-suite approach). Multi and many-objective optimisation algorithms (MOAs) have never been applied to this problem, because existing algorithms do not scale to the number of coverage objectives that are typically found in real-world software. In addition, the final goal for MOAs is to find alternative trade-off solutions in the objective space, while in test generation the interesting solutions are only those test cases covering one or more uncovered targets. In this paper, we present Dynamic Many-Objective Sorting Algorithm (DynaMOSA), a novel many-objective solver specifically designed to address the test case generation problem in the context of coverage testing. DynaMOSA extends our previous many-objective technique Many-Objective Sorting Algorithm (MOSA) with dynamic selection of the coverage targets based on the control dependency hierarchy. Such extension makes the approach more effective and efficient in case of limited search budget. We carried out an empirical study on 346 Java classes using three coverage criteria (i.e., statement, branch, and strong mutation coverage) to assess the performance of DynaMOSA with respect to the whole-suite approach (WS), its archive-based variant (WSA) and MOSA. The results show that DynaMOSA outperforms WSA in 28 percent of the classes for branch coverage (+8 percent more coverage on average) and in 27 percent of the classes for mutation coverage (+11 percent more killed mutants on average). It outperforms WS in 51 percent of the classes for statement coverage, leading to +11 percent more coverage on average. Moreover, DynaMOSA outperforms its predecessor MOSA for all the three coverage criteria in 19 percent of the classes with +8 percent more code coverage on average.
SECTION 1Introduction
Automated structural test case generation aims at producing a set of test cases that maximises coverage of the test goals in the software under test according to the selected adequacy testing criterion (e.g., branch coverage). Search-based test case generators use meta-heuristic optimisation algorithms, such as genetic algorithms (GA), to produce new test cases from the previously generated (initially, random) ones, so as to reduce and eventually nullify their distance from each of the yet uncovered targets [34] , [48]. The search for a test sequence and for test input data that bring the test execution closer to the current coverage target is guided by a fitness function, which quantifies the distance between the execution trace of a test case and the coverage target. The actual computation of such a distance depends on the type of coverage criterion under consideration. For instance, for branch coverage, the distance is based on the number of control dependencies that separate the execution trace from the target (approach level ) and on the variable values evaluated at the conditional expression where the execution diverges from the target (branch distance).

In traditional evolutionary testing, to cover all targets a meta-heuristic algorithm is applied multiple times, each time with a different target, until all targets are covered or the total search budget is consumed. The final test suite is thus composed of all the test cases that cover one or more targets, including those that accidentally cover previously uncovered targets (collateral coverage).

Searching for one target at a time is problematic in several respects. First of all, targets may require higher or lower search effort depending on how difficult it is to produce test cases that cover them. Hence, uniformly distributing the search budget across all targets might be largely sub-optimal. Even worse, the presence of unfeasible targets wastes entirely the search time devoted to their coverage. The whole-suite (WS) approach [3], [16] to test case generation is a recent attempt to address these issues. WS optimises entire test suites, not just individual test cases, and makes use of a single fitness value that aggregates the fitness values measured for the test cases contained in a test suite, so as to take into consideration all coverage targets simultaneously. In fact, each test case in a test suite is associated with the target closest to its execution trace. The sum over all test cases of such minimum distances from the targets provides the overall, test-suite-level fitness. The additive combination of multiple targets into a single, scalar objective function is known as sum scalarization in the theory of optimisation [10]. The aim is to apply single-objective algorithms, like GA, to an intrinsically multi-objective problem.

While being more effective than targeting one target at a time, the WS approach suffers all the well-known problems of sum scalarization in many-objective optimisation, among which the inefficient convergence occurring in the non-convex regions of the search space [10]. On the other hand, there are single-objective problem instances on which many-objective algorithms have been shown to be much more convenient than the single-objective approach. In fact, reformulating a complex single-objective problem as a many-objective problem defined on multiple but simpler objectives can reduce the probability that the search remains trapped in local optima, eventually leading to faster convergence [20], [27]. However, there are two main challenges to address when applying many-objective optimisation to test case generation: (i) none of the available multi or many-objective solvers can scale to the number of objectives (targets) typically found in coverage testing  [3]; and (ii) multi and many-objective solvers are designed to increase the diversity of the solutions, not just to fully achieve each objective individually (reducing to zero its distance from the target), as required in test case generation [30].

To overcome the aforementioned limitations, in our previous work [38] we introduced Many-Objective Sorting Algorithm (MOSA), a many-objective GA tailored to the test case generation problem. MOSA has three main features: (i) it uses a novel preference criterion instead of ranking the solutions based on their Pareto optimality; (ii) it focuses the search only on the yet uncovered coverage targets; (iii) it saves all test cases satisfying one or more previously uncovered targets into an archive, which contains the final test suite when the search ends.

Recently, Rojas et al. [44] developed the Whole Suite with Archive approach (WSA), a hybrid strategy that incorporates some of MOSA’s routines inside the traditional WS approach. While WSA still applies the sum scalarization and works at the test suite level, it incorporates an archive strategy which operates at the test case level. Moreover, it also focuses the search only on the yet to cover targets. From an empirical point of view, WSA has been proved to be statistically superior to WS and to the one-target at a time approaches. However, from a theoretical point of view, it does not evolve test suites anymore since the final test suite given to developers is artificially synthesised by taking those test cases stored in the archive rather than returning the best individual (i.e., test suite) from the last generation of GAs  [44]. Rojas et al. [44] arose the following still unanswered questions: (1) To what extent do the benefits of MOSA stem from the many-objective reformulation of the problem or from the use of the archiving mechanism? (2) How does MOSA performs compared to WSA since both of them implement an archiving strategy?

In this paper, we provide an in-depth analysis of many-objective approaches to test case generation, thus, answering the aforementioned open questions. First, we present Many-Objective Sorting Algorithm with Dynamic target selection (DynaMOSA) which extends MOSA with the capability to dynamically focus the search on a subset of the yet uncovered targets, based on the control dependency hierarchy. Thus, uncovered targets that are reachable from other uncovered targets positioned higher in the hierarchy are removed from the current vector of objectives. They are reinserted in the objective vector dynamically, when their parents are covered. Since DynaMOSA optimises a subset of the objectives considered by MOSA using the same many-objective algorithm and preference criterion, DynaMOSA is ensured to be equally or more efficient than MOSA.

Second, we conduct a large empirical study involving 346 Java classes sampled from four different datasets  [17], [38], [45], [49] and considering three well known test adequacy criteria: statement coverage, branch coverage and mutation coverage. We find that DynaMOSA achieves significantly higher coverage than WSA in a large number of classes (27 and 28 percent for branch and mutation coverage respectively) with an average increment, for classes where statistically significant differences were observed, of +8 percent for branch coverage and +11 percent for mutation coverage. It also produces higher statement coverage than WS1 in 51 percent of the classes, for which we observe +11 percent of covered statements on average. As predicted by the theory, DynaMOSA also improves MOSA for all the three criteria. This happens in 19 percent of the classes with an average coverage improvement of 8 percent.

This paper extends our previous work [38] with the following novel contributions:

DynaMOSA: a novel algorithm for dynamic target selection, which is theoretically proven as subsuming MOSA.

Larger scale experiments: one order of magnitude more classes are considered in the new experiments.

Empirical comparison of DynaMOSA with MOSA and WSA to provide support to the usage of many-objective solvers in addition to the archiving strategy, which is implemented in all the three approaches.

Empirical assessment of the selected algorithms with respect to different testing criteria, i.e., branch coverage, statement coverage and strong mutation coverage.

Direct comparison with Pareto dominance ranking based algorithms, i.e., NSGA-II enriched with an archiving strategy.

Empirical assessment of our novel preference criterion, to understand if the higher effectiveness of DynaMOSA is due to the preference criterion alone or to its combination with dominance ranking.

The remainder of this paper is organised as follows. Section 2 presents the reformulation of the test case generation problem as a many-objective problem. Section 3 presents the many-objective test generator MOSA, followed by its extension, DynaMOSA, with dynamic selection of the coverage targets. Section 4 presents the description of the experiments we conducted for the evaluation of the proposed algorithm. Section 5 reports the results obtained from the experiments, while Sections 6 and 7 provide additional empirical analyses and discuss the threats to validity, respectively. Section 8 summarises the research works most closely related to ours. Section 9 draws conclusions and identifies possible directions for future work.

SECTION 2Problem Formulation
This section presents our many-objective reformulation of the structural coverage problem. First the single-objective formulation used in the whole-suite approach  [16] is described, followed by our many-objective reformulation, highlighting its main differences and peculiarities.

2.1 Single Objective Formulation
In the whole-suite approach, a candidate solution is a test suite consisting of a variable number of individual test cases, where each test case is a sequence of method calls of variable length. The fitness function measures how close the candidate test suite is to satisfy all coverage targets (aka, test goals). It is computed with respect to full coverage (e.g., full branch coverage), as the sum of the individual distances to all coverage targets (e.g., branches) in the program under test. More formally, the problem of finding a test suite that satisfies all test targets has been defined as follows:

Problem 2.1.
Let U={u1,…,uk} be the set of structural test targets of the program under test. Find a test suite T={t1,…,tn} that minimises the fitness function:
minfU(T)=∑u∈Ud(u,T),(1)
View Sourcewhere d(u,T) denotes the minimal distance for the target u according to a specific distance function.

In general, the function to minimise d is such that d(u,T)=0 if and only if the target goal is covered when a test suite T is executed. The difference between the various coverage criteria affects the specific distance function d used to express how distant the execution traces are from covering the test targets in U when all test cases in T are executed.

In Branch Coverage, the test targets to cover are the conditional statement branches in the class under test. Therefore, Problem 2.1 can be instantiated as finding a test suite that covers all branches, using as function d the traditional branch distance [37] for each individual branch to be covered. More formally [17]:

Problem 2.2.
Let B={b1,…,bk} be the set of branches in a class. Find a test suite T={t1,…,tn} that covers all the feasible branches, i.e., one that minimises the following fitness function:
minfB(T)=|M|−|MT|+∑b∈Bd(b,T),(2)
View Sourcewhere |M| is the total number of methods, |MT| is the number of executed methods by all test cases in T and d(b,T) denotes the minimal normalised branch distance for branch b∈B.

The term |M|−|MT| accounts for the entry edges of the methods that are not executed by T. The minimal normalised branch distance d(b,t) for each branch b∈B is defined as  [16]
d(b,t)=⎧⎩⎨⎪⎪⎪⎪0Dmin(t∈T,b)Dmin(t∈T,b)+11ifbhas been coveredif the predicate has beenexecuted at least twiceotherwise,(3)
View Sourcewhere Dmin(t∈T,b) is the minimal non-normalised branch distance, computed according to any of the available branch distance computation schemes  [34]; minimality here refers to the possibility that the predicate controlling a branch is executed multiple times within a test case or by different test cases. The minimum is taken across all such executions.

In Statement Coverage, the optimal test suite is the one that executes all statements in the code. To reach a given statement s, it is sufficient to execute a branch on which such a statement is directly control dependent. Thus, the distance function d for a statement s can be measured using the branch distances for the branches to be executed in order to reach s . More specifically, the problem is [16]:

Problem 2.3.
Let S={s1,…,sk} be the set of statements in a class. Find a test suite T={t1,…,tn} that covers all the feasible statements, i.e., one that minimises the following fitness function:
minfS(T)=|M|−|MT|+∑b∈BSd(b,T),(4)
View SourceRight-click on figure for MathML and additional features.where |M| is the total number of methods, |MT| is the number of executed methods by all test cases in T; BS is the set of branches that hold a direct control dependency on the statements in S; and d(b,T) denotes the minimal normalised branch distance for branch b∈BS.

In Strong Mutation Coverage, the test targets to cover are mutants, i.e., variants of the original class obtained by injecting artificial modifications (mimicking real faults) into them. Therefore, the optimal test suite is the one which is able to cover (kill, in the mutation analysis jargon) all mutants. A test case strongly kills a mutant if and only if the observable object state or the method return values differ between original and mutated class under test. Such a condition is usually checked by means of automatically generated assertions, which assert the equality of method return values and object state with respect to those observed when the original class is tested. Hence, such assertions fail when evaluated on the mutant if a different return value or object state is produced. An effective fitness function f for strong mutation coverage can be defined based on the notions of infection and propagation. The execution state of a mutated class instance is regarded as infected if it differs from the execution state of the original class when compared immediately after the mutated statement. This ensures that the mutant is producing some immediate effect on the class attributes or method variables, but it does not ensure that such an effect will propagate to an externally observable difference. Infection propagation accounts for the possibility of observing a different state between mutant and original class in the statements that follow the mutated one along the execution trace. The corresponding formal definition is the following  [18]:

Problem 2.4.
Let M={m1,…,mk} be the set of mutants for a class. Find a test suite T={t1,…,tn} that kills all mutants, i.e., one that minimises the following fitness function:
minfM(T)=fBM(T)+∑m∈M(di(m,T)+dp(m,T)),(5)
View Sourcewhere fBM(T) is the whole-suite fitness function for all branches in T holding a direct control dependency on a mutant in M; di estimates the distance toward a state infection; and dp denotes the propagation distance.

From an optimisation point of view, in the whole-suite approach the fitness function f considers all the targets at the same time and aggregates all corresponding distances into a unique fitness function by summing up the contributions from the individual target distances, i.e., the minimum distance from each individual target. In other words, multiple search targets are conflated into a single search target by means of an aggregated fitness function. Using this kind of approach, often named scalarization [10] , a problem which involves multiple targets is transformed into a traditional single-objective, scalar one, thus allowing for the application of single-objective meta-heuristics such as standard GA.

2.2 Many-Objective Formulation
In this paper, we reformulate the coverage criteria for test case generation as many-objective optimisation problems, where the objectives to be optimised are the individual distances from all the test targets in the class under test. More formally, in this paper we consider the following reformulation:

Problem 2.5.
Let U={u1,…,uk} be the set of test targets to cover. Find a set of non-dominated test cases T={t1,…,tn} that minimise the fitness functions for all test targets u1,…,uk, i.e., minimising the following k objectives:
⎧⎩⎨⎪⎪⎪⎪minf1(t)=d(u1,t)⋮minfk(t)=d(uk,t),(6)
View Sourcewhere each d(ui,t) denotes the distance of test case t from covering the test target ui. Vector ⟨f1,…,fk⟩ is also named a fitness vector.

According to this new generic reformulation, branch coverage, statement coverage and strong mutation coverage can be reformulated as follows:

Problem 2.6.
Let B={b1,…,bk} be the set of branches of a class. Find a set of non-dominated test cases T={t1,…,tn} that minimises the fitness functions for all branches b1,…,bk, i.e., minimising the following k objectives:
⎧⎩⎨⎪⎪⎪⎪minf1(t)=al(b1,t)+d(b1,t)⋮minfk(t)=al(bk,t)+d(bk,t),(7)
View Sourcewhere each d(bj,t) denotes the normalised branch distance for the branch, executed by t, which is closest to bj (i.e., which is at minimum number of control dependencies from bj), while al(bj,t) is the corresponding approach level (i.e., the number of control dependencies between the closest executed branch and bj).

Problem 2.7.
Let S={s1,…,sk} be the set of statements in a class. Find a set of non-dominated test cases T={t1,…,tn} that minimises the fitness functions for all statements s1,…,sk, i.e., minimising the following k objectives:
⎧⎩⎨⎪⎪⎪⎪minf1(t)=al(s1,t)+d(b(s1),t)⋮minfk(t)=al(sk,t)+d(b(sk),t),(8)
View Sourcewhere each d(b(sj),t) denotes the normalised branch distance of test case t for the branch closest to b(sj), the branch that directly controls the execution of statement sj, while al(sj,t) is the corresponding approach level.

Problem 2.8.
Let M={m1,…,mk} be the set of mutants for a class. Find a set of non-dominated test cases T={t1,…,tn} that minimises the fitness functions for all mutants m1,…,mk, i.e., minimising the following k objectives:
⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪minf1(t)=al(m1,t)+d(b(m1),t)+di(m1,t)+dp(m1,t)⋮minfk(t)=al(mk,t)+d(b(mk),t)+di(mk,t)+dp(mk,t),(9)
View Sourcewhere d(b(mj),t) and al(mj,t) denote the normalised branch distance and the approach level of test case t for mutant mj respectively; di(mj,t) is the distance from state infection, while dp(mj,t) measures the propagation distance.

In this formulation, a candidate solution is a test case, not a test suite, which is scored by a single objective vector containing the distances from all yet uncovered test targets. Hence, the fitness is a vector of k values, instead of a single aggregate score. In many-objective optimisation, candidate solutions are evaluated in terms of Pareto dominance and Pareto optimality [11]:

Definition 1.
A test case x dominates another test case y (also written x≺y) if and only if the values of the objective function vector satisfy the following conditions:
∀i∈{1,…,k}fi(x)≤fi(y)and∃j∈{1,…,k}such thatfj(x)<fj(y).
View SourceRight-click on figure for MathML and additional features.

Conceptually, the definition above indicates that x is preferred to (dominates) y if and only if x is strictly better on one or more objectives and it is not worse for the remaining objectives. For example, in branch coverage x is preferred to (dominates) y if and only if x has a lower branch distance + approach level for one or more branches and it is not worse for the remaining branches.

Fig. 1 provides a graphical interpretation of Pareto dominance for a simple case with two test targets to cover, i.e., the problem is bi-objective. All test cases in the grey rectangle ( A and C) are dominated by D, because D is better for both the two objectives f1 and f2. On the other hand, the test case D does not dominate B because it is closer to cover f1 than B, but it is worse than B on the other test target, f2. Moreover, D does not dominate E because it is worse for the test target f1. Similarly, the test case B does not dominate D and E but it dominates A. Thus, B, D, and E are non-dominated by any other test case, while A and C are dominated by either D or B. The test case B satisfies (covers) the target f2, thus, it is a candidate to be included in the final test suite.


Fig. 1.
Graphical interpretation of Pareto dominance.

Show All

Among all possible test cases, the optimal test cases are those non-dominated by any other possible test case:

Definition 2.
A test case x∗ is Pareto optimal if and only if it is not dominated by any other test case in the space of all possible test cases (feasible region).

Single-objective optimisation problems have typically one solution (or multiple solutions with the same optimal fitness value). On the other hand, solving a multi-objective problem may lead to a set of Pareto-optimal test cases (with different fitness vectors), which, when evaluated, correspond to trade-offs in the objective space. While in many-objective optimisation it may be useful to consider all the trade-offs in the objective space, especially if the number of objectives is small, in the context of coverage testing we are interested in finding only the test cases that contribute to maximising the total coverage by covering previously uncovered test targets, i.e., test cases having one or more objective scores equal to zero: fi(t)=0, as test B in Fig. 1. These are the test cases that intersect any of the m Cartesian axes of the vector space where fitness vectors are defined. Such test cases are candidates for inclusion in the final test suite and represent a specific sub-set of the Pareto optimal solutions.

SECTION 3Algorithm
Search algorithms aimed at optimising more than three objectives have been widely investigated for classical numerical optimisation problems. A comprehensive survey of such algorithms can be found in a recent paper by Li et al. [31]. The following sections provide a discussion of the most relevant related works on many-objective optimisation and highlight how our novel many-objective algorithm overcomes their limitations in the context of many-objective structural coverage. In particular, Section 3.1 provides an overview on traditional many-objective algorithms, while Section 3.2 presents the many-objective algorithms that we have developed for solving the many-objective reformulation of structural coverage criteria.

3.1 Existing Many-Objective Algorithms
Multi-objective algorithms, such as the Non-dominated Sorting Genetic Algorithm II (NSGA-II)  [12] and the improved Strength Pareto Evolutionary Algorithm (SPEA2)  [59], have been successfully applied within the software engineering community to solve problems with two or three objectives, including software refactoring, test case prioritisation, etc. However, such classical multi-objective evolutionary algorithms present scalability issues because they are not effective in solving optimisation problems with more than three-objectives [31] .

To overcome their limitations, a number of many-objective algorithms have been recently proposed in the evolutionary computation community, which modify multi-objective solvers to increase the selection pressure. According to Li et al. [34], existing many-objective solvers can be categorised into seven classes: relaxed dominance based, diversity-based, aggregation-based, indicator-based, reference set based, preference-based, and dimensionality reduction approaches. For example, Laumanns e t al. [30] proposed the usage of ϵ-dominance (ϵ-MOEA), which is a relaxed version of the classical dominance relation that enlarges the region of the search space dominated by each solution, so as to increase the likelihood for some solutions to be dominated by others  [31]. Although this approach is helpful in obtaining a good approximation of an exponentially large Pareto front in polynomial time, it presents drawbacks and in some cases it can slow down the optimisation process significantly [24].

Yang et al. [54] introduced a Grid-based Evolutionary Algorithm (GrEA) that divides the search space into hyper-boxes of a given size and uses the concepts of grid dominance and grid distance to improve diversity among individuals by determining their mutual relationship in a grid environment. Zitzler and Künzli [58] proposed the usage of the hypervolume indicator instead of Pareto dominance when selecting the best solutions to form the next generation. Even if the new algorithm, named Indicator Based Evolutionary Algorithm (IBEA), was able to outperform NSGA-II and SPEA2, the computation cost associated with the exact calculation of the hypervolume indicator in a high-dimensional space (i.e., with more than five objectives) is too expensive, making it unfeasible with hundreds of objectives as in the case of structural test coverage (e.g., in mutation coverage a class/program may have hundreds of mutants).

Zhang and Li [57] proposed a Decomposition based Multi-objective Evolutionary Algorithm (MOEA/D), which decomposes a single many-objective problem into many single-objective sub-problems by employing a series of weighting vectors. Specifically, each sub-problem is obtained by using a specific weighting vector that aggregates different objectives using a weighted-sum approach. Different weighting vectors assign different importance (weights) to objectives, thus, delimiting the searching direction of these aggregation-based algorithms. However, the selection of weighting vectors is still an open problem especially for problems with a very large number of objectives [31].

Di Pierro et al. [13] used a preference order-approach (POGA) as an optimality criterion in the ranking phase of NSGA-II. This criterion considers the concept of efficiency of order in subsets of objectives and provides a higher selection pressure towards the Pareto front than Pareto dominance-based algorithms.

Deb and Jain [11] proposed NSGA-III, an improved version of the classical NSGA-II algorithm, where the crowding distance is replaced with a reference-set based niche-preservation strategy. It results in a new Pareto ranking/sorting based algorithm that produces well-spread and diversified solutions.

Yuan et al. [55] developed θ-NSGA-III, which enriches the non-dominated sorting scheme with the concepts of θ-dominance to rank solutions in the environmental selection phase, so as to ensure both convergence and diversity.

All the many-objective algorithms mentioned above have been investigated mostly for numerical optimisation problems with less than 15 objectives. Moreover, they are designed to produce a rich set of optimal trade-offs between different optimisation goals, by considering both proximity to the real Pareto optimal set and diversity between the obtained trade-offs [30]. As explained in Section 2.2, this is not the case of structural coverage for test case generation. The interesting solutions for this problem are test cases having one or more objective scores equal to zero (i.e., fi(t)=0). Trade-offs between objectives scores are useful just for maintaining diversity during the optimisation process. Hence, there are two main peculiarities that have to be considered in many-objective structural coverage as compared to more traditional many-objective problems. First, not all non-dominated test cases have a practical utility since they represent trade-offs between objectives. Instead, in structural coverage the search has to focus on a specific sub-set of the non-dominated solutions: those solutions (test cases) that satisfy one or more coverage targets. Second, for a given level of structural coverage, shorter test cases (i.e., test cases with a lower number of statements) are preferred to reduce the oracle cost [6], [16] and to avoid the bloat phenomenon [16].

3.2 The Proposed Many-Objective Algorithms
Previous research in many-objective optimisation [31], [51] has shown that many-objective problems are particularly challenging because of the dominance resistance phenomenon, i.e., most of the solutions are incomparable since the proportion of non-dominated solutions increases exponentially with the number of objectives. As a consequence, it is not possible to assign a preference among individuals for selection purposes and the search process becomes equivalent to a random one [51]. Thus, problem/domain specific knowledge is needed to impose an order of preference over test cases that are non-dominated according to the traditional non-dominance relation. For test case generation, this means focusing the search effort on those test cases that are closer to one or more uncovered targets (e.g., branches) of the program. To this aim, we propose the following preference criterion in order to impose an order of preference among non-dominated test cases:

Definition 3.
Given a coverage target ui, a test case x is preferred over another test case y (also written x≺uiy) if and only if the values of the objective function for ui satisfy the following condition:
fi(x)<fi(y)ORfi(x)=fi(y)∧size(x)<size(y),
View Source

where fi(x) denotes the objective score of test case x for coverage target ui (see Section 2), and size measures the test case length (number of statements). The best test case for a given coverage target ui is the one preferred over all the others for such target (xbest≺uiy, ∀y∈T). The set of best test cases across all uncovered targets ({x|∃i:x≺uiy, ∀y∈T}) defines a subset of the Pareto front that is given priority over the other non-dominated test cases in our algorithms. When there are multiple test cases with the same minimum fitness value for a given coverage target ui, we use the test case length (number of statements) as a secondary preference criterion. We opted for this secondary criterion since generated tests require human effort to check the candidate assertions (oracle problems). Therefore, minimising the test cases is a valuable (secondary) objective to achieve [6], [16] since smaller tests involve a lower number of method calls (and corresponding covered paths) to manually analyse.


Show All

Our preference criterion provides a way to distinguish between test cases in a set of non-dominated ones, i.e., in a set where test cases are incomparable according to the traditional non-dominance relation, and it increases the selection pressure by giving higher priority to the best test cases with respect to the currently uncovered targets. Since none of the existing many-objective algorithms considers this preference ordering, which is a peculiarity of test case generation, in this paper we define our novel many-objective GA by incorporating the proposed preference criterion in the main loop of NSGA-II, a widely known Pareto efficient multi-objective genetic algorithm designed by Deb et al. [12].

In a nutshell, NSGA-II starts with an initial set of random solutions (random test cases), also called chromosomes, which represents a random sample of the search space (line 3 of Algorithm 1). The population then evolves through a series of generations to find better test cases. To produce the next generation, NSGA-II first creates new test cases, called offsprings, by combining parts from two selected test cases (parents) in the current generation using the crossover operator and randomly modifying test cases using the mutation operator (function GENERATE-OFFSPRING, at line 5 of Algorithm 1). Parents are selected according to a selection operator, which uses Pareto optimality to give higher chance of reproduction to non-dominated (fittest) test cases in the current population. The crowding distance is used in order to make a decision about which test cases to select: non-dominated test cases that are far away from the rest of the population have higher probability of being selected (lines 10-15 of Algorithm 1). Furthermore, NSGA-II uses the FAST-NONDOMINATED-SORT algorithm to preserve the test cases forming the current Pareto frontier in the next generation (elitism). After some generations, NSGA-II converges to “stable” test cases, which represent the Pareto-optimal solutions to the problem.


Show All

The next sections describe in detail our proposed many-objective algorithms starting from NSGA-II, first describing the MOSA algorithm followed by its extension, DynaMOSA.

3.2.1 MOSA: A Many Objective Sorting Algorithm
Many Objective Sorting Algorithm replaces the traditional non-dominated sorting with a new ranking algorithm based on our preference criterion. As shown in Algorithm 2, MOSA shares the initial steps of NSGA-II: it starts with an initial set of randomly generated test cases that forms the initial population (line 3 of Algorithm 2); then, it creates new test cases using crossover and mutation (function GENERATE-OFFSPRING, at line 6 of Algorithm 2).

Selection. Differently from NSGA-II, in MOSA the selection is performed by considering both the non-dominance relation and the proposed preference criterion (function PREFERENCE-SORTING, at line 9 of Algorithm 2). In particular, the PREFERENCE-SORTING function, whose pseudo-code is provided in Algorithm 3, determines the test case with the lowest objective score (e.g., branch distance + approach level for branch coverage) for each uncovered target ui∈U, i.e., the test case that is closest to cover ui (lines 2-6 of Algorithm 3). All these test cases are assigned rank 0 (i.e., they are inserted into the first non-dominated front F0), so as to give them a higher chance of surviving in to the next generation (elitism). The remaining test cases (those not assigned to the first rank) are then ranked according to the traditional non-dominated sorting algorithm used by NSGA-II [12], starting with a rank equal to 1 and so on (lines 11-15 of Algorithm 3). To speed-up the ranking procedure, the traditional non-dominated sorting algorithm is applied only when the number of test cases in F0 is smaller than the population size M (condition in line 8). Instead, when the condition |F0|>M is satisfied, the PREFERENCE-SORTING function returns only two fronts: the first front (rank 0) with all test cases selected by our preference criterion; and the front with rank 1 that contains all remaining test cases in T, i.e., F1=T−F0.


Show All


Show All

Dominance. It is important to notice that the routine FAST-NONDOMINATED-SORT assigns the ranks to the remaining test cases by considering only the non-dominance relation for the uncovered targets, i.e., by focusing the search toward the interesting sub-region of the remaining search space. Such non-dominance relation is computed by a specific dominance comparator which iterates over all the uncovered targets when deciding whether two test cases t1 and t2 do not dominate each other or whether one (e.g., t1) dominates the other (e.g., t2). While the original dominance comparator defined by Deb et al. [12] iterates all over the objectives (targets), in MOSA we use the dominance comparator depicted in Algorithm 4. Such a comparator iterates only over the uncovered targets (loop condition in line 4 of Algorithm 4). Moreover, as soon as it finds two uncovered targets for which two test cases t1 and t2 do not dominate each other (lines 11-12 Algorithm 4), the iteration is terminated without analysing further uncovered targets.


Show All

Crowding Distance. Once a rank is assigned to all candidate test cases, the crowding distance is used to give higher probability of being selected to some test cases in the same front. As measure for the crowding distance, we use the sub-vector dominance assignment proposed by Köppen et al. [25] for many-objective optimisation problems. Specifically, the loop at line 12 in Algorithm 2 and the following lines 16 and 17 add as many test cases as possible to the next generation, according to their assigned ranks, until reaching the population size. The algorithm first selects the non-dominated test cases from the first front (F0 ); if the number of selected test cases is lower than the population size M , the loop selects more test cases from the second front (F1 ), and so on. The loop stops when adding test cases from current front Fd exceeds the population size M. At end of the loop (lines 16 and 17), when the number of selected test cases is lower than the population size M, the algorithm selects the remaining test cases from the current front Fd according to the descending order of crowding distance.

Archiving. As a further peculiarity with respect to other many-objective algorithms, MOSA uses a second population, called archive, to keep track of the best test cases that cover targets of the program under test. Specifically, whenever new test cases are generated (either at the beginning of search or when creating offsprings) MOSA stores those tests that satisfy previously uncovered targets in the archive as candidate test cases to form the final test suite (lines 4 and 7 of Algorithm 2). To this aim, function UPDATE-ARCHIVE (reported in Algorithm 5 for completeness) updates the set of test cases stored in the archive with the new generated test cases. This function considers both the covered targets and the length of test cases when updating the archive: for each covered target ui it stores the shortest test case covering ui in the archive.

In summary, generation by generation MOSA focuses the search towards the uncovered targets of the program under test (both PREFERENCE-SORTING and FAST-NONDOMINATED-SORT routines analyse the objective scores of the candidate test cases considering the uncovered targets only); it also stores the shortest covering test cases in an external data structure (i.e., the archive) to form the final test suite. Finally, since MOSA uses the crowding distance when selecting the test cases, it promotes diversity, which represents a key factor to avoid premature convergence toward sub-optimal regions of the search space  [25].

3.2.2 Graphical Interpretation
Let us consider the simple program shown in Fig. 2a. Let us assume that the coverage criterion is branch coverage and that the uncovered targets are the true branches of statements 1 and 3, whose branch predicates are (a == b) and (b == c) respectively. According to the proposed many-objective formulation, the corresponding problem has two residual optimisation objectives, which are f1=al(b1)+d(b1)=abs(a−b) and f2=al(b2)+d(b2)=abs(b−c). Hence, any test case produced at a given generation g corresponds to some point in a two-dimensional objective space as shown in Figs. 2b and 2c. Unless both a and b are equal, the objective function f1 computed using the combination of approach level and branch distance is greater than zero. Similarly, function f2 is greater than zero unless b and c are equal.


Fig. 2.
Graphical comparison between the non-dominated ranks assignment obtained by the traditional non-dominated sorting algorithm and the ranking algorithm based on the preference criterion proposed in this paper.

Show All

Let us consider the scenario reported in Fig. 2b where no test case is able to cover the two uncovered branches (i.e., in all cases f1>0 and f2>0). If we use the traditional non-dominance relation between test cases, all test cases corresponding to the black points in Fig. 2b are non-dominated and form the first non-dominated front F0. Therefore, all such test cases have the same probability of being selected to form the next generation, even if test case A is the closest to the Cartesian axis f2 (i.e., closest to cover branch b2) and test case B is the closest to the Cartesian axis f1 (branch b1). Since there is no preference among test cases in F0, it might happen that A and/or B are not kept for the next generation, while other, less useful test cases in F0 are preserved. This scenario is quite common in many-objective optimisation, where the number of non-dominated solutions increases exponentially with the number of objectives [51]. However, from the branch coverage point of view the two test cases A and B are better (fitter) than all other test cases, because they are the closest to covering each of the two uncovered branches.

Our novel preference criterion gives a higher priority to test cases A and B with respect to all other test cases, guaranteeing their survival in the next generation. In particular, using the new ranking algorithm proposed in this paper (see Algorithm 3), the first non-dominated front F0 will contain only test cases A and B (see Fig. 2c), while all other test cases will be assigned to other, successive fronts. When there are multiple test cases that are closest to one axis, associated with a given uncovered branch, the preference criterion provides a further strategy to impose an order to the non-dominated test cases by assigning rank 0 to the shortest test case only.

3.2.3 DynaMOSA: Dynamic Selection of the Optimisation Targets
One main limitation in MOSA is that it considers all coverage targets as independent objectives to optimise since the first generation. However, there exist structural dependencies among targets that should be considered when deciding which targets/objectives to optimise. For example, some targets can be satisfied if and only if other related targets are already covered.

To better explain this concept, let us consider the example in Fig. 3. The four branches to be covered, b1,b2,b3,b4, are not independent from each other. In fact, coverage of b2,b3 can be achieved only after b1 has been covered, since the former targets are under the control of the latter. In other words, there is a control dependency between b1 and b2,b3, which means that the execution of b2,b3 depends on the outcome of the condition at node 2, which in turn is evaluated only once target b1 is covered. If no test case covers b1, the ranking in MOSA is determined by the fitness function f1=d(b1). When tests are evaluated for the two dependent branches b2 and b3, the respective fitness functions will be equal to f1+1, since the only difference from coverage of b1 consists of a higher approach level (in this case, +1), while the branch distance d is the same. Since the values of f2 and f3 are just shifted by a constant amount (the approach level) with respect to f1, the test case ranking is the same as the one obtained when considering f1 alone. This means that taking into account objectives f2,f3 during preference sorting is useless, since they do not contribute to the final ranking.


Fig. 3.
Since b1 holds a control dependency on b2,b3, targets b2,b3 are dynamically selected only once b1 is covered.

Show All

The example illustrated above shows that coverage targets can be organised into a priority hierarchy based on the following concepts in standard flow analysis:

Definition 4 (Dominator).
A statement s1 dominates another statement s2 if every execution path to s2 passes through s1.

Definition 5 (Post-dominator).
A statement s1 post-dominates another statement s2 if every execution path from s2 to the exit point ( return statement) passes through s1.

Definition 6 (Control dependency).
There is a control dependency between program statement s1 and program statement s2 iff: (1) s2 is not a postdominator of s1 and (2) there exists a path in the control flow graph between s1 and s2 whose nodes are postdominated by s2.

Condition (1) in the definition of control dependency ensures that s2 is not necessarily executed after s1. Condition (2) ensures that once a specific path is taken between s1 and s2, the execution of s2 becomes inevitable. Hence, s1 is a decision point whose outcome determines the inevitable execution of s2.

Definition 7 (Control dependency graph).
The graph G=⟨N,E,s⟩, consisting of nodes n∈N that represent program statements and edges e∈E⊆N×N that represent control dependencies between program statements, is called control dependency graph. Node s∈N represents the entry node, which is connected to all nodes that are not under the control dependency of another node.

The definition of control dependency given above can be easily extended from program statements to arbitrary coverage targets. For instance, given two branches to be covered, b1,b2, we say b1 holds a control dependency on b2 if b1 is postdominated by a statement s1 which holds a control dependency on a node s2 that postdominates b2.


Show All

In DynaMOSA, the control dependency graph is used to derive which targets are independent from any others (targets that are free of control dependencies) and which ones can be covered only after satisfying previous targets in the graph. The difference between DynaMOSA and MOSA are illustrated in Algorithms 6. At the beginning the search process, DynaMOSA selects as initial set of objectives only those targets that are free of control dependencies (line 2). Once the initial population of random test cases is generated (line 4), the current set of targets U∗ is update using the routine UPDATE-TARGETS highlighted in Algorithm 7. This routine is also called at each iteration in order to update the current set of targets U∗ to consider at each generation depending on the results of the execution of the offspring (line 10 in Algorithms 6). Therefore, fitness evaluation (line 8), preference sorting (line 12), and crowding distance (line 16) are computed only considering the targets in U∗⊆U.


Show All

The routine UPDATE-TARGETS is responsible for dynamically updating the set of selected targets U∗ so as to include any uncovered targets that are control dependent on the newly covered target. It iterates over all targets in U∗⊆U (loop in lines 2-6 of Algorithm 7) and in case of newly covered targets (condition in line 3) it visits the control flow graph to find all control dependent targets. Indeed, Algorithm 7 uses a standard graph visit algorithm, which stops its visit whenever an uncovered target is encountered (lines 7-13). In such a case, the encountered target is added to the set of targets U∗, to be considered by DynaMOSA in the next generation. If the encountered target is covered, the visit continues recursively until the first uncovered target is found or all targets have been already visited (lines 9-11). This ensures that only the first uncovered target, encountered after visiting a covered target, is added to U∗. All following uncovered targets are just ignored, since the graph visit stops. 2

In this way, the ranking performed by MOSA remains unchanged (a formal proof of this is provided below), while convergence to the final test suite is achieved faster, since the number of objectives to be optimised concurrently is kept smaller. Intuitively, the ranking of MOSA is unaffected by the exclusion of targets that are controlled by uncovered targets because such excluded targets induce a ranking of the test cases which is identical to the one induced by the controlling nodes.

There are two main conditions that must be satisfied to justify the dynamic selection of the targets produced by the execution of Algorithm 6:

Since at each generation, the set U∗ used by DynaMOSA contains all targets with minimum approach level, for each remaining target ui in U−U∗ there should exist a target u∗∈U∗ such that f(ui)=f(u∗)+K.

The computation cost of the routine UPDATE-TARGETS in Algorithm 7 should be negligible if compared to cost of computing preference sorting and crowding distance for the all uncovered targets.

The first condition is ensured by Theorem 1, presented in the following. The second is not ensured theoretically, but it was found empirically to hold in practice.

DynaMOSA generates test cases with minimum distance from U∗, the set of uncovered targets directly reachable through a control dependency edge of one or more test case execution trace. On the other hand, MOSA generates test cases with minimum distance from the full set of uncovered targets, U. The two minimisation processes are equivalent if and only if the test cases at minimum distance from the elements of U∗ are also the test cases at minimum distance from U. The following Theorem provides a theoretical justification for the dynamic selection of a subset of the targets produced by the execution of Algorithm 7, by proving that the two sets of test cases optimised respectively by MOSA (Tmin) and DynaMOSA ( T0) are the same.

Theorem 1.
Let U be the set of all uncovered targets; Tmin the set of test cases with minimum approach level from U and T0 the test cases with approach level zero; U∗ be the set of uncovered targets at approach level zero from one or more test cases t∈T0, then the two sets of test cases Tmin and T0 are the same, i.e., Tmin=T0 .

Proof.
Since the approach level is always greater than or equal to zero, the approach level of the elements of T0 is ensured to be minimum. Hence, T0⊆Tmin. Let us now prove by contradiction that there cannot exist any uncovered target u associated with a test case t whose approach level from u is strictly greater than zero, such that t does not belong to T0. Let us consider the shortest path from the execution trace of t to u in the control dependency graph. The first node u′ in such path belongs to U∗, since it is an uncovered target and it has approach level zero from the trace of t. Indeed, it is an uncovered target because otherwise the test case t′ that covers it would have an execution trace closer to u than t. Since it satisfies the relation approach_level(t,u′) = 0, test case t must be an element of T0. Hence, Tmin cannot contain any test case not in T0 and the equality Tmin=T0 must hold.

Since the same set of test cases are at minimum distance from either U or U∗, the test case selection performed by MOSA, based on the distance of test cases from U, is algorithmically equivalent to the test case selection performed by DynaMOSA, based on the distance of test cases from U∗.

The fact that DynaMOSA iterates over a lower number of targets (U∗⊆U) with respect to MOSA has direct consequences on the computational cost of each single generation. In fact, in each generation the two many-objective algorithms share two routines: the PREFERENCE-SORTING and the CROWDING-DISTANCE-ASSIGNMENT routines, as shown in Algorithms 2 and 6. For the fronts following the first one, the routine PREFERENCE-SORTING relies on the traditional FAST-NON-DOMINATED-SORT by Deb et al.  [12], which has an overall computation complexity of O(M2×N) where M is the size of the population and N is the number of objectives. Thus, for MOSA the cost of such routine is O(M2×|U|) where U is the set of uncovered targets in a given generation. For DynaMOSA, the computational cost is reduced to O(M2×|U∗|) with U∗⊆U being the set of uncovered targets with minimum approach level. For trivial classes where all targets have no structural dependencies (i.e., classes with only branchless methods), the cost of PREFERENCE-SORTING will be the same since U∗=U. However, for non-trivial classes we would expect U∗⊂U leading to a reduced computational complexity.

A similar analysis can be performed for the other shared routine, i.e., CROWDING-DISTANCE-ASSIGNMENT. According to Köppen et al. [28], the cost of such a routine when using the sub-vector dominance assignment is O(M2×N) where M is the size of the population while N is the number of objectives. Therefore, for MOSA the cost of such routine is O(M2×|U|) where U is the set of uncovered targets in a given generation; while for DynaMOSA it is O(M2×|U∗|). For non-trivial classes, DynaMOSA will iterate over a lower number of targets as long as the condition U∗⊂U holds.

Moreover, MOSA requires to compute the fitness scores (e.g., approach level and branch distances in branch coverage) for all uncovered targets U and for each newly generated test case. In particular, for each test case MOSA requires to compute the distances between execution traces and all targets in U. Instead, DynaMOSA focuses only on coverage targets with minimum approach levels, thus, requiring to compute the fitness scores for U∗⊆U only.

The additional operations of DynaMOSA with respect to MOSA are the construction of the intra-procedural control dependency graph and its visit for target selection. The cost for intra-procedural control dependency graph construction is paid just once for all test case generation iterations and the computation of the graph can be carried out offline, before running the algorithm. The visit of the control dependency graph for coverage target selection can be performed very efficiently (for structured programs the graph is a tree). As a consequence, the saving in the computation of PREFERENCE-SORTING, CROWDING-DISTANCE-ASSIGNMENT and fitness scores performed by DynaMOSA versus MOSA is expected to dominate the overall execution time of the two algorithms, while the visit of the control dependency graph performed by DynaMOSA is expected to introduce a negligible overhead. This was confirmed by our experiments. In fact, in our experiments the intra-procedural control dependency graphs of the methods under test and their visit consumed a negligible amount of execution time in comparison with the time taken by the extra computations performed by MOSA.

SECTION 4Empirical Evaluation
This section details the empirical study we carried out to evaluate the proposed many-objective test case generation algorithms with respect to three testing criteria: (i) branch coverage, (ii) statement coverage, and (iii) strong-mutation coverage.

4.1 Subjects
The context of our study is a random selection of classes from four test benchmarks: (i) the SF110 corpus [17]; (ii) the SBST tool contest 2015  [49]; (iii) the SBST tool contest 2016  [45]; and (iv) the benchmark used in our previous conference paper [38] .

The SF110 benchmark3 is a corpus of Java classes composed of a representative sample of 100 projects from the SourceForge.net repository, augmented with 10 of the most popular projects in the repository, for a total of 23,886 classes. We selected this benchmark since it has been widely used in the literature to assess test case generation tools  [17], [44], [47]. However, Shamshiri et al.  [47] showed that the vast majority of classes in the FS110 corpus are trivial to cover fully, even with random search algorithms. In fact, they observed that most of the classes in this corpus do not require the usage of evolutionary algorithms. Hence, for our evaluation we selected only non-trivial classes from this benchmark. To this aim, we first computed the McCabe’s cyclomatic complexity [33] for each method in FS110 by means of the extended CKJM library.4 The McCabe’s cyclomatic complexity of a method is defined as the number of independent paths in the control flow graph and can be shown (for structured programs) to be equal to the number of branches plus one. We found that 56 percent of the classes in SF110 are trivial, containing only methods with cyclomatic complexity equal to 1. These are branchless methods that can be fully covered by a simple method call. Therefore, we pruned this benchmark by removing all classes whose methods have a cyclomatic complexity lower than five.5 This filtering resulted in a smaller benchmark composed of 96 projects and 23 percent of classes from the original SF110 corpus. Among those 96 projects, 18 projects contain only a single non-trivial class after our filter. For our experiment, we randomly sampled two classes from each of the 78 projects containing multiple non-trivial classes, and we selected the available non-trivial classes from each of the remaining 18 projects. In addition, we randomly sampled two further classes for each of the most popular projects (11 projects), and one additional class for each of the eight larger projects in the repository. This resulted in 204 non-trivial classes.

From the latest two editions of the SBST unit test tool contests [45], [49] we selected 97 subjects: 46 classes from the third edition  [49] and 51 classes from the fourth edition  [45] of the contest. From the original set of classes in the two competitions, we removed all duplicate classes belonging to different versions of the same library since we would not expect any relevant difference over different versions in terms of branch, statement, or mutation coverage. We selected these two benchmarks because they contain java classes with different characteristics and belonging to different application domains. In addition, they have been recently used to assess the performance of different test generation techniques.

Our last benchmark is the one used in our previous conference paper [38] , from which we consider all classes used in our previous empirical study. While the original dataset includes 64 classes, in the present study we report the results for 45 classes, since 19 classes are shared with the SF110 corpus and with the latest SBST unit test tool contest [49].

In total, the selected 346 Java classes have 360,941 statements, 61,553 branches, and 117,816 mutants that are considered as coverage targets in our experiment. Table 1 reports the characteristics of the selected classes, grouped by project. Specifically, it reports the number of classes selected for each project as well as minimum, average (mean), and maximum number of statements, branches and mutants contained in those classes. The number of branches in each class ranges between 2 (a class from the SBST contest) and 7,939, while the number of statements ranges between 14 and 16,624; the number of mutants ranges between 0 and 1,086 (EvoSuite was unable to inject any mutant into two of the selected classes; however, we kept them for the other two coverage criteria). It is worth noticing that, according to the survey by Li et al.  [31], existing many-objective algorithms can deal with up to 50 objectives, while in our study most of selected classes contain hundreds and thousands of objectives (e.g., mutants to kill). This confirms the need for designing new many-objective algorithms suitable for test case generation given the very high number of coverage targets as compared to the number of objectives (≤50) encountered in traditional many- and multi-objective problems.

TABLE 1 Java Projects and Classes in Our Study

4.2 Research Questions
The empirical evaluation aims at answering the following research questions:

RQ1: How does DynaMOSA perform compared to alternative approaches on branch coverage?

RQ2: How does DynaMOSA perform compared to alternative approaches on statement coverage?

RQ3: How does DynaMOSA perform compared to alternative approaches on strong mutation coverage?

The three research questions above aim at evaluating the benefits introduced by the many-objective reformulation of testing criteria—branch, statement and mutation—and to what extent the proposed DynaMOSA algorithm is able to cover more test targets (effectiveness) when compared to alternative approaches for test generation problems. When no difference is detected in the number of covered test targets, we analyse to what extent the proposed approach is able to reach higher code coverage when considering different search time intervals ( efficiency).

To evaluate the internal functioning of our new sorting algorithm (Algorithm 3), which combines both our preference criterion and Pareto-based ranking, we formulate the following additional research question:

RQ4 (internal assessment) : How do preference criterion and Pareto dominance affect the effectiveness of DynaMOSA?

In answering this research question, we consider the two sub-questions below:

RQ4.1: What is the effectiveness of DynaMOSA’s Pareto-based ranking alone, when applied without the preference criterion?

RQ4.2: What is the effectiveness of DynaMOSA’s preference criterion alone, when applied without Pareto-based dominance ranking?

4.3 Baseline Selection
To answer our first three research questions, we selected the following approaches for comparison with DynaMOSA:

Many-objective sorting algorithm. It is the many-objective solver we developed in our previous paper [38] for branch coverage. While in our previous paper we used MOSA only for branch coverage criterion, in this paper (see Section 3.2) we have described how this algorithm can be also used for other criteria. Therefore, we use MOSA as baseline for the first three research questions.

Whole Suite approach with Archive (WSA). It is an extension of the more traditional whole suite approach recently proposed by Rojas et al.  [44] and implemented in EvoSuite. In particular, WSA is a hybrid approach that implements some routines of our MOSA algorithm along with the original whole suite approach. First, WSA uses an archive to store test cases that cover one or more coverage targets (e.g., branches)  [44]. Such an archive operator works at test case level and not at test suite level, which is the usual granularity of the whole suite approach. Second, WSA focuses the search on uncovered coverage targets only, which is also one of the core properties we developed in MOSA (and DynaMOSA too). Finally, the final test suite is not represented by the best individual (candidate suite) from the last generation of GA, but it is artificially synthesised by taking those test cases stored in the archive and that come from different suites. While WSA has been shown to outperform both standard WS and one-target approaches, it departs from the original WS principle of test suite evolution  [44]. For our study, we could use WSA as baseline only for RQ 1 and RQ 3 because no archive strategy has been implemented in EvoSuite for statement coverage.

Traditional Whole Suite approach (WS). This is the traditional, pure whole suite approach without any archive strategy [16]. Thus, the final test suite provided to developers is the best individual (candidate suite) from the last generation of the genetic algorithm. We used WS as baseline only for RQ2 since for statement coverage no WSA variant is available in EvoSuite. While WS could be used as additional baseline also for RQ1 and RQ 3, we discarded this scenario since Rojas et al. [44] already showed the superiority of WSA over WS for branch coverage.

MOSA is a natural baseline to compare with, in order to investigate the benefits achieved when dynamically reducing the number of objectives to be optimised at each generation. This comparison allows us also to empirically validate our discussion on the computational cost of DynaMOSA versus MOSA reported in Section 3. WSA is the main baseline, because it has been shown to outperform standard WS and the single-target approach. Since WSA is a hybrid solution that incorporates the archiving core routines of MOSA (and DynaMOSA), such comparison allows us to understand to what extent benefits in effectiveness or efficiency stem from our many-objective reformulation and not from our archiving mechanism. It is also worth noticing that WSA won the last SBST tool contest (edition 2016) against other tools, such as JTexPert  [46] (evolutionary), T3 [41] (random testing). Finally, DynaMOSA, MOSA, WS and WSA are implemented in the same tool (i.e., EvoSuite) avoiding possible confounding factors [44] due to the usage of different tools with different implementation choices (e.g., different libraries to build the control flow graph).

To address our last research question (RQ4), we selected the following algorithms to compare against DynaMOSA:

DynaMOSA with Pareto ranking alone (Dyna-MOSA Rank). This variant of DynaMOSA uses only the Pareto ranking to assign test cases to different non-dominated fronts, without using our preference criterion to further increase the selection pressure (see Algorithm 3). Therefore, this variant corresponds to the traditional NSGA-II algorithm, enriched with the archive strategy and with a dynamic selection of coverage targets. DynaMOSARank is used as baseline to answer RQ4.1.

DynaMOSA with preference criterion alone (Dyna-MOSA Pref). This variant of DynaMOSA uses our preference criterion without applying the Pareto ranking algorithm to build the other non-dominated fronts. Therefore, at each generation it creates only two fronts: the first containing all best test cases selected according to preference criterion, while all the remaining tests are assigned to the second front (see Algorithm 3). This variant also uses an archive to keep track of test cases as soon as they reach uncovered targets (see Algorithm 2). DynaMOSAPref is used as baseline to answer RQ4.2.

4.4 Prototype Tool
We have implemented our many-objective algorithms MOSA and DynaMOSA in a prototype tool that extends the EvoSuite test data generation framework [16], [44]. In particular, we implemented the two many-objective GAs as described in Section 3 within EvoSuite version 1.0.3, downloaded from GitHub 6 on May 15th, 2016. All other details (e.g., encoding schema, genetic operators, etc.) are those implemented in EvoSuite [16], [44]. According to the encoding schema available in EvoSuite for test cases, a test case is a statement sequence t=⟨s1,s2,…,sn⟩ of length n. Each statement has a type belonging to one of five kinds of statements: (i) primitive statement, (ii) constructor statement, (iii) field statement, (iv) method statement and (v) assignment statement. Note that the representation has variable size, i.e., the number of statements n in each test can vary during the GA search. Test cases are evolved through the application of selection, crossover and mutation operators. Pairs of test cases (parents) are selected using tournament selection according to their non-dominance ranks. Parents are recombined using the single-point crossover, which generates offsprings by exchanging statements between the two parent test cases. Finally, test cases are mutated using uniform mutation, which randomly removes, changes or inserts statements in a given test case. Since our many-objective algorithm uses Pareto-based ranking, we implemented the tournament selection operator based on dominance ranking and crowding distance as defined in NSGA-II (see Algorithm 2).

The tool, along with a replication package, is available for download here: http://selab.fbk.eu/kifetew/mosa.html.

4.5 Parameter Setting
There are several parameters that control the performance of the algorithms being evaluated. We adopted the default parameter values used by EvoSuite [16], as it has been empirically shown [2] that the default values, which are also commonly used in the literature, give reasonably acceptable results. Thus, here we only report a few of the important search parameters and their values:

Population size: we use the default population size in EvoSuite, which is set to 50 individuals.

Crossover: we use the default crossover operator in EvoSuite, which is single-point crossover with crossover probability set to 0.75 [16].

Mutation: test cases are mutated by adding, deleting, changing statements using uniform mutation with mutation probability equal to 1/size, where size is the number of statements contained in the test case to mutate [16].

Selection: we use the tournament selection available in EvoSuite, with default tournament size equal to 10.

Search Timeout: the search stops when the maximum search budget (max_time) is reached or if 100 percent coverage is achieved before consuming the total allocated time. We set max_time to five minutes for branch and statement coverage, while we increased the search timeout up to eight minutes for strong mutation coverage. We used a larger budget for strong mutation coverage because of the additional overhead required by this criterion to re-execute each test case against the target mutants.

It is important to notice that WSA, MOSA and DynaMOSA work at two different granularity levels: WSA evolves test suites while MOSA and DynaMOSA evolve test cases. Therefore, in our many-objective algorithms the number of individuals (i.e., test cases) is always the same (50 test cases). Differently, in WSA each individual is a test suite which usually contains a variable number of test cases. The population is composed of 50 test suites, where each suite can have a variable number of test cases that can be added, deleted or modified at each generation. The crossover operator also works at two different levels of granularity: the single point crossover in MOSA and DynaMOSA creates two offsprings by recombining statements from the two parent test cases; in WSA suite offsprings are generated by recombining test cases composing the two parent suites. For WS and WSA the winner of the tournament selection is the test suite with the smaller (fitter) whole-suite fitness function, while in MOSA and DynaMOSA the winner is the test case with the lowest non-dominance rank or with the largest crowding distance for test cases having the same non-dominance rank.

4.6 Experimental Protocol
For each class, and for each coverage criterion (statement, branch, mutation), each search strategy (WSA, MOSA, DynaMOSA) is executed and the resulting coverage is recorded. The three coverage criteria are implemented as fitness functions in EvoSuite and are used as guidance for the search algorithms. For strong mutation, EvoSuite uses its own mutation engine that generates mutated versions of the code under test, which correspond to the adequacy targets (mutants to kill). In each execution, an overall time limit is imposed so that the run of an algorithm on a class is bounded with respect to time. Hence, the search stops when either full coverage is reached or the total allocated time is elapsed. To allow reliable detection of statistical differences between the strategies, each run is repeated 10 times. Consequently, we performed a total of 3 (search strategies) × 3 (coverage criteria) × 346 (classes) × 10 (repetitions) = 31,140 test generator executions.

To answer RQ1, RQ 2, and RQ 3 in terms of effectiveness, we measure the percentage of covered branches, statements, and the percentage of killed mutants
branch_covstatement_covmutation_cov=#covered branches#total branches to be covered=#covered statements#total statements to be covered=#killed mutants#total mutants to be killed.
View SourceTo measure the aforementioned metrics, we rely on the internal engine of EvoSuite  [44], which performs some post-processing by re-executing the test suites obtained at the end of the search process. During post-processing, EvoSuite removes those statements that do not contribute to the final coverage from each test case. It also removes redundant test cases from the final test suite. After this minimisation process, the final test suite is re-executed to collect its final coverage metric. We execute the three selected algorithms separately for each single coverage criterion, thus obtaining the corresponding coverage score after the post-processing.

We also statistically analyse the results, to check whether the differences between two different algorithms are statistically significant or not. To this aim we used the non-parametric Wilcoxon test  [8] with a p-value threshold of 0.05. Significant p-values indicate that the null hypothesis can be rejected in favour of the alternative hypothesis, i.e., that one of the algorithms reaches a significantly higher coverage. Besides testing the null hypothesis, we used the Vargha-Delaney (Â12) statistic [50] to measure the effect size, i.e., the magnitude of the difference between the coverage scores achieved by two different algorithms. The Vargha-Delaney (Â12) statistic is equal to 0.50 if the two compared algorithms are equivalent, Â12≠0.50 otherwise. In our case, an Â12>0.50 indicates that DynaMOSA reaches a higher coverage than the alternative algorithm, while Â 12<0.50 indicates that the alternative algorithm is better than DynaMOSA.

To quantify the efficiency of the compared algorithms, we analyse their capability of reaching higher code coverage at different points in time. While the effectiveness measures the algorithm performance only at the end of the allocated time (i.e., after five or eight minutes), we want also to analyse how algorithms perform during the search. A simple way to perform such a comparison consists of plotting the percentage of branches, statements or mutants killed by each algorithm at predefined time intervals during the search process. Such a convergence graph allows us to compare two or more search algorithms, showing the percentage of covered test goals at the same point in time. To this aim, we collected the coverage achieved after intervals of 20 seconds for each independent run, resulting in 16 coverage points for each run (25 points for strong mutation coverage). To summarise the efficiency of the experimented algorithms using a single scalar value, we computed the overall convergence rate as the area under the curve delimited by the convergence graph. More formally, let PA={p0,…,ph} be the set of points in time considered for a given algorithm A. Let cov(pi) be the percentage of targets covered by A at time pi. Let I be the time elapsed between two consecutive points pi and pi+1. The Area-Under-Curve (AUC) enclosed by these points is computed using the trapezoidal rule
AUC(PA)=∑h−1i=0[cov(pi)+cov(pi+1)]×I2×TotalRunningTime×100.(10)
View SourceSuch metric takes values in [0:100]. Higher AUC values are preferable because they indicate that the algorithm is faster in achieving higher levels of code coverage.

It is important to remark that we consider efficiency only for classes where we do not observe a significant difference in terms of effectiveness, i.e., branch coverage, statement coverage or mutation coverage. In other words, the efficiency performance metric is considered for comparison only in cases where the algorithms under analysis achieve the same coverage level. This is because focusing only on efficiency without considering effectiveness is not meaningful (low coverage can be easily achieved very efficiently). As done for effectiveness, we measure the statistical significance of differences in efficiency using the non-parametric Wilcoxon test  [8] with a p-value threshold of 0.05, and the Vargha-Delaney (Â12) statistic [50] to measure the effect size. In this case, significant p-values indicate that one of the two algorithms being compared converges quicker than the other to the final coverage. Â12>0.50 indicates that DynaMOSA converged to the final coverage more quickly than the alternative algorithm, while Â12<0.50 indicates that DynaMOSA was less efficient than the alternative algorithm, e.g., MOSA).

For the internal assessment (RQ4), we investigate the effectiveness of DynaMOSA against two of its variants: (i) a first variant, namely DynaMOSA Rank, which uses only the Pareto-based dominance ranking; and (ii) a second variant, namely DynaMOSA Pref, which uses our preference criterion alone. For DynaMOSA and for both DynaMOSA variants we use an archive to store test cases as soon as they cover any yet uncovered target. Therefore, any difference in efficiency can be attributed only to the different ranking strategies used by DynaMOSA or by its variants. To save space, the comparison is reported only for branch coverage, since consistent results are obtained for statement and strong mutation coverage.

Given the large number of classes considered in the experiments, we cannot report all p-values and Vargha-Delaney Â12 scores measured for each class. Therefore, for each project in our sample we report the number of times (number of classes) the comparison is statistically significant at level 0.05, as well as the corresponding number of times (percentage) the Vargha-Delaney (Â12) statistic is greater or lower than 0.50. Results at class level are available in our replication package for the interested readers.

SECTION 5Results
This section discusses the results of our study with the aim of answering the research questions formulated in Section 4. In the following, when comparing two different algorithms we will refer to difference in code coverage as the arithmetic difference between the two corresponding percentage coverage scores.

5.1 RQ1: How Does DynaMOSA Perform Compared to Alternative Approaches on Branch Coverage?
Table 2 summarises the results achieved by WSA, MOSA, and DynaMOSA for branch coverage. In particular, it reports the percentage of classes in each project where the effect size (Â12) is statistically greater/lower than 0.50. In the following, we will refer to 335 classes only since in 11 classes in our sample EvoSuite crashed for some internal errors.

TABLE 2 Mean Branch Coverage Achieved for Each Project

From Table 2 we can observe that DynaMOSA is significantly better than WSA in 93 classes out of 335, corresponding to 28 percent of classes in our sample. The improvement achieved in branch coverage using DynaMOSA, for classes where statistically significant differences were observed, ranges between 1 and 67 percent, with an average improvement of 8 percent. On the other hand, we observe that WSA is significantly better than DynaMOSA in 10 classes only, while no statistically significant difference is observed in the remaining 232 cases. Looking at the results at project level, we find that in 43 percent of the projects DynaMOSA achieved higher branch coverage than WSA on at least one class in the project.

For example, if we consider class ControlFlowAnalysis extracted from the compiler project, we can observe that WSA covers 184 branches (on average) while DynaMOSA covers 287 (+25 percent) branches (on average) using the same search budget. For larger classes, differences are even larger. For example, for class JavaParserTokenManager extracted from the project jmca, WSA covers 994 branches (on average) against 1,157 (+10 percent) branches covered (on average) by DynaMOSA using the same search budget. Project jmca is particularly interesting since it contains the largest classes in our sample (two of the five selected classes have more than 1,500 branches). On this project DynaMOSA is significantly better than WSA in 75 percent of the selected classes with Â12=1 in all cases.

Only in a few cases WSA is significantly better than DynaMOSA (10 classes out of 335), with an average difference, over classes with significant differences, of 3 percent in terms of branch coverage. The class with the highest difference is JSJshop extracted from the project shop. For this class, WSA is able to cover 93 branches (on average) against 46 branches covered by DynaMOSA. After manual investigation, we discovered that such a negative result is due to test cases discarded because of the usage of test case length ( size) as secondary selection criterion in our preference criterion. In order to verify this conjecture, we ran DynaMOSA with the test size as secondary criterion disabled. The obtained results show that DynaMOSA covers on average 95 branches (+1.75 percent), with an effect size equal to 0.83 and p-value=0.01. Therefore, we conclude that test case size as secondary preference criterion can have sometimes negative effects on the final coverage. Thus, an adaptive version of the secondary preference criterion (e.g., disabling size when there is no improvement of branch distance for some generations) might be beneficial.

For the 232 classes on which there is no statistically significant difference in branch coverage between WSA and DynaMOSA, we compare the two search strategies along the efficiency metric. The results of this comparison are summarised in Table 3. The table reports the median AUC metric for WSA and DynaMOSA as well as the results of the effect size (Â12) aggregated by project. From the comparison, we observe that in 27 classes out of 232, DynaMOSA has a statistically significantly higher AUC metric compared to WSA, which means that the former would reach higher branch coverage if given a shorter search time. The average improvement for the AUC metric is 3 percent. The minimum improvement, 0.1 percent, occurs for class Title (from project wikipedia ), and the maximum, 17 percent, for DynamicSelectModel (from project wheelwebtool). On the other hand, WSA turned out to be more efficient than DynaMOSA in 13 classes out of 232. However, for these classes the differences in terms of AUC scores are very small, being <0.1 percent on average.

TABLE 3 Efficiency of DynaMOSA and WSA for Branch Coverage

Let us now consider the comparison between DynaMOSA and MOSA. Results for branch coverage are reported in Table 2. We can observe that in the majority of the cases there is no statistically significant difference between the two many-objective algorithms according to the Wilcoxon test. In total, in 64 classes out of 335 (19 percent) DynaMOSA achieved significantly higher branch coverage, while in only one class (<1 percent) MOSA turned out to be better than DynaMOSA. Looking at the magnitude of the differences, we notice that in those cases where DynaMOSA achieved higher branch coverage, the improvements range between 1 and 71 percent, with an average increase of 7 percent. The largest improvement is obtained for JTailPanel (jtailgui project), for which DynaMOSA covers 21 branches (91 percent) while MOSA covers only three branches (13 percent). For the single class where MOSA outperformed DynaMOSA, namely ServerGameModel from hft-bomberman, the difference in terms of branch coverage is 4 percent. We observe that this class is very peculiar: it contains 128 branches and it is very expensive in execution time. Indeed, both DynaMOSA and MOSA could only execute less than 5 generations of their evolutionary algorithm (on average) within the five minutes of search budget.

In terms of efficiency, DynaMOSA and MOSA are statistically equivalent in the majority of those classes where there is no statistically significant difference on branch coverage. Indeed, as reported in Table 4 in only 12 classes out of 273 DynaMOSA yielded a statistically significantly higher AUC metric as compared to MOSA. More specifically, we observe that on these classes DynaMOSA produces +5 percent of branch coverage on average when considering a search budget lower than five minutes, which is the total search time set for branch coverage. The minimum improvement of 0.2 percent is yielded for class Conversion (from the apache commons library), and the maximum of 29 percent is achieved for ExploitAssigns (from project compiler). On the other hand, MOSA turned out to be more efficient than DynaMOSA in only one class, namely TimePeriodValues from jfree-chart, where the difference in terms of AUC is equal to just 0.21 percent.

TABLE 4 Efficiency of DynaMOSA and MOSA for Branch Coverage

Fig. 4 shows an example of (average) branch coverage achieved over time by MOSA, DynaMOSA and WSA on SortingTableModel, a class for which we did not find any statistically significant difference in terms of efficiency or effectiveness for the three experimented approaches. From Fig. 4, we can observe that in the first 20 seconds DynaMOSA is particularly efficient compared to the other alternatives. In less than 10 seconds, it reaches 96 percent of branch coverage. MOSA required almost 30 seconds to reach the same branch coverage, while WSA had the worst branch coverage scores within this initial time window. This result is very unexpected considering that WSA benefits from a larger population size (50 test suites with more than one test case each) with respect to DynaMOSA and MOSA (50 initial test cases in total). After consuming 60 seconds, all the three approaches reach the maximum branch coverage, which is 98 percent for this class. As a consequence, we did not detect any statistically significant difference in the final efficiency among DynaMOSA, MOSA and WS, but a substantial difference can be observed in the first 20 seconds (i.e., before we collect the first coverage point when computing the AUC metric). Hence, the results in Fig. 4 show that when a very limited search budget is given to the search ( <20-30 s), DynaMOSA reaches higher coverage much more quickly than the other two alternatives. We obtained consistent results for the other classes in our study for which we did not find any significant difference in the final effectiveness and efficiency: DynaMOSA has always the steepest coverage increase in the initial time window.


Fig. 4.
Branch coverage over time for class SortingTableModel from project caloriecount.

Show All

In summary, we can conclude that DynaMOSA achieves higher or equal branch coverage as compared to both WSA and MOSA. Moreover, DynaMOSA converges to such coverage more quickly especially when limited time is given to the search. Finally, our many-objective reformulation provides further benefits to branch coverage as compared to the usage of the archive strategy, since it outperformed WSA, which extends WS with the archive strategy.

5.2 RQ2: How Does DynaMOSA Perform Compared to Alternative Approaches on Statement Coverage?
Table 5 reports the statement coverage achieved by WS, MOSA, and DynaMOSA for the classes in our experiment, grouped by project. Table 5 also reports the percentage of classes in each project where the effect size (Â12) is statistically greater or lower than 0.50. Since in seven classes in our sample EvoSuite crashed for some internal errors, in the following we will refer to 339 classes only for which EvoSuite could generate tests.

TABLE 5 Mean Statement Coverage Achieved for Each Project

For statement coverage, we can notice that DynaMOSA is significantly better than WS on 173 classes out of 339 (51 percent), while WS is significantly better in only five cases (<2 percent). In the remaining 160 classes, no statistically significant difference is observed according to the Wilcoxon test. At project level, we find that in 65 percent of the projects DynaMOSA achieves significantly higher coverage than WS on at least one class in the project. Overall, the usage of DynaMOSA leads to an increase in statement coverage equal to 11 percent on average, over classes where significant differences were observed. The largest improvement is obtained for class TwitterImpl selected from project twitter4j, where WS covers 883 statements (14 percent) on average against 6,042 statements (96 percent) covered by DynaMOSA on average using the same search budget. Once again, particularly high differences are detected for larger classes. Indeed, for the largest class in our sample, which is JavaParser from project jmca , WS covers (on average) 2,415 statements while DynaMOSA covers 8,472 statements (+6,057 statements) using the same search budget.

Only in five classes WS is significantly better than DynaMOSA, e.g., for CycleHandler from project wikipedia and WhoIS from project ipcalculator. For these classes, the average difference (decrement) in statement coverage is of 2 percent. As in the case of branch coverage, we notice that by disabling the test case size as secondary preference criterion, DynaMOSA achieves equal or higher statement coverage for these two classes as well.

For the 160 classes with no statistically significant differences in terms of effectiveness (statement coverage) between WS and DynaMOSA, we compare the efficiency measured using our AUC metrics. The results of this comparison are summarised in Table 6. We observe that in 49 classes out of 160 (31 percent), DynaMOSA yielded statistically significant higher AUC scores, meaning that it is significantly able to reach higher statement coverage than WS for search time lower than 5 minutes. The average improvement for the AUC metric is 3 percent. The minimum improvement, 0.1 percent, occurs for class FastDateFormat (from project apache commons lang), and the maximum, 21 percent, for ForwardingObserver (from project hft-bomberman). Vice versa, WS turned out to be more efficient than DynaMOSA in six classes out of 160 with an average difference (decrement) in terms of AUC scores are very small, being <2 percent on average.

TABLE 6 Efficiency of DynaMOSA and WS for Statement Coverage

Regarding the comparison between DynaMOSA and MOSA, results are reported in Table 5. From the results, we observe that in 40 classes (12 percent) DynaMOSA leads to statistically significantly higher statement coverage with effect size Â12≫0.5. In these cases, improvements range between 0.10 and 88 percent with an average increment of 5 percent. The class with the largest improvement with respect to MOSA is SchurTransformer selected from the apache commons math, for which DynaMOSA covers (on average) 1,250 statements (92 percent) while MOSA covers 45 statements (3 percent). On the other hand, MOSA is significantly better in eight classes (2 percent) in our study. In such cases, the average difference in statement coverage is 2 percent, with a minimum of 0.07 percent for class SpecialMath from jsci and a maximum difference of 8 percent for class OperationsHelperImpl from xisemele.

The results of the comparison between DynaMOSA and MOSA in terms of efficiency are reported in Table 7. Among 291 classes with no statistically significant difference in statement coverage, DynaMOSA has statistically higher AUC scores in 46 classes (16 percent). On the other hand, in five classes (2 percent) MOSA required significantly less search time than DynaMOSA to reach the same final coverage. On the remaining 240 classes, no statistically significant difference is observed, meaning that MOSA and DynaMOSA reached the maximum statement coverage consuming similar search times.

TABLE 7 Efficiency of DynaMOSA and MOSA for Statement Coverage
Table 7- 
Efficiency of DynaMOSA and MOSA for Statement Coverage
To provide a deeper view on efficiency, Fig. 5 plots the average statement coverage achieved by DynaMOSA, MOSA and WS over time for class SmallCharMatcher , a class for which we did not find any statistically significant difference in neither effectiveness nor efficiency for the three approaches. From Fig. 5, we can notice that at the beginning of the search (first 15 seconds) DynaMOSA quickly reaches 98 percent of statement coverage against 79 percent reached by WS and 71 percent by MOSA. Between 16 and 21 seconds the three approaches have the same statement coverage (notice that we pick the first coverage point for the AUC metric at 20 s); while between 22 and 58 seconds both DynaMOSA and MOSA outperform WS. Furthermore, on this class WS outperforms MOSA for a while within the first 15 seconds of search. Then, after 56 seconds (on average) all the three approaches reach 99 percent of statement coverage. However, if a shorter time were allocated for the search (< 20 s), we notice that DynaMOSA would reach higher coverage more quickly than the other two alternative approaches. We obtained consistent results on the other classes in our study for which we did not observe significant difference in terms of final statement coverage.


Fig. 5.
Statement coverage over time for class SmallCharMatcher from project guava.

Show All

In summary, we can conclude that DynaMOSA achieves higher or equal statement coverage as compared to both WS and MOSA. On classes with no improvement in statement coverage, DynaMOSA converges more quickly to the final coverage, especially when limited time is given to the search.

5.3 RQ3: How Does DynaMOSA Perform Compared to Alternative Approaches on Strong Mutation Coverage?
Results on the strong mutation coverage are reported in Table 8, grouped by software projects. For 34 classes, we could not obtain the mutation coverage score because of internal EvoSuite crashes. Therefore, in the following we refer to 312 classes only.

TABLE 8 Mean Strong Mutation Coverage Achieved for Each Project

Results show that DynaMOSA statistically significantly outperforms WSA in 85 out of 312 classes (27 percent), which corresponds to 41 percent of the projects in our study. The average improvement, over classes where significant differences were observed, in mutation score achieved by DynaMOSA is 11 percent, with minimum and maximum improvement of 0.30 and 69 percent respectively. The class with largest improvement is TableMeta, from project schemaspy, for which the total number strong mutants to kill is equal to 21. On this class, DynaMOSA kills on average 15 mutants (71 percent) while WSA kills only one mutant on average. This result shows that DynaMOSA can achieve high mutation scores even on small classes (i.e., classes with a small number of mutants). WSA turned out to be statistically significantly better on 22 classes (7 percent), corresponding to the 16 percent of the projects in our sample. However, in this case the average difference in strong mutation coverage is just 5 percent.

For the remaining 205 classes in our study, no statistically significant difference was found for mutation coverage. For these classes, we compared DynaMOSA and WSA in terms of efficiency, using the AUC metric. The results of this comparison are summarised in Table 9, grouped by project. We observe that in 64 classes (31 percent) DynaMOSA reaches a statistically significant higher AUC score than WSA, with an average improvement of 8 percent, and minimum improvement of 1 percent for Region ( templateit project) and maximum of 78 percent for SimpleKeyListenerHelper ( caloriecount project). Vice versa, WSA is better than DynaMOSA on 11 classes (5 percent), with an average AUC difference of 6 percent. It is important to notice that on these 11 cases, the difference in efficiency is mainly due to the test prioritisation performed by WSA: WSA prioritises the test cases in a test suite according to their execution time before re-running them on each infected mutant. Hence, mutants that are killed by quicker tests in a test suite do not need to be evaluated again on the other more expensive tests in the same test suite. This strategy can be particularly relevant in the case of the strong mutation coverage, as every mutant requires the re-execution of the test suite. Differently, in DynaMOSA (and MOSA) all test cases (offsprings) are evaluated, without applying any prioritisation strategy. Hence, there is room for improvement of DynaMOSA when using it with the strong mutation coverage. In fact, we could also prioritise the offspring test cases by execution time.

TABLE 9 Efficiency of DynaMOSA and WSA for Strong Mutation

Let us now consider the comparison between DynaMOSA and MOSA. Results are similar to those obtained for branch and statement coverage. As reported in Table 8, DynaMOSA achieved significantly higher mutation score in 82 classes (26 percent) against six classes (2 percent) on which MOSA yields statistically significantly higher scores. In those classes where DynaMOSA turned out to be better, the improvements in terms of mutation score range between 0.51 and 79 percent, being 12 percent on average. In those few cases (2 percent) where MOSA yields higher mutation score, the average increase is 7 percent, with minimum of 1 percent for class BrentOptimizer (from apache commons math) and maximum of 23 percent for class Verse (from project biblestudy).

The largest improvement achieved by DynaMOSA is 79 percent, and it is obtained for class AuswertungGrafik from project nutzenportfolio, whose total number of mutants is 131. On this class, DynaMOSA is able to kill 103 mutants compared to zero mutants killed by MOSA on average. The explanation for such large difference is due to (i) the number of objectives evaluated at each generation, and (ii) the heavy evaluation cost of each mutant for such a class. Indeed, with MOSA all 131 mutants are evaluated since the first generation, even if not all mutants are within control of covered branches. Differently, DynaMOSA exploits the control dependencies between test targets and selects less than 100 mutants in the first generation, i.e., only mutants under root branches (and their dependencies, if covered) in the code. The remaining mutants are dynamically added as further search objectives during later generations according to the control dependencies of newly covered branches.

In terms of efficiency, DynaMOSA and MOSA are statistically equivalent in the majority of the classes where there is no statistically significant difference in the number of killed mutants. Indeed, as reported in Table 10, in only 18 classes out of 224 DynaMOSA yielded a significantly higher AUC scores (average coverage over search time) as compared to MOSA; vice versa in only eight classes MOSA produced significantly higher AUC scores than DynaMOSA.

TABLE 10 Efficiency of DynaMOSA and MOSA for Strong Mutation

In summary, DynaMOSA kills more or the same number of mutants as compared to both WSA and MOSA. Moreover, for classes with no improvement in mutation score DynaMOSA converges more quickly.

5.4 RQ4: How Do Preference Criterion and Pareto Dominance Affect the Effectiveness of DynaMOSA?
Our approach is designed to improve the selection pressure by giving higher priority to the test cases that are close to reach uncovered targets. Specifically, selection is based on a new sorting algorithm (Algorithm 3), which combines two key ingredients: (i) traditional Pareto-based ranking and (ii) our novel preference criterion. Table 11 reports the results achieved on branch coverage by three variants of DynaMOSA: (i) a first variant, namely DynaMOSA Rank, which uses only Pareto-based ranking; (ii) a second variant, namely DynaMOSAPref, which uses only our preference criterion; and (iii) full DynaMOSA, which combines both Pareto-based ranking and preference criterion (DynaMOSAFull). In all three variants, we use an archive to keep track of test cases as soon as they reach uncovered targets (see Algorithm 5). Therefore, any difference in the final branch coverage can be interpreted as the effect of the different selection strategies used by the DynaMOSA variants.

TABLE 11 Projects with Statistically Significant Difference in Branch Coverage for Three Variants of DynaMOSA

From the comparison, we observe that DynaMOSAFull outperforms DynaMOSARank in 172 classes out of 335 (51 percent) with an average difference in branch coverage of 14 percent and maximum difference of 91 percent (for class TableMeta from the project schemaspy). In 46 percent of the projects there is at least one class for which DynaMOSAFull outperforms DynaMOSARank. Since DynaMOSA Rank uses only Pareto-based ranking, the achieved results show that the usage of the preference criterion has a strong impact on the effectiveness of our many-objective algorithm.

There are only four classes in which DynaMOSARank has significantly higher branch coverage scores, namely ScopeUtils from checkstyle , CacheBuilderSpec from guava, InternalChatFrame from dsachat, and ReflectionSearch from beanbin. However, for these classes we note that the difference is small (<5 percent on average) especially if compared to the average improvement that can be obtained by using our preference criterion, i.e., by using DynaMOSAFull. The explanation for these few exceptional classes is that the statistically significant differences are due to the usage of test case length ( size) as secondary selection criterion in our preference criterion. Indeed, re-running DynaMOSAFull by disabling the test size criterion, the average branch coverage (over 10 independent runs) for ReflectionSearch is 73.20 percent, becoming statistically indistinguishable from DynaMOSA Rank (p-value =0.92 and Â12=0.52).

The results of the comparison between Dyna-MOSAFull and DynaMOSAPref are mixed: in the majority of classes (302 out of 335 classes) there is no statistically significant difference between these two variants of DynaMOSA. This means that the preference criterion is the most critical ingredient in our many-objective algorithm, since it achieves the same branch coverage (and sometimes higher coverage) even if it is not combined with Pareto-based ranking. In 39 classes (12 percent) DynaMOSA Full outperforms DynaMOSAPref, with an average difference in branch coverage of 14 percent and maximum difference of 85 percent for class SchurTransformer from the apache commons math. In the remaining 14 classes (4 percent), the usage of preference criterion alone (DynaMOSA Pref) leads to statistically higher branch coverage if compared to its combination with the traditional non-dominance ranking (DynaMOSAFull).

We notice that DynaMOSAPref is significantly better for extremely large classes, i.e., classes with hundreds or thousands of branches. Vice versa, DynaMOSAFull achieves better coverage for relatively small/medium size classes. In fact, the mean size of classes for which DynaMOSAPref turned out to be significantly better than DynaMOSAFull is 247 branches. On the other hand, the mean size of classes for which DynaMOSA Full outperforms DynaMOSAPref is 177.

The explanation for such findings could be that Pareto-based ranking computes the non-dominance ranks using the Fast-Non-Dominated-Sort algorithm, whose computational complexity is O(M2×N), where M is the population size and N is the number of uncovered test targets. When the number of targets becomes too large (the order of thousands), DynaMOSA Full spends too much time in computing the non-dominance ranks rather than in evolving the test cases. Hence, potentially better coverage scores may be achieved by developing an adaptive strategy which enables or disables Pareto-based ranking based on the number of uncovered branches considered as objectives.

In summary, our preference criterion is a critical component of DynaMOSA, necessary to improve the selective pressure when dealing with many objectives (RQ4). Indeed, Pareto-based ranking (even with the archive) is not sufficient to deal with hundreds of objectives, as is the case of test case generation.

SECTION 6Additional Analyses
In this section, we analyse qualitatively some examples and discuss co-factors that could have played an important role in the performance of the experimented algorithms. Even though these analyses are not directly related to the research questions of the study described in Sections 4 and 5, they helped us understand the conditions in which DynaMOSA outperforms the alternative algorithms.

6.1 Qualitative Analysis
Fig. 6 shows an example of a branch covered by DynaMOSA but not by WSA for the class MatrixUtils extracted from the Apache commons math library, i.e., the false branch of line 137 of method createRealMatrix. The related branch condition checks the size of the input matrix data and returns an object of class Array2DRowRealMatrix or of class BlockRealMatrix depending on the outcome of the branch condition. In particular, below 212 elements (i.e., 4,096 elements or 64×64 for a square matrix) an Array2DRowRealMatrix instance is built since it can store up to 32 kB array. Above this threshold a BlockRealMatrix instance is built. At the end of the search process, the final test suite obtained by WSA has a whole suite fitness f=21.50. Within the final test suite, the test case closest to cover the considered target is shown in Fig. 6b. It is a test case with branch distance d=0.9998 , for the branch under analysis.


Fig. 6.
Example of uncovered branch for MatrixUtils.

Show All

This test case executes method createRealMatrix giving it as input an array with 1×6=6 elements (<<4,096). However, by analysing all the test cases generated by WSA during the search process we found that TC1 is not the closest test case to the false branch of line 137 across all generations. For example, at some generation WSA generated a test case TC2 with a lower branch distance d=0.9997 that is reported in Fig. 6c. As we can see, TC2 also executes the lines 131-136, hence being equivalent to TC1 in terms of coverage. However, in TC1 the method createRealMatrix is called by using as input an array with 50×50=2,500 elements, thus, such a test case is much closer to satisfy the condition data.length * data[0].length > 4,096 that lead to cover the false branch of line 137. However, TC2 was generated within a candidate test suite with a poor whole suite fitness f=98.37, which happened to be the worst candidate test suite in its generation. Thus, in the next generation this test suite is not selected to form the next generation and the promising test case TC2 is lost. By manual investigation we verified that this scenario is quite common, especially for classes with a large number of targets to cover. As we can see from this example, the whole suite fitness is really useful in increasing the global number of covered goals, but when aggregating the branch distances of uncovered branches, the individual contribution of single promising test cases may remain unexploited.

Unlike WSA, DynaMOSA selects the best test case (instead of the best test suite) within the current population for each uncovered branch. Therefore, in a similar scenario it would place TC2 in the first non-dominated front F0 according to the proposed preference criterion. Thus, generation by generation test case TC2 will remain in front F0 until it is replaced by a new test case that is closer to covering the target branch. Over few generations, DynaMOSA covers the false branch of line 137, while WSA does not, even after 10 minutes of search budget.

6.2 Co-Factors Analysis for DynaMOSA versus WSA and WS
Fig. 7 plots the relation among McCabe’s cyclomatic complexity, number of coverage targets, and Vargha-Delaney Â12 scores achieved for those classes with statistically significant differences between DynaMOSA and WSA. Basically, the figure reports three x-y scatter plots with circles to express a third dimension: each circle represents a class in our study, whose x coordinate is the Â 12 score achieved when comparing DynaMOSA against WSA, the y coordinate is its McCabe’s cyclomatic complexity score, while the radius of the circle is proportional to the number of coverage targets. We compute the McCabe’s cyclomatic complexity score of each class as the sum of the complexity scores of its methods. Each coverage criterion—i.e., branch, statement and strong mutation—is reported in a different graph.


Fig. 7.
DynaMOSA versus WSA and WS: Interaction between Â12 statistic, McCabe’s cyclomatic complexity, and number of targets (branches, statements, mutants).

Show All

For branch coverage (Fig. 7a), classes with Â12<0.50 (i.e., with WSA outperforming DynaMOSA) have low cyclomatic complexity (average < 51) and contain only few branches to cover (average <85). Vice versa, classes with Â12>0.50 (i.e., with DynaMOSA outperforming WSA) have a more variable complexity, ranging between 4 and 1,244, and number of branches varying between 17 and 7,938. Therefore, on this coverage criterion we observe a general trend for DynaMOSA to reach higher branch coverage (Â12> 0.50) regardless of class size and complexity.

For statement coverage (Fig. 7b), Â12> 0.50 scores are achieved mainly for classes with more than 500 statements, which represent 75 percent of the classes for which DynaMOSA achieves significantly higher statement coverage. For cyclomatic complexity, there is more variability since it ranges between 5 and 1,244. Only few circles (classes) fall on the left of the Â12=0.50 point. For those few cases, the average complexity is 20 while the total number of statements to cover is 404 on average.

Finally, for strong mutation coverage, the results are mixed: classes with both Â12<0.50 and Â 12>0.50 show variable complexity (y coordinate) as well as a variable number of mutants to kill (circle’s radius). Indeed, Fig. 7 c reports large circles on the left of point Â12= 0.50 indicating that there are classes with a large number of mutants for which WSA achieves better strong mutation scores. It is important to notice that for this criterion WSA prioritises the test cases to be run according to their execution time, while DynaMOSA does not do that. For classes with an extremely high number of mutants, this strategy helps in reducing the time consumed in each generation, thus, allowing the algorithm to spend more effort evolving the test suites. However, there are many more circles on the right of point Â12= 0.50, indicating that DynaMOSA outperforms WSA in many more cases even without using any prioritising strategy based on test case execution time.

To provide statistical support to the observations made above, we used a two-way permutation test  [4] to verify whether any interaction between the Â12 statistics, cyclomatic complexity and number of targets is statistically significant or not. The two-way permutation test is a non-parametric test equivalent to the two-way Analysis of Variance (ANOVA), thus, it does not make any assumption on data distributions. For this test, we use the implementation available in the lmPerm package for R. It uses an iterative procedure to compute the p-values, thus, it can produce different results over multiple runs when few iterations are used. For this reason, we set the number of iterations to 108.

The two-way permutation test shows that the Â12 scores are significantly influenced by cyclomatic complexity and number of targets for branch coverage (p-values ∈{0.04,0.02}), as well as by their combination (p-value = 0.02). Therefore, on this criterion DynaMOSA improves branch coverage over WSA especially for classes with high number of branches and/or high computational complexity. For statement coverage, the permutation test reveals a significant interaction between Â12 scores and cyclomatic complexity combined with number of targets (p-value = 0.02). However, the two factors taken alone do not significantly interact with the Â12 scores (p-values ∈{0.24,1.00}). Therefore, DynaMOSA provides better coverage scores (with Â12>0.50) especially for classes with high cyclomatic complexity and with large number of mutants/statements to be covered. For strong mutation coverage, the two-way permutation test shows that the Â 12 scores are not significantly influenced by cyclomatic complexity and number of targets for branch coverage (p-values ∈{0.98,0.35}), neither by their combination (p-value = 0.75).

6.3 Co-Factors Analysis for DynaMOSA versus MOSA
Fig. 8 reports the results of the interaction between McCabe’s cyclomatic complexity (y axis), number of coverage targets (circle’s radius), and Vargha-Delaney Â12 scores (x axis) for those classes with statistically significant differences between DynaMOSA and MOSA. For branch coverage ( Fig. 8a), classes with Â 12> 0.50 show a variable number of branches that ranges between 21 and 2,373, and a variable cyclomatic complexity ranging between 4 and 679. Vice versa, for the single class with Â12< 0.50 the corresponding circle has a small size (low number of branches) and low complexity scores. Therefore, for branch coverage DynaMOSA reaches higher coverage, with respect to MOSA, independently of the considered factors.


Fig. 8.
DynaMOSA versus MOSA: Interaction between Â12 statistics, McCabe’s cyclomatic complexity, and number of targets (branches, statements, mutants).

Show All

For statement coverage (Fig. 8b), Â12< 0.5 scores are achieved for classes with complexity ranging between 16 and 63, while the corresponding number of statements ranges between 152 and 1,436. However, these two factors have higher values for classes with Â12> 0.50. Indeed, for such classes the complexity values range between 10 and 775, while the number of statements to cover ranges between 179 and 7,809. Hence, for this criterion DynaMOSA tends to outperform MOSA for classes with both high complexity and large size.

For strong mutation, most of the classes in Fig. 8c have low complexity when Â12< 0.50 while for Â 12> 0.50 we observe a larger variability in terms of complexity. There is an extreme class on the right of the line Â12=0.50 with a very large complexity (755) for which DynaMOSA is more effective than MOSA. For what concerns the number of mutants (radius of circles), we do not observe any relevant differences between the data points on the left and on the right sides of line Â12=0.50. Thus, we do not observe any influence of the number of mutants to kill on the Â12 statistics.

To test the statistical significance of the interactions reported above, we used a two-way permutation test  [4]. According to this test, for branch coverage, Â12 scores obtained from the comparison between DynaMOSA and MOSA are not significantly influenced by cyclomatic complexity and by number of branches to cover (p -values ∈{0.73,0.16}), and neither from their combination (p-value = 0.13). Similarly, for statement coverage the permutation test reveals no interaction between Â12 statistics and both cyclomatic complexity (p-value = 0.14) and number of statements (p -value = 1.00). Moreover, there is no significant interaction between their combination and Â12 scores (p-value = 0.29). For strong mutation, only cyclomatic complexity has a marginal interaction with Â12 statistics ( p-value = 0.07) while there is no significant influence for the number of mutants ( p-value = 1.00) as well as for its combination with cyclomatic complexity (p-value = 0.92). Hence, DynaMOSA outperforms MOSA independently of size and complexity of the class under test for all three coverage criteria considered in this study.

SECTION 7Threats to Validity
Threats to construct validity regard the relation between theory and experimentation. The comparison among different search algorithms is based on performance metrics that are widely adopted in the literature: structural coverage, strong mutation scores, and running time. In the context of test case generation, these metrics give reasonable estimation of the effectiveness (coverage metrics) and efficiency (running time) of the test case generation techniques.

Threats to internal validity regard factors that could influence our results. To deal with the inherent randomness of GA, we repeated each execution 10 times and reported average performance together with rigorous statistical analysis to support our findings. Another potential threat arises from GA parameters. We used default parameter values suggested in related literature since they have been shown to give reasonably acceptable results as compared to fine-tuned settings [2]. All experimented algorithms are implemented in the same tool, thus, they share the same implementation for the genetic operators. This avoids potential confounding effects due to usage of different tools with different operator implementations.

Threats to conclusion validity stem from the relationship between the treatment and the outcome. In analysing the results of our experiments, we have used appropriate statistical tests coupled with enough repetitions of the experiments to enable the statistical tests. In particular, we have used the two-tailed Wilcoxon test and the two-way permutation test, two non-parametric tests that do not make any assumption on the data distributions being compared. We also use the Vargha-Delaney effect size statistics for estimating the magnitude of the observed difference. We drew conclusions only when results were statistically significant according to these tests.

Threats to external validity affect the generalisation of our results. We carried out experiments on 346 Java classes randomly taken from 116 open-source projects belonging to four different datasets  [17], [38], [45], [49]. These software projects have been used in many previous works on test case generation (e.g., [2], [18]). Moreover, in order to increase the generalisability of our findings, we evaluated all the algorithms with respect to three different, widely used, coverage criteria, i.e., branch coverage, statement coverage and strong mutation coverage.

SECTION 8Related Work
The application of search algorithms for test data and test case generation has been the subject of increasing research efforts. As a result, several techniques [5], [22], [23], [42], [52] and tools  [6], [7], [9], [26], [32], [37], [39], [53], [56] have been proposed in the literature.

Search-Based Approaches. Existing works on search-based test generation rely on the single objective formulation of the problem, as discussed in Section 2. In the literature, two variants of the single objective formulation can be found: (i) targeting one branch at a time [1], [19], [23], [33], and (ii) targeting all branches at once (whole-suite approach) [3], [16], [18]. The first variant (i.e., targeting one branch at a time) has been shown to be inferior to the whole-suite approach [3], [18], mainly because it is significantly affected by the inevitable presence of unreachable or difficult targets. Recently, Rojas et al. [44] further improved the whole-suite approach by incorporating some basic routines shared with our MOSA algorithm. The corresponding approach, is a hybrid strategy which combines the traditional evolution of test suites with test case level operators: (1) the usage of an archive to keep track of test cases (and not test suites) covering some targets; and (2) synthesising the final suite by taking test cases stored in the archive rather than picking up the best individual (candidate suite) from the last generation of GA. Since the archive based whole-suite approach (WSA) has been proved to outperform both the pure whole-suite and targeting one branch at a time approaches, we focused on WSA as a state-of-the-art representative of the single objective approach.

Multi-Objective Approaches. Although in the related literature there are previous works applying multi-objective approaches in evolutionary test data generation, they all considered structural coverage as a single objective, while other, domain-specific objectives have been added as further objectives the tester would like to achieve [1], [20], such as memory consumption [29], execution time  [36], test suite size [36], etc. For example, Harman et al. [21] proposed a search-based multi-objective approach in which the first objective is branch coverage (each coverage target is still targeted individually) and the second objective is the number of collateral targets that are accidentally covered. Ferrer et al. [15] proposed a multi-objective approach that considers two conflicting objectives: coverage (to be maximised) and oracle cost (to be minimised). They also used the targeting one branch at a time approach for the branch coverage criterion, i.e., their approach selects one branch at time and then runs GA to find the test case with minimum oracle cost that covers such a branch. Pinto and Vergilio [40] considered three different objectives when generating test cases: structural coverage criteria (targeting one branch at a time approach), ability to reveal faults, and execution time. Oster and Saglietti [36] considered two objectives to optimise: branch coverage (to be maximised) and number of test cases required to reach the maximum coverage (to be minimised). Lakhotia et al. [29] experimented bi-objective approaches, considering as objectives branch coverage and dynamic memory consumption for both real and synthetic programs. Even if they called this bi-objective formulation as multi-objective branch coverage , they still represent branch coverage as a single objective function, by considering one branch at a time. According to McMinn [35], there are several other potential non-coverage objectives that could be added to test case generation tools, such as minimising the oracle cost, maximising the test case diversity to increase the likelihood to expose more faults, etc.

It is important to notice that all previous multi-objective approaches for evolutionary test data generation used the targeting one branch at a time strategy [34]. The branch distance of a single targeted branch is one objective, considered with additional non-coverage objectives. From all these studies, there is no evidence that the usage of additional (non-coverage) objectives provides benefits in terms of coverage with respect to the traditional single-objective approach based on branch coverage alone. As reported by Ferrer et al. [15] the usage of such additional objectives can be even harmful for the final structural coverage. It is also important to highlight that the number of objectives considered in these studies remains limited to a relatively small number, being always ≤ 3.

Non-Evolutionary Approaches. Other than evolutionary approaches, other techniques have been proposed in the literature for test case and test data generation, such as dynamic symbolic execution  [7], [26], [53], [56] and random testing  [9], [32], [36], [37]. Dynamic symbolic execution techniques encode all constraints that should be satisfied to execute a particular path in a specific formula to be solved using constraint solvers. A solution to such a formula consists of method sequences and test data allowing to cover the corresponding path in the program [26]. Although symbolic execution has been widely applied in the literature [7], [53], there are several challenges to address for real-world programs  [14], such as: path explosion, complexity constraints , dependencies to external libraries, and paths related to exceptions. Eler et al. [14] performed a large-scale study on the SF110 corpus to identify factors that negatively impact symbolic execution techniques for object-oriented software. They observed that less than 10 percent of java methods in SF110 have only integer or floating-point data types. Therefore, most of constraints to solve are related to complex data types (e.g., objects) posing several challenges to constraint solvers [14]. Moreover, handling calls to external libraries limits the applicability of symbolic execution for real-world software projects [14] .

Direct comparison between evolutionary testing and random testing has been the subject of investigation of several researchers (e.g., [22], [45], [49], [52]) in the last decade. Most of these studies have shown that evolutionary testing outperforms random testing  [22], [45], [52], which usually fails to cover hard-to-reach branches that require a quite sophisticated search [22]. For example, Fraser and Arcuri  [17] conducted a large empirical study on open source projects (the SF110 corpus) and industrial projects and compared EvoSuite with Randoop [47], a popular random testing tool for Java. Their results showed that GAs (whole suite approach) lead to a large improvement over random testing [17], despite the presence of a large number of classes in SF110 that are trivial to cover. Similar results have been obtained by Shamshiri et al. [47]. On the SF110 corpus they compared the whole suite approach with a random search algorithm implemented in the same tool, i.e., EvoSuite. However, they observed that most of classes in SF110 are so trivial to cover that random search could generate test cases without much relative disadvantage for such a dataset.

Recently, Ma et al. [32] introduced an improved version of random testing, namely guided random testing (GRT). GRT extracts constants from the software under test via light-weight static analysis and reuses such knowledge to seed the generation process. Empirical results [32], [49] have shown GRT is able to reach competitive (and sometimes higher) coverage than the pure whole-suite approach in implemented EvoSuite. However, no empirical comparison has been performed between GRT and a more recent version of EvoSuite implementing WSA (archive-based whole suite approach), which won the latest SBST tool contest  [45].

Whereas a systematic comparison of evolutionary testing (e.g., WSA, DynaMOSA) with other techniques (e.g., GRT, Randoop, etc.) would be an interesting analysis to perform, it escapes the scope of this paper. In fact, the main goal of this paper is to determine a proper formulation of the test case generation problem in the context of evolutionary testing. As a consequence, we have presented a new many-objective solver for evolutionary-based techniques. Further investigations and comparisons with different categories of techniques are part of our future work agenda.

Our Paper. Although other existing techniques already target all branches/statements at the same time (e.g., WSA [44], GRT [32] , etc.), none of them consider such coverage targets as explicit objectives to optimise. In this paper, we regard coverage itself as a many-objective problem, since the goal is to minimise simultaneously the distances between the test cases and the uncovered structural targets in the class under test. In our previous ICST 2015 paper [38] we provided a first reformulation of branch coverage as a many-objective problem. In this paper, we refine the target selection mechanism and we dynamically add yet uncovered targets that are under direct control dependency of a covered condition. This allows us to reduce the number of objectives simultaneously active during test case evolution, which further increases the effectiveness and efficiency of the proposed approach. Such extension is especially useful when the number of targets is very high, as in mutation testing or when dealing with classes that have high cyclomatic complexity/size.

Our empirical results provide also evidence that attempting all coverage targets at the same time is not equivalent to applying a many-objective solver. In fact, all algorithms investigated in our paper already attempt all coverage targets at the same time: WSA and WS consider such targets as components of a test-suite level single function to optimise; MOSA and DynaMOSA (and its variants) use such targets as independent objectives. Our results show that there exists a statistically significant difference in code coverage among the aforementioned algorithms. In particular, we found that the usage a many-objective strategy to select the candidate tests during the search process plays a paramount role on both effectiveness and efficiency in test case generation (see RQ4 on the role of the preference criterion).

SECTION 9Conclusion and Future Work
We have reformulated the test case generation problem as a many-objective optimisation problem, where different coverage targets are considered as different objectives to be optimised. Our novel many-objective genetic algorithm, DynaMOSA, exploits the peculiarities of coverage testing with respect to traditional many-objective problems to overcome scalability issues when dealing with hundreds of objectives (coverage targets). In particular, (i) it includes a new preference criterion that gives higher priority to a subset of Pareto optimal solutions (test cases), hence increasing the selective pressure; (ii) it dynamically focuses the search on a subset of the yet uncovered targets, based on the control dependency hierarchy.

Our empirical study, conducted on 346 Java classes extracted from 117 java projects belonging to four different datasets, shows that the proposed algorithm, DynaMOSA, yields strong, statistically significant improvements for coverage with respect to its predecessor, MOSA, and the whole-suite approach. Specifically, the improvements can be summarised as follows: coverage is significantly higher with respect to WSA in 28 percent of the classes for branch and 27 percent strong mutation; for these classes, DynaMOSA leads to +8 and +11 percent more coverage respectively. The comparison with WS shows a significant improvement for statement coverage in 51 percent of classes, for which the average improvement is 11 percent. Finally, DynaMOSA outperforms its predecessor MOSA in 12 percent of the classes with an average coverage improvement of 8 percent. Consistent results have been obtained across all three coverage criteria—i.e., branch coverage, statement coverage and strong mutation coverage. On classes with no improvement in coverage, DynaMOSA converges more quickly, especially when limited time is given to the search. Therefore, we conclude that many-objective algorithms can be applied to test case generation, but they need to be suitably customised to deal with the hundreds and thousands of targets/objectives that are typical of test case generation.

Given the results reported in this paper, there are a few potential directions for future works. First of all, we intend to incorporate also non-coverage criteria within our many-objective algorithm, such as execution time  [40] and memory consumption [29] . We also plan to further improve our preference criterion by developing adaptive strategies to enable/disable the test size (secondary non-coverage criterion) to avoid genetic drift, to enable/disable Pareto-based ranking and to activate the upper bound strategy when the number of uncovered targets/objectives is extremely high.

Recent research endeavours have resulted in improved variants of random testing techniques (e.g.,  [32]) which have been shown to be effective alternatives for test data generation, as opposed to being mere baselines for comparing other techniques. However, such effectiveness needs to be investigated with respect to the nature of the program under test, in particular, when the program under test exhibits complex structures. In future work, we plan to design a comprehensive experimental evaluation for investigating the comparative pros and cons of random testing techniques with respect to our DynaMOSA.

Finally, we plan to investigate the performance of our many-objective algorithm when combining multiple coverage criteria at the same time compared to the sum-scalarization strategy used by Rojas et al.  [47] for WS and WSA.