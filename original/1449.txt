Software defect prediction, which predicts defective code regions, can assist developers in finding bugs and prioritizing their testing efforts. Traditional defect prediction features often fail to capture the semantic differences between different programs. This degrades the performance of the prediction models built on these traditional features. Thus, the capability to capture the semantics in programs is required to build accurate prediction models. To bridge the gap between semantics and defect prediction features, we propose leveraging a powerful representation-learning algorithm, deep learning, to learn the semantic representations of programs automatically from source code files and code changes. Specifically, we leverage a deep belief network (DBN) to automatically learn semantic features using token vectors extracted from the programs' abstract syntax trees (AST) (for file-level defect prediction models) and source code changes (for change-level defect prediction models). We examine the effectiveness of our approach on two file-level defect prediction tasks (i.e., file-level within-project defect prediction and file-level cross-project defect prediction) and two change-level defect prediction tasks (i.e., change-level within-project defect prediction and change-level cross-project defect prediction). Our experimental results indicate that the DBN-based semantic features can significantly improve the examined defect prediction tasks. Specifically, the improvements of semantic features against existing traditional features (in F1) range from 2.1 to 41.9 percentage points for file-level within-project defect prediction, from 1.5 to 13.4 percentage points for file-level cross-project defect prediction, from 1.0 to 8.6 percentage points for change-level within-project defect prediction, and from 0.6 to 9.9 percentage points for change-level cross-project defect prediction.
SECTION 1Introduction
Software defect prediction techniques [22], [29], [31], [41], [45], [53], [59], [62], [80], [95], [111] have been proposed to detect defects and reduce software development costs. Defect prediction techniques build models using software history data and use the developed models to predict whether new instances of code regions, e.g., files, changes, and methods, contain defects.

The efforts of previous studies toward building accurate prediction models can be categorized into the two following approaches: The first approach is manually designing new features or new combinations of features to represent defects more effectively, and the second approach involves the application of new and improved machine learning based classifiers. Researchers have manually designed many features to distinguish defective files from non-defective files, e.g., Halstead features [19] based on operator and operand counts; McCabe features [50] based on dependencies; CK features [8] based on function and inheritance counts, etc.; MOOD features [21] based on polymorphism factor, coupling factor, etc.; process features [29], [80] (including number of lines of code added, removed, meta features, etc.); and object-oriented features [3], [11], [49].

Traditional features mainly focus on the statistical characteristics of programs and assume that buggy and clean programs have distinguishable statistical characteristics. However, our observations on real-world programs show that existing traditional features often cannot distinguish programs with different semantics. Specifically, program files with different semantics can have traditional features with similar or even the same values. For example, Fig. 1 shows an original buggy version, i.e., Fig. 1a, and a fixed clean version, i.e., Fig. 1b, of a method from Lucene. In the buggy version, there is an IOException when initializing variables os and is before the try block. The buggy version can lead to a memory leak1 and has already been fixed by moving the initializing statements into the try block in Fig. 1b. Using traditional features to represent these two code snippets, e.g., code complexity features, their feature vectors are identical. This is because these two code snippets have the same source code characteristics in terms of complexity, function calls, raw programming tokens, etc. However, the semantic information in these two code snippets is significantly different. Specifically, the contextual information of the two variables, i.e., os and is, in the two versions is different. Features that can distinguish such semantic differences are needed for building more accurate prediction models.


Fig. 1.
A motivating example from Lucene.

Show All

To bridge the gap between the programs’ semantic information and defect prediction features, we propose leveraging a powerful representation-learning algorithm, namely, deep learning [27], to learn semantic representations of programs automatically. Specifically, we use the deep belief network (DBN) [26] to automatically learn features from token vectors extracted from source code, and then we utilize these features to build and train defect prediction models.

DBN is a generative graphical model, which learns a semantic representation of the input data that can reconstruct the input data with a high probability as the output. It automatically learns high-level representations of data by constructing a deep architecture [4]. There have been successful applications of DBN in many fields, including speech recognition [58], image classification [9], [43], natural language understanding [56], [86], and semantic search [85].

To use a DBN to learn features from code snippets, we first convert the code snippets into vectors of tokens with the structural and contextual information preserved, and then we use these vectors as the input into the DBN. For the two code snippets presented in Fig. 1, the input vectors are [..., IndexOutput, createOutput(), IndexInput, openInput(), IOException, try, ...] and [..., IndexOutput, IndexInput, IOException, try, createOutput(), openInput()...] respectively (details regarding the token extraction are provided in Section 3.1). As the vectors of these two code snippets are different, the DBN will automatically learn features that can distinguish them.

We examine our DBN-based approach to generating semantic features on both file-level defect prediction tasks (i.e., predict which files in a release are buggy) and change-level defect prediction tasks (i.e., predict whether a code commit is buggy), because most of the existing approaches to defect prediction are on these two levels [2], [24], [29], [67], [82], [90], [94], [102], [103]. Focusing on these two different defect prediction tasks enables us to extensively compare our proposed technique with state-of-the-art defect prediction features and techniques. For file-level defect prediction, we generate DBN-based semantic features by using the complete Abstract Syntax Trees (AST) of the source files, while for change-level defect prediction, we generate the DBN-based features by using tokens extracted from code changes, as detailed in Section 3.

In addition, most defect prediction studies have been conducted in one or two settings, i.e., within-project defect prediction [29], [57], [90], [103] and/or cross-project defect prediction [24], [67], [94], [102]. Thus, we evaluate our approach in these two settings as well.

In this work, we explore the performance of the DBN-based semantic features using different measures under different evaluation scenarios. We first evaluate the prediction performance by using Precision, Recall, and F1, as they are commonly used evaluation measures in defect prediction studies [2], [67], [82], [96], which we refer to as the non-effort-aware scenario in this work. In addition, we also conduct an effort-aware evaluation [52] to show the practical aspect of defect prediction by using PofB20, i.e., the percentage of bugs that can be discovered by inspecting 20 percent lines of code (LOC) [29]. For example, when a team can afford to inspect only 20 percent LOC before a deadline, it is crucial to inspect the 20 percent that can assist the developers in discovering the highest number of bugs.

This paper makes the following contributions:

Shows the incapability of traditional features in capturing the semantic information of programs.

Proposes a new technique to leverage a powerful representation-learning algorithm, deep learning, to learn semantic featuresfrom token vectors extracted from programs’ ASTs (for file-level defect prediction models) and source code changes (for change-level defect prediction models) automatically.

Conducts rigorous and large-scale experiments to evaluate the performance of the DBN-based semantic features for defect prediction tasks under both the non-effort-aware and effort-aware scenarios; and

Demonstrates that DBN-based semantic features can significantly improve defect prediction. Specifically, the improvements of semantic features against existing traditional features (in F1) range from 2.1 to 41.9 percentage points for file-level within-project defect prediction, from 1.5 to 13.4 percentage points for file-level cross-project defect prediction, from 1.0 to 8.6 percentage points for change-level within-project defect prediction, and from 0.6 to 9.9 percentage points for change-level cross-project defect prediction.

The rest of this paper is summarized as follows. Section 2 provides the backgrounds on defect prediction and DBN. Section 3 describes our approach to learning semantic features followed by leveraging the learned features to predict defects. Section 4 presents the experimental setup. Section 5 evaluates the performance of the learned semantic features. Section 6 discusses our results and threats to the validity. Section 7 surveys the related work. Section 8 summarizes our study. This paper extends our prior publication [96] presented at the 38th International Conference on Software Engineering (ICSE'16). New materials with respect to the conference version include:

Examining the effectiveness of the proposed approach for generating semantic features on two change-level defect prediction tasks, i.e., change-level within-project defect prediction (WCDP) and change-level cross-project defect prediction (CCDP). The details are presented in Section 4.5.2 (experimental design), Section 4.8 (WCDP), Section 4.9 (CCDP), Section 5.3 (results of WCDP), and Section 5.4 (results of CCDP).

New techniques to process incomplete code from source code changes for generating DBN-based features automatically are proposed. The details are presented in Section 3.1.2.

The model performance assessment scenarios are updated. Both the non-effort-aware and effort-aware evaluation processes are employed to comprehensively evaluate the performance of the DBN-based semantic features on file-level and change-level defect prediction tasks. The details are presented in Section 5.

New experiments on open-source commercial projects and additional details regarding the experimental design and results are provided. In addition, statistical testing and Cliff's delta effect size analysis are conducted to measure and demonstrate the significance of the prediction performance of the DBN-based semantic features. The details are provided in Section 5.

SECTION 2Background
This section provides the backgrounds of file-level and change-level defect prediction and deep belief network. Table 1 shows the investigated prediction tasks and their corresponding abbreviations.

TABLE 1 Defect Prediction Tasks Investigated in This Work

2.1 File-level Defect Prediction
Fig. 2 presents a typical file-level defect prediction process that is adopted by existing studies [31], [45], [55], [66], [67], [76], [99]. The first step is to label the data as buggy or clean based on post-release defects for each file. One could collect these post-release defects from a Bug Tracking System (BTS) via linking bug reports to its bug-fixing changes. Files related to these bug-fixing changes are considered as buggy. Otherwise, the files are labeled as clean. The second step is to collect the corresponding traditional features of these files. Instances with features and labels are used to train machine learning classifiers. Finally, trained models are used to predict new instances as buggy or clean.


Fig. 2.
Defect prediction process.

Show All

We refer to the set of instances used for building models as a training set, whereas the set of instances used to evaluate the trained models is referred to as a test set. As shown in Fig. 2, when performing within-project defect prediction (following existing work [66], we call this WPDP), the training and test sets are from the same project, i.e., project A. When performing cross-project defect prediction (following existing work [66], we call this CPDP), the prediction models are trained by a training set from project A (source), and a test set is from a different project, i.e., project B (target).

In this study, for file-level defect prediction, we examine the performance of the learned DBN-based semantic features on both WPDP and CPDP.

2.2 Change-Level Defect Prediction
Change-level defect prediction can predict whether a change is buggy at the time of the commit so that it allows developers to act on the prediction results as soon as a commit is made. In addition, since a change is typically smaller than a file, developers have much less code to examine in order to identify defects. However, for the same reason, it is more difficult to predict buggy changes accurately.

Similar to file-level defect prediction, change-level defect prediction also consists of the following processes:

Labeling process: Labeling each change as buggy or clean to indicate whether the change contains bugs.

Feature extracting process: Extracting the features to represent the changes.

Model building and testing process: Building a prediction model with the features and labels and then using the model to predict testing data.

Different from labeling file-level defect data, labeling change-level defect data requires further linking of bug-fixing changes to bug-introducing changes. A line that is deleted or changed by a bug-fixing change is a faulty line, and the most recent change that introduced the faulty line is considered a bug-introducing change. We could identify the bug-introducing changes by a blame technique provided by a Version Control System (VCS), e.g., git or SZZ algorithm [40]. Such blame techniques are widely used in existing studies [29], [40], [57], [90], [106]. In this work, the bug-introducing changes are considered as buggy, and other changes are labeled clean. Note that, not all projects have a well maintained BTS, and we consider changes whose commit messages contain the keyword “fix” as bug-fixing changes by following existing studies [29], [90].

In this work, similar to the file-level defect prediction, we also examine the performance of DBN-based features on both change-level within-project defect prediction and change-level cross-project defect prediction.

2.3 Deep Belief Network
A deep belief network is a generative graphical model that uses a multi-level neural network to learn a representation from the training data that could reconstruct the semantic and content of the training data with a high probability [4]. DBN contains one input layer and several hidden layers, and the top layer is the output layer that contains final features to represent input data as shown in Fig. 3. Each layer consists of several stochastic nodes. The number of hidden layers and the number of nodes in each layer vary depending on users’ demand. In this study, the size of learned semantic features is the number of nodes in the top layer. The idea of DBN is to enable the network to reconstruct the input data using generated features by adjusting weights between nodes in different layers.

Fig. 3. - 
Deep belief network architecture and input instances of the buggy version and the clean version presented in Fig. 1. Although the token sets of these two code snippets are identical, the different structural and contextual information between tokens enables DBN to generate different features to distinguish them.
Fig. 3.
Deep belief network architecture and input instances of the buggy version and the clean version presented in Fig. 1. Although the token sets of these two code snippets are identical, the different structural and contextual information between tokens enables DBN to generate different features to distinguish them.

Show All

DBN models the joint distribution between input layer and the hidden layers as follows:
P(x,h1,…,hl)=P(x|h1)(∏k=1lP(hk|hk+1)),(1)
View Sourcewhere x is the data vector from input layer, l is the number of hidden layers, and hk is the data vector of kth layer (1≤k≤l). P(hk|hk+1) is a conditional distribution for the adjacent k and k+1 layers.

To calculate P(hk|hk+1), each pair of two adjacent layers in DBN are trained as a Restricted Boltzmann Machines (RBM) [4]. An RBM is a two-layer, undirected, bipartite graphical model where the first layer consists of observed data variables, referred to as visible nodes, and the second layer consists of latent variables, referred to as hidden nodes. P(hk|hk+1) can be efficiently calculated as
P(hk|hk+1)=∏j=1nkP(hkj|hk+1)(2)
View Source
P(hkj=1|hk+1)=sigm(bkj+∑a=1nk+1Wkajhk+1a),(3)
View Sourcewhere nk is the number of nodes in layer k, sigm(c)=11+e−c, b is a bias matrix, bkj is the bias for node j of layer k, and Wk is the weight matrix between layer k and layer k+1. sigm is the sigmod function, which serves as the activation function to update the hidden units. We use the sigmod function because it outputs a more smooth range of nonlinear values with a relatively simple computation [20].

DBN automatically learns W and b matrices using an iteration process. W and b are updated via log-likelihood stochastic gradient descent
Wij(t+1)=Wij(t)+η∂log(P(v|h))∂Wij(4)
View Source
bok(t+1)=bok(t)+η∂log(P(v|h))∂bok,(5)
View Sourcewhere t is the tth iteration, η is the learning rate, P(v|h) is the probability of the visible layer of an RBM given the hidden layer, i and j are two nodes in different layers of the RBM, Wij is the weight between the two nodes, and bok is the bias on the node o in layer k.

To train the network, one first initializes all W matrices between two layers via RBM and sets the biases b to 0. They can be well-tuned with respect to a specific criterion, e.g., the number of training iterations, error rate between reconstructed input data and original input data. In this study, we use the number of training iterations as the criterion for tuning W and b. The well-tuned W and b are used to set up a DBN for generating semantic features for both training and test data. Also, we discuss how these parameters affect the performance of learned semantic features in Section 4.5.

The DBN model generates features with more complex network connections. These network connections enable DBN models to generate features with multiple levels of abstraction and high-level semantics. DBN features are weighted combinations/vectors of input nodes, which may represent patterns of the usages of input nodes (e.g., methods, control-flow nodes, etc.). We believe such DBN-based features can help distinguish the semantics of different source code snippets, which traditional features cannot handle well. For example, Fig. 4 shows the distribution of the DBN-based semantic features of the two code snippets shown in Fig. 1. Specifically, we use the trained DBN model on project Lucene (details are in Section 4.8) to generate a feature set that contains 50 different features for each of the two code snippets. As we can see in the figure, the distributions of features of the two code snippets are different. Specifically, most of the features of code snippet shown in Fig. 1b have larger values than those of the features of code snippet shown in Fig. 1a. Thus, the new features are capable of distinguishing these two code snippets with a proper classifier.


Fig. 4.
The distribution of DBN-based features of the two code snippets shown in Fig. 1.

Show All

SECTION 3Approach
In this work, we use DBN to generate semantic features automatically from source files and code changes and further leverage these features to improve defect prediction. Fig. 5 illustrates the workflow of our approach to generating features for both file-level defect prediction (inputs are source files) and change-level defect prediction (inputs are source code changes). Specifically, for file-level defect prediction, our approach takes AST node tokens from the source code of the training and test source files as the input, and generates semantic features from them. Then, the generated semantic features are used to build the models for predicting defects. Note that for change-level defect prediction, the input data to our DBN-based feature generation approach are changed code snippets. Since building AST for an incomplete code snippet is challenging, in this work we propose a heuristic approach to extracting important structural and context information from code change snippets (details are in Section 3.1.2). The DBN requires input data in the form of integer vectors, to satisfy this requirement, we first build a mapping between integers and tokens and then convert the token vectors to integer vectors, to generate semantic features, we first use the integer vectors of the training set to build and train a DBN. Then, we use the trained DBN to automatically generate semantic features from the integer vectors of the training and test sets. Finally, based on the generated semantic features, we build defect prediction models from the training set, and evaluate their performance on the test set.


Fig. 5.
Overview of our DBN-based approach to generating semantic features for file-level and change-level defect prediction.

Show All

Our approach consists of four major steps: 1) parsing source code (source files for file-level defect prediction and changed code snippets for change-level defect prediction) into tokens, 2) mapping tokens to integer identifiers, which are the expected inputs to the DBN, 3) leveraging the DBN to automatically generate semantic features, and 4) building defect prediction models and predicting defects using the learned semantic features of the training and test data.

3.1 Parsing Source Code
3.1.1 Parsing Source Code for Files
For file-level defect prediction tasks, we utilize the Java Abstract Syntax Tree to extract syntactic information from source code files. Specifically, three types of AST node are extracted: 1) nodes of method invocations and class instance creations, e.g., in Fig. 3, method createOutput() and openInput() are recorded as their method names, 2) declaration nodes, i.e., method declarations, type declarations, and enum declarations, and 3) control-flow nodes such as while statements, catch clauses, if statements, throw statements, etc. Control-flow nodes are recorded as their statement types, e.g., an if statement is simply recorded as if. In summary, for each file, we obtain a vector of tokens of the three categories. We exclude AST nodes that are not one of these three categories, such as assignment and intrinsic type declaration, because they are often method-specific or class-specific, which may not be generalizable to the whole project. Adding them may dilute the importance of other nodes.

Since the names of methods, classes, and types are typically project-specific, methods of an identical name in different projects are either rare or of different functionalities. Thus, for cross-project defect prediction, we extract all three categories of AST nodes, but for the AST nodes in categories 1) and 2), instead of using their names, we use their AST node types such as method declarations and method invocations. Take project xerces as an example. As an XML parser, it consists of many methods named getXXX and setXXX, where XXX refers to XML-specific keywords including charset, type, and href. Each of these methods contains only one method invocation statement, which is in form of either getAttribute(XXX) or setAttribute(XXX). Methods getXXX and setXXX do not exist in other projects, while getAttribute(XXX) and setAttribute(XXX) have different meanings in other projects, so using the names getAttribute(XXX) or setAttribute(XXX) is not helpful. However, it is useful to know that method declaration nodes exist, and only one method invocation node is under each of these method declaration nodes, since it might be unlikely for a method with only one method invocation inside to be buggy. In this case, compared with using the method names, using the AST node types method declaration and method invocation is more useful since they can still provide partial semantic information.

3.1.2 Parsing Source Code for Changes
Different from file-level defect prediction data, i.e., program source files, for which we could build ASTs and extract AST token vectors for feature generation, change-level defect prediction data are changes that developers made to source files, whose syntax information is often incomplete. These changes could have different locations and include code additions and code deletions, which are syntactic incomplete. Thus, building ASTs for these changes is challenging. In this study, for tokenizing changes, instead of building ASTs, we tokenize a change by considering the code addition, the code deletion, and the context code in the change. Code additions are the added lines in a change, code deletions are the deleted lines in a change, and the code around these additions or deletions is considered the context code. For example, Fig. 6 shows a real change example from project Lucene. In this change, the code addition contains lines 15 and 16, the code deletion contains lines 7 to 14, and the context contains lines 4 to 6, 17, and 18. Note that the contents of the source code lines in the additions, deletions, and context code are often overlapping, e.g., the deleted line 7 and the added line 15 contain the same line of code for class instance creation, i.e., SolrIndexSearcher.GroupCommandFunc gc;. Thus, to distinguish these lines, we add different prefixes to the raw tokens that are extracted from different types of changed code. Specifically, for the addition, we use prefix “added_”, for the deletion, we use prefix “deleted_”, and for the context code, we use prefix “context_”. The details of the three types of tokens extracted from the example change (in Fig. 6) are shown in Table 2.


Fig. 6.
A change example from Lucene (commit id is 9535bb795f6d1ec4c475a5d35532f3c7951101da).

Show All

TABLE 2 Three Types of Tokens Extracted from the Example Change Shown in Fig. 6
Table 2- 
Three Types of Tokens Extracted from the Example Change Shown in Fig. 6
From Table 2, we could observe that different types of tokens from the changed code snippets contain different information. For example, the context nodes show that the code is changed inside a for loop, an if statement is removed from the source code in the deletions, and an instantiation of class GroupCommandFunc was created in the additions.

Intuitively, DBN-based features generated from different types of tokens may have different impacts on the performance of the change-level defect prediction. To extensively explore the performance of different types of tokens, we build and evaluate change-level defect prediction models with seven different combinations among the three different types of tokens, i.e., added: only considers the additions; deleted: only considers the deletions; context: only considers the context information; added+deleted: considers both the additions and the deletions; added+context: considers both the additions and the context tokens; deleted+context: considers both the deletions and the context tokens; and added+deleted+context: considers the additions, deletions, and context tokens together. We discuss the effectiveness of these different combinations in Section 5.

Note that some of the tokens extracted from the changed code snippets are project-specific, which means that they are rare or never appear in changes from a different project. Thus, for change-level cross-project defect prediction we first filter out variable names, and then use method declaration, method invocation, and class instantiation to represent a method declaration, a method call, and an instance of a class instantiation respectively.

3.2 Handling Noise and Mapping Tokens
3.2.1 Handling Noise
Defect data are often noisy and suffer from the mislabeling problem. Studies have shown that such noises could significantly erode the performance of defect prediction [25], [39], [92]. To prune noisy data, Kim et al. proposed an effective mislabeling data detection approach named Closest List Noise Identification (CLNI) [39]. It identifies the k-nearest neighbors for each instance and examines the labels of its neighbors. If a certain number of neighbors have opposite labels, the examined instance will be flagged as noise. However, such an approach cannot be directly applied to our data because their approach is based on the euclidean Distance of traditional numerical features. Since our features are semantic tokens, the difference between the values of two features only indicates that these two features are of different tokens.

To detect and eliminate mislabeling data and to help the DBN learn the common knowledge between the semantic information of buggy and clean instances, we adopt the edit distance similarity computation algorithm [68] to define the distances between instances. The edit distances are sensitive to both the tokens and the order among the tokens. Given two token sequences A and B, the edit distance d(A,B) is the minimum-weight series of edit operations that transform A to B. The smaller d(A,B) is, the more similar A and B are.

Based on edit distance similarity, we deploy CLNI to eliminate data with potential incorrect labels. In this study, since our purpose is not to find the best training or test set, we do not spend too much effort on well tuning the parameters of CLNI. We use the recommended parameters and find them to work well. In our benchmark experiments with traditional features, we also perform CLNI to remove the incorrectly labeled data.

In addition, we also filter out infrequent tokens extracted from the source code, which might be designed for a specific file and cannot be generalized to other files. Given a project, if the total number of occurrences of a token is less than three, we filter it out. We encode only the tokens that occur three or more times, which is a common practice in the NLP research field [48]. The same filtering process is also applied to change-level prediction tasks.

3.2.2 Mapping Tokens
DBN takes only numerical vectors as inputs, and the lengths of the input vectors must be the same. To use the DBN to generate semantic features, we first build a mapping between integers and tokens, and encode token vectors to integer vectors. Each token has a unique integer identifier. Since our integer vectors may have different lengths, we append 0 to the integer vectors to make all the lengths consistent and equal to the length of the longest vector. Adding zeros does not affect the results, and it is simply a representation transformation to make the vectors acceptable by the DBN. Taking the code snippets in Fig. 3 as an example, if we only consider the two versions, the token vectors for the “Buggy” and “Clean” versions would be mapped to [1, 2, 3, 4, 5, 6, ...] and [1, 3, 5, 6, 2, 4, ...] respectively. Through this encoding process, the method invocation information and inter-class information are represented as integer vectors. In addition, some program structure information is preserved since the order of tokens remains unchanged. Note that, in this work we employ the same token mapping mechanism for both the file-level and change-level defect prediction tasks.

3.3 Training the DBN and Generating Features
3.3.1 Training the DBN
As we discussed in Section 2, to train an effective DBN for learning semantic features, we need to tune three parameters, which are: 1) the number of hidden layers, 2) the number of nodes in each hidden layer, and 3) the number of training iterations. Existing studies that leveraged DBN models to generate features for NLP [86], [87] and image recognition [9], [43] reported that the performance of DBN-based features is sensitive to these parameters. A few hidden layers can be trained in a relatively short period of time, but result in poor performance as the system cannot fully capture the characteristics of the training datasets. Too many layers may result in overfitting and a slow learning time. Similar to the number of hidden layers, too few or too many hidden nodes or iterations result in either slow learning or poor performance [87]. We show how we tune these parameters in Section 4.5.

To simplify our model, we set the number of nodes to be the same in each layer. Through these hidden layers and nodes, DBN obtains characteristics that are difficult to observe but are capable of capturing semantic differences. For each node, the DBN learns the probabilities of traversing from this node to the nodes of its top level. Through back-propagation validation, the DBN reconstructs the input data using generated features by adjusting the weights between nodes in different hidden layers.

The DBN requires the values of the input data to range from 0 to 1, while the data in our input vectors can have any integer values due to our mapping approach. To satisfy the input range requirement, we normalize the values in the data vectors of the training and test sets by using min-max normalization [101]. In our mapping process, the integer values for different tokens are just identifiers. One token with a mapping value of 1 and one token with a mapping value of 2 only means that these two nodes are different and independent. Thus, the normalized values can still be used as token identifiers since the same identifiers pertain the same normalized values.

3.3.2 Generating Features
After we train a DBN, both the weights w and the biases b (details are in Section 2) are fixed. We input the normalized integer vectors of the training data and the test data into the DBN, and then obtain semantic features for the training and the test data from the output layer of the DBN.

3.4 Building Models and Performing Defect Prediction
After we obtain the generated semantic features for each instance from both the training and the test datasets, we then build defect prediction models by following the standard defect prediction process described in Section 2. The test data are used to evaluate the performance of the built defect prediction models.

Note that, as revealed in existing work [90], [91], the widely used validation technique, i.e., k-fold cross-validation often introduces nontrivial bias for evaluating defect prediction models, which makes the evaluation inaccurate. In addition, for change-level defect prediction, the k-fold cross-validation may make the evaluation incorrect. This is because the changes follow a certain order in time. Randomly partitioning the dataset into k folds may cause a model to use future knowledge which should not be known at the time of prediction to predict changes in the past. Thus, cross-validation may use information regarding a change committed in 2017 to predict whether a change committed in 2015 is buggy or clean. This scenario would not be a real case in practice, because at the time of prediction, which is typically soon after the change is committed in 2015 for the earlier detection of bugs, the change committed in 2017 is not yet existent.

To avoid the above validation problem, we do not use the k-fold cross-validation in this work. Specifically, for file-level defect prediction, we evaluate the performance of our DBN-based features and traditional features by building prediction models with data from different releases. For change-level defect prediction, we collect the training and test datasets following the time order (details are in Section 4.3.2) to build and evaluate the prediction models without k-fold cross-validation.

SECTION 4Experimental Setup
In this section, we describe the detailed settings for our evaluation experiments. All experiments are run on a 2.5 GHz i5-3210M machine with 4 GB RAM.

4.1 Research Questions
Table 3 lists the scenarios for the investigated research questions. Specifically, we evaluate the performance of our DBN-based semantic features by comparing it with traditional defect prediction features under each of the four different prediction scenarios. These questions share the following format.

TABLE 3 Research Questions Investigated in This Work
Table 3- 
Research Questions Investigated in This Work
RQi (1≤i≤4): Do DBN-based semantic features outperform traditional features at the <level> <scope> under the non-effort-aware and effort-aware evaluation scenarios?

For example, in RQ1, we explore the effectiveness of the DBN-based semantic features for within-project defect prediction at the file-level under both the non-effort-aware and effort-aware evaluation scenarios.

4.2 Evaluation Metrics
4.2.1 Metrics for Non-effort-aware Evaluation
Under the non-effort-aware scenario, we use three metrics: Precision, Recall, and F1. These metrics have been widely adopted to evaluate defect prediction techniques [31], [54], [55], [67], [90], [111]. Here is a brief introduction
Precision=true positivetrue positive+false positive(6)
View SourceRight-click on figure for MathML and additional features.
Recall=true positivetrue positive+false negative(7)
View SourceRight-click on figure for MathML and additional features.
F1=2∗Precision∗RecallPrecision+Recall.(8)
View SourcePrecision and recall are composed of three numbers in terms of true positive, false positive, and false negative. True positive is the number of predicted defective files (or changes) that are truly defective, while false positive is the number of predicted defective ones that are actually not defective. A false negative records the number of predicted non-defective files (or changes) that are actually defective. Higher precision is demanded by developers who do not want to waste their debugging efforts on the non-defective code, while higher recall is often required for mission-critical systems, e.g., revealing additional defects [111]. However, comparing defect prediction models by using only these two metrics may be incomplete. For example, one could simply predict all instances as buggy instances to achieve a recall score of 1.0 (which will likely result in a low precision score) or only classify the instances with higher confidence values as buggy instances to achieve a higher precision score (which could result in a low recall score). To overcome the above issues, we also use the F1 score (i.e., F1), which is the harmonic mean of precision and recall, to measure the performance of the defect prediction.

4.2.2 Metrics for Effort-aware Evaluation
For effort-aware evaluation, we employ PofB20 [29] to measure the percentage of bugs that a developer can identify by inspecting the top 20 percent lines of code.

To calculate PofB20, we first sort all the instances in the test dataset based on the confidence levels (i.e., probabilities of being predicted as buggy) that a defect prediction model generates for each instance. This is because an instance with a higher confidence level is more likely to be buggy. We then simulate a developer that inspects these potentially buggy instances. We accumulate the lines of code that are inspected and the number of bugs identified. The process will be terminated when 20 percent of the LOC in the test data have been inspected and the percentage of bugs that are identified is referred to as the PofB20 score. A higher PofB20 score indicates that a developer can detect more bugs when inspecting a limited number of LOC.

4.2.3 Statistical Tests
Statistical tests can help understand whether there is a statistically significant difference between two results. In this work, we used the Wilcoxon signed-rank test to check whether the performance difference between prediction models with DBN-based semantic features and prediction models with traditional features is significant. For example, in RQ3, we want to compare the performance of DBN-based features and traditional features for change-level within-project defect prediction for the projects listed in Table 6. To conduct the Wilcoxon signed-rank test, we first run experiments with these two sets of features and obtain prediction results for each test subject. We then apply the Wilcoxon signed-rank test on the results of the test subjects. The Wilcoxon signed-rank test does not require the underlying data to follow any distribution. In addition, it can be applied to pairs of data and is able to compare the difference against zero. At the 95 percent confidence level, p-values that are less than 0.05 indicate that the difference between subjects is statistically significant, while p-values that are 0.05 or larger indicate that the difference is not statistically significant.

4.2.4 Cliff's Delta Effect Size Analysis
To further examine the effectiveness of our DBN-based features, following the existing work in [64], [102], we employ Cliff's delta (δ) [10] to measure the effect size of our approach. Cliff's delta is a non-parametric effect size measure that quantifies the amount of difference between two approaches. In this work, we use Cliff's delta to compare the defect prediction models that are built with our DBN-based features to the defect prediction models that are built with traditional features. Cliff's delta is computed using the formula delta=(2W/mn)−1, where W is the W statistic of the Wilcoxon rank-sum test, and m and n are the sizes of the result distributions of two compared approaches. The delta values range from -1 to 1, where δ=−1 or 1 indicates the absence of an overlap between the performances of the two compared models (i.e., all F1 values from one prediction model are higher than the F1 values of the other prediction model, and vice versa), while δ=0 indicates that the two prediction models completely overlap. Table 4 describes the meanings of the different Cliff's delta values [10].

TABLE 4 Cliff's Delta and the Effectiveness Level [10]

4.3 Evaluated Projects and Data Sets
In this work, we use different datasets for evaluating file-level and change-level defect prediction tasks. Specifically, for evaluating the performance of DBN-based features on file-level defect prediction, we use publicly available data from the PROMISE data repository, which are widely used for evaluating file-level defect prediction models [24], [31], [66], [67], [102]. For change-level defect prediction, we adopt the dataset from previous studies [29], [90], [103].

The main reason for adopting different datasets for file-level and change-level defect prediction tasks is that using existing widely used datasets enables us to directly compare our approach with existing defect prediction models on the same datasets, which makes the comparison more reliable.

4.3.1 Evaluated Projects for File-Level Defect Prediction
To facilitate the replication and verification of our experiments, we use publicly available data from the PROMISE data repository. Specifically, we select all the Java projects from PROMISE2 whose version numbers are provided. We need the version numbers of each project because we need its source code archive to extract token vectors from the ASTs of the source code to feed our DBN-based feature generation approach. In total, 10 Java projects are collected. Table 5 lists the versions, the average number of source files (excluding test files), and the average buggy rate of each project. The average number of files of the projects ranges from 122 to 815, and the buggy rates of the projects have a minimum value of 9.4 percent and a maximum value of 62.9 percent.

TABLE 5 Evaluated Projects for File-Level Defect Prediction
Table 5- 
Evaluated Projects for File-Level Defect Prediction
4.3.2 Evaluated Projects for Change-Level Defect Prediction
We choose six open-source projects: Linux kernel, PostgreSQL, Xorg, Jdt (from Eclipse), Lucene, and Jackrabbit. They are large and typical open source projects covering operating systems, database management systems. These projects have sufficient change histories to build and evaluate change-level defect prediction models and are commonly used in the literature [29], [90], [103]. For Lucene and Jackrabbit, we use manually verified bug reports from Herzig et al. [25] to label the bug-fixing changes, and the keyword search approach [88] is used for the others.

Table 6 shows the evaluated projects for change-level defect prediction. The LOC and the number of changes in Table 6 include only source code (C and Java) files3 and their changes because we want to focus on classifying source code changes only. Although these projects are written in C and Java, our DBN-based feature generation approach is not limited to any particular programming language. With the appropriate feature extraction approach, our DBN-based feature generation approach can easily be extended to projects in other languages.

TABLE 6 Evaluated Projects for Change-Level Defect Prediction in This Work
Table 6- 
Evaluated Projects for Change-Level Defect Prediction in This Work
Change-level defect data are often imbalanced [23], [29], [34], [35], i.e., there are fewer buggy instances than clean instances in the training dataset. For example, as shown in Table 6, the average ratio of the buggy and the clean changes is 1.0 to 3.1. The imbalanced data can lead to poor prediction performance [90]. For change-level data, we borrow the data collection process introduced by Tan et al. [90]. Specifically, a gap between the training set and the test set (see Fig. 7) is used because the gap allows more time for buggy changes in the training set to be discovered and fixed. For example, the time period between time T2 and time T4 is a gap. In this manner, the training set will be more balanced, i.e., the training set will have a higher buggy rate. A reasonable setup is to make the sum of the gap and the test set, e.g., the duration from time T2 to T5, close to the typical bug-fixing time (i.e., the time from when a bug is introduced until it is fixed). We use the recommended gap values in [90] to collect multiple runs of experimental data, e.g., Linux has four different runs during the given time period (between the First Date and Last Date) as shown in Table 6. Note that our previous study [90] tuned and evaluated the defect prediction models based on their precision values. In this work, we do not have a bias on either precision or recall, and we tune and evaluate the prediction models based on the harmonic of the precision and recall, i.e., F1 (details are in Section 4.2.1).

Fig. 7. - 
Change-level data collection process [90].
Fig. 7.
Change-level data collection process [90].

Show All

Imbalanced data issues occur in both the file-level and the change-level defect data, and as shown in Tables 5 and 6, most of the examined projects have buggy rates less than 50 percent. To build optimal defect prediction models, we also perform the re-sampling technique used in existing work [90], i.e., SMOTE [6], on the imbalanced projects.

4.4 Baselines of Traditional Features
4.4.1 Baselines for Evaluating File-Level Defect Prediction
To evaluate the performance of semantic features for file-level defect prediction tasks, we compare the semantic features with two different traditional features. Our first baseline of traditional features consists of 20 traditional features. Table 7 shows the details of the 20 features and their descriptions. These features and data have been widely used in previous work to build effective defect prediction models [24], [31], [54], [55], [67], [111].

TABLE 7 Benchmark Metrics Used for File-Level Defect Prediction

We choose the widely used PROMISE data so that we can directly compare our approach with previous studies. For a fair comparison, we also perform the noise removal approach described in Section 3.2.1 on the PROMISE data.

The traditional features from PROMISE do not contain AST nodes, which were used as the input by our DBN models. For a fair comparison, our second baseline of traditional features is the AST nodes that were given to our DBN models, i.e., the AST nodes in all files after handling the noise (Section 3.2.1). Each instance is represented as a vector of term frequencies of the AST nodes.

4.4.2 Baselines for Evaluating Change-Level Defect Prediction
Our baseline features for change-level defect prediction include three types of change features, i.e., bag-of-words features, characteristic features, and meta features, which have been used in previous studies [29], [90].

Bag-of-words features:The bag-of-words feature set is a vector representing the count of occurrences of each word in the text of changes. We employ the snowBall stemmer to group words of the same root, then we use Weka [18] to obtain the bag-of-words features from both the commit messages and the source code changes.

Characteristic features:Inspired by the Deckard tool [28], we use characteristic vectors as features. Characteristic vectors represent the syntactic structure by counting the numbers of each node type in the Abstract Syntax Tree. Bag-of-words and characteristic vectors have different abstraction levels. Although bag-of-words can capture keywords, such as if and while, it cannot capture abstract syntactic structures, such as the number of statements. Suppose that we are using if and elsenode types for characteristic vectors, the characteristic vector of the code before the changes shown in Fig. 6 is (1, 1). After obtaining the characteristic vectors for the file before the change and the file after the change, we subtract the two characteristic vectors to obtain the difference. For each change, we use Deckard [28] to automatically generate two characteristic vectors: one for the source code file before the change and one for the source code file after the change. We use the difference between the two characteristic vectors and the characteristic vector of the file after the change as two sets of features.

Meta features:In addition to characteristic and bag-of-words vectors, we also use a set of metadata features, which includes the basic information of changes, e.g., commit time, filename, developers, etc. It also contains code change metrics, e.g., the added line count per change, the deleted line count per change, etc.

4.5 Parameter Settings for Training a DBN
Many DBN applications [9], [43], [58] report that an effective DBN requires well-tuned parameters, i.e., 1) the number of hidden layers, 2) the number of nodes in each hidden layer, and 3) the number of iterations. In this section, we study the impact of the three parameters on defect prediction models.

4.5.1 Setting Parameters for File-Level Defect Prediction
For file-level defect prediction, we tune the three parameters by conducting experiments with different values of the parameters on ant (1.5, 1.6), camel (1.2, 1.4), jEdit (4.0, 4.1), lucene (2.0, 2.2), and poi (1.5, 2.5). Each experiment has specific values for the three parameters and runs on the five projects individually. Given an experiment, for each project, we use the older version of the project to train a DBN with respect to the specific values of the three parameters. Then, we use the trained DBN to generate semantic features for both the older and newer versions of the project. After this, we use the older version to build a defect prediction model and apply it to the newer version. Finally, we evaluate the specific values of the parameters by the average F1 score of the five projects for file-level defect prediction.

Setting the Number of Hidden Layers and the Number of Nodes in Each Layer. Because the number of hidden layers and the number of nodes in each hidden layer interact with each other, we tune these two parameters together. For the number of hidden layers, we experiment with 11 discrete values that include 2, 3, 5, 10, 20, 50, 100, 200, 500, 800, and 1,000. For the number of nodes in each hidden layer, we experiment with eight discrete values i.e., 20, 50, 100, 200, 300, 500, 800, and 1,000. When we evaluate these two parameters, we set the number of iterations to 50 and keep it constant.

Fig. 8 illustrates the average F1 scores obtained when tuning the number of hidden layers and the number of nodes in each hidden layer together for file-level defect prediction. When the number of nodes in each layer is fixed while increasing the number of hidden layers, all the average F1 scores are convex curves. Most curves peak at the point where the number of hidden layers is 10. If the number of hidden layers remains unchanged, the best F1 score occurs when the number of nodes in each layer is 100 (the top line in Fig. 8). As a result, we choose the number of hidden layers as 10 and the number of nodes in each hidden layer as 100. Thus, the number of the DBN-based features for file-level defect prediction tasks is 100.


Fig. 8.
File-level defect prediction performance with different parameters.

Show All

Setting the Number of Iterations. The number of iterations is another important parameter for building an effective DBN. During the training process, the DBN adjusts the weights to narrow down the error rate between the reconstructed input data and original input data in each iteration. In general, the higher the number of iterations, the lower the error rate. However, there is a trade-off between the number of iterations and the computational time cost. For tuning the parameters for file-level defect prediction, we choose the same five projects to conduct experiments with ten discrete values for the number of iterations. The values range from 1 to 10,000. We use the error rate to evaluate this parameter. Fig. 9 demonstrates that, as the number of iterations increases, the error rate decreases slowly as the corresponding time cost increases exponentially. In this study, we set the number of iterations to 200, with which the average error rate is approximately 0.098 and the time cost is 15 s.


Fig. 9.
Average error rates and time costs for different numbers of iterations for tuning file-level defect prediction.

Show All

4.5.2 Setting Parameters for Change-Level Defect Prediction
For change-level defect prediction, we use the same parameter tuning process as the file-level defect prediction to explore the best parameter values with all the runs of each of the six projects listed in Table 6. For each run of a project, we use its training data to train a DBN with respect to the specific values of the DBN parameters. Then, we use the trained DBN to generate semantic features for both the training and test datasets. Afterward, we use the training dataset to build a defect prediction model and apply it to the test dataset. Last, we evaluate the specific values of the parameters by using the average F1 score of the 41 runs from the six projects.

Note that, for change-level defect prediction, as we described in Section 3.1.2, we have seven different approaches available to extract the source code token vector for a source code change. Our tuning process considers these different types of tokens, the number of hidden layers, and the number of nodes in each layer together. Specifically, for each type of tokens we input them into our DBN model to generate features with different configurations. Similar to our tuning process of file-level defect prediction, for the number of hidden layers, we experiment with 11 discrete values, i.e., 2, 3, 5, 10, 20, 50, 100, 200, 500, 800, and 1,000. For the number of nodes in each hidden layer, we experiment with eight discrete values, i.e., 20, 50, 100, 200, 300, 500, 800, and 1,000. When we evaluate the seven different types of tokens and the two parameters, we set the number of iterations to 50 and keep it constant.

Table 8 shows the F1 scores of the change-level defect prediction with DBN-based semantic features generated by each of the seven types of tokens. Note that among the three basic token types (i.e., added, deleted, and context), the DBN-based features generated by added and deleted deliver better performance than context on all six projects. The improvement could be up to 20.8 percentage points (on project Jdt) and on average the improvement is larger than 8 percentage points. In addition, all the four different combinations, i.e., added+deleted, added+context, deleted+context, and added+deleted+context, can generate better performance than the corresponding three basic token types. This may be because the combinations provide more information to the DBN model for generating more effective features to capture buggy changes (a detailed discussion is provided in Section 6.2). Among the four combinations, added+deleted+context achieves the best performance.

TABLE 8 The Comparison of F1 Scores Among Change-Level Defect Prediction with Different DBN-Based Features Generated by the Seven Different Types of Tokens

In this work, we use the combination of added, deleted, and context tokens as input to DBN models to generate features. The corresponding best value of the number of hidden layers is 5 and the best value of the number of nodes in each hidden layers is 50. This means that the number of generated DBN-based features for change-level defect prediction is 50. Additionally, for change-level defect prediction, we also set the number of iterations to 200, with which the average error rate is less than 0.05 and the time cost for feature generation is less than 5 seconds.

4.6 File-Level Within-Project Defect Prediction
To examine the performance of our semantic features on file-level within-project defect prediction, we build defect prediction models using three machine learning classifiers, i.e., ADTree, Naive Bayes, and Logistic Regression, which have been widely explored in previous work [31], [54], [55], [67], [111]. We use two consecutive versions of each project listed in Table 5 as the training and test data sets. We use the source code of an older version to train the DBN and generate the training feature set. Then we use the trained DBN to generate features for instances from a newer version. We compare our semantic features with the traditional features as described in Section 4.4. For a fair comparison, we use the same classifiers on these traditional features.

4.7 File-Level Cross-Project Defect Prediction
Due to a lack of defect data, it is often difficult to build accurate prediction models for new projects. To overcome this problem, cross-project defect prediction techniques train prediction models using data from mature projects (called source projects), and use the trained models to predict defects for new projects (called target projects). However, because the features of source projects and target projects often have different distributions, making an accurate and precise cross-project defect prediction model is still challenging [66].

We believe that the semantic features can capture the common characteristics of defects, which implies that the semantic features trained from one project can be used to predict defects in a different project, and so is applicable in cross-project defect prediction. To measure the performance of the semantic features in cross-project defect prediction, we propose a technique called DBN Cross-Project Defect Prediction (DBN-CP). Given a source project and a target project, DBN-CP first trains a DBN by using the source project and generates semantic features for both projects. Then, DBN-CP trains an ADTree based defect prediction model using data from the source project and uses the built model to perform defect prediction on the target project.

We choose TCA+ [67] as our baseline. To compare with TCA+, we design two different experiments. First, for each of the 16 test versions (which are the target versions in cross-project prediction) from the within-project experiments list in Table 9, we randomly select two source projects that are different from the target projects. Thus, 32 test pairs are collected. Our first experiment can help evaluate the performance of DBN-CP compared to TCA+ and the corresponding within-project defect prediction. Then, to extensively examine the performance of DBN-CP, we use each version from one project as a target project and each version from the other projects as a source project. In total, 606 test pairs are formed.

TABLE 9 Comparison between Semantic Features and Two Baselines of Traditional Features (PROMISE Features and AST Features) Using ADTree

The reason why we use TCA+ for the comparison that TCA+ is one of the state-of-the-art techniques in cross-project defect prediction [67]. In our reproduction, we follow the processes described in [67]. We first implement all five of their proposed normalization methods and assign them the same conditions as given in the TCA+ paper. We then perform Transfer Component Analysis [73] on the source projects and the target projects together, and map them onto the same subspace while minimizing the data difference and maximizing the data variance. Finally, we use the source projects and target projects with the new features to build and evaluate the ADTree-based prediction models.

4.8 Change-Level Within-Project Defect Prediction
To examine the effectiveness of the learned DBN-based features for change-level defect prediction tasks, we compare the performance of the DBN-based features to the three types of traditional features described in Section 4.4.2. By examining the combination of these traditional features, we should be able to generate the best performance for change-level defect prediction [29], [90]. In this work, we use the combination as the benchmark for change-level defect prediction.

To generate DBN-based semantic features, for each run of a project listed in Table 6, we use its training data to train a DBN (with the combination of all the tokens in a change as the input to the DBN). Then, we use the trained DBN to generate semantic features for both the training and test datasets. We then use the training data to build a defect prediction model and apply it to the test data. For the classification algorithm, we use ADTree in Weka [18] as the classifier, because it has delivered the best performance in previous work [29], [67], [90].

4.9 Change-Level Cross-Project Defect Prediction
Similar to file-level defect models, change-level models also require a large amount of training data to train and build prediction models. However, sufficient training data are not often available when projects are in their initial development phases. To address this limitation, cross-project models for change-level prediction tasks are needed [34]. To explore the performance of the DBN-based semantic features in change-level cross-project defect prediction, we propose a technique called DBN Change-level Cross-Project defect Prediction (DBN-CCP). Specifically, given a source project and a target project, DBN-CCP first trains a DBN by using the source project and generates semantic features for both the source project and the target project. Then, DBN-CCP trains a defect prediction model using data from the source project, and uses the built model to perform defect prediction on the target project.

For evaluating the performance of DBN-CCP, we also choose TCA+ [67] as our baseline. Note that TCA+ requires that the target and source projects have the same features for learning TCA+ based features. As described in Section 4.4.2, in this study we leverage three different types of features for change-level defect prediction, i.e., bag-of-words features, characteristic features, and meta features. Both the bag-of-words features and characteristic features are project-specific and vary for different projects. Thus, for TCA+ on change-level prediction, we only use the meta features.

To extensively evaluate the performance of DBN-CCP, we use each test dataset in all runs from one project as a target dataset and each training dataset in all runs from the other projects as a source dataset to form change-level cross-project test pairs. For example, one test pair could be a training set from Run 1 of Project A and a test set from Run 1 of Project B, a training set from Run 2 of Project A and a test set from Run 1 of Project B, etc. In total, 1,380 test pairs are formed.

SECTION 5Results
5.1 RQ1: Performance of Semantic Features for File-Level Within-Project Defect Prediction
5.1.1 Non-Effort-Aware Evaluation Scenario
We build file-level within-project defect prediction models to compare the impact of three sets of features: semantic features that are automatically learned by DBN, PROMISE features, and AST features. The latter two are the baselines of traditional features. We conduct 16 sets of file-level within-project defect prediction experiments, each of which uses two versions from the same project (listed in Table 5). The older version is used to train the prediction models, and the newer version is used as the test set to evaluate the trained models.

Table 9 shows the performance of the file-level within-project defect prediction experiments. The highest F1 values of the three sets of features are shown in bold. For example, by using ant 1.6 as the training set, and ant 1.7 as the test set, the F1 of using semantic features is 94.2 percent, while the F1 is only 54.2 percent with the first baseline of traditional features (from PROMISE), and the F1 is 47.0 percent with the second baseline of traditional features (AST nodes). For this comparison, the only difference is the three sets of features, meaning that the same classification algorithm, namely ADTree and the same training and test sets are used.

The results demonstrate that by using the DBN-based semantic features instead of the PROMISE features, we can improve the F1 by 14.2 percentage points on the 16 experiment pairs on average. The average improvements in the precision and recall are 14.7 percentage points and 11.5 percentage points respectively.

Since the DBN algorithm has randomness, the generated features vary between different runs. Therefore, we run our DBN-based feature generation approach five times for each experiment. Among the runs, the difference in the generated features is at the level of 1.0e-20, which is too small to propagate to precision, recall, and F1. In other words, the precision, recall, and F1 of all five runs are identical.

5.1.2 Effort-aware Evaluation Scenario
For the effort-aware scenario, we rerun the 16 pairs of file-level within-project defect prediction experiments listed in Table 9, and calculate the PofB20 of the test data in each experiment based on our setup description in Section 4.2.2.

Table 10 presents the PofB20 of file-level within-project defect prediction models with DBN-based semantic features and the PROMISE features. As we can see, in all the experiments, DBN-based features could achieve better PofB20 than the corresponding PROMISE features. Compared to the PROMISE features, the improvement could be as much as 26.7 percentage points (ant 1.6 ⇒ ant 1.7) and is, on average, 13.3 percentage points.

TABLE 10 PofB20 Scores of DBN-Based Features and Traditional Features for WPDP
Table 10- 
PofB20 Scores of DBN-Based Features and Traditional Features for WPDP
We further conduct the Wilcoxon signed-rank test (p<0.05) to compare the performance of the DBN-based semantic features and PROMISE features for file-level within-project defect prediction with the 16 experiment pairs under both the non-effort-aware and effort-aware evaluation scenarios. The results suggest that the DBN-based semantic features are significantly better than the PROMISE features.

Our DBN-based approach is effective in automatically learning semantic features, which significantly improves the performance of file-level within-project defect prediction under both non-effort-aware and effort-aware evaluation scenarios with large effect sizes.

5.1.3 RQ1a: Do Semantic Features Outperform Traditional Features with Other Classification Algorithms?
To answer this question, we build file-level within-project defect prediction models by using two alternative classification algorithms, i.e., Naive Bayes and Logistic Regression. We conduct 16 sets of file-level within-project defect prediction tests, where the training sets and the test sets are exactly the same as those in RQ1. Table 11 shows the F1 scores of running Naive Bayes and Logistic Regression on semantic features and PROMISE features. Take ant as an example, when the model is built on Naive Bayes, by choosing version 1.5 as the training set and 1.6 as the test set, the semantic features produce an F1 of 63.0 percent, which is 7.0 percentage points higher than using PROMISE features. For the same example with Logistic Regression as the classification algorithm, the semantic features achieve an F1 of 91.6 percent, while using PROMISE features produces an F1 of 50.6 percent only. Among the experiments with either Naive Bayes or Logistic Regression as the classification algorithm, the semantic features outperform the PROMISE features 14 out of the 16 times. On average, the Naive Bayes based defect prediction model with semantic features achieves an F1 of 60.0 percent, which is 14.8 percentage points higher than the Naive Bayes with PROMISE features. Similarly, the average F1 of using semantic features with Logistic Regression is 59.7 percent, which is 10.7 percentage points higher than Logistic Regression with PROMISE features.

TABLE 11 Comparison of F1 Scores between Semantic Features and PROMISE Features Using Naive Bayes and Logistic Regression
Table 11- 
Comparison of F1 Scores between Semantic Features and PROMISE Features Using Naive Bayes and Logistic Regression
The semantic features automatically learned from the DBN improve the file-level within-project defect prediction and the improvement is not tied to a particular classification algorithm.

5.2 RQ2: Performance of Semantic Features for File-Level Cross-Project Defect Prediction
5.2.1 Non-Effort-Aware Evaluation Scenario
To answer this question, we compare our file-level cross-project defect prediction technique DBN-CP with TCA+ [67]. DBN-CP runs on the semantic features that are automatically generated by the DBN, while TCA+ uses the PROMISE features. For a fair comparison, we also provide a benchmark of within-project defect prediction. As described in Section 4.3, our preliminary experimental evaluation includes a set of 32 cross-project test pairs. Each experiment takes two versions separately from two different projects, with one used as the training set and the other used as the test set. The benchmark of the file-level within-project defect prediction uses the data from an older version of the target project as the training set.

Table 12 lists the F1 scores of the DBN-CP, TCA+, and the benchmark within-project defect prediction. The better F1 scores between the DBN-CP and TCA+ are in bold. Regarding the average F1, DBN-CP achieves 53.9 percent, which is 7.8 percentage points higher than the 46.1 percent of TCA+.

TABLE 12 F1 Scores of the File-Level Cross-Project Defect Prediction for Target Projects Explored in RQ1

As described in Section 4.3, to extensively evaluate the performance of DBN-CP, we use each version from one project as the target project and one version from the other projects as the source project to form a file-level cross-project experiment test pair. This experiment includes 606 test pairs. Specifically, DBN-CP runs on the semantic features and TCA+ runs on generated features by using the PROMISE features. We also provide two benchmarks, i.e., Baseline and Within-Project. Baseline is the result of cross-project defect prediction with the original PROMISE features.

Table 13 shows the average F1 scores of the DBN-CP, TCA+, Baseline, and Within-Project defect prediction on each of the file-level projects. Overall, both DBN-CP and TCA+ deliver better performance than Baseline. Moreover, DBN-CP generates a better F1 than TCA+ on all 10 projects listed, and the improvement is as much as 12.8 percentage points (ivy) and is, on average, 6.0 percentage points higher. Compared with the within-project defect prediction, DBN-CP improves the cross-project defect prediction by reducing the gap to approximately 12 percentage points. The statistical tests also show that overall DBN-CP is significantly better than both TCA+ and Baseline.

TABLE 13 F1 Scores of File-Level Cross-Project Defect Prediction for All Projects Listed in Table 5

Fig. 10 shows the boxplots of the F1 scores for DBN-CP, TCA+, and Baseline for the 10 projects listed in Table 5. Specifically, each boxplot presents the F1 distribution (median and upper/lower quartiles) of each of the three approaches for cross-project file-level defect prediction. The boxplots indicate that overall, both DBN-CP and TCA+ perform better than Baseline, and that DBN-CP performs better than TCA+ and Baseline on almost all projects.


Fig. 10.
Results of the DBN-CP, TCA+, and Baseline for CPDP.

Show All

5.2.2 Effort-Aware Evaluation Scenario
For the effort-aware evaluation, we also calculate the PofB20 for the DBN-CP, TCA+, and Baseline approaches on each of the target projects.

Table 14 shows the PofB20 of the three file-level cross-project defect prediction models. The highest PofB20 values among the three approaches are shown in bold. In all the experiments, DBN-CP achieves better PofB20 than both TCA+ and Baseline. The PofB20 scores of DBN-CP vary from 21.8 to 37.6 percentage points across the 606 experiments, and the average PofB20 score of DBN-CP is 29.5 percentage points. Compared to TCA+, the improvement is as high as 21.8 percentage points (Poi) and is, on average, 10.3 percentage points. The results of the Wilcoxon signed-rank test (p<0.05) also indicate that DBN-CP is overall significantly better than both TCA+ and Baseline.

DBN-CP significantly improves the performance of file-level cross-project defect prediction under both non-effort-aware and effort-aware evaluation scenarios with a nontrivial effect. This implies that the semantic features learned by the DBN are effective and are able to capture the common characteristics of defects across projects.

TABLE 14 PofB20 Scores of DBN-based Features and Traditional Features for CPDP

5.3 RQ3: Performance of Semantic Features for Change-Level Within-Project Defect Prediction
5.3.1 Non-Effort-Aware Evaluation Scenario
To answer this question, we use different features to build change-level within-project defect prediction models, e.g., DBN-based semantic features, and three change features described in Section 4.4.2 (i.e., the bag-of-words features, the characteristic features, and the meta features). As we described in Section 4.3.2, in the change-level dataset, each project has multiple runs. Thus, we use the training data from each run to build and train the ADTree based prediction model and evaluate its performance on the test data in this run. To show the overall performance, we use the weighted average precision, recall, and F1 following existing work [29], [90].

Table 15 shows the precision, recall, and F1 of the within-project change-level defect prediction experiments. Overall, the DBN-based features generate better results than the traditional change features in terms of F1. Specifically, for all the projects, the DBN-based features could improve the best existing change features up to 8.6 percentage points in F1, and the improvement is 4.7 percentage points, on average.

TABLE 15 Overall Results of the Change-Level Within-Project Defect Prediction

5.3.2 Effort-aware Evaluation Scenario
We further evaluate the DBN-based semantic features and traditional change features for change-level within-project defect prediction with the PofB20 metric.

Table 16 shows the PofB20 of the change-level within-project defect prediction models with DBN-based semantic features and the traditional change features. The DBN-based features could achieve better PofB20 scores than the corresponding change features in all the experiment pairs. The PofB20 scores (measured as a percentage) of DBN-based features vary from 23.8 to 37.6 across the experiments, and the average PofB20 score of the defect prediction models with DBN-based features is 29.2. Compared to the change features, the improvement could be up to 21.1 percentage points (PostgreSQL) and is 9.9 percentage points, on average. In addition, the statistical test, i.e., the Wilcoxon signed-rank test (p<0.05), also suggests that the DBN-based features are overall significantly better than the change features under both the non-effort-aware and effort-aware evaluation scenarios.

The semantic features automatically learned from the DBN could improve the change-level within-project defect prediction with statistical significance under both non-effort-aware and effort-aware evaluation scenarios with nontrivial effect sizes.

TABLE 16 PofB20 Scores of the DBN-based Features and the Traditional Features for WCDP

5.4 RQ4: Performance of Semantic Features for Change-Level Cross-Project Defect Prediction
5.4.1 Non-Effort-Aware Evaluation Scenario
To answer this question, we compare our cross-project change-level defect prediction technique DBN-CCP with TCA+. For a fair comparison, we also provide two benchmarks, i.e., Baseline and Within-Project. The Baseline is the result of change-level defect prediction with the original change features. As we described in Section 4.9, we use the test data of one run from one project as the target project and the training data of one run from a different project as the source project to form the change-level cross-project test pairs (in total 1,380 pairs). For each test pair, we build the ADTree based defect prediction model using the three different sets of features.

Table 17 shows the average F1 scores of the DBN-CCP, TCA+, Baseline, and Within-Project for each of the change-level projects. Overall, both DBN-CCP and TCA+ deliver better performance than Baseline. Moreover, DBN-CCP generates a better F1 than TCA+ on all the projects on average. The improvement is as high as 6.0 percentage points and is 2.2 percentage points higher, on average. Compared to the within-project defect prediction, DBN-CCP improves the cross-project defect prediction by reducing the gap to only 8.0 percentage points.

TABLE 17 F1 Scores of Change-Level Cross-Project Defect Prediction for All Projects

Fig. 11 shows the boxplots of the F1 scores for DBN-CCP, TCA+, and Baseline for the six projects listed in Table 6. Specifically, each boxplot presents the F1 distribution (median and upper/lower quartiles) of each of the three approaches for the change-level cross-project defect prediction. The boxplots show that overall both DBN-CCP and TCA+ perform better than Baseline, moreover DBN-CCP performs better than TCA+ and Baseline on almost all projects.

Fig. 11. - 
Results of DBN-CCP, TCA+, and Baseline for CCDP.
Fig. 11.
Results of DBN-CCP, TCA+, and Baseline for CCDP.

Show All

5.4.2 Effort-Aware Evaluation Scenario
We also calculate the PofB20 score for the DBN-CCP, TCA+, and Baseline approaches on each of the target projects when conducting change-level cross-project defect prediction. Table 18 shows the PofB20 values of the three change-level cross-project defect prediction models. The highest PofB20 values among the three approaches are shown in bold. DBN-CCP achieves better PofB20 scores than both TCA+ and Baseline. On average, the PofB20 score (measured as a percentage) of DBN-CCP is 21.9. Compared to TCA+, the improvement can be up to 3.0 percentage points (Jdt) and is 1.3 percentage points, on average. The results of the Wilcoxon signed-rank test (p<0.05) also indicate that the performance of DBN-CCP is, overall, significantly better than TCA+ and Baseline under both the non-effort-aware and effort-aware evaluation scenarios.

DBN-CCP significantly improves the performance of the change-level cross-project defect prediction compared to the traditional change features with a nontrivial effect.

TABLE 18 PofB20 Scores of the DBN-Based Features and Traditional Features for CCDP

5.5 Time and Memory Overhead
To understand the space cost of file-level defect prediction, during file-level defect prediction experiments, we keep track of the time cost and memory space cost for our DBN-based feature generation process (details are in Section 3.3). In addition, we also have recorded the time cost for tuning the DBN models in our experiments. The other processes, including parsing source code, handling noise, mapping tokens, building models, and predicting defects, are all common procedures, so we do not analyze their costs.

As described in Section 4.5.1, we tune the three parameters, i.e., the number of hidden layers, the number of nodes in each layer, and the number of iterations, for the randomly selected five projects. To find the best combination among the three parameters, we have 11×8×10 experiments. In total, the tuning process costs approximately 5 hours.

Table 19 shows the time cost and the memory space cost of each project for generating semantic features. As shown in Table 9, ant has two sets of within-project defect prediction experiments, which are ant 1.5 ⇒ 1.6 and ant 1.6 ⇒ 1.7. On average, it takes the two experiments 15.5 seconds and 2.8 MB memory for the DBN to generate the semantic features for both the training data and the test data. Among all the projects, the time cost of automatically generating the semantic features varies from 8.0 seconds (ivy) to 32.0 seconds (camel). For the memory space cost, it takes less than 6.5 MB for all the examined projects.

TABLE 19 Time and Space Costs of Generating Semantic Features for File-Level Defect Prediction (s: second)

In addition, we also keep track of the time and memory space cost for generating DBN-based features for the change-level defect prediction during our experiments. Different from the file-level defect prediction that predicts whether a file contains bugs or not, change-level defect prediction predicts whether a change is buggy or clean. Source files often contain hundreds of LOC, while changes often have fewer lines than files. Thus, both the time and memory costs of generating DBN-based features for changes are smaller than those for files. In our experiments, the average time cost and memory cost of generating DBN-based features for changes are 2.4 seconds and 0.6 MB.

Our DBN-based approach to automatically learning semantic features is applicable in practice.

SECTION 6Discussion
6.1 Why Do DBN-based Semantic Features Work?
Our experiments in Section 5 show that compared to traditional features, the DBN-based features that are directly learned from source code deliver significantly better performance for all the four defect prediction tasks investigated in this work. The probable reasons for the outstanding performance of DBN-based features are summarized as follows.

First, the DBN models generate features with more complex network connections. These network connections enable the DBN models to generate features with multiple levels of abstraction and high-level semantics. In this work, the generated DBN features are weighted combinations/vectors of original input source code, which could represent patterns of the usages of the input source code, e.g., method usages, control-flow usages, etc. While traditional features often focus on statistical information of the source code, e.g., LOC, the number of function calls, etc., which cannot capture the semantic information. Although the Bag-of-words feature or the Characteristic feature (details are in Section 4.4) are derived from the raw programming tokens, they consider each token as an independent feature element and cannot represent the contextual and structural information among the raw tokens. Thus, these features have underperformed.

Second, the DBN-based features are more capable of distinguishing between the semantic information of different code snippets, especially for code snippets that have similar source code characteristics. For example, as shown in Fig. 1, the traditional features (e.g., code complexity) of the two code snippets are identical. Training prediction models containing them will degrade the discrimination ability of classifiers and consequently hurt the prediction performance. While the DBN-based features can make a difference, as shown in Fig. 4, the different structural and contextual information among tokens of these two code snippets enables the DBN model to generate different features to distinguish between these two code snippets.

6.2 Efficiency of Different Types of Tokens in Change-Level Defect Prediction
As described in Section 4.5.2, to achieve better prediction performance for change-level defect prediction, we use the combination of the three types of tokens, i.e., added, deleted, and context, to generate DBN-based semantic features. In this section, we further examine the reason why the combination outperforms each of the three types of tokens extracted from changes. One possible reason is that, the combination contains more information than any of the three types of tokens. To explore this, we leverage the information gain [13], which is a widely used metric to measure how much information there is in a given event, to measure the information in each of the three types of tokens and their different combinations.

Specifically, given a document s={a1 … an} of length n, a1 to an are tokens in the document s. The information gain of this document H(s) is measured as follows:
H(s)=∑i=1n−pilogpi,(9)
View Sourcewhere pi is the probability of token ai in the document s. We use the TF (term frequency) of token ai to represent its probability in the document s.

To calculate the information gain of a specific type of token in changes, we first collect all seven types of tokens from all the changes in a project. Then, we calculate the information gain of a specific type of tokens extracted from all changes of a project. Table 20 shows the various information gains of the three types of tokens and their combinations. Overall, among the basic three types of tokens, added and deleted contain more information than context. The combination of either two of them could achieve better performance than either of the two types of tokens. In addition, the combination of all the three types of tokens contains more information than any other combinations. We further compute the Spearman correlation between the value of information gain and the prediction result of different types of tokens in a project. The high correlation value (on average 0.86) indicates that the prediction result of DBN-based features generated from a specific type of token has a positive correlation with its information gain. This explains why DBN-based features generated from the combination of all the three types of tokens achieve the best performance.

TABLE 20 Information Gain of Different Types of Tokens and Their Combinations That Are Used for Generating DBN-Based Semantic Features in Change-Level Defect Prediction

6.3 Analysis of the Performance
In this section, we evaluate the performance of our proposed DBN-based semantic features on both file-level and change-level defect prediction tasks. We can observe that the improvement of DBN-based semantic features on file-level defect prediction is generally better than change-level defect prediction. The main reason for this phenomenon is that a file generally contains more information than a change. Thus, file-level defect prediction data often provide more context to a DBN model, allowing it to learn more accurate features.

We also note that our approach achieves better performance on some projects than others for file-level with-project defect prediction, e.g., it achieves an F1 of 94.2 percent on ant and an F1 of approximately 80 percent on camel, lucene, poi, and jEdit. This is because we use these projects, i.e., ant, camel, lucene, poi, and jEdit, as data to train a DBN model for generating features. During the training process, we tune the DBN parameters based on the performance of the defect prediction models with the generated features for the five projects (details are presented in Section 4.5.1). Because the training process is an optimization task to generate features that may produce the best performance for the training dataset, the features fit the training dataset better. Thus, our approach achieves relatively higher F1 values for the five projects (ant, camel, lucene, poi, and jEdit) than other projects. This may be a risk of overfitting. However, this may also suggest that training a DBN model by using a project's own history data is appropriate when applying our approach to the project.

6.4 Performance on Open-Source Commercial Projects
In Section 4, we evaluated the DBN-based semantic features on 15 open source projects (i.e., the projects listed in Tables 5 and 6). To explore the performance of the DBN-based semantic features on commercial projects, we apply our approach to four additional open-source commercial projects, i.e., Buck,4 Hhvm,5 Guava,6 and Skia.7 Buck is a build system developed and used by Facebook. Hhvm is a virtual machine, which was also developed and is currently used by Facebook. Guava is a set of Google's core libraries for Java. Skia is a complete 2D graphics library for drawing text, geometries, and images developed and used by Google. These four projects were originally developed and maintained by Facebook and Google and became open-source projects recently. We selected these four projects, because they are the largest Java/C++ projects (in terms of commit size). To collect the change-level data, we use the same approaches as we described in Sections 2.2 and 4.4 to label the changes and collect the features for each change. The details of the four open-source commercial projects are listed in Table 21.

TABLE 21 The Four Open-Source Commercial Projects Evaluated
Table 21- 
The Four Open-Source Commercial Projects Evaluated
With these four additional projects, we conduct change-level within-project and change-level cross-project defect prediction tasks to compare the DBN-based semantic features to traditional features under both the non-effort-aware and effort-aware scenarios. Note that we adopt the same procedures to tune the DBN models and generate semantic features as described in Sections 4.8 and 4.9.

Table 22 shows the results of the change-level within-project prediction on the four projects. Overall, the DBN-based features generate better results than traditional change features in terms of F1, which is consistent with our previous experiment results on pure open-source projects 15. Specifically, for all four projects, DBN-based features could improve the best existing change features up to 6.6 percentage points in F1, and on average the improvement is 5.4 percentage points. These improvements are consistent with our previous experimental results on open-source projects (i.e., the best improvement is 8.6 percentage points and the average improvement is 4.7 percentage points).

TABLE 22 Results of WCDP on the Four Projects

Table 23 shows the results of the change-level cross-project prediction for the four projects. Overall, DBN-CCP generates a better F1 than both TCA+ and Baseline for all four projects on average. The improvement is up to 9.9 percentage points and is 3.2 percentage points on average. The results are also consistent with our previous change-level cross-project defect prediction results listed in 17.

TABLE 23 F1 Scores of Change-Level Cross-Project Defect Prediction for the Four Projects

We also calculate the PofB20 values for both the change-level within-project and cross-project approaches. Table 24 shows the PofB20 values of the change-level within-project defect prediction models with DBN-based semantic features and the traditional change features. DBN-based features achieve better PofB20 values than the corresponding change features for all experiment pairs. The improvement is up to 13.5 percentage points (Buck) and is 7.4 percentage points on average. Table 25 shows the PofB20 values of the three change-level cross-project defect prediction approaches. Similar to our previous results, DBN-CCP achieves better PofB20 values than both TCA+ and Baseline on average. Compared to TCA+, the improvement is as high as 7.1 percentage points and is 2.3 percentage points, on average.

TABLE 24 PofB20 Scores of DBN-Based Features and Traditional Features for WCDP on the Four Projects

TABLE 25 PofB20 Scores of DBN-Based Features and Traditional Features for CCDP on the Four Projects

DBN-based semantic features outperform traditional features on four open-source commercial projects from Facebook and Google, which indicates that DBN-based semantic features are also applicable for improving defect prediction for open-source commercial projects.

6.5 Threats To Validity
6.5.1 Implementation of TCA+
For the comparative analysis, we compare our cross-project defect prediction models with TCA+ [67], which is the state-of-the-art cross-project defect prediction technique with traditional features. Since the original implementation is not released, we reimplemented our own version of TCA+. Although we strictly followed the procedures described in their work, our new implementation may not reflect all the implementation details of the original TCA+. We test our implementation with the data from their work. Since our implementation generates the same results, we are confident that our implementation reflects the original TCA+.

In this work we did not evaluate our DBN-based feature generation approach on projects used for evaluating TCA+ [67]. This is because our DBN-based feature generation approach to within-project defect prediction works on data of two different versions from the same project. However, the datasets used in [67] only provided one version of defect data for each of their eight projects, which are unsuitable for evaluating our approach to within-project defect prediction. To reduce this threat, we evaluated TCA+ and our approach on the publicly available projects from PROMISE.

6.5.2 Project Selection
The examined projects in this work have a large variance in average buggy rates. We have tried our best to make our dataset general and representative. However, it is still possible that the projects used in our experiments are not generalizable enough to represent all software projects. Our approach might generate better or worse results for other projects that are not used in the experiments. We mitigate this threat by selecting projects of different functionalities (e.g., operating systems, servers, etc.) that are developed in different programming languages (C and Java).

Our approach to generating semantic features is only evaluated on open source projects. While we believe that this approach should be generalizable to proprietary software, evaluating our approach on proprietary software is challenging, because the approach requires AST analysis of source code. We mitigate this threat by applying our approach to four open source commercial projects that were originally developed and maintained by Google and Facebook and are open source now. The performance of these projects suggests our proposed DBN-based semantic features could deliver better results than traditional features.

6.5.3 Labeling Data
Following previous work [40], [88], the labeling process is automatically completed with the annotating or blaming function in VCS. It is known that this process can introduce noise [29], [39]. The noise in the data can potentially harm the performance of defect prediction. Manual inspection of the process shows reasonable precision and recall on open source projects [29]. To mitigate this threat, we use the noise data filtering algorithm introduced in [39].

SECTION 7Related Work
7.1 Software Defect Prediction
There are many software defect prediction techniques [15], [22], [29], [31], [41], [45], [53], [59], [61], [62], [63], [65], [70], [80], [83], [95], [98], [108], [109], [111], most of which leverage features that are extracted from the repositories of projects to train machine learning based classifiers [55]. Commonly used features can be divided into code features and process features [54]. Code features, e.g., Halstead [19], McCabe's cyclomatic complexity [50], CK [8], and MOOD features [21], have been widely examined and used for defect prediction. Recently, process features have been proposed and used for defect prediction. Moser et al. [59] used the number of revisions, authors, past fixes, and ages of files as features to predict defects. Nagappan et al. [62] proposed code churn features, and showed that these features were effective for defect prediction. Hassan et al. [22] used entropy of change features to predict defects. Their evaluation of six projects showed that their proposed features can significantly improve the results of defect prediction in comparison to other change features. Lee et al. [45] proposed 56 micro interaction metrics to improve defect prediction. Their evaluation results on three Java projects showed that their proposed features can improve defect prediction results compared to traditional features. Other process features, including the developers’ characteristics [29], [71] and collaboration between developers [45], [55], [76], [99], have also been used to build defect prediction models. In this work, we adopted process features such as the commit time, filename, developers, the added line count, the deleted line count, the changed line count, etc., which are included in the meta features (details are presented in Section 4.4.2). We did not compare the DBN-based semantic features with the meta features alone since we found that combining them with the bag-of-words and characteristic vector outperforms using them alone. The results in Section 5 show that compared with the combination of the three benchmark features (including some process features), our DBN-based semantic features produce better performance.

The main difference between our DBN-based semantic features and the above traditional features is that traditional features are manually encoded and mainly focus on the statistical information of the source code, e.g., LOC, the number of function calls, etc., while our DBN-based semantic features are automatically learned by using deep learning techniques and try to capture patterns of the usages of tokens (e.g., method usages, control-flow usages, etc.) in the source code.

In this work, we rigorously compare the DBN-based semantic features with traditional defect prediction features on two different defect prediction tasks—within-project defect prediction and cross-project defect prediction.

7.1.1 Within-Project Defect Prediction
Within-project defect prediction uses training data and test data that are from the same project. Many machine learning algorithms have been adopted for within-project defect prediction, including Support Vector Machine (SVM) [12], Bayesian Belief Network [1], Naive Bayes (NB) [93], Decision Tree (DT) [14], [37], [95], Neural Network (NN) [72], [78], and Dictionary Learning [31].

Elish et al. [12] evaluated the capability of SVM in predicting defect-prone software modules, and they compared SVM against eight statistical and machine learning models on four NASA datasets. Their results showed that SVM is generally better than, or at least, is competitive against other models, e.g., Logistic Regression, Bayesian techniques, etc. Amasaki et al. [1] proposed an approach to predicting the final quality of a software product by using the Bayesian Belief Network. They evaluated their approach on a closed project, and the results showed that their proposed approach can predict bugs that the Software Reliability Growth Model (SRGM) cannot handle. Wang et al. [95] and Khoshgoftaar et al. [37] examined the performance of Tree-based machine learning algorithms on defect prediction, their results suggested that Tree-based algorithms could help defect prediction. Tao et al. [93] proposed a Naive Bayes based defect prediction model, and they evaluated the proposed approach on 11 datasets from the PROMISE defect data repository. Their experiment results showed that the Naive Bayes based defect prediction models could achieve better performance than J48 (decision tree) based prediction models. Our previous work [96] used the deep belief network to generate semantic features for file-level defect prediction tasks. The new contributions made by this paper are described in Section 1.

In this work, to evaluate the performance of our DBN-based semantic features and the traditional defect prediction features, we built prediction models by using three typical machine learning algorithms, i.e., ADTree, Naive Bayes, and Logistic Regression. Our experiment results show that the learned DBN-based semantic features consistently outperform the traditional defect prediction features on these machine learning classifiers.

Most of the above approaches are designed for file-level defect prediction. For change-level defect prediction, Mockus and Weiss [57] and Kamei et al. [35] predicted the risk of a software change by using change measures, e.g., the number of subsystems touched, the number of files modified, the number of added lines, and the number of modification requests. Kim et al. [38] used the identifiers in added and deleted source code and the words in change logs to classify changes as being defect-prone or clean. Jiang et al. [29] and Xia et al. [103] built separate prediction models with the characteristic features and meta features for each developer to predict software defects in changes. Tan et al. [90] improved the change classification techniques and proposed the online defect prediction models for imbalanced data. Their approach used time sensitive change classification to address the incorrect evaluation introduced by cross-validation. McIntosh et al. [51] studied the performance of change-level defect prediction as software systems evolve. Change classification can also predict whether a commit is buggy or not [75], [77].

In this work, we also compare the DBN-based semantic features with the widely used change-level defect prediction features, and our results suggest that the DBN-based semantic features can also outperform these change-level defect prediction features.

7.1.2 Cross-Project Defect Prediction
Due to the lack of data, it is often difficult to build accurate models for new projects. Some studies [42], [94], [110] have been done on evaluating cross-project defect prediction against within-project defect prediction and show that cross-project defect prediction is still a challenging problem. He et al. [24] showed the feasibility to find the best cross-project models among all available models to predict defects on specific projects. Turhan et al. [94] proposed using a nearest-neighbor filter to improve cross-project defect prediction. Nam et al. [67] proposed TCA+, which adopted a state-of-the-art technique called Transfer Component Analysis (TCA) and the optimized TCA's normalization process to improve cross-project defect prediction. Xia et al. [102] proposed HYDRA, which leverages a genetic algorithm and ensemble learning (EL) to improve cross-project defect prediction. HYDRA requires massive training data and a portion (5 percent) of labeled data from test data to build and train the prediction models.

TCA+ [67] and HYDRA [102] are the two state-of-the-art techniques for cross-project defect prediction. However, in this work, we only use TCA+ as our baseline for cross-project defect prediction. This is because HYDRA requires that the developers manually inspect and label 5 percent of the test data, while in real-world practice, it is very expensive to obtain labeled data from software projects, which requires the developers’ manually inspection, and the ground truth might not be guaranteed.

Most of the above existing cross-project approaches are examined for file-level defect prediction only. Recently, Kamei et al. [34] empirically studied the feasibility of change-level defect prediction in a cross-project context. In this work, we also examine the performance of the DBN-based semantic features on change-level cross-project defect prediction tasks.

The main differences between our approach and existing approaches for within-project defect prediction and cross-project defect prediction are as follows. First, existing approaches to defect prediction are based on manually encoded traditional features which are not sensitive to the programs’ semantic information, while our approach automatically learns the semantic features using a DBN and uses these features to perform defect prediction tasks. Second, since our approach requires only the source code of the training and test projects, it is suitable for both within-project defect prediction and cross-project defect prediction.

7.2 Deep Learning and Semantic Feature Generation in Software Engineering
Recently, deep learning algorithms have been adopted to improve research tasks in software engineering. Yang et al. [106] proposed an approach that leveraged deep learning to generate features from existing features and then used these new features to build defect prediction models. This work was motivated by the weaknesses of logistic regression (LR), which is that LR cannot combine features to generate new features. They used a DBN to generate features from 14 traditional change level features, including the following: the number of modified subsystems, modified directories, modified files, code added, code deleted, line of code before/after the change, files before/after the change, and several features related to developers’ experience [106].

Our work differs from the above study mainly in three aspects. First, we use a DBN to learn semantic features directly from the source code, while features generated from their approach are relations among existing features. Since the existing features cannot distinguish between many semantic code differences, the combination of these features would still fail to capture semantic code differences. For example, if two changes add the same line at different locations in the same file, the traditional features cannot distinguish between the two changes. Thus, the generated new features, which are combinations of the traditional features, would also fail to distinguish between the two changes. Second, we evaluate the effectiveness of our generated features using different classifiers for both within-project and cross-project defect prediction, while they only use LR for within-project defect prediction. Third, we focus on both file and change-level defect prediction, while they only work on change-level defect prediction.

There also many existing studies that leverage deep learning techniques to address other problems in software engineering [16], [17], [30], [32], [44], [46], [60], [74], [84], [100], [105], [107]. Mou et al. [60] used deep learning to model programs and showed that deep learning can capture the programs’ structural information. Deep learning has also been used for malware classification [74], [107], test report classification [32], link prediction in a developer online forum [105], software traceability [30], etc.

How to explain deep learning results is still a challenging question to the AI community. To interpret deep learning models, Andrej et al. [36] used character-level language models as an interpretable testbed to explain the representations and predictions of a Recurrent Neural Network (RNN). Their qualitative visualization experiments demonstrate that RNN models could learn powerful and often interpretable long-range interactions from real-world data. Radford et al. [79] focused on understanding the properties of representations learned by byte-level recurrent language models for sentiment analysis. Their work reveals that there exists a sentiment unit in the well-trained RNNs (for sentiment analysis) that has a direct influence on the generative process of the model. Specifically, simply fixing its value to be positive or negative can generate samples with the corresponding positive or negative sentiment. The above studies show that to some extent deep learning models are interpretable. However, these two studies focused on interpreting RNNs on text analysis. In this work we leverage a different deep learning model, i.e., the deep belief network (DBN), to analyze the ASTs of source code. The DBN adopts different architectures and learning processes from RNNs. For example, an RNN (e.g., LSTM) can, in principle, use its memory cells to remember long-range information that can be used to interpret data it is currently processing, while a DBN does not have such memory cells (details are provided in Section 2.3). Thus, it is unknown whether DBN models share the same property (i.e., interpretable) as RNNs.

Many studies used a topic model [5] to extract semantic features for different tasks in software engineering [7], [47], [69], [70], [89], [104]. Nguyen et al. [70] leveraged a topic model to generate features from source code for within-project defect prediction. However, their topic model handled each source file as one unordered token sequence. Thus, the generated features cannot capture structural information in a source file.

SECTION 8Conclusions and Future Work
This work leverages a representation-learning algorithm, i.e., deep learning, to learn semantic representation directly from source code for defect prediction. Specifically, we deploy a deep belief network to learn semantic features from programs’ ASTs (for file-level defect prediction models) and source code changes (for change-level defect prediction models) automatically, and leverage the learned semantic features to build prediction models.

We examined the effectiveness of the learned DBN-based semantic features on two file-level defect prediction tasks, i.e., file-level within-project defect prediction (WPDP) and file-level cross-project defect prediction (CPDP), and two change-level defect prediction tasks, i.e., change-level within-project defect prediction and change-level cross-project defect prediction. To conduct comprehensive performance evaluations, we employed both non-effort-aware and effort-aware evaluation metrics.

For file-level defect prediction tasks, our evaluations were conducted on 26 versions of data from 10 open source projects. Our results show that the DBN-based semantic features improve WPDP on average by 13.3 percentage points (in F1), and outperform the state-of-the-art CPDP with traditional features on average by 6.0 percentage points. For change-level defect prediction, our evaluations were conducted on more than 1M changes from six open source projects and four open-source commercial projects. The experimental results indicate that the DBN-based semantic features can improve WCDP on average by 5.1 percentage points, and improve the state-of-the-art CCDP technique with traditional change-level features, on average, by 2.9 percentage points. In addition, under the effort-aware evaluation scenario, our DBN-based semantic features can outperform traditional features for both the file-level and the change-level defect prediction.

In the future, we would like to extend our DBN-based approach to generate semantic features for method-level defect prediction, which helps in predicting the buggy methods in software projects. It could be promising to leverage the defect prediction result to facilitate other practices during software development and maintenance. For example, software defect prediction has been used by QA teams to help prioritize test cases [97], enhance static bug finders [81], etc. We plan to explore the potential applications of defect prediction for improving risk management, quality control, and project planning.