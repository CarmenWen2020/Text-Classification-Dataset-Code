Code flaws or vulnerabilities are prevalent in software systems and can potentially cause a variety of problems including deadlock, hacking, information loss and system failure. A variety of approaches have been developed to try and detect the most likely locations of such code vulnerabilities in large code bases. Most of them rely on manually designing code features (e.g., complexity metrics or frequencies of code tokens) that represent the characteristics of the potentially problematic code to locate. However, all suffer from challenges in sufficiently capturing both semantic and syntactic representation of source code, an important capability for building accurate prediction models. In this paper, we describe a new approach, built upon the powerful deep learning Long Short Term Memory model, to automatically learn both semantic and syntactic features of code. Our evaluation on 18 Android applications and the Firefox application demonstrates that the prediction power obtained from our learned features is better than what is achieved by state of the art vulnerability prediction models, for both within-project prediction and cross-project prediction.
SECTION 1Introduction
A software vulnerability – a security flaw, glitch, bug, or weakness found in software systems – can potentially cause significant damage to businesses and people's lives, especially with the increasing reliance on software in all areas of our society. For instance, the Heartbleed vulnerability in OpenSSL exposed in 2014 has affected billions of Internet users [1]. Cyberattacks are constant threats to businesses, governments and consumers. The rate and cost of a cyber breach is increasing rapidly with annual cost to the global economy from cybercrime being estimated at $400 billion [2]. In 2017, it is estimated that the global security market is worth $120 billion [3]. Central to security protection is the ability to detect and mitigate software vulnerabilities early, especially before software release to effectively prevent attackers from exploit them.

Software has significantly increased in both size and complexity. Identifying security vulnerabilities in software code is very difficult as they are rare compared to other types of software defects. For example, the infamous Heartbleed vulnerability was caused only by two missing lines of code [4]. Finding software vulnerabilities is commonly referred to as “searching for a needle in a haystack” [5]. Static analysis tools have been routinely used as part of the security testing process but they commonly generate a large number of false positives [6], [7]. Dynamic analysis tools rely on detailed monitoring of run-time properties including log files and memory, and require a wide range of representative test cases to exercise the application. Hence, standard practice still relies heavily on domain knowledge to identify the most vulnerable part of a software system for intensive security inspection.

Software engineers can be supported by automated tools that explore the remaining parts of the software code which are more likely to contain vulnerabilities and raise an alert on these. Such predictive models and tools can help prioritize effort and optimize inspection and testing costs. They aim to increase the likelihood of finding vulnerabilities and reduce the time required by software engineers to discover vulnerabilities. In addition, a predictive capability that identifies vulnerable components early in the software lifecycle is a significant achievement since the cost of finding and fixing errors increases dramatically as the software lifecycle progresses [8].

A common approach to building vulnerability prediction models is by using machine learning techniques. A number of features representing software code are selected for use as predictors for vulnerability [5], [9], [10], [11], [12], [13], [14]. The most commonly used features in previous work are software metrics (e.g., size of code, number of dependencies, and cyclomatic complexity) (e.g., [11]), code churn metrics (e.g., the number of code lines changed) and developer activity (e.g., [10]), and dependencies and organizational measures (e.g., [5]). However, using those features does not help us recognize the code that is semantically different and hence potentially vulnerable [15]. In many cases, two pieces of code may have the same complexity metrics but they behave differently and thus have a different likelihood of vulnerability to attack. Furthermore, the choice of which features are selected as predictors is manually chosen by knowledgeable domain experts. This has the disadvantage that it may carry outdated experience and underlying biases. In addition, in many situations the handcrafted features do not generalize well: features that work well for a certain software project may not perform as well in other projects [16].

An emerging approach is treating software code as a form of text and leveraging Natural Language Processing (NLP) techniques to automatically extract features. Recent work (e.g., [8]) has used Bag-of-Words (BoW) to represent a source code file as a collection of code tokens associated with frequencies. The terms are the features which are used as the predictors for their vulnerability prediction model. BoW features rely on the code tokens used by the developers, and thus they are not fixed or pre-determined (as seen in the software metric model). However, the BoW approach has two major weaknesses. First, it ignores the semantics of code tokens, e.g., fails to recognize the semantic relations between “for” and “while”. Second, a bag of code tokens does not necessarily capture the semantic structure of code, especially its sequential nature.

Software programs not only follow a well-defined syntax, but also have semantics which describe what the programs mean and how they execute. Thus, approaches that do not capture the semantics of code structure or individual code tokens may miss important information about the programs [17]. In fact, previous studies have demonstrated that semantic information hidden in a program is useful for various software engineering tasks such as code completion, bug detection and defect prediction [15], [18], [19], [20], [21]. This semantic information can also help provide richer representations for vulnerable code and thus improve vulnerability prediction.

The recent advances of deep learning techniques [22] in machine learning offer a powerful alternative to software metrics and BoW in representing software code. One of the most widely-used deep learning models is Long Short-Term Memory (LSTM) [23], a special kind of recurrent neural network that is highly effective in learning long-term dependencies in sequential data such as text and speech. LSTMs have demonstrated ground-breaking performance in many applications such as machine translation, video analysis, and speed recognition [22] .

This paper presents a novel deep learning-based approach to automatically learn features for predicting vulnerabilities in software code. We leverage LSTM to capture the long context relationships in source code where dependent code elements are scattered far apart. For example, pairs of code tokens that are required to appear together due to programming language specification (e.g., try and catch in Java) or due to API usage specification (e.g., lock() and unlock()), but that do not immediately follow each other in the textual code files. Our previous work [24] has provided a preliminary demonstration of the effectiveness of a language model based on LSTM. However, that work merely sketched the use of LSTM in predicting the next code tokens. Our current work in this paper presents a comprehensive framework where features are learned and combined in a novel way, and are then used in a novel application, i.e., building vulnerability prediction models. The learned features represent both the semantics of code tokens (semantic features) and the sequential structure of source code (syntactic features). Our automatic feature learning approach eliminates the need for manual feature engineering which occupies most of the effort in traditional approaches. Results from our experiments on 18 Java applications [8] for the Android OS platform and Firefox application [12] from public datasets demonstrate that our approach is highly effective in predicting vulnerabilities in code.

The outline of this paper is as follows. In the next section, we provide a motivation example. Section 3 provides a brief background on vulnerability prediction and the neural networks used in our model. We then present an overview of our approach in Section 4, the details of how features are automatically learned in Section 5, and the implementation our approach in Section 6. We report the experiments to evaluate it in Section 7, and discuss the threats to validity in Section 8. In Section 9, we discuss related work before summarizing the contributions of the paper and outlines future work in Section 10.

SECTION 2Motivation
In this section, we present a motivating example which demonstrates some major limitations of existing approaches in representing software components for vulnerability prediction. Fig. 1 shows two code listings in Java that we adapted from [25]. Both pieces of code aim to avoid data corruption in multi-threaded Java programs by protecting shared data from concurrent modifications and accesses (e.g., file f in this example). They do so by using a reentrant mutual exclusion lock l in order to enforce exclusive access to the file f. Here, a thread executing this code means to: (i) acquire the lock before reading file f; and then (ii) release the lock when it finishes reading the file so that other threads are able to access the file.

Fig. 1. - 
A motivating example.
Fig. 1.
A motivating example.

Show All

The use of such locking can however result in deadlocks. Listing 1 in Fig. 1 demonstrates an example of deadlock vulnerability. While it reads file f, an exception (e.g., file not found) may occur and control transfers to the catch block. Hence, the call to unlock() never gets executed, and thus it fails to release the lock. An unreleased lock in a thread will prevent other threads from acquiring the same lock, leading to a deadlock situation. Deadlock is a serious vulnerability, which can be exploited by attackers to organise Denial of Service (DoS) attacks. This type of attack can slow or prevent legitimate users from accessing a software system.

Listing 2 in Fig. 1 rectifies this vulnerability. It fixes the problem of the lock not being released by calling unlock() in the finally block. Hence, it guarantees that the lock is released regardless of whether or not an exception occurs. In addition, the code ensures that the lock is held when the finally block executes by acquiring the lock (calling lock()) immediately before the try block.

The two code listings are identical with respect to both software metric and Bag-of-Words measures used by most current predictive and machine learning approaches to code-level vulnerability detection. The number of code lines, the number of conditions, variables, and branches are the same in both listings. The code tokens and their frequencies are also identical in both pieces of code. Hence, the two code listings are indistinguishable if either software metrics or BoW are used as features for a vulnerability detection or analysis recommendation approach. Existing work which relies on those features would fail to recognize that the left-hand side listing contains a vulnerability while the right-hand side does not.

SECTION 3Background
3.1 Vulnerability Prediction
Vulnerability prediction involves determining whether a software component is likely to be vulnerable or not. Most of existing work (e.g., [5], [8], [9], [10], [11], [26], [27], [28]) in vulnerability prediction refer to a component as a source file (e.g., a “.java” file) in a software system. Hence, this level of granularity has become a standard in the literature of predicting vulnerabilities in software code in terms of both benchmark techniques and datasets. The objective here is to alert software engineers with parts of the software system require special focus (e.g., manual inspection or running targeted test case suites), rather than pinpointing exactly the code line(s) where a vulnerability resides [8]. Hence, we also chose to work at the level of files since this is also the scope of most existing work (e.g., [8], [10]) with which we would like to compare our approach against.

Determining if a component is likely to be vulnerable can be considered as a function vuln(x) which takes as input a file x and returns a boolean value: true indicates that the component is likely to be vulnerable, while false indicates that the component is likely to be clean. Vulnerability prediction is therefore to approximate this classification function vuln(x) by learning from a number of examples (i.e., components known to be vulnerable or clean) provided in a training set (see Fig. 2). After training, the learned function (or also referred to as the model) is used to automatically determine the vulnerability of new components in the same project (within-project prediction) or in a different project (cross-project prediction). Within-project prediction also has two settings: within-version (new components are from the same version as the training components) and cross-version (new components are from a later version).


Fig. 2.
Vulnerability prediction (adapted from [15]).

Show All

To date, various machine learning techniques have been widely used to learn function vuln(x). To make it mathematically and computationally convenient for machine learning algorithms, file x needs to be represented as a n-dimensional vector where each dimension represents a feature (or predictor).

3.2 Long Short Term Memory
The feature vector representation of file x is critical in building an accurate vulnerability prediction model. While high-level representations such as code complexity metrics are useful, they do not reveal the semantics hidden deeply in source code (as demonstrated in the motivation example in Section 2). Long Short-Term Memory, a deep learning architecture, offers a powerful representation of source code. It is able to automatically learn both syntactic and semantic features which represent long-term dependencies (e.g., a code element may depend on other code elements which are not immediately before it) in source code.

Long Short-Term Memory (LSTM) [23], [29] is a recurrent neural network [30], which maps a sequence of input vectors into a sequence of output vectors (see Fig. 3). A Long Short-Term Memory (LSTM) neural network architecture is a special variant of a Recurrent Neural Network (RNN), which is capable of learning long-term dependencies. This is the key difference from a feedforward neural network which maps an input vector into an output vector.


Fig. 3.
A recurrent neural network.

Show All

An LSTM network can be considered as a sequence of LSTM units. Let w1,…,wn be the input sequence (e.g., code tokens), which has a sequence of corresponding labels (e.g., the next code tokens). At each step t, an LSTM unit reads the input wt, the previous output state st−1 and the previous memory ct−1 and uses a set of model parameters to compute the output state st. The output state is used to predict the output (e.g., the next code token based on the previous ones) at each step t.

Each LSTM unit has a memory cell ct which stores accumulated memory of the context (see Fig. 4). This is the key feature allowing an LSTM model to learn long-term dependencies. The information stored in the memory is refreshed at each time step through partially forgetting old, irrelevant information and accepting fresh new input. Specifically, the amount of information flowing through the memory cell is controlled by three gates (an input gate it, a forget gate ft, and an output gate ot), each of which returns a value between 0 (i.e., complete blockage) and 1 (full passing through).


Fig. 4.
The internal structure of an LSTM unit.

Show All

All those gates are learnable, i.e., are trained with the whole code corpus. All LSTM units in the same network share the same parameters since they perform the same task (e.g., predicting the next code token), just with different inputs. Hence, comparing this to traditional feedforward networks, using an LSTM network significantly reduces the total number of model parameters which we need to learn.

An LSTM model is trained using many input sequences with known true output sequences. The errors between the true outputs and the predicted outputs are passed backwards through the network (i.e., backpropagation) during training to adjust the model parameters such that the errors are minimized. More details about LSTMs can be found in the seminal paper [23].

LSTM is highly effective in learning representations of sequential data, such as natural text and speech, as demonstrated in many recent breakthroughs in machine translation and speed recognition [22]. Since software code is also typically produced by humans, it shares many important properties with natural language text (e.g., repetitive, predictable, and long-term dependencies) [20]. In this study we investigate how a LSTM can be used to learn representations of software code and then use these for vulnerability prediction.

SECTION 4Architectural Overview
Our process of automatic feature learning goes through multiple steps (see Fig. 5). We consider each Java source file as consisting of a header (which contains a declaration of class variables) and a set of methods. We treat a header as a special method (method 0). We parse the code within each method into a sequence of code tokens (step 1 in Fig. 5), which is fed into a Long Short-Term Memory (LSTM) system to learn a vector representation of the method (i.e., method features – step 2 in Fig. 5). This important step transforms a variable-size sequence of code tokens into a fixed-size feature vector in a multi-dimensional space. In addition, for each input code token, the trained LSTM system also gives us a so-called token state, which captures the distributional semantics of the code token in its context of use.

Fig. 5. - 
Overview of our approach for automatic feature learning for vulnerability prediction based on LSTM. The codebook is constructed from all bags of token states in all projects, and the process is detailed in Fig. 7.
Fig. 5.
Overview of our approach for automatic feature learning for vulnerability prediction based on LSTM. The codebook is constructed from all bags of token states in all projects, and the process is detailed in Fig. 7.

Show All

After this step, we obtain a set of method feature vectors, one for each method in a file. The next step is aggregating those feature vectors into a single feature vector (step 3 in Fig. 5). The aggregation operation is known as pooling. Pooling aims to transform the joint feature representation (e.g., method features) into a new, more usable one (e.g., file features) that maintain important information while removing irrelevant detail. For example, the simplest statistical pooling method is mean-pooling where we take the sum of the method vectors and divide it by the number of methods in a file. More complex pooling methods can be used and we will discuss these in more detail. This step produces a set of local features for a file.

Those learned features are however local to a project. For example, method names and variables are typically project-specific. Hence, using only those features alone may be effective for within-project prediction but may not be sufficient for cross-project settings. Our approach therefore learns another set of features to address this generalization issue. To do so, we build up a universal bag of token states from all files across all the studied projects (step 4 in Fig. 5). We then automatically group those code token states into a number of clusters based on their semantic closeness (step 5). The centroids in those clusters form a so-called “codebook”, which is used for generating a set of global features for a file through a centroid assignment process (step 6). The two sets of learned features are fed into a classifier, which is then trained to predict vulnerable components.

Vulnerability prediction in new projects is often very difficult due to the lack of suitable training data. One common technique to address this issue is training a model using data from a (source) project and applying it to the new (target) project. Since our approach requires only the source code of the source and target projects, it is readily applicable to both within-project prediction as well as for cross-project prediction.

SECTION 5Feature Learning, Generation, and Usage
In this section, we describe in detail how our approach automatically learns and generates features representing a source code file and uses them for vulnerability prediction and recommendation to software engineers.

5.1 Parsing Source Code
To use our approach we must convert programs into vectors for our LSTM. To begin, we build Abstract Syntax Trees (AST) to extract key syntactic information from the source code of each source file in a project. To do so, we utilize a parser to lexically analyze each source file and obtain an AST. Each source file is parsed into a set of methods and each method is parsed into a sequence of code tokens. All class attributes (i.e., the header) are grouped into a sequence of tokens.

During this processing comments and blank lines are ignored as they do not contribute to the actual behaviour of the code. Following standard practice (e.g., as done in [17]), we replace integers, real numbers, exponential notation, and hexadecimal numbers with a generic ⟨num⟩ token, and replace constant strings with a generic ⟨str⟩ token. Doing this allows us to generalize from the numbers and strings that are specific to a source file or project. We also replace less popular tokens (e.g., occurring only once in the corpus) and tokens which exist in test sets but do not exist in the training set with a special token ⟨unk⟩ since learning is limited for these tokens. A fixed-size vocabulary V is constructed based on top N popular tokens, and rare tokens are assigned to ⟨unk⟩. Doing this makes our corpus compact to the computation costs, but still provides sufficient semantic information.

5.2 Learning Code Token Semantics
After the parsing and tokenizing process, each method is now a sequence of code tokens ⟨w1,w2,…,wn⟩. We then perform the following steps to learn a vector representation for each code token.

We first represent each code token as a low dimensional, continuous and real-valued vector. This process is known as code token embedding, which is described in detail in Section 5.2.1.

Each sequence of code tokens is then input into a sequence of LSTM units. We then train each LSTM unit by predicting the next code token in a sequence (see Section 5.2.2 for details).

The trained LSTM is used to generate an output vector (the so-called code token state) for each code token. This vector representation captures the distribution of semantics of a code token in terms of its context of use (see Section 5.2.3 for details).

5.2.1 Code Token Embedding
An LSTM unit takes as its input a vector representing a code token. Hence, we need to convert each code token into a fixed-length continuous vector. This process is known as code token embedding. We do so by maintaining a token embedding matrix M∈Rd×|V| where d is the size of a code token vector and |V| is the size of vocabulary V. Each code token has an index in the vocabulary, and this embedding matrix acts as a look-up table: each column ith in the embedding matrix is an embedded vector for the token ith. We denote xt as a vector representation of code token wt. For example, token “try” is converted in vector [0.1, 0.3, -0.2] in the example in Fig. 6.


Fig. 6.
An example of how a vector representation is obtained for a code sequence.

Show All

5.2.2 Model Training
The code sequence vectors that make up each method are then input to a sequence of LSTM units. Specifically, each token vector xt in a sequence ⟨x1,x2,…,xn⟩ is input into an LSTM unit (see Fig. 6). As LSTM is a recurrent net, all the LSTM units share the same model parameters. Each unit computes the output state st for an input token xt. For example in Fig. 6, the output state vector for code token “try” is [1, -0.5, -3]. The size of this vector can be different from the size of the input token vector (i.e., d≠d′), but for simplicity in training the model we assume they are the same. The state vectors are used to predict the next tokens using another token weight matrix denoted as U∈Rd′×|V|.

Our LSTM automatically learns both model parameters, the token weight matrix U and the code token embedding matrix M by maximizing the likelihood of predicting the next code token in the training data. Specifically, we use the output state vector of code token wt to predict the next code token wt+1 from a context of earlier code tokens w1:t by computing a posterior distribution:
P(wt+1=k∣w1:t)=exp(U⊤kst)∑k′exp(U⊤k′st).(1)
View Sourcewhere k is the index of token wt+1 in the vocabulary, U⊤ is the transpose of matrix U, and U⊤k indicates the vector in column kth of U⊤, and k′ runs through all the indices in the vocabulary, i.e., k′∈{1,2,…,|V|}. This learning style essentially estimates a language model of code. In fact, our previous work [24] has demonstrated a good language model can be built based on LSTM, which suggest LSTM's capability to automatically learn a grammar for code [31].

Our LSTM is automatically trained using code sequences from all of the methods extracted from our dataset. During training, for every token in a sequence ⟨w1,w2,…,wn⟩, we know the true next token. For example, the true next token after “try” is “{” in the example Fig. 6. We use this information to learn the model parameters which maximize the accuracy of our next token predictions. To measure the accuracy, we use the log-loss (i.e., the cross entropy) of each true next token, i.e., −logP(w1) for token w1, −logP(w2∣w1) for token w2,…,−logP(wn∣w1:n−1) for token wn. The model is then trained using many known sequences of code tokens in a dataset by minimizing the following sum log-loss in each sequence:
L(P)=−logP(w1)−∑t=1n−1logP(wt+1∣w1:t),(2)
View SourceRight-click on figure for MathML and additional features.which is essentially −logP(w1,w2,…,wn).

Learning involves computing the gradient of L(P) during the back propagation phase, and updating the model parameters P, which consists of M, U and other internal LSTM parameters, via stochastic gradient descent.

5.2.3 Generating Output Token States
Once the training phase has been completed we use the learned LSTM to compute a code token state vector st for every code token wt extracted in our dataset. The use of LSTM ensures that a code token state contains information from other code tokens that come before it. Thus, a code token state captures the distributional semantics, a Natural Language Processing concept which dictates that the meaning of a word (code token) is defined by its context of use [32]. The same lexical token can theoretically be realized in infinite number of usage contexts. Hence a token semantics is a point in the semantic space defined by all possible token usages. Code tokens that share common usage contexts in the corpus have their token semantics located in close proximity to one another in the space. Hence, the token states capture both syntactic and semantic of code tokens. The token states are thus used for generating two distinct sets of features for a file.

5.3 Generating Local Features
Generating local features for a file involves two steps. First, we generate a set of features for each method in the file. To `do so, we first extract a sequence of code tokens ⟨w1,w2,…,wn⟩ from a method, feed it into the trained LSTM system, and obtain an output sequence of token states ⟨s1,s2,…,sn⟩ (see Section 5.2.3). We then compute the method feature vector by aggregating all the token states in the same sequence so that all information from the start to the end of a method is accumulated (see Fig. 6). This process is known as pooling and there are multiple ways to perform pooling, but the main requirement is that pooling must be length invariant, that is, pooling is not sensitive to variable method lengths.

We employ a number of simple but often effective statistical pooling methods:

Mean pooling, i.e., s¯=1n∑t=1nst;

Variance pooling, i.e., σ=1n∑t=1n(st−s¯)∗(st−s¯)−−−−−−−−−−−−−−−−−−√, where ∗ denotes element-wise multiplication; and

A concatenation of both mean pooling and variance pooling, i.e., [s¯,σ].

After the previous step, we obtain the method features for each method in a file. Since a file contains multiple methods, the next step involves aggregating all these method vectors a single vector for file. We employ again another statistical pooling mechanism to generate a set of local features for the file.

5.4 Generating Global Features
Local features are useful for within-project vulnerability prediction since they are capture the local usage context and thus tend to be project-specific. To enable effective cross-project vulnerability prediction, we need another set of features for a file which reflect how the file positions in a semantic space across all projects. We refer to these features as global features, similarly to the spirit of local and global models for defect prediction in [33].

We view a file as a set of code token states (generated from the LSTM system), each of which captures the semantic structure of the token usage contexts. This is different from viewing the file as a Bag-of-Words where a code token is nothing but an index in the vocabulary, regardless of its usage. We partition this set of token states into subsets, each of which corresponds to a distinct region in the semantic space. Suppose there are k regions, each file is then represented as a vector of k dimensions. Each dimension is the number of token state vectors that fall into the respective region.

The next challenge is how to partition the semantic space into a number of regions. To do so, we borrow the concept from computer vision by considering each token in a file as an analogy for a salient point (i.e., the most informative point in an image). The token states are akin to the set of point descriptors such as SIFT [34]. The main difference here is that in vision, visual descriptors are calculated manually, whereas in our setting token states are learnt automatically through LSTM. In vision, descriptors are clustered into a set of points called codebook (not to be confused with the software source code), which is essentially the descriptor centroids.

Similarly, we can build a “codebook” that summarizes all token states, i.e., the semantic space, across all projects in our dataset. Each “code” in the codebook represents a distinct region in the semantic space. We construct a codebook by using k-means (with Euclidian distance measure) to cluster all state vectors in the training set, where k is the pre-defined number of centroids, and hence the size of the codebook (see Part A in Fig. 7, each small circle representing a token state in the space). The example in Fig. 7 uses k=3 to produce three state clusters. For each new file, we obtain all the state vectors (Part B in Fig. 7) and assign each of them to the closest centroids (Part C in Fig. 7). The file is represented as a vector of length k, whose elements are the number of centroid occurrences. For example, the new file in Fig. 7 has 10 token vectors. We then compute the distances between those vectors to the three centroids established in the training set. We find that 3 of them are closest to centroid #1, 2 to centroid #2 and 5 to centroid #3. Hence, the feature vector for the new file is [3, 2, 5].


Fig. 7.
An example of using “codebook” to automatically learn and generate global features of a new source file.

Show All

This technique provides a powerful abstraction over a number of code tokens in a file. The intuition here is that the number of code tokens in an entire dataset could be large but the number of usage context types (i.e., the token state clusters) can be small. Hence, a file can be characterized by the types of the usage contexts which it contains. This approach offers an efficient and effective way to learn new features for a file from the code tokens constituting it. In effect, a file is a collection of distinct regions in the semantic space of code tokens. To the best of our knowledge, our work is the first to utilize the concepts of a “codebook” from computer vision to automatically learn and generate features for software code.

5.5 Vulnerability Prediction
The above process enables us to automatically generate both local and global features for all the source files in the training set. These files with their features and labels (i.e., vulnerable or clean) are then used to train machine learning classifiers. The trained model is used to predict the vulnerability of new files followed the standard process described in Section 3.1. Previous studies (e.g., [35]) conducted in industry have demonstrated that vulnerability prediction at the file level is indeed actionable. Although large source code files sometimes exist, the average file size is in the range of the hundreds of lines. Hence, inspecting a predicted source file to locate the exact locations of vulnerabilities is still feasible [35]. Although predictions at a finer level of granularity (e.g., line-level) potentially decrease manual inspection effort, they may well come with the cost of a severe reduction in accuracy. For example, the study in [35] shows that predicting vulnerabilities at the binary level (i.e., collections of source files) was more accurate than doing that at the file level. Predicting many false positives may reduce the usefulness of the machinery, and thus the user trust in the results. In addition, our approach learns features at the code token level. Those features can be aggregated to represent a line of code, or at a method and a whole source file level as we have demonstrated in this paper. We chose to apply our approach at the file level because almost all existing work and datasets in vulnerability prediction operates at this level of granularity. This facilitated us in training our models and performing comparisons.

To evaluate the impact of a classifier, we tested our approach with four widely-used classifiers: Random Forests, Decision Tree, Naive Bayes, and Logistic Regression. Random Forests has been shown to be an effective classifier for vulnerability prediction in previous studies [8], [26]. Random Forests belongs to a family of randomized ensemble methods which combines the estimates from many “weak classifiers” to make their prediction. Random Forests (RFs) [36] uses decision trees as weak learners. Those trees are trained using randomly sampled subsets of the full dataset. At each node of a decision tree, RFs find the best splitting feature (i.e., predictor) for the node from a randomly selected subset of features. For example, we might select a random set of 10 features (among 200 features) in each node, and then split using the best feature among these 10 features. Thus, RFs randomizes both training samples and feature selection to grow the trees. The same process is applied to generate more trees, each of which is trained using a slightly different sample each time. In practice, 100 to 500 trees are usually generated. To make predictions for a new instance, RFs combines all separate predictions made by each of the generated decision tree typically by averaging the outputs across all trees. Decision Tree (C4.5) generates decision nodes based on the information gain using the value of each factors. Decision tree classifier is widely used in practice due to its explainability. Naive Bayes works based the assumption that features are conditionally independent when the outcome is known. Despite of this naive assumption, Naive Bayes has been found to be an effective classifier. Logistic Regression uses the logistic sigmoid function to return a probability value for a given set of input features. This probability is then mapped into two or more discrete classes for classification purposes.

SECTION 6Implementation
The proposed approach is implemented in Theano [37] and Keras [38] frameworks, running in Python. Theano supports automatic differentiation of the loss in Eq. (2) and a host of powerful adaptive gradient descent methods. Keras is a wrapper making model building much easier.

6.1 Training Details
We use RMSprop as the optimizer and use the standard learning rate of 0.02, and smoothing hyper-parameters: ρ=0.99, and ϵ=1e−7. The model parameters are updated in a stochastic fashion, i.e., after every mini-batch of size 50. We use |V|=5,000 most frequent tokens for learning the code language model discussed in Section 5.2. We use dropout rate of 0.5 at the hidden output of LSTM layer. These parameter settings are the standard ones used in the literature.

The main classifier used is Random Forest implemented using the scikit-learn toolkit. Hyper-parameters are tuned for best performance and include (i) the number of trees, (ii) the maximum depth of a tree, (iii) the minimum number of samples required to split an internal node and (iv) the maximum number of features per tree. The code is run on Intel(R) Xeon(R) CPU E5-2670 0 @ 2.6 GHz. There machine has two CPUs, each has 8 physical cores or 16 threads, with a RAM of 128 GB.

6.2 Handling Large Vocabulary
To evaluate the prediction probability in Equation (1) we need to iterate through all unique tokens in the vocabulary. Since the vocabulary's size is large, this can be highly expensive. To tackle the issue, we employ an approximate method known as Noise Contrastive Estimation [39], which approximates the vocabulary at each probability evaluation by a small subset of words randomly sampled from the vocabulary. We use 100 words, as it is known to work well in practice [24].

The Noise Contrastive Estimation method replaces the expensive normalization through all tokens in the vocabulary by a simple logistic model on a small set of tokens. This ensures theoretically that the proper probability of each seen token is maintained given enough data. Hence, for large datasets, there is little loss in accuracy. This is a simple mathematical technique to improve the computation time of using an expensive normalization in a probabilistic model (e.g., the softmax), not a way to trade off quality for speed. Please note that we did not split long tokens into short ones.

6.3 Handling Long Methods
Methods are variable in size. This makes learning inefficient because we need to handle each method separately, not making use of recent advances in Graphical Processing Units (GPUs). A better way is to handle methods in mini-batches of fixed size. A typical way is to pad short methods with dummy tokens so that all methods in the same mini-batch have the same size. However, since some methods are very long, this approach will result in a waste of computational effort to handle dummy tokens. Here we use a simple approach to split a long method into non-overlapping sequences of fixed length T, where T=100 is chosen in this implementation due to the faster learning speed. For simplicity, features of a method are simply the mean features of its sequences.

SECTION 7Evaluation
7.1 Datasets
To carry out our empirical evaluation, we exploited two publicly available datasets that have been used in previous work for vulnerability prediction.

7.1.1 Android Dataset
This dataset [40] that has been used in previous work [8] for vulnerability prediction. This dataset originally contained 20 popular applications which were collected from F-Droid and Android OS in 2011. The dataset covers a diversity of application domains such as education, book, finance, email, images and games. However, the provided dataset only contained the application names, their versions (and dates), and the file names and their vulnerability labels (i.e., clean or vulnerable files). It did not have the source code for the files, which is needed for our study. Using the provided file names and version numbers, we then retrieved the relevant source files from the code repository of each application.

We could not find the code repository for two applications since they appeared no longer available1 For some apps, the number of versions we could retrieve from the code repository is less than that in the original datasets. For example, we were able to retrieve the source files for 16 versions of Crossword while the original dataset had 17 versions. The source files for some older versions were no longer maintained in the code repository.

Our resultant dataset contains applications from two sources: 9 applications from F-Droid repository and 9 applications pre-installed with Android OS. All the nine F-Droid applications had over 10,000 downloads, and five of them had more than 1 million downloads. There were 21 types of vulnerabilities existing across all of the applications in the dataset, such as log forging, information leak, unreleased resource, denial of service, race condition, cross-site scripting, command injection, privacy violation and header manipulation. Among them, privacy violation, log forging and denial of service are the top three common vulnerabilities found in the dataset. Table 1 provides some descriptive statistics for 18 apps in our dataset, including the number of versions, the total number of files, the average number of files in a version, the average number of lines of code in a version, the average number of vulnerable files in a version, and the ratio of vulnerable files. The dataset contains more than 240K sequences, in which, 200k sequences are used for training and the others are used for validation. The LSTM which achieved the best perplexity on the validation set was finally kept for feature extraction.

TABLE 1 Dataset Statistics

7.1.2 Firefox Dataset
The Firefox vulnerabilitity dataset was previously built by Shin and Williams [12]. They collected vulnerabilities reported for Firefox 2.0 from the Mozilla Foundation Security Advisories (MFSAs). They linked these MFSA reports with bug reports in Firefox, and used this information to identify the files in Firefox 2.0 that contain the reported vulnerabilities. The dataset has all 11,051 source files, all of them from Firefox 2.0. There are 363 vulnerable files, accounting for 3 percent of the total files. From the dataset they provided, we retreived the code of those source files from the archived Firefox code repository.

7.2 Research Questions
We followed previous work in vulnerability prediction [8] and aimed to answer the following standard research questions:

RQ1. Within-project prediction: Are the automatically learned features using our LSTM-based approach suitable for building a useful vulnerability prediction model?

RQ2. Cross-version prediction: How does our proposed approach perform in predicting future versions, i.e., when the model is trained using an older version in application and tested on a newer version in the same application?

RQ3. Cross-project prediction: Is our approach suitable for cross-project predictions where the model is trained using an application and tested on a different application?

7.3 Experimental Settings
We designed three different experimental settings to answer the above research questions.

7.3.1 Within-Project Prediction
In this setting, both training and testing data is from the same version of an application. Specifically, for each application in our dataset, we select all the source files in the first version, and use them for training and testing a prediction model. We employ stratified cross-fold validation by dividing the files into 10 folds {fo1,fo2,…,fo10}, each of which has the approximately same ratio between vulnerable files and clean files. For each fold foi, we select it as the test set, and the remaining folds {fo1,…,foi−1,foi+1,…,fo10} are used for training a prediction model. We then measure the performance of the prediction model using the source files in the fold foi. We repeat this process for each of the 10 folds, and then compute the average performance.

7.3.2 Cross-Version Prediction
In this second experimental setting, a prediction model is trained using all the source files in one version of an application. It is then tested using the source files from all subsequent versions. For example, the first version in the Crosswords app is used to training and each of the remaining 15 versions is used as a test set.

We also experimented with another setting in which the model training is updated over time. Specifically, the model is trained using all of the previous releases rather than only the first one. For example, if an application has 10 versions, we conduct 9 different runs. In first run, the model is trained using the version 1 and tested using the remaining 9 versions. In the second run, the model is trained using versions 1 and 2, and tested using the remaining 8 versions. We repeat this process and compute the average performance.

7.3.3 Cross-Project Prediction
In this third experiment, we used all the source files in the first version of each application to train a prediction model. The model is then tested on the first version of the remaining 17 applications, i.e., it is tested 17 times. We compute the performance in each test and use the average performance. We repeat this procedure for all applications, resulting in 18 different prediction models, one for each application used for training.

7.4 Benchmarks
We compare the performance of our approach against the following benchmarks:

Software Metrics. Complexity metrics have been extensively used for defect prediction (e.g., [41]) and vulnerability prediction (e.g., [9], [10], [11]). This is resulted from the intuition that complex code is difficult to understand, maintain and test, and thus has a higher chance of having vulnerabilities than simple code. We have implemented a vulnerability prediction models based on 60 metrics. These features are commonly used in existing vulnerability prediction models. They covers 7 categories: cohesion metrics (i.e., measure to what extent the source code elements are coherent in the system), complexity metrics (i.e., measure the complexity of source code elements such as algorithms), coupling metrics (i.e., measure the amount of interdependencies of source code elements), documentation metrics (i.e., measure the amount of comments and documentation of source code elements), inheritance metrics (i.e., measure the different aspects of the inheritance hierarchy), code duplication metrics (i.e.), and size metrics (e.g., number of code lines, and number of classes or methods).

Bag of Words. This technique has been used in previous work [8], which also considers source code as a special form of text. Hence, it treats a source code file as a collection of terms associated with frequencies. The term frequencies are the features which are used as the predictors for a vulnerability prediction model. Lexical analysis is done to source code to break it into a vector of code tokens and the frequency of each token in the file is counted. We also followed previous work [8] by discretizing the BoW features since they found that this method significantly improved the performance of the vulnerability prediction models. The discretization process involves transforming the numerical BoW features into two bins. If a code token occurs more than a certain threshold (e.g., 5 times) than it is mapped to one bin, otherwise it is mapped to another bin.

Deep Belief Network. Recent work [15] has demonstrated that Deep Belief Networks (DBN) [42] work well for defect prediction. DBN is a family of stochastic deep neural networks that extract multiple layers of data representation. In our implementation, DBN takes the word counts per file as input and produces a latent posterior as output, which is then used as a new file representation. Since the standard DBN accepts only input in the range [0,1], we normalize the word counts by dividing each dimension to its maximum value accross the entire training data. The DBN is then built in a stage-wise fashion as follows. At the bottom layer, a Restricted Boltzmann Machine (RBM) is trained on the normalized word count. An RBM is a special two-layer neural network with binary neurons. Unlike the standard neural networks, which learn a mapping from an input to an output in an supervised manner, RBM learns a distribution of data using unsupervised learning (i.e., without labels). Following the standard practice in the literature, the RBM is trained using Contrastive Divergence [42]. After the first RBM is trained, its posterior is used as the input for the next RBM, and the training is repeated. Finally, the two RBMs are stacked on top of each other to form a DBN with two hidden layers. The posterior of the second RBM is used as the new file representation. In our implementation, the two hidden layers have the size of 500 and 128, respectively.

To enable a fair comparison, we used the same classifier for our prediction models and all the benchmarks. We chose Random Forests (RF), an ensemble method which combines the estimates from multiple estimators since it has been shown to be one of the most effective classifiers for vulnerability prediction [8]. We used the implementation of RF provided with the scikit-learn2 toolkit. We performed tuning with the following hyper-parameters: the number of trees, the maximum depth of a tree, the minimum number of samples required to split an internal node, and the maximum number of features. The same hyper-parameter tuning was done for our prediction models and all the benchmarks.

7.5 Performance Measures
A confusion matrix is used to evaluate the performance of our predictive models. The confusion matrix is then used to store the correct and incorrect decisions made by a classifier. For example, if a file is classified as vulnerable when it was truly vulnerable, the classification is a true positive (tp). If the file is classified as vulnerable when actually it was not vulnerable, then the classification is a false positive (fp). If the file is classified as clean when it was in fact vulnerable, then the classification is a false negative (fn). Finally, if the file is classified as clean and it was in fact clean, then the classification is true negative (tn). The values stored in the confusion matrix are used to compute the widely-used Precision, Recall, and F-measure for the vulnerable files. These measures have also been widely used in the literature (e.g., [5], [8], [10], [11]) for evaluating the predictive performance of vulnerability prediction models.

Precision (Prec): The ratio of correctly predicted delayed issue over all the issues predicted as delayed issue. It is calculated as:
pr=tptp+fp.
View Source

Recall (Re): The ratio of correctly predicted delayed issue over all of the actually issue delay. It is calculated as:
re=tptp+fn.
View Source

F-measure: Measures the weighted harmonic mean of the precision and recall. It is calculated as:
F−measure=2∗pr∗repr+re.
View Source

Area Under the ROC Curve (AUC) is used to evaluate the degree of discrimination achieved by the model. The value of AUC is ranged from 0 to 1 and random prediction has AUC of 0.5. The advantage of AUC is that it is insensitive to decision threshold like precision and recall. The higher AUC indicates a better predictor.

7.6 Results
7.6.1 Learned Code Token Semantics
An important part of our approach is learning the semantics of code tokens using the context of its usage through a LSTM approach. Fig. 8 shows the top 2,0003 frequent code tokens used in our dataset. They were automatically grouped in 100 clusters (only 10 are shown in Fig. 8) using K-means clustering based on their token states learned through LSTM. Recall that these clusters are the basis for us to construct a codebook (discussed in Section 5.4). We used t-distributed stochastic neighbor embedding (t-SNE) [43] to display high-dimensional vectors in two dimensions. We show here some representative code tokens from some clusters for a brief illustration. Code tokens that are semantically related are grouped in the same cluster. For example, code tokens related to exceptions such as IllegalArgumentException, FileNotFoundException, and NoSuchMethodException are grouped in one cluster. This indicates, to some extent, that the learned token states effectively capture the semantic relations between code tokens, which is useful for us to later learn both syntactic and semantic features.

Fig. 8. - 
Top 2,000 frequent code tokens were automatically grouped into clusters (each cluster has a distinct color).
Fig. 8.
Top 2,000 frequent code tokens were automatically grouped into clusters (each cluster has a distinct color).

Show All

7.6.2 Within-Project Prediction (RQ1)
We experimented with a number of variations of our approach by varying the pooling techniques (mean pooling and standard deviation pooling) and the use of local and global features. We report here the results of using Random Forests as the classifier (the same for RQ2 and RQ3). For space reasons, the detailed results of using Decision Tree, Naive Bayes, and Logistic Regression are reported in Appendix A, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TSE.2018.2881961. Table 2 reports the precision, recall, F-measure and AUC for the three variations of our approach: using only local features, using only global features, and using a joint set of both features types. Note that both the local feature option and the joint feature option we reported here used mean pooling for generating method features and standard deviation pooling for generating syntactic file features.

TABLE 2 Within-Project Results (RQ1) for the Three Variations of Our Approach

Our approach obtained a strong result: in all 18 Android applications, they all achieved well above 80 percent in across all the four measures: precision, recall, F-measure, and AUC (see Table 2). Among the three variations of our approach, using only local features appeared to be the best option for vulnerability prediction. This result is consistent with our underlying theory that local features are project-specific and are thus useful for within-project prediction. This option delivered the highest precision, recall, F-measure, and AUC averaging across 18 applications. The prediction model using local features achieved over 90 percent F-measure in 13 out of 18 applications and over 90 percent AUC in all 18 applications.

Our approach outperforms the three benchmarks to varying extents (see Fig. 9). Fig. 9 shows an number of box plots to demonstrate the performance improvement of our approach over an existing technique. Each box plot shows the distribution of the improvement across the seventeen projects used in our dataset (e.g., AnkiAndroid, Boardgamegeek, etc.) for a specific performance indicator (e.g., F-measure). The bottom and top of a box are the first and third quartiles, and the band inside the box indicate the second quartile (i.e., the median). The lines extending parallel from the boxes indicate variability outside the upper and lower quartiles, and individual dots indicate outliers. For example, the first box plot on the left-hand-side of Fig. 9 shows the improvement distributed across the seventeen projects of our model over the software metrics approach in terms of F-measure. As can also be seen in Fig. 9, across the 17 projects, the median improvement is approximately 45 percent (refer to Appendix C, available in the online supplemental material, for the distribution of the absolute values of a given performance measure).


Fig. 9.
The percentage performance difference when applying the three benchmarks (software metrics, Bag of Words and Deep Belief Network) against using our local feature approach for each performance measure in within-project prediction.

Show All


Fig. 10.
Within-project results (RQ1) for the three variations of our approach on the Firefox dataset with classification threshold varying from 0.01 to 0.91. “Joint features” indicates the use of both local features and global features.

Show All


Fig. 11.
The percentage performance difference when applying the three benchmarks (software metrics, Bag of Words and Deep Belief Network) against using our local feature approach for each performance measure in cross-version prediction.

Show All

For all 18 Android applications, our local feature approach consistently outperformed the software metric approach in all performance measures. For over half of the applications, our approach offered over 50 percent improvement in F-measure over the software metrics approach. In some applications (e.g., Boardgamegeek and Deskclock), the improvements were over 200 percent. The Bag-of-Words (BoW) and Deep Belief Network approaches also achieved good results (on average 89–90 percent F-measure). This is consistent with the findings in previous work [8] in the case of BoW. Hence, the improvements brought by our approach over these two benchmarks were not as big as those over the software metric approach (see Fig. 9). Our approach outperformed the BoW approach in 15 out of 18 applications, and in most cases the improvements ranged from 4 to 20 percent. The results show that Random Forests clearly outperform Decision Tree, Logistic Regression and Naive Bayes for within-project vulnerability prediction in all three settings: using local features, global features, and both of them. The other three classifiers (Logistic Regression, Naive Bayes and Decision Tree) produced mixed results (see Figs. 12, 13, and 14 in Appendix A, available in the online supplemental material). The improvement brought by our approach over BoW and DBN was not as clear for using those classifiers as it was for using Random Forests.

For the Firefox application, the dataset is highly unbalanced4 in that only 3 percent of the 11,051 files are vulnerable (i.e., minor class). Thus, a classification model tends to have a bias towards predicting files as being clean (i.e. major class) in order to reducing training errors. This may result in the vulnerable class having high false negatives. To deal with this issue, previous studies (e.g., [12]) often employ class rebalancing techniques (e.g., oversampling or undersampling) to make the training set balanced. Those resampling techniques however create an artificial bias towards minor classes. An alternative is imposing an additional cost on the model for making classification mistakes on the minor class during training (i.e., penalized classification). These costs enable the model to pay more attention to the minority class. We have conducted experiments using undersampling, oversampling and penalized classification. The results of using penalized classification are reported here, while the results of using undersampling and oversampling techniques are reported in Appendix D, available in the online supplemental material.

Similarly to previous study [12] on this Firefox dataset, we have performed a sensitivity analysis on the classification threshold in which we evaluated our prediction models at 91 classification thresholds, ranging from 0.01 to 0.91 at an interval of 0.01 (instead of selecting a single default threshold). The classification threshold is used by the model to classify files based on the predicted probabilities.

Overall, the models using global features and joint features outperformed the model using local features (see Fig. 10). Using global or joint features consistently achieved well above 0.8 AUC for all classification thresholds while the AUC produced by the model using local features is below 0.6. Using local features produced high recall but low precision at the threshold up to 0.45, but recall dramatically decreases and precision rapidly increases when the threshold is set above 0.45. The model using global or joint features produced a more balance between precision and recall although there are also regions where recall or precision is optimized and vice versa. If the security engineers are reluctant to take risks and they have sufficient resources for testing and inspection, it is recommended that they select a model which has a good performance at a low threshold. On the other hand, if they have limited resource for testing and inspection, they can select a model which produces high precision at a high threshold. Compared to the model developed by Shin and Williams [12], our models using joint or global features consistently produced higher precision at all the thresholds. For example, at threshold 0.91, our model using joint features achieved 0.60 percent precision, offering 15 percent improvement over the model developed by Shin and Williams [12], while the improvement was 130 percent at threshold 0.5. Our model also produced a better balance between precision and recall, resulting in higher F-measure at most of the thresholds compared to the Shin and Williams’ models.

Answer to RQ1: Features automatically learned from source code using LSTM can be used to build highly accurate prediction models for detecting vulnerabilities in software applications.

7.6.3 Cross-Version Prediction (RQ2)
Table 3 reports the results in a cross-version setting where a prediction model was trained using the first version of an application and tested using the subsequent versions of the same applications. Note that the Boardgamegeek application had only one version and thus was excluded from this experiment. Hence, there were 17 application evaluated in this experiment. The results demonstrate that the three variations of our approach again achieved strong predictive performance in all criteria. Average precision, recall and F-measure values are approximately 85 percent, while average AUC values are above 90 percent.

TABLE 3 Cross-Version Results (RQ2) for the Three Variations of Our Approach

The use of global features has improved the generalization of the prediction models in the cross-version setting. Using both local features and global features appeared to be the best option in this setting. This approach has achieved well above 80 percent in all the four performance measures in 14 out of 17 applications. Our approach (using joint features) outperformed the software metrics approach with respect to all performance measures in all 17 applications (see Fig. 21 and refer to Appendix C, available in the online supplemental material, for the distribution of the absolute values of a given performance measure). The average improvement over the software metric approach is 15 percent for F-measure and 20 percent for AUC.

Our approach outperformed the BoW approach in 16 out of 17 cases in terms of F-measure and AUC, and the DBN approach in 15 out of 17 cases in F-measure (and 16 cases for AUC) see Fig. 11. Note that the BoW and DBN approaches also performed well in cross-version prediction, i.e. their average F-measure are above 80 percent. In some cases (e.g., the Camera application), they achieved high recall (e.g., 88 percent for DBN and 90 percent for BoW) but low precision (e.g., 58 and 59 percent). In those cases, our approach achieved a more balance performance between recall and precision. For example, in the Camera application, our approach achieved lower recall (i.e. 71 percent) than BoW and DBN did, but it produced a higher precision (i.e. 79 percent), and thus lead to 10–12 percent improvement in F-measure and 8–10 percent improvement in AUC. The results show that Random Forests clearly outperform Decision Tree, Logistic Regression and Naive Bayes for cross-version vulnerability prediction in all three settings: using local features, global features, and both of them. When using Logistic Regression and Decision Tree as the classifier, similar improvements were also observed (see Figs. 15 and 16 in Appendix A, available in the online supplemental material). The improvement brought by our approach over BoW and DBN was not as clear for Naive Bayes (see Fig. 17 in Appendix A, available in the online supplemental material) as it was for using the other three classifiers.

Table 4 shows the results in the setting which the model training is updated over time. Here, the model was trained using using all the previous releases (rather than only the first one). The results were averaged of all runs in each project. The results are slightly improved, suggesting that updating model training over time is helpful.

Answer to RQ2: Our predictive model, which is trained using an older version of a software application, can produce highly accurate predictions of vulnerable components in the new versions of the same application.

TABLE 4 Cross-Version Results (RQ2) for the Three Variations of Our Approach, Using Random Forest in the Updated Training Setting

7.6.4 Cross-Project Prediction (RQ3)
This experiment followed the setup in previous work [8]. We first built 18 prediction models, each of which use the first version of each application for training. Each model was then tested using the first version of the other 17 applications. Hence, for each prediction method, there are 18 x 17 (i.e. 306) different settings. We ran this experiment with the three benchmarks and variations of our approach. In this experiment, we do not focus on the raw performance in each setting. Instead, we follow the same procedure as previously done in our benchmark [8], and focus on assessing how many applications a model can be effectively applied to. We used the same baseline as in previous work [8]: a model is applicable to a tested application if both precision and recall are above 80 percent.

For each application, Table 5 reports the number of other applications to which the corresponding models can be applied. The results show that using global features improves the general applicability of prediction models. All the models using both semantic and syntactic features were successfully applicable to at least one other application. Some of them (e.g., K9 and Mustard) are even applicable to 8 other applications. In this cross-project prediction setting, our approach also offers bigger improvements over the BoW and DBN benchmarks. On average, a joint-feature model is applicable to around 4 other applications, approximately doubling the number of applications achieved by BoW or DBN models. The results also show that Random Forests clearly outperform Decision Tree, Logistic Regression and Naive Bayes for cross-project vulnerability prediction in all three settings: using local features, global features, and both of them. In addition, our approach also outperforms BoW and DBN when using Decision Tree and Naive Bayes as the classifier (see Tables 12 and 14 in Appendix A, available in the online supplemental material), but this was not the case when using Logistic Regression as the classifier (see Table 13 in Appendix A, available in the online supplemental material).

TABLE 5 Cross-Project Results (RQ3) for the Three Benchmarks and Three Variations of Our Approach

Cross-project prediction is always challenging due to the potentially significant differences (e.g., coding styles and functionalities) between projects. Although the result is encouraging, we however acknowledge that further work is needed to improve this result to make it more applicable in practice. One potential improvement is learning the features from a set of diverse applications (rather than one single application).

Answer to RQ3: Some predictive models, which were trained and used features automatically learnt from a software application, can predict vulnerable software components in other software applications.

7.7 Discussion
The high performance of BoW on within-project prediction (RQ1 and RQ2) is not totally surprising for two reasons. One is that BoW has been known as a strong representation for text classification, and source code is also a type of text representing an executable programming language. The other reason is that although the training files and testing files are not identical, a project typically has many versions, and the new versions of the same file may carry a significant amount of information from the old versions. The repeated information used can come from multiple forms: fully repeated pieces of code, the same BoW statistics, or the same code convention and style. Thus any representation that is sufficiently expressive and coupled with highly flexible classifiers such as Random Forests, will likely to work well.

However, this is not the case for cross-project prediction (RQ3). This is when the BoW statistics are likely to be different between projects, and knowledge learned from one project may not transfer well to others. In machine learning and data mining, this problem is known as domain adaptation, where each project is a domain. The common approach is to learn the common representation across domains, upon which classifiers will be built. This is precisely what is done using the LSTM-based language model and codebook construction. Note that the LSTM and codebook are learned using all available data without supervision. This suggests that we can actually use external data, even if there are no vulnerability labels. The competitive performance of the proposed deep learning approach clearly demonstrates the effectiveness of this representation learning. To conclude, when doing within-project prediction, it is useful to use BoW due to its simplicity. But when generalizing from one project to another, it is better to use representation learning. We recommend using LSTM for language model, and codebook for semantic structure discovery.

Finally, almost all existing work (e.g., [5], [8], [9], [10], [11], [26], [27], [28]) in vulnerability prediction operates at the file level. Since this type of prediction (i.e. with the file granularity) is standard in the related work, we chose to work at the file level to leverage existing datasets and facilitate comparison against state-of-the-arts. However, our approach is able to learn features at the code token level, and thus it may work beyond the file granularity. In fact, since we consider a method as a sequence of code tokens, our current model is already able to automatically learn and generate features for the method. These features can be used to build a model for predicting vulnerabilities at the method level (discussed in Section 5). In the same manner, we can treat each line of code as a sequence of code tokens and use aggregation to obtain features for each code line. Thus, our approach is also potentially applicable to vulnerability prediction at line level. Training a prediction model at those levels of granularity requires corresponding groundtruths, i.e. methods or code lines which have been labelled as vulnerable or clean. The existing vulnerability datasets (like the one we used from [8] and others used in previous studies) unfortunately do not contain labels at the method or code line levels. Once such datasets become available, our model is readily extensible to leverage them. Alternatively, some recent studies in the general defect prediction area target at the method level (e.g., [44]) and the line level (e.g., [45]). Hence, another possibility, which we will investigate in our future work, is extending our model to general defect prediction and making use of those datasets.

SECTION 8Threats to Validity
There are a number of threats to the validity of our study, which we discuss below.

Construct Validity. We mitigated the construct validity concerns by using a publicly available dataset that has been used in previous work [8]. The dataset contains real applications and vulnerability labels of the files in those applications. The original dataset did not unfortunately contain the source files. However, we have carefully used the information (e.g., application details, version numbers and date) provided with the dataset to retrieve the relevant source files from the code repository of those applications. We acknowledge that this dataset may contain false positives and one approach to deal with this is manually removing them from the dataset. However, the dataset from [8] does not provide detailed reports of the vulnerabilities, and thus we were unable to remove the false positives. However, as reported in [8], they have removed the false positives in two applications (AnkiDroid and Mustard) and found that it did not negatively affect the result in these two cases. This may suggest that the performance of our approach would not also be negatively affected by removing false positives from the dataset. In addition, as can be seen in Appendix B, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/2881961, our models also produce highly accurate predictions of clean components (i.e. negative cases) in all three settings (within-project, cross-version and cross-projects).

Conclusion Validity. We tried to minimize threats to conclusion validity by using standard performance measures for vulnerability prediction [8], [10], [11], [26]. We however acknowledge that a number of statistical tests [46] can be applied to verify the statistical significance of our conclusions. Although we have not seen those statistical tests being used in previous work in vulnerability prediction, we plan to do this investigation in our future work. The benchmarks (Software Metrics, Bag of Words, Deep Belief Network) are techniques that have been proposed and implemented in existing work (e.g., [8], [15]). However, their original implementations (e.g., the tools) were not made publicly available, and thus we were not able to have access to them. Hence, we had to re-implement our own version of those techniques. Although we closely followed the procedures described in their work, we acknowledge that our implementation might not have all of the details, particularly those not explicitly described in their papers, of the original implementation. However, to ensure that our implementation reflects the original implementation, we tested our implementation using the same dataset, and our implementation produces similar and consistent results.

Internal Validity. The dataset we used contains vulnerability labels only for Java source files. In practice, other files (e.g., XML manifest files) may contain security information such as access rights. Another threat concerns the cross-version prediction where we replicated the experiment done in [8] and allowed that the exactly same files might be present between versions. This might have inflated the results, but all the prediction models which we compared against in our experiment benefit from this.

External Validity. We have considered a large number of applications which differ significantly in size, complexity, domain, popularity and revision history. We however acknowledge that our data set may not be representative of all kinds of software applications. Further investigation to confirm our findings for other types of applications such as web applications and applications written in other programming languages such as PHP and C++.

SECTION 9Related Work
9.1 Vulnerability Prediction
Machine learning techniques have been widely used to build vulnerability prediction models. Early approaches (e.g., [9]) employed complexity metrics such as (e.g., McCabe's cyclomatic complexity, nesting complexity, and size) as the predictors. Later approaches enriched this software metric feature set with coupling and cohesion metrics (e.g., [11]), code churn and developer activity metrics (e.g., [10])), and dependencies and organizational measures (e.g., [5]). Those approaches require knowledgeable domain experts to determine the metrics that are used as predictors for vulnerability.

Recent approaches treat source code as another form of text and leverage text mining techniques to extract the features for building vulnerability prediction models. The work in [8] used the Bag-of-Words representation in which a source code file is viewed as a set of terms with associated frequencies. They then used the term-frequencies as the features for predicting vulnerability. BoW models produced higher recall than software metric models for PHP applications [26]. A recent case study [27] for the Linux Kernel also suggested the superiority of BoW in vulnerability prediction compared to using code metrics as features. However, the BoW approach carries the inherent limitation of BoW in which syntactic information such as code order is disregarded. These approaches also rely on manual feature engineering in which discriminative features are extracted by a deliberate process of combining primitive components such as tokens. Examples of such features are n-grams, topics, special selection of keywords, types of expression, etc. These features are often sufficient for shallow models such as Naive Bayes, SVM and Random Forests to perform well. In deep learning, tokens are used as the starting point because they are the smallest unit readily available in code (we can argue that characters are the smallest, but tokens are easier to reason about). No further domain knowledge is assumed other than the fact that code is a sequence of tokens. The higher level features are learnt automatically by estimating parameteric neurons at multiple levels of abstraction.

9.2 Defect Prediction
Predicting vulnerabilities is related to software defect prediction, which is a very active area in software analytics. Since defect prediction is a broad area, we highlight some of the major work here, and refer the readers to other comprehensive reviews (e.g., [47], [48]) for more details. Code metrics were commonly used as features for building defect prediction models (e.g., [41]). Various other metrics have also been employed such as change-related metrics [49], [50], developer-related metrics [51], organization metrics [52], and change process metrics [53]. The study in [12] found that some defect prediction models can be adapted for vulnerability prediction. However, most of those models are not directly transferred to predicting secutiry vulnerabilities [8].

Recently, a number of approaches (e.g., [15], [54]) have leveraged a deep learning model called Deep Belief Network (DBN) [55] to automatically learn features for defect prediction and have demonstrated an improvement in predictive performance. DBN however does not naturally capture the sequential order and long-term dependencies in source code. Most of the studies in defect prediction operate at the file level. Recent approaches address this issue at the method level (e.g., [44]) and the line level (e.g., [45]). Since our approach is able to learn features at the code token level, it may work at those finer levels of granularity. However, this would require the development of a vulnerability dataset for training that contains methods and codelines with vulnerability labels, which do not currently exist.

9.3 Deep Learning in Code Modeling
Deep learning has recently attracted increasing interests in software engineering. In our recent vision paper [56], we have proposed DeepSoft, a generic deep learning framework based on LSTM for modeling both software and its development and evolution process. We have demonstrated how LSTM is leveraged to learn long-term temporal dependencies that occur in software evolution and how such deep learned patterns can be used to address a range of challenging software engineering problems ranging from requirements to maintenance. Our current work realizes one of those visions.

The work in [17] demonstrated the effectiveness of using recurrent neural networks (RNN) to model source code. Their later work [57] extended these RNN models for detecting code clones. The work in [58] uses a special RNN Encoder–Decoder, which consists of an encoder RNN to process the input sequence and a decoder RNN with attention to generate the output sequence, to generate API usage sequences for a given API-related natural language query. The work in [59] also uses RNN Encoder–Decoder but for fixing common errors in C programs. The work in [60] uses Convolutional Neural Networks (CNN) [61] for bug localization. Preliminary results from our earlier work [24] also suggest that LSTM is a more effective language model for source code. Our work is built on this language model to automatically learn both syntactic and semantic features for predicting vulnerable code components.

SECTION 10Conclusions and Future Work
This paper proposes to leverage Long-Short Term Memory, a representation deep learning model, to automatically learn features directly from source code for vulnerability prediction. The learned syntactic features capture the sequential structure in code at the method level, while semantic features characterize a source code file by usage contexts of its code tokens. We performed an evaluation on 18 Android applications from a public dataset provided in previous work [8]. The results for within-project prediction demonstrate that our approach achieved well above 80 percent in all performance measures (precision, recall, F-measure, and AUC) in all 18 Android applications. When using Random Forests as the classifier, our approach also outperforms the traditional software metrics approach (74 percent improvement on average), the Bag-of-Words approach (4.5 percent improvement on average) and another deep learning approach, Deep Belief Network (5.2 percent improvement on average). For cross-project prediction, the results suggest that a predictive model, which was trained from an Android application using our approach, can predict vulnerable software components in (on average) 4 other Android applications with both precision and recall above 80 percent – doubling the number of applications achieved by either Bag-of-Words or Deep Belief Network. An evaluation on the Firefox application also demontrates that our models improved from 23 to 175 percent in precision compared to existing models.

Our future work involves applying this approach to other types of applications (e.g., Web applications) and programming languages (e.g., PHP or C++) where vulnerability datasets are available. We also aim to leverage our approach to learn features for predicting vulnerabilities at the method and code change levels. In addition, we plan to explore how our approach can be extended to predicting general defects and safety-critical hazards in code. Finally, our future investigation involves building a fully end-to-end prediction system from raw input data (code tokens) to vulnerability outcomes.