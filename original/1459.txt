The complexity and dynamism of microservice systems pose unique challenges to a variety of software engineering tasks such as fault analysis and debugging. In spite of the prevalence and importance of microservices in industry, there is limited research on the fault analysis and debugging of microservice systems. To fill this gap, we conduct an industrial survey to learn typical faults of microservice systems, current practice of debugging, and the challenges faced by developers in practice. We then develop a medium-size benchmark microservice system (being the largest and most complex open source microservice system within our knowledge) and replicate 22 industrial fault cases on it. Based on the benchmark system and the replicated fault cases, we conduct an empirical study to investigate the effectiveness of existing industrial debugging practices and whether they can be further improved by introducing the state-of-the-art tracing and visualization techniques for distributed systems. The results show that the current industrial practices of microservice debugging can be improved by employing proper tracing and visualization techniques and strategies. Our findings also suggest that there is a strong need for more intelligent trace analysis and visualization, e.g., by combining trace visualization and improved fault localization, and employing data-driven and learning-based recommendation for guided visual exploration and comparison of traces.
SECTION 1Introduction
Microservice architecture [1] is an architectural style and approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. Microservice architecture allows each microservice to be independently developed, deployed, upgraded, and scaled. Thus, it is particularly suitable for systems running on cloud infrastructures and requiring frequent updating and scaling of their components.

Nowadays, more and more companies have chosen to migrate from the so-called monolithic architecture to microservice architecture [2], [3]. Their core business systems are increasingly built based on microservice architecture. Typically a large-scale microservice system can include hundreds to thousands of microservices. For example, Netflix's online service system [4] uses about 500+ microservices and handles about two billion API requests every day [5]; Tencent's WeChat system [6] accommodates more than 3,000 services running on over 20,000 machines [7].

A microservice system is complicated due to the extremely small grained and complex interactions of its microservices and the complex configurations of the runtime environments. The execution of a microservice system may involve a huge number of microservice interactions. Most of these interactions are asynchronous and involve complex invocation chains. For example, Netflix's online service system involves 5 billion service invocations per day and 99.7 percent of them are internal (most are microservice invocations); Amazon.com makes 100-150 microservice invocations to build a page [8]. The situation is further complicated by the dynamic nature of microservices. A microservice can have several to thousands of physical instances running on different containers and managed by a microservice discovery service (e.g., the service discovery component of Docker swarm). The instances can be dynamically created or destroyed according to the scaling requirements at runtime, and the invocations of the same microservice in a trace may be accomplished by different instances. Therefore, there is a strong need to address architectural challenges such as dealing with asynchronous communication, cascading failures, data consistency problems, discovery, and authentication of microservices [9].

The complexity and dynamism of microservice systems pose great and unique challenges to debugging, as the developers are required to reason about the concurrent behaviors of different microservices and understand the interaction topology of the whole system. A basic and effective way for understanding and debugging distributed systems is tracing and visualizing system executions [10]. However, microservice systems are much more complex and dynamic than traditional distributed systems. For example, there lacks a natural correspondence between microservices and system nodes in distributed systems, as microservice instances can be dynamically created and destroyed. Therefore, it is not clear whether or how well the state-of-the-art debugging visualization tools for distributed systems can be used for microservice systems.

In spite of the prevalence and importance of microservices in industry, there exists limited research on the subject, with only a few papers on microservices in the software engineering research community, and even fewer in major conferences. The existing research focuses on a wide range of topics about microservices, including design [11], testing [3], [12], [13], [14], deployment [15], [16], [17], verification [18], composition [19], architecture recovery [20], legacy migration [21], and runtime adaptation [22]. There exists little research on the fault analysis and debugging of microservice systems. Moreover, the existing research on microservices is usually based on small systems with few microservices (e.g., 5 microservices or fewer [2]). Such lack of non-trivial open source benchmark microservice systems results in a gap between what the research community can produce and what the industrial practices really need. There have been appeals that practitioners and researchers develop and share a common microservice infrastructure that can emulate the production environments of typical microservice applications for more repeatable and industry-focused empirical studies [23], [24].

To fill this gap and pursue practice-relevant research on microservices, we conduct an industrial survey on fault analysis of typical faults of microservice systems, current practice of debugging, and the challenges faced by the developers. Our survey shows that the current techniques used in practice are limited and the developers face great challenges in microservice debugging. We then conduct an empirical study to further investigate the effectiveness of existing industrial debugging practices and whether the practices can be facilitated by state-of-the-art debugging visualization tools.

To enable our study and also provide a valuable practice-reflecting benchmark for the broad research community, we develop a medium-size benchmark microservice system named TrainTicket [25]. Within our knowledge, our system is the largest and most complex open source microservice system. Upon the system, we replicate the 22 representative fault cases collected in the industrial survey. Based on the benchmark system and replicated fault cases, we empirically evaluate the effectiveness of execution tracing and visualization for microservice debugging by extending a state-of-the-art debugging visualization tool [10] for distributed systems. Based on the study results, we summarize our findings and suggest directions for future research.

In this work, we make the following main contributions:

We conduct a survey on industrial microservice systems and report the fault-analysis results about typical faults, current practice of debugging, and the challenges faced by the developers.

We develop a medium-size benchmark microservice system (being the largest and most complex open source microservice system within our knowledge) and replicate 22 representative fault cases upon it. The system and the replicated fault cases can be used as a benchmark for the research community to further conduct practice-relevant research on microservice fault analysis and debugging, and potentially other practice-relevant research on microservices.

We experimentally evaluate the effectiveness of execution tracing and visualization for microservice debugging and propose a number of visualization analysis strategies for microservice debugging.

In this work we also extend the benchmark system presented in our earlier 2-page poster paper [26] by introducing more microservices (from 24 to 41) and characteristics (e.g., more languages, interaction modes). The replicated fault cases have been released as an open-source project [27], which can be easily integrated into the benchmark system [25]. The details of our industrial survey and empirical study (along with the source code of our open source benchmark system and replicated faults) can be found in our replication package [28].

The rest of the article is structured as follows. Section 2 presents background knowledge of microservice architecture. Section 3 describes the industrial survey, including the process and the results. Section 4 introduces the benchmark system and the 22 replicated fault cases. Section 5 presents the effectiveness evaluation of execution tracing and visualization based on the replicated fault cases and discusses our observations and suggestions. Section 6 discusses threats to validity. Section 7 reviews related work. Section 8 concludes the paper and outlines future work.

SECTION 2Background
Microservice architecture arises from the broader area of Service Oriented Architecture (SOA) with a focus on componentization of small lightweight microservices, application of agile and DevOps practices, decentralized data management and governance among microservices [2]. With the migration from monolithic architecture to microservice architecture, architectural complexity moves from the code based to the interactions of microservices. The interactions among different microservices must be implemented using network communication. Microservice invocations can be synchronous or asynchronous. Synchronous invocations are considered harmful due to the multiplicative effect of downtime [9]. Asynchronous invocations can be implemented by asynchronous REST invocations or using message queues. The former provides better performance whereas the latter provides better reliability. As a user request usually involves a large number of microservice invocations and each microservice may fail, the microservices need to be designed accordingly, i.e., taking possible failures of microservice invocations into account.

Microservice architecture is supported by a series of infrastructure systems and techniques. Microservice development frameworks such as Spring Boot [29] and Dubbo [30] facilitate the development of microservice systems by providing common functionalities such as REST client, database integration, externalized configuration, and caching. Microservice systems widely employ container (e.g., Docker [31]) based deployment for portability, flexibility, efficiency, and speed [9]. Microservice containers can be organized and managed by clusters with configuration management, service discovery, service registry, load balancing by using runtime infrastructure frameworks such as Spring Cloud [32], Mesos [33], Kubernetes [34], and Docker Swarm [35].

The unique characteristics of microservices pose challenges to existing debugging techniques. Existing debugging techniques are designed based on setting up breakpoints, manual inspection of intermediate program states, and profiling. However, these techniques are ineffective for microservices. For instance, due to the high degree of concurrency, the same breakpoint might be reached through very different executions resulting in different intermediate program states. Furthermore, a microservice system contains many asynchronous processes, which requires tracing multiple breakpoints across different processes; such tracing is considerably more challenging than debugging monolithic systems. Besides inspecting intermediate program states, it is equally, if not more, important to comprehend how microservices interact with each other for debugging. Profiling similarly becomes more complicated due to the high dynamism of microservices.

In addition, existing fault localization techniques [36] are ineffective for microservices. Program-slicing-based fault localization [37] works by identifying program statements that are irrelevant to the faulty statement and allowing the developers to investigate the fault based on the remaining statements. Program slicing for microservices is complicated since we must slice through many processes considering different interleavings of the processes. Spectrum-based fault localization [38] computes the suspiciousness of every statement using information such as how many times it is executed in passing test executions or failed test executions, and ranks the statements accordingly so that the developers can focus on the highly suspicious ones. There is no evidence that such techniques work for highly concurrent and dynamic systems such as microservices. Similarly related fault localization techniques are designed mainly for sequential programs, such as statistic-based fault localization [39], and machine-learning-based ones [40], [41].

In recent years, fault localization has been extended to concurrent programs [42], [43], [44] and distributed systems [45], [46], [47]. Both groups of work start with logging thread (and node) level execution information and then locate faults using existing techniques. Applying such techniques to microservices is highly non-trivial since the container instances in microservices are constantly changing, causing difficulty in log checking and overly fragmented logs.

SECTION 3Industrial Survey
In order to precisely understand the industry needs, we start with an industrial survey and then proceed with the collection of typical fault cases and the understanding of the current practice on microservice debugging.

3.1 Participants and Process
We identify an initial set of candidates for the survey from the local technical community who have talked about microservice in industrial conferences or online posts (e.g., articles, blogs). These candidates further recommend more candidates (e.g., their colleagues). Among these candidates, we select and invite 46 engineers for interview based on the following criteria. The candidate must have more than 6 years’ experience of industrial software development and more than 3 years’ experience of microservice development.

Among the invited engineers, 16 of them accept the invitation. The 16 participants are from 12 companies and their feedback is based on 13 microservice systems that they are working on or have worked on. These participants have a broad representation of different types of companies (i.e., traditional IT companies, Internet companies, and non-IT companies), different types of microservice systems (Internet services, enterprise systems) of different scales (50 to more than 1,000 microservices), and different roles in development (technical roles and managerial roles). The information of the participants and subject systems are listed in Table 1, including the company (C.), the participant (P.), the subject system, the number of microservices (#S.), and the position of the participant.

TABLE 1 Survey Participants and Subject Systems
Table 1- 
Survey Participants and Subject Systems
Among the 12 companies, C1, C6, C7, C8, C11, and C12 are leading traditional IT companies, of which C1 and C8 are Fortune 500 companies; C3, C4, C5, and C10 are leading Internet companies; C2 and C9 are non-IT companies. The 13 subject systems can be categorized into two types. One type is Internet microservices that serve consumers via the Internet, including A3, A4, A5, A6, A10, and A11. The other type is enterprise systems that serve company employees, including A1, A2, A7, A8, A9, A12, and A13. The number of microservices in these systems ranges from 50 to more than 1,000, with a majority of them involving about 100-200 microservices. The 16 participants take different positions in their respective companies. Among these positions, Junior Software Engineer, Staff Software Engineer, Senior Software Engineer, and Architect are technical positions; and Manager is a managerial position that manages the development process and project schedule.

We conduct a face-to-face interview with each of the participants. The participant is first asked to recall a microservice system that he/she is the most familiar with and provide his/her subsequent feedback based on the system. The participant introduces the subject system and the role that he/she takes in the system-development project. Then we interview and discuss with the participant around the following questions:

Why does your company choose to apply the microservice architecture in this system? Is the system migrated from an existing monolithic system or developed as a new system?

How does your team design the system? For example, how does your team determine the partitioning of microservices?

What kinds of techniques and what programming languages are used to develop the system?

What challenges does your team face during the maintenance of the system?

Afterwards, the participant is asked to recall those fault cases that he/she has handled. For each fault case, the participant is asked to describe the fault and answer the following questions:

What is the symptom of the fault and how can it be reproduced?

What is the root cause of the fault and how many microservices are involved?

What is the process of debugging? How much time is spent on debugging and what techniques are used?

After the interview, whenever necessary, we conduct follow-up communication with the participants via emails or phone calls to clarify some details.

3.2 General Practice
Our survey shows that most of these companies, not only the Internet companies but also the traditional IT companies, have adopted microservice architecture to a certain degree. Independent development and deployment as well as diversity in development techniques are the main reasons for adopting microservice architecture. Among the 13 surveyed systems, 6 adopt microservice architecture by migrating from existing monolithic systems, while the remaining 7 are new projects using microservice architecture for a comparatively independent business. The migration of some systems is still incomplete: the systems include both microservices and old monolithic modules. The decisions on the migration highly depend on its business value and effort.

Feedback in response to the second question mainly comes from the participants who hold the positions of manager or architect. 4 of 5 choose to take a product perspective instead of a project perspective on the architectural design and consider the microservice partitioning based on the product business model. They express that this strategy ensures stable boundary and responsibility of different microservices.

Among the 13 surveyed systems, 10 use more than one language, e.g., Java, C++, C#, Ruby, Python, and Node.js. One of the systems (A6) uses more than 5 languages. 9 of the participants state that runtime verification and debugging are the main challenges, and they heavily depend on runtime monitoring and tracing of microservice systems. The managers and architects are interested in using runtime monitoring and tracing for verifying the conformance of their systems to microservice best practices and patterns, while the developers are interested in using them for debugging. Debugging remains as a major challenge for almost all of the participants. They often spend days or even weeks analyzing and debugging of a fault.

3.3 Fault Cases
In total, the 16 participants report 22 fault cases as shown in Table 2. For each case, the table lists its reporter, symptom, root cause, and the time (in days) used to locate the root cause. Detailed descriptions of these fault cases (along with the source code of our open source benchmark system and replicated faults) can be found in our replication package [28]. Note that developers take several days to locate the root causes in most cases. These faults can be grouped into 6 common categories as shown in Table 3 based on their symptoms (functional or non-functional) and root causes (internal, interaction, or environment).

TABLE 2 Microservice Fault Cases Reported by the Participants

TABLE 3 Fault Categories

Functional faults result in malfunctioning of system services by raising errors or producing incorrect results. Non-Functional faults influence the quality of services such as performance and reliability. From Table 3, it can be seen that most of the faults are functional, causing incorrect results (F1, F2, F8, F9, F10, F11, F12, F13, F14, F18, F19, F21, F22), runtime failures (F7, F15, F16), or no response (F20); only 4 of them are non-functional, causing unreliable services (e.g., F3, F5) or long response time (F4, F17).

The root causes of Internal faults lie in the internal implementation of individual microservices. For example, F14 is an internal fault caused by a mistake in the calculation of Consumer Price Index (CPI) implemented in a microservice. The root causes of Interaction faults lie in the interactions among multiple microservices. These faults are often caused by missing or incorrect coordination of microservice interactions. For example, F1 is caused by the lack of sequence control in the asynchronous invocations of multiple message delivery microservices; F12 is caused by the incorrect behaviors of a microservice resulted from an unexpected state of another microservice. The root causes of Environment faults lie in the configuration of runtime infrastructure, which may influence the instances of a single microservice or the instances of a cluster of microservices. For example, F3 and F20 are caused by improper configuration of Docker (cluster level) and JBoss (service level), respectively. These faults may influence the availability, stability, performance, and even functionality of related microservices.

To learn the characteristics of the faults in microservice systems, we discuss with each participant to determine whether the reported fault cases are particular to microservice architecture. The criterion is whether similar fault cases may occur in systems of monolithic architecture. Based on the discussion, we find that internal faults and service-level environment configuration faults are common in both microservice systems and monolithic systems, while interaction faults and cluster-level environment configuration faults are particular to microservice systems.

3.4 Debugging Practice
Based on the survey, we summarize the existing debugging process of microservice systems and identify different maturity levels of the practices and techniques on debugging. We also analyze the effectiveness of the debugging processes of the reported fault cases.

3.4.1 Debugging Process
Our survey shows that all the participants depend on log analysis for fault analysis and debugging. Their debugging processes are usually triggered by failure reports describing the symptoms and possibly reproduction steps of the failures, and ended when the faults are fixed. The debugging processes typically include the following 7 steps.

Initial Understanding (IU). The developers get an initial understanding of the reported failure based on the failure report. They may also examine the logs from the production or test environment to understand the failure. Based on the understanding, they may have a preliminary judgement of the root causes or decide to further reproduce the failure for debugging.

Environment Setup (ES). The developers set up a runtime environment to reproduce the failure based on their initial understanding of the failure. The environment setup includes the preparation of virtual machines, a deployment of related microservices, and configurations of related microservice instances. To ease the debugging processes, the developers usually set up a simplified environment, which for example includes as less virtual machines and microservices as possible. In some cases the developers can directly use the production or test environment that produces the failure for debugging and thus this step can be skipped.

Failure Reproduction (FR). Based on the prepared runtime environment, the developers execute the failure scenario to reproduce the failure. The developers usually try different data sets to reproduce the failure to get a preliminary feeling of the failure patterns, which are important for the subsequent steps.

Failure Identification (FI). The developers identify failure symptoms from the failure reproduction executions. The symptoms can be error messages of microservice instances found in logs or abnormal behaviours of the microservice system (e.g., no response for a long time) observed by the developers.

Fault Scoping (FS). The developers identify suspicious locations of the microservice system where the root causes may reside, for example implementations of individual microservices, environment configurations, or interactions of a group of microservices.

Fault Localization (FL). The developers localize the root causes of the failure based on the identified suspicious locations. For each suspicious location, the developers confirm whether it involves real faults that cause the failure and identify the precise location of the faults.

Fault Fixing (FF). The developers fix the identified faults and verify the fixing by rerunning related test cases.

Note that these steps are not always sequentially executed. Some steps may be repeated if the subsequent steps can not be successfully done. For example, the developers may go back to set up the environment again if they find they can not reproduce the failure. Some steps may be skipped if they are not required. For example, some experienced developers may skip environment setup and failure reproduction if they can locate the faults based on the logs from the production or test environment and verify the fault fixing by their special partial execution strategies.

3.4.2 Maturity Levels of Debugging Practices
We find that the practices and techniques on debugging for the 13 systems can be categorized into 3 maturity levels as shown in Table 4.

TABLE 4 Maturity Levels of Debugging

The first level is basic log analysis. At this level, the developers analyze original execution logs produced by the system to locate faults. The logs record the execution information of the system at specific points, including the time, executed methods, values of parameters and variables, intermediate results, and extra context information such as execution threads. Basic log analysis follows the debugging practices of monolithic systems and requires only common logging tools such as Log4j [48] for capturing and collecting execution logs. To locate a fault, the developers manually examine a large number of logs. Successful debugging at this level depends heavily on the developers’ experience on the system (e.g., overall architecture and error-prone microservices) and similar fault cases, as well as the technology stack being used.

The second level is visual log analysis. At this level, execution logs are structured and visualized for fault localization. The developers can flexibly retrieve specific execution logs that they are interested in using conditions and regular expressions, and sort the candidate results according to specific strategies of debugging. The selected execution logs can be aggregated and visualized by different kinds of statistical charts. Log retrieval and visualization are usually combined to allow the developers to interactively drill up and down through the data (execution logs). For example, to locate a fault resulting in abnormal execution results for a microservice, the developers can first use a histogram to learn the range and distribution of different results and then choose a specific abnormal result to examine related execution logs. To support visual log analysis, the developers need to use a centralized logging system to collect the execution logs produced in different nodes and include information about the microservice and its instances in execution logs. Log analysis at this level highly depends on tools for log collection, retrieval, and visualization. A commonly used toolset is the ELK stack, i.e., Logstash [49] for log collection, ElasticSearch [50] for log indexing and retrieval, and Kibana [51] for visualization.

The third level is visual trace analysis. At this level, the developers further analyze collected traces of system executions with the support of trace visualization tools. A trace is resulted from the execution of a scenario (e.g., a test case), and is composed of user-request trace segments. A user-request trace segment consists of logs that share the same user request ID (created for each user request). In particular, when a user request comes in the front door of the system, the adopted tracing framework [52] creates a unique user request ID, which is passed along with the request to each directly or indirectly invoked microservice. Thus, logs collected for each such invoked microservice record the user request ID. The developers can use visualization tools to analyze user requests’ microservice-invocation chains (extracted from the traces) and identify suspicious ranges of microservice invocations and executions. As a microservice can invoke multiple microservices in parallel, the visualization tools usually organize the microservice-invocation chains into a tree structure. For example, a visualization tool can vertically show a nested structure of microservice invocations and horizontally show the duration time of each microservice invocation with colored bars. Analysis at this level highly depends on advanced microservice execution tracing and visualization tools. Commonly used toolset include Dynatrace [52] and Zipkin [53]. Our survey shows that most companies choose to implement their own tracing and visualization tools, as they are specific to the implementation techniques of microservice architecture.

Visual log analysis provides better support for most types of faults than basic log analysis. Flexible log retrieval provides a quick filtering of execution logs. Visualized statistics of microservice executions (e.g., variable values or method execution counts) reveal patterns of microservice executions. These patterns can help locate suspicious microservice executions. For example, for F22, the developers can easily exclude those methods that are executed fewer times than that of failure occurrences based on the statistics. However, locating interaction-related faults often requires the developers to understand microservice executions in the context of microservice-invocation chains. Visual trace analysis further improves visual log analysis by embedding log analysis in the context of traces. For example, for F1, the developers can compare the traces of success scenarios with the traces of failure scenarios, and identify the root cause based on the different orders of specific microservice executions.

Tables 2 and 4 show that there is often a mismatch between the log analysis level and the faults. For example, the system A7 is still at the basic log analysis level, which cannot help locate the fault F8 reported from this system. In such cases, the developers often need to manually investigate a lot of execution logs and code. They usually start with the failure-triggering location in the logs and then examine the logs backwards to find suspicious microservices and check the code of the microservices.

3.4.3 Effectiveness Analysis
To analyze the effectiveness of different debugging practices we collect the maturity levels of debugging practices and the time consumed by each step of the 22 fault cases. Table 5 shows the results, including the fault type, the number of microservices involved in the fault case (#MS), the supported maturity level and the actually adopted maturity level of debugging, and the time consumed for the whole debugging process and individual steps. The last line shows the average of the #MS and the average of the time consumed for the entire debugging process and individual steps.

TABLE 5 Time Analysis of Debugging Practices from Industrial Survey

Note that for some fault cases the maturity levels of the debugging practices adopted by the developers are lower than the levels supported in their teams. For example, for F5 the developers choose to use basic log analysis while they are equipped with visual trace analysis. Moreover, the developers may also combine practices of different levels. For example, when they adopt visual trace analysis or visual log analysis they may also use basic log analysis to examine details.

The time consumed for the whole debugging process and individual steps is obtained from the descriptions of the participants during the interviews. The participants are asked to estimate the time by hours. To validate their estimation, the participants are asked to confirm the estimation with their colleagues and examine the records (e.g., the debugging time indicated by the period between bug assignment and resolution) in their issue tracking systems.

On average the time used to locate and fix a fault increases with the number of microservices involved in the fault: 9.5 hours for one microservice, 20 hours for two microservices, 40 hours for three microservices, 48 hours for more than three microservices. For some fault cases (e.g., F6) the overall time is less than the sum of the time spent on each step. This is usually caused by the simultaneous execution of multiple steps. For example, when confirming a suspicious location of the fault in fault localization, the developers can simultaneously conduct fault scoping to identify more suspicious locations.

We find that the advantages of visual log analysis and visual trace analysis are more obvious for interaction faults. On average, the developers spend 20, 35, 45 hours in these fault cases adopting visual trace analysis, visual log analysis, basic log analysis, respectively.

In general, initial understanding, fault scoping, fault localization are more time consuming than the other steps, as these steps require in-depth understanding and analysis of logs. Also in these steps the advantages of visual log/trace analysis are more obvious. For example, the average time for initial understanding is 3, 7, 21 hours when using visual trace analysis, visual log analysis, basic log analysis, respectively. In some cases (e.g., F9, and F17-19) the developers choose to skip environment setup and failure reproduction, as they can easily identify the failure symptoms from user interfaces or exceptions. In other cases (e.g., F17, F19, F21, F22) the developers choose to skip failure identification and fault scoping, as they can identify potential locations of the faults based on failure symptoms and past experience.

According to the feedback of the participants, 11 out of 13 of them who have experiences of visual log/trace analysis believe that the visual analysis tools and practices are very useful. But how much the tools and practices can help depends on the faulty types and developers’ experiences, skills, and preferences.

SECTION 4Benchmark System and Fault-Case Replication
Our survey clearly reveals that the existing practices for fault analysis and debugging of microservice systems can be much improved. To conduct research in this area, one of the difficulties faced by researchers is that there is a lack of benchmark systems, which is due to the great complexity in setting up realistic microservice systems. We thus set up a benchmark system: TrainTicket [25]. Our empirical study is based on TrainTicket and the 22 fault cases that are reported in the survey and replicated in the system. The system and the replicated fault cases can be used as a valuable benchmark for the broad research community to further conduct practice-relevant research on microservice fault analysis and debugging, and even other broad types of practice relevant research on microservices.

TrainTicket provides typical train ticket booking functionalities such as ticket enquiry, reservation, payment, change, and user notification. It is designed using microservice design principles and covers different interaction modes such as synchronous invocations, asynchronous invocations, and message queues. The system contains 41 microservices related to business logic (without counting all database and infrastructure microservices). It uses four programming languages: Java, Python, Node.js, and Go. Detailed description of the system (along with the source code of our open source benchmark system and replicated faults) can be found in our replication package [28].

We replicate all the 22 fault cases collected from the industrial survey. In general, these fault cases are replicated by transferring the fault mechanisms from the original systems to the benchmark system. In the following, we describe the replication implementation of some representative fault cases. Descriptions of the replication implementation of the other fault cases can be found in our replication package [28].

F1 is a fault associated with asynchronous tasks, i.e., when we send messages asynchronously without message sequence control. We replicate this fault in the order cancellation process of TrainTicket. In the process, there are two asynchronous tasks being sent, which have no additional sequence control. The first task should always be completed before the second one. However, if the first task is delayed and completed only after the second one, the order reaches an abnormal status, leading to a failure.

F3 is a reliability problem caused by improper configurations of JVM and Docker. JVM's max memory configuration conflicts with Docker cluster's memory limitation configuration. As a result, Docker sometimes kills the JVM process. We replicate this fault in the ticket searching process. We select some microservices that are involved in this process and revise them to be more resource consuming. These revised microservices are deployed in a Docker cluster with conflicting configurations, thus making these microservices sometimes unavailable.

F4 is a performance problem caused by improper configuration of Secure Sockets Layer (SSL) applied for many microservices. The result is frequent SSL offloading at a fine granularity, which slows down the executions of related microservices. We replicate this fault by applying the faulty SSL configuration to every microservice of TrainTicket. Then when a user requests a service (e.g., ticket reservation), he/she will feel that the response time is very long.

F5 is a reliability problem caused by improper usage of a thread pool. The microservice uses a thread pool to process multiple different types of service requests. When the thread pool is exhausted due to the high load of a type of service requests, another type of service requests will fail due to timeout. We replicate this fault in the ticket reservation service, which serves both the ticket searching process and the ticket booking process. When the load of ticket searching is high, the thread pool of the service will be exhausted and the ticket booking requests to the service will fail due to timeout.

F8 is caused by missing or incorrect parameter passing along an invocation chain. We replicate this fault in the order cancellation process. When a VIP user tries to cancel a ticket order, the login token saved in Redis [54] (an in-memory data store) is not passed to some involved microservices that require the token. This fault causes that the user gets unexpectedly lower ticket refund rate.

F10 is caused by an unexpected output of a microservice, which is used in a special case of business processing. We replicate this fault in the ticket booking process. In the ticket ordering service, we implement two APIs, which respectively serve for general ticket ordering and ticket ordering of some special stations. The API for special ticket ordering sometimes returns an unexpected output that is not correctly handled, thus making the ticket booking process fail.

F11 is a fault that occurs in asynchronous updating of data, caused by the missing of sequence control. When the bill of material (BOM) tree is updated in an unexpected order, the resulting tree is incorrect. But when the user turns on the “strict mode” on product BOM services, the resulting tree is rebuilt when the BOM tree includes some negative numbers, leading to a correct tree. We replicate this fault in the order cancellation process, which includes two microservices (payment service and cancel service) that asynchronously set the same value in the database. Due to the missing of sequence control, the two microservices may set the value in a wrong sequence, thus causing an incorrect value. But if the user turns on the “strict order” mode on the order service, the incorrect value will be corrected eventually.

F12 is caused by an unexpected output of a microservice when it is in a special state. We replicate this fault in the ticket booking process. We introduce state admDepStation/admDesStation for the ticket reservation service instance to indicate the departure/destination station of which the administrator is examining the tickets. if no administrator examining things happened, the corresponding ticket reservation service instance will be without state. If the departure/destination station of a ticket reservation request is admDepStation/admDesStation, and the ticket reservation service is accessed by the same request thread twice or more times including both with and without state instance, the request will be denied with an unexpected output and the ticket booking process returns an error.

For each of these preceding faults, we create a development branch for its replication in the fault case repository [27]. Researchers using the repository can easily mix and match different faults to produce a faulty version of TrainTicket including multiple faults.

SECTION 5Empirical Study
Our empirical study with the TrainTicket system and the replicated fault cases includes two parts. In the first part, we investigate the effectiveness of existing industrial debugging practices for the fault cases. In the second part, we develop a microservice execution tracing tool and two trace visualization strategies for fault localization based on a state-of-the-art debugging visualization tool [10] for distributed systems, and investigate whether it can improve the effectiveness of debugging interaction faults. A group of 6 graduate students who are familiar with TrainTicket and have comparable experiences of microservice development serve as the developers for conducting debugging independently. For each fault case, the developers locate and fix the faults based on a given failure report, and the developers who debug with different practices are different. To provide a fair comparison, we randomly select a developer for each fault case and each practice to allow a developer to use different practices for different fault cases. The developers follow the general process presented in Section 3.4.1 for debugging. For any step, if the developers cannot complete in two hours they can choose to give up and the step and the whole process fail.

5.1 Debugging with Industrial Debugging Practices
In this part of the study, we investigate the effectiveness of the debugging practices of the three maturity levels by qualitative analysis and quantitative analysis respectively. For each fault case three developers are selected to debug with the practices of different maturity levels. The tools provided for different maturity levels are as follows.

Basic Log Analysis. The developers use command line tools to grab and analyze logs.

Visual Log Analysis. The developers use the ELK stack, i.e., Logstash [49] for log collection, ElasticSearch [50] for log indexing and retrieval, and Kibana [51] for visualization.

Visual Trace Analysis. The developers use both the ELK stack and Zipkin [53] for debugging.

5.1.1 Qualitative Analysis
We qualitatively compare different levels of practices based on the debugging processes of F8 as shown in Fig. 1.


Fig. 1.
Qualitative comparison of different levels of debugging practices.

Show All

Fig. 1a presents a snapshot of basic log analysis, which shows the logs captured from a container running a microservice of food service. The developers identify a suspicious log fragment in the red box and find that the food refund rate is 68 percent, which is lower than the predefined VIP refund rate for food ordering. Thus they can regard the calculation of food refund rate as a potential fault location. A shortage of basic log analysis is the lack of context of microservice invocation, which makes it hard for the developers to analyze and understand the logs in the context of user requests and invocation chains.

Fig. 1b presents a snapshot of visual log analysis. It shows the histograms of average refund rate of the instances of two related microservices (food service and consign service) in different virtual machines, as well as corresponding logs. The refunds of these two services are both included in the ticket refund. As the failure symptom is low ticket refund rate, the developers choose the lowest bar which shows the average food refund rate in VM3 (see the red box in Fig. 1b) to check the logs. From the logs the developers find that the lowest food refund rate is 65 percent, and thus regard the calculation of food refund rate as a potential fault location. Compared with basic log analysis, visual log analysis provides aggregated statistics of variables and quality attributes (e.g., response time), and thus can help developers to identify suspicious microservices and instances. However, it lacks the context of user requests and invocation chains, and thus can not support the analysis of microservice interactions.

Fig. 1c presents a snapshot of visual trace analysis. It shows the entire trace of the order cancellation process, including the nested invocations of microservices, and the consumed time of each invocation. The developers find that the ticket cancellation process invokes not only the food service and the consign service, but also the config service and route service. Then they further analyze the logs of the config service and find that a suspicious general refund rate (which can be 36 percent in the lowest case) is used by the ticket cancellation process to calculate the final refund rate. They thus regard the calculation of general refund rate in the config service as a potential fault location. This fault localization is more precise than the localization supported by the basic log analysis and the visual log analysis.

Compared with visual log analysis, visual trace analysis supports the understanding of microservice executions in the context of user requests and invocation chains.

5.1.2 Quantitative Analysis
The results of the study are shown in Table 6, including the time used for the whole debugging process and that of each step. A mark “-” means that the developer skips the step. If all the steps are skipped, it means that the fault can be easily identified and fixed with lower level practices (e.g., basic log analysis) thus there is no need for higher level practices. A mark “failed” means that the developer fails to complete the step. If a step of a debugging process fails, the whole process fails also.

TABLE 6 Time Analysis of Debugging with Industrial Debugging Practices

The developers fail in F3 and F4 with all the three levels of industrial practices. Both of them are non-functional Environment faults. For F9, F19, F21, F22, the developers easily locate and fix the faults with basic log analysis. All of them are Internal faults. For the other faults, there is a general trend of reduced debugging time with the employment of higher level debugging practices (from basic log, visual log, to visual trace analysis). In these fault cases Interaction faults are the ones that benefit the most from higher levels of debugging practices.

Similar to the industrial survey, initial understanding, fault scoping, fault localization are more time consuming than the other steps, and environment setup and failure reproduction are sometimes skipped. The time used for environment setup and failure reproduction varies with the employed debugging practices. According to the feedback from the developers, they often try to make a simplest failure reproduction based on the initial understanding, so the accuracy of the initial understanding influences the time used for environment setup and failure reproduction

5.2 Debugging with Improved Trace Visualization
From the above, we observe that tracing and visualizing can potentially help fault analysis and debugging of microservice systems. Thus, in this part of the study, in order to better support fault analysis and debugging of microservice systems, we investigate the effectiveness of the state-of-the-art distributed system debugging techniques for microservice system debugging.

5.2.1 Tracing and Visualization Approach
ShiViz [10] is a state-of-the-art debugging visualization tool for distributed systems. It visualizes distributed system executions as interactive time-space diagrams that explicitly capture distributed ordering of events in the system. ShiViz supports pairwise comparison of two traces by highlighting their differences. It compares the distribute-system nodes and events from two traces by names and descriptions, and highlights the nodes or events (in one trace) that do not appear in the other. ShiViz supports the selection of a part of a trace for comparison. For example, we can select a user-request trace segment, i.e., the events for a specific user request based on the request ID.

Fig. 2 presents an example of trace visualization by ShiViz, which shows the nodes (colored boxes at the top), the node timelines (vertical lines), events (circles on timelines), and partial orders between events (edges connecting events). The rhombuses (events) on the left side highlight the differences between the two traces, and we can also click to see the detail of the rhombuses.

Fig. 2. - 
Trace visualization by ShiViz.
Fig. 2.
Trace visualization by ShiViz.

Show All

This pairwise comparison can be used to locate suspicious nodes and events in microservice system debugging when execution information (e.g., service name, user request ID, and invoked method) is added to the names and descriptions of nodes and events, by treating a microservice unit as a distribute-system node. We can leverage ShiViz to visualize the traces of microservices by transforming the trace logs into the log formats of ShiViz. However, a basic problem for visualizing microservice traces is how to map microservice units to nodes. Microservice instances run on containers and can be dynamically created or destroyed together with their containers. Moreover, the instance that is assigned to handle a microservice invocation is uncertain. Therefore, microservice instances (containers) cannot be treated as the nodes in trace visualization. We thus propose the following two visualization strategies for microservice traces.

Microservice as Node (Service-Level Analysis). All the instances of the same microservice are treated as one node. Thus the events at different instances of the same microservice are aggregated to the same node.

Microservice State as Node (State-Level Analysis). All the instances of the same microservice and with the same state (determined based on predefined state variables or expressions) are treated as one node. Thus the events at different instances of the same microservice and during the same state are aggregated to the same node. This technique depends on predefined state variables or expressions of each microservice.

A trace resulted from executing a microservice system's scenario (e.g., test case) may include many user requests (e.g., a ticket query request triggered by a button click on a web page). In the trace, a user-request trace segment resulted from each user request includes a series of microservice invocations. Furthermore, each microservice invocation may involve multiple execution events occurring on different microservice instances such as sending/receiving an invocation request or a call back. Therefore, an execution tracing tool needs to log all the execution events and attach a user request ID (request ID in short) and a microservice invocation ID (invocation ID in short) to each log. Our tool is implemented in Java. For REST invocations, we use the servlet filter and interceptor to inject tracing information of the caller into the http header of the HTTPRequest object. The injected information includes the request ID, invocation ID, microservice name, instance ID, IP address, port number, along with the class name and method name of the invoked microservice and the caller, respectively. Such information is sent together with the http request to the callee. For message queues, we use a message channel interceptor to inject and catch the interaction queue data. Based on such tracing information, each microservice instance records tracing logs, which are then collected by our central logging system.

Based on the two visualization strategies, we leverage ShiViz to diagnose microservice faults by pairwise comparison of traces. This characteristic makes ShiViz superior to previous microservice tracing/visualization tools, e.g., ELK stack, Zipkin.

5.2.2 Debugging Methodology
Based on the tracing tool and the two visualization strategies, we define a debugging methodology as shown in Fig. 3 for our empirical study based on general debugging practices of distributed systems. The rationale of the process is based on the assumption that the fault-revealing parts of a failure trace are different from the corresponding parts in a success trace, and are shared with another failure trace.


Fig. 3.
Debugging methodology.

Show All

For each fault, we collect a set of traces resulted from executing the same scenario (with different parameter values) including both success traces and failure traces. We select a success trace TraceS and two failure traces TraceF1 and TraceF2 for comparison. We first compare TraceS and TraceF1 based on user requests. For example, a scenario of ticket booking includes a series of user requests such as ticket query, train selection, and passenger selection, and the events for each of these requests in two traces are compared separately. To ease the selection of user requests, we attach a readable label (e.g., “ticket query”) for each request ID in the logs. Based on the comparison, we can obtain a set of diff ranges DR, each of which are multiple consecutive events that are different between the two traces. We then compare between TraceF1 and TraceF2 by user requests to confirm the ranges in DR that are shared in the two traces. The confirmed ranges in DR are identified as fault-revealing ranges.

The purpose of this study is to investigate whether trace-based debugging can benefit from improved tracing and visualization. Therefore, we select 12 fault cases from all the 22 ones for the study according to the following two criteria. First, they are Interaction or Environment faults, as Internal faults can not benefit from trace analysis. Second, their debugging time is more than 1 hour by visual trace analysis (see Table 6). The selected fault cases are: F1, F2, F3, F4, F5, F7, F8, F10, F11, F12, F13, F16. For each fault case a developer who did not debug the fault using the industrial debugging practices is selected to locate and fix the fault.

5.2.3 Qualitative Analysis
The benefits that different debugging steps can obtain from the improved trace visualization are different. For initial understanding the developers can get an impression of the overall differences between success traces and failure traces, including the number and size of diff ranges. Some developers can even directly identify suspicious fault locations in initial understanding. Environment setup, failure reproduction, and failure identification can indirectly benefit from the analysis through more accurate initial understanding of the failure. Fault scoping and fault localization can directly benefit from the analysis by identifying diff ranges. Fault fixing can indirectly benefit from the analysis in the verification of fault fixing.

Fig. 4 shows an example of service-level analysis for faulty microservice invocation in F10, which is caused by incorrect handling of an unexpected output of one of the APIs of a microservice. The developers can see there are two differences (rhombus) in the red box, indicating that the invoked APIs are different (although the invoked microservices are the same). As the corresponding range in the comparison of failure traces involves no differences, they can identify this range as a potential fault location.


Fig. 4.
An example of service level analysis for faulty microservice invocation.

Show All

Fig. 5 shows an example of service-level analysis for faulty interaction sequence in F1, which is caused by missing coordination of asynchronous invocations. Based on the structures of the events, the developers can identify two pairs of corresponding ranges in the success trace and the failure trace, showing as red boxes and green boxes respectively. They can find that the orders of the two ranges in the two traces are different. As the orders have no difference in the comparison of failure traces, they can identify the difference as a potential fault location.


Fig. 5.
An example of service level analysis for faulty interaction sequence.

Show All

Fig. 6 shows an example of service- and state-level debugging for F12. When conducting service-level analysis as shown in Fig. 6a, there is no difference between the failure trace and the success trace. Then the developers choose to use state-level analysis to refine the comparison of the traces by introducing state variables or expressions. Based on the understanding of the system, the developers choose to try the following two state variables of the ticket order service: administrator examining station indicating the departure/destination station of which the administrator is examining the tickets, and order processing thread pool indicating whether the limit of the pool is not reached, reached, or exceeded (0, 1, or 2). As the administrator examining station has more than 20 different values, the developers choose to use state expression instead of the state variable, e.g., use the country region information: GetRegion (administrator examining station). Then the developers get the state-level analysis results as shown in Fig. 6b. It can be seen that the nodes for the ticket order service are annotated with the combined states (“region5”/0 and “region3”/1) and thus the differences (nodes and events) between the failure trace and the success trace are identified and highlighted as rhombuses (in the red box). The developers further examine the comparison between the two failure traces as shown in Fig. 6c, which compares the ticket order service with the same administration state (“region3”) and different thread pool states (2 and 1). These two comparisons provide preliminary evidence that the state of administrator examining “region3” of the ticket order service is relevant to the fault and the range highlighted in Fig. 6b is a candidate fault-revealing range.

Fig. 6. - 
An example of service- and state-level analysis.
Fig. 6.
An example of service- and state-level analysis.

Show All

When necessary, the developers can introduce more state variables/expressions for comparison, paying the price of having more nodes in the visualized trace. The developers can gradually adjust the strategies and attempt different combinations of state variables/expressions. Heuristics can be applied to identify such combinations. For instance, desirable state variables/expressions are likely built from static variables, singleton-member variable, or key-values in temp storage, e.g., in Redis [54].

We find that all the successful analyses are based on the following four tactics on comparing a failure trace and a success trace.

T1 (Single-Event Difference). The fault-revealing range is a single event, and the difference lies only in the descriptions (e.g., invoked method) of the event.

T2 (Single-Range Difference). The fault-revealing range involves different interaction sequences among nodes.

T3 (Multiple-Range Difference). The execution orders of multiple fault-revealing ranges are different.

T4 (Multiple-Request Difference). The execution orders of multiple user requests are different.

Among these tactics, T1 does not involve differences in node interaction sequences, while the other three tactics do. The tactics used for the analysis of each fault case are also shown in Table 7. It can be seen that tactics may be combined for debugging to locate a fault, because a fault may involve multiple fault-revealing ranges at different levels. The difficulty of debugging increases from T1 to T4 with the analysis of trace differences in a larger scope.

TABLE 7 Empirical Study Results of Debugging with Improved Trace Visualization

T4 is relatively hard to use, as it involves complex interactions among different user requests. F2 is an example for which T4 must be used. Due to the extensive usage of asynchronous interactions in microservice systems, the processing orders of user requests do not always correspond to their receiving order. If there are interactions among different user requests, it is likely that a fault will be introduced due to erroneous coordination of processing user requests. F2 is an example of this case. As the trace analysis involves a large number of events across multiple user requests, and the events of different requests are interleaved, F2 cannot be effectively analyzed based on existing visualization techniques, unless the differences between success and failure traces are reflected in the trace comparison of a single request. For F2, the developer spends a lot of time in seeking the root cause. But for F13, as the trace analysis involves a much smaller number of events across multiple user requests compared to F2, and the events of different requests on ShiViz can be easily distinguished, F13 can be effectively analyzed, and less time consumed.

5.2.4 Quantitative Analysis
The time-analysis results of debugging with improved trace visualization are shown in Table 7. Among the 12 fault cases, the developers fail in 2 cases (F3 and F4) in which they also fail with visual trace analysis. For F16, the developers succeed but use more time than visual trace analysis. These three cases are all Environment faults (F3 and F4 are non-functional, F16 is functional); such result suggests that debugging such faults cannot benefit from trace analysis.

In all the other 9 fault cases, the developers achieve improved debugging effectiveness with decreased average debugging time from 3.23 to 2.14 hours. Note that these 9 fault cases are all Interaction faults. For these faults, fault localization, initial understanding, failure reproduction are the three steps that benefit the most from the analysis. The time used for these steps is reduced by 49, 28, and 24 percent, respectively compared with visual trace analysis.

Table 7 also shows the detailed analysis processes of the developers on each of the 12 fault cases, including the used visualization strategy, number of nodes (#N.), number of events (#E.), number of user requests (#UR.), number of fault-revealing ranges identified in each analysis (#FR.), number of events in fault-revealing ranges in each analysis (#FE.), and hit (i.e., the analysis that succeeds in identifying at least one true fault-revealing range, ‘Y’ indicates successfully identified, ‘N’ indicates failed) in each analysis. A mark “-” indicates that the developers fail in identifying the ranges or events. The results show that each debugging with a combination of success and failure traces involves about 7-22 nodes (representing services or service states) and hundreds to thousands of events. These events belong to 2-7 user requests, and the traces of each user request are compared separately.

Each successful analysis identifies several fault-revealing ranges with dozens of events. For some fault cases (F1, F2, F7, F8, F10, and F13), service-level analysis can effectively identify the fault-revealing ranges (Hit is ‘Y’). Some other fault cases (F5, F11, and F12) require service state-level analysis to identify the fault-revealing ranges. For F12, both developers successfully identify one of the issue states.

There are also unsuccessful cases (F3, F4), indicating that the developers fail in locating at least one fault-revealing range. The main reason is that the two cases are environmental faults, and they are not sufficiently supported by our debugging methodology.

5.3 Findings
Our study shows that most fault cases except those caused by environmental settings can benefit from trace visualization, especially those related to microservice interactions. By treating microservices or microservice states as the nodes, we can further improve the effectiveness of microservice debugging using state-of-the-art debugging visualization tools for distributed systems. A difficulty for state-level tracing and visualization mainly lies in the definition of microservice states. As a microservice system may have a large number of microservices and state variables/expressions, it highly depends on the experience of the developers to achieve effective and efficient fault analysis by identifying a few key states that can help reveal the faults.

A challenge for the trace visualization lies in the huge number of nodes and events. Large-scale industrial microservice systems have hundreds to thousands of microservices and tens of thousands to millions of events in a trace. Such a number of nodes and events can make the visualization analysis infeasible. This problem can be alleviated from two aspects. First, better trace visualization techniques such as zoom in/out and node/event clustering are required to allow the developers to focus on suspicious scopes. For example, node/event clustering can adaptively group cohesive nodes and events together, and thus reduce the number of nodes and events to be examined by progressively disclosing information. Second, fault localization techniques such as spectrum based fault localization [38], [55] and delta debugging [56] can be combined with visualization analysis for microservice debugging. On the one hand, the combination can suggest suspicious scopes in traces by applying statistical fault localization on microservice invocations, and on the other hand, the combination can provide results of code-level fault localization (e.g., code blocks) within specific microservices.

In view of the great complexity caused by the scale of microservice interactions and the dynamics of infrastructure, we believe that debugging of microservices needs to be supported in a data-driven way. For instance, one way is to combine human expertise and machine intelligence for guided visual exploration and comparison of traces. The supporting tools can take full advantage of the large amount of data produced by runtime monitoring and historical analysis and make critical suggestion and guidance during the visual exploration and comparison of traces. For example, the tools can suggest suspicious scopes in traces and sensitive state variables that may differentiate success and failure traces based on probabilistic data analysis, or recommend historical fault cases that share similar patterns of traces. Based on these suggestions and guidance, the developers can dig into possible segments of traces or add relevant state variables to trace comparison and visualization. These actions are in turn collected and used by the tools as feedback to improve further suggestion and guidance.

SECTION 6Threats to Validity
One common threat to the external validity of our studies lies in the limited participants and fault cases. The industrial experiences learned from these participants may not represent other companies or microservice systems that have different characteristics. The fault cases collected from the industrial participants may not cover more complex faults or other different fault types. One major threat to the internal validity of the industrial survey lies in the accuracy of the information (e.g., time of each debugging step) collected from the participants. As such information is not completely based on precise historical records, some of the information may not be accurate.

The threats to the external validity of the empirical study mainly lie in the representativeness of the benchmark system. The system currently is smaller and less complex (e.g., less heterogeneous) than most of the surveyed industrial systems, despite being the largest and most complex open source microservice system within our knowledge. Thus some experiences of debugging obtained from the study may not be valid for large industrial systems.

There are three major threats to the internal validity of the empirical study. The first one lies in the implementation of the fault cases based on our understanding. The understanding may be inaccurate, and the replication of some faults in a different system may not fully capture the essential characteristics of the fault cases. The second one lies in the uncertainty of runtime environments such as access load and network traffic. Some faults may behave differently with different environment settings, thus need different debugging strategies. The third one lies in the differences of the experiences and skills of the developers who participate in the study. These differences may also contribute to the differences of debugging time and results with different practices.

SECTION 7Related Work
Some researchers review the development and status of microservice research using systematic mapping study and literature review. Francesco et al. [2] present a systematic mapping study on the current state of the art on architecting microservices from three perspectives: publication trends, focus of research, and potential for industrial adoption. One of their conclusions is that research on architecting microservices is still in its initial phases and the balanced involvement of industrial and academic authors is promising. Alshuqayran et al. [57] present a systematic mapping study on microservice architecture, focusing on the architectural challenges of microservice systems, the architectural diagrams used for representing them, and the involved quality requirements. Dragoni et al. [58] review the development history from objects, services, to microservices, present the current state of the art, and raise some open problems and future challenges. Aderaldo et al. [23] present an initial set of requirements for a candidate microservice benchmark system to be used in research on software architecture. They evaluate five open source microservice systems based on these requirements and the results indicate that none of them is mature enough to be used as a community-wide research benchmark. Our open source benchmark system offers a promising candidate to fill such vacancy. Our industrial survey well supplements these previous systematic mapping studies and literature reviews.

There has been some research on debugging concurrent programs [38], [59], [60] and distributed systems [10], [61], [62], [63]. Asadollah et al. [64] present a systematic mapping study on debugging concurrent and multicore software in the decade between 2005 and 2014. Bailis et al. [61] present a survey on recent techniques for debugging distributed systems with a conclusion that the state-of-the-art of debugging distributed systems is still in its infancy. Giraldeau et al. [62] propose a technique to visualize the execution of distributed systems using scheduling, network, and interrupt events. Aguerre et al. [63] present a simulation and visualization platform that incorporates a distributed debugger. Beschastnikh et al. [10] discuss the key features and debugging challenges of distributed systems and present a debugging visualization tool named ShiViz, which our empirical study investigates and extends. In contrast to such previous research, our work is the first to focus on debugging support for microservice systems.

SECTION 8Conclusion
In this work, we have presented an industrial survey to conduct fault analysis on typical faults of microservice systems, current industrial practice of debugging, and the challenges faced by the developers. Based on the survey results, we have developed a medium-size benchmark microservice system (being the largest and most complex open source microservice system within our knowledge) and replicated 22 representative fault cases from industrial ones based on the system. These replicated faults have then been used as the basis of our empirical study on microservice debugging. The results of the study show that, by using proper tracing and visualization techniques or strategies, tracing and visualization analysis can help debugging for locating various kinds of faults involving microservice interactions. Our findings from the study also indicate that there is a need for more intelligent trace analysis and visualization, e.g., by combining techniques of trace visualization and improved fault localization, and employing data-driven and learning-based recommendation for guided visual exploration and comparison of traces.

Industrial microservice systems are often large and complex. For example, industrial systems are highly heterogeneous in microservice interactions and may use not only REST invocations and message queues but also remote procedure calls and socket communication. Moreover, industrial systems are running on highly complex infrastructures such as auto-scaling microservice cluster and service mesh (a dedicated infrastructure layer for service-to-service communication [65]). Such complexity and heterogeneity pose additional challenges on execution tracing and visualization. Our future work plans to further extend our benchmark system to reflect more characteristics of industrial microservice systems, and explore effective trace visualization techniques and the combination of fault localization techniques (e.g., spectrum-based fault localization [38] and delta debugging [56]) for microservice debugging. Moreover, we plan to explore a more technology-independent way to inject tracing information at every service invocation via service mesh tools such as Linkerd [66] and Istio [67].