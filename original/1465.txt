This paper presents a novel end-to-end approach to program repair based on sequence-to-sequence learning. We devise, implement, and evaluate a technique, called SequenceR, for fixing bugs based on sequence-to-sequence learning on source code. This approach uses the copy mechanism to overcome the unlimited vocabulary problem that occurs with big code. Our system is data-driven; we train it on 35,578 samples, carefully curated from commits to open-source repositories. We evaluate SequenceR on 4,711 independent real bug fixes, as well on the Defects4J benchmark used in program repair research. SequenceR is able to perfectly predict the fixed line for 950/4,711 testing samples, and find correct patches for 14 bugs in Defects4J benchmark. SequenceR captures a wide range of repair operators without any domain-specific top-down design.
SECTION 1Introduction
People have long dreamed of machines capable of writing computer programs by themselves. Having machines writing a full software system is science-fiction but teaching machines to modify an existing program to fix a bug is within the reach of current software technology; this is called automated program repair [1].

Program repair research is very active and dominated by techniques based on static analysis (e.g., Angelix [2]) and dynamic analysis (e.g., CapGen [3]). While great progress has been achieved, the current state of automated program repair is limited to simple small fixes, mostly one line patches [3], [4]. These techniques are heavily top-down, based on intelligent design and domain-specific knowledge about bug fixing in a given language or a specific application domain. In this paper, we also focus on one line patches, but we aim at doing program repair in a language-agnostic generic manner, fully relying on machine learning to capture syntax and grammar rules and produce well-formed, compilable programs. By taking this approach, we aim to provide a foundation for connecting program repair and machine learning, allowing the program repair community to benefit from training with more complete bug datasets and continued improvements to machine learning algorithms and libraries.

As the foundation for our model, we apply sequence-to-sequence learning [5] to the problem of program repair. Sequence-to-sequence learning is a branch of statistical machine learning, mostly used for machine translation: the algorithm learns to translate text from one language (say French) to another language (say Swedish) by generalizing over large amounts of sentence pairs from French to Swedish. The training data comes from the large amount of text already translated by humans, starting with the Rosetta stone written in 196 BC [6]. The name of the technique is explicit: it is about learning to translate from one sequence of words to another sequence of words.

Now let us come back to the problem of programming: we want to learn to ’translate’ from one sequence of program tokens (a buggy program) to a different sequence of program tokens (a fixed program). The training data is readily available: we have millions of commits in open-source code repositories. Yet, we still have major challenges to overcome when it comes to using sequence-to-sequence learning on code: 1) the raw (unfiltered) data is rather noisy; one must deploy significant effort to identify and curate commits that focus on a clear task; 2) contrary to natural language, misuse of rare words (identifiers, numbers, etc) is often fatal in programming languages [7]; in natural language some errors may be tolerable because of the intelligence of the human reader while in programming languages the compiler (or interpreter) is strict 3) in natural language, the dependencies are often in the same sentence (“it” refers to “dog” just before), or within a couple of sentences, while in programming, the dependencies have a longer range: one may use a variable that has been declared dozens of lines before.

We are now at a tipping point to address those challenges. First, sequence-to-sequence learning has reached a maturity level, both conceptually and from an implementation point of view, that it can be fed with sequences whose characteristics significantly differ from natural language. Second, there has been great recent progress on using various types of language models on source code [8]. Based on this great body of work, we present our approach to using sequence-to-learning for program repair, which we created to repair real bugs from large open-source projects written in the Java programming language.

Our end-to-end program repair approach is called SequenceR and it works as follows. First, we focus on one-line fixes: we predict the fixed version of a buggy programming line. For this, we create a carefully curated training and testing dataset of one-line commits. Second, we devise a sequence-to-sequence network architecture that is specifically designed to address the two main aforementioned challenges. To address the unlimited vocabulary problem, we use the copy mechanism [9]; this allows SequenceR to predict the fixed line, even if the fix contains a token that was too rare (i.e., an API call that appears only in few cases, or a rare identifier used only in one class) to be considered in the vocabulary. This copy mechanism works even if the fixed line should contain tokens which were not in the training set. To address the dependency problem, we construct abstract buggy context from the buggy class, which captures the most important context around the buggy source code and reduces the complexity of the input sequence. This enables us to capture long range dependencies that are required for the fix.

We evaluate SequenceR in two ways. First, we compute accuracy over 4,711 real one-line commits, curated from three open-source projects. The accuracy is measured by the ability of the system to predict the fixed line exactly as originally crafted by the developer, given as input the buggy file and the buggy line number. Our golden configuration is able to perfectly predict the fix for 950/4,711 (20 percent) of the testing samples. This sets up a baseline for future research in the field. Second, we apply SequenceR to the mainstream evaluation benchmark for program repair, Defects4J. Of the 395 total bugs in Defects4J, 75 have one-line replacement repairs; SequenceR generates patches which pass the test suite for 19 bugs and patches which are semantically equivalent to the human-generated patch for 14 bugs. To our knowledge, this is the first report ever on using sequence-to-sequence learning for end-to-end program repair, including validation with test cases.

Overall, the novelty of this work is as follows. First, we create and share a unique dataset for evaluating learning techniques on one-line program repair. Second, we report on using the copy mechanism on seq-to-seq learning on source code. Third, on the same buggy input dataset, SequenceR is able to produce the correct patch for 119 percent more samples than the closest related work [10].

To sum up:

Our key contribution is an approach for fixing bugs based on sequence-to-sequence learning on token sequences. This approach uses the copy mechanism to overcome the unlimited vocabulary problem in source code.

We present the construction of an abstract buggy context that leverages code context for patch generation. The input program token sequences are at the level of full classes and capture long-range dependencies in the fix to be written. We implement our approach in a publicly-available program repair tool called SequenceR.

We evaluate our approach on 4,711 real bug fixing tasks. Contrary to the closest related work [10], we do not assume bugs to be in small methods only. Our golden trained model is able to perfectly fix 950/4,711 testing samples. To the best-of-our knowledge, this is the best result reported on such a task at the time of writing this paper [10], [11], [12].

We evaluate our approach on the 75 one-line bugs of Defects4J, which is the most widely used benchmark for evaluating programming repair contributions. SequenceR is able to find 2,321 patches for these bugs, 761 compile successfully, 61 are plausible (they pass the full test suite) and 18 are semantically equivalent to the patch written by the human developer.

We provide a qualitative analysis of 8 interesting repair operators captured by sequence-to-sequence learning on the considered training dataset.

SECTION 2Background on Neural Machine Translation with Sequence-to-Sequence Learning
SequenceR is based on the idea of receiving buggy code as input and producing fixed code as output. The concept is similar to neural machine translation where the input is a sequence of words in one language and the output is a sequence in another language. In this section, we provide a brief introduction to neural machine translation (NMT).

In neural machine translation, the dominant technique is called “sequence-to-sequence learning”, where “sequence” refers to the sequence of words in a sentence. An early example of a sequence-to-sequence network [5] used a recurrent neural network to read in tokens and to generate an output sequence, as shown in Fig. 1. Let us consider that the input tokens are denoted xt, and after receiving all of the input tokens a special <EOS> token is used. The output tokens are denoted yt, and at training time the output tokens are fed into the network to learn proper generation of the next token. In the following equations, ht is the hidden state of a recurrent neural network, Whx is the weight matrix that computes how the input xt affects the hidden state, Whh is the weight matrix related to recurrence (i.e., how the previous hidden state affects the current hidden state), and Wyh is the weight matrix used to predict which token should be output given the hidden state. All weights are learned with supervised learning and back-propagation:
ht=σ(Whxxt+Whhht−1)yt=Wyhht.
View SourceRight-click on figure for MathML and additional features.

Fig. 1. - 
Figure from Sutskever, et al. [5] showing example of early sequence-to-sequence model.
Fig. 1.
Figure from Sutskever, et al. [5] showing example of early sequence-to-sequence model.

Show All

A softmax function is then used to turn the yt values into probabilities to choose the most likely token from a learned vocabulary. In this example, one can see how the weight matrices capture the learning of common patterns; after processing the input sequence, the hidden state h<eos> encodes the most likely initial token to begin the output and each subsequent ht uses the W matrices to predict the most likely next token given the input as well as preceding tokens just produced in the output. The W matrices thus learn the long range dependencies in the full input.

A problem with the sequence generation described above is that only tokens which are in the training set are available for output as yt. In the case of natural human language, words such as proper names (e.g., Chicago, Stockholm) may be so rare that they do not appear in the training vocabulary, but those words may be necessary for proper output. One successful approach to overcome the vocabulary problem is to use a copy mechanism [9]. The basic intuition behind this approach is that rare words not available in the vocabulary (i.e., unknown words, referred as <unk>), may be directly copied from the input sentence over to the output translated sentence. This relatively simple idea can be successful in many cases - especially when translating sentences containing proper names - where these tokens can be easily copied over.

For example, let’s consider the task of translating the following English sentence “The car is in Chicago” to French. Let’s also assume that all the tokens in the sentence are in the vocabulary, except “Chicago”. An NMT model might output the following sentence: “La voiture est à <unk>”. With a copy mechanism, the model would be able to automatically replace the unknown token with one of the tokens from the input sentence, in this case, “Chicago”.

The copy mechanism can be particularly relevant for source code, where the size of the vocabulary can be several times the size of a natural language corpus [13]. This results from the fact that developers are not constrained by any vocabulary (e.g., English dictionary) when defining names for variables or methods. This leads to an extremely large vocabulary containing many rare tokens, used infrequently only in specific contexts. Thus, the copy mechanism applied to source code allows a system to generate rare out-of-vocabulary identifier names and numeric values as long as they are somewhere in the input. Furthermore, in natural language, a human recipient may be able to use context to cope with one missing word in an automatically translated sentence. In a programming language, the compiler does not make any semantic inference, and the generation has to be complete. For example, if the code to predict is “if (i < num_cars)”, then generating “if (i < int)” is not going to work at all. We discuss the mathematics of the copy mechanism in the context of SequenceR in Section 3.3.1. Readers interested in more detail are referred to the work by See et al. [9].

Tufano et al. [10] proposed using NMT with the goal of learning bug-fixing patches by translating the entire buggy method into the corresponding fixed method. Before the translation, the authors perform a code abstraction process which transforms the source code into an abstracted version, which contains: (i) Java keywords and identifiers; (ii) frequent identifiers and literals (a selection of 300 idioms); (iii) typified IDs (e.g., METHOD_1, VAR_2) that replace identifiers and literals in the code. In Section 6 we highlight differences and improvements introduced in SequenceR.

Another approach to addressing the vocabulary size problem in code is to use byte pair encoding (BPE), which has been widely used in NLP and also applied to source code [14]. For SequenceR, we did preliminary experiments with BPE to solve the unlimited vocabulary problem, but our early results showed that it is less effective than the copy mechanism.

SECTION 3Approach to Using Seq-to-Seq Learning for Repair
SequenceR is a sequence-to-sequence deep learning model that aims at automatically fixing bugs by generating one-line patches (i.e., the bug can be fixed by replacing a single buggy line with a single fixed line). We do not consider line deletion because: 1) it does not require a method for token generation (and is thus less interesting to our research) and 2) if desired, SequenceR could be combined with the lightweight Kali [11] to include line deletion. We do not consider line addition because spectrum based fault localization, used in most of the related work, is not effective for line addition patches [15]. We note that in 64 percent of all 395 bugs in Defects4J are fixed by replacing existing source code [16]. Given a Software System with a faulty behavior (i.e., failing test case), state-of-the-art fault localization techniques are used to identify the buggy method and the suspicious buggy lines. Such techniques have been shown to predict the correct buggy line as one of the top 10 candidates in 44 percent of the time [15]. SequenceR then performs a novel Buggy Context Abstraction (Section 3.2) process which intelligently organizes the fault localization data (i.e., buggy classes, methods, and lines) into a representation that is concise and suitable for the deep learning model yet able to preserve valuable information regarding the context of the bug, which will be used to predict the fix. The representation is then fed to a trained sequence-to-sequence model (Section 3.3.1) which performs Patch Inference (Section 3.4) and is capable of generating multiple single-lines of code that represent the potential one-line patches for the bug. Finally, SequenceR in the Patch Preparation (Section 3.5) step generates the concrete patches by formatting the code and replacing the suspicious line with the proposed lines. Fig. 2 shows the aforementioned steps both for the training phase (left) and inference phase (right). In the remainder of this section we will discuss the common steps as well as those specific for training and inference.


Fig. 2.
Overview of our approach using sequence-to-sequence learning for program repair.

Show All

3.1 Problem Definition
Given a buggy system bs, and test suite t, we assume a fault localization technique, FL, which identifies an ordered set of potential bug locations l={l1,l2,…}, where each location li consists of the buggy class bci, buggy method bmi, and the buggy line bli:
l∀li∈l,li={loc∣loc∈FL(bs,t)}={bci,bmi,bli}andbli⊂bmi⊂bci.
View SourceThe problem is to predict (i.e., generate) a fixed line fli, where li is the true bug location, such that by replacing bli with fli in bmi, the resulting system fs passes the test suite and the bug is considered fixed. SequenceR tackles this problem by taking as input the fault localization data (i.e., l={l1,l2,…}) of a buggy system and attempts to generate fixed line fli for each li in order. The bs, t, l, li, bci, bmi, bli, fli and fs notations are used throughout this work.

3.2 Buggy Context Abstraction
The context of a bug plays a fundamental role in understanding the faulty behavior and reasoning about the possible fix. During bug-fixing activities, developers usually identify the buggy lines, then analyze how they interact with the rest of the method’s execution, and observe the context (e.g., variables and other methods) in order to reason about the possible fix and possibly select several tokens in the context to build the fixed line [17].

SequenceR mimics this process by constructing the abstract buggy context and organizing the fault localization data into a representation that is concise yet retains the necessary context that allows the model to predict the possible fix. During this process SequenceR needs to balance two contrasting goals: (i) reduce the buggy context into a reasonably concise sequence of tokens (since sequence-to-sequence models suffer from long sentences [18]), (ii) while at the same time retaining as much information as possible to allow the model to have enough context to predict a possible fix.

Given the bug locations l={l1,l2,…}, for each li∈l,li={bci,bmi,bli}, SequenceR performs the following steps:

Buggy Line <START_BUG> is inserted before the first token in the buggy line bli and <END_BUG> is inserted after the last token. The rationale is that we would like to propagate the information extracted by the fault localization technique and indicate to the model what is a buggy line. In doing so, we mimic developers who focus on the buggy lines during their bug-fixing activities.

Buggy Method The remainder of the buggy method bmi is kept in the representation. The rationale is that the method provides crucial information on where the buggy line is placed and its interaction with the rest of the method.

Buggy Class From the buggy class bci we keep all the instance variables and initializers, along with the signature of the constructor and non-buggy methods even if they are not called in the buggy method. The body of the non-suspcious methods is stripped out. The rationale for this choice is that the model could use variables and method signatures as potential sources when building the fixed line fli.

After these steps, SequenceR performs tokenization and truncation to create the abstract buggy context. Truncation is used to limit the abstract buggy context to a predetermined size in cases where the input sequence is too long. This allows SequenceR to process input files of arbitrary size without running out of memory. The truncation process can be summarized as: 1) the truncation size will be chosen such that most input files do not require truncation 2) if the buggy line itself is over the truncation limit, as many tokens as possible from the start of the line are included up to the limit 3) otherwise, the buggy line is included in abstract buggy context and twice as many tokens are included before the line as after the line. For example, if the truncation limit is 1,000 tokens and a 5,000 token file has a buggy line with 100 tokens (including the START_BUG and END_BUG tokens) in the middle of the file, then abstract buggy context will consist of 600 tokens before the buggy line, then 100 tokens of the buggy line, then 300 tokens after the buggy line. Generally, truncation will delete the actual class definition from the input, but context near the buggy line is preserved to aid in patch generation.

The abstract buggy context represents the input to the sequence-to-sequence network which will be used to predict the fixed line. Internally, abstract buggy context is represented as a sequence of tokens belonging to a vocabulary V. The out-of-vocabulary tokens (token∉V) are replaced with the unknown token <unk>. In Section 3.6 we describe how we empirically derive the vocabulary V and in Section 3.3.1 we explain how the copy mechanism helps in overcoming the unknown tokens problem.

Fig. 3 shows the output of this process. The original class is presented in Listings 1 and 2 displays the buggy class after Buggy Context Abstraction. Listing 3 illustrates the class when tokens that are out of vocabulary are replaced with the unknown token <unk>. Programming language tokens such as class and int are not replaced with <unk> because they are part of the vocabulary. Other in-vocabulary tokens include common variable names such as i. Our sequence-to-sequence network receives Listing 2 as input.


Fig. 3.
Illustration of the abstract buggy context step in SequenceR. bc is highlighted in yellow, bm is highlighted in orange and bl is highlighted in red.

Show All

3.3 Sequence-to-Sequence Network
In this phase we train SequenceR to learn how to generate a fix for a given bug. Specifically, we train a Sequence-to-Sequence Network with Encoder-Decoder model (with attention and copy mechanism) to translate the abstract buggy context of a bug to the corresponding target fixed line fl. To train such a network we rely on a large dataset of bug fixes mined from different sources, explained in Section 4.3. The bug fixes are divided into training and testing data, which are used to train and evaluate the Sequence-to-Sequence Network described in Section 3.3.1.

3.3.1 Model
Fig. 4 shows our model for sequence-to-sequence learning to create Java source code patches. The basis of our model is a recurrent neural network similar to a natural language processing architecture [5]. During training, the source token sequence X=[x1,…,xn] (i.e., abstract buggy context) is provided to the encoder, where n is the token length of abstract buggy context. Then, the decoder produces the target sequence Y=[y1,…,ym] (i.e., the fixed line), where m is the token length of the fixed line. Back propagation is used to update the parameters in the network with stochastic gradient decent during training [19]. The trained parameters are unchanged during inference (patch generation in our case).


Fig. 4.
Sequence-to-sequence model used in SequenceR.

Show All


Fig. 5.
Patch preparation step using copy mechanism.

Show All

Encoder. The encoder is a recurrent neural network using LSTM gates to process the input [20]. It is a bidirectional encoder which allows the encoding for a token to incorporate information from other tokens both before and after it in the input data [21]. The encoder converts the source sequence X=[x1,…,xn] into a sequence of encoder hidden states hi using a learnable recurrence function ge. After reading the last token, the last hidden state, hen is used as the context vector c for use in initializing the decoder [22]:
hei=ge(xi,hei−1).(1)
View Source

Decoder. The decoder is also a recurrent neural network using LSTM gates. When initialized by the encoder, it begins production of the patch candidate by receiving the special start token as input y0. For each previous output token yj−1, the decoder updates its hidden state hdj using the learnable recurrence function gd [22]:
hdj=gd(yj−1,hdj−1,c).(2)
View Source

The initial value hd0 is provided by a learnable bridge function of the encoder state. The decoder states hdj are used in for token generation by the attention and copy mechanisms in Equations (4) and (5). The decoder stops generating new tokens when the last token generated by the model is a special end-of-sequence token.

Attention. In addition, we use an attention mechanism that provides a way to create a more specific context vector cj for each output token yj from the decoder using a linear combination of the hidden encoder states hei [23]:
cj=∑i=1nαjihei.(3)
View SourceWhere αji represents learnable attention weights. This context vector cj is used by a learnable function ga to allow each output token yj to pay “attention” to different encoder hidden states when predicting a token from vocabulary V:
PV(yj∣yj−1,yj−2,…,y0,cj)=ga(hdj,yj−1,cj).(4)
View Source

Copy. In Section 2 we presented the intuition behind the copy mechanism, while in this section we describe how it operates during patch generation. The copy mechanism can significantly improve the performance of the system by allowing the model to select a token from any of the tokens provided in the abstract buggy context, even when the tokens are not contained in the training vocabulary. We empirically show the improvements offered by this approach by comparing it to the vanilla sequence-to-sequence model without a copy mechanism in Section 4.4.2. The copy mechanism contributes to Equation (4) to produce a token candidate. This component calculates pgen, the probability that the decoder generates a token from its initial vocabulary. And 1−pgen is the probability to copy a token from the input tokens depending on the attention vector αj in Equation (3) [9]:
pgen=gc(hdj,yt−1,cj)(5)
View Source
P(yj)=pgenPV(yj)+(1−pgen)∑i:xi=yjaji,(6)
View Sourcegc in Equation (5) is learnable function. Using Equation (6), the output token yj for the current decoder state is selected from the set of all tokens that are either: 1) tokens in the training vocabulary (including the <unk> token) or 2) tokens in the abstract buggy context. Although there are no <unk> targets in the training set for patches, if the PV computation is very uncertain which token is correct, it may happen to have a high likelihood for <unk>. If at the same time, pgen is high then a <unk> token will be produced as the copy mechanism did not replace it. Such outputs are discarded as discussed in Section 3.5.

3.4 Patch Inference
Once the sequence-to-sequence network is trained, it can be used to generate patches for projects outside of the training dataset. During patch inference, we still generate abstract buggy context for the bug, as described in Section 3.2. But we will use beam search to generate multiple likely patches for the same buggy line, as done in related work [10], [24]. Beam search works by keeping the n best sequences up to the current decoder state. The successors of these states are computed and ranked based on their cumulative probability; and the next n best sequences are passed to next decoder state. n is often called the width or beam size, and beam search with an infinite n corresponds to doing a complete breath-first-search. In Listing 5, we have an example of predictions with beam size 5 for the bug presented in Listing 2. Each row is one prediction from the model, representing one potential bug fix, and each of them is further processed by the patch preparation step described below.

3.5 Patch Preparation
The raw output from the sequence-to-sequence network cannot be used as a patch directly. Fig. 5 shows the process used to prepare patches from the raw output. First, the predictions might still contain <unk> tokens not handled by the copy mechanism. Listing 4 illustrates token values before the copy mechanism replaces <unk> for samples 4 and 5. But the copy mechanism may not replace all such tokens as seen in sample 3 of Listing 5. Second, the predictions contain a space between every token, which is not well-formed source code in many cases. (For example, a space is not allowed between the dot separator, “.”, and a method call, but a space is required between a type and the corresponding identifier name.)

Consequently, we have a final patch preparation step as follows. We discard all line predictions that contain <unk> and we reformulate the remaining predictions into well-formed source code by removing or adding the required spaces. An example is shown between Listings 5 and 6, whitespaces are adjusted and the third prediction from Listing 5 is removed since it contains <unk> token. Each one of the line predictions is used to create a candidate program by replacing the original buggy line bli (i.e., the <START_BUG>, <END_BUG> and all tokens in between are replaced with the model output).

More formally, the remaining candidate fixed lines, candi={pre1i,pre2i,..}, will replace the buggy line bli in buggy system bs and generate candidate patches {patch1i,patch2i,…}, which should be verified with any patch validation technique, such as test suite validation. When the test suite is weak to specify the bug, we can have different patches {patch1i,patch1j,…} for different bug locations {li,lj,…} that passed the test suite. Then, the correctness can be verified, for example, by manual inspection.

3.6 Implementation Details & Parameter Settings
Library. We have implemented our Encoder-Decoder model using OpenNMT-py [25], built in the Python programming language and the PyTorch neural network platform [26].

Vocabulary. In this paper, we consider a vocabulary of the 1,000 most common tokens. To the best of our knowledge, this is one of the largest vocabularies considered for machine learning for patch generation: for comparison, DeepFix [27] has a vocabulary size of 129 words, and Tufano et al. [10] considered a vocabulary size of 430 words.

Limit for Truncation. We truncate if the abstract buggy context is longer than 1,000 tokens. This is motivated by Fig. 7, where we can see that abstract buggy context is often less than 1,000 tokens long. SequenceR truncates by keeping the buggy line but removing statements, class definitions, and method definitions until abstract buggy context is 1,000 tokens or less.

Network Parameters. We explored a variety of settings and network topologies for SequenceR. Most major design decisions are verified with ablation experiments that change a single variable at a time as detailed further in Section 5. We train our model with a batch size of 32 for 10,000 iterations. To prevent overfitting, we use a dropout of 0.3. In relation to the components shown in Fig. 4, below are the primary matrix sizes associated with each component along with a reference to the equations in Section 3.3.1 to which they relate:

Token embedding (our model uses the same embedding for both ge and gd): 1,004x256 (1,000 + 4 special tokens)

Encoder bidirectional LSTM (part of ge fuction): 256x256x4x2x2

Decoder LSTM (part of gd function): 512x256x4x2 + 256x256x4x2

Token generator (part of ga function): 256x1004

Bridge between encoder and decoder (path for hei to initialize hd0): 256x256x2

Global Attention (αji weights): 256x256 + 512x256

Copy selector (gc function): 256x1

We use a beam size of 50 during inference, which is the default value used in the literature [10], [24] and which proves to be good empirically.

Input and output Summary. The input to SequenceR is a Java class of any size. The non-empty faulty line within a method on which to attempt repair has been identified by another technique (usually line-based fault localization). The output is the fixed line which must have fewer than 100 tokens with our current model.

Usage. After SequenceR is trained, we can use it to predict fixes to a bug. SequenceR takes as input the buggy file and a line number indicating where the bug is. The output is a list of patches in the diff format, so that the user can run their own patch validation step, which could either be test validation or manual inspection.

The source code of SequenceR is available at https://github.com/kth/SequenceR, together with the best model we have identified and the synthesized patches.

SECTION 4Evaluation
In this section, we describe our evaluation of SequenceR.

4.1 Research Questions
The two first research questions focus on machine learning:

RQ1: To what extent can the fixed line be perfectly predicted?

RQ2: How often does the copy mechanism generate out-of-vocabulary tokens for a patch, and which parts of abstract buggy context are referenced for the copy?

The last two research questions look at the system from a domain-specific perspective: we assess the performance of SequenceR from the viewpoint of program repair research.

RQ3: How effective is SequenceR’s sequence-to-sequence learning in fixing bugs in the well-established Defects4J benchmark?

RQ4: What repair operators are captured with sequence-to-sequence learning?

4.2 Experimental Methodology
4.2.1 Methodology for RQ1
We train SequenceR with the parameter settings described in Section 3.6. The training and validation accuracy and perplexity will be plotted. Perplexity (ppl) is a measurement of how well a model predicts a sample and is defined as:
ppl(X,Y)=exp(−∑∣Y∣i=1logP(yi∣yi−1,…,y1,X)∣Y∣),
View Sourcewhere X is the source sequence, Y is the true target sequence and yi is the ith target token [25]. Luong et al. found a strong correlation between a low perplexity value and high translation quality [28].

The resulting model is tested on our testing dataset, CodRep4 (see Section 4.3.1). Next, in order to compare SequenceR against the state-of-the-art approach by Tufano et al. [10], we created CodRep4Medium. It is a subset of CodRep4 containing 1,116 samples where the buggy method length is limited to 100 tokens.

4.2.2 Methodology for RQ2
To evaluate the effectiveness of the copy mechanism (described in Section 3.3.1), we consider all samples from CodRep4. For each successfully predicted line, we categorize tokens in that line based on whether the token is in the vocabulary or not. And at the same time, for tokens that are out-of-vocabulary but are copied from the input sequence, we try to find the original location of the copied token. By analyzing the original location of out-of-vocabulary tokens, we can measure the importance of the context, in particular of the abstract buggy context we define in this paper. The copy mechanism allows the system to be more powerful by providing more tokens beyond the vocabulary to be used in the patch.

4.2.3 Methodology for RQ3
We evaluate SequenceR on Defects4J [16], which is a collection of reproducible Java bugs. Most recent approaches in program repair research on Java use Defects4J as an evaluation benchmark [3], [12], [29], [30], [31].

Since the scope of our paper is on one-line patches, we first focus on Defects4J bugs that have been fixed by developers by replacing one single line (there are 75 such bugs). In order to study the effectiveness of sequence-to-sequence itself, we isolate the fault localization step as follows: the input to SequenceR is the actual buggy file and the buggy line number. SequenceR then produces a list of patches (recall that beam search produces several candidate patches). All patches are compiled and then executed against the test suite written by the developer.

Each candidate patch generated by SequenceR is then categorized as follows:

Compilable patch: The patch can be compiled.

Plausible patch: The patch is compilable and passes the test suite. The patch may yet be incorrect because of the overfitting problem [32].

Correct patch: The patch passes the test suite, and is semantically equivalent to the human patch. We hand-check for semantic equivalence for this evaluation.

As per the definitions, there is a strict inclusion structure in those categories: correct patches are necessarily plausible and compilable, plausible patches are necessarily compilable.

4.2.4 Methodology for RQ4
For RQ4, we aim at having a qualitative understanding of the cases for which our sequence-to-sequence repair approach works. This research question is motivated by the need to understand what grammatically correct code transformations are captured by SequenceR, even though it is purely a token-based approach with no first class AST or grammar knowledge. For gaining this understanding, we use a mixed method combining grounded theory and targeted analysis. The results would be an understanding of the variety of repair operators and programming language syntax captured by SequenceR in cases where the model output correctly matches the test data. For the grounded theory, we have been regularly sampling successful cases, i.e., cases in our testing dataset CodRep4 for which SequenceR was able to predict the fixed line, for each case, the authors reached a consensus to know whether 1) the case is interesting from a programming perspective (e.g., it represents a common bug fix pattern), and 2) the case highlights a phenomenon that has already been covered in a previously found case. For the targeted analysis, we specifically searched for 2 kinds of results: cases where the copy mechanism was used and cases where a specific programming construct was involved (method call, field reference and string literals).

4.3 Training Data
SequenceR is trained based on past modifications made to source code, i.e., it is trained on past commits. In our experiments, we combine two sources of past commits, the CodRep dataset [33] and the Bugs2Fix dataset [10], into what appears to be the largest dataset of one-line bug fixes published to date. Both datasets 1) consider Java code and 2) have been built based on the history of open-source projects.

The CodRep dataset focuses solely on one-line source code fixes (aka one-line patches), it contains 5 datasets curated from real commits on open-source projects. The Bugs2Fix dataset contains diffs mined from Github between March 2010 and October 2017 for bug-fixing commits (based on heuristics to only consider bug-fixing commits). Neither dataset requires the buggy project to have a test suite for exposing the buggy behavior, instead they are focusing on collecting bug fix commits.

4.3.1 Data Preparation
Since CodRep and Bugs2Fix datasets are in different formats, we first unify these two datasets as follows. First, we only keep diffs from Bugs2Fix which are fixes with a single line replacement. Further, we filter out certain diffs if the changes are outside of a method.

Since the Bugs2Fix dataset comes from a generic bug-fix data mining which includes multi-line fixes and fixes outside of methods, we can look at its statistics to help understand the generality of SequenceR. Bugs2Fix contains 92,849 commits. 15,548 of these (17 percent) are one-line patches within a method, and are within the problem domain of SequenceR.

After preparing the dataset, we divide it into training and testing data. CodRep is originally split into 5 parts, numbered from 1 to 5, with each part containing commits from different groups of projects. Our training data consists of CodRep datasets 1,2,3 & 5 and the Bugs2Fix dataset. Our testing data is CodRep dataset 4 (or CodRep4 for short). We chose dataset 4 because it is approximately 20 percent of the entire CodRep data (data set 1 is less than 10 percent and data set 5 is over 30 percent) and because CodRep 4 contains a broad and representative set of projects on which to evaluate [33].

Furthermore, we ensure there are no duplicate samples between the training and testing datasets. During the model setup, we use a random subset of 95 percent of the training data for model training and 5 percent as our validation dataset.

4.3.2 Descriptive Statistics of the Datasets
In total, we have 35,578 samples in our training set and 4,711 samples in our testing set.

Input Size. Fig. 7 shows the size distribution of the abstract buggy context in number of tokens before truncation is done. The CodRep training data has a median token length of 372; the Bugs2Fix dataset has a median length of 340 tokens; and the testing dataset has a median length of 411. These variations are a result of using different Java projects in the datasets, but we observe that the distribution of lengths is similar.

Prediction Size. The lines from the abstract buggy context samples in our dataset had a median length of 6. 99 percent of the lines were 30 tokens or fewer, which fits well typical output sizes used for natural language processing. To sum up, the order of magnitude of the sequence-to-sequence prediction receives an input sequence with an average length of 350 tokens and produces an output sequence with an average length of 6 tokens.

Vocabulary Size. In our training data, the full vocabulary is 567,304 different tokens. Fig. 6 shows the distribution of the number of occurrences for the whole vocabulary. It is a typical power-law like distribution with a long tail. We limit our training vocabulary to the 1,000 most common tokens.

Fig. 6. - 
Overview of vocabulary: token count occurrences follow a Zipf’s law distribution.
Fig. 6.
Overview of vocabulary: token count occurrences follow a Zipf’s law distribution.

Show All


Fig. 7.
Only 14 percent of samples exceed the 1K token length limit and require truncation.

Show All

4.4 Experimental Results
4.4.1 Answer to RQ1: Perfect Predictions
We trained our model on a GPU (Nvidia K80) for 1.2 hours. For a typical training run on our golden model, Fig. 8 shows the training and validation accuracy per token generated (the accuracy for the entire patch would be lower) and Fig. 9 shows the perplexity (ppl) per token generated over the training and validation datasets. In this particular run, the best results for both the perplexity and accuracy on the validation dataset occur at 10,500 iterations. We chose 10,000 iterations as the standard training time for our model.


Fig. 8.
Training and validation accuracy.

Show All


Fig. 9.
Training and validation perplexity.

Show All

CodRep4. On the 4,711 prediction tasks of our best model, SequenceR is able to generate the perfect fix in 950 cases (from Table 1). In all those cases, the predicted line that replaces the buggy line is exactly the line fix implemented by the developer. The copy mechanism is used in a number of cases, this will be further discussed in Section 4.4.2.

TABLE 1 Comparison with State-of-the-Art Approach by Tufano et al.

Comparison to state-of-the-Art. To the best of our knowledge, the state-of-the-art approaches are from Tufano et al. [10] and Hata et al. [34]. We only compare against Tufano et al. since their approach has been open sourced while that one of Hata et al. was not made available at the time of writing this paper. The approach used by Tufano et al. is limited to fixes only inside small methods, consisting of less than 100 tokens. The limitation is due to the fact that their approach generates the entire fixed source code method as output of the decoder. This means that the decoder may need to generate a long sequence of source code tokens, which is one of the major challenges for NMT models [35]. SequenceR does not make any assumption on the size of the buggy method. In order to compare against [10], we select those 1,116 tasks from CodRep4 where the buggy line resides in a method smaller than 100 tokens. Those 1,116 tasks are called the CodRep4Medium testing dataset.

Our testing accuracy for both CodRep4 and CodRep4Medium are shown in Table 1. From the table, we see that the accuracy of SequenceR is 344/1,116 (30.8 percent) while Tufano et al. [10] is 157/1,116 (14.1 percent). This is a clear indicator that SequenceR outperforms the current state-of-the-art showing twice as many correct predictions. It shows that our construction of the abstract buggy context, together with the copy mechanism, leads to higher accuracy than only having the buggy method as context with a specific encoding for variables. Recent fault localization research [15] indicates that best-in-class techniques can predict the faulty line 44 percent of the time and the faulty method 68 percent of the time. If we extrapolate these percentages to our data, SequenceR is more likely to find correct one-line patches than the prior work [10] is to find method replacements, and SequenceR can process and repair larger methods as demonstrated by the right-hand column of Table 1.

We now concentrate on the effectiveness of the approach depending on the buggy method length. Overall, we observe that SequenceR has a lower accuracy on longer methods (30.8 percent accuracy on CodRep4Medium, 20.2 percent accuracy on CodRep4). This phenomenon is explained by the fact that fixes in long methods are usually more complex and involve more context variables, identifiers and literals that are not easily captured by the learning system. This phenomenon has also been previously observed [10].

4.4.2 Answer to RQ2: Copy Mechanism
We now look at to what extent the copy mechanism is used. Fig. 10 shows the origin of tokens in successfully predicted lines, per patch size. Let us consider the highest bar, corresponding to all successfully predicted lines consisting of 7 tokens. For those 7-token patches, the black bar means that all tokens are taken from the vocabulary. The non-black bars mean that the copy mechanism has been used to predict the line fix. Overall, there is a minority of patches (216/950, 23 percent) for which all tokens come from the vocabulary. At the extreme, the longest successful patch generated by SequenceR was 68 tokens long, but the longest successful patch without the copy mechanism was only 27 tokens long.


Fig. 10.
Histogram showing correctly generated patches: 1) That only use tokens in our 1,000 token vocabulary, 2) that need to copy tokens from the buggy line, 3) from the buggy method and 4) from the buggy class.

Show All

Fig. 10 also lets us analyze the location origin of the copied token. The brown bars represent those patches for which copied tokens all come from the buggy line: this is the majority of cases (641/950, 68 percent). However, we also observe cases where some copied tokens have been taken from the buggy method (green bars) and cases where the copied tokens has been taken from the buggy class (red bars), i.e., taken from the class context as captured in our encoding.

As an example, Listing 7 replaces variable masterNode with nonMasterNode as in the correct human patch. nonMasterNode in the fixed line does not occur in our training data and hence it is not in our 1000 token vocabulary. Therefore, SequenceR was able to generate this patch because it copied the out-of-vocabulary token nonMasterNode from within the buggy method. As this example is a 4 token long patch, it would contribute to the green bar for patch length 4 in Fig. 10.

Listing 7. Example of the Copy Mechanism Creating a Correct Patch by Incorporating a Variable which is Not in the Vocabulary from the Broader Context Around the Buggy Line
while(nonMasterNode == null) {

nonMasterNode=randomFrom(internalCluster().getNodeNames());

if(nonMasterNode.equals(masterNode)) {

- masterNode = null;

+ nonMasterNode = null;

}

}

Overall, Fig. 10 shows that the copy mechanism is extensively used (734/950, 77 percent) and that our class level abstraction enables us to predict difficult cases where only the buggy line or the buggy method would not have been enough.

In order to understand the benefits of context size with the copy mechanism, we measured the distance in tokens to reach a copied token used to generate a patch. In the 87 cases where a copied token was needed from the buggy method bm, the median distance from the buggy line bl to the nearest use of the copied token was 9 tokens, 90 percent of the 87 cases were within 49 tokens of bl, and 100 percent were found within a 122 token distance. In the 7 cases when a copied token was needed from the buggy class bc, the median distance to the copied token from bm was 25 tokens, and 100 percent were found within a 241 token distance. In addition to ablation study results discussed is Section 5, the preceding data supports our decision to create the abstract buggy context.

4.5 Answer to RQ3: Defects4J Evaluation
As explained in Section 4.2.3, we consider 75 Defects4J bugs that have been fixed with a one-line patch by human developers. In total SequenceR finds 2,321 patches for 58 of the 75 bugs. The main reason that we are unable to fix the remaining 17 bugs is due to fact that some bugs are not localized inside a method, which is a requirement for the fault localization step that SequenceR assumes as input. Listing 8 is one such example where the Defects4j bug is not localized inside a method. We have 2,321 patches instead of 2,900 (58x50) because some predictions are filtered by the patch preparation step (Section 3.5), i.e., patches that contain the <unk> token. The statistics about all bugs can be found in Fig. 11. Out of 75 bugs, SequenceR successfully generated at least one patch for 58 bugs, 53 bugs have at least one compilable patch, 19 bugs have at least one patch that passed all the tests (i.e., are plausible) and 14 bugs are considered to be correctly fixed (semantically identical to the human-written patch). Of these 14 bugs, in 12 cases the plausible patch with the highest ranking in the beam search results was the semantically correct patch.

Fig. 11. - 
SequenceR results on the 75 one-line Defects4J bugs.
Fig. 11.
SequenceR results on the 75 one-line Defects4J bugs.

Show All

Listing 8. An Example of Defects4J Defect (Math 104) where the Bug is not Localized Inside a Method. In this Case, a Class Variable is Changed
- private static final double DEFAULT_EPSILON = 10e-9;

+ private static final double DEFAULT_EPSILON = 10e-15;

Fig. 12 gives a different perspective on this data, focusing on patches (and not bugs). SequenceR is able to generate 761 compilable patches (33 percent of all patches). SequenceR finds 61 plausible patches spread over 19 bugs, thus there can be several plausible patches for the same bug, a phenomenon well-known in the program repair field [12]. One reason is that some Defects4J bugs have a weak test suite. To the best of our knowledge, we are the first to report the correctness of patches generated by a sequence-to-sequence model, where correctness means passing the test suite and being semantically equivalent to the human patch. In the end, SequenceR is able to generate 18 patches that are semantically equivalent to the correct bug fix.


Fig. 12.
Stastistics on patches synthesized by SequenceR for the 75 one-line Defects4J bugs.

Show All

For SequenceR applied to Defects4J bugs, we observe that out of 61 plausible patches, 18 are correct, which is a ratio of 30 percent. An analysis of prior techniques which used a different benchmarck in C (GenProg [36], RSRepair [37], and AE [38]) shows that they have a correct patch ratio of less than 12 percent [11]. We did not evaluate SequenceR on the same benchmark as this prior work (we target Java not C), but the ratio is evidence that SequenceR has learned to produce outputs which represent reasonable patch proposals.

Although we did not directly include fault localization in our evaluation of SequenceR, we can estimate the performance of a repair system which includes state-of-the-art fault localization techniques [15] as follows. It has been shown that there is an estimated 44 percent success of correctly identifying a faulty line in the top 10 candidates. Hence, in order to process 75 total bugs from Defects4J, 750 candidate abstract buggy contexts would need to be prepared for input to our model. We have run fault localization with Gzoltar [39] and found that it successfully localized the faulty line for 9 of the 14 bugs for which SequenceR found a correct fix.

Let us now discuss timing. We estimate the machine time required to automatically find patches for 75 bugs with the summation below1:

Estimated time to run fault localization on 75 bugs and identify 10 likely faulty line locations: 112 minutes

Time to create 750 abstract buggy contexts (10 created for each bug): 29 minutes

Time to create 37,500 patch candidates (50 candidates created from beam size 50 for each abstract buggy context): 9 minutes

Estimated time to prune raw patches down to 23,210 total patches: 2 minutes

Time to attempt compile on 23,210 patches: 1378 minutes

Time to run test cases on 7,610 patches: 6287 minutes

Final result estimated to take 130 total machine hours to find patches which correctly fix 9 bugs.

Listing 9 shows the SequenceR patch for Math 75, which is semantically equivalent to the human patch. We observe that it contains some unnecessary parentheses, and the same behavior occasionally occurs in other patches found by SequenceR. We have observed unnecessary parenthesis in some of the human-generated patches in our training data and SequenceR occasionally replicates this human style. In this case, the parentheses do not change the order of evaluation. Therefore the SequenceR patch for Math 75 is semantically equivalent to the human patch.

Interestingly, getPct is not part of the vocabulary, and it did not appear in the buggy method. The getPct method is defined in the same buggy class, as captured by our abstract buggy context. In Defects4J, the copy mechanism is also useful to capture the right tokens to add in the patch.

Listing 9. Found Patch for Math 75
- return getCumPct((Comparable<?>) v);

+ return getPct((Comparable<?>) v); // Human patch

+ return getPct(((Comparable<?>)(v))); // SEQUENCER patch

We now compare those results against the patches found by recent program repair tools that are publicly available. Elixir [4], CapGen [3] and SimFix [31] have reported 26, 22, 34 correctly repaired bugs for all Defects4J bugs, where the patch is identical to the human patch or claimed as correct. Of those correctly repaired bugs, 22, 19 and 17 respectively are for the 75 one-line bugs that we consider for SequenceR. We notice that the majority of claimed correct patches are for one-line bugs. We observe that SequenceR does not fix more one-line Defects4J bugs.

While Elixir, CapGen, and SimFix are driven with intelligent design and require substantial configuration and handcrafted rules, our goal with SequenceR is to be agnostic and to not design any repair operator upfront. For example, CapGen implements context-aware operator selection and context-aware ingredient prioritization [3]. The CapGen implementation heavily relies on code transformation tools and carefully selected algorithms/parameters/metrics. In constrast, our SequenceR can be considered less heavyweight. We note that the required parameter tuning in SequenceR can easily be performed using grid search or other meta-optimization techniques [40]. To that extent, it is remarkable that such a generic approach is able to learn bug-fixing patterns and synthesizes 18 patches that are semantically equivalent to the human repair, without any static or dynamic analysis. By providing a generic approach, SequenceR will improve in the future as machine learning sequence-to-sequence techniques improve, and as more bug fix training data is provided. Also, since SequenceR learns repair operators from examples, it could be trained on less common languages (such as COBOL).

We assume perfect fault localization while other related tools ran fault localization to localize the buggy source code. Yet, different papers use different fault localization algorithms, implementations, and granularity (e.g., methods versus line). Liu et al. pointed out that because of different assumptions about fault localization, it is hard to compare different repair techniques [41]. By assuming perfect fault localization, we purely focus on the patch generation step of the algorithm.

4.6 Answer to RQ4: Qualitative Case Studies
We now present the diversity of repair operators that are captured by SequenceR. These cases are culled from the 950 correct patches SequenceR generated for the CodRep4Full test dataset. Both the buggy line that was part of the input is shown and the correct patch which includes examples of repair operators. We also highlight again the effectiveness of the copy mechanism by using a bold underlined font for those tokens that were copied (i.e., that are outside the vocabulary of the 1,000 most common tokens).

4.6.1 Case Study: Method Call Change
Our training and evaluation data consist of object-oriented Java software. We observe that SequenceR captures different kinds of operations related to method calls.

Call Change. Here a call to method writeUTF is replaced by a call to method writeString.

Listing 10. Call Change
- out.writeUTF(failure);

+ out.writeString(failure);

Call Deletion. The buggy line chains two method calls; this successful prediction consists of deleting one of them.

Listing 11. Call Deletion
- FieldMappers x = context.mapperService().smartNameFieldMappers(fieldName);

+ FieldMappers x = context.smartNameFieldMappers(fieldName);

Argument Addition. In this patch, SequenceR adds an argument (which in Java, means calling another method).

Listing 12. Argument Addition
- stage.getViewport().update(width, height);

+ stage.getViewport().update(width, height, true);

Target Change. In this successful case, the patch also calls method isTerminated but on another target (scheduledExecutorService instead of executorService, which is copied from the input context).

Listing 13. Target Change
- if(!(executorService.isTerminated())){

+ if(!(scheduledExecutorService.isTerminated())){

4.6.2 Case Study: If-Condition Change
SequenceR can change if conditions, and in this particular case, removes two clauses from the boolean formula.

Listing 14. if-Condition Change
- if(((t >= 0) && (t <= 1)) && (intersection != null))

+ if(intersection != null)

4.6.3 Case Study: Java Keyword Change
SequenceR is also able to generate patches involving the replacement of programming language keywords, indicating clues of syntax understanding.

Listing 15. Java Keyword Change
- break ;

+ continue ;

4.6.4 Case Study: Change from Field Access to Method Call
A good practice of software engineering is to implement encapsulation by calling methods instead of directly accessing fields, this is handled by SequenceR as follows (size to size())

Listing 16. Change from Field Access to Method Call
- app.log(“PixmaPackerTest”, (“Number of textures: ” + (atlas.getTextures().size)));

+ app.log(“PixmaPackerTest”, (“Number of textures: ” + (atlas.getTextures().size())));

4.6.5 Case Study: Off-by-One Repair
Finally, SequenceR is also able to repair classical off-by-one errors.

Listing 17. off-by-One Repair
- nextIndex = currentIndex;

+ nextIndex = (currentIndex) - 1;

Overall, SequenceR uses all three kinds of token operations: 1) Token deletion, e.g., Listing 11; 2) Token addition, e.g., Listing 12; 3) Token replacement, e.g., Listing 13.

SECTION 5Ablation Study
We perform an ablation study to understand the relative importance of each component of our approach. The process is as follows. First, we identify the golden model based on a greedy optimization in the parameter search space. This is the model that we described in Section 4. Then we change one single parameter to a different reasonable value and report the performance on the same testing dataset. The ablation results demonstrate that parameter selections for the golden model produce the highest acceptance rates for the configurations we tested. The model parameters we found with our dataset are likely to yield reasonable results when training for other computer languages so long as a form of abstract buggy context can be done to provide context related to the buggy line. We provide details on our ablation results to aid future researchers in understanding which variables are most likely to improve their own models.

Due to randomness in learning, for each parameter, we run each configuration multiple times and report the mean and standard deviation for the model as recommended for assessment of random algorithms [42]. As our goal is to select the best model for use in our Defects4J evaluation, we use the test set from CodRep4Full to select the best run of each model, hence we report the percentage decrease of the best run for a given model from the best result found with the golden model. Due to computational constraints, we only run each model 10 times; for the 18 configurations reported, almost 200 GB of disk storage was used and 400 machine-hours. When using SequenceR to learn new datasets, we would recommend a similar approach where a validation set is used to select the best performing model after multitple training runs.

First, we consider the very coarse grain features. Table 2 shows the performance of four models, starting from a simplistic seq-to-seq model that only takes a single buggy line bl as input when learning to produce the fixed line fl. Then we show beam search, copy, and the use of the abstract buggy context improving the model performance. These results confirm our answer to RQ2 that the copy mechanism is essential to the performance of the system.

TABLE 2 Performance Impact of the Key Features of Beam Size, Copy, and Context

Second, Table 3 shows the results of our ’Golden model’ against the results of single specific, targeted changes made to the model. Ablation ID 1 shows that our 10 K training limit is sufficient given our training data. ID 2 shows that a vocabulary smaller than 1 K tokens performs worse - likely due to a loss of learned tokens that can be used even if an instance of the token is not in the abstract buggy context. ID 3 shows that a vocabulary larger than 1K tokens performs worse - perhaps due to the additional tokens having insufficient training examples for learning a proper embedding. To further understand the effect of vocabulary size, we analyzed the raw output of our model before the patch preparation step. For the golden model (vocab = 1000), 38 percent of the generated patches on CodRep4 have <unk> tokens and would be discarded; with ID 2 (700) it is 43 percent, and with ID 3 (1400) it is 37 percent. Hence, although a larger vocabulary had fewer raw <unk> tokens, the 1000 token vocabulary was able to produce better optimized models.

TABLE 3 Results with Selected Configurations in the Parameter Neighborhood of the Golden Model

ID 4 is about pretraining; in order to provide more opportunities to learn a quality embedding, we created unsupervised pretraining data for the encoder/decoder. Using this unsupervised data did not improve the model, it worsened it.

ID 5 a and b show the value of combining the CodRep and Bugs2Fix data sets to improve the generalization of the model. ID 6 demonstrates the effect of removing the bridge between the encoder and decoder, which improved the mean for the model but tightened the standard deviation and hence produced a lower best result that the golden model. This is perhaps due to the bridge layer allowing for more variation in the encoder hidden state embedding and decoder hidden state embedding.

IDs 7 through 10 demonstrate that our LSTM network is sized correctly; presumably a smaller network cannot generalize on the model data well enough whereas a larger network has too many degrees of freedom. Our speculation is that a 2 layer encoder/decoder network allows the layer connected directly to the token embedding to ’focus’ the weight matrix on input syntax while the layer connected to the attention/copy mechanism ’focuses’ on output generation. ID 11 shows the loss in accuracy when abstract buggy context is reduced to just the buggy line.

ID 12 shows that truncation is necessary otherwise an out-of-memory error crashes the system, due to too many time steps being stored in memory per token in the sequence. ID 13 shows that if we truncated to 4,000 tokens then the system passes, but the increased context size (4,000 vs the golden model 1,000) did not improve accuracy of the model. ID 14 shows that using a 500 token limit for abstract buggy context hurts accuracy presumably because there are less opportunities for token copy. We also speculate that a possible advantage of 1 K truncation instead of 500 could be that 1 K provides a type of unsupervised learning for the encoder hidden states, the global attention, and the copy mechanism.

ID 15 removes the <START_BUG> and <END_BUG> tokens from the abstract buggy context input. The target output is still the correct single-line patch. Without these labels, SequenceR must learn line break positions and learn a type of fault localization in order to create a valid patch. Because abstact buggy context does not include test coverage data or other information useful for fault localization, there is a significant accuracy loss for this ID, but the network was still able to create 356 correct patches.

Our primary use case modeled in this paper is to use our golden model for SequenceR on projects for which it was not trained. This allows for a simpler use model than retraining the model periodically on an ongoing project. ID 16 explores the use case where SequenceR is trained with samples from the same projects that the buggy test cases come from. CodRep4 is added to the training set data and then 4,711 random samples are removed for testing (these samples may be from CodRep or Bugs2Fix project files). When the training data includes bugs from the same projects as the test data, we see a 12 percent improvement in the best model. This use model is viable, but it does require more complete integration of SequenceR into a project regression system.

SECTION 6Related Work
The work presented here is on built on top of two big and active research fields: program repair and machine learning on code. We refer to recent surveys for getting a good overview on them: [1] for program repair and Allamanis et al.’s [8] for the latter. In the following, we focus on those works that are about learning and automatic repair.

sk_p is a program repair technique for syntactic and semantic errors in student programs submitted to MOOCs [43]. First, it uses the previous and next statement to predict the statement in the middle, i.e., to replace the current statement. The probability of a patch is the product of the probabilities for all chosen statements. As we do, sk_p uses beam search to produce the top n predictions.

Another paper on MOOCs [44] repairs student submissions in Python by combining learning and sketch-based synthesis. The approach by Wang et al. [45] considers MOOC but the technique itself is completely different: [45] does deep learning on program traces in order to predict the kind of bug affecting a student submission. The main differences between those works and ours are that 1) we consider a larger context (the buggy class) and 2) we consider real programs for training and testing that are bigger and more complex than student’s submissions. Shin et al. [46] consider simple programs in the educational programming language Karel. As SequenceR, their system predicts to delete, insert or replace tokens. Henkel et al. [47] compute an embedding for symbolic traces and perform a pilot experiment for fixing error-handling code, which is very different from concrete bug fixing as we do here.

DeepFix is a program repair tool for fixing compiler errors in introductory programming courses [27]. The input is the whole program, (100 to 400 tokens long for their data), and the output is a single line fix. The vocabulary size is set to 129, which was enough to map every distinct token type to a unique word in the vocabulary. TRACER is another program repair tool for fixing compiler errors which outperforms DeepFix in terms of success rate [24]. Santos et al.’s [48] further refines the idea and evaluates it with an even larger dataset. The focus of those three works and ours is very different, they focus on compiler errors, we focus on logical bugs. For compiler errors, one does not need to consider the whole vocabulary, but only token types. On the contrary, we have to address this problem and we do so by using the copy mechanism.

DeepRepair [49] is an early attempt to integrate machine learning in a program repair loop. DeepRepair leverages learned code similarities, captured with recursive autoencoders [50], to select repair ingredients from code fragments that are similar to the buggy code. Our usage of learning is different, DeepRepair uses machine learning to select interesting code, SequenceR uses machine learning to generate the actual patch.

Tufano et al. investigated the feasibility of using neural machine translation for learning bug-fixing patches via NMT [10]. The authors first perform a source code abstraction process that relies on a combination of Lexer+Parser which replaces identifiers and literals in the code. The goal of this abstraction is it reduce the vocabulary while keeping the most frequent identifiers/literals. In their work the authors analyzed small methods (no longer than 50 tokens) and medium methods (no longer than 100 tokens) and observed a drop in performance for longer methods. Since their approach takes a buggy method as input and generates the entire fixed method as output, the maximum method length Tufano et al. considered is only 100 tokens. Their work addressed the vocabulary problem by renaming rare identifiers through a custom abstraction process. SequenceR is different in the following ways. First, we consider the entire context of the buggy class, rather than only the buggy method, in order for the model to access more tokens when predicting the fix. Second, our abstraction process uniquely utilizes the copy mechanism (which they do not), which allows SequenceR to utilize a larger set of tokens when generating the fix and to include information about the context within the abstract buggy context in which a token is used. Beyond those two major qualitative differences, a quantitative one is that they only consider small methods, no longer than 100 tokens, while we have no such restriction; SequenceR can potentially generate a one-line patch within a method of any size.

Parallel work by Hata et al. [34] discusses a similar network architecture, also applied to one-line diffs. The major differences between [34] and our work are the following: First, they do project-specific training, which means that their approach is only evaluated on testing data coming from the same project. On the contrary, we do global training and we show that SequenceR captures repair operators applicable to any project. Our qualitative case studies are unique with that respect. Second, they only look at wellformedness of the output, while we also compile and execute the predicted patch. Our work is an end-to-end test-suite based repair approach. Third, their input is limited to the precise buggy code to replace, while SequenceR uses abstract buggy context, which allows for a broader set of tokens for the copy mechanism to select from.

SECTION 7Conclusion
In this paper, we have presented a novel approach to program repair, called SequenceR, based on sequence-to-sequence learning. Our approach uniquely combines an encoder/decoder architecture with the copy mechanism to overcome the problem of large vocabulary in source code. On a testing dataset of 4,711 tasks taken from projects which were not in the training set, SequenceR is able to successfully predict 950 changes. On Defects4J one-line bugs, SequenceR produces 61 plausible, test-suite adequate patches. To our knowledge, our paper is the first ever to show the effectiveness of the copy mechanism for program repair, which provides a mechanism to alleviate the unlimited vocabulary problem.

This work opens promising research directions. First, we aim to improve and adapt SequenceR with the goal of addressing multi-line patches. We believe there are different ways we can tackle this: (i) for fixes modifying contiguous lines of code (i.e., hunk) we can extend SequenceR to learn to generate multiple lines of code as output, with the special tokens (i.e., <START_BUG> and <END_BUG>) surrounding the entire hunk; (ii) for fixes modifying multiple lines in different locations, we could envision SequenceR generating a finite set of combinations of the program containing a predicted fixed line for each of the suspicious locations. Second, there is some preliminary work on tree-to-tree transformation learning [51], which conceptually is very appropriate for code viewed as parse trees. Such techniques may augment or supersede sequence-to-sequence approaches. Finally, the originality of our context abstraction is to capture class-level, long range dependencies: we will study whether such a network architecture is able to capture dependencies beyond that, at the package or application level.