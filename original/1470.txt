Abstract
This paper is the updated version (2013–2020) of the series of papers on emerging themes, top scholars and institutes on software engineering (SE), published by the Journal of Systems and Software for almost 25 years. The paper reports the findings of a bibliometric study by applying the systematic mapping technique on top-quality software engineering venues (handling a dataset of 11.668 studies). The design of the study remains the same for the complete decade, so that the results are consistent and comparable: As the ranking metric for institutions, we used the count of papers in which authors affiliated with the corresponding institute have been identified in the obtained dataset. Regarding scholars we computed the corresponding rankings based on the number of published papers and the average number of citations. In this version, the analysis of emerging trends and themes has been promoted compared to the previous years to provide more insights on what a newcomer in the software engineering domain should look at, as well as to recap the state-of-research in terms of themes to more experienced SE researchers.

Keywords
Bibliometrics
Top-scholars
Top-institutions
Software engineering research areas

1. Introduction
Software Engineering (SE) is the discipline of computer science that studies the complete lifecycle of software development: starting from project inception to software maintenance. The importance of software engineering as a discipline of computer science is emphasized by the fact that: (a) most guides to computer science promote software engineering as a top-level entity in its research,1 and teaching2; and (b) software industry is one of the fastest-growing world-wide3 including a wide range of applications from trivial computer games to safety-critical systems. To gain a better understanding about the research advancements in the field of software engineering, the Journal of Systems and Software (a leading journal in the field) has been periodically publishing a retrospective analysis of the most emerging themes, top scholars and institutions on SE (Glass, 1994). This assessment provides the journal’s audience with different research/technical background an important reference to help them smoothly get involved in the SE research community.

While trying to answer the three unchanged questions: (a) “What are the most emerging themes in SE research?” (b) “Who are the most published scholars in the field of systems and software engineering?” and (c) “Which are the most published institutions?” it is vital to keep the venue and paper screening processes unbiased and evaluate the academic load related to each theme/author/institution objectively (Parnas, 2007). This report is a follow-up of the work of Karanatsiou et al. (2019), which assessed the period starting from the beginning of 2010 until the end of 2017. Retaining the same time-frame, as well as the same study setup, we have updated the study (using a sliding window technique), i.e., reporting for the period 2013–2020. In summary, the contributions of our study are the following:

•
Identify the SE themes that have been studied the most (#papers) in the last 8 years. Among them, we highlight the ones with the highest increase rate of research load (#papers) within the 2013–2020 period;

•
Assess the top SE scholars and institutions (2013–2020) based on a large sample of 11.668 research papers, published in 25 leading conferences and journals during this period. The assessment is performed based on research load and influence analysis. Regarding scholars, the analysis considers the research age of researchers, classifying them into early stage, consolidators, and experienced.

The remainder of this paper is organized as follows: Section 2 presents an overview of the history of this series of studies. Next, in Section 3, we pre-sent the study methodology and research questions, whereas the results are presented in Section 4. Finally, we discuss threats to validity in Section 5, and in Section 6 we conclude the paper and discuss the main findings.

2. The history the series
The series of bibliometric reports on software engineering (SE) started in 1994 by Glass (1994). This series was ongoing and annual between 1994 and 1999; their main goal was to identify top scholars and institutes. For compiling these catalogs, the authors relied on six journals (Information and Software Technology, Journal of Systems and Software, Software Practice and Experience, IEEE Transactions on Software Engineering, ACM Transactions on Software Engineering and Methodology, and IEEE Software). To calculate the score of each scholar, the authors used a weighted count of papers—each paper was weighed, based on the number of authors of the published paper: a single author of a published paper received a score of one, while each author of a multiple-authored paper initially received a score equal to their fractional representation on the paper.4 An author’s raw score (without the transformation) was attributed to the institution he/she belonged to on a paper. The series continued from 1996 to 2011 with some changes in the synthesis of the authors’ group. The last paper of the series has been published by Wong et al. (2011) using the same assessment for a sliding five-year period. At some point, keywords started to be considered in these studies to identify emerging research SE topics. As means of validation, e-mails were sent to each of the top-20 scholars asking them; (a) to check the findings; and (b) to provide a set of keywords, that can describe their research focus within the study period. During this period, the set of considered publication venues has been updated. From 2002–2006, an additional journal (i.e., Empirical Software Engineering) was included to emphasize the importance of having empirical components in SE research.

The study was continued in 2018 by Karanatsiou et al. (2019) through a report summarizing the SE research activity between 2010 and 2017. This version of the series came with some substantial differences, compared to the previous ones: (a) it included both top-journal and conference papers—leading to the exploration of 25 publication venues, (b) it also reported citations and paper counting, acknowledging the influence of the scholars; and (c) it differentiated the reporting based on the research age of researchers (early-stage, consolidated, and experienced researchers)—to provide a fair assessment for younger researchers. On top of that, given the rise of the evidence-based software engineering paradigm (Kitchenham et al., 2010) in the 2010s, the authors have used the mapping study process to systemize the planning, execution, and reporting of that bibliometric analysis. Finally, regarding the reporting of top institutes the scoring system of Karanatsiou et al. (2019) counted full points for each author of the institutes participating in a single paper. We note that for this study, we have switched back to the strategy that was applied in the original series (as performed until 2011).

3. Study design
To assess the most studied research SE themes; the research output and impact of scholars; and the research output of institutions in the SE domain, we have used the mapping study methodology (Petersen et al.) to systematize the design and the reporting of this study. However, this study is not a systematic mapping study, but a bibliometric one. For the parts of the study design that no deviations from Karanatsiou et al. (2019) have been performed, only a brief summary is provided. We note that the key-wording of the abstract step has been used only for most studied themes—nevertheless, applying the step on paper titles instead of abstracts, due to the large volume of data obtained from more than 11,500 studies.

3.1. Objectives and research questions
The goal of this study is to analyze the existing literature on software engineering for the purpose of characterization of themes, scholars and institutions with respect to their research output and impact, from the perspective of software engineering researchers. Based on this goal, we set the following research questions:

: What is the research landscape in SE?

RQ1 relates to the analysis of research themes. The analysis of themes is performed based on the title of the identified papers. The most studied research theme is obtained in terms of published papers on this theme, its emergence is based on the count of papers related to each theme, per year.

: Which are the most active institutions in SE research?

RQ2 relates to the research load (number of papers) produced by institutions. The point system for institutions is related to the number of researchers that are listed as authors of each paper (Wong et al., 2011).

: What is the ranking of SE scholar?

Finally, RQ3 focuses on individual scholars. When examining Karanatsiou et al. (2019):

•
Total number of papers. This is an indicator of the overall work of the researcher between 2013 and 2020.

•
Number of papers published only in journals. The motivation for this choice is the need to compare the results of this study to the previous ones, in which only journals were considered. For this reason, we only considered the venues analyzed by Wong et al. (2011).

•
Impact of their research. For the impact of an article, we use the average number of citations per year as the evaluation criterion. To evaluate the impact of a scholar’s research, we use the average impact score of his/her publications. The decision to normalize citations per year, per article is to avoid any bias from article age and the total number of articles published by the scholars. This indicator expresses how frequently other scholars use the results presented in an article.

Moreover, we retain the decision of Karanatsiou et al. (2019) to report on top-scholars, based on their research age: early stage (up to 7 years of research by the end of 20165— first peer-reviewed papers between 2010 and 2020), consolidators (8–12 years of research by the end of 2016 — first peer-reviewed papers between 2005 and 2009), and experienced (more than 12 years of research by the end of 2016 — first peer-reviewed before 2004). This classification of researchers, is based on the EU classification of researchers in the European Research Council (ERC) Grants.6


Table 1. Selected publication venues.

Journals	Conferences
Information and Software Technology (IST)	International Conference on Software Architecture (ICSA)a
Journal of Systems and Software (JSS)	Automated Software Engineering Conference (ASE)
IEEE Software (SW)	European Symposium on Programming (ESOP)
IEEE Transactions on Software Engineering (TSE)	International Conference on Software Engineering (ICSE)
Software: Practice and Experience (SPE)	Working Conference on Reverse Engineering (SANER)b
Software Testing, Verification and Reliability (STVR)	International Conference on extreme Programming (XP)c
Transactions on Programming Languages and Systems (TOPLAS)	Symposium on Principles of Programming Languages (POPL)
Transactions on Software Engineering and Methodology (TOSEM)	International Symposium on Software Testing and Analysis (ISTTA)
Journal of Software: Evolution and Process (JSEP)d	International Symposium on Code Generation & Optimization (CGO)
International Journal on Software Tools for Technology Transfer (STTT)	International Conference on Evaluation and Assessment in Software Engineering (EASE)
Empirical Software Engineering (EMSE)	International Symposium on the Foundations of Software Engineering (FSE)
International Symposium on Empirical Software Engineering and Measurement (ESEM)
International Conference on Software Maintenance and Evolution (ICSME)
Fundamental Approaches to Software Engineering (FASE)
a
Conference on the Quality of Software Architectures (QoSA) - IEEE/IFIP Working Conference on Software Architecture (WICSA) - International Symposium Component-Based Software Engineering (CBSE).

b
European Conference on Software Maintenance and Reengineering (CSMR) jointed with Working Conference on Reverse Engineering (WCRE).

c
Conference on Agile Software Development (AGILE).

d
Journal of Software Maintenance and Evolution (JSME).

3.2. Search process
We reuse the search strategy of Karanatsiou et al. (2019), i.e., the latest version of this series of studies to ensure the continuity of the series of papers, also considering that from the publication of that work and now, no substantial changes in the landscape of SE venues has been performed. From the next version of this series, the search process will be re-applied to safeguard an up-to-date evaluation of venues’ quality. The only change to the set of selected venues is the exclusion of Journal of Software (JSW)–published by IAP (International Academy Publishing)– because it is not archived in Scopus from 2014 and on; thus, data collection for this venue was not possible. The final list of publication venues used in this study is presented in Table 1. As a candidate primary studies set, we retained all articles published in these venues, between 2013 and 2020 (including first and final year). The identification of candidate primary studies has been performed from DBLP, using the export in XML functionality. To double-check the process, we have exported all articles published in the selected venues, based on Scopus (using the export to CSV functionality). From Scopus, we have retained information on the type of article: research, editorial, review, etc. that is used in the next step of the process.

3.3. Article filtering phases
As part of article filtering a two-step process has been followed: First, based on Scopus, we have been able to filter-out editorials, position papers, keynotes, opinion papers, tutorials, posters, panels, tool demos, etc. We note that the relevance of the candidate primary studies to SE has been safeguarded, from the fact that identified venues publish only SE papers. Apart from the aforementioned automated process, the filtering has also been manually checked by one of the authors. In the second step, we have emailed 277 top-scholars, who have been ranked in the top-50 lists of all categories and asked them to validate their data. The scholars have been provided with the list of papers that we have identified them to have authored in each venue. Out of the 277 researchers reached, 140 responded and all corrections have been evaluated by the authors of this study. Based on the input we had received, it turned out that we needed to manually check the papers published in IEEE Software (for editorial or conference summaries), as well as papers published in Automated Software Engineering and Foundations of Software Engineering conferences for 2020, which had not been correctly retrieved from DBLP and Scopus, at the point of data collection.

3.4. Data collection & analysis
During the data collection phase, we collected a set of variables that describe each primary study. The data extraction was fully automated from Scopus and DBLP, so no subjectivity was involved. For every study, we extracted and assigned values to the following variables:

[V1]
Author: Records the list of authors of the paper.

[V2]
Institution: Records the list of institutions of the paper

[V3]
Title: Records the title of the paper.

[V4]
Month/Year: Records the publication date of the paper (available online).

[V5]
Publication Venue: Records the name of the corresponding journal or conference.

[V6]
Number of Citations in Scopus on December 2020.

Given the scores of these variables, we calculated some general indices for each paper:

[V7]
Age of the paper in years (two decimal digits): CURRENT_DATE – [V4].

[V8]
Paper Impact [V6] / [V7]: Average annual number of citations.

Subsequently, for each scholar, we record three variables: (a) count of papers in which the author is involved, (b) average impact of papers (i.e., average [V8] for the papers in which the author is involved), and (c) seniority level (i.e., early stage, consolidator, or experienced) based on the year of the first paper published in DBLP.

The analysis on the most studied and emerging SE research themes was performed based on the titles [V3] of the studies. In this regard, each title was subjected to necessary pre-processing and cleaning procedures (e.g., transformation to lowercase, removal of punctuation marks, special characters, stop-words and whitespaces, application of tokenization and stemming processes on textual data). In the next step, each title was transformed into a set of -grams defined as a continuous sequence of  terms with a varying number of  (). Based on the frequency distribution of the extracted -grams, the most popular SE research terms were identified, whereas a synthesis process was followed in order to concatenate synonyms and closely related terms. For example, “code”, “sourc” and “sourc cod” have been merged into the term “code”, so as to be understandable. At a second level, to bring the analysis to a more coarse-grained level, we have mapped each term to orthogonal categories of a taxonomy of SE themes developed for this study. The taxonomy of identified SE themes encompasses six broad categories, namely: (a) development activity/artifact; (b) practice/concept; (c) context; (d) quality property; (e) research method; and (f) analysis method. The first theme is linked to classic software lifecycle models, such as RUP, Waterfall, Agile, etc., that organize software development into activities, that produce artifacts. The two are not separated since the discriminating line between them is thin in many cases: e.g., the term “design” can refer to both the artifact and the activity. Along these activities certain practices are applied (2nd theme). These categories of the taxonomy have been used in various secondary studies (Heaton and Carver, 2015, Behutiye et al., 2020, Paternoster et al., 2014, Barricelli et al., 2019). The 3rd category has been derived due to the importance of the context in software engineering research (Petersen and Wohlin); the 4th one is because quality properties are important drivers for software development (Khalifa and Verner, 2000). The 5th category is also a very common study categorization parameter (Charalampidou et al., 2020, Bischoff et al., 2019, Molléri et al., 2019) since it can lead to terms for which empirical evidence are not sufficient; an aspect that is considered important in SE rigor and relevance (Ivarsson and Gorschek, 2011). Finally, the 6th category focuses on a specific step of research methodology, i.e., data analysis. Through this category, we explore the extent to which modern analysis methods (such as deep learning, big data) have been employed in SE research or if more standard approaches (such as statistics) are still in use. The mapping of terms to themes has been performed individually by two researchers, and no conflicts have been identified in the classification of the top-100 terms. To assess the popularity of the terms belonging to each one of the above SE themes within the examined period, we have (a) graphically explored the association through bubble charts; (b) conducted the chi-square test of independence; and (c) performed correspondence analysis (Greenacre, 2007) to gain insight regarding the association between specific terms and years of publication.


Table 2. Top fifty n-grams and their related themes in Software Engineering Research.

Term	Theme	#papers	Term	Theme	#papers
test	artifact	896	literature review	research method	179
code (incl. implementation)	artifact	856	java	practice/concept	176
Bug (incl. fault, defect)	artifact	699	compon	practice/concept	172
empir	research method	574	evolution	quality property	167
design/architecture	artifact	446	configur	practice/concept	161
agil	practice/concept	326	relation	analysis method	160
case stud	research method	315	trace	practice/concept	158
servic	context	312	verificat	quality property	149
industr	context	294	continu	practice/concept	147
android	context	287	team	artifact	142
web	context	277	secure	quality property	141
tool	practice/concept	270	survey	research method	138
pattern	practice/concept	266	measur	practice/concept	138
cloud	context	265	symbol	artifact	137
languag	practice/concept	255	effort	practice/concept	137
open	context	220	repair	practice/concept	125
requirements	artifact	218	product line	practice/concept	124
Metric (incl. measurement)	quality property	203	formal	practice/concept	124
mobil	context	201	simul	research method	119
experim	research method	194	model check	practice/concept	119
api	practice/concept	189	cost	artifact	116
refactor	practice/concept	189	theor	research method	116
change	quality property	186	systematic map	research method	114
performance	quality property	186	parallel	context	111
distribut	context	183	classif	analysis method	108
4. Results
Trends in SE Research Following the process described in Section 3.4, a total set of 84 concatenated terms was considered for extracting emerging SE themes. The top-50 terms based on their frequency distribution are presented in Table 2. At this point, we have to note that a paper may be mapped to more than a single SE term, while other papers may not be classified at all. Overall, a total number of 8.452 (72%) papers were classified into at least one SE theme based on the taxonomy of the extracted terms.

To gain an insight regarding the evolution of SE themes during the examined period, the year-wise distribution of papers across the six themes is presented in Fig. 1. The results of the chi-square test of independence revealed a statistically significant association between the SE terms organized by SE themes and the Year of publication.7 In this regard, Correspondence Analysis (CA), a multivariate ordination method, was used to identify trends in SE research terms over the examined period. To retain the length of this work in the usual size, the correspondence analysis matrices are presented online in the supplemental material, as well as the bubble charts in their original size—see Appendix B.

For illustration purposes, we indicatively present the findings of CA for terms related to the analysis method SE theme. The reason for this choice is due to the fact that the first two dimensions of CA account for roughly 88.59% (Fig. 2) of the total variability; this is the best dimension reduction achieved in the set of the conducted experiments. To interpret the bi-plot, the following rules of thumb can be used: points represent either SE terms (rows in bubble charts) or years (columns in bubble charts); dots that are close to one another represent similar profiles, and points that are far from the origin demonstrate discriminating profiles. In contrast, the distance between any terms and years cannot be straightforwardly interpreted but rather, one should draw a line connecting a specific term and year with the origin and inspect the formatted angle. Generally, a small angle is an indication of a strong association between specific pairs of SE terms and years.


Download : Download high-res image (2MB)
Download : Download full-size image
Fig. 1. Evolution of SE research terms over the examined period.

Based on the previous considerations, the years 2020, 2019 and 2018, and the SE terms “deep learn”, “neural”, “fuzz” and “machine learning” are mostly represented by the first dimension of the CA solution, whereas negatively correlated SE terms or years are represented on opposite sides of the plot. Concerning the association between SE terms and years of publications, the exploration of the bubble chart and CA bi-plot indicates that there is an increasing trend in the utilization of deep learning, neural network and machine learning approaches mostly in 2020. An overview of the result analysis is presented in Section 6.


Download : Download high-res image (185KB)
Download : Download full-size image
Fig. 2. Evolution of analysis method SE over the examined period.

Top Institutions in Software Engineering Research: In Table 3, Table 4, we present the top-50 institutions, based on the number of papers that involve authors affiliating the specific organizations. In these tables, we are not reporting on individual department or faculty level, but on organization/institution level. In contrast to previous studies, we present the top-50 institutions. This decision is taken to present a similar number of institutes and scholars (we present three ranks of top-20 scholars). Table 3 lists the institutions using the complete dataset, whereas Table 4 only considers the publication venues used in previous versions of this series (i.e., EMSE, IST, JSS, JSEP, SPE, STTT, STVR, SW, TOSEM, TOPLAS and TSE). The comparison of results is performed in Section 6, in which we cumulatively discuss all the findings of this study.

Top Scholars in Software Engineering Research: In this section, we present the results regarding the top scholars in software engineering. In Table 5, Table 6, Table 7, we present the top-20 experienced, consolidated, and early-stage researchers, ranked by the total number of papers.


Table 3. Most active institutions in software engineering research (all publication venues).

Rank	Name	Country	#score	Rank	Name	Country	#score
1	University of California	United States	99,076	26	Google	United States	25,367
2	Carnegie Mellon University	United States	62,279	27	Oregon State University	United States	24,892
3	Nanjing University	China	60,776	28	IBM	United States	24,292
4	Microsoft Research	United States	59,055	29	University of Texas at Austin	United States	24,200
5	Singapore Management University	Singapore	58,083	30	Purdue University	United States	23,400
6	Queen’s University Kingston	Canada	57,283	31	University of Saskatchewan	Canada	23,033
7	Blekinge Institute of Technology	Sweden	53,516	32	University of Edinburgh	United Kingdom	22,876
8	University of Illinois	United States	47,376	33	Technical University of Munich	Germany	22,667
9	Delft University of Technology	Netherlands	44,167	34	Eindhoven University of Tech.	Netherlands	22,200
10	University of Luxembourg	Luxembourg	41,900	35	University of Sannio	Italy	22,059
11	North Carolina State University	United States	40,950	36	University of Oulu	Finland	21,775
12	Imperial College London	United Kingdom	40,026	37	Iowa State University	United States	21,750
13	University College London	United Kingdom	37,930	38	University of Passau	Germany	21,534
14	University of Waterloo	Canada	37,020	39	University of Oxford	United Kingdom	21,408
15	Nanyang Technological University	Singapore	36,995	40	Indian Institute of Science	India	20,833
16	National University of Singapore	Singapore	36,866	41	College of William and Mary	United States	20,281
17	Chinese Academy of Sciences	China	36,283	42	Rochester Institute of Technology	United States	20,266
18	University of British Columbia	Canada	36,000	43	University of Stuttgart	Germany	19,534
19	Concordia University	Canada	34,583	44	University of Texas at Dallas	United States	19,200
20	University of Southern California	United States	33,416	45	Tel Aviv University	Israel	18,450
21	University of Zurich	Switzerland	31,459	46	INRIA	France	17,928
22	Hong Kong Univ. of Science and Tech.	China	29,167	47	University of Cambridge	United Kingdom	17,917
23	ETH Zurich	Switzerland	28,950	48	Fudan University	China	17,050
24	TU Darmstadt	Germany	25,367	49	Zhejiang University	China	16,766
25	University of Alberta	Canada	25,366	50	University of Adelaide	Australia	16,635

Table 4. Most active institutions in software engineering research (journals only).

Rank	Name	Country	#score	Rank	Name	Country	#score
1	Queen’s University Kingston	Canada	42,250	26	George Mason University	United States	10,250
2	Blekinge Institute of Technology	Sweden	34,991	27	McGill University	Canada	10,117
3	Nanjing University	China	25,350	28	DePaul University	United States	10,083
4	Concordia University	Canada	21,583	29	Nimble Research	United States	10,000
5	North Carolina State University	United States	21,150	30	University of Zurich	Switzerland	9892
6	IBM Research	United States	19,833	31	Aalto University	Finland	9417
7	University of Oulu	Finland	18,250	32	University of Groningen	Netherlands	9283
8	University College London	United Kingdom	18,028	33	King Fahd Univ. Petr. & Minerals	Saudi Arabia	8912
9	Simula Research Laboratory	Norway	15,483	34	Microsoft Research	United States	8861
10	Chalmers University	Sweden	15,417	35	Johannes Kepler University	Austria	8333
11	University of Alberta	Canada	15,416	36	VU University	Netherlands	8283
12	University of Victoria	Canada	15,043	37	Fondazione Bruno Kessler	Italy	8167
13	Delft University of Technology	Netherlands	14,767	38	Beihang University	China	7917
14	University of California	United States	14,750	39	Lund University	Sweden	7867
15	Singapore Management University	Singapore	14,533	40	Nanyang Technological University	Singapore	7667
16	University of Waterloo	Canada	14,210	41	Universidad Politécnica de Madrid	Spain	7333
17	Aristotle University of Thessaloniki	Greece	14,083	–	University of Sannio	Italy	7333
18	University of Hong Kong	China	13,728	–	Zhejiang University	China	7333
19	University of Luxembourg	Luxembourg	13,200	44	Eindhoven Univ. of Technology	Netherlands	7283
20	University of Melbourne	Australia	12,590	45	Mälardalen University	Sweden	7283
21	Sharif University of Technology	Iran	12,450	46	University of Macedonia	Greece	7167
22	Brunel University	United Kingdom	12,200	47	University of York	United Kingdom	6917
23	Chinese Academy of Sciences	China	12,000	48	University of Notre Dame	United States	6803
24	Carnegie Mellon University	United States	11,567	49	University of Stuttgart	Germany	6459
25	University of Alabama	United States	11,475	50	University of Maryland	United States	6416
In Table 8, we present the ranking considering only the journal venues used in previous versions of this series. Finally, in Table 9, we present the most impactful SE researchers in terms of number of citations per article, per month.


Table 5. Most active experienced researchers.

Rank	Name	# papers	ASE	CGO	EASE	EMSE	ESEM	ESOP	FASE	FSE	ICSA	ICSE	ICSME	ISSTA	IST	JSEP	JSS	POPL	SANER	SPE	STTT	STVR	SW	TOPLAS	TOSEM	TSE	XP
1	Ahmed E. Hassan	118	2			51	2			4		15	8		2	2	2		4				9		1	16	
2	Massimiliano Di Penta	88	5			13				9		14	14		3	4	5		9			1			3	8	
3	Lionel C. Briand	73	11			6	2			7		5		6	7		4		2			3	2		11	7	
4	Rocco Oliveto	68	3			9				4		10	6	2	3	6	3		4			1			5	12	
5	Zhenchang Xing	66	13			8	1			7		11	7			2	1		11						3	2	
6	Mark Harman	64	4			2	2			9		11		7	6		5		1			1		1	4	11	
7	Andrea De Lucia	54	1			7						7	6	2	4	4	6		5			1			3	8	
8	Paris Avgeriou	48			2		3				5		1		15	3	10		1	1			3		1	3	
9	Sven Apel	47	10			11	1		1	7		8			2								1		2	4	
–	Jun Sun	47	17						2	3		8	1	6					1	1	1				1	6	
–	Hongyu Zhang	47	7			2	1			12		12	4	2		1	1	1	2							2	
–	Tim Menzies	47	4			8	1			7		6			6	1	1						2		1	10	
13	Yann-Gaël Guéhéneuc	44				11	1							2	5	5	4		10	1						5	
–	Baowen Xu	44	4			2	3			5		6	1		9		3		2						4	5	
–	Jan Bosch	44				1	1			1	5	1	2		4	8	8						6				7
16	Shing-Chi Cheung	43	7			2				8		11		3	1		3	1					1		1	5	
17	Yves Le Traon	42	1			5	1			3		8	4	5	6	1	2	1	1			1				3	
18	Andy Zaidman	40	1			7				1		3	6	1	1	2	6		6			2				4	
19	Giuliano Antoniol	39				9						1	2		2	5	2		14							4	
–	Gordon Fraser	39	6			4				6		2	1	8	1		1					5			2	3	
–	Michael Felderer	39			8	2	2								7		1				6	1	10		1		1
–	Xiangyu Zhang	39	5	1						11		13		5			1						1		1	1	

Table 6. Most active consolidated researchers.

Rank	Name	# papers	ASE	CGO	EASE	EMSE	ESEM	ESOP	FASE	FSE	ICSA	ICSE	ICSME	ISSTA	IST	JSEP	JSS	POPL	SANER	SPE	STTT	STVR	SW	TOPLAS	TOSEM	TSE	XP
1	David Lo	139	15		5	24	5			9		7	15	3	7	4	3		25	1					6	10	
2	Yang Liu	71	19			2				11	2	16	2	9					1		1				1	7	
3	Denys Poshyvanyk	62	8			6				6		9	10	2		5	2		3						4	7	
4	Foutse Khomh	58	2			12	4					1	11		3	2	7		13				2			1	
5	Bram Adams	56				15	3			1		5	5		2	2			8			1	11			3	
6	Chanchal Kumar Roy	54	6			5				1	1	2	16		1	2	5		15								
7	Vahid Garousi	46			7	2	2								14	3	8						10				
8	Christian Kästner	44	4			5				13		8	1		1	1			1	1			2		1	6	
9	Kai Petersen	41			4	2	3					1	1		12	7	7			1					1		2
10	Lingming Zhang	38	7					1		5		7		11		1	1	1							2	2	
11	Christian Bird	36	1			2	3			7		12		1									4			6	
–	Emad Shihab	36				11	3			2		2	4		4		2		3				2			3	
–	Weiyi Shang	36	2			15				2		6	3		1	1	1		1				2			2	
–	Tien N. Nguyen	36	7							8		11	6						1						1	2	
15	Annibale Panichella	34	3			2				1		5	2	4	3	1			4			2				7	
–	Christoph Treude	34	2			7	2			4		5	6				3		2				1			2	
17	Ali Mesbah	33	7				3			7		7	3	1	1							1			1	2	
–	Andrea Arcuri	33	4			8				1		1		2	2		2					5			5	3	
19	Meiyappan Nagappan	32				6	1			3		2	7				4						7			2	
–	Mario Linares Vásquez	32	4			6				4		4	8	1		1	2								1	1	

Table 7. Most active early-stage researchers.

Rank	Name	# papers	ASE	CGO	EASE	EMSE	ESEM	ESOP	FASE	FSE	ICSA	ICSE	ICSME	ISSTA	IST	JSEP	JSS	POPL	SANER	SPE	STTT	STVR	SW	TOPLAS	TOSEM	TSE	XP
1	Bavota Gabriele	83	4			12				7		18	14		2	3	5		3				1		6	8	
2	Xin Xia	77	12		1	15	2			3		6	7	1	6	2	2		7						5	8	
3	Fabio Palomba	45	2			5				1		4	10	1	4	1	8		3				1			5	
4	Tegawendé Bissyandé	39	5			5				3		5	4	6	3	1	1		5	1							
5	Shane McIntosh	35	2			7	2			2		5	5						5							7	
6	Li Li	29	7			1				3		2	3	4	2	1	1	1	2				1		1		
7	Gustavo Pinto	27	1		3	1	7		1			2	2		1		4		3	1			1				
–	Daniel M. Fernández	27	1		3	2	7					2			3	2	3						3		1		
9	Yepang Liu	24	7			1				3		5		2			1		2				1		1	1	
10	Bogdan Vasilescu	23	3			3				8	1	3	1		1	1			2								
11	Davide Fucci	22			2	3	9			1		1	1		2											2	1
12	Leandro L. Minku	21			1	2	2			1		2			1		2						6		2	2	
–	Ferdian Thung	21	2		1	5				1		1	4			1			6								
14	Yan Cai	19	3							6		4					1		1	1						3	
–	Lingfeng Bao	19	2			4				3		2	2						3						1	2	
–	Chunyang Chen	19	5			2	1			1		6	1						2						1		
17	Ali Ouni	17	1			2	1						1		3	1	3		1						2	2	
–	Junjie Chen	17	6							5		3	1	1											1		
–	Jeff Huang	17	1							6		5		1										1	1	2	
–	Mauricio F. Aniche	17	1			3				3		4	3				1		1							1	
–	Haipeng Cai	17	2							1			1	1	2		2		4						3	1	

Table 8. Most active SE researchers in top-quality journal.

Rank	Experienced	Rank	Consolidators	Rank	Early-Stage
Name	# papers		Name	# papers		Name	# papers
1	Ahmed E. Hassan	81	1	David Lo	51	1	Bavota Gabriele	37
2	Lionel C. Briand	37	2	Vahid Garousi	35	2	Xin Xia	36
3	Paris Avgeriou	33	3	Bram Adams	31	3	Fabio Palomba	23
4	Massimiliano Di Penta	32	4	Foutse Khomh	25	4	Leandro L. Minku	15
–	Rocco Oliveto	32	5	Kai Petersen	23	5	Shane McIntosh	14
6	Tony Gorschek	30	6	Emad Shihab	22	6	Michael Unterkalmsteiner	13
7	Mark Harman	28	7	Cor-Paul Bezemer	21	7	Ali Ouni	12
–	Andrea De Lucia	28	–	Weiyi Shang	21	–	Daniel M. Fernández	12
9	Tim Menzies	27	9	Andrea Arcuri	20	9	Tegawendé F. Bissyandé	10
–	Rafael Prikladnicki	27	10	Meiyappan Nagappan	19	10	Simone Romano	9
–	Rajkumar Buyya	27	–	Denys Poshyvanyk	19	–	Kelly Blincoe	9
–	Natalia Juristo Juzgado	27	–	Burak Turhan	19	12	Tse-Hsun Chen	8
–	Jan Bosch	27	13	Tao Yue	18	–	Feng Zhang	8
14	Yann-Gaël Guéhéneuc	26	–	Shaukat Ali	18	–	Shuai Wang	8
–	Gerard J. Holzmann	26	–	Marouane Kessentini	18	–	Mohamed Wiem Mkaouer	8
16	Daniel M. Germán	23	16	Christian Kästner	16	–	Gustavo Pinto	8
–	Jane Cleland-Huang	23	–	Krzysztof Wnuk	16	–	Maleknaz Nayebi	8
–	Baowen Xu	23	18	Apostolos Ampatzoglou	15	–	Haipeng Cai	8
–	Eduardo S. de Almeida	23	19	Christoph Treude	13	19	Lingfeng Bao	7
–	Markku Oivo	23	–	Shin Yoo	13	–	Matias Martinez	7
–	Chanchal Kumar Roy	13	–	Nauman Bin Ali	7
–	Daniel Graziotin	7
–	Fabian Fagerholm	7
–	Antonio Martini	7
–	Ivan do Carmo Machado	7
–	Paulo Silveira Neto	7
–	Valentina Lenarduzzi	7
–	Davide Fucci	7
–	Jin Liu	7
–	Dario Di Nucci	7
–	Kwabena Ebo Bennin	7
–	Yibiao Yang	7
–	Sarah Gregory	7

Table 9. Most impactful SE researchers.

Rank	Experienced	Rank	Consolidators	Rank	Early-Stage
Name			Name			Name	
1	Tsong Yueh Chen	10.385	1	Baishakhi Ray	11.500	1	Chakkrit Tantithamthavorn	10.363
2	Rajkumar Buyya	10.333	2	Klaas-Jan Stol	11.461	2	Matias Martinez	8.818
3	Brian Fitzgerald	10.286	3	Yue Jia	11.333	3	Michele Tufano	8.666
4	Cesare Pautasso	9.800	4	Xiwei Xu	10.846	4	Fabio Palomba	7.289
5	Sunghun Kim	9.640	5	Rui Abreu	8.000	5	Li Li	6.720
6	Reid Holmes	8.214	6	Lin Tan	7.250	6	Shane McIntosh	6.686
7	Mark Harman	8.061	7	Denys Poshyvanyk	6.710	7	Gabriele Bavota	6.183
8	Earl T. Barr	7.954	8	Andrea Arcuri	6.636	8	Carlos E. Bernal-Cárdenas	5.692
9	Franz Wotawa	7.700	9	Jacques Klein	6.241	9	Song Wang	5.642
10	Premkumar T. Devanbu	7.692	10	Rodrigo O. Spínola	6.231	10	Michael Unterkalmsteiner	5.187
11	Andrea De Lucia	7.537	11	Christian Bird	6.222	11	Ali Ouni	5.118
12	James D. Herbsleb	7.363	12	Georgios Gousios	6.200	12	Tegawendé F. Bissyandé	5.108
13	Rocco Oliveto	7.348	13	Dongsun Kim	6.133	13	Christopher Vendome	5.000
14	Phil McMinn	7.055	14	Shin Yoo	6.050	14	Brittany Johnson	4.900
15	Xiao-Yuan Jing	7.000	15	Jifeng Xuan	6.000	15	Anh Tuan Nguyen	4.812
16	Barbara A. Kitchenham	6.923	16	Alberto Bacchelli	5.724	16	Antonio Filieri	4.538
17	Gordon Fraser	6.769	17	Mike Papadakis	5.689	17	Feng Zhang	4.461
18	Len Bass	6.700	18	William G. J. Halfond	5.619	18	Bogdan Vasilescu	4.434
19	Abhik Roychoudhury	6.567	19	Sebastiano Panichella	5.571	19	Mohamed Wiem Mkaouer	4.363
20	Yves Le Traon	6.405	20	Emad Shihab	5.515	20	Pavneet Singh Kochhar	4.181
5. Threats to validity
In this section, we discuss the threats to the validity of this work. Regarding the construction of the dataset, the results are threatened by the selection of venues: a different set of venues could possibly lead to different results. Nevertheless, the set of selected venues has been obtained through a rigorous process, is intuitive, and is not in favor of specific communities. Additionally, the used metrics are ad/hoc; however, the counting of papers, and the number of citations are the most known metrics for bibliometric assessments. We note that especially regarding research impact the results should be treated with caution, since the reflection of research impact is an extremely difficult task that is connected to the subfield of research. For example, if a researcher opens up a completely new line of research, he/she may not receive a lot of citations initially, but might get many over time. Also, the way of citing a paper can be different, ranging from a simple reference to the actual use of the proposed method or tool. Although the latter is more important, such a distinction cannot be performed in this bibliometric study. Furthermore, rewarding reproducibility, the whole process is completely replicable, in the sense that all data are freely available8 and no subjectivity is introduced for answering the research questions, since the aggregation is purely quantitative. The automated analysis has inserted a threat to the validity of the results regarding organizations, e.g., University of California that hold many campuses is counted as one organization. Such a decision might be unfair for single campus universities; however, the automated analysis performed in this study was unable to comprehensively handle such cases. The final threat to validity is related to possible errors that might have occurred during data collection. To mitigate this threat, as much as possible, a systematic validation process was conducted, by contacting 275 SE scholars by email (as described in Section 3).

6. Conclusions
This study is a follow-up of the bibliometric series of papers on SE research, published in the Journal of Systems and Software for more than two decades. The study has been designed based on the footsteps of the previous studies, without any deviation from the 2010–2017 study. The main findings of this study can be summarized as follows:

•
Sensitivity of results to venues. The ranking of researchers is quite similar regardless of the number and type of publication venues being considered. In particular, 36 (out of 63) researchers are ranked as top in their categories, based on all selected publication venues, exist in the listing of top scholars by considering the top-7 journals. This result suggests that: (a) the ranking is not extremely sensitive to the selected venues; and (b) that top-scholars are not substantially differentiating between journal and conference as venues for publishing their research.

•
SE themes and yearly distribution of papers. A few interesting conclusions derived from the aggregated results of the bubble charts exploration along with the conduction of the chi-square test of independence and CA biplots are summarized as follows: The terms “vulner”, “bug”, “librar” (belonging to the development activity/artifact SE theme), “api” and “debt” (belonging to the practice/concept SE theme), “smart” (belonging to the context SE theme), “secure” and “safety” (belonging to the quality property SE theme), “deep learn”, “neural”, “fuzz” and “machine learn” (belonging to the analysis method SE theme) are mostly associated to 2020 based on the terms extracted from the titles of the examined papers. The analysis also indicates other associations between specific pairs of SE terms and years, i.e. “android” and 2018, “case study” and 2014, “web” and 2013, “cloud” and 2017 and 2018 etc.

•
Comparison against past studies. By comparing the set of researchers that are presented to the previous one (i.e., covering the complete 2010–2020 decade) we highlight that 48% of top-scholars (59 out of 122) are the same. Among them, 28 researchers have remained in the list (but changed a seniority level), and 31 remained as top-scholars in the same research age category. Regarding research organizations only 2 out of 15 that existed from 2010 to 2017, are not presented in the top-institutions list for the 2013–2020 period; suggesting a research continuity within the same decade. Finally, similarly to the previous study, only 21 (out of 101) researchers that are ranked as top in their categories in terms of activity are ranked as highly cited ones