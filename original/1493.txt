Abstract
Program semantic properties such as class names, method names, and variable names and types play an important role in software development and maintenance. Method names are of particular importance because they provide the cornerstone of abstraction for developers to communicate with each other for various purposes (e.g., code review and program comprehension). Existing method name prediction approaches often represent code as lexical tokens or syntactical AST (abstract syntax tree) paths, making them difficult to learn code semantics and hindering their effectiveness in predicting method names. Initial attempts have been made to represent code as execution traces to capture code semantics, but suffer scalability in collecting execution traces.

In this paper, we propose a hybrid code representation learning approach, named Meth2Seq, to encode a method as a sequence of distributed vectors. Meth2Seq represents a method as (1) a bag of paths on the program dependence graph, (2) a sequence of typed intermediate representation statements and (3) a sentence of natural language comment, to scalably capture code semantics. The learned sequence of vectors of a method is fed to a decoder model to predict method names. Our evaluation with a dataset of 280.5K methods in 67 Java projects has demonstrated that Meth2Seq outperforms the two state-of-the-art code representation learning approaches in F1-score by 92.6% and 36.6%, while also outperforming two state-of-the-art method name prediction approaches in F1-score by 85.6% and 178.1%.

Previous
Next 
Keywords
Code representation learning

Method name prediction

Deep learning

1. Introduction
Representation learning (Bengio et al., 2013) transfers raw data into low dimensional vectors (i.e., embeddings) that preserve the properties of raw data and can be fed into downstream deep learning models. It allows the model to learn how to extract the features and how to use them in specific tasks. For example, word embedding (Mikolov et al., 2013b, Mikolov et al., 2013a, Pennington et al., 2014) and paragraph embedding (Le and Mikolov, 2014) are the two most widely adopted representation learning approaches in natural language processing (NLP), and significantly enable the recent success and advance in NLP tasks. Basically, they learn continuous fixed-length vector representations of words, sentences, paragraphs and documents from a text corpus so that the vectors of semantically similar words, sentences, paragraphs and documents are close to each other in the vector space.

With the availability of a massive collection of code corpus from open-source repository hosting platforms (e.g., GitHub), leveraging machine learning models for various programming language and software engineering tasks has recently become a trending topic of much interest (Yahav, 2015, Bielik et al., 2015, Ernst, 2017, Allamanis et al., 2018b). One of the programming comprehension tasks is to predict program semantic properties such as class names, method names, variable names and variable types. This task is very important because such semantic properties or labels are common and important during software development and maintenance. On one hand, developers must select meaningful semantic names for every class, method, variable and parameter they declare in the program. On the other hand, developers often rely on names to understand behaviors of program or API elements during code review or maintenance. However, predicting program semantic properties is a non-trivial task; and poor semantic labels hinder code comprehension and maintenance (Arnaoudova et al., 2016, Lawrie et al., 2006b, Liblit et al., 2006, Takang et al., 1996, Binkley et al., 2013, Butler et al., 2011), or even lead to program bugs (Butler et al., 2009, Abebe et al., 2012, Butler et al., 2010).

Existing machine learning-based approaches for predicting program semantic properties (Allamanis and Sutton, 2013, Allamanis et al., 2015a, Allamanis et al., 2016, Liu et al., 2019, Cvitkovic et al., 2019, Allamanis et al., 2018, Alon et al., 2018, Alon et al., 2019b, Alon et al., 2019a, Wang and Su, 2019, Bavishi et al., 2018, Hellendoorn et al., 2018, Malik et al., 2019) can be categorized into three groups according to how they represent the code, i.e., token-based (Allamanis and Sutton, 2013, Allamanis et al., 2015a, Allamanis et al., 2016, Liu et al., 2019, Bavishi et al., 2018, Hellendoorn et al., 2018, Malik et al., 2019), abstract syntax tree (AST)-based (Cvitkovic et al., 2019, Allamanis et al., 2018, Alon et al., 2018, Alon et al., 2019b, Alon et al., 2019a), and execution-based (Wang and Su, 2019). Token-based approaches represent the code as a sequence of tokens at the lexical level, and apply -gram models or deep neural network models to predict program properties. AST-based approaches represent the code as a sequence of AST nodes or paths at the syntax level. However, these two types of approaches characterize the code syntax, but it is difficult for them to learn code semantics. To better learn code semantics, Wang and Su (2019) proposed an execution-based method to learn code representations from symbolic and concrete execution traces at the semantics level. While achieving promising accuracy in predicting program properties, this method may suffer scalability issues in collecting execution traces. In summary, the key challenge for learning-based program semantic property prediction is how to represent code in a semantically enhanced way such that code semantics can be learned through code representation learning.

To address this challenge, we propose Meth2Seq, a hybrid code representation learning approach, to encode a method as a sequence of distributed vectors. Meth2Seq is designed based on the following insights: (i) program dependence graph (i.e., control/data flows) captures semantic structures of a method; (ii) intermediate representation (IR) expresses a method as a sequence of statements of limited types, and each type implicitly carries certain operational semantics; (iii) natural language comment summarizes the high-level semantics of a method; and (iv) program dependence graph, intermediate representation, and natural language comment capture different aspects of the method semantics. Therefore, we represent a method as (1) a bag of paths on the program dependence graph, (2) a sequence of typed IR statements based on the context free grammar that describes the operational semantics of each IR statement type, and (3) a sentence of natural language comment. Then, we apply a neural network on these representations to generate a sequence of vectors. The learned sequence of vectors is fed to a decoder model to predict method names. We focus on the task of method name prediction in this work because (i) method names are of particular importance as they provide the cornerstone of abstraction for developers to communicate with each other for various purposes (e.g., code review and program comprehension) (Høst and Østvold, 2009) and (ii) method name prediction is the difficult task in program semantic property prediction (Alon et al., 2019b).

To demonstrate the effectiveness of Meth2Seq, we compared it against two state-of-the-art code representation learning approaches (i.e., Code2Vec Alon et al., 2019b and Code2Seq Alon et al., 2019a) as well as two state-of-the-art method name prediction approaches (i.e., LiuName1 (Liu et al., 2019) and HeMa (Jiang et al., 2019)) with a dataset of 280.5K methods in 67 Java projects. Our experimental results have demonstrated that Meth2Seq can significantly outperform Code2Vec and Code2Seq in F1-score respectively by 92.6% and 36.6%, and also significantly outperform LiuName and HeMa in F1-score respectively by 85.6% and 178.1%. Besides, F1-score respectively decreases by 26.2%, 33.5% and 17.7% after we exclude program dependence graph, intermediate representation and comment from our hybrid code representation. Thus, each of the three code representations contributes to the achieved effectiveness of Meth2Seq.

In summary, this paper makes the following contributions.

•
We proposed a hybrid code representation learning approach, named Meth2Seq, to embed methods based on a mixture of three code representations, i.e., a bag of paths on the program dependence graph, a sequence of typed IR statements, and a sentence of natural language comment.

•
We evaluated Meth2Seq on the task of method name prediction with a dataset of 280.5K methods in 67 Java projects, and it significantly outperformed state-of-the-art code representation learning approaches and method name prediction approaches in F1-score.

2. Preliminaries on encoder–decoder model
As program property prediction is usually formulated as a machine translation problem, we briefly introduce the encoder–decoder model that our approach is built upon.

Encoder–Decoder Model. Encoder–decoder model has shown promising performance in various fields, especially in machine translation field (Cho et al., 2014, Sutskever et al., 2014). Sequence-to-sequence model (Cho et al., 2014) has become the state-of-art method on a wide range of translation tasks. Given a sentence  of the source language, it generates a sentence  of the target language. To achieve this goal, the encoder computes a hidden state  at each time step  as ; i.e.,  is a non-linear function that maps  to a hidden state  by integrating the previous hidden state . The last hidden state  can be used as a context vector  to the decoder. The decoder predicts the word  at time step  as ; i.e.,  is a non-linear function that predicts the output depending on the context vector  from the encoder, the previous output , and the hidden state of the decoder . During training, the model parameters are learned to maximize the conditional log-likelihood (Cho et al., 2014).

Attention. While sequence-to-sequence model has gained huge successes, it suffers poor performance on translating long sentences because it is very likely that the model “forgets” the long term dependency information in previous words if the sentence grows. Then, attention mechanism (Luong et al., 2015) is proposed to address this problem. It makes the model only pay attention to relevant words in source sentence. Specifically, at each time stamp  in decoding, a context vector  is computed through weighing over each hidden state in the encoder. Thus, the current word  is predicted by .

Transformer. The transformer model uses only attention as the model’s encoder and decoder. The insight behind this model is: if attention performs so good when it is placed between encoder and decoder, it should be used inside encoder and decoder too. As shown in the left part of Fig. 1, the encoder takes each word’s embedding as input, and generates as output two attention vectors for each word. The attention vectors characterize the relationship between the corresponding word and all input words. The encoder has four layers. The self-attention layer computes two attention vectors for each input word, and the feed-forward layer applies a non-linear mapping. Each of the two layers are followed by an add and norm layer which applies a residual connection and normalization to speed up training and prevent over-fitting. The decoder shares the similar structure to the encoder. However, the self-attention layer in the decoder also takes as input the encoder’s output. The masked self-attention operates differently from the encoder’s self-attention. It takes as input only the previous words of the predicted word, and masks the words after the predicted word in order to prevent the model to “see” the word before it actually predicts it. At last, the decoder’s output is put in a linear and softmax layer to compute each word’s probability at the current time stamp.


Download : Download high-res image (214KB)
Download : Download full-size image
Fig. 1. The architecture of the transformer model.


Download : Download high-res image (302KB)
Download : Download full-size image
Fig. 2. Overall of our approach.

3. Methodology
We first present our approach overview. Then, we explain how we represent a method, learn the code representation, and use the representation for predicting method names.

3.1. Approach overview
At a high level, our goal is to represent methods in such a way that captures semantic information, enables learning across a massive collection of methods, and can be used to predict method names. To this end, our main idea is to use the code semantic structures in program dependence graph (PDG), the implicit operational semantics in intermediate representation (IR), and the explicit summarized semantics in natural language comment. In particular, control and data flows in PDG respectively characterize the execution order of statements and the data definition and usage among statements. Statements in IR have a limited number of types, and each type of IR statement is an operation that carries simple operational semantics. Compared with AST, both PDG and IR provide a higher abstraction of code. As a higher abstraction requires capturing more semantic information of code (Zhao and Huang, 2018), PDG and IR are more powerful in capturing code semantics than AST. Besides, method comment offers a good source of semantics summarized by developers. Hence, these three code representations capture code semantics from different perspectives, whose integration helps to better characterize code semantics.

Based on these insights, as shown in Fig. 2, we propose Meth2Seq to encode a method body as a sequence of distributed vectors. It represents a method body as a bag of PDG paths (Section 3.2), a sequence of typed IR statements (Section 3.3), and a sentence of method comment (Section 3.4). Then, it first applies positional encoding and a fully connected layer to initialize each PDG path, each typed IR statement and the method comment as a vector, and uses positional encoding and Transformer encoder to generate a sequence of vectors for each method (Section 3.5). The learned sequence of vectors can be fed to a Transformer decoder to predict method names (Section 3.6).

It is worth mentioning that our approach is general and can be extended to support other programming languages and intermediate representations, we focus on Java programs in this paper and use Jimple (Vallee-Rai and Hendren, 1998) as the IR.

3.2. Representing method as a bag of PDG paths
As shown in the top part of Fig. 2, our method representation with PDG has two steps, i.e., PDG construction and PDG path representation. We will elaborate each step below.

PDG Construction. Given the method corpus in projects, we construct a PDG for each method. The PDG of each method is denoted as a 5-tuple , where  denotes a set of nodes, and each node  denotes a (Jimple) statement;  denotes a set of edges, and each edge  denotes a control flow between  and ;  denotes a set of edges, and each edge  denotes a data flow between  and ;  denotes the exit node; and  denotes a function that maps each node  to a (Jimple) statement . In particular, We use  to denote the out edges from . We use Soot (Vallée-Rai et al., 1999) to compute the control flows and data flows for each method at the Jimple statement level, and then construct .

Example 3.1

Fig. 3 shows an example of a Java method; and Fig. 4 presents its PDG generated by Soot with the option use-original-names enabled. The black arrows denote control flows and the red arrows denote data flows. It has eight Jimple statements, , , …, ; and  is the exit statement.  obtains the this variable of the residing class of the method in Fig. 3.  uses this to get the field info and assigns it to stack2.  invokes nodeList on stack2, and assigns its return value to available.  invokes a static method shuffle that takes available as the argument.  invokes get on available, and assigns the return value to stack4.  casts the type of stack4, and assigns it to stack5.  invokes getHostName on stack5, and assigns the return value to stack6. Finally,  returns stack6.

PDG Path Representation. A PDG path in  of a length at most  is denoted as a sequence , where ; for each ,  is a Jimple statement; and for each ,  is a control or data flow between  and . We use , } to denote the flow type of , use  to denote  if , and use  to denote  if . We use a bag of PDG paths to represent a method body as it has several advantages. First, PDG paths can capture diverse local semantic structure information. Second, PDG paths transform the non-sequential graph structure into sequences, and thus reduce the efficiency of representation learning. Third, PDG paths are automatically generated and are applicable to other programming languages.

To automatically generate a PDG path from , we use a random walk strategy. It first chooses a node from  uniformly at random as the current node , and then works in iterations until the maximum length  is reached or the exit node  is reached. In each iteration, it selects an edge from ’s out edges  uniformly at random, and uses the edge’s target node as the current node to continue the iteration. To generate a bag of PDG paths , we apply the previous procedure for multiple times. As PDGs for different methods have different size, we limit the number of generated PDG paths and the maximum length of a PDG path with respect to the size of . For each method, we generate  PDG paths of length at most 
 
. Hence, the larger the PDG, the more PDG paths are generated to capture the semantic structure more completely, and the more local semantic structure information is captured in a PDG path.

Example 3.2

Given the PDG in Fig. 4, if we set  to 2, then a PDG path has the maximum length of 5. If our generation starts at , a possible PDG path is , and if starts at , a possible PDG path is .

We regard each PDG path  as a sentence in two steps. First, for , we represent each node  () as its corresponding Jimple statement , and each edge  () as the flow type ; i.e.,  is represented as . Then, we use tokenizing, splitting according to camel notation, removing non-alphabet characters and locals (e.g., stack2), and transforming to lower cases to pre-process the previous representation. Finally, we have  sentences for each method.

Example 3.3

 in Fig. 4 is represented as a sub-sentence of “virtualinvoke java net inet address java lang string get host name”, the data flow edge between  and  is represented as a word of “data”, while the control flow edge between  and  is represented as a word of “control”.

3.3. Representing method as a sequence of typed IR statements
As shown in the middle part of Fig. 2, our method representation with IR is composed of two steps, i.e., IR generation and typed IR representation. We will elaborate each step below.

IR Generation. As we use Jimple as the IR, we run Soot (Vallée-Rai et al., 1999) to generate the Jimple representation, i.e., a sequence of Jimple statements, for each method. Jimple is a typed and compact 3-address code representation for Java bytecode (Vallee-Rai and Hendren, 1998). Statements are restricted to the least number of operands (two in most cases, but possibly more for some method invocation statements), and the operands must either be constants or local variables; and local variables must be explicitly declared and typed.

Example 3.4

The Jimple representation of the method in Fig. 3 is actually already showed in Fig. 4, i.e., the eight statements in the eight nodes. Type information is contained in statements; e.g., in , stack2 is of type BucketInfo, and in , available is of type List.

Typed IR Representation. The underlying operational semantics of Jimple statements are valuable, but are not explicitly but are implicitly reflected and thus are hard to be learned. For example, it is difficult to learn the operational semantics of the Jimple statement in  although it is obvious for human developers to known that it is a cast operation. Fortunately, in Jimple, there are a total of 15 types of Jimple statements with the operational semantics well-defined in its context free grammar (Vallee-Rai and Hendren, 1998). A context free grammar contains a set of production rules. Each production rule is composed of a left-hand side and right-hand side. The left-hand side is a non-terminal symbol, and the right-hand side is a sequence of non-terminal and terminal symbols. Based on the context free grammar, each Jimple statement is derived via a set of production rules by recursively rewriting each non-terminal symbol in the left-hand side of a production rule until there is no non-terminal symbol. In that sense, Jimple statements consist of terminal symbols that lose the type information of non-terminal symbols where the terminal symbols come. For example,  contains a cast expression but we lose this semantic information after the derivation.

Based on this observation, we try to add the non-terminal (or type) information for each Jimple statement according to the context free grammar of Jimple, in order to explicitly express the underlying operational semantics of each statement. Particularly, for each statement, we first add the non-terminal symbol that represents the type of the statement (e.g., assign statement); then we parse each Jimple statement according to the context free grammar and add for each terminal symbol the non-terminal symbol where it comes. Based on these two rules, we automatically transform each Jimple statement into typed Jimple statements which have more naturalness (Hindle et al., 2012) than original Jimple statements and hence can ease the burden of deep learning in extracting semantics.


Download : Download high-res image (332KB)
Download : Download full-size image
Fig. 5. Typed IR statements of the method in Fig. 3.

Example 3.5

Fig. 5 presents the generated typed Jimple statements based on the original Jimple statements in Fig. 4.  is of type JAssignStmt, which could assign a rvalue to a local, or an immediate (a local or a constant) to a static field, to an instance field or to an array reference according to the context free grammar.  is an assign statement with its rvalue being a cast expression. Compared with the original  in Fig. 4, it has the type information of the statement (i.e., assign statement), the type information of stack2 (i.e., local variable), the type information of the enclosed expression (i.e., cast expression), and the type information of stack4 (i.e., local variable). Obviously, the semantics expressed in  in Fig. 5 is implied by  in Fig. 4, and can hard to be learned from  in Fig. 4.

We regard each typed Jimple statement as a sentence, and use the same pre-processing steps as in method representation with PDG (see Section 3.2). Finally, we have  sentences for each method.

3.4. Representing method as a sentence of comment
As shown in the bottom part of Fig. 2, our method representation with comment is simply to use the first sentence of the method comment because the first sentence is often a summary of a method according to the JavaDoc guidance.2 We use common strategies (i.e., tokenizing, removing stop words, and stemming) to pre-process the extracted sentence. Finally, we have one sentence of comment for each method. Notice that for method without comments, we leave the comment empty with the intuition that as long as the comment is available, we should use any available semantic information from it.

3.5. Learning code representation in a hybrid way
After we represent each method as  sentences for PDG paths,  sentences for typed IR statements, and one sentence for method comment, we define the input vocabulary as the words in these  sentences for each method in corpus, and randomly initialize the embedding of each word which will be later learned simultaneously with the neural network during training. For each of the  sentences for a method, we first use positional encoding to encode the position information of the words in each PDG path, each typed IR statement, and the method comment. Then, we use a fully connected layer to combine the embedding of each word in this sentence into a vector. Here we also try a position-wise feed forward layer and a one-dimensional convolution kernel. It turns out that position-wise feed forward layer and fully connected layer have significant advantages over convolution kernel, while position-wise feed forward layer is only slightly better than fully connected layer. Considering the model complexity, we finally choose fully connected layer as the fusion layer. Then, the  vectors are sequentially fed into positional encoding to encode the position information (especially useful for the vectors for typed IR statements) and then the Transformer encoder (see Section 2) to generate a sequence of  attention vectors that represent a method in a hybrid way.

3.6. Predicting method names
Using the sequence of attention vectors generated by Meth2Seq, we use a Transformer decoder (see Section 2) to achieve the task of method name prediction. To construct the output vocabulary, we extract method names and use them as the output vocabulary.

To train the network, we use cross-entropy loss between the predicted probability distribution and the true probability distribution. To use the trained network for method name prediction, we get PDG paths, typed IR statements and comment for the method under prediction, and then put them to Meth2Seq to predict one token at a time until “EOS” (denoting end of sentence) is outputted. Finally, the token list (except for the last “EOS” token) predicted is used as the method name.

Example 3.6

For the method in Fig. 3, Meth2Seq accurately predicts the token list as “random”, “available”, “host”, “name” and “EOS”. This shows that Meth2Seq captures the semantics underlying the shuffle operation on node list.

4. Evaluation
We have implemented Meth2Seq in 11.8K lines of Python and Java code, using Soot for static analysis and PyTorch for deep learning. We have released the code of Meth2Seq at our website3 with the dataset used in our evaluation.

4.1. Evaluation setup
We compared our approach with state-of-the-art code representation learning approaches and method name prediction approaches, and analyzed the contribution of code representations in Meth2Seq, using 67 GitHub Java projects. Our evaluation is designed to answer the following research questions.

•
RQ1: What is the effectiveness of Meth2Seq in predicting method names, compared with state-of-the-art approaches?

•
RQ2: What is the contribution of the three code representations in Meth2Seq to the achieved effectiveness?

•
RQ3: What is the sensitivity of Meth2Seq’s effectiveness to the number and length of PDG paths?

Dataset. As our approach needs to apply static analysis at the Jimple representation, we need to ensure that selected projects can be successfully compiled. Due to this restriction, the existing datasets used in the literature (Allamanis et al., 2016, Alon et al., 2018, Alon et al., 2019b, Alon et al., 2019a) cannot be used. We attempted to manually compile GitHub Java projects, but found that most compilation problems were caused by library dependency. Hence, we decided to leverage Travis CI to automatically build the projects. Finally, we created a dataset of 67 highly-starred Java projects, and applied Soot to generate the PDG and IR of each method. Following previous works (e.g., Allamanis et al., 2016 and Liu et al., 2019), we removed main methods, constructor methods, empty methods, test methods and example methods, and created a corpus of 280.5K methods. We split our method dataset into training, validation and testing dataset by 7:1:2.

State-of-the-Art Approaches. We selected Code2Vec (Alon et al., 2019b) and Code2Seq (Alon et al., 2019a) as the state-of-the-art code representation learning approaches because they have been empirically demonstrated to outperform the existing code representation learning approaches (e.g., Alon et al., 2018) on the task of method name prediction, and they also outperform previous method name prediction approaches (Allamanis and Sutton, 2013, Allamanis et al., 2015a, Allamanis et al., 2016). Notice that both Code2Vec and Code2Seq are AST-path based approaches. We did not compared Meth2Seq with the recent work Dypro (Wang and Su, 2019) as it is not open-sourced and it requires dynamic execution traces which are not easy to obtain. Furthermore, we also selected two recent method name prediction approaches, i.e., LiuName (Liu et al., 2019) and HeMa (Jiang et al., 2019). Specifically, LiuName also uses a deep learning-based approach but represents code as tokens, while HeMa relies on heuristic rules to predict method names and demonstrates better effectiveness than Code2Vec.

Evaluation Metrics. Following prior works (Allamanis et al., 2016, Alon et al., 2018, Alon et al., 2019b, Alon et al., 2019a), we used four metrics to measure the effectiveness of method name prediction over case-insensitive sub-tokens. The first metric is exact match accuracy, i.e., the percent of method names predicted exactly; e.g., given a method name totalCount, a prediction of total_count is considered as exactly matched. The other three metrics are precision, recall and F1-score. For example, given a method name totalCount, a prediction of total has full precision but low recall, while a prediction of totalUserCount has full recall but low precision.

Training Configuration. We set all the layers inside the encoder and decoder identical in shape with 256 hidden units, and the 256 hidden units are equally divided into 8 heads in the self-attention layer. Our model is trained via Adafactor optimizer with ,  and . In the training process, the learning rate starts from an initial value of 0 and varies along each training step. It increases linearly in the first warmup=4,000 steps and decreases proportionally with the step number thereafter. To prevent over-fitting, we add a dropout in each self-attention head with a probability of 50%. For Code2Vec, Code2Seq, LiuName and HeMa, we directly use the model parameter settings described in their original work. We set the training epochs to 100 and the training batch size to 128 for all the approaches. Notice that the training has converged (i.e., reach the best performance) within 100 epochs for all the approaches. All models are trained on a machine with a Nvidia Titan X GPU and a 2.4GHZ CPU with 16 cores and 128G memory.

4.2. Quantitative study (RQ1)
Table 1 presents the results of Code2Vec, Code2Seq, LiuName, HeMa and Meth2Seq with respect to the four metrics. Overall, Meth2Seq significantly outperformed Code2Vec, Code2Seq, LiuName and HeMa in all the four metrics, which demonstrates that leveraging PDG paths, typed IR statements and method comment to encode method code can better capture code semantics as code has well-defined semantics that would be difficult to capture by only considering source code tokens or syntactical AST paths. Code2Seq achieved better performance than Code2Vec as it uses LSTMs to encode AST paths node-by-node rather than monolithically. LiuName had similar performance to Code2Vec but worse performance than Code2Seq because it only treats code as tokens. Besides, HeMa, a rule-based approach, achieved the worst performance.


Table 1. Quantitative comparative results in four metrics.

Approach	Exact	Precision	Recall	F1
Code2Vec	0.296	0.407	0.399	0.403
Code2Seq	0.329	0.626	0.520	0.568
LiuName	0.227	0.457	0.385	0.418
HeMa	0.200	0.443	0.204	0.279
Meth2Seq	0.582	0.778	0.778	0.776
Numerically, Meth2Seq outperformed Code2Seq in exact match accuracy by 76.9%. Predicting the exact same name to the original method name is challenging because method names are often project-specific, and developers may use semantically equivalent or slightly misleading method names. Meth2Seq significantly outperformed Code2Seq in precision by 24.3% and recall by 49.6%, and hence had an improvement of 36.6% in F1-score. Moreover, Meth2Seq had a more balanced result for precision and recall which are both important for method name prediction. Compared to Code2Vec, Meth2Seq had 96.6% and 92.6% improvement in exact match accuracy and F1-score, respectively. These results indicate that Meth2Seq can capture code semantics much better than the existing state-of-the-art code representation learning approaches. Besides, Meth2Seq improved exact match accuracy and F1-score over LiuName by 156.4% and 85.6%, and over HeMa by 191.0% and 178.1%, showing the superior advantage over token-based or rule-based approaches.

As prior works (Fu and Menzies, 2017, Hellendoorn and Devanbu, 2017, Liu et al., 2018, Menzies et al., 2018) suggest that it is a good practice to look into the data when leveraging deep learning on programming language and software engineering tasks. Hence, we manually analyzed all exact match predictions in the five approaches, and found that methods starting with “get” or “set” accounted for a high percentage. This is also observed by Jiang et al. (2019), and motivates one of their rules to generate get/set methods. This result motivated us to systematically analyzed the percentage of get/set methods in the testing data, in all predictions, and in all exact match predictions. It turns out that there are around 30% of get/set methods in the testing data. Table 2 reports the percentage of get/set methods in prediction results. We can observe that the five approaches had a similar percentage of get/set methods in all their predictions. However, in the exact match predictions, 44.5% and 56.1% were get/set methods in Code2Vec and Code2Seq, and 39.6%, 38.1% and 37.4% were get/set methods in LiuName, HeMa and Meth2Seq, which were much lower than Code2Vec and Code2Seq. These results indicate that Meth2Seq is more diverse in predicting method names, and it is also worthwhile to combine AST-path approaches.


Table 2. Get/Set methods in all and exact match predictions.

Approach	All Prediction (%)	Exact Prediction (%)
Code2Vec	30.9	44.5
Code2Seq	39.3	56.1
LiuName	35.9	39.6
HeMa	32.7	38.1
Meth2Seq	34.6	37.4


Download : Download high-res image (220KB)
Download : Download full-size image
4.3. Ablation study (RQ2)
We conducted an ablation study to understand the contribution of various settings in code representations in Meth2Seq to the achieved effectiveness, and thus ran six experiments.

Removing PDG Path Representation. We removed the PDG path representation, and only represented methods as a sequence of typed IR statements and method comment. The result is listed in the third row of Table 3. We can observe that after removing PDG paths from our model, the performance of Meth2Seq decreased in exact match accuracy by about 18% and in precision, recall and F1-score by more than 24%. It shows that PDG paths significantly contribute to the semantics characterization.


Table 3. Code representation contribution analysis.

Approach	Exact	Precision	Recall	F1
Meth2Seq	0.582	0.778	0.778	0.776
Meth2Seq w/o PDG	0.475	0.571	0.585	0.573
Meth2Seq w/o IR	0.431	0.513	0.530	0.516
Meth2Seq w/o Comment	0.534	0.638	0.648	0.639
Meth2Seq w/o Type in IR	0.462	0.569	0.565	0.561
Meth2Seq with only PDG	0.414	0.472	0.509	0.489
Meth2Seq with only IR	0.460	0.548	0.541	0.544
Removing IR Representation. We removed the typed IR representation, and only represented methods as a bag of PDG paths and method comment. The result is shown in the fourth row of Table 3. It turns out that after removing IR statements from our model, Meth2Seq had a larger performance degradation than in the previous setting, i.e., around 25% decrease in exact match accuracy and more than 31% decrease in precision, recall and F1-score. It indicates that typed IR statements have more contribution to capture code semantics than PDG paths.

Removing Comment Representation. We removed the available method comments, and only represented methods as a bag of PDG paths and a sequence of typed IR statements. The result is shown in the fifth row of Table 3. After removing comments, Meth2Seq had a slight performance degradation (i.e., 8%) in exact match accuracy but a more than 16% decrease in precision, recall and F1-score. This indicates that comments do contribute to the semantics understanding in Meth2Seq but in a less significant way than PDG paths and typed IR statements.

Removing Type in IR Representation. We directly used the original Jimple statements rather than type IR statements for method IR representation. The result is shown in the sixth row of Table 3. After removing the type information in IR statements, the performance of Meth2Seq decreased in exact match accuracy by about 20% and in precision, recall and F1-score by more than 26%. It indicates that type information encoded by context free grammar does contribute to better reflect code semantics, and significantly improves the contribution of IR statements.

Only Keeping PDG Path Representation. We only represented methods as a bag of PDG paths. The result is shown in the seventh row of Table 3. We can see that the performance of Meth2Seq decreased in exact match accuracy by about 29% and in precision, recall and F1-score by more than 34%. It indicates that PDG path alone cannot effectively capture code semantics.

Only Keeping IR Representation. We only represented methods as a sequence of typed IR statements. The result is shown in the last row of Table 3. It can be observed that Meth2Seq had a smaller performance degradation than in the previous setting, i.e., around 21% decrease in exact match accuracy and more than 29% decrease in precision, recall and F1-score.



Download : Download high-res image (81KB)
Download : Download full-size image
4.4. Sensitivity analysis (RQ3)
Our method representation with PDG paths relies on two parameters: the maximum length of PDG paths (i.e., 
 
) and the number of PDG paths (i.e., ). In Sections 4.2 Quantitative study (RQ1), 4.3 Ablation study (RQ2), we reported the results when  and  was set to 4 and 8. We analyzed Meth2Seq’s sensitivity to them.

Sensitivity to the Maximum Length of PDG Paths. We set  to 1, 2, 4 and 8 and fixed  to 8, and ran the experiments. The result is presented in Table 4. When  was smaller than 4, the performance of Meth2Seq degraded in all the four metrics. As  decreases, the maximum length of a PDG path increases. However, a long PDG path might fail to capture local semantics but cause noises and pollute relevant information. On the other hand, when  was larger than 4, the performance of Meth2Seq also gradually decreased in all the four metrics. As  increases, the maximum length of a PDG path decreases. A short path may capture incomplete or fragmented semantics, and thus causes the performance degradation. Based on these results, we suggest to set  to 4.

Sensitivity to the Number of PDG Paths. We set  to 2, 4, 6 and 8 and fixed  to 4, and ran the experiments. The result is reported in Table 5. We can see that, as  increased, the performance of Meth2Seq also gradually increased in almost all the four metrics. This indicates that a larger number of PDG paths can capture code semantics more completely.



Download : Download high-res image (80KB)
Download : Download full-size image
4.5. Qualitative study
Apart from the quantitative study in Section 4.2, we report some interesting cases to illustrate Meth2Seq’s capability.

For the method in Fig. 3, Meth2Seq accurately predicted its name. Code2Vec predicted “get” and “hostname”, Code2Seq predicted “get” and “host”, LiuName predicted “get”, “host” and “name”, and HeMa predicted “to” and “string”, all failing to precisely capture and reflect the semantics.


Table 4. Sensitivity analysis of the maximum path length.

Path Length 
 
Exact	Precision	Recall	F1
a  1	0.545	0.736	0.740	0.738
a  2	0.562	0.755	0.753	0.754
a  4	0.582	0.778	0.778	0.776
a  8	0.569	0.762	0.759	0.760

Table 5. Sensitivity analysis of the number of paths.

Path Number 	Exact	Precision	Recall	F1
b  2	0.538	0.734	0.732	0.733
b  4	0.554	0.715	0.743	0.729
b  6	0.565	0.746	0.753	0.750
b  8	0.582	0.778	0.778	0.776
Fig. 6 shows a method where Meth2Seq achieved an exact match, while Code2Vec predicted “convert”, Code2Seq predicted “get”, “message” and “converter”, LiuName predicted “init”, “default” and “converters”, and HeMa failed to predict as there was no matched heuristic rule. The code at Line 5–9 gets message converters of a specific type. Meth2Seq successfully captured this code semantics, and predicted “for” and “type” as the last two tokens for this method name.


Download : Download high-res image (299KB)
Download : Download full-size image
Fig. 6. An example of exact match prediction.

Fig. 7 shows a method where Meth2Seq correctly predicted the first token of the method name. Meth2Seq predicted “convert”, Code2Vec predicted “decode”, Code2Seq predicted “get”, “request” and “content”, LiuName predicted “copy”, “to”, “byte” and “array”, and HeMa predicted “to”, “byte” and “array”. Code2Seq fails to capture the underlying action, Meth2Seq and Code2Vec fail to capture the action target, and LiuName HeMa directly use tokens in the code.


Download : Download high-res image (188KB)
Download : Download full-size image
Fig. 7. An example of first-token match prediction.

Fig. 8 shows an example that Meth2Seq predicted the incorrect name. Meth2Seq predicted “add”. However, “add” is semantically equivalent to “put” in this case. Predicting exact names is difficult as developers may use semantically equivalent names. In that sense, the results in Section 4.2 are an underapproximation of Meth2Seq’s capability to predict meaningful method names. Code2Vec predicted “to” and “json”, Code2Seq predicted “add” and “payload”, LiuName predicted “parse”, and HeMa predicted “get”. All are not accurate.


Download : Download high-res image (48KB)
Download : Download full-size image
Fig. 8. An example of incorrect prediction.

Fig. 9 shows an example that two methods have different implementations but have similar functionality and thus the same method name. Specifically, Line 3–4 and Line 10–12 in the first and second method prepares a query, while Line 6 and Line 14 in the first and second method returns a list of all query results. Meth2Seq correctly predicted “find” and “all” for both methods. Code2Vec predicted “execute” for the first method, and “sort” for the second. Code2Seq predicted “find” and “pageable” for the first method, and predicted correctly for the second. LiuName predicted “find” and “query” for the first method, and “get”, “result” and “list” for the second. HeMa predicted “get” and “pageable” for the first method, and failed to predict for the second due to no matched rule.

Fig. 10 gives an example that two methods have similar code but different functionality and thus different method names. Line 3–11 in the first method and Line 23–31 in the second method are exactly the same, preparing a query statement by an id. The key difference is their return statement at Line 12 and Line 32: the former returns one of the fetched results, and the latter returns all fetched results. Meth2Seq captured this difference, and correctly predicted the method names. Code2Vec also had a correct prediction, Code2Seq respectively predicted “get”, “all” and “id”, and “get”, “all” and “mapper” for the two methods, LiuName predicted “get” and “maps”, and “get”, “mapped” and “object” for them, while HeMa predicted “get”, “topology” and “conf”, and “supports” for them.


Download : Download high-res image (269KB)
Download : Download full-size image
Fig. 9. An example of semantically similar methods.

Besides, we report a method, as shown in Fig. 11, where Meth2Seq failed to predict the correct name. This method verifies whether the type of a value is a certain and supported type. Meth2Seq predicted “is” and “value”, failing to understand the code semantics. In fact, the method call at Line 7 reflects the semantics of this method. However, Meth2Seq failed to capture it. One potential reason is that our code representation only considers intra-procedural data/control flows but does not consider inter-procedural data/control flows. Therefore, one potential improvement is to leverage call graph information in code representation learning. Notice that Code2Vec predicted “get”, “class” and “type”, Code2Seq predicted “get” and “value”, LiuName predicted “equals”, and HeMa predicted “value” and “of”. All are not accurate.


Download : Download high-res image (496KB)
Download : Download full-size image
Fig. 10. An example of semantically different methods.

4.6. Discussion
We discuss the threats to our evaluation and the potential applications of our approach.

Threats. One threat to our evaluation is the size of the dataset, when compared with the dataset used in AST path-based approaches (Allamanis et al., 2016, Alon et al., 2018, Alon et al., 2019b, Alon et al., 2019a). As our approach relies on heavyweight static analysis to generate PDG and Jimple representations, we selected 67 projects that could be successfully built by Travis CI as our dataset. We believe that the current promising results are convincing enough to indicate the capability of static analysis in capturing code semantics for predicting method names. In fact, our dataset is actually much larger than the deep learning-based works that use control flow graphs (e.g., 28K methods for code search Wan et al., 2019, and 9 projects for bug detection Li et al., 2019b). We argue that it is the step we must take to apply program analysis on “big code” for better program comprehension. We have released our dataset at our website. To the best of our knowledge, this is the first dataset of program methods with PDGs and Jimple representations. We are also continuously enlarging this dataset, and investigating the possibility of using automatic repository building techniques (Hassan et al., 2017) to ease the enlarging.

Another threat to our evaluation is that we only evaluated the effectiveness of Meth2Seq on the task of method name prediction. In fact, we deliberately picked this task because method name prediction is difficult in the sense that it needs to have an accurate and summarized semantic understanding of the potentially large method body. The same strategy is also used by prior work (Alon et al., 2019b), where they also claimed that “succeeding in [method name prediction] would suggest that the code vector has indeed accurately captured the functionality and semantic role of the method”. We are applying Meth2Seq to other program property prediction tasks such as variable name prediction to empirically demonstrate the generality of our approach over different tasks.

Applications. We design Meth2Seq for the task of method name prediction. We believe that the overall idea of Meth2Seq that uses a hybrid code representation is general and can be used in more applications, e.g., method comment generation, semantic clone detection, and code search. Moreover, we are applying this idea to “big code” driven code change comprehension; i.e., we generate the PDG for the methods before and after changes, and use differencing algorithms to extract PDG changes as the representation of code changes.

Furthermore, we are exploring other orthogonal possibilities to improve Meth2Seq. First, we are investigating other possibilities to learn from PDGs. Graph embedding techniques (Cai et al., 2018) can be a potential way to directly learn from graphs. However, the learning efficiency might be reduced. Second, we are investigating visualization techniques to explain which PDG paths, typed IR statements or method comment contribute to the prediction. Third, we currently use random walk, inspired by DeepWalk (Perozzi et al., 2014), to generate representative PDG paths. We ran our approach for five times, and found no significant difference. We plan to investigate how to systematically generate PDG paths and whether such systematic approach can achieve better performance. Last but not the least, we believe that each code representation at the lexical, syntactical and semantic level has its merit in understanding code semantics. Hence, we plan to combine lexical tokens and AST paths into our approach to investigate whether it can further improve the performance.

5. Related work
We review and discuss the most closely related work in predicting program semantic properties and deep learning for programming language and software engineering.

5.1. Predicting program semantic properties
Advances have been recently made to predict program semantic properties (e.g., method names, variable names, and variable types) from a massive collection of programs (Yahav, 2015, Bielik et al., 2015, Ernst, 2017, Allamanis et al., 2018b).

Allamanis and Sutton (2013) learned a n-gram model to predict method names, variable names and variable types. Allamanis et al. (2015a) learned a log-bilinear neural context model to suggest method names and class names. While their neural context model can capture long-distance context that is difficult to be captured by a n-gram model, it contains a set of hard-coded features that hinder the generality. To improve the generality, Allamanis et al. (2016) proposed a convolutional attention neural network to predict method names. Different from previous approaches that represent code as tokens at the lexical level, Alon et al. (2018) proposed to represent code as AST paths at the syntax level, and achieved a higher accuracy in predicting method names, variable names and variable types. Then, Alon et al., 2019b, Alon et al., 2019a improved their previous work (Alon et al., 2018) by adopting an attention mechanism to learn the weights over AST paths and using LSTMs to represent AST paths node-by-node, and achieved a higher accuracy in predicting method names. Bui et al. (2021) adapted the idea of self-supervised training and developed a pre-training task on code ASTs to learn code representation. Their output embeddings can be used for multiple downstream tasks. They achieved a comparable result with Code2Seq on the task of method name prediction. Wang and Su (2019) learned code representations from symbolic and concrete execution traces at the semantics level. While achieving higher accuracy in predicting method names, their approach might suffer scalability in collecting execution traces. Our approach also represents code at the semantic level but uses the semantics in program dependence graph and intermediate representation which are easier to obtain. Recently, Sui et al. (2020) proposed a new code embedding approach that precisely preserves interprocedural program dependence (a.k.a value-flows) and embeds control-flows and alias-aware data-flows of a program in a low-dimensional vector space, and applied it to method name prediction and code classification. Differently, instead of only relying on control and data flows, we further consider IR and comment to achieve a hybrid code semantic representation learning.

Allamanis et al. (2014) leveraged n-gram model to suggest identifier names. They first retrieved a set of candidate names that have occurred in the same context in other code snippets, and then measured the naturalness by a learned n-gram model to rank candidate names. Raychev et al. (2015) modeled relations among variables and program elements as a dependency network, and predicted variable names with conditional random fields. Vasilescu et al. (2017) recovered variable names through statistical machine translation, while Tran et al. (2019) leveraged information retrieval to recover variable names. Recently, deep learning techniques are applied to predict variable names by representing code as tokens (Bavishi et al., 2018, Liu et al., 2019) or graphs (i.e., ASTs augmented with semantic relationships between AST nodes) (Cvitkovic et al., 2019, Allamanis et al., 2018). Most of them rely on lexical or syntactic knowledge of code, and hence it is difficult for them to learn code semantics, hiderning their effectiveness in predicting variable names. Note that naming variables often relies on relatively local context, while naming methods is more difficult as it often requires non-local information from method bodies (Allamanis et al., 2015a). This is also why we select method naming as the application scenario of our approach. Besides, a number of advances have been made to detect naming inconsistencies (Lee et al., 2021, Arnaoudova et al., 2013, Høst and Østvold, 2009, Binkley et al., 2011, Kim and Kim, 2016, Deissenboeck and Pizka, 2006, Lawrie et al., 2006a). Our approach can be applied to detect naming inconsistencies and suggest method names for identified inconsistencies.

Phan et al. (2018) adopted statistical machine translation to learn fully qualified names for API elements. Hellendoorn et al. (2018) and Malik et al. (2019) applied a sequence-to-sequence model to infer types, respectively using code tokens and natural language information in code. It is also difficult for these approaches to learn code semantics, while our approach uses program dependence graph and intermediate representation to better capture the semantics of a method.

After reviewing these approaches, we can observe a trend that code is first represented at the lexical level and then at the syntax level. Following this trend, we represent code at the semantic level by a hybrid combination of program dependence graph, intermediate representation and natural language comments. It is also interesting to further investigate the effectiveness of a hybrid combination across these representation levels (i.e., lexical, syntax and semantic levels).

It is worth mentioning that Yefet et al. (2020) introduced a novel approach for attacking trained neural models of code using adversarial examples, i.e., forcing a given trained model to make an incorrect prediction. To defend a model from suck attacks, they also proposed several defense strategies to drop the success rate of attacker and thus improve the model robustness. It is interesting to investigate whether our model can be attacked by their approach.

5.2. Deep learning for PL and SE
Apart from program semantic property prediction, an increased interest has been attracted in learning a probabilistic model of code from a corpus for various programming language and software engineering tasks (Yahav, 2015, Bielik et al., 2015, Ernst, 2017, Allamanis et al., 2018b). The previous successful attempts to model code range from using n-gram models (Hsiao et al., 2014, Campbell et al., 2014, Nguyen et al., 2013a, Karaivanov et al., 2014, Nguyen et al., 2013b, Tu et al., 2014, Hindle et al., 2012, Oda et al., 2015, Franks et al., 2015) to graph-based object usage model (Nguyen et al., 2009, Nguyen et al., 2018b, Nguyen et al., 2012, Nguyen and Nguyen, 2015, Nguyen et al., 2016b) to probabilistic grammars (Allamanis and Sutton, 2014, Bielik et al., 2016, Allamanis et al., 2018a, Raychev et al., 2016). Recent advances in deep learning techniques have triggered a new wave of applying them to address various programming language and software engineering tasks, e.g., defect prediction (Wang et al., 2016, Li et al., 2017b, Yang et al., 2015, Hoang et al., 2019), bug localization (Lam et al., 2015, Huo et al., 2016, Lam et al., 2017, Xiao et al., 2019, Huo et al., 2019, Seidel et al., 2017, Li et al., 2019), bug detection (Murali et al., 2017, Li et al., 2018b, Pradel and Sen, 2018, Habib and Pradel, 2019, Zhou et al., 2019, Li et al., 2019b, Li et al., 2021), bug fixing (Tufano et al., 2018a, Wang et al., 2018), syntax error fixing (Pu et al., 2016, Gupta et al., 2017, Bhatia et al., 2018, Mesbah et al., 2019, Gupta et al., 2019), code clone detection (White et al., 2016, Li et al., 2017a, Büch and Andrzejak, 2019, Zhao and Huang, 2018, Tufano et al., 2018b, Zhang et al., 2019), code migration (Nguyen et al., 2016a, Nguyen et al., 2017, Gu et al., 2017, Nguyen et al., 2018a), code generation (Sun et al., 2019, Quirk et al., 2015, Yin and Neubig, 2017, Rabinovich et al., 2017, Desai et al., 2016, Gvero and Kuncak, 2015, Chen et al., 2018, Sun et al., 2020), code completion (Brockschmidt et al., 2018, Maddison and Tarlow, 2014, Li et al., 2018a, White et al., 2015, Raychev et al., 2014), code search (Allamanis et al., 2015b, Gu et al., 2018, Wan et al., 2019), code change learning (Tufano et al., 2019), API usage search (Gu et al., 2016, Van Nguyen et al., 2016, Li et al., 2020), API analogy mining (Henkel et al., 2018, Chen et al., 2021), API retrieval (Nguyen et al., 2018c), code classification (Peng et al., 2015, Mou et al., 2016, Bui et al., 2018, Ben-Nun et al., 2018, DQ et al., 2019, Zhang et al., 2019, Wang, 2019), function synonym detection (DeFreez et al., 2018), performance prediction (Ben-Nun et al., 2018), traceability linking (Ye et al., 2016, Guo et al., 2017, Xie et al., 2019), specification mining (Le and Lo, 2018), comment generation (Movshovitz-Attias and Cohen, 2013, Iyer et al., 2016, Wan et al., 2018, Hu et al., 2020, Hu et al., 2018a, Guerrouj et al., 2015, LeClair et al., 2019, Hu et al., 2018b), and commit message generation (Cortés-Coy et al., 2014, Loyola et al., 2017, Jiang et al., 2017). They exploit the availability of a code corpus and the naturalness of and regularities in code (Hindle et al., 2012).

Among these deep learning-based approaches, except for Zhao and Huang, 2018, Tufano et al., 2018b, DeFreez et al., 2018, Ben-Nun et al., 2018, Henkel et al., 2018, Wang et al., 2018, Wang, 2019, Zhou et al., 2019, Li et al., 2019b, Wan et al., 2019 and Li et al. (2021), they represent code at the level of tokens, AST nodes, AST subtrees, AST paths or grammar rules, characterizing the code syntax but often having difficulty in learn code semantics. To better learn code semantics, Tufano et al. (2018b) investigated the effectiveness of different code representations (i.e., tokens, AST nodes, control flow graph, and bytecode) in detecting clones, and found that these code representations are complementary to each other. Inspired by this work, we use two code representations (i.e., program dependence graph and IR statements) to encode a method and use a neural network to combine the code representations for the problem of method name prediction. DeFreez et al. (2018) represented a method as paths in its inter-procedure control flow graph, and learned a method embedding where vectors for method synonyms are close. Ben-Nun et al. (2018) represented code as contextual flow graphs. Zhao and Huang (2018) represented code as a semantic matrix that encodes control flow and data flow information, and developed a neural network model to measure code similarity based on semantic matrices. We also use control and data flows, but further combine them with IR to represent a method. Zhou et al. (2019), Li et al. (2019b) and Wan et al. (2019) attempt to combine AST, control flow and data flow, while we intentionally only combine different semantic code representations (i.e., PDG, IR and comment) to reflect their advantage over syntax representations. Similarly, Li et al. (2021) used PDG to extract semantic representations of vulnerabilities and code tokens to extract syntax representations of vulnerabilities for the task of vulnerability detection. Differently, we combine different semantic code representations for the task of method name prediction. It is interesting to further consider AST representation in our approach.  Henkel et al. (2018) represented code as abstracted symbolic execution traces for learning word embedding. Wang et al., 2018, Wang, 2019 proposed a dynamic program embedding that is learned from dynamic concrete execution traces. While execution traces can capture deep and precise program semantics, these approaches may suffer scalability in collecting symbolic or concrete execution traces.

6. Conclusions
We have proposed Meth2Seq to encode a method as a sequence of distributed vectors based on a hybrid combination of code representations, i.e., a bag of PDG paths, a sequence of typed IR statements, and method comment. The learned sequence of vectors of a method is fed to a decoder model to predict method names. Our evaluation results with a dataset of 280.5K methods in 67 Java projects have indicated that Meth2Seq outperforms the two state-of-the-art code representation learning approaches in F1-score by 92.6% and 36.6%, while outperforming two state-of-the-art method name prediction approaches in F1-score by 85.6% and 178.1%. Our evaluation has also demonstrated the significant contribution of PDG paths, typed IR statements and method comment to the effectiveness of Meth2Seq. In future, we plan to explore potentials of learning from combined code representations.

