Abstract
Background
Software defect prediction is one of the most important research topics in software engineering. An important product measure to determine the effectiveness of software processes is the defect density (DD). Cased-based reasoning (CBR) has been the prediction technique most widely applied in the software prediction field. The CBR involves k-nearest neighborhood for finding the number (k) of similar software projects selected to be involved in the prediction process.

Objective
To propose the application of a transformed k-nearest neighborhood output distance minimization (TkDM) algorithm to predict the DD of software projects to compare its prediction accuracy with those obtained from statistical regression, support vector regression, and neural networks.

Method
Data sets were obtained from the ISBSG release 2018. A leave-one-out cross validation method was performed. Absolute residual was used as the prediction accuracy criterion for models.

Results
Statistical significance tests among models showed that the TkDM had the best prediction accuracy than those ones from statistical regression, support vector regression, and neural networks.

Conclusions
A TkDM can be used for predicting the DD of new and enhanced software projects developed and coded in specific platforms and programming languages types.

Previous
Keywords
Software defect density prediction

Case-based reasoning

Transformed k-nearest neighborhood output distance minimization

Support vector regression

Neural networks

ISBSG

1. Introduction
Software Engineering Process is a knowledge area of Software Engineering (SE) concerned with work activities accomplished by software engineers to develop, maintain, and operate software (Bourque and Fairley, 2014). An important product measure to determine the effectiveness of software processes is defect density (DD), which has been defined as the total number of defects divided by the size of the software (Shah et al., 2012). Software size has been mainly measured in either source lines of code or function points (FP) (Sheetz et al., 2009).

A defect has been defined as an ‚Äúimperfection or deficiency in a work product where that work product does not meet its requirements or specifications and needs to be either repaired or replaced.‚Äù, it is caused by a person committing an error, and it is associated to other SE knowledge area termed Software Quality that refers to ‚Äúdesirable characteristics of software products, to the extent to which a particular software product possess those characteristics, and to processes, tools, and techniques used to achieve those characteristics‚Äù (Bourque and Fairley, 2014).

On the other hand, the Software Engineering Management knowledge area is the application of management activities such as planning, coordinating, measuring, monitoring, controlling, and reporting to ensure that software products and software engineering services are delivered efficiently, effectively, and to the benefit of stakeholders, whereas software project planning (SPP) addresses the activities undertaken to prepare for a successful software engineering project from the management perspective (Bourque and Fairley, 2014). Software prediction (SP) belongs to SPP. SP has been applied to variables related to software projects such as size, effort, duration or defects. The types of software projects can be classified as new or enhancement (ISBSG, 2018).

Machine learning techniques have been applied in the SP field such as cased-based reasoning (CBR, also termed analogy based estimation (Bardsiri et al., 2014; Idri et al., 2015)), neural networks (NN), support vector regression (SVR), genetic programming, genetic algorithms, decision trees, and association rules (Afzal and Torkara, 2011; Wen, et al., 2012; Gautam and Singh, 2018;). As reference, in the software effort prediction field, CBR has been the technique reported as the most applied, whereas SVR and NN have been reported with the best prediction accuracy (Wen et al., 2012). A SVR is a type of support vector machine (SVM) having regression as purpose (Garc√≠a-Floriano et al., 2018.). CBR, NN and SVR are useful to learn from non-linear relationships (L√≥pez-Mart√≠n, 2015; Vapnik, 1998; Cortes and Vapnik, 1995) and non-linear relationships are common between software metrics and software defects (Kumar, 2013). Systematic reviews on CBR studies showed that the NN and SVR performances have commonly been compared to that of CBR (Bardsiri et al., 2014; Idri et al., 2015).

Since the CBR has outperformed other prediction models, that the application of CBR is still limited, and that its application has been recommended for SP (Idri et al., 2015), in the present study, we propose the application of CBR for predicting the DD of software projects. CBR includes as components to the similarity function, the associated retrieval rules, and solution function (Bardsiri et al., 2014). We propose a novel transformation for retrieving the correct output, by minimizing the distance among the outputs of the k-nearest neighborhoods of the project we want to predict. In addition, we suggest a strategy to predict the correct output, and we introduce an inverse transformation as solution function. Our proposed algorithm is based on the concepts of transformation (Poularikas, 2010), and minimization (Walser, 1999). We have termed to our algorithm as transformed k-nearest neighborhood output distance minimization (TkDM).

The k-nearest neighborhood does not tend commonly to have good performances for regression purposes. Due to the differences in the outputs for regression, and to the difficulty of finding adequate distance functions, we assuming that applying mathematical transformations to the outputs may contribute to solving this drawback.

In CBR, the k-nearest neighborhood corresponds to the number of similar projects selected to be involved in the CBR prediction process. The search of those k software projects has been an important issue in software prediction field since the k value can significantly affect the prediction accuracy (Bardsiri et al., 2014). A systematic review on CBR revealed that 42% of the studies defined a fixed number for k, 37% of them determined a range to find the best number for k, whereas in 21% of the studies, a dynamic selection of k was performed (Bardsiri et al., 2014). In our study, the k value coincides to that type of studies which a range is determined and tried to find the best number for k.

In relating the CBR process (Bardsiri et al., 2014) to our study: (1) since DD differs between types of development (L√≥pez-Mart√≠n, 2019), we involve separated historical data sets of new and enhancement software projects, (2) variables from them are selected from those data sets to be compared, and (2) we retrieve projects similar to x project, and predict the DD for that x project.

Software defect prediction models have been used to (1) identify software metrics, modules, classes, files or subsystems that are more likely to be defective (defect-prone) (Choudhary, et al., 2018; Malhotra, 2015), (2) predict the number of defects (Marinescu, 2014; Rathore and Kumar, 2017; Ostrand et al., 2005), and (3) predict DD (L√≥pez-Mart√≠n et al., 2018; Verma and Kumar, 2017; Kumar et al., 2013; Mandhanet al., 2015; Yadav and Yadav 2015; Rahmani and Khazanchi, 2010; Kutlubay et al., 2007; Knab et al., 2006; Nagappan and Ball, 2005; Sherriff et al., 2005).

Regarding DD, we identified studies on (1) data analysis (Li et al., 2017; Tengeri et al., 2016; Yamashita et al., 2016; Nugroho and Chaudron, 2014; Faller and Zhu 2013; Garousi et al., 2013; Shah et al., 2012a; Shah et al., 2012b; Hasim and Rahman, 2011), (2) reduction (Verma and Kumar, 2014), and (3) definitions (Shah et al., 2013). As for DD prediction, the techniques applied have been support vector regression (L√≥pez-Mart√≠n, et al., 2018), neural networks (Kumar et al., 2013; Kutlubay, et al., 2007), fuzzy logic (Kumar et al., 2013; Yadav and Yadav, 2015), decision trees (Kutlubay et al., 2007; Knab et al., 2006), and statistical regressions (Verma and Kumar, 2017; Mandhan et al., 2015; Rahmani and Khazanchi, 2010; Nagappan and Ball, 2005; Sherriff et al., 2005).

The contribution of our study is the application of the TkDM algorithm to predict the DD in new and enhancement projects using FP as the independent variable. The TkDM prediction accuracy is compared to those obtained from the application of two types of SVR (i.e., Œµ-SVR, and  ã-SVR) and four types of NNs (i.e., multilayer backpropagation neural network (MLP), radial basis function neural network (RBFNN), general regression neural network (GRNN), and cascade correlation neural network (CCNN)). As for Œµ-SVR and  ã-SVR, four types of kernels were used by type of SVR (i.e., linear, polynomial, radial basis function, and sigmoid). Moreover, since the prediction accuracy of a new proposed prediction model should at least outperform that of a statistical regression (Kitchenham and Mendes, 2009), the prediction accuracies obtained from the TkDM, Œµ-SVR,  ã-SVR, MLP, RBFNN, GRNN, and CCNN models are also compared to that of a simple linear regression (SLR).

In the present study, the prediction models are applied to software projects obtained from the International Software Benchmarking Standards Group (ISBSG) Release 2018. The data sets are selected following the ISBSG guidelines by taking into account the quality data, type of development, development platform, and programming language type (ISBSG Guidelines, 2018).

A systematic review on CBR considered to the wide diversity of criteria used such as data sets, metrics, and evaluation methods, as the most important limitation to compare the prediction accuracy among CBR and other types of models (Bardsiri et al., 2014), thus, in the experiments of our study, we used the same data sets, dependent and independent variables, and validation method.

The hypothesis to be investigated in our research is the following:

H1:The DD prediction accuracy with the TkDM is statistically better than the accuracy obtained by SVR, NN, and SLR when function points is used as the independent variable.

Section 2 presents studies where DD has been predicted. Section 3 describes the criteria to evaluate the prediction accuracy of prediction models. Section 4 describes the TkDM, Œµ-SVR,  ã-SVR, MLP, RBFNN, GRNN, and CCNN used to predict DD. Section 5 presents the criteria followed to select the data sets of software projects. Section 6 details how the models are trained and tested. Section 7 includes the results obtained from the models, whereas Section 8 presents the discussion including our conclusions, limitations, and future work.

2. Related work
Ten studies related to DD prediction were identified. We classified them into three categories: (1) studies analyzing the relationship between metrics (i.e., independent variables) and DD by means of statistical methods such as coefficients of correlation and determination, as well as linear or multiple linear regression models, (2) studies comparing the prediction accuracy between two machine learning models, and (3) studies applying a machine learning model. They are described next.

2.1. Studies that analyze the correlation between DD and metrics
Verma and Kumar (2017) analyze the relationship between DD and five metrics obtained from 62 open source software projects. The metrics are size (lines of code), defects, number of developers, number of downloads, and commits. The authors formulate six hypotheses (1) software size has a negative relationship with DD, (2) number of defects has a positive relationship with DD, (3) more number of developers has lower DD, (4) more downloads increase the DD, (5) number of commits has a positive relationship with DD, and (6) software size, number of defects, number of developers, number of downloads and number of commit are jointly related to DD. Their analysis is based on statistically significance of the coefficient of determination. As conclusions, the first, second, third and sixth hypotheses were accepted.

Mandhan et al. (2015) analyze the relationship between DD and seven class level metrics obtained from 145 software projects. These metrics are coupling, depth, cohesion, response, weighted methods, comments, and lines of code. Their analysis is based on statistically significance of the coefficient of determination. They conclude that there is a significant level of acceptance for DD prediction with these static metrics individually and jointly.

Rahmani and Khazanchi (2010) analyze the relationship between DD and three metrics obtained from 44 open source software projects. The metrics are software size, number of downloads, and number of developers. The authors formulate four hypotheses (1) software size is positively correlated with DD, (2) number of developers are positively correlated with DD, (3) the number of downloads is positively correlated with DD, and (4) software size, and number of developers jointly have a relationship with DD. Their analysis is based on statistically significance of the coefficient of determination. As conclusions, the second and fourth hypotheses were accepted.

Nagappan and Ball (2005) propose eight relative code churn metrics. Data were obtained from 2465 binaries compiled from 96,189 files (in some cases, one binary consists of more than one file). Data were obtained from a large project. A hold-out was used: 1645 and 820 binaries were used to train and test, respectively. Their analysis is based on Pearson and Spearman correlation coefficients by correlating actual versus predicted DD. Results show all correlations positive and statistically significant. They conclude that their proposed measures can be used as efficient predictors of DD.

Sherriff et al. (2005) propose five metrics. The validation method used was hold-out: 14 and 6 projects were used to train and test, respectively. They present a scatter plot showing actual versus predicted DD by project. They conclude that the resulting DD prediction is indicative of the actual system DD.

2.2. Studies that compare the prediction accuracy between two machine learning models
Kumar et al. (2013) apply fuzzy logic (FL) and MLP to predict the DD of subsequent product releases of two software projects. The three independent variables used are complexity, total lines of code, and pre-release defects. The accuracy criteria for comparing the prediction of models were mean absolute residual (MAR) and root mean square error (RMSE). They conclude that the MLP provides better results than the fuzzy logic system.

Kutlubay et al. (2007) classify modules obtained from nine NASA data sets as defective and defect-free, and then they selected the defective modules to apply RBFNN and decision tree (DT) to predict DD. The validation method used was k-fold (k=10). The accuracy criterion for comparing the prediction of models was Root Mean Square Error (RMSE). They conclude that the accuracy of DT was statistically better than that of RBFNN for the nine data sets.

2.3. Studies that apply a machine learning model
Knab et al. (2006) use a DT to predict DD in source code files of seven releases of an open source web browser project. Sixteen metrics are analyzed. The validation method used was k-fold (k=10). They did not compare the DT prediction accuracy to other prediction model. They conclude that (1) metrics from the same release, and evolution data (such as the number of modification reports), are useful for DD prediction, and (2) lines of code, number of functions, and change couplings have little predictive power with regard to DD.

Yadav and Yadav (2015) apply a FL model for predicting DD at each phase of development life cycle of 20 projects from the top most reliability relevant metrics by design, coding, and testing phase. The accuracy criteria for comparing the prediction of models were Mean Magnitude of Relative Error (MMRE) and Balanced Mean Magnitude of Relative Error (BMMRE). They conclude that the predicted DD are found very near to the actual defects detected during testing.

L√≥pez-Mart√≠n et al. (2018) apply two types of SVR termed Œµ-support vector regression (Œµ-SVR) and  ã-support vector regression ( ã-SVR) to a set of 21 software projects also used in the present study. The prediction accuracy of the mentioned SVRs was compared to that of statistical regression. Based on a statistical significance test, they conclude that the  ã-SVR with polynomial kernel was better than that of statistical regression when new software projects were developed on mainframes and coded in programming languages of third generation.

After the analysis of the previous ten studies, we can identified that five of them analyze the correlation between DD and proposed metrics, two studies apply two machine learning models and compare their prediction accuracy to each other (one of them MLP and RBFNN, and the second one FL and MLP), and the remaining three studies, a machine learning model is applied by study (DT, FL, and SVR), and in two of these latter three studies, the prediction accuracy of each model is compared regarding either releases of a project, or development life cycle phase.

In our study, we propose the application of the TkDM algorithm to predict the DD of new and enhancement projects using FP as the independent variable. A leave-one-out cross validation method (LOOCV) is used to train and test the models, whereas the prediction accuracy for evaluate the performance of models is absolute residuals (AR).

3. Accuracy criteria
Absolute residuals (AR) has been recommended over others accuracy criteria such as magnitude of relative error (MRE) or the magnitude of error relative to the estimate (MER) since AR is a non-asymmetric measure (Shepperd and MacDonell, 2012). Thus, in our study we used the AR as criterion to evaluate the accuracy prediction of models. The equation AR is the following:

The AR are calculated for each observation i the DD of which are predicted. The aggregation of the AR over multiple observations (N) can be obtained by their mean (MAR) as shown as follows:

The MdAR correspond to the median of the ARs. The accuracy of a prediction model is inversely proportional to MAR.

In addition to MAR, and MdAR, we used two accuracy measures: standardized accuracy (SA) and effect size (Œî). The SA examines whether the prediction model generates predictions better than random guessing, whereas the Œî is used to examine that the predictions are not produced by chance. The value of Œî is recommended to be larger or equal to 0.5. SA and Œî are calculated as follows (Shepperd and MacDonell, 2012):
 
 
 
 

4. Description of prediction models
4.1. Transformed k-nearest neighborhood output distance minimization
This algorithm is described from formulated assumptions, steps and based on a LOOCV.

Let X = set of FPs, Y = set of DDs, FPt = a testing value for a project in X, and DDt = that testing value in DD corresponding to FPt, then:

Assumption 1: To near values in X, the corresponding values in Y are also near.

Step 1: To select a k positive integer whose value is smaller than the data size of X.

Step 2: To find in X those k-nearest neighborhood near to FPt. These k-nearest neighborhood are termed as xi,‚Äâ1 ‚â§ i ‚â§ k, and their corresponding values in Y as yi,‚Äâ1 ‚â§ i ‚â§ k.

Assumption 2: Based on elementary mathematical operations, it is possible to apply a T transform to yi,‚Äâ1 ‚â§ i ‚â§ k such that the maximum distance between pairs from the k transformed values yT(i),‚Äâ1 ‚â§ i ‚â§ k is smaller than a PV predetermined value.

Step 3: To apply the T transform to yi,‚Äâ1 ‚â§ i ‚â§ k, values for obtaining the k transformed values as follows:
 
Where a y b are parameters to be adjusted to PV.

Step 4: To find the a y b parameter values by means of a seeding algorithm such that they allow that the maximum distance between pairs of the k transformed values yT(i),‚Äâ1 ‚â§ i ‚â§ k is smaller than PV.

Assumption 3: The mean obtained from the k transformed values yT(i),‚Äâ1 ‚â§ i ‚â§ k is an acceptable estimation for the transformed value of DDt, which corresponds to FPt.

Step 5: To calculate the mean from those k transformed values yT(i),‚Äâ1 ‚â§ i ‚â§ k as follows:
 

Step 6: Based on the expressions obtained from the steps 3 and 5, and setting fixed values for the a y b parameters, it is possible to obtain the P predicted value for DDt from the values FPt and mean(yT(i)). This is achieved by means of elementary algebraic operations obtaining the inverse transform as follows:
 

By finding P:
 

Step 7: To calculate the absolute residual (AR) by applying the following expression:

Step 8: To calculate MAR from all ARs.

4.2. Support vector regression (SVR)
In SVM, the optimization problem implies to find the maximum margin separating a hyperplane by correctly classify the training points as possible. An SVM model represents this optimal hyperplane with support vectors. An SVR, a type of SVM that can be applicable to regression problems, is characterized by the use of kernels, sparse solution, and Vapnik-Chervonenkis control of the margin and the number of support vectors (Awad and Khanna, 2015).

An SVM has a set of slack variables ùìî
, which correspond to the distance of the input vector xi from the decision hyperplane. These slack variables are associated with a parameter C, where C>0, which is used to control the over-training (Garc√≠a-Floriano et al., 2018).

SVR is trained based on a symmetrical loss function (i.e., Vapnik's Œµ-insensitive), which equally penalizes high and low misestimates. This loss function corresponds to Œµ-insensitive of Vapnik, which a flexible tube of minimal radius is formed symmetrically around the estimated function, such that the absolute values of errors less than a certain threshold Œµ are ignored both above and below the estimate. It permits that points outside the tube are penalized, but those points within the tube, either above or below the function, receive no penalty (Awad and Khanna, 2015).

An SVR finds a function f(x) that has most Œµ deviation from the actually obtained target yi for the training data xi, and at the same time is as flat as possible (Ma and Guo, 2014).

There are two types of SVR named Œµ-support vector regression (Œµ-SVR) and  ã-support vector regression ( ã-SVR) (Chang and Lin, 2011; Smola and Sch√∂lkopf, 2004; Sch√∂lkopf et al., 2000). The Œµ-insensitive loss function is the base for Œµ-SVR prediction. It is minimized by the  ã-SVR, which uses other  ã parameter having as possible values the [0,1] interval. The number of support vectors is also controlled by the  ã parameter, that is, the  ã-SVR compresses data and generalizes the prediction error bounds. Thus, there are two parameters Œµ and C in Œµ-SVR, and  ã-SVR has the two parameters  ã and C.

The kernel functions for Œµ-SVR and  ã-SVR are the following (Vapnik, 1998; Cortes and Vapnik, 1995):

¬≠
Linear: , where x and z are data patterns

¬≠
Polynomial: 
, where Œ≥: slope parameter, c0: trade-off between major terms and minor terms of the generated polynoms, d: polynom degree, and x,z: data patterns

¬≠
Radial basis function (RBF): 
, where Œ≥ controls the radial basis function spread, and x,z: data patterns.

¬≠
Sigmoid: 
, where Œ≥ controls the radial base function spread, c0: independent term, and x,z: data patterns

4.3. Multilayer backpropagation neural network (MLP)
A MLP is a traditional feedforward neural network (Werbos, 1974; Rumelhart et al., 1985; Reed and Marks, 1999). The structure of a MLP involves an input layer to perceive the signal, at least one hidden layer that represents the computational engine of the MLP, and an output layer to make a prediction. The nodes represent neurons with logistic activation. Fig. 1 shows an MLP network where neurons of one layer used as input for neurons on the next layer (Riedmiller and Lernen, 2014). After a network receives the input, the neurons‚Äô layers process the signal and the output layer generates the output. The signal is processed by neurons in parallel within the same layer. The signal flows from the input layer to the output layer.

Fig. 1
Download : Download high-res image (162KB)
Download : Download full-size image
Fig. 1. Multilayer Perception Netwrok (Riedmiller and Lernen, 2014).

The training phase for an MLP network works by adjusting the bias and weights parameters in order to maximize some measure of fit to the training data. During training, the network is processing the input and changing the parameters using learning algorithms through several iterations, in order to enhance its performance (Riedmiller and Lernen, 2014). Back-Propagation (BP) algorithm which is a variation of gradient decent learning is one of the most popular methods for MLP training (Rumelhart et al., 1985). The conjugate gradient algorithm is another algorithm which can be employed to train the network (Nassif et al., 2016; M√∏ller, 1993).

4.4. Radial basis function neural network (RBFNN)
An RBFNN network is a feedforward artificial neural network whose structure contains an input layer, a hidden layer, and an output layer (Haykin, 1999). Fig. 2 shows the architecture of an RBFNN network (Orr, 1996). Each component of the input vector feeds to m basis functions in the hidden layer. The outputs of the basis functions are combined with weights into the output f(x). An RBFNN network mostly employs the Gaussian function and it computes the distance from the center Ci to the input X. Each RBF function has a radius denoted by œÉ. Each neuron may have a different radius. The RBF function is shown in Eq. (1).(1)
 

Fig. 2
Download : Download high-res image (161KB)
Download : Download full-size image
Fig. 2. The traditional RBFNN Netwrok (M√∏ller, 1993).

Where Ci represents center and œÉi represents the radius of the ith neuron. The Euclidean distance is usually used as the distance between the input X and the center Ci. The RBFNN models are known to be less sensitive to noise and usually faster than other neural networks (M√∏ller, 1993).

4.5. General regression neural network (GRNN)
A GRNN is a variation to RBFNN (Specht, 1991). It contains an input layer, two hidden layers, and an output layer. Fig. 3 shows the structure of GRNN. The first hidden layer takes input from the input neurons and is consisted of pattern neurons. Each neuron applies an RBF function to compute the distance between the input X and the center Ci. The output of the first hidden layer feeds into the second hidden layer (summation neurons). The second hidden layer is composed of the summation neurons namely the denominator and the numerator. The denominator summation adds the values of the weights fed from each of the first hidden layer neurons while the numerator adds the weights multiplied by the output values of the pattern neurons. The output neuron computes the predicted output value by dividing the numerator neuron's value by the denominator neuron's value (M√∏ller, 1993).

Fig. 3
Download : Download high-res image (209KB)
Download : Download full-size image
Fig. 3. The GRNN Netwrok (Galorath and Evans, 2006).

4.6. Cascade correlation neural network (CCNN)
In a CCNN, the learning process begins with no hidden units. A connection with an adjustable weight is added between every input and every output units (Fahlman and Lebiere, 1990). The hidden units are added to the CCNN network one at a time in order to reduce the residual errors. Each new hidden unit obtains a connection from each input unit and from each hidden unit. The input weights for the hidden unit are frozen when the unit is added to the CCNN network. Each hidden unit adds one layer to the network; creating high-order feature detectors. The learning process repeats up until the error becomes small. The structure of CCNN is shown in Fig. 4. There is a bias input that is set to + 1. CCNN learns quickly, does not require backpropagation, and can build deep nets faster than deep backpropagation networks (Fahlman and Lebiere, 1990).

Fig. 4
Download : Download high-res image (116KB)
Download : Download full-size image
Fig. 4. The CCNN Netwrok with two hidden units (Fahlman and Lebiere, 1990).

5. Data sets of software projects
We searched available data related to our study. There are two main repositories commonly used in the software engineering field: PROMISE (PROMISE, 2019) and ISBSG (Fern√°ndez-Diego and Gonz√°lez-Ladr√≥n-de-Guevara, 2014). We analyzed the attributes of the PROMISE data sets with the objective of finding any attribute related to defects. In CM1, JM1, KC1, KC2, PC1, DATATRIEVE, and Class-level data for KC1 data sets, a defect categorical attribute is reported (i.e. false or true); Nickle, XFree86, and X.org do not report any attribute related to defects; MODIS, CM1, and QoS are data sets related to software requirements rather than to defects; whereas Desharnais, Cocomo81, Cocomo NASA, and COCOMO NASA 2 do not provide data regarding defects but of development effort.

As for ISBSG, it contains an attribute specifically related to DD by software project. In our study, FP is the independent variable used to predict DD. FP is obtained by calculating the internal logical files, and external interface files, as well as external inputs, external outputs, and external inquiries of the software project (L√≥pez-Mart√≠n, 2015), whereas the dependent variable is DD, which is defined as defects by 1000 FP and calculated as follows (ISBSG Field Descriptions, 2018):

DD is defined as the number of defects by 1000 functional size units of delivered software in the first month of use of the software. It is expressed as defects by 1000 function points.

The software projects used in our study were obtained from the public ISBSG data set Release 2018. This release contains 8,261projects developed between the years 1989 and 2016. The data of these projects were submitted to the ISBSG from 26 different countries. Among them are United States, Spain, Australia, Japan, Netherlands, Finland, France, India, Canada and Denmark (ISBSG Demographics, 2018). The type of organization for which projects were developed are Communication, Insurance, Manufacturing, Government, Banking, Medical & health care, Financial, Electronics, Service industry, retail, and Professional services.

ISBSG release 2018 contains four types of developments for software projects: new, enhanced, re-development, and migration (ISBSG Field Descriptions, 2018). In accordance with the ISBSG Guidelines for use of the ISBSG data (ISBSG Guidelines, 2018), data sets should be selected taking into account their data quality rating, unadjusted function point rating, type of development, development platform, programming language type, as well as functional sizing method. Table 1 shows the number of projects as selected from the first three mentioned guidelines (A and B mean higher quality), and Table 2 according to the remaining guidelines. Pre-IFPUG V4 projects should not be mixed with V4 and post V4, and NESMA can only be mixed with IFPUG V4+ (ISBSG Guidelines, 2018).


Table 1. Software projects selected from the ISBSG Guidelines.

Atribute	Selected value(s)	Number of projects
Data quality rating	A and B	7,780
Unadjusted function point rating	A and B	6,429
Defect density	Not null	669
Type of development	New	211
Enhanced	440
Re-development	18

Table 2. Criteria for selecting software projects according their development platform (DP), language type (LT), and functional sizing methods (FSM).

Attribute	Selected value(s)	Number of projects
New	Enhanced	Re-development
DP	Not null	170	363	18
LT	Not null	167	361	18
FSM	IFPUG V4+ and NESMA	140	341	18
Two of the final 140 new software projects of Table 2 were excluded because they had a defect density value lower than one (zero and 0.1 values). Table 3 classifies the software projects of Table 2 based on their develop platform (MF: mainframe, MR: midrange, Multi: multiplatform, PC: personal computer) and language type (3GL and 4GL: third and fourth generations, respectively, ApG: application generator).


Table 3. Projects classified by development platform and language type (DP: development platform, LT: language type).

DP	LT	Number of projects
New	Enhanced	Re-development
MF	3GL	24	77	3
4GL	8	12	2
ApG	4	36	0
MR	3GL	11	45	0
4GL	10	11	0
Multi	3GL	31	78	7
4GL	13	33	2
PC	3GL	11	32	2
4GL	26	15	2
ApG	0	2	0
Total		138	341	18
In our study, with the goal of obtaining a better generalization from our conclusions based on statistical significance, those nine data sets from Table 3 higher than twenty projects were selected to be analyzed. A scatter plot (FP vs. DD) was generated by data set. The nine scatter plots showed skewness (i.e., there were fewer large projects than small projects), heteroscedasticity (i.e., the variability of DD decreased with FP), and outliers. Table 4 includes four statistical distribution tests commonly used to data normality applied for dependent and independent variables by data set (the Chi-squared test was not possible apply it to four datasets because it needs at least thirty data).


Table 4. Normal statistical tests by data set (DP: development platform, LT: language type, NP: number of projects, FP: function points, DD: defect density).

Type of development	DP	LT	NP	Variable	Statistical test
Chi-squared	Shapiro-Wilk	Skewness	Kurtosis
New	MF	3GL	24	FP	‚Äî	0.0006	0.0223	0.0132
DD	‚Äî	0.0017	0.0461	0.0606
Multi	3GL	31	FP	0.0000	0.0000	0.0264	0.2732
DD	0.0000	0.0000	0.0000	0.0000
PC	4GL	26	FP	‚Äî	0.0000	0.0018	0.0005
DD	‚Äî	0.0000	0.0002	0.0000
Enhancement	MF	3GL	77	FP	0.0000	0.0000	0.0016	0.0269
DD	0.0000	0.0000	0.0004	0.0020
MF	ApG	36	FP	0.0000	0.0000	0.0035	0.0015
DD	0.0000	0.0000	0.0000	0.0000
MR	3GL	45	FP	0.0203	0.0009	0.1471	0.5812
DD	0.0000	0.0000	0.0000	0.0000
Multi	3GL	78	FP	0.0009	0.0000	0.0001	0.0000
DD	0.0000	0.0000	0.0000	0.0000
Multi	4GL	33	FP	0.0005	0.0000	0.0137	0.0146
DD	0.0075	0.0033	0.1707	0.6547
PC	3GL	32	FP	0.0000	0.0000	0.0001	0.0000
DD	0.0005	0.0000	0.0014	0.0002
Since in Table 4 the smallest p-value amongst the tests performed is less than 0.01 for the eighteen cases, we can reject the idea that the variables come from a normal distribution with 99% confidence. Thus, the natural logarithm (ln) was applied to data of the nine datasets for normalizing them as suggested by Kitchenham and Mendes (2009). In Table 5, outliers were identified by means of studentized residuals greater than 2 in absolute value.


Table 5. Outliers analysis by data set (DP: development platform, LT: language type, NP: number of projects).

Type of development	DP	LT	NP	Outliers	NP final	Statistics without outliers	Simple linear regression equation
Number	Percentage	r	p-value r	r2
New	MF	3GL	24	3	0.13	21	-0.80	0.0000	0.64	ln(DD) = 6.42 - 0.79*ln(FP)
Multi	3GL	31	13	0.42	18	-0.16	0.5150	0.02	‚Äî
PC	4GL	26	8	0.31	18	-0.22	0.3782	0.04	‚Äî
Enhancement	MF	3GL	77	7	0.09	70	-0.83	0.0000	0.70	ln(DD) = 6.86 - 0.84*ln(FP)
MF	ApG	36	4	0.11	32	-0.32	0.0690	0.10	‚Äî
MR	3GL	45	3	0.07	42	-0.74	0.0000	0.55	ln(DD) = 8.18 - 1.07*ln(FP)
Multi	3GL	78	15	0.19	63	-0.71	0.0000	0.51	‚Äî
Multi	4GL	33	4	0.12	29	-0.77	0.0000	0.59	ln(DD) = 6.05 - 0.65*ln(FP)
PC	3GL	32	16	0.50	16	-0.98	0.0000	0.97	‚Äî
Regarding coefficient of correlation (r), tree data sets of Table 5 present a p-value higher than 0.05, thus, they did not presented statistically significant non-zero correlation at the 95% confidence level when outliers were removed. Two data sets had a higher percentage of outliers (one around the fifth part, and the second one corresponding to the half of the data set). Therefore, in our study, a data set of 21 new projects, and three data sets of 70, 42 and 29 enhancement projects are used to train and test the DD prediction models. Table 5 includes four simple linear regression equations corresponding to these four data sets. The ANOVA p-value for these four equations was equal to 0.0000, that is, there is a statistically significant relationship between the variables at the 99% confidence level. As for coefficient of determination (r2), it has values between 0.55 and 0.70, which means that the four SLRs as fitted explain between 55% and 70% of the variability in the DD.

Therefore, in our study the following four data sets are used:

¬≠
Twenty-four new software projects developed in MF and coded in 3GL.

¬≠
Seventy software enhancement projects developed in MF and coded in 3GL.

¬≠
Forty-four software enhancement projects developed in MR and coded in 3GL.

¬≠
Forty-four software enhancement projects developed in Multi and coded in 4GL

6. Training and testing the models
Holdout, leave-one-out cross-validation (LOOCV), and k-fold cross-validation (k > 1) are the validation methods majorly used in the software prediction field when machine learning models have been applied (Wen et al., 2012). Since LOOCV is a deterministic procedure which can be exactly reproduced by any other researcher with the same particular data set (Gautam and Singh, 2018), we used the LOOCV to train and test the DD prediction models.

The TkDM was trained and tested in accordance with the algorithm described in the Section 4.1 ‚ÄúTransform k-nearest neighborhood unidimensional minimization‚Äù of this study. We experimentally found that the number of k = 6 was that value having best results when predicted DD for the four data sets.

As for statistical regressions, since data sets with 21, 70, 42 and 29 software projects were used, then 21, 70, 42 and 29 SLRs of the type ln(DD) = a - b*ln(FP) were generated. In the following subsections, the procedures to train and test the SVR, and the four types of NNs are described.

Regarding SVR, Table 6 shows the final parameters by type of SVR that generated the best accuracy in its data set. A  ã-SVR had the best accuracy for the four data sets. A polynomial kernel having as expression Œ≥xix+coef0degree) had the best accuracy in three cases, and a linear kernel in the fourth one. If Coefficient coef0 is equal to zero, then the kernel is homogeneous. The shrinking heuristic had ‚Äúyes‚Äù as value for the four cases, it means that it saves training time by identifying and removing some bounded elements (like C or Œ±i = 0), which leads to a smaller optimization problem (Fan et al., 2005).


Table 6.  ã-SVR parameter values by type of support vector regression.

Parameter	Data set size
21	70	42	29
Kernel Type	Polynomial	Polynomial	Linear	Polynomial
Œ≥	0.2	0.0	‚Äî	0.0
coef0	0	0.08	‚Äî	0.0
Degree	2	2	‚Äî	2
C	1	1	0.85	1
 ã	0.09	0.4	0.9	0.5
In accordance with neural networks, in the MLP one hidden layer with one hidden neuron generated the best accuracy in all the datasets. The activation function used in the hidden layer was logistic, whereas a linear function was used in the output neuron. The conjugate gradient algorithm was used in training the model.

As for RBFNN, two hidden neurons were used for the data set with size equal to 21, where six, four and three hidden neurons were used for data sets with 70, 42 and 29 projects, respectively.

Regarding GRNN, the sigma value for the predictor for the data set with size equal to 21 was 0.83, whereas for the predictor in data sets having 70, 42 and 29 projects were 0.14, 0.48 and 0.38, respectively.

In the CCNN, the number of neurons in the input, hidden and output layers were 1, 0, and 1, respectively for data sets with 21 and 29 projects, whereas the number of neurons in the input, hidden and output layers were 1, 1, and 1, respectively for data sets with 70 and 42 projects.

7. Results
The MAR and MdAR obtained by model are shown in Table 7. Since conclusions related to software prediction should be based on statistically significance (Kitchenham and Mendes, 2009), a suitable statistical test is selected based on the number of data sets to be compared, data dependence, and data distribution (Ross, 2004). In our study, the ARs of two data sets at the same time will be compared (one from TkDM and one from each other prediction model), data are dependent because DD by project was predicted applying all the models. As for data distribution, normality tests by a new data set obtained from the differences between ARs obtained between the two models compared at time are performed. Table 8 includes the results of normality statistical tests applied by new data set. If the smallest p-value amongst the tests performed for the data sets is less than 0.05, we can reject the idea that each data set of differences comes from a normal distribution with 95% confidence, and the comparison between the accuracy of the two models should be based on their medians (i.e., based on Wilcoxon test), otherwise on their means (i.e., based on t-paired test). Table 9 includes the p-values obtained by comparing SLR accuracy with that of each model.


Table 7. Accuracy results for prediction models.

Data set	Model	MAR	MdAR	SA	Œî
21	TkDM	0.29	0.30	67.3	0.98
SLR	0.42	0.49	52.9	0.85
 ã-SVR	0.36	0.40	60.0	0.97
MLP	0.45	0.52	49.5	0.80
RBFNN	0.49	0.52	44.5	0.72
GRNN	0.47	0.39	47.3	0.76
CCNN	0.45	0.40	49.7	0.80
70	TkDM	0.44	0.40	65.1	0.98
SLR	0.54	0.55	56.9	0.88
 ã-SVR	0.49	0.52	61.0	0.94
MLP	0.55	0.56	56.9	0.87
RBFNN	0.53	0.50	58.3	0.89
GRNN	0.54	0.48	57.2	0.87
CCNN	0.56	0.51	55.6	0.85
42	TkDM	0.41	0.33	66.0	0.99
SLR	0.59	0.53	51.2	0.81
 ã-SVR	0.57	0.60	53.1	0.84
MLP	0.66	0.64	45.7	0.72
RBFNN	1.03	0.61	12.7	0.25
GRNN	0.63	0.52	48.4	0.76
CCNN	0.63	0.48	48.0	0.76
29	TkDM	0.34	0.27	69.3	0.99
SLR	0.55	0.45	51.8	0.84
 ã-SVR	0.51	0.49	55.3	0.90
MLP	0.64	0.56	43.9	0.72
RBFNN	0.79	0.61	30.9	0.50
GRNN	0.60	0.48	49.6	0.78
CCNN	0.54	0.54	49.4	0.80

Table 8. Normality distribution test results (p-values) by data set (ID: Insufficient data for performing statistical test).

Data set	Model compared to TkDM	Normality test
Chi-Squared	Shapiro-Wilk	Skewness	Kurtosis
21	SLR	ID	0.4130	0.3773	0.9037
 ã-SVR	ID	0.4751	0.6028	0.6934
MLP	ID	0.3477	0.6625	0.3009
RBFNN	ID	0.1639	0.8014	0.0457
GRNN	ID	0.6455	0.5537	0.9620
CCNN	ID	0.4582	0.4289	0.9293
70	SLR	0.2694	0.4982	0.6292	0.7923
 ã-SVR	0.4042	0.3348	0.9828	0.0925
MLP	0.5646	0.4683	0.5680	0.9339
RBFNN	0.0094	0.3624	0.6320	0.2746
GRNN	0.0997	0.2184	0.6244	0.4384
CCNN	0.3675	0.2203	0.2072	0.1986
42	SLR	0.0411	0.4407	0.8632	0.5595
 ã-SVR	0.0642	0.5637	0.8416	0.7548
MLP	0.3463	0.0200	0.0958	0.3596
RBFNN	0.0000	0.0000	0.0000	0.0000
GRNN	0.0203	0.0596	0.3093	0.9554
CCNN	0.0642	0.0333	0.1569	0.2531
29	SLR	ID	0.3654	0.3852	0.3788
 ã-SVR	ID	0.7436	0.9581	0.6403
MLP	ID	0.9115	0.9699	0.8439
RBFNN	ID	0.0000	0.0000	0.0000
GRNN	ID	0.6197	0.5614	0.5778
CCNN	ID	0.5475	0.4704	0.6080

Table 9. Statistical test results by comparing the SLR accuracy with accuracy of each model.

Data set	Model compared to TkDM	Statistical test	p-value
21	SLR	t-paired	0.0152
 ã-SVR	t-paired	0.0497
MLP	t-paired	0.0079
RBFNN	Wilcoxon	0.0091
GRNN	t-paired	0.0173
CCNN	t-paired	0.0378
70	SLR	t-paired	0.0568
 ã-SVR	t-paired	0.0499
MLP	t-paired	0.0465
RBFNN	Wilcoxon	0.1325
GRNN	t-paired	0.0404
CCNN	t-paired	0.0292
42	SLR	Wilcoxon	0.0172
 ã-SVR	t-paired	0.0241
MLP	Wilcoxon	0.0003
RBFNN	Wilcoxon	0.0029
GRNN	Wilcoxon	0.0024
CCNN	Wilcoxon	0.0000
29	SLR	t-paired	0.0042
 ã-SVR	t-paired	0.0223
MLP	t-paired	0.0027
RBFNN	Wilcoxon	0.0006
GRNN	t-paired	0.0027
CCNN	t-paired	0.0245
In addition, the results showed in Table 7 on SA and effect size show the meaningful of the prediction. In other words, it can display how better the model prediction rather than random guessing. The results of SA over all datasets confirm that not all models are predicting well. We can then notice that TkDM is the superior model amongst all models, over all datasets. Also, the MAR accuracy measure tells us that the TkDM is the best model with less average of absolute residuals.

8. Discussion
Software defect prediction is one of the most important research topics in software engineering (Li et al. 2018), and DD is a software quality attribute, which gives the reliability measure of the software product (Kumar et al., 2013). Therefore, we proposed the application of a transform k-nearest neighborhood unidimensional minimization (TkDM) to predict the DD of new and enhancement projects. The TkDM prediction accuracy based on AR, standardized accuracy (SA), and effect size (Œî) was compared to those obtained from two types of SVR termed (i.e. Œµ-SVR and  ã-SVR, each type used linear, polynomial, radial basis function, and sigmoid kernels), and four types of neural networks (i.e., MLP, RBFNN, GRNN, and CCNN)

Since DD varies according to properties such as project type (Raja and Tretter, 2009), the data set of projects selected for our study observed the ISBSG Guidelines, which suggest the type of project as one of their criteria taking into account their data quality rating, unadjusted function point rating, functional sizing method, type of development, development platform, and programming language generation.

In accordance with Tables 7 and 9, the following hypotheses derived from that formulated in the Introduction section of this study, can be accepted at 95% confidence level when function points is used as the independent variable:

H1:The DD prediction accuracy of new software projects developed in MF and coded in 3GL with TkDM is statistically better than the accuracies obtained by SLR,  ã-SVR, MLP, RBFNN, GRNN and CCNN

H2:The DD prediction accuracy of software enhancement projects developed in MF and coded in 3GL with TkDM is statistically better than the accuracies obtained by SLR,  ã-SVR, MLP, RBFNN, GRNN and CCNN

H3:The DD prediction accuracy of software enhancement projects developed in MidRange and coded in 3GL with TkDM is statistically better than the accuracies obtained by SLR,  ã-SVR, MLP, RBFNN, GRNN and CCNN

H4:The DD prediction accuracy of software enhancement projects developed in Multiplatform and coded in 4GL with TkDM is statistically better than the accuracies obtained by SLR,  ã-SVR, MLP, RBFNN, GRNN and CCNN

We can then conclude that a TkDM can be used for predicting the DD in the first month of use of the software of new and enhancement projects developed in MF and coded in 3GL, as well as for enhancement projects developed in MidRange and coded in 3GL, and developed in Multiplatform and coded in 4GL.

In comparing our study with previous ones:

‚Ä¢
In spite of the importance of DD prediction, we only identified ten studies related to this topic since 2005, and only three of them compare the prediction accuracy between machine learnig models (L√≥pez-Mart√≠n et al., 2018; Kumar et al., 2013, Kutlubay et al., 2007).

‚Ä¢
We did not find any study applying a TkDM to predict DD when using projects selected by following the guidelines suggested for the ISBSG.

‚Ä¢
We identified two studies which applied NN, one of them a MLP was better than FL (Kumar et al., 2013), and the second one DT was better than RBFNN (Kutlubay et al., 2007), and one applying SVR in which a  ã-SVR was better than a SLR (L√≥pez-Mart√≠n et al., 2018).

‚Ä¢
Researchers have approached their efforts in analyzing how DD changed with size (Koru et al., 2009). Figs. 2 to 5 of our study coincides with conclusions of previous studies in the sense that large projects exhibited lower DD than medium and small projects. This pattern may be explained because larger projects tend to be developed more carefully (Moeller and Paulish, 1993). A recent study published in 2018, reports that larger modules tend to have more defects but have a lower DD (Zhou et al., 2018).

‚Ä¢
The value for k reported in a systematic review on CBR is between 1 and 23, and the majority of studies reported between one and five (Bardsiri et al. 2014). In our study, after applying several experiments, we found that six was a useful value for k for the four data sets

Regarding limitations of our study, although the last version of the ISBSG release 2018 consists of 8,261 projects, after we followed the criteria suggested by the ISBSG for selecting the data sets for new and enhancement projects, we could only use data sets of 21, 70, 42 and 29 projects to apply the models. A second limitation is related to the empirical manner for finding the parameters for SVR and NN, as well as for finding the k value for TkDM.

A validation threat of our study is related to the independent variable used (i.e., FP), which is also predicted, therefore, the TkDM prediction accuracy depends of the size prediction accuracy.

Since k = 6 was empirically found, future work could be related with finding it from an intelligent algorithm. Moreover, in accordance with step 4 described in Section 4.1 of this study, an empirically procedure was performed for finding the a y b parameter values, thus, future work could be related to the proposal of an intelligent algorithm such as a metaheuristic for finding those two mentioned parameters. Finally, future work could be related to the application of other types of transforms.