Abstract
Static program analysis is in general more precise if it is sensitive to execution contexts (execution paths). But then it is also more expensive in terms of memory consumption. For languages with conditions and iterations, the number of contexts grows exponentially with the program size. This problem is not just a theoretical issue. Several papers evaluating inter-procedural context-sensitive data-flow analysis report severe memory problems, and the path-explosion problem is a major issue in program verification and model checking.

In this paper we propose -terms as a means to capture and manipulate context-sensitive program information in a data-flow analysis. -terms are implemented as directed acyclic graphs without any redundant subgraphs.

To show the efficiency of our approach we run experiments comparing the memory usage of -terms with four alternative data structures. Our experiments show that -terms clearly outperform all the alternatives in terms of memory efficiency.

Previous
Next 
Keywords
Static program analysis

Data-flow analysis

Context-sensitivity

1. Introduction
Static program analyses approximate the run-time behavior of a given program. This is done by abstracting from the concrete semantics of programs and from concrete data values. Such analyses can be context-sensitive or -insensitive, i.e., an analysis may or may not distinguish different analysis results for different execution paths, i.e. different contexts, e.g., different call contexts of a method or alternative intra-procedural executions paths due to control statements. Context-sensitive analyses are, in general, more precise than their context-insensitive counterparts but also more expensive in terms of time and memory consumption.

With conditional execution, the number of different contexts grows, in general, exponentially with the program size. Adding iterations leads, in general, to countably (infinitely) many contexts. Merging the analyzed information of different contexts reduces memory consumption at the cost of analysis precision. We should therefore aim for compact representations of the mappings of context information to analysis information.

The memory usage problem related to context-sensitive analyses is not just a theoretical issue; several papers, e.g., Lhoták and Hendren, 2008a, Lundberg and Löwe, 2013, evaluating various inter-procedural context-sensitive data-flow approaches report severe memory problems when using call-depths , and the path-explosion problem, e.g., Cadar and Sen, 2013, Boonstoppel et al., 2008 has for a long time been a major issue in program verification and model checking.

In this paper, we present a technique to capture context-sensitive analysis information. Hence, we do not distinguish between inter-procedural call context sensitivity (Shivers, 1991) and intra-procedural trace or path sensitivity (Rival and Mauborgne, 2007). In both cases we map contexts to analysis values for each program point in a memory efficient way. Our approach is based on so-called -terms that capture the analysis results of different contexts.

We assume a Static Single Assignment (SSA) (Cytron et al., 1991, Muchnick, 1997) representation of a program. We further assume a program analysis following a standard data-flow analysis approach as given, e.g., in Marlowe and Ryder (1990). It iterates over an SSA representation of the program and updates the analysis information at each node using the node’s transfer functions until a fixed point is reached.

In SSA graphs, -nodes are used to select between different definitions of a variable. There is a close relation between -nodes and -terms. We create a new -term each time we apply the transfer function of a -node and each -term encodes the various control-flow dependent analysis value options that were available when the transfer function was applied. We distinguish the -term operator symbols (
) by the static block number  of the related -nodes. If a -node occurs in a loop and is, hence, analyzed several times, a numerical index  is added to the operator symbols (
), and a new set of -terms is generated for each analysis iteration  over block  containing -nodes. This index increases each time a -node is updated, i.e., each time its transfer function is applied in the analysis.

Our approach is an extension and formalization of the ideas first presented by Martin Trapp in his dissertation (Trapp, 1999). There are also similarities between our -terms (represented as directed acyclic graphs) and the (directed, cyclic) value graphs that are used in Global Value Numbering (Alpern et al., 1988), since they also capture the history of control flow. The main difference between these two approaches is the memory efficiency we achieve by avoiding data redundancy.

-terms can also be understood as a generalization of Binary Decision Diagrams (BDD) (Akers, 1978) used to represent logical functions. The main idea from BDDs that we use in our -terms is the graph representation and the redundancy elimination for memory efficiency. The represented decisions based on the logical values (true/false) are used to represent analysis after conditional executions. To this end, -terms are ordered decision diagrams (like OBDDs) but allow for multiple decisions in the nodes (not just true/false of a parameter). Subtrees represent analysis values from multiple control-flow predecessors of a -node. They also allow for multiple target values in the leaves (not true/false as a result). Leaves represent analysis (lattice) values.

Traditionally context-sensitivity is used for inter-procedural data-flow analysis. For the sake of simplicity, however, our presentation of -terms in Section 2 will focus on intra-procedural context-sensitivity where the various contexts are due to control statements. In our experiments in Section 3, we will use context-sensitive data from an inter-procedural data-flow analysis. It is, however, important to notice that -terms can be used for any type of SSA-based context-sensitive static program analysis, e.g., for an intra-procedural compiler optimization as well as for an intra-procedural program verification.

This paper is a complement to our paper A Framework for Memory Efficient Context-Sensitive Program Analysis (Hedenborg et al., 2021). In Hedenborg et al. (2021) we give a formal presentation of -terms as context-sensitive analysis values that forms an abstract value lattice. We prove that any conservative context-insensitive analysis can be transformed into an approximated conservative context-sensitive analysis with a finite number of -terms. This implies that the resulting context-sensitive analysis is guaranteed to reach a fixed point. Hence, -terms are not only a memory efficient data structure for context-sensitive analysis information, they are also the backbone of a theoretical framework for context-sensitive program analysis.

The formal presentation in Hedenborg et al. (2021) outlines a theoretical framework of -terms without any supporting experimental evidence of being memory efficient. The aim of this paper is to complement that study with a set of experiments. Our contributions in this paper are the following:

1.
We propose approximated -terms as a memory efficient representation, to capture context-sensitive analysis values in an SSA-based program analysis.

2.
We show in a set of experiments the memory efficiency of using -terms compared to four other approaches that handle context-sensitive information. The four approaches are context table, context tree in two variants, and double hash map. These approaches will be explained in detail in Section 3.2.

The remainder of the paper is structured as follows: In Section 2, we give an informal introduction to -terms. We use examples from an intra-procedural analysis here as they provide an easy understanding of the theory. In Section 3 we present experiments and demonstrate the memory efficiency of -terms in relation to four other data structures. Here we apply the theory to inter-procedural points-to analysis. In Section 4 we present related works and Section 5 concludes the paper.

2. Introduction to -terms
In this section we give an informal introduction to -terms aiming for basic understanding rather than formality. As stated earlier, a formal presentation of -terms, as well as algorithms and proofs can be found in Hedenborg et al. (2021). Brief presentations of -terms can also be found in Trapp (1999) (published in German), Lundberg (2004), and Trapp et al. (2015). Algorithms for manipulating -terms only outlined in this section are described in Hedenborg et al. (2021).

We assume an SSA-based program representation and we will refer to the used program representation as the SSA graph. In Fig. 1, we show a simple piece of code, e.g., a method body (left), containing three if-statements and the corresponding basic block structure (middle) and SSA graph (right). In the SSA graph, we have annotated each -node 
 with their basic block number #. The nodes 1, 2, 3 and 4 (in boxes) in the SSA graph represent integer values.


Download : Download high-res image (246KB)
Download : Download full-size image
Fig. 1. A source code example with corresponding basic block and SSA graphs.

The SSA graphs are the method body parts of a program representation. They are flow-sensitive: every def-use relation of represented operations in a method is explicitly represented as an edge between the defining operation and the using operation (Hasti and Horwitz, 1998). An SSA method graph  is a directed, ordered multi-graph where  is a set of SSA nodes,  is a set of SSA edges,  is a unique graph entry node satisfying , and  is a unique graph exit node satisfying .  dominates all nodes and  post-dominates all nodes in .

In a context-insensitive data-flow analysis, the interpretation of the -function can be seen as a merger of all possible definitions of a given variable. This results in an approximation of the run-time values, e.g., by sets of possible values. For example, the approximated values of the variables , , and  in Fig. 1 is , and the value of  is .

A -term is a representation of how different control-flow options affect the value of a variable. For example, we can write down the value of variable  in Fig. 1 using -term as 
. Interpretation: variable  has the value 3 if block #7 was reached from the predecessor block #5 in the control-flow graph;  has the value 4 if it was reached from the predecessor block #6. In addition to a set representation containing possible values, a -term also contains information about the control-flow path that generated each of these values. It abstracts, however, from the conditions leading to the different paths. Using -terms, the values for the variables in Fig. 1 are: 
 Notice that a variable value is depending on a sequence of control-flow conditions, e.g., the value of variable , correspond to -terms over -terms, i.e., they are constructed as a composition of the -terms representing previous control-flow conditions. The variable , defined by , where  and  are two -terms, generates a new -term involving an operator .

2.1. -term construction
The construction of the -terms including the numbering of the -symbols for their distinction is a part of a context-sensitive analysis, more precisely, the result of applying an analysis update function of a -node. When the analysis method selects a -node for variable  in block , its transfer function “asks” all the predecessor blocks to give their last definition of  and constructs a new -term 
 where 
 is the -term for  given by the -th predecessor.

If the -th predecessor block does not define  itself, it “asks” its predecessors for the value, which may lead to the construction of a -subterm. This process continues recursively until each predecessor has presented a -term for . The process will terminate if any use of a value also has a corresponding definition and the SSA graphs are loop free.


Download : Download high-res image (74KB)
Download : Download full-size image
Fig. 2. A simple code fragment with a loop that encloses an if-statement and the corresponding SSA graph.

Loops in the control-flow cause problems, as they would lead to ever growing -terms, i.e., analysis would not reach a fixed point. We will solve this termination problem by approximating -terms with values from the context-insensitive analysis lattice after a fixed number  of analysis iterations. For a first try, we may assume a countably infinite number of -terms with the same block number  since every -node in  generates a new -term when the analysis reaches that block.

This situation is illustrated in Fig. 2 where we show a while-loop that encloses an if-statement that assigns a new value to a variable x.

Initially, the 
-node in block  containing the while-loop header generates the value 
 where the bottom element  symbolizes “value undefined”. This value is later propagated inside the loop body where the operators + and - has been pushed inside the -block by an operation called Shannon expansion, which will be explained in detail in Section 2.4. The 
 node after the if-statement in block  generates the value 
, a value that is propagated back to the loop header block . Using this approach, analysis would generate all possible values for x after an arbitrary but fixed number of loop iterations. The values for x after at most three interactions that might escape the loop are: 
 There are a few things to notice. First, 
 is the value of x after at most  loop iteration. Secondly, each -term 
 can be interpreted as: Either x has the value  (no iterations), or the value  after at most  loop iterations. Note that such an analysis would not terminate, but grow the terms indefinitely. Approximations will take care of termination; they are described in Section 2.6 below.

2.2. Common switching behavior of -terms with same index
A new set of -terms is generated for each analysis iteration  over block  containing -nodes. We denote this set with 
. All -terms generated in block  at iteration  have the same switching behavior. That is, for any two -terms 
 and 
, and for any execution of the program, it holds that if  and branch 
 in 
 is selected, then also branch 
 in 
 is selected. This property will be important later on when we introduce operations on -terms. It allows us to reduce the terms significantly even if we do not know actual values statically.

2.3. -term representation
Every -term can be naturally viewed as a tree. This is illustrated in Fig. 3 (left) where we show the tree representation of the -term for the variable  in Fig. 1. Each edge represents a particular control-flow option and each path from the root node to a leaf value contains the sequence of control-flow decisions required for that particular leaf value to come into play.


Download : Download high-res image (71KB)
Download : Download full-size image
Fig. 3. Term for variable 
 is illustrated by its tree and directed acyclic graph representations (edge direction from bottom to top).

Actually representing -terms as trees is in practice by far too expensive. A more cost efficient representation is a directed graph, similar to how BDDs are represented in Bryant, 1986, Bryant, 1992, which avoid redundant subtrees (cf. Fig. 3 (right)).

2.4. Basic -term operations
In this section we present basic operations on -terms based on Trapp et al. (2015). We focus on restrictions, Shannon expansion and redundancy. Most important is the Shannon expansion that can be used to manipulate -terms without affecting their value.

The restriction of a -term 
 to the :th branch of 
, denoted 
, is a new -term where every subterm 
 with switching behavior  in  has been replaced by its :th child 
.

For example: 
 

It should also be noticed that if we restrict us to the :th branch of 
, when 
 does not occur in ,  is left unaffected.

The Shannon expansion of a -term 
 over 
 is a new -term defined as: 

Notice, the Shannon expansion creates a new -term but not a new -term value. It is just a rewrite rule that can be used to manipulate a -term expression. This expansion is valid for terms with the same switching behavior in this manipulation, and according to the definition of switching behavior the index needs to be the same and can therefore be removed.

We illustrate the Shannon expansion (without any redundancy elimination) with the following example


A -term is redundant and can be simplified (redundancy elimination) if all its sub-terms are equivalent. 

Redundancy elimination is a rewrite rule producing new -terms encoding the same context-sensitive information, and implies that a -term  containing a redundant subterm 
 can be reduced without any loss of information. The process of removing redundant -subterms is called redundancy elimination and uses the pattern 
In the tree view of a -term, this corresponds to replace a subtree rooted by 
 by any of its subterms (which are all equivalent).

We illustrate redundancy elimination with the following example. 
 Redundancy elimination, e.g., 
, removes redundant control-flow information from a -term. Repeated redundancy eliminations can remove all superfluous control-flow information from a -term. This results in a compact representation of context-sensitive information where only control-flow decisions that actually have an effect on a certain value are kept. Redundancy elimination is the major reason why -terms outperform all the presented alternatives in Section 3 in terms of memory efficiency.

2.5. Operations on -terms
The code fragment at the left-hand side of Fig. 4 assigns different values to the two variables  and  in the two different branches of the if-statement and then returns the sum .

Using -terms, we can express the values of  and  before we add them as: The corresponding -term is illustrated in the center of Fig. 4 where we consider the addition as an operator 
 defined on -terms over (sets of) integers rather than on integers directly.


Download : Download high-res image (133KB)
Download : Download full-size image
Fig. 4. A method with the corresponding -term representation (center) of the return-expression . The right-most figure illustrates how the -operator is “pushed” to the leaves together with the exploitation of switching behavior.

Furthermore, we know that any execution takes either the first or the second branch of the if-statement while we do not know which. This observation leads us to the following rewrite:  That is, we can in this case make use of the fact that both -terms have the same switching behavior and apply the ＋ operator on each of the two branches separately before we merge the result. This transformation is illustrated in the right-most part of Fig. 4. The fact that we can rewrite the addition of two -terms as a -term over the addition of  and  for each individual branch is in this case quite obvious. However, this rule can be applied on any operator  and any -term 
.

This rewrite rule for -term expressions makes use of Shannon expansion (see 2.4), and is also taken from the OBDD literature where it is used to symbolically operate over Boolean functions represented as OBDDs.

Finally, once we applied this rewrite rule for the operator ＋ in Fig. 4, we can apply ＋ on a set of leaf values in which case we can fall back on ordinary integer arithmetic. The resulting simplifications together with the redundancy rule1  can symbolically be written as:  The result indicates that no matter what branch of the if-statement we use, we will always get the result . This simple example illustrates one of the strengths of using -terms. We can by using a few simple rewrite-rules make use of having stored flow-path information and “compute” more precise results than would have been possible in a context-insensitive approach.

2.6. -term approximations
We need to introduce some approximations to handle the ever growing -term generated during analysis, and to ensure analysis termination when handling loops in the control-flow. The first approximation we need is to state how many control-flow options a -term shall remember. This will be handled by introducing a precision parameter , which can be seen as the size of “context-memory”. Secondly we will approximate loops to guarantee the termination of the analysis.

In the analysis process a -term is created when analyzing a -node in the SSA graph. The -term represents how different control-flow options affect the value of a variable, and the size of the term grows larger as the analysis proceeds. To reduce the size of a -term we make a finite  approximation by restricting the number of control-flow options to . This approximation can be seen as a tree manipulation in which we make a post order traversal and, starting at the leaf nodes, replace each -term by its context-insensitive approximation until we have a -term of height k.

Fig. 5 shows the result of two different finite  approximations of the same -term . On the left-hand side we have the result in print and on the right-hand side we have the same result depicted as an original tree and two trees where the depth have been reduced and the leaf values have been merged.


Download : Download high-res image (138KB)
Download : Download full-size image
Fig. 5. Two different finite  approximations of the same -term .

From the loop example in Section 2.1 we can see that an analysis of a loop will generate -terms like 
. That is, the newly created -term will have a subterm with the same block number and a lower iteration index. This pattern will occur over and over again since each loop iteration results in a new composition of 
 with itself. This will result in a -term of infinite depth and a non-terminating analysis if no measure is taken to stop the iterations. To be able to guarantee a termination of the analysis we need to make loop approximations.

During the analysis of a loop we approximate each newly generated -term by approximating each subterm (with same block number as the root) by the union of the context-insensitive values for the subterms. This approximation is easy to understand as a tree manipulation. We make a post order traversal of the tree and replace each -term having the same block number as the root node with their context-insensitive approximation. We can then drop iteration index since the only control-flow option we use is the from the last visit to the block. Options from earlier visits is merged by the approximation.

Let us use a so-called “flat” lattice for integers where  and  for any two lattice elements  and , . Moreover, we assume the following transfer functions for the  (resp. ) operation:  for integers ,  and . This will give the following loop approximation for the loop example given in Section 2.1 

After three iterations we can see that we have a stable value. That will end the analysis of the given loop and guarantee a termination of the analysis. Thus, by introducing the finite  approximation and the loop approximation we can reduce the memory demand and ensure analysis termination. We also get a precision parameter  that we can adjust the get a reasonable balance between analysis precision and memory costs.

In summary, -terms as presented in this section is a data structure for context-sensitive analysis information that (similar to BDDs) avoid redundancies. They can be used to capture intra-procedural (this section) as well as inter-procedural (in the evaluation, Section 3) context information. In addition to BBDs, they form the backbone of a theoretical framework guaranteeing that any conservative context-insensitive analysis can be transformed into an approximated conservative context-sensitive analysis guaranteed to reach a fixed point (Hedenborg et al., 2021).

2.7. Size of -terms during analysis
In this section we show that the intermediate results of -terms never consume more memory than the final result. This means that the final result of the -terms is actually an upper bound approximation of the memory footprint during the whole analysis. In Section 3, we will exploit this result by assessing and comparing only memory sizes the final analysis results for -terms and its alternatives.

We understand the -terms as directed acyclic graphs; their size is determined by their number of nodes  and edges . Without loss of generality, we assume forward and may analyses constructing the -terms. SSA nodes are analyzed and updated in a data-driven way: nodes before their successors; inner loop nodes before outer loops. Each point in the (SSA based) program representation is eventually attached with the -term representing the context-sensitive value analyzed for this program point as a fixed point; initially with the smallest -term . Leafs of -terms represent context-insensitive values, e.g., , , constants or elements of a power set lattice. Recall that these context-insensitive values are special -terms (without -nodes). All elements of the same analysis lattice require the same amount of memory, e.g., a symbolic constant or a bit vector. Inner nodes of -terms represent context-sensitive values. They require a unique -node (root) and references to their child - or leaf-nodes.

Each  node of the program contributes to the analysis with a join function ; all other SSA nodes  with a transfer function 
. Updating a -node during analysis updates the corresponding -term. Either of the following situations might occur:

1.
The -terms analyzed for the predecessors are all the same, in which case the current output value points to this one and the same -term.

2.
At least two predecessors -terms are different, in which case the current output value points to a -node corresponding to this -node (a new such -node is created the first time this situation occurs). Its children pointers are set or updated to the -terms analyzed for the predecessor SSA nodes (so far).

Updating another node  during analysis updates the corresponding -term according to its transfer function 
. Either of the following situations might occur:

3.
The predecessor -terms are all context-insensitive values, in which case the current output is updated to the context-insensitive value according to 
, which is selected if exists or created, otherwise.

4.
At least one predecessors is a -term representing an context-insensitive value (with a -node as a root), in which case the current output value is updated to the -term of the Shannon expansion of 
 applied to the predecessor -terms.

It is important to note that each of the cases only keeps the -term sizes constant or increases it. This is obvious for the cases 1–3. In case 4, the Shannon expansion pushes the 
 over the -nodes in a recursive descent without creating new -terms. At the leafs, the context-insensitive value according to 
 are selected and created if necessary, respectively. The recursive ascent of the Shannon expansion may create new or updates existing -terms that are not used elsewhere, as described in the cases 1 and 2.

The fact that -terms are monotonously increasing their memory footprint throughout an analysis motivates our use of the final result of a points-to analysis to evaluate the memory efficiency of -terms in Section 3.3.

3. Evaluating memory efficiency
In this section we present the result of an experiment where we evaluate the memory efficiency of using -terms to represent context-sensitive information. That is, we compare the memory costs of using -terms with three other frequently used internal representations based on tables, trees, and hash-tables – details will be given in Section 3.2 – in a context-sensitive points-to analysis.

In Section 3.1 we give a brief presentation of context-sensitive points-to analysis, in Section 3.2 we present the memory metrics we used to compare the different internal representations, in Section 3.3 we describe a context-sensitive points-to analysis and the experiment setup, and in Sections 3.4 Memory sizes for 3-this-sensitivity, 3.5 Memory sizes for different this-sensitivity we present and discuss the results of our evaluation.

3.1. Context-sensitive points-to analysis
Points-to analysis is an inter-procedural static program analysis that computes precise object reference information by tracking the flow of abstract objects from one part of a program to another.

An abstract object  is an analysis abstraction that represents one or more run-time objects and in this section each syntactic creation point  corresponds to a unique abstract object 
 representing all run-time objects created at  in any execution of an analyzed program. During analysis, each variable and object field  in the analyzed program is associated with a points-to set , the set of abstract objects that may be referenced by .

Data-flow based points-to analysis will in general track the flow of abstract objects from their creation points along all possible (may) paths in the data dependency graph. In a typical approach to points-to analysis, e.g. Lhoták and Hendren (2003), each method in the program is represented by a graph. The SSA -functions 
 is modeled as a specific node type that merges the values (
) transported to it along its in-edges.

In context-insensitive analysis, arguments of different calls to the same target method (graph)  get propagated and mixed there. The result of ’s analysis is then the merger of all calls targeting  and the effects of  itself. Furthermore, the analysis results, i.e., over-approximated return values and heap updates, affect other callees of .

A context-sensitive analysis aims to reduce this over-approximation by partitioning all calls targeting  into a finite number of call contexts (cloned method graphs). Context-sensitivity gives, in general, a more precise points-to analysis since the arguments of calls targeting the same method do not get mixed when the calls are analyzed in different contexts. This partitioning can be made even more fine-grained by taking more than one level (call-depth ) of call history into account.

As a concrete example, consider a control-flow analysis with call-depth 3 (3-CFA) that uses the three most recent call sites in the call history to define a context (Shivers, 1991). That is, each analysis value is represented by a quintuple like: 
. That is, the value of variable  in method  is  when  is called from call site 
, which is called from call site 
, which is called from call site 
. Hence, the triple 
 identifies the call context,  identifies the variable, and  is the actual value stored in the variable in this particular context.

3.2. Memory size metrics
The major drawback of context-sensitivity is the increased memory costs that come with maintaining a large number of contexts, and that increasing the call-depth comes with an exponential increase in memory costs. The memory problem is not just a theoretical issue, several papers, e.g., Lhoták and Hendren, 2008a, Milanova et al., 2005a, Lundberg and Löwe, 2013, evaluating various context-sensitive points-to analysis approaches report severe memory problems when using call-depths .

In this section we present memory size metrics for five different data structures (context table, context tree in two variants, double hash map, and -terms) to represent context-sensitive analysis information. As our running example we will use call-depth 32 where each analysis value is specified by a quintuple 
That is, the variable , in context 
, has the value . This is not only general enough to handle any type of points-to analysis with call-depth 3, it can handle any type of context-sensitive data-flow analysis having a context-depth 3. In an approximation of -terms () each such quintuple would contribute with a single root-to-leaf path in the tree representation of a -term for variable .

In order to compare the memory size of different techniques we use abstract memory size metrics rather than actual memory measurements during an actual analysis. The reason for this is twofold: (1) We would like to avoid presenting implementation details that might affect the actual results. Hence, the memory size metrics is an abstraction indicating what memory costs can be expected in an ideal case where all other memory influencing parameters are kept constant, (2) Identifying what part of the actual memory used in an analysis that is due to handling context information is non-trivial.

Context table
The most obvious approach to store all the quintuples generated during an analysis is a table having one quintuple in each row. The memory size of such a table is: 
where 
 is the number of rows in the table (the number of quintuples).

The left-hand side of Fig. 6 shows a small fraction of such a table related to a single variable . In this artificial example each row corresponds to unique context-sensitive value. For example, row 1 states that variable , in context , has the points-to value 
. A realistic program has thousands of such variables and each variable have 10–30 context-sensitive values, cf. Table 1. The memory size  related to the seven tuples we associate with variable  is .

Besides their memory footprint, the major drawback with context tables is the lookup speed. Accessing the current value for a given variable, in a given context, is 
. However, we will use  as a base case against which other representations are compared.

Context tree
Another possible approach to represent the context-sensitive values for a given variable  is a tree where each tuple 
 forms a unique path starting in root node  and having the value  as its leaf. The right-hand side of Fig. 6 shows the context tree for the example variable . Each context 
 can be seen as a selection over possible context values, e.g.,  or  in 
. The tree representation guarantees a speedy lookup. It can also be more memory efficient than a table, since many variable values might have partially the same call history, e.g., 
 or 
, and can therefore “share” the internal tree nodes. We have one tree for each variable and the size of each tree is given by the number of edges and nodes: 
where 
, 
 are the number of nodes and edges in the tree for variable . The memory size  associated with example variable  in Fig. 6 is .

A simple improvement of the tree representation is to avoid duplicate leaf nodes representing the same value. Thus, we merge all leaf nodes for a given value into one representative node (thus breaking the tree structure). The left-hand side of Fig. 7 shows the corresponding data structure for the example variable . We use the same lookup mechanism, have the same number of edges, but reduce the total number of nodes. 
where 
 is the number of internal nodes and 
 is the number of unique values. The memory size  associated with example variable  is .

Double hash map
The representation currently used in our in-house points-to analysis is optimized for a speedy lookup and uses two hash maps (map to map). The analysis algorithm is analyzing one method at the time in a given context 
, and for each such context it keeps a variable-to-value map. Hence, the technique can be seen as 2-step map context-to-(variable-to-value). Furthermore, it uses a similar avoid-duplicate-values approach as described in . The memory size using this approach, using context depth 3, can then be computed as 
where  is the set of all context’s triples, 3 is the size to store such a triple, and 
 is the number of variables in context , and 
 is the number of unique values. Notice also that the double hash map approach, in contrast to the tree representations and -terms, is not explicitly describing the possible values for a single variable . Hence, we do not present any figure and size calculation for example variable .

-terms
Each variable in an analysis with context depth 3 has their own -term (with maximum depth 3) representing their possible values in different contexts. The right-hand side of Fig. 7 shows the -term for the example variable . Due to redundancy elimination, the -term only keeps the essential context information. In general, it is a directed acyclic graph and its size is therefore: 
where 
, 
 are the number of nodes and edges in the -term for variable . Consequently, the memory size for the example variable  is .

Data structure time-complexity
The main reason for introducing -terms is memory efficiency since memory usage problems is the biggest issue related to context-sensitive analyses. However, time efficiency is also import when analyzing large programs. The time-complexity for looking up the current value for variable  in a context 
 is  in a context table with  contexts and  for the two trees and the -terms, since  It is  for the double hash map. Hence, we do not expect the -terms to be a more time efficient data structure; its strength is the memory efficiency.

3.3. Experiment setup
The experiments to be presented in Section 3.4 are all based on context information taken from a points-to analysis. We run the analysis on ten different programs, extract the context-sensitive analysis results of points-to variables, and compute the five memory size metrics presented in Section 3.2. Using the final analysis results to construct the memory size metrics (rather than growing the metrics during the analysis) is motivated since: (1) It allows us to compare the memory size metrics of five different context-information data structures without having to implement each one of them, and (2) as shown in Section 2.7, the final results represents the maximum memory size for an analysis based on -terms.

The context-sensitive points-to analysis we use is presented in three papers (Lundberg and Löwe, 2007, Lundberg et al., 2009, Lundberg and Löwe, 2013) and forms the core of a PhD thesis (Lundberg, 2014). It uses a flow-sensitive data-flow algorithm called SSA-Based Simulated Execution and a context-sensitive approach to points-to analysis called This-Sensitivity. Simulated Execution is a data-flow algorithm simulating an abstract execution of the program where the processing of a method is interrupted when a call occurs, and later resumed when the processing of the called method is completed. This-sensitivity is a modified version of object-sensitivity presented by Milanova et al. (2005a) where the target context associated with a call site  is determined by the pair , where  is the points-to set of the target expression . Hence, we distinguish analysis contexts of a method by its implicit variable this whereas the target contexts in object-sensitivity is given by a pair 
 for each 
. Experiments in Lundberg et al., 2009, Lundberg and Löwe, 2013 show that this-sensitivity gives almost as precise results as object-sensitivity (similar results in 3 out of 4 precision metrics). However, the slightly higher precision of object-sensitivity comes at the price of a much higher (a magnitude) time and memory costs.

The major reason for using this-sensitivity rather than object-sensitivity in this article is the (comparably) low memory costs for this-sensitivity. We show that -terms have a substantial effect on the memory costs even when using the already memory efficient this-sensitive points-to analysis instead of object-sensitivity that has a larger memory footprint.

In our experiments we use k-this-sensitivity to generate realistic context data (tuples). -this-sensitivity makes use of the top  abstract this-sets on the call stack. This is similar to the way -CFA is defined in terms of the top  call sites 
 on the call stack (Shivers, 1991). Hence, a unique variable value in 3-this-sensitivity is given by a quintuple where  is a unique identifier of an SSA variable, [ ] is the calling context, and  is the points-to set representing the analysis value of variable  in the given context.

We have collected context information (tuples) using a benchmark containing 10 different programs. Since we analyze Java bytecode, we characterize the size of a program in terms of number of classes, methods, and SSA variables rather than lines of code. The benchmark programs range from 691 to 1294 classes. All programs are presented in Table 1. The programs in the upper part of the table are taken from well-known test suites3; we have picked programs that were (i) larger than 600 classes, and (ii) freely available on the Internet. In the lower half, we have our own set of “more recent” benchmark programs, which are also freely available. All programs are analyzed using version 1.6.1 of the Java standard library.

The right-hand side in Table 1 shows the relative tuple count for each analysis. We use a relative measure indicating the average number of tuples for each variable. Hence, 33.2 for chart 3-TS indicates that each variable (on average) comes with 33.2 quintuples when analyzed using 3-this-sensitivity.

The total tuple count for each case can be obtained by multiplying the number of SSA Variables and the Relative Tuple Count. For example, the total tuple count for chart 3-TS is . As expected, the number of tuples used for each variable increases with context depth .

We use the -this-sensitivity benchmark results with variable tuples

 to construct the five different memory data structures presented in Section 3.3. All constructed structures have a correct value semantic and are immutable. The main purpose of these structures is just to allow us to compute the corresponding memory size metrics.

3.4. Memory sizes for 3-this-sensitivity
Fig. 8 shows the memory sizes for each program in the benchmark when analyzed using 3-this-sensitivity (3-TS). The bars in the chart show the relative metrics () where we have used the result of the table metric  as baseline. Hence, the value 1.10 for  in antlr means that  is 10% percent larger than  in this case. Similarly, 0.11 for  in antlr means that  only need 11% percent of the memory used by .

The results are rather clear. According to the memory metrics we have a ranking of their memory usage as where the numbers in the parenthesis are the average values compared to our baseline .  requires on average 6% more memory space than . This can be explained by the fact that the tree metric also takes the number of edges into account, and that this additional memory cannot be compensated by quintuples having the same call history that can “share” the internal tree nodes.

 is by definition always as good and in most cases better than . This is no surprise and the 19% reduction in memory size indicates that reusing leaf nodes is a simple, yet effective, way to reduce the memory costs.  was initially designed to provide a speedy lookup. However, the 22% reduction in memory size compared to  shows that it is also rather memory efficient.

The representation with the smallest memory footprint (without any loss of precision), according to our memory metrics, is . -terms use on average only 13% of the memory used by second best approach . In order to understand this result, we must take a closer look at what typical context-sensitive values look like.

The Single Path bars in Fig. 9 shows the percentage of single path values in the different benchmarks. A single path value is a context-sensitive value that in a tree representation only consists of a single tree path connecting the root and the leaf. Put in another way, a variable value is defined by a single quintuple 
. It turns out that, on average, about 40% of all variable values are single path values. This type of value takes the memory size 7 (4 nodes + 3 edges) according to  and , but only 1 according to . The major difference is that -terms due to its redundancy elimination removes all internal nodes and edges and stores only the variable value.


Download : Download high-res image (269KB)
Download : Download full-size image
Fig. 9. Various metrics showing how redundancy elimination reduces the -term memory footprint.


Table 1. Benchmark information and relative tuple counts.

General	Relative tuple count
Program	Classes	Methods	SSA Variables	1-TS	2-TS	3-TS
Antlr	691	4,122	21,435	3.1	6.2	13.5
Chart	1,002	7,192	41,876	4.0	13.1	33.2
Javadoc	897	5,648	27,962	3.3	9.6	28.0
Jython	969	7,154	33,446	2.8	8.7	22.5
Sablecc-j	995	6,476	34,339	3.1	8.4	16.8
Soot-c	1,294	6.731	34,094	3.0	9.7	24.3
Javacc	691	4,710	20,021	5.0	12.3	30.0
Javazoom	765	4,917	22,319	3.1	6.7	18.7
Recoder	1,256	10,052	58,913	3.1	12.1	34.4
Pmd	987	6,503	33,363	3.2	6.8	12.6
Average				3.4	9.4	23.4
Median				3.2	9.7	26.2
The Single Value bars in Fig. 9 are even more important if we want to understand the memory efficiency of -terms. They show the percentage of variable values consisting of quintuples that, independent of what contexts are used, all result in the same value . It turns out that (on average) 68% of all variable values in a 3-this-sensitive analysis are of type single value. And then we should still have in mind (see Table 1) that the context-sensitive value for each variable on average is defined by 23.4 quintuples.

The fact that 68% of all variables do not benefit from a points-to analysis using a call-depth  might be used as an argument to say that adding context-sensitivity does not pay off as suggested in Lhoták and Hendren (2008a). However, Lundberg and Löwe (2013) found that being able to increase the call-depth beyond  is important in certain types of applications that require very fine-grained context information.

In any case, these numbers show that context-sensitive variable values have in general a rather complex structure when presented as a tree (, ) or a double hash map (), it is only the -terms with their redundancy elimination that can deduce (and make use of) the fact that a majority of these structures can be simplified, and made much smaller.

The Single Value result in Fig. 9 shows that -terms can identify and reduce the size of analysis values that are context independent (i.e. same value in all contexts). The two remaining bars (Depth Reduction, Redundancy Elimination) show that the redundancy elimination in -terms also reduces the size of truly context-sensitive values. Depth Reduction (a superset of Single Value) shows that -terms manage to reduce the -term depth in no less than (on average) 97.6% of all variable values, and Redundancy Elimination (a superset of Depth Reduction) shows that the -term construction of every single variable value, in all benchmarks (average 100%), involves at least one redundancy elimination. Hence, according to our memory size metrics, and for 3-this-sensitivity, -term are not only very good at reducing the memory size for context independent values, they manage also to reduce the size of more complex context-sensitive analysis values.

3.5. Memory sizes for different this-sensitivity
We repeated our experiments using the same memory metrics and the same benchmark programs, but now for 1- and 2-this-sensitivity. A summary of the results for  is presented in Table 2.

We find that their mutual rankings based on their memory footprints are maintained.  still requires significantly less memory than the other ones. However, since the maximum depth is lower and number of tuples per variable is much reduced (see Table 1), the effect of the redundancy elimination is not as pronounced for the lower values of .


Table 2. Summary of ranking results for memory usage (MetricX/TableSize).

Analysis	TreeSize	MergeSize	DoubleHash	ChiSize
1-TS	1.43	1.13	1.04	0.77
2-TS	1.22	0.98	0.80	0.24
3-TS	1.06	0.86	0.67	0.09
Finally, Fig. 10 shows a summary of all memory related experiments. In this case we have chosen memory size as an indicator for memory cost. The -axis shows the average relative memory costs for the different memory size metrics using the context-insensitive analysis (Ins, size 1 by definition) as our baseline. , , , and  show a fast growth rate in memory costs when the call-depth (and the number of contexts) is increased, indicating that a further increase of the call-depth would be very costly when using any of these data structures to handle the context-sensitive program information. -terms () on the other side shows (due to redundancy elimination) a very moderate growth rate when we increase the call-depth, indicating that a further increase in the call-depth () could be possible.


Download : Download high-res image (207KB)
Download : Download full-size image
Fig. 10. Relative memory sizes for various analyses (Ins, 1-TS, 2-TS, 3-TS).

4. Relation to previous work
In this paper we reuse the informal description and definitions of -terms from our conference paper (Trapp et al., 2015). A formal presentation of -terms and a framework for context-sensitive program analyses based on -terms, are presented in Hedenborg et al. (2021).

Rüthing and Steffen, 1999, Knoop and Rüthing, 2000 present an efficient approach to constant propagation using value graphs. Their approach is restricted to constant propagation whereas ours can be applied on any data-flow analysis problem. We use the Shannon expansion to push operators to the leaves, where the operation can be evaluated. This is not the case in the value graphs, where the operator nodes remain scattered over the value graphs. Moreover, by using -terms and pushing the evaluation to the leaves, we automatically remove the redundancies.

Harris et al. (2010) use Satisfiability Modulo Theory (SMT) to create a path-sensitive analysis to verify safety properties of C programs. Their approach, called Satisfiability Module Path Programs, enumerates the existing paths in the program by using the Satisfiability Theory (SAT) formulas given by the control-flow in the program. Their approach is more precise than ours but does not scale to larger programs.4

Heinze and Amme (2016) take a similar approach. They apply data-flow analysis on an SSA representation of the program to derive variable path predicates for each SSA variable in the program. The path predicate contains detailed information (predicates) about every control decision that might influence a given variable value. These variable predicates can later be fed to an SMT solver to verify certain program properties.

Our -terms is an abstraction of the path-sensitive approaches used in Rütting et al. Harris et al. and Heinze et al. We only keep track of the last  contexts that might influence a given variable value, and we disregard detailed information under which control-flow predicates these contexts get active. Thus, we trade precision for performance allowing us to handle much larger programs.

In general, higher precision can be reached by using context-sensitive analysis at the cost of a larger memory consumption. This implies the need of data structures with efficient memory usage and operations that makes quick manipulations on these structures. Here the usage of Binary Decision Diagrams (BDD) offer such an approach, which was exploited before, especially, for Points-to Analysis:

•
Zhu, 2002, Zhu and Calman, 2004 present an approach to points-to analysis that uses Symbolic Pointers. Blocks of memory are source (domain) and target (range) of references (pointers). Points-to relations are modeled as directed graph of such blocks. Each block has an id corresponding to a unique Boolean formula. An edge is represented as a pair of domain and range block ids, i.e., pairs of Boolean domain and range functions. These Boolean functions are captured as BDDs and updated during analysis using BDD operations. This saves memory and preserves update performance.

•
Berndl et al. (2003) use BDDs to minimize the representation of the points-to data. The points-to relations between variables and sets of abstract objects are represented by binary strings. For large programs, the number of such sets can be very large. The BDD approach reduces the memory of partially redundant points-to sets. An evaluation shows that the approach is beneficial for both execution time and memory consumption.

•
Whaley and Lam (2004) create a clone for different invocations of a method call (call paths). A context-insensitive analysis on the extended call graph representing all clones results in a context-sensitive analysis. The context-sensitive relations are captured using BDDs leading to an efficient memory usage.

•
Based on benchmarks on different context-insensitive and context-sensitive analysis variants, Lhoták and Hendren (2006) conclude that the best method for points-to analysis is object-sensitive (Milanova et al., 2005b). The usage of BDDs in capturing the analysis data allows to increase the size of the analyzed programs. The same authors also discuss the effect on precision and efficiency of context-sensitive points-to analyses (Lhoták and Hendren, 2008b). It shows that the precision is application dependent and that the efficiency depends on the used analysis method. The analysis framework PADDLE is based on object-sensitivity and uses a BDD representation for the context-sensitive information. The resulting reduction of memory usage opens up for a more sensitive analyses to get a better accuracy.

•
Ball and Rajamani introduce Bebop (Ball and Rajamani, 2001), a path-sensitive inter-procedural data-flow analysis tool for C programs. It adds data-flow facts to each vertex in a control-flow graph allowing to rule out paths that are not feasible in the analysis. For memory efficiency, the set of facts are captured in BDDs.

All papers above show that the usage of BDD provides memory efficient approaches to capture context-sensitive points-to information. Our paper introduces a systematic approach to generalize context-insensitive to context-sensitive analyses using BDDs. Applying it to points-to analysis as one of many possible examples.

Kim et al. (2018) state the importance of utilizing different dimensions of sensitivity in static program analysis to improve the analysis precision. They present a general framework to capture many different dimensions of sensitivity, such as, context-, flow-, trace-sensitivity, that encompass a large variety of well-known analyses in all these dimensions and combination thereof. Using abstract interpretation, they show that each context-sensitive analysis instance of their framework is sound. Furthermore, they point out the importance of using a “sparse representation” based on “dictionaries” to handle the analysis data without providing any further implementation details. We focus on a data structure that have a sparse representation, and therefore suggest -terms as an efficient and flexible data structure to handle analysis information from any analysis generated by this framework.

5. Summary
Static program analysis is an important part of both optimizing compilers and software engineering tools for program verification and model checking. Such analyses can be context-sensitive or -insensitive, i.e., an analysis may or may not distinguish different analysis results for different execution paths. Context-sensitive analyses are, in general, more precise than their context-insensitive counterparts but also more expensive in terms of time and memory consumption. The memory problem related to context-sensitive analyses is not just a theoretical issue, several papers (including, e.g., Lhoták and Hendren, 2008a, Lundberg and Löwe, 2013) evaluating various inter-procedural context-sensitive data-flow approaches report severe memory problems when using call-depths , and the path-explosion problem, cf. (Cadar and Sen, 2013, Boonstoppel et al., 2008), has for a long time been a major issue in program verification and model checking.

This paper presents -terms as a mean to capture context-sensitive analysis values for programs represented as SSA-graphs. Each meet point of execution paths in the program, i.e., each -node, is mapped to a -term whose children represent the alternative analysis values of these paths. The -terms are represented by graphs without any redundancy which generalizes the idea behind OBDDs (Bryant, 1986, Bryant, 1992). This leads to a very memory efficient representation of context-sensitive analysis information. Furthermore, operations over context-sensitive analysis values, needed to implement transfer functions of the analysis, are defined using restriction and Shannon expansion, which are also similar to the corresponding OBDD operations.

For languages with conditional execution, the number of different contexts grows, in general, exponentially with the program size. Adding iterations lead, in general, to countably (infinitely) many contexts.

In order to show the memory efficiency of -terms, we set up an experiment, using context data from an existing points-to analysis, to compare the memory usage of -terms with four other data structures for saving context-sensitive information. Our experiments show that -terms outperform all of the presented alternatives in terms of memory efficiency. In the case of a points-to analysis with call-depth 3, -terms used only 13% of the memory of the second-best data structure without any loss of precision. In a context-sensitive data-flow analysis, this memory gain can be used to increase the number of contexts that will be used in the analysis phase, e.g. by increasing the call-depth, thus allowing a more precise analysis.

Evaluating the memory efficiency using an implementation of -terms, in concrete analysis scenarios is future work. Experiments for future work would apply the methodology for some other types of inter-procedural analysis (e.g., for static garbage collection or synchronization barrier removal) and to intra-procedural value analysis.