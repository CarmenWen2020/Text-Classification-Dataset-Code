Deep Neural Networks (DNNs) are being used in various daily tasks such as object detection, speech processing, and machine translation. However, it is known that DNNs suffer from robustness problems — perturbed inputs called adversarial samples leading to misbehaviors of DNNs. In this paper, we propose a black-box technique called Black-box Momentum Iterative Fast Gradient Sign Method (BMI-FGSM) to test the robustness of DNN models. The technique does not require any knowledge of the structure or weights of the target DNN. Compared to existing white-box testing techniques that require accessing model internal information such as gradients, our technique approximates gradients through Differential Evolution and uses approximated gradients to construct adversarial samples. Experimental results show that our technique can achieve 100% success in generating adversarial samples to trigger misclassification, and over 95% success in generating samples to trigger misclassification to a specific target output label. It also demonstrates better perturbation distance and better transferability. Compared to the state-of-the-art black-box technique, our technique is more efficient. Furthermore, we conduct testing on the commercial Aliyun API and successfully trigger its misbehavior within a limited number of queries, demonstrating the feasibility of real-world black-box attack.

Previous
Next 
Keywords
Adversarial samples

Differential evolution

Black-box testing

Deep Neural Network

1. Introduction
In the past few years, Deep Neural Networks (DNNs) (LeCun et al., 2015) have achieved great success in many important applications, such as image classification (Krizhevsky et al., 2012), speech processing (Sak et al., 2015) and machine translation (Bahdanau et al., 2014). Their efficacy even outperforms humans. Modern software applications increasingly include DNNs as a critical component, exampled by autonomous software (Bojarski et al., 2016), Apple face ID,1 and Amazon Echo.2 It can be envisioned that DNN model engineering will become an essential step in the software development lifecycle. As such, testing and debugging DNN models is of importance.

However, researchers have revealed that DNNs have robustness problems. That is, they are vulnerable to adversarial samples, i.e., benign inputs that add small and imperceptible perturbation and cause DNNs to misclassify. Adversarial samples hinder the utilization of DNNs in safety-critical systems, especially those related to computer vision, including face recognition (Sharif et al., 2019), self-driving vehicles (Evtimov et al., 2018) and medical analysis (Litjens et al., 2017). To DNN based applications, adversarial samples are threats but also a method to test DNN models. Our work falls into efficiently and effectively generating adversarial samples to expose robustness problems of DNNs.

Adversarial sample generation methods  (Szegedy et al., 2013, Goodfellow et al., 2015, Carlini and Wagner, 2017, Papernot et al., 2017, Su et al., 2019, Chen et al., 2017, Jere et al., 2019) have two categories: white-box techniques and black-box techniques. The former requires access to model internals such as model structure, neuron weight values, and gradients. In contrast, the latter treats the subject model as a black box and does not require access to model internals, but rather just model outputs. Black-box techniques have broader applicability. They can be used to test remote applications powered by underlying DNNs. For example, internet service providers such as Alibaba Cloud3 and Google Cloud4 offer pay-for-use APIs, which hide the internal details from users so that white-box techniques are not applicable. As such, black-box techniques are necessary.

In this paper, we develop a new black-box adversarial sample generation method in image classification. For a subject model, we only assume access to model inputs and outputs. Our approach called Black-box Momentum Iterative Fast Gradient Sign Method (BMI-FGSM) then modifies the original inputs to trigger model misclassification. It aims to achieve the goal with as little mutation as possible, utilizing Differential Evolution (Storn and Price, 1997) to approximate gradient direction, and leveraging double step size and candidate reuse to improve efficiency, which will be explained later.


Download : Download high-res image (685KB)
Download : Download full-size image
Fig. 1. Our proposed black-box BMI-FGSM on ImageNet samples. All adversarial samples are misclassified by target DNN (Perturbations are magnified for better visualization).

We compare BMI-FGSM with other state-of-the-art white-box and black-box techniques in both the untargeted setting (i.e., misclassifying to any output label) and the targeted setting (i.e., misclassifying to a specific output label). We also consider the transferability, that is, whether an adversarial sample generated for one model can be used to trigger misbehavior of another model. The experiments on a set of widely used datasets and models show that our proposed BMI-FGSM can achieve a high success rate in both untargeted and targeted settings, comparable or even better than white-box methods. Our approach can generate an adversarial sample within half a minute, faster than other black-box techniques. Finally, we apply our approach to a real-world image recognition system and expose its robustness problem.

In summary, we make the following contributions.

•
We propose a novel black-box adversarial sample generation method named BMI-FGSM. This technique does not require knowledge about model architecture, weight values, or gradients.

•
We propose two novel methods to improve performance, double step size to enlarge exploration distance and candidate reuse to approximate momentum that provides guidance for input perturbation, which is critical for generating effective adversarial samples.

•
We compare BMI-FGSM with the state-of-the-art methods. Experimental results on the MNIST, CIFAR10, and ImageNet datasets show that our approach is more efficient than the black-box method ZOO (Chen et al., 2017) and has a comparable success rate as the white-box method MI-FGSM (Dong et al., 2018). Furthermore, we use BMI-FGSM to test the commercial Aliyun Image Recognition API and successfully trigger misbehavior.

The remainder of this paper is structured as follows. Section 2 reviews existing research related to adversarial samples. Section 3 presents our black-box test case generation technique. Section 4 posts three research questions and then answers them with experiments. Section 5 presents the conclusions and future work.

2. Related work
Existing works on the DNN robustness problem and adversarial sample generation method in image classification will be reviewed in the following subsections.

2.1. DNN robustness
Robustness is critical to the application of DNNs. A popular approach to improving model robustness is to provide additional training and validation data. There are mainly two ways of generating additional data, data augmentation (Zhong et al., 2017, Perez and Wang, 2017) and using Generative Adversarial Network (GAN) (Goodfellow et al., 2014, Wang et al., 2018). The former augments training dataset by transforming original data, e.g., move, rotate, flip, and scale an image to produce new ones. A GAN model is composed of a generator and a discriminator. The generator takes random input and tries to mutate it to a valid input, while the discriminator determines if the mutated one looks like a real input. The two parts compete with each other and ideally the generator would learn to generate real samples. However, existing data augmentation techniques and GANs have limited effectiveness. Hence, a more practical method to gauge and improve DNN robustness is through adversarial samples. Specifically, original input samples are perturbed to generate adversarial samples to trigger model misclassification. The training set can be enhanced with the adversarial samples to retrain the DNN model to improve robustness (Tramer et al., 2017, Madry et al., 2017). With adversarial training, DNNs are expected to be less sensitive to noises or perturbations.

2.2. Adversarial sample generation
Adversarial sample generation methods can be categorized into white-box methods and black-box methods.

White-box methods assume full knowledge of the target DNN, such as model architecture and neuron weights. Szegedy et al. (2013) observed that adding small perturbations to input images can cause DNN model misclassification and converted the generation of adversarial samples to a constrained minimization problem. Goodfellow et al. (2015) proposed “fast gradient sign method” (FGSM), a gradient-based method aiming at a very short generation time. Kurakin et al. (2016) proposed a basic iterative method to generate more powerful samples. Dong et al. (2018) introduced momentum and proposed “momentum iterative fast gradient sign method” (MI-FGSM) to further balance the trade-off between success rate and transferability. Carlini and Wagner (2017) proposed the C&W method, an optimization-based technique systematically builds examples by directly optimizing the perturbation with an Adam optimizer. Additional proposed mechanisms include binary search and change of variable space.

Black-box methods assume no access to model internals. Instead, they can query the target model by sending inputs and observe the corresponding outputs. Compared with white-box techniques, black-box methods can be used to perform testing to third-party models and have much broader applicability. Papernot et al. (2017) assumed no internal information and insufficient training data, and proposed to train a substitute model with a small synthetic training dataset. Narodytska and Kasiviswanathan (2017) discovered the phenomenon that is merely modifying a single pixel might lead to model misclassification. Su et al. (2019) exposed DNN robustness problems by leveraging Differential Evolution to search for this kind of pixel. Chen et al. (2017) proposed Zeroth Order Optimization (ZOO), a derivative-free generation method. The authors exploited a finite differencing method to calculate the approximate gradient by analyzing two very close points in the loss function.

Another type of DNN robustness testing is the transferability testing (or no-box method) (Szegedy et al., 2013, Moosavi-Dezfooli et al., 2017). These techniques do not query the target model, neither do they generate any samples. Instead, they use adversarial samples produced by another model to test the target DNN. The underlying assumption is that if an adversarial sample can confuse a model, it is likely that it is equally confusing for another model. Therefore, transferability can be considered an important quality metric of generalization for the adversarial test cases.

We aim at developing a black-box adversarial sample generation method that features high success rate, high transferability, and cost-effectiveness. Many parallel works have also studied the problem of black-box adversarial generation, but our work remains unique in the approach. Ilyas et al. (2018) uses a natural evolution strategy (which can be seen as a finite differences estimate on a random gaussian basis) to estimate the gradients for use in the projected gradient descent method. Following this work, Ilyas et al. (2019) formalizes the gradient estimation problem and develop a bandit optimization framework incorporating time and data-dependent information, to generate black-box adversarial samples. In the same threat model, we leverage Differential Evolution to approximate gradient sign to convert a white-box iterative gradient-based method to its black-box version that only requires accessing model outputs. Alzantot et al. (2019) develops a gradient-free approach for generating adversarial examples by leveraging genetic optimization, where the fitness function is defined similarly to CW loss, using prediction scores from the black-box model. The authors adopt dimensionality reduction and adaptive parameter scaling for boosting gradient-free optimization. In contrast, our approach models the gradient sign, combining with our double step size and candidate reuse strategies enables attacks that can reliably generate adversarial samples. Another line of work aims to generate adversarial samples in different scenarios. Cheng et al. (2019) focuses on the label-only setting and propose a generic optimization algorithm, which can be applied to discrete and non-continuous models other than neural networks, such as the decision tree. Suya et al. (2019) simulates a scenario where the attacker has access to a large pool of seed inputs and proposes a hybrid strategy that combines optimization-based and transferability-based methods.

2.3. Testing DNNs
Compared with traditional software, the behavior of a Deep Learning (DL) system is determined by the structure and weights of DNNs. The dimension and test space of DNN is often larger. DeepXplore (Pei et al., 2017) proposes a white-box differential testing algorithm for systematically finding inputs that can trigger different behaviors between multiple DNNs. They propose neuron coverage for systematically measuring the parts of a DNN exercised by test inputs. Tensorfuzz (Odena and Goodfellow, 2018) provides coverage-guided fuzzing methods for neural network by using the approximate nearest neighbor algorithm. DeepTest (Tian et al., 2018) generates test cases that maximize the numbers of activated neurons and finds erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, and fog). DeepGauge (Ma et al., 2018a) proposes multi-granularity testing coverage for DL systems based on the internal state of DNN. Their testing criteria are scalable to complex DNNs. DeepCT (Ma et al., 2019) adapts the notion of combinatorial testing and introduces a set of coverage criteria based on neuron input interactions for each layer of DNNs to guide test generation towards balancing the defect detection ability and the number of tests. DeepCover (Sun et al., 2018) proposes a set of four test criteria for DNNs, inspired by the MC/DC test criteria in traditional software. They evaluate the proposed test criteria on small scale neural networks and show a higher defect detection ability than random testing. DeepMutation (Ma et al., 2018b) adapts the concept of mutation testing and proposes a mutation testing framework specialized for DNNs to measure the quality of test data. They believe that mutation testing is a promising technique that helps to generate higher quality test data.

Existing works on testing the DL system detect erroneous behaviors of DNNs under realistic circumstances (e.g., lighting, occlusion), and mostly focus on testing criteria, which requires DNNs to be transparent. In contrast, adversarial sample generation demonstrates a particular type of erroneous behavior of DNNs. Our proposed method tests black-box DL systems with adversarial samples that are indistinguishable from the original ones, to expose misbehaviors from the attacker’s perspective.

3. BMI-FGSM algorithm
Our technique, called Black-box Momentum Iterative Fast Gradient Sign Method (BMI-FGSM), is inspired by iterative gradient-based methods and Differential Evolution. It features mechanisms to improve the efficiency and effectiveness of generation. Fig. 2 is the basic framework of our method. Given a clean input image and a pre-trained DNN, Differential Evolution first derives a gradient sign population, children candidates compete with their parents using the corresponding perturbed inputs. Then, we perturb the inputs with the approximate gradient signs. The process repeats until an adversarial sample very similar to the benign input is produced.


Download : Download high-res image (232KB)
Download : Download full-size image
Fig. 2. Basic framework of the Black-box Momentum Iterative Fast Gradient Sign Method.

In the following sections, iterative gradient-based methods are first introduced, and Differential Evolution is presented, followed by the complete algorithm with two improvement mechanisms double step size and candidate reuse.

3.1. Iterative gradient-based methods
Throughout this paper, we focus on a -classifier DNN model (LeCun et al., 1998) of image classification task where 
 is the input image and 
 is the -dimension output that denotes the predicted labels. More formally, we define a DNN as follows (1)(2)
 Here,  is computed by hidden layers and a set of neuron weight matrices, known as the logits. The softmax function normalizes the output so  denotes the probability distribution of the predicted labels i.e. 
, where 
 is the probability that input  belongs to label . Thus the final classification label  is the label with the highest probability.

During the black-box adversarial sample generation, we assume the DNN model is pre-trained and fixed (i.e., neuron weight values are fixed). We assume access to the input image, output label, and output confidence, which is common for black-box methods. We do not assume any access to the structure, neuron weights, or intermediate outputs. Given a clean input  and its ground-truth label , we call 
 an adversarial sample if (3)
(4)
 where in untargeted setting sample is misclassified to any false label and in targeted setting sample misclassified to a specific false label . A threshold  controls the perturbation magnitude by limiting the 
-norm distance.

Iterative gradient-based methods are white-box generation methods that iteratively calculate the gradient and perturb the input. These methods feature high efficiency, success rate, and extensibility, usually require a small number of back propagations to craft adversarial samples in the 
 neighborhood of the original input. For example, the basic iterative method (Kurakin et al., 2016) produces adversarial sample by applying gradient  times with a small step size: (5)
 
where  represents the gradient and  is the cost function. It can finish the generation fast and yield superior results, also known as the “iterative fast gradient sign method”. To further balance the trade-off between success rate and transferability, an improved version of the basic iterative method introduces momentum (Dong et al., 2018) by replacing the raw gradient with an accumulated gradient 
: (6)
 

The critical step of iterative gradient-based methods is the gradient computation. In white-box attacks, gradients are usually computed by back propagation.

3.2. Differential evolution
Differential Evolution (DE) (Storn and Price, 1997) is an evolutionary algorithm to solve optimization problems. DE tries to find the input that best fits the objective through input mutation. It has three phases: evolution, crossover, and selection. During the phases of evolution and crossover, new children candidate solutions are generated. In selection, by comparing children with their parents, the ones with better fitness survive and are chosen for the next iteration of evolution. DE has the following advantages: (i) DE algorithm is independent of DNN, and hence any improvements of DE (Das and Suganthan, 2011, Das et al., 2009, Zhang and Sanderson, 2009) are also directly applicable. (ii) DE solves non-differentiable, noisy, and dynamic problems such that it can handle both continuous and discontinuous problems without requiring understanding internal structure of the problems. (iii) DE has a heuristic-based diversity insurance mechanism which can prevent from being trapped in local maxima and minima while standard greedy search and gradient descent often cannot.

In our algorithm, we employ DE to search for the gradient sign. Denote 
 as the input image,  as the ground-truth label, 
 as the outputs of DNN. We initialize the first generation  as follows. (7)
where 
 denotes the gradient signs with the same size as , and is the th candidate of the 0th generation. Variable 
 is the th feature randomly initialized with 1 or −1.  is the size of population (i.e, the number of candidates in a generation). Fig. 3(a) visualizes a candidate in . The candidate is essentially a matrix of the same size as the input image.


Download : Download high-res image (112KB)
Download : Download full-size image
Fig. 3. Visualization of two candidates in Differential Evolution. White pixels denote the corresponding matrix elements are 1. (a) a candidate in the initial population ; (b) a candidate in the last population .

The evolution and crossover at the th generation is defined as follows. (8)
(9)
  where  denotes the scaling factor of differential vector,  denotes the crossover probability to control crossover variants. Intuitively, Eq. (8) denotes that a mutant 
 is derived from three randomly selected candidates from the previous generation , denoted by the three random numbers 
, 
, and 
. We assign a sign function because it returns only two valid values representing two different gradient directions, and leads to fast convergence. Eq. (9) denotes that an offspring 
 is formed by recombining the mutant with the previous candidate, and 
 is the th feature of 
.

Next, compare fitness of  and  to determine the next generation . However, it is hard to set a fitness function for candidates since they represent gradient signs, so we convert candidates to a set of perturbed images first (Eq. (5), replace  with candidate), and then apply the fitness function. For the DNNs we test, although making the confidence of  as fitness works acceptably well, we ultimately settle on this following fitness function. (10)
 
 
 The lower the 
, the higher fitness value an input 
 has. This fitness function aims to suppress the probability of the ground-truth label while enhancing the maximum probability among other false labels. Fig. 3(b) visualizes one of the candidates which have evolved  generations. We can observe the pattern that serves as approximate gradient signs.


Download : Download high-res image (95KB)
Download : Download full-size image
3.3. Black-box momentum iterative fast gradient sign method
Using DE, we leverage Algorithm 1 to approximate the gradient signs. Children candidates are generated by Eqs. (8), (9). We cannot directly compare candidates, so we convert candidates to perturbed images (line 4). The perturbed images’ fitness values are compared to determine the offspring using Eq. (10). Candidate with the lowest fitness score of  is the final approximate gradient signs. Note that in black-box testing, the additional overhead by DE is inevitable because in white-box testing the “exact gradient signs” can be computed by back propagation in milliseconds. Note that in the black-box attack context, precise estimation of gradient signs is neither possible nor necessary.


Download : Download high-res image (134KB)
Download : Download full-size image
We then extend our work to BMI-FGSM, an algorithm inheriting the advantages of both DE and iterative gradient-based method. As shown in Algorithm 2, the loop denotes the main procedure of the iterative gradient-based method. After approximating gradient signs with a base sample 
, we apply  to 
 again and generate perturbed images , the next base sample 
 will be the fittest one of . After  iterations, image 
 is returned as the adversarial sample. The most challenging part of BMI-FGSM is to escape local optimums and re-gain momentum for better results. Hence, we design two mechanisms called double step size and candidate reuse.

3.3.1. Double step size
There are two types of perturbations in our proposed BMI-FGSM. The first one happens in approximate gradient signs (Algorithm 1, line 4). It is a “temporary perturbation” in which we always perturb the same base sample , and generate a set of temporary images  for fitness evaluation. The other one is in the loop of iterative gradient-based method (Algorithm 2, line 6). It is the “permanent perturbation” in which we iteratively perturb the clean image from 
 to 
.


Download : Download high-res image (105KB)
Download : Download full-size image
Fig. 4. An example of double step size. (a) no double step size; (b) double step size.

Either perturbation requires a perturbation distance, usually it is , but there would be a problem if we just use the same distance — the DE loop would evaluate candidates using temporary images that are perturbed with a very small distance. The DE procedure hence runs the risk of being stuck in some local optima. Note that the white-box version of iterative gradient-based method does not suffer from this step size problem as they always calculate exact gradient signs by back propagation.

As a result, we design the double step size: exploiting two distances for different perturbations. Specifically,  initialized with  is for gradient signs calculation, and  initialized with  is for the permanent perturbation. At the end of iteration , we have perturbed 
 to 
 with a distance , and hence decreased the exploration distance by updating  to satisfy , where parameter  is still the only overall perturbation distance of adversarial sample. Fig. 4 shows the difference with and without double step size. The circle area denotes local optima. Points 
 represent the movement of permanent perturbations. Points 
 denote the best position that DE can find. Without double step size, perturbations are short-sighted so that the path would be trapped for a while. With double step size, we can explore further and hence guide the procedure to escape from the local optima area.

Intuitively, the double step size enables us to use a more considerable perturbation distance when approximating gradient signs. Dynamic exploration distance enlarges the potential to achieve better fitness. With the assistance of double step size, we improve the success rate of sample generation on ImageNet.

3.3.2. Candidate reuse
In the training of DNNs, momentum (Polyak, 1964) is introduced to improve gradient descent. Using accumulated gradients rather than the raw gradients helps escape local optima and converge fast. Researchers in the white-box MI-FGSM project (Dong et al., 2018) leverage this idea and integrate momentum in their techniques. Accumulated gradient balances the trade-off between transferability and perturbation distance, producing high-quality adversarial samples. But according to our tests, the way of constructing an accumulated gradient cannot be directly ported to our method for two reasons: (i) The accumulation of approximate gradient signs also means the accumulation of errors. (ii) Candidates have only two valid values 1 and −1, so the accumulated gradient Eq. (6) is not appropriate.


Download : Download high-res image (75KB)
Download : Download full-size image
Fig. 5. An example of candidate reuse.

We design a novel mechanism candidate reuse to achieve effect similar to gradient accumulation. Specifically, while generation  holds  candidate gradients, our technique saves the best  candidates as part of the initial population of next round, where  is the keeping rate. Fig. 5 gives an example of how candidate reuse works. The reused candidates that carry previous iterations’ gradient information, participate in the evolution and play the role of “momentum”. The candidate reuse method is more practical in the black-box attack context than the accumulated gradient. Applying candidate reuse, we make BMI-FGSM available on ImageNet.

4. Experiments
We use an Intel Xeon E5 CPU and an Nvidia Tesla K80 GPU for all model trainings and sample generations. We aim to answer the following three research questions:

•
RQ1: How does BMI-FGSM perform compared with existing methods?

•
RQ2: How is the transferability of BMI-FGSM?

•
RQ3: How do double step size and candidate reuse improve BMI-FGSM on large model and dataset?

•
RQ4: How dose BMI-FGSM perform testing third-party applications?

We first report that BMI-FGSM is substantially faster than existing black-box techniques and achieves comparable performance with existing white-box methods. Then, we show that the two novel mechanisms do improve performance. Finally, we present the results of generating adversarial samples on a third-party system: the Aliyun Image Recognition API.

4.1. RQ1: Performance comparison
4.1.1. Dataset and models
For the MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky and Hinton, 2009) datasets. We randomly choose 100 images for evaluation and sample 10 images from each class. For each image, we apply one untargeted attack and nine targeted generations for the nine other classes. Thus there are 100 untargeted samples and 900 targeted samples in total for each dataset.

The target DNN models we used are based on the settings of Carlini and Wagner (2017, Table 1). We train and get 99% accuracy for MNIST and 84% accuracy for CIFAR10. Data were normalized before classification. Misclassified clean images are ignored during generation.

4.1.2. Comparison methods
On MNIST and CIFAR10, our proposed BMI-FGSM is compared with three state-of-the-art adversarial sample generation techniques C&W (Carlini and Wagner, 2017), ZOO (Chen et al., 2017) and MI-FGSM (Dong et al., 2018) in both untargeted and targeted settings. For all comparison methods, we refer to the code implementation of Foolbox5 with necessary modifications.

•
C&W (Carlini and Wagner, 2017, white-box, optimization-based, 
-norm) builds examples by directly optimizing the perturbation with an optimizer.

•
ZOO (Chen et al., 2017, black-box, optimization-based, 
-norm) exploits finite difference method to calculate approximate gradient.

•
MI-FGSM (Dong et al., 2018, white-box, gradient-based, 
-norm) introduces momentum to improve the basic iterative method.


Table 1. Evaluation of adversarial sample generation methods on MNIST.

Methods	Untargeted	Targeted
Success	Avg. dist	Avg. time	Success	Avg. dist	Avg. time
C&W	100.0%	–	33.4 s	100.0%	–	34.0 s
MI-FGSM	100.0%	0.195	0.1 s	100.0%	0.247	0.2 s
ZOO	100.0%	–	68.4 s	97.8%	–	87.4 s
BMI-FGSM	100.0%	0.227	16.7 s	98.2%	0.314	23.7 s

Table 2. Evaluation of adversarial sample generation methods on CIFAR10.

Methods	Untargeted	Targeted
Success	Avg. dist	Avg. time	Success	Avg. dist	Avg. time
C&W	100.0%	–	11.3 s	100.0%	–	12.7 s
MI-FGSM	100.0%	0.020	0.1 s	100.0%	0.035	0.1 s
ZOO	100.0%	–	143.9 s	94.8%	–	189.4 s
BMI-FGSM	100.0%	0.034	20.6 s	96.3%	0.047	30.4 s
4.1.3. Parameters
For C&W, we conduct 9 binary searches for the best scale parameter  starting from 0.01. We use 
 for the Adam optimizer. The confidence  for robustness and the max perturbation distance is set to 20. We run 1000 iterations for both MNIST and CIFAR10. The optimization terminates if the loss does not decrease after each tenth of the iterations. For ZOO, we use a batch size of 128 i.e., evaluate 128 gradients and update 128 pixels at each step. The ZOO attack is based on C&W’s framework, so we keep the setting consistent with C&W except running 3000 iterations for MNIST and 1000 iterations for CIFAR10.

For MI-FGSM, we set the maximum iterations  for MNIST and  for CIFAR10, and use a step size  per iteration. The upper bounds of perturbation distance are 0.4 and 0.05, respectively. The decay factor . We check the adversarial sample at the end of each iteration and return early if it successfully causes misclassification. For our BMI-FGSM, we set . In the case of MNIST, we set . In the case of CIFAR10, we set . Early return is activated.

4.1.4. Results
We report the success rate, average perturbation distance, and time for the four methods on MNIST and CIFAR10 in Table 1 and Table 2, respectively. From column 2, we can observe that all methods achieve 100% success rates in the untargeted setting. From column 5, The success rate of our approach in the targeted setting is over 95%, which is higher than the black-box ZOO. In terms of the perturbation distance in columns 3 and 6, we ignore the results of C&W and ZOO because they optimize 
-norm distance while MI-FGSM and BMI-FGSM use 
-norm. BMI-FGSM shows very close perturbation distance to the MI-FGSM, that is, our black-box method generates samples with similar quality as the white-box method. Columns 4 and column 7 illustrate the time cost. Our black-box approach can generates a valid adversarial sample within about 20 s for untargeted attacks and 30 s for targeted attacks, which is more efficient than the black-box ZOO. Note that our approach is an iterative gradient-based method, which is originally designed for fast sample generation.

Overall, our approach features high success rate and high efficiency. Fig. 6 visualizes some generated adversarial samples. We can observe that BMI-FGSM is able to generate images that show a similar visual impression as original images. These malicious samples with imperceptible distortion to human eyes confuse DNNs with high accuracy.

4.2. RQ2: Transferability
Transferability indicates the cross-model usability of adversarial samples. We evaluate the transferability on MNIST and CIFAR10. We define the transfer rate to be the percentage of transferable adversarial samples generated in Section 4.1, i.e., samples that can trigger the misbehavior of another model.


Table 3. Evaluation of transfer rate on MNIST.

Methods	Untargeted	Targeted
FC	LeNet5	FC	LeNet5
C&W	2.0%	5.0%	6.0%	11.0%
MI-FGSM	30.0%	20.0%	42.2%	45.4%
ZOO	3.0%	1.0%	3.1%	0.2%
BMI-FGSM	38.0%	36.0%	39.2%	24.0%

Table 4. Evaluation of transfer rate on CIFAR10.

Methods	Untargeted	Targeted
AllConv	NiN	VGG16	AllConv	NiN	VGG16
C&W	12.9%	8.2%	16.5%	18.8%	11.8%	18.8%
MI-FGSM	26.8%	19.4%	31.6%	46.7%	38.3%	55.2%
ZOO	11.8%	9.4%	10.6%	22.1%	11.7%	14.3%
BMI-FGSM	21.5%	14.5%	18.4%	25.4%	22.8%	20.1%
In the case of MNIST, we train a simple FC network (3x fully connected layers) and a LeNet5 (LeCun et al., 1998) as targets. In the case of CIFAR10, we train an All Convolution Network (AllConv) (Springenberg et al., 2014), a Network in Network (NiN) (Lin et al., 2013) and a VGG16 (Simonyan and Zisserman, 2015) network as targets. All models above are kept as similar as possible to their original framework with minor modifications at the softmax layer to fit the output.

4.2.1. Results
We report the transferability in Table 3, Table 4. Values in the table denote the transfer success rate, represent the generalization of the adversarial samples generated by different methods. We can observe that adversarial samples created by gradient-based methods (MI-FGSM, BMI-FGSM) have a higher transfer rate than those by optimization-based methods (C&W, ZOO). The optimization-based methods must tolerate larger perturbation distance in order to obtain higher transferability. Besides, our proposed BMI-FGSM takes advantage of momentum, which is introduced for balancing success rate and transferability, achieving a comparable score to white-box MI-FGSM. The transferability of our approach is higher than the black-box ZOO method on both MNIST and CIFAR10.

4.3. RQ3: Strategies on large dataset
Modern image classification applications may have a larger dataset and a complex model. Attacks in such settings are challenging and expensive due to the large input space. We evaluate the performance and transferability of four methods on the large ImageNet dataset. Then, we study the efficacy of our proposed two mechanisms, double step size and candidate reuse, by generating an image with a hard limit of iterations.

We randomly choose 100 images from ImageNet validation set (Krizhevsky et al., 2012) and apply one untargeted and one targeted generation for each image. Thus there are 100 untargeted samples and 100 targeted samples in total. We use VGG16 (Simonyan and Zisserman, 2015) as the target model, InceptionV3 (Szegedy et al., 2016) and ResNet101 (He et al., 2016) as the transfer models. We adapt the CIFAR10 parameter settings in Section 4.1. For untargeted setting, an adversarial sample is valid only when its ground-truth label not in the top-5 prediction.


Table 5. Adversarial sample generation on ImageNet.

Methods	Untargeted	Targeted
Success	Avg. dist	Avg. time	Success	Avg. dist	Avg. time
C&W	100.0%	–	8.2 min	98.6%	–	12.0 min
MI-FGSM	98.6%	0.003	0.1 min	98.6%	0.005	0.1 min
ZOO	98.6%	–	18.1 min	21.6%	–	230.9 min
BMI-FGSM	98.6%	0.025	7.4 min	93.2%	0.046	19.5 min

Table 6. Transfer rate on ImageNet.

Methods	Untargeted	Targeted
InceptionV3	ResNet101	InceptionV3	ResNet101
C&W	16.2%	14.9%	15.1%	13.7%
MI-FGSM	11.0%	12.3%	11.0%	9.6%
ZOO	37.0%	37.0%	25.0%	31.3%
BMI-FGSM	28.8%	30.1%	29.0%	33.3%
4.3.1. Results
Table 5 demonstrates the performance and Table 6 shows the transferability. Our proposed BMI-FGSM achieves a success rate of 98.6% in the untargeted setting, 93.2% in targeted setting. Compared with the white-box technique MI-FGSM, our approach has a higher perturbation distance due to large scale input images, and hence more gradients need to be approximated. Compared with the black-box ZOO, our approach significantly reduces time consumption and generates more transferable samples. We plot some of our generated samples in Fig. 1. All are hard to distinguish from the original samples.

The effect of different strategies is illustrated in Table 7. Observe that the average distance and the first valid iteration (the iteration where the first successful adversarial sample was found) have no evident distinction, while the success rates change noticeably — the success rate decreases by about 28% without double step size. The algorithm is almost unusable without candidate reuse. We also evaluate an alternative that uses the perturbed images as the population rather than gradient sign, while keeping everything else the same. Observe that the success rate decrease as well, suggesting incompatibility. This result indicates that new strategies are surely needed to apply this alternative.

To further investigate the advantages of double step size and candidate reuse, we set a hard  iterations and report the confidence of ground-truth label versus iterations in Fig. 7. The lower ground-truth label confidence, the higher attack ability of sample. The curve of no candidate reuse ends at about 0.99 shows the importance of momentum information. The curve of no double step size ends at about 0.55 because a single short perturbation distance runs the risk of being trapped in some local optima. In contrast, the dashed curve that indicates the best temporary perturbed images when approximate gradient signs drop quickly, and guide BMI-FGSM to achieve lower confidence of 0.05. With all the techniques applied, BMI-FGSM is able to generate visually undetectable adversarial samples that effectively suppress the probability of ground-truth label.


Table 7. Different strategies comparison on ImageNet.

Strategy	Success rate	Avg. dist	First valid
BMI-FGSM	98.6%	0.025	861
No double step size	70.3%	0.021	808
No candidate reuse	0%	–	–
Perturbed images as population	63.5%	0.020	797

Download : Download high-res image (149KB)
Download : Download full-size image
Fig. 7. Confidence of ground-truth versus iterations.

Adversarial samples represent a particular type of misbehavior of the DL system. To illustrate the ability of BMI-FGSM to test the robustness of DNN, we use random testing without coverage guidance and Tensorfuzz6 with coverage guidance as baselines of DNN testing techniques. Table 8 reports the performance of different methods in generating untargeted adversarial samples. Observe that our approach outperforms random testing and Tensorfuzz, which means BMI-FGSM is more appropriate for robustness testing of adversarial attacks.


Table 8. Comparison of testing techniques on ImageNet.

Strategy	Success rate	Avg. dist
Random	0%	–
Tensorfuzz	31.1%	0.046
BMI-FGSM	98.6%	0.025
4.4. RQ4: Testing third-party API
In order to validate the reliability and applicability of our method to third-party applications, we test the Aliyun Image Recognition API,7 a commercial computer vision toolkit powered by Alibaba Cloud. The API is a n-way classifier that can be queried and outputs label-score pairs for a given image, without any internal details. Our goal is to perform adversarial sample generation to trigger black-box API misbehavior. However, testing Aliyun API is more challenging than that on a common black-box model because of the following reasons: (i) The Aliyun API provides a free trial of 5000 queries. After that, it costs about $1.5 per 5000 queries. From the perspective of the attacker, a query-efficient adversarial generation algorithm is highly desirable. (ii) The number of classes is even larger, but we do not know how many and cannot enumerate all labels. (iii) The API only outputs scores for up to 5 top labels. The scores do not sum to one. They are neither probabilities nor logits.


Download : Download high-res image (953KB)
Download : Download full-size image
Fig. 8. Aliyun Image Recognition API. Left: clean image. Right: adversarial image.

In this experiment, we adapt the parameter settings in Section 4.3. We set the perturbation distance bound to be  and perform an untargeted generation with a budget of 5000 queries to the target API. We use API’s top-1 prediction of the original image as the fitness. The algorithm early terminates when the top-1 prediction change.

4.4.1. Results
Our approach achieves a 92% success rate against the Aliyun API on an ImageNet sample set of 25 images. Some adversarial samples against the image recognition API are given in Fig. 8. The left part is the original predictions, showing that the API correctly classifies clean images with high confidence. The right part is the adversarial predictions, showing adversarial samples mislabeled by the API. For example, the sign image with a 97% confidence score is misclassified as a restaurant, while the two images look very similar. Overall, we successfully trigger the Aliyun API misbehavior with perturbed images.

5. Conclusion
In this paper, we introduce Differential Evolution and develop a new type of black-box adversarial sample generation method called BMI-FGSM. We generate adversarial samples that are hard to detect and successfully attack DNNs on MNIST, CIFAR10, and ImageNet. We propose two mechanisms, double step size and candidate reuse, to be the essential part of our black-box method and conduct experiments to validate their efficacy. Experimental results show that our approach obtains success rate, perturbation distance, and transferability comparable to the state-of-the-art white-box techniques while reducing the time consumption of black-box method considerably. It even achieves similar performance to a white-box method on a large dataset. Finally, we craft adversarial samples for the Aliyun Image Recognition API and expose its robustness problem, demonstrating that our approach is able to test real-world third-party systems from the perspective of the attacker.

Future work includes testing models with defense techniques or models in other areas. This work also opens up new opportunities on how to combine adversarial techniques with evolutionary algorithms like Differential Evolution.