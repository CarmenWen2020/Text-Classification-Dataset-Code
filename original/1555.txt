Abstract
Automatic program repair papers tend to repeatedly use the same benchmarks. This poses a threat to the external validity of the findings of the program repair research community. In this paper, we perform an empirical study of automatic repair on a benchmark of bugs called QuixBugs, which has been little studied. In this paper, (1) We report on the characteristics of QuixBugs; (2) We study the effectiveness of 10 program repair tools on it; (3) We apply three patch correctness assessment techniques to comprehensively study the presence of overfitting patches in QuixBugs. Our key results are: (1) 16/40 buggy programs in QuixBugs can be repaired with at least a test suite adequate patch; (2) A total of 338 plausible patches are generated on the QuixBugs by the considered tools, and 53.3% of them are overfitting patches according to our manual assessment; (3) The three automated patch correctness assessment techniques, ,  and , achieve an accuracy of 98.2%, 80.8% and 58.3% in overfitting detection, respectively. To our knowledge, this is the largest empirical study of automatic repair on QuixBugs, combining both quantitative and qualitative insights. All our empirical results are publicly available on GitHub in order to facilitate future research on automatic program repair.

Previous
Keywords
Automatic program repair

Patch correctness assessment

Bug benchmark

1. Introduction
Automatic program repair aims to provide fixes to software bugs in an automated way. Test suite based repair, notably introduced by GenProg (Le Goues et al., 2012), is a widely studied family of techniques in program repair. In test suite based repair, test suites are used as an executable specification of the program, with at least one failing test that reveals the bug. Test suite based repair can be further divided into generate-and-validate techniques and synthesis-based techniques. Generate-and-validate techniques, such as GenProg (Le Goues et al., 2012), Astor (Martinez and Monperrus, 2019), CapGen (Wen et al., 2018), first generate as many patches as possible and then use the test suite to validate if the patch makes all tests pass. On the other hand, synthesis-based techniques such as AutoFix (Pei et al., 2014), SemFix (Nguyen et al., 2013), and Nopol (Xuan et al., 2017) first extract constraints based on test suite execution and then synthesize a patch (Monperrus, 2018, Gazzola et al., 2019).

Recent automatic program repair papers tend to repeatedly use the same benchmarks. In program repair for C code, the ManyBugs (Le Goues et al., 2015) benchmark or its derivative is dominant. In the context of program repair for Java, Defects4J (Just et al., 2014) is used in almost all evaluations of recent program repair approaches, including recently (Hua et al., 2018, Wen et al., 2018, Xiong et al., 2018). However, repeatedly using the same benchmarks poses a threat to the external validity of the community’s knowledge. The main threat is that the improvement that we now observe in the literature may only be valid for the benchmark under consideration but would not hold for other benchmarks. Even worse, those claimed improvements, if they only hold on the benchmark, maybe decorrelated from for real usages by practitioners. Fortunately, the importance of external validity is acknowledged by many researchers.

Problem

Research on program repair tends to repeatedly use the same benchmarks. This is a threat to the external validity of the results of our research community

As building sound and conclusive empirical knowledge is key to science, reducing this major threat of external validity in the context of program repair is the main motivation of this paper. To reduce the threat, we aim at doing a empirical program repair study on a new and well-formed bug benchmark.

In this paper, we perform an automatic repair empirical study on a benchmark called QuixBugs which was recently presented by Lin et al. (2017). QuixBugs is a program repair benchmark with 40 buggy algorithmic programs specified by test cases. The buggy programs are both available in Python and Java. In this paper, we conduct the following four experiments on Quixbugs: (1) We prepare QuixBugs for automatic program repair in Java; (2) We select ten representative test suite based repair tools, Arja (Yuan and Banzhaf, 2018), Cardumen (Martinez and Monperrus, 2018), Dynamoth (Durieux and Monperrus, 2016a), JGenProg (Martinez and Monperrus, 2019), JMutRepair (Martinez and Monperrus, 2019), JKali (Martinez and Monperrus, 2019), Nopol (Xuan et al., 2017), NPEFix (Cornu et al., 2015), Tibra (Martinez and Monperrus, 2019), and the Java implementation of RSRepair (Qi et al., 2015), and execute them over all buggy programs of QuixBugs. This results in 16/40 buggy programs being repaired by 338 different plausible patches; (3) We perform manual assessment for the generated plausible patches and manually classify them as 158 correct patches and 180 overfitting patches; (4) We assess the correctness of the plausible patches by three automated patch correctness assessment techniques: ,  and . We compute the accuracy of these three automated techniques are 98.2%, 80.8% and 58.3%, respectively.

This novel empirical study on a benchmark never used in a program repair context provides valuable findings that improve the external validity of program repair research. Our empirical study sets a baseline for future research of automatic program repair on QuixBugs.

To sum up, our contributions are:

•
A new version of QuixBugs that is usable for automatic repair research on Java programs, together with extensive data about the characteristics of QuixBugs.

•
The confirmation of two empirical facts of program repair, improving their external validity: (1) Our manual assessment shows that 53.3% of generated patches are overfitting, this confirms that the state-of-the-art of program repair tools produces a large number of overfitting patches (Xin, 2017, Smith et al., 2015, Le et al., 2018); (2) Our empirical study shows the considered automatic program repair tools are able to correctly repair seven buggy programs, this confirms the state-of-the-art program repair tools also produce correct patches (Qi et al., 2015, Martinez et al., 2017).

•
Three new and important findings about automatic program repair: (1) Certain program repair tools are able to repair programs with only failing test cases and no passing tests at all; (2) It is feasible and effective to use automated patch assessment techniques to identify overfitting patches with an accuracy of up to 98.2%; (3) Invariants based patch assessment suffers from a large number of false positives.

•
Experimental data that is made publicly available for facilitating future research (Ye et al., 2020). Our 338 plausible patches on QuixBugs and their correctness labels are consolidated for future studies on program repair.

This paper supersedes a previous version (Ye et al., 2019a) presented at the International Workshop on Intelligent Bug Fixing. In comparison, this article makes the following extensions. The program repair empirical study involves ten repair tools (expanding from five in the previous version). This new work presents and discusses the 338 plausible patches versus only 64 patches discussed in the previous version. This study considers a third automated patch assessment technique based on invariants. To our knowledge, this technique has only been studied by Yang and Yang (2020), and at a smaller scale (our dataset of patches is three times larger than that of Yang and Yang, 2020 – 338 versus 96). This journal extension provides novel results that compare the accuracy of three assessment techniques and discuss the false positive problem of invariants based patch correctness assessment, both of which have never been reported before.

The remainder of this paper is organized as follows. Section 2 presents how we prepare a new version of QuixBugs for the usage of automatic repair for Java programs. Section 3 presents four research questions (RQs) of our study and corresponding methodologies for these RQs. Section 4 presents our empirical results to answer the RQs. Section 5 analyses the threat of our study. Section 6 discusses the new findings of using QuixBugs and the future improvements for program repair tools. Section 7 compares the related work of our study and we conclude our study in Section 8.

2. Benchmark preparation
QuixBugs by Lin et al. (2017) is a benchmark of 40 bugs from 40 classic algorithms such as sorting algorithms of bucket sort, merge sort and quick sort. All bugs of QuixBugs were collected from the Quixey Challenges (Lawler, 2012), which consisted of giving human developers one minute to fix one program with a bug on a single line. The original QuixBugs benchmark contains: (1) A set of 40 buggy programs available both in Python and in Java; (2) For 31 out of 40 programs: JSON files with a set of inputs and expected outputs for each program; (3) An engine that takes a program name, executes the program using the inputs from the corresponding JSON file, and prints the expected and obtained output; (4) For the remaining 9 out of 40 programs, a Java class that has encoded the inputs and outputs and prints the obtained output.

However, the initial version of QuixBugs was not usable for doing automatic program repair in Java. Monperrus (2018) states that, in the context of test suite based repair, a “usable” benchmark must have (Monperrus, 2014): (1) A clear, explicit, and not biased construction methodology; (2) Regression oracles. For test suite based repair approaches such as GenProg (Le Goues et al., 2012) the oracles are the test suites: the failings test cases are the bugs oracles and assert the presence of a bug, while the passing test cases are the regression test cases that assert the correctness of the program w.r.t the inputs–outputs encoded in the test suite; (3) Real bugs (i.e., not seeded).

Unfortunately, the initial version of QuixBugs does not fulfill some of the aforementioned criteria. We summarize the problems of the initial version of QuixBugs as: (1) It did not provide any regression oracle, this not fulfill the second requirement of a usable benchmark (bugs and regression oracles); (2) Programs contained compilation errors (for 5 programs), this does not satisfy the first requirement of a usable benchmark (a clear, explicit and not biased construction methodology); (3) Incorrect values to test buggy programs (for 3 programs), which also not fulfill the first requirement of a usable benchmark; (4) Missing test assertions (for 9 programs), this violates the second requirement of a usable benchmark; (5) Missing a ground truth Java version (for all programs), without the ground truth patches provided, the correctness assessment of generated patches is harder.

To overcome the mentioned limitations that hamper its use by test suite based repair approaches, we introduce a new version of QuixBugs supplemented with test cases for reproducing buggy behaviors and a ground truth version for evaluating automatic repair patches. This new version of QuixBugs was already peer-reviewed and accepted by the QuixBugs authors and integrated to their public repository at GitHub. The steps we carried out for creating the new version are:

(1) Fix uncompilable Java programs. By compiling the initial version Java programs of QuixBugs, we noticed that there were compile errors in some programs (e.g., BREADTH_FIRST_ SEARCH). Some compilation errors were designed as part of buggy programs. However, most automatic repair tools do dynamic analysis of buggy programs. Hence, we need them all to be compilable and able to run the original buggy programs.

(2) Fix incorrect test data. To test 31 out of 40 buggy Java programs, QuixBugs provides pairs of inputs and expected outputs written in JSON files. However, we found that some expected outputs from programs KNAPSACK, SQRT and PASCAL were incorrect. Once we detected all incorrect inputs and outputs, we corrected them.

(3) Creation of JUnit tests from JSON files. QuixBugs uses a specific test driver based on JSON test cases. It executes the program using the inputs, and prints both the expected and actual outputs. However, automatic repair tools usually expect JUnit tests as oracle specifications: each test executes the program passing the inputs via parameters and then compares the obtained output with that one expected via assertions. Thus, we implement an automatic JUnit test generator to generate JUnit tests from the JSON files. In total, we generated 224 JUnit tests (test methods in JUnit) for the 31 programs having their inputs–outputs encoded in the JSON files.

(4) Creation of JUnit tests from ad-hoc assertion-less tests. There are 9 out of 40 Java programs from QuixBugs that are tested through a simple ad-hoc main method that starts with encoded inputs, calls the program using them as arguments, and finally prints the obtained output. This method is not usable by a test suite based program repair tool. Thus, we have manually rewritten those methods to produce 35 JUnit tests for these 9 programs. In total, our preparation has resulted in 259 JUnit test methods over 40 programs.

(5) Creation of ground truth Java programs. By default, the QuixBugs does not provide a ground truth version for the Java buggy programs. Automatic program repair researchers need those ground truths to compare them with the generated patches to assert their correctness. We created ground truth versions based on those provided by QuixBugs originally written in Python.

To summarize, QuixBugs was initially not usable for automatic repair tools in Java. In this section, we presented the tasks we carried out to build a new version of QuixBugs that can be used to evaluate the effectiveness of the test suite adequate repair tools. The new version of QuixBugs contains JUnit test oracles and ground truth programs, it was public peer-reviewed by the QuixBugs authors, organized with Travis and Gradle components. All those changes have already been contributed to the research community on the QuixBugs repository.

3. Empirical study
We now present our empirical study on the effectiveness of test suite based repair approaches on the QuixBugs benchmark. The empirical study covers several dimensions of automatic repair: benchmark analysis, repair effectiveness, patch correctness assessment. First, we list the research questions (RQs) of our work, we then describe the research methodology for each RQ.

3.1. Research questions
For this empirical study on program repair for QuixBugs, we pose the following research questions (RQs):

•
RQ1: What are the main characteristics of the QuixBugs benchmark?

•
RQ2: How many buggy programs of QuixBugs can be automatically repaired with test suite adequate patches?

•
RQ3: To what extent are the generated patches for QuixBugs correct?

•
RQ4: To what extent do automated patch assessment techniques accurately classify overfitting patches?

In RQ1, we are interested in the statistics of QuixBugs, including the type of bug, lines of code (LOC), JUnit tests, code coverage, etc. In RQ2, we consider one kind of automatic repair called test suite based repair. In test suite based repair, a bug is said to be repaired if a patch makes all tests pass. In that case, such a patch is called test suite adequate patch or plausible patch. We focus on how many test suite adequate patches could be generated by the state-of-the-art test suite based repair approaches. In RQ3, we conduct a manual assessment to evaluate how many patches generated in the experiment of RQ2 are actually correct. Finally, in RQ4, we study the effectiveness of three techniques to automatically classify correct and overfitting patches, and we compare their results with those from the manual assessment.

3.2. Protocols
This section presents the protocols of our empirical study of automatic program repair on QuixBugs.

3.2.1. RQ1: QuixBugs Benchmark Analysis
Bug understanding Zhang et al. (2016) is important for designing program repair tools and to analyzing the effectiveness of those tools. For each buggy program of QuixBugs, we gather and compute the following information:

Types of bugs.
The previous research reports the existence of the observational correlation between the bug fix and the cause of bugs (Ni et al., 2020). In our study, we collect and present the type of bug. QuixBugs contains various types of bugs such as incorrect comparison operators, incorrect array slice, etc. This allows us to analyze the capability of the program repair tools to repair buggy programs and to determine the most repaired bug types.

Numerical characteristics.
We compute numerical characteristics: the lines of code (LOC) of the program, the number of passing JUnit tests, failing JUnit tests, the test execution time and branch coverage. We rely on Cobertura1 to calculate the branch coverage for each program.

Input domain.
We extract the program preconditions and the input domain of each program. The program preconditions are constraints for the input domain. We discuss this to remind the future work on QuixBugs to sample tests that should be aware of the program preconditions.

Failures types.
We manually collect the failure symptoms when executing test cases for each buggy program of QuixBugs dataset. A bug can produce: (1) An incorrect output that triggers an assertion fail; (2) An error in the execution (e.g., array index error); (3) An exception thrown by the program (e.g., null pointer exception or stack overflow); (4) A timeout/infinite loop.

Unique characteristics.
We discuss the unique characteristics of the QuixBugs dataset compared with the benchmarks from the literature.

3.2.2. RQ2: Repairability of QuixBugs
To conduct our program repair empirical study on QuixBugs, we first select appropriate program repair tools. For this, we consider three criteria: (1) The repair tool must handle Java programs as QuixBugs programs are written in this programming language2 ; (2) The repair tool must implement a test suite based repair approach; (3) The repair tool must be publicly-available and continuously maintained.

According to these criteria, we finally select ten of program repair tools: Arja (Yuan and Banzhaf, 2018), JGenProg (Martinez and Monperrus, 2019), JKali (Martinez and Monperrus, 2019), JMutRepair (Martinez and Monperrus, 2019), Cardumen (Martinez and Monperrus, 2018), Tibra (Martinez and Monperrus, 2019), Nopol (Xuan et al., 2017), Dynamoth (Durieux and Monperrus, 2016a), NPEFix (Cornu et al., 2015) and the Java implementation of RSRepair (Qi et al., 2015) by Yuan and Banzhaf (2018). The ten repair tools target Java programs, are test suite based, and are publicly available on GitHub. All the ten repair tools take as input the source code of a buggy program and the corresponding test suite which contains at least one failing test case, and generate, when it is possible, one or more patches that make all test cases pass. We combine the patches generated during our empirical study in Lin et al. (2017) with patches generated from our recent work (Durieux et al., 2019). Each of the ten repair tools has been executed on all QuixBugs programs. We do not stop the repair process after finding the first patch, and we consider all generated patches, even if there are several patches for the same bug.

We carefully record and discuss: (1) The number of bugs that are repaired by the considered 10 systems; (2) The bug types of the repaired programs; (3) How test cases impact the repair tools; (4) The test failure symptoms of repaired programs.

3.2.3. RQ3: Manual Patch Correctness Assessment
Previous works have shown that program repair tools tend to generate a large number of overfitting patches (i.e., flawed repairs). In our work, per the previous terminology (Smith et al., 2015, Xiong et al., 2018, Xin and Reiss, 2017), we use the term overfitting to refer to those patches that pass all human-written test cases (i.e., test suite adequate patch) but still do not correctly repair the bug. Those flawed repairs are produced because of the weaknesses of the test suite used as an oracle, which is not able to completely specify the expected program behavior. To assess the correctness of patches generated for Quixbugs’ buggy programs, we perform the manual assessment as previous researchers have done on other benchmarks (Xiong et al., 2017, Wen et al., 2018, Martinez et al., 2017, Yuan and Banzhaf, 2018). We manually compare the automatically generated patches with the human-written patches. If a generated patch is identical or semantically equivalent (i.e., the actual behavior is the same) to the human-written patch, it is considered as correct. Otherwise, a patch is deemed as overfitting if (1) it does not/partially fix the bug, or (2) it introduces a new bug. To overcome the bias of manual assessment, all results are discussed among at least two authors. Our evaluation of patch correctness is publicly available on our GitHub repository (Ye et al., 2020).

3.2.4. RQ4: Automated Patch Correctness Assessments
As shown in previous research (Xiong et al., 2018, Ye et al., 2019b), manual assessment of program repair patches is a hard, time consuming and biased task. Thus, we also consider three automated patches correctness assessment techniques to identify overfitting patches, proposed by previous research: (a) Using automatically generated tests based on a ground truth version (i.e., the human-written patch) (Yu et al., 2019); (b) Using automatically generated tests by a program specific generator based on a ground truth version (Arcuri et al., 2012); (c) Using dynamic program invariants based on a ground truth version (Yang and Yang, 2020). We now describe how each of those techniques works.

Search-based test generation for patch assessment.
Using automated test generation is one way for assessing patch correctness (Xiong et al., 2018, Xin and Reiss, 2017, Yu et al., 2019, Yang et al., 2017). The idea of this technique is to generate new test cases that complement the already provided (potentially incomplete) test suite. In this paper, we consider Evosuite (Fraser and Arcuri, 2011), a state-of-the-art automated test generation tool, for generating those new correctness assessment tests. We have chosen Evosuite according to the results of Shamshiri et al., 2016, Ye et al., 2019b, which have shown that Evosuite is the most effective tool for this usage. The search-based test generator technique takes as input a ground truth program that is used as oracles, which means that the outputs from the ground truth programs on given inputs are the expected outputs (i.e., oracles), including both values and exceptions. Per the previous terminology, this patch assessment technique is named , which is based on the ground truth programs for test generation.

For each of the 40 buggy programs in the QuixBugs dataset, we invoke Evosuite a fixed number  of times. Eventually, we obtain  different independent JUnit test suites for each program. Since Evosuite is a randomized algorithm, we take  per the recommended practice (Arcuri and Briand, 2014, Yu et al., 2019). We always remove those generated tests that fail on the ground truth version, because they are ill-formed for our task. We execute these generated tests over the patches generated by the repair tools. In the assessment of , a patch is assessed as overfitting if it makes any automatically generated Evosuite test cases fail. If no generated test fails, we consider that the correctness of the patch is unknown (and not correct because the generated tests only sample the input domain, they do not assess the behavior over the full input domain).

Program specific test generation.
We consider a second random testing approach called , which randomly samples the test inputs based on the ground truth programs.  is an implementation of random testing (Arcuri et al., 2012) for QuixBugs. It samples the input space according to a specification of the input space, a uniform distribution for sampling and it uses the ground truth version as oracles (Mechtaev et al., 2018). If the ground truth version throws an exception on a generated input, the input is considered as invalid, the input is discarded. For implementing , we manually identify the domain of each input variable for each program in QuixBugs. The test generator is configured to sample the input space with the goal of getting a fixed number of valid test cases with no exception. For each program in QuixBugs, we generate 300 test cases with . We run those test cases on all generated patches of QuixBugs programs. In the assessment of , a patch is assessed as overfitting if it makes any randomly generated program specific test cases fail.

Invariants detection for patch assessment.
We consider a third automated patch assessment based on invariants captured from ground truth program execution, such assessment technique is called ground truth invariants, aka, . An invariant is a property that holds at a certain point or points in a program. A program point is a specific place in the source code, such as immediately before a particular line of code. Invariants detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed program executions. The invariants based patch assessment technique first infers program invariants from the ground truth version and uses them to assess overfitting patches, per the technique of Yang and Yang (2020). It executes the patched programs based on the provided manual tests and checks whether all inferred invariants still hold. In the assessment of , a patch is assessed as overfitting if it violates any invariants hold for ground truth program executions. To capture invariants in the ground truth programs and checking whether they hold for the captured programs, we use the tool Daikon (Ernst et al., 2007).

Accuracy of automated patch assessment.
To evaluate the accuracy of automated patch assessments, we compare the automated patch assessment results with manual assessment, where manual assessment is considered as ground truth. Specifically, we compute the corresponding false positives and true negatives as follows:

•
True Positive (TP): a patch classified as overfitting by manual assessment is also classified as overfitting by an automated assessment.

•
False Positive (FP): a patch classified as correct by manual assessment is classified as overfitting by an automated assessment.

•
True Negative (TN): a patch classified as correct by manual assessment is classified as correct by an automated assessment.

•
False Negative (FN): a patch classified as overfitting by manual assessment is classified as correct by an automated assessment.

Finally, the accuracy of an assessment technique is computed with the following evaluation formula: (1)
 


Table 1. Descriptive Statistics about the QuixBugs Benchmark.

Buggy Program Name	Bug Type	LOC	Passing	Failing	Code	Exe	Failure Symptoms
Tests	Tests	Coverage	Sec.	
BITCOUNT	Incorrect logical operator	10	0	9	100%	900	timeout/infinite loop
BREADTH_FIRST_SEARCH	Missing boolean expression	30	4	1	100%	1	array index error
BUCKETSORT	Reference to an incorrect variable	17	0	6	100%	1	incorrect output
DEPTH_FIRST_SEARCH	Missing lines with a function call	23	4	1	100%	1	stack overflow
DETECT_CYCLE	Missing boolean expression	17	4	1	100%	1	null pointer
FIND_FIRST_IN_SORTED	Incorrect comparison operator	22	4	3	90%	120	timeout/infinite loop (1)
array index error (2)
FIND_IN_SORTED	Missing ‘＋1’	19	5	2	100%	1	stack overflow
FLATTEN	Missing function call	18	1	6	83%	1	stack overflow
GCD	Expression swap	10	0	5	100%	1	stack overflow
GET_FACTORS	Wrong constructor call	17	1	10	100%	1	incorrect output
HANOI	Reference to an incorrect variable	53	0	7	100%	1	incorrect output
IS_VALID_PARENTHESIZATIONl	Other code replacement	15	2	1	100%	1	incorrect output
KHEAPSORT	Missing function call	29	1	3	100%	1	incorrect output
KNAPSAC	Incorrect comparison operator	30	4	6	100%	2	incorrect output
KTH	Reference to an incorrect variable	25	3	4	100%	1	array index error
LCS_LENGTH	Incorrect array slice	48	1	8	95%	1	incorrect output
Missing boolean expression
LEVENSHTEIN	Missing ‘＋1’	15	1	6	100%	1	incorrect output
LIS	Missing logic	27	0	4	91%	1	incorrect output
LONGEST_COMMON_SUBSEQUENCE	Missing function call	14	6	4	91%	1	incorrect output
MAX_SUBLIST_SUM	Missing function call	13	2	4	100%	1	incorrect output
MERGESORT	Incorrect arithmetic expression	40	0	12	100%	1	stack overflow
MINIMUM_SPANNING_TREE	Missing logic	67	0	3	72%	1	concurrent modification
NEXT_PALINDROME	Missing ‘’	28	4	1	87%	1	incorrect output
NEXT_PERMUTATION	Incorrect comparison operator	32	0	8	83%	1	incorrect output
PASCAL	Missing ‘＋1’	29	1	4	100%	1	array index error (3)
incorrect output (1)
POSSIBLE_CHANGE	Missing boolean expression	23	0	9	100%	1	array index error
POWERSET	Missing logic	24	1	4	100%	1	incorrect output
QUICKSORT	Incorrect comparison operator	37	12	1	87%	1	incorrect output
REVERSE_LINKED_LIST	Missing Assignment	12	1	2	100%	1	null pointer
RPN_EVAL	Expression swap	28	3	3	100%	1	incorrect output
SHORTEST_PATH_LENGTH	Other code replacement	49	2	2	92%	1	incorrect output
SHORTEST_PATH_LENGTHS	Expression swap	31	0	4	100%	1	incorrect output
SHORTEST_PATHS	Missing function call	55	0	3	100%	1	incorrect output
SHUNTING_YARD	Missing lines with a function call	31	0	4	100%	1	incorrect output
SIEVE	Incorrect method called	35	1	5	75%	1	incorrect output
SQRT	Incorrect arithmetic expression	9	1	6	100%	360	timeout/infinite loop
SUBSEQUENCES	Missing lines with a function call	22	2	12	100%	1	incorrect output
TO_BASE	Expression swap	14	0	7	100%	1	incorrect output
TOPOLOGICAL_ORDERING	Incorrect method called	25	0	3	100%	1	incorrect output
WRAP	Missing lines with a function call	22	0	5	75%	1	incorrect output
Total	–	1034	70	189	–	–	–
4. Empirical results
We now present and discuss the empirical results of our four research questions.

4.1. Results for RQ1: QuixBugs Benchmark Analysis
Table 1 presents the characteristics of QuixBugs, including the numerical statistics (e.g., LOC) and failure symptoms (e.g., incorrect output, null pointer exception). Program names are given in the first column in alphabetical order.

Type of bugs.
The second column presents the type of bug in each program. There are 17 different bug types. The most frequent bug types on QuixBugs are: (1) Missing function call in five programs. In those buggy programs, function invocations are missing. This means the patch that repairs this type of bug typically adds a function invocation. For example, to repair the bug for FLATTEN, the existing variable x should be replaced by flatten(x); (2) Incorrect comparison operator  in four programs, where comparison operators include ==, , and , etc. For example in QUICKSORT, the operator  is used instead of ; (3) Missing lines with a function call in four programs. This bug type refers to buggy programs that miss one or more lines of code. For example, the fix for the buggy program WRAP is to insert an additional line of code lines.add(text).

This diversity of bug types implies that repair approaches should also consider a diverse set of repair transformations: for example, some bugs could be repaired by replacing operators (e.g., incorrect comparison operator); other bugs could be repaired by replacing code (e.g., reference to an incorrect variable); or by inserting a new line of code (e.g., missing lines with a function call). This implies that, for repairing all of QuixBugs buggy programs, we need one or more approaches capable of applying a wide set of repair transformations. Thus, if one repair approach can repair most of the buggy programs in QuixBugs, it would mean that this approach is general in essence.

Program size.
The third column gives the lines of code (LOC) per program ranging from 9 to 67 lines, which can be considered as small. However, we note that 14 are recursive programs and 13 programs contain nested loops. It means that, despite a small program size (which can lead to a small search space), the time complexity or space complexity of those programs is sometimes non-trivial.

Characteristics of test suites.
Table 1 also summarizes the statistics about JUnit tests: the fourth and fifth columns present the number of passing tests and failing tests. As we discussed in Section 2, all programs from the new version of the QuixBugs have at least one failing JUnit test to expose the bug, which means that the prerequisite of test suite repair is met. There are 15 programs with no passing tests. All benchmarks of the literature, to our knowledge, contain at least one passing test. Passing tests are important for repair approaches to model the expected behavior of the program, which means that, without these passing tests, synthesis-based approaches such as Nopol have degenerated synthesis problems when repairing QuixBugs programs.

The sixth column gives the branch coverage information of JUnit tests. We observe that the majority of the QuixBugs are completely covered by the test cases (i.e., coverage 100%). The least covered program (MINIMUM_SPANNING_TREE) has a 72% of coverage. This high coverage implies that, for most of the branches from the buggy programs, there is at least one test case that executes it. Thus, any candidate patch applied on those branches will be executed at least once.

Execution time.
The seventh column presents the test execution time for each program. There are 37/40 programs whose tests run in less than 2 s, which suggests that program repair will evaluate fast each candidate patch, and eventually repair approaches can completely navigate the search space. For those 3 programs where the bug triggers a timeout/infinite loop, the tests timeout after 60 s, which explains the 3 large execution time values of programs (BITCOUNT, SQRT, and FIND_FIRST_IN_SORTED).

Failure symptoms.
The last column presents the failure symptoms. We observe 6 different symptoms. There are 26 programs with incorrect output failures, 5 programs with stack overflow failures, 5 programs with index out of bounds failures, 3 programs with timeout/infinite loop failures, 2 programs with null pointer failures, and 1 program with concurrent modification failure. Moreover, we found that two programs, PASCAL and FIND_FIRST_IN_SORTED, have test cases that expose different failures. In addition to the diversity of bug types we previously discussed, QuixBugs also has a diversity of test failure symptoms. This involves that automated program tools must take into account different situations after the bug is executed. For example, in the case of timeout/infinite loop failures, an approach must avoid handling itself, and in the case of stack overflow failures or index out of bound failures the repair tool must proceed after the failure and complete the dynamic program analysis such as fault localization.

Input preconditions.
Preconditions of the input domain for each program are important in our study as we use them as the constraints to automatically generate patches and to discard overfitting patches. We computed them for the 40 programs. All preconditions are given in our online appendix (Ye et al., 2020). Just to mention one as an example, the program GET_FACTOR factors an integer value using trial division. It has a unique function with signature get_factor(Integer n): List . The precondition we found is that the value for integer variable  must be greater than 1. Otherwise, the program is meaningless when the input is a negative integer or zero. And the precondition violated test case generation will influence the patch assessment results in our study.

Unique characteristics of QuixBugs .
Comparing the benchmarks of literature (Le Goues et al., 2015, Durieux and Monperrus, 2016b, Just et al., 2014, Do et al., 2005, Madeiral et al., 2019, Benton et al., 2019, Saha et al., 2018), we found three unique characteristics in QuixBugs: (1) There is a focus on algorithmic tasks such as sorting algorithms, search algorithms, towers of Hanoi puzzle, whose time complexity or space complexity is non-trivial. The existing benchmarks from the literature (such as Defects4J (Just et al., 2014) and Bears (Madeiral et al., 2019)) contain buggy programs of real and large open-source libraries, with procedures and modules that implement several functionalities. That is, a buggy program of those benchmarks is not a single textbook algorithm implemented in one single class as each program from QuixBugs is; (2) There are 15/40 programs with only failing tests. To our knowledge, all other benchmarks in the literature always contain at least one passing test; (3) The benchmark contains 3 timeout/infinite loop failures and 5 stack overflow failures, which is uncommon in bug benchmarks. Thus, using QuixBugs for program repair will give new insights into the successes and limitations of current repair tools.


Download : Download high-res image (111KB)
Download : Download full-size image

Download : Download high-res image (199KB)
Download : Download full-size image
Fig. 1. The distribution of 338 QuixBugs patches by 10 repair tools. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

4.2. Results for RQ2: Repairability of QuixBugs
The execution of the ten repair tools produced on the 40 buggy programs of QuixBugs produced 1470 program repair patches. Surprisingly, we observe that the repair tools generate duplicated patches. We conduct a sanity check and discard syntactically duplicated patches per repair tool. As a result, we discard 1132 duplicated patches and obtain 338 unique patches.


Table 2. Number of generated patches per tool.

Repair Tool	# Patches	# Repaired Programs
JGenProg	164	4
Arja	113	4
RSRepair	31	3
NPEFix	9	2
Cardumen	5	5
Tibra	4	2
Nopol	4	4
JKali	3	2
JMutRepair	3	3
Dynamoth	2	2
Total	338 patches	16 patched programs
Repaired bugs.
We present the results of generated unique patches from our empirical study in Fig. 1. In total, we have obtained 338 patches that repair 16 different buggy programs. Overall, 40% of QuixBugs can be repaired with at least one test suite adequate patch. Note that we have more patches than repaired programs because: (1) Some bugs are repaired by more than one repair tools (e.g., QUICKSORT); (2) Some repair tools generate two or more not duplicated patches for a specific bug (e.g., Arja for QUICKSORT). Note that this empirical study is novel and at scale. To our knowledge, this is the most comprehensive QuixBugs repair empirical study done ever, with the largest number of repair tools and the largest number of patches generated.

Effectiveness of repair tools.
We summarize the effectiveness of repair tools in Table 2. The first column gives the name of the repair tools. The second and third columns indicate the number of patches generated and the number of programs they repair, respectively. Cardumen is the approach that repairs the largest number of buggy programs: 5 buggy programs in total can be repaired. Notably, we observe in Fig. 1 that 4 of them are only repaired by Cardumen. This shows the extracted code templates in Cardumen are diverse and effective. Moreover, JGenProg and Arja are two systems that generate the largest number of test suite adequate patches (164 and 113 patches). This is because JGenProg and Arja leverage multi-objective genetic programming to evolve multiple patches over a series of generations progressively. There are 4 programs that are repaired by more than three repair tools: LIS, QUICKSORT, SHORTEST_PATH_LENGTHS and DEPTH_FIRST_SEARCH. The remaining 12 programs are repaired by only one repair tool. This implies that specific repair strategies are useful to repair specific bugs. We believe that the global effectiveness of the automatic program repair has to be considered by combining diverse repair approaches together, and not by building a single silver-bullet system.

The repaired bug types.
Recall Table 1 introduces 17 types of bugs in QuixBugs. In our empirical study, there are 10/17 types of bugs that are patched by the considered automatic repair approaches. We summarize the repair bug types in Table 3, where the first column gives the name of the bug type. The second column gives the number of repaired programs belonging to the bug type over the total number of this bug type. We list the repaired programs in the third column. The most repaired type of bug is the incorrect arithmetic expression, with 3 programs. The 3 repaired programs were repaired by 6 different repair approaches. This means that a particular bug type can be repaired using different repair strategies, the reason is that there are different strategies to repair the same bug type. For example, Nopol is able to synthesize new statements that use the correct variables instead of the incorrect ones, while JGenProg replaces the buggy statement having the incorrect variable by another one similar that has the correct variable.


Table 3. Repaired bug types.

Bug Type (Identified)	#	Repaired Programs
Incorrect comparison operator	3/4	knapsack, next_permutation, quicksort
Incorrect arithmetic expression	2/2	mergesort, sqrt
Expression swap	2/2	shortest_path_lengths, rpn_eval
Missing logic	2/3	powerset, lis
Missing ‘＋1’	2/3	find_in_sorted, levenshtein
Other code replacement	1/2	is_valid_parenthesization
Reference to an incorrect variable	1/3	hanoi
Wrong constructor call	1/1	get_factors
Missing boolean expression	1/4	detect_cycle
Missing line with call	1/4	depth_first_search
In this empirical study, the considered repair tools could not repair 7 bug types: (1) Incorrect logical operator; (2) Missing ‘’; (3) Incorrect array slice; (4) Incorrect method called; (5) Missing function call; (6) Missing Assignment; (7) Missing arithmetic expression.

We now study the reasons for what some types could not be repaired by any approach. We identify three main reasons: (1) No repair operator implemented; (2) No fixing ingredients; (3) Limitation of repair implementations.

No repair operator implemented. No repair tool repaired the bug type Incorrect logical operator from BITCOUNT program. The ground truth patch updates the a logical operator “Ω AÂ ” to “&”. Even if JMutRepair is able to generate patches that change logical and relational operators, it does not implement any mutation of the operator “Ω AÂ ”.

No fixing ingredients. The advantage of redundancy based repair approaches such as JGenProg, Arja or Cardumen is that they create patches from code already written in the buggy program. Those approaches could eventually repair a bug of type Incorrect logical operator if the patch’s code (in this case, a binary expression with an operator of &) is present in the buggy program. Unfortunately, that is not the case in BITCOUNT. A similar case happens with bug Missing function call: the ground truth patch for buggy program SIEVE replaces a method invocation  by . However, in that buggy program, there is no piece of code that invokes . As a consequence, the redundancy-based repair approaches considered in this empirical study cannot synthesize a fix.

Limitation of repair implementations. The buggy program LCS_LENGTH that has not one single bug, but two: incorrect array slice and missing boolean expression, and the ground truth patch modifies two different locations correspondingly. Even if, in theory, that buggy program could be repaired by Arja or JGenProg, we observe in practice they could not find a patch because it is a complex multi-location patch. Multi-location and multi-bug repair are indeed an open research challenge (Khaireddine et al., 2019).

Impact of test cases on the capability of program repair tools.
For the 16 repaired programs, 5/16 of them have only failing tests and no passing tests. To our knowledge, all benchmarks of the literature contain at least one passing test case. Here, our empirical study shows that program repair with only failing tests can be successful. There are four programs with no passing tests that are repaired by JGenProg, Arja and RSRepair. This clearly shows that generate-and-validate repair techniques do not require passing tests for synthesizing a patch. This is because the generate-and-validate repair tools, such as JGenProg, Arja and RSRepair do not need to infer semantic constraints from passing test cases. They generate the test suite adequate patch through searching the fixing ingredients regardless of passing test cases. However, not all passing test absent programs can be repaired by generate-and-validate approaches because of the three limitations we have presented above. On the contrary, the synthesis-based repair approaches, e.g., Nopol, require passing tests to infer semantic constraints. The absence of passing tests creates a degenerated synthesis problem that hampers repair effectiveness. To this extent, because it contains bugs with no passing test cases, QuixBugs is more appropriate for generate-and-validate repair techniques than for synthesis-based ones.

On the test failure symptoms of patches programs.
We have aggregated the failure symptoms of the 16 patched programs: 10 incorrect output, 4 stack overflow errors, 1 timeout/infinite loop error, and 1 null pointer exception. This confirms the results on Defects4J showing that program repair is effective not only for assertion errors. No repair approach could repair a bug exposed by a concurrent modification exception or an array index error. One possible explanation for this is that test suite based repair tools typically determine suspicious buggy locations based on the root cause of the test failures. For those bug types, the test failure symptoms make repair tools difficult to identify the right buggy locations. For example, the buggy program FIND_FIRST_IN_SORTED is a typical incorrect comparison operator bug, which requires the modification of a comparison operator from “” to “”, at a location that no repair tools identify as suspicious. This suggests the need for alternative fault localization strategies to handle more diverse test failure symptoms, such as pattern-based bug localization (Sun et al., 2019).

On the differences of program repair tools on other benchmarks.
Now, we compare the program repair tools’ differences depending on the benchmark, by comparing the repairability over Defects4J and Quixbugs. The considered Defects4J patches are those of Durieux et al. (2019), who executed on Defects4J with the same repair tools that we have also considered in this work. The results are moderately different in repairability rate, failure rate and proportions of duplicated patches.

First, the repairability rate is the percentage of unique repaired bugs over all bugs from a benchmark. The repair tools show a slightly higher repairability rate in Defects4J, i.e., 47.34% (187/395) (Durieux et al., 2019), than in QuixBugs, i.e., 40% (16/40). The higher repairability of Defects4J bugs could be explained, to some extent, in the fact that those bugs are larger (in LOC) than those from QuixBugs. This implies that redundancy-based repair approaches (e.g., Cardumen) have more fixing ingredients available to synthesize a candidate patch, increasing the probability of synthesizing a test suite adequate patch.

Second, we observe that the repair failure rate – the percentage of repair attempts that finished due to an error – is for QuixBugs (4.31%) compared to 21.08% for Defects4J (Durieux et al., 2019). The reason is that Defects4J compared to QuixBugs involves more modules and dependencies during the program execution, the complexity of Defects4J is higher and hits the limitation of the current automatic patch generation tools. This calls for future research to investigate the implementation of repair tools to mitigate the failure rate.

Third, in both benchmarks, the 10 considered program repair tools generate a large number of syntactically identical, i.e., duplicated patches, but in different proportions. Specifically, there exists 51% (19019/37224) duplicated patches on Defects4J and 77% (1132/1470) duplicated patches on QuixBugs. We suspect that the larger number of duplicates on QuixBugs is due to the small size of QuixBugs programs: the amount of fixing ingredients is fewer, those are less diverse than Defects4J, thus they are reused more frequently, and to produce more duplicated patches.


Download : Download high-res image (165KB)
Download : Download full-size image
4.3. Results for RQ3: Manual Patch Correctness Assessment
Fig. 2 shows the manual assessment results for 338 automatic program repair patches synthesized for 16 buggy programs of QuixBugs. The green and red legends indicate correct patches and overfitting patches, respectively. In total, 158 of 338 are classified as correct, and the remaining 180 patches are classified as overfitting by our manual assessment. Overall, per this ground truth based manual assessment, there are 7 out of 40 QuixBugs buggy programs are correctly repaired.

This means that the repair rate for QuixBugs is 17.5% (7/40 bugs are correctly repaired), which is respectively 8.1 percentage points and 9.1 percentage points higher than the state-of-the-art evaluations on Defects4J (21/224 bugs are correctly repaired) and IntroClassJava (25/297 bugs are correctly repaired) reported by CapGen (Wen et al., 2018). We explain the higher repair rate for the following two reasons. First, QuixBugs are small programs, this narrows down the search space of fixing ingredients, and allows for repair tools to precisely locate buggy lines and to find fixing ingredients for patch generation. Second, our empirical study considers more repair tools than previous ones (10 different repair tools) which mechanically increases the number of repaired bugs (Xiong et al., 2017, Yuan and Banzhaf, 2018, Jiang et al., 2018).

Now, we talk about the overfitting rate over the generated test suite adequate patches. According to our manual assessment, 53.3% (180/338) of patches for QuixBugs are overfitting. This further confirms that program repair tends to generate more overfitting patches than correct patches (Smith et al., 2015). Since our empirical study is on a new benchmark, this further strengthens the external validity of this important finding.


Download : Download high-res image (310KB)
Download : Download full-size image
Fig. 2. Manual assessment of 338 QuixBugs patches spread over 16 QuixBugs bugs . (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Notably, we observe that for 15 of 16 buggy programs repaired with test suite adequate patches, either all the generated patches are classified as correct or all overfitting. This suggests that all tools are identically impacted when the specification is weak. Moreover, recall that different repair tools have overlapping repair strategies. For example, Arja and JGenprog are both based on genetic programming search techniques to rearrange the ingredients already existent in the buggy program. The outlier is the eight generated patches for the DETECT_CYCLE program, all by NPEFix. NPEFix generates eight patches to fix the null pointer exception by adding null checks for variable hare. However, we observe that four of those patches, beyond fixing the original bug, introduce a new bug which is not exposed by the original test suite of DETECT_CYCLE. Thus, we consider those four patches as overfitting, and the other four, which do not suffer that problem, as correct.

Fig. 3 presents the number of programs that are correctly repaired per program repair tool. This means the number of programs for which there exists at least one correct patch according to the manual assessment we have done. The green and red legends indicate the number of buggy programs that are respectively correctly and incorrectly repaired per repair tool. We have the following observations: (1) All 10 repair tools are able to correctly repair at least one buggy program of QuixBugs; (2) Cardumen correctly repaired 3 buggy programs, which outperforms the other 9 repair tools; (3) Three repair tools contribute to more correctly than incorrectly repaired programs (Cardumen, RSRepair and JMutRepair), three tools perform the same number (Arja, Nopol and Dynamoth), and four tools produce more incorrectly than correctly repaired programs (NPEFix, JKali, Tibra and JGenProg); (4) Overall, 7 unique programs are correctly repaired by all repair tools, they are complementary to each other.

To the best of our knowledge, our study is the first ever to manually assess 338 patches for QuixBugs. Having this large amount of manually labeled patches is valuable: it paves the way to use machine learning techniques to do patch correctness prediction (Ye et al., 2019c).


Download : Download high-res image (169KB)
Download : Download full-size image

Download : Download high-res image (453KB)
Download : Download full-size image
Fig. 3. The number of buggy programs correctly and incorrectly repaired per repair tool. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)


Table 4. Number of patches classified as overfitting for QuixBugs programs with at least one patch.

Programs	# Generated	Overfitting patches detected
patches				Manual assessment
LIS	122	0	0	118	0
SHORTEST_PATH_LENGTHS	91	91	91	85	91
DEPTH_FIRST_SEARCH	59	58	0	55	59
QUICKSORT	28	0	0	3	0
SQRT	18	18	18	18	18
DETECT_CYCLE	8	0	0	8	4
POWERSET	3	3	3	0	3
IS_VALID_PARENTHESIZATION	1	1	1	1	1
FIND_IN_SORTED	1	1	1	1	1
HANOI	1	1	1	0	1
GET_FACTORS	1	1	0	0	1
NEXT_PERMUTATION	1	0	0	0	1
RPN_EVAL	1	0	0	0	0
KNAPSACK	1	0	0	0	0
LEVENSHTEIN	1	0	0	0	0
MERGESORT	1	0	0	0	0
Sum	338	174	115	289	180
4.4. Results for RQ4: Automated Patch Correctness Assessments
In this section, we analyze the results of automated patch correctness assessments. We compare them against the results obtained from the manual assessment. This comparison allows us to calculate the accuracy of the considered automated assessment techniques. Finally, we discuss their true/false positive and negative cases.

4.4.1. RQ4a: Patches classified as overfitting by automated patch assessments
Table 4 shows the overfitting patch assessment results produced by the three considered techniques over 338 generated patches. The first column gives the names of buggy programs patched by at least one repair tool. The second column shows the total number of generated patches over all tools. The third to fifth columns give the number of patches classified as overfitting by the three automated assessments. We present the manual assessment results in the last column. For example, the first row shows there are 122 patches generated for bug LIS,  and  identify all of them as correct patches (i.e., 0 overfitting patch), which is fully aligned with manual assessment results. However, the  classifies 118 of them as overfitting, which contradicts the manual assessment.

The automated patch assessment techniques classify patches as overfitting with different magnitude:  identifies 174/338 (51.5%) patches from 8 buggy programs as overfitting,  identifies 115/338 (34%) patches from 6 buggy programs as overfitting, and  identifies 289/338 (85.5%) patches from 8 buggy programs as overfitting. The differences between those numbers can be explained by the kind of information they collect, and are influenced by outliers. For example,  classifies more overfitting patches than the two other techniques, this is mostly because it classifies 118 patches for program LIS as overfitting while the other two techniques do not. On the contrary, , which classifies as overfitting less patches than the other two techniques, does not classify as overfitting any patch from program DEPTH_FIRST_SEARCH, while the other two techniques do it for at least 55 patches.

Now, we compare the results from automated assessment with those from manual assessment, which classifies as overfitting 180 patches. That comparison allows us to detect the misclassification of the automated assessments.

The Venn diagram in Fig. 4 shows the overlap between the different assessments. First, we observe that the manual assessment and the three techniques agree on the majority (105) of overfitting patches (the intersection of all circles). This shows that all considered automated assessment techniques can correctly identify, at least, 58.3% (105/180) of overfitting patches. Second, the assessment result from  is the closest to manual assessment with 174 of 180 cases. Overall,  is able to correctly classify the 174/180 overfitting patches. Third, all overfitting patches classified by  can be found by . The overlap between them is due to the similarity of the techniques, both based on the generation of test inputs. However,  is able to correctly classify 59 additional patches. Fourth,  classifies as overfitting the largest number of patches (289). However, that is due to the misclassification of 125 correct patches.

The three automated techniques achieve patch correctness assessment results consensus on the majority of buggy programs (9/16 programs). For three buggy programs, they agree on the overfitting diagnostic for all patches, for which our manual assessment also classified them as overfitting. Moreover, the three techniques achieve consensus on the absence of overfitting patches for 5 buggy programs (the last 5 rows of Table 4). However, according to our manual assessment, there is one patch for program NEXT_PERMUTATION which is actually overfitting.

4.4.2. RQ4b: Accuracy of automated patch assessments
The manual assessment enables us to compute the accuracy of the three automated correctness assessments.

Accuracy. Table 5 gives the results of this analysis. The first column gives the name of the automated assessment technique, and the second to fourth columns indicate the number of true positives, true negatives, false positives, and false negatives. The individual accuracy is given in the last column according to Eq. (1). For instance,  yields 174 true positives, 0 false positive, 158 true negatives, and 6 false negatives. This means there are 174 and 158 patches that are correctly classified as overfitting and correct, respectively. On the contrary,  fails to identify 6 overfitting patches.

The , , and  respectively achieve an accuracy rate of 98.2%, 80.8%, and 58.3%.  achieves the best accuracy among the three assessment techniques. This is inline with previous results having shown that Evosuite performs better than pure random test generation  (Shamshiri et al., 2016, Ye et al., 2019b, Kifetew et al., 2019). Also,  has better accuracy than , this is because  produces fewer false positives than  (0 versus 125).

True and false negatives.  and  report the same number of true negatives cases (158), which are correct patches not classified as overfitting. Now, we see that all three techniques have false negatives, which are overfitting patches classified as correct. In the case of test generation techniques such as  and , the false negative cases appear when the generated tests do not contain inputs that expose the incorrect behavior of an overfitting patch.  produces the most false negative cases (65). This shows the weaker effectiveness of the domain-specific generators developer to sample test inputs. Finally, no technique is able to identify all overfitting patches and have zero false negatives.


Download : Download high-res image (74KB)
Download : Download full-size image
We discuss a case of an overfitting patch that is not identified as such by any automated techniques. Listing 1 gives the patch of NEXT_PERMUTATION generated by JMutRepair. The bug is present in an if condition which compares the position of two elements in a list perm. The generated patch uses the “” operator to fix the bug. This patch is not identical to the ground truth version, which uses the operator “” instead. The manual assessment reveals that this patch is overfitting: if the list perm contains the same values, the behaviors of the JMutRepair patch and the ground truth patch differ and the programs produce different outputs. The aforementioned specific input was neither generated by  nor . Consequently, the JMutRepair patch is not classified as overfitting. This case study illustrates that automated correctness assessment cannot fully replace manual assessment.


Download : Download high-res image (533KB)
Download : Download full-size image
True and false positives. Table 5 shows that  has the largest number of true positive cases among the three assessment techniques.  classifies 59 and 10 more overfitting patches than  and . All overfitting patches classified by  and  are true positive cases (i.e., they do not suffer any false positive case). Those two approaches are 100% precise. On the contrary,  suffers from a large number of false positive cases (125). Thus, the precision of this technique is 56.7% (164/289). We identify two main reasons behind those false positives. First, the invariants may capture a specific value of the test case, instead of capturing the full range. For instance, an invariant may capture a.value == 0 instead of a.value  0 because only 0 is used in the test case. In this case, the invariant itself is overfitting and results in a false positive. Second, invariants detection is sensitive to procedure exit and entry points, where the preconditions and postconditions are obtained from. When a patched program adds new exit points (e.g., new return statements), all invariants that hold for the ground truth program are expected to hold at new exit points, otherwise, a patch is assessed as a violation, and this is the major reason for those false positives.

Listing 2 is an example of false positive for . Listing 2a is the ground truth patch for the buggy program QUICKSORT. The invariants captured from the ground truth program execution are given in Listing 2b. That ground truth patch modifies an operator, from “” to “”.

Our empirical study found two patches for this QuixBugs subject: one presented in Listing 2c classified as correct by , and a second one presented in Listing 2d classified as overfitting by the same assessment. Both patches modify the conditions of else block, using “else“ to replace “else if (x  pivot)“. These changes are semantically equivalent to those proposed by the ground truth patch, so the patches are correct.

We note that these two generated patches add redundant statements of if (arr.isEmpty(), which do not influence the correctness of the patches. However, those statements impact the correctness evaluation done by  because they introduce new exit points (new return statements). In Listing 2b, we can see that an invariant states a property for variable arr at the method exit point. The invariants in gray hold for the patch in Listing 2c, because the exit point return new ArrayList (); meets all captured invariants, all four properties captured for variable arr (e.g., arr[]  []). However, in the patch at Listing 2d, a new program exit point is added one line after the first exit point, and one invariant is not satisfied. When the arr is not empty, the program enters into the new exit point, and invariant arr[]  [] is violated.

To sump, capturing behavioral differences with invariants violations is hard. In particular, invariants that hold at a certain point in a program typically no longer hold in the patched program when new exit points that are added. This is an important caveat for assessing patches generated by genetic programming (e.g., JGenProg and Arja), which tend to generate many new statements with exit points. This suggests interesting future research directions to improve .


Download : Download high-res image (155KB)
Download : Download full-size image
5. Threats to validity
The major threat in our study lies in the manual correctness assessment, which may result in misclassification due to a lack of expertise or mistakes. This threat holds for all program repair results based on manual assessment. The best mitigation to this threat is to make patches and analyses publicly available. Then other researchers are able to further assess them, this is what we have done in our open-science repository: .

We note that manual assessment done over QuixBugs patches is less error-prone than for Defects4J and more complex benchmarks because: (1) QuixBugs contains well-known algorithms (e.g., QUICKSORT), thus the patch analyst does not need to be an expert in the buggy program’s application domain; (2) As presented in Table 2, QuixBugs programs are short, thus it is easier to read, debug and understand the generated patches.

The second threat is about construct validity. The automatic repair tools that we used in this empirical study could have bugs that prevent them to discover all possible patches. For this reason, the results we have reported are likely an under-estimation of the repairability of QuixBugs using automatic program repair.

6. Discussion
The benefits of using QuixBugs.
Studying QuixBugs provides two major benefits for the research community. First, it enables the community to make new findings about program repair that have never been reported in other benchmarks, that we will discuss next. Second, it strengthens the external validity of the findings previously found on other benchmarks.

The study presented in this paper enables us to identify the following new findings: (1) Generate-and-validate approaches are capable of generating patches for buggy programs with only failing test cases, while synthesis-based approaches cannot. The other benchmarks do not contain buggy programs with only failing tests, thus this finding has never been reported before; (2) 7 of 40 (17.5%) buggy programs of QuixBugs are correctly repaired, which is 8.1% and 9.1% higher than for Defects4J and IntroClassJava benchmarks, respectively (see Section 4.3); Our explanation is that, as the QuixBugs programs are smaller than for other benchmarks, the corresponding search spaces are also smaller, thus, the repair tools are able to navigate a bigger portion of the search space, increasing the probability of finding the patch; (3) The automated patch assessment technique  has the best effectiveness in our study, it is able to identify 98.2% overfitting patches.

Our novel empirical study on QuixBugs confirms the following findings found on other benchmarks, showing their generalizability: (1) Our study on QuixBugs confirms the existence of overfitting patches by a large amount; (2) Our study on QuixBugs confirms that both generate-and-validate approaches and synthesis-based approaches work, but on different bugs.

The potential future improvement of program repair tools.
Although the repair rate of QuixBugs is higher than the existing benchmarks of Defects4J and IntroClassJava, still 33/40 of buggy programs are not able to be repaired with correct patches. For instance, some bugs that could be repaired with a simple one-liner fix (e.g., GCD) are not able to be automatically repaired by any repair tool. Our empirical study suggests potential future improvements of program repair tools.

First, we observe the generate-and-validate approaches (e.g.,  Arja and JGenProg) based on genetic programming, commonly generating complex repair code. This results in redundant code being repeatedly used in multiple locations of the buggy programs, which are not necessary for repairing the bugs. The redundant and complex codes lead to the difficulty in patch understanding for researchers and developers. This has been discussed by Yuan and Banzhaf (2018), we advocate the future improvement in the simplicity of repair code into the search process of genetic programming algorithms.

Second, we observe that program repair tools tend to generate high-granularity patches, which results in the abstract syntax tree (AST) edit operations often appearing in the root nodes of the buggy statements. These coarse-grained repair operations are not effective for repairing finer-grained bugs, such as those that can be repaired by swapping two variables or replacing incorrect reference variables (e.g., BUCKETSORT and GCD). Thus, we consider lower-granularity patch generation will benefit future program repair research.

Third, program repair tools could benefit from more diverse fixing ingredients, including (1) New operators: for example, in Section 4.2 we explained the absence of operators for repairing BITCOUNT; (2) Repair patterns: some considered repair tools such as JGenProg and Cardumen only consider ingredients taken from the buggy program, which not enough to repair a bug; (3) New code synthesis mechanism: for example, a repair tool could use a new mechanism to create patches with visible and invisible method invocations, The visible and invisible method invocations are respectively referring to the method that exists in the buggy programs and widely used utility packages (e.g., Commons Math project). In the considered 10 repair tools, we do not observe any patches with an invisible method invocation.

7. Related work
7.1. Datasets of bugs
The benchmarks used in automatic program repair research include Introclass (Le Goues et al., 2015), ManyBugs (Le Goues et al., 2015), SIR (Do et al., 2005), Codeflaws (Tan et al., 2017), Defects4J (Just et al., 2014), IntroClassJava (Durieux and Monperrus, 2016b), Bugs.jar (Saha et al., 2018), Bears (Madeiral et al., 2019), and Defexts (Benton et al., 2019).

Smith et al. (2015) evaluate overfitting patches generated by GenProg and TrpAutoRepair on IntroClass. Le et al. (2018) systematically characterize the nature of overfitting in semantics-based automatic program repair on the IntroClass and Codeflaws benchmarks. Ke et al. (2016) evaluate SearchRepair on IntroClass. Qi et al. (2015), Mechtaev et al. (2016), and Long and Rinard (2015) evaluate repair approaches on ManyBugs. Stratis and Rajan (2016) evaluate their approach on SIR. Nguyen et al. (2013) propose SemFix and evaluate it on SIR. Mike et al. (2018) collect and analyze mutant quality indicators based on Codeflaws. Chekam et al. (2018) propose a new technique for mutant selection and evaluate their work on Codeflaws. Martinez et al. (2017) and Yu et al. (2019) report on their experiments using Defects4J. Xiong et al. (2017) propose the ACS repair tool and evaluate it on four projects of the Defects4J benchmark. Wen et al. (2018) propose CapGen, a context-aware patch generation technique and evaluate this technique on the Defects4J. Hua et al. (2018) propose and evaluate SketchFix on Defects4J. Wen et al. (2018) and Le et al. (2017) propose and evaluate their repair technique on IntroClassJava.

The recent work by Durieux et al. (2019) conducted a large scale empirical study on five benchmarks, Defects4J, Bears, IntroClassJava, Bugs.jar and QuixBugs. However, they do not provide any assessment for the generated patches.

To the best of our knowledge, our study is the first ever to assess 338 patches for QuixBugs.

7.2. Patch correctness assessment
Synthesizing new inputs for patch correctness assessment has been studied in a few papers. Xin and Reiss (2017) propose DiffTGen to identify overfitting patches with tests generated by Evosuite (Fraser and Arcuri, 2011). Those tests are meant to detect behavioral differences between a generated patch and a human-written patch.

Xiong et al. (2018) propose PATCH-SIM and TEST-SIM to heuristically determine patch correctness by comparing the execution similarity of the original and newly generated tests before and after the patch.

Yang et al. (2017) propose Opad and Gao et al. (2019) propose Fix2Fit, two approaches based on implicit oracles for detecting overfitting patches that introduce crashes or memory-safety problems. These two approaches for automatic patch correctness assessment cannot be applied to QuixBugs which contain Java programs with no low-level memory problems.

Tan et al. (2016) aim to identify the overfitting patches with predefined templates to capture typical overfitting behaviors. While their approach is static, our approach is dynamic. In our study, the test inputs are executed and the invariants are captured from program runtime behavior in order to detect overfitting patches.

Yang and Yang (2020) study invariants generation to infer behaviors of generated patches. Their study shows that the majority of plausible patches (92/96) expose different runtime behaviors. Our study also considers invariants based patch assessment, but at a much larger scale: first, our dataset is three times larger (338 versus 96 patches) and second, we also measure the accuracy and false positives which have not been done in Yang and Yang (2020).

8. Conclusion
We have presented a novel program repair empirical study, studying the QuixBugs benchmark (Lin et al., 2017) and 10 repair tools. We have compared three automated patch assessment techniques over 338 generated patches. Lastly, we have comprehensively studied the accuracy and false positives of the three considered assessment techniques. Our empirical study yields major findings for program repair research: (1) It is possible to repair programs with no passing tests at all (only failing test cases); (2) Patch assessment with  has the highest accuracy over the three considered techniques in discarding overfitting patches. Finally, our empirical study results in 338 patches with correctness labels, which is a valuable asset for future study on overfitting in automatic program repair.