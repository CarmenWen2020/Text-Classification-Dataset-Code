Abstract
Regression is one of the most commonly used statistical techniques. However, testing regression systems is a great challenge because of the absence of test oracle in general. In this paper, we show that Metamorphic Testing is an effective approach to test multiple linear regression systems. In doing so, we identify intrinsic mathematical properties of linear regression, and then propose 11 Metamorphic Relations to be used for testing. Their effectiveness is examined using mutation analysis with a range of different regression programs. We further look at how the testing could be adopted in a more effective way. Our work is applicable to examine the reliability of predictive systems based on regression that has been widely used in economics, engineering and science, as well as of the regression calculation manipulated by statistical users.

Keywords
Multiple linear regression
Metamorphic testing
Metamorphic relation

1. Introduction
Being a cornerstone in statistics, regression is a fundamental prediction technique. Due to its simplicity and robustness, it has been widely used in numerous disciplines of science and engineering as well as other fields such as economics and business. Once a regression model is established, we are able to adopt it for predicting expected values within a certain level of confidence.

Calculating regression coefficients is an essential statistical function of various software and systems. It has been integrated as part of both commercial software such as MATLAB (MathWorks), SPSS Statistics (IBM) and Excel (Microsoft) and open-source libraries including GNU Scientific Library (C/C++), Scikit-learn, Scipy, StatsModels (Python), Accord, MathNet (F#), and GoLearn (Go). On the other hand, new systems may be built from scratch in some situations, such as the development of statistical libraries in new programming language. Besides, some programmers may need to modify the regression, for examples, transforming the variables (Lin, 2000, Luu et al., 2015, Wu et al., 2017, Luu et al., 2018) or parallelizing the computations (Baboulin, 2006, Dau et al., 2019). In most cases, the calculation involves extensive computations. It is thus prone to programming bugs, and data mishandling. Blunders in computing regression by these software or systems would definitely cast doubts on the reliability and robustness of data and scientific analyzes based on them.

Advancements in machine learning enable us to cope with the growth of big data, having the fastest rate over recent decades (Goodfellow et al., 2016). Despite the rapid change, linear regression remains as the technique to be widely used due to two main reasons. First, it has been serving as a baseline for evaluating more sophisticated modeling systems for years (Weisberg, 2005, Montgomery et al., 2012). For example, simple linear regression is frequently adopted to assess the performance of various artificial neural networks (ANN) systems (Bibault et al., 2018, Ahmad et al., 2019, Mondal et al., 2020, Ayoubloo et al., 2011). Secondly, the ANN equations are reducible to the linear regression form, being referred to as deep linear network (Saxe et al., 2014, Lathuilière et al., 2019). Recently, the deep linear network has received lots of attention as it helps to better understand the properties and reliability of general ANN (Lathuilière et al., 2019, Lee et al., 2019, Bah et al., 2020). Reliability of many systems therefore somehow relies on the correctness of linear regression calculation.

Traditionally, the testing of regression system is difficult because we do not have a test oracle in general. For example, if we have some data points, say  and , and want to find a line that has the predicted values “closest” to these points. (In case of higher dimensions, the plane or hyperplane is used instead of line.) When the regression program returns the line as , we have no simple way to tell whether or not this line is the best fitted line. Testers may find the regression line manually by solving the required equations constructed from these data points. However, this is not viable and is very tedious.

Alternatively, to define a simple test oracle, one could start with generating the dependent variable from a known-but- withheld estimator, what is later adopted to assert the correctness of computed estimator. However, as all generated points are now strictly in the same line or hyperplane, the method does not cover more popular cases in which points are arbitrary or freely distributed. Another way is to directly solve the linear regression problem with a small amount of data. This task is sometimes non-trivial, even with few data, say, 4–5 sampling points, since deriving estimator requires the computation related to the inverse matrix. In general, we are unable to have in advance the exact estimator computed by a linear regression program for a dataset with non-zero residuals.

Testing such programs becomes even more difficult in reality because we often never know the true line due to round-off errors (Higham, 2002, Solomon, 2015). In other words, we are not able to determine whether or not the computed value from a regression algorithm is the exact estimator that should be returned by the algorithm. In fact, previous studies (Higham, 2002, Gratton et al., 2013, Marland et al., 2018) pointed out that solving the linear regression may cause a large accumulation of round-off and truncation errors because achieving the regression solutions requires intensive matrix calculations. In general, when the true value is unavailable, there is no direct way to validate the regression systems.

Incorrect linear regression computation may lead to severe consequences. In many cases, even a marginal difference in estimating coefficients of regression may cause a huge impact. For instance, while a previous study estimated the global mean sea level has been rising at a rate of 1.7 mm per year since the beginning of this century (Church and White, 2011), another work suggested that the rate should be 1.2 mm per year (Hay et al., 2015). Due to the acceleration of global warming, the two regression coefficients after 50 years will lead to a gap larger than 2.5 cm in the predicted sea level that miscounted multi billion dollars per year in flood damage cost, according to a recent research (Jevrejeva et al., 2018). Given that fact, while round-off error was attributed to the US Army’s Patriot Missile Defense System failure in shooting down the SCUD missile causing death of 28 American soldiers (Zhivich and Cunningham, 2009), the rounding is also known to deteriorate the accuracy of regression (Higham, 2002). Indeed, a faulty regression program may cause damages more severe than the round-off error. Validating the accuracy of regression is therefore not solely a computational issue but also has real-life impacts. Considering its very diversified range of applications, testing the correctness of regression systems is certainly of great importance.

In view of such problem, we should find an appropriate approach to tackle the testing of linear regression-based systems. The software testing technique of Metamorphic Testing (MT), does allow us to alleviate the absence of oracle, and to generate test sets automatically (Segura et al., 2016, Chen et al., 2018). It has been applied successfully in testing various software systems, such as numerical analysis (Chan et al., 1998), partial differential equations (Chen et al., 2002) search engines (Zhou et al., 2012, Zhou et al., 2016), Web Application Programming Interfaces (Segura et al., 2018a), feature models (Segura et al., 2010, Segura et al., 2011), scientific software (Ding et al., 2016, Lin et al., 2018), programming language compilers (Le et al., 2014), graphics shader compilers (Donaldson et al., 2017), context-sensitive middleware-based applications (Chan et al., 2005), object detection (Wang and Su, 2019), cybersecurity (Chen et al., 2016, Mai et al., 2019).

Recently, MT has been applied efficiently to validate various machine learning and artificial intelligence systems (Xie et al., 2011, Xie et al., 2018, Segura et al., 2018b). A testing tool named DeepTest was developed to detect the erroneous responses of autonomous car-driving systems backed by deep neural networks (DNN) (Tian et al., 2018). It helped revealing thousands of dangerous situations under different weather conditions affecting the functioning of DNN-based programs that were submitted to the Udacity self-driving challenge. Another example in testing a real-life driver-less car system named Apollo of Baidu (Zhou and Sun, 2019) pointed out that the random noises added will significantly alter the system’s responses, potentially leading to fatal crashes. In fact, MT has been evolving beyond the domain of conventional testing. It is extended to validation (Lin et al., 2018, Zhou and Sun, 2019), program understanding (Zhou et al., 2018) and quality assessment (Pullum and Ozmen, 2012). MT was also combined with “statistical hypothesis test” to deduce the likelihood of a system’s faultiness (Guderlei and Mayer, 2007). Despite of its many successes with complex systems, it remains unclear to what extent that MT is applicable to test linear regression-based systems.

The aim of this paper is to propose the use of MT to test multiple linear regression systems. We focus on testing the implementation of linear regression algorithm for three reasons. First of all, the algorithm is the core component of these regression systems. While testing a specific system may require a particular set of features and properties specific to that system, the technique for testing such system may not be applicable to test another system. For example, testing a C++ program may involve an explicit declaration of variables, while it is not required in testing a Python or Ruby program, making dissimilar outcomes in assessing the bugs associated with initializing variables in different systems. Secondly, we may inspire interested readers to come up with new intrinsic properties, or extend them for non-linear regression-based systems. Thirdly, it has been reported that many statistical users have mistakenly manipulated the regression calculation, and thus might derive incorrect predictive models. For example, many users came up with erroneous results while applying linear regression in Python (Ra, 2019, Sushod, 2019, Sam, 2018), Matlab (Bog, 2013) or R (Digit, 2013, Strictly, 2014) software, and it turned out that they manipulated the regression wrongly by mishandling the intercept or other coefficients. By considering our different ways to check the implementation of regression algorithm, they may be able to test their own manipulation process, and thus improve the reliability of their works.

For MT to work on the regression systems, testers or users need to know some intrinsic properties of the regression. These properties should be supported by formal “mathematical proofs” to avoid misuses. For example, there has been some reports about users adopting some properties that are not necessary properties of the algorithms to be implemented (Xie et al., 2011). This will cause misinterpretations of the results. One of the strengths of this paper is that we prove all these intrinsic properties of the regression systems.

Contributions of this paper are summarized as follows:

•
We derive a set of mathematical properties of estimator of multiple linear regression related to the addition of data points, the rescaling of inputs, the shifting of variables, the reordering of data, and the rotation of independent variables.

•
We then develop 11 Metamorphic Relation (MRs) which are grouped in 6 categories to support the testing of related regression systems.

•
The technique of mutation analysis is applied on a wide range of regression programs to examine the performance of these MRs.

•
We further examine how the testing of regression could be carried out in a more effective way.

In this study, the investigated MRs are all manually defined. As explored in recent studies (Zhang et al., 2019, Ayerdi et al., 2021), MRs could be systematically constructed. We anticipate that automatically generated MRs may be different to our manually generated MRs, which however is not the main theme of this study.

The structure of this paper is organized as follows. After the introduction, we start with recalling basic concepts of linear regression and MT. In Section 3, we present selected properties of estimator, and associated MRs that will later be used for verifying the regression estimates. Section 4 is on experimentation, presenting the mutation set-up, the generation of the input datasets, and the assessment criteria. The effectiveness of MRs is reported and discussed in Section 5, before a comparison between MT and random testing is presented in Section 6. We describe the threats to validity of our experiments in Section 7. Finally, the paper is enclosed by concluding remarks in Section 8.

2. Preliminaries
2.1. Multiple linear regression
Regression is a model to describe the behavior of a certain random variable of interest. This variable may be the price of stocks in the financial market, the growth of a biological species, or the detection opportunity of the gravitational wave. It is referred to as the dependent variable, and denoted with . Information of the dependent variable is provided on the basis of predictor or explanatory variables; such as time for the price of stocks, food for the species to grow or the number of observations for the detection of wave. These terms are usually called the independent variables, and denoted with 
. The regression model is to relate dependent variable to given independent variables by means of a function  (1)

Given the form of function , the regression model is said to be determined when the unknown parameter, being referred to as the estimator , is obtained. The optimal estimator 
 is chosen such that the modeled dependent variable 
 becomes closest to the true dependent variable . In multiple linear regression model, the variable 
 has a linear relationship with respect to the components of optimized estimator 
 by means of: (2)
where 
 in which the upper symbol  represents the transpose of the relevant matrix. Each individual variable 
 ( = 0, 1, …, ) can be, for instance, a trend rate, an oscillatory factor or a function in the multiple linear regression (Luu et al., 2015, Luu et al., 2018), but it must be independent of all other variables 
. The intercept form is adopted if we set 
 as a constant, say 
, in contrast with other input variables 
 that are fed with data. Then we have (3)

Assume we have  data points for the model, and denote 
 and  (all 
) as corresponding vectors of data for the variables 
 and , and 
 (
) is the vector of modeled dependent variable 
. Their matrices are expressed as (4)
 
 
 
 
The linear regression equation to derive 
 is (5)

To establish the model, a cost function (also named as objective function) is adopted to quantify the estimator 
 based on a certain metrics. The idea is to minimize the difference between the true  and the modeled 
, which is referred to as the residual (or error). In general, the cost function consists of the residual, the estimator, and different forms of parameters and weights. Some regression methods may add a penalty quantity associated with 
, or take into account the standard deviation. In the ordinary least square (OLS) regression, the cost function is simply defined as the total of square of residuals. The optimized estimator is determined by the minimization () of this cost function using the square of Euclidean norm (
), that is (6)
 
There are different ways to derive the solution of this equation such as derivatives of sum of square errors, maximum likelihood, projection, generalized method of moments. When the matrix 
 has the full rank, i.e. all its rows and columns are linearly independent, we have the unique solution derived from the normal equation: (7)
The upper numeric subscript () represents the inverse of a related matrix. Once the model is established after the derivation of the estimator 
, we may predict the change of dependent variable 
 under varying conditions of 
. In the prediction system based on linear regression, the core model is trained with a set of input data to obtain the optimized estimator (
), before it can be used to predict. A more comprehensive description about linear regression can be found in (Weisberg, 2005, Montgomery et al., 2012).

2.2. Metamorphic testing
Metamorphic Testing (MT) has been developed to alleviate the test oracle problem, which is referred to as the situations where test results of a program are impossible or extremely difficult to be validated. Consider an example of a program  that implements a particular algorithm  to find the shortest path from one node to another in a graph. Assume that  is a graph having 100 nodes, and that, on average, each node has about 20 edges. In general, given the graph  and two nodes  and  in , it is very time-consuming to verify that the program’s actual output  is really a shortest path in  from  to  because a manual process may involve validating against  possible paths connecting x and y.

The core of MT is the concept of Metamorphic Relations (MRs), which are derived from properties of the targeted algorithm to be implemented. If the program does not uphold these properties, we can conclude that the program has errors. Using the shortest path example mentioned earlier, assume further that ,  and  are three different nodes in . Based on the domain knowledge of the shortest path in a graph, one can derive the metamorphic relation – “if  is in the shortest path in  from  to , the length of the shortest path in  from  to  is equal to the sum of the length of the shortest path in  from  to  and that from  to ”. Since the program  is implementing a particular “shortest path” algorithm, one can expect that  upholds this metamorphic relationship in the sense that “If  is in the actual output , the length of  is equal to the sum of the lengths of  and . The original test case  is considered as a source test case in MT. The two test cases  and  are referred to as the follow-up test cases in MT because these test cases are follow-ups of the source test case. In this example, the MR involves one source test case and two follow-up test cases, and the follow-up test cases may depend on the actual output of the program with the source test case as input. In general, an MR can involve multiple source test cases and multiple follow-up test cases. Following is the formal definitions of MR and other concepts used in MT (Chen et al., 2018).

Definition 1

Let  be a target function or algorithm to be implemented. A Metamorphic Relation (MR) is a necessary property of  over a sequence having two or more inputs 
, where , and the sequence of corresponding outputs 
. The relation is denoted by 
, where  is the subset relation, and 
 and 
 are the Cartesian products of  input and  output spaces, respectively. We may simply adopt 
 to represent 
.

Definition 2

Consider an MR 
. Suppose that each 
 () is constructed based on 
 according to . For any , 
 is referred to as a source input. For any , 
 is referred to as a follow-up input. That is, for a given , if all source inputs 
 () are specified, then the follow-up inputs 
 () can be constructed based on the source inputs and, if necessary, their corresponding outputs. The sequence of inputs 
 is referred to as a metamorphic test group (MTG) of inputs for the MR.

Process for Metamorphic Testing (MT). Let  be an implementation of a target function . For an MR , suppose that we have 
. Metamorphic Testing (MT) based on this MR for  involves the following steps:

(1) Define 
 from  by replacing  by 

(2) Given a sequence of source test cases 
. After execution, their respective source outputs are given by 
. Construct and execute a sequence of follow-up test cases 
 with reference to 
, and obtain their corresponding follow-up outputs 
.

(3) Compare the executed results against the expectation given by 
. If 
 is not satisfied, then  is determined to be faulty by this MR.

In other words, to apply MT to test a program  using a given MR, we first need to generate the relevant source test cases. We then execute the program  with these source test cases to obtain their respective source outputs. Based on the given MR, we can generate the relevant follow-up test cases. After that, the program  is executed with these follow-up test cases to obtain the respective follow-up outputs. Finally, we can check whether the given MR is satisfied by the source test cases, the source outputs, the follow-up test cases, and the follow-up outputs. If the MR is violated, it means that the program under test  is faulty. This entire process of MT can be automated.

3. Metaphoric relations of estimator
In this section, we propose 11 MRs that can be used to test linear regression based systems. As mentioned earlier, they are derived from the properties of the targeted algorithm to be implemented. It is important to discuss the intuition of each property, understand the properties and explain how MRs could be derived from the relevant properties. Interested readers may find the mathematical proofs of these properties in Appendix. For ease of discussion and illustration, we will use 2-dimensional examples, if needed. Last, but not least, all properties discussed in this section involve the optimized estimator 
.

3.1. Properties of estimator
3.1.1. Property 1. Inserting new data
This property is about inserting an additional data point to the original set of data points for linear regression. Proposition 1 shows that, after a new data point is added to the original set of data points, the new linear regression line obtained from the new set of data points can be derived from a relationship with the original regression line and the new data point.

Proposition 1

Suppose that the dependent variable  is related to a linear relationship with independent variables 
 with the estimator 
 derived from the least square fitting. Let  and 
 (all 
) denote the vectors of data for the variables  and 
, respectively, whose matrices are expressed in Eq. (4). And let 
 and 
 be a data point being added into the original data set. The linear estimator 
 obtained from the new data set 
 
 and 
 
 can be derived from the following equation (8)
where  is the vector of size  defined by  and 
 as follows (9)
 

We now derive an important property that forms the foundation of the first two metamorphic relations, namely MR 1.1 and MR 1.2 in Section 3.2.

Corollary 1

Same notation as in Proposition 1. If 
, then 
.

Proof

Suppose 
. It is straightforward from Eq. (8) that 
.

In other words, if the new data point is generated from the model derived by the original data set, we do expect that the new model obtained from the new data set will be the same as the original model. For example, suppose we have 3 data points  in the data set for linear regression and that the linear regression system returns the line . If we use this line equation to generate a new data point, say (7, 15), and feed all 4 (original 3 plus this new) data points to the linear regression system, we expect that the new regression line obtained will be the same as the original one.

Corollary 2

Same notation as in Proposition 1. If (10)
 
 
(11)
 
 
 for , then 
 for the regression with intercept, being explicitly expressed in Eq. (3).

Proof

The summation of both sides of Eq. (3) gives us (12)
The linear regression with intercept is unbiased, that is (13)
 
 
As a result, from the definitions of 
 
 and 
 
, we obtain (14)
 
 
This equation can be rewritten in form of vector as 
. It follows immediately after Corollary 1 that 
.

3.1.2. Property 2. Scaling data
The second property is about the scaling of certain coordinates of data points. Proposition 2 shows that, if certain coordinates of data points along a certain axis are scaled by a given factor, the slope of regression line with respect to this axis will be scaled proportionally by the same factor.

Proposition 2

Suppose that the values 
 of the dependent variable are scaled by a factor of  with respect to the original values , and the values of an independent variables 
 being factorized by a factor of  with respect to the original values 
, that is (15)
(16)
 where the constants  and  are non-zero real numbers (). The new estimator 
 can be computed from the original estimator 
 as follows (17)
 
 

Based on this proposition, we can further derive two properties of mirroring the components of the data points, and two properties of scaling the components of the data points. These properties are the basis of four MRs, namely MR 2.1, MR 2.2, MR 3.1 and MR 3.2 in Section 3.2. The proofs of these properties are straightforward from Eq. (17). We will leave them to the readers.

Corollary 3

Same notation as in Proposition 2. If  and , then 
.

That is, if we reflect the sign of the dependent variable, we expect that the sign of estimator will also be reflected. This is the basis of our MR 2.1.

Corollary 4

Same notation as in Proposition 2. If  and , then 
 is given by the following equation (18)
 

More simply, if we only reflect a particular independent variable, only the component of estimator related to this independent variable is reflected. This is the basis of our MR 2.2.

Corollary 5

Same notation as in Proposition 2. If  and , then 
.

Intuitively, if we scale the dependent variable by a factor of , the estimator will also be scaled by the same factor. This is the basis of our MR 3.1.

Corollary 6

Same notation as in Proposition 2. If  and , then 
 is given by the following equation (19)
 
 

That is, if we only scale a particular independent variable by a factor of , only the component of the estimator related to this independent variable is scaled reciprocally. This is the basis of our MR 3.2.

3.1.3. Property 3. Shifting data
The third property is about the shifting of certain components of the data in the original data set. Proposition 3 shows that, if the coordinates of data points along a certain axis are shifted by a given distance, the projection of the regression line with intercept is also shifted by a proportional distance.

Proposition 3

Suppose that the values 
 of the dependent variable are shifted by a distance of  with respect to the original values , and the values of an independent variables 
 being shifted by a distance of  with respect to the original values 
, that is (20)
(21)
 where  and  are real constants (). In the linear regression with intercept, the new estimator 
 can be determined from the original estimator as follows (22)
 

Based on this proposition, we can further derive two properties of shifting the components of the data points. These properties are the basis of two MRs, namely MR 4.1 and MR 4.2 in Section 3.2. The proofs of these properties are straightforward from Eq. (22).

Corollary 7

Same notation as in Proposition 3. If  and , then 
 is given by the following equation (23)
 

In other words, if we add up the dependent variable by a value of , the intercept component of estimator will also be increased by the same value. This is the basis of our MR 4.1.

Corollary 8

Same notation as in Proposition 3. If  and , then 
 is given by the following equation (24)
 

That is, if we add up a particular independent variable by a value of , the intercept component of estimator will also be decreased by an amount proportional to both this value and the component of estimator related to this variable. This is the basis of our MR 4.2.

3.1.4. Property 4. Permuting data
The fourth property is about the permutation of certain components or certain samples of the data. Proposition 4 shows that, if the coordinates of the data points along certain axes are permuted, the corresponding projections of the regression line along these axes are permuted in the same order. Moreover, the proposition shows that, permuting the data points does not change the regression line.

Proposition 4

Suppose that the samples 
 of the dependent variable are permuted by the function 
 with respect to the original variable ; and the samples 
 of the independent variables are permuted by both functions 
 and 
 with respect to the original values 
, such that (25)
(26)
 where 
 is a permutation (bijective function) from set 
 to 
; whilst 
 is the corresponding bijective renumbering of the set of sample index . The new estimator 
 can be determined from the original estimator as follows (27)
 

Based on this proposition, we can further derive two properties of permuting the data points. These properties are the basis of two MRs, namely MR 5.1 and MR 5.2 in Section 3.2. The proofs of these properties are straightforward from Eq. (27).

Corollary 9

Same notation as in Proposition 4. If 
 is the identity function (i.e., 
) and 
 is a bijective function, then 
.

Put it differently, if we permute the data points only without permuting the dependent and independent variables, the estimator will remain unchanged. This is the basis of our MR 5.1.

Corollary 10

Same notation as in Proposition 4. If 
 is a permutation that only swaps 
 and 
 where  and , while 
 is the identity function, then 
 is given by the following equation (28)
 

That is, if we swap two independent variables and their relevant values in the data points, the corresponding components of the estimator will be swapped accordingly. This is the basis of our MR 5.2.

3.1.5. Property 5. Rotating data
The fifth property is about the rotation of coordinate system. Proposition 5 shows that, if the axes related to the independent variables of the data points are rotated by a given rotation angle, the corresponding projections of the regression line are rotated by the same angle.

Proposition 5

Suppose that the values 
 of the dependent variable is kept unchanged, while the components 
 of the independent variables are rotated by the matrix  of size . In other words,  rotates the matrix 
 with respect to the original matrix , that is (29)
Then the new estimator 
 can be determined from the original estimator as follows (30)

Based on this proposition, we can derive the property of rotating the data points. This property is the basis of the MR 6 in Section 3.2. The proof of this property is straightforward from Eq. (30).

Corollary 11

Same notation as in Proposition 5. Suppose two components 
 and 
 with  are rotated by an angle  in the counter-clockwise direction, that is (31) 
 in other words, the rotation matrix has the form (32)then the new estimator 
 is determined from the original estimator as follows (33)
 

That is, if we rotate two components of the data points by an angle, the corresponding component of the estimator will be rotated by the same angle.

3.2. Metamorphic relations
In this subsection, we will present 11 MRs for testing multiple linear regression systems. They are derived from properties presented in Section 3.1 and are grouped into 6 categories. We will describe the regression form with intercept first, and discuss the regression form without intercept later. Table 1 summaries these MRs and their applicable types of regression, and Fig. 1 illustrates the transformation examples for each MR.

Before we discuss the individual MR, let us introduce some notations for the regression with intercept. This regression is adopted as the default unless stated otherwise. First, let 
 denote the source input of a linear regression system under test (SUT), and it consists of a set of  data points (34)
where each data point 
 () can be denoted as (35)
in which 
 () is the x-coordinate and 
 is the y-coordinate of the point 
. The output of the SUT using the source input 
 can be expressed by (36)
 
The output 
 is referred to as the source output. That is, the equation of the regression form with intercept is (37)
Let us denote the follow-up input 
 be a set of  data points, being generated using a given MR, that is (38)
where (39)
for . The output 
 of the same program using the follow-up input 
 is expressed by (40)
 
The output 
 is referred to as the follow-up output. For convenience in describing the MRs, we refer 
 and 
 (
 and 
) to as the source (follow-up) data set and output, respectively.


Table 1. Summary of MRs and their applicable types of regression.

Category	MR	Name	Forma
Unchanged	1.1	Inserting a predicted point	I,C
 predictions	1.2	Inserting the centroid	I
Mirrored	2.1	Reflecting the dependent variable	I,C
 regression	2.2	Reflecting an independent variable	I,C
Scaled	3.1	Scaling the dependent variable	I,C
 regression	3.2	Scaling an independent variable	I,C
Shifted	4.1	Shifting the dependent variable	I
 regression	4.2	Shifting an independent variable	I
Reordered	5.1	Swapping samples	I,C
 regression	5.2	Swapping two independent variables	I,C
Rotated	6	Rotating two independent variables	I,C
 regression			
a
I: Regression with intercept as in Eq. (3) where 
; C: Constrained regression (without intercept) as in Eq. (2) where 
 is an input variable whose data are provided by user.


Download : Download high-res image (333KB)
Download : Download full-size image
Fig. 1. Simplified original points and their regression line (hyperplane) in (a) two-dimensional and (b) three-dimensional views, and their transformations due to (c) MR1.1, (d) MR1.2, (e) MR2.1, (f) MR2.2, (g) MR3.1, (h) MR 3.2, (i) MR4.1, (j) MR4.2, (k) MR5.1, (l) MR5.2, and (m) MR6. The origin of coordinates is denoted by the black point. The dashed gray circles represent original points, i.e. the source inputs. The open black circles denote transformed points, which may overlay the original points if there is no transformation. Newly added points are marked by the red circles. The follow-up inputs thus consist of transformed and added points. Black lines (gray hyperplanes) exhibit the follow-up regression lines (hyperplanes). The dashed red arrows sketch the transformation applied. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

In the context of MT, if 
, 
, 
 and 
 do not satisfy the relevant MR, the SUT is said to be faulty. Now, we are going to describe the MRs based on the previously mentioned properties, to be used in our experimentation to validate regression systems.

3.2.1. Category 1: Unchanged predictions
Two MRs in this category are based on Corollary 1, that is, adding a point predicted by the regression line into the original data set shall compute the same regression line.

MR1.1.Inserting a predicted point

The regression line will remain the same after being updated by adding a point selected arbitrarily from the line into the original data set. This MR is illustrated in Fig. 1c.

Given the source input 
 and source output 
, the follow-up input 
 is formed by adding a new point 
 to the source input 
, that is (41)
where (42)
such that 
 are arbitrary real numbers, and 
 is computed from these 
 () values and 
 using the following equation (43)
In other words, 
 is a point that falls in the “predicted” regression equation. Then we expect to have the follow-up output be the same as the source output. (44)

To sum up, the follow-up estimator is the same as the source estimator when a point predicted from the source model is included in the follow-up estimation. Instead of a single point, we can add several new points at the same time for the regression. The add-up will preserve the source estimators as long as the points are derived from the regression model.

It is noted that, in practice, floating-point computations may give us slightly different values due to rounding. We thus need to consider the impact of round-off error in determining the violation of each MR. For example, instead of having the equality 
 in this MR, the comparison should be relaxed by an inequality that accounts for an error tolerance. This will be explained in details in Section 4.3 (Assessment).

MR1.2.Inserting the centroid

The linear regression line will remain the same after being updated by adding the centroid into the original data set for the intercept form, as illustrated in Fig. 1d. The centroid can be derived from the arithmetic mean of all values of its data points, which can be easily computed. This point belongs to the linear regression line.

Given the source input 
 and source output 
, the follow-up input 
 is formed by adding the centroid point 
 to the source input 
, that is (45)
where (46)
is such that 
 and 
 are the averages of the corresponding values in the source data 
, that is (47)
 
 
for . If the regression program is configured to run with intercept, then we have (48)

In other words, adding the centroid of data into the source input to form a follow-up input will not change the follow-up estimator for the regression form with intercept.

3.2.2. Category 2: Mirrored regression
The MRs in this category are based on Corollary 3, Corollary 4 with the scaling parameter be set as . This set of MRs is related to how reflecting the data points will reflect the regression line accordingly.

MR2.1.Reflecting the dependent variable

Reflecting the points over a certain -axis will reflect the regression line over the same axis, as illustrated in Fig. 1e.

Given the source input 
 and source output 
, the follow-up input 
 consists of  points (49)
where for each 
 (), the value of its x-coordinate remains unchanged (that is 
, ) and its y-coordinate is reflected, that is (50)
Then, we have (51)

That is, the follow-up estimator is a reflection of the source estimator when the sign of the dependent variable is reversed in the follow-up input set.

MR2.2.Reflecting an independent variable while keeping the others unchanged

Reflecting the points over the -axis will reflect the regression line over the same axis, as illustrated in Fig. 1f.

Given the source input 
 and source output 
, the follow-up input 
 is defined to consist of  points (52)
where for each 
 (), the x-coordinate of an independent variable, say 
, is reflected while the x-coordinates of other independent variables and the y-coordinates remain the same, that is (53)
 The follow-up output 
 (54)
 
is determined by the following equation (55)
 

In a nutshell, if the sign of an independent variable is reversed, the sign of corresponding component of the estimator should also be reversed.

3.2.3. Category 3: Scaled regression
The MRs in this category are also based on Corollary 5 (MR3.1) and Corollary 6 (MR3.2) for an arbitrary positive scaling factor. When we scale particular coordinates of data points along an axis by a positive factor, the slope of regression line will be scaled accordingly.

MR3.1.Scaling the dependent variable

After the points are scaled in the y axis by a given factor, the regression line will be scaled by the same ratio, as shown in Fig. 1g.

Given the source input 
 and source output 
, the follow-up input 
 is defined to consist of  points (56)
where for each 
 (), the values of its x-coordinates remain unchanged (
, ) and its y-coordinate is scaled by a factor  (), that is (57)
Then, the sign of follow-up output 
 would be scaled by the same factor (58)
In other words, when values of the dependent variable is scaled by a given factor, all components of the estimator will be scaled proportionally by the same factor.

MR3.2.Scaling an independent variable while keeping the others unchanged

After the points are scaled in a particular x axis by a given factor, the slope of regression line will be scaled reciprocally with respect to that axis, as illustrated in Fig. 1g.

Given the source input 
 and source output 
, the follow-up input 
 is defined to consist of  points (59)
where for each 
 (), the x-values of an independent variable, say 
, are scaled by a factor  (), while the values of other independent variables and the dependent variable remain the same, that is (60)
 and (61)
The follow-up output 
 (62)
 
would be defined by the following equation (63)
 
 

In other words, if the values of an independent variable are scaled by a given factor, the corresponding component of the predicted estimator would be scaled by the reciprocal of the scaling constant.

3.2.4. Category 4: Shifted regression
The MRs in this category are based on Corollary 7, Corollary 8, and applicable only for the regression with intercept. The intercept of the regression line will accumulate all modified distances if you shift the points along an axis.

MR4.1.Shifting the dependent variable

When the points are shifted by a given distance along the y axis, the regression line will be shifted along this axis by the same distance, as illustrated in Fig. 1i.

Given the source input 
 and source output 
, the follow-up input 
 is defined to consist of  points (64)
where for each 
 (), the value of its y-coordinate is shifted by a distance  (65)
Then, the follow-up output 
 (66)
 
would be defined by the following equation (67)
 

In brief, if a constant is added into the values of dependent variable, the intercept of the new estimator would be increased by the same value.

MR4.2.Shifting an independent variable while keeping the others unchanged

When the points are shifted by a given distance along a certain x axis, the regression line will be shifted in parallel along this axis accordingly, as illustrated in Fig. 1j.

Given the source input 
 and source output 
, the follow-up input 
 is defined to consist of  points (68)
where for each 
 (), the value of an independent variable, say 
, is shifted by a distance  while the x-coordinates of other independent variables and the y-coordinates remain the same, that is (69)
 The follow-up output 
 (70)
 
would be defined by the following equation (71)
 

So, if a constant is added into values of an independent variable, the intercept component of follow-up estimator would be decreased by an amount equal to the product of the constant and the value of the corresponding component of source estimator.

3.2.5. Category 5: Reordered regression
The MRs in this category are associated with Corollary 9, Corollary 10, such that reordering the axes of data points may need changes of the order of axes of the regression hyperplane.

MR5.1.Swapping samples

Swapping any two data points does not alter the regression hyperplane, as illustrated in Fig. 1k.

Given the source input 
 and source output 
, suppose that we swap two data points, say 
 and 
 (), to define the follow-up input 
 which consists of  points (72)
where (73)
 Then, the follow-up output 
 would be the same as the source input 
, that is (74)

As a result, we would have the same estimator no matter how we change the order of data points for the regression.

MR5.2.Swapping two independent variables while keeping the others unchanged

Swapping two axes does not actually change the essence of regression hyperplane in individual axes, as illustrated in Fig. 1l.

Assume that two axes of the data points, say 
 and 
 () are swapped. Given the source input 
 and source output 
, the follow-up input 
 is defined to consist of  points (75)
where for each 
 (), the values of its coordinates are determined as follows (76)
 and (77)
Then, the follow-up output 
 (78)
 
would be defined as follows (79)
 

In other words, swapping two independent variables while keeping the others unchanged will only swap the two relevant components of the follow-up estimator.

3.2.6. Category 6: Rotated regression
This MR is based on Corollary 11, which specifies how the regression hyperplane will change after rotating axes related to independent variables.

MR6.Rotating two independent variables while keeping the others unchanged

When the points are rotated in the plane perpendicular to the y axis, the regression hyperplane will be rotated accordingly, as illustrated in Fig. 1m.

Suppose that the axes of two independent variables, say 
 and 
 (), are rotated by an angle  in the counter-clockwise direction. Given the source input 
 and source output 
, the follow-up input 
 is defined to consist of  points (80)
where for each 
 (), the values of its x-coordinates are defined by the following equation (81)
 and the values of its y-coordinate remain the same, that is (82)
Then, the follow-up output 
 (83)
 
would be defined by the following equation (84)
 

Basically, if we rotate the axes of any two independent variables by an angle, the corresponding components of estimator would be also “rotated” by the same angle.

3.3. Testing scheme
The process to test the linear regression system using MT is shown in Fig. 2. The steps are summarized as follows:

1.
Preparing source input: Preparing the source input 
 to test the SUT. In testing the regression system, the source input consists of data points associated with independent and dependent variables 
.

2.
Obtaining source output: After executing the SUT using the source input, we obtain the source output 
. Here, the source output contains 
.

3.
Generating follow-up input: We generate the follow-up input 
 from the source-input 
 and the source-output 
 with the reference to the given MR. For instance, we can create a follow-up input by adding a new point, say  predicted from the regression line  as given in the fourth paragraph in Section 1 as an example. Now that the follow-up input has all these 4 points:  and .

4.
Obtaining follow-up output: We execute the SUT using the follow-up input to get the follow-up output 
. For the regression program, the follow-up output is 
.

5.
Validating MR: We compare 
 and 
 against the MR. If it is violated, the SUT is concluded to be faulty. As per above example, we require the follow-up output 
 and source output 
 to have 
 for  according to Eq. (44) for MR1.1 while in practice we may further consider the computational error for asserting this equality. In general, all sets 
 and 
 can be adopted to validate the MR.

4. Experiments
4.1. System under test and mutation analysis
To measure the effectiveness of these identified MRs, mutation analysis is applied in this study. Since our focus is to test the implementation of linear regression algorithm, mutation analysis is a good option because it allows us to evaluate the effectiveness of MT against a wide range of possible faults. It should be noted that we have not customized the testing to a particular system. As explained earlier in the introduction, while testing such system is important, it requires a particular set of features and properties specific to the system that may not be applicable to be used to test other ones. Moreover, each real-life system may subject to a specific type of faults, which is unable to give us a comprehensive overview of effectiveness of MT.

For the mutation analysis, we used five C/C++ regression programs, referred to as Press, Vijayan, Oscar, Quinn–Curtis, and Barr. Five programs are shortlisted from a collection of 15 programs after applying our predefined set of 5 criteria: (i) The execution of the program should be fast in our dedicated computer; (ii) The chosen program must contain full source code that implements multiple linear regression; (iii) The program must be open source or freely accessible; (iv) The code is searchable from Google with predefined keywords; and (v) The compilable program can run independently without relying on external libraries. Press is a standard regression program provided in the well-known C++ textbook “Numerical Recipes 3rd Edition: The Art of Scientific Computing” (Press et al., 2007). Published by William H. Press and colleagues, the source program consists of fitsvd.h and svd.h, having a total 430 lines of code (LOC). Two programs, namely Vijayan and Oscar, are from the GitHub repository. Vijayan is a C program developed by Vijayan (2017), having a total number of 350 LOC. Oscar is coded by Oscar Hamilton, consisting of 3 different files main.c, matrix.c and matrix.h, and having a total number of 650 LOC (Oscar, 2019). Another two programs are from the websites of two academic institutions, Quinn–Curtis from Pazmany Peter Catholic University (Hungary) and Southern Methodist University (United States). Quinn–Curtis has 430 LOC and originally supplied by Quinn–Curtis company (Quinn-Curtis, 1992); while Barr program has 590 LOC made by Barr (2006). All programs are then complied using the gcc 4.2.1 associated with Apple LVM version 10.0.0 (clang-1000.11.45.5), and run on a system of iMac computer (OS version 10.13.6).


Table 2. Summary of generated mutants, compilable mutants and non-equivalent mutants used for testing, being classified by mutated keywords.

Mutation category	Mutated keywords	Description	Generated	Compilable	Used
array_construct	( ), (*0), (*(−1)), (*2)	Mistaken construction of array	498	293	98
array_index	[], [-1+], [1+], [0*]	Misplaced array index	1512	1506	894
array_swap1
[i], [j], [k], [0], [1]	Misplaced element of one-dimensional array	1452	1222	710
array_swap2
[i][j], [j][i], [i][k], [k][i], [j][k],[k][j]	Misplaced elements of multi-dimensional array	505	412	292
condition_if	if(), if (!), if(true), if(false&&)	Illogical branching condition	45	24	16
condition_index	i=, j=, k=	Misused counter in loops	200	170	111
condition_loop	break, continue, {;}	Discontinuation of loops	14	14	7
data_complex
Doub, MathDoub/I, VecDoub/I	Wrong data type for numeric array	472	6	0
data_simple
Doub, Int	Wrong data type for numeric variable	94	23	11
function_parameter	(), (*0), (*(−1)), (*2)	Mistaken use of function numeric parameter	1832	824	356
function_return	return 0, 1, −1, 2, -1*, NULL	Wrong value of function’s return	168	103	39
logic_combination	, &&, &, , && !	Illogical combination of conditions	430	90	45
logic_comparison		Mistaken logical operators	1575	1336	564
logic_disable	?,  false ?  true ?	Mistaken logical return	14	14	1
logic_not	!, & Mistaken negation operator	31	8	5	
logic_value	true, false	Mistaken logical condition	2	2	0
math_increment	++, - -, +=2, -=2	Misused incremental operator	600	522	223
math_initial	=0, =1, =2	Wrong assignment of initial value	226	226	96
math_operator 
+, -, *, /, %	Wrong use of mathematical operators	4800	2097	840
math_values 
+1, −1, +2, −2	Mistaken incremental values	285	285	122
Total	14755	9177	4430
When a mutant belongs to two groups at the same time, it will be manually categorized. A mutant belonged to both array_swap1 and array_swap2 will be only classified as array_swap2; math_operator and math_values will be only classified as math_values.

Customized data types for matrix, vector, floating and integer numbers (Press et al., 2007).

Given a program, if we modify the original version by making a small syntactic change, we will obtain a mutant of the original program. The small syntactic change is usually referred to as a mutation operator. Table 2 lists the mutation operators we used to generate the mutants of our subject program. These mutation operators mimic typical programming mistakes. We developed a tool to systematically apply the mutation operators in Table 2, one at a time, to generate the mutants for our experiments. Each mutant is a result of one application of the mutation operators. We applied mutation operators one at a time because of three reasons: (1) using programs with lower failure rate (that is, single mutation operator rather than multiple mutation operators), the effectiveness of a testing method can be better understood; (2) a method that reveals simple faults (caused by single mutation operators) can likely detect complex faults (caused by multiple mutation operators), which is referred to as the software testing’s coupling effect (Offutt et al., 1996); and (3) using single mutation operator is a practice that has been used in the majority of existing studies (DeMillo et al., 1978, Offutt, 1992, Wah, 1995, Offutt et al., 1996, Jia and Harman, 2011).

After using the mutation generation tool, we have obtained a total number of 14755 mutants from 5 different original programs, namely Press, Vijayan, Oscar, Quinn–Curtis, and Barr (Table 3). Among these mutants, only 9177 of them can be successfully compiled using the system’s gcc 4.2.1 compiler (Table 3). Their categories of mutation types are presented in (Table 2).

Another major issue of mutation analysis is to eliminate the equivalent mutants, or more precisely, the mutants that are equivalent to the original programs. A mutant is equivalent to the original program when the outputs of the mutant and the original program are exactly the same for all possible inputs. There are various situations that mutants generated from applying mutation operators can be equivalent to the original program. For example, changing the operator from “” to “” in the statement “if (x0) then y=2*x else y=x” will lead to an equivalent program. Empirical studies pointed out that about 10%–40% generated mutants are equivalent, subject to original program and mutation tool used (Jia and Harman, 2011). There are usually two methods to determine a mutant is equivalent to the original program. The first one is manual inspection. For this method, manually inspecting 9177 mutants, one at a time, to judge whether the inspected mutant is equivalent to the original program is too time consuming and resource intensive. The second method is to compare whether the outputs of the mutant and the original program are the same for all possible input values. As for linear regression program, there are infinitely many inputs. Hence, this second method is still practically infeasible. Nonetheless, we adopt a working definition of an equivalent mutant: A mutant is equivalent to its original (linear regression) program if the outputs of the mutant and the original program are the same for 100 randomly generated inputs.


Table 3. Summary of programs and number of mutants generated, compilable and non-equivalent (used).

Program	Generated	Compilable	Non-equivalent
Press	6127	3592	2299
Vijayan	2163	1226	439
Oscar	1419	1213	686
Quinn–Curtis	2803	1533	446
Barr	2243	1613	560
Total	14755	9177	4430
Our working definition uses “100 randomly generated inputs” to determine the equivalence rather than using “all possible inputs”. We chose 100 because we have performed a sensitivity analysis on the effect of the number of remaining non-equivalent mutants versus the number of randomly generated inputs. Fig. 3 plots the number of remaining non-equivalent mutants versus the number of randomly generated inputs from 1 to 100. Please be reminded that, for linear regression program, the input is a set of data points for linear regression calculations. We can see that the number of remaining non-equivalent mutants changes from 2265 (63.1% of 3592) with 1 randomly generated input to 2286 (63.6%) with 10, to 2299 (64.0%) with 100 for the Press program (Fig. 3). In general, we observed from Fig. 3 that there are 21 more non-equivalent mutants if we adopt 100 randomly generated inputs instead of 10 for all programs. This number accounts for only 0.2% of the total number of compilable mutants. We anticipated that there will still be some non-equivalent mutants found if we use 1000 instead of 100 randomly generated inputs in our definition. However, the additional detections will be very minimal and negligible based on the trends that we observed in Fig. 3. Using our working definition of “equivalent” mutant, we can further remove 4747 mutants from the successfully compiled 9177 mutants. As a result, we have 4430 non-equivalent mutants for our experiments.

4.2. Datasets
We have generated 100 source datasets for the mutation analysis. The size of a dataset used for testing is basically characterized by two factors: the number of independent variable () and the number of sampling points (). We randomly choose the numbers of independent variables  in between 2 and 16, and the number of samples in between 20 and 200. These values are adopted not only because they are often used in practice (Wu et al., 2017, Luu et al., 2015, Luu et al., 2018), but also because of the availability of resources.

Then the independent variables 
 and the dependent  are randomly generated using normal distributions. Each one of these variables, say , is a set of  points 
 () that have the mean value 
 
 and the deviations 
 (85)
 
(86)
 where  is a function to generate random numbers in a normal distribution with mean 0 and variance 1. The range  is a fixed value for each , being selected randomly in between from 0 to 100. The maximum signal-to-noise ratio () is set as  (that is, 10%). Such a configuration allows some independent variables to vary significantly, while keeping others more stable. The data 
 for the variable 
 () are generated by (87)
 
for each sampling point 
 (). Here 
 
 is a random number but a constant for each set of 
. Data for  are computed from (88)
in which the components of original estimator  are known-but-withheld values, being generated randomly with the given size. The variants 
 (), derived from Eq. (85), are added into the estimator. Without these variants, the computed estimator is the same as the original estimator, i.e. 
.

For testing, we adopt the linear regression form with intercept (Eq. (3)) as all MRs could be applied. All random numbers are bounded by the range of . In testing a mutant, a dataset is regarded as the source input.

4.3. Assessment
We validate the relation  against the sets 
 and 
 to determine whether the MR is violated. For instance, assume we have the source output 
 for a given source input 
. If we create a follow-up input 
 by adding a point generated from regression line obtained from the source output as per Eq. (43) and reusing all original points, then we expect the follow-up output 
 to have the condition 
 according to Eq. (44).

Round-off errors always exist in regression computation that affects the accuracy of estimator (Higham, 2002). As a result, we are unable to know the true values of estimator. However, we can estimate the bound of error in the computed estimator. Due to the round-off error, a manually predefined bound for each program to determine the violation of MR may be inappropriate in our study. Instead, we use a general and automatic way to obtain the error bound associated with the estimator, namely, forward error (Higham, 2002). It takes advantage of the relationship between the input (independent and dependent variables) and the output (estimator), and hence it does not require a manually defined round-off threshold. In this study, we have adopted the estimation of forward error bound  associated with the estimator as suggested by Higham (2002). It is approximated by the product of backward error and condition number bounds of linear regression. We adopt the backward error bound in Waldén et al. (1995) and Chang et al. (2020), and the condition number estimation from Winkler (2007). The forward error bounds for the source (
) and the follow-up (
) are used to determine whether the computed estimators 
 and 
 are acceptable solutions of the regression program that finds the true value 
 and 
, respectively. As for above-mentioned example, a buggy mutant is regarded to be revealed when the source (
) and follow-up (
) estimators satisfy the following relationship (89)
that is, the relevant MR is said to be violated.

For the measurement of failure detection effectiveness of an MR, we use the metric of ratio of violation which is defined with reference to a set of mutants 
 and a set of MTGs for this MR, namely 
. Each mutant 
 is executed using every 
 to see whether the MR is violated or satisfied by this pair of 
. The ratio of violation for this MR is defined as follows (90)
 
In this study with the exception in Section 6, we only consider pairs of 
 of which the outputs have the correct numeric formats in calculating the ratio of violations. Such pairs are referred to as the survived pairs of 
.

5. Effectiveness of metamorphic testing
5.1. Synoptic effectiveness


Download : Download high-res image (24KB)
Download : Download full-size image
We have carried out a total number of 9.746  millions of individual experimental executions. As shown in Table 4, the numbers of mutants survived after having both source and follow-up test cases with correct numeric output format range from 3127 to 3146 (which are averaged from 100 MTGs) over the total of 4430 (=9177-4747) non-equivalent mutants used. In other words, a quarter of 4430 non-equivalent mutants can be detected as faulty just by simply checking the formats of outputs.

We then look at how MT can help reveal failures for the remaining mutants. Table 5 presents the ratios of violations of all 11 MRs. It shows that all proposed 11 MRs are effective in detecting failures, having an average ratio of violations equal to 31.64%.


Download : Download high-res image (31KB)
Download : Download full-size image

Table 4. Statistics of the number of mutants of which the outputs have correct numeric format for a given MTG (out of a total of 4430 non-equivalent mutants and over 100 samples of MTGs).

Statistics	MR	
1.1	1.2	2.1	2.2	3.1	3.2	4.1	4.2	5.1	5.2	6	
average	3127	3140	3139	3138	3135	3125	3146	3139	3140	3135	3140	
median	3128	3130	3131	3128	3123	3116	3137	3128	3131	3126	3130	
minimum	3067	3073	3075	3078	3063	3063	3077	3072	3074	3071	3071	
maximum	3196	3257	3246	3239	3251	3236	3263	3258	3237	3233	3249	
standard deviation	29	39	39	38	40	38	39	39	38	38	38	
In other words, our results show that for one MT-based test execution, there are on average approximately 31.64% chances to reveal a failure. This is quite high, despite that we have adopted a “conservative” approach to consider only the survived pairs of mutants and MTGs. Even with such a conservative approach, the least effective MR still has a ratio of violation of 8.68%. That is, this least effective MR can reveal failures with the chance of about one out of 11 cases. The finding suggests that our 11 MRs can serve as the benchmark for testing regression systems.


Table 5. MR’s ratio of violations.

Summary	MR	
1.1	1.2	2.1	2.2	3.1	3.2	4.1	4.2	5.1	5.2	6	
Number of survived pairs	312673	313994	313938	313804	313501	312480	314582	313914	314029	313520	314004	
Number of violation	154735	115810	27260	66983	49708	134575	133330	150516	44125	92715	121803	
Ratio of violation	49.49%	36.88%	8.68%	21.35%	15.86%	43.07%	42.38%	47.95%	14.05%	29.57%	38.79%	
5.2. Individual performance


Download : Download high-res image (27KB)
Download : Download full-size image
It is found that the effectivenesses of these MRs vary significantly. In testing the selected regression programs, the most effective relation (MR1.1) has a ratio of violation that is 5.5 times higher than the lowest one (MR2.1), as shown in Table 5. The best ratios are achieved by MR1.1, MR4.2, MR3.2 and MR4.1, each of which individually is able to reveal failures with a chance higher than 40% (Table 5).

MR1.1 constructs the follow-up input by adding predicted points to the source input. The experimental data show that it has 50% chances in revealing a failure. The runner-up is MR4.2, which has the ratio of violation of 48% and is associated with the shifting of an independent variable by a given distance. Based on the scaling of an independent variable, the MR3.2 also achieves a ratio as high as 43%.


Download : Download high-res image (32KB)
Download : Download full-size image
The wide range in performance indicates that the selection of “good” MR is important in testing the regression system. To further understand why there are such differences, we examine the performance of MR over different types of bugs with the following question.

5.3. Effectiveness for certain types of bugs


Download : Download high-res image (29KB)
Download : Download full-size image
A regression system may consist of different types of bugs when the software engineer implements the algorithm. Therefore, it is useful to quantify the effectiveness of MRs for different types of mutation, which is linked to programming bugs in practice. For this purpose, we categorize the type of buggy mutants detected by first tallying the total numbers of survived pairs of mutant and MTG (Table 6) and then obtaining the ratio of violations for each mutation group corresponding to each MR (Table 7).
The effectiveness of MR is subject to the type of defects existed in a system. In our situation, MR1.1 can help revealing 73% the bugs associated with the wrong construction of array (array_construct). Their runner-ups for this type of mutation are MR4.1, MR4.2, MR1.2, and MR3.2, which have the ratio around 60% or higher. MR1.1 is also able to further detect a half or more of buggy mutants associated with wrong handling of logics and branching (logic_combination, 71%; logic_not, 67%; and conditional_if, 62%) one-dimensional array (array_swap1, 50%), misuse of arguments for function and procedure (function_parameter, 59%), bugs in computing mathematical values (math_values, 53%; math_increment, 52%; and math_operator, 49%) as well as misplaced indices of array (array_index, 48%).


Table 6. Number of survived pairs of mutant and MTG for each mutation group.

Mutation group	MR	Total
1.1	1.2	2.1	2.2	3.1	3.2	4.1	4.2	5.1	5.2	6	
array_construct	5951	5642	5661	5661	5655	5636	5648	5642	5658	5656	5659	62469
array_index	65106	66295	66043	65980	66214	65951	66342	66219	66130	65917	66252	726449
array_swap1	57510	58054	58069	57994	57745	57676	58148	58004	58024	57956	57985	637165
array_swap2	19948	20075	20068	20054	19802	19863	20093	20063	20037	20032	20053	220088
condition_if	550	548	550	552	547	549	549	550	549	552	552	6048
condition_index	4276	4313	4293	4307	4319	4291	4320	4309	4314	4318	4315	47375
condition_loop	346	348	346	348	348	344	348	344	348	348	348	3816
data_simple	602	606	606	606	524	604	606	605	606	605	606	6576
function_parameter	28079	28111	28308	28284	28308	27983	28314	28116	28232	28165	28031	309931
function_return	2165	2098	2099	2099	2099	2099	2099	2099	2099	2099	2099	23154
logic_combination	3435	3435	3435	3435	3435	3435	3435	3435	3435	3435	3435	37785
logic_comparison	35879	36163	36089	36067	36160	36069	36155	36142	36116	36065	36156	397061
logic_not	300	300	300	300	300	300	300	300	300	300	300	3300
math_increment	8616	8510	8434	8404	8491	8441	8502	8490	8458	8406	8498	93250
math_initial	8958	8964	8964	8966	8966	8965	8966	8963	8969	8966	8969	98616
math_operator	61515	61040	61226	61247	61094	60844	61258	61148	61262	61223	61266	673123
math_values	9437	9492	9447	9500	9494	9430	9499	9485	9492	9477	9480	104233
Maximum value of each row is underlined.


Table 7. Mutation group’s ratios of violations.

Mutation group	MR	Average
1.1	1.2	2.1	2.2	3.1	3.2	4.1	4.2	5.1	5.2	6	
array_construct	72.59%	65.19%	0.09%	2.72%	32.73%	58.53%	71.44%	67.07%	8.13%	10.77%	59.73%	40.81%
array_index	47.58%	32.01%	10.61%	27.9%	11.52%	42.78%	38.0%	45.87%	14.45%	31.27%	34.29%	30.57%
array_swap1	49.87%	33.33%	10.66%	26.36%	11.64%	46.33%	38.94%	50.25%	18.42%	31.92%	35.83%	32.14%
array_swap2	45.42%	34.78%	12.54%	26.22%	10.96%	47.61%	33.42%	44.68%	22.72%	32.24%	37.28%	31.62%
condition_if	61.64%	52.01%	0.36%	25.0%	0.0%	27.87%	62.66%	57.27%	3.64%	59.06%	59.6%	37.19%
condition_index	44.6%	31.35%	16.82%	22.8%	21.93%	47.35%	35.23%	46.39%	27.93%	27.74%	29.87%	32.0%
condition_loop	4.91%	0.57%	0.0%	0.57%	0.0%	0.0%	1.72%	0.58%	0.0%	4.31%	4.02%	1.51%
data_simple	52.16%	34.82%	0.17%	0.17%	41.98%	52.81%	52.97%	63.47%	0.33%	12.23%	14.85%	29.63%
function_parameter	59.28%	57.05%	0.61%	20.95%	38.68%	52.28%	59.13%	58.41%	12.66%	22.99%	57.29%	39.93%
function_return	36.86%	31.32%	0.0%	17.53%	1.05%	25.87%	39.07%	31.06%	1.76%	25.54%	26.44%	21.49%
logic_combination	71.47%	67.25%	0.0%	27.95%	12.14%	51.67%	72.17%	72.05%	7.86%	55.92%	66.32%	45.89%
logic_comparison	43.33%	31.05%	8.01%	12.57%	10.33%	34.24%	36.35%	39.54%	9.69%	24.97%	31.53%	25.6%
logic_not	66.67%	66.67%	0.0%	0.0%	0.0%	66.67%	66.67%	66.67%	0.0%	0.0%	66.67%	36.36%
math_increment	52.33%	43.43%	6.84%	15.72%	19.39%	49.67%	46.53%	51.53%	18.94%	42.92%	46.61%	35.81%
math_initial	44.24%	22.79%	12.18%	10.21%	14.3%	39.34%	40.3%	43.45%	15.17%	20.58%	25.79%	26.21%
math_operator	48.79%	38.01%	9.01%	19.12%	16.85%	39.02%	45.26%	46.9%	10.02%	29.15%	40.33%	31.13%
math_values	52.95%	35.34%	6.03%	10.71%	19.16%	36.29%	40.54%	48.04%	12.24%	38.88%	44.91%	31.37%
Maximum percentage of each row is underlined.

On average, all MRs have 30% or more chances to reveal failures in 13 out of 17 mutation groups (Table 7). Note that the ratios of violation of MR for 2 mutation groups (condition_loop and logic_not) are not as reliable as others because they consist of a much smaller number of pairs 
 for testing.


Download : Download high-res image (38KB)
Download : Download full-size image
5.4. Nature of data manipulation


Download : Download high-res image (40KB)
Download : Download full-size image
We observe that modifying the values of dependent variable may reveal less failures than changing the independent ones (Table 5). Scaling the dependent variable (MR3.1) has the ratio of violation to be one third of the ratio of the squeeze of the independent variable (MR3.2) (Table 5). Reflecting the dependent variable (MR2.1) also has the ratio that is half of the changing the sign of an independent one (MR2.2). Shifting the dependent variable (MR4.1) likewise has a lower ratio (by 5%) than shifting the independent one (MR4.2).
On the other hand, we found that the scaling of the dependent variable (MR3.2) has a higher ratio of violation than the simple change of the its sign (MR2.2), although they all originate from the same property  of estimator. In addition, swapping independent variables (MR5.2) is more effective than swapping the data points (MR5.1).


Download : Download high-res image (95KB)
Download : Download full-size image
6. A comparison with random testing
Regression systems have no test oracle (also referred to as the untestable systems). That is, for any input, we are unable to validate whether or not the computed result is correct. Nevertheless, for some special or trivial inputs, their outputs may be well known prior to computations. Thus, it is common to test an untestable system with such special or trivial test cases (Chen et al., 2004). Such an approach is of limited capability because the amount of such special inputs is negligible as compared with the number of all feasible inputs. Another way that we can notice incorrect results, is by the occurrence of crashes which include improper halting of the program execution, overflow; or by having a “long runtime”, which is significantly longer than the duration normally needed to execute a regression program, given the same input. In this study, if a mutant has the runtime that is 1000 times longer than the one of original program in all 100 test cases, it will then be regarded as having a long runtime, and hence will be considered as a failure. Irrespective of whatever method used to generate test cases, they face the same problem that incorrect results could only be revealed through the occurrence of crashes. In fact, crashes are often caused by omission of checking some conditions or omission of implementing some functions, where white-box test case generation is generally less useful because it cannot generate test cases from existing code to detect the fault related to missing code. Furthermore, current literature does not show which test case generation method is the best to lead to crashes. Therefore, we chose the simplest but also the most unbiased technique, namely, random testing (RT), as a benchmark for comparison with our method. That is, we try to answer the following research question:


Download : Download high-res image (26KB)
Download : Download full-size image
In doing so, we take into account the fact that linear regression systems perform many mathematical computations, and thus it is possible for a randomly generated data to cause the program under test to run into run-time computational errors such as being improperly halted, producing non-numeric outputs (i.e., ‘not-a-number” (NaN)), having long runtime, or outputting wrong data size. They are the criteria to assert the effectiveness of RT in the comparison.

For a fair comparison between MT and RT, we need to resolve four issues. First, we need to use the same number of test cases for both testing methods. Since our 11 MRs use one source test case, which is randomly generated, and one follow-up test case, which is computed based on the relevant MR, we need to use two randomly generated test cases in RT when applying MT once. We denote this random testing approach as R2, with the ‘2’ indicating that we are using two randomly generated test cases for comparison with MT.

Secondly, we need to deal with the issue of determining whether the program under test succeeds or fails for random testing. For testing a program that implements linear regression with randomly generated data, if the program returns a linear regression model (that is, an estimator 
), there is no way that we can tell whether the program succeeds or fails, due to the lack of test oracle. On the contrary, if the output of the program consists of errors such as not-a-number, empty value, null output, and incorrect dimension of estimator, we know that the program fails. Therefore, we adopt this “working definition” of revealing a failure for a mutant by a randomly generated test case. However, this causes another issue that we cannot use the same approach as in our experimentation with MT in Section 5 which is based on the survived pairs 
 that have estimators in correct numeric format . As a result, we have to perform our comparison experiment using all possible (443000) pairs of mutants and test sets instead of employing only survived pairs as used in Section 5. Note that in MT, the test set is referred to as 
 associated with a MR; while in RT, the test group consists of a pair of random datasets.

Thirdly, we need to deal with the issue whether the difference in performance between MT and R2 is really caused by the MR and not by the “randomly generated source test case”. To resolve this, we reuse the randomly generated source test case in MT as the first randomly generated test case in our R2. By doing this, we have a common ground for better comparison between MT and R2. The second test case in R2 will be generated randomly whereas the follow-up test cases in our MT will be generated based on each individual MR. As a result, for each source test case, we have 11 follow-up test cases, one per MR.

Fourthly, we perform the following process to compare MT with R2 100 times to avoid pre-mature conclusions with just a few instances of comparison. We generate an R2 test set and 11 individual MT test sets, one per MR, using the approach discussed earlier. For each of the 4430 mutants, we execute the mutant with these test sets. We then record whether a mutant is revealed as failure by a particular test set. Each test set has only two test cases as mentioned earlier. For R2, a pair of mutant and test set can be referred to as failure if the mutant fails on any test case in the test set. For MT, the pair of mutant and MTG is regarded as failure for either one the following two reasons. One scenario is that the program runs into “error” with either the source test case or the follow-up test case. The other scenario is to have the associated MR to be violated. We then define the extended ratio of failure detection with reference to a set of mutants and a test set for the performance comparison between of R2 and MT as follows (91)
 
Altogether, we have a ratio for R2 and a list of 11 ratios for MT, each per MR over 100 test sets.

The random testing (R2) using two test cases has the extended ratio of failure detection of 32.76% (Fig. 4). The R2 ratio is about two thirds of the ratios of our MT, whose median is about 55.26% and mean is 51.59% (Fig. 4). It is half the ratio (64.34%) of the most effective MR. Such results allow us to assert that MT outperforms random testing in the testing scenario in which both random testing and MT involve the same number of test cases.


Download : Download high-res image (31KB)
Download : Download full-size image
MT outplays R2 because it utilizes the relationship between two outputs together with their inputs for validation. If we adopt more effective MRs (MR1.1, MR3.2 or MR4.2), the extended ratio of failure detection can be even higher, with the range of 59%–64%. Although this comparison (using all 4430 non-equivalent compilable mutants and 100 datasets) between MT and R2 gave us certain insights into the effectiveness of MT, it is worth noting that R2 is not able to cope with test oracle problem that MT is designed for.
Differential testing has been suggested by some researchers as a testing method for untestable systems. It is actually the fault tolerance technique of N-version programming. It requires more than one implementation for the same problem. First, this requirement may not be satisfied in practice, while RT and MT are not limited by this constraint. Secondly, there are two possible test outcomes returned by all the implementations, namely, identical outputs or different outputs. However, neither of these two scenarios can definitely determine whether or not the implementations under test are correct. On the other hand, if the test outcome from RT is a crash or if the test outcome from MT is an MR violation, the implementation under test is guaranteed to be incorrect. For these two reasons, it is useful to compare MT with RT, but not fruitful to compare MT with differential testing.


Download : Download high-res image (90KB)
Download : Download full-size image
Fig. 4. Comparison of effectivenesses of Metamorphic Testing (MT) against random testing (R2) using the set of 4430 mutants and 100 datasets. The percentile of MT is constructed from the ratios of failure detection of all 11 MRs. The green triangles and the green number represent the arithmetic means.

7. Threats to validity
Among factors that may threaten the internal validity of our study, the main concern is about the faults of the source programs being used to conduct the experiment. Prior to the mutation analysis, the source programs have been subject to a careful manual review, whilst its complied version has been tested rigorously in both MacOS and Windows operating systems against random datasets. Together with the fact that these programs have been well tested before being published (we only modified those statements which are related to input and output functions, and such changes had been carefully checked), no fault in MR violation after a total number of 500 test cases (100 cases per program) indicates that such programs are reliable to be used in our mutation analysis.

The second factor that may affect the causal relation in our study is the experimental bias, which is mainly attributed by the datasets selected for the testing and the uniqueness of mutants. To address this concern, we have randomized not only the regression values, but also the number of independent variables and the size of dataset, so that the impact of sampling bias is the most minimal. In addition to the comparison against the programs with 100 different datasets, we have spent a large amount of time in comparing the sources of mutant, as well as examining their semantics to be confident that each mutant used is unique. Besides, while the mutation tool allows us to arguably obtain reasonable results with a large number of non-equivalent executable mutants (4430), the ratio of violation depends on the source program and metamorphic test groups involved. The mutation keywords used (Table 5) were reasonably generic, so that they are applied to generate mutants for a wide range of programs efficiently (Table 3). In comparison with random testing, the “extended” effectivenesses of MT and R2 might not be the same as Fig. 4 in case a different number of test sets is used. However, a large number of test cases (4.873 millions for MT and 0.443 millions for RT) implies that the difference between MT and R2 is likely to remain significant.

Another factor that may have an impact on the rigorousness of the results is the determination of MR violation. With the involvement of round-off errors and floating point arithmetic, we can only know the true values of the estimator are within a numeric interval instead of an exact value. This will lower the ratio of violations when the forward error bound is large. However, we believe this trade-off is necessary because it helps us avoiding the type-II statistical error, i.e., the false positive assessment asserting that a non-faulty program is faulty. It is noted to mention that while large condition number may reduce the precision of forward error, the experiments with 5 different programs against 100 random datasets return no single false-positive result. In addition to the above-mentioned experiments with 100 datasets, we further extended the testing up to 1000 datasets for the program that has the largest number of mutants (Press), and no single false-positive case was identified. Such rigorous validations confirm that our estimation of forward error is reasonable.


Download : Download high-res image (110KB)
Download : Download full-size image
Fig. 5. The effectiveness of MT in testing different programs. The green triangles represent the arithmetic means; and the open black circle denotes the outlier. The percentiles are constructed from the ratios of violation of 11 MRs.

The main concern with the external validity is about whether our method is applicable beyond the mutation analysis. In this study, it is worth mentioning that the mutation analysis is not based on a single but a total of five different programs, in which MT are effective in all cases as shown in Fig. 5. Furthermore, the use of mutation analysis could still give trustworthy results by generating mutants which are similar to real-life faults (Andrews et al., 2005). Further studies with real-life applications are worthy. This will require a close collaboration with software developers.

8. Concluding remarks
In summary, we have proposed a novel testing approach to validate the implementation of multiple linear regression algorithm by taking advantage of the MT. The benefit of the approach is that it alleviates the problem of absence of test oracle, which is a major constraint in verifying the regression system. The originality of this paper is that we have quantified the change of estimators under different transformations from their intrinsic mathematical properties. Formally backed by the mathematical proofs, the established formulations allow us to devise the follow-up values of estimator after adding data points, scaling inputs, shifting variables, reordering data, and rotating independent variables. We then take advantage of these properties to propose 11 different MRs to test multiple linear regression systems as well as simple linear regression estimations.

To examine the effectiveness of ascribed MRs, we have applied the testing on a set of five different regression programs. The mutation analysis uses 4430 non-equivalent mutants. A total number of 100 datasets are generated for testing, with a total of 9.746 millions of experimental executions. We found that all developed MRs are effective in revealing failures. Some MRs are more effective than others, which help detect up to nearly half of survived pairs of 
. The effectiveness of MR varies in accordance with the mutation type. And in general, MT is shown to be better than random testing. The 11 MRs proposed in this study can serve as a benchmark of MRs for testing new regression systems. In addition, statistical users can take advantage of the method to ensure that there is no mistake in their process manipulating the multiple linear regression.

Predictive systems based on regression are commonly adopted in many disciplines including economics, engineering and sciences. Our future work will be to apply and extend this proposed technique to validate such systems and explore the possibility of newly developed techniques that help identifying the MRs automatically.