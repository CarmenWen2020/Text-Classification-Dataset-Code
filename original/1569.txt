The quality assurance for machine learning systems is becoming increasingly critical nowadays. While many efforts have been paid on trained models from such systems, we focus on the quality of these systems themselves, as the latter essentially decides the quality of numerous models thus trained. In this article, we focus particularly on detecting bugs in implementing one class of model-training systems, namely, linear classification algorithms, which are known to be challenging due to the lack of test oracle. Existing work has attempted to use metamorphic testing to alleviate the oracle problem, but fallen short on overlooking the statistical nature of such learning algorithms, leading to premature metamorphic relations (MRs) suffering efficacy and necessity issues. To address this problem, we first derive MRs from a fundamental property of linear classification algorithms, i.e., algorithm stability, with the soundness guarantee. We then formulate such MRs in a way that is rare in usage but could be more effective according to our field study and analysis, i.e., Past-execution Dependent MR (PD-MR), as contrast to the traditional way, i.e., Past-execution Independent MR (PI-MR), which has been extensively studied. We experimentally evaluated our new MRs upon nine well-known linear classification algorithms. The results reported that the new MRs detected 37.6–329.2% more bugs than existing benchmark MRs.

Previous
Keywords
Metamorphic testing

Metamorphic relation

Machine learning program

Linear classifier

Algorithm stability

1. Introduction
Machine learning has been widely applied in a great number of computational fields over the past few years. In addition to those successful applications, such as spam email filtering, item recommendation, and image recognition, machine learning has also been intensively applied recently to some mission-critical tasks, e.g., medical diagnosis, financial distress forecasting, and autonomous driving (Chen and Du, 2009, Kononenko, 2001, Menze and Geiger, 2015). However, despite its popularity in application, the quality assurance of machine learning, especially for its kernel learning programs, is still missing adequate attention, while such assurance plays a vital role in the life cycle of deploying machine learning systems.

Machine learning systems typically deploy previously trained models from learning programs to provide smart-decision services. Much recent research has targeted on the deployed machine learning models, and found that they could be vulnerable to adversarial attacks (Chakraborty et al., 2018). In order to validate the reliability of such machine learning systems, existing work has mostly emphasized on testing those trained machine learning models for deployment. For example, a series of testing techniques based on neuron coverage have been proposed for examining whether a machine learning model, e.g., deep neural networks (Ma et al., 2019, Ma et al., 2018a, Pei et al., 2017, Xie et al., 2019), could make wrong predications upon dedicatedly designed inputs. With quite a few reported successful cases, existing work, however, lacks enough attention on the quality assurance for learning programs themselves, which is actually the foundation for the quality of thus trained models. Even worse, such an overlooking of the learning programs could cause its generated models to keep suffering from unknown quality problems.

To this end, in this article, we focus particularly on the quality of machine learning programs themselves, and aim to effectively detect potential bugs in them (such programs typically refer to the implementations of relevant machine learning algorithms). We say that this problem has been more or less overlooked by many machine learning researchers and developers. The reason is that they tend to naturally believe that implementing machine learning programs faithfully according to respective algorithms should not be a big problem. Besides, in practice, when the accuracy of a trained model is somewhat low, developers could tend to attribute the problem to their incorrect settings of specific algorithm hyperparameters rather than to possible implementation bugs. On the other hand, since a trained model’s quality problems are likely to be caused by the potential bugs of its corresponding learning program, fixing these bugs would be extremely useful and of great advantage to avoiding future problematic trained models. Nevertheless, although testing such machine learning programs is vital and critical, it is not that easy.

One of the key obstacles to testing machine learning programs is the oracle problem (Barr et al., 2014). Considering that a typical machine learning program contains two parts, namely, training and predicting, we focus mainly on the training part, which is typically more complex and tend to contain bugs, while the remaining predicting part commonly refers to classic searching and reporting functionalities upon trained models from the training part. With this setting, the input of a learning program is the training set, and its output is the model parameter for the thus trained model. The training set usually consists of instances sampled from an unknown distribution, and the model parameter is usually a high-dimensional vector. Hence, it is impossible to automatically compute a correct model parameter on a dataset randomly sampled from this unknown distribution based on the learning algorithm (a.k.a. oracle problem). In other words, one cannot easily obtain the expected output of a learning algorithm for any given input. Even if considering the overall accuracy for the model instead of giving detailed model parameters, one can only obtain a rough estimate (or with) of the accuracy (Valiant, 1984).

The other key obstacle to testing a machine learning program is the statistical nature of its implemented machine learning algorithm. A machine learning algorithm is usually designed on the basis of statistics, which should be inherently capable of hiding errors (Cheng et al., 2018). For example, in a classification task, suppose that the model obtained by a machine learning program may get an accuracy of 90%. One may confidently consider that this learning program is good (even bug-free) due to this acceptable accuracy. However, there is still a possibility that a bug resides in its implementation, and fixing this bug may further improve the accuracy to 95%. Note that this is an interesting observation that bugs of learning programs may not always cause an accuracy reduction and they could instead increase the accuracy in some cases. 1

Therefore, the accuracy itself can give wrong hints on the existence of bugs in machine learning programs. This observation thoroughly reveals the difference between traditional program bugs and machine learning program bugs, and it also challenges the efficacy of traditional testing techniques that detect program bugs through observing a trained model’s performance. Besides, it also reminds us of considering what properties of learning algorithms should be used to effectively detect bugs.

In this article, we focus on testing the programs of linear machine learning algorithms, which have been widely applied in popular machine learning applications, such as logistic regression, support vector machine, and linear discriminant analysis (Balakrishnama and Ganapathiraju, 1998, Cortes, 1995, Hilbe, 2009). Some existing work (Dwarakanath et al., 2018, Xie et al., 2009, Xie et al., 2011) has proposed to apply metamorphic testing to alleviate the oracle problem in testing learning programs. However, they are restricted by the following limitations: (1) their selected properties (i.e., metamorphic relations or MRs), e.g., shuffling the training data, do not produce different models, thus would not reveal the kernel statistical nature of learning programs, and are likely to be not that effective on detecting bugs in machine learning programs; (2) their considered properties for metamorphic testing sometimes lack theoretical guarantee and can be accidentally violated. Therefore, we in this article aim for a nice property that should touch the kernel statistical nature of learning programs, and provide a theoretical guarantee for the effectiveness of such MR-based testing.

At first glance, it is a very simple and even trivial issue. But in fact, the existing methods are still not sufficiently effective. The main reason is due to the target property of metamorphic testing, i.e., metamorphic property. The metamorphic property fundamentally determines the efficacy of methods, but the currently selected properties lack statistical characteristics and theoretical guarantee. Thus it still needs further exploration for the metamorphic testing of machine learning programs.

Moreover, by investigating existing MR usages across different fields, we observe that MRs could be divided into two categories according to how they are formulated, namely, Past-execution Independent MR (PI-MR) and Past-execution Dependent MR (PD-MR). They differ in whether the follow-up input generation in MR depends on specific characteristics in past executions (e.g., output in the last execution). For example, in PI-MR, the generation of a follow-up input, a.k.a. metamorphic transformation, would depend only on its corresponding source input, while in PD-MR, the dependency would expand to both the source input and source output, making the transformation non-trivial. According to our observations and experiences with MR-based testing, our second focus in this article is to exercise and explore PD-MR for better effectiveness in testing machine learning programs.

To sum up, we in this article aim to (1) find a fundamental property associated with the kernel statistical natural of machine learning algorithms, (2) formula the property as the form of PD-MR, and (3) enable theoretical guarantee on the formulated MRs. We would consider the kernel stability nature of linear classification algorithms, and encode it into two PD-MRs with theoretical guarantee, aiming to verify the fundamental property of machine learning programs.

Systematic empirical evaluations show the high effectiveness of our proposed MRs on bug detection. We conducted experiments on programs of nine well-known linear classification algorithms. To simulate bugs in learning programs, we generated 1265 mutants through an improved version of the Python mutation tool mutmut. The extensive experiments showed the nice effectiveness of our MRs on detecting bugs that disturb the algorithm stability. Compared with the six state-of-the-art MRs proposed by existing work (Dwarakanath et al., 2018, Xie et al., 2011, Xie et al., 2020), our MRs can be much more effective in detecting bugs, with an improved mutant killing rate ranging from 37.6% to 329.2%.

The rest of this article is organized as follows. Section 2 introduces background knowledge used in this work, including machine learning foundations, linear classification algorithms, and metamorphic testing. Section 3 elaborates on how we encode the stability of linear classification algorithms into two PD-MRs. Section 4 evaluates our proposed MRs with nine algorithm implementations. Finally, Section 5 discusses related work in recent years, and Section 6 concludes this article.

2. Background
In this section, we introduce some background knowledge involved with our work, including the machine learning foundations, linear classification algorithms, and metamorphic testing.

2.1. Machine learning foundations
Machine learning is the study of computer algorithms that improve system efficiency through experience (Mitchell et al., 1997). Typically, a machine learning algorithm is designed to build a model that gives predictions or makes decisions based on the previous observation. For details, it would train a model based on a labeled dataset (called training set) and expect such a trained model to generalize its prediction ability to data that it has not seen before. Such generalization ability is the key property of a machine learning algorithm.

In machine learning fields, the probably approximately correct (PAC) learning theory is used to provide a formal description of an algorithm’s generalization ability (Valiant, 1984), by calculating a boundary of a trained model’s generalization error. For example, in a binary classification task, assume that a training set  is sampled from a distribution  independently. In this case, given a trained classification model or classifier , its empirical error denotes the accuracy of this classifier  with respect to the training set : (1)where  is an indicator function that takes value  when  holds and takes value  if otherwise. Then, the generalization error is calculated by the accuracy of classifier  on the distribution  as follows: (2)Thus, we can bound the gap between the empirical error  and the generalization error  through the HoefIding inequality (Hoeffding, 1994): (3)

Although there have been several theoretical studies similar to HoefIding inequality in  (3) when estimating the generalization errors, they cannot give analysis results for specific algorithms. To overcome this weakness, one starts to explore the stability of algorithms (Shalev-Shwartz et al., 2010) to describe how the output of a specific algorithm changes when modifying its input. For a machine learning algorithm, its input refers to a training set, and the modifications upon it can be normally divided into two categories:

•
Obtaining a new training set  by replacing th example in the training set : 

•
Obtaining a new training set  by removing th example from the training set : 

Therefore, as can be seen from the above equations, another advantage of stability analyses is that only the training set needs to be used.

Furthermore, the equivalence between stability and generalization error bounds has also been extensively studied (Bousquet and Elisseeff, 2002, Devroye and Wagner, 1979, Kearns and Ron, 1999, Mukherjee et al., 2006). Therefore, based on such equivalence studies, stability can be considered as a core property of machine learning algorithms, and it may be more ideal in application.

2.2. Linear classification algorithms
Since a multi-class classification task can be decomposed into multiple binary classification tasks, regardless of whether one-vs.-one or one-vs.-all strategy is applied, we simply consider a binary classification task here. Given any example , the goal of classification is to use its features (or attributes), i.e., , to identify which class it belongs to. A linear classifier attempts to make such classification decisions based on the value of a linear combination of these features. For details, the classifier  provides a weight vector denoted by , and a bias term denoted by . The decision logic of classifier  is (4)The weight vector  and bias term  are computed based on the training set. Let  be the training set. Then, the classifier parameters  are obtained by solving the following optimization problem: (5)where function  is the (surrogate) loss function, such that  can achieve the lowest misclassification error on the training set, and function  is the regularization to avoid overfitting (Bühlmann and Van De Geer, 2011) during the optimization.

Besides, in some machine learning literatures (Chiu et al., 2020, Hsieh et al., 2008), the bias term  may be formulated into the weight vector  such that  for simpler presentation. Then, correspondingly, we add a constant feature to , i.e., take . In this sense, the problem (5) can be written as (6)Actually, there is a slight difference between the problems (5) and (6). That is, the former will not impose constraints on the bias , while the latter will indeed give some penalty to the bias .

Despite its relatively simple usage, the linear classification model is still a preferred choice in many fields due to its adequate inter-predictability and superior efficiency. Especially, as the scale of tasks increases, a series of algorithms are proposed to solve the optimization problem (5) (Chou et al., 2020, Defazio et al., 2014, Hsieh et al., 2008, Ito et al., 2017a, Schmidt et al., 2017, Yu et al., 2011). Unfortunately, these well-designed algorithms often are very complicated and involve massive hyper-parameters, and different tricks in the implementation might make it even worse. Therefore, it can be extremely difficult to guarantee the correctness of the corresponding programs.

It is also worth noting that, the essence of a machine learning algorithm is basically an optimization algorithm, i.e., solving an optimization problem to obtain parameters of its expected model. However, the purpose is radically different from the conventional optimization algorithm, which aims to find an optimal solution, while the goal of a machine learning algorithm is actually to reduce the generalization error. Such fact also renders the analysis of machine learning programs more challenging, since certain assumptions may no longer be held, e.g., with the parameter  not being the optimal solution of problem (5), with the gradient of loss function w.r.t  vanishing, and so on.

2.3. Metamorphic testing
Metamorphic testing is an effective software testing method against oracle problems (Chen et al., 2018). From its first being published in 1998 until now, metamorphic testing has been well-adopted in various real-life applications (Chen et al., 2009, Le et al., 2014, Xie et al., 2013). It successfully helped to detect a massive number of software bugs.

The most critical step of applying metamorphic testing is to find an effective metamorphic relation (MR), which is a functional relation established among multiple inputs and outputs of the tested program (Chen et al., 2018). Instead of testing by validating the output for a given single input, metamorphic testing tries to test the program by checking whether the relation among several input–output pairs is held or not. An MR indeed encodes a necessary property of program (also called metamorphic property in some literature Murphy et al., 2008, Segura et al., 2016, Singh et al., 2012) into a relation among several inputs and outputs. Specifically, these inputs include source inputs and follow-up inputs, with their corresponding outputs being source outputs and follow-up outputs. Generally, source inputs are given by the testing program and the follow-up inputs are generally according to the source inputs and the source outputs. Take the sine function as an example. There is a relation that requires . To test a program  which realizes function , let input  be the source input. Then, the follow-up input  is calculated by . Their corresponding outputs,  and , naturally become source output and follow-up output. They should satisfy the relation that . In this case, the relation  is an MR, and it is easily derived from the oddness of the sine function.

As machine learning programs often lack oracle, metamorphic testing also showed its prowess in previous researches. Some metamorphic relations have been carefully designed to test machine learning programs (Dwarakanath et al., 2018, Murphy et al., 2008, Xie et al., 2011, Xie et al., 2020). In our work, we conduct a more in-depth study on the application of MR in testing machine learning programs.

2.4. PI-MR and PD-MR
In previous work, a few MRs were designed for testing machine learning programs. However, these MRs still not be effective enough. We focus on linear classification algorithms in this paper. We conclude the two main weaknesses of existing MRs as follows:

(1) Lack of efficacy. The targeted metamorphic property in their designed MR is not sound for testing machine learning programs, such that they cannot detect bugs very effectively. For example, according to the mutation analysis results in (Xie et al., 2011), some MRs cannot detect any bug. Furthermore, the proposed metamorphic relations are too weak to touch the key property of machine learning algorithms. For example, the MR that shuffling the training data does not make use of the statistical properties of machine learning programs, which leads to its poor performance (Xie et al., 2020, Dwarakanath et al., 2018).

(2) Lack of necessity. The metamorphic relations are too intuitive and lack a theoretical guarantee. For example, some MRs were demonstrated not necessary for the algorithms being implemented (Xie et al., 2011), and some MRs were defined based on the users’ intuitive expectations and specific requirements (Xie et al., 2020). As we discussed earlier, the metamorphic relation should encode the necessary property of the algorithm. However, even though the tested learning program is bug-free, the current relations still may not hold. We analyzed this kind of MR in detail in Section 4.6.2.

To overcome these weaknesses, we try to borrow the wisdom from the proposed MRs applied to other fields. Through summarizing existing MRs in various fields (Chen et al., 2009, Zhou et al., 2015, Le et al., 2014), we find that the current MRs can be divided into two categories, i.e., PI-MR and PD-MR. When an MR is unrelated to the past execution result, we call it Past-execution Independent MR, referred to as PI-MR. Contrarily, when an MR utilizes the past execution result, we call it Past-execution Dependent MR, referred to as PD-MR. Fig. 1 and Fig. 2 give the main procedures of PI-MR and PD-MR, respectively. As shown in these figures, the only variance between the two MRs is how to generate the follow-up input, i.e., the so-called metamorphic transformation  in the figures.

We use a typical example (i.e., shortest path problem) to illustrate the difference between PI-MR and PD-MR. Suppose the program  implements a search of the shortest path from  to  in a given graph, we can design the following two typical MRs:

(I)
Find the shortest paths from  to , and check whether  is the reverse of .

(II)
Let  is the output of , and choose any integer , where . Then, check whether .

In MR (I), the corresponding metamorphic transformation  is  that only requires the source input, thus being PI-MR. However, in MR (II), the transformation  is  that needs both source input and source output, thus being PD-MR.

Through some existing works that investigate what kind of MRs were effective (Chen et al., 2010, Cao et al., 2013) and the efficacy of MRs across several fields, such as compilers (Le et al., 2014, Tao et al., 2010), bio-informatics (Chen et al., 2009, Ramanathan et al., 2012), and so on. We conjecture that PD-MR can perform better than PI-MR, which is also reflected in some existing research (Le et al., 2014, Le et al., 2015, Cao et al., 2013). Based on this conjecture, we later indeed attempt to translate our designed metamorphic property into PD-MRs.

3. Methodology
3.1. Encode stability into PD-MR
When applying metamorphic testing to machine learning programs, we first need to choose a reasonable property as metamorphic property. Since the generalization property is the key property of a machine learning algorithm, we take it as the most straightforward and hopefully effective choice. However, encoding the algorithm generalization into a suitable MR is impractical, because we do not naturally own the true generalization error (to some extent, it is an oracle) of the algorithm. Although one often uses the test accuracy as an estimate of the generalization error, this is just an estimate rather than the true generalization error, as we discussed in Section 2. Based on this consideration, we hereby use algorithm stability instead of generalization as metamorphic property, since the equivalence between stability and generalization error bounds has also been extensively studied (Bousquet and Elisseeff, 2002, Devroye and Wagner, 1979, Kearns and Ron, 1999, Mukherjee et al., 2006). In addition to the feasibility of encoding this property into the MR, stability is also algorithm-specific to be more optimized to analyze the specific machine learning program.

However, there is another problem remaining to be solved. As we discussed earlier in Section 2, a PI-MR is deficient inability to encode the critical property of the algorithm, and it may also destroy the necessity of metamorphic property. Thus, establishing a PD-MR of the algorithm is undoubtedly a better choice. Furthermore, the output of machine learning programs is usually a set of parameters, which is actually a vector. In this case, it is extremely difficult to find and construct a relation between (source and follow-up) outputs. To address this issue, we attempt to build a relation between inference results of a given example. For example, the source and follow-up output of linear classifier are  and , respectively. Therefore, for a given example , we can identify an equality (or inequality) relation between  and .

Combining the above two points, we can formally define our problem by the following.

Problem 1

Given source input , an example , and a linear classification program , how to generate the follow-up input  based on the algorithm stability, such that we can obtain a metamorphic relation , where (7)

3.2. Two stability-based MRs
At the first glance, to encode the stability only requires the replacement or removal of the original training set (i.e., the source input). In other words, such modifications based on stability is not directly related to the model parameter (i.e., the source output). From this perspective, it may be more appropriate for establishing a PI-MR. But in fact, How to replace or remove an example of training set depends on the source output. To further explain this statement, we provide the following two propositions.

Proposition 1

Given a dataset , an additional example , and a linear classification algorithm , let the output of the algorithm be . Perturb th example of  by (8)where  is any vector orthogonal to  (i.e., ), and obtain (9)Suppose the output of the algorithm to be . Then, we have that (10)is monotone with respect to .

Proposition 2

Given a dataset , an additional example , and a linear classification algorithm , let the output of the algorithm be . Remove th example of , and obtain (11)Suppose the output of the algorithm to be . Then, we have that (12)is monotone with respect to .

The proofs of these two propositions are simple and we include them in Appendix for completeness.

Through the above two propositions, we can comfortably translate the stability (replacement and removal) into two PD-MRs, respectively. Specifically, based on the stability of replacing/removing an example of the training set, we can derive a monotone function with respect to the modified example, which can be naturally encoded into a partial order relation.

As shown in Fig. 3, Fig. 4, we provide a visual illustration for each proposition and its corresponding MR. The green points and blue points represent positive and negative instances used for training, and the source input  is made up of these instances. By executing the given linear classification program  with input , we attain the output . The black line is the linear classification hyperplane determined by . After replacing/removing an example  from dataset, we got the follow-up input . The follow-up output  yields the next linear classification hyperplane, which is indicated by the red dashed line. Actually, the proposed two MRs observe the hyperplane movements when replacing/removing different examples. Instead of directly monitoring the hyperplane (i.e., the changes of ), MRs use the result  for a given example  to build a partial order relation.


Download : Download high-res image (277KB)
Download : Download full-size image

Download : Download high-res image (254KB)
Download : Download full-size image
3.3. Algorithms
In Algorithm 1, we use Gram–Schmidt process to yield the vector  that is orthogonal to the vector  (Cheney and Kincaid, 2009). To ensure the perturbation small enough, the Euclidean norm of  is fixed by 10−3, i.e., 
. We set the iteration number  to . Theoretically, on the one hand, too few iterations may make the lists  and 
 too short, and thus not be able to detect inconsistencies between them. On the other hand, too large  will cause too many iterations, which costs a lot of time. We did some experiments on different  (i.e., 15, 30, 45, and 60) to verify this theory. The experimental results show that despite the algorithm with an iteration number of 15 performed a little worse, the algorithm with other iteration number performed exactly the same. These results are consistent with our theory. Therefore, setting  by a moderate value is a reasonable choice.

In Algorithm 2, the rationale behind the setting of the iteration number  is similar to that in Algorithm 1, and we also set  to .

4. Evaluation
In this section, we evaluate our designed MRs, and compare them to MRs proposed by existing work on their bug detection effectiveness, for programs of nine well-known linear classification algorithms.

4.1. Research questions
We aim to answer the following two research questions:

RQ1 (Effectiveness): How effective are our designed MRs on detecting bugs for linear classification programs, as compared to MRs proposed by existing work?

RQ2 (Sensitivity): How sensitive are our designed MRs on detecting bugs with respect to their different stability consequences for linear classification programs, as compared to MRs proposed by existing work?


Table 1. Algorithm list.

Type	Name
Logistic Regression	NAG (Ruder, 2016)
Newton (Bishop, 2006)
LBFGS (Zhu et al., 1997)
APG_L1 (Ito et al., 2017b)
APG_L2 (Ito et al., 2017b)
Support Vector	ADMM_L1 (Ye et al., 2011)
Machine	ADMM_L2 (Ye et al., 2011)
SQP_L1 (Chiu et al., 2020)
SQP_L2 (Chiu et al., 2020)
4.2. Experimental subjects
We selected nine well-known linear classification algorithms as shown in Table 1, including three logistic regression algorithms and six support vector machine algorithms. They are either classic algorithms in the linear classification field or recently published in top journals/conferences with nice credits. Since their source codes are not directly accessible, we chose to implement them by ourselves, according to their algorithms presented in corresponding documents or papers. By a careful manual verification process, we utilized our implementation as corresponding golden versions. To ensure the credibility of the golden versions, we also compared the execution results of our implemented program and the linear classification program in the scikit-learn library (Pedregosa et al., 2011). The error (i.e., the Euclidean norm of two outputs) of the output results (i.e., a pair of ) of these two programs is less than 10−6, suggesting the nice quality of our implementation.

In order to better evaluate different MRs’ bug detection effectiveness, we adopted popular mutation analyses (Offutt and Li, 2013, Denisov and Pankevich, 2018, Hovmöller) in software engineering, and generated mutants for those programs, each of which contains one inserted syntactic bug. Mutation analyses aim to generate mutants with syntactic bugs for simulating realistic bugs in practice (Jia and Harman, 2010, Vu Do et al., 2006), and are used as a relatively promising way to evaluate different treatments’ bug detection effectiveness. We used a well-established mutation tool for Python, i.e., mutmut (Hovmöller), and Table 2 presents its supported seven mutation operators for generating mutants.

However, we observe that mutmut’s supported mutation operators have quite a limited ability to generate numerical bugs, which are extremely common in machine learning programs due to massive mathematical calculations. To be specific, the only operator to generate numerical bugs in mutmut is AOR, which substitutes an arithmetic operator ＋ to , tending to largely interrupt a program’s logic and trigger a program crash, thus easily detected. Therefore, in order to better simulate such common numerical computational bugs in machine learning, we additionally designed mutation operation POA as in Table 3, which inserts a random constant as coefficient  to programs’ numeric calculations, including a calculation expression, an assignment expression, a function parameter, and so on. This coefficient follows a normal distribution .


Table 2. Mutation operators of mutmut.

Name	Description	Example
AOR	Replace arithmetic operator	‘+’ to ‘-’
LOR	Replace logical operator	‘and’ to ‘or’
ROR	Replace relational operator	‘==’ to ‘!=’
SOR	Replace shift operator	‘ ’ to ‘ ’
ASR	Replace shortcut assignment operator	‘+=’ to ‘=’
KR	Replace keyword	‘break’ to ‘continue’
AVR	Replace assignment value	‘x = 1’ to ‘x = None’

Table 3. The newly added mutation operator.

Name	Description	Example
POA	Add parameter operation	‘1+x’ to ‘1+0.3*x’

Table 4. Details of generated mutants.

Mutants (#)	Algorithms	Sum
NAG	Newton	LBFGS	ADMM_L1	ADMM_L2	APG_L1	APG_L2	SQP_L1	SQP_L2	
Num	101	96	240	199	205	123	97	88	116	1265
For example, the Code Listing 1 shows a practical numerical calculation in the ADMM algorithm, and line 2 contains a square root calculation. Simply altering the symbol ‘＋ ’ to ‘’ will cause illegal calculation, and thus lead to the program exception (e.g., RuntimeWarning) or even a program crash, which can be easily discarded, thus being relatively worthless. By using POA, Code Listing 2 can be generated by adding a coefficient of 0.5 to the formula in line 2. This type of bug often appears when misreading the algorithm formula in practice, and we believe it can be indeed more relatively worthy in bug detection evaluation.

After that, combining operators in both Table 2, Table 3, we eventually generated 1265 mutations in total. Details are in Table 4. We discarded not executable mutants or some clearly equivalent ones with manual checking. Considering that we only focus on generating mutants for machine learning programs’ training part, which is relatively short in code length, we believe such mutants can be already adequate for evaluating MRs.


Download : Download high-res image (53KB)
Download : Download full-size image

Download : Download high-res image (51KB)
Download : Download full-size image
4.3. Experimental design
Then, we introduce our experimental design, including dataset preparation for feeding into the former machine learning programs, and descriptions about our considered MRs for comparisons later.

Datasets. We randomly generate datasets (i.e., the original dataset ) due to the following reasons. Firstly, our designed MRs are indeed dataset-independent, not specifically designed for a specific task of the dataset, thus making dataset selection unrestricted. Secondly, existing studies have also pointed out that the randomly generated dataset can work better in detecting bugs (Duran and Ntafos, 1984). Although several existing real-world datasets are applicable for testing the effectiveness of machine learning algorithms, they maybe not as sensitive as the randomly generated datasets in detecting bugs of the machine learning program. Therefore, we adopted scikit-learn (Pedregosa et al., 2011) to generate synthetic datasets randomly.

Moreover, in order to reduce the running time of our experiment to save time, we restricted the scale of randomly generated datasets by 300 and its concerned feature number between two to ten. Then, for any generated dataset, we also divided it into two parts for our latter usage, a training set (240 in size) and a test set (60 in size). The scale of our datasets is much larger than some previous work (Xie et al., 2011, Xie et al., 2020), and the feature number also covers feature numbers used in these work. Since larger datasets are usually more statistically significant, we believe our generated datasets are not only enough for the linear classification task, but also produce feasible running time for training in experiments. Furthermore, as we discussed earlier, a multi-class classification task can be converted easily into multiple binary classification tasks, thus we directly apply these algorithms to a binary classification task. Thus, only two labels are considered in generating datasets.

MRs. To evaluate our designed MRs’ bug detection effectiveness, we have prepared both clean buggy programs as long as randomly generated datasets. For comparisons, we aim to use both our designed MRs and those proposed by existing work for bug detection later. Here, we briefly introduce them as follows.

We chose six classical MRs (listed in Table 5) proposed in previous work for comparisons. All of these six MRs can be applied directly to the linear classification programs, and have been evaluated with superior results in previous work. For example, MR-2 is previously found to find all 12 bugs of linear-SVM and RBFSVM (Dwarakanath et al., 2018), and MR-3 achieved a high killing rate of 71.4% (15 out of 21) (Xie et al., 2011). The details of these MRs are as follows. In these MRs, dataset  is source input, and dataset 
 is follow-up input.

–
MR-1: Shuffle of training data (Dwarakanath et al., 2018, Xie et al., 2020). Dataset 
 is obtained by shuffling . The accuracy of models trained by  and 
 should be the same.

–
MR-2: Permutation of data features (Xie et al., 2011; Dwarakanath et al., 2018). Dataset 
 is obtained by permuting the features of . The accuracy of models trained by  and 
 should be the same.

–
MR-3: Permutation of class labels (Xie et al., 2011). Dataset
 is obtained by permuting the class labels of . The accuracy of models trained by  and 
 should be the same.

–
MR-4: Addition of uninformative attributes (Xie et al., 2011, Xie et al., 2020). Dataset 
 is obtained by adding an uninformative attribute to all data in . The accuracy of models trained by  and 
 should be the same.

–
MR-5: Re-prediction (Xie et al., 2011). Dataset 
 is obtained by adding a random sample to the testing dataset to  for re-prediction. The prediction results of models trained by  and 
 should be the same.

–
MR-6: Additional training samples (Xie et al., 2011). Dataset 
 is obtained by duplicating samples with a specific label in . The prediction results of the samples with the specific label of models trained by  and 
 should be the same.


Table 5. Six MRs in previous work.

No	Description
MR-1	Shuffle of training data
MR-2	Permutation of training and testing features
MR-3	Permutation of class labels
MR-4	Addition of uninformative attributes
MR-5	Consistence with re-prediction
MR-6	Additional training samples
4.4. Experimental process and setup
We now introduce our experimental process with prepared resources and set up to answer the aforementioned two research questions.

Process. With subjects (golden versions and corresponding mutants), datasets (both training and testing sets), and MRs prepared, we now introduce our experimental process. For any considered MR, we aim to check whether it is satisfied or not upon any considered mutant fed by generated datasets, according to a typical procedure presented as in Fig. 5. We call a mutant can be “killed” by an MR when this MR is eventually violated upon this mutant following this procedure. Note that, due to some statistical and numerical bias during machine learning programs’ execution, we choose randomly repeat the procedure for any mutant and MR 100 times, only an MR is violated by a mutant more than five times out of 100, we recognized as its killing upon this mutant successfully.

In this procedure, a training set  is generated as the source input, and then fed into a program (clean or buggy). To be specific, the program actually points to a program written to train a linear classifier and produce weight vector and bias as , possibly with an unknown bug. Then, if one uses this classifier to predict, the output  would be the model parameter, and used to give predictive results for the given testing set. The predictive results and model parameter  are collectively referred to as the source output. Based on the source output and source input, the follow-up input 
 can be generated. Similarly, the follow-up output consists with output 
 that is the output of executing program with input 
, and its corresponding predictive results of the same test set. Any MR actually points out some relationships among source input and output, follow-up input and output, that must be satisfied, otherwise violated.

•
Subject. We generate more than 1265 mutants and used them as our subjects for evaluating MR’s bug detection effectiveness.

•
MR. We controlled to select different MRs in experiments. For selection, we consider both our designed MRs as MR-P1 (for Proposition 1) and MR-P2 (for Proposition 2), and six MRs proposed in existing work, i.e., from MR-1 to MR-6 as in Table 5.

•
Algorithm. We used a total of nine different linear classification algorithms for generating mutants. Different concerned algorithms in mutants may lead to different experimental results when evaluating MR’s bug detection effectiveness.

•
Stability degree. We study how different MR can detect bugs with respect to different stability degree interruption to existing programs. We divided them into three categories by considering their associated accuracy consequences. For any mutant leading to a large accuracy change (more than an absolute value of 0.05), we consider its stability degree to be “severe”. For any leading to a relatively small accuracy change (less than an absolute value of 0.05), we consider its stability degree to be “light”. For those otherwise, we consider it to be “negligible” or simply refer to it as “other” category.

To measure the bug detection effectiveness, we focus on the following dependent variable:

•
Killing rate. It refers to the proportion of killed mutants among all mutants considering any specific MR, which is for measuring this MR’s bug detection effectiveness.

All experiments were conducted on a CPU server with AMD EPYC 7401 24-Core processor and 128G of memory, running Ubuntu 20.04.1 with GNU/Linux kernel 5.4.0. Our code and experimental data are also available at https://github.com/yingzhuoy/MRs-of-linear-models.

To answer RQ1 (Effectiveness). We conducted experiments for our designed MRs upon all generated mutants for programs of different linear classification algorithms, as compared to existing MR-1 to MR-6, and study their effectiveness on bug detection, suggested by their corresponding killing number of mutants and killing rates.

To answer RQ2 (Sensitivity). We divided mutants according to their different interruption degrees to programs’ stability, and study their effectiveness upon mutants concerning different stability degrees, i.e., “severe”, “light”, and “other”, as mentioned before to see how sensitive our designed MRs can be on detecting bugs that lead to different stability interruption degrees.

4.5. Experimental results and analyses
We report and analyze experimental results, and answer the preceding research questions in turn.


Table 6. Killed mutants when applying different MRs on experimental subjects.

MRs	Subjects	Sum
NAG	Newton	LBFGS	ADMM_L1	ADMM_L2	APG_L1	APG_L2	SQP_L1	SQP_L2	
MR-1	1	2	21	55	70	4	7	30	30	220
MR-2	2	2	24	56	67	3	7	3	21	185
MR-3	74	27	139	53	68	13	16	16	17	423
MR-4	73	13	133	83	104	65	44	34	28	577
MR-5	7	4	28	57	68	6	8	4	20	202
MR-6	53	10	103	88	89	32	27	40	24	466
MR-P1	82	23	144	115	113	52	53	69	48	699
MR-P2	54	31	128	94	109	110	47	38	45	656
P1+P2	82	38	145	120	121	110	54	73	51	794
All MRs	84	46	158	136	123	121	64	79	51	862

Download : Download high-res image (171KB)
Download : Download full-size image
Fig. 6. Ovarall killing rate of MRs.

4.5.1. RQ1: effectiveness
Fig. 6 shows the total killing rate of concerned MRs on all subjects (i.e., 1265 mutants). We use MR-1 to MR-6 to represent studied MRs proposed by existing work, whose details can also be found in Table 5, while MR-P1 (or P1) and MR-P2 (or P2) represent our proposed MRs in this work, i.e., aforementioned Proposition 1, Proposition 2 in Section 3. MR-P1 and MR-P2 can achieve a nice killing rate of 55.3% and 51.9% mutants among all 1265 ones, much more than those of existing MRs (MR-1 to MR-6), i.e., only 14.6%–45.6% in killing rates per MR. Moreover, when combining our MR-P1 and MR-P2 together, the killing rate (mutants that are killed by either MR-P1 or MR-P2) can be up to 62.8% among all 1265 mutants, already covering 92.2% of all detected mutants by all studied eight MRs (68.1% mutants). Still, our MRs’ killing rate indeed seems to be not so high, it may be due to the following reasons: (1) there may exist some statistically equivalent mutants in fact, and this denotes the inevitable problem of mutation analyses (Madeyski et al., 2013), and has not been perfectly addressed yet, (2) there may also exist some mutants that do not destroy the program stability, which are not specifically designed in our MRs’ consideration, i.e., out of scope. We also give a few examples in our case study section (Section 4.6), in order to look a little deeper into those survived mutants for your reference.

Table 6 shows some details of killed mutants when different MRs are applied to subjects associated with different LC algorithms. As shown in Table 6, our approach can kill the most mutants for all subjects, with either MR-P1 or MR-P2 achieving the largest number of killed mutants. Especially, for most subjects, MR-P1 or MR-P2 killed far more mutants than the other six MRs. For example, for the ADMM_L1 subject, although MR-6 killed 88 mutants successfully, which has already been the most effective one among all the six MRs (MR-1 to MR-6) for comparison, our proposed MR-P1 can kill 115 mutants, with 27 more mutants (with a 38.5% degree larger in the killing number) than MR-4. Furthermore, for the APG_L1 subject, MR-P2 killed 110 mutants (69.2% larger in number), while the most effective MR-4 in existing work only killed 65 mutants.

When further combining the killed mutants of different subjects together for an MR individually, our proposed MR-P1 and MR-P2 also show their significant superiorities with being the best two MRs clearly, i.e., 699 for MR-P1, and 656 for MR-P2 in total. Statistically, MR-P1 and MR-P2 killed 122–514 and 79–471 more mutants than any existing MR, with an increasing rate of 21.1%–277.8% and 13.7%–254.6%, respectively. If one compares them to the best MR in existing work, i.e., MR-4, which killed 577 mutants in total, our proposed MRs (P1+P2) indeed killed 37.6% (MR-4) to 329.2% (MR-2) more mutants, suggesting their great effectiveness in detecting buggy programs, as well as their significant superiorities over existing MRs.

Therefore, we answer RQ1 as follows: our proposed MRs (MR-P1 and MR-P2) show nice effectiveness on their bug detection. When compared to existing MRs, they also achieve significant superiorities (21.1%–277.8% and 13.7%–254.6% improvement on killing rates) over the studied six MRs proposed by existing work.


Table 7. The number of mutants of three stability degree categories.

Category	Subjects
NAG	Newton	LBFGS	ADMM_L1	ADMM_L2	APG_L1	APG_L2	SQP_L1	SQP_L2
Light	13	10	25	47	20	90	16	14	12
Severe	59	28	105	80	104	30	48	35	36
Other	29	58	110	72	81	3	33	39	68

Table 8. Killing rate of MRs on with respect to mutants in the “light” category.


Table 9. Killing rate of MRs with respect to mutants in the “severe” category.


Download : Download high-res image (141KB)
Download : Download full-size image
Fig. 7. Killing rate under different thresholds.

4.5.2. RQ2: sensitivity
As aforementioned, in order to better evaluate our proposed MRs’ effectiveness and their sensitivity on bug detection with respect to different stability interruptions, we divided all 1265 mutants into three categories: “severe”, “light”, and “other”, according to their stability interruption degree. To do so, we hereby used accuracy as the standard and then partitioned all mutants as follows: (1) mutants leading to a large accuracy change into the “severe” category, (2) mutants leading to a relatively small accuracy change into the “light” category, (3) the remaining ones into the “other” category. Table 7 shows the number of mutants of three stability degree categories.

Fig. 7 shows the killing rate of all MRs of “severe” and “light” categories under different thresholds (i.e., 0.025, 0.05, 0.075, and 0.1). According to the figure, the killing rate of all MRs increased with the increase of thresholds in both “severe” and “light” categories and the killing rate of “severe” category are higher than “light” category. These results indicate when accuracy changes more, the stability is more damaged and bugs can be easier detected, which shows the rationality of using accuracy as the standard. According to our statistics, different thresholds have little effect on the results of MR evaluation, so we randomly choose 0.05 as the threshold to determine stability degree in our evaluation.

By investigating different effectiveness to different categories, we aim to evaluate MR’s effectiveness sensitivities on bugs with different stability interruptions. We specifically look into the experimental results for category “severe” and “light”, as shown in Table 8, Table 9.

Table 8 shows the killing rate of MRs with respect to mutants in the “light” category (“light” mutants). We can observe that MR-P1 and MR-P2 can achieve nice 60.3% and 72.5% killing rates when applying to “light” mutants of all subjects, much higher than any MR from MR-1 to MR-6 (highest at 55.9%). Besides, for the APG_L2 subject, MR-P1 achieves a killing rate of 100% and MR-P2 achieves a killing rate of 87.5%, but the highest killing rate from MR-1 to MR-6 is only 31.3% (MR-4). Moreover, the joint killing rate of MR-P1 and MR-P2 is also very high on most of the subjects. In subject NAG, LBFGS, APG_L2 and SQP_L1, the joint killing rates all exceed 90.0% and are the same as the joint killing rate of all MRs. On some subjects, MR-P1 and MR-P2 behave not so well, especially the Newton subject (the killing rate of MR-P1 and MR-P2 are both 30.0%) and the ADMM_L1 subject (the killing rate of MR-P1 is 56.9%, the killing rate of MR-P2 is 38.3%). In these subjects, the killing rate of other MRs is also very low, indicating that bugs in these mutants are not easy to be detected. Still, our MR-P1 and MR-P2 can achieve the best killing rate in these cases (30.0% to Newton, and 59.6% to ADMM_L1).

Table 9 shows the killing rate of MRs with respect to mutants in the “severe” category (“severe” mutants). MR-P1 and MR-P2 achieve very high killing rates of 89.9% and 83.8% separately when applying to all “severe” mutants, which is also higher than any other MRs from MR-1 to MR-6. Besides, the killing rate of either MR-P1 and MR-P2 is undoubtedly the highest for almost every subject, except for the APG_L2 subject, whose killing rate of MR-P1 (75.0%) is slightly lower than that of MR-4 (81.3%). Moreover, MR-P1 and MR-P2 showed much higher killing rates than other MRs in many subjects. For example, in the SQP_L2 subject, both MR-P1 and MR-P2 have the killing rate 91.7%, while the killing rate from MR-1 to MR-6 is at most 58.3% (MR-1). The joint killing rates of MR-P1 and MR-P2 even exceed 95.0% in most subjects, suggesting its great effectiveness in detecting bugs of “severe” mutants.

When combining Table 8, Table 9, the two categories of mutants indeed lead to varied stability interruptions to programs, as well as their different difficulties to be detected. The killing rates of “severe” mutants are relatively lower than those of “light” mutants. This also echoes our intuition that bugs that break properties more severely are relatively easier to be detected. We can still observe that our proposed MR-P1 and MR-P2 are stably effective and achieve nice superiorities for both “severe” and “light” mutants, suggesting their nice sensitivity with respect to bugs with different stability interruptions.

Therefore, we answer RQ2 as follows: our proposed MR-P1 and MR-P2 show their great effectiveness with nice sensitivity. Its nice effectiveness holds with respect to bugs with varied stability interruptions to programs.

4.6. Case studies
In this section, we present case studies in two aspects. First, we analyze some stubborn mutants with the same performance as the golden version. Bugs in these mutants are difficult to be detected by accuracy or some PI-MRs, but can be detected by our PD-MRs. Second, we discuss whether the previously proposed MRs successfully encode a necessary property of algorithms.

4.6.1. Stubborn mutants
A few generated mutants confirm the machine learning algorithms’ capability of hiding errors. These mutants should have performed poorly, but the fact is just the opposite. This phenomenon not only explains the difficulty in testing machine learning programs, but also clarifies the gap between testing conventional software and machine learning programs.

Code Listing 3 provides a typical example. In subject NAG, a conditional statement If in code piece of BUG-73 conducts a line search to ensure the value of Lw (i.e., loss function) decreased. In this case, the mutation operator AOR alters the symbol ‘*’ to ‘/’, which causes the sufficient decrease of Lw to be destroyed. This mutant is highly inefficient in convergence or even cannot converge, but it achieved the same accuracy as the golden version on all ten datasets. While the mutant was not killed by any of the existing MRs (MR-1 to MR-6), it was killed by MR-P1 and MR-P2. Similarly, BUG-87 (shown in Code Listing 4) is a mutant that modifies the calculation of the Sigmoid function. Actually, this mutant will be equivalent to the golden version if we applied this mutated Sigmoid in both training and testing process. In our experiment, we apply the mutated  function in the training process and the correct  function in the test process. Thus it should be recognized as a bug. MR-P1 and MR-P2 killed this mutant but the other six MRs did not.


Download : Download high-res image (55KB)
Download : Download full-size image

Download : Download high-res image (64KB)
Download : Download full-size image

Download : Download high-res image (47KB)
Download : Download full-size image
A more interesting example is BUG-97 (as shown in Code Listing 5), which sets the value of the bias  by the first element of vector 
 rather than the last element. Such mutation operator significantly changes the bias of the trained model, yet the model obtained by this mutant did not be impaired and even achieved higher accuracy than the model obtained by the golden version (with an increase from 88.3% to 93.3%). It should be more difficult to be killed, but four MRs (MR-2, MR-4, MR-6 and MR-P1) succeeded.
4.6.2. The necessity of MRs
Since most of MRs in previous work were intuitive, they are probably not consistent with machine learning theories sometimes. We take the MR-4, which attain the best performance among the previous six MRs, as an example to illustrate this issue. As in Section 4.3, MR-4 adds an informative attribute to all examples of training set  to create the follow-up input 
. This transformation is actually equivalent to introducing an additional bias 
 to model parameter, i.e., which makes the follow-up output be 
. In this case, the bias  will be added into the regularization function, but the additional bias 
 will not. Thus, we can easily find the following two situations where MR-4 cannot hold.

(1)
The algorithms that do not have the bias,

(2)
The algorithms that apply the regularization.

In practice, the loss functions of most machine learning algorithms contain the regularization function, which severely limits the usage of MR-4. We also used scikit-learn to verify this analysis: When we set the bias to false (intercept = False), or set the hyperparameter C of regularization larger than three, MR-4 is violated.

Although we have tried our best to avoid the mutation operator on the algorithms’ hyperparameters (e.g., mutate C = 1.0 into C = 1.5) because the modification of hyperparameters should not be considered as a program bug, Some extreme examples are still inevitable. For example, BUG74 in Newton subject (as shown in Code Listing 6) is equal to modifying the coefficient of the regularizer. In this case, only MR-4 killed this mutant. The example was a little particular, but similar mutants may also exist in other algorithms of our experiment. Discarding such mutants will only reduce the performance of MR-4 and improve the efficacy results of other MRs. Incidentally, in our experiment, we adopted the same hyperparameter setting as scikit-learn, which applies the regularization by default. For the above reasons, the advantages of MR-4 in our experiment were also seriously suspected.


Download : Download high-res image (60KB)
Download : Download full-size image
4.7. Threats analyses
The creation of datasets is one of external threats in our experiment. To alleviate this threat, we tried two aspects. (1). Refer to previous work to set the data scale. In two previous works, the size of the training dataset is a random number from 20 to 50 and the feature numbers are two and four respectively (Xie et al., 2011, Xie et al., 2020). Our dataset size, 300, is larger than the size in previous works and our feature number, randomly selected from two to ten, also covers the numbers in previous works. Thus, our generated datasets are enough for the linear classification task. (2). Randomly generate data values. Since our MRs are dataset-independent and randomly generated datasets can work better in detecting bugs (Duran and Ntafos, 1984), we randomly generated the data values to make the datasets of better diversity. We used the built-in library in scikit-learn (Pedregosa et al., 2011) to randomly generate datasets, which makes the generated results with no bias.

The mutants generation is another external threat. We selected a widely used Python mutation tool mutmut (Hovmöller), which was used to generate mutants for machine learning programs (Dwarakanath et al., 2018). This tool supports many traditional mutation operators, which can create many different kinds of bugs. Besides, we added a new mutation operator POA to make this tool more adaptable to machine learning programs. We generated over 1000 mutants in total using this tool, which is enough for our experiment.

Possible bias in MR selection is also an external threat. To avoid bias, we selected six MRs for comparison from three previous works (Xie et al., 2011, Xie et al., 2020, Dwarakanath et al., 2018). Some of these MRs appeared in more than one work, which indicates these MRs are classical and widely used. As for the performance, we selected both well-performed MRs and badly performed MRs in previous work to ensure the diversity of MRs.

For the above efforts we made to avoid external validity, our experiment settings should be appropriate.

The internal threat mainly comes from the implementation of nine linear classification algorithms we used. Although these algorithms were published in famous publications, they were not open source. We implemented these algorithms on our own according to their papers. To avoid possible faults we would make during the implementation, we carefully compared the training results of our implementation with the results of scikit-learn (Pedregosa et al., 2011) and ensure the tolerance less than 10−6. In this way, this internal threat is generally avoided.

5. Related work
5.1. Testing on machine learning program
Metamorphic testing has been one of the most popular techniques used for testing machine learning programs. Various MRs have been designed for different machine learning algorithms, e.g., k-nearest neighbor classifier, support vector machine, etc. In the following, we briefly overview related work in this direction.

Xie et al. proposed 11 MRs to test programs that implement k-nearest neighbor (kNN) algorithm or the Naïve Bayes (NBC) algorithm (Xie et al., 2011). These MRs were divided into two types according to whether they encode the necessary properties of algorithms or not. MRs encoding necessary properties were typically used for (software) verification, and those remaining ones were typically used for software validation. These MRs were evaluated on Weka (Hall et al., 2009) and some introduced mutants of Weka.

Dwarakanath et al. designed four MRs for support vector machine (SVM) and Deep Residual Network (ResNet) (Dwarakanath et al., 2018), respectively. These MRs are specifically designed for the characteristics of the convolution operation or the RBF kernel function, including the permutation of input channels, the shift of training and test features, etc. Experimental results showed that such proposed MRs effectively killed 71% of generated mutants.

Xie et al. applied metamorphic testing to unsupervised machine learning fields (Xie et al., 2020). They focused on the clustering algorithms and proposed 11 generic MRs of six kinds of data transformation. However, these MRs were designed according to user expectations instead of solid machine learning theories.

Cheng et al. mainly focused on the properties of implementation bugs in machine learning programs (Cheng et al., 2018). The Weka programs implement four algorithms (NBC, kNN, SVM, and Decision Tree) were selected as the golden version. The experiments were conducted on four different kinds of datasets, the dataset of different distribution, dataset of different sizes, imbalanced dataset, and special dataset. The experimental results showed that there are some logically nonequivalent but statistically equivalent mutants, and some of these mutants were very stubborn that they had the same results with golden versions on all datasets used in the experiment.

5.2. Testing on data, model, and framework
As in the survey of machine learning testing (Zhang et al., 2020, Braiek and Khomh, 2020), many studies focus on other kinds of defects in machine learning systems besides learning programs. We list some typical researches in the following.

Defects in data. Since data is one of the most critical elements in machine learning, there were many works aiming at detecting bugs in data. Some of those works aimed at debugging errors in polluted training datasets (Hynes et al., 2017, Ma et al., 2018b), some focused on detecting improper model inputs (Metzen et al., 2017, Wang et al., 2019), and some focused on investigating the effects of dirty data (Qi et al., 2018).

Defects in models. Besides machine learning programs, the trained model is another key component of the machine learning system. In recent years, lots of work has been proposed to test deep learning models.

Some of these studies borrowed the idea of structural coverage in conventional software testing and proposed a series of coverage criteria for deep neural networks, such as DeepXplore (Pei et al., 2017), DeepGauge (Ma et al., 2018a), DeepTest (Tian et al., 2018), etc. Despite using the concept of coverage, many other methods were proposed for testing deep neural networks. For example, Surprise Adequacy (Kim et al., 2019) measured how many surprises did give input give compared to the training dataset. DeepCheck (Gopinath et al., 2019) used the idea of program analysis, especially symbolic execution to test deep neural networks.

Defects in frameworks. This line of work mainly concentrates on the bugs in the code of deep learning frameworks. For example, Pham et al. proposed to test the implementation of deep learning libraries (TensorFlow, CNTK and Theano) through differential testing (Pham et al., 2019). There were also many other works in testing machine learning framework (Srisakaokul et al., 2018, Wang et al., 2020).

6. Conclusion and future work
The quality assurance for machine learning systems has gained growing popularity. However, existing researchers have mainly focused on the quality of the machine learning models deployed in the systems, and neglected the quality of the learning programs, which actually the foundation for the quality of thus trained models. We, in this paper, focus particularly on the quality of machine learning programs, especially those implementing linear classification algorithms. To do so, we derive fundamental stability properties from linear classification algorithms’ kernel statistical nature and propose two PD-MRs accordingly for effectively detecting bugs in learning programs potentially.

We also experimentally evaluated our proposed MRs upon nine well-known linear classification algorithms, and observe a significant improvement over six benchmark MRs proposed by existing work, with 37.6–329.2% bugs being detected. Our experimental results also somehow confirm our conjecture that PD-MRs can be more effective than PI-MRs, which is also reflected in some other research (Le et al., 2014).

Our research also deserves further research along this line. On one hand, our proposed MRs’ specialties and their effectiveness in detecting different types of machine learning bugs might deserve more researches. On the other hand, we also expect more effective PD-MRs to be explored in the future, for different machine learning algorithms besides linear classification ones, in order to better maintain the quality assurance of machine learning systems.

Declaration of Competing Interest
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

Acknowledgments
We thank the editor and anonymous reviewers for their constructive suggestions. This work is supported by the National Natural Science Foundation of China (61932021, 61690204) and the Collaborative Innovation Center of Novel Software Technology and Industrialization, Jiangsu, China .

Appendix. Proof of Propositions 1 and 2
We start by defining some notation. Given dataset 
, where 
 is an -dimensional example, and 
. A linear classification algorithm attempts to solve the following optimization problem:  
 where 
 is the vector of model parameter, function 
 is any given loss function (e.g., Cross Entropy, Hinge loss, modified Huber loss, and so on), and 
 is any regularization function (e.g., 
 norm, 
 norm, and even Elastic loss). Since the loss function  can be decomposed into the summation of losses for each example, the objective function can be reformed as 
 
Let 
, we can obtain that (13)
The above equation holds due to that function  is the surrogate loss of -loss (Cohen, 2014). Besides, it also needs to be mentioned is that the surrogate loss functions are all convex functions (with respect to variable ).

Similar to (Schulam and Saria, 2019), we also assume that the output 
 of algorithm subjects to (14)
 
 
where 
, and 
 is any constant vector. For simplicity, we denote the  by . We can compute the Hessian matrix of  (i.e., the Jacobian of ) by 
 
 

Next, we try to prove Proposition 1, and the proof of Proposition 2 is similar to the proof of Proposition 1. Without loss of generality, we consider that the th example 
 is replaced.

Proof

Let the perturbed example be 
, where 
. In this case, the modified dataset 
 is 
, and the corresponding loss function is 
 
Now that the loss function has changed, the output of the algorithm has also changed accordingly. We denote the changed output by 
, and it should also meet Eq. (14): 
 
 
where 
. Due to the (semi)-smoothness of loss and regularizer, we can use implicit function theorem (Wainwright et al., 2005) to conclude that there exists a local function such that 
. In other words, there is a map from example 
 to trained model’s parameter 
. Hence, we can roughly compute the 
 through 
 
 
 
 
To avoid the computation of Jacobian, we set the perturbation  as a vector orthogonal to , i.e., 
. In this sense, we have 
 
 
 
 
where we denote 
 by  since it is a constant scalar. Now, we can obtain that (15)
 
 
Since the  is convex w.r.t. variable , the first-derivation of  should be non-decreasing, which indicates the monotonicity of function 
. □