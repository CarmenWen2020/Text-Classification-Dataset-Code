Abstract
Background:
Metrics teams play an increasingly important role in handling data and information in modern software development organizations; they manage their companies’ measurement programs, collect and process data, and develop and distribute information products. Metrics teams can comprise several roles, and their set-up can differ between companies, as can the metrics maturity of host organizations. These differences impact the effectiveness and quality of a team’s measurement program.

Objective:
Our objective was to design and evaluate a model to describe the characteristics of a mature metrics team, which efficiently designs, develops, maintains, and evolves its organization’s measurement program.

Method:
We conducted an action research study on four metrics teams of four distinct companies. We designed and evaluated a domain-specific model for assessing the maturity of metrics teams – MeTeaM – and also assessed the four metrics teams per se.

Results:
Our results were two-fold: the creation of the metrics team maturity model MeTeaM and a template to assess metrics teams. Our evaluation showed that the model captures the characteristics of successful metrics teams and quantifies the maturity status of both the metrics teams and their host organizations.

Conclusions:
More mature metrics teams score higher in the MeTeaM model than less mature teams. The assessment provides less mature metrics teams with valuable insights on what factors to improve. Such insights can be shared with and acted upon successfully with their organizations.

Previous
Next 
Keywords
Metrics

Measurements

Software

Team maturity

Software engineering

MeTeaM

1. Introduction
Modern software development organizations – so called data-driven organizations (Olsson and Bosch, 2014) – often utilize data to drive the development of their products and processes. Data are collected, stored and processed using defined methods, tools and processes often referred to as measurement programs (Offen and Jeffery, 1997, Tahir et al., 2016). The measurement programs are socio-technical systems in which a group of dedicated roles collaborates to support stakeholders within host organizations with insights, decisions and warnings about their products, processes and organizations (Staron and Meding, 2018).

Although the concept of data-driven organizations is relatively new, the concept of using measurement data in decision processes is not. Already in 1977 in his seminal work on software metrics, Gilb (1977) demonstrated the need for professional support for measuring software development organizations. The concept of measurement programs and their professional support was further developed in the late 1990s, the focus here being the efficiency and effectiveness of such programs and the ability to use measurement results/data in software process improvement initiatives (Kilpi, 2001). An example of a modern data-driven organization is Microsoft, which uses A/B testing and online experimentation (Fabijan et al., 2018) as input for decisions concerning further development of its products.

Data-driven organizations and measurement programs are supported by software teams that focus mostly on the development, maintenance and evolution of the measurement programs, measurement and data infrastructure, as well as data collection infrastructure.

With the popularization of agile software development, empowered teams, Development and Operations (DevOps) and continuous integration, deployment and delivery, different types of software metrics teams have evolved. These can be dedicated teams or virtual ones, where team members are distributed among product-development software development teams. In this study, therefore, we define the metrics team as a team that plans, defines, develops, maintains and operates the measurement program. Metrics teams are defined by their roles based on the ISO/IEC 15939 standard, such as Measurement Designer, Measurement Analyst or Measurement Librarian.

The concept of the metrics team has evolved over time into, for example, a DevOps team with the holistic responsibility for the measurement programs or a virtual team, where metrics team members (and competence) are distributed throughout different software development teams. This evolution aligned with that of modern organizations towards distributed software development, agility and data-driven development. Diverse examples of measurement teams can be found in modern organizations, including dedicated metrics teams (Jeffery and Berry, 1993, Offen and Jeffery, 1997), measurement librarians (Díaz-Ley et al., 2008), and measurement-oriented Software Process Improvement (SPI) teams (Humphrey, 1989, Iversen and Mathiassen, 2000).

In particular, modern software metrics teams within empowered organizations need to understand how to handle leadership, stakeholder diversity, and the balance of infrastructure commonality with individual needs of teams/roles (Moe et al., 2009). The organizations themselves need to understand how to support the distributed metrics teams, balance resource allocation between the metrics teams/roles and software development, and monitor the evolution of the measurement programs (Stray et al., 2018). Despite its appeal, the idea requires structured processes, methods and teams to support data-driven organizations.

Here, we studied the interaction between the metrics team, the measurement programs, and the host organization. The need for the study was identified by companies A and C, which required a method and tool to drive the development of their respective software metrics teams. These companies used and implemented measurement-related standards that were associated with some limitations. One limitation of these (and other) standards is that they usually define the “what” and not the “how”. For instance, ISO/IEC 15939 defines measurement roles yet provides only one statement about their respective purposes without clarifying how they can/should handle problems and prioritize tasks. Another limitation is that standards are seldom updated. During the time between updates, companies introduce new technologies and evolve ways of working. Thus, the speed at which metrics teams must evolve as regards technology, ways of working, best practice, etc., exceeds that of the updating of standards. The quality of measurement programs can be assessed using established models (Staron and Meding, 2016), and the MeTEAM model presented here builds upon previous knowledge and specifically addresses the question of how to assess the maturity of software metrics team, i.e.,

what characterizes a mature metrics team operating within an empowered measurement organization?

As a result of our 4-month study, we developed and evaluated a metrics team assessment model. The model captured a team’s formal assignment to support the measurement program, as well as the team’s impact, teamwork and longevity in its host organization. The model provided software metrics teams with support in understanding which they areas should develop as regards their formal role in the organization and their assignment, and a measure of their performance. The model provided host organizations with the support they needed to effectively use the results of the metrics teams’ work (the measurement programs) in their organizations. The team assessment model helped organizations to increase their trust in the measurement results, use the results in decision processes and, ultimately, improve their software products and development processes.

The remainder of this paper is structured as follows: Section 2 reviews work related to team maturity and software measurement programs; Section 3 describes the research methodology we followed and our research design; Section 4 presents the MeTeaM model; Section 4.1 presents its operationalization; Section 5 presents the results of the model’s evaluation; Section 6 discusses the validity of our work; and, finally, Section 7 presents our conclusions.

2. Related work on Team Maturity and Software Measurement Programs
Any evaluation of the maturity of software metrics teams builds upon research in team maturity in general and software measurement. As regards team maturity, we studied which perspectives were important in assessing a team’s maturity, as well as the different types of teams that exist, i.e., the software metrics teams, software development teams, and high-performance teams in other domains. In the area of software measurement, we focused on studying the different ways teams and their results can be assessed, i.e., measurement programs, and also investigated how these have evolved over time, beginning with the seminal work of Gilb (1977).

2.1. Team maturity
Team maturity has been studied from a number of perspectives and has gained much attention in software engineering over the last two decades. The first perspective considers the competence of a team, for example, as studied by Cooke et al., 2000, Cooke et al., 2001. The meta-analysis of Cooke et al., 2000, Cooke et al., 2001. provides valuable insights about teams in general, albeit mostly beyond the field of software engineering. The insights do, however, establish that a team is more effective and efficient if there is a good and systematic assembly of skills in the team, i.e., each team has all necessary competences (collectively) with a little or no overlap (which reduces the risks of conflicts). Whilst this is true for teams in general, it is important to note that software engineering teams often stimulate job rotation to reduce fatigue and increase the morale of the team as a whole. Thus, understanding the relationship between the roles of team members and their performance is vital. Software metrics teams are suitable for such studies as the roles in the team are defined by specific measurement standards (e.g., ISO/IEC 15959).

The variable impact of a team’s maturity and teamwork on its performance has been studied by Dingsøyr and Lindsjørn (2013) and Lindsjørn et al. (2016). Whilst the link between a team’s performance and its maturity was not described, this can, however, be traced back to who the respondent was. Nevertheless, the mutual support and learning of a team were found to be the most important aspects as regards its effectiveness.

On the other hand, there is general consensus in the literature concerning project performance that a team’s self-awareness and customer focus are very important, at least in Agile software development projects. The work of Lohan et al. (2013) is a good example; here, the success of projects could be linked to principles, such as acting as a leader (self-awareness) and rewarding success. This shows that the effect of team maturity on successful delivery of a team’s tasks can be interpreted differently depending on the study objectives. It is important, therefore, to include a team’s awareness when assessing teams to collect more data so that dependency between the types of teams and tasks, team maturity, and the effects of a team’s work can be established.

Typical aspects studied in this context are group formation (e.g., homogeneous/heterogeneous as regards knowledge), the ability to solve complex tasks, and sharing the same conceptual model. However, the concept “software team” has greatly evolved over time in a similar way to software development. The evolution of teams is captured well by the study of Lindsjørn et al. (2016), which compared Agile and non-Agile teams.

The evolution of teams in the area of software measurement, however, is linked to more than just a software development paradigm. The first teams were treated as support functions for metrics storage, which is apparent in the titles of team members, i.e., “librarians” (Gilb, 1977). Later, when quantitative methods were more commonly used to assess projects and products, software metrics teams were usually perceived as software quality teams or software process improvement teams (Humphrey, 1989, Iversen and Mathiassen, 2000). This perspective on metrics teams gradually changed as companies started to use measurement programs as a method of continuously monitoring their performance and not only as process improvement initiatives as shown by Kilpi (2001). Finally, the introduction of the ISO/IEC 15939 standard for software measurement programs provided teams with a defined set of roles. Today, Lean and Agile software development is based on concepts of empowerment (Tessem, 2014), such that many software development teams have members focusing on measurement roles in the team while concurrently providing communities of practice (Fægri et al., 2005).

A substantial amount of knowledge exists on the processes and tools available for assessing a team’s maturity in software engineering. Gren et al., 2020, Gren et al., 2017 focus on the psychological aspects of a team’s members; psychological security, the ability to rely on team members, and the balance between personal goals and those of the team/organization are crucial in this context. One of the interesting aspects raised by such research is the psychology of teams as studied, for example, by Lenberg and Feldt (2018), particularly that the clarity of norms and psychological safety align well with the positive self-assessment of teams and, in general, their high performance.

Although generic tools exist for assessing teams and their maturity, we must understand that the task of a team is also important. For example, Kettunen, 2014a, Kettunen, 2014b studied high-performance software teams and found that their performance correlates with their understanding of the goal, e.g., responsiveness to customer needs or changes in the environment.

Furthermore, although a number of models and frameworks have been designed to capture software teams in general (e.g., the teamwork model (Moe et al., 2010) or self-organization of roles (Hoda et al., 2012)), they capture only parts of the specific placement of the metrics teams – they are neither product teams delivering new features to customers nor methods/tools teams providing support for product development teams. Software metrics teams support the goals of their companies and other teams by providing data and insights that can be applied to improve overall performance, not methods and tools (Simard and Lapalme, 2019).

Therefore, in software metrics teams it is important to focus on the leadership of the team and the link to the leadership of the company. Magpili and Pazos (2018), for example, conducted a literature analysis of self-managing software teams. Leadership in self-managing teams is a key factor for a team’s success and maturity, where members of high-performance software teams exhibit stronger leadership abilities. Tabassi et al. (2017) confirmed these results in the context of evolving teams and transformational leadership.

Finally, the motivations of software teams are an important part of performance and must be included in assessment models. In a previous study by Noll et al. (2017), autonomy and distance were found to influence the motivation of a team. Autonomous teams tend to perform better due to their ability to exhibit creativity and innovation.

Autonomy does not, however, only concern freedom; it is also about self-established norms and rules. McHugh et al. (2010) found that routines, such as daily scrum meetings, are important for motivation. These routines, together with team roles and the ability of a team to provide fast feedback are cornerstones of successful teams (Kortum et al., 2019). The ability of a team’s members to voice their opinion helps to increase both the team’s ability to become more effective and efficient, and to build trust within the team (McHugh et al., 2011). Team autonomy is evident in software metrics teams, which support entire organizations by maintaining the company’s measurement program. Such teams, therefore, usually have greater flexibility in establishing their own processes compared to product development teams that deliver products to a company’s customers.

2.2. Software measurement
The tasks of metrics teams focus on the design, development, management, and evolution of the measurement program. Assessment of a metrics team, therefore, is sometimes included in the overall assessment of a measurement program.

One such assessment was conducted by Gopal et al. (2002), where success factors included professional team support (resource sufficiency) and maturity of the organization. Gopal et al. developed one of the first assessment frameworks to include organizational aspects; however, as this preceded the introduction of the ISO/IEC 15939 standard, no formal roles in the metrics team were discussed (Gopal et al., 2002).

The success of the measurement program extends beyond its availability. Gopal et al. (2002) focused on the usage of the measurement program in decision making. Harjumaa et al. (2008) extended this study to include the evolution of measurement programs. One finding of this model was that developers’ commitment to using the measures was crucial, and even more important in the context of Agile organizations and virtual metrics teams.

The efficiency and effectiveness of measurement programs have also been found to be important aspects in the governance of such programs as studied, for example, by Dino and Stouffs (2014). In particular, measurement programs should outlive organizational changes. In dynamic organizations based on empowered teams and Agile/Lean software development, the ability to adapt the measurement program to new needs is vital. These studies have been included in our model’s longevity area, where we linked a team’s performance to the ability of the measurement program to outlive organizational changes (Goodman and Dean Jr., 1982).

The measurement and assessment of the performance of the measurement program itself have been studied by Salmanoğlu et al. (2017), Kamulegeya et al. (2018), and Hebig and Wang (2017). All of these models either explicitly include a metric team’s ability to support the measurement program or implicitly acknowledge professional support. However, as all of these models focus on the assessment of measurement programs, they reveal no insights either on how to provide this support or how the support is structured.

Finally, measurement programs can also be referred to as data management or data-driven organizations (Olsson and Bosch, 2013, Figalist et al., 2020), where the organizations utilize measurement data during their product development. These data science teams are similar in structure; however, their roles are not standardized unlike the roles of metrics teams.

2.3. Summary of the related work
Our review of the related literature revealed a number of trends and findings important for modern software metrics teams. Table 1 summarizes the main findings of this review, which are related to the MeTeaM model presented in this paper.

The findings in Table 1 enabled us to select representative companies for the evaluation of the MeTeaM model and provided input to several aspects of our model (included as references in the model).


Table 1. Main findings of the review of related research on team maturity and software measurement programs.

Finding	Relation to MeTeaM	References
Team performance depends on the goals of the team and its tasks.	The MeTeaM model includes specification of a team’s goals — to support the measurement programs.	Lindsjørn et al., 2016, Fagerholm et al., 2015, Kettunen, 2014b, Magpili and Pazos, 2018, Salmanoğlu et al., 2017, Kamulegeya et al., 2018
Structure is important for the daily work of a team.	The MeTeaM model includes the need for formalization of both the team and meeting structure.	Fagerholm et al., 2015, McHugh et al., 2010, McHugh et al., 2011
Measuring a team’s knowledge must be based on its task using dedicated performance measures/instruments.	The MeTeaM model includes staying up-to-date with research, collaboration with other companies, standardization of work, and organizing Hackathons.	Cooke et al., 2000, Cooke et al., 2001, Gren et al., 2020, Kettunen, 2014a
Empowerment is an important aspect of modern software teams.	The MeTeaM model includes the ability to work as a team, making own decisions and prioritization.	Lohan et al., 2013, Magpili and Pazos, 2018
Group development is a key success factor when building agile teams; both competence and autonomy are important for teams.	The MeTeam model includes the need for professional development.	Gren et al., 2017, Lenberg and Feldt, 2018, Kettunen, 2014a, Noll et al., 2017
Virtual teams allow the work and learning processes to be scaled flexibly.	The MeTeaM model is applicable to both virtual and on-site teams.	Fægri et al., 2005, Hoda et al., 2012
Positive feedback increases team performance.	The team area of the MeTeaM model includes these aspects.	Lohan et al., 2013, Kortum et al., 2019
Team orientation, leadership and coordination are important for effectiveness.	Leadership is part of MeTeaM. Except for common administrative tasks, our model emphasizes the importance of team member also driving technical aspects, innovation and networking.	Moe et al., 2010, Magpili and Pazos, 2018, Hebig and Wang, 2017
Explicit roles in the team are important for a team’s efficiency and effectiveness.	The MeTeaM model includes the same roles as defined in the ISO 15939 standard.	Hoda et al., 2012, Simard and Lapalme, 2019, Magpili and Pazos, 2018, Harjumaa et al., 2008
Improvement teams need to overcome resistance to change.	Team Longevity is incorporated into the MeTeam model.	Magpili and Pazos, 2018, Goodman and Dean Jr., 1982
Adoption of measures and measurement programs depends on both the organizations adopting them and the organizations/teams driving the adoption.	The MeTeaM model explicitly assesses both the teams and their host organizations.	Staron and Meding, 2018, Wallace and Sheetz, 2014, Harjumaa et al., 2008, Zheng et al., 2009, Dino and Stouffs, 2014, Goodman and Dean Jr., 1982
Measurement programs differ in effectiveness between established organizations and start-ups (without dedicated teams).	This finding is fundamental to MeTeaM, which has been designed to capture these differences.	Salmanoğlu et al., 2017, Kamulegeya et al., 2018, Özcan-Top and Demirors, 2019, Salmanoğlu et al., 2018
Data-driven (measurement-driven) organizations need professional teams to support data collection, management and usage.	The MeTeaM model is applicable since we perceive data-collection and management teams to be similar to metrics teams.	Olsson and Bosch, 2013, Figalist et al., 2020
3. Research design
Our chosen research methodology was action research (Baskerville, 1999, Dos Santos and Travassos, 2011) since we were able to include practitioners in the research team. Moreover, this would allow interventions to be made at each company and so provide feedback on the MeTeaM model and its impact after each action research cycle.

The initiative for this project originated with the authors (both academic and industrial) based on their work with measurement programs and an initial diagnosis that software metrics teams need specific support that is currently unavailable. In particular, software metrics teams need support to combine research on a team’s development/performance with its task, i.e., the support and development of their organization’s measurement programs. The authors formed the Action Research Team to design and conduct the study at four companies.

The criteria used to select four companies were based on the review of the related work. Two companies (one with a longer track record and one that was newly established) with formal metrics teams were selected to enable validation of the model’s ability to capture the right aspects of team longevity. Two companies (one with more organizational support and one willing to obtain such support) with distributed, “virtual” metrics teams were selected to enable understanding of the impact of the organization’s support on the metrics team.

Thus, the selection included small and large companies, as well as companies with dedicated and virtual metrics teams, and each company’s goals for the measurement programs were different. The action research cycles are summarized in Table 2.

In the first cycle, we focused on understanding what characterizes a high-performing metrics team. Based on the literature, we found that existing models of a team’s performance are usually targeted towards generic software development teams. Concomitantly, we found that assessment of a team’s performance should be linked to its tasks. We identified relevant team maturity models (e.g., the Rocket Model) and determined how to apply them. Our main focus was on whether these models were specific enough to provide teams with actionable improvement points. Applying our model at Company A showed that capturing the task – support of the measurement program – is important, but that the model should also include aspects of a team’s development and formalization of its task. We found that we needed to design a new, measurement-specific model, which we did in this cycle.


Table 2. Summary of the research cycles conducted.

#	Diagnosing	Action planning	Action taking	Evaluation	Learning
1	How to characterize mature metrics teams?	Analysis of literature and assessment with the metrics team at one company.	Design of the initial model and assessment of the metrics team at Company A, based on the team’s task; identifying improvement potential for the team.	No model specific for either software metrics teams or the related software process improvement teams was found.	Development of the MeTeaM model was continued and elements related to group dynamics were introduced.
2	How well does the model capture the difference between successful and newly established teams?	Additional analyses of literature and addition of one company.	Assessment of the metrics team at Company B.	The MeTeaM model holds for a successful metrics team with a clear difference between the maturity of successful and newly established metrics teams evident.	Further investigations made to ascertain whether there was any difference between teams that operate in small vs. large companies.
3	Is there a difference between teams in small and large companies?	Assessments of the metrics teams at companies not involved in the design of the model.	Assessment of the metrics teams at companies C and D.	The MeTeaM model captured differences between small and large companies.	The MeTeam model required more comprehensive visualization of the results.
4	How can the team communicate its situation to its organization using the model?	Presentations to the teams’ organizations were made using radar charts and two-dimensional scatter plots.	Results were discussed with companies B, C and D. Results were sent to Company A for off-line validation.	The MeTeaM model can be used to communicate strengths and weaknesses to the teams’ organizations.	The MeTeaM model can be used easily by the teams to successfully communicate their strengths and weaknesses.
In the second cycle, we invited an additional company (Company B) to make the model more generic. We chose Company B as it had a virtual metrics team and we wanted to validate the model in this context. A virtual metrics team is one in which team members primarily work in separate software product development teams, and their work is partly related to software metrics. At the same time, they meet regularly to exchange experiences and knowledge about the measurement assessment with one another. The sum of their work forms the measurement program of their company. An example of such a virtual team is a group of measurement designers who create dashboards for different teams. We found that the size of Company B, which was smaller than Company A, could make a difference in how the ways of working of the team. We also found that formalization of the set-up is an important factor for team recognition at Company B. We decided, therefore, to include Company C (has a dedicated metrics team, but is larger than Company A) and Company D (has a virtual metrics team, and is as large as Company C) in the evaluation.

In the third cycle we invited two more companies (C and D) to study how MeTEAM captures the diversity of the metrics teams. We found that the model captures the characteristics correctly, but the results of the assessments had to be easier to understand. The evaluation showed that the selection of improvements the companies made was based on the way results were presented. Thus, visualization of the results had to be improved to facilitate changes/improvements recommended by the metrics teams.

Our aim in the final cycle was to design how the position of the company and its improvement trajectory could be best visualized. The result was visualization using a two-dimensional diagram with the team’s fulfillment of aspects in all areas of team maturity and the organization’s maturity shown on the -axis and -axis, respectively (see Fig. 3 in Section 5 on model evaluation).

The assessments at companies A, B and D were performed by the Metrics Team Leader at Company C. The assessment at Company C was performed by the Metrics Team Leader at Company A. This was done to would avoid any biasing of interviewees and/or steering of assessment results in any specific direction.

3.1. Selected companies
We assessed four metrics teams at four companies.

Company A.
The company is a medium-sized consumer product provider from Sweden with over 3600 employees. Its products are sold worldwide. The company is now owned by a multinational corporation that employs approximately 200,000 employees worldwide. The studied organization within Company A has over 400 developers who work according to Agile principles, and has adopted continuous integration for over five years. They develop an embedded, Linux-based software product. Compared to Company C, Company A has a much smaller product (at least one order of magnitude) and much larger variability as it is sold to consumers. The metrics team consists of 10 employees (all full time) and has been in place for one year.

Company B.
The company is an innovation and technology company from Sweden with over 2000 employees. It is a subsidiary of an Original Equipment Manufacturer (OEM) group with global presence. The studied organization within Company B is a software supplier for embedded systems with about 80 developers who work according to Agile principles and continuous integration. The metrics team consists of four employees who work part time with metrics, corresponding to approximately one full-time employee.

Company C.
The company is a large infrastructure provider from Sweden with over 100,000 employees worldwide. The company has a strong global presence with its products being sold worldwide. The studied organization within Company C has over 2000 developers who work using a combination of Agile and Lean principles, and has adopted continuous integration practices for over ten years. They develop embedded software products that have been on the market for over ten years and have a stable, mature code base. The products are sold to infrastructure operators who subsequently provide these services to their customers. The metrics team consists of eight employees some of whom work part time, corresponding to five full-time employees. Six of the eight team members were senior developers before joining the metrics team, which has been active in metrics research for over 14 years.

Company D.
The company is an automotive OEM from Sweden with over 50,000 employees. The company is a global leader in its market, and has transformed its operations from the classical, automotive V-model-based development to the modern Scaled Agile Framework (SAFe) development during the last two years. The studied organization within Company D has over 400 developers who work using a combination of Agile and Lean principles, and has adopted practices of continuous integration for over five years. They develop embedded software products that have been on the market for over ten years and have a stable, mature code base. The metrics team consists of nine employees some of whom work part time. Eight of the nine team members are senior developers and have been part of the team (established five years ago) for one to five years.

4. Metrics team maturity model
Our MeTeaM model is based on observations that metrics teams must be assessed alongside their host organization and the measurement program that they support. During the diagnostic phase of the first action research cycle, we studied Company A and its metrics team. We compared the set-up of the metrics team to both those described in the literature and the previous experiences of the Action Team. We conducted a workshop with the company’s representatives to elicit the organization’s needs for the software metrics team. The observed needs were as follows:

1.
A mature metrics team needs a formal assignment from the organization and must be recognized by the organization. The formal assignment provides the necessary mandate for the team to access all systems needed to collect data, govern access of relevant stakeholders to the measurements, and for its results to be officially adopted by the organization as part of its data-driven decision processes.

2.
A mature metrics team needs to work according to a prescribed process/ways-of-working, particularly according to established standards and best practices. The structured ways-of-working provide confidence that the team’s results can be trusted, and that their quality has been controlled and assured. It is particularly important that the team works according to the same standards as its organization.

3.
A mature metrics team needs to provide high-quality information products. For measurement data to be used in decision processes, the data must be delivered to stakeholders in a succinct, clear and informative way using relevant information products.

4.
A mature organization needs to recognize the value of the results of the metrics teams and use these results in its decision processes. Measurement data can only be used in decision processes (both decision formulation and decision execution) when the organization trusts the results of the measurement programs. The trust is based on the structured processes used for data acquisition (definition and collection), processing (creating information products), and dissemination (presentation and delivery).

5.
A mature metrics team needs to evolve together with its organization, for example, handling new products and ways-of-working, as well as decommissioning unused information products and data. A team’s ability to evolve allows it to support the evolving needs of its organization, which are necessary to ensure that the company’s customers are satisfied with its products/services, and for continuous software process improvement activities.

These aforementioned high-level needs are captured in six areas of our MeTeaM model, each of which is organized into organization and team maturity checks. In addition to the observed needs, our model also captures the ability of the team to function as a team, i.e., as an organizational unit or as a community of practice depending on the team. We have operationalized these needs in our model using six areas that can be assessed and mapped to these needs, as listed by the model in Fig. 1.

1.
Set-up — How a metrics team is formally set-up, the roles it adopts, and its organizational context. This addresses the need for formalization of a metrics team and its mandate in the organization, as well as the organization’s ability to provide the necessary mandate.

2.
Commitment — How committed a metrics team is to its mission, and how well it follows its processes and standards. This addresses the need for formal ways-of-working and a team’s commitment to them.

3.
Impact — How the results of a metrics team’s work (the measurement program) impact the organization.This addresses the need for capturing how a team’s results impact the organization, its products, processes and – ultimately – its customers.

4.
Longevity — The ability of a team to evolve over time can be measured by its ability to survive changes in its respective organization. Mature metrics teams can adapt to organizational changes and provide value to the organizations both before and after the changes.

5.
Efficiency/Ways-of-working — The ability of a team to quickly transform information needs into high-quality information products. It is important that an organization’s needs are met promptly to ensure that the measurement program is relevant for the organization. The organization also needs to be able to use new measures in their decision processes. Typical challenges relate to the ability to compare different measures in meaningful ways.

6.
Team performance — The ability of team members to function as a high-performance team. Not only are the measurement program and information products important, but a team’s well-being and ability to collaborate are also important. Each team member’s respect for other team members and their ability to help and collaborate well with the organization are crucial.

The Set-up, Commitment, Impact and Team Performance areas are multidimensional and so are further divided into sub-areas; Longevity and Efficiency/Ways-of-working do not require sub-areas as they are meant to support the other areas. Sub-areas are used in the model to group specific aspects together, for example, distinguishing the formal set-up of a team from the presence of formal roles in a team.

4.0.1. Set-up
The formal set-up of a metrics team provides the team with both the recognition and the mandate to operate within its organization. The elements constituting the formal set-up are listed in Table 3. The first sub-area, “Formalization of the metrics team”, focuses on a team’s assignment, resources, and ways-of-working as defined by Staron and Meding (2018). It also requires formalization of the team’s vision, mission and strategic plan, which are crucial for its continuous focus on the needs of its organization.

The second and third sub-areas, “Roles in the team” and “Roles in the organization”, respectively, assess the presence of measurement-related roles. These roles need to be formalized for the team to be able to structure its work. Whilst everyone knows/expects the team to have measurement-related roles, the organization needs to have dedicated roles to interact with the metrics teams. All roles must be present to ensure the success of both the team and its organization (ISO/IEC/IEEE, 2017).

The main source of the measurement-related roles in a team is the ISO/IEC 15939 standard (ISO/IEC/IEEE, 2017). Since the standard focuses only on the process part of the assessment and not practical issues (e.g., how to build a measurement system or set-up an infrastructure), we complemented it with additional roles, namely the Metrics Team Leader (Gilb, 1977),1 the Measurement Designer (Staron et al., 2011), the Database Administrator (Gilb, 1977), the Data Analyst (Abbott, 2014), and the Data Scientist (Van der Aalst, 2014). The latter two roles have emerged in the last five years in the measurement community and have become key roles in data handling to tackle the increasing importance and quantity of data in modern software organizations, as well as the need to ensure data quality.


Table 3. Team maturity area — Set-up.

Sub-area	#	Statement	T/O	R.
Formalization	1	The metrics team set-up is formal, e.g., the team is officially recognized by the organization.	Org.	Staron and Meding (2018)
2	The metrics team has a defined assignment, e.g., there is a purpose and set-up of the team, approved by a Line Manager.	Org.	Staron and Meding (2018)
3	The metrics team has dedicated resources, e.g., team members work full- or part-time in the team, and the organization knows who belongs to the team.	Org.	Staron and Meding (2018)
4	The metrics team has defined ways-of-working, e.g., the ways-of-working are documented.	Team	Staron and Meding (2018)
5	The metrics team has an approved definition of its vision, mission, and strategic plan.	Org.	Staron and Meding (2018)
6	There is a formal point of contact with the metrics team, e.g., JIRA.	Team	Meding and Staron (2020)
Roles in the team	7	Metrics team leader: manages the team and ascertains the adherence to the assignment from the organization.	Team	Gilb (1977)
8	Measurement designer: designs, implements and tests measurement systems.	Team	ISO/IEC/IEEE (2017)
9	Measurement analyst: responsible for planning, execution, evaluation and improvement of measurement specifications.	Team	ISO/IEC/IEEE (2017)
10	Database administrator: responsible for managing data store(s), definition of the architecture of the measurement infrastructure, automation of access to data sources, and monitoring of the information quality.	Team	ISO/IEC/IEEE (2017)
11	Data analyst: conducts analysis on data and interacts with stakeholders and measurement users.	Org.	Abbott (2014)
12	Data scientist: helps stakeholders to interpret data, performs predictions, leads AI activities, and supports management in taking data-based decisions.	Org.	Van der Aalst (2014)
Roles in the organization	13	Measurement Sponsor: authorizes and supports the establishment of the measurement process.	Org.	Abran, 2010, ISO/IEC/IEEE, 2017
14	Measurement process owner: responsible for the specification of what is measured, when and how, i.e., the measurement process.	Org.	ISO/IEC/IEEE (2017)
15	Stakeholder: has a share, claim or interest in a system/its characteristics that meet their needs and expectations.	Org.	ISO/IEC/IEEE (2017)
16	Measurement User: uses the measurement information products in his/her work.	Org.	ISO/IEC/IEEE (2017)
4.0.2. Commitment
Commitment is a promise or firm decision to use measurement data in the decision processes as early warnings, insights or decision support (Rodgers et al., 1993). Commitment is vital for both the measurement program to succeed and a metrics team to provide value to its organization. It is supported by the formal set-up, although formalization is not required for the basic commitment. A committed metrics team excels in its performance and is willing to improve continuously, always with the best interests of its team and organization in mind. Commitment leads to the constant evolution of teams and organizations, enabling them to comply with continuous evolution of their products and ways-of-working.

Table 4 assesses the commitment of the team and the organization, which must be mutual. Organizations that focus strongly on the commitment of their metrics teams yet ignore requirements on themselves do not utilize the measurement programs provided by metrics teams. For instance, the commitment of an organization’s management guarantees the resources necessary for the metrics team, and the commitment of an organization to the metrics team guarantees its compliance to the metrics rules/standards and ways-of-working stipulated by the metrics team. Commitment of the organization is what, ultimately, will enable the organization to become and/or remain a mature, measurement organization (Staron and Meding, 2018, Gopal et al., 2002, Ferris, 2007).


Table 4. Team maturity area — Commitment.

Sub-area	#	Statement	T/O	R.
Team’s commitment	1	The metrics team improves/evolves its ways-of-working, e.g., the team strives, continuously, to improve the time it takes to develop information products.	Team	Magpili and Pazos (2018)
2	The metrics team identifies new information needs that should be addressed by the organization, e.g., the team identifies and supports new projects and teams.	Team	Abran (2010)
3	The metrics team identifies new stakeholders, e.g. the team identifies and supports employees who become project and team leaders for the first time.	Team	Staron and Meding (2018)
4	The metrics team provides continuous training to the organization to disseminate the competence, e.g., the team organizes courses, presentations, and/or seminars covering all/parts of a measurement program.	Team	Gopal et al. (2002)
5	The metrics team conducts research projects, e.g., the team conducts research projects under the guidance and/or participation of academic personnel. The research projects should result in (a) the improvement of the measurement program(s) of the organization, and, (b) academic publications.	Team	Sandberg et al. (2011)
Organization’s commitment	6	The metrics team receives management support, e.g., the line supports the team financially so that it can acquire new tools, software programs, hardware, etc., and employ new team members when needed.	Org.	Gopal et al. (2002)
7	The organization utilizes the metrics team for metrics-related issues, e.g., the metrics team is the first choice of contact when stakeholders and measurement users need new measures.	Org.	Gopal et al. (2002)
8	The metrics team receives explicit management support for research, e.g., the team receives time to conduct research projects.	Org.	Sandberg et al. (2011)
9	A metrics community exists within the organization, e.g., a virtual metrics team, Community of Practice.	Org.	Ferris (2007)

Table 5. Team maturity area — Impact.

Sub-area	#	Statement	T/O	R.
On the organization	1	The metrics team provides deliverables, e.g., information products, measurement data, infrastructure, and a measurement experience base.	Team	Kilpi (2001)
2	The metrics deliverables are used in the organization, e.g., the status of information products is an input to the decision-making process.	Org.	Kilpi (2001)
3	The metrics deliverables are perceived as valuable, e.g. they are used as an integrated part of the organization’s everyday work.	Org.	Gencel et al. (2013)
4	The metrics team impacts norms and practices in the organization, e.g., by providing valuable information products. It is easy for the organization to understand what has been measured and how well the organization performs.	Org.	Kilpi (2001)
5	The Measurement Experience Base is used by the metrics team and the organization, e.g., the team studies information stored in this database prior to developing new measures.	Org.	Staron and Meding (2018)
6	All (main) information needs of the organization are covered by the measurement systems provided by the metrics team, e.g., goals related to business, departments and projects/programs are formally established and followed-up.	Org.	Staron and Meding (2018)
7	All business areas are covered by the metrics team, e.g., all areas listed in the organization’s balanced scorecard have information products developed by the metrics team.	Org.	Butler et al. (1997)
8	The organization matures in measuring over time thanks to the efforts of the metrics team, e.g., they use metrics related-notions from standards and take data-based decisions.	Team	Staron and Meding (2018)
9	Information products include information quality indicators, i.e., information that shows if the products can be trusted or not.	Team	Staron and Meding (2009)
Outside/outreach	10	The metrics team’s work has a significant positive impact on the customers’/organization’s businesses, e.g., the team provides early warning information products that enable customers to be pro-active.	Org.	Kilpi (2001)
11	The metrics team actively participates in conferences, e.g., metrics, data science, machine learning and AI conferences.	Team	Buse and Zimmermann (2012)
12	The metrics team is active in international standardization committees, e.g., ISO/IEC/IEEE 15939, ISO/IEC/IEEE (2017), and the ISO/IEC 25000 family, ISO/IEC (2016).	Team	Staron and Meding (2018)
13	The metrics team has contact with other companies.	Team	Fenick, 1990, Staron and Meding, 2016
14	The metrics team organizes and/or participates in Hackathons.	Team	Staron and Meding (2018)
4.0.3. Impact
Formalization of a metrics team, its commitment to its organization and the organization’s commitment to the team can be seen as activities that need to produce results. The metrics team’s results can be measured/assessed as its impact on the organization and its processes and products. The next area to assess, therefore, is the impact of a metrics team on the organization in general, and more specifically on the organization’s performance. Table 5 presents the aspects included in this area.

In short, a metrics team delivers information products and knowledge and its impact needs to be assessed by the impact of the information products and knowledge. The information products are defined as “one or more indicators and their associated interpretations that address an information need” (ISO/IEC/IEEE, 2017). Examples of information products are MS Excel files, dashboards, e-mails with diagrams and measurement results, and mobile phone apps. By providing knowledge, the team supports its organization with metrics-related questions, such as “What should we measure?” and subsequently “How should we measure that?”. The team can also provide training, courses and seminars with the goal of elevating the metrics knowledge and competence of its organization.

It is essential for information products to capture the needs of the organization and concomitantly be perceived as valuable and trustworthy by the organization. Moreover, the work of the metrics team must continuously contribute to the measurement maturing of its organization, as first indicated by DeMarco (1986) and later by Kneuper (2018).

The impact of a truly mature metrics team extends beyond its immediate organization, positively impacting, for example, sister organizations in its company, customers, other companies, and the academic community. One important way for a metrics team to achieve this outreach is to participate in and/or organize Hackathons. For a metrics team to reach its full potential and achieve its highest impact, it must conduct research; such research will provide in-depth knowledge and enable the team to break new ground.

The rationale behind the Impact area listed in Table 5 is that it should assess the impact of the metrics team on its organization and its outreach.

4.0.4. Longevity
The impact on an organization can be momentary when its metrics team solves a specific task. The value of metrics teams, however, is in their ability to deliver value continuously and to reinvent themselves over time. Such reinventing teams are important for modern, dynamic, software development organizations, as described by Laloux (2014). Changes in an organization’s structure, introduction of new technologies and ways-of-working are natural parts of the life-cycles of organizations. They occur periodically and can vary in size. Metrics teams must continuously address – and preferably be pro-active to – such events and stay tuned into their organizations, as described by Ferris (2007) and Krell (2000). The rationale behind the Longevity area, therefore, is to assess the evolution of a metrics team in its organization.

Table 6 groups the important aspects to consider if a metrics team is to endure for longer and provide value throughout the lifetime of its organization.


Table 6. Team maturity area — Longevity.

#	Statement	T/O	R.
1	The metrics team covers more than one organizational unit/product/project, e.g., the support of the team stretches beyond the organization it belongs to.	Team	Ferris (2007), Krell (2000)
2	The metrics team survives organizational changes, e.g., the team always has a designated place and function during/after any change in an organization’s structure.	Org.	Ferris (2007), Krell (2000)
3	The metrics team evolves to meet technology changes, e.g., the team is capable of supporting its organization as old products change and/or new products evolve.	Team	Ferris (2007), Krell (2000)
4	The metrics team evolves its ways-of-working to meet its organization’s new ways-of-working, e.g. when the organization goes from Waterfall to Agile ways-of-working.	Team	Ferris (2007), Krell (2000)
5	The metrics team adjusts its ways-of-working to meet its organization’s needs, e.g., the team either improves its efficiency if the organization needs more information products or starts to develop and use machine learning algorithms if the organization needs more advanced data handling.	Team	Ferris (2007), Krell (2000)
6	The metrics team is perceived as an interesting/cool place to work, e.g., the perception of the organization is that the team has skilled and experienced engineers, works with the latest technology, has networks both inside and outside the company and in the academic world, and that working in the team results in career advancements.	Org.	Ferris (2007), Krell (2000)
7	The metrics team’s assignment is perceived to be valuable to the organization, e.g., its deliverable, such as information products, are a natural and necessary part of the organization’s everyday work.	Org.	Ferris (2007), Krell (2000)
8	The metrics team provides career development opportunities, e.g., working as Metrics team leader either acts as a springboard for managerial tasks or is considered meritorious when being evaluated for the position of Senior Designer.	Org.	Ferris (2007), Krell (2000)
9	The metrics team provides competence development for its members, e.g., team members can take courses to become Data analysts and/or data scientists.	Team	Ferris (2007), Krell (2000)
4.0.5. Efficiency/Ways-of-working
The ability to provide value over time must take into consideration the balance between the cost of having a metrics team and the benefits it generates. The rationale behind the Efficiency/Ways-of-working area listed in Table 7, therefore, is to assess the ability of the metrics team to define, develop and implement information products as quickly as possible, while maintaining high-quality results. A key to achieving this is to have a robust, scalable, and easily maintained infrastructure. The measurement infrastructure constitutes the very ecosystem of measurement systems, information products, storage areas, and distribution mechanisms.

Contemporary software organizations have a high focus on efficiency. Measures of speed, impediments and throughput are examples of measures that constitute a natural part of modern measurement programs. One aspect of addressing the challenges of its company is that a metrics teams must act quickly, with high accuracy and quality. Moreover, the metrics team also needs to consider that budget restrictions exist and will limit the size of the team.

Table 7 lists aspects that assess a metrics team’s efficiency and helps the team to evolve its efficiency.


Table 7. Team maturity area — Efficiency/Ways-of-working.

#	Statement	T/O	R.
1	The metrics team has a robust, scalable, maintainable infrastructure, e.g., the architecture of the measurement infrastructure is based on measurement standards, data flow and integrity are monitored, the execution both of calculations and product information updates is efficient.	Team	Staron and Meding (2018)
2	The metrics team has a short lead-time for establishing new measures and information products, e.g., one interviewee at Company C stated that their goal was to design and implement an information product in less than 30 min.	Team	Staron and Meding (2018)
3	The metrics team can easily add/remove/change/evolve measures and information products, e.g., the team works according to well defined ways-of-working.	Team	Staron and Meding (2018)
4	The metrics team uses inter-operable tools (whenever possible), e.g. the team uses tools that are used and/or recognized by the organization.	Team	Staron and Meding (2018)
5	Assessments are performed to ascertain the quality of measures used in the organization, e.g., the Measurement Sponsor, Measurement process owner, and quality managers perform assessments regularly to assess how well the team fulfills the measuring needs of its organization.	Org.	Staron and Meding (2018)
4.0.6. Team performance
The rationale of the team performance area is to assess how well the metrics team members perform as one team. Focusing on soft issues, such as trust and courage, is equally as important as focusing on hard issues, such as efficiency and documentation.

After reviewing a number of models, we chose to base this area on the Rocket Model described by Curphy (2012). This model of team performance covers core areas, such as resources or buy-in, and consists of eight areas:

1.
Context: the situation in which the team operates.

2.
Mission: the team’s purpose and goals.

3.
Talent: the people that comprise the team.

4.
Norms: the team’s formal and informal work processes.

5.
Buy-in: the level of motivation among team members.

6.
Resources: the team’s assets.

7.
Courage: the team’s approach to managing conflicts.

8.
Results: the team’s track record.

The Rocket Model lists 138 questions, some of which have already been covered in other areas (e.g., Set-up in Section 4.0.1). Table 8 lists the areas selected specifically for assessment of metrics teams.


Table 8. Team maturity area — Team performance.

Sub-area	#	Statement	T/O	R.
Context	1	The metrics team shares a common understanding of its key internal and external stakeholders, i.e., customers, competitors, regulators, suppliers, the broader organization, and other internal teams.	Team	Curphy, 2012, Elrod and Tippett, 1999
2	Metrics team members agree on the most important challenges facing the team, e.g., there is team consensus as to what the most important challenges are.	Team	Curphy, 2012, Elrod and Tippett, 1999
Mission	3	The metrics team has developed effective strategies to overcome obstacles and achieve its goals, e.g., when overwhelmed with tasks, the team knows how and what to prioritize.	Team	Curphy, 2012, Espinosa et al., 2007
4	The metrics team regularly reviews progress against team goals and plans, e.g., documented goals and plans are followed up regularly at team meetings (e.g., Scrum or Kanban).	Team	Curphy (2012)
Talent	5	The metrics team has the right combination of skills and experience, e.g., the team has a balanced mix of age and gender, and experienced and novice employees.	Org.	Curphy, 2012, Faraj and Sproull, 2000, Liang et al., 2007
6	Metrics team members are all effective team players, e.g., each member contributes and supports others.	Team	Curphy, 2012, Sjøvold, 2006, Faraj and Sproull, 2000
Norms	7	Metrics team meetings are effective and efficient, e.g., the team has defined different types of meetings, such as backlog follow-up, planning, retrospect, education, and an agenda exists for each type of meeting.	Team	Curphy, 2012, Sjøvold, 2006, Ryan and O’connor, 2009
8	Metrics team members communicate openly and directly with each other and gossiping is rare, e.g., every member feels he/she can express his/her opinion freely. Feedback from/to the team is constructive.	Team	Curphy (2012)
9	Metrics team members are held accountable for their attitudes, behaviors, and deliverables.	Org.	Curphy (2012)
10	The metrics team routinely reviews how to work together more effectively, e.g., team meetings are focused and short; each member is well aware of what others are working on.	Team	Curphy, 2012, Ryan and O’connor, 2009
Buy in	11	Metrics team members understand how their actions contribute to the team’s overall success, e.g., each team member can trace his/her contribution to a robust infrastructure, efficient ways-of-working, and valuable information products.	Team	Curphy, 2012, Faraj and Sproull, 2000, Sjøvold, 2006
12	Metrics team members faithfully adhere to team decisions and rules, e.g., they follow-up on decisions made, established plans, and set strategy.	Team	Curphy, 2012, Sjøvold, 2006
Resources	13	The metrics team has the necessary level of political sponsorship to be successful, e.g,. all managerial levels support the work of the team and communicate this to its organization frequently.	Org.	Curphy (2012)
14	The metrics team is empowered to make key decisions, e.g., the team itself decides how prioritize any backlog.	Org.	Curphy (2012)
15	The metrics team proactively re-negotiates deliverables and/or backlog when faced with resource shortfalls, e.g., before vacation periods.	Team	Curphy (2012)
Courage	16	Metrics team members share a high degree of trust and collaboration, e.g., seniority and experience are respected, and team members work as one when emergencies occur.	Team	Curphy, 2012, Espinosa et al., 2007
17	Metrics team members feel safe challenging one another, e.g., senior members respect newcomers, and old ways of working can be freely challenged.	Team	Curphy, 2012, Espinosa et al., 2007
18	The metrics team has lively debates and even the most difficult issues are raised and handled respectfully, e.g., team meetings are characterized by energy, respect and the active participation of all members.	Team	Curphy, 2012, Liang et al., 2007
19	The metrics team constructively resolves disagreements.	Team	Curphy, 2012, Liang et al., 2007
4.1. Using the MeTeam model
An assessment of the maturity of each metrics team can be performed by anyone with experience of leading assessments (e.g., a Quality Manager or a leader of another metrics team) or by another Software Engineer external to the team. An assessment is performed by interviewing members of the metrics team and its stakeholders. Complementary yet optional sources of data that can be considered are participation in meetings and document studies.

During the assessment, each aspect of the MeTeaM model is used as a question to be answered with one of the following responses:

•
Fully compliant means that the statement is fulfilled and documented in one of the team’s documents (e.g. mission, strategy plan or ways of working) and that the way the statement is implemented and used is as described in the document.

•
Partially compliant means that the team understands the statement, works towards it, but does not fully comply. More specifically, this means that: (a) the statement is implemented and used by the team, but not documented; or (b) the statement is documented, but not implemented and used by the team; or (c) there is a deviation between how the statement is documented and how the team has implemented and uses it.

•
Not needed means that the statement is not relevant to the team. For example, the organization has defined and documented ways-of-working to identify new stakeholders so the team does not need to identify new stakeholders, thereby making the question “The team identifies new stakeholders” (item 3 in Table 4) obsolete.

•
Missing means that the team does not address the question; the statement is neither documented nor implemented/used.

Instead of using only two alternatives – compliant and non-compliant – we decided to follow the same model as SPI assessments. In particular, we provided the possibility to state that a question was not applicable or that the team/organization was working on a specific aspect without yet being fully compliant. Examples of team/organization assessments conducted similarly are described by Ozcan-Top and Demirörs, 2013, Gren et al., 2017, Lenberg and Feldt, 2018 and Kettunen (2014a).

Table 9 is an example of an assessment template in the form of table that can be used to collect the results, and shows an excerpt of questions from the Impact area.

Data collected from interviews is validated during a workshop in which the metrics team, stakeholders, measurement sponsors, and line managers participate. The goal of this validation workshop is two-fold: firstly, it is designed to align the views of the metrics team on compliance with each aspect of the assessment; and secondly, should the team/organization not be fully compliant, to determine a constructive prioritization of which elements to improve.


Table 9. Assessment template table for the impact area.

Sub-area	#	Statement	Fully compliant	Partially compliant	Not needed	Missing
On the organization	1	The metrics team provides metrics deliverables.				
2	The metrics deliverables are used in the organization.				
3	The metrics deliverables are perceived as valuable.				
4	The metrics impact norms and best practices in the organization.				
9	Information products include information quality indicators.				
Results should be presented by separating those aspects with which the metrics team is fully compliant and partially compliant. To enable results to be clearly visualized, we use radar charts for the company/organization, where we calculate the percentage of the compliance per area. This type of visualization provides a good overview of the compliance per area. An example of such a diagram is shown the following section.

5. Evaluation of MeTeaM at four companies: results and interpretation
According to our research design, we evaluated the model by applying it at the four companies selected and discussing its quality with these companies. All assessments were performed on site, and included the metrics team, line manager(s), and other leading roles (release managers and quality managers).

Company A’s metrics team consisted of ten employees; all worked full time in the team and most had a long experience (more than seven years) in software development. The metrics team was established approximately one year ago, and has begun its journey to becoming a mature metrics team. There was a strong measurement of commitment from both the team itself and its organization.

Company B’s metrics team consisted of four employees, who worked part time with metrics (equivalent to one employee) as the company had just started its metrics team development.

Company C’s metrics team had eight employees, half of whom work part time in the team; one worked as a Quality Manager, two as product development leaders, and one as a Technical Expert. The team had been in place for approximately eleven years, had strong management support, and operated in a mature measurement program.

Company D’s metrics team had been in place for five years and had nine employees, all of whom bar one were senior software developers who had been in the company for over seven years. There was a will, both in the team and its organization to become mature in measuring.

5.1. Evaluation of the model
The detailed results from the assessments of each team are presented in Table 10, Table 11, Table 12, Table 13, Table 14, Table 15 and A, B, C and D correspond to the answers from teams of the respective companies. Each data point is the result of the evaluation after the workshop, which means that it had been agreed by the team/organization (i.e., it does not represent individual answers of respondents in the interviews).

After presenting the results to each team, we asked members to give their opinion on the model. We asked specific questions about missing or superfluous aspects, and the degree to which the model both quantified the maturity of the team/organization and was understandable/applicable without the researchers.

The metrics teams at companies A and C commented on the results immediately after the assessment. All team members from both companies agreed that the model captured the characteristics of mature metrics teams and that the results accurately reflected the current status.

We presented the results to companies B and D twice; firstly to the respective metrics teams, and secondly to larger groups. The second presentations were made on request by the teams themselves; our role was solely as moderators and experts. Except for the metrics teams per se, participants at the second meetings in both companies were line representatives and those with leadership roles, such as release leaders and quality managers. The purpose of the second meetings was to make the organizations aware of the situation of the metrics teams and initiate actions for improvements. During all four meetings, we received unanimous feedback that our model captured the characteristics of mature metrics teams, and that the results of the assessments were in line with how the metrics teams and their organizations perceived the situation.

The results also reflected positive feedback on our model. There were very few answers under the column “Not Needed”. Companies A, B, C and D scored a total of 3, 0, 1, 0, respectively (see Table 8, Table 9, Table 10, Table 11, Table 12, Table 13). For companies A and C, this was due to their specific ways-of-working. Company C, for example, has a strong policy about career development for all of its employees; since it provides many opportunities for personal development, there is no need for the metrics team per se to provide such opportunities (item 8, Table 13).

Finally, the main finding was a unanimous response from all teams (and their respective organizations) that the MeTeaM model accurately captured all aspects of mature metrics teams.

5.2. Insights for the metrics teams
The metrics team in Company A is well-established, which is also demonstrated by the high management support (67%) and a formalized team set-up (69%). The team has a mandate from the organization, resources, established measurement roles, and buy-in from the organization (e.g,. a clear Measurement Sponsor). Not only does management actively support the team (e.g., the team members work full time in the team), but they also have the necessary tools and conduct research projects. Management support and commitment, by extension, provides a solid foundation for the longevity of the team (this was low as the team had only been established recently — approximately 6 months prior to assessment).

During the workshop, the metrics team decided to focus its development on the following areas:

1.
Information Quality (item 9, Table 5). Rationale: the team found that trust in the results of the measurement processes were crucial for increasing the impact of their measurement products on the organization.

2.
Assessments are performed to ascertain the quality of measures used in the organization (item 5, Table 7). Rationale: the team decided to enhance contact between itself and the organization by providing the organization with dedicated processes for evaluating the quality of the measures and indicators.

Company B had a virtual metrics team, which was the least formal of the four teams studied; this was also captured by our model. Results suggest that the company should continue with the development of their set-up, as was also decided during the workshop with specific focus on the following items:

1.
The set-up of the metrics team is formal, and, The metrics team has a defined assignment (items 1 and 2, respectively, Table 3). Rationale: the management acknowledged the existence and usefulness of the metrics team, but it needed formalization to make the team more visible within the organization.

2.
Stakeholder (item 15, Table 3). Rationale: the very first step (and a cornerstone) in promoting successful handling of measures in the organization and the team was to ensure the stakeholder role was in place; thus, finding the stakeholder with the right mandate, formalizing the collaboration, and establishing the communication process were prioritized.

The metrics team in Company C was fully compliant in several areas yet still had room for improvement and was working to address outstanding issues:

1.
Data Analyst (item 11, Table 10). Rationale: recruitment would help the team evolve their information products and thus provide more valuable insights to its organization.

2.
Information products include information quality indicators (item 9, Table 12). Rationale: this would increase the organization’s trust in the delivered information products.

3.
The metrics team is active in international standardization committees (item 9, Table 12). Rationale: this would help the team to evolve further and contact relevant international authorities.

Company D was on its way to establishing an effective measurement culture based on an empowered metrics team. A number of aspects were in place (e.g., a Measurement Sponsor) and the metrics team was evolving in line with changes in technology. Company D asked us for our recommendations to further enhance ongoing team activities. We suggested that the following recommendations were addressed by the team:

1.
Stakeholder and Measurement User roles (items 15 and 16, Table 3). Rationale: improving these two roles to “Fully Compliant” would increase team visibility in the organization, establish its ways-of-working, and increase its impact on the organization.

2.
The metrics team conducts research projects (item 5, Table 4) and, The metrics team organizes and/or participates at Hackathons (item 14, Table 5). Rationale: the activities would widen the team’s network, elevate its competence, broaden its experience, and maintains its high morale.

5.3. Detailed results of the assessments
The first area – Set-up – is presented in Table 10. None of the metrics teams found any of these aspects “Not Relevant” with the majority of aspects having been fulfilled either completely or partially.

Table 11 shows the results of the Commitment area assessment, which show similar patterns, i.e., more mature teams comply with more aspects than non-mature teams.


Table 10. Assessment results of team maturity area — Set-up.

#	Statement	Fully compliant	Partially compliant	N.n	Mis.
1	The metrics team set-up is formal.	A, C	D		B
2	The metrics team has a defined assignment.	A, C			B, D
3	The metrics team has dedicated resources.	A, C	B, D		
4	The metrics team has defined ways-of-working.	C	B, D		A
5	The metrics team has an approved definition of its vision, mission, and strategic plan.	A, C			B, D
6	There is a formal point of contact with the metrics team.	A, C, D	B		
7	Metrics team leader	A, C	D		B
8	Measurement designer	A, B, C, D			
9	Measurement analyst	A	B, C		D
10	Database administrator	C	A, B, D		
11	Data analyst		A, C, D		B
12	Data scientist	A	C		B, D
13	Measurement sponsor	A, C, D	B		
14	Measurement process owner	C	B		A, D
15	Stakeholder	C	A, B, D		
16	Measurement user	A, B, C	D		
Table 12 shows the results of the Team Impact assessment. Compared to the previous two areas (Set-up and Commitment), even the most mature teams showed potential for improvement. Impact varies over time and would be expected to require constant attention from both the team and its host organization.


Table 11. Assessment results of team maturity area — Commitment.

#	Statement	Fully compliant	Partially compliant	N.n	Mis.
1	The metrics team improves/evolves its ways-of-working.	A, C	B, D		
2	The metrics team identifies new information needs that should be addressed by the organization.	C, D	A, B		
3	The metrics team identifies new stakeholders.	C, D	A, B		
4	The metrics team provides continuous metrics training to the organization.	C	A, D		B
5	The metrics team conducts research projects.	A, C			B, D
6	The metrics team receives management support.	A, C	B, D		
7	The organization utilizes the team for metrics-related issues.	A, C	D		B
8	The team receives explicit management support for research.	A, B, C			D
9	A metrics community exists within the organization.	A, C			B, D
Only team C was “Fully Compliant” with the majority of the statements. Two statements (9 and 12) were not fulfilled by any of the assessed teams. Not all information products included information quality indicators and none of the teams participated in international standardization committees. Their lack of participation in standardization committees means that they are unable to directly influence how industry standards evolve, which has been found to be important by Abran (2010).

The assessments shown in Table 13 suggested that three metrics teams, A, C and D, had higher maturity. Company B’s metrics team was the one most recently created, which was reflected in their lower compliance. Table 13 also shows that a few items (2 and 8) were perceived as “Not Needed”. This interesting finding suggested that these items are not necessary in our model at all. We investigated this by asking the metrics teams if this was the case. Team A, for example, did not find item 2 at all necessary; this was based on their heritage as part of a dynamic organization where empowerment had been practiced before the metrics team was established. As regards item 8, Team C was perceived by its organization to be very interesting to work with so the opportunity to work with this team was considered to be a career path per se.


Table 12. Assessment results of team maturity area — Impact.

#	Statement	Fully compliant	Partially compliant	N.n	Mis.
1	The metrics team provides metrics deliverables.	C, D	A, B		
2	The metrics deliverables are used in the organization.	C	A, B, D		
3	The metrics deliverables are perceived as valuable.	A, C	D		B
4	The metrics impact norms and best practices in the organization.	C	A, B, D		
5	The Measurement Experience Base is used by the metrics team and the organization.	C			A, B, D
6	All (main) information needs of the organization are covered by the measurement systems provided by the team.		A, B, C, D		
7	All business areas are covered by the metrics team.	C	B, D		A
8	The organization matures in measuring, over time, thanks to the efforts of the metrics team.	C	A, B, D		
9	Information products include information quality indicators.		B, C, D		A
10	The work of the metrics team has a significant impact on the customers’/organization’s businesses.	C	A, D		B
11	The metrics team actively participates in conferences.	C	A, B, D		
12	The metrics team is active in international standardization committees.			A	B, C, D
13	The metrics team has contact with other companies.	A, C	B, D		
14	The metrics team organizes and/or participates in Hackathons.	A, C			B, D
Efficiency was important for all teams, as shown in Table 14. Most items were either “Fully Compliant” or “Partially Compliant”, as teams recognized the importance of Efficiency and Ways-of-working and worked actively to maintain or improve their status. The only item stated as “Missing” was item 5, quality-related assessments, at companies A and D.


Table 13. Assessment results of team maturity area — Longevity.

#	Statement	Fully compliant	Partially compliant	N.n	Mis.
1	The metrics team covers more than one organizational unit/product/project.	A, C, D			B
2	The metrics team survives organizational changes.	C, D	B	A	
3	The metrics team evolves to meet technology changes.	A, B, C, D			
4	The metrics team evolves its ways-of-working to meet its organization’s new ways-of-working.	A, C, D	B		
5	The metrics team adjusts its ways-of-working, to meet its organization’s needs.	B, C, D	A		
6	The metrics team is perceived as an interesting/cool place to work.	C	A, D		B
7	The metrics team’s assignment is perceived to be valuable to the organization.	A, C	B, D		
8	The metrics team provides career development opportunities.	A, D		C	B
9	The metrics team provides competence development for its members.	A, C	D		B
Finally, Table 15 shows the results of the Team Performance area. All teams were working actively in this area, and on becoming a high-performing teams. Even those companies where the teams were distributed and not-yet formalized were actively looking for ways to establish team spirit. Statement #12, adhering to decisions, was perceived as “Not Needed” by Team A, which comes from a company with traditions of empowerment, and thus taking responsibility for documentation and rules.


Table 14. Assessment results of team maturity area — Efficiency/Ways-of-working.

#	Statement	Fully compliant	Partially compliant	N.n	Mis.
1	The metrics team has a robust, scalable, maintainable infrastructure.	A, C	B, D		
2	The metrics team has short lead-time for establishing new measures and information products.	B	A, C, D		
3	The metrics team can easily add/remove/change/evolve measures and information products.	A, B, C, D			
4	The metrics team uses inter-operable tools (whenever possible)	C, D	A, B		
5	Assessments are performed to ascertain the quality of measures used in the organization.	C	B		A, D
The results for each company can be visualized in Fig. 2. This visualization is a very basic way of showing the position of each metrics team and organization (fully compliant), as well as the areas which they are working on (partially compliant).


Table 15. Assessment results of team maturity area — Team performance.

#	Statement	Fully compliant	Partially compliant	N.n	Mis.
1	The metrics team shares a common understanding of its key internal and external stakeholders.	A, B, C, D			
2	Metrics team members agree on the most important challenges facing the team.	B, C, D	A		
3	The metrics team has developed effective strategies to overcome obstacles and achieve its goals.		A, B, C, D		
4	The metrics team regularly reviews progress against team goals and plans.	C	A, D		B
5	The metrics team has the right combination of skills and experience.	B	A, C, D		
6	Metrics team members are all effective team players.	B, C, D	A		
7	Metrics team meetings are effective and efficient.	B, C, D	A		
8	Metrics team members communicate openly and directly with each other and gossiping is rare.	A, C, D	B		
9	Metrics team members are held accountable for their attitudes, behaviors, and deliverables.	A, C, D	B		
10	The metrics team routinely reviews how to work together more effectively.	B, C, D	A		
11	Metrics team members understand how their actions contribute to the team’s overall success.	B, C	A, D		
12	Metrics team members faithfully adhere to team decisions and rules.	B, C, D		A	
13	The metrics team has the necessary level of political sponsorship to be successful	A, D	B, C		
14	The metrics team is empowered to make key decisions.	A, C, D	B		
15	The metrics team proactively re-negotiates deliverables and/or backlog when faced with resource shortfalls.	B, C	A		D
16	Metrics team members share a high degree of trust and collaboration.	B, C, D	A		
17	Metrics team members feel safe challenging each other.	A, B, C, D			
18	The metrics team has lively debates and even the most difficult issues are raised and handled respectfully.	A, B, C, D			
19	The metrics team constructively resolves disagreements.	A, B, C, D			
5.4. Cross-company comparison
A summary of all companies can also be depicted in the same diagram, thereby emphasizing aspects related to teams and organizations. This emphasis is important to highlight where the largest improvement potential is — within the team, within the organization, or within both. A summary of the results from all teams is presented in Fig. 3, with the maturity of the team and that of its organization shown on -axis and -axis, respectively. To indicate the current compliance status and the desired position of each team, the team is shown as two data points based only on “Fully Compliant” and “Partially Compliant” items.

Interestingly, none of the teams aimed for full compliance. Even the most mature team, Company C, had one item “Missing” (international standardization work) and one “Not Relevant” (providing career opportunities).


Download : Download high-res image (202KB)
Download : Download full-size image
Fig. 3. Metrics teams comparison chart. Each team is shown as two data points — a solid mark and an empty mark. Solid marks include only full compliance aspects, whereas empty marks show both the fully and partially compliant aspects. The lines show the direction in which teams want to evolve (established during the workshops).


Download : Download high-res image (250KB)
Download : Download full-size image
Fig. 4. Radar diagram showing “Fully Compliant” results for all four companies.

The development trajectory for each team/organization was over the diagonal showing that development of maturity is symbiotic — both the team and the organization plan to mature together. During the workshops, we observed that the teams began putting requirements onto their organizations and vice-versa. The teams required that the results of their work (measurement information products) were used by the organization, and organizations required formalization of the tasks or information quality management performed by the teams.

A better understanding of each company and its development can be achieved by visualizing the company using a radar diagram. For each of the six areas, we used the percentage of the “Fully Compliant” aspects, and then both “Fully Compliant” and “Partially Compliant” (on a separate diagram).

The percentage of statements with which companies were “Fully Compliant” is presented in Fig. 4. The diagram shows all companies scored very high in the area of Team Maturity. The result was intuitive for teams A and C, which are formal metrics teams. For teams B and D, however, this result highlighted sense of community and the need to become formal teams, as these were virtual teams during our study. This formality was clear in the area of Set-up, where the formal teams scored higher than the up-and-coming teams.

The diagram shows that most of the teams struggled with Impact. Only Team C scored high in this area, which correlated with the fact it was the team active the longest. It also means that Team C could provide valuable learnings for other teams about the most useful indicators, measures and KPIs, and those which should be avoided.

All teams were working actively to improve both their respective companies’ market positions and themselves. This could be observed when we analyzed areas that the teams answered they partially fulfilled. The percentage of statements with which the companies were “Fully Complaint” and “Partially Compliant” is presented in Fig. 5.

The diagram shows that each team was actively working on the Impact area. Each team strove to provide value to its company and ensure that its company was data-driven. Being data-driven provides many benefits to companies, but requires high-quality data to be used as input. Thus, the value and importance of metrics teams becomes apparent here and drives their activity in this area.

Since teams A, B, and D were relatively young compared to Team C, they focused on ensuring their longevity by actively working on their attractiveness as a workplace and the perception their organization had of them — here again, the value created by metrics teams is important.

An area where the teams put much effort into was efficiency. Their constant strive for efficiency was the basis for their pursuit of improving aspects, such as shortening the lead-time, or using interoperable tools to ensure an exchange of experience.


Download : Download high-res image (314KB)
Download : Download full-size image
Fig. 5. Radar diagram showing “Fully Compliant” and “Partially Compliant” results for all four companies.

Notably, there was a difference in the areas of Team Maturity and Impact. The Team Maturity area seemed to be fulfilled to a large extent by all teams, whereas the Impact area was fulfilled to a lesser degree. This demonstrates the value of our model for metrics teams — MeTeaM can distinguish between teams with high social competence (Team Maturity) and those that can provide value (Impact). Such information can help teams to increase their contribution to their companies. Moreover, this shows that by being domain specific, MeTeaM leads to improved metrics teams’ results.

6. Validity evaluation
Construct validity reflects the set-up of a research project (Staron, 2019) and how we operationalized the concepts of maturity into the MeTeaM model. When constructing our model, we used a number of measures to capture state-of-the-art assessments and prevent bias of interviewees in any direction when applying the model; for example, we repeated the “what and how” at the assessment occasion before starting the questions. To reduce the researcher bias, the Metrics Team Leader of Company A performed the interview at Company C without the presence of the Team Leader of Company C since the latter was one of the main drivers of this research project. To minimize the risks of bias in design of the MeTeaM model, we constructed our model based on: research projects performed together with software-intensive development companies (e.g., the study of Meding and Staron (2020)); related standards, including ISO/IEC standards (ISO/IEC/IEEE, 2017, ISO/IEC, 2016); and theories, such as that of Abran (2010).

Internal validity addresses the risks to the validity of actions taken during the execution of a research project. To address this, we took a number of actions. For example, we kept the metrics teams updated on progress and limited the study to four months to avoid history and maturation threats (Staron, 2019). During the assessment, interviewees were not stressed (time-wise) to answer, and we ensured that everyone gave his/her opinion to avoid more active team members from biasing the answers of less active ones.

We must also address conclusion validity (Staron, 2019) to be confident in our conclusions. We did so by sharing our conclusions both directly after the assessments and later when presenting the results to larger audiences at each company and receiving their approval. Moreover, all participants acknowledged that our model captured the characteristics of mature metrics teams, suggesting confidence.

Finally, external validity addresses how extensively the research project results can be translated across all software development industrial contexts (Staron, 2019). To address this, we ensured that our research project addressed actual, real-life needs experienced by software development companies, and also expanded our initial project set-up by studying four rather than two companies of varying size and scope.

7. Conclusions and future work
Here we have addressed the question of how to assess the maturity of a metrics team in a modern organization. We conducted an action research project with four different organizations, where we designed and evaluated a metrics team maturity model, so-called MeTeaM. Our intervention in this action research study was the assessment of the metrics teams in these organizations and providing suggestions for their improvement.

A well-functioning, competent and active metrics team is a significant determinant of success of measurement programs in organizations. Since the metrics team designs, deploys and governs the measurement program, its host organization must understand how to optimally support the team. This symbiotic relationship is captured in the MeTeaM model, and emphasizes the fact that both team and organization must mature together.

In contrast to existing models for assessing team maturity, the MeTeaM model is domain specific, i.e., it is designed specifically to focus on the work of a metrics team. MeTeaM provides, therefore, actionable results that can used by metrics teams. By distinguishing between the focus of an organization and its metrics team, the model helps to place the improvements in the right location — either the team or its host organization must act on them. Such ambidexterity also provides metrics teams with the insight that they must act together with their host organizations, for example, to elevate their competence, increase the impact of their work, and better understand the needs of their host organizations.

The application of MeTeaM in the organizations studied led to a number of improvement initiatives. The up-and-coming teams turned their focus to identifying stakeholders, ensuring information quality, and satisfying their information needs. The more mature teams identified the need to work with impact and visibility as the outcome of our assessments, whereas the up-and-coming teams focused on their ability to communicate with stakeholders. Our results reveal new research directions in the field of evolution of metrics teams. By quantifying the maturity of a metrics team, we can observe both its evolution and that of the importance of each area assessed. By adding a domain-specific perspective to the metrics team assessment, we open the discussion on the importance of a domain for assessing the maturity of a team.