Abstract
Context:
Code Review (CR) is the cornerstone for software quality assurance and a crucial practice for software development. As CR research matures, it can be difficult to keep track of the best practices and state-of-the-art in methodology, dataset, and metric.

Objective:
This paper investigates the potential of benchmarking by collecting methodology, dataset, and metric of CR studies.

Methods:
A systematic mapping study was conducted. A total of 112 studies from 19,847 papers published in high-impact venues between the years 2011 and 2019 were selected and analyzed.

Results:
First, we find that empirical evaluation is the most common methodology (65% of papers), with solution and experience being the least common methodology. Second, we highlight 50% of papers that use the quantitative method or mixed-method have the potential for replicability. Third, we identify 457 metrics that are grouped into sixteen core metric sets, applied to nine Software Engineering topics, showing different research topics tend to use specific metric sets.

Conclusion:
We conclude that at this stage, we cannot benchmark CR studies. Nevertheless, a common benchmark will facilitate new researchers, including experts from other fields, to innovate new techniques and build on top of already established methodologies. A full replication is available at https://naist-se.github.io/code-review/.

Previous
Next 
Keywords
Code Review

Mining Software Repositories

Mapping study

1. Introduction
Code Review (CR) has always been the cornerstone for software quality assurance and is a crucial practice for software development teams. CR not only has the benefits of finding defects, but assists with other activities such as knowledge transfer and team awareness within software teams (Bacchelli and Bird, 2013). Microsoft reveals how “CRs at Microsoft are an integral part of the development process that thousands of engineers perceive it as a great best practice and most high-performing teams spend a lot of time doing”.1 The rise of contemporary review tools has brought the availability of data, with the review process now being light-weight. Contemporary tool-based reviews (such as Gerrit,2 Codestriker,3 and ReviewBoard4) are widely used in both open source and proprietary software projects. As CR research increases, so does the diversity of research methodologies, datasets, and metrics also increases, making it difficult to keep track of best practices.

This paper collects methodology, dataset, and metric for CR studies, with the end-goal to investigate the potential of benchmarking. Other fields, such as bio-medicine,5 use benchmarking studies to address the issue of: ‘as increasing numbers of methods are published in certain fields, it can be difficult to keep track of best practices for their use’. Large-scale studies that benchmark these methodologies on a wide range of datasets can be extremely useful to the scientific community. In this regard, we conduct a systematic mapping study, executing the guidelines provided by Petersen et al. (2008). The scope of our systematic study revolves around three research questions: to uncover (RQ1) the state of contributions and methodologies for research, (RQ2) replicability of existing research, and (RQ3) the metrics used in CR research.

Through a collection of 19,847 papers from the high-impact SE venues, we generate visual maps for the 112 collected papers including 80 conferences and 32 journals. For RQ1, we find that evaluation is the most common methodology (i.e., 73 papers), targeting particularly socio-technical and the understanding aspects of the CR process. However, there is a lack of papers that report the experience and propose solutions to deal with CR problems (i.e., four papers and thirteen papers, respectively). For RQ2, our results show that CR research not only relies on the data sources from the CR process but also largely uses the data sources from the software development process (i.e., issue tracking system and GitHub). We observe that 50% of researches provide replicable datasets, i.e., 42 papers out of 84 papers that use quantitative or mixed-method. For RQ3, we grouped 457 metrics that are used in the quantitative research into sixteen core sets (i.e., experience, code, ownership, comment, file, participant, temporal, revision, description, module, defect, queue, workload, decision, language, log, and others) and classified nine research topics (i.e., Quality Assurance, Review Process Prediction, Acceptance Predication, Review Process, Review Participation, Review Process Prediction, Review Process Comments, CI & Review, Test & Review and Technical/Non-technical & Review). We observe that the SE topic of quality assurance is more likely to use metrics to conduct the research, with thirteen papers being studied. In addition, our mapping shows that experience and code metric sets are the two most frequent metrics used in the quantitative study. From the mapping between metric sets and research topics, we find that different research topics tend to use particular metric sets.

Upon further mapping between the common datasets and the metrics, we conclude that at this stage, a benchmark for CR studies is not mature but has a much-needed potential. The map shows that papers prefer to construct their own metrics and datasets. To promote the common dataset usage, we encourage the future researches to strive for a replicable dataset. With the rise of machine learning and AI techniques, CR researchers will soon need to evaluate performance accurately against a state-of-the-art benchmark. We envision that such a benchmark will facilitate new researchers, including experts from other fields, to propose new techniques and build on top of already established methodologies.

To highlight the novelty of our mapping study, we did a comparative analysis of existing systematic reviews in CR, following the protocol provided by Petersen et al. (2008). Three secondary studies (Kollanus and Koskinen, 2009, Badampudi et al., 2019, Coelho et al., 2019) are identified before or in 2019. Kollanus and Koskinen (2009) conducted a mapping study on the software inspections, which is outdated, not targeting the tool-based reviews. Coelho et al. (2019) focused on the specific theme of tool-based code review, i.e., refactoring-awareness. Badampudi et al. (2019) did a preliminary study on understanding the research topic evolution of the tool-based review field. Although these two systematic studies are highly relevant to the field, our study first gives a visual summary of the potential of the benchmark with in-depth analysis. Specifically, we only study those papers that are published in the premium venues, as we believe that high-quality papers are deemed to form a best representative view for future researches. Furthermore, the outcome of this mapping study is a listing of methodologies and contributions, 42 available datasets, and 457 metrics.

The remainder of this paper is organized as follows. Section 2 presents the systematic mapping process, including research questions, search conduction, screening process, classification schemes, and data extraction. Section 3 shows the results of the systematic mapping study. Section 4 describes the comparative analysis of existing systematic reviews relevant to CR. Section 5 discloses the challenges for the benchmark of dataset and metric. Section 6 explains the threats to the validity of the research. Finally, we summarize this paper in Section 7.

2. The systematic mapping process
Our process is based on the work of Petersen et al. (2008) and similar to the systematic mapping study performed by Abelein and Paech (2016). Essentially, our process steps of the systematic mapping study are the definition of the research questions, search conduction of papers, screening process, keywording for the mapping, and data extraction.

2.1. Research questions
To define the scope of the mapping study, we formulate the following research questions:

1.
(RQ1): What contributions and methodologies does CR research target? The motivation for the first research question is to understand the current focus of research. Based on the work of Bacchelli and Bird (2013), we would like to map out the outcomes, expectations, and contributions that the most impactful CR research tackles from the point of view of both a practitioner and researcher.

2.
(RQ2): How much CR research has the potential for replicability? The motivation for the second research question is to understand how the data source impacts CR research. Understanding the sources can provide insight into the current state and gaps in terms of the data collection and availability. Furthermore, there has been growing initiatives to make data open and replicability which is encouraged in the community.6

3.
(RQ3): What metrics and topics are used with CR studies? The motivation for the third research question is to uncover what kinds of metrics are used in CR empirical studies. Understanding how the metrics are used and the associated topics can motivate the potential to benchmark CR studies.


Download : Download high-res image (106KB)
Download : Download full-size image
Fig. 1. Defined terms used in the search strings.


Table 1. Corpus of venues (conferences and journals) studied in this paper. Note that ICSM now is called ICSME; and WCRE and CSMR are fused into SANER. In addition, the GPCE h5-index is not retrieved but is ranked as B.

Journal	Name	Impact factor	Established
TSE	IEEE Transactions on Software Engineering	6.112	1991
ESE	Empirical Software Engineering	3.156	1996
IST	Information and Software Technology	2.726	1992
S/W	IEEE Software	2.589	1991
TOSEM	Transactions on Software Engineering and Methodology	2.516	1992
JSS	The Journal of Systems and Software	2.450	1991
REJ	Requirements Engineering Journal	1.933	1996
SOSYM	Software and System Modeling	1.876	2002
ASEJ	Automated Software Engineering Journal	1.857	1994
SPE	Software: Practice and Experience	1.786	1991
SQJ	Software Quality Journal	1.460	1995
STVR	Software Testing, Verification and Reliability	1.226	1992
SMR	Journal of Software: Evolution and Process	1.178	1991
ISSE	Innovations in Systems and Software Engineering	0.950	2005
IJSEKE	International Journal of Software Engineering and Knowledge Engineering	0.886	1991
NOTES	ACM SIGSOFT Software Engineering Notes	0.490	1999
Conference	Name	h5-index	Established
ICSE	International Conference on Software Engineering	75	1994
FSE	ACM SIGSOFT Symposium on the Foundations of Software Engineering	51	1993
ASE	IEEE/ACM International Conference on Automated Software Engineering	40	1994
MSR	Working Conference on Mining Software Repositories	38	2004
ISSTA	International Symposium on Software Testing and Analysis	35	1989
ICSM	IEEE International Conference on Software Maintenance	33	1994
ICPC	IEEE International Conference on Program Comprehension	33	1997
SANER	IEEE International Conference on Software Analysis, Evolution and Re-engineering	30	2014
ICST	IEEE International Conference on Software Testing, Verification and Validation	27	2008
RE	IEEE International Requirements Engineering Conference	25	1993
CSMR	European Conference on Software Maintenance and Re-engineering	25	1997
WCRE	Working Conference on Reverse Engineering	22	1995
MDLS	International Conference On Model Driven Engineering Languages And Systems	21	2005
ESEM	International Symposium on Empirical Software Engineering and Measurement	20	2007
FASE	International Conference on Fundamental Approaches to Software Engineering	18	1998
SSBSE	International Symposium on Search Based Software Engineering	15	2011
SCAM	International Working Conference on Source Code Analysis & Manipulation	15	2001
GPCE	Generative Programming and Component Engineering		2000
2.2. Conduct search
We use the following strict characteristics as recommended by Kitchenham (2007) to formulate our search string: (C1) a defined search strategy, (C2) a defined search string, based on a list of synonyms combined by ANDs and ORs, (C3) a broad collection of search sources, (C4) strict documentation of the search, (C5) paper selection should be checked by at least two researchers. Fig. 1 shows the defined search string. For Term 1 in the search string, as shown in Fig. 1, We apply commonly used terminologies in CR to search, such as code inspection, code review, peer review, peer inspection, or tools such as pull requests or pull-based. To enlarge the paper dataset, other terminologies such as patch, code changes, and reviewing are also considered as appropriate for CR research. For Term 2 in the search string, we do not include the CR related papers that are in the form of the systematic review. Nevertheless, we separately conduct a comparative analysis of existing systematic reviews in Section 4.

Based on RQ1, to ensure papers of high quality and understand the state-of-the-art in the field, we specifically searched for papers published in the high-impact journals and conferences from the software engineering domain between 2011 and 2019. Inspired by the work of Badampudi et al. (2019), we select the 2011 time-frame as our starting point, since Badampudi et al. identified a steady upward trend regarding publications related to the contemporary tool-based review starting in 2011. In addition, several well-known open-source projects (e.g., OpenStack and Qt projects) use the Gerrit platform since 2011. Table 1 shows the summary of paper collection source. Regarding the publication venue selection, similar to the mapping study conducted by Mathew et al. (2018), papers were extracted from 18 conferences (i.e., International Conference on Software Engineering) with relatively high h5-index and 16 journals with high impact factors. h5 is the h-index for articles published in a period of 5 complete years obtained from Google Scholar. We rely on Guide2Research7 to retrieve the h5-index. Although Generative Programming and Component Engineering (GPCE) is not recorded with the h5-index, it is ranked as B according to core ranking.8 The Impact Factor (IF) is an index (numerical value) to evaluate how much impact a journal (scientific journals) has, based on Clarivate Analytics.9 The conferences with higher h5-index or the journals with higher impact factors are deemed to carry more intrinsic prestige in their respective fields. To reduce its selection bias, we selected from a wide range of digital resources to follow (C3) a broad collection of search sources: ACM Digital Library, IEEE Xplore, Science Direct, and SpringerLink databases. For example, the data from 2012 to 2019 for Mining Software Repositories Conference is collected through IEEE Xplore, but the data from 2011 is available in ACM Digital Library. We extracted 19,847 papers from the above four search sources that were published in the last nine years (i.e., 20112019), as shown in Table 2.

Assessing the quality of primary papers can be used as an additional criterion for the exclusion (Kitchenham, 2004). As part of our quality assessment, we exclusively consider papers from these premium venues as we assume they are of high quality and widely get recognized within the SE domain. Additionally, to ensure only technical contributions, in the further data processing, we filter out short papers, editorials, tutorials, panels, poster sessions and prefaces, and opinions (8 pages or less). Nonetheless, internal and conclusion threats may exist, and we further discuss the threats with regard to this assessment in Section 6. After the conduct search, we were able to get 437 initial papers, as shown in Table 2.

2.3. Screening process
Our screening process is comprised of inclusion and exclusion criteria. For this manual exclusion, the following inclusion and exclusion criteria were applied to the abstract of each paper. Inclusion criteria: Three inclusion criteria are defined, namely, (IC1): paper should focus on topics on code inspections, code review, code review tools, pull request, (IC2): the paper is peer reviewed, (IC3): the paper is written in English and the paper has full text available. Exclusion criteria: Four exclusion criteria were defined that cover the datasets, purposes and the evaluation of the studies. The following papers were excluded that met these criteria: (EC1): the paper does not mention any CR activities, (EC2): the paper focuses on other software development process, e.g., issue tracking process, continuous integration, testing, (EC3): the paper is out of scope with focusing on other sub-fields such as program analysis, code clone, defect prediction, refactoring, social technique, (EC4): the paper is outsides our studied time-frame.

To reduce bias and follow (C5), this manual paper selection was conducted by the first and the second authors. As a result of the screening process, we were able to collect 112 papers out of 437 initial papers, which include 80 premium conference papers and 32 high-impact journal papers, as shown in Table 2. Fig. 2(a) depicts the distribution of these 112 papers based on conferences and journals during our studied time frame. In detail, the figure shows that the CR research publications keep an upward trend in the recent three years, i.e., fourteen, sixteen, and twenty papers are published in 2017, 2018, and 2019. We also observed that papers submitted to journals are on the upward trend from 2015, i.e., eight papers were submitted to journals in 2019.


Table 2. Statistics of the filtering of the papers during the conduct search and screening process.

# of papers	
Conduct search			
Search string result	19,847	
All papers			437
Screening of papers			
Conference paper	 80	
Journal paper	 32	
Total papers			112
To further explore the trend of the research type, we manually classified the types of research papers (e.g., quantitative and qualitative) according to the work of Bernard (2011). We classify the research types into four categories: (i) Quantitative only, (ii) Quantitative only, (iii) Mixed-Method, and (iv) Survey only. The Mixed-Method refers to those papers using the combination of quantitative method and qualitative/survey method. For the Survey only type, it not only includes survey but also includes interview and user/control studies. We classify papers which do not fit the above types into others. We classify research paper types with two rounds. First, two authors classified them in the first round. In the second round, the third author full with research experience joined to validate each collected paper. Fig. 2(b) shows the research type distribution of 112 papers during our studied time frame. We observe that the Mixed-Method papers become popular in the recent time, i.e., eleven Mixed-method papers are published in 2019.


Download : Download high-res image (233KB)
Download : Download full-size image
Fig. 2. The distribution of paper publication and their research types yearly from 2011 to 2019. CR papers keep an upward trend and the journal becomes a popular choice for publication. Mixed-Method papers becomes popular in the recent time.


Table 3. Summary of the classification scheme used to identify contribution, methodology, replication, and metric.

Class	Sub-class	Category	Description
Contributions	Practitioner	Communication (Bacchelli and Bird, 2013)	The developers are provided with the need of richer communication than comments annotating the changed code when reviewing. Teams should provide mechanisms for in-person or, at least, synchronous communication.
Potential benefit (Bacchelli and Bird, 2013)	Modern CR provides benefits beyond finding defects. CR can be used to improve code style, find alternative solutions, increase learning, share code ownership, etc. This should guide CR policies.
Quality assurance (Bacchelli and Bird, 2013)	CR does not result in identifying defects as often as project members would like and even more rarely detects deep, subtle, or “macro” level issues.
Understanding (Bacchelli and Bird, 2013)	When reviewers have prior knowledge of the context and the code, they complete reviews more quickly and provide more valuable feedback to author.
Researcher	Automation (Bacchelli and Bird, 2013)	Tools for enforcing team code conventions, checking for typos, and identifying dead code already exist. Even more advanced tasks such as checking boundary conditions or catching common mistakes have been shown to work in practice on real code. Automating these tasks frees reviewers to look for deeper, more subtle defects.
Program comprehension (Bacchelli and Bird, 2013)	Context and change understanding are challenges that developers face when reviewing, with a direct relationship to the quality of review comments.
Socio-technical effect (Bacchelli and Bird, 2013)	These are studies that involves the consideration of both human and technical aspects. In terms of CR, Studies can be designed and carried out to determine if and how team collaboration, coordination, awareness and learning occurs.
Methodologies	–	Validation research (Wieringa et al., 2005)	Techniques investigated are novel and have not yet been implemented in practice. Techniques used are for example experiments, i.e, work done in lab
Evaluation research (Wieringa et al., 2005)	Techniques are implemented in practice and an evolution of the technique is conducted.That means, it is shown how the technique is implemented in practice (solution implementation) and what are the consequences of the implementation in terms of benefits and drawbacks (implementation evaluation). This also includes to identify problems in industry.
Solution proposal (Wieringa et al., 2005)	A solution for a problem is proposed, the solution can be either novel or a significant extension of an existing technique. The potential benefits and the applicability of the solution is shown by small example or a good line of argumentation.
Experience paper (Wieringa et al., 2005)	Experience papers explain on what and how something has been done in practice. It has to be the personal experience of the author
Survey paper	These papers are qualitative studies that use a questionnaire or interviews to evaluate some phenomena
Replication	–	Private datasets	Neither dataset nor the source code is available. The study may not be replicated.
Partial datasets	Part of the dataset is available. The study could not be replicated fully with partial datasets.
Public datasets	Replication including either full dataset or the source code is provided via hyperlinks or paper references. The study is deemed to be replicable using provided datasets.
Metric	–	Metric sets used in empirical studies	Metrics that are used in CR research can be classified according to the level of three aspects: product, process, and people.
2.4. Keywording of relevant papers
Inspired by Petersen et al. (2008), we classified each paper based on the scope outlined in each research question with results shown in Table 3. During the classification, it not only includes the detailed reading of the abstract, but sometimes requires a careful reading of the whole paper itself.

Contributions and Methodologies (RQ1). To classify research contributions of the papers, we base our work on the work of Bacchelli and Bird (2013). They classify contributions for two objectives (i.e., contributions to benefit practitioner and researcher). For the classification process, three co-authors sat in a round-table and labeled each contribution based on seven category features shown in Table 3. The process was to first read the abstract and decide the classification. If there was a dispute, then the paper was quickly analyzed and a discussion of the paper started between the co-authors before the consensus reached.

To classify methodologies that were applied to the studies, we used existing definitions of research facets (Petersen et al., 2008). For the classification, three co-authors sat in a round-table and labeled each methodology based on the category features. The first keywords relating to the methodology were searched and discussed. Similar to the keywording of contributions, the full contents of the paper were consulted if a dispute arose among the co-authors.

Replication (RQ2). To classify the replicability of papers, we identified the source of the data, whether the dataset is either available via the link or is referred to a prior dataset. Our scope is limited to the quantitative and mixed-method papers. Since detailed information of the dataset is not likely to be in the abstracts, co-authors were required to scan the papers to extract any online links of a dataset or a reference to an existing dataset. Furthermore, as shown in Table 3, authors classified the papers according to the nature of the studied systems (i.e., open source projects or industry). Note that the classification is non-exclusive as some studies involved projects that were both open and closed data.

Metrics (RQ3). To group the metrics used in collected papers, we only scan the papers conducted in quantitative method and mixed-method. For the classification, we do the following two: (1) metrics mapping research aspects and (2) metrics mapping research topics. For the first classification, we pick up all metric description tables from papers. We then apply open card sorting to construct a taxonomy of codes of the metrics. In detail, following the metric descriptions, the coded metrics are merged into cohesive groups that can be represented by a similar high-level code, i.e., metric sets. In the card sorting process, three authors sit together and sort metrics until all achieve the consensus. Based on prior work (Yang et al., 2016), the aspects of CR research can be divided into three targets: product, process, and people. Following these aspects, we then classify the constructed high-level metric sets using ticks. For the second classification, the first author classified initial research topic groups by reading the abstracts and introductions. After that, another experienced author did the validation to assure the constructed topic groups that were distinguished.

2.5. Data extraction and mapping of studies
Using the classification scheme, we then utilize visual mappings of the results to highlight states in the collected papers. To identify which categories have been emphasized in past research and show possible opportunities for future work, we use three plot types to show maps (i) tables, (ii) bar plots, and (iii) bubble maps. Once the scheme is in place, we used excel spreadsheets to store the data and applied R scripts to extract and categorize the papers. Furthermore, we put rationales to decide why we believe each paper is categorized. Below are the visual techniques and rationale for answering each RQ:

Visual Map of RQ1. To answer RQ1, we show a visual mapping of the contributions (with the researchers and practitioners separately) against the methodologies. We intend to find out how the methodologies influence the contributions and what is the popular combination of contributions and methodologies. A bubble map will be used to show results. The map should show what contributions are saturated and which perceived contributions have the potential for future work. We will also pick up examples of each classified paper for an in-depth discussion of the maps.

Visual Map of RQ2. To answer RQ2, we show a visual mapping of the replicability of the collected papers. We intend to determine how much CR research has the potential to be replicated. A bar chart will be used to visualize the main results. The map should show the proportion of how many papers can be replicated and show what forms are used to provide replication (i.e., via links or reference to the dataset). For a deeper understanding of the data sources, we perform additional sub-classification of the source: (i) research that extracts data from pure code review tools (e.g., Gerrit tools in OSS and special review systems or tools in industry such as CodeFlow tool in Microsoft), (ii) research that extracts data that not only contains CR, but expands on other software development tools such as mailing lists, version control system, GitHub, and issue tracking system, (iii) research that extracts data from observational experiments in the form of interviews, survey, and control study. Additionally, we classify the platforms where the available datasets are provided into four types: (i) online storage, i.e., dropbox, (ii) permanent storage, i.e., zenodo, (iii) GitHub /BitBucket, and (iv) personal or university.

Visual Map of RQ3. To answer RQ3, we show a visual mapping of the metric benchmark in terms of research aspects (i.e., product, process, and human) and research topics. We intend to formulate a systematic metric group for a future research guide and understand in which topic what kinds of metrics should be included. Two tables will be drawn to present our benchmark details. The first table map should show (1) how many different metrics are used with their frequency in papers and (2) what research aspects do these metrics target. The second table map should show different combinations of metrics are used in different research topics.

3. Results: Maps of CR research
The results will answer the research questions, with the visual maps of the categories of the papers.

3.1. (RQ1): What contributions and methodologies does CR research target?
Fig. 3 shows both the saturation of papers as well as the potential research opportunities for the field. Note that a paper can target multiple contributions, using more than one methodology. The figure clearly shows that evaluation is the most popular methodology, benefiting both the practitioners and researchers. For practitioners, most of the papers have contributions to potential benefits and understanding aspects. Potential benefits mean that modern CR provides benefits beyond the fundamental need to find defects. CR is demonstrated to be useful for other tasks. We introduce three examples in detail below. For the task of improving code style, Zhang et al. (2015) presented an interactive approach named CRITICS for inspecting systematic changes and the results show that it should improve developer productivity during this process. For the task of increasing the learning, Gousios et al. (2015) conducted a large-scale survey to investigate work practices and challenges in pull-based development model and results show that integrator should consider several factors in their decision making. For the task of review comments usefulness, Rahman et al. (2017) found that useful comments share more vocabulary with the changed code, contain salient items like relevant code elements, and their reviewers are generally more experienced. For instance, exploring how CR is conducted can be used for practitioners to better implement review activity and improve the review quality. Understanding is when reviewers have prior knowledge of the context and the code, they complete reviews more quickly and provide more valuable feedback to the author. Key examples are researches that look into the quality of review and types of defects. Kononenko et al. (2016) provided a deep insight into how developers define review quality, what factors contribute to how they evaluate submitted code, and what challenges they face when performing review tasks. Beller et al. (2014) conducted a manual research to increase the understanding of practical benefits that the MCR process produces on reviewed source code. Their results show that types of changes due to the MCR process in OSS are strikingly similar to those in the industry and academic systems.

On the other hand, researcher-oriented CR studies mostly focus on socio-technical contributions. Socio-technical related papers are studies that involve the consideration of both human and technical aspects. One popular topic is reviewers recommendation and many studies have been done on this topic. Xia et al. (2015) put textual information and file location analyses together to recommend reviewers more accurately. Hannebauer et al. (2016) recommended code reviewers based on their expertise. Apart from reviewer recommendation topic, many other human related researches have been conducted such as review participation (Thongtanunam et al., 2017), evaluation of contributions (Tsay et al., 2014b) and broadcast during CR process (Rigby and Storey, 2011). In terms of CR, studies can be designed and carried out to determine if and how team collaboration, coordination, awareness and learning occurs. In terms of opportunities, Fig. 3 highlights the lack of experience papers. This is crucial and shows a lack of reporting and feedback from developers. Instead, we see that there are a stable number of survey papers. Other notable potential methodologies are the experience and solution, which indicates that more practical tools need to be developed to help practitioners in reality.

Table 4 shows a listing of top five paper contributions with the methodologies used, illustrating how evaluation studies are dominant. According to our results, two most popular combinations are the evaluation study that has a understanding contribution (e.g., 53 papers), then followed by the study with socio-technical effect target following the evaluation methodology (e.g., 48 papers). For the understanding target with evaluation methodology, we introduce two representative studies. In the work of Tao et al. (2014), they investigate the patch rejected reasons with 300 samples and formulate the practical suggestions for patch author submission. This work helps patch authors to better understand what kinds of patch should be submitted so as to reduce the rejection chance. Another work conducted by Zampetti et al. (2017) empirically analyze how developers document pull requests with external references using mixed-method. Their results indicate that for developers, external resources are useful to learn something new or to solve specific problems. One example of the evaluation study with a social–technical effect is Thongtanunam et al. (2015), which investigates CR practices in defective files combined with human factors (e.g., the participation of the reviewer in the process). In detail, authors evaluate the results using a detailed empirical study of the Gerrit review system within the Qt project. Similarly, Kononenko et al. (2015) reported on a case study investigating CR quality for Mozilla and explore the relationships between the reviewers’ code inspections and a set of factors, both personal and social. It is interesting to note that 36 out of 53 papers, i.e., around 68%, that contribute to better understanding also have socio-technical contributions. For example, Thongtanunam et al. (2017) studied what factors influence review participation in the CR process which in turn helps practitioners understand the situation when they tend to join. While within the validation methodology, we find the most popular combination is papers that target contributions of potential benefits with such methodology. The majority of the validation papers are based on recommendation or prediction models. An example of this type of paper is Rahman et al. (2016), where authors suggest an approach of reviewer recommendation based on cross-project and technology experience.

Apart from the referred papers using two popular methodologies that are listed in Table 4, we also provide the complete paper list for those papers using survey, solution, and experience methodology. Among these three methodologies, survey is relatively frequent with eighteen papers being retrieved (Sadowski et al., 2018, German et al., 2018, Gousios et al., 2015, Krusche et al., 2016, Kononenko et al., 2016, Gousios et al., 2016, Bacchelli and Bird, 2013, Bird et al., 2015, Baum et al., 2017, Baum et al., 2016, Tao et al., 2012, Ram et al., 2018, Armstrong et al., 2017, Zampetti et al., 2019, Ebert et al., 2019, Bosu et al., 2017, MacLeod et al., 2018, Bosu and Carver, 2013). Thirteen papers are classified as solution (Shimagaki et al., 2016, Bosu et al., 2015, Rahman et al., 2016, Zhang et al., 2015, Barnett et al., 2015, Allamanis et al., 2014, Menarini et al., 2017, Wen et al., 2018, Tymchuk et al., 2015, Wang et al., 2019, Liu et al., 2019, Hanam et al., 2019, Markovtsev et al., 2019). The last experience methodology is the most rare case, only four papers being found (Shimagaki et al., 2016, Bosu et al., 2015, Rigby et al., 2012, Shull et al., 2010).


Download : Download high-res image (185KB)
Download : Download full-size image

Table 4. Top 5 combination of contribution and methodology.

Contribution
Socio-technical effects	Understanding	Potential Benefits
Methodology	Evaluation	Zhu et al., 2016, Rigby et al., 2014, Rigby and Storey, 2011, Xia et al., 2015, Hellendoorn et al., 2015, Jiang et al., 2017a, McIntosh and Kamei, 2018, Kamei et al., 2013, Bosu et al., 2015, Mcintosh et al., 2016, Spadini et al., 2018, Beller et al., 2014, Shimagaki et al., 2016, Zhang et al., 2018, Thongtanunam et al., 2016, Thongtanunam et al., 2015, Yu et al., 2016, Rahman et al., 2017, Tsay et al., 2014a, Baysal et al., 2015, Jiang et al., 2017b, Kononenko et al., 2015, Kovalenko et al., 2018, Mirhosseini and Parnin, 2017, Fan et al., 2018, Tsay et al., 2014b, Rigby and Bird, 2013, Gousios et al., 2014, Thongtanunam et al., 2017, Wang et al., 2015, Kononenko et al., 2018, Jiang et al., 2013, Soares et al., 2018, McIntosh et al., 2014, Bosu and Carver, 2014, de Mello et al., 2017, Goswami et al., 2015, Baysal et al., 2013, Morales et al., 2015, Thongtanunam et al., 2015, Ebert et al., 2019, Paul et al., 2019, Asri et al., 2019, Ruangwan et al., 2018, Kavaler et al., 2019 and Hirao et al. (2019)	Zhu et al., 2016, Tao et al., 2014, Rigby et al., 2014, Rigby and Storey, 2011, Hellendoorn et al., 2015, Jiang et al., 2017a, McIntosh and Kamei, 2018, Bosu et al., 2015, Mcintosh et al., 2016, Spadini et al., 2018, Beller et al., 2014, Shimagaki et al., 2016, Gupta et al., 2014, Zhang et al., 2018, Thongtanunam et al., 2016, Thongtanunam et al., 2015, Yu et al., 2016, Tsay et al., 2014a, Baysal et al., 2015, Paixao et al., 2017, Walker et al., 2012, Kononenko et al., 2015, Bavota and Russo, 2015, Mirhosseini and Parnin, 2017, Bernardo et al., 2018, Rigby and Bird, 2013, Gousios et al., 2014, Thongtanunam et al., 2017, Wang et al., 2015, Kononenko et al., 2018, Jiang et al., 2013, Soares et al., 2018, Malavolta et al., 2018, McIntosh et al., 2014, Bosu and Carver, 2014, Zanaty et al., 2018, Baum et al., 2017, Goswami et al., 2015, de Lima Júnior et al., 2018, Baysal et al., 2013, Zampetti et al., 2017, Armstrong et al., 2017, Morales et al., 2015, Panichella et al., 2015, Zampetti et al., 2019, Ebert et al., 2019, Paul et al., 2019, Wang et al., 2019, Ruangwan et al., 2018, Zhao et al., 2019, Zou et al., 2019, Kavaler et al., 2019 and Hirao et al. (2019)	Jiang et al., 2017b, Rigby et al., 2014, Rigby and Bird, 2013, Xia et al., 2015, Tao and Kim, 2015, Gupta et al., 2014, Malavolta et al., 2018, Fan et al., 2018, Yu et al., 2016, Kamei et al., 2013, Rahman et al., 2017, Baysal et al., 2015, Mcintosh et al., 2016, Bosu et al., 2014, de Mello et al., 2017, Sharma and Sodhi, 2019, Tourani and Adams, 2016, di Biase et al., 2016, Paixao and Maia, 2019, Wang et al., 2019 and Maddila et al. (2019)
Validation	Rahman et al., 2016, Jiang et al., 2017b, Xia et al., 2015, McIntosh and Kamei, 2018, Thongtanunam et al., 2015, Thongtanunam et al., 2016, Ouni et al., 2016, Mirhosseini and Parnin, 2017, Yu et al., 2016, Fan et al., 2018, Rahman et al., 2017, Kamei et al., 2013, Mcintosh et al., 2016, Thongtanunam et al., 2017, McIntosh et al., 2014, Baysal et al., 2012 and Alami et al. (2019)		Rahman et al., 2016, Jiang et al., 2017b, Xia et al., 2015, Balachandran, 2013, Hannebauer et al., 2016, Menarini et al., 2017, Ouni et al., 2016, Zhang et al., 2015, Allamanis et al., 2014, Yu et al., 2016, Fan et al., 2018, Barnett et al., 2015, Rahman et al., 2017, Zanjani et al., 2016, Kamei et al., 2013, Mcintosh et al., 2016, Kollanus, 2011 and Baysal et al. (2012)
3.2. (RQ2): How much CR research has the potential for replicability?
In Table 5, we divide all premium papers into different classification according to the definition of data sources shown in Section 2.5. We describe each data source in detail below. In CR Process, for example, Mcintosh et al. (2016) conducted the research to see the impact of code reviews on software quality. They only focus on review process and extract the data from QT, VTK, ITK projects using review tools (e.g., Gerrit). For Software Development Process, for example, Kononenko et al. (2015) investigated whether people and participation matter the quality of review. In their research, they collected data from issue tracking system (e.g., Bugzilla) which belongs to the development process. In Interview/Survey/Control Study, for instance, Bosu et al. (2017) analyzed the process aspects and social dynamics of CR from the diverse surveys of Microsoft and other open source projects. In another example, Floyd et al. (2017) researched the representation of code in the brain with fMRI study. They involved 29 participants to carry out the controlled experiment and got result feedback. We find that code review process related dataset is the most extracted from the well-studied Gerrit tool. One advancement has been the release of the rest API, in which anyone is able to download and collect data on projects. As shown in Table 5, we summarize and draw the top 3 popular projects using Gerrit tools. We observe that for these CR papers, Qt project is the most studied project, with sixteen, fifteen, and twelve papers investigating Qt, OpenStack, and Android respectively.


Table 5. Data source classification, Top-3 studied projects applying Gerrit review tools, and platform distribution. Mixed-method papers can be classified into Inter./Sur./Control Study as well.

Relevant papers
CR process	Sadowski et al., 2018, Spadini et al., 2018, Thongtanunam et al., 2016, Shimagaki et al., 2016, Barnett et al., 2015, Balachandran, 2013, Rahman et al., 2017, Thongtanunam et al., 2015, Bosu et al., 2015, McIntosh et al., 2014, Beller et al., 2014, Ouni et al., 2016, Shimagaki et al., 2016, Xia et al., 2015, Bavota and Russo, 2015, Allamanis et al., 2014, Bosu et al., 2014, Rigby and Bird, 2013, Paixao et al., 2017, Hannebauer et al., 2016, Wen et al., 2018, Fan et al., 2018, Thongtanunam et al., 2017, Mcintosh et al., 2016, McIntosh and Kamei, 2018, Zanjani et al., 2016, Bosu and Carver, 2014, Soltanifar et al., 2016, Zanaty et al., 2018, Morales et al., 2015, Thongtanunam et al., 2015, Panichella et al., 2015, Paul et al., 2019, Tourani and Adams, 2016, Wang et al., 2019, Asri et al., 2019, Ruangwan et al., 2018 and Hirao et al. (2019)
Data source	Software Dev. Process	Kononenko et al., 2018, Rahman et al., 2016, Gousios et al., 2014, Tsay et al., 2014a, Gupta et al., 2014, Rigby and Storey, 2011, Hellendoorn et al., 2015, Tao and Kim, 2015, Shimagaki et al., 2016, Kononenko et al., 2015, Bavota and Russo, 2015, Tao et al., 2014, Zhu et al., 2016, Allamanis et al., 2014, Rigby and Bird, 2013, Walker et al., 2012, Menarini et al., 2017, Mirhosseini and Parnin, 2017, Hannebauer et al., 2016, Bernardo et al., 2018, Zhang et al., 2018, Malavolta et al., 2018, Rigby et al., 2014, Baysal et al., 2015, McIntosh and Kamei, 2018, Kamei et al., 2013, Jiang et al., 2017a, Jiang et al., 2017b, Yu et al., 2016, Wang et al., 2015, Soares et al., 2018, Jiang et al., 2019, de Lima Júnior et al., 2018, Baysal et al., 2012, Baysal et al., 2013, Zampetti et al., 2017, Armstrong et al., 2017, Zampetti et al., 2019, Paixao and Maia, 2019, Zhao et al., 2019, Zou et al., 2019, Kavaler et al., 2019, Wang et al., 2019, Liu et al., 2019, Hanam et al., 2019, Hirao et al., 2019 and Maddila et al. (2019)
Interview/Survey/Control study	Shimagaki et al., 2016, Barnett et al., 2015, Floyd et al., 2017, Zhang et al., 2015, Wilkerson et al., 2012, Tao et al., 2014, Kovalenko et al., 2018, Zampetti et al., 2019, Ruangwan et al., 2018, Zhao et al., 2019, Spadini et al., 2019, Wang et al., 2019, Liu et al., 2019, Hanam et al., 2019 and Maddila et al. (2019)
Qt	Spadini et al., 2018, Thongtanunam et al., 2016, Thongtanunam et al., 2015, Ouni et al., 2016, Xia et al., 2015, Thongtanunam et al., 2017, Mcintosh et al., 2016, McIntosh et al., 2014, McIntosh and Kamei, 2018, Hannebauer et al., 2016, Morales et al., 2015, Thongtanunam et al., 2015, Paul et al., 2019, Wang et al., 2019, Ruangwan et al., 2018 and Hirao et al. (2019)
Top-3 Gerrit projects	OpenStack	Spadini et al., 2018, Thongtanunam et al., 2016, Ouni et al., 2016, Xia et al., 2015, Fan et al., 2018, Thongtanunam et al., 2017, McIntosh and Kamei, 2018, Hannebauer et al., 2016, Zanaty et al., 2018, Thongtanunam et al., 2015, Tourani and Adams, 2016, Wang et al., 2019, Asri et al., 2019, Ruangwan et al., 2018 and Hirao et al. (2019)
Android	Ouni et al., 2016, Xia et al., 2015, Bosu et al., 2014, Thongtanunam et al., 2017, Zanjani et al., 2016, Bavota and Russo, 2015, Shimagaki et al., 2016, Thongtanunam et al., 2015, Paul et al., 2019, Asri et al., 2019, Ruangwan et al., 2018 and Hirao et al. (2019)
GitHub/Bit Bucket	Zhang et al., 2018, Zhang et al., 2015, Gupta et al., 2014, Thongtanunam et al., 2015, McIntosh et al., 2014, Shimagaki et al., 2016, Xia et al., 2015, Zanaty et al., 2018, Armstrong et al., 2017, Thongtanunam et al., 2015, Ebert et al., 2019, Thongtanunam et al., 2017, Baysal et al., 2015, Mcintosh et al., 2016, McIntosh and Kamei, 2018, Yu et al., 2016 and Ruangwan et al. (2018)
Online storage	Zampetti et al. (2019)
Dataset platform	Personal/University	Asri et al., 2019, Kononenko et al., 2018, Floyd et al., 2017, Rahman et al., 2017, Ouni et al., 2016, Kononenko et al., 2015, Bavota and Russo, 2015, Bosu et al., 2014, Paixao et al., 2017, Hannebauer et al., 2016, Bernardo et al., 2018, Baysal et al., 2013, Panichella et al., 2015, Tourani and Adams, 2016, Spadini et al., 2019, Hirao et al., 2019 and Kamei et al. (2013)
Permanent storage	Spadini et al., 2018, Paixao and Maia, 2019 and Liu et al. (2019)
Reference to existing dataset	Thongtanunam et al., 2016, Thongtanunam et al., 2015 and Rigby and Bird (2013)
Fig. 4 shows two important findings with the proportion of replicability. The first finding is that there are in total 38 papers (i.e., 9 papers with closed data, 23 papers open data, and 6 papers open data/closed data) out of 84 papers (around 45%) that do not provide any access to their datasets. Taking a closer look at the closed data, studies are usually conducted within industries, surveys and control studies. An example of this paper is Balachandran (2013), where the authors conducted research on how to reduce human efforts and improve review quality using the data from industry project named VMware. For papers that labeled as open data, the researchers collected data from open source projects but did not share a replication package. For instance, Mirhosseini and Parnin (2017) investigated whether or not pull requests encourage developers to upgrade out-of-date dependencies with the data from OSS (i.e., 7470 projects in GitHub). It could be argued that since the data is open source and available for anyone to download themselves. The second finding is that, as shown in Fig. 4, we observe that 42 papers out of 84 papers (around 50%) released a replication package, either referred to a published dataset or released their own dataset via an online link. For the work of Thongtanunam et al. (2016), authors referred to a dataset that was previously published (Hamasaki et al., 2013) to revisit code ownership and its relationship with software quality. Usually, papers release a link to the dataset. For instance, Baysal et al. (2015) shared the dataset link (e.g., WebKit and Blinkin projects). Upon a closer inspection on the dataset platforms of these replication package links, we found that GitHub/Bit Bucket and Personal/University are the most common dataset platforms (i.e., seventeen, respectively). While few researches make their dataset immutable (i.e., Permanent Storage), with three papers being classified. We summarize the available replication datasets with their URL links from these quantitative and mixed-method papers, referred to our Appendix (Appendix A.2).


Download : Download high-res image (164KB)
Download : Download full-size image
3.3. (RQ3): What metrics and topics are used with CR studies?
Table 6 shows sixteen core metrics sets based on 457 identified metrics with their corresponding review aspect and frequency. We summarize three findings. First, we observe that Experience and Code are the two most frequently used metric sets, which are far more than other classified metric sets. In detail, 76 and 72 metrics are involve in Experience and Code sets respectively. Experience is referred to those metrics computing the patch author or reviewer experience in submitting historical patches. Given an example, Ruangwan et al. (2018) took four experience related metrics into account: Reviewer Code Authoring Experience, Reviewer Reviewing Experience, Patch Author Code Authoring Experience, Patch Author Reviewing Experience. Code denotes to those metrics that focus on measuring the source code of a system. For instance, McIntosh and Kamei (2018) computed the number of added and deleted code lines. Apart from the general patch size, Maddila et al. (2019) proposed in-depth code related metrics such as Class churn, Loop churn, and Method churn. Others refers to those metric sets that cannot fit to the common metric sets with their definition. Second, from the view of the referred paper number, the result indicates that Code is considered as the first ranking basic metric set applied to model constructions. 28 out of total 31 papers (i.e., around 90%) take such metric set in to account. The third observation is that as we see from Table 6, one metric set can represent multiple CR aspects. For example, File metric set can either belong to Product and Process. Those metrics that compute the number of added and deleted files in the patch are regarded as Process aspect (Fan et al., 2018, Kamei et al., 2013, Gousios et al., 2014, Zhang et al., 2018). On the other hand, researchers calculate the file entropy (Thongtanunam et al., 2017, McIntosh and Kamei, 2018, Kamei et al., 2013). Such metrics are more likely to be dynamic and are viewed as Process aspect. To answer the correlation between metrics and their corresponding research topics, we summarize nine topics regarding code review and list related papers below:


Table 6. Metric sets used in code review paper.

Metric set	Product	Process	People	Others	Referred papers	# Related metrics
Experience		✓	✓		Kononenko et al., 2018, Shimagaki et al., 2016, Gousios et al., 2014, Rahman et al., 2017, Jiang et al., 2013, McIntosh et al., 2014, Kononenko et al., 2015, Bernardo et al., 2018, Zhang et al., 2018, Fan et al., 2018, Thongtanunam et al., 2017, Baysal et al., 2015, McIntosh and Kamei, 2018, Kamei et al., 2013, Baysal et al., 2013, Morales et al., 2015, Zampetti et al., 2019, Tourani and Adams, 2016, Wang et al., 2019, Zhao et al., 2019, Spadini et al., 2019, Maddila et al., 2019 and Soares et al. (2018)	76
Code	✓	✓			Bernardo et al., 2018, Zampetti et al., 2019, Baysal et al., 2013, Kononenko et al., 2018, Jiang et al., 2013, Fan et al., 2018, Thongtanunam et al., 2016, Shimagaki et al., 2016, Thongtanunam et al., 2015, McIntosh et al., 2014, Kononenko et al., 2015, Mcintosh et al., 2016, McIntosh and Kamei, 2018, Kamei et al., 2013, Soltanifar et al., 2016, Morales et al., 2015, Tourani and Adams, 2016, Thongtanunam et al., 2017, Ruangwan et al., 2018, Gousios et al., 2014, Armstrong et al., 2017, Zhang et al., 2018, Zhao et al., 2019, Maddila et al., 2019, Beller et al., 2014, Baysal et al., 2015, Tsay et al., 2014a and Spadini et al. (2018)	72
Ownership			✓		Zampetti et al., 2019, Jiang et al., 2013, Fan et al., 2018, Thongtanunam et al., 2016, Shimagaki et al., 2016, Thongtanunam et al., 2015, McIntosh et al., 2014, Kononenko et al., 2015, Mcintosh et al., 2016, Morales et al., 2015, Tourani and Adams, 2016, Thongtanunam et al., 2017, Soares et al., 2018, Rahman et al., 2017, Beller et al., 2014, Spadini et al., 2018 and Soltanifar et al. (2016)	38
Comment		✓			Bernardo et al., 2018, Zampetti et al., 2019, Kononenko et al., 2018, Jiang et al., 2013, Shimagaki et al., 2016, Thongtanunam et al., 2015, Kononenko et al., 2015, McIntosh and Kamei, 2018, Soltanifar et al., 2016, Morales et al., 2015, Tourani and Adams, 2016, Thongtanunam et al., 2017, Gousios et al., 2014, Rahman et al., 2017, Zhang et al., 2018, Tsay et al., 2014a and Ruangwan et al. (2018)	37
File	✓	✓			Bernardo et al., 2018, Zampetti et al., 2019, Kononenko et al., 2018, Jiang et al., 2013, Fan et al., 2018, Thongtanunam et al., 2016, Shimagaki et al., 2016, McIntosh et al., 2014, Kononenko et al., 2015, McIntosh and Kamei, 2018, Kamei et al., 2013, Soltanifar et al., 2016, Tourani and Adams, 2016, Thongtanunam et al., 2017, Soares et al., 2018, Gousios et al., 2014, Zhang et al., 2018, Zhao et al., 2019, Maddila et al., 2019, Beller et al., 2014 and Tsay et al. (2014a)	31
Participant			✓		Kononenko et al., 2018, Jiang et al., 2013, Thongtanunam et al., 2016, Shimagaki et al., 2016, Thongtanunam et al., 2015, McIntosh et al., 2014, McIntosh and Kamei, 2018, Kamei et al., 2013, Morales et al., 2015, Ruangwan et al., 2018, Soares et al., 2018, Armstrong et al., 2017, Zhao et al., 2019, Maddila et al., 2019, Beller et al., 2014, Tsay et al., 2014a, Kononenko et al., 2015, Gousios et al., 2014 and Soltanifar et al. (2016)	31
Temporal		✓			Bernardo et al., 2018, Zampetti et al., 2019, Jiang et al., 2013, Shimagaki et al., 2016, Thongtanunam et al., 2015, McIntosh and Kamei, 2018, Kamei et al., 2013, Soltanifar et al., 2016, Morales et al., 2015, Tourani and Adams, 2016, Thongtanunam et al., 2017, Armstrong et al., 2017 and Spadini et al. (2019)	26
Revision	✓	✓			Zampetti et al., 2019, Bernardo et al., 2018, Kononenko et al., 2018, Jiang et al., 2013, Fan et al., 2018, McIntosh and Kamei, 2018, Kamei et al., 2013, Thongtanunam et al., 2015, McIntosh et al., 2014, Morales et al., 2015, Tourani and Adams, 2016, Soares et al., 2018, Armstrong et al., 2017, Gousios et al., 2014, Maddila et al., 2019, Zhang et al., 2018 and Zhao et al. (2019)	24
Description	✓	✓			Bernardo et al., 2018, Fan et al., 2018, Kamei et al., 2013, Thongtanunam et al., 2017, Maddila et al., 2019, Beller et al., 2014, Zampetti et al., 2019, Tourani and Adams, 2016 and Zhao et al. (2019)	21
Module	✓				Zampetti et al., 2019, Baysal et al., 2013, Jiang et al., 2013, Fan et al., 2018, Kononenko et al., 2015, McIntosh and Kamei, 2018, Kamei et al., 2013, Tourani and Adams, 2016, Thongtanunam et al., 2017, Gousios et al., 2014, Zhao et al., 2019, Maddila et al., 2019, Beller et al., 2014, Baysal et al., 2015, Tsay et al., 2014a and Spadini et al. (2018)	21
Defect		✓			Thongtanunam et al., 2017, Baysal et al., 2015, Mcintosh et al., 2016, Soltanifar et al., 2016, Baysal et al., 2013 and Armstrong et al. (2017)	14
Queue		✓			Bernardo et al., 2018, Baysal et al., 2013, Armstrong et al., 2017, Maddila et al., 2019, Baysal et al., 2015 and Kononenko et al. (2015)	6
Workload			✓		Ruangwan et al., 2018, Bernardo et al., 2018 and Thongtanunam et al. (2017)	5
Decision		✓			Zhang et al., 2018, Maddila et al., 2019, Jiang et al., 2013 and Soares et al. (2018)	5
Language	✓				Fan et al. (2018) and Maddila et al. (2019)	4
Log		✓			Bernardo et al. (2018)	1
Email				✓	Jiang et al. (2013)	11
Collaboration				✓	Tsay et al. (2014a) and Fan et al. (2018)	10
Build related				✓	Zampetti et al. (2019)	4
Project				✓	Tsay et al. (2014a) and Baysal et al. (2015)	9
Others				✓	Zampetti et al., 2019, Baysal et al., 2013, Kononenko et al., 2018, Kononenko et al., 2015, Morales et al., 2015, Ruangwan et al., 2018, Soares et al., 2018, Maddila et al., 2019 and Spadini et al. (2019)	11
Total					31	457
•
Quality Assurance: refers to papers focusing on the code quality such as bug fixing (Thongtanunam et al., 2016, Shimagaki et al., 2016, Thongtanunam et al., 2015, McIntosh et al., 2014, Kononenko et al., 2015, Soltanifar et al., 2016, Morales et al., 2015, Tourani and Adams, 2016, Mcintosh et al., 2016, McIntosh and Kamei, 2018, Kamei et al., 2013)

•
Acceptance Predication: refers to papers focusing on predicting the decision of the patches (Kononenko et al., 2018, Jiang et al., 2013, Fan et al., 2018) .

•
Review Process: refers to papers exploring or comparing the different peer review models (Gousios et al., 2014, Armstrong et al., 2017).

•
Review Participation: refers to papers focusing on the reviewer participation (Thongtanunam et al., 2017, Soares et al., 2018, Ruangwan et al., 2018).

•
Review Process Prediction: refers to papers focusing on predicting the period taken to complete the review (Zhang et al., 2018, Maddila et al., 2019, Zhao et al., 2019, Beller et al., 2014).

•
Review Process Comments: refers to papers focusing on predicting usefulness of review comments (Rahman et al., 2017).

•
CI & Review: refers to papers focusing on the correlation between CI implementation and code review (Bernardo et al., 2018, Zampetti et al., 2019).

•
Test & Review: refers to papers focusing on the correlation between test and code review (Spadini et al., 2018, Spadini et al., 2019).

•
Technical/Non-Technical & Review refers to papers investigating the technical and non-technical factor impact on the code review (Baysal et al., 2015, Tsay et al., 2014a, Baysal et al., 2013).

Fig. 5 maps the metric sets with nine CR research topics. Two findings are observed based on the related paper list and Fig. 5. First, we find that Quality Assurance related topics are more likely to use CR metrics to conduct the research, i.e., eleven papers are identified within this topic. For instance, Mcintosh et al. (2016) conducted an empirical study to investigate the relationship between post-release defects and code review practices such as coverage, participation, and reviewer expertise. Their findings confirmed the intuition that poorly-reviewed code has a negative impact on software quality. The second popular topic is Reviewer Process Prediction, i.e., four papers retrieved, which refers to papers focusing on predicting the review period. One example of the process prediction topic is that in the study of Zhao et al. (2019), they applied a couple of eighteen metrics like source code, textual information, experience, and social connection related metrics to recommend pull requests that can be quickly reviewed by reviewers. The second finding is that different research topics use particular metric sets. As shown in Fig. 5, Review Process Comments only adopt Experience, Code, and Comment into their statistic model construction. However, for Quality Assurance, Acceptance Predication, and Review Participation, the metric sets are diverse considering comprehensive angles. For instance, the Review Participation topic takes almost all kinds of metric sets into accounts except for language, Queue, and Log. In addition, for those specific research topic, the particular metric sets will be computed specially such as Build related metric sets used in the CI & Review topic. The detailed metrics within each metric set are listed in the Appendix (Appendix A.2).


Download : Download high-res image (229KB)
Download : Download full-size image

Download : Download high-res image (407KB)
Download : Download full-size image
Fig. 5. Nine research topics with their target metric sets. The figure shows that different research topics tend to target particular metric sets.

4. Comparative analysis
In this section, we followed the protocol of comparative analysis provided by the work of Petersen et al. (2008) to compare and highlight the novelty of our work against existing systematic reviews. The systematic review studies were identified using the following search string: “systematic review” AND “code review” and by searching a broad collection source (i.e., ACM Digital Library, IEEE Xplore, Science Direct, and SpringerLink databases). We exclude papers that did not explicitly state in the title or abstract that they were systematic reviews or were not published in the traditional SE domain based venues. The search results in a total of three systematic reviews (Kollanus and Koskinen, 2009, Badampudi et al., 2019, Coelho et al., 2019) before or in 2019.

For each of the three CR systematic reviews, we characterize them based on their research goals, criteria for inclusion requirements, the number of papers, and means of analysis. Table 7 shows the characteristics of the three existing systematic reviews. Below, we now discuss the differences between our study and the existing systematic reviews in detail, with the aspect of research goals, the process, and the breadth and depth:


Table 7. Existing systematic review characteristics.

Reference systematic reviews	Kollanus and Koskinen (2009)	Badampudi et al. (2019)	Coelho et al. (2019)
Research goals			
Identify best and typical practices	✓	✓	
Classification and taxonomy	✓		✓
Emphasis on topic categories	✓	✓	
Identify publication fora	✓	✓	✓
Inclusion requirements			
Research is within focus area	✓	✓	✓
Empirical methods used	✓	✓	✓
Number of included articles			
Potentially relevant studies	229	873	–
Relevant studies (Included)	153 (1980–2008)	177 (2005–2018)	13 (2007–2018)
Means of analysis			
Meta study	✓	✓	✓
Comparative analysis			
Thematic analysis	✓	✓	
Narrative summary	✓		
•
Difference in Research Goals: The systematic review conducted by Kollanus and Koskinen (2009) aims at reviewing how the software inspection field has evolved between 1980 and 2008. Their focus is set in the context of the software inspection. However, our mapping study addresses the contemporary tool-based code review, which is a light variant of the software inspection and has been widely adopted in both industrial and open-source projects. Although Badampudi et al. (2019) and Coelho et al. (2019) conducted the systematic review related to contemporary tool-based code review, the goal of our mapping study is also different from them. The main goal of the work by Badampudi et al. (2019) is to observe the evolution of the research topic, while the work by Coelho et al. (2019) is to gather evidence on the extent of the work related to refactoring-awareness during code review. Differently, our study aims at investigating the potential of benchmarking in the aspect of datasets and metrics. We believe that a common benchmark could facilitate future CR related researches and help researchers to propose new approaches and compare against existing ones.

•
Difference in Process: We observe two main differences in the process when compared to the two systematic reviews related to the tool-based review. First, compared to the work of Coelho et al. (2019), we include the thematic analysis (i.e., classification schema of methodologies, contributions, data, and metrics). For the systematic mapping study, thematic analysis is an interesting analysis method, which helps to see which categories are well covered in terms of number of publications (Petersen et al., 2008). Second, compared to both work by Coelho et al. (2019) and Badampudi et al. (2019), we conduct an in-depth narrative summary analysis with qualitative review of each paper, as both of them are served as a preliminary study.

•
Difference in Breadth and Depth: Two differences are summarized, based on the systematic reviews related to the tool-based review. On the one hand, Coelho et al. (2019) focused on the specific theme of tool-based code review, i.e., refactoring-awareness. However, our study covers all potential themes. On the other hand, in the study ofBadampudi et al. (2019), they extracted CR related papers from all possible venues (i.e., 177 papers are retrieved between 2005 and 2018). While to ensure the paper quality and form a best representative view for future researches, we only focus on the papers that are published in the premium venues (i.e., 112 papers are retrieved between 2011 and 2019).

5. Toward a common benchmark of dataset and metric
Although our results suggest that CR research is mostly driven by empirical evaluation, we conclude that at this stage, we cannot benchmark CR studies. However, the existing datasets and metrics do show potential for creating a benchmark. With the rise of machine learning and AI techniques, CR researchers will soon need to agree on the common set of metrics that should be included to accurately compare such techniques against each other. Having a benchmark will facilitate new researchers, including experts from other fields, to innovate new techniques and build on top of already established methodologies. This mapping commonalities between metrics and datasets is shown in Table 8.


Table 8. A summary of common metric sets and datasets used in various SE topics.

CR topic	# papers	Common metric sets	Common datasets	Review settings
Quality assurance	11	Code (100%), Ownership (82%), File, Participant (73%), Experience (64%)	18%	Gerrit (8), Pull-based (0), Others (3)
Review process prediction	4	Code (100%), Participant, File, Module, Description, Experience, Revision (75%)	0	Gerrit (1), Pull-based (3), Others (0)
Acceptance prediction	3	Code, Experience, File (100%), Comment, Ownership (66%)	0	Gerrit (1), Pull-based (1), Others (1)
Review participation	3	Experience (100%), Code, File, Workload, Participant, Comment (66%)	0	Gerrit (2), Pull-based (1), Others (0)
Review process	2	Code, Revision, Participant (100%)	0	Gerrit (0), Pull-based (1), Others (1)
CI & Review	2	Code, Comment, Description, Experience, File, Revision, Temporal (100%)	0	Gerrit (0), Pull-based (2), Others (0)
Test & Review	2	–	0	Gerrit (1), Pull-based (0), Others (1)
Technical/Non-Technical & Review	3	Code, Module (100%), Defect, Experience, Defect (66%)	67%	Gerrit (0), Pull-based (1), Others (2)
Table 8 suggests that there exists a regular group of metric set combinations commonly used for papers that are addressing a specific SE topic. We selected metrics that are commonly mentioned in more than 60% of the classified papers (i.e., from RQ3). For instance, in Quality Assurance related papers, Code is computed for all (i.e., 100%) and around 82% of papers take Ownership into account. In Acceptance Prediction related papers, all three papers compute the metrics of Code, File, and Experience. On the other hand, Table 8 also shows that common datasets were not commonly adopted by researchers. For instance, since two of the three papers in Technical/Non-Technical and Review were written by the same authors, the ratio for having a common dataset is high (i.e., 67%). This means that researchers from different groups prefer to construct their own datasets to conduct their study. Another reason is because the technology used to generate datasets constantly evolve, thus, deeming any prior datasets as being outdated. Furthermore, since more datasets are taken from either the Gerrit or GitHub Pull-request API, they sometimes miss the essential elements needed for a specific study. This process can be time-consuming, and could be easily resolved by using a benchmark. We also find that different review settings (i.e., Gerrit and Pull-based) have different emphases on the SE topics. For example, Quality Assurance datasets are almost from Gerrit while in CI and Review datasets are all from Pull-based review settings. One possible reason is that some specific metric sets are not easily available to be retrieved from different review settings.


Table 9. Listing of fully replicated papers with their available datasets.

Reference	Available dataset
Kononenko et al. (2018)	https://cs.uwaterloo.ca/ okononen/shopify
Spadini et al. (2018)	https://doi.org/10.5281/zenodo.1172419
Floyd et al. (2017)	http://dijkstra.cs.virginia.edu/fmri/
Thongtanunam et al. (2016)	Who does what during a Code Review? An extraction of an OSS Peer Review Repository (Hamasaki et al., 2013)
Zhang et al. (2015)	https://sites.google.com/a/utexas.edu/critics/
Gupta et al. (2014)	https://github.com/Mining-multiple-reposdata/experimentaldataset
Rahman et al. (2017)	http://homepage.usask.ca/masud.rahman/revhelper
Thongtanunam et al. (2015)	Who does what during a Code Review? An extraction of an OSS Peer Review Repository (Hamasaki et al., 2013) / The Impact of Code Review Coverage and Code Review Participation on Software Quality (McIntosh et al., 2014)
McIntosh et al. (2014)	http://sailhome.cs.queensu.ca/replication/reviewing_quality/
Ouni et al. (2016)	Mining the modern code review repositories: A dataset of people, process and product (Yang et al., 2016)
Shimagaki et al. (2016)	https://github.com/yiu31802/icsme2016
Kononenko et al. (2015)	https://cs.uwaterloo.ca/ okononen/bugzilla_public_db.zip
Xia et al. (2015)	Who should review my code? (Thongtanunam et al., 2015)
Bavota and Russo (2015)	http://tinyurl.com/kzw43n6
Bosu et al. (2014)	http://amiangshu.com/VCC/index.html
Rigby and Bird (2013)	Gerrit software code review data from Android (Mukadam et al., 2013)
Paixao et al. (2017)	https://mhepaixao.github.io/architecture_awareness/
Mirhosseini and Parnin (2017)	https://github.com/alt-code/Research/tree/master/VersionBot
Hannebauer et al. (2016)	https://www.uni-due.de/ hw0433/
Bernardo et al. (2018)	https://prdeliverydelay.GitHub.io/#datasets
Zhang et al. (2018)	http://cstar.whu.edu.cn/p/cpr/
Zanaty et al. (2018)	https://github.com/software-rebels/DesignInCodeReviews-ESEM2018
Baysal et al. (2013)	https://cs.uwaterloo.ca/ obaysal/webkit_data.sqlite
Zampetti et al. (2017)	http://tinyurl.com/pull-resourcesreplication
Armstrong et al. (2017)	https://bitbucket.org/foundjem/ml-issuetk/src
Thongtanunam et al. (2015)	http://github.com/patanamon/revfinder
Panichella et al. (2015)	http://ser.soccerlab.polymtl.ca/ser-repos/public/tr-data/2015-saner-code-reviews.zip
Zampetti et al. (2019)	https://goo.gl/KjTpxp
Ebert et al. (2019)	https://github.com/felipeebert/confusion-code-reviews
Tourani and Adams (2016)	http://gsyc.es/ jgb/repro/2015-msr-grimoire-data
Paixao and Maia (2019)	https://zenodo.org/record/3354510#.X1i3W2czbLA
Spadini et al. (2019)	https://www.mediafire.com/folder/3b5ey849y9flx/Test-Driven_Code_Review_-_Online_Appendix
Liu et al. (2019)	https://tinyurl.com/y3yk6oey, https://github.com/Tbabm/PRSummarizer
Hirao et al. (2019)	https://github.com/software-rebels/ReviewLinkageGraph
Thongtanunam et al. (2017)	http://sailhome.cs.queensu.ca/replication/review_participation/
Baysal et al. (2015)	https://cs.uwaterloo.ca/ obaysal/webkit_data.sqlite
Mcintosh et al. (2016)	https://sailhome.cs.queensu.ca/replication/reviewing_quality_ext/
McIntosh and Kamei (2018)	https://github.com/software-rebels/JITMovingTarget
Kamei et al. (2013)	http://research.cs.queensu.ca/ kamei/jittse/jit.zip
Yu et al. (2016)	https://github.com/yuyue/pullreq_ci
Asri et al. (2019)	Mining the modern code review repositories: A dataset of people, process and product (Yang et al., 2016)
Ruangwan et al. (2018)	https://github.com/sruangwan/replication-human-factors-code-review/
6. Threats to validity
We now discuss threats to the validity of our mapping study.

External validity. External validity is concerned with our ability to generalize based on our results. The results of this mapping study are considered with regard to the CR domain, while the validity of conclusions is applicable only to the CR context. The external validity threats are thus not applicable.

Construct validity. Construct validity is concerned with the degree to which our measurements capture what we aim to study. During the qualitative analysis, especially for the methodology and the contribution classification, methodologies and contributions may be miscoded due to the subjective nature of our coding approach. To mitigate this threat, three co-authors sat in a round-table and did the classification. If a dispute occurred, the full contents of the papers were discussed before the consensus was reached.

Internal validity. Internal validity is the approximate truth about inferences regarding cause–effect or causal relationships. We summarize three potential internal threats. The first threat is related to the venue selection. In this mapping study, we only consider 34 top venues considering their online citation indices and feedback from the software engineering community, similar to the work of Mathew et al. (2018). Thus, there will always be a venue missing from such a study and can be considered. However, we believe these 34 top venues can represent the best practice for the CR research. The second threat is related to the paper selection of the studies during the screening process. Due to the large amount of hits from our search string, our initial step includes the first author scanning through and discarding papers based on titles and abstracts, which potentially raises a bias in the paper selection. Nevertheless, we are confident of this threat, as the first author is an existing code review researcher and is familiar with the domain. The third possible internal threat is with regard to the terms that are used in our search string. The case might exist that the search string will not cover all terms. To reduce such risk, in the initial round, we manually checked twenty CR related papers to group the term candidates, and we were confident that the existing search term is sufficient.

Conclusion validity. Conclusion validity is the degree to which conclusions we reach about relationships in our data are reasonable. In the case of our datasets, there is a threat that our grouping is not accurate. Since there is no related work that has similar results, we cannot verify our findings. To mitigate this, we rely on the systematic guidelines for our outcomes. In addition, we publish a website and open it to both researchers and practitioners to criticize or add to our results.

7. Conclusion
Code review (CR), as a well-known practice, plays a vital role in software quality assurance. In the recent decade, the contemporary review tools have made the review process now being light-weight and have been widely adopted in both open-source and industrial projects. Due to the availability of datasets brought by these review tools, CR related researches are largely carried out. In order to understand the state-of-the-art practices, we first conduct a systematic mapping study to provide a visual summary of the benchmark potential within the CR domain through 112 papers that are published in premium conferences and journals.

Three main maps are visualized. First, concerning the map between methodologies and contributions, we find that 65% of CR researches use sound evaluation methodology (i.e., 73 papers), targeting particularly socio-technical and understanding of CR. While, there is a lack of papers that report the experience and propose solutions to deal with CR problems (i.e., four papers and thirteen papers, separately). Second, concerning the map of the datasets, our results show that few researches provide replicable datasets, i.e., around 50%. Third, concerning the map of metrics, we identify 457 metrics which are grouped into sixteen core metrics sets, and we observe that the SE topic of quality assurance is more likely to use CR metrics to conduct the research, with eleven papers being classified. Additionally, we find that different research topics use particular metric sets, which provides the potential for a benchmark.

The next step is the creation of a benchmark to facilitate future research against the state-of-the-art. With the rise of machine learning and AI techniques, a common benchmark is needed as it will facilitate CR researchers to accurately compare techniques against each other and propose new approaches. To encourage this framework construction, we provide a listing of methodologies and contribution, 42 public datasets, and 457 identified metrics across nine research topics which is available at https://naist-se.github.io/code-review/.