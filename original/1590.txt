Abstract
BPMN collaboration models have acquired increasing relevance in software development since they shorten the communication gap between domain experts and IT specialists and permit clarifying the characteristics of software systems needed to provide automatic support for the activities of complex organizations. Nonetheless, the lack of effective formal verification capabilities can hinder the full adoption of the BPMN standard by IT specialists, as it prevents precisely check the satisfaction of behavioral properties, with negative impacts on the quality of the software. To address these issues, this paper proposes BProVe, a novel verification approach for BPMN collaborations. This combines both standard model checking techniques, through the MAUDE’s LTL model checker, and statistical model checking techniques, through the statistical analyzer MultiVeStA. The latter makes BProVe effective also on those scenarios suffering from the state–space explosion problem, made even more acute by the presence of asynchronous message exchanges. To support the adoption of the BProVe approach, we propose a complete web-based tool-chain that allows for BPMN modeling, verification, and result exploration. The feasibility of BProVe has been validated both on synthetically-generated models and on models retrieved from two public repositories. The performed validation highlighted the importance and complementarity of the two supported verification strategies.

Previous
Next 
Keywords
BPMN

Collaboration

Verification

Model checking

Statistical model checking

1. Introduction
A business process model describes a set of activities that an organization should perform to fulfill a specific business goal (Lindsay et al., 2003). Furthermore, it is possible to use the so-called collaborations to describe the coordination of processes belonging to different organizations willing to cooperate to achieve a shared goal. As it happens for any modeling activity, the description of the reality of interest through the usage of a modeling notation permits to reduce the communication gap between the various users of the model, keeping the focus just on those aspects are considered relevant for the specific objective. In this respect, collaboration models help maintain the modeler’s attention on the alignment of the internal behavior of a set of processes concerning inter-process communication.

Business process modeling has been initially introduced for documentation purposes by business analysts, but it shortly started to be adopted in software development, in particular about requirements engineering activities (Campos and Oliveira, 2013, Dumas and Pfahl, 2016, Li et al., 2012, de Vasconcelos et al., 2012). Successively it has gained popularity in the development of software systems supporting business process execution, and as a starting point for model-driven development of distributed systems (Pastor, 2017, Aldazabal et al., 2008), as also testified by the emergence of several engines enabling the direct execution of business process specifications (e.g., Camunda,1 Signavio,2 Bonita.3)

A similar path has been followed in the last years by the Business Process Model and Notation (BPMN 2.0) (OMG, 2011), an Object Management Group (OMG) standard. The notation emerged as one of the most adopted proposals to define business process models. The success of BPMN comes from its versatility and capability to represent business processes for different purposes. The notation acquired acceptance, at first, within the business analysts community, and successively it has been more and more adopted by IT specialists to drive the development and settlement of IT systems supporting the execution of a specified process model. This shift in notation usage is particularly relevant, and it poses the basis for our work. Indeed, the adoption of BPMN for shaping IT systems and for the application of model-driven approaches to automatic code generation requires the definition of a formal verification approach to increase confidence in the quality of implemented software systems (D’silva et al., 2008).

While the research community has devoted a relevant effort to support verification of single business processes, to the best of our knowledge, there is still no concrete proposal supported by tools to analyze large collaborative processes. On the other hand, modeling of such scenarios has become more and more common in practice. This is certainly a consequence of the extensive introduction of effective software system integration technologies, such as REST-based services. This has permitted to derive collaborative systems from the integration of independently developed and managed software, and to figure an API economy where openly documented interfaces, for instance adopting OAI4 formats, are made available and can be accessed using precisely described interaction protocols, so to create an ecosystem fostering software system collaborations (Hesenius et al., 2019, Tan et al., 2016).

In deriving a verification approach for collaborative scenarios, which could be used in real contexts, we identified two additional characteristics that we judged of primary relevance. The first one concerns the fact that a collaborative scenario is by its very nature a parallel scenario, which then could easily lead to a possible explosion of the state space to be managed, making traditional verification strategies not always effective. We initially experimented with such an issue while running the experiments reported in Section 8 using the “standard” verification strategy. Therefore, the approach has been augmented to include a statistical model checking strategy. The second characteristic refers instead to the typical difficulties of introducing formal verification techniques in general modeling contexts, where it is often the case that a modeler does not have a strong background in formal methods. So, to make accepting the proposed approach easier, we decided to make available basic verification features via pre-configured and stereotyped properties. We provide a GUI where properties can be derived by selecting and composing the entries made available via a set of pop-up menus. In such a way, a modeler can start to use and experiment with our approach even without a clear understanding of the verified properties. This could help the interested modeler to acquire confidence in the approach through its usage. It is worth noticing that the modeler is not asked to modify, in any way, his/her modeling habits. The approach includes optional mechanisms for those users with a good understanding of LTL (Linear Temporal Logic) verification, enabling them to define and check customized temporal properties.

The resulting verification approach, called BProVe,5 is offered as a web-based tool-chain6 that allows to graphically observe the results of the verification directly on the models under scrutiny. The verification component is provided as a REST web service that, thanks to the BPMN standard usage, allows to check properties of BPMN models independently from the modeling environment used to create them. The web service has also been integrated within an Eclipse plugin, allowing its integration in the Eclipse platform.

We have extensively validated the effectiveness of the BProVe verification approach by running scalability tests on ad-hoc designed and synthetically generated models, as well as on models from two open repositories “BPM Academic Initiative Model Collection (BPMAI)”7 and “Camunda BPMN for Research”.8 This analysis has shown the potential of BProVe both on limiting-case scenarios and on realistic ones and highlighted the advantages of having two analysis engines offering complementary analysis techniques.

Summing up, the most distinctive features of our BProVe approach are following reported.

1.
The use of direct semantics for BPMN models, both for single processes and collaborations, without requiring any intermediate encoding. This allows for an easy result exploration, i.e., to graphically interpret the analysis results directly on the actual BPMN model.

2.
An automated formal verification approach that permits the analysis of BPMN collaboration models with potentially large state spaces by integrating standard and statistical model checking analysis capabilities. The need for the inclusion of two different verification strategies emerged after observing the presence of collaborative models with large state space for which standard verification strategies were not able to provide an answer. The validation we performed aimed at assessing that the inclusion of two different strategies is indeed valuable, as they show complementary characteristics. It also permits to enlarge the set of models for which the approach can provide an answer.

3.
The robust, efficient and accessible tool support allowed us to perform an extensive validation of the approach confirming its scalability and the complementarity of the two supported analysis techniques. The tool is offered as a REST service. A web-based front-end is available, hiding all the formalism involved, thus enabling the verification as a service paradigm.

The paper is organized as follows. Section 2 provides an overview of the BPMN standard and of the operational semantics at the basis of our work, using a collaboration scenario exploited as a running example in the rest of the paper. Section 3 discusses the BProVe approach presenting its main characteristics. Section 4 describes how properties can be defined in the BProVe approach, while Section 5 discusses how they can be verified, and Section 6 exemplifies this on the running example. Section 7 presents the tool-chain’s architecture and user interface. Section 8 illustrates the conducted validation experiments. Finally, Section 9 thoroughly compares our approach with related works available in the literature and Section 10 concludes by also touching upon directions for future work.

2. Background notions
In this section we introduce the BPMN standard together with a scenario, used as running example throughout the paper, to illustrate our proposed verification approach. Then, we introduce the implemented operational semantics at the basis of BProVe.

2.1. Modeling collaborations in BPMN
BPMN, an OMG standard (OMG, 2011), is currently acquiring a clear predominance among the proposed notations to model business processes thanks to: (i) its intuitive and graphical notation that is widely accepted by the industry and the academia; and (ii) the support provided by a broad spectrum of modeling tools.9

Here we discuss the BPMN elements supported by our approach and reported in Fig. 1. In our proposal, we selected a subset of BPMN elements following the pragmatic approach of retaining those features most used in practice (Muehlen and Recker, 2008). Pools represent participants or organizations providing details on internal process specifications and related elements. Pools are drawn as rectangles. Tasks represent specific jobs to be performed within a process. Tasks are drawn as rectangles with rounded corners. Gateways manage the flow of a process both for parallel activities and choices. Gateways are drawn as diamonds and act as either join nodes (merging incoming sequence edges) or split nodes (forking into outgoing sequence edges). Different types of gateways are available: a XOR gateway describes choices, an AND gateway enables parallel execution flows, an Event-Based gateway activates its outgoing branches according to the taking place of catching events, and an OR split gateway which allows to execute one or more of its outgoing flows. Events are used to represent something that can happen. An event can be a Start Event, representing the point from which the process starts, an Intermediate Event representing something that happens during process execution (e.g., the sending/receiving of a message), or an End Event representing the process termination. Events are drawn as circles. We also refer to a particular type of end event, the Terminate End Event, displayed by a thick circle with a darkened circle inside; it stops and aborts the running process. Connecting Edges connect process elements in the same or different pools. A Sequence Edge is a solid connector used to specify the internal flow of the process, thus ordering elements in the same pool, while a Message Edge is a dashed connector used to visualize communication flows between organizations. A set of pools interacting through message exchanges form a collaboration model.

The model depicted in Fig. 2 presents a collaboration process between three participants: a Travel Agency, an Airline reservation system, and a Customer. The goal of the collaboration is to provide a travel offer to a customer and handle the response. The Travel Agency triggers the collaboration process, which elaborates a travel offer and sends it to a Customer. The Customer evaluates the received offer deciding whether to accept it or to reject it. The Travel Agency and the Airline handle the customer response, either by confirm the booking and handling the payment, or by terminating their processes. The execution of BPMN models is based on the notion of tokens, graphically denoted as black dots labeling BPMN elements (see the start events in Fig. 2). The presence of such tokens over the start events enables the execution of the three processes (representing the collaboration’s initial status). Tokens traverse the sequence edges of processes and pass through their elements enabling their execution. The notation element’s specific characteristics define the rules to follow to move, consume and generate tokens.

2.2. BPMN operational semantics
The interpretation, or semantics, of the BPMN standard is given in informal natural language. In order to obtain a formal verification approach for such standard, we had first to make this semantics formal in Corradini et al. (2018). This formal semantics has been then implemented in the form of an executable interpreter in MAUDE in Corradini et al., 2017a, Corradini et al., 2017b. The full MAUDE implementation of our BPMN interpreter is available at https://github.com/PROSLab/BPMNOS-Maude. In the following, using our running example, we exemplify the BPMN syntax and semantics as implemented in MAUDE to allow the reader to grasp the main concepts behind it.

BPMN syntax in MAUDE
Listing 1 provides part of the textual representation in our MAUDE interpreter for our running example. Notably, Listing 1 specifies that the model is in the initial configuration as depicted in Fig. 2, with a token in the start event of each process in the configuration. The complete specification of our example is given in Appendix.


Download : Download high-res image (176KB)
Download : Download full-size image
As we can see from Listing 1, a collaboration is specified using the operator collaboration (Line 1), which takes a set of pools as arguments (one for each pool in the model). A pool, defined by means of the operator pool (Lines 2, 8, and 14), takes as argument its name, a BPMN process identified by the operator proc, and a set of incoming and outgoing messages listed after in: and out:, respectively, to communicate with other pools. We can see from Line 6 that a message is declared using its name ("Offer"), followed by the operator .msg, in turn followed by an integer denoting the number of tokens present at the message element. The BPMN semantics, indeed, makes use of the token concept to intuitively describe the execution flow. We note that the operator andmsg is used to compose sets of messages, while eventual further messages are denoted in Listing 1 by a place-holder IMsgSet to represent incoming messages, and OMsgSet for outgoing ones.

A BPMN process is specified using the operator proc, having as argument the set of BPMN elements (separated by |) that composes it. For example, Lines 3–5 show the process of the Customer. Similarly to what has been done for messages, for the sake of readability in the listing we explicitly report only the elements depicted in the left-most part of the pools in Fig. 2, while we use the place-holder ProcElements to denote the other elements. The control flow is specified by the presence of tokens assigned to each process element. Process elements are allowed to act only when enabled, which means they hold a token. In Line 9 we see that the start element of the pool Travel Agency is enabled, meaning that it has a token in input (denoted by the black dot within the start event in Fig. 2), and hence it is allowed to initiate the process. Even if the start events of Customer and Airline (Lines 3 and 17) have a token, they are not allowed to initiate until the corresponding messages are received. The topology of the process is defined by the edges specified as arguments of the process components. In the example, the start node of the Customer (operator startRcv in Line 3) is connected via sequence edge e1 to the input of the task (operator task) defined in Line 4, whose output is in turn connected to other elements in ProcElements via sequence edge o1. The start node of the Travel Agency (operator start in Line 9) is connected via sequence edge e2 to the input of the task (operator task in Line 10), whose output is in turn connected to the other elements in ProcElements via sequence edge o2. Finally, the start node of the Airline (operator startRcv in Line 17) is connected via sequence edge e3 to the other elements in ProcElements via sequence edge o3. Sequence edges that have an associated value 0 do not include any token.

BPMN semantics in MAUDE
In MAUDE, the semantics is specified in terms of rewriting rules which are exhaustively applied by pattern matching on each generated state, starting from the initial one, until no new states can be generated. A rewriting rule has the following form: crl [Label] : Term-1 => Term-2 if Condition(s) .

The keyword crl stands for ‘conditional rewriting rule’, whose optional name is specified in the square brackets. The body of the rule, Term-1 => Term-2, specifies that if Term-1 can be matched on the part of a state; then a new state can be obtained by (i) removing the matched part from the state, and (ii) adding Term-2 to the remaining part of the state. In the example, one of such terms can be the entire collaboration, a pool, a process, or a BPMN element. The if defines a guard that has to be satisfied by the considered state to enable the application of the rule. In case no condition is required, then the if clause is omitted, and the keyword rl is used in place of crl.

The BPMN semantics we defined is multi-layer, meaning that it has rules for collaborations (layer 4) that depend on rules for pools (layer 3), which in turn depend on rules for processes (layer 2), triggered by rules for single BPMN elements (layer 1). Roughly, the semantics is given in this form: if a BPMN element el1 can evolve in an element el2, then a process proc1 containing el1 can evolve in a process proc2 obtained by replacing el1 with el2, and similarly for the higher layers, if necessary keeping into account interactions with other processes or pools. This can be mimicked in MAUDE using the conditions if el1 => el2 and if proc1 => proc2 as sketched in Listing 2.


Download : Download high-res image (82KB)
Download : Download full-size image
3. The BProVe approach
In the Introduction, we have discussed the rationale of combining business process management and software development. In this section, we concentrate on modeling and analysis, which are activities usually completed in an iterative way until reaching a stable version of the model at the basis of the software system to be developed. In particular, we present how the BProVe approach allows shortening the distance between modeling and analysis supporting all the phases of the model development cycle reported in Fig. 3.

Model design
Model Design involves stakeholders in collecting domain requirements to produce a model suitable to represent as-is or to-be scenarios within organizations. Our BPMN collaboration model and corresponding semantics give specific support for representing processes of different organizations which interact to achieve a common goal.

The BProVe approach makes it possible to reason directly in terms of BPMN models with an arbitrary topology without making any assumption on the structure of the collaboration model (i.e., well-structuredness, which asks for the proper composition of nested structures Dumas et al., 2012). This gives us the possibility to analyze both well-structured and unstructured models available in the literature (Corradini et al., 2021). As an example, the process in Fig. 5 is the well-structured version of the unstructured process in Fig. 4. Notice, the notion of well-structuredness is extended from processes to collaborations requiring to be satisfied by all the processes involved in a collaboration (Corradini et al., 2021). By using BProVe, the analysis can be done directly on the designed models, including poorly designed models, without any redesign imposed by the analysis techniques as other verification approaches require (e.g., El-Saber and Boronat, 2014). Modelers are free to represent the reality they perceive up to their modeling experience (Polyvyanyy and Bussler, 2013), hence the modeling activity results to be less complex (Kiepuszewski et al., 2000) and more expressive (Polyvyanyy et al., 2012, Polyvyanyy et al., 2014).

Property definition
Property Definition relates both the internal characteristics of a single process in a collaboration and the whole collaboration. This holds both for generic properties that are well-established in the business process domain, such as soundness (Dumas et al., 2018) and safeness (van der Aalst, 2000), and for ad-hoc properties specifically defined for given application scenarios.

Soundness can be described as the combination of three basic properties concerning the behavior of a process model:

(i)
Option to Complete: any running process instance must eventually complete;

(ii)
Proper Completion: at the moment of completion, each token of the process instance must be in a different end event;

(iii)
No dead activities: any activity can be executed in at least one process instance.

On the other hand, safeness refers to the occurrence of no more than one token at the same time along the same sequence edge. The satisfaction of these properties is generally considered a minimum guarantee to avoid unexpected behaviors (Weske, 2012).

Besides system-independent properties, such as the ones mentioned above, we also consider application-dependent properties specifically defined for the given application scenario. Such kind of properties are relevant both at the process level, i.e., considering the execution of tasks within a single process, as well as at the collaboration level, i.e., considering the effects of message exchanges. As a relevant example, and in relation to the model reported in Fig. 2, we refer to the following properties/queries.

(P1)
Does the start of the Confirm Booking task in the Travel Agency pool implies that the same task will sooner or later complete (Property 1)?

(P2)
Does the completion of a specified task in the Airline pool, say Handle Payment, implies the completion of another task in the same pool, say Confirm Payment (Property 2)?

(P3)
If the Customer has sent the payment to the Travel Agency, may it happen that the corresponding confirmation, by the Airline, is never received (Property 3)?

BProVe directly allows the verification of soundness and safeness properties for any model, as well as it gives the possibility to specify and verify application-dependent properties.

Property verification
Property Verification enables to check the considered properties detecting behavioral issues of the model. To do that, the model behavior is systematically explored to establish if a property of interest formally holds (Baier and Katoen, 2008).

In checking properties, BProVe offers two different analysis techniques, i.e. LTL model checking and statistical model checking, as alternative and, in some cases, complementary means to analyze BPMN models.

The LTL model checking technique we use is the one supported by the MAUDE LTL model checker (Eker et al., 2004). It allows for an exhaustive exploration of the model reachable states and provides a reliable response in the case of systems with a finite state space. However, model checking techniques are known to be affected by the state–space explosion problem, where a model generates so many states that model checking cannot complete in a reasonable amount of time, or may fail due to high memory requirements. Indeed the design of models with large state spaces is not uncommon in the context of business process modeling. Therefore, in order to be able to provide a valid response in those cases, we resort to a simulation-based technique known as statistical model checking (SMC) (Agha and Palmskog, 2018, Legay et al., 2010, Legay et al., 2019). This allows to analyze the model by running a finite number of finite executions, so to provide statistical evidence on the satisfaction or violation of a property. In other words, SMC allows to infer information on the entire state space while looking only to a reduced set of states at a time obtained via multiple simulations. An SMC analysis consists in performing independent simulations as long as a required level of statistical accuracy has not been reached.

The SMC technique we use is supported by MultiVeStA (Sebastio and Vandin, 2013). It enables us to easily associate a discrete uniform probability distribution to the states that can be reached in one step from a specific state, allowing for probabilistic simulations. The approach is depicted in Fig. 6: from the current state of the model we compute all () states reachable from it in one step. Each such state gets assigned the same probability (
 
) of being chosen as next state of a simulation. This is further discussed in Section 5.2. We note that this approach to path selection allows to introduce a form of fairness within our approach, which is not considered when using the MAUDE LTL model checker. In MultiVeStA, any time a state is (re-)visited in a different simulation or during the same one (e.g., for the presence of a loop in the flow) the choice of the actual next state is done using the same probabilistic distribution. Therefore, the continuous occurring of this situation will hardly result in the process always executing the same path, so we can assume that in the presence of a choice which gets repeated, sooner or later all the paths will be explored. The fairness assumption is reasonable in the context of workflow management since all choices are made (implicitly or explicitly) by applications, humans or external actors. Indeed, we use a fair scheduler in order to resolve all forms of non-determinism and choose probabilistically the states to be considered as next state in each simulation (see, e.g., Belzner et al., 2014 for a similar approach in the MAUDE context). In general, SMC tends to be faster and more scalable than model checking, with a price to pay in accuracy for not covering the entire state space of a system.

Result report
Result Report makes possible to report the result of the verification. Model checking techniques usually generate counterexamples which witness that a given property does not hold. In this way, it can be shown whether a given model satisfies or not a certain property (Baier and Katoen, 2008).

BProVe takes advantage of such counter-example generation by signaling to the end-user the model execution that falsifies the analyzed property. BProVe enables formal reasoning at the level of BPMN model, so that diagnostic information and counter-example can be directly reported on the BPMN diagram in a way that is understandable by process stakeholders. This is especially useful when many parties interact on the basis of the models. BProVe differs from other approaches which typically provide counter-examples on low-level representations in third-party formalisms, thus hindering the interpretation of verification results at BPMN level; we provide examples of such approaches in Section 9.

Upon the interpretation of verification results, the designer can decide whether to restart or conclude the process.

4. Properties definition
As stated in Section 3, BProVe enables the verification of properties over BPMN collaborations. To ease the definition of properties we have defined in MAUDE some recurrent predicates representing high-level BPMN-related concepts. In this section, we first describe the MAUDE predicates and how on top of them we define (i) LTL formulae to be verified with the MAUDE LTL Model Checker and (ii) MultiQuaTEx queries to be verified by the MultiVeStA statistical model checker. Anyone can extend our approach by writing in MAUDE new predicates, formulae and queries.

4.1. MAUDE Predicates
The defined MAUDE predicates are evaluated over a single state of the model execution: in our case a configuration of the collaboration model. Here we introduce some MAUDE predicates we have used to define the LTL formulae to be verified with the MAUDE LTL Model Checker. In particular, we consider the predicates used in checking the running example against the properties in Table 1, Table 2 and the MultiQuaTEx queries in Listings 8–12.

processCompletion. It is used for the implementation of the Option to Complete property, to check whether it is possible to reach a configuration with tokens only on end events implying that the process of a specific organization (OrgName) completed the execution. The property is encoded as shown in Listing 3: processCompletion evaluates to true if at least one end event, in the considered state, has been enabled and no other token remains in the process, apart from end elements.


Download : Download high-res image (93KB)
Download : Download full-size image
noProperCompletion. It is used for the implementation of the Proper Completion property, to check if the process of a specific organization (OrgName) completes with more than one token on the same end event. The property is encoded as shown in Listing 4: noProperCompletion evaluates to true if at least one end event has been enabled and it has more than one token. The first condition noTokenPresent(ProcElements1) evaluates to true if there is no token assigned to the set of process elements named ProcElements1. The second and third conditions, noMessagePending(outputMsgSet) =/= 0 and noMessagePending(inputMsgSet) =/= 0, evaluate to true whether a token related to a message exchange is still pending. The last condition OEToken  evaluates to true whether a token is present on the considered end event; note that this end event has already a token assigned since its state is set to enabled. The predicate has been appropriately defined also for Message end and Terminate end events; for presentation purposes we report only the definition involving the simple end event.


Download : Download high-res image (127KB)
Download : Download full-size image
aTaskRunning. It is used for the implementation of the No Dead Activities property, which establishes that a given task can be set, at least once, in the status running (meaning that the task is currently being executed). If this property holds for all the tasks in the model, then the model does not have dead activities. The property is encoded as shown in Listing 5: aTaskRunning evaluates to true if a label running(TaskName) is produced.



Download : Download high-res image (48KB)
Download : Download full-size image
Besides soundness and safeness, we also consider ad-hoc properties for the given application scenario. The MAUDE predicates we use are: aTaskComplete, satisfied if a specified Task in the Collaboration will always complete; aBPoolSndMsg (resp. aBPoolRcvMsg), satisfied if a specified message is always sent (resp. received).

These predicates had to be slightly modified when performing statistical model checking. Essentially, as shown in Listing 7, we had to change the predicates so that they would evaluate to either 1.0 or 0.0 rather than true or false depending on whether the predicate is satisfied or not.


Download : Download high-res image (95KB)
Download : Download full-size image

Table 1. LTL Model checker: soundness and safeness.

Property	LTL formula
Soundness (i): Option to complete	[] processStart(orgName) ->
<>processCompletion(orgName)
Soundness (ii): Proper completion	[]  noProperCompletion(orgName)
Soundness (iii): No dead activities	[]  aTaskRunning(taskName)
Safeness	[] safeState(orgName)
4.2. LTL formulae
The above mentioned MAUDE predicates are used to define LTL formulae (Pnueli, 1977) that can be checked on BPMN specifications using our BPMN interpreter and the MAUDE LTL model checker (Eker et al., 2004). The formulae we show here are obtained as a composition of the following basic operators:

•
, where the operator  (corresponding to the LTL operator ) is used to verify if a formula  eventually holds. That is, in any possible execution path we always encounter a state where  holds.

•
, where the operator  (corresponding to the LTL operator ) is used to verify if a formula  globally holds. That is,  holds in all states encountered in any possible execution path.

•
, where the operator  is the standard boolean implication.

We discuss next the LTL formulae corresponding to the Soundness and Safeness properties as shown in Table 1.

Option To Complete. This property relies on predicate processCompletion. It checks whether it is possible to reach a configuration with tokens only on end events implying that the process of an organization completed its execution.

Proper Completion. This property requires that the process of an organization always correctly completes its execution. In particular, we check that we never reach a state that satisfies the predicate noProperCompletion, i.e. a state with an erroneous completion. This is stated in the corresponding formula in Table 1, which checks that the predicate does not hold in any state of any execution of the model.

No Dead Activities. This property relies on the predicateaTaskRunning, which evaluates to true if the considered task has status running in the current state. Note that the corresponding formula in Table 1 actually verifies the opposite condition: it checks that the predicate does not hold in any state of any execution of the model. Therefore, if this formula is not satisfied, then there exists at least a state in at least one execution where the considered task has status running. In other words, if the formula is false for a task, then that task is not a dead activity. Furthermore, this formula has to be evaluated for all tasks in the model: if the formula is false for all the tasks in the model, then the model does not have dead activities.

Safeness. This property relies on predicate safeState and checks if the property holds for all the states of each model execution.

Concerning the verification of application-dependent properties, some predefined properties in our approach are reported in Table 2. They allow to check whether:

1.
having a task in the running state implies that the task will sooner or later complete (passing from the state running to complete);

2.
the completion of a task implies the completion of another task highlighting a relation between those tasks;

3.
the exchange of messages between different processes is happening correctly.

Anyone can use and combine the predicates implemented in MAUDE to express more complex LTL formulae.


Table 2. LTL Model checker: scenario-dependent properties.

LTL formula
1	[] (aTaskRunning("Confirm Booking") ->
(<>aTaskComplete("Confirm Booking")))
2	[] (aTaskComplete("Handle Payment") ->
(<>aTaskComplete("Confirm Payment")))
3	[] (aBPoolSndMsg("Customer", "Payment") ->
(<>aBPoolRcvMsg("Customer","PaymentConfirmation")))
4.3. MultiQuaTEx query
As later discussed in Section 5.2, MultiVeStA requires queries to be evaluated to 1 or 0 (corresponding to true or false, respectively) in each simulation. Here we present queries used by MultiVeStA based on MAUDE predicates from Section 4.1, modified in order to evaluate to 1 or 0 rather than true or false, respectively. The query used in MultiVeStA to check Option to Complete is given in Listing 8. This is written in MultiQuaTEx, MultiVeStA’s query specification language (Sebastio and Vandin, 2013). The upper part of the listing (Lines 1–8) defines a response operator.10 This evaluates to 1 in a simulation if in the first nSteps simulation steps, every time we first encounter a state where the state observation premise holds, then we also encounter a state where the state observation conclusion holds. In particular, the operator Response takes care of evaluating premise observations, while a second operator (ObsAtStep, Lines 10–17) is used to evaluate conclusion observations. Finally, Lines 10–17 state that we want to study property Response in the first N steps of simulation, using processStart("OrgName") as premise, and processCompletion("OrgName") as conclusion. The parameter N comes from the fact that MultiVeStA considers bounded properties. In all experiments in this paper we have set N as  as we empirically found that it is adequate to consider ‘most’ of the behavior of the models analyzed in this paper. We empirically obtained value 300 by considering some of the largest models in the repositories and performing single simulations and SMC analysis iteratively by increasing the value of such parameter. We chose a value after which the results stabilized.

MultiVeStA runs several simulations and keeps track of the results of the property evaluation. In the lower part of Listing 8 we ask MultiVeStA to make an evaluation of the processCompletion property.


Download : Download high-res image (175KB)
Download : Download full-size image
The query used to check Proper Completion is given in Listing 9. The upper part of the listing specifies the operator Reach, a simpler version of the ObsAtStep one from Listing 8 which focuses on studying just the reachability of states satisfying a given property (does observation obs evaluates to 1?). In particular, Reach evaluates to 1 in a simulation if in any of the first nSteps it meets a state where obs evaluates to 1. Lines 2–3 check whether the condition is met in the current state. Otherwise, if the number of permitted simulation steps has been reached it evaluates to 0 (Lines 4–5), or it triggers the execution of a new step of simulation and re-evaluates Reach in the next simulation step (the # operator in Line 6 is a one-step next operator, which triggers the execution of a step of simulation). MultiVeStA runs several simulations and keeps track of the results of the property evaluation. In the lower part of Listing 9 we ask MultiVeStA to make an evaluation of the noProperCompletion property for simulations of maximum length of N steps.


Download : Download high-res image (76KB)
Download : Download full-size image
The queries used to check No Dead Activities and Safeness are implemented similarly to Listing 9.

Ad-hoc properties may be handled as well by MultiVeStA. As an example, the queries from Table 2 can be expressed as shown in Listings 10, 11 and 12. The first query checks whether a Task identified by a name “TaskName” with status “running” will eventually reach the status “completed”. The second query checks whether the completion of a Task identified by a name “Handle Payment” (with status “completed”) implies the completion of another task named “Confirm Payment” (with status “completed”). The third query checks whether the sending of a message “Payment” from pool “Customer” implies the receiving of a message “Payment Confirmation” from the same pool. The three queries have a similar structure. For example, Listing 10 is defined as Listing 8 with the exception that Lines 19–20 state that we want to study property Response in the first N steps of simulation, using aTaskRunning("TaskName") as premise, and aTaskComplete("TaskName") as conclusion. This property corresponds to Property 1 of Table 2 because aTaskRunning and aTaskComplete alternate for a given task, meaning that a task cannot get twice or more times in status running (resp. complete) without getting in status complete (resp. running).


Download : Download high-res image (177KB)
Download : Download full-size image
Listing 11 is similar to Listing 10, but we modify Response. We use a variable premObs which takes value 1 if we meet a state where the premise holds (Line 7), and is reset to 0 as soon as a state satisfying the conclusion is met (Line 5). In all other states we just perform a new simulation step without modifying the variable Line 8). The evaluation for each simulation terminates as soon as we reach the required simulation steps (Lines 2–3). In particular, we return 1 - premObs, because we want to return 1 in all simulations in which all occurrences of the premise (aTaskComplete("Handle Payment")) are followed by an occurrence of the conclusion (aTaskComplete("Confirm Payment")).


Download : Download high-res image (147KB)
Download : Download full-size image
Listing 12 has same structure as Listing 10. However, we use aBPoolSndMsg("Customer", "Payment") as premise, while for conclusion we use aBPoolRcvMsg("Customer", "Pay mentConfirmation") (see Lines 8–9). This corresponds to Property 3 of Table 2 because a message that has been sent (aBPool SndMsg) should also be received (aBPoolRcvMsg).


Download : Download high-res image (83KB)
Download : Download full-size image
5. Verification
The analysis techniques featured by BProVe are based on the state space exploration capabilities offered by our BPMN interpreter implemented in the MAUDE language.

In fact, given a state of the collaboration under analysis, the interpreter permits to compute its set of one-step next states, i.e. all states reachable from the given state in one execution step, by applying the rewriting rules defining the BPMN semantics (given in Corradini et al., 2018). On the one hand this allows the MAUDE LTL model checker to derive and explore the whole state–space of the collaboration by considering all generated one-step next states. On the other hand it allows the MultiVeStA statistical model checker to generate single execution runs in the form of probabilistic simulations by iteratively selecting in a probabilistic way a one-step next state.

In this section, we overview how the two model checking strategies exploited in BProVe use the information produced by the interpreter to perform the required analysis.

5.1. Verification with the MAUDE LTL Model Checker
As shown in Listing 13, the MAUDE LTL Model Checker takes as input the initial state of the modeled collaboration and an LTL formula. In particular, in this case initialState denotes the term shown in Listing 1, while the considered LTL formula checks the property Option to Complete on the process of the Travel Agency pool.

The MAUDE LTL model checker computes the required analysis by executing our interpreter starting from the given initial state, traversing if necessary the whole state space. The tool returns true if the property is satisfied, i.e. holds in all possible executions of the model. Otherwise it returns false and a counterexample. The counterexample consists in an execution (a sequence of states) that violates the property. As discussed later, other components in the BProVe tool-chain will parse such textual counterexample to map directly on the BPMN model the example of execution violating the property.


Download : Download high-res image (54KB)
Download : Download full-size image
5.2. Verification with MultiVeStA
From non-determinism to probabilistic simulations
Our BPMN semantics, and therefore our MAUDE interpreter, is inherently non-deterministic, meaning that different rules of the semantics might be applied to different enabled components of a BPMN specification, leading to a set of next-step states. As usual (see, e.g. Belzner et al., 2014, Bentea and Ölveczky, 2012, Bruni et al., 2015, Agha et al., 2006, Eckhardt et al., 2012 for some examples in the MAUDE context), in order to obtain probabilistic behaviors out of non-deterministic ones we need to resolve this non-determinism. Two main approaches exist in the MAUDE context: (i) MAUDE specifications can be enriched with probabilities and quantities (obtaining probabilistic rewrite theories (Agha et al., 2006)) and schedulers are used to solve the remaining non-determinism (see e.g. Bruni et al., 2015, Agha et al., 2006, Eckhardt et al., 2012, ter Beek et al., 2016, ter Beek et al., 2015); (ii) or probabilistic strategy languages can be used to associate probabilities to rule applications (Bentea and Ölveczky, 2012, Belzner et al., 2014), computing for each state a probability distribution towards its one-step next states. Both approaches resolve non-determinism by probabilistic choices. Our proposal combines aspects of the two approaches as discussed in the following. Intuitively, with the first approach it shares the use of a Java scheduler that exploits our MAUDE implementation to generate all one-step next states of the current one, and then probabilistically selects one of them (as discussed for Fig. 6, if there are  one-step next states, we select each with probability ). Instead, from the second approach it takes the idea of labeling rule applications with probabilities, as it implicitly specifies the same probability to every rule application outgoing from the current state. We made this choice in order to follow a conservative approach, as we did not have to modify our MAUDE implementation of the BPMN semantics, but we added an external probabilistic scheduler to resolve non-determinism. Crucially, we decided to avoid to use the approach (i) where models have to be enriched explicitly with quantitative aspects (e.g. adding probabilities to the choices of a XOR gateway) because we are not aware of model repositories that contain BPMN models enriched with such quantitative aspects.

MultiVeStA
Similarly to the MAUDE LTL model checker, MultiVeStA takes in input the initial state of the collaboration of interest and a MultiQuaTEx query. When performing an analysis, MultiVeStA interacts with the probabilistic version of our interpreter to run a number of simulations, and provides as a result the average of the evaluations computed for each simulation. We consider properties discussed in Section 4.3, corresponding to those used for the MAUDE LTL model checker. As discussed, each such property takes either value 1 or 0 in a simulation, depending on whether it is satisfied or not in that execution. The number of required simulations is automatically computed in order to give a result with a statistical confidence required by the user. In particular, MultiVeStA estimates MultiQuaTEx queries according to a confidence interval  provided by the user. MultiVeStA estimates the expected value of a MultiQuaTEx query as the mean 
 
 of n samples (taken from n simulations), with n large enough (but minimal) to guarantee that the size of the  confidence interval is bounded by d. In other words, with statistical confidence of , the actual expected value x belongs to the interval 
 
 
.

In order to present in a coherent way the results obtained using the MAUDE LTL model checker and MultiVeStA, if MultiVeStA returns exactly 1 we interpret it as true: the property is verified in all considered executions. Otherwise, we interpret it as false: the property was not verified in at least one of the considered executions.

6. Result report on the scenario
As we know from Section 2, the Soundness property can be encoded, for both the LTL model checker and the statistical model checker, as the combination of the result for three sub-properties: Option to Complete, Proper Completion and No Dead Activities. The result for those sub-properties is combined using the logical AND operator to provide the result for the Soundness property. Notably, these sub-properties refer to a single process; however, the analyses are carried out on the whole collaboration model, so to consider the effects that message exchanges between processes may have on the single process (e.g., the non-reception of a message may cause a process to deadlock). Therefore, the sub-properties are checked for each process in the collaboration and the results obtained are combined through the logical AND operator to provide the result for the whole model.

Table 3 reports the analysis of Soundness and Safeness over our running example from Section 2.1. We can see that the property Option to Complete is not satisfied. This is because the property does not hold for the pool Customer, as the LTL model checker evaluates it to false. The counterexample generated by the LTL model checker is reported in Fig. 7. The model in the figure is the same as in Fig. 2; the only difference is that the executed tasks and the executed sequence flows are highlighted in red. As discussed in Section 7.2, this figure is automatically generated by the graphical component of our tool-chain. Focusing on the Customer pool (the top one), we can see that the Customer process is stuck waiting for the payment confirmation from the Airline pool (the bottom one). This message will never arrive because the Airline process refused the payment. This situation is something that must be avoided, especially if the model will be used for the enactment of such a process and possibly for the development of an application which will remain stuck waiting for a message.


Table 3. Soundness and safeness analysis of the scenario in Section 2.1.

Property	Org/Task name	LTL MC	MultiVeStA
Whole model	F	F
Soundness (i)	Customer	F	0.72
Option to complete	Travel agency	T	1.0
Airline	T	1.0
Whole model	T	T
Soundness (ii)	Customer	T	0.0
Proper completion	Travel agency	T	0.0
Airline	T	0.0
Whole model	T	T
Soundness (iii)	Make travel offer	F	1.0
No dead activities	Check offer	F	1.0
Handle payment	F	0.48
Safeness	Whole model	T	T
Customer	T	1.0
Travel agency	T	1.0
Airline	T	1.0
Considering MultiVeStA, we can see that it confirms the analysis of the Option to Complete property. In fact, it computes value 0.72 for the pool Customer. As discussed in Section 5.2, we can interpret this result as false. Considering Proper Completion, we can see that the LTL model checker evaluates all the corresponding formulae to true, therefore the model satisfies the property. This is confirmed by MultiVeStA, which tells us that none of the considered simulations violates Proper Completion (as none satisfies noProperCompletion). As regards property No Dead Activities, we can see that the LTL analysis states that this property holds in the model. As discussed in Section 4.2, for each task we check that it can never be run in the model, and then we negate the obtained result. The results obtained for each task are then conjoined making No Dead Activities be evaluated to true. This is confirmed also by MultiVeStA. When using MultiVeStA we can check this property directly, obtaining 1.0 in every simulation where the considered task is executed at least once. Given that for all tasks we get a value greater than 0.0, we have that all tasks can be executed, therefore we do not have any dead activity.

Since the soundness property corresponds to combined values of Option to Complete, Proper Completion and No Dead Activities, we have that the considered model is unsound. In addition, we can see that the model is safe. In fact, the corresponding LTL formula is satisfied by each pool. Likewise, MultiVeStA computes value 1.0 for each pool.

Table 4 reports results for the application-dependent properties of our scenario. Property 1 is evaluated to true by both the MAUDE LTL model checker and MultiVeStA, implying that once the Confirmation Booking task is in the running state, it can always reach the completed state. As regards Property 2, we have that the MAUDE LTL Model Checker evaluates it to false. This is because the task Handle Payment can actually be executed without necessarily executing first the task Confirm Payment. This is confirmed by MultiVeStA, which computes value 0.74. Finally, both the MAUDE LTL Model Checker and MultiVeStA state that Property 3 is violated. This is because even if the Customer has sent the payment, it may happen that the corresponding confirmation will never be received.


Table 4. Properties of Table 2 and Listings 10–12 on scenario of Section 2.1.

Property	LTL MC	MultiVeStA
1	T	1.0
2	F	0.74
3	F	0.73
Result interpretation and model fix
Once the analysis results have been interpreted, eventually discovered bugs should be fixed running again the model design activity, triggering a new iteration of the model development cycle in Fig. 3. Our analysis discovered that property Option to Complete is violated for the process of pool Customer. Fig. 8 depicts a new version of the model focusing only on the parts of the model that have been changed to fix the problem. The fix has been driven by the counterexample in Fig. 7. In the new model, after the payment the Customer waits for the payment management involving Travel Agency and Airline. This is represented by means of an event-based gateway. If the Airline rejects the offer, the Customer marks the travel as not paid as soon as the reject message is received. Otherwise, the payment is confirmed to the Customer. This new model successfully passes the verification of the Option to Complete property, as well as of the overall Soundness property. More in general, a new execution of Property Verification and Result Exploration activities confirms that the new version of the model satisfies the expected and desirable properties.

7. BProVe tool-chain
In this section we provide details on the architecture and usage of the BProVe verification tool-chain.

7.1. Architecture
BProVe is a tool-chain of open source software. For using the tool-chain we provide a web interface at http://pros.unicam.it/bprove-web-interface/ as well as a VirtualBox virtual machine containing a local installation of our tool-chain at http://pros.unicam.it/bprove-testing-machine/. Additional information about the tool-chain can be found at http://pros.unicam.it/bprove.

The overall architecture of the BProVe tool-chain is synthesized in the Component Diagram of Fig. 9. It is based on a standard client/server architecture concerning the provisioning of APIs for a BPMN verification service. Reading the figure from left to right we can see the following main components: Modeling Environment, BProVe WebService, MAUDE Model Checker, MultiVeStA and BPMN Interpreter.

The Modeling Environment component allows to design BPMN models and to select or specify properties to be verified. In principle, any BPMN editor can be used as such component, especially those compliant with the BPMN 2.0 standard such as: bpmn-js,11 Eclipse BPMN2 Modeler,12 Camunda Modeler13 and Signavio Editor.14 Currently, we offer two different deploys of the tool-chain using two different BPMN editors. The first one, based on bpmn-js, allows us to make BProVe available as a web application. The latter is distributed as an Eclipse plug-in and is based on the Eclipse BPMN2 Modeler. Both modeling environments have been augmented with the possibility to make calls to the BProVe WebService and to graphically interpret answers and counterexamples received from it. This de facto enriches the modeling environments with advanced analysis capabilities for designed models. In particular, we provide a menu to perform verification by selecting one of the two supported model checkers. BProVe offers a set of predefined properties that the user can select and verify without requiring any specific knowledge of the used analysis techniques. In addition, for the MAUDE LTL model checker we allow to specify LTL formulae describing application-dependent properties. In this case the user might use the predicates defined in the MAUDE interpreter discussed in Section 4.1.


Download : Download high-res image (142KB)
Download : Download full-size image
Fig. 9. Component diagram of the BProVe tool-chain.

The Modeling Environment interacts with the BProVe WebService via HTTP requests. In particular, a verification request has to include a BPMN model formatted according to the OMG standard and a property to be checked on the model. After the reception of the request, the service formats the BPMN model and the property into the syntax accepted by the back-end components, and then passes such data to the requested model checking components (MAUDE Model Checker or MultiVeStA). The results of the model checkers are successively formatted in an XML file that is returned to the modeling environment which visualizes the results in natural language. In case a counterexample is returned to signal a property violation, this is visualized directly on the model highlighting the erroneous path.

The remaining components offer the supported analysis techniques as discussed in Section 5. The MAUDE LTL Model Checker component consists of a running instance of MAUDE (Clavel et al., 2007) loaded with the MAUDE modules implementing our BPMN interpreter15 and the LTL Model Checker embedded in MAUDE (Eker et al., 2004). Instead the MultiVeStA component consists of a running instance of the statistical analyzer MultiVeStA, which interacts with the probabilistic version of our interpreter discussed in Section 5.2.

7.2. User interface
The BProVe user interface aims at fostering the usage of formal verification for BPMN models also to non-experts of formal methods. In particular, after having designed or loaded a model in the modeling environment, the user can access the BProVe functionalities by just pushing a button we added to the toolbar. As a result, a menu that requires to select a model to parse is displayed. After the parsing request is sent to the BProVe WebService, a parsed model is returned. At this point, the user can request property verifications, and if he/she does it, the “BProVe Verification” menu pops up as shown in Fig. 10.

The menu enables to select one of the supported properties (or define a new one) according to different needs and expertise of the user. In particular, the menu includes three different sections permitting to specify properties in different ways: (i) general domain properties, (ii) application-dependent properties, and (iii) properties builder. The general domain properties section embeds a drop-down menu permitting to select properties that are generally desirable for any BPMN model. In particular, the user can ask for the verification of the soundness and safeness properties discussed in Section 4. Those properties are evaluated over the whole model. The application-dependent properties section embeds several drop-down menus which allow the user to select specific elements in the model (i.e., Pools, Tasks, and Messages) to check if specific conditions on them are satisfied. In particular, the “Verification of Task execution” allows to select a Task and to check whether it will ever reach the statuses “Enabled”, “Running” or “Completed”. It also allows to verify whether the execution of a Task implies the execution of another one. The “Verification of Message exchange” allows to select Pools and Messages and to check whether a specific Pool will send a specified Message, or to check whether a specific Pool will Receive a specified Message. The section named “property builder” is meant for highly skilled users able to write LTL formulae. In specifying the property, the user can make use of the LTL MAUDE operators and can take advantage of predefined subformulae (see Section 4).

The last section, named “MultiVeStA Simulation”, allows to switch to another menu which includes the same sections previously described, but with the possibility of running property verifications with MultiVeStA rather than with the MAUDE LTL Model Checker.16

Finally, verification results are reported in a text area where the user can also find the time, in milliseconds, needed to run the verification. In case the property is violated, the BProVe service returns a counter-example that can be visualized on the original model highlighting the model elements belonging to the corresponding execution path as shown in Fig. 10.

8. Experimental evaluation
In this section, we discuss the validation we performed to assess the scalability and feasibility of the BProVe tool-chain, considering how the standard and the statistical model checking techniques complement each other. By scalability we mean the ability of BProVe to handle the verification of BPMN models with increasing size. This allows us to assess to what extent the tool can deal with large models and how its performance degrades as the size of models increases. By feasibility, instead, we mean the ability of BProVe to be applicable to real(istic) scenarios. This allows us to assess whether the tool may be useful in practice.

The evaluation we have performed has been then structured over two main steps. The first one extensively validated the proposed verification approach by running a scalability analysis via ad-hoc designed and synthetically generated models. The rationale in using synthetic models was to do a scalability analysis on models on which we have full control and to show the tool capabilities in extreme scenarios.

The second one considered models retrieved from two freely accessible collections of models: the “BPM Academic Initiative Model Collection” and the “Camunda BPMN for Research”. At the time of writing, BPMAI and CAMUNDA are the most used open repositories of BPMN models to the best of our knowledge. Different practitioners and students have designed the models in the repositories during BPMN training sessions; therefore, they vary in size and usage of the BPMN notation. The rationale in using existing repositories of BPMN models is to assess the feasibility of BProVe, showing that it can also process real(istic) models designed by third-party, without knowing a priori the quality of the designed models.

Experiments have been performed on a dedicated machine running Ubuntu Linux 18.04.3, equipped with a processor Intel Core i7-6700HQ (2.60 GHz), and 8 GB of RAM.

More details on the set-up and the results of the performed experiments are described in the following. For the LTL model checking analysis we used MAUDE 2.7.117 loading our BPMN interpreter. For the statistical analysis we used MultiVeStA, which interacted with the same version of MAUDE, and with the probabilistic version of our interpreter. MultiVeStA requires a number of parameters, which we fixed for all models: we set a block-size of 30 in relation to how many simulations we ask to run before iteratively re-evaluating if the required statistical confidence has been reached. In case the desired confidence interval is not reached after performing a block of simulations, MultiVeStA runs another block of simulations and repeats the checks. We set the  and  values describing the required confidence interval (see Section 5.2) to 0.1 and 0.15, respectively.18 We used the automatic parallelization feature provided by MultiVeStA Pianini et al., 2014) using 3 cores of the machine to parallelize the simulations. Finally we defined a time limit of 600 s considering it as the maximum amount of time a user, adopting the development model presented in Section 3, may wait for receiving a response from the tool; we use this time limit to run our tests and to timeout the computations that exceed such limit.

8.1. Experiment set-up on synthesized models
To conduct a scalability analysis of our approach on models of increasingly large ‘size’, we have specifically designed some families of synthetic models. The need to devise such families comes from the fact that, although the typical notion of size for a BPMN model refers to the number of its elements, here we are more interested in the number of its execution states, which is what mainly affects the verification performance. The number of states of BPMN models may grow not only based on the growth of the number of elements but also on how they are structured. To cope with this aspect, we identified four ‘dimensions’ of growth for a BPMN model:

•
sequential growth, where the number of sequential elements of the model increases;

•
nesting growth, where the number of nested element blocks increases;

•
internal parallel growth, where the number of parallelbranches within an AND block increases;

•
external parallel growth, where the number of processes within a collaboration increases.

Therefore, we defined the following families of models whose elements systematically increase along with one of these dimensions. In particular, for the sequential growth, we considered a family of process models with increasing sequential tasks. We considered a family of process models with three branches of exclusive split and joined gateways with varying nesting for the nesting growth. For internal parallel growth, we considered a family of process models with a block of parallel split and joined gateway with varying branches. Finally, we considered a family of collaboration models with a pool sending and/or receiving a message with varying number of pools for the external parallel growth. Each family contains 6 models. In all the considered models we added a block of elements that will lead the process into a deadlock. As a consequence, all the designed models violate the Option to Complete and the No Dead Activities properties, and therefore they are not sound. The error block is reported in Fig. 11 and it has been added in a place that could make hard its identification when the model grows in size. This should permit us to identify possible limits of the different verification strategies. We made all the models available through the RePROSitory platform (Corradini et al., 2019).19

The second family, Nested Exclusive Branches, is reported in Fig. 13. The baseline model contains a start event connected to an exclusive split gateway with three branches. Two of such branches are connected with a task per branch that merge into an exclusive join gateway, which is in turn connected with an end event. We say that those elements have a level of nesting equal to zero. The third branch is attached to the elements at the next level of nesting. At the highest level of nesting, level N in Fig. 13, we placed the error block from Fig. 11. We designed the remaining models by increasing level of nesting, where the elements in higher levels of nesting are structured in the same way as those in level 0.


Download : Download high-res image (42KB)
Download : Download full-size image
Fig. 12. Model with sequential tasks.

The third family, Parallel Branches, is reported in Fig. 14. The baseline model is composed of a start event connected to a parallel split gateway with two branches and a single task per branch; the two branches successively merge into a parallel join gateway. At this point we placed the error block, then an end event that closes the model. We designed the remaining models by increasing the number of branches and tasks between the two parallel gateways.


Download : Download high-res image (125KB)
Download : Download full-size image
Fig. 13. Model with nested exclusive branches.

The fourth family, Process Collaboration, is depicted in Fig. 15. The baseline model is composed of three pools. The first one presents a start event, a send task, a receive task, the error block and an end event. The second one presents a start receiving message event, a send task and an end event. The third one presents a start event, a send task and an end event. The remaining models are obtained by inserting iteratively one copy of the intermediate pool before the last pool. In all models, the pools interact in sequence: the first pool sends a message to the second one, which sends a message to the third one, and so on, until getting to the th pool that sends a message back to the first pool.


Download : Download high-res image (63KB)
Download : Download full-size image
Fig. 14. Model with parallel branches.

8.2. Results on synthesized models
By analyzing the synthetic models from Section 8.1 we performed a systematic analysis of the feasibility of our approach for both analysis techniques. In order to study how the analysis engines perform in terms of the actual size of the models, we used the state space generation capabilities offered by MAUDE (Clavel et al., 2007) to compute the size of the state space of each model. This information, provided in the columns states in the tables presenting the analysis results, is crucial to properly study the scalability of the approach; in fact, the cost of verification depends on the size of the semantic model, which is due not only to the number of elements in the BPMN diagrams, but also to the degree of parallelism and non-determinism.


Table 5. Experimental results over synthetic models with the MAUDE LTL model checker and MultiVeStA.

Model	Option to complete	Proper completion	No dead activities	Safeness
ID	States	LTL MC	MultiVeStA	LTL MC	MultiVeStA	LTL MC	MultiVeStA	LTL MC	MultiVeStA
Res.	ms	Res.	ms	Res.	ms	Res.	ms	Res.	ms	Res.	ms	Res.	ms	Res.	ms
Experimental results over Sequential Tasks models
t1	16	F	1	F	926	T	1	T	973	F	62	F	4448	T	2	T	973
t2	20	F	1	F	981	T	2	T	980	F	68	F	5757	T	2	T	1018
t3	24	F	2	F	1018	T	3	T	1041	F	63	F	7176	T	3	T	1022
t4	28	F	3	F	1036	T	3	T	1103	F	67	F	8779	T	4	T	1099
t5	32	F	3	F	1083	T	4	T	1174	F	64	F	10441	T	5	T	1159
t6	36	F	4	F	1141	T	5	T	1225	F	65	F	12116	T	6	T	1167
Experimental results over Nested Exclusive Branches models
e1	12	F	1	F	880	T	1	T	844	F	65	F	2036	T	1	T	867
e2	27	F	3	F	1310	T	5	T	1000	F	68	F	5224	T	6	T	943
e3	41	F	7	F	1513	T	13	T	1050	F	74	F	7960	T	14	T	1022
e4	55	F	12	F	1178	T	24	T	1076	F	84	F	8981	T	26	T	1090
e5	69	F	18	F	1072	T	40	T	1123	F	99	F	5037	T	42	T	1129
e6	83	F	26	F	1123	T	61	T	1175	F	114	F	8712	T	64	T	1154
Experimental results over Parallel Branches models
p1	54	F	4	F	1064	T	13	T	1072	F	73	F	5946	T	14	T	1121
p2	314	F	9	F	1200	T	176	T	1198	F	231	F	8641	T	172	T	1219
p3	2014	F	21	F	1397	T	2183	T	1396	F	2176	F	12932	T	2194	T	1416
p4	12 514	F	44	F	1790	T	24 659	T	1861	F	29 266	F	19026	T	24 773	T	1804
p5	T.O.	F	81	F	2436	T	257 662	T	2359	F	104 030	F	29868	T	260 008	T	2229
p6	T.O.	F	143	F	3093	N.A.	T.O.	T	3177	N.A.	T.O.	F	45118	N.A.	T.O.	T	3255
Experimental results over Process Collaboration models
c1	1520	F	4	F	1340	T	656	T	4202	F	474	F	8592	T	449	T	4148
c2	10 025	F	6	F	1585	T	3513	T	6272	F	3284	F	10955	T	3470	T	6184
c3	62 250	F	8	F	1807	T	25 967	T	9191	F	25 339	F	13 649	T	26 508	T	8925
c4	371 875	F	10	F	2043	T	186 592	T	12728	F	185 538	F	16 746	T	195 246	T	12382
c5	T.O.	F	14	F	2306	N.A.	T.O.	T	16825	N.A.	T.O.	F	20 294	N.A.	T.O.	T	16315
c6	T.O.	F	20	F	2612	N.A.	T.O.	T	21887	N.A.	T.O.	F	24 057	N.A.	T.O.	T	21231
Table 5 reports the verification results, including running times in milliseconds, for our four families of synthetic models. To facilitate the comparison, for each experiment we highlight in bold the best performance between those of the two engines.

Sequential tasks & nested exclusive branches.
The first two blocks of lines provide the verification results for the families Sequential Tasks and Nested Exclusive Branches, respectively. From a qualitative perspective, the two analysis engines always compute the same, correct, results.20 Considering performances, for these two families the LTL MAUDE Model Checker consistently outperforms MultiVeStA. This is because the selected models have very small state spaces, containing up to 83 states. In these cases, the LTL model checker is able to quickly explore all states and provide a response faster than MultiVeStA. Exhaustive analysis techniques like LTL model checking are particularly well suited in these cases.

Parallel branches & process collaboration.
Moving our attention to the families Parallel Branches and Process Collaboration, shown in the third and fourth block of lines of Table 5, we note a complementary outcome. We can observe that the models of such families have much larger state spaces. In fact, an increase in the number of parallel branches or of pools leads to an exponential increase on the size of the state space, leading to the well-known state space explosion problem. In some cases, such a state space could not even be fully explored within our time limit of 600 s, forcing us to add the label T.O. (Time Out) in the corresponding entries. For these two families, the MAUDE LTL model checker succeeded in performing the verification of all the properties within the time limit of 600 s only on models with less than 6 tasks in parallel or 5 pools. We write T.O. (Time Out) in the entries corresponding to analysis that did not terminate in the imposed time limit and N.A. (Not Available) to indicate that a result is not available for that property verification. An exception is the Option To Complete property, which could be computed on all models by the LTL model checker. For such a property the MAUDE LTL model checker is able to find a counterexample by considering only a small portion of the state–space, so we have a response on all models without ever generating their full state spaces. This is enabled by the fact that the model checker adopts an on-the-fly approach to state–space generation (Grumberg et al., 1999) where the state–space is generated at need while performing the analysis. Apart for this property, MultiVeStA offers better runtime for all models with more than about 10 000 states, and computes the correct result also in the cases where the LTL model checker fails to complete in the time limit. Even if not shown in the table, MultiVeStA was able to analyze within the chosen time limit all properties up to the twentieth instance of each family.

8.3. Experiments on models from open repositories
As a proof of concept, we also run a validation of the BProVe approach on models from two open repositories “BPM Academic Initiative Model Collection”21 and “Camunda BPMN for Research”.22 We have chosen those repositories because they have been already successfully used for validation purposes elsewhere (e.g., Bergmann and Müller, 2018, Wiśniewski and Ligkeza, 2018, Skouradaki et al., 2015, Lapeña et al., 2018, Schoknecht and Oberweis, 2017). In Corradini et al., 2017a, Corradini et al., 2017b we used the former repository to evaluate a preliminary version of our approach and tool-chain, involving only LTL model checking.

We started from a set of 7639 models from the BPMAI repository and 3739 from the Camunda repository. From these sets we filtered out models that could hinder the validity of our experiment. Especially, we filtered out syntactically invalid models and those that presented issues in the usage of the BPMN syntax (e.g., elements drawn without a specified source or a target, or some sequence flows crossing pool boundaries). The sets have been reduced respectively to 6304 and 2979 models. Given that our focus is on collaboration models, we filtered out from the remaining models those with less than 5 elements (which are typically required to form a meaningful collaboration, i.e. a pool including a start, a message task and an end event, interacting with an empty pool) resulting in 5336 and 2961 models. We also removed models presenting disconnected elements (arguably they do not represent realistic models) ending up with 4379 models for the BPMAI repository and 2232 models for the Camunda repository. Out of the final filtered sets of models, we successfully processed 2544 and 713 models, respectively. The remaining models could not be handled by BProVe due to the presence of not supported elements (e.g., timer events, conditional events, and compensation events).

In conclusion, we analyzed 3257 models which we classify in Table 6 according to the size of their state spaces. We analyzed all such models using both our analysis engines. For each class of models, we write in column Best Analysis Strategy the one that best performed on average as discussed below. We note that the majority of the models were concentrated in the first three classes 1–99, 100–999 and 1000–9999, meaning that their state space did not pose a threat for the LTL model checker so making it a valid option for such models. The about 50 models with a state space of size between 10 000 and 79 000 could be analyzed as well with the LTL model checker, but MultiVeStA offered smaller running times. The about 140 models with state space greater than 79 000 could not be analyzed by the LTL model checker within the time limit of 600 s. We deepened our analysis by manually inspecting the models within class 79 000. We found that all of such models, not surprisingly, presented a high degree of parallelism resulting from the use of such BPMN elements as AND gateways, OR gateways, and Pools with message exchange. Given the interleaving nature introduced by these elements on the resulting behavior, models become difficult to handle using an LTL model checker which exhaustively explore the state space. The introduction of MultiVeStA allowed us to provide a response also for such kind of models. We report in Table 7 the results for the properties verification over the analyzed models of the two open repositories.


Table 6. Classification of models from open repositories according to number of states and best performing analysis strategy.

Number of states	Number of models	Best analysis strategy
BPMAI	Camunda	
1–99	1838	72.2%	52	7.3%	LTL MC
100–999	406	16.0%	502	70.4%	LTL MC
1000–9999	135	5.3%	132	18.5%	LTL MC
10 000–79 000	37	1.5%	16	2.3%	MultiVeStA
79 000	128	5.0%	11	1.5%	MultiVeStA

Table 7. Percentages of the BPMAI and Camunda models satisfying the properties.



8.4. Highlights from the experiments
The validation performed on the synthetic models highlights that MultiVeStA offers better run-time for all models with more than about 10 000 states (especially those presenting a high degree of parallelism), and computes the correct result also in the cases where the MAUDE LTL model checker fails to complete in a fixed time limit. The MAUDE LTL model checker instead performs better on sequential models, where the usage of an on-the-fly approach to state–space generation allows to find a counterexample by considering only a small portion of the state–space, so providing a response without ever generating the full state space of a model. The validation performed on the two open, widely used repositories confirmed the advantages of having complementary analysis approaches, so to enlarge the set of models for which it is possible to get a feedback. The results thus justify our choice of adding a statistical strategy to the analysis facilities of BProVe. Furthermore, as somehow expected, this suggests that there is not a best analysis strategy among the two, but that they should be selected depending on the characteristics of the considered models.

9. Related work
The analysis of business processes is a largely investigated topic, and different approaches for the analysis of BPMN models are available in the literature (e.g., Morimoto, 2008, Groefsema and Bucur, 2013, Fellman and Zasada, 2014, Souri et al., 2018). In this section we split the research works related to our into two categories: () those that map the model to another notation for which analysis techniques have already been consolidated, and () those that define a custom semantics for BPMN and that develop specific tools for the analysis.

9.1. BPMN process analysis via mapping to other formalisms
The most common formalizations of BPMN resorting to well-known formalisms are given through the definition of mappings into Petri Nets (Dijkman et al., 2008, Huai et al., 2010, Koniewski et al., 2006, Ramadan et al., 2011, Awad et al., 2010, Xiu et al., 2017, Mutarraf et al., 2018, Heinze et al., 2018). The Petri Net resulting from the encoding of a BPMN model can serve as input to a Petri Net based verification tool. The ProM platform is probably the most used environment enabling such kind of strategy, since it can embed many components adopting an encoding-based approach to verification (van Dongen et al., 2005).

To analyze a BPMN model with ProM a user has to perform the following steps: 1. design a model with an external tool and export it in the BPMN format; 2. import the BPMN model into ProM, which then translates it into an internal PROM format; 3. choose one of the mappings available in ProM, and request the generation of the corresponding Petri Net; 4. select the resulting Petri Net, and then choose one of the analysis tools available in ProM (e.g. Woflan Verbeek et al., 2001) to analyze the Petri Net model; 5. read and interpret the responses from the analysis tool, which are referred to the Petri Net model, and interpret them back on the original BPMN model. Clearly, the most critical steps in this chain are 3, 4 and 5. Step 3 is critical due to the possible absence of a rule in the chosen mapping for a specific BPMN element included in the model. In such a case the tool may miss to generate the Petri Net, or may generate a Petri Net that defines a behavior not fully representative of the original model. Indeed, while for the basic BPMN modeling elements the encoding in Petri Nets is rather straightforward, for other elements such encodings could be cumbersome and quite challenging to define. For example, the management of termination end events, permitting to abort a running process, is usually not supported by such encodings. This is due to the inherent complexity of managing non-local propagation of tokens in Petri Nets. In addition, at the time of writing, no mapping tool available in ProM is able to properly translate BPMN Collaboration Diagrams due to the lack of mapping rules for message exchanges. Moreover, the existence of different mappings poses the problem of choosing one among them for the analysis of a given BPMN model, or it may lead the user to make more tentatives using the various mappings, which may produce contrasting verification results. Step 4 is critical since the possibility to define ad-hoc properties is often limited, only general properties are typically supported, and anyway when this is possible the properties will refer to the elements in the Petri Net and not to the original BPMN model. Step 5 is critical since, as said above, in case of a property violation the result has to be reconducted to the original BPMN model directly by the user.

Other approaches are available in the literature. The BPA Analyzer, defined as a module for the tool PromniCAT,23 supports analysis based on BPMN models reported in a specific JSON format. The models are translated into Petri Nets, and the LoLA analyzer is used for verifying properties expressed in CTL (Eid-Sabbagh et al., 2013).

Corradini et al. supported business process verification transforming BPMN 1.2 models into CSP specifications (Corradini et al., 2010a). The solution presents an Eclipse based tool-chain BP4PA24 integrating the modeling environment with the PAT modelchecker (Corradini et al., 2010b). The solution has been tested on simple business processes related to the Public Administration. Tools supporting the transformation from BPMN to YAWL Nets are also available. Ye and Son implemented an open-source plug-in called BPMN2YAWL that uses ILog BPMN Modeler as a graphical editor to create BPMN models, and implements transformation and verification as ProM 5.0 plug-in (Ye and Song, 2010). Both the modeler and the verification tool seem not anymore available.

Kheldoun et al., 2015, Kheldoun et al., 2017 proposed an encoding of BPMN in Recursive ECATNets, expressed in terms of rewriting logic and analyzed using the MAUDE LTL model checker. Even if we use the same model checker, the approach suffers from the mentioned encoding problems, and does not consider neither messages nor the event-based gateway. Moreover, the approach is tested on three simple examples only, without extensively validating it.

In summary, to the best of our knowledge, our proposal is the only one fully satisfying the relevant features we listed in Section 1, while all the proposals in this subsection share to some extent the issues described for the ProM based approaches.

9.2. BPMN process analysis via direct formalizations
Not many are the approaches that permit to directly conduct analysis activities over collaborations expressed with the BPMN notation. The approach that relates the most to our is the one proposed by Houhou et al. (2019), and that is supported by a tool called fbpmn.25 The authors give a direct formalization in First-Order Logic, which is then implemented in TLA+ to enable formal verification using the TLC model checker from the TLA+ tool box. As it is the case of BProVe, also fbpmn allows to evaluate properties such as option to complete, proper completion, no dead activity, safety, soundness and message-relaxed soundness. In addition, it includes the possibility to select seven different communication semantics for message exchanges, feature that is not supported in BProVe. Nevertheless, our approach supports the user in the specification of ad-hoc properties based on the specific scenario of the BPMN model, and it allows to be extended to include new properties like the ones reported in Section 4. This is not possible in fbpmn.

We tested the fbpmn tool over models of the Parallel Branches and Process Collaboration families in Section 8.1, focusing on those with a high number of states for which the BProVe implementation with the LTL Model Checker could not provide an answer. However, we have to specify that the implementation of the BPMN semantics that stand at the basis of the two approaches are different, so performance can vary based on the amount of details reported in the implementation. For instance, with respect to fbpmn, in our semantics we have the possibility to express different statuses for tasks (enabled, running, completed, etc.) which allows us to express more detailed properties (e.g. implication between the execution of a task and another). This, on the other hand, contributes to increase the state space of a model. To provide a fair comparison between the two tools, we selected in fbpmn the Bag communication semantics, that corresponds to the one implemented in BProVe. In addition, since the implementation of the Soundness and Message Relaxed Soundness properties is slightly different between the two tools, we limited our experiments to the Safeness property. In Table 8 we reported the results for the verification of the Safeness property obtained with fbpmn and those obtained by the BProVe implementation based on MultiVeStA. The tools provided the same result for the safeness property (in fact, all considered models are safe), but they required different amounts of time. BProVe resulted capable of handling models of the Parallel Branches family always in less time then fbpmn. For the Process Collaboration family, fbpmn is able to process models up to c12 in less time than BProVe, while BProVe starts outperforming fbpmn from model c13, thus demonstrating a better scalability. Indeed, by increasing the number of parallel branches in the former family, or the number of pools in the latter, the verification time of fbpmn increases significantly faster than that of BProVe.

When comparing the whole tool-chains, it is worth mentioning that fbpmn requires to perform several installation steps before actually being able to use it. In addition, it comes as a command line tool hardly usable by business process practitioners. Our tool-chain, instead, is immediately available to the final user, who can access it from the previously discussed web front-end. Moreover, all the components of our tool-chain are open-source, and a full installation is provided in a downloadable virtual machine, permitting to anyone to test every part of it without the need to follow complex installation procedures. Finally, fbpmn includes some low level technical constraints that hinder its usability; for example, the collaboration id must be equal to the name of the .bpmn file, which is not common especially with the Camunda modeler used in the fbpmn’s tool-chain.


Table 8. BProVe MultiVeStA and fbpmn results for the safeness property.

Model	BProVe MultiVeStA	fbpmn
Result	Time (ms)	Result	Time (ms)
Parallel Branches models
p6	T	3255	T	3277
p7	T	4483	T	5550
p8	T	5648	T	12945
p9	T	7703	T	42340
Process Collaboration models
c6	T	21231	T	3589
c7	T	26596	T	4759
c8	T	33029	T	7766
c9	T	36245	T	13636
c10	T	40995	T	10749
c11	T	43654	T	18830
c12	T	45091	T	32786
c13	T	50833	T	77262
c14	T	65120	T	156696
In Wong and Gibbons, 2008, Wong and Gibbons, 2011 the authors present a tool,26 implemented in Haskell, that relies on a translation from a subset of well-formed BPMN 1.2 process diagrams to a CSP-like language based on the Z notation. The tool permits to check message-based properties, like consistency between BPMN diagrams with different levels of abstraction and compatibility between participants within a business process collaboration. The benefits of the proposed solution were illustrated only through simple scenarios, without an extensive validation like the one we conducted on BProVe using synthetic and real-word models. This tool, differently from BProVe, does not bring back the verification results directly on the analyzed BPMN model. In addition, the tool usage is limited to models designed with the ILOG JViews BPMN Modeler which is not anymore available, making the comparison with this tool unfeasible.

9.3. Detailed comparison with other approaches
Table 9 reports the list of all the BPMN elements that the various approaches are able to handle. The symbol X means that the element is correctly represented, X* means that the element is represented but it is mapped to its simplest variant (e.g., Start Receive Message Event is mapped and treated as a simple Start Event, User Task is mapped to a simple Task, etc.), while a cell left blank means that the element is not handled. Columns M0, M1, M2, M3 refer to four mappings supported by ProM. M0 (Select BPMN Diagram) is a plugin by H.M.W. Verbeek that allows ProM to convert a BPMN file into a BPMN Diagram and then apply other plugins over such representation; M1 (Convert BPMN diagram to Petri net), M2 (Convert BPMN Diagram to Data Petri net) and M3 (Convert BPMN to Petrinet) are plugins respectively by D. Fahland, A. Kalenkova and D. Fahland, and R. Conforti, that can be used to map BPMN Diagrams into Petri Nets. All the other approaches have been already discussed in Sections 9.1 BPMN process analysis via mapping to other formalisms, 9.2 BPMN process analysis via direct formalizations. Table 9 also highlights the subset of the supported BPMN notation. We notice that few elements are supported by all the approaches emphasizing a core set of BPMN elements composed of None Start and None End events; Task activities; Parallel and Exclusive gateways; and Sequence flows. Instead, the less supported elements (that in our table are considered by less than four approaches) are: Signal Start and Conditional Start events; Terminate End events; Conditional, Signal and Escalation Intermediate events; User, Manual, Script, Service Rule Based Activities; Data Objects; Conditional Default and Association Flows. Overall, the table underlines the lack of a comprehensive approach supporting the whole list of elements. Our approach performs quite well with respect to BPMN elements covering 61% of the elements in the table, and only (Houhou et al., 2019) performs better covering 68% of the elements.


Table 9. BPMN elements supported by approaches based on encodings or on direct semantics. (X stands for a correctly represented element, X* for an element represented but mapped to its simplest variant, a blank cell for an unhandled element.) .

Encodings	Direct semantics
Notation element	M0	M1	M2	M3	Eid-Sabbagh et al. (2013)	Corradini et al. (2010a)	Ye and Song (2010)	Kheldoun et al. (2015) and Kheldoun et al. (2017)	Houhou et al. (2019)	Wong and Gibbons (2008) and Wong and Gibbons (2011)	BProVe
Start events	None	X	X	X	X	X	X	X	X	X	X	X
Receive message	X	X*	X*	X*	X*	X		X	X	X	X
Conditional										X	
Timer	X	X*	X*	X*	X*				X	X	
Signal										X	
End events	None	X	X	X	X	X	X	X	X	X	X	X
Send message	X	X*	X*	X*		X		X	X	X	X
Terminate event									X	X	X
Error	X	X*	X*	X*				X		X	
Intermediate events	None throwing	X	X	X		X						
Message send	X	X*	X*			X		X	X	X	X
Message receive	X	X*	X*			X	X		X	X	X
Timer	X	X*	X*			X	X	X	X	X	
Error (catch)						X	X	X			
Error (throw)							X	X			
Conditional										X	
Signal							X				
Escalation								X			
Activities	Task	X	X	X	X	X	X	X	X	X	X	X
Send	X	X	X	X		X			X		X
Receive	X	X	X	X		X			X		X
User									X		X*
Manual									X		X*
Script									X		X*
Service									X		X*
Rule based									X		X*
Loop	X	X	X	X			X	X		X	
Multiple instance	X	X*	X*	X*			X	X		X	
Subprocesses	X	X*	X*	X*	X		X*	X*	X	X*	X*
Gateways	Parallel	X	X	X	X	X	X	X	X	X	X	X
Exclusive	X	X	X	X	X	X	X	X	X	X	X
Inclusive (split)	X	X	X	X		X	X		X		X
Inclusive (join)	X	X	X	X			X		X		
Event-based	X	X*	X*			X	X	X	X		X
Data	Data object								X			
Flows	Sequence	X	X	X	X	X	X	X	X	X	X	X
Conditional							X		X		X*
Default							X		X		X*
Message	X				X	X		X	X	X	X
Association								X			
Swimlanes	Collaboration pool	X					X			X	X	X
Table 10 summarizes the main characteristics of theapproaches relying on encodings from BPMN to another notation (under the column Encodings) and of the ones relying on the definition of a direct semantics for BPMN (under the column Direct Semantics). With the term Predefined Properties we refer to such properties that can be verified over the encoded model; Custom Properties refers to the possibility to verify model dependent properties and the possibility to encode new properties; Tool Support specifies which tool, if any, supports the approach (N.A. stands for Not Available); Diagnostic specifies how verification results are reported to the user (e.g. Graphical means that a violating trace is shown directly over the graphical representation of the analyzed model); Necessary Background refers to the formalism and tools required to use the approach, in the case of BProVe some knowledge on how to compose LTL formulae is required only for adding and verifying custom properties that the user may want to verify over a model; Installation Process indicates if a user has to install the tool by following an automatic procedure, a manual procedure (meaning that each component of the tool must be installed separately by the end user), or if he/she does not have to install it at all; Automated verification indicates whether the property verification is fully automatized, meaning that only few clicks are needed to request the property verification, or if the user has to perform some additional steps before achieving property verification.


Table 10. Main features of the considered analysis approaches for BPMN models.

Approach	Encodings	Direct semantics
van Dongen et al. (2005)	Eid-Sabbagh et al. (2013)	Corradini et al. (2010a)	Ye and Song (2010)	Kheldoun et al., 2015, Kheldoun et al., 2017	Houhou et al. (2019)	Wong and Gibbons (2008) and Wong and Gibbons (2011)	BProVe
Predefined properties	Option to complete, Proper completion, No dead activities	Soundness, Weak soundness, RelaxedSoundness, Dead transitions, Uncovered transitions, Unbounded places, Quasi liveness, Liveness, Transitioncover, isBounded, isCyclic, isFreeChoice, isExtendedFreeChoice, isSNet, isTnet, isWorkflowNet	Coordination, Control, Sharing, Transparency	Soundness, Deadlock freedom, No dead task, Proper completion, No OR-join	Soundness, Proper termination	Soundness, Message relaxed soundness, Safeness	Absence, Universality, Existence, Bounded existence, Order, Precedence, Response, Chain precedence, Chain response, Deadlock freedom	Soundness, Option to complete, Proper completion, No dead activities, Message relaxed soundness, Safeness
Custom properties								Supported
Tool support	Woflan plugin	PromniCat module	Eclipse plugin	N.A.	N.A.	Standalone	N.A.	Web App, Eclipse plugin, Standalone
Diagnostic	Textual	Textual	Textual & Graphical		Textual	Graphical	Textual	Textual & Graphical
Necessary background	PN, ProM	PN	PAT	PN, ProM	LTL, Maude RECATNet	Haskell	CSP, Haskell	LTL (for custom properties)
Installation process	Mandatory, Automatic	Mandatory, Manual	Mandatory, Automatic	Mandatory, Automatic	Mandatory, Manual	Mandatory, Manual	Mandatory, Manual	Not mandatory
Automated verification	Partial	Partial	Full	Partial	Partial	Partial	Partial	Full
For the two tables it is evident that there is not a single best approach. A user may chose the approach based on its main features (e.g., list of properties it can verify, possibility to encode his/her own properties, etc.), based on the BPMN elements used in the model, or even based on the usability of the supporting tool. In our case, a tool like BProVe that requires no installation for being exploited and that allows for a fully automated verification of properties, hiding the formalism which it relies on, could reasonably be considered easier to use compared to the other available tools. Especially, BProVe may be appealing for BPM practitioners who are not required to go through any manual installation or configuration steps to require and interpret the verification of properties over their designed BPMN models.

10. Conclusions and future work
The wider adoption of BPMN to support the development of software systems asks for a clearer and rigorous interpretation of BPMN models, and in particular of the properties they satisfy. The adoption of formal verification contributes to striving model-driven software development, contributing to increase software quality. The literature discusses several approaches for BPMN verification, but available proposals generally miss to consider business process collaborations.

In this paper we propose the BProVe approach to support the entire model development cycle of BPMN collaboration models. BProVe relies on a direct formal semantics for BPMN models, avoiding typical problems of approaches based on intermediate encodings. Within the BProVe approach, both standard model checking and statistical model checking techniques are integrated to effectively support verification. In particular, the use of statistical model checking allows to analyze business processes whose behavior generates large state spaces that are not easily manageable by classic exhaustive model checking techniques.

The BProVe approach has been instantiated in a complete web-based tool-chain, which makes transparent to the final user the inner formal layer. Our tool-chain overcomes the limitation of tools available in the literature, which in most of the cases are just prototypes mainly used for demonstration purposes. Indeed, some of them are not maintained anymore, becoming practically obsolete due to the development of new and incompatible versions of modeling environments and programming languages.

Both the approach and the tool-chain have been extensively validated to assess their capabilities on realistic scenarios, and to possibly highlight scalability issues. Performed experiments confirmed the opportunity to offer an integrated verification approach providing complementary support both to non-deterministic and to statistical verification techniques.

In the future, we plan to continue bringing forward our program fostering the introduction of formal methods for modeling and verification of BPMN models. In particular, we intend to study and define structural metrics that could permit to establish a priori which verification techniques is the most suitable for analyzing a given BPMN model. For instance, a metric based on the degree of parallelism could suggest the adoption of the statistical approach.

Specification and verification of time constraints and resource allocation over BPMN models is certainly an interesting future extension. This will asks to revise the semantics, that currently does not include any information related to quantitative aspects, and to adapt mechanisms for quantitative analysis to the BPMN context.

