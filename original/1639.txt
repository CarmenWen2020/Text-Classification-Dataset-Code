Abstract
Bots are frequently used in Github repositories to automate repetitive activities that are part of the distributed software development process. They communicate with human actors through comments. While detecting their presence is important for many reasons, no large and representative ground-truth dataset is available, nor are classification models to detect and validate bots on the basis of such a dataset. This paper proposes a ground-truth dataset, based on a manual analysis with high interrater agreement, of pull request and issue comments in 5,000 distinct Github accounts of which 527 have been identified as bots. Using this dataset we propose an automated classification model to detect bots, taking as main features the number of empty and non-empty comments of each account, the number of comment patterns, and the inequality between comments within comment patterns. We obtained a very high weighted average precision, recall and F1-score of 0.98 on a test set containing 40% of the data. We integrated the classification model into an open source command-line tool to allow practitioners to detect which accounts in a given Github repository actually correspond to bots.

Previous
Next 
Keywords
Distributed software development

Bot identification

GitHub repositories

Text similarity

Classification model

1. Introduction
The collaborative nature of software development has inherently made it a social phenomenon, which has led to the advent of social coding platforms such as GitHub, BitBucket, and GitLab (Dabbish et al., 2012). These online platforms have taken the collaborative nature of open source software development to a new level, by integrating mechanisms such as issue reporting, pull requests (PR), commenting and reviewing support into distributed version control tools (Gousios et al., 2014, Tsay et al., 2014a, Tsay et al., 2014b). The pull-based development process is the primary means for integrating code from thousands of developers in distributed development platforms such as GitHub (Gousios et al., 2014). This model has had a significant impact on the development of open-source software, but at the same time has significantly increased the workload of repository maintainers to communicate with other contributors, review source code, deal with contributor license agreement issues, explain project guidelines, run tests and build code, and merge pull requests (Gousios et al., 2016).

To reduce this workload, developers have been adopting automated tools to perform repetitive tasks in the development process (Wessel et al., 2018), such as updating dependencies (Mirhosseini and Parnin, 2017) (e.g. dependabot) and fixing vulnerabilities (e.g. snykbot), improving code reviews (e.g. Review bot) (Balachandran, 2013) and documenting code refactorings (Rebai et al., 2019). Such tools are commonly known as DevBots (Erlenhov et al., 2020b), or bots for short. They are generally seen as a promising approach to deal with the ever-increasing complexity of contemporary distributed software development.

While the use of bots in open source software repositories can alleviate maintainer workload, their presence poses challenges for empirical software engineering researchers that aim to study socio-technical aspects of software development. For example, in a previous study we analysed the impact of discussions on pull request (PR) decisions in GitHub repositories (Golzadeh et al., 2019) by studying these discussions in 188K PRs of GitHub repositories. We ignored the presence of bots in that study, deferring it to future work. Repeating the same analysis taking into account the bots allowed us to discover that 20% of those comments belong to bots, and that bots were involved in 31% of all PRs. Bots were responsible for accepting or rejecting 25% of all PRs. Moreover, we found that the proportion of successfully integrated PRs was twice as high for PRs involving bots.

Other empirical socio-technical analyses based on historical software repository data are likely to have been biased as well by not considering the presence of bots. Some empirical studies explicitly acknowledge the presence of bots, and attempt to remove them during data preprocessing (e.g. filtering out bots) or postprocessing (e.g. removing outliers) (Dey et al., 2019, Liu et al., 2019). It is therefore important to consider the presence of bots in such studies, and to treat them differently than humans.

A prerequisite for considering bots is the ability to identify their presence in software development activities. This is not a simple task because, depending on the considered data source, bots often do not have a distinct representation in social coding platforms, and may look, act like or even impersonate humans. Our review of the research literature (see Section 2) revealed a few attempts to manually identify and classify bots. We only came across one study attempting to automate the bot identification process based on commit activity in GitHub repositories (Dey et al., 2020). The current paper has a similar focus, but based on a different data source, namely PR and issue comments in GitHub repositories.

As a first major contribution, we propose a large and reliable ground-truth dataset, consisting of 5000 distinct GitHub accounts of which 527 were manually identified as bots based on their PR and issue commenting contents. As a second major contribution, we use this ground-truth to create and evaluate a classification model that relies on comment-related features to accurately classify accounts as either bot or human. As a third contribution, we propose an open-source tool based on the classification model to allow GitHub contributors to detect which accounts in their repositories actually correspond to bots.

The remainder of this paper is structured as follows: Section 2 presents the related work. Section 3 explains the steps to create the ground-truth dataset. Section 4 details which features we selected for the classification model. Section 5 explains the workflow to select an appropriate classification model and evaluates the selected classification model. Section 6 presents an open source tool implementing this model. Section 7 discusses the results. Section 8 presents the main threats to validity of the research. Section 9 outlines future research avenues and Section 10 concludes.

2. Related work
The earliest idea of computer software imitating humans dates back to the ideas by Alan Turing in 1950 (Turing, 1950). In recent years, the development of AI and machine learning has led to a proliferation of automated tools that substitute humans to perform particular repetitive tasks (Dale, 2016). For example, chatbots imitate natural language to communicate with humans through a conversational interface (Lebeuf et al., 2018), and automated social actors (ASA) automatically create content on social networks (Abokhodair et al., 2015). Bots are also widely used in other contexts such as education (Kerry et al., 2009, Benotti et al., 2014, Fryer et al., 2017), e-commerce (Ben Mimoun et al., 2017, Thomas, 2016), customer services (Gnewuch et al., 2017, Jain et al., 2018), peer production communities such as Wikipedia (Cosley et al., 2007, Geiger, 2013), and social networks (such as Twitter). Social network bots are generally aimed at spamming and sending fake news and hence they seek to hide their true nature. Because of this, numerous studies are focused on identifying them (Minnich et al., 2017, Efthimion et al., 2018, Rodríguez-Ruiz et al., 2020, Amleshwaram et al., 2013). These studies have proposed machine learning models which aim to identify bots based on features such as profile specification (e.g., age, location and biography), tweet content (e.g., hashtags, URLs and similarity of sentiments), tweet time (e.g., burstness and average tweets per day), and user network profile (e.g., interaction between users). Such features either do not have any equivalent in social coding platforms (e.g., hashtags) or require a lot of effort to collect (e.g., user network profiles). Moreover, bots in social networks appear to be quite different from bots in social coding platforms (i.e., DevBots), whose purpose is to help developer teams carry out automated activities in the software development process. We did not find any evidence of intentionally malicious use of DevBots.

In the context of software development, bots are automated software agents that perform well-defined repetitive tasks that support and integrate with the activities of human developers (Wessel et al., 2018, Farooq and Grudin, 2016). They are capable of communication and decision making (Storey and Zagalsky, 2016) and carry out tasks that involve interactions with humans (Lebeuf et al., 2017b). They support both technical and social activities (Lin et al., 2016) to coordinate collaborative software development (Perez-Soler et al., 2017), such as improving feedback on code contributions (Hu and Gehringer, 2019), repairing continuous integration build failures (Urli et al., 2018), and deployment and evaluation of software engineering analysis techniques (Beschastnikh et al., 2017).

Recent research has focused on the practical value of bot adoption in software engineering, such as how bots increase software development productivity (Storey and Zagalsky, 2016), how bots enable faster software dependency updates (Mirhosseini and Parnin, 2017) and how bots can help reduce the friction points software developers face when working collaboratively (Lebeuf et al., 2017a). Other studies have introduced new bots and analysed their effect on software repository activities such as test bots (Erlenhov et al., 2020a), bots to improve newcomers’ experience and help them to better engage in the project (Dominic et al., 2020), bots for answering developer questions using historical Q&A data (Romero and Parra, 2020), bots for assisting in the development of microservice architecture and the use of NLP (Lin et al., 2020).

A prerequisite for studying the impact of bots on software production processes is the ability to identify such bots in the first place. We found very few studies trying to identify and categorize bots. Wessel et al. (2018) conducted a study about prevalence and effect of bots in GitHub repositories. They manually analysed 351 repositories and found that 26% of them use bots. By manual inspection of GitHub accounts they identified 48 different bots in 93 projects. They found statistical differences regarding the number of commits, number of changed files, and closing time of PRs between projects before and after bot adoption. They reported both positive and negative challenges of bot adoption from integrators and contributors’ viewpoints. In another study, they discuss six useful bots in GitHub’s PR process (Wessel and Steinmacher, 2020). They analysed the negative aspects of bots in code contributions and introduce a meta-bot that acts as a middleman to mitigate this effect.

Erlenhov et al. (2019) presented a taxonomy that classifies 11 existing development-related bots in GitHub and Slack. Lebeuf (2018) provided a multi-faceted classification of bots (including many well-known examples of bots), combining their properties and behaviour. None of these studies proposes an automated approach to identify bots.

Dey et al. (2020) did propose an automatic method to identify bot accounts in git projects. Each identity in their dataset consists of an author name and email address. They studied three different approaches to find bots, based on (i) the presence of the string “bot” at the end of the author name, (ii) commit messages, and (iii) features related to files changed in commits and projects the commits are associated with. They combined these three different approaches into a single ensemble model that was validated on a dataset of 67 bots of which 58 cases (85%) were effectively captured by the model.

Their study is fundamentally different from ours, since their dataset is based on commit data in GitHub repositories, whereas we will focus exclusively on GitHub issue and PR comments. They also identified authors based on the author name and email address, whereas we rely on the GitHub account name exclusively. Both datasets are quite complementary, as we found many examples of bots that are only involved in commit activity and others that are only involved in issue and PR activities. Moreover, the nature and contents of commit comments is quite different from issue and PR comments, requiring other features to establish an accurate classification model.

3. Ground truth dataset
In order to be able to evaluate an automated algorithm to detect bots based on their commenting activity in GitHub issues and pull requests, a ground truth dataset is required. Such a ground truth dataset indicates, given a contributor commenting in an issue or a pull request, whether this contributor is a human or a bot. To be effective and representative, the ground truth dataset should be large enough, i.e., it should cover a considerable number of GitHub repositories, contributors, issues and pull requests.

Since we did not encounter any such representative ground truth dataset in the research literature, we set out to create it ourselves. To do so, we downloaded and manually examined comments from thousands of issues and pull requests, labelling each contributor either as a bot or a human commenter. Despite the considerable effort needed to create such a dataset, it was a worthwhile endeavour, since it will be a valuable resource for other researchers as well.

This section explains how we proceeded to create and validate our ground truth dataset, from the raw data we downloaded to the process of rating and labelling each contributor.

3.1. Terminology
In the context of this paper, we will consistently use the following terminology. We use the term bot to refer to a GitHub bot, defined by Wessel et al. (2018) as “a task-oriented bot, responsible for automating well-defined tasks on GitHub repositories. A GitHub bot behaves like a human user, serving as an interface between users and services.”

Since our study focuses on distributed software development on GitHub, we use the term repository to refer to a GitHub repository. Contributors to a repository can be identified by their unique (GitHub) account. Contributions to a repository can take different forms, such as code commits, issues and pull requests (PR). The focus of this paper will be on issues and PRs.

Contributors can add (uniquely identifiable) comments to PRs and issues in a repository. We use the term commenter to refer to the GitHub account having provided this comment. We also use the term comment to refer to its actual textual content. Since a commenter can be either a bot or a human contributor, we will refer to them as bot commenter and human commenter, which we will abbreviate to bot and human, respectively.

3.2. Data extraction
Our goal is to identify bot and human commenters based on the comments they made in issues and pull requests of collaborative software development repositories on GitHub. GitHub is one of the leading online collaborative development platform. As of November 2020, GitHub reported having over 48 million users and more than 195 million repositories (including at least 37 million public repositories).

Following the guidelines provided by Kalliamvakou et al. (2014), we want to avoid repositories that have been created merely for experimental or personal reasons, or that only show sporadic traces of issue and PR comments. Moreover, since our focus is on software development repositories, we want to exclude repositories that are not related to software development. To comply with these constraints, we relied on libraries.io (Katz, 2020), a monitoring service indexing information for several million packages being distributed through 37 software package registries, such as npm, PyPI, etc.

We downloaded the data dump of January 20201 containing, among others, links to the GitHub repositories related to these distributed software packages. Since it contains more than 3.3 million GitHub repositories, we randomly selected around 136K of them as the starting point of our dataset creation process. For each of these repositories, we extracted on 16 February 2020 the last 100 comments of the last 100 issues and pull requests using GitHub’s GraphQL API. This resulted in over 10 million comments covering a period of more than 10 years (ranging from 17 December 2009 to 15 February 2020). These comments were made by more than 837K distinct contributors, corresponding to more than 3.5 million issues and pull requests. The extracted comments also include the textual description of each considered PR. While the GitHub API does not consider PR descriptions as comments, we do, since the GitHub web interface does not visually distinguish them from other comments.

Since our goal is to distinguish between bots and human contributors based on their comments, we require a sufficiently large number of comments for each commenter. Hence, we decided to exclude commenters who made fewer than 10 comments based on a threshold we identified in a previous study (Golzadeh et al., 2020). At this stage of the process, the dataset contains 6,307,489 comments belonging to 79,342 contributors, spanning 42,492 repositories.

Since this is too much data to process manually, we extracted a subset covering 5082 commenters. This subset was composed of 4644 randomly selected commenters to which we manually added 438 extra commenters that are more likely to correspond to bots based on previous studies (Golzadeh et al., 2020, Wessel et al., 2018) (52 cases), or because they contained a specific substring in their GitHub account name (386 cases). The substrings we considered were “bot”, “ci”, “cla”, “auto”, “logic”, “code”, “io” and “assist”. By doing so, we increased the likelihood of having a sufficient number of bots in the dataset.

The resulting subset contains 5082 commenters and covers 3975 repositories, 186,991 issues and pull requests, and contains 301,557 comments. Table 1 summarizes the main characteristics of the considered datasets.

3.3. Data labelling and rating process
The next step to create a ground truth dataset is to manually identify bots and humans. To ease this process, we developed a web application through which the list of comments of each commenter was presented to at least two raters (among the four authors of this paper). Comments were displayed by batches of 20, starting with the most recent comments first, and the rater had an option to display more comments if needed. The account name of the commenter was not revealed to avoid bias, as the goal was to classify commenters based on their comments only. The rater could select whether the commenter is considered as a “Bot” or a “Human”. In case a rater was uncertain whether the commenter was a bot of a human being, a third option could be selected: “I don’t know”. Furthermore, the rater was asked to select a difficulty level among “Very easy”, “Easy”, “Difficult” and “Very difficult” for his decision.


Download : Download high-res image (580KB)
Download : Download full-size image
Fig. 1. Anonymized screenshot of the rating application in action.

Fig. 1 shows a screenshot of the rating application in action. For the specific example being shown, raters could easily decide that the commenter is a bot based on the content and repetitiveness of all visible comments.


Table 1. Summary of the dataset characteristics.

Raw dataset	Number
GitHub repositories	136,529
  from # distinct owners	84,983
Issues	1,588,363
Pull requests (PR)	1,951,705
Issue and PR comments	10,874,611
  from # distinct commenters	873,489
Selected subset	
GitHub repositories	3,975
  from # distinct owners	3,425
Issues	50,241
Pull requests (PR)	136,750
Issue and PR comments	301,557
  from # distinct commenters	5,082
In total, 5082 commenters were rated, ending up with exactly 5000 commenters after having filtered out 82 commenters during the following process. The rating process was performed in two steps to come with an optimal inter-rater agreement, relying on Landis agreement levels (Landis and Koch, 1977). The rating process is summarized in Fig. 2. Each commenter was initially rated by two distinct raters. All cases that were agreed either as bot or human were included in the ground-truth dataset. In order to assess the reliability of the ground-truth dataset, we computed the inter-rater reliability (IRR) (Campbell et al., 2013) between each pair of ratings based on Cohen’s kappa  (McHugh, 2012). The results are presented in Table 2.

The first step of the rating process ended up with 472 bots and 4364 humans, with a “substantial” agreement () between raters. At the end of this step, there were 246 cases because they were either not agreed (177 cases) or agreed as “I don’t know” (69 cases). Additionally, 91 cases were evaluated as “difficult” or “very difficult”, leading to a total of 268 cases for the second step.

In a second step, we involved a third rater for the cases that were identified as “difficult” or “very difficult” during the first step. We then discussed all together all cases for which an agreement could not be achieved, or the cases where the third rater disagreed with one of the two former ones. During these discussions, we sometimes relied on additional information (e.g., we looked at the GitHub account of the commenter, at time intervals between comments, the overall activity of the account, etc.) to come to a decision.

The large majority of cases we discussed were resolved on the basis of a unanimous decision between raters, leading to an “almost perfect” inter-rater reliability (). At the end of the second step, only 82 cases were left out of the ground-truth dataset, either because no agreement could be reached (4 cases), or because we agreed on the “mixed” nature of these commenters. These “mixed” commenters correspond to human commenters that relied on automatic tools to generate comments, therefore “mixing” the behaviour of a human and a bot at the same time.


Download : Download high-res image (147KB)
Download : Download full-size image
Fig. 2. Workflow of the rating process.


Download : Download high-res image (210KB)
Download : Download full-size image
Fig. 3. Mean Levenshtein and Jaccard distances between pairs of comments, per commenter. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)


Table 2. Summary of two-step rating process.

First	Second
step	step
Commenters agreed as bot	472	527
  from # repositories	457	505
Commenters agreed as human	4364	4473
  from # repositories	3425	3515
Proportion of bots	9.8%	10.5%
Commenters agreed as “I don’t know”	69	–
Commenters without agreement	177	4
Commenters agreed as “mixed”	–	78
 agreement score	0.84	0.96
For example, some of these accounts rely on an automated tool to facilitate code review by sending PRs to Reviewer, a code review tool for GitHub. Other examples include the use of tools such as StyleCI to improve code style, or semantic-release to automatically determine the next version number of a release, generate release notes and publish a package. We will discuss these “mixed” commenters in more details in Section 7.

This left us with 5000 commenters, of which 527 (i.e., 10.5%) are bots. Table 3 summarizes the characteristics of final ground-truth dataset. Since we believe such a ground-truth dataset is valuable for the research community (e.g., to have a list of known bots, to study their characteristics or to train other models), we share it publicly on http://dx.doi.org/10.5281/zenodo.4000388. This dataset contains the name of the repository, the name of the commenter and whether it is a bot or a human. Due to GDPR regulations and in order to protect GitHub users’ privacy, we do not provide additional information (e.g., their comments).


Table 3. Summary characteristics of final ground truth dataset.

Number of...	Bot	Human	Total
Commenters	527	4,473	5,000
Repositories with at least 1 commenter	505	3,515	3,909
Comments	28,287	268,504	296,791
Issues with at least 1 commenter	2,749	46,959	49,623
PRs with at least 1 commenter	16,937	118,896	134,208
4. Feature selection
In this section, we explain the features that will be used by the classification model to distinguish bots from human commenters. These features include the number of comment patterns, the number of (empty) comments, and the number of comments within each pattern. The following subsections explain these features and the rationale behind their selection.

4.1. Text distance between comments
Based on the assumption that bots perform more repetitive and automated tasks, we hypothesize that bot commenters exhibit more repetitive comments than human commenters. Consequently, we expect comments belonging to a bot to exhibit more similarity than comments belonging to a human commenter. In order to measure the similarity between comments of each commenter, both in terms of content and structure, we rely on text distance metrics that are commonly used for this purpose in natural language processing. The two metrics we consider are the Jaccard (1912) and Levenshtein (1966) distances. The first one aims to quantify the similarity of two texts based on their content, and the second one captures the structural difference by counting single character edits.

More precisely, the Jaccard distance  measures the distance between two texts  and  by comparing the number of distinct common words in  and  with the total number of distinct words in  and . If  denotes the set of words in , then  is computed as: 
 

The second distance we consider is the Levenshtein edit distance  that measures the difference between two character sequences  and  by counting the minimum number of single-character edits (insertion, deletion, or substitution) required to convert  into . We rely on its normalized version, computed as: 
 

To support our assumption that comments made by a bot have higher similarity than comments made by a human, we computed for each commenter in the ground truth dataset the Jaccard and Levenshtein distances between all pairs of comments belonging to that commenter. In order to compute the Jaccard distance, we first needed to split comments into words, a process also known as tokenization. To do so, we relied on spaCy, an “industrial-strength natural language processing library”2 that notably offers a fast but robust tokenization algorithm, among others.

Fig. 3 shows the mean Levenshtein and Jaccard distances for each commenter, distinguishing between bots (blue triangles) and humans (orange triangles). We observe that many humans are grouped in the top right part of the figure, i.e., they have high mean values for both distances. On the other hand, most bots have lower values for their mean distances. For instance, 91.6% of all bots have mean Jaccard and Levenshtein distances below 0.75. For comparison, only 7.2% of all human commenters exhibit mean Jaccard and Levenshtein distances below 0.75.

Despite this, there is still a lot of overlap between bots and humans in Fig. 3, indicating that the mean distances are not enough to properly distinguish between bots from humans. By manually inspecting the comments belonging to bots having high mean distances, we found that their comments usually form sets of similar comments. Even if the distance between comments in a set (i.e., intra-set distance) is low, the distance between comments belonging to different sets (i.e., inter-set distance) is high. As a consequence, the overall mean distances between all comments tends to remain high, rivalling the distances observed for most human commenters.

We found many of these cases. One example is the bot that was identified in Fig. 1. We observe that it has two different sets of similar comments. The first set consists of comments of the form “You did it @…! Thank you for signing the …Contribution License Agreement. We will have a look at your contribution!”. The second set consists of comments of the form “Hi @…, many thanks for your contribution! In order for us to evaluate and accept your PR, we ask that you [sign a contribution license agreement] …It’s all electronic and will take just minutes.”. The mean distance between pairs of all 20 comments belonging to the first set (i.e., intra-set distance) is very low (0.06 and 0.08 for Levenshtein and Jaccard distance respectively) and even lower (0.04 and 0.05 respectively) for the second set of 27 comments. However, the intra-set distance (i.e., the distance obtained by comparing comments from the first pattern with comments for the second pattern) is much higher (0.70 and 0.81 for Levenshtein and Jaccard distance respectively). Consequently, the overall mean distances between all pairs of comments are 0.37 for Levenshtein and 0.43 for Jaccard distance. These distances are usually observed for human commenters, not for bots.

4.2. Repetitive comment patterns
Since high mean distances between comments of a commenter could correspond to either a human or, in many cases, to a bot having sets of similar comments, we cannot exclusively rely on these mean distances to distinguish between bots and humans. However, we observed that bots tend to have sets of many similar comments (i.e., they follow comment patterns), while we found that most comments from humans are unique and only a few of them seem to follow a pattern (e.g., “Thank you!”, “LGTM”3 or “+1”4).

Based on this observation, we expect bots to have a lower number of comment patterns than humans. In order to capture these comment patterns, we rely on a clustering algorithm. Clustering aims to group items into sets (“clusters”), in such a way that items belonging to the same cluster are more similar than items belonging to different clusters.

We selected DBSCAN (Density Based Spatial Clustering of Applications with Noise) (Ester et al., 1996), a well-known density-based clustering algorithm that notably has the ability (i) to generate clusters of unequal size (i.e., we can have patterns with unequal numbers of comments), (ii) to generate a single cluster if needed (e.g., a commenter whose comments are all the same), and (iii) to generate single item clusters (e.g., a commenter whose comments are all very different). Additionally, DBSCAN permits not to specify the number of clusters in advance, fitting our use case wherein we do not know the number of patterns of each commenter in advance.

Since we aim to capture both the structural and content distance between comments, we rely on a combination of the Levenshtein and Jaccard distance, defined as follows: 
 

For each commenter, we computed  for each pair () of comments. The resulting distance matrix, one per commenter, is then passed to DBSCAN to group the comments based on their similarity. Fig. 4 reports on the number of patterns (i.e., clusters), distinguishing between bot and human commenters. Since the number of patterns could depend on the number of comments, we report on the number of patterns relative to the number of considered comments.


Download : Download high-res image (352KB)
Download : Download full-size image
Fig. 4. Number of comment patterns (clusters) and number of considered comments per commenter.

Compared to Fig. 3 we can observe a much clearer separation between bots and humans based on the number of comment patterns and the number of comments, although it is not perfect. We observe that most humans are along the diagonal line which indicates that the number of patterns is close to the number of comments, and that almost all bots are along the horizontal axis. This means that the number of comment patterns for bots remains stable, and low, regardless of the number of comments they made. This confirms our assumption that bots have a limited set of comment patterns, contrarily to humans that seems to make much more varied comments.

4.3. Inequality between comments in patterns
Although we expected human comments to be mostly non-repetitive (i.e., each comment corresponds to a different pattern), we found instances in which a human commenter had a non-negligible number of repetitive comments (e.g., “Thank you!”, “LGTM” or “+1”) alongside other messages. This leads to having human commenters whose number of comment patterns is much lower than the number of comments, which is exactly the assumption we had for bots due to their repetitive comments. However, we found that those human commenters correspond to cases having at the same time a few patterns with many comments and many patterns with a few (mostly single) comments. On the other hand, bots exhibit single comment patterns less often. For instance, among the 2431 patterns corresponding to bots, 50% are composed of a single comment, while this proportion is much higher (95.9%) for the 230,711 patterns we have for humans.

This observation lead us to consider the inequality in the number of comments in each pattern as a supplementary feature to distinguish between bots and humans. The Gini coefficient (Dorfman, 1979) provides a way to quantify the inequality (i.e., the distribution) of the number of comments for each pattern. A value of 0 expresses perfect equality (i.e., each comment pattern consists of the same number of comments). A value of 1 expresses maximal inequality among values (i.e., a few patterns capture many comments, and the remaining comments are spread into many single-comment patterns).

Let us consider the example of a specific human commenter in our dataset. This human made 73 comments belonging to 12 patterns. 9 of these patterns have exactly one comment. The other ones correspond to “LGTM” (37 comments), “##Fixes{Number}” (22 comments) and “lgtm” (5 comments). As a result, the Gini coefficient for this commenter is very low 0.04, since most patterns (9 out 12) have the same number of comments. Let us compare this to a bot in our dataset with a similar number of comments (61) and comment patterns (10). The number of comments in each pattern is more unequally distributed, ranging from 1 to 49 comments per pattern, a consequence of much more repetitive messages. As a result, its Gini coefficient is much higher, namely 0.52.

Fig. 5 shows the distribution of the Gini coefficient for all bots and humans in our dataset, by means of boxen plots (Hofmann et al., 2011). We observe that humans exhibit a lower inequality than bots with respect to the spread of comments within patterns. We statistically compared these distributions using a Mann–Whitney-U test (Mann and Whitney, 1947). The null hypothesis, stating that the two distributions are the same was rejected (), indicating a statistically significant difference between the two distributions. The effect size turned out to be large (Cliff’s delta ) (Cliff, 1993, Romano et al., 2006). This confirms that humans tend to have a lower inequality than bots, a consequence of many of their patterns containing a single comment. Therefore, the Gini coefficient can help in distinguishing between bots and humans.


Download : Download high-res image (60KB)
Download : Download full-size image
Fig. 5. Distribution of Gini coefficient for bot and human commenters.

4.4. Number of comments and empty comments
In addition to the number of patterns and the unequal distribution of comments within patterns, we also consider the number of comments made by each commenter as a feature for our model. This feature makes it possible to distinguish between commenters having a similar number of patterns. Indeed, consider for example two commenters having exactly 10 patterns. Assume they have respectively 10 and 100 comments. The first commenter is likely to be a human (since it has 10 patterns each containing exactly one comment, i.e., all comments are different), while the second one is more likely to be a bot.

We also consider the number of empty comments as a feature of our model. Indeed, during the rating process, we found that a non-negligible proportion (6.5%) of the considered comments were empty. The presence of such comments in the dataset may seem strange. Even if the GitHub user interface does not allow empty comments in a discussion, it does not prevent comments to be composed of white characters. Moreover, the GitHub user interface allows the creation of pull requests whose description is empty. Since this description is the very first comment of a pull request, it explains why we found empty comments in the dataset.

Interestingly, we found that empty comments are mostly created by human commenters and not by bots. For instance, only 7% of all bots generated at least one such comment, whereas this proportion reaches 41.2% for human commenters. This should not come as a surprise, since one could expect bots mainly to generate informative comments and, by definition, empty comments are uninformative. Consequently, we decided to consider the number of empty comments as a feature of our classification model.

In summary, based on the analysis in this section we decided to use four distinct features for commenters to train the classification model: (i) the number of comment patterns; (ii) the inequality between comments in patterns; (iii) the total number of comments for the commenter; and (iv) the number of empty comments.

5. Classification model
All the scripts and data used to carry out the experiments in this section are available in a replication package on:

https://github.com/mehdigolzadeh/IdentifyBots_ReplicationPackage

5.1. Classifier selection
A wide variety of algorithms can be used to construct a classification model. In this section we compare different classification algorithms to determine which one is the most appropriate to distinguish between bot and human commenters. Among the classifiers having the ability to perform binary classification, we consider decision trees (DT) (Safavian and Landgrebe, 1991), random forest (RF) (Breiman, 2001, Frank and Hall, 2001), support vector machines (SVM) (Gunn, 1998), logistic regression (LR) (Burridge, 1991), and k-nearest neighbours (kNN) (Aha et al., 1991). Since the performance of these classifiers could depend on the input parameters, we follow a standard workflow of hyper-parameter tuning using a grid-search cross-validation process (Witten et al., 2011) (see Fig. 6). To do so, we rely on scikit-learn (Pedregosa et al., 2011), a well-known machine learning library for Python.

We first divided the ground-truth dataset into two disjoint sets: a training set containing 60% of the data that will be used in a grid-search cross-validation process to determine the best input parameters and the best classifier, and a test set composed of the remaining 40% that will be used to evaluate the performance of the selected classifier and parameters on new data. Since we have many more humans than bots in our datasets, we relied on a stratified train-test split method to create these two sets with the same ratio of bots and humans.


Download : Download high-res image (133KB)
Download : Download full-size image
Fig. 6. Standard workflow for grid-search cross-validation.

Selecting an appropriate model with the best possible parameters requires hyper-parameter tuning. Based on the supported parameters of each classifier, we implemented a grid-search process based on a limited set of values for each parameter. For example, DT and RF were evaluated by setting the split criterion to Gini and entropy, among others. Doing so resulted in 91 different classifiers. To address the class imbalance problem (He and Garcia, 2009) and avoid affecting the performance of the classifiers (Grbac et al., 2013), we rely on a cost-sensitive learning approach (Elkan, 2001). Practically, this means we set the class weight parameter in scikit-learn to balanced for each supported classifier.

We then trained and evaluated the performance of all classifiers using a 10-fold cross-validation process. This approach splits the dataset into  subsets of equal size, and for each fold a model is trained using  subsets and is evaluated on the remaining one. The overall performance of the model is averaged from the performance of these 10 models. To ensure that the created subsets preserve the same proportion of bots and humans as in the complete training set, we relied on a stratified shuffle split to create them.

The performance of the resulting models is measured using the classical metrics of precision , recall  and -score. We use these metrics for the population of each class (i.e., for bots  and humans ). For the entire population we computed the weighted version of these metrics to take into account the class imbalance. We aim to achieve an as high -score as possible. Since our goal is to identify bots, we also strive to keep bot recall  high enough, given that the population of bots is significantly smaller than the population of humans, and that it is much easier and faster to recover from humans misclassified as bots than the opposite. All these metrics are summarized in Table 4, and are defined in terms of the number of true positives TP (the number of bots that are correctly classified as such by the model), true negatives TN (the number of humans that are correctly classified as such by the model), false positives FP (humans that are wrongly classified as bots), and false negatives FN (bots that are wrongly classified as humans).

Following the grid-search cross-validation process described above, we trained and obtained 91 classifiers. For each of them, we computed the resulting bot, human and overall precision, recall and -score. Table 5 reports on these metrics, in descending -score order. To ease readability, rather than reporting on all 91 classifiers, we selected for each classifier category (e.g., DT, RF, …) the instance whose parameters resulted in the highest -score. We also compared the precision, recall and -score of these classifiers against a baseline classifier, ZeroR. ZeroR is a very simple classifier that has no predictive power: it ignores the features and always predicts the majority class (i.e., “human” in our case). We observe that all classifiers exhibit a high overall performance (in terms of precision, recall and ) and surpass ZeroR by a wide margin.


Table 4. Definitions of precision, recall and -score.

Population	Precision 	Recall 	-score
Bots 	
 
 
 
Humans 	
 
 
 
 
 
 
The overall scores for ,  and  of all classifiers are consistently higher than the ZeroR baseline, and range between 0.974 and 0.984. Even though the best SVM and LR classifiers have higher bot recall  than the best RF classifier (0.925 and 0.931 compared to 0.916, respectively), the overall ,  and  scores are highest for the RF classifier. We therefore decided to use the best RF classifier, which was obtained with the entropy split criterion, 10 estimators (i.e., trees) and a maximum depth of 10 for these trees.


Table 5. Precision, recall and -score of the best classifiers per family of classifiers (in descending order of -score).

Bots	Humans	Overall
Classifier							
RF	0.932	0.916	0.990	0.992	0.984	0.984	0.984
kNN	0.943	0.853	0.983	0.994	0.978	0.979	0.978
SVM	0.876	0.925	0.991	0.984	0.979	0.978	0.978
DT	0.882	0.884	0.986	0.985	0.975	0.974	0.974
LR	0.839	0.931	0.992	0.978	0.975	0.973	0.974
ZeroR	-	0.000	0.893	1.000	0.798	0.893	0.843
5.2. Evaluation
In this subsection, we aim to evaluate the actual performance of that model on data that were not used to train the model, i.e., on new data contained in the test set. Following the workflow presented in Fig. 6, we start by constructing a new classification model instance based on the selected RF classifier, its parameters, and the training set containing 60% of the ground-truth dataset.

We evaluate and report the accuracy of the model based on the test set, corresponding to the remaining 40% of the ground-truth dataset. This test set includes 2000 commenters, of which 1789 are humans and 211 are bots. The evaluation results are reported in Table 6.

We see that most bots and humans are correctly classified by the model. For instance, only 19 out of 211 bots were misclassified as humans (FN), and only 13 out of 1789 humans were misclassified as bots (FP). The overall -score is very high (0.98), a consequence of the high precision (0.98) and high recall (0.98) of the model. Thanks to the fact that we have taken into account class imbalance during the training phase, these high scores can also be observed individually for each class, even if the precision and recall for bots is slightly lower than for humans. These results confirm what we already observed in previous section, that is, the model is effective in identifying bots and humans.


Table 6. Evaluation of the classification model using the test set.

Classified	Classified	P	R	F1
as bot	as human			
Bot	TP: 192	FN: 19	0.94	0.91	0.92
Human	FP: 13	TN: 1776	0.99	0.99	0.99
Weighted avg			0.98	0.98	0.98
Scrutinizing all 19 misclassified bots (FN) we found that ten of them were already problematic during the first step of the manual rating process, where they were rated as either “Human” or “I don’t know” by one of the raters. Moreover, the final decision to classify them as bots during the discussion session among raters was based on additional information that is not available in the comments themselves, explaining why the model is not able to classify them correctly.

The model also misclassified 13 humans. The fact that the model misclassified these humans as bots is not surprising given that, during the first step of the rating process, 10 out of 13 cases were manually rated as difficult or very difficult, 2 cases as “I don’t know” by both raters and one case was even rated as a bot by one of the raters. Section 7 provides a detailed analysis of these misclassified commenters.

Since the model relies on features computed on comments to distinguish bots from humans, it is worthwhile to consider and measure the impact of the number of considered comments on the performance of the model. In particular, we aim to identify the minimal number of non-empty comments required to reliably classify bots and humans. To this end, we evaluated our model and computed the -score for commenters in the test set, grouped by their number of non-empty comments.

Fig. 7 shows the resulting -scores of the model grouped by bins based on the number of non-empty comments. The colour of a bin indicates how many commenters there are in that bin. The bins with 10 to 24 and 30 to 34 non-empty comments have the highest number of commenters, while bins between 85 to 94 have the lowest number of commenters. The -score increases from 0.87 (bin 0-4) and becomes stable around 0.96 to 1.00 after 10 non-empty comments are reached (from bin 10–14). This suggests that having at least 10 non-empty comments is enough to achieve good performance with the model.


Download : Download high-res image (121KB)
Download : Download full-size image
Fig. 7. -score of the model when applied on commenters, grouped by their number of non-empty comments. The colour indicates the number of commenters in each bin.

6. The BoDeGHa bot detector tool
Since the classifier we trained to identify bots presents very good performance, we implemented it as part of a tool. The tool is called BoDeGHa (Bot Detector for GitHub activity), is developed for Python 3.7 and is easily installable through pip, the official package manager for Python.5 BoDeGHa can be used by any researcher or practitioner to classify accounts of a given GitHub repository either as bot or as human based on their issue and PR comments.

In its simplest form, BoDeGHa accepts the name of a GitHub repository and a GitHub API key. BoDeGHa computes its output in three steps, summarized in Fig. 8. The first step consists of downloading all comments from the specified repository thanks to GitHub’s GraphQL API. This step results in a list of commenters and their corresponding comments. The second step consists of computing the number of comments, empty comments, comment pattern and inequality between number of comments within patterns (i.e., the features of the classification model). The third step simply applies the pre-trained model on these examples, and outputs the prediction made by the model.

BoDeGHa supports several additional parameters. The minimum and maximum number of comments to download and to consider can be specified, as well as the start date from which to consider comments. It is also possible to provide a list of specific accounts for the tool to consider. To ease its reuse by other tools, it is also possible to export the results either as comma-separated values or JSON. The command-line interface of BoDeGHa is summarized in Fig. 9.


Download : Download high-res image (177KB)
Download : Download full-size image
Fig. 8. The BoDeGHa architecture.

Fig. 10 presents the output of BoDeGHa for a randomly chosen GitHub repository. The output shows, for each GitHub account (first column), the number of extracted comments (second column), the number of empty comments (third column), the number of computed comment patterns (fourth column), and the inequality among them (fifth column). The last column provides the predicted class of each account. This example shows that three commenters are identified as bots, and all remaining commenters as humans.


Download : Download high-res image (119KB)
Download : Download full-size image
Fig. 9. List of command-line arguments for BoDeGHa 1.0.1.

7. Discussion
The evaluation of the classifier revealed several commenters that the model was not able to properly classify. We specifically look at the commenters that have been misclassified by the model. During the evaluation of the model on the test set, we found 19 bots and 13 humans that were misclassified. In order to have a more complete categorization of misclassified commenters, we also applied the model on the training set and obtained 4 additional bots and 12 additional humans that are misclassified.

Starting with the 24 (19+5) bots, we found that in most cases they correspond to bots that use, convert or copy text that was initially produced by humans. Even if these bots perform repetitive tasks (i.e., copy information) and even if some of these bots use templates to transfer or copy comments that are recognizable to the human eye (e.g., “Jira issue originally created by user {username}: {content of the issue}”), it is difficult for an automated algorithm to detect such cases.

Copy from humans (9 bots):
We found some instances of bots whose comments were generated based on content made by humans (e.g., taskcat-ci, trax-robot). Since our model solely relies on features derived from comments, bot comments originating from human messages increase the likelihood of an incorrect classification. Among these cases, we found several bots that transfer data (including issues, PRs and their associated comments) to GitHub from issue trackers, code review support tools, email etc. For example, neos-bot transfers all issues from a Jira issue tracker, suchabot duplicates comments and issues from another system to GitHub, and wallabag-bot migration from email content to GitHub.

Insufficient comments (9 bots):
We found 9 bots (e.g., devtools-bot and egg-bot) that were wrongly identified as humans due to the lack of a sufficient number of non-empty comments. Since our model relies on comment contents, bots with too few non-empty comments may lead to incorrect predictions even if these comments have similar comment patterns. We do not see any direct way to overcome this, since bots are expected to provide relevant information about what they are doing, and as such, one can expect their comments to be informative and non-empty.

Diverse comments (6 bots):
We found 6 cases of bots that are used for the purpose of reporting, logging, or proposing code changes. The variation of comments in these bots increases the number of comment patterns, which prevents the model from identifying these bots. The source of the comment diversity comes from the reports they send for each task. For example sentry-io creates an issue each time an error occurs in the software project, along with the details of this error (e.g., stack trace). Another example is violinist-bot that submits a PR to update outdated dependencies and to report about the changes of this update. Despite these comments starting with a similar sentence (e.g., “Sentry Issue:” or “If you have a high test coverage index, and your tests for this pull request are passing, it should be both safe and recommended to merge this update. Here is a list of changes between the version you use, and the version this pull request updates to:”), they mainly consist of details related to the submitted issue or PR (i.e., stack traces for sentry-io and lists of issues for violinist-bot) and are considered as different comment patterns.

We also looked at the 17 (13+4) humans that were misclassified as bots, and created the following categories6:

Repetitive comments (8 humans):
We found 8 instances of human commenters whose comments are mostly composed of repetitive messages, such as thank you or LGTM and that have nearly no other comments. Since repetitive messages are usually indicative of the presence of a bot, the model failed to correctly classify these commenters.

Insufficient comments (3 humans):
We found 3 humans with few comments, most of them being empty. Most of these comments were created in the context of a pull request whose title was already sufficiently informative. Since these empty comments are grouped in a single comment pattern, and since they form the large majority of the comments made by these commenters, they were wrongly considered as being generated by a bot due to their repetitive nature. We also found instances where the comment content is too short or there are too few non-empty comments. This prompts our algorithm to group them into a small number of patterns and consequently provide wrong predictions.

Mostly unfilled issue templates (3 humans):
It is not unusual in GitHub repositories to require commenters to follow a comment template or a checklist when creating issues or pull requests.7 We found 3 commenters whose comments were mostly composed of unfilled or barely filled templates, leading these comments to be considered as a single pattern, and leading the model to misclassify them as bots. Relying on an analysis of the content of such comments could prevent them from being misclassified, by taking into account the presence of such templates.

Others (3 humans):
These cases do not fall into any of the above categories, and we have found no specific reason to explain their misclassification. Some of them have a small number of comments, while others only have a few patterns (e.g., due to the presence of similar long URLs in comments) despite the fact that they do not seem to have duplicated or similar comments.

Most commenters that were misclassified by the classification model were also hard to recognize by the raters during the process of creating the ground-truth dataset. In the test set, about 84.6% (11 out of 13) of the humans that were misclassified as bots and about 63.1% (12 out of 19) of the bots that were misclassified as humans were originally rated as “I don’t know”, “difficult”, or “very difficult” by at least one of the raters. In contrast, among the correctly classified commenters, a much lower percentage of bots (12.5%, 24 out of 192) and humans (9.5%, 169 out of 1772) were rated as such.

Furthermore, during the creation of the ground-truth dataset, we encountered several examples of commenters whose features and comments were reminiscent of both humans and bots. Such so-called “mixed” commenters are the result of GitHub accounts belonging to humans allowing automatic tools to use their account for carrying out certain specific tasks. Hence, the comments of such commenters include both human-like and bot-like behaviour. We identified 78 such commenters out of 5082 commenters (i.e., 1.5%) during the rating phase and we consistently excluded them from the ground-truth dataset since we could not decide whether these commenters should be classified as bots or humans.

Nevertheless, it is interesting to report how our model behaves when exposed to these specific “mixed” cases. Out of these 78 identified “mixed” commenters, 21 were classified as bots (26.9%) and 57 as a humans (73.1%). The fact that the proportion of “mixed” commenters classified as bots is higher than the one in the training set (10.3%) suggests that their behaviour is perceived to be closer to that of a bot than a human by the classification model.

The presence of mixed accounts as well as the categories of bots that have been misclassified as humans suggests that it is not easy to come up with a single definition for a bot. Two persons could easily disagree on whether a given account is a bot or a human if they have a different interpretation of what it means to be a bot. This calls for a more precise definition of bots. Erlenhov et al. (2020b) started doing so based on qualitative interviews with developers. This enabled them to identify three distinct DevBot personas that differ in terms of features like autonomy, chat interfaces, and smartness. This more fine-grained classification of DevBots and their characteristics paves the way for more sophisticated classification models.

The approach presented in this paper is not the first one to have been proposed in the literature to detect bots in social coding platforms. Dey et al. (2020) proposed three different approaches for identifying bot accounts in GitHub projects, mostly based on their commit messages. One of them consists of checking for the presence of the string “bot” in the account name of the committer. We partially relied on this heuristic to add more potential bot candidates during our data collection. However, solely relying on it to identify bots is likely to lead to a large number of both false positives and false negatives. To confirm this, we applied their approach on our ground-truth dataset. We found 169 humans out of 4473 (3.8%) containing the string “bot” in their account name, either at the end (46 cases) or in the middle (123 cases). Out of the 527 bots we have in the dataset, 394 of them (i.e., 74.7%) actually contained “bot” in their account name, usually at the end of the name (378 cases). Although this may seem high for such a simple heuristic, it still implies that more than one out of four bots is missed with this method, and about one out of 25 humans is mistakenly considered a bot. For comparison, around only one out of 25 (3.8%) bots have been misclassified as humans by our model, and around only one out of 100 humans (1.1%).

8. Threats to validity
Based on the structure recommended by Wohlin et al. (2012) we discuss the threats that might call into question the validity of our findings, their potential impact and how we have tried to mitigate them.

Construct validity examines the relationship between the theory behind the experiments performed and the observations found. This threat is mainly related to the correctness of the dataset used in the experiments. The results of our study are strongly dependent on the correctness of the ground-truth dataset. We are confident that the ground truth contains very few errors, since we achieved an almost perfect agreement () based on an iterative rating process involving all authors of this paper. One of the most likely threats is the existence of “mixed” commenters in the dataset. Such commenters are difficult to classify, even by human raters, since they combine both bot-like and human-like behaviour. Mixed commenters constitute a very small proportion of our dataset (78 cases, corresponding to 1.5% of all considered accounts). We excluded all these cases from the dataset since we could not agree on them. However, it is possible that the dataset still contains such cases that were not identified by the raters. Given the very low ratio of such mixed accounts, it is however, unlikely to affect our findings.

Internal validity concerns choices and parameters of the experimental setup that could affect the results of the observations. Given that our classification method is fully based on features computed from comments, we required each commenter included in the dataset to have contributed at least 10 (possibly empty) comments. This threshold is based on previous experiences and findings (Golzadeh et al., 2020). As such, we cannot claim that our model applies on commenters who made fewer than 10 comments. Similarly, we considered at most 100 comments for each commenter but, as explained in Section 5.2, this upper limit on the number of comments is unlikely to have biased our results, since we already achieved a high -score starting from 10 non-empty comments.

Conclusion validity concerns whether the conclusions derived from the analysis are reasonable. Our conclusions are based on the evaluation and application of the classification model on the test set. Given that we properly followed a standard grid-search cross-validation method to identify the best classifier, and that we evaluated the model on the test set (i.e., examples that have not been used to train or select the classifier), the results we obtained and conclusions we reached are unlikely to be affected.

External validity concerns the degree to which the conclusions we derived are generalizable outside the scope of this study. The main threat to external validity is related to the construction of the ground-truth dataset. To avoid any potential bias, we randomly selected a large collection of GitHub repositories related to software development and corresponding to actual packages being officially distributed, following the guidelines of Kalliamvakou et al. (2014). While this dataset can be regarded as representative of bots contributing to GitHub repositories through PR and issue comments, we do not make any claim about its generalizability to other activities (e.g., commit messages) or other social coding platforms (e.g., BitBucket or GitLab). Nevertheless, the underlying approach could be made applicable to such activities or platforms.

9. Future work
As future work, we intend to use our classification model in socio-technical empirical analyses of collaborative software development, by studying the effect of the presence of bots on various development-related activities, such as the productivity and quality of handling issues, bugs and pull requests, code reviewing, intra- and inter-repository collaboration, developer onboarding, and so on.

With the emergence of more advanced AI, machine learning and natural language processing techniques, we can expect future bots to behave more and more like humans. These new technological advances may make our classification model less capable of distinguishing bots from humans. An example of such technique is Generative Pre-trained Transformer 3 (GPT-3), an autoregressive language model developed by OpenAI and integrating 175 billion parameters. GPT-3 generates text of sufficiently high quality that it is difficult to distinguish them from that written by humans (Brown et al., 2020). To cope with this, we will explore more advanced machine learning methods that could take into account the semantics of the comments. In particular, we will consider techniques relying on natural language processing and deep neural networks to develop classification models that are more resilient to human-like bots, as well as “mixed accounts” corresponding to bots that copy, transfer or translate human comments.

In this study, we provided a binary definition of commenters being either bots or humans. The aforementioned study by Erlenhov et al. (2020b) revealed that a more fine-grained definition would be needed, and they came up with three DevBot personas based on their autonomy, chat interface, and smartness. Such a more fine-grained classification could be used to refine our ground-truth dataset, and to provide more advanced classification models, possibly even computing the probability that an account belongs to each of the considered personas.

Because of the growing use of bots during collaborative development activities (Erlenhov et al., 2019), we can expect to see a proliferation of bots to automate software development in GitHub repositories. For instance, GitHub introduced in November 2019 GitHub Actions,8 a feature providing automated workflows for repository maintainers. These actions, fully integrated with GitHub, allow the automation of tasks based on a various set of triggers (e.g., commits, pull request, issue, comments, etc.). Since they are easily shareable from one repository to another one (through the GitHub Marketplace), we expect their use to become more widespread, even in smaller repositories and, as a result, to see more “bots” and their comments in GitHub repositories. However, tasks triggered through GitHub actions are automatically labelled as such by the GitHub API, eliminating the need to create a model to identify these “bots”. Recently, GitHub action variants of many well-known bots (e.g., Coveralls, Codecov, Snyk) have been published to the GitHub Marketplace, and these actions are rapidly increasing in popularity. Consequently, we expect their GitHub action variant to replace progressively the bots currently being used in GitHub repositories.

We aim to extend our study, including the ground truth dataset, classification model and tool to accommodate other social coding platforms such as GitLab and BitBucket. This will generalize our approach, and allow us to study to which extent the selected platform affects the way in which bots are being used as part of the development process. We also aim to evaluate our model on other types of activities (e.g., git commit messages). This will allow us to understand to what extent our model can be used to identify bots based on commit activities.

10. Conclusion
In this paper, we proposed a novel approach to distinguish between bots and humans in collaborative software development repositories on GitHub, based on the comments they made in issues and PRs.

Our first contribution is the creation of a ground-truth containing 5000 GitHub accounts including 527 bots (10.5%), based on a manual rating process with very high inter-rater agreement ().

Using this ground-truth dataset, we developed a classification model to identify bots based on four features: the total number of comments of a commenter; its number of non-empty comments; its number of comment patterns; and the inequality between the numbers of comments in each pattern. The chosen features align with behavioural differences we observed between bots and humans. Indeed, we found that most human commenters tend to have diverse sets of comments with little repetition, while bots tend to frequently use a limited set of comment patterns.

Following a standard grid-search 10-fold cross validation process, we evaluated and compared five families of classifiers (random forest, k-nearest neighbours, decision trees, logistic regression and support vector machines) on a training set including 60% of all data. We performed hyper-parameter tuning to select the best parameters of each classifier family based on their precision, recall and -score. We selected the random forest classifier since it achieved the highest -score (98.4%).

We evaluated the selected classifier on new data, and found that it achieves high precision and recall. Based on a manual assessment and categorization of bots and humans that were misclassified, we identified why the classification model had difficulties with detecting them, and we provided suggestions for further improvements to the classification model.

We implemented the classification model into a Python command-line tool, called BoDeGHa. This open source tool is made freely available to practitioners and researchers to allow them to analyse GitHub repositories and to identify which accounts correspond to bots and which correspond to humans.