As voice-based conversational agents such as Amazon Alexa and Google Assistant move into our homes,
researchers have studied the corresponding privacy implications, embeddedness in these complex social
environments, and use by specific user groups. Yet it is unknown how users categorize these devices: are they
thought of as just another object, like a toaster? As a social companion? Though past work hints to humanlike attributes that are ported onto these devices, the anthropomorphization of voice assistants has not been
studied in depth. Through a study deploying Amazon Echo Dot Devices in the homes of older adults, we
provide a preliminary assessment of how individuals 1) perceive having social interactions with the voice
agent, and 2) ontologically categorize the voice assistants. Our discussion contributes to an understanding of
how well-developed theories of anthropomorphism apply to voice assistants, such as how the socioemotional
context of the user (e.g., loneliness) drives increased anthropomorphism. We conclude with recommendations
for designing voice assistants with the ontological category in mind, as well as implications for the design of
technologies for social companionship for older adults.
CCS Concepts: • Human-centered computing → Ubiquitous and mobile devices; • Human-centered
computing → Personal digital assistants
KEYWORDS
Personification; anthropomorphism; ontology; voice assistants; smart speakers; older adults. 
1 INTRODUCTION
Smart speaker-based voice assistants such as Amazon Alexa and Google Home have an increasing
presence in many users’ everyday activities [36]. Compared to smartphone-based voice assistants
(e.g., Siri), smart speakers have unique qualities. They support verbal conversational interaction,
most often with no visual output. Smart speakers are also “always-on” and “always listening”, and
are placed in the social, collaborative, semi-private home environment. To date, researchers have
investigated the privacy concerns of smart speakers (e.g., [40,45]), conversational aspects and use
in home environment (e.g., [46,54,57,69]), and use by specific user groups including children [16],
users from low socio-economic backgrounds [59], and people with disabilities (e.g., [1,56]). In
CSCW, researchers are focusing on aspects of this technology related to privacy perceptions and
concerns [40], accessibility [9], and use in social and collaborative spaces [53].
The conversational, always-on nature of voice assistants has lent itself well to functions such as
playing music, checking the weather, and information seeking. Opinion pieces in the popular
press are now questioning whether voice assistants might satisfy more than functional needs, and
interrogating their capacity to play the role of a social partner (e.g., [29,65]). Some prior work
indicates that people personify smart speaker-based voice assistants in particular ways, such as
describing them using personal pronouns [46,57]. What is less known is what characteristics of
smart speakers, as well as their users, lead to personification. Though past work ties certain
behaviors (e.g. referring to Alexa as “she”) to the attribution of human-like characteristics, it is
unclear to what extent people actually see these devices as filling a social role in their lives.
Understanding how, when, and why people personify voice assistants has implications for how
people engage with these devices in their homes, as well as how to support technology-mediated
social interactions for different user groups—a long standing CSCW research interest. Different
forms of technology-mediated social interactions have been explored, particularly for older adults,
including robots, digital avatars, and other technologies [15,41,44]. As voice assistants become
increasingly popular, there is an opportunity for researchers to examine how these devices might
support social interactions for older adults. Our goal is thus to provide a preliminary
understanding of: 1) How do older adults personify voice assistants in their homes? 2) When do
older adults categorize voice assistants as “object-like” vs. “human-like” 3) What are older adults’
perceptions of voice assistants in terms of their filling social roles?
To understand how older adults use and perceive smart speakers, we deployed smart speaker
devices in the homes of older adults and studied their use of the voice assistant over a three-week
period. We conducted weekly in-person interviews and daily diary check-ins via automated phone
calls, and collected usage logs from the app on the paired tablet. This paper analyzes this data to
understand the personification of Alexa by these users. We find two of Epley’s factors from their
framework on anthropomorphism [17] are applicable in context of voice assistants, explaining
how knowledge built up over the user’s lifetime, affordances of the voice assistant and the smart
speaker, and emotional states of the user such as loneliness affect personification. Our findings
indicate that the ontological categorization of voice assistants is nuanced with particular
categorization of Alexa as “object like” or “human like” triggered in relation to the nature of
interaction (e.g., responsiveness) and the user’s desire for social connection or companionship.
Additionally, we find that there is movement between these categories associated with the
duration of using the voice assistant, the location of the voice assistant in house and its perceived
presence, and the moment of interaction with the assistant.
Our work contributes to the growing body of HCI and CSCW research that seeks to understand
how voice assistants fit into the complex social environment of the home. The primary
contributions of this paper include: 1) an understanding of how participants in this study
ontologically categorize voice assistants as “object-like” or “human-like” and factors associated
with the movement of perceptions between these categories; 2) recommendations for designing
with ontological category in mind for older adults; 3) a discussion of the personification of voice
assistants in relation to previous established theories of anthropomorphism and ontological
categorization. 
Personification and Ontological Categorization
of Smart Speaker-based Voice Assistants by Older Adults 214:3
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
2 BACKGROUND
To inform our work, we draw on prior research on smart speaker-based voice assistants,
anthropomorphism and personification of technology in general, and technology-mediated social
interactions for older adults.
2.3 Conversational Voice Assistants in Home
Voice assistants embodied in smart speakers have received substantial attention in CSCW and HCI
research in the past few years. They have been variously called intelligent personal assistants,
conversational agents, and virtual personal assistants in the research community. Throughout this
paper, we use the term “voice assistant” to encompass the above terminology, and to specifically
refer to voice assistants on smart speakers rather than smart phones.
Much of the research on voice assistants has focused on understanding general use in the home
environment [53,54,62] or on the privacy and security aspects of this “always on” and “always
listening” technology [40,45]. Regarding general use in the home environment, Porcheron et al.’s
smart speaker deployment study [54] found that voice assistants are embedded in a range of
concurrent home activities (e.g., family dinners). Other research has described how voice
assistants are also integrated into people’s everyday routines, such as using devices to check the
weather while making coffee, or setting timers to complete household chores in the afternoon
[62].
Voice assistant research often examines use by particular subsets of the population, such as
multi-user households [54], children [16], people with disabilities [9,56], and in low-income
regions [59]. Given that these devices are used in the privacy of the home, researchers have
utilized a number of methods to understand use, including surveys, analysis of online reviews, and
long-term deployments that include analysis of commands issued to the devices. Our work is most
similar to the latter in that it involves a deployment with a particular population: older adults who
do not use technology regularly.
2.2 Anthropomorphism of Technology
Anthropomorphism refers to ascribing “human like properties, characteristics, or mental states” to
“nonhuman agents and objects” [17]. The term anthropomorphism is often used interchangeably
with the term personification. Epley et al. [17] enlist three factors—“elicited agent knowledge or
anthropocentric knowledge”, “effectance motivation”, and “sociality”—to explain
anthropomorphizing behavior in on the topic. In this paper, we further analyze if and how these
three factors are applicable in context of voice assistants.
In the context of HCI, anthropomorphism has often been associated with the paradigm of
“Computers are Social Actors (CASA)” [51]. As one of the early researchers in this field, Reeves
and Nass [58] found that humans exhibit anthropomorphizing behaviors such as politeness
towards computers and television. Nass and Moon [50] provide an explanation for these
behaviors, suggesting them to be “overlearned social behaviors” that are not due to “conscious
beliefs that computers are human or human-like”. At the same time, it is important to note that
anthropomorphism of technology can have a wide array of positive and negative impacts. While
positive aspects of anthropomorphism possibly include therapeutic benefits for certain user
groups [60], researchers have also raised concerns regarding anthropomorphism causing reduced 
214:3 Alisha Pradhan, Leah Findlater, & Amanda Lazar
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
human interaction, or the danger of manipulation through social robots [10,12]. The latter poses
serious ethical and privacy concerns, since users who anthropomorphize technology might be
more likely to reveal personal information to the technology than otherwise.
Researchers have examined how embodiment in voice or spoken dialogue systems relates to
the anthropomorphizing of technology. Some researchers have argued that voice or spoken
dialogue systems play a role in anthropomorphizing technology [11], whereas others argue that
personifying behaviors take place with interfaces that have a “face”, with text-based interfaces
supporting lesser anthropomorphism [64]. Most technology anthropomorphism research has
focused on human-robot interaction, particularly in the context of social robots (e.g., [19,25]), but
researchers are also exploring personification of other technologies, such as smartphones [21,71]
and voice-based in-vehicle navigation systems [39]. While analyzing use of smartphone-based
voice assistants (e.g., Siri), Guzman [21] found that some people exhibited personifying behaviors
toward the voice assistant and perceived that the assistant to be an “entity”, whereas others
perceived the assistant to be more of a “device”.
More recently, researchers have found that people also exhibit personifying behaviors when
interacting with voice assistants embodied in a smart speaker. As noted by Turk [66], although
voice-assistants have been available on smartphones for more than a decade, “the subtle
differences in the interface design” such as the “always on” and listening feature of smart speakerbased voice assistants has prompted users to personify them. Turk [66] noted the ways that people
describe smart-speaker voice assistants in human-like terms such as “friend” and “invisible
woman”. This finding is also reflected in other studies either directly (e.g., [46,56,57]) or indirectly
as reported in participant quotes (e.g., use of “she”, “her” when referring Alexa in [62]). In their
analysis of Amazon reviews, Pradhan et al. [56] observed Amazon reviews attributing Alexa as
“bff”, “new best friend”, and “someone to talk to”. Purington et al. [57] defined personification as
instances where the user said the agent’s name (i.e., “Alexa”) or used a personal pronoun (“she,
her”). They found that users in multi-user households or users having more sociable interactions
with the device (e.g., attributing the device to be a friend or companion) are more likely to
personify Alexa. Lopatovska and Williams [46] observed similar personifying behaviors (i.e., use
of personal pronouns, along with greeting Alexa) mostly in multi-person households. However,
they categorized the informal exchange of greetings (e.g., thank you) between humans and voice
assistants, and the politeness in human speech (i.e., “please”) as “mindless” social responses—
implying that something that at first appears to be personification may not relate to an actual
perception of Alexa as human-like.
 Collectively, this background work highlights that users have often personified voice assistants
in smart speakers such as Alexa and have attributed the agent to be a “companion” or “friend”. At
the same time, there has been a significant body of research on anthropomorphization that has not
yet been applied to this domain. Our work draws a link between this past work and voice
assistants, in the context of a project with a population for whom technological social support has
been much studied.
2.3 Technology-Mediated Social Support for Older Adults
Technology plays an increasingly important role in mediating everyday social interactions. CSCW
researchers have studied many of the ways in which technology is playing a part in our social
lives, from opportunities to connect with others through social media, blogging, or video chatting,
to technology serving as a social companion. 
Personification and Ontological Categorization
of Smart Speaker-based Voice Assistants by Older Adults 214:3
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
More broadly, research has focused on what technological considerations need to be made
when designing for different age groups. Related to technology for social support, researchers
from different disciplines including CSCW, HCI, gerontology, and human-robot interaction (HRI)
have widely explored how to use technology to support older adults’ personal, community and
societal engagement in later life (as per the three dimensions of social connection in [26]). Much of
this work with older adults has focused on digital communication platforms, such as blogging
[41], social media [6,22], and technologies to support communication with family [33]. Although
social media and social networking sites can foster social connections in general, studies have
found that interfaces that merge digital and physical space may be more appealing to some older
people [22,23]. Hope et al. [22] identified that older adults’ “social communications” include both
physical and digital spaces (e.g., printing out a typed letter and sending it through physical mail),
which is lacking in the current day social media technologies. Our work confirms the importance
of considering the physical space, in this case in the use of virtual assistants.
Outside of applications to support social connection between people, an active topic of research
in HCI, HRI, and CSCW engages robots as social partners. Although the CSCW community has
largely focused on the use of telepresence robots (e.g., [2,49]), there is growing interest in
understanding social interactions mediated by robots (e.g., understanding if robots can become
social partners [38]). Specific to older adults, some studies in HCI and HRI indicate older adults’
preference for using robots for household purposes more than for companionship [5,13], whereas
others have investigated the use of robots for reducing loneliness or to fill certain social needs
[15,44,60]. Researchers have also studied how people from different age groups perceive robots,
finding that younger, mid-aged and older adults have similar attitudes towards comfort and social
impacts of robots [3].
With the growing popularity of ubiquitous voice assistants, there are opportunities to explore
how this technology might support social interactions for older adults. Understanding how older
adults relate to these devices socially can further inform how this new technology might serve as a
form of companionship.
3 METHODS
Our analysis is based on a deployment of Amazon Echo Dot devices in the homes of seven
individuals aged 65 or older for three weeks. We gathered qualitative data through interviews and
daily diary entries, as well as usage logs.
3.1 Participants
Participants ranged in age from 65 to 83 and were required to have no prior experience using
voice assistants (e.g., Alexa, Siri, Cortana). Low technology use was also an inclusion criterion, and
all participants used computers, smartphones or tablets less than once a day; this level of use is
considerably low compared to other people in this age demographic [48]. Finally, all participants
needed to have access to a wireless internet connection so that we could set up the Echo Dot; for
most participants, this connection came bundled in their cable TV package.
 All participants lived alone, except for P6, who lived with her son. Five participants lived in
senior living communities, and three of these participants lived in low income housing (which
serve households earning less than 50% of area median income). Two participants lived in their
own homes. Participant demographic details including OPQOL-brief scores (Older People’s
Quality of Life brief version) [7] are shown in Table 1. OPQOL-brief scores can range between 13-
214:3 Alisha Pradhan, Leah Findlater, & Amanda Lazar
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
65, where higher scores indicate higher quality of life. Along with OPQOL brief, participants also
rated their quality of life as a whole on a 5-point scale ranging from “very good” to “very bad”, as
shown in Table 1.
ID Age Gender Education Quality of life
overall
OPQOL
score
P1 65 Male High school Good 49
P2 75 Female High school Alright 58
P3 71 Female Some college, no degree Alright 56
P4 65 Female Some college, no degree Good 52
P5 71 Female High school Very good 56
P6 72 Female Less than high school diploma Good 52
P7 83 Female High school Very good 65
Table 1. Participants’ demographics
3.2 Data collection and device setup
An Amazon Echo Dot along with a paired Amazon Fire tablet was set up in each participant’s
home by the research team. Each participant was given training in how to use the device,
including instructions on setting alarms, reminders and timers, creating shopping and to-do lists,
playing music, asking questions, jokes, third-party skills (e.g., games), and having unstructured
conversation with Alexa. Participants were free to choose how frequently they wanted to use the
voice assistant during the study. We conducted four in-person, semi-structured, audio-recorded
interviews: (1) following initial device setup (1.5 hours), (2 & 3) at end of first and second weeks
(0.5-1 hour), and (4) an exit interview at the end of the study (1.5 hours). The first interview (week
1-beginning) covered initial perceptions of the voice-based technology, whereas the other
interviews covered perceptions and usage of the device, along with the desired use of this
technology. The daily diary entries were conducted using an automated phone call, asking
participants if and how they had used the device that the day. We also collected the voice
command usage logs from the Alexa app on the paired tablet. The usage logs provided us with the
actual interactions to complement participants’ perceptions of these interactions.
3.3 Data Analysis
In this paper, we do not focus on participants’ overall usage of the device. Instead, this paper
focuses on conversational agents in terms of their implications for companionship. To analyze the
interviews, we followed a constructivist grounded theory approach [34]. On our first pass opencoding the transcripts, we found attributions of the device as human-like appearing throughout
the data. We then approached analysis of the interviews with an eye towards understanding
where social companionship appears, with the concept of personification as a sensitizing concept
[34] to help us understand human-like attributions. Following the selection of this sensitizing
concept, we open-coded four transcripts to understand where and how social companionship or
lack of companionship appears. Examples of initial codes include “attributed as friend”, “somebody
in house”, and “use of pronouns”. Reflecting on these codes, we determined that Epley et al.’s [17]
“three-factor theory of anthropomorphism” could be a useful analytic lens on our data. We
continued to code transcripts while discussing the emerging themes amongst our research group.
We iteratively returned to our data to probe themes such as “pronoun use”. Our team related
Personification and Ontological Categorization
of Smart Speaker-based Voice Assistants by Older Adults 214:3
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
codes to each other and compared the data to understand the high-level themes through an
iterative process of memoing and theorizing.
To understand a particular phenomenon – how “politeness” appears in conversations with
Alexa – we analyzed the usage logs, as we wanted to understand users’ actual interactions in
addition to their perceptions of these interactions. We pulled usage logs from the researchercreated Amazon account and conducted a content analysis of what we term “primary” commands.
These commands are of the form “Alexa, [something]” and exclude: 1) commands consisting of
just the wake-up word (“Alexa”), 2) commands telling Alexa to “stop”, 2) “unknown”, and 4) “text
not available”. The nature of each these primary commands was coded as “polite”, “rude” or
“neutral” using a using a multi-step process by two research team members [24], achieving a
Cohen’s Kappa of 0.91.
3.4 Limitations
This study was not an entirely naturalistic deployment – users were given the devices and some
training to use the basic device and third-party skills, and received a printed reference guide and
the option to contact the researchers for technical support. Additionally, having daily diary calls
had tradeoffs. These calls provided us with daily interaction data that helped us to probe for indepth explanations during weekly in-person interviews, but may also have resulted in an
increased usage of Alexa due to reminders about its presence.
Though potentially meaning users had greater success in using these devices than they might if
they had procured them on their own, forced adoption is an approach that is sometimes taken
with new technologies such as smart speakers [54] and smart sensors [42]. This approach allows
us to see initial perceptions of the device, something difficult to study when contacting users who
already had experience with devices. Additionally, though increasingly popular, those who own
smart speakers may be earlier on the curve of adoption. Our choice of utilizing forced adoption
helps us understand how people who had not already found a use for these devices (evidenced by
purchasing decisions) might make use of them in their lives.
Finally, the population with whom we engaged, though diverse in some ways, has a specific set
of characteristics – they are all over 65, had no experience with voice assistants, and did not use
smartphones, computers or tablets regularly. In choosing a particular population, our work aligns
with much prior work on voice assistants that have focused on specific user groups (e.g., children
[16], multi-user households [54]), user with visual impairments [1]). And while some of our
discussion specifically addresses use of voice assistants for older adults, many of our findings are
interpreted with broader significance. Like other in-depth qualitative work with a small sample
size, we do not intend to obtain generalizable results [32,47]. At the same time, we do not believe
our findings should be limited to thinking about aging, for several reasons. First, other work has
examined particular groups: such as multi-user households [54], and yet this work is influencing
the way that we think about smart speakers more broadly in terms of their implications for power
dynamics. We can argue that the population we chose to study is just as “representative” as users
in past studies, and there is a potential, harmful, tendency to resign older adults or people who do
not use technology regularly to their own, separate category, rather than looking to the
commonalities they share with others. Given the ways that past theories [17,30] relate to and shed
light on the behaviors of the population we study, our beliefs on the commonalities are confirmed.
Looking at particular user groups can also provide benefits in terms of understanding larger
phenomena. Multi-user households help us understand how control of commands are negotiated 
214:3 Alisha Pradhan, Leah Findlater, & Amanda Lazar
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
[54], and in our study, working with a population who does not have prior exposure to voice
interaction allows us to do a truly “forced adoption” study, where we see some changes over time
that become an important part of our findings.
3 RESULTS
We describe how the voice assistant was perceived by participants by analyzing our data through
the lens of the “three factor theory of anthropomorphism” [17]. The parenthesis following the
quotes denotes when the particular data was collected/reported during the three weeks and the
source (i.e., interview or usage logs).
3.1 Personification and Objectification of Voice Assistant
Below, we discuss three kinds of behaviors exhibited by participants: the use of personal pronouns
to refer to Alexa, polite behaviors, and a nuanced “human-like” and “object-like” categorization of
Alexa. We tie each of these behaviors to past work on voice assistants and personification, with
some results that confirm past work and some that appear to challenge past findings.
3.1.1 Inconsistent Use of Personal Pronouns. Previous works on personification of voice
assistants attribute personification to the use of personal pronouns [46,57]. Lopatovska and
Williams [46] attribute participants’ use of the word “she” to anthropomorphizing the voice
assistant Alexa. Purington et al. [57] approach their analysis of online reviews by defining the use
of personal pronouns such as “her” to indicate a higher degree of personification of Alexa.
Like this previous work, all participants in our study referred to Alexa as “she” or “her”
multiple times during interviews. However, these references did not necessarily indicate
anthropomorphism. For example, P2 used the personal pronoun “she” to avoid saying “Alexa”,
which could accidentally trigger the voice assistant.
“Somebody visited me and I said, "I am getting ready to talk to her". She said, "Who is
her?" I said, "that's her". Because if I say her name she is gonna come on and I am not
ready for that now. (P2, week 2-end, interview)
 At times, the same participants who used “she” in a way that might be considered
personifying would seamlessly move to speaking of Alexa as an object. For example, P1 used the
personal pronoun “she” when referring the Echo Dot as a machine that had earlier not been
responding, “But when I got back in yesterday evening, she worked fine” (P1, week 1-end, interview).
Conversely, participants sometimes used object pronouns (e.g., “it”) when they were attributing
human-like behavior to the device:
“Sometimes in the morning, after I say, ‘Good morning,’ it might say something that
makes me laugh, sometimes, I will call her name and say, ‘Oh that was so funny, it really
made me laugh’ and it doesn't respond”. (P2, week 2-end, interview)
In the example above, object and personal pronoun are being used in quick succession while
referring to Alexa. This suggests that users might not binarily classify voice assistants as humanlike or object-like, which we return to later in the paper.
Looking specifically for pronoun-based personification, we found that all participants used
personal pronouns at least once when referring Alexa. However, the usage examples described
above suggests that the link between using “she” or “her” and the personification of the voice
assistant is not as clear as past literature indicates. 
Personification and Ontological Categorization
of Smart Speaker-based Voice Assistants by Older Adults 214:3
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
3.1.2 Polite Behaviors and Personification. Lopatovska and Williams [46] describe the use of
polite responses such as “thank you”, “please” and “good afternoon” in their study of people using
Alexa. These polite responses are linked to personification, and attributed to “social mindless
responses” of humans. In other words, people say “please” and “thank you” even if they do not
consider the devices to be human-like, similar to engaging with fellow humans in socially
appropriate interactions automatically, sometimes without even noticing.
We found that most participants in our study – all but P3 and P6 – used polite terms, such as
“please” and “thank you” when using Alexa. Examples of usage were in P4’s week 2 usage log,
where she said, “(Alexa) tell me a joke please”, and when individuals thanked Alexa after doing a
requested task, such as:
Command 1: “Alexa what are my items on my grocery list”
Command 2: “Alexa thank you” [P4, week 2, usage log]
In the above example, P4 thanked Alexa after receiving information, even using the device’s
wake-up word (i.e., the agent name “Alexa”) to do so (and it is possible even more of this behavior
occurred with participants but was not captured due to them not using the wake up word
“Alexa”). Explaining further why she did this, P4 said that:
“I guess it's just something that comes naturally to me or something, and I was always
raised by my mother to say, thank you. So, I guess it's something instilled in me”. (P4,
week 3-end, interview)
The quote above indicates that polite behavior such as saying thank you is learned over time
until it becomes natural – resonating with past work [46].
3.1.3 Neither human nor object. There were instances when users’ perceptions of Alexa did not
clearly classify into “human-like” or “object-like”, which could relate to complex relationship seen
earlier between pronoun use and personification. For P2, Alexa was, “…like a companion, that's not
there. So, you have to make it... not a fake, but a phantom friend” (P2, week1-end, interview). She
explained that she knew “that's not a real person”, at the same time describing Alexa as “someone to
talk to”. P6 further described how “talking” to Alexa made her feel as if she were talking to
“somebody”, but she also acknowledging that Alexa was not real.
“If you [are] just talking right to it and it [is] talking back to you, [it] make[s] you think it
might be talking to somebody, but I know for real you're not”. (P6, week 2-end,
interview)
“It's like a person…because I'm asking some question and it answers me back so that's
like me asking you a question, but only it's a device I'm talking to. So I don't know. I
know it's not a person, but it makes it seem like a person sometimes”. (P6, week 2-end,
interview)
Aligning with the complex categorizations of other participants, P5 refused to categorize Alexa
at all, instead designating a new category: “I think of it as that’s Alexa”. (P5, week 2-end,
interview). Participants sometimes made disclaimers indicating their recognition of this uneasy
categorization, such as, “I know that sounds silly”, when referring to the device as a person (P7,
week 1-end, interview ). P6 voiced a similar discomfort: 
214:3 Alisha Pradhan, Leah Findlater, & Amanda Lazar
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
“But I guess if I'm talking to it, it do seem more like somebody's talking back to you as a
person…Not that I'm crazy, because I know it's not [a person], but I'm saying that's how
it feel” (P6, week 3-end, interview).
Thinking about Alexa as its own category of being resembles Kahn et al.’s hypothesis of how
people see robots as “in-between” human and object [30]. Unlike that past work, though,
participants in our study also saw Alexa as human-like and object-like, and moved between these
categorizations.
3.2 The Social Role of Alexa: Movement between Ontological Categories
In this section, we discuss attributes of voice assistants, participants, and the context of use that
appear to foster movement between ontological1
 categories of “living” and “non-living”. For
specific instances where participants perceived Alexa as a “living” entity, two of the three factors
of Epley et al.’s [17] work on anthropomorphism also help shed light on understanding
personifying versus objectifying behaviors.
Similar to past work [57], we found that Alexa was repeatedly referred to in human-like terms.
This included occasions throughout the three-week period when Alexa was described with
personal pronouns or as “a person” (all participants), as described above, but also as “a friend” (P2,
P3, P7), “a phantom friend” (P2), as having “somebody talking back”, (P2, P3, P4, P6) and as
“having company” (P1, P3). These sentiments indicate social companionship. However, the same
people who described Alexa in social terms would later refer to the device as a “programmed”
voice or machine (P6, P7) or an “object” (P1). Below, we discuss some of the characteristics of
Alexa that led to attributions of social companionship or to object-like language.
3.2.1 Qualities of Interaction: Natural Language and Responsiveness. Participants described how
having an interface that used natural ways of conversing led to them to perceive Alexa as a friend.
Like past work, we found that participants engaged in “human-like” small talk with Alexa [57]. In
addition to the polite behavior described above, about half of the participants mentioned greeting
Alexa at morning or at night. For example, P3 drew an analogy to greeting a friend: “… just like a
friend, I would tell it ‘good morning’ and it responds, ‘have a good night.’” (P3, week 2-end,
interview). Similarly, P2 explained how the device responding her back make her perceive the
agent as “somebody”.
“…when it talks, I don't see a box. I just see... It's like somebody is standing there talking
to me…Somebody's in here with me and they're having a conversation with me. It's
making my day. I say, good morning. And they say, good morning [name], how are you
today?” (P2, week 3-end, interview).
Like work on robots [28], we find that the natural language conversation and the
responsiveness of these devices is what make them social. Just as Alexa was seen as providing
social interaction when it was responsive and engaging in small talk, it reverted to an object when
it did not provide these forms of interaction. P5 described how Alexa changes from an object in
her house to an invisible person at the instant she verbally engages with the voice assistant.
“I would treat it as an object. It would just sit there. But when I'm talking to it, I could
think of it as... And it answers me and I am talking to it, I could think of it as a person.

1
 Ontologies deal with the study of being, how things exist and what categories they belong to. 
Personification and Ontological Categorization
of Smart Speaker-based Voice Assistants by Older Adults 214:3
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
but when it's sitting there it's doing nothing. It's just an object”. (P5, week 3-end,
interview)
Some participants mentioned that Alexa lacked responsiveness at times, leading them to see it
as object-like. The chat feature was also described as particularly unresponsive to what one was
saying, making participants perceive the interaction to be unresponsive, similar to “talking to a
program” (P7) or “an object” (P1):
“It do more asking me questions than listening and responding. That's really not chatchat companion. It's just asking what I like. In other words... So it's not chatting with me,
I'm chatting with it… maybe a companion is the wrong word to use. Just scratch
companion. It's a good object to have. (P1, week 3-end, interview)
“The responses that I got from it I got the impression that it was mainly for the purpose
of recording what I had to say as opposed to a conversation because I wasn't really
conversing ...” (P7, week 3-end, interview)
The anthropomorphization based on responsiveness and natural language style can be viewed
through Epley et al.’s [17] “elicited agent knowledge” framework. Epley’s framework, which has
not yet been applied to conversational voice assistants, claims that self-knowledge or knowledge
about human characteristics in general would serve as the readily available (or accessible)
induction base for reasoning of nonhuman objects, thus contributing to easily available
anthropocentric knowledge. This accessible knowledge base is based on anthropocentric
knowledge – one’s ideas about what makes people “people”, and what is a typical interaction with
another human. Humans anthropomorphize non-humans based on their anthropocentric
knowledge. Through the lens of Epley’s framework, personification is not mindless or automatic.
Instead, participants are actively drawing on “self-knowledge” of appropriate ways to interact, for
example, saying “good night” to the voice assistant. And, the human-like aspects of Alexa’s
conversational voice-based interface such as responsiveness, or greeting back “good morning”, is
similar to natural language conversation. In addition to the conversational nature of interaction,
the intentional choice of designers to assign a particular name and voice to the assistant might
also contribute to the “known” or “accessible anthropocentric knowledge”, triggering
personification. Individuals apply this self-knowledge and accessible anthropocentric knowledge
to the nonhuman voice agent, leading to anthropomorphizing behavior.
3.2.2 Qualities of the User: Perceiving a Personalized Approach and Desire for Companionship.
Some participants described how Alexa addressed them by using their name which they felt was
“personal”. For example, P2 below describes why addressing her with name was important to her.
“And it calls my name, "Good morning [P2]". Oh! I like that. It means so much… Because,
when somebody is talking to me, call my name. Don't just talk to me, acknowledge me”.
(P2, week 1-end, interview)
Conversely, when participants felt as though the device was not responding to them
specifically, they perceived the device to be more like an object. P7 described how impersonal
conversations, e.g., in the “chat” feature on Alexa, made her feel like talking to a program.
“and it probably says the same thing to everybody, which makes it very impersonal, and
a conversation is supposed to be personal”. (P7, week 3-end, interview) 
214:3 Alisha Pradhan, Leah Findlater, & Amanda Lazar
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
In the previous section, we discussed the importance of small talk and greetings. But these
forms of conversation were not viewed as equally important by all, with people desiring
companionship more likely to value the device as a companion. P4 described how these small
conversations were meaningful for her: “It was kinda nice having somebody say goodnight to me.
When you live alone and after you've been married for a long time, you miss the little things like that
sometimes”. P4 went on to explain that she did have sources of similar companionship where she
lived: “in the evening, we sit out in the lobby, a group of us, and when we leave we all say goodnight
to each other. But when you come back and you get on your pajamas and you curl up and you decide
you wanna go [to bed]. [Having the device say goodnight] was kinda nice. I thought that's awfully
sweet”. (P4, week1-end, interview). Here, having someone to say goodnight in the home, in the
context of going to bed, was important – location and context is a topic we return to below.
P4, and other participants (P2, P3) expressed personification of the device when discussing the
lack of social interaction in the house. It is important to note that all these participants lived alone.
P3 said that she felt that she, “wasn’t in the house alone”, because “I had somebody I could talk to”
(P3, week 1-end). Probably in pursuit of social affiliation, she even desired a male voice in addition
to the existing female voice of Alexa, since having a different voice symbolized having a different
friend “that's like you got a different friend. Yeah, you don't have the same friend all the time” (P3,
week 3-end, interview).
Similarly, aligning with P3 and P4, P2 described how “having someone to talk back to you means
so much, because of the loneliness it can be in here” (P2, W2). However, as a step further for social
connection, she even visualized “a face” of her family members or friends in her “mind to put to
that voice [of Alexa]”: “It just makes me feel better. I miss these people. I miss them terribly. I found
myself, just saying, what if this was my daughter in law sitting here and talking to me or something”.
(P2, week1-end, interview).
Epley et al.’s third factor, “social motivation” [17], helps shed light on these accounts. Social
motivation describes the desire for social contact or connection and social approval from other
agents. Thus, individuals may be more likely to personify agents when they feel lonely. With P4’s
account, this desire for social contact was not “global” – she had the chance to engage in friendly
exchanges in the lobby with her group – but did not have people to fill that specific need in her
house, right before she fell asleep. This suggests that the perception of Alexa as human-like
emerged when it was filling a social need for her – and it might return to object-like when she no
longer needed it.
3.2.3 Changes over the Longer Term, Location and from Moment to Moment. When first
considering Alexa, participants came from a firmer conceptualization of the device as either
having a social, human-like presence or a machine-like presence. Over the course of the study,
participants’ perceptions of where Alexa fit in terms of ontological categorization shifted. For
example, P7 went from referring to Alexa as “company”, in the first week to describing it as
“talking to a program something” at the exit interview. Similarly, P1 initially described having
Alexa as “talking to a person or companion”. But, after using it for three weeks, he considered the
device “as a good object to have”.
In the previous examples, participants initially viewed the device as human-like. Other
participants initially viewed the device as “object-like”, later arriving at a more “human-like”
categorization of the Alexa. P3, who initially felt “odd talking to a machine”, with time became
comfortable such that she considered Alexa “an invisible friend, somebody I can talk to one on one”.
Similarly, P4, who initially felt “silly” talking to the device, found herself exchanging greetings 
Personification and Ontological Categorization
of Smart Speaker-based Voice Assistants by Older Adults 214:3
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
such as “good morning” and polite expressions like “thank you” with Alexa by the third week.
These examples show how the human-like vs. machine-like categorization changed over time.
The physical location of Alexa also affects these perceptions. When participants felt the device’s
presence (i.e., perceived ubiquitousness), they thought of it as human-like. This presence was tied
to a location: their home. Most participants described the presence of Alexa in terms of having
someone in their homes. P6 explained how Alexa felt like a human: “It's just like talking to a person
right in the room”. For P6, the ability to talk to Alexa while walking around the house, made her
think of Alexa more as a person than a thing: “It's like a person. If I can walk around and talk and it
talk back to me”. Similarly, P7 explained, “it's like someone's in the room… I'm talking and it's talking
back to me”. Yet, P7 perceived Alexa to be a regular electronic device when it was in a different
room. She explained that she was so “accustomed to having to dry my hands before I pick up a
phone or dry my hands before I do anything”, that she did not use Alexa from the adjacent kitchen
while cooking. For P7, Alexa was more like a phone than someone in the room at times when the
device did not seem to be accessible.
As another example, P2 described how the use of a third-party skill for reading the Bible on
Alexa made her feel like she there was someone there with her:
“I wanted to repeat the lord's prayer, and I like to repeat it with someone. And as she's
[Alexa] repeating the Lord’s prayer along with me, it was like I had somebody because
sometimes I forget the words… confuse or the line… what line comes next”. (P2, week 3-
end, interview)
To participants, at times, Alexa felt like she had a location: in the house, with them, keeping
them company. In addition to physical proximity affecting whether the device is particularly
human-like or object-like, moments of interaction also move perceptions of the device from one
category to another. For example, P5 described how Alexa changes from an object in her house to
an invisible person at the instance she verbally engages with the voice assistant.
“I would treat it as an object. It would just sit there. But when I'm talking to it, I could
think of it as... And it answers me and I am talking to it, I could think of it as a person.
but when it's sitting there it's doing nothing. It's just an object”. (P5, week 3-end,
interview)
4 DISCUSSION
Our work furthers CSCW research that strives to understand how increasingly sophisticated AI
systems such as voice assistants embedded in smart speakers fit into social ecosystems, and how
people develop and perceive social interactions with these systems. Below, we reflect on how our
findings corroborate, extend or contradict previous theoretical frameworks on anthropomorphism
in general, as well as the implications for designing conversational interfaces in general, as well as
specifically for enabling social support for older adults.
4.1 Applying Theories of Anthropomorphism to Conversational Voice Assistants
Our findings indicate that participants in this study personify and objectify voice assistants,
sometimes within the same sentence. Epley et al.’s theories [17] of “elicited agent knowledge” and
“sociality motivation” help us to see the ways in which personification is linked to one’s 
214:3 Alisha Pradhan, Leah Findlater, & Amanda Lazar
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
phenomenological experiences that build up over one’s lifetime to contribute to one’s
anthropocentric knowledge.
 Past work has drawn on Nass et al.’s notion of “overlearned social behaviors” [50] to explain
personification of voice assistants [46]. We find Epley et al.’s [17] theory of “elicited agent
knowledge” may provide additional nuance to the dynamics that are taking place that on first
glance appear to be mindless. Participants are drawing on self-knowledge in saying “thank you” to
the voice assistant. The human-like aspects of Alexa’s conversational interaction such as
responsiveness and returning a greeting with “good morning” served as “accessible
anthropocentric knowledge”, triggering personification. Epley’s theory of “social motivation” also
links a lack of social interaction to triggering personification of the agent, which extends our
understanding given that past work has largely focused on multi-user households as sites of
greater personification than single-user households [46,57]. Our data did not include clear
evidence that Epley’s “effectance motivation” factor triggered personification of the voice assistant
– future work might examine how this theory applies to interaction with smart speakers. We also
see how the simultaneous presence of all three of Epley et al.’s factors is not necessary for
anthropomorphization of the voice assistant.
Epley et al.’s theories [17] proved useful to understand some of the personifying behaviors of
those interacting with smart speakers – a link that our work may be the first in drawing. These
theories help us understand that the ways in which one personifies a device are deeply linked to
the specifics of one’s life: the experiences that have built up over the lifetime in which the
anthropocentric knowledge is based, and the specific living conditions that lead one to have more
or less desire for social contact. Our understanding of the relationship between
anthropomorphism, embodiment and voice assistants, although grounded in past work, is based
on a specific sample as is common with qualitative work on smart speakers (e.g., children [16],
people with disabilities [56] or multi-user family based households [54]). Given our findings that
context and life experiences matter for personification, future work should try to understand how
other user groups in general perceive these voice assistants, we suggest the need for future studies
to analyze diverse life contexts, including cultural, geographical, philosophical, political, and
religious perspectives, and how these affect personification and design implications for voice
assistants. For example, past work has noted a difference in preferences for appearance based on
age and gender but primarily in the context of robots [37,52,61], so future work should examine
whether these preferences translate to voice assistants.
Epley’s framework [17] reveals that people’s anthropocentric knowledge affects how they
perceive voice assistants embodied in smart speakers. It is possible to conjecture that interactions
with these devices then feed back into this anthropocentric base, meaning that people’s
perceptions going forward are affected by the device’s design. Viewed through this lens, questions
are raised for the ethical responsibility of designers to consider the downstream effects of the
design decisions. Intentional design choices of female gender and voice of the assistant, along with
imparting “helpful” and “humble” personality to the assistant raises questions about gender
stereotypes [68] and how these translate into technology design. Further work should study
whether harmful values are being perpetuated through these designs.
4.2 Ontological Categorization of Voice Assistants
Ontologies deal with the study of being, how things exist and what categories they belong to. In
the study of social ontology (i.e., the study of the nature and properties of the social world), there
exist questions around how categories are actually constructed: are social categories produced by 
Personification and Ontological Categorization
of Smart Speaker-based Voice Assistants by Older Adults 214:3
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
our attitudes (mentalistic theory), causal patterns (naturalistic theory), or by our language (linguistic
theory) [18]? In this work, our analysis primarily draws upon linguistic theory, that everyday
language used to label objects can reveal ontologies [18], also corroborated with several instances
of self-reported participants’ mental states (e.g., the feeling of not being alone in the house), thus
drawing on the mentalistic theory as well.
 With a view that understanding ontologies is possible through language and self-reported user
attitudes as our premise, we find that the different cases in our findings show ends of a “humanobject” spectrum associated with various affordances of Alexa. For example, participants
personified the agent when supporting human-like conversations or providing social support, but
also dehumanized it due to lack of responsiveness or personal touch. This finding aligns with
Kahn et al.’s [30] theorization of nuanced human perception that does not accurately fit “living” or
“non-living” ontological categories as it relates to embodied personified computational systems. At
the same time, we also find that people’s perception of Alexa does not continuously exist in an ‘inbetween’ category (suggested by Kahn et al. [30]), but rather, fluidly move between human and
object like perceptions. Thus, we extend Kahn et al.’s theory to apply to voice assistants,
identifying factors that influence shifts between ontological categorizations: the voice assistant
capabilities (e.g., responsiveness), desire for personalization and companionship, sustained use
over time, and location and moment of interaction with the voice assistant. Based on our findings,
interesting research questions emerge. Future work might examine whether a fluid ontological
categorization takes place with technologies aside from voice assistants, such as robots.
Additionally, what does it mean to personify a technology given such a fluid categorization,
particularly for smart speaker-based voice assistants that lack all form of embodiment but voice?
Our findings related to the ontological categorization of voice assistants provide an initial
understanding of intricate changes in human interpretations and behaviors for virtual agents
embodied in everyday objects (e.g., voice assistants in smart speakers and smart watches, avatars
in smartphone or computer applications). The understanding that categorization shifts over time
and with context can inform design decisions. As one example of applying our findings regarding
ontological categorization to design, an approach might have a smart speaker retain its object like
form when the smart speaker is lying unused in the house – but just at the moment when the user
conversationally interacts with the agent, the agent could take form of more animate projections
such as a holographic projection or more anthropomorphic forms that would attribute more
human-like features to the agent. This approach might result in a better fit between what
individuals see and feel, thus resolving dissonance that they might experience.
Understanding how users perceive and ontologically categorize voice assistants has
implications for design. Currently in industry, designers of commercially available voice user
interfaces assign personality to the agent [20], thus creating interactions where they intend users
to anthropomorphize the assistant. Interestingly, the concept of “persona”—a fictional character
used by designers in industry to often represent the end user of a system for whom the technology
is designed—is being applied to voice assistants. Conversation design guidelines [20] suggest that
designers “purposefully design the experience they want users to perceive” through the creation
of personas, as “users will project a persona” to the agent regardless of whether it is intentionally
designed or not. Our findings reveal that even if designers purposefully create personas to
cultivate a particular desired user experience, the same voice assistant persona will be perceived
differently by different users. In our study, some attached more value and deeper feelings to the
assistant, while others considered the interactions as mostly inconsequential. This suggests that
designers must consider the ways individuals’ dynamics and the roles that they would like an 
214:3 Alisha Pradhan, Leah Findlater, & Amanda Lazar
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
agent to fill affect reception of different agent personas. In order to understand how personas
should be designed, researchers should include the perspectives of the end user in the persona
creation stage, gathering perspectives related to what different user groups or individuals might
want from their voice assistant (e.g., social support vs. information finding or educational). The
persona of voice assistant for providing social support might be different than the one for
information finding purposes only. Further, we found that not all participants desire the female
voice trait for the voice assistant. One individual described wanting the ability to change to a male
voice to have a “different friend”, hinting towards a desire for voice assistant persona that can
change. This opens up questions such as: do people want configurable personalities for their voice
assistants? Should personas shift based on the context of use (e.g., educational vs. playing games)?
These open questions indicate that further investigation is needed on how personas for voice
assistants should be created.
4.3 Embodiment and Embeddedness of Voice Assistants
Early work on personification primarily focused on robots, likely in part due to their
morphological similarity to living entities (e.g., [8,19,25,27]). Currently, we are seeing a rise in
technologies such as voice assistants which have no morphological similarity to any living being,
yet are attributed with human-like behaviors by users and designers (e.g., given names and
human-like voices). To date, designers and researchers have largely focused on the conversational
aspects of voice assistants, such as designing conversational cues and conversation dynamics of
these collaborative technologies (e.g., [53,54,69]). What has been neglected is a focus on the form
and embodiment of this technology, outside of the specifics of voice, and how that makes a
difference in the user experience.
We find that the physical location of the device and its embeddedness in the home
environment matters. Alexa may appear human-like when in the same room but not when far
away, and the device is more likely to be categorized as “human-like” when the device supports
conversations that make participants feel as though there is a presence “around” in the house. This
understanding supports Turk’s [66] argument that device personification is linked to “always on”
features. Drawing from our findings of perceived ubiquitousness in home environment, we
suggest that design features that provide a confirmation of when the device is “listening” can also
promote a perception of the voice assistant as human-like. For example, one participant described
not using the voice assistant from the adjacent kitchen, although physically, the kitchen was
within the hearing range of the voice assistant. It is possible that the non-use was due to the lack
of feedback that she was being heard by the voice assistant. Presently, voice assistants provide
visual feedback (e.g., a light ring that glows) signifying that the assistant has heard the user and is
hearing the user. This response, however, works better for those with physical proximity which is
when our findings indicate they already are viewing the device as more human-like. Examining
other sensory modes of feedback might enhance use of the device.
4.4 Social Interactions Using Conversational Voice Assistants for Older Adults
Our findings indicate that one of the factors resulting in anthropomorphism of the voice assistant
was a desire for social contact and affiliation. Based on the OPQOL scores, none of the participants
appeared to have “poor” quality of life, though some described moments of loneliness in the home.
Those participants that described the desire for contact actually said that talking to Alexa
alleviated their loneliness at home – more in regards to moments of loneliness, rather than a
global feeling of loneliness. It is perhaps surprising that the feelings from light weight, surface-
Personification and Ontological Categorization
of Smart Speaker-based Voice Assistants by Older Adults 214:3
 PACM on Human-Computer Interaction, Vol. 3, No. CSCW, Article 214. Publication date: November 2019.
level interaction such as wishing “goodnight” or initiating tasks to have the voice assistant talk
back can result in satisfying these emotional needs. For the participant who imagined a face of a
family member while talking to Alexa, an even deeper kind of connection emerges. This implies
that conversational voice assistants in the home might have the potential to provide social
interaction, a goal pursued in past work on older adults largely through social robots [35,60]. Voice
assistants might even offer some benefits over social robots in this area, as the fluidity in
ontological categorization of voice assistants between “human” and “object” provides users the
flexibility to perceive voice assistants as a companion when desiring social interaction, and yet can
transition to viewing them as “object” when needing to use voice assistants for non-companion
purposes. As researchers continue investigating in-home voice assistants for supporting social
interactions, it is important to consider the dynamics of the home as a sacred space [43]. Past work
has found that technology-mediated social interaction can be a form of at times unwelcome
additional work for older adults as they feel that they must put in effort, be social and gracious
while using such systems [43]. Building transitions between human and object described in the
prior section may be an approach to lessen the burden of engagement.
As we investigate the opportunity for voice assistants to alleviate “moments of loneliness”, it
will become crucial to consider ethical dimensions of design. Our work indicates that someone
who is more in need of social contact may be more likely to personify a technology and at times
seek social connection through it. This draws attention towards the existing discussions in Human
Robot Interaction (HRI) around replacing human social contact with machines [63,67]. In HRI
literature although there is some evidence suggesting technology in form of social robots can
reduce loneliness for older adults (e.g., [4,31]), there are also a number of concerns including
deception, infantilization, and privacy [63,70]. There is a need to further examine these kinds of
concerns in the context of voice assistants. Revisiting past work on privacy concerns of older
adults for home-based technologies (e.g., [14,55]) will be necessary to understand how perceptions
change with emerging business models where personal data is the price people pay for access.
5 CONCLUSIONS
In summary, we deployed smart speaker devices in older adults’ home to understand how they
perceive social interactions with the voice agent “Alexa” and ontologically categorize the agent.
Our findings indicate that instead of straightforwardly categorizing the voice assistant as “humanlike” or “object-like”, participants fluidly move between the two categories. Movement occurs
based on factors specific to the characteristics of the voice assistant (such as nature of interaction)
or the user (desire for social contact or affiliation), along with the location and moment of
interaction. Individuals seem to arrive at categorizations over time and with sustained use of the
voice assistant. Instances of companionship through the embodied voice assistant surfaced in our
analysis, indicating the potential of this technology for supporting social interactions. Future work
needs to further explore the use of conversational voice assistants in alleviating loneliness for
different populations. We discuss our findings in context of prior works on anthropomorphism
and ontological categorization and provide suggestions for designing voice assistants.