Abstract
Context:
Software projects involve technical and managerial activities, including software estimation. Inaccurate estimates are harmful and improving estimation methods is not enough: we need to understand more of the factors that impact estimates.

Objective:
Our study aims to identify the existing evidence about the factors that affect estimates in software projects when using expert judgment.

Method:
We executed a Systematic Literature Mapping (SLM) based on database and snowballing searches, selecting papers by first reading their titles and abstracts and later reading the full text.

Results:
Researchers investigated a wide range of different factors employing mostly laboratory research strategies and relying primarily on differences of estimates and participants’ perceptions to measure the factors’ effects. Resulting from our analysis, we present the SEXTAMT (Software Estimates of eXperts: A Map of influencing facTors), a map of factors affecting estimates built on three dimensions: project/iteration phase, stakeholders, and type of effect.

Conclusion:
Over the years, researchers have investigated a varied set of factors. Many of them were explored in different studies, employing diverse research strategies. Such studies provide compelling evidence on the elements that influence expert judgment estimates, which can be used to assess and improve everyday estimation in the software industry.

Previous
Next 
Keywords
Expert judgment

Software effort estimation

1. Introduction
An estimate is a quantitative assessment of a variable’s likely outcome, such as project costs, resources, effort, or duration (IEEE, 2017b). Estimating tasks and projects is a critical part of developing and maintaining software, and researchers devoted a significant amount of effort to creating and assessing software estimation methods (Jorgensen and Shepperd, 2007). One such method is expert judgment: it is the preferred estimation method in the industry (Molokken and Jorgensen, 2003, Trendowicz et al., 2011). In agile software development, Planning Poker – based on expert judgment – is the most applied method (Usman et al., 2015). Expert judgment is also on the rise as a research topic in software effort estimation (Sehra et al., 2017).

Expert judgment differs from other estimation methods because the quantification step for generating the estimate is judgmental rather than mechanical (Halkjelsvik and Jørgensen, 2012). That is, experts use their human mind as a measurement instrument (Kahneman et al., 2021a). Therefore, the processes that we use for arriving at a prediction are largely unconscious (Halkjelsvik and Jørgensen, 2018b). Discovering and understanding the factors that affect expert judgment estimates is crucial for reducing errors and improving our accuracy when using such a method, and research on these factors is also a trend (Sehra et al., 2017). In addition, research and practice in other domains where evaluations and predictions rely on expert judgment have shown that countless triggers can drive variability in judgments, leading to bias, noise — and consequently, to error, unfairness, and losses (Kahneman et al., 2021b). For instance, in the seemingly exact science of forensic fingerprint analysis, where professionals have to decide whether fingerprints collected in crime scenes match exemplar fingerprints, researchers found that examiners can be misled by contextual information, such as eyewitness recognition (Kahneman et al., 2021e). This led forensic laboratories to change their practices, sequencing information to which examiners are exposed before they analyze fingerprints.

Likewise, getting a comprehensive perspective of the factors researched in software estimation so far can guide researchers willing to build on the existing body of knowledge, to propose and assess new practices that minimize error and enhance the software estimation process. In addition, it can also help practitioners willing to identify the factors relevant to their context, to identify the good practices to adopt. In this article, we provide such perspective of factors through a Systematic Literature Mapping (SLM) using the guidelines of Kitchenham et al. (2015) and Petersen et al. (2015).

We found 131 relevant articles in our SLM, reporting 235 different factors — a myriad of diverse elements that somehow influence estimation results using expert judgment. Most of them (166 factors) was reported in one article and are provided as part of our supplementary material (Matsubara et al., 2021). Still, understanding the remaining 69 factors investigated in two or more articles is challenging. Therefore, we propose an instrument for researchers and practitioners to navigate the seas of factors affecting estimates: the SEXTAMT (Software Estimates of eXperts: A Map of influencing facTors).

Typically, a sextant is an instrument to aid overseas navigation by measuring the angle between the horizon and a celestial reference object like the sun, planets, or stars. The celestial object chosen as a reference depends on the period of the day the observer will take a sight. The observer can use the sun during the day or planets and stars during dawn or night. The measured angle serves as input for calculations that allow for identifying positions with the aid of nautical charts, thus supporting navigation overseas. The time the observer took the sight is also a necessary input (Hugh, 1911).

Likewise, the SEXTAMT uses reference points in the form of dimensions, which the interested reader can use to navigate these wide seas of factors. A temporal dimension alludes to the importance of time for calculating correct positions when using the physical sextant. In the SEXTAMT, it refers to a software project or iteration phases: initiating, planning, executing, monitoring and controlling, and closing — which we borrowed from the PMBOK (Project Management Body of Knowledge) group processes (Project Management Institute, 2017a). Most of the factors we found group at the planning and the executing phases. That is understandable because estimates emerge primarily at the planning phase, and the dynamics of project execution also affect our perceptions of accuracy and error of estimates.

Instead of finding a celestial object as a reference point, we included a stakeholder dimension to the SEXTAMT. The reader can define a stakeholder of interest to investigate only the factors associated with them, either because it relates to a task that the stakeholder is responsible for or because that stakeholder directly causes the factor. In some situations, the factor impacts the stakeholder somehow. Most factors are related to the estimator role, which is natural since stakeholders playing this role are responsible for estimating. However, we found factors associated with clients and users, higher management, project managers, requirement engineers, software developers, and testers. We also discovered factors that applied to the entire software team or no specific stakeholder at all.

The SEXTAMT also has a dimension regarding the type of effect of the factors. According to the direction of the effect, we had four types: positive direction for accuracy factors, negative direction for error factors, and neutral direction for value adjusting characteristics and empirical influence factors. If the reader wants to identify only the factors that increase accuracy when present, they can navigate the accuracy factors. Additionally, we grouped the factors in categories that represent the larger oceans and some smaller seas of our map.

2. Background
In this section, we present the relevant concepts for the context of our study (Section 2.1) and the related work Section 2.2, including two previous related reviews we found.

2.1. Software estimation
Software estimates are predictions about a variable, like the software project effort, cost, or duration (McConnell, 2006c). Given the importance of software estimation for industry, one critical concern is to devise improved methods to estimate software projects. More than 60% of research papers about estimation before 2007 proposed and evaluated estimation methods (Jorgensen and Shepperd, 2007). Boehm classifies these methods as algorithmic models, expert judgment, analogy-based, Parkinson, price-to-win, top-down, and bottom-up (Boehm, 1984). Our SLM focuses on expert judgment estimation, as it is the most used method in the industry (Trendowicz et al., 2011). To delineate what we mean by expert judgment-method, we used the guideline of Halkjelsvik and Jørgensen (2012): if the quantification step of the estimation method is judgmental, then the method is categorized as judgment-based. If this step is mechanical, then the method is categorized as model-based.

Another critical concern of software project estimation is the predicted variable, either size, effort, schedule, or cost of features (McConnell, 2006a). For instance, the functions of algorithmic models use size as their input (Jørgensen, 2007b). Then, considering software size estimates and productivity assumptions, estimators can generate effort estimates. From effort estimates and the project resources, estimators can generate estimates about cost, features, and duration (in calendar days), which project stakeholders use to establish the project commitments (McConnell, 2006a).

Nevertheless, many of the relationships among these software project variables are unstable and change from one context to another (Jorgensen, 2014), hampering the creation of a universal model of estimation. This instability may also explain why complex estimation models are not necessarily more accurate than simpler ones (Jorgensen, 2014). Despite this, many of the existing estimation methods can be applied to any software project variables (McConnell, 2006b). Therefore, in our SLM, we are not excluding studies based on the project variable.

2.2. Related work
Researchers have been investigating factors affecting estimates such as the anchoring bias (Aranda and Easterbrook, 2005), the impact of the development method (Molokken-Ostvold and Jorgensen, 2005), the influence of using checklists (Usman et al., 2018b), and others. In one of the related works, Halkjelsvik and Jørgensen (2012) present a review of studies about factors affecting judgment-based predictions of performance time, integrating results from the areas of psychology, engineering, and management science. Their review later inspired writing a more recent book about time predictions in general (Halkjelsvik and Jørgensen, 2018a). Given the multidisciplinary nature of their review, they opted to term performance time predictions as an equivalent for effort estimation. The authors described (i) the characteristics of estimates presented in the primary studies (ii) the details about the processes and strategies used in estimation, and (iii) the influence of task characteristics, estimators’ characteristics, and contextual factors on estimates.

Halkjelsvik and Jørgensen (2012) included in their review studies correlational, quasi-experimental, and experimental designs. They excluded studies based on questionnaires and interviews describing respondents’ opinions about reasons for estimation errors and biases because the authors affirm that they do not have a suitable method to evaluate their validity. Also, they have included gray literature, like reports and unpublished manuscripts, bringing back the practice perspective and the practitioners’ voice to their results that otherwise would be lost because of the exclusion of studies based on questionnaires and interviews.

In another related work, Basten and Sunyaev (2014) conducted an SLM focused on factors affecting software effort estimation accuracy. The authors presented four categories of factors affecting estimates: (i) factors related to the estimation process, (ii) factors related to the estimators’ characteristics, (iii) features of the project to be estimated that may affect the estimates, and (iv) factors related to the external context, more specifically associated with the client. Although Basten and Sunyaev (2014) published their SLM in 2014, they only included papers written up to 2010. Also, their search strategy consisted of a manual search and snowballing procedures (Basten and Sunyaev, 2014). An automatic search may provide additional papers. Diverging from Halkjelsvik and Jørgensen (2012) and Basten and Sunyaev (2014) included papers reporting opinions from software experts, as they may indicate potentially influential factors.

Thus, we foresaw a need for an update and an expansion of such reviews. We executed our SLM on the scope of software engineering, including articles up to 2020, to satisfy this. On the one hand, our SLM differentiates from the review from Halkjelsvik and Jørgensen (2012) by applying a systematic mapping method and focusing on the software engineering domain alone. On the other hand, our SLM differentiates from the review of Basten and Sunyaev (2014) by extending the timeline of included papers, focusing on expert judgment only, and by including automated search instead of manual.

3. Research method
We started the SLM by defining a systematic mapping protocol, following the guidelines presented by Kitchenham et al. (2015) and Petersen et al. (2015), and by collectively inspecting it. The remaining of this section presents our research questions. It also presents our search, selection, extraction, and analysis procedures.

3.1. Research questions
Our primary research question is: RQ 1 – How have researchers investigated the factors that affect expert judgment software estimation? As we want to explore different aspects of the existing evidence about the factors, we further refined our primary research question in the following set of secondary research questions:

•
SQ 1.1 – What are the factors that affect expert judgment software estimation?

•
SQ 1.2 – How was the impact of the factors over the expert judgment estimates measured?

•
SQ 1.3 – What are the software project estimate variables investigated?

•
SQ 1.4 – When and where are published the studies about factors affecting expert judgment software estimates? and

•
SQ 1.5 – What research strategies and methods are used to investigate factors that affect expert judgment software estimation?

3.2. Search and selection
We started the search process by defining a known set of papers, which we used as an oracle to validate our search string’s outcomes. Our oracle had 25 papers.1 Our next step was defining the search string. The results of automated searches are highly dependent on the search string’s quality (Petersen et al., 2015, Zhang et al., 2011). We defined ours based on the extraction of the keywords of the titles and abstracts from the articles in our known set of papers, as Petersen et al. (2015) recommend.

We executed the automated search restricting the search to title, abstract, and keywords whenever possible. Our sensitivity2 goal for the automated search was 70%, as Zhang et al. (2011) recommended. After the first search round, we got a sensitivity of 60%—below our goal of 70%. We ran a trial search without restricting the search to title, abstract, and keywords, but the high number of results made this change prohibitive.3 We refined the search string, leading us to the second and final version, presented in Table 1.

We carried out the automated search on ACM, IEEExplore, Scopus, and El Compendex (Engineering Village), as illustrated in Fig. 1 (Step 1), resulting in 5113 articles and a sensitivity of 84%, satisfying our goal of more than 70%. We did not include other publisher-specific databases, like SpringerLink and ScienceDirect, as they would probably yield a larger number of duplicates, according to Dyba et al. (2007).


Table 1. Second version of the search string.

(‘‘effort estimation’’ OR ‘‘effort estimate’’ OR ‘‘cost estimation’’ OR ‘‘cost estimate’’ OR ‘‘duration estimation’’ OR ‘‘duration estimate’’ OR ‘‘schedule estimation’’ OR ‘‘schedule estimate’’ OR ‘‘size estimation’’ OR ‘‘size estimate’’) AND (factor OR reason OR cause OR ‘‘anchor’’ OR ‘‘impact’’ OR ‘‘risk identification’’ OR ‘‘customer collaboration’’) AND (software OR system)

After eliminating duplicates from the 5113 articles, we came to a total of 3654 articles (Fig. 1, Step 2). Next, we executed the selection procedures, considering the following inclusion criteria: IC01 — The paper presents an empirical study that investigates factors that affect software project estimates related to expert judgment. We also selected the papers based on the exclusion criteria that we present in Table 2. Additionally, Table 2 presents the relationship between each exclusion criteria and the filter in which we applied it mostly: Filter 1 (title and abstract) and/or Filter 2 (full-text).


Download : Download high-res image (298KB)
Download : Download full-size image
Fig. 1. Search and selection results.

To reduce bias during the selection process, we independently selected a random sample of the articles retrieved by the search by reading their titles and abstracts. We calculated the researchers’ level of inter-rater agreement on this sample of articles through the kappa coefficient (Kitchenham et al., 2015). We got a kappa level of 0.83, which is very good, according to Kitchenham et al. (2015). So, we considered the kappa level adequate, and we proceeded with the selection, getting to a total of 173 papers selected based on title and abstract (Fig. 1, Step 3). After reading the full text of all the 173 articles, we selected 81 that satisfied the inclusion criteria and that we could not eliminate with our exclusion criteria (Fig. 1, Step 4).


Table 2. Exclusion criteria and their relationship with the selection filters.

ID	Exclusion criteria description	Filter
EC01	The paper presents a systematic mapping/review, lessons learned, or opinion paper, rather than an empirical study on factors that affect software project estimates related to expert judgment.	1, 2
EC02	The paper focus on factors affecting estimates related to estimation methods other than expert judgment.	1, 2
EC03	The paper presents non-peer-reviewed results.	1
EC04	The paper is not written in English.	1
EC05	The paper is not accessible in full-text online.	1
EC06	The study is published as a book or gray literature.	1
EC07	The paper is a duplicate or a previous version of another already selected paper.	2
EC08	The paper does not describe the factors to allow for categorization	2
The final set of papers selected from the database search formed the start-set for backward and forward snowballing (Wohlin, 2014). We aimed for a sensitivity of 100% after the snowballing step. We got to a total of 5413 articles through backward and forward snowballing (Fig. 1, Step 5), and to 2618 after removing duplicates (Fig. 1, Step 6). We selected a total of 234 of them based on their metadata – title, authors, and venue – and on their citation context on the original articles in the case of backward snowballing (Fig. 1, Step 7). We read their abstracts, reducing the number to a total of 70 articles (Fig. 1, Step 8). Following, we read their full text, leading to the inclusion of 50 articles (Fig. 1, Step 9). Therefore, the final list of articles included in our SLM contains 131 articles, and we satisfied our goal of 100% sensitivity of papers from our known set of papers.

3.3. Data extraction
We extracted the data using a form4 created and later refined after a pilot data extraction over the known set of papers. We extracted the following data:

•
Title, authors and their affiliation, venue and year of publication;

•
research strategy, according to the classifications of Stol and Fitzgerald (2018) and Storey et al. (2020), and research method;

•
observations and context;

•
factors and discussion about them;

•
project variables that were the focus of estimation. These variables could be either size, effort, cost, productivity, or duration;

•
how authors measured the impact of the factors over the estimates.

3.4. Data analysis
In Fig. 2, we provide an overview of our data analysis. After reading the full text of all selected articles and extracting text and data to our extraction form, we created codes to summarize the findings from the primary studies,5 supporting the aggregation of data into factors later during the analysis process.

Most of the codes we generated followed the structure we show in Fig. 2, with some variations. The candidate factor was the label that the original study authors provided. The quantitative results summarized whether the authors found significant results, sometimes informing p-values or other relevant information. It was optional, once only quantitative studies needed such data. The brief description of effects highlighted whether the candidate factor was a reason for accuracy, a reason for errors, an effort predictor, among others.


Download : Download high-res image (231KB)
Download : Download full-size image
Fig. 2. Overview of the analysis.

Next, we created mind maps aggregating similar candidate factors under a final factor label. We chose the final label to reflect the core of the candidate factors. In some situations, we had an intermediary factor label, reflecting essential variations of the core factor. We held regular meetings to review the mind maps with the categories, candidate factors, and codes. We analyzed the factors through the lenses of a few dimensions we considered relevant to interpret the results. The categories we used to organize the data relate to three dimensions, shown in Fig. 3.

The temporal dimension regards the phase of a software project/iteration that a factor is likely to happen or to cause an impact, based on the PMBOK project phases (Project Management Institute, 2017a). The stakeholder dimension informs one stakeholder or a group responsible for a task or process to which the factor is linked or that directly causes the factor. In some situations, the factor impacts the stakeholder. The type of effect dimension indicates the nature of the impact of the factor over the estimates, considering the results of the primary studies: (i) error factors are negative when present; (ii) accuracy factors lead to improvements in estimates’ accuracy when present; (iii) value adjusting characteristics lead to a need for a higher or lower value of estimate and are inputs to estimation; and (iv) empirical influence indicate factors whose impact on the estimates are not definitely negative, positive, or leading to a need to a higher or lower value: it varies in direction and nature. Some of the factors under this label can lead to improvements in accuracy in some circumstances, but to inaccuracies in others. For instance, the client’s expectation factor has an empirical influence over the estimates. If, by chance, such expectations are realistic, their impact are on the direction of making the estimate more accurate. Otherwise, they may lead to estimation error.


Download : Download high-res image (288KB)
Download : Download full-size image
Fig. 3. SEXTAMT dimensions.

Finally, we created the SEXTAMT. We used the dimensions as the cornerstone for the navigation through the factors. However, we excluded from the SEXTAMT all the factors reported in only one article due to space restrictions, reporting them in our supplementary material. In the next section, we explore our results.

4. Results
In this SLM, we aim to answer the following primary research question: RQ 1 - How have researchers investigated the factors that affect expert judgment software estimation? In this section, we explore our results, considering each secondary research question presented in Section 3.1.

4.1. SQ 1.1 – What are the factors that affect expert judgment software estimation?
After analyzing all papers, we found 235 factors in total, from which we report the 69 that were explored in more than one research article. We present the 69 factors in Table 3, with an ID code in parenthesis, and the articles with the evidence about them.

In Section 5, we detail the factors, presenting them as part of the SEXTAMT. We also organized the factors considering the dimensions we described in Fig. 3.


Table 3. List of factors.

Factor	Articles
Diligence (Dili)	Basten and Mellis (2011) and Lederer and Prasad (1995a)
Anchoring effect (Anch)	Shepperd et al., 2018, Aranda and Easterbrook, 2005, Løhre and Jørgensen, 2016, Jorgensen and Grimstad, 2012 and Jørgensen and Gruschke (2009)
Effect of more and/or irrelevant information (EMII)	Jørgensen and Grimstad, 2008, Usman et al., 2018a, Jørgensen and Grimstad, 2011 and Grimstad and Jorgensen (2007)
Optimism (Opti)	Jørgensen et al. (2007) and Magazinius et al. (2012)
Sequence effects (Sequ)	Grimstad and Jorgensen, 2009, Jørgensen, 2016a, Jørgensen and Halkjelsvik, 2020 and Jørgensen (2013b)
Time frame size (TFSi)	Jørgensen and Halkjelsvik (2010) and Halkjelsvik and Jorgensen (2011)
Unit effects (UnEf)	Jørgensen (2016a) and Jørgensen (2015)
Size (PrSi)	Conoscenti et al., 2019, Usman et al., 2015, Usman et al., 2018a, Lagerström et al., 2012, Layman et al., 2008, Silva-de-Souza and Travassos, 2017, He et al., 2010, Vijayakumar, 1997 and Usman et al. (2017)
Complexity (Comp)	Conoscenti et al., 2019, Usman et al., 2018b, Magazinius and Svensson, 2014, Morgenshtern et al., 2007, Lee et al., 2011, Silva-de-Souza and Travassos, 2017, Zapata and Chaudron, 2013, Subramanian et al., 2006 and Altaleb et al. (2020a)
Integration and dependencies (InAD)	Britto et al., 2015, Magazinius and Svensson, 2014, Usman et al., 2015 and Lee et al. (2011)
Platform (Plat)	Lagerström et al., 2012, Huang et al., 2015 and Altaleb et al. (2020a)
Programming language (Prog)	He et al. (2010) and Huang et al. (2015)
Collaboration and communication (CCAC)	Lederer and Prasad, 1995a, Usman et al., 2017, Lederer and Mirani, 1990 and Molokken-Ostvold and Furulund (2007)
Availability of knowledgeable/competent clients (AKCC)	Matos et al. (2013) and Grimstad et al. (2005)
Client’s expectations (ClEx)	Jørgensen and Grimstad (2008) and Jørgensen and Sjøberg (2004)
Clarity of client’s needs (ClCN)	Lederer and Prasad (1995a) and Matos et al. (2013)
Cultural differences (CuDi)	Britto et al. (2015) and Altaleb et al. (2020a)
Tool support and availability (TSAv)	Lee et al. (2011) and Jorgensen and Molokken-Ostvold (2004)
Use of historical data (UHDa)	Yang et al., 2008, Magazinovic and Pernstål, 2008, Lederer and Prasad, 1995a, Conoscenti et al., 2019, Lee et al., 2011, Furulund and Molkken-stvold, 2007, Jorgensen and Molokken-Ostvold, 2004, Rahikkala et al., 2018, Shmueli et al., 2016 and Lederer and Mirani (1990)
Padding (Padd)	Magazinovic and Pernstål, 2008, Lederer and Prasad, 1995a, Jorgensen and Molokken-Ostvold, 2004, Lederer and Mirani, 1990, Glass et al., 2008 and Lederer and Prasad (1991)
Anticipation of project’ participants’ skills (APPS)	Yang et al., 2008, Rahikkala et al., 2015a, Lederer and Prasad, 1995a, Usman et al., 2018a and Lederer and Mirani (1990)
Use of checklists (UsCh)	Usman et al., 2018b, Furulund and Molkken-stvold, 2007 and Jorgensen and Molokken-Ostvold (2004)
Combination strategy of individual estimates (CSIE)	Jørgensen and Moløkken, 2002, Mahnič and Hovelja, 2012, Gandomani et al., 2019, Haugen, 2006, Moløkken-Østvold and Jørgensen, 2004 and Moløkken-Østvold et al. (2008)
Involvement of technical staff (ITSt)	Yang et al., 2008, Lederer and Prasad, 1995a and Altaleb and Gravell (2019)
Informal basis for estimating (IBEs)	Usman et al., 2015, Lederer and Prasad, 1998 and Keaveney and Conboy (0000)
Impact of early estimates (IEEs)	Jorgensen and Carelius (2004) and Jørgensen and Sjøberg (2001)
Reestimation and revision of estimates (REEs)	Usman et al., 2018a, Lagerström et al., 2012 and Layman et al. (2008)
Standards in estimation (StEs)	Yang et al., 2008, Magazinovic and Pernstål, 2008, Lederer and Prasad, 1995a, Rahikkala et al., 2018, Altaleb and Gravell, 2019 and Lederer and Mirani (1990)
Enough effort and resources spent on estimation (EERE)	Yang et al., 2008, Jorgensen and Carelius, 2004, Lederer and Prasad, 1995a, Jorgensen and Molokken-Ostvold, 2004, Jørgensen and Gruschke, 2009 and Rahikkala et al. (2018)
Overall experience (OvEx)	Britto et al., 2015, Usman et al., 2015, Morgenshtern et al., 2007, Magazinius et al., 2012, Matos et al., 2013, Jorgensen and Molokken-Ostvold, 2004 and Karna and Gotovac (2014)
Technical experience (TeEx)	Conoscenti et al., 2019, Halstead et al., 2012 and Altaleb et al. (2020a)
Experience with similar/previous projects/tasks (ExSP)	Jorgensen and Molokken-Ostvold (2004) and Usman et al. (2017)
Familiarity with the product (FWTP)	Lee et al. (2011) and Davis (1989)
Estimation experience (EsEx)	Rahikkala et al. (2018) and Altaleb et al. (2020a)
Manager experience (MgEx)	Morgenshtern et al. (2007) and Altaleb et al. (2020b)
Monitoring and control (MACo)	Yang et al., 2008, Morgenshtern et al., 2007, Jorgensen and Molokken-Ostvold, 2004, Grimstad et al., 2005 and Keaveney and Conboy (0000)
Risk assessment (RiAs)	Yang et al. (2008) and Morgenshtern et al. (2007)
Pressure (Press)	Yang et al., 2008, Lederer and Prasad, 1995a, Magazinius et al., 2012, Zarour and Zein, 2019, Keaveney and Conboy, 0000 and Glass et al. (2008)
Price-to-win issues (PTWI)	Yang et al., 2008, Usman et al., 2015, Magazinius et al., 2012 and Jorgensen and Molokken-Ostvold (2004)
Goals and targets (GATa)	Magazinovic and Pernstål (2008) and Magazinius et al. (2012)
Negotiations games in estimates (NGIE)	Magazinius et al. (2012) and Glass et al. (2008)
Use of flexible/agile development model (UFAM)	Molokken-Ostvold and Jorgensen, 2005, Koch and Turk, 2011 and Brown et al. (2013)
Resources dependencies (ReDe)	Conoscenti et al., 2019, Usman et al., 2018a and Layman et al. (2008)
Simplicity (Simp)	Jorgensen and Molokken-Ostvold (2004) and Jørgensen and Gruschke (2009)
Project flexibility (PrFl)	Jorgensen and Molokken-Ostvold (2004) and Grimstad et al. (2005)
Similarity with previous tasks/projects (SWPP)	Jorgensen and Molokken-Ostvold (2004) and Jørgensen and Gruschke (2009)
Task size (TaSi)	Usman et al. (2018b) and Hill et al. (2000)
Business area (BuAr)	He et al. (2010) and Huang et al. (2015)
Type of project (TyPr)	He et al. (2010) and Altaleb et al. (2020a)
Longer projects (LoPr)	Lagerström et al. (2012) and He et al. (2010)
Familiarity with the technology (FWTT)	Basten and Mellis, 2011, Lee et al., 2011, Furulund and Molkken-stvold, 2007, Jørgensen and Gruschke, 2009 and Keaveney and Conboy (0000)
Clear requirements specification (CRSp)	Yang et al., 2008, Lederer and Prasad, 1995a, Conoscenti et al., 2019, Usman et al., 2015, Furulund and Molkken-stvold, 2007, Jorgensen and Molokken-Ostvold, 2004, Grimstad et al., 2005, Altaleb et al., 2020b, Zarour and Zein, 2019 and Arnuphaptrairong (2018)
Changes to requirements or scope (CTRS)	Yang et al., 2008, Usman et al., 2015, Usman et al., 2018a, Layman et al., 2008, Grimstad et al., 2005, Zapata and Chaudron, 2013, Arnuphaptrairong, 2018, Keaveney and Conboy, 0000, Lederer and Mirani, 1990, Lederer and Prasad, 1995a, Matos et al., 2013 and Jorgensen and Molokken-Ostvold (2004)
Misunderstanding of requirements (MiRe)	Conoscenti et al., 2019, Magazinius et al., 2012, Matos et al., 2013, Jorgensen and Molokken-Ostvold, 2004, Jørgensen and Gruschke, 2009 and Keaveney and Conboy (0000)
Non-functional requirements (NFRe)	Usman et al., 2015, Lee et al., 2011, Silva-de-Souza and Travassos, 2017 and Usman et al. (2017)
Familiar problem or requirements (FPRe)	Layman et al. (2008) and Jørgensen and Gruschke (2009)
Dependencies between user stories/backlog items (DUBI)	Conoscenti et al. (2019) and Altaleb et al. (2020a)
Technical skill (TeSk)	Jørgensen et al., 2007, Furulund and Molkken-stvold, 2007, Jorgensen and Molokken-Ostvold, 2004, Jorgensen et al., 2020 and Keaveney and Conboy (0000)
Estimation skills (EsSk)	Magazinovic and Pernstål (2008) and Keaveney and Conboy (0000)
Training in Estimation (TrEs)	Yang et al. (2008) and Rahikkala et al. (2018)
Team Size (TeSi)	Conoscenti et al., 2019, Lagerström et al., 2012, Silva-de-Souza and Travassos, 2017, He et al., 2010, Huang et al., 2015, Altaleb et al., 2020a and Hill et al. (2000)
Team Collaboration and communication (TCAC)	Yang et al., 2008, Britto et al., 2015, Usman et al., 2018a, Matos et al., 2013 and Altaleb et al. (2020a)
Turnover (Turn)	Lederer and Prasad, 1995a, Lenarduzzi, 2015, Magazinius and Svensson, 2014, Usman et al., 2015 and Lind and Sulek (1998)
New team members (NTMe)	Yang et al., 2008, Conoscenti et al., 2019 and Keaveney and Conboy (0000)
Team Stability (Stab)	Usman et al. (2015) and Silva-de-Souza and Travassos (2017)
Team Skill (Skil)	Britto et al., 2015, Usman et al., 2015 and Usman et al. (2017)
Overlooked and unplanned tasks (OUTa)	Lederer and Prasad, 1995a, Conoscenti et al., 2019, Furulund and Molkken-stvold, 2007, Magazinius et al., 2012, Jorgensen and Molokken-Ostvold, 2004, Jørgensen and Gruschke, 2009 and Lederer and Mirani (1990)
Incorrect assumptions (InAs)	Conoscenti et al., 2019, Furulund and Molkken-stvold, 2007 and Jørgensen and Gruschke (2009)
Occurrence of unforeseen problems (OUPr)	Yang et al., 2008, Conoscenti et al., 2019 and Jørgensen and Gruschke (2009)
4.2. SQ 1.2 – How was the impact of the factors over the expert judgment estimates measured?
This question’s motivation was to identify how researchers evaluate the impact of the factors over the estimates. Table 4 presents the associations between the strategy that researchers used for impact measurement with each article. Each article could have multiple different ways to measure impact.

Researchers’ most used strategy for investigating the impact of factors was participants’ perceptions: 45 articles adopted it, using either respondents or field research strategies. Some of these studies required participants to evaluate their companies or project accuracy subjectively. Another strategy widely used was assessing the difference of estimates between an experimental and a control group, with 44 occurrences. This is common in laboratory experiments, which was the most applied research strategy as discussed in Section 4.5. By analyzing the difference of estimates, researchers investigated the factors that could cause a shift from more realistic estimates to more optimistic ones — supposing that lower estimates lead to higher chances of error. Regarding more objective measures of accuracy, bias, and error, researchers used metrics like MRE (Magnitude of Relative Error), MREBias, BRE (Balanced Relative Error), and BREBias, as we show in Table 5.


Table 4. Impact measurement strategy by article.

Impact measurement strategy	Article
Difference of estimates	Grimstad and Jorgensen, 2009, Jorgensen and Carelius, 2004, Passing and Shepperd, 2003, Shepperd et al., 2018, Aranda and Easterbrook, 2005, Jørgensen and Grimstad, 2008, Jørgensen et al., 2007, Valerdi, 2007, Jørgensen, 2011, Moløkken and Jørgensen, 2005, Jørgensen and Løhre, 2012, Jørgensen, 2010a, Lagerström et al., 2012, Jørgensen and Sjøberg, 2001, Gren et al., 2017, Løhre and Jørgensen, 2016, Atas et al., 2018, Jørgensen, 2010b, Jorgensen and Grimstad, 2012, Tripathi et al., 2017, Jørgensen and Halkjelsvik, 2010, Jørgensen and Sjøberg, 2004, Jørgensen and Grimstad, 2011, Grimstad and Jorgensen, 2007, McDonald, 2005, Jørgensen, 2016a, Brown et al., 2013, Subramanian et al., 2006, Huang et al., 2015, Subramanian et al., 2017, Shmueli et al., 2016, Jørgensen, 2007c, Jorgensen et al., 2020, Jørgensen and Halkjelsvik, 2020, Jørgensen, 2015, Jørgensen, 2014c, Halkjelsvik and Jorgensen, 2011, Tamrakar and Jørgensen, 2012, Jørgensen and Moløkken, 2004, Hill et al., 2000, Moløkken-Østvold and Jørgensen, 2004, Jørgensen, 2013b, Jørgensen and Halkjelsvik, 2010 and Moløkken-Østvold et al. (2008)
Participants’ perception	Yang et al., 2008, Rahikkala et al., 2015a, Britto et al., 2015, Jorgensen and Carelius, 2004, Passing and Shepperd, 2003, Magazinovic and Pernstål, 2008, Jørgensen et al., 2004, Lederer and Prasad, 1995a, Conoscenti et al., 2019, Lenarduzzi, 2015, Usman et al., 2018b, Magazinius and Svensson, 2014, Usman et al., 2015, Tanveer et al., 2017, Usman et al., 2018a, Henry et al., 2007, Suliman and Kadoda, 2017, Lee et al., 2011, Jørgensen and Sjøberg, 2001, Magazinius et al., 2012, Mendes et al., 2005, Silva-de-Souza and Travassos, 2017, Lederer and Prasad, 1995b, Matos et al., 2013, Jorgensen and Molokken-Ostvold, 2004, Grimstad et al., 2005, Jørgensen and Gruschke, 2009, Rahikkala et al., 2018, Rahikkala et al., 2015b, Rozalina and Mansor, 2018, Altaleb et al., 2020b, Usman et al., 2017, Altaleb and Gravell, 2019, Zapata and Chaudron, 2013, Altaleb et al., 2020a, Magazinius and Feldt, 2011, Ramessur and Nagowah, 2020, Zarour and Zein, 2019, Arnuphaptrairong, 2018, Jørgensen, 2016b, Lind and Sulek, 1998, Lederer and Prasad, 1998, Lederer and Mirani, 1990, Glass et al., 2008 and Lederer and Prasad (1991)
MRE	Molokken-Ostvold and Jorgensen, 2005, Basten and Mellis, 2011, Jørgensen et al., 2007, Grimstad and Jørgensen, 2007, Mendes et al., 2005, Jorgensen and Molokken-Ostvold, 2004, Vicinanza et al., 1991, Jørgensen and Gruschke, 2009, Karna and Gotovac, 2014, Gandomani et al., 2019, Gruschke and Jørgensen, 2008, Haugen, 2006, Host and Wohlin, 1998 and Cao (2008)
MREBias	Molokken-Ostvold and Jorgensen, 2005, Jorgensen and Molokken-Ostvold, 2004, Jørgensen and Gruschke, 2009, Jørgensen, 2013a and Haugen (2006)
BRE	Molokken-Ostvold and Jorgensen, 2005, Usman et al., 2018a, Mahnič and Hovelja, 2012, Grapenthin et al., 2016, Molokken-Ostvold and Furulund, 2007 and Moløkken-Østvold et al. (2008)
BREBias	Molokken-Ostvold and Jorgensen, 2005, Basten and Mellis, 2011, Usman et al., 2018b, Usman et al., 2018a, Furulund and Molkken-stvold, 2007, Mahnič and Hovelja, 2012, Grapenthin et al., 2016 and Moløkken-Østvold et al. (2008)
Deviation	Branco et al., 2015, Jorgensen and Carelius, 2004, Bergeron and St-Arnaud, 1992, McGarry et al., 1998, Halstead et al., 2012, Lind and Sulek, 1998 and Ohlsson et al. (1998)
Absolute error	Passing and Shepperd, 2003, Gandomani et al., 2019 and Jorgensen et al. (2020)
Total effort	Mendes et al., 2005, He et al., 2010, Vijayakumar, 1997, Arifin et al., 2017, Nugroho and Lange, 2008 and Davis (1989)
Interval of over/underrun (over/underestimation)	Britto et al., 2015, Lederer and Prasad, 1995a and Benschop et al. (2020)
Pred(X)	Basten and Mellis (2011) and Gray et al. (1999)
Confidence related	Jørgensen et al., 2004, Jørgensen and Moløkken, 2002, Løhre and Jørgensen, 2016, Jorgensen, 2004, Grimstad and Jorgensen, 2007, Jørgensen and Gruschke, 2009, Jørgensen, 2016a, Jørgensen, 2018, Gruschke and Jørgensen, 2008 and Jørgensen and Teigen (2002)
Not informed/not defined	Bhatt et al., 2006, Boetticher and Lokhandwala, 2007, Bukhari and Malik, 2012, Morgenshtern et al., 2007, Koch and Turk, 2011, Keaveney and Conboy, 0000, Javed et al., 2013, Taff et al., 1991 and Bratthall et al. (2001)
Other	Passing and Shepperd, 2003, Jørgensen, 2014b, Layman et al., 2008, Jorgensen and Grimstad, 2005, Little, 2006, Vicinanza et al., 1991 and Jørgensen (2014a)
Seven studies relied on less traditional metrics involving the estimated and actual values. While the critiques of MRE and MREBias focus on the use of actual values at the denominator of the formula – which is resolved in BRE and BREBias by using the minimum value between the estimated and actual values – seven studies use the estimated value at the denominator. We categorized these studies under the term “deviation”, since the researchers of such articles disagree about the best name for the metric, calling it effort deviation (Branco et al., 2015, Ohlsson et al., 1998), effort overrun (Jorgensen and Carelius, 2004), accuracy (Bergeron and St-Arnaud, 1992), effort variance (McGarry et al., 1998), overrun factor (Halstead et al., 2012),6 or project overrun (Lind and Sulek, 1998). Another three studies use the absolute error (estimated - actual value).


Table 5. Objective metrics of accuracy, bias, and error.

Accuracy metrics	#	Bias metrics	#
MRE	13	MREBias	5
BRE	6	BREBias	8
A total of six studies evaluates total effort. They are either based on regression analysis (He et al., 2010, Mendes et al., 2005) or correlations of effort with other variables (Arifin et al., 2017, Davis, 1989, Nugroho and Lange, 2008, Vijayakumar, 1997). Three studies relied on classifying projects according to ranges of over/underestimation or over/underruns. Two of them were respondent studies, and therefore the classification depended on respondents’ memories (Britto et al., 2015, Lederer and Prasad, 1995a). The other study was a data one (Benschop et al., 2020). Also, two studies used pred(x) (Jørgensen, 2007a).

4.3. SQ 1.3 – What are the software project estimate variables investigated?
Regarding the project variables investigated in the primary studies, we extracted the metrics that authors reported as within their studies’ scope. Fig. 4 shows the results we obtained, making evident that most of the studies focus on effort estimation.

Most of the studies focused on effort estimation (96 in total). Twenty-five studies claimed to investigate factors related to cost, while 13 focused on duration. Eight studies explored prediction intervals – mostly of effort – and we classified them separately to emphasize the importance of avoiding single values when estimating. Three studies reported factors associated with productivity. Only two studies claim to investigate factors associated with size, probably because the focus is on other metrics when using expert judgment.


Download : Download high-res image (74KB)
Download : Download full-size image
Fig. 4. Variables investigated in primary studies.

4.4. SQ 1.4 – When and where are published the studies about factors affecting expert judgment software estimates?
Our sample includes articles published between 1989 and 2020. The past two decades have been very fruitful regarding research about factors affecting estimates, as shown in Fig. 5, revealing an increasing interest in them. We also show a trendline reporting the moving average (past five years), revealing a relative degree of stability of the number of papers published regarding factors affecting expert judgment estimates since 2016.

There is a balance between publishing in conferences (63 occurrences) and journals (68 occurrences). The Journal of Systems and Software, IEEE Transactions on Software Engineering and Information and Software Technology, concentrated the highest number of articles.


Table 6. Top venues.

Venue	# citations
Journal of Systems and Software	15
IEEE Transactions on Software Engineering	10
Information and Software Technology	8
Euromicro Conference on Software Engineering and Advanced Applications	5
International Conference on Evaluation and Assessment in Software Engineering	5
IEEE Software	4
Empirical Software Engineering	4
International Symposium on Empirical Software Engineering and Measurement	4
International Conference on Product Focused Software Process Improvement	4
International Journal of Project Management	3
International Software Metrics Symposium	3
4.5. SQ 1.5 – What research strategies and methods are used to investigate factors that affect expert judgment software estimates?
To answer SQ 1.5, we classified the studies considering the taxonomies proposed by Storey et al. (2020), which is focused on human factors of software engineering, identifying four research strategies: respondents, lab, field, and data, as we show in Table 7. Each article can report more than one study and, accordingly, could be associated with more than one research strategy.

In general, the different available research strategies had been used in a balanced way, except for lab strategies, which detach from the others as the most used one. That is, most of the studies in our sample evaluate one factor in a controlled setting through hypothesis testing (Storey et al., 2020). Studies investigating or reporting more than one factor generally employ respondent or field strategies, each one having 33 and 31 occurrences, respectively, in our data. In Fig. 6 we show the use of the research strategies throughout the years.


Table 7. Research strategies distribution.

Research strategy	Number of studies
Data	31
Field	31
Lab	51
Respondents	33
Research about factors affecting estimates became prolific after the year 2005. Since then, the distribution of studies using different strategies has been relatively uniform. However, it seems that laboratory strategies are outperforming the others in the past decade.


Download : Download high-res image (342KB)
Download : Download full-size image
Fig. 6. Research strategies throughout the years.

5. The SEXTAMT
As we informed in Section 4.1, we found a total of 235 factors, of which 69 were reported in two or more articles. We gathered these 69 factors in one instrument: the SEXTAMT. It has three dimensions to allow the navigation through the seas of factors:

1.
The temporal dimension provides a view of the factors relevant for different software project or iteration phases.

2.
The stakeholders’ dimension focuses on the factors associated with different roles in the software process.

3.
The type of effect’ dimension, based on the direction of the effect of the factor.

In Fig. 7, we present the overall map of factors affecting estimates — a bird’s eye view of the SEXTAMT. We represent the factors as rounded rectangles, labeled with the factors’ codes we indicated in Table 3. We marked some of them with symbols related to their stakeholders’ dimension. The size and color of each factor represent the number of articles investigating it. We also grouped them by major categories represented in the form of ellipses. We also provided an expanded view of Fig. 7 as part of our supplementary material (Matsubara et al., 2021), in which we added the studies that investigated each factor.

Fig. 7 shows two larger oceans, formed by categories that share common factors. The larger one contains the categories: estimation process, biases, management, experience, skill issues, team issues and project and task characteristics. It also concentrates many of the top investigated factors: the use of historical data, padding, the combination strategy of individual estimates, standards in estimation, enough effort and resources spent on estimation, overall experience, and team size.


Download : Download high-res image (694KB)
Download : Download full-size image
Fig. 7. The SEXTAMT.. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Client/customer issues, requirements, and product’ characteristics are categories that also share factors, forming another larger ocean with some of the factors that stand out: changes to requirements or scope, clear requirement specifications, misunderstanding of requirements, complexity, and product size. The map also has some categories representing smaller seas, of which political issues and unexpected events are the larger ones. Pressure and overlooked and unplanned tasks are the most investigated factors, respectively.

The remaining of this section describes the factors composing the SEXTAMT in more detail, from the perspective of dimensions we presented in Fig. 3. In each of the following subsections, we show the factors for each different class of stakeholders, organizing them per project phase. Therefore, the reader may easily navigate through the factors by stakeholder and by phase. We also present the type of effect for each factor.

5.1. Customer/client
Fig. 8 shows all the factors related to customers/clients, each one represented by a blue box. We wrote the factors using positive statements representing the presence of a factor, like in the clarity of the client’s needs, representing such presence through green circles in Fig. 8. However, the existing evidence may refer to the absence of such an aspect, like the lack of clarity of the client’s needs, represented in Fig. 8 by a red circle inside the factor box. Fig. 8 also presents the timeline of the typical project or iteration phases when a factor may happen or cause an impact over the estimates: the temporal dimension of the SEXTAMT. We also mapped each factor to their type of effect at the right of the figure. Some factors are organizational or overarching, and we represent them at the left of the image. We did not present their types of effects on the figure to keep it simple: we discuss it in the text only. In addition, the gray hexagons associated with each factor represent the articles that published results regarding them. The numbering of each hexagon indicates the article ID in the extraction forms (part of our supplementary material).

At the planning phase, four factors stand out. Two studies report findings related to the lack of clarity of client’s needs as an error factor. Lederer and Prasad (1995a) present a survey where the users’ lack of understanding of their requirements is a reason for inaccuracy. Matos et al. (2013) report a qualitative study where clients who do not know what they want hinder software estimation and accuracy in the context of web effort estimation. Other two studies report that longer projects relate to higher costs (Lagerström et al., 2012) and that increasing calendar time will increase total effort (He et al., 2010). Therefore, it is a value adjusting characteristic.

Eight studies declare that pressure impacts estimating, either as an error factor or as a value adjusting characteristic. Nevertheless, the articles describe pressure in varying levels and originating from different sources. It can, for example, be an overall pressure, directed by management or related to the schedule alone. Therefore, we created intermediary factors for pressure, and in this section, we explore only the customer pressure, which appears in two studies. Yang et al. (2008) point out that pressure from senior managers and clients to set or change the estimation results is a reason for inaccurate estimates. Keaveney and Conboy (0000) report that pressures from customers or managers result in lower estimates than would be realistically expected.


Download : Download high-res image (475KB)
Download : Download full-size image
Fig. 8. Factors related to Customer/Client.. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

The final factor at the planning phase is the client’s expectations, which have an empirical influence over the estimates. Estimators were impacted by the effort informed by the client at the specification of one experiment (Jørgensen and Sjøberg, 2004). This result repeated even when estimators are told to disregard such information (Jørgensen and Grimstad, 2008).

At the executing phase, changes to requirements or scope emerge as an error factor when present, with twelve studies discussing it. Some studies report that requirement changes are a reason for inaccuracies (Layman et al., 2008, Usman et al., 2015, Zapata and Chaudron, 2013), and two studies indicate that frequent changes are the problem (Arnuphaptrairong, 2018, Lederer and Prasad, 1995a). Others emphasize that requirement changes contribute to overruns (Grimstad et al., 2005, Halstead et al., 2012), are a challenge (Usman et al., 2018a), or a potential problem for estimation (Keaveney and Conboy, 0000, Lederer and Mirani, 1990). Finally, some researchers identify changes in scope (Layman et al., 2008) and scope creep (Jorgensen and Molokken-Ostvold, 2004, Usman et al., 2015) as reasons for inaccuracies. When the client’s needs are stable, it facilitates software estimation and raises accuracy (Matos et al., 2013), so the absence of changes to requirement or scope is an accuracy factor.

Some factors intersect all phases. For instance, the availability of clients who understand the project’s business rules facilitates software estimation and accuracy (Matos et al., 2013) - therefore, the availability of knowledgeable/competent clients is an accuracy factor. Moreover, the lack of it leads to errors (Matos et al., 2013) and is a reason contributing to overruns (Grimstad et al., 2005). Collaboration and communication with the customer and users is an additional factor trespassing all phases. Researchers report that good collaboration with customers, facilitated by frequent communication, was associated with projects that experienced a lesser magnitude of effort overruns (Molokken-Ostvold and Furulund, 2007). Also, researchers found that insufficient user-analyst communication and understanding was a potential cause of estimating problems in a case study (Lederer and Mirani, 1990), confirming it is a reason for inaccuracy later on in a survey (Lederer and Prasad, 1995a). Additionally, in the agile context, customer communication is an effort predictor (Usman et al., 2017). Thus, collaboration and communication with the customer and users is an accuracy factor and a value adjusting characteristic. When absent, it is also an error factor.

5.2. Estimator
Fig. 9 presents all the factors related to anyone assuming the role of an estimator. Only one factor is related to the initiating phase: early estimates — two studies indicate that they impact estimates in later phases (Jorgensen and Carelius, 2004, Jørgensen and Sjøberg, 2001). In one of them, project leaders believed that pre-planning estimates impacted detailed estimates, an effect confirmed in a laboratory experiment (Jørgensen and Sjøberg, 2001). In a field experiment about project bidding, companies providing early price indications based on limited and uncertain information gave higher estimates in the next bidding round. Such findings surprised the researchers, who expected the early estimates to act as anchors, leading to lower bids. Next, they carried out a laboratory experiment to explore further this finding, concluding that early estimates act as anchors to final estimates only when estimators have nothing to lose (Jorgensen and Carelius, 2004).

All the other factors mapped to estimators concentrate on the planning phase. Many of them are biases, such as the anchoring effect, which is our tendency to be influenced by values presented to us before the estimation activity (Løhre and Jørgensen, 2016). In a field study it is reported to hinder the creation of a meaningful estimate (Rahikkala et al., 2018) and, thus, is an error factor. Many laboratory experiments also report that the anchoring effect impacts software estimation (Aranda and Easterbrook, 2005, Jorgensen and Grimstad, 2012, Løhre and Jørgensen, 2016, Shepperd et al., 2018) — therefore providing evidence of its empirical influence over the estimates. Aranda and Easterbrook (2005) found a statistically significant impact of numerical anchors on time estimates. Jorgensen and Grimstad (2012) also found a significant impact of numerical anchors over estimates, reporting a medium to large effect size. They also found a small to medium effect size when using a textual anchor: putting the same requirements specification as a “minor extension” work led to lower estimates than putting it as “new functionality” work. Løhre and Jørgensen (2016) found a slight tendency for a larger anchoring effect with interval anchors compared to single value anchors when dealing with numerical anchors. Additionally, they expected the expertise – defined as the length of experience – of the anchor’s source would act as a moderator for the anchoring effect. Surprisingly, they found that the receiver’s expertise that acted as such. Beyond investigating anchoring itself, Shepperd et al. (2018) discovered that raising awareness about anchoring reduces the impact of high anchors on productivity estimations but does not eliminate the effect.


Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 9. Factors related to estimators.

Another relevant factor for estimators is the effect of more and/or irrelevant information over the estimates. Usman et al. (2018a) found that the availability of more detailed information may increase underestimation bias. Grimstad and Jorgensen (2007) report that specifications with irrelevant information lead to higher estimates in laboratory experiments. Jørgensen and Grimstad (2008) explored different aspects of irrelevant and misleading information that have an effect over the estimates: (i) the client’s cost expectations, (ii) the wording of the specification (words associated with small and simple tasks lead to underestimation, while words associated with complex and large tasks lead to overestimation), (iii) the suggestion of future opportunities for work contingent on performance in current projects (lead to underestimation), and (iv) the amount of information, even when they are irrelevant (more information leads to overestimation). Asking people to highlight relevant information or strike irrelevant ones is not enough to eliminate the observed impact (Jørgensen and Grimstad, 2008). Additionally, in a field experiment, Jørgensen and Grimstad concluded that informing that the customer required development in a short period with start-up several months ahead also led to lower estimates, though supposedly this information is irrelevant to estimation (Jørgensen and Grimstad, 2011).

Optimism is an additional error factor, leading to estimates’ unintentional distortions, for instance Magazinius et al. (2012). Jørgensen et al. (2007) measured general optimism in varying ways in an experiment. They discovered that explanatory style, life orientation, and higher self-assessed level of optimism are all weakly connected with optimistic predictions. Also, merely asking estimators whether they assess themselves to be more or less optimistic seems to be enough as an indicator of optimistic predictions — instead of using more complex measures of optimism as the scales for explanatory style or life orientation (Jørgensen et al., 2007).

Estimators should also be aware of sequence effects relative to the order of estimation of tasks and projects with different sizes.7 Overall, starting the estimation processes with small tasks or projects leads to lower estimates for the subsequent project or task. The opposite happens when starting with larger ones (Grimstad and Jorgensen, 2009, Jørgensen, 2013b). Also, when estimating projects or tasks of similar sizes in a sequence, estimators tend to estimate the target project as more extensive compared to the reference project (the first one) (Jørgensen, 2013b, Jørgensen and Halkjelsvik, 2020).

Two articles address the time frame size: shorter time frames tend to lead to more optimistic estimates than larger ones (Halkjelsvik and Jorgensen, 2011, Jørgensen and Halkjelsvik, 2010). Another two articles investigate unit effects: asking for estimates using a lower granularity time unit led to lower estimates compared with using a higher granularity one (Jørgensen, 2015, Jørgensen, 2016a). Therefore, both time frame size and unit effects are error factors.

A comprehensive set of factors affecting estimates relates to the estimation process’s particularities, such as the use of historical data. A field study connected it with a lesser magnitude of effort overruns (Furulund and Molkken-stvold, 2007). A relevant number of studies also reported that the lack or no use of historical data is related to errors and problems in estimating — with evidence coming from respondent studies (Lederer and Prasad, 1995a, Yang et al., 2008), laboratory studies (Shmueli et al., 2016), and field studies (Conoscenti et al., 2019, Jorgensen and Molokken-Ostvold, 2004, Lederer and Mirani, 1990, Magazinovic and Pernstål, 2008, Rahikkala et al., 2018).

The combination strategy of individual estimates rose as a factor in our SLM, either for combining single values or interval estimates — with minimum and maximum values. We found evidence for three strategies regarding single values: statistical combination, unstructured group estimates, or Planning Poker. Three articles report evidence in favor of estimating in groups over averaging: unstructured group estimates (Moløkken-Østvold and Jørgensen, 2004) and Planning Poker (a structured approach) (Gandomani et al., 2019, Moløkken-Østvold et al., 2008) led to less optimistic estimates compared with the average of individual estimates. When combining interval estimates, the results also favor group discussion over averaging (Jørgensen and Moløkken, 2002). Mahnic and Hovelja (Mahnič and Hovelja, 2012) found the same result for Planning Poker estimates compared with the statistical combination, but only when the participants in their experiments were software professionals. They found the opposite effect when students were estimating. In another study, the results suggested that planning poker is more accurate when the team has previous experience from similar tasks compared to unstructured group estimation sessions (Haugen, 2006). In summary, there is evidence in favor of estimating in groups over averaging estimates in general and in favor of Planning Poker more specifically.

Padding also impacts estimates’ accuracy. The inclusion of a large buffer to deal with unexpected events and/or changes in the specification is a reason for accurate estimates (Jorgensen and Molokken-Ostvold, 2004). The greater the preference for projects completed within the estimates, the greater the padding frequency (Lederer and Prasad, 1991). More evidence about it comes from the fact that the removal of padding by management is related to estimating problems (Lederer and Mirani, 1990, Magazinovic and Pernstål, 2008) and is a reason for inaccuracies (Lederer and Prasad, 1995a). Nevertheless, it is reported as an intentional increase in estimates aimed at the holding back reserves, which gives it a negative denotation (Glass et al., 2008).

The anticipation of project’ participants skills emerged as a relevant factor for estimators. The inability to anticipate the team members’ skills, abilities, or characteristics is a problem for estimating (Lederer and Mirani, 1990) and a reason for inaccuracies (Lederer and Prasad, 1995a, Yang et al., 2008). The knowledge about who will execute testing allows for the definition of testing effort (Rahikkala et al., 2015a). However, one study suggests that the team’s knowledge of who will work on the project may increase underestimation bias (Usman et al., 2018a). It might be the case that anticipating the project participants’ skills may not work for all contexts.

Another essential aid is the use of checklists, leading to a lesser magnitude of effort overruns (Furulund and Molkken-stvold, 2007). Personalized checklists reduces the underestimation bias (Usman et al., 2018b). Such evidence indicates that the use of checklists is an accuracy factor. Also, the lack of checklists is a reason for estimation error (Jorgensen and Molokken-Ostvold, 2004).

The lack of involvement of technical staff during estimating is a reason for inaccuracies in three respondent studies (Altaleb and Gravell, 2019, Lederer and Prasad, 1995a, Yang et al., 2008). Other three studies (Keaveney and Conboy, 0000, Lederer and Prasad, 1998, Usman et al., 2015) also reported that an informal basis for estimating is an error factor. Lederer and Prasad (1998) considered informal bases for estimating, comparing similar, past projects based on personal memory, guessing, and intuition as reasons for inaccuracies. The other two studies emphasized the lack of formality of the estimation process as a reason for inaccurate estimates (Keaveney and Conboy, 0000, Usman et al., 2015).

Four factors associated with estimators regard their experience and skills. The first one is the estimation experience: an effort predictor in the context of mobile development (Altaleb et al., 2020a), whose absence hinders the creation of a meaningful estimate (Rahikkala et al., 2018). The second is experience with similar/previous projects/tasks, which is also an effort predictor (Usman et al., 2017) and a reason for accurate estimates (Jorgensen and Molokken-Ostvold, 2004). The third factor is the lack of estimation skills, an estimation inhibitor (Magazinovic and Pernstål, 2008) that can cause estimation problems (Keaveney and Conboy, 0000). The fourth is the lack of training in estimation, which hinders creating a meaningful estimate (Rahikkala et al., 2018) and is a reason for inaccurate estimates (Yang et al., 2008).

The final factor related to estimators at the planning phase is enough effort and resources spent on estimation, which is an accuracy factor and, when lacking, an error factor. On the one hand, a respondent study reports that spending enough time on estimating is a reason for accurate estimates (Jorgensen and Molokken-Ostvold, 2004). On the other hand, making quick, rough estimates is not motivating and hinders creating a meaningful estimate (Rahikkala et al., 2018). Also, insufficient time, effort, or resources for estimating is a reason for inaccurate estimates (Jorgensen and Molokken-Ostvold, 2004, Jørgensen and Gruschke, 2009, Magazinovic and Pernstål, 2008, Yang et al., 2008).

Two factors intersect all the phases. One of them is the overall experience of the estimator. In one study, experts’ experience (including total experience, company experience, project experience, and the number of projects expert has participated) predicted estimation performance, leading to less estimation error (Karna and Gotovac, 2014). Therefore, the presence of overall experience improved accuracy. Additionally, other studies indicate that the lack of overall experience is an error factor, leading to unintentional distortions of software estimates in varying directions — reducing or increasing them (Magazinius et al., 2012), hindering software estimation and accuracy (Matos et al., 2013), being a reason for estimation error (Jorgensen and Molokken-Ostvold, 2004).

The other factor affecting all phases is standards in estimation. All evidence about it is related to its shortage, and all results point to it as an error factor. It has many facets, in any case. For instance, in one case study, participants revealed that the lack of methodology or guidelines and the lack of setting and review of standards is a potential cause of estimating problems (Lederer and Mirani, 1990). A follow-up survey confirms that these are reasons for inaccuracies (Lederer and Prasad, 1995a). Also, no development of estimation standards and no record-keeping of estimates and actual results make it difficult to capitalize on lessons learned (Magazinovic and Pernstål, 2008), and no documented estimation procedure hinders the creations of a meaningful estimate (Rahikkala et al., 2018). Researchers also report that the lack of appropriate software cost estimation methods and processes (Yang et al., 2008) and the lack of clear guidance for estimating (Altaleb and Gravell, 2019) are reasons for the inaccuracy of estimates.

5.3. Management roles
We present the factors regarding management roles – including higher management, project managers, and the Software Engineering Process Group (SEPG) – in Fig. 10. We explored some of them thoroughly in previous sections: longer projects (Section 5.1), enough effort and resources spent on estimation (Section 5.2), and standards in estimation (Section 5.2). We explore all the others in the current section.

At the planning phase, pressure came up as an error factor. Yang et al. (2008) report that the company’s survival pressure and that the senior manager’s pressure to set or change the estimation results is a reason for inaccurate estimates — a finding that echoes in other studies (Lederer and Prasad, 1995a, Zarour and Zein, 2019). It leads people to change their estimates intentionally (Magazinius et al., 2012), to cave in to people with more power (Glass et al., 2008), resulting in lower estimates than would be realistically expected (Keaveney and Conboy, 0000). Another facet of pressure is work pressure, which Altaleb, Altherwi, and Gravell report as an effort predictor (Altaleb et al., 2020a). A final facet is schedule pressure, which leads to more effort in test tasks (Silva-de-Souza and Travassos, 2017) — and thus is a value adjusting characteristic.


Download : Download high-res image (512KB)
Download : Download full-size image
Fig. 10. Factors related to managers.

Risk assessment is another factor in the planning phase. Systematic risk assessment related to lower error in duration estimates (Morgenshtern et al., 2007), and the lack of it is a reason for inaccurate estimates (Yang et al., 2008). Surprisingly, some laboratory experiments’ results indicate that identifying more risks immediately before software estimation leads to increased over-confidence (Jørgensen, 2010a). Nevertheless, the authors stress that they have not investigated a complete risk management process — only the impact of simple risk identification.

Low technical skills also are among factors related to managers. One study report that project managers not skilled in planning multi-disciplinary projects are reasons for estimation error (Jorgensen and Molokken-Ostvold, 2004). Other studies also report technical skill issues but concerning the team, and we describe them further in Section 5.5.

At the executing phase, the only factor is the reestimation and revision of estimates. In a large company with two estimation points in its process, the reestimation at the analysis stage improves the accuracy of the effort estimates (Usman et al., 2018a). In a data study, more budget revisions were related to higher costs (Lagerström et al., 2012) — and therefore, we considered it a value adjusting characteristic. Nevertheless, in another data study, more estimation updates were connected with larger errors in effort estimates (Layman et al., 2008). Regarding the last result, the authors explain that more extensive features had more frequent estimation updates. Another possible explanation is that projects already in trouble may undergo more estimation updates.

The only factor at the monitoring and control phase is its homonym and is an accuracy factor. One field study reports that good cost control is a reason for accurate estimates (Jorgensen and Molokken-Ostvold, 2004). One a respondent study reports that adequate project administration is a reason for the prevention of overrun (Grimstad et al., 2005).

The factor that intersects all phases is the manager’s experience. For instance, the number of projects previously managed correlates with duration error — more projects managed leads to lower error (Morgenshtern et al., 2007). It is, therefore, an accuracy factor. Also, when the estimates used for the project contract are based on the project manager’s previous experience only, it requires the developers to work over their capacity, which is a reason for low accuracy (Altaleb et al., 2020b).

5.4. Technical roles
We found factors related to technical roles: requirement engineers, software designers, developers, and testers. Fig. 11 brings such factors to the surface. None of them apply to all phases. We explained two of them in Section 5.1: changes to requirements or scope and pressure — including the factor associated with the tester role.

We found four factors related to requirements at the planning phase, which we associated with the requirements engineer role. One of them is a clear requirements specification. Some studies present results in more general terms, indicating that poor, unclear, or ill-defined requirements are one reason for inaccuracies (Grimstad et al., 2005, Jorgensen and Molokken-Ostvold, 2004, Lederer and Prasad, 1995a, Usman et al., 2015, Yang et al., 2008, Zarour and Zein, 2019). Other studies emphasize specific facets that make requirements poor, like the redundancy of user stories (Conoscenti et al., 2019), missing requirements (Usman et al., 2015), weak or ambiguous requirements (Furulund and Molkken-stvold, 2007), incomplete requirements (Jorgensen and Molokken-Ostvold, 2004), and the user’s lack of understanding of their requirements (Arnuphaptrairong, 2018). All this evidence indicates that the lack of clear requirements specifications is an error factor.


Download : Download high-res image (866KB)
Download : Download full-size image
Fig. 11. Factors related to people in technical roles.

Familiar problems or requirements was also classified as an error factor when they are absent. Layman et al. (2008) report that unfamiliar feature requirements are a reason for estimation inaccuracy. Jørgensen and Gruschke (2009) report that too little knowledge about the problem is a reason for estimation inaccuracy.

The third factor associated with the requirements engineer is dependencies between user stories/backlog items. Conoscenti et al. (2019) found that links to other stories serve as indicators for a possible inaccurate estimation. Altaleb et al. (2020a) found that dependency between backlog items is an effort predictor in the mobile development context.

The fourth factor we found regards studies reporting that non-functional requirements are an effort predictor or a cost driver (Usman et al., 2017, Usman et al., 2015). We also found studies reporting that specific non-functional requirement types are associated with higher effort, like the high legal or regulatory impact of the code (Lee et al., 2011), the required level of performance, and the required security level (Silva-de-Souza and Travassos, 2017). So, we classified it as a value adjustment characteristic.

Still in the planning phase, three factors emerge for the developer role. One of them is integration and dependencies. One study report that technical dependencies are an effort predictor in agile global development (Britto et al., 2015). Another one considers that integration issues are a cost driver, also in the context of agile development (Usman et al., 2015). In the context of corrective maintenance of object-oriented systems, a high level of code/system dependencies leads to higher effort (Lee et al., 2011). Therefore, the integration and dependencies factor is a value adjustment characteristic. Another study informs that integration complexity is an estimation challenge (Magazinius and Svensson, 2014), suggesting it is also an error factor.

The other factor regarding developers is the platform. In the context of mobile development, the supported Platform type (IOS/Android./Win./etc.) and the supported device (phone, tablet, smartwatch) are both effort predictors (Altaleb et al., 2020a). Other two studies report that the type of platform impacts software costs (Lagerström et al., 2012) and that the interaction of team size and development platform has a significant impact on productivity (Huang et al., 2015).

Finally, the developer’s familiarity with the product is a value adjustment characteristic. When low, it leads to more need for effort (Lee et al., 2011). In another study, the programmer’s familiarity in the number of months of experience with the system was a significant predictor of debugging effort (more experience leads to less effort) (Davis, 1989).

Two data studies inform the programming language’s importance as an empirical influence over the estimates related to the developer role at the executing phase. It has a significant impact on the effort needed (He et al., 2010) and on time-to-market (Huang et al., 2015). Huang et al. (2015) also report that team size and language type interaction significantly impact productivity.

The technical experience related to the developer role is an additional factor we found. Altaleb et al. (2020a) evidence that developer implementation experience is an effort predictor. Also, developers’ lack of experience leads to estimation inaccuracy (Conoscenti et al., 2019), and the lack of technology experience leads to a higher probability of effort overrun (Halstead et al., 2012).

5.5. Team
Some of the factors we found regarded the whole software team. We show them in Fig. 12. We thoroughly discussed two of these factors in previous sections: involvement of technical staff in estimating and experience with similar/previous projects/tasks – both at Section 5.2.

At the planning phase, familiarity with the technology is a value adjustment characteristic because when it is low, it leads to a higher need for effort (Lee et al., 2011). Other studies also indicate that the use of new or little-known technology is a reason for estimation inaccuracies (Basten and Mellis, 2011, Furulund and Molkken-stvold, 2007, Jørgensen and Gruschke, 2009) and a significant threat to estimates (Keaveney and Conboy, 0000). Also, many studies report results regarding how the misunderstanding of requirements leads to estimation inaccuracy and errors (Conoscenti et al., 2019, Jorgensen and Molokken-Ostvold, 2004, Jørgensen and Gruschke, 2009, Keaveney and Conboy, 0000, Matos et al., 2013). It also causes unintentional distortions of software estimates in different directions: either as increases or decreases of estimates (Magazinius et al., 2012).


Download : Download high-res image (679KB)
Download : Download full-size image
Fig. 12. Factors related to the team.

The team’s skill is a value adjustment characteristic at theexecuting phase once three studies present it as either an effort predictor or a cost driver (Britto et al., 2015, Usman et al., 2017, Usman et al., 2015). Another more specific factor is the technical skill, which we partially addressed in Section 5.3. The presence of unskilled members in the team is a reason for inaccurate estimates (Usman et al., 2015). Lack of technical skills (Furulund and Molkken-stvold, 2007) and technical expertise in a particular area (Keaveney and Conboy, 0000) lead to estimation inaccuracies. Less software development skill is weakly connected with optimistic predictions too (Jørgensen et al., 2007). More specifically, Jorgensen et al. (2020) reported that lower programming skills connect with higher over-optimism in larger tasks and higher over-pessimism in smaller tasks.

Two respondent studies report how diligence issues may impact estimates negatively. Lack of diligence by systems analysts and programmers is a reason for inaccuracy (Lederer and Prasad, 1995a). Also, the delay of decisions concerning requirements due to team members’ lack of responsibility and motivation is a reason for a higher need for effort than estimated (Basten and Mellis, 2011). So, lack of diligence is an error factor.

Many studies report findings regarding a range of issues related to team’s size and stability issues. The team’s size is an effort predictor (Altaleb et al., 2020a, Huang et al., 2015), and larger teams connect with higher effort and costs (He et al., 2010, Lagerström et al., 2012, Silva-de-Souza and Travassos, 2017). It is, therefore, a value adjustment characteristic. The interaction of team size and language type and the interaction of team size and development platform significantly impact productivity (Altaleb et al., 2020a, Huang et al., 2015). Interestingly, two studies suggest that multiple developers’ involvement in a story or a task may lead to over or underestimations (Conoscenti et al., 2019, Hill et al., 2000). So, larger team size also is an error factor.

The last three factors of the executing phase are intricately connected. Turnover is a reason for inaccuracies in estimates (Lederer and Prasad, 1995a, Lind and Sulek, 1998, Usman et al., 2015) and estimating problems (Lederer and Mirani, 1990). The loss of organizational knowledge due to high turnover is an estimation challenge (Magazinius and Svensson, 2014). The existence of new team members leads to estimation inaccuracies (Conoscenti et al., 2019) and a higher need for effort than estimated (Basten and Mellis, 2011). Another study reports that the introduction of new people is a major threat to accurate estimates (Magazinius and Svensson, 2014) — and therefore, we classified it as an error factor. Finally, regarding team stability, one study reports it as a cost driver (Usman et al., 2015), while another one stresses that team continuity leads to less effort in the context of testing tasks (Silva-de-Souza and Travassos, 2017). Therefore, team stability is a value adjustment characteristic that estimators should account for when estimating.

Two factors impact all phases. The team’s overall experience is one of them — and we explored some of its facets in Section 5.2. Three studies report it as more specifically connected with the team. Two respondent studies put the team’s overall experience as an effort predictor or a cost driver (Britto et al., 2015, Usman et al., 2015). Another respondent study indicates that low team experience correlates with duration error (Morgenshtern et al., 2007).

The other factor related to all phases is collaboration and communication. The communication process and the communication model are effort predictors (Altaleb et al., 2020a, Britto et al., 2015). On the one hand, team collaboration facilitates software estimation and accuracy (Matos et al., 2013). On the other hand, the lack of stakeholder collaboration is a reason for inaccurate estimates (Yang et al., 2008). Also, inherent difficulties related to communication and coordination present in multi-site arrangements lead to higher effort overruns (Usman et al., 2018a).

5.6. No specific role
In Fig. 13, we present a whole set of factors we found that is not specifically connected with any roles. They may impact or be caused by any or all of them.

During the planning phase, price-to-win issues play a role in estimation when present. Price-to-win is described as an estimate defined by the price or schedule needed to win a job (Boehm, 1984). An estimate strongly impacted by price-to-win is a reason for estimation error (Jorgensen and Molokken-Ostvold, 2004). Allowing the project bidding requirements to predefine the project cost (Yang et al., 2008) or purposefully underestimating the effort to obtain a contract (Usman et al., 2015) are reasons for inaccurate estimates. Magazinius et al. (2012) also report intentional distortions of software estimates in varying directions because estimates are budget determined. Somewhat related is the goals and targets factor. In field studies, the authors report that personal goals affect the estimates (Magazinovic and Pernstål, 2008), and that personal or organizational agendas lead to intentional distortions of software estimates in varying directions (Magazinius et al., 2012).


Download : Download high-res image (769KB)
Download : Download full-size image
Fig. 13. Factors unrelated to any specific role.

We identified that some of the project and task characteristics also are relevant factors for estimation, such as the similarity with previous tasks/projects. Task similarity is a reason for improving estimation accuracy (Jørgensen and Gruschke, 2009). However, projects frequently different from previous ones are a reason for estimation error (Jorgensen and Molokken-Ostvold, 2004). The task size is also an error factor: larger tasks are more prone to effort overruns (Usman et al., 2018b), and tasks with more subtasks were underestimated compared to tasks involving fewer ones (Hill et al., 2000). Project type also matters: whether it is related to a new or enhanced application in mobile development (Altaleb et al., 2020a). He et al. (2010) also report that re-development needs less effort than enhancement, and new development consumes even less than re-development. Therefore, the project type is a value adjustment characteristic. Finally, two studies inform that task or project simplicity is a reason for accuracy (Jorgensen and Molokken-Ostvold, 2004, Jørgensen and Gruschke, 2009).

A subset of the planning phase factors regards the product characteristics: the product size and complexity. Size is a value adjustment characteristic since many studies report it as a cost driver, effort predictor, or as correlated to effort (He et al., 2010, Usman et al., 2017, Usman et al., 2015, Vijayakumar, 1997) — with larger project sizes leading to more effort (Lagerström et al., 2012). Size is also an error factor. For instance, Conoscenti et al. (2019) report that user story size serves as an indicator for a possible inaccurate estimation. In a data study, more extensive features correlated to larger errors in effort estimates (Layman et al., 2008). Finally, a field study indicates that smaller product customizations tend to be overestimated, while larger ones tend to be underestimated (Usman et al., 2018a).

Complexity is a factor with many facets. Requirements complexity (Silva-de-Souza and Travassos, 2017) and high technical complexity (Lee et al., 2011, Silva-de-Souza and Travassos, 2017, Subramanian et al., 2006) leads to more effort. In mobile development, application form complexity is an effort predictor (Altaleb et al., 2020a). Therefore, complexity is a value adjustment characteristic. Some studies report technical complexity (Magazinius and Svensson, 2014, Morgenshtern et al., 2007, Usman et al., 2018b, Zapata and Chaudron, 2013) and feature complexity (Conoscenti et al., 2019, Magazinius and Svensson, 2014) as estimation challenges or as related to inaccuracies, delays, and under or overestimations.

Overlooked and unplanned tasks is another impacting error factor: it is a challenge for estimation (Lederer and Mirani, 1990) and a source of inaccuracies and errors (Conoscenti et al., 2019, Furulund and Molkken-stvold, 2007, Jorgensen and Molokken-Ostvold, 2004, Jørgensen and Gruschke, 2009, Lederer and Prasad, 1995a, Magazinius et al., 2012). Unplanned tasks or re-work also is a reason for estimation error (Jorgensen and Molokken-Ostvold, 2004). Closely related, incorrect assumptions when estimating is also an error factor that may be related to the code (Jørgensen and Gruschke, 2009), functionality (Conoscenti et al., 2019), or complexity (Furulund and Molkken-stvold, 2007, Jørgensen and Gruschke, 2009).

At the execution phase, distributed development issues also play a role when they are present. Two studies report cultural differences as an effort predictor (Altaleb et al., 2020a, Britto et al., 2015). Thus, estimators should consider it a value adjustment characteristic if there are multiple development sites with differing cultures.

The use of flexible/agile development models is an accuracy factor regarding project and task characteristics. Molokken-Ostvold and Jorgensen (2005) report that flexible models are associated with lower effort overruns than sequential models. Koch and Turk (2011) also report that the use of agile methods is related to less effort deviation from estimate than rigid models. However, Brown et al. (2013) inform that software developers give lower estimates when the development method is agile than when the development method is a waterfall, suggesting their estimates were optimistic.

Resources dependencies also stood out as one factor affecting estimates. Depending on external resources may lead to delays and/or higher effort that should be considered when estimating (Jørgensen, 2011). Also, dependencies (such as for code reviews) on specific human resources (e.g., product architects) introduce delays (Usman et al., 2018a), and developer resource constraints and external commitments are a reason for estimation inaccuracy (Layman et al., 2008).

Project flexibility is another relevant accuracy factor: a high degree of flexibility in implementing the requirement specification is a reason for accurate estimates (Jorgensen and Molokken-Ostvold, 2004). Another study reports that project flexibility to reduce the scope (functionality/quality) in order to meet plan and budget is a factor more frequent in projects with lower overrun (less than 20% overrun) compared to projects with higher overrun (more than 20% overrun) (Grimstad et al., 2005).

The occurrence of unforeseen problems is a factor that impacts estimates negatively. The occurrence of risks, unexpected events, or technical problems leads to a higher need for effort than estimated and estimation errors (Basten and Mellis, 2011, Conoscenti et al., 2019, Jørgensen and Gruschke, 2009).

Two of the factors affect all phases. The business area has an impact on the effort (He et al., 2010) and productivity (Huang et al., 2015). The other factor is tool support and availability. Software development tools have an empirical influence over management and testing efforts (Subramanian et al., 2017). Additionally, insufficient tool support for project management is a reason for estimation error (Jorgensen and Molokken-Ostvold, 2004), and the low availability of required tools leads to higher effort (Lee et al., 2011).

6. Discussion
Our primary research question for this SLM was RQ 1 - How have researchers investigated the factors that affect expert judgment software estimation? In this section, we summarize our current answer to this question and discuss our findings.

6.1. The seas of factors that researchers explored the most
The top-five factors in the SEXTAMT regarding the number of articles reporting them are changes to requirements or scope (12 articles), clear requirement specifications (10 articles), product size, complexity, and use of historical data (9 articles each). Most factors (40, representing around 58% of the total) were reported in three or more articles. The remaining 29 factors (around 42%) were investigated in two research articles only, indicating that they could benefit from further investigation.

In addition, many of the top factors were probably investigated extensively because they have a relevant impact on the estimates. Nevertheless, others may have been investigated because of a controversial result. Controversies possibly exist either because of differences in research design or because such factors are more sensitive to the context. Future research efforts should aim to clarify which is the case. For instance, regarding the combination strategy of individual estimates most of the results shows that group estimation led to less optimistic estimates compared with averaging. However, one study found the opposite when participants were students (Mahnič and Hovelja, 2012). It is unclear whether this controversial result is due to the difference in choice of participants (software professionals or students) or whether experience interacts with the combination strategy to define which one will bring superior results (more on this in Section 6.5).

Also, if a factor is shown to influence estimates through the employment of varied research strategies, we can more confidently believe that such an effect exists. Each research strategy has its inherent limitations and strengths (Stol and Fitzgerald, 2018). Also, each one has the potential to maximize one research quality criteria at the expense of others. For instance, respondent strategies have the potential to maximize generalizability; field strategies can maximize realism; laboratory strategies can maximize control; and data strategies can maximize precision (Storey et al., 2020). Therefore, we evaluated the existing evidence for the factors in the SEXTAMT by considering the research strategies that researchers employed to investigate them.

Fig. 14 represents only the factors investigated in five or more articles — 21 factors in total, represented by the light gray edges surrounding the top of the circle. We also mapped the factors to the research strategies that researchers employed to investigate them, represented at the bottom of the circle: respondent (R, in dark red), field (F, in blue), data (D, in dark gray), or laboratory (L, in orange). Next, we discuss the type of evidence derived from such studies, considering all these research strategies.

First, six factors have been investigated employing at least three different research strategies: product size (1 R, 4 F, 4 D), complexity (4 R, 4 F, 1 D), use of historical data (2 R, 5 F, 1 D, 1 L), overall experience (4 R, 2 F, 1 D), team size (1 R, 2 F, 4 D) and turnover (2 R, 2 F, 1 D). Most of them were investigated through a combination of research, field, and data strategies — suggesting the generalizability, realism of context, and precision of data regarding the supporting findings. Some of these factors are classic cost drivers, such as product size and complexity, and software companies may not have much control over them. Other factors are more controllable but may not be so easy to implement. Still, software practitioners and organizations can organize themselves to use historical data when estimating, increase their overall experience, regulate team sizes to keep them small, and reduce turnover.


Download : Download high-res image (1007KB)
Download : Download full-size image
Fig. 14. Top factors and the studies’ research strategies.. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

All the remaining factors in Fig. 14 were investigated using two different research strategies. In summary, these factors indicate that improving the estimation process is necessary, but not enough to get better results. Practitioners need to enhance the requirements engineering and management process. For example, we need to work on getting clear requirements specifications (6 R, 4 F). Moreover, human and social aspects play a crucial role, as highlighted by factors pertaining to the categories of political issues (such as pressure — 5 R, 3 F), experience (familiarity with the technology — 3 R, 2 F), skill issues (technical skill — 3 F, 2 L), biases (anchoring — 1 F, 4 L), and team issues (team communication and collaboration — 4 R, 1 F). Unexpected events also have their role: overlooked and unplanned tasks (3 R, 4 F). Reducing such events is necessary — possibly with the use of checklists, another factor from SEXTAMT.

The SEXTAMT factors excluded from Fig. 14 were reported in four or fewer articles and investigated through no more than two research strategies. They can further enrich our understanding of the impact of the requirements and the estimation process, for instance. Nevertheless, they expand our perspectives to other directions as well, such as the impact of product characteristics, client and user issues, environment, attitudes, and maturity, and testing and rework.

In any case, software organizations and practitioners aiming to diagnose the factors more relevant to their context to improve their estimation results can use the SEXTAMT factors to guide what to include in internal surveys, for instance. Practitioners can also use the SEXTAMT factors (especially those classified as value adjusting characteristics) to build internal checklists. For instance, Usman et al. (2018b) proposed a process to build checklists to support expert judgment estimation, and the first step is to understand the estimation context. This step has the objective to elicit the factors that should be included in the checklists by, for instance, surveying the literature on the search for effort or cost drivers. The SEXTAMT already provide a map of such factors, and practitioners can save time by using it instead of surveying the literature themselves — a process that involves high costs.

In addition, some of the SEXTAMT factors can be helpful in the debiasing strategy that Kahneman et al. (2021c) proposed to help improve judgments in general: decision observers, i.e., people in charge of observing others making judgments in real-time to identify and alert on the occurrence of biases. Decision observers use checklists to accomplish their tasks, which should be adapted to their specific domain. The SEXTAMT factors can guide such adaptation to the software estimation domain. Particularly, the factors from the bias and the estimation process seas at SEXTAMT can provide valuable items.

Also, practitioners can use the SEXTAMT factors as input for risk analysis for their projects, improving their project planning, monitoring, and control. For instance, projects planned to deliver more extensive or more complex products, with less experienced software teams, or where estimators cannot anticipate the participants’ skills when estimating run a larger risk of estimating error and, therefore, of failing to meet their commitments. Thus, project managers of such projects need to be especially caring for monitoring these factors.



Download : Download high-res image (374KB)
Download : Download full-size image
6.2. Looking through the lenses of the temporal and stakeholder dimensions
When it comes to the process phases in which factors cluster, the planning and executing phases are the ones that stand out. It is natural to have factors at the planning phase because estimating occurs primarily during such stage. At the executing phase, factors emerge because the dynamics of projects impact estimating error and accuracy. For instance, our software projects have a moving target (Magazinius et al., 2012), and we found in our SLM that changes to requirements or scope are an error factor, especially if the original estimates are not modified to reflect the changes. Overlooked and unplanned tasks may also be revealed by project execution dynamics, leading to a higher need for effort, costs, and duration than expected.

It is noticeable that only one factor emerged at the initiating phase and none at the closing phase. However, when looking for the factors reported in one article only, we can find more about such phases. For instance, bidding situations are relevant at the initiating phase, with one field experiment reporting that companies selected on the criteria of the low bid have higher cost overruns, a phenomenon known as the “winner’s curse” (Jorgensen and Grimstad, 2005). Therefore, estimators might need to pay special attention to the initiating phase in bidding contexts.

Additionally, more investigation on learning and feedback has the potential to shed some light on what is relevant at the closing phase. For instance, at least four studies (Jorgensen and Molokken-Ostvold, 2004, Matos et al., 2013, Jørgensen et al., 2007, Jørgensen and Gruschke, 2009) suggest that estimation error, feedback, and learning from past projects and tasks might be beneficial to reducing overconfidence and improving estimates.

Regarding stakeholders, many of the factors are related to estimators, which is expected once they are the primarily responsible people for estimates. Our results also indicate the power of other roles that might not be directly involved with the estimating process, such as the client and managers.



Download : Download high-res image (219KB)
Download : Download full-size image
6.3. The strategies researchers employed to explore the seas
As for the project variables, most studies focused on effort, which is understandable — as (McConnell, 2006a) suggested by his flow of well-estimated projects that the effort is an intermediary estimate in software projects, ideally used as input to cost and duration estimates. Therefore, factors that impact effort estimates indirectly impact both cost and duration, and because of that, researchers may consider it more beneficial to focus on them.

The mechanism for measuring the impact of the factors that researchers applied the most is rather indirect: the participants’ perceptions of reasons for errors and accuracy. Such an approach may provide rich insights into the phenomena that cause errors when estimating or promote accuracy in field settings. Considering that many participants in respondents and field studies in our SLM are experts in software development and maintenance tasks, we cannot overlook their opinions about the factors affecting estimates. Nevertheless, the approach has drawbacks also. For instance, people may attribute different meanings to the term “estimate”, even when they work at the same company (Jørgensen, 2014a), making it difficult to interpret the results of surveys (Jørgensen, 2007a).

Another widely employed mechanism for measuring the impact of factors over the estimates was the difference of estimates between groups. The difference of estimates does not provide direct evidence about accuracy, but it can evidence when a factor causes an estimate to increase or decrease for reasons beyond the estimation process. This allows us to identify factors that can induce optimism in estimators, leading them to provide low estimates instead of realistic ones. Considering that extensive projects tend to be underestimated with a median time overrun of 20% (Halkjelsvik and Jørgensen, 2018a), identifying such factors can be very useful.

Additionally, researchers have used objective error measures, such as MRE, MREBias, BRE, and BREBias. Nevertheless, since the 90’s at least, MRE has been criticized because it has the disadvantage of weighing differently under and overestimations. Underestimations are not weighted sufficiently, leading to higher penalization of overestimations (Jørgensen, 2007a). MREBias suffer from this same problem. BRE and BREBias are balanced metrics in this sense (Molokken-Ostvold and Jorgensen, 2005). In Fig. 15, we grouped MRE and MREBias under the label “Unbalanced” and BRE and BREBias under “Balanced”. It shows that, gradually, researchers are moving to the use of more balanced metrics over the years.

Also, researchers prefer accuracy metrics over bias: with 19 occurrences for MRE and BRE together versus 15 occurrences of MREBias and BREBias. Accuracy is the average unsigned error, irrespective of whether the estimate is too high or too low; bias is the average tendency to generate too high or too low estimates (Halkjelsvik and Jørgensen, 2018b).


Download : Download high-res image (187KB)
Download : Download full-size image
Fig. 15. Balanced (BRE & BREBias) x unbalanced (MRE & MREBias) over the Years.

In any case, using MRE or BRE and similar metrics can be misleading because they depend on actual values, and work can be adjusted to fit an initial estimate (Jørgensen and Sjøberg, 2001), leading to a “moving target problem” (Jørgensen, 2007a) and to a distorted perception of accuracy. For instance, this makes it harder to understand exactly whether a factor contributed effectively to improving estimation accuracy, or whether a software team just took advantage of a higher project flexibility to create an illusion of accuracy. A possible solution comes from the literature about judgment in general: the measurement of noise instead of bias or accuracy. Noise is the random scatter of judgments that should ideally be identical — or in other words, unwanted variability, a significant component contributing to judgment error, along with bias (Kahneman et al., 2021d). The advantage of measuring noise over bias or accuracy is that we do not need to know actual values. One issue that emerges from this discussion is how to measure noise. A common measure from statistics is the standard deviation (Kahneman et al., 2021f).

Nevertheless, we found very few studies discussing the variability of estimates in the software domain. Only one study explores explicitly the issue, showing a high level of inconsistency when software practitioners estimate the same task, based on the same information and under the same conditions, but at different times (Grimstad and Jørgensen, 2007). In addition, very few studies in our SLM report the standard deviation of estimates, when using the difference of estimates as a measurement strategy (see Shepperd et al., 2018, Passing and Shepperd, 2003). This reveals a low awareness of researchers in our community regarding noise, its relationship with error in expert judgment estimation, and the benefits of measuring and reducing it. Regarding software estimation practice, it is unclear whether practitioners share the perspective of researchers about this concept. In any case, software organizations can benefit from investigating how much disagreement there is among their professionals estimating the same tasks independently.

Regarding research strategies, researchers employed the laboratory research strategy widely, and the respondents’ strategy was quite popular too. Laboratory research strategies favor the investigation of only a few factors at once. In contrast, the articles employing respondents strategies tended to reveal much more factors in each study, contributing significantly to the wide variety of factors we found. The factors with more articles using a laboratory experiment strategy were also the ones that researchers refined the most by investigating relevant variations. For instance, researchers investigated different nuances of the anchoring effect, assessing the impact of both numerical and textual anchors (Jorgensen and Grimstad, 2012), as well as of single and interval anchors (Løhre and Jørgensen, 2016). Another refinement was the investigation of the moderating effect of the expertise of the source and of the receiver of the anchor value (Løhre and Jørgensen, 2016) and the impact of one intervention to reduce its effects (Shepperd et al., 2018). Another example is the sequence effect, whose impact over the estimates varies with the size of the tasks estimates in the sequence (Jørgensen and Halkjelsvik, 2020). Researchers perceived an assimilation effect (the estimate become more similar to the one of a previously estimated task) for tasks of different sizes, and a contrast effect (the estimate become more different than the previous one) for tasks of similar sizes.

When considering the taxonomy of Stol and Fitzgerald (2018) for research strategies, it is interesting to notice that the studies employing the field strategy, there are very few field experiments — a total of 10. In other words, when it comes to factors affecting estimates, researchers are more likely to enter natural settings to collect data without manipulating variables. Probably such manipulations are hard to be approved by administrative staff or to be adequately carried out. Thus, they restrict the manipulations of variables to the lab, reinforcing the need for triangulation of strategies (Stol and Fitzgerald, 2018) to evaluate further the impact of factors investigated.

Additionally, considering that the potential for generalizability from respondent studies and the potential for realism from field studies can be taken as proxies of the relevance of research results for practice, from all the 69 factors from the SEXTAMT, most (62) have this type of evidence. From the seven factors with no evidence from respondent or field studies, three are related to biases on estimation and were investigated through lab studies only: sequence effects, time frame size, and unit effects. The client’s expectation was a factor investigated only through lab studies. The programming language, business area, and longer projects emerged from data studies only. Nevertheless, the lack of evidence from respondents and field studies for these factors does not mean they are irrelevant. For instance, practitioners are not aware of the biases affecting them in many cases, which makes it hard for them to identify this kind of factor in respondent studies. Therefore, combining research strategies reveals complementary findings in research topics so complex as this one. This has been highlighted before in the study of reasons for software effort estimation error in one single company: combining information sources, data collection methods, and data analysis methods leads to complementary insights (Jorgensen and Molokken-Ostvold, 2004).



Download : Download high-res image (448KB)
Download : Download full-size image
6.4. Into the wild — part 1: underexplored seas
We excluded from the SEXTAMT a total of 166 factors reported in one research article only each.8 Therefore, we consider they are in a gray area, and there is a need to execute more research to strengthen the evidence about their impact. Some of them have the potential to enlarge the territory of existing seas in the SEXTAMT. In contrast, others have the potential to reveal new seas of their own.

Such a myriad of factors does not mean that all factors reported by unique articles are worthy of further investigation. We need some filtering on them to decide which ones are good candidates for more studies. For instance, luck is a factor reported in a respondents study. However, what does it mean? Also, the presence of other factors we identified in our SLM might explain luck to some extent: we can consider that a software project was luckier because requirements did not change, for example.

In addition, we classified some of these factors as a satellite to others, meaning they are somewhat related, even though not enough to be united to create a final factor to include at the SEXTAMT. One example is “team process experience” and “expertise of new team members”. Although they are related, the first can relate to all team members, not only to new ones, while the latter is very specific in including only new people. Therefore, we cannot unify them to form a single factor investigated in three articles, allowing its inclusion among the SEXTAMT factors. Therefore, we kept them as part of the unique factors, marking them as satellites of each other.

Another example is the case of the factor forcing to stay within the estimate. It is a satellite of one SEXTAMT factor: project flexibility. For instance, software practitioners need the flexibility to deliver less polished features when they are forced to stay within a deadline, no matter what. We can also argue that forcing to stay within the estimate is a repercussion of other factors from unique articles, such as estimates interpreted as commitments or the use of uncertain estimates as baselines. Nevertheless, researchers have not validated such relationships. In any case, we indicate the satellite factors as part of our supplementary material.

Factors investigated through laboratory research strategies are good candidates for field experiments to assess whether their impact is kept in real-life contexts. Take the format factor, for example, which is about using the traditional request format — “How much effort is required to complete X?” – versus using an alternative format — “How much can be completed in Y work-hours?”. In the laboratory, the alternative format has led to more optimistic estimates (Jørgensen and Halkjelsvik, 2010). However, it is precisely the format we expect when using agile methodologies. Does it impact estimates negatively in the trenches, making them more optimistic? Another factor whose effect is relevant in the same context is the use of Fibonacci scales that, compared to linear scales, led to lower estimates when using Tamrakar and Jørgensen (2012).



Download : Download high-res image (87KB)
Download : Download full-size image
6.5. Into the wild — part 2: validated relationships among the factors
The discussion of satellite factors leads us to another underexplored issue: the relationship among different factors. Therefore, after answering the main research questions that we presented in Section 3.1, we decided to extract and analyze data for an additional question: “SQ 1.6 – What are the validated relationships among the factors affecting expert judgment expert estimates?”

Only nine articles had results regarding such relationships. We illustrate the relationships we found in Fig. 16, where each light blue rounded rectangle represents one SEXTAMT factor. Each gray rounded rectangle represents one factor we did not include in the SEXTAMT because it was investigated in only one article.

Fig. 16 shows that overall experience moderates the impact of the combination strategy of individual estimates: more experience is connected with less optimistic estimates when using Planning Poker compared to when using a statistical combination of estimates. In the context where estimators have less experience, the result is inverted: the statistical combination leads to less optimism (Mahnič and Hovelja, 2012). This result suggests that without experience discussions lack the benefit of meaningful divergent perspectives about the task complexity, or the wisdom to recognize forgotten tasks, or other flaws in judgment. It also seems that higher experience is needed to overcome the effect of the social influence bias (Lorenz et al., 2011) in group discussions of the estimates. Nevertheless, we should consider this result carefully because researchers contrasted a sample of students (representing less experience) with a sample of software professionals (representing more experience).


Download : Download high-res image (366KB)
Download : Download full-size image
Fig. 16. Relationships among factors.. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Overall experience also reduces the impact of the anchoring effect over the estimates (Løhre and Jørgensen, 2016), as well as debiasing workshops (Shepperd et al., 2018) and the use of subsequent anchors aimed at neutralizing first impressions caused by the first anchor (Jørgensen and Løhre, 2012). However, in none of these studies, the anchoring effect was completely removed: only reduced. In another study, the researchers showed that mixed-handers were more strongly influenced by anchors compared with strong-handers (Jørgensen, 2007c), revealing how handedness can influence estimates.

Handedness also impacts the effect of more and/or irrelevant information: mixed-handers also are more impacted by irrelevant information (Jørgensen, 2007c). In addition, people who score high in interdependence are also more strongly influenced by more and/or irrelevant information than people who score low (Jorgensen and Grimstad, 2012). Higher interdependence refers to higher connectedness to others and higher importance to social context and relationships. In addition, another study showed that higher technical skill reduces the impact of more and/or irrelevant information (Jørgensen and Grimstad, 2008).

Also, the time frame size moderates the impact caused by using an alternative format for requesting estimates. Smaller time frames increase the impact by leading to more optimistic estimates (Halkjelsvik and Jorgensen, 2011, Jørgensen and Halkjelsvik, 2010).

Interestingly, many of the factors on the leftmost side of Fig. 16 are related to a psychological or social bias. For instance, combination strategies of individual estimates are subject to social biases. The anchoring effect is a psychological bias. The presence of more and/or irrelevant information can also bias judgment, leading people to think the task is larger than it truly is, for example. Additionally, the moderating factors give us hints about interventions to deal with such biases. For instance, if we know estimators have low experience, it might be wiser to use the statistical combination of estimates instead of Planning Poker. Another example is composing estimating teams to include people with higher experience and technical skills whenever possible because this helps reduce the effects of psychological biases.

These results show the relevance of studying the relationships among factors. Such relationships can reveal the paths of interaction among factors and the ones that can trigger chains of negative or positive effects over the estimates. Therefore, in software process improvement initiatives regarding the estimation process, focusing in factors mediating or moderating others can be a cost-effective strategy to improve accuracy.



Download : Download high-res image (118KB)
Download : Download full-size image
7. Threats to validity
We analyzed the validity threats to this SLM, considering threats to the study selection validity, threats to data validity, and threats to research validity (Ampatzoglou et al., 2019). One of the threats for study selection validity is the adequacy of initial relevant publications identification, addressed with an automatic search in known digital libraries. Another mitigation action to this threat was the use of a known set of papers to evaluate the search strategy (Zhang et al., 2011). The goal of this evaluation was to reach a sensitivity of 70% in automated search (Zhang et al., 2011). A final mitigation action to this threat was snowballing procedures to enlarge the number of retrieved relevant papers, reaching a sensitivity of 100% afterward. Another threat to study selection validity for this SLM is the study inclusion/exclusion bias, addressed through the definition of study inclusion and exclusion criteria in the research protocol. Additionally, the authors executed the selection process over a sample of the articles, discussing any inclusion or exclusion conflicts. Their agreement level was measured with the kappa statistic, leading to the refinement of the inclusion and exclusion criteria.

A threat to data validity in this SLM is the data extraction bias, addressed through a pilot data extraction. The authors reviewed and discussed a pilot data extraction sample to improve the data extraction form. Another threat is the bias of classification schema. To avoid it, we relied on previous existing classifications when possible, such as the research strategies framework of Storey et al. (2020). We used the process groups from PMBOK (Project Management Institute, 2017a) for the phases and familiar stakeholders’ roles regarding the factors. We aggregated similar findings under labels that reflected the articles’ original texts for naming the factors affecting software estimates. The authors held meetings for reviewing the factors and the categories in the SEXTAMT, and the types of effects of each factor.

As for research validity, there is the threat of lack of repeatability. One of the mitigation actions for this threat was involving more than one researcher during the process. Another action is to make all the SLM data publicly available, including decisions about inclusion and exclusion of papers, extracted data from primary studies, among others. Finally, we developed a research protocol to ensure replications or updates to this SLM. The protocol we developed and the discussions among the researchers involved helped mitigate the research method bias, another threat to research validity.

8. Conclusion
In this article, we presented an SLM about factors affecting expert judgment software estimates. We present such factors by three dimensions: the project phase they are likely to happen or to cause an impact over the estimates; the stakeholder that is responsible for a task or process to which the factor is linked, that directly causes the factor or that is directly impacted by the effects of the factor; and type of effect the factor causes. Some factors can have a negative effect, leading to errors when they are present, while others may have a positive or neutral effect. Such dimensions allow for easier navigation through the myriad of factors we found.

Most of the factors clustered at the planning and executing phases. It is natural to have factors at the planning phase because estimating occurs primarily during such stage. At the executing phase, factors emerge because the dynamics of projects impact estimating error and accuracy. Moreover, most of the studies employed a research strategy of laboratory experiments, investigating one factor in a controlled setting with an experimental and control group. Also, they evaluated the difference of estimates between these groups to assess the impact of the factors.

Top factors – those that emerged in a higher number of studies – revealed the importance of issues beyond the estimation process. It is also necessary to improve the requirements engineering process, to deal with political issues, to consider the product characteristics, among others. Researchers have investigated a wide and varied set of factors. Therefore, we created a map to support readers in navigation through them: the SEXTAMT. If an interested reader desires to identify all factors that affect only one project phase, we provide them a classification through this dimension. If the reader desires to identify all factors given one stakeholder, we also provide this. Finally, if the reader wants to find out a class of factors given a specific effect – for instance, all factors that lead to improved accuracy – our map also has a dimension regarding this.

Our research confirms and aggregates existing results about factors affecting expert judgment estimates, a relevant contribution to move knowledge forward, especially when we organize such knowledge to facilitate understanding and future uses (for both research and practice). Also, the classification of measurement strategies is an additional relevant contribution. This enabled us to spot that our research community is missing the benefits of investigating more of noise as component of error.

The SEXTAMT can have many valuable uses in practice and software practitioners can employ its factors as part of many different initiatives, such as:

•
Diagnosing improvement opportunities to their estimation processes through the investigation of the most relevant factors in their contexts, considering their types of effects;

•
building checklists to support estimators, considering especially the value adjustment characteristics;

•
adapting checklists to aid debiasing interventions, considering especially the factors from the bias and estimation process categories;

•
analyzing project risks by identifying the factors leading to larger risks of estimating error in their contexts and, therefore, of leading to failures to meet their commitments.

As for future work, we need to keep the SEXTAMT updated. Special care is due to the factors coming from unique articles: more investigation about them is needed. However, some filtering to identify the best candidates for more assessment is also necessary. Another critical issue is investigating the relationships among the factors to enrich the map with relevant mediation and moderation connections. A more complex framework can be helpful to identify the factors more likely to cause a more considerable impact over the estimates, focusing on them to adopt more cost-effective interventions during software improvement initiatives regarding estimation processes.

We highlight that another research issue comes from the software project dynamics that allows practitioners to adjust their work to fit an estimate when they need to, creating a “moving target” problem. This makes it harder to measure error and accuracy correctly. It also makes it harder to understand whether a factor contributed effectively to improving estimation accuracy or whether a software team just took advantage of higher project flexibility to create an illusion of accuracy. The solution to this comes from the judgment literature: measuring noise — unwanted variability from judgments that ideally should be identical (Kahneman et al., 2021d). Nevertheless, few studies from our SLM discuss this issue, revealing that our research community can benefit from understanding and using more of this concept.