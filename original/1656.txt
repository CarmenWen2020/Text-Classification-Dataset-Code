Abstract
Regression test case prioritization (RTCP) aims to improve the rate of fault detection by executing more important test cases as early as possible. Various RTCP techniques have been proposed based on different coverage criteria. Among them, a majority of techniques leverage code coverage information to guide the prioritization process, with code units being considered individually, and in isolation. In this paper, we propose a new coverage criterion, code combinations coverage, that combines the concepts of code coverage and combination coverage. We apply this coverage criterion to RTCP, as a new prioritization technique, code combinations coverage based prioritization (CCCP). We report on empirical studies conducted to compare the testing effectiveness and efficiency of CCCP with four popular RTCP techniques: total, additional, adaptive random, and search-based test prioritization. The experimental results show that even when the lowest combination strength is assigned, overall, the CCCP fault detection rates are greater than those of the other four prioritization techniques. The CCCP prioritization costs are also found to be comparable to the additional test prioritization technique. Moreover, our results also show that when the combination strength is increased, CCCP provides higher fault detection rates than the state-of-the-art, regardless of the levels of code coverage.

Previous
Next 
Keywords
Software testing

Regression testing

Test case prioritization

Code combinations coverage

1. Introduction
Modern software systems continuously evolve due to the fixing of detected bugs, the adding of new functionalities, and the refactoring of system architecture. Regression testing is conducted to ensure that the changed source code does not introduce new defects. However, it can become expensive to run an entire regression test suite because its size naturally increases during software maintenance and evolution: In an industrial case reported by Rothermel et al. (1999), for example, the execution time for running the entire test suite could become several weeks.

Regression test case prioritization (RTCP) has become one of the most effective approaches to reduce the overheads in regression testing (Li et al., 2007, Jiang et al., 2009, Mei et al., 2012, Saha et al., 2015, Ledru et al., 2009). RTCP techniques reorder the execution sequence of regression test cases, aiming to execute those test cases more likely to detect faults (according to some award function) as early as possible (Hao et al., 2016, Hao et al., 2014, Zhang et al., 2009).

Traditional RTCP techniques (Rothermel et al., 1999, Zhang et al., 2013a, Wang et al., 2017) usually use code coverage criteria to guide the prioritization process. Intuitively speaking, a code coverage criterion indicates the percentage of some code units (e.g. statements) covered by a test case. The expectation is that test cases with higher code coverage value have a greater chance of detecting faults (Zhu et al., 1997). Because of this, a goal of maximizing code coverage has been incorporated into various RTCP techniques, including greedy strategies (Rothermel et al., 1999). Given a coverage criterion (e.g., method, branch, or statement coverage), the total strategy selects the next test case with greatest absolute coverage, whereas the additional strategy selects the one with greatest coverage of code units not already covered by the prioritized test cases. Furthermore, Li et al. (2007) proposed two search-based RTCP techniques (a hill-climbing strategy and a genetic strategy) to explore the search space (the set of all permutations of the test cases) to find a sequence with a better fault detection rate. Jiang et al. (2009) investigated adaptive random techniques (Huang et al., 2019) to prioritize test cases using code coverage criteria. In an attempt to bridge the gap between the two greedy strategies, Zhang et al. (2013a) proposed a unified approach based on the fault detection probability for each test case (referred to as a p value).

In this paper, we propose a new coverage criterion, code combinations coverage, that combines the concepts of code coverage (Zhu et al., 1997) and combination coverage (Nie and Leung, 2011): Given a set of regression test cases , each test case is first transferred to an equally-sized tuple. Each position in this tuple is a binary value representing whether the corresponding item (such as branch, statement, or method) is covered by this test case. In other words,  is represented by a set of abstract test cases with binary values 
. The code combinations coverage of  is measured by the traditional combination coverage of 
. We apply this new coverage criterion to RTCP, proposing a new prioritization technique: code combinations coverage based prioritization (CCCP).

We conducted empirical studies on 14 versions of four Java programs, and 30 versions of five real-world Unix utility programs. Our goal was to investigate the testing effectiveness and efficiency of CCCP compared with four widely-used RTCP techniques — total, additional, adaptive random, and search-based test prioritization. The results show that when the lowest combination strength is assigned, overall, our approach has better fault detection rates than the other four test prioritization techniques. It not only achieves comparable testing efficiency to additional, but also requires much less prioritization time than the adaptive random and search-based techniques. In addition, while the code coverage granularity does not impact on the testing effectiveness of CCCP, the test case granularity does significantly impact on it. Furthermore, when the combination strength is increased, CCCP provides better fault detection rates than all other RTCP techniques, regardless of the level of code coverage.

The main contributions of this paper are:

•
We propose a new coverage criterion called code combinations coverage that combines the concepts of code coverage and combination coverage.

•
We apply code combinations coverage to RTCP, leading to a new prioritization technique called code combinations coverage based prioritization (CCCP).

•
We report on empirical studies conducted to investigate the test effectiveness and efficiency of CCCP compared to four widely-used prioritization techniques, and also analyze the impact of code coverage granularity and test case granularity on the effectiveness of CCCP.

•
We provide some guidelines for how to choose the combination strength and code-coverage level for CCCP, under different testing scenarios.

The rest of this paper is organized as follows: Section 2 presents some background information. Section 3 introduces the proposed approach. Section 4 presents the research questions, and explains details of the empirical study. Section 5 provides the detailed results of the study and answers the research questions. Section 6 discusses some related work, and Section 7 concludes this paper, including highlighting some potential future work.

2. Background
In this section, we provide some background information about abstract test cases and test case prioritization.

2.1. Abstract test cases
For the system under test (SUT), there are some parameters 
 that may influence its performance, such as configuration options, components, and user inputs. Each parameter 
 can take some discrete values to form the set 
, which is finite. By selecting a value for each parameter, its combination becomes an abstract test case (Grindal et al., 2006).

Definition 1 Abstract Test Case

An abstract test case is a discrete test case that can be represented by a -tuple 
, where 
 is a value of a parameter 
 from a finite set 
 (i.e., 
).

Each abstract test case covers some -wise tuples (called -wise parameter-value combinations Zhang and Zhang, 2011 or -wise schemas Nie and Leung, 2011), where . For example, an abstract test case  covers the six 2-wise parameter-value combinations , , , , , and ; and also covers the four 3-wise parameter-value combinations , , , and . Intuitively speaking, when , a -wise parameter-value combination becomes an abstract test case.

For ease of description, we define a function  that returns a set of all -wise parameter-value combinations covered by an abstract test case 
, i.e., (1)
Obviously, the size of  is equal to  (the number of -combinations from  elements). To calculate the -wise parameter-value combinations covered by the set  of abstract test cases, the function  is defined as: (2)

2.2. Test case prioritization
Regression test case prioritization (RTCP) (Rothermel et al., 1999) aims to reorder the test cases to realize a certain goal, such as exposing faults earlier. RTCP is formally defined as:

Definition 2 Regression Test Case Prioritization:

Given a regression test suite ,  is the set of its all possible permutations, and  is an object function from  to real numbers. The problem of RTCP (Rothermel et al., 1999) is to find 
, such that 
.

RTCP is an effective means to reduce the cost of regression testing, and has been widely investigated (Rothermel et al., 1999, Li et al., 2007, Jiang et al., 2009, Zhang et al., 2009), with a large number of studies focusing on the coverage criterion and prioritization algorithms. Intuitively, code coverage criteria can be regarded as characteristics of the test cases, and many prioritization algorithms have used coverage criteria to guide the prioritization process (such as the greedy strategies Rothermel et al., 1999, search-based strategies Li et al., 2007, and adaptive random strategies Jiang et al., 2009).

3. Approach
In this section, we introduce the details of test case prioritization by code combinations coverage.

3.1. Greedy techniques
There are two widely investigated RTCP strategies: the total greedy strategy and the additional greedy strategy. The total strategy selects test cases according to a descending order of code units covered by the test case. The additional strategy also selects test cases according to a descending order, but uses the number of code units not already covered by previously selected test cases. According to previous studies (Luo et al., 2016, Luo et al., 2019, Henard et al., 2016), although seemingly simple, the greedy strategies (especially additional) perform better than most other RTCP techniques in terms of the fault detection rate. Therefore, in our study, we used a simple greedy algorithm to instantiate the CCCP prioritization function for statement, branch, and method coverage criteria. As we just want to evaluate the performance of code combinations coverage against traditional code coverage (e.g. statement) and the additional strategy has been widely accepted as the most effective prioritization strategy, we thus implemented greedy strategies based on the work of Rothermel et al. (1999) as the control techniques for evaluation of our proposed approaches.

3.2. Code combinations coverage
Various RTCP approaches, based on different prioritization strategies, have been proposed to reduce regression testing overheads. Many of these approaches used individual code unit coverage of a test case to guide the prioritization process. For example, greedy strategies only take the number of covered code units into account, with the code units considered as parameters, or individually, and in isolation. However, this may lead to a loss of coverage information, and regression testing has traditionally used historical testing information to guide future testing. Thus, the degree to which the information is used is significant for regression testing, and if we consider the combination between code units, it may be possible to devise strategies to take further advantage of the code coverage information. In our hypothesis, the code units are related, not isolated, and faults may be triggered by combinations amongst them. Based on this, we can make use of more accurate testing information than traditional RTCP approaches to guide the prioritization process.

In our work, a code unit is a general term describing one structural code element — a statement, branch, or method. Consider a program  that has  code units (statements, branches, or methods) that form the code unit set 
, and a regression test set  with  test cases (
). We define a function  to measure whether or not a test case  covers the code unit , as follows: (3) 
 As a result, each test case  can be represented by an -wise binary array through the  function: 
, 
. For ease of description, with the increase of  for each 
 in , we make use of the incremental values to describe whether or not  covers the code unit 
: 
, where , defined as: (4)
 In other words, an odd number represents the situation where the code unit in question is covered by a given test case; and an even number means that it is not covered. For example, using Eq. (3), the values  for a test case  would mean that the first three code units are covered by , but that the last two are not. Eq. (4) would allow this to be represented as . In effect, each code unit can be considered a parameter that contains binary parameter values (an odd number and an even number): the first code unit takes the value 1 or 2; the second code unit takes 3 or 4; and so on. In other words, each test case becomes an abstract test case (as defined in Section 2.1). The -wise code combinations coverage (CCC) value of  against the test set  is defined as the number of -wise code-unit combinations covered by  that are not covered by : (5)
where 
.


Download : Download high-res image (154KB)
Download : Download full-size image
3.3. Code combinations coverage based prioritization
In our model, we view CCCP as a general strategy that can be applied to different prioritization algorithms using different coverage criteria. As greedy strategies are among the most widely-adopted prioritization strategies (Rothermel et al., 1999, Zhang et al., 2013a), and the additional greedy strategy is considered to be one of the most effective RTCP approaches (Jiang et al., 2009, Luo et al., 2016, Luo et al., 2019, Lu et al., 2016), in terms of fault detection rate, we adopted a simple greedy strategy to instantiate the function for the proposed code combinations coverage.


Download : Download high-res image (391KB)
Download : Download full-size image
Generally speaking, the approach chooses an element from candidates as the next test case such that it covers the largest number of -wise code-unit-value combinations that have not already been covered by previously selected test cases. Accordingly, the test case with the maximum number of uncovered code-unit-value combinations compared with the already selected test cases  will be selected: , from Eq. (5). Algorithm 1 provides the detailed  calculation process. Furthermore, when there are no further new code-unit-value combinations, then all remaining test cases are prioritized according the previous process, in a manner similar to the additional greedy strategy.

Algorithm 2 formally presents the pseudocode of CCCP. A Boolean array  denotes whether or not test case 
 has been selected for prioritization; and another Boolean array  identifies whether or not test case 
 covers the code unit 
. Similarly, an array  stores the -wise code-unit-value combinations covered by 
; and a set  stores all uncovered -wise code-unit-value combinations. In addition, the variable  indicates whether or not all -wise code-unit-value combinations have been covered.

In Algorithm 2, Lines 1–24 perform initialization, and also choose the first test case from the candidates; while Lines 25–49 prioritize the test cases. More specifically, since each candidate test case covers the same number of uncovered -wise code-unit-value combinations before prioritization, our approach follows the total and additional test prioritization techniques to choose the first test case: the one covering the largest number of units (Lines 9–16). Then, the number of uncovered -wise code-unit-value combinations against previously selected test cases () is calculated for each remaining test case, and a candidate with the maximum value is selected as the next test case is appended to  (Lines 31–39). Before choosing the next test case, our approach examines whether or not there are any -wise code-unit-value combinations that are not covered by the test cases in : If there are not, the remaining candidate test cases are prioritized by restarting the previous process (Lines 26–28). Once an element is selected as the next test case, our approach updates the set of uncovered -wise code-unit-value combinations (Lines 23 and 46). This process is repeated until all elements from  have been added to . Similar to additional test prioritization, when facing a tie where more than one test case has the largest number of uncovered code-unit-value combinations, our approach randomly selects one.

To further explain the details of the proposed approach, Fig. 1 illustrates an example of the CCCP process with . Similar to the total and additional test prioritization techniques, CCCP chooses the first test case that covers the largest number of code units (the maximum amount of odd numbers). Since there are two candidates with the maximum number of code units, 
 and 
 (both covering three code units), CCCP randomly chooses one of them (in this case, 
), and adds it to . CCCP then updates the set , and calculates the CCC value for each candidate: 
 and 
. Since 
 has the greater CCC value, it is selected as the next test case, and added to . In contrast, the total prioritization technique would choose 
 as the second test case, because 
 covers more code units than 
; and the additional technique would randomly select one from 
 and 
 as the second test case, because both candidates cover the same number of uncovered code units. Finally, in our approach, the last candidate 
 is added to , resulting in 
.


Download : Download high-res image (281KB)
Download : Download full-size image
Fig. 1. An illustrative example of CCCP.

4. Empirical study
In this section, we present our empirical study, including the research questions underlying the study. We also discuss some independent and dependent variables, and explain the subject programs, test suites, and experimental setup in detail.

4.1. Research questions
Due to space limitations and practical performance constraints (higher  values may result in more substantial running time), we present the evaluation of our proposed approach’s performance when  and . Unless explicitly stated,  is used as the default value for CCCP. The empirical study was conducted to answer the following six research questions.

RQ1
How does CCCP compare with other RTCP approaches in terms of testing effectiveness measured by the fault detection rates?

RQ2
How does CCCP compare with other RTCP approaches in terms of testing effectiveness measured by the cost-cognizant fault detection rates?

RQ3
How does the granularity of code coverage impact the comparative effectiveness of CCCP?

RQ4
How does the granularity of test cases impact on the comparative effectiveness of CCCP?

RQ5
How does the efficiency of CCCP compare with other RTCP approaches, in terms of execution time?

RQ6
How does the use of code combinations coverage with  impact on the testing effectiveness of CCCP?

4.2. Independent variables
In this study, we consider the following three independent variables.


Table 1. Studied RTCP techniques.

Mnemonic	Description	Reference
Greedy total test prioritization	Rothermel et al. (1999)
Greedy additional test prioritization	Rothermel et al. (1999)
Adaptive random test prioritization	Jiang et al. (2009)
Search-based test prioritization	Li et al. (2007)
Our proposed CCCP technique	This study

Table 2. Subject program details .

Language	Program	Version	KLoC	#Branch	#Method	#Class	#Test_Case	#Mutant	#Subsuming_Mutant
#T_Class	#T_Method	#All	#Detected	#SM_Class	#SM_Method
Java		v1_9	25.80	5240	2511	228	34 (34)	137 (135)	6,498	1332	59	32
1.4	39.70	8797	3836	342	52 (52)	219 (214)	11,027	2677	90	47
1.4.1	39.80	8831	3845	342	52 (52)	219 (213)	11,142	2661	92	47
v1_7_3	33.70	3815	2919	334	26 (21)	78 (61)	8,850	573	38	20
v1_8	33.10	3799	2838	319	29 (24)	80 (74)	8,777	867	37	22
v1_8_1	37.30	4351	3445	373	33 (27)	78 (77)	9,730	1667	47	25
v1_9_RC1	38.40	4484	3536	380	33 (27)	78 (77)	10,187	1703	47	25
v1_9_RC2	41.10	4888	3613	389	37 (30)	97 (83)	10,459	1651	53	29
0.4	1.89	519	284	19	10 (10)	126 (126)	704	399	29	9
0.5.1	2.03	583	302	21	11 (11)	128 (128)	774	446	34	10
0.6	5.36	1491	748	50	18 (16)	209 (207)	1,906	1024	57	16
v1_0_4	18.30	3534	1627	179	15 (15)	92 (91)	5,501	1198	32	12
v1_0_5D2	19.00	3789	1629	180	15 (15)	94 (94)	5,725	1204	33	12
v1_0_71	16.90	3156	1398	145	13 (13)	84 (84)	3,833	1070	27	10
C		2.4.3	8.96	2005	138	–	500	–	–	–
2.4.7	9.47	2011	147	–	500	13,873	6177	32
2.5.1	12.23	2656	162	–	500	14,822	6396	32
2.5.2	12.25	2666	162	–	500	775	420	20
2.5.3	12.38	2678	162	–	500	14,906	6417	33
2.5.4	12.37	2680	162	–	500	14,922	6418	32
2.0	8.16	3420	119	–	144	–	–	–
2.2	11.99	3511	104	–	144	23,896	3229	56
2.3	12.72	3631	109	–	144	24,518	3319	58
2.4	12.83	3709	113	–	144	17,656	3156	54
2.5	20.84	2531	102	–	144	17,738	3445	58
2.7	58.34	2980	109	–	144	17,108	3492	59
1.0.7	4.32	1468	81	–	156	–	–	–
1.1.2	4.52	1490	81	–	156	7,429	639	8
1.2.2	5.05	1752	98	–	156	7,599	659	8
1.2.3	5.06	1610	93	–	156	7,678	547	7
1.2.4	5.18	1663	93	–	156	7,838	548	7
1.3	5.68	1733	97	–	156	8,809	210	7
3.75	17.46	4397	181	–	111	–	–	–
3.76.1	18.57	4585	181	–	111	36,262	5800	37
3.77	19.66	4784	190	–	111	38,183	5965	29
3.78.1	20.46	4845	216	–	111	42,281	6244	28
3.79	23.13	5413	239	–	111	48,546	6958	29
3.80	23.40	5032	268	–	111	47,310	7049	28
3.01	7.79	676	66	–	324	–	–	–
3.02	7.79	712	65	–	324	2,506	1009	16
4.0.6	18.55	1011	65	–	324	5,947	1048	18
4.0.8	18.69	1017	66	–	324	5,970	450	18
4.1.1	21.74	1141	70	–	324	6,578	470	19
5	4.2	26.47	1412	98	–	324	7,761	628	22

Table 3. Statistical effectiveness comparisons of APFD for Java programs at the test-class level. For a comparison between two methods 
 and , where 
, the symbol ✔ means that 
 is better (-value is less than 0.05, and the effect size 
 is greater than 0.50); the symbol ✖ means that  is better (the -value is less than 0.05, and 
 is less than 0.50); and the symbol ❍ means that there is no statistically significant difference between them (the -value is greater than 0.05) .

Program name	Statement coverage	Branch coverage	Method coverage
✖(0.00)	❍(0.49)	✔(1.00)	❍(0.50)	✔(0.76)	❍(0.51)	✔(1.00)	❍(0.49)	✔(1.00)	❍(0.49)	✔(1.00)	✖(0.36)
✔(1.00)	❍(0.51)	✔(1.00)	✔(0.57)	✔(1.00)	❍(0.49)	✔(1.00)	❍(0.52)	✔(1.00)	❍(0.49)	✔(1.00)	✖(0.47)
✔(1.00)	❍(0.50)	✔(1.00)	✔(0.59)	✔(1.00)	❍(0.50)	✔(1.00)	✔(0.54)	✔(1.00)	❍(0.49)	✔(1.00)	✖(0.46)
✔(0.89)	❍(0.50)	✔(1.00)	✔(0.53)	✔(0.97)	❍(0.50)	✔(1.00)	❍(0.51)	✔(1.00)	❍(0.49)	✔(1.00)	✖(0.47)
✖(0.00)	❍(0.50)	✔(1.00)	❍(0.50)	❍(0.50)	❍(0.50)	✔(1.00)	❍(0.50)	✖(0.00)	❍(0.50)	✔(1.00)	✖(0.46)
✖(0.00)	❍(0.50)	✔(1.00)	❍(0.50)	✖(0.24)	❍(0.51)	✔(1.00)	❍(0.50)	✖(0.14)	✔(0.53)	✔(1.00)	❍(0.49)
✖(0.00)	❍(0.51)	✔(1.00)	❍(0.48)	✖(0.00)	✖(0.34)	✔(1.00)	✔(0.59)	✖(0.00)	❍(0.50)	✔(1.00)	✖(0.47)
✖(0.00)	✖(0.47)	✔(1.00)	✖(0.46)	✖(0.00)	✖(0.33)	✔(1.00)	✔(0.57)	✖(0.00)	❍(0.49)	✔(1.00)	❍(0.48)
✖(0.00)	❍(0.50)	✔(1.00)	❍(0.50)	✖(0.32)	✖(0.43)	✔(1.00)	✔(0.72)	✖(0.00)	❍(0.51)	✔(1.00)	❍(0.50)
✖(0.36)	❍(0.50)	✔(1.00)	❍(0.50)	✖(0.40)	✖(0.47)	✔(1.00)	✔(0.52)	✖(0.32)	❍(0.50)	✔(1.00)	❍(0.49)
❍(0.50)	❍(0.50)	✔(1.00)	❍(0.50)	❍(0.50)	❍(0.50)	✔(1.00)	❍(0.50)	✖(0.00)	❍(0.50)	✔(1.00)	❍(0.50)
❍(0.50)	❍(0.50)	✔(1.00)	❍(0.50)	❍(0.50)	❍(0.50)	✔(1.00)	❍(0.50)	❍(0.50)	❍(0.50)	✔(1.00)	❍(0.50)
❍(0.50)	❍(0.50)	❍(0.50)	❍(0.50)	❍(0.50)	❍(0.50)	❍(0.50)	❍(0.50)	❍(0.50)	❍(0.50)	❍(0.50)	❍(0.50)
❍(0.50)	❍(0.50)	✔(0.93)	❍(0.50)	❍(0.50)	❍(0.50)	✔(0.91)	❍(0.50)	✖(0.33)	❍(0.50)	✔(0.94)	❍(0.50)
✔(1.00)	❍(0.50)	✔(1.00)	❍(0.50)	✔(1.00)	✖(0.31)	✔(1.00)	❍(0.52)	✔(1.00)	❍(0.51)	✔(1.00)	✔(0.54)
✔(1.00)	❍(0.50)	✔(1.00)	❍(0.50)	✔(1.00)	❍(0.50)	✔(1.00)	❍(0.51)	✔(1.00)	✖(0.32)	✔(1.00)	✔(0.54)
✔(1.00)	❍(0.50)	✔(1.00)	❍(0.50)	✔(1.00)	❍(0.52)	✔(1.00)	❍(0.51)	✔(1.00)	✖(0.16)	✔(1.00)	✖(0.18)
✔(1.00)	❍(0.50)	✔(1.00)	❍(0.50)	✔(0.97)	✖(0.48)	✔(1.00)	❍(0.50)	✔(1.00)	✖(0.31)	✔(1.00)	✖(0.40)
All Java Programs	✔(0.58)	❍(0.50)	✔(0.98)	❍(0.50)	✔(0.58)	❍(0.50)	✔(0.96)	❍(0.50)	✔(0.59)	✖(0.49)	✔(0.97)	✖(0.49)
4.2.1. Prioritization techniques
Since our proposed CCCP technique takes advantage of the information about dynamic coverage and test cases as inputs, for a fair comparison, it is necessary to choose other RTCP techniques that use similar information to guide the test cases prioritization. In this study, we selected four such RTCP techniques: total test prioritization (Rothermel et al., 1999), additional test prioritization (Rothermel et al., 1999), adaptive random test prioritization (Jiang et al., 2009), and search-based test prioritization (Li et al., 2007). The total test prioritization technique prioritizes test cases based on the descending number of code units covered by those tests. The additional technique, in contrast, greedily chooses each element from candidates such that it covers the largest number of code units not yet covered by the previously selected tests. The adaptive random technique greedily selects each element from random candidates such that it has the greatest maximum distance from the already selected tests. Finally, the search-based technique considers all permutations as candidate solutions, and uses a meta-heuristic search algorithm to guide the search for a better test execution order — a genetic algorithm was adopted in this study, due to its effectiveness (Li et al., 2007). These four test prioritization techniques, whose details are presented in Table 1, have been widely used in RTCP previous studies (Luo et al., 2016, Luo et al., 2019, Lu et al., 2016).

4.2.2. Code coverage granularity
When using code coverage information to support RTCP, the coverage granularity can be considered a constituent part of the prioritization techniques. To enable sufficient evaluations, we used structural coverage criteria at the statement-, branch-, and method-coverage levels.

4.2.3. Test case granularity
For the subject programs written in Java, we considered an additional factor in the prioritization techniques, the test-case granularity. Test-case granularity is at either the test-class level, or the test-method level. For the test-class level, each JUnit test-case class was a test case; for the test-method level, each test method in a JUnit test-case class was a test case. In other words, a test case at the test-class level generally involves a number of test cases at the test-method level. For C subject programs, however, the actual program inputs were the test cases.

4.3. Dependent variables
Because we were examining the fault detection capability, we adopted the widely-used APFD (average percentage faults detected) as the evaluation metric for fault detection rates (Rothermel et al., 1999). Given a test suite , with  test cases. If 
 is a permutation of , in which 
 is the position of first test case that reveals the fault , then the APFD value for 
 is given by the following equation: (6)
 
 

Although APFD has often been used to evaluate RTCP techniques, it assumes that each test case incurs the same time cost, an assumption that may not hold up in practice. Elbaum et al. (2001), therefore, introduced an APFD variant, APFD, a cost-cognizant version of APFD that considers both the fault severity and the test case execution cost. Similar to APFD, APFD has also been applied to the evaluation of RTCP techniques, resulting in a more comprehensive evaluation (Epitropakis et al., 2015). APFD is defined as: (7)
 
 
where 
 are the severities of the  detected faults, 
 are the execution costs of  test cases, and 
 has the same meaning as in APFD. Because of the difficulty involved in estimating fault severity, following previous studies (Epitropakis et al., 2015), we assumed that all faults had the same severity level for this study. Accordingly, the definition of APFD can be described as: (8)
 
 
Intuitively speaking, prioritized test suites that both find faults faster and require less execution time, will have higher APFD values.

4.4. Subject programs, test suites and faults
We conducted our study on 14 versions of four Java programs (three versions of ; five versions of ; three versions of ; and three versions of ) downloaded from the Software-artifact Infrastructure Repository (SIR) (Do et al., 2005, SIR project, 2017), and 30 versions of five real-life Unix utility programs, from the GNU FTP server (Free Software Foundation, 2017). Both the Java and C programs have been widely used as benchmarks to evaluate RTCP problems (Jiang et al., 2009, Zhang et al., 2013a, Henard et al., 2016, Eghbali and Tahvildari, 2016). Table 2 summarizes the subject program details, with Columns 3 to 7 presenting the version, size, number of branches, number of methods, and number of classes (including interfaces), respectively.

Each version of the Java programs has a JUnit test suite that was developed during the program’s evolution. These test suites have two levels of test-case granularity: the test-class and the test-method. The numbers of JUnit test cases (including both test-class and test-method levels) are shown in the #Test_Case column, as #T_ Class and #T_Method: The data is presented as , where  is the total number of test cases; and  is the number of test cases that can be successfully executed. The test suites for the C programs are available from the SIR (Do et al., 2005, SIR project, 2017): The number of tests cases in each suite is also shown in the #Test_Case column of Table 2.

Because the seeded-in SIR faults were easily detected and small in size, for both C and Java programs, we used mutation faults to evaluate the performance of the different techniques. Mutation faults have previously been identified as suitable for simulating real program faults (Andrews et al., 2005, Do and Rothermel, 2005, Just et al., 2014), and have been widely applied to regression test prioritization evaluations (Rothermel et al., 1999, Zhang et al., 2013a, Luo et al., 2016, Luo et al., 2019, Henard et al., 2016, Lu et al., 2016, Elbaum et al., 2000). Eleven mutation operators from the “NEW_DEFAULTS” group of the PIT mutation tool (Matt Kirk, 2020a) were used to generate mutants for the Java programs. These operators, whose detailed descriptions can be found on the PIT website (Matt Kirk, 2020b), were: conditionals boundary, increments, invert negatives, math, negate conditionals, void method calls, empty returns, false returns, null returns, primitive returns, and true returns. We downloaded the mutants from previous RTCP studies (Henard et al., 2016) for the C programs, which had been generated using seven mutation operators from Andrews et al. (2006): statement deletion, unary insertion, constant replacement, arithmetic operator replacement, logical operator replacement, bitwise logical operator replacement, and relational operator replacement.

Equivalent mutants (Schuler et al., 2009, Papadakis et al., 2015) are functionally equivalent versions of the original program, and thus cannot be killed: no test case applied to both the mutant and the original program could result in different behavior. In our study, therefore, all equivalent mutants were removed, leaving only those mutants that could be detected by at least one test case. In Table 2, the #Mutant column gives the total number of all mutants (#All), and the (#Detected) column gives the number of detected mutants. Although all detected mutants were considered in our study, some mutants, called duplicated mutants (Papadakis et al., 2015), were equivalent to other mutants (but not to the original program). Similarly, some mutants, called subsumed mutants (Just et al., 2012, Kaminski et al., 2013) were subsumed by others: If a subsuming mutant (Papadakis et al., 2016) is killed, then its subsumed mutants are also killed. We used the Subsuming Mutants Identification (SMI) algorithm (Papadakis et al., 2016) to remove the duplicate and subsuming mutants in our fault set. SMI first removed duplicate mutants, and then greedily identified the most subsuming mutants — those mutants which, when killed, result in the highest number of other mutants being “collaterally” killed. The #Subsuming_Mutant column gives the number of subsuming mutants used in our study (the subsuming faults are classified as either test-class level (#SM_Class) or test-method level (#SM_Method) for the Java programs).


Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 2. Effectiveness: APFD results for Java programs at the test-class level.

4.5. Experimental setup
The experiments were conducted on a Linux 4.4.0-170-generic server with two Intel(R) Xeon(R) Platinum 8163 CPUs (2.50 GHz, two cores) and 16 GBs of RAM.

The Java programs were compiled using Java 1.8 (Oracle, 2020). The coverage information for each program version was obtained using the FaultTracer tool (Zhang et al., 2012, Zhang et al., 2013b), which, based on the ASM bytecode manipulation and analysis framework (Zhang, 2020), uses on-the-fly bytecode instrumentation without any modification of the target program.

There were six versions of each C program: 
, 
, 
, 
, 
, and 
. Version 
 was compiled using gcc 5.4.0 (GCC team, 2019), and then the coverage information was obtained using the gcov tool (Free Software Foundation, 2019), which is one of the gcc utilities.

After collecting the code coverage information, we implemented all RTCP techniques in Java, and applied them to each program version under study, for all coverage criteria. Because the approaches contain randomness, each execution was repeated 1000 times. This resulted in, for each testing scenario, 1000 APFD or APFD
 values (1000 orderings) for each RTCP approach. To test whether there was a statistically significant difference between CCCP and the other RTCP approaches, we performed the unpaired two-tailed Wilcoxon–Mann–Whitney test, at a significance level of , following previously reported guidelines for inferential statistical analyses involving randomized algorithms (Arcuri and Briand, 2014, Gligoric et al., 2015b). To identify which approach was better, we also calculated the effect size, measured by the non-parametric Vargha and Delaney effect size measure (Vargha and Delaney, 2000), 
, where 
 gives probability that the technique  is better than technique . The statistical analyses were performed using R (R Development Core Team).


Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 3. Effectiveness: APFD results for Java programs at the test-method level.


Table 4. Statistical effectiveness comparisons of APFD for Java programs at the test-method level. For a comparison between two methods 
 and , where 
, the symbol ✔ means that 
 is better (-value is less than 0.05, and the effect size 
 is greater than 0.50); the symbol ✖ means that  is better (the -value is less than 0.05, and 
 is less than 0.50); and the symbol ❍ means that there is no statistically significant difference between them (the -value is greater than 0.05).

Program name	Statement coverage	Branch coverage	Method coverage
✔(1.00)	✔(0.53)	✔(1.00)	✔(0.88)	✔(1.00)	❍(0.48)	✔(1.00)	✔(0.91)	✔(1.00)	❍(0.50)	✔(0.98)	✔(0.72)
✔(1.00)	❍(0.51)	✔(1.00)	✔(1.00)	✔(1.00)	✖(0.46)	✔(1.00)	✔(1.00)	✔(1.00)	❍(0.50)	✔(1.00)	✔(0.90)
✔(1.00)	❍(0.50)	✔(1.00)	✔(1.00)	✔(1.00)	❍(0.51)	✔(1.00)	✔(1.00)	✔(1.00)	❍(0.51)	✔(1.00)	✔(0.90)
✔(1.00)	❍(0.51)	✔(1.00)	✔(0.98)	✔(1.00)	❍(0.49)	✔(1.00)	✔(0.97)	✔(1.00)	❍(0.50)	✔(1.00)	✔(0.84)
✔(1.00)	❍(0.50)	✔(1.00)	✔(0.62)	✔(1.00)	✖(0.30)	✔(1.00)	✔(0.84)	✔(0.56)	❍(0.50)	✔(0.98)	✖(0.47)
✔(1.00)	✔(0.96)	✔(1.00)	✔(0.68)	✔(1.00)	❍(0.49)	✔(1.00)	✔(0.82)	✔(0.90)	❍(0.51)	✔(0.98)	❍(0.48)
✔(1.00)	✔(0.55)	✔(1.00)	✔(0.82)	✔(1.00)	❍(0.48)	✔(1.00)	✔(0.64)	✔(1.00)	✖(0.26)	✔(1.00)	✔(0.55)
✔(1.00)	✔(0.60)	✔(1.00)	✔(0.84)	✔(1.00)	❍(0.49)	✔(1.00)	✔(0.64)	✔(1.00)	✖(0.25)	✔(1.00)	✔(0.56)
✔(1.00)	❍(0.51)	✔(1.00)	✔(0.89)	✔(1.00)	✖(0.45)	✔(1.00)	✔(0.76)	✔(1.00)	✖(0.32)	✔(1.00)	✔(0.64)
✔(1.00)	✔(0.54)	✔(1.00)	✔(0.63)	✔(1.00)	✖(0.48)	✔(1.00)	✔(0.68)	✔(0.86)	✖(0.43)	✔(0.98)	✔(0.52)
✔(1.00)	✔(0.98)	✖(0.00)	✔(0.77)	✔(1.00)	✖(0.46)	✖(0.00)	✖(0.43)	✔(1.00)	❍(0.49)	✔(0.73)	❍(0.50)
✔(1.00)	✔(0.79)	✖(0.00)	✔(0.53)	✔(1.00)	✔(0.89)	✖(0.00)	✔(0.70)	✔(1.00)	❍(0.50)	✖(0.27)	✖(0.35)
✔(1.00)	❍(0.50)	✖(0.00)	✖(0.16)	✔(1.00)	❍(0.49)	✖(0.00)	✖(0.31)	✔(1.00)	❍(0.49)	✔(0.67)	✔(0.62)
✔(1.00)	✔(0.59)	✖(0.00)	❍(0.51)	✔(1.00)	✔(0.54)	✖(0.00)	✖(0.47)	✔(1.00)	❍(0.50)	✔(0.55)	✖(0.48)
✔(1.00)	❍(0.49)	✔(1.00)	✖(0.37)	✔(1.00)	❍(0.49)	✔(1.00)	✔(0.78)	✔(1.00)	✔(0.53)	✔(1.00)	✔(0.58)
✔(1.00)	❍(0.49)	✔(1.00)	❍(0.50)	✔(1.00)	❍(0.50)	✔(1.00)	✔(0.77)	✔(1.00)	❍(0.51)	✔(1.00)	✔(0.60)
✔(1.00)	✖(0.46)	✔(1.00)	❍(0.52)	✔(1.00)	✖(0.30)	✔(1.00)	✔(0.63)	✔(1.00)	❍(0.51)	✔(1.00)	✔(0.56)
✔(1.00)	❍(0.49)	✔(1.00)	✖(0.47)	✔(1.00)	✖(0.43)	✔(1.00)	✔(0.72)	✔(1.00)	❍(0.51)	✔(1.00)	✔(0.56)
All Java Programs	✔(0.97)	✔(0.51)	✔(0.70)	✔(0.57)	✔(0.90)	✖(0.49)	✔(0.59)	✔(0.59)	✔(0.96)	✖(0.49)	✔(0.76)	✔(0.54)

Download : Download high-res image (2MB)
Download : Download full-size image
Fig. 4. Effectiveness: APFD results for C programs.


Table 5. Statistical effectiveness comparisons of APFD for C programs. For a comparison between two methods 
 and , where 
, the symbol ✔ means that 
 is better (-value is less than 0.05, and the effect size 
 is greater than 0.50); the symbol ✖ means that  is better (the -value is less than 0.05, and 
 is less than 0.50); and the symbol ❍ means that there is no statistically significant difference between them (the -value is greater than 0.05).

Program name	Statement coverage	Branch coverage	Method coverage
✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(0.83)	✔(0.96)
✔(1.00)	✔(0.98)	✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(0.78)	✔(0.93)
✔(1.00)	✔(0.74)	✔(1.00)	✔(0.88)	✔(1.00)	✔(0.70)	✔(1.00)	✔(0.94)	✔(1.00)	✔(0.99)	✔(0.99)	✔(0.96)
✔(1.00)	✔(0.98)	✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(0.77)	✔(0.92)
✔(1.00)	✔(0.98)	✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(0.77)	✔(0.92)
✔(1.00)	✔(0.89)	✔(1.00)	✔(0.94)	✔(1.00)	✔(0.86)	✔(1.00)	✔(0.95)	✔(1.00)	✔(0.86)	✔(0.80)	✔(0.88)
✔(1.00)	✔(0.94)	❍(0.49)	✔(0.70)	✔(1.00)	✔(0.75)	✖(0.23)	✔(0.57)	✔(1.00)	✔(0.86)	✖(0.10)	✖(0.15)
✔(1.00)	✔(0.89)	✔(0.55)	✔(0.82)	✔(1.00)	✔(0.70)	✖(0.21)	✔(0.70)	✔(1.00)	✔(0.96)	✖(0.14)	✖(0.28)
✔(1.00)	✔(0.82)	✔(0.56)	✖(0.44)	✔(1.00)	✔(0.72)	✖(0.45)	✖(0.43)	✔(1.00)	✔(0.86)	✖(0.20)	✖(0.27)
✔(1.00)	✔(0.79)	✔(0.81)	✔(0.90)	✔(1.00)	✔(0.61)	✔(0.89)	✔(0.91)	✔(1.00)	✔(0.84)	✖(0.21)	✖(0.25)
✔(1.00)	✔(0.65)	✔(0.91)	✔(0.95)	✔(1.00)	✔(0.54)	✔(0.93)	✔(0.93)	✔(1.00)	✔(0.63)	✖(0.18)	✖(0.21)
✔(1.00)	✔(0.81)	✔(0.65)	✔(0.75)	✔(1.00)	✔(0.64)	✔(0.56)	✔(0.71)	✔(1.00)	✔(0.74)	✖(0.25)	✖(0.30)
✔(1.00)	✔(1.00)	✔(0.85)	✔(0.89)	✔(1.00)	✔(1.00)	✔(0.89)	✔(0.90)	✔(0.88)	✔(0.88)	❍(0.48)	❍(0.48)
✔(1.00)	✔(1.00)	✔(0.82)	✔(0.86)	✔(1.00)	✔(1.00)	✔(0.87)	✔(0.88)	✔(0.88)	✔(0.88)	❍(0.48)	❍(0.49)
✔(1.00)	✔(0.80)	✔(0.60)	✖(0.39)	✔(1.00)	✔(0.95)	✔(0.53)	✖(0.36)	✖(0.46)	✖(0.46)	❍(0.50)	❍(0.49)
✔(1.00)	✔(0.80)	✔(0.60)	✖(0.39)	✔(1.00)	✔(0.95)	✔(0.53)	✖(0.36)	✖(0.46)	✖(0.46)	❍(0.50)	❍(0.49)
✔(1.00)	✔(0.80)	✔(0.60)	✖(0.39)	✔(1.00)	✔(0.95)	✔(0.53)	✖(0.36)	✖(0.46)	✖(0.46)	❍(0.50)	❍(0.49)
✔(1.00)	✔(0.79)	✔(0.62)	✔(0.52)	✔(1.00)	✔(0.95)	✔(0.61)	✔(0.52)	✔(0.55)	✔(0.56)	❍(0.50)	❍(0.49)
✔(0.92)	✔(0.56)	✖(0.31)	✖(0.30)	✔(1.00)	✔(0.64)	✖(0.36)	✖(0.47)	✔(0.92)	✔(0.97)	✔(0.64)	✔(0.66)
✔(1.00)	✔(0.81)	✖(0.40)	✖(0.40)	✔(1.00)	✔(0.61)	✖(0.35)	❍(0.52)	✔(0.99)	✔(0.99)	✔(0.62)	✔(0.65)
✔(0.99)	✔(0.61)	✖(0.29)	✖(0.29)	✔(1.00)	✔(0.62)	✖(0.36)	✔(0.56)	✔(0.99)	✔(1.00)	✔(0.66)	✔(0.68)
✔(0.89)	✔(0.60)	✖(0.26)	✖(0.26)	✔(1.00)	✔(0.58)	✖(0.32)	❍(0.51)	✔(0.99)	✔(1.00)	✔(0.68)	✔(0.69)
✔(1.00)	✔(0.83)	✖(0.38)	✖(0.39)	✔(1.00)	✔(0.63)	✖(0.31)	❍(0.50)	✔(0.99)	✔(0.99)	✔(0.62)	✔(0.65)
✔(0.92)	✔(0.67)	✖(0.34)	✖(0.34)	✔(1.00)	✔(0.60)	✖(0.36)	❍(0.51)	✔(0.93)	✔(0.96)	✔(0.63)	✔(0.65)
✔(1.00)	✔(0.83)	✔(0.62)	✔(0.61)	✔(1.00)	✔(0.77)	✔(0.67)	✔(0.67)	✔(1.00)	✔(1.00)	✔(0.63)	✔(0.63)
✔(1.00)	✔(0.82)	✖(0.23)	✔(0.71)	✔(1.00)	✔(0.86)	✖(0.11)	✔(0.63)	✔(1.00)	✔(1.00)	✖(0.15)	✖(0.42)
✔(1.00)	✔(0.82)	✖(0.23)	✔(0.71)	✔(1.00)	✔(0.86)	✖(0.11)	✔(0.63)	✔(1.00)	✔(1.00)	✖(0.15)	✖(0.42)
✔(1.00)	✔(0.86)	✖(0.30)	❍(0.51)	✔(1.00)	✔(0.73)	✖(0.15)	✖(0.35)	✔(1.00)	✔(0.64)	✖(0.10)	✖(0.13)
✔(1.00)	✔(0.96)	✖(0.09)	❍(0.49)	✔(1.00)	✔(0.99)	✖(0.14)	✔(0.56)	✔(1.00)	✔(0.57)	✖(0.08)	✖(0.10)
✔(1.00)	✔(0.78)	✖(0.33)	✔(0.60)	✔(1.00)	✔(0.78)	✖(0.28)	✔(0.57)	✔(1.00)	✔(0.89)	✖(0.25)	✖(0.35)
All C Programs	✔(0.93)	✔(0.64)	✔(0.54)	✔(0.56)	✔(0.95)	✔(0.65)	✔(0.53)	✔(0.56)	✔(0.84)	✔(0.70)	✖(0.48)	✔(0.52)

Table 6. An analysis of statistical effectivenessresults of APFD. Each cell represents the total times of (✔), worse (✖), and (❍) for corresponding prioritization scenarios described in Table 3, Table 5 .

Language	Status	Statement coverage	Branch coverage	Method coverage	Sum 
Java (test-class)	✔	5	0	13	2	6	0	13	4	6	1	13	2	17	1	39	8
✖	6	1	0	1	4	4	0	0	6	2	0	6	16	7	0	7
❍	3	13	1	11	4	10	1	10	2	11	1	6	9	34	3	27
Java (test-method)	✔	14	6	11	10	14	1	11	12	14	1	13	10	42	8	35	32
✖	0	1	3	2	0	5	3	2	0	3	1	2	0	9	7	6
❍	0	7	0	2	0	8	0	0	0	10	0	2	0	25	0	4
C	✔	25	25	15	14	25	25	13	16	22	22	11	11	72	72	39	41
✖	0	0	9	9	0	0	12	6	3	3	9	9	3	3	30	24
❍	0	0	1	2	0	0	0	3	0	0	5	5	0	0	6	10
Sum 	✔	44	31	39	26	45	26	37	32	42	24	37	23	131	81	113	81
✖	6	2	12	12	4	9	15	13	9	8	10	17	19	19	37	42
❍	3	20	2	15	4	18	1	8	2	21	6	13	9	59	9	36

Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 5. Effectiveness:APFD
 results for Java programs at the test-class level.

5. Results and analysis
This section presents the experimental results to answer the research questions.

To answer RQ1 to RQ4, Fig. 2, Fig. 3, Fig. 4, Fig. 5, Fig. 6, Fig. 7, Fig. 8 present box plots of the distribution of the APFD or APFD
 values (averaged over 1000 iterations). Each box plot shows the mean (square in the box), median (line in the box), and upper and lower quartiles (25th and 75th percentile) for the APFD or APFD
 values for the RTCP techniques. Statistical analyses are also provided in Table 3, Table 4, Table 5, Table 6, Table 7, Table 8, Table 9, Table 10, Table 11 for each pairwise APFD or APFD
 comparison between CCCP and the other RTCP techniques. For example, for a comparison between two methods 
 and , where 
, the symbol ✔ means that 
 is better (-value is less than 0.05, and the effect size 
 is greater than 0.50); the symbol ✖ means that  is better (the -value is less than 0.05, and 
 is less than 0.50); and the symbol ❍ means that there is no statistically significant difference between them (the -value is greater than 0.05).

To answer RQ5, Table 12 provides comparisons of the execution times for the different RTCP techniques. To answer RQ6, Fig. 9 shows the APFD and APFD
 results for CCCP with , and Table 13 presents the corresponding statistical analysis.

5.1. RQ1: Effectiveness of CCCP measured by APFD
Here, we provide the APFD results for CCCP for different code coverage and test case granularities. Fig. 2, Fig. 3, Fig. 4 show the APFD results for the Java programs at the test-class level; at the test-method level; and the C programs, respectively. Each sub-figure in these figures has the program versions across the -axis, and the APFD values for the five RTCP techniques on the -axis. Table 3, Table 4, Table 5 present the corresponding statistical comparisons.

5.1.1. APFD results: Java programs (test-class level)
Based on Fig. 2 and Table 3, we have the following observations:

(1) Compared with the total test prioritization technique, CCCP achieves better performances for the program , irrespective of code coverage granularity, with differences between the mean and median APFD values reaching about 3%. For the other programs (, , and ), however, they have very similar APFD results.

(2) CCCP performs much better than adaptive random test prioritization, regardless of subject program and code coverage granularity, with the maximum mean and median APFD differences reaching about 12%.

(3) CCCP has very similar performance to the additional and search-based test prioritization techniques, with the mean and median APFD differences approximately equal to 1%.

(4) There is a statistically significant difference between 
 and 
, which supports the above observations. However, none of the other three techniques (
, 
, or 
) is either always better or always worse than 
, with 
 sometimes performing better for some programs, and sometimes worse.

(5) Considering all Java programs: Overall, because all -values are less than 0.05, and the relevant effect size 
 ranges from 0.58 to 0.98, 
 performs better than 
 and 
. However, CCCP has very similar (or slightly worse) performance to 
 and 
, with 
 values of either 0.49 or 0.50.

5.1.2. APFD results: Java programs (test-method level)
Based on Fig. 3 and Table 4, we have the following observations:

(1) Our proposed method achieves much higher mean and median APFD values than 
 for all programs with all code coverage granularities, with the maximum differences reaching approximately 30%.

(2) CCCP has very similar performance to 
, with their mean and median APFD differences at around 1%.

(3) Other than for some versions of , CCCP has much better performance than 
.

(4) Other than for a few cases (such as  with method coverage, and  with statement coverage), CCCP usually has better performance than 
.

(5) Overall, the statistical analysis supports the above box plots observations. Looking at all Java programs together, CCCP is, on the whole, better than 
, 
, and 
: The -values are all less than 0.05, indicating that their differences are significant; and the effect size 
 values range from 0.54 to 0.97, which means that 
 is better than the other three RTCP techniques. Finally, while the -values for comparisons between 
 and 
 are less than 0.05 (which means that the differences are significant), the 
 values range from 0.49 to 0.51, indicating that they are very similar.

5.1.3. APFD results: C subject programs
Based on Fig. 4 and Table 5, we have the following observations:

(1) Our proposed CCCP approach has much better performance than 
 and 
, for all programs and code coverage granularities, except for  with method coverage (for which 
 has very similar, or better performance). The maximum difference in mean and median APFD values between 
 and 
 is more than 40%; while between 
 and 
, it is about 10%.

(2) 
 has similar or better APFD performance than 
 and 
 for some subject programs (such as  and , with all code coverage granularities), but also has slightly worse performance for some others (such as  with method coverage and  with statement coverage). However, the difference in mean and median APFD values between 
 and 
 or 
 is less than 5%.

(3) Overall, the statistical results support the box plot observations. All  values for the comparisons between 
 and 
 or 
 are less than 0.05, indicating that their APFD scores are significantly different. The 
 values are also much greater than 0.50, ranging from 0.56 to 1.00 (except for the programs , , and , with method coverage). However, although all  values for the comparisons between 
 and 
 or 
 are also less than 0.05, their 
 values are much greater than 0.50 in some cases, but also much less than 0.50 in others. Nevertheless, considering all the C programs, not only does 
 have significantly different APFD values to the other four RTCP techniques, but it also has better performances overall (except for 
 with method coverage).

Table 6 summarizes the statistical results, presenting the total number of times 
 is better (✔), worse (✖), or not statistically different (❍), compared to the other RTCP techniques. Based on this table, we can answer RQ1 as follows:


Table 7. Statistical effectiveness comparisons of APFD
 for Java programs at the test-class level. For a comparison between two methods 
 and , where 
, the symbol ✔ means that 
 is better (-value is less than 0.05, and the effect size 
 is greater than 0.50); the symbol ✖ means that  is better (the -value is less than 0.05, and 
 is less than 0.50); and the symbol ❍ means that there is no statistically significant difference between them (the -value is greater than 0.05) .

Program name	Statement coverage	Branch coverage	Method coverage
✔(1.00)	✔(1.00)	✔(1.00)	✔(0.97)	✔(1.00)	✔(0.59)	✔(1.00)	✔(0.58)	✔(1.00)	✔(0.59)	✔(1.00)	✔(0.56)
✔(1.00)	✔(0.75)	✔(0.59)	✔(0.70)	✔(1.00)	✔(1.00)	✖(0.21)	✔(0.96)	✔(1.00)	❍(0.50)	✖(0.32)	✔(0.53)
✔(1.00)	✔(0.65)	✔(0.98)	✔(0.66)	✔(1.00)	✔(1.00)	✖(0.45)	✔(0.85)	✔(1.00)	❍(0.50)	✖(0.44)	✔(0.53)
✔(1.00)	✔(0.60)	✔(0.85)	✔(0.59)	✔(0.83)	✔(0.62)	✔(0.61)	✔(0.60)	✔(0.98)	❍(0.51)	✔(0.59)	✔(0.52)
✖(0.00)	✖(0.00)	✔(1.00)	✖(0.00)	✖(0.00)	✔(0.56)	✔(0.96)	❍(0.52)	✖(0.00)	✔(0.67)	✔(1.00)	✔(0.64)
✖(0.00)	✖(0.14)	✔(0.99)	✖(0.12)	✖(0.00)	❍(0.49)	✔(0.86)	✖(0.47)	✖(0.00)	✔(1.00)	✔(1.00)	✔(1.00)
✖(0.00)	✔(0.66)	✔(0.69)	✔(0.67)	✖(0.00)	✔(0.54)	✔(0.86)	❍(0.48)	✖(0.00)	✖(0.47)	✔(0.63)	✖(0.46)
❍(0.49)	✔(0.73)	✔(0.78)	✔(0.75)	✖(0.00)	✔(0.53)	✔(0.81)	✖(0.46)	✖(0.00)	✔(0.70)	✔(0.58)	✔(0.72)
✔(1.00)	✔(0.61)	✔(0.92)	✔(0.64)	✔(0.92)	✔(0.62)	✔(0.92)	✔(0.55)	✔(1.00)	✔(0.57)	✔(0.98)	✔(0.62)
✖(0.33)	❍(0.49)	✔(0.90)	❍(0.50)	✖(0.18)	❍(0.51)	✔(0.86)	❍(0.50)	✖(0.36)	✔(0.54)	✔(0.84)	✔(0.54)
✔(1.00)	✔(1.00)	✔(0.69)	✔(1.00)	✔(1.00)	✔(1.00)	✔(0.62)	✔(1.00)	✔(1.00)	✔(0.74)	✔(1.00)	✔(0.74)
✔(1.00)	✔(1.00)	❍(0.49)	✔(1.00)	✔(1.00)	✔(1.00)	✖(0.44)	✔(1.00)	✔(1.00)	✔(0.76)	✔(0.88)	✔(0.74)
✔(1.00)	❍(0.50)	✖(0.12)	❍(0.50)	✔(1.00)	❍(0.50)	✖(0.10)	❍(0.50)	✔(1.00)	❍(0.50)	✖(0.40)	❍(0.49)
✔(0.78)	✔(0.61)	❍(0.50)	✔(0.61)	✔(0.78)	✔(0.61)	✖(0.47)	✔(0.61)	✔(0.87)	✔(0.56)	✔(0.73)	✔(0.55)
✖(0.00)	❍(0.50)	✔(0.87)	❍(0.50)	✖(0.00)	✖(0.36)	✔(0.72)	❍(0.52)	✖(0.00)	✖(0.00)	✔(0.76)	✖(0.00)
✖(0.00)	❍(0.50)	✖(0.22)	❍(0.50)	✔(1.00)	✔(0.75)	✔(0.64)	✔(0.81)	✖(0.00)	✔(0.66)	✖(0.16)	✔(0.76)
✖(0.00)	✖(0.00)	✖(0.27)	✖(0.00)	✖(0.00)	✖(0.27)	✖(0.28)	✖(0.27)	✖(0.00)	✖(0.20)	✖(0.19)	✖(0.20)
✖(0.22)	✖(0.44)	✖(0.48)	✖(0.44)	✖(0.28)	❍(0.49)	✔(0.54)	❍(0.51)	✖(0.00)	✖(0.41)	✖(0.35)	✖(0.44)
All Java Programs	✔(0.58)	✔(0.52)	✔(0.68)	✔(0.53)	✔(0.53)	✔(0.52)	✔(0.65)	✔(0.52)	✔(0.58)	❍(0.50)	✔(0.67)	✔(0.51)
1.
When prioritizing Java test suites at the test-class level, 
 performs much better than 
; similarly to 
 and 
; and slightly worse than 
.

2.
When prioritizing Java test suites at the test-method level, 
 performs much better than 
, 
, and 
; and similarly to 
.

3.
When prioritizing C test suites, 
 performs much better than 
, 
, and 
; and slightly better than 
.

In other words, as will be discussed in detail later (Sections 5.3 RQ3: Impact of code coverage granularity, 5.4 RQ4: Impact of test case granularity), code coverage granularity and test case granularity may impact on the effectiveness of CCCP, in terms of APFD. Nevertheless, the ratios of 
 performing better (✔) rather than worse (✖) than 
, 
, 
, and 
, are: 6.89 (131/19), 4.26 (81/19), 3.05 (113/37), and 1.93 (81/42), respectively. In conclusion, overall, the proposed CCCP approach is more effective than the other four RTCP techniques, in terms of testing effectiveness, as measured by APFD.

5.2. RQ2: Effectiveness of CCCP measured by APFD
Next, we provide the APFD
 results for CCCP for different code coverage and test case granularities. Fig. 5, Fig. 6, Fig. 7 show the APFD
 results for the Java programs at the test-class level; at the test-method level; and the C programs, respectively. Each sub-figure in these figures has the program versions across the -axis, and the APFD
 values for the five RTCP techniques on the -axis. Table 7, Table 8, Table 9 present the corresponding statistical comparisons.


Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 6. Effectiveness:APFD
 results for Java programs at the test-method level.


Table 8. Statistical effectiveness comparisons of APFD
 for Java programs at the test-method level. For a comparison between two methods 
 and , where 
, the symbol ✔ means that 
 is better (-value is less than 0.05, and the effect size 
 is greater than 0.50); the symbol ✖ means that  is better (the -value is less than 0.05, and 
 is less than 0.50); and the symbol ❍ means that there is no statistically significant difference between them (the -value is greater than 0.05) .

Program name	Statement coverage	Branch coverage	Method coverage
✖(0.00)	✔(0.54)	✔(1.00)	✖(0.45)	✖(0.00)	❍(0.50)	✔(1.00)	✔(0.68)	✖(0.00)	✖(0.42)	✔(1.00)	❍(0.48)
✔(0.99)	❍(0.50)	✔(1.00)	✔(0.56)	✔(1.00)	❍(0.52)	✔(1.00)	✔(0.87)	✖(0.44)	❍(0.52)	✔(0.99)	✔(0.64)
✖(0.00)	❍(0.49)	✔(1.00)	✔(0.63)	✔(1.00)	❍(0.48)	✔(1.00)	✔(0.77)	✖(0.17)	❍(0.47)	✔(0.99)	✔(0.64)
✖(0.40)	❍(0.51)	✔(1.00)	✔(0.54)	✔(0.63)	❍(0.50)	✔(1.00)	✔(0.76)	✖(0.24)	✖(0.47)	✔(0.99)	✔(0.59)
✔(1.00)	✔(0.53)	✔(1.00)	✔(0.63)	✔(1.00)	❍(0.50)	✔(0.93)	✔(0.80)	✔(0.76)	❍(0.50)	✔(0.63)	✔(0.53)
✔(1.00)	✔(0.53)	✔(1.00)	✔(0.65)	✔(1.00)	❍(0.49)	✔(0.90)	✔(0.82)	✔(1.00)	❍(0.50)	✔(0.88)	✔(0.56)
✔(1.00)	❍(0.52)	✔(1.00)	✖(0.47)	✔(1.00)	✖(0.44)	✔(0.99)	✔(0.66)	✔(1.00)	✖(0.45)	✔(1.00)	✖(0.47)
✔(1.00)	❍(0.51)	✔(1.00)	❍(0.48)	✔(1.00)	❍(0.49)	✔(1.00)	✔(0.69)	✔(1.00)	✖(0.41)	✔(1.00)	✖(0.44)
✔(1.00)	❍(0.50)	✔(1.00)	✔(0.61)	✔(1.00)	✖(0.30)	✔(1.00)	✔(0.77)	✔(1.00)	❍(0.48)	✔(0.86)	✖(0.41)
✔(1.00)	❍(0.51)	✔(1.00)	✔(0.57)	✔(1.00)	✖(0.46)	✔(0.97)	✔(0.69)	✔(0.95)	✖(0.48)	✔(0.85)	❍(0.49)
✔(1.00)	✔(1.00)	✖(0.00)	✔(0.81)	✔(1.00)	✖(0.37)	✖(0.00)	✖(0.36)	✔(1.00)	✔(0.53)	✖(0.31)	✖(0.47)
✔(1.00)	✔(0.95)	✖(0.00)	❍(0.51)	✔(1.00)	✔(0.64)	✖(0.00)	❍(0.50)	✔(1.00)	✔(0.53)	✖(0.04)	✖(0.33)
✔(1.00)	❍(0.51)	✖(0.00)	✖(0.18)	✔(1.00)	❍(0.50)	✖(0.00)	✖(0.28)	✔(1.00)	❍(0.49)	✖(0.05)	❍(0.51)
✔(1.00)	✔(0.61)	✖(0.00)	❍(0.50)	✔(1.00)	❍(0.50)	✖(0.00)	✖(0.42)	✔(1.00)	✔(0.52)	✖(0.14)	✖(0.43)
✔(1.00)	✖(0.27)	✖(0.38)	✖(0.38)	✔(1.00)	✔(0.53)	✖(0.36)	❍(0.52)	✔(1.00)	✔(0.53)	✔(0.69)	✔(0.56)
✔(1.00)	❍(0.50)	✔(0.83)	✔(0.67)	✔(1.00)	✔(0.57)	✔(0.56)	✔(0.78)	✔(1.00)	❍(0.49)	✖(0.46)	✔(0.54)
✔(1.00)	❍(0.50)	✖(0.12)	✔(0.61)	✔(1.00)	✖(0.45)	✖(0.01)	✖(0.14)	✔(1.00)	❍(0.50)	❍(0.48)	✔(0.55)
✔(1.00)	✖(0.47)	✖(0.46)	✔(0.54)	✔(0.83)	❍(0.51)	✖(0.34)	✔(0.53)	✔(1.00)	❍(0.50)	✔(0.53)	✔(0.54)
All Java Programs	✔(0.85)	❍(0.51)	✔(0.64)	✔(0.54)	✔(0.77)	❍(0.50)	✔(0.62)	✔(0.57)	✔(0.87)	✖(0.49)	✔(0.67)	✔(0.51)

Table 9. Statistical effectiveness comparisons of APFD
 for C programs. For a comparison between two methods 
 and , where 
, the symbol ✔ means that 
 is better (-value is less than 0.05, and the effect size 
 is greater than 0.50); the symbol ✖ means that  is better (the -value is less than 0.05, and 
 is less than 0.50); and the symbol ❍ means that there is no statistically significant difference between them (the -value is greater than 0.05) .

Program name	Statement coverage	Branch coverage	Method coverage
✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(0.82)	✔(0.96)
✔(1.00)	✔(0.98)	✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(0.76)	✔(0.93)
✔(1.00)	✔(0.75)	✔(1.00)	✔(0.88)	✔(1.00)	✔(0.71)	✔(1.00)	✔(0.94)	✔(1.00)	✔(0.99)	✔(0.99)	✔(0.96)
✔(1.00)	✔(0.99)	✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(0.75)	✔(0.92)
✔(1.00)	✔(0.98)	✔(1.00)	✔(0.99)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(1.00)	✔(0.75)	✔(0.92)
✔(1.00)	✔(0.89)	✔(1.00)	✔(0.94)	✔(1.00)	✔(0.86)	✔(1.00)	✔(0.95)	✔(1.00)	✔(0.86)	✔(0.79)	✔(0.87)
✔(1.00)	✔(0.97)	✖(0.34)	✔(0.57)	✔(1.00)	✔(0.84)	✖(0.17)	❍(0.49)	✔(1.00)	✔(0.79)	✖(0.06)	✖(0.08)
✔(1.00)	✔(0.93)	✖(0.39)	✔(0.71)	✔(1.00)	✔(0.80)	✖(0.14)	✔(0.61)	✔(1.00)	✔(0.92)	✖(0.09)	✖(0.18)
✔(1.00)	✔(0.90)	✖(0.42)	✖(0.33)	✔(1.00)	✔(0.82)	✖(0.37)	✖(0.36)	✔(1.00)	✔(0.79)	✖(0.13)	✖(0.17)
✔(1.00)	✔(0.87)	✔(0.70)	✔(0.82)	✔(1.00)	✔(0.73)	✔(0.85)	✔(0.88)	✔(1.00)	✔(0.75)	✖(0.14)	✖(0.16)
✔(1.00)	✔(0.75)	✔(0.84)	✔(0.91)	✔(1.00)	✔(0.67)	✔(0.90)	✔(0.91)	✔(1.00)	❍(0.51)	✖(0.13)	✖(0.13)
✔(1.00)	✔(0.88)	✔(0.54)	✔(0.66)	✔(1.00)	✔(0.74)	❍(0.50)	✔(0.66)	✔(1.00)	✔(0.68)	✖(0.19)	✖(0.22)
✔(1.00)	✔(1.00)	✔(0.86)	✔(0.89)	✔(1.00)	✔(1.00)	✔(0.89)	✔(0.90)	✔(0.88)	✔(0.88)	❍(0.48)	❍(0.48)
✔(1.00)	✔(1.00)	✔(0.82)	✔(0.86)	✔(1.00)	✔(1.00)	✔(0.87)	✔(0.88)	✔(0.88)	✔(0.88)	❍(0.48)	❍(0.49)
✔(1.00)	✔(0.80)	✔(0.61)	✖(0.40)	✔(1.00)	✔(0.95)	✔(0.53)	✖(0.36)	✖(0.46)	✖(0.46)	❍(0.50)	❍(0.49)
✔(1.00)	✔(0.80)	✔(0.61)	✖(0.40)	✔(1.00)	✔(0.95)	✔(0.53)	✖(0.36)	✖(0.46)	✖(0.46)	❍(0.50)	❍(0.49)
✔(1.00)	✔(0.80)	✔(0.61)	✖(0.40)	✔(1.00)	✔(0.95)	✔(0.53)	✖(0.36)	✖(0.46)	✖(0.46)	❍(0.50)	❍(0.49)
✔(0.76)	✔(0.69)	✔(0.59)	✔(0.52)	✔(0.76)	✔(0.74)	✔(0.57)	✔(0.51)	✔(0.55)	✔(0.55)	❍(0.50)	❍(0.49)
✔(0.98)	✖(0.44)	✖(0.23)	✖(0.19)	✔(1.00)	✔(0.70)	✖(0.23)	✖(0.30)	✔(1.00)	✔(1.00)	✔(0.79)	✔(0.80)
✔(1.00)	✔(0.71)	✖(0.31)	✖(0.27)	✔(1.00)	✔(0.67)	✖(0.21)	✖(0.34)	✔(1.00)	✔(1.00)	✔(0.76)	✔(0.79)
✔(1.00)	❍(0.50)	✖(0.22)	✖(0.19)	✔(1.00)	✔(0.68)	✖(0.23)	✖(0.39)	✔(1.00)	✔(1.00)	✔(0.79)	✔(0.81)
✔(0.94)	❍(0.49)	✖(0.19)	✖(0.16)	✔(1.00)	✔(0.64)	✖(0.20)	✖(0.35)	✔(1.00)	✔(1.00)	✔(0.79)	✔(0.81)
✔(1.00)	✔(0.73)	✖(0.29)	✖(0.26)	✔(1.00)	✔(0.69)	✖(0.18)	✖(0.32)	✔(1.00)	✔(1.00)	✔(0.76)	✔(0.79)
✔(0.98)	✔(0.56)	✖(0.30)	✖(0.27)	✔(1.00)	✔(0.63)	✖(0.29)	✖(0.37)	✔(1.00)	✔(1.00)	✔(0.73)	✔(0.75)
✔(1.00)	✔(0.85)	✔(0.57)	✔(0.64)	✔(1.00)	✔(0.83)	✔(0.61)	✔(0.68)	✔(1.00)	✔(1.00)	✔(0.68)	✔(0.71)
✔(1.00)	✔(0.86)	✖(0.18)	✔(0.73)	✔(1.00)	✔(0.91)	✖(0.08)	✔(0.65)	✔(1.00)	✔(1.00)	✖(0.19)	❍(0.52)
✔(1.00)	✔(0.86)	✖(0.18)	✔(0.73)	✔(1.00)	✔(0.91)	✖(0.08)	✔(0.65)	✔(1.00)	✔(1.00)	✖(0.19)	❍(0.52)
✔(1.00)	✔(0.90)	✖(0.24)	✔(0.53)	✔(1.00)	✔(0.82)	✖(0.12)	✖(0.37)	✔(1.00)	✔(0.76)	✖(0.13)	✖(0.19)
✔(1.00)	✔(0.98)	✖(0.06)	❍(0.51)	✔(1.00)	✔(0.99)	✖(0.10)	✔(0.57)	✔(1.00)	✔(0.73)	✖(0.10)	✖(0.15)
✔(1.00)	✔(0.82)	✖(0.29)	✔(0.63)	✔(1.00)	✔(0.82)	✖(0.24)	✔(0.59)	✔(1.00)	✔(0.94)	✖(0.29)	✖(0.43)
All C Programs	✔(0.79)	✔(0.58)	✔(0.52)	✔(0.54)	✔(0.80)	✔(0.59)	✔(0.51)	✔(0.53)	✔(0.75)	✔(0.61)	✖(0.49)	✔(0.51)

Download : Download high-res image (2MB)
Download : Download full-size image
Fig. 7. Effectiveness:APFD
 results for C programs.

5.2.1. APFD
 results: Java programs (test-class level)
Based on Fig. 5 and Table 7, we have the following observations:

(1) Compared with 
, 
 has much better APFD
 results for the programs  and , irrespective of program version and code coverage granularity, with the maximum difference between the mean and median values being up to about 30%. For the programs  and , however, 
 performs worse than 
, with a maximum APFD
 difference of about 15%.

(2) Although 
 performs better than 
 in many cases (for example, with , for all code coverage granularities), it also sometimes performs worse (including with  and  for the branch and method coverage levels). Nevertheless, the 
 APFD
 values have much lower variation than 
.

(3) 
 has very similar performance to 
 and 
, sometimes performing slightly better (for example, with , using statement coverage) or worse (such as with , for method coverage). The differences among the mean and median APFD
 values for the three RTCP techniques are very small, at most, about 5%.

(4) Overall, the statistical analyses support the box plot observations. Looking at all Java programs together, 
 is better than 
 and 
, for all code coverage granularities: The -values are much less than 0.05; and the 
 values range from 0.53 to 0.68. Furthermore, 
 performs similarly, or slightly better, compared with 
 and 
, with the 
 values ranging from 0.50 to 0.53.

5.2.2. APFD
 results: Java programs (test-method level)
Based on Fig. 6 and Table 8, we have the following observations:

(1) Apart from some cases with the program  (for example, ), 
 has much better APFD
 performance than 
, with the maximum mean and median differences reaching about 50%.

(2) 
 performs much better than 
 for the programs  and , with the maximum mean and median APFD
 differences being about 30%. In contrast, 
 performs much better than 
 for the program . For the program , however, neither 
 nor 
 is always better: At branch coverage level, for example, 
 is better for version ; 
 is better for version ; and they have similar performance for version .

(3) The 
 and 
 APFD
 distributions are very similar, in most cases, which means that, on the whole both techniques have very similar effectiveness (according to APFD
).

(4) Although there are some large performance differences between 
 and 
 (such as for  and , with branch coverage), overall, they have similar mean and median APFD
 values. In most cases, 
 has lower variation in APFD
 values than 
.

(5) Overall, the statistical analyses support the box plot observations. Looking at all Java programs together, 
 is better than 
 and 
, with -values much less than 0.05, and the 
 values ranging from 0.62 to 0.87. 
 is better than 
, with the -values less than 0.05, and the 
 values ranging from 0.51 to 0.57. Finally, 
 and 
 have very similar performance: The 
 values only range between 0.49 and 0.51; and the -values are greater than 0.05 for statement and branch coverage, but less than 0.05 for method coverage.


Table 10. An analysis of statistical effectiveness results of APFD
. Each cell represents the total times of (✔), worse (✖), and (❍) for corresponding prioritization scenarios described in Table 7, Table 9 .

Language	Status	Statement coverage	Branch coverage	Method coverage	Sum 
Java (test-class)	✔	7	8	10	8	8	10	9	7	7	8	9	10	22	26	28	25
✖	6	3	3	3	6	2	5	3	7	3	5	3	19	8	13	9
❍	1	3	1	3	0	2	0	4	0	3	0	1	1	8	1	8
Java (test-method)	✔	12	5	9	8	13	3	9	9	11	3	9	7	36	11	27	24
✖	2	1	5	4	1	4	5	3	3	3	4	5	6	8	14	12
❍	0	8	0	2	0	7	0	2	0	8	1	2	0	23	1	6
C	✔	25	22	13	15	25	25	13	14	22	21	11	11	72	68	37	40
✖	0	1	12	9	0	0	12	10	3	3	9	7	3	4	33	26
❍	0	2	0	1	0	0	0	1	0	1	5	7	0	3	5	9
Sum 	✔	44	35	32	31	45	46	38	31	40	32	29	28	129	113	99	90
✖	8	5	20	16	4	7	6	22	13	9	18	15	25	21	44	53
❍	1	13	1	6	4	0	9	0	0	12	6	10	5	25	16	16

Table 11. Statistical effectiveness comparisons of APFD and APFD
 between different granularities for CCCP. Each cell in the Mean, Median, and Comparison columns represents the mean APFD or APFD
 value, the median value, and the -values/effect size 
 for the different code coverage granularity comparisons, respectively .

Metric	Language	Mean	Median	Comparison
Statement	Branch	Method	Statement	Branch	Method	Statement vs. Branch	Statement vs. Method	Branch vs. Method
APFD	Java (test-class)	0.5385	0.5430	0.5369	0.5342	0.5420	0.5379	2.5E−60/0.44	6.1E−07/0.52	9.9E−106/0.58
Java (test-method)	0.6542	0.6626	0.6664	0.6390	0.6339	0.6698	1.83E−40/0.45	1.4E−75/0.44	2.5E−07/0.48
C	0.9115	0.9156	0.8901	0.9076	0.9077	0.8842	4.4E−18/0.48	0/0.62	0/0.64
APFD
Java (test-class)	0.6270	0.6002	0.6171	0.6679	0.5904	0.6336	2.3E−247/0.62	7.6E−08/0.52	1.1E−29/0.46
Java (test-method)	0.7130	0.6972	0.7227	0.7112	0.7651	0.7279	2.3E−08/0.48	4.9E−02/0.49	5.0E−25/0.54
C	0.7722	0.7758	0.7581	0.8909	0.8937	0.8565	1.1E−11/0.48	7.9E−222/0.58	1.4E−252/0.59
5.2.3. APFD
 results: C programs
Based on Fig. 7 and Table 9, we have the following observations:

(1) Except for some very few cases (such as , , and , with method coverage), 
 has much higher APFD
 values than 
 and 
, with the maximum mean and median APFD
 differences between 
 and 
 reaching more than 35%; and between 
 and 
 being about 15%.

(2) 
 performs differently compared with 
 and 
 for different programs and different code coverage granularities: With the program , for example, for all versions and code coverage granularities, 
 is more effective; however, with , for both statement and branch coverage, 
 and 
 are more effective.

(3) Overall, the statistical results support the box plot observations. Considering all C programs, the  values for all comparisons between 
 and 
, 
, 
, and 
 are less than 0.05, indicating that the APFD
 scores are all significantly different. According to the effect size 
 values, 
 outperforms 
 and 
; and performs slightly better than 
 and 
 (except at the method coverage level).

Table 10 summarizes the statistical results, presenting the total number of times 
 is better (✔), worse (✖), or not statistically different (❍), compared to the other RTCP techniques. Based on this table, we can answer RQ2 as follows:

1.
When prioritizing Java test suites at the test-class level, 
 performs much better than 
, 
 and 
; and slightly better than 
.

2.
When prioritizing Java test suites at the test-method level, 
 performs much better than 
, 
, and 
; and slightly better than 
.

3.
When prioritizing C test suites, 
 performs much better than 
, 
, and 
; and slightly better than 
.

Similar to the APFD results, the CCCP APFD
 performance is influenced by different factors, including the type of test suite, and the code coverage granularity — both of which will be discussed in detail in the following two sections (Sections 5.3 RQ3: Impact of code coverage granularity, 5.4 RQ4: Impact of test case granularity). Nevertheless, the ratios of 
 performing better (✔) rather than worse (✖) than 
, 
, 
, and 
, are: 5.16 (129/25), 5.38 (113/21), 2.25 (99/44), and 1.70 (90/53), respectively. In conclusion, overall, the proposed CCCP approach is more effective than the four other RTCP techniques, in terms of testing effectiveness, as measured by APFD
.

5.3. RQ3: Impact of code coverage granularity
In this study, we examined three types of code coverage (statement, branch, and method). According to the APFD results (Fig. 2, Fig. 3, Fig. 4) and APFD
 results (Fig. 5, Fig. 6, Fig. 7), in spite of some cases where the three types of code coverage provide very different APFD or APFD
 results for CCCP, they do, overall, deliver comparable performance. This means that the choice of code coverage granularity may have little overall impact on the effectiveness of CCCP.

Fig. 8 presents the APFD and APFD
 results for the three types of code coverage, according to the subject programs’ language or test suites (the language or test case granularity is shown on the -axis; the APFD scores on the left -axis; and the APFD
 on the right -axis). It can be observed that for C programs, statement and branch coverage are very considerable, but are more effective than method coverage. For Java programs, however, there is no best one among them, because they have similar APFD and APFD
 values.


Download : Download high-res image (213KB)
Download : Download full-size image
Fig. 8. Effectiveness: CCCP APFD and APFD
 results with different code coverage and test case granularities for all programs.

Table 11 presents a comparison of the mean and median APFD and APFD
 values, and also shows the -values/effect size 
 for the different code coverage granularity comparisons. It can be seen from the table that the APFD and APFD
 values are similar, with the maximum mean and median value differences being less than 3%, and less than 8%, respectively. According to the statistical comparisons, there is no single best code coverage type for CCCP, with each type sometimes achieving the best results. Nevertheless, branch coverage appears slightly more effective than statement and method coverage for CCCP.

In conclusion, the code coverage granularity may only provide a small impact on CCCP testing effectiveness, with branch coverage possibly performing slightly better than statement and method coverage.

5.4. RQ4: Impact of test case granularity
To answer RQ4, we focus on the Java programs, each of which had two levels of test cases (the test-class and test-method levels). As can be seen in the comparisons between Fig. 2, Fig. 3, and between Fig. 5, Fig. 6, CCCP usually has significantly lower average APFD and APFD
 values for prioritizing test cases at the test-class level than at the test-method level.

Considering all the Java programs, as can be seen in Table 11, the mean and median APFD and APFD
 values at the test-method level are much higher than at the test-class level, regardless of code coverage granularity. One possible explanation for this is: Because a test case at the test-class level consists of a number of test cases at the test-method level, prioritization at the test-method level may be more flexible, giving better fault detection effectiveness (Zhang et al., 2013a).

In conclusion, CCCP has better fault detection effectiveness when prioritizing test cases at the test-method level than at the test-class level.

5.5. RQ5: CCCP efficiency
Table 12 presents the time overheads, in milliseconds, for the five RTCP techniques. The “Comp.” column presents the compilation times of the subject programs, and the “Instr.” column presents the instrumentation time (to collect the information of statement, branch, and method coverage). Apart from the first four columns, each cell in the table shows the prioritization time using each RTCP technique, for each program, presented as  (where  is the mean time and  is the standard deviation over the 1000 independent runs).

The Java programs had each version individually adapted to collect the code coverage information, with different versions using different test cases. Because of this, the execution time was collected for each Java program version. In contrast, each 
 version of the C programs was compiled and instrumented to collect the code coverage information for each test case, and all program versions used the same test cases. Because of this, each C program version has the same compilation and instrumentation time. Furthermore, because all the studied RTCP techniques prioritized test cases after the coverage information was collected, they were all deemed to have the same compilation and instrumentation time for each version of each program.

Based on the time overheads, we have the following observations:

(1) As expected, the time overheads for all RTCP techniques (including CCCP) were lowest with method coverage, followed by branch, and then statement coverage, irrespective of test case type. The reason for this, as shown in Table 2, is that the number of methods is much lower than the number of branches, which in turn is lower than the number of statements; the related converted test cases are thus shorter, requiring less time to prioritize.

(2) It was also expected that (for the Java programs) prioritization at the test-method level would take longer than at the test-class level, regardless of code coverage granularity. The reason for this, again, relates to the number of test cases to be prioritized at the test-method level being more than at the test-class level.

(3) 
 requires much less time to prioritize test cases than 
 and 
, and very similar time to 
, irrespective of subject program, and code coverage and test case granularities. Also, because 
 does not use feedback information during the prioritization process, it has much faster prioritization speeds than 
.

In conclusion, 
 prioritizes test cases faster than 
 and 
; has similar speed to 
; but performs slower than 
.

5.6. RQ6: CCCP effectiveness with 
To answer RQ6, this section briefly discusses the effectiveness of CCCP when . Fig. 9 shows the detailed APFD and APFD results for the C programs, with the code coverage granularity on the -axis, and the -axis giving the APFD or APFD scores. For ease of presentation, 
 and 
 denote CCCP with  and , respectively. Table 13 presents the statistical comparisons of the 
 APFD and APFD scores with those of the other five RTCP techniques: Each data cell shows the -value/effect size 
 value.

Based on the experimental data, we have the following observations:

(1) 
 has the higher mean and median APFD and APFD
 values than 
 and 
, and better or similar to 
, 
, and 
, regardless of code coverage granularity.

(2) The statistical results confirm the box plot observations. All -values are much less than 0.05, indicating a statistically significant difference between the 
 and each of other five RTCP techniques, regardless of APFD and APFD
 values. The 
 results also show 
 to outperform 
, 
, 
, 
, and 
, with probabilities ranging from 74% to 97%, 62% to 77%, 52% to 60%, 54% to 62%, and 53% to 56%, respectively.

These observations partly confirm our hypothesis about the performance of CCCP: As the  for unit combination increases, the testing information for guiding prioritization is greater, which may will result in improved performance.

Finally, regarding the prioritization time: 
 requires about 351 073.3, 159 881.4, and 501.8 ms for the C programs when using statement, branch and method coverage, respectively. This low prioritization time of 501.8 ms for 
 with method coverage is less than the prioritization time for 
 using statement or branch coverage (2192.0 and 973.6 ms, respectively). As shown in Fig. 9, 
 with method coverage has comparable fault detection effectiveness to 
 with statement or branch coverage. Because method coverage is usually much less expensive to achieve than statement or branch coverage, 
 with method coverage should be a better choice than 
 with statement or branch coverage. Furthermore, method-level coverage — which is the most natural for projects that are large in scale and high in complexity — has greater potential practical application than statement and branch criteria, making 
 with method coverage more feasible (2-wise code combinations coverage may incur much more complex calculations for statement and branch coverage than for method coverage).

5.7. Practical guidelines
Here, we present some practical guidelines for how to choose the combination strength and code-coverage level for CCCP, under different testing scenarios:

(1) When testing resources are limited, it is (obviously) recommended that the lowest combination strength () be chosen for CCCP. This not only achieves better testing effectiveness than other prioritization techniques, but also has comparable testing speed to the additional test prioritization technique.


Table 12. Efficiency: Comparisons of execution costs in milliseconds for different RTCP techniques. The “Comp.“ column presents the compilation times of the subject programs, and the “Instr.” column presents the instrumentation time (to collect the information of statement, branch, and method coverage). Apart from the first four columns, each cell in the table shows the prioritization time using each RTCP technique, for each program, presented as  (where  is the mean time and  is the standard deviation over the 1000 independent runs) .

Language	Program	Time	Statement coverage	Branch coverage	Method coverage	Sum 
Comp.	Instr.	
10,386	62,671	0.3/0.5	6.4/0.5	149.6/13.2	8629.9/100.4	5.0/0.7	0.1/0.3	1.3/0.5	40.9/5.2	1899.6/59.1	1.4/0.6	0.0/0.2	1.0/0.2	29.1/4.3	1491.3/94.5	0.9/0.3	0.4/–	8.7/–	219.6/–	12,020.8/–	7.3/–
14,123	115,818	0.8/0.9	19.2/1.4	739.9/54.0	21,885.1/799.4	19.2/2.5	0.2/0.7	4.9/1.6	206.7/15.9	5999.2/210.5	5.3/1.2	0.1/0.3	3.1/0.2	131.9/9.7	3832.8/150.5	3.4/0.7	1.1/–	27.2/–	1078.5/–	31,717.1/–	27.9/–
36,126	116,404	0.8/0.5	19.2/1.9	740.2/52.3	22,076.9/2,134.4	19.2/2.3	0.2/0.4	4.9/1.1	204.3/16.4	6120.5/234.1	5.3/0.7	0.1/0.4	3.1/0.9	132.5/10.4	3903.5/172.4	3.5/0.6	1.1/–	27.2/–	1077.0/–	32,100.9/–	28.0/–
4212	38,406	0.1/0.3	0.9/0.3	6.6/1.1	7343.6/71.2	0.9/1.7	0.1/0.3	0.3/0.5	2.5/0.6	1842.0/52.1	0.3/0.5	0.0/0.1	0.1/0.2	0.6/0.5	276.2/11.2	0.1/0.2	0.2/–	1.3/–	9.7/–	9461.8/–	1.3/–
4737	40,469	0.2/0.7	1.1/0.2	8.8/1.4	7740.5/76.5	1.0/0.1	0.1/0.2	0.4/0.5	3.3/0.8	1992.8/49.5	0.4/0.5	0.0/0.1	0.1/0.3	0.8/0.5	316.4/7.5	0.1/0.2	0.3/–	1.6/–	12.9/–	10,049.7/–	1.5/–
6290	45,950	0.3/0.5	2.9/0.3	38.2/5.7	17,497.2/220.2	3.2/0.4	0.1/0.3	1.1/0.2	13.8/2.0	6032.3/80.6	1.2/0.4	0.0/0.1	0.3/0.5	3.4/0.8	763.6/25.8	0.2/0.4	0.4/–	4.3/–	55.4/–	24,293.1/–	4.6/–
Java		13,395	41,501	0.1/0.3	1.1/0.6	21.0/2.6	4865.3/57.4	1.3/0.5	0.0/0.1	0.2/0.4	4.5/0.7	613.8/20.1	0.3/0.8	0.0/0.1	0.2/0.4	4.1/0.7	516.8/14.7	0.2/0.4	0.1/–	1.5/–	29.6/–	5995.9/–	1.8/–
(test-class)		13,070	41,397	0.2/0.4	2.0/1.2	50.0/4.8	7661.5/84.7	3.1/1.1	0.0/0.2	0.3/0.5	7.7/1.2	784.0/22.5	0.4/0.5	0.0/0.2	0.4/0.5	8.9/1.2	761.4/22.7	0.5/0.5	0.2/–	2.7/–	66.6/–	9206.9/–	4.0/–
15,327	22,317	0.2/0.5	3.1/1.1	83.5/7.2	9028.7/129.9	4.0/0.3	0.0/0.2	0.4/0.5	11.9/1.5	867.4/25.9	0.6/0.7	0.1/0.3	0.7/0.5	17.7/1.8	1050.6/33.9	0.8/0.4	0.3/–	4.2/–	113.1/–	10,946.7/–	5.4/–
15,028	24,571	0.2/0.4	3.1/1.3	85.5/7.2	9156.8/141.8	4.2/0.6	0.0/0.2	0.4/0.5	12.2/1.5	929.8/27.3	0.6/0.6	0.0/0.2	0.7/0.5	18.3/2.0	1106.2/35.4	0.9/1.4	0.2/–	4.2/–	116.0/–	11,192.8/–	5.7/–
16,879	35,646	0.2/0.4	4.0/0.8	122.3/9.9	9996.7/258.5	5.4/2.2	0.0/0.2	0.6/0.5	14.9/1.7	1021.1/37.7	0.7/0.4	0.1/0.2	0.9/0.4	25.4/2.4	1249.2/51.8	1.0/0.1	0.3/–	5.5/–	162.6/–	12,267.0/–	7.1/–
9722	12,570	0.2/0.4	1.6/0.5	21.0/2.3	6781.8/74.3	1.3/0.5	0.0/0.2	0.5/0.5	4.9/1.0	716.8/17.3	0.3/0.5	0.0/0.2	0.2/0.4	3.2/0.7	536.7/12.6	0.2/0.4	0.2/–	2.3/–	29.1/–	8035.3/–	1.8/–
10,352	25,234	0.2/0.4	1.7/0.5	21.8/2.7	6781.6/728.6	1.4/0.5	0.1/0.2	0.4/0.5	5.0/1.2	808.6/18.2	0.3/0.5	0.0/0.2	0.2/0.4	3.1/0.7	512.6/12.6	0.2/0.4	0.3/–	2.3/–	29.9/–	8102.8/–	1.9/–
10,276	31,700	0.2/0.7	1.3/0.4	14.5/1.9	4926.8/986.1	1.1/0.8	0.0/0.2	0.4/0.5	3.4/0.9	643.7/16.3	0.3/0.4	0.0/0.2	0.2/0.4	2.0/0.5	409.8/10.8	0.1/0.4	0.2/–	1.9/–	19.9/–	5980.3/–	1.5/–
–	–	1.1/0.6	82.7/3.2	7895.2/363.0	25,552.6/685.0	67.1/4.5	0.4/0.8	17.9/2.4	723.8/42.6	5950.3/400.1	19.3/3.1	0.3/1.0	15.1/1.9	1456.9/70.7	5055.7/339.3	13.0/2.0	1.8/–	115.7/–	10,075.9/–	36,558.6/–	99.4/–
–	–	2.7/1.3	308.3/5.6	46,943.6/2,484.2	56,464.7/12,834.1	287.1/10.0	0.9/1.6	77.3/1.5	2307.4/151.0	17,620.8/1,007.5	77.9/5.9	0.5/1.1	56.6/2.5	8740.3/309.3	13,052.6/755.0	52.7/3.9	4.1/–	442.2/–	57,991.3/–	87,138.1/–	417.7/–
–	–	2.8/1.5	303.8/5.2	46,487.4/1,950.7	56,055.4/12,828.2	284.0/8.9	0.8/1.0	77.0/1.4	2284.0/146.0	11,463.7/2,429.0	76.8/2.7	0.5/1.0	55.0/0.9	8591.8/308.4	9456.1/1,669.7	52.0/2.3	4.1/–	435.8/–	57,363.2/–	76,975.2/–	412.8/–
–	–	1.0/1.0	63.4/1.8	10,945.7/3,639.1	48,223.8/3,045.9	71.9/3.3	0.4/0.7	23.7/1.1	2912.9/109.8	16,169.4/880.2	26.5/1.7	0.1/0.3	6.3/0.5	727.7/29.2	2697.8/355.8	5.5/0.5	1.5/–	93.4/–	14,586.3/–	67,091.0/–	103.9/–
–	–	1.0/0.5	66.7/1.8	11,800.5/3,838.7	50,776.5/3,564.1	75.3/3.0	0.4/0.5	25.0/1.5	3061.5/128.3	17,139.4/1,003.3	28.2/1.6	0.1/0.3	6.6/0.5	792.7/31.6	3122.9/341.7	6.0/0.3	1.5/–	98.3/–	15,654.7/–	71,038.8/–	109.5/–
–	–	2.7/2.1	263.0/4.6	67,417.5/24,856.1	115,587.2/31,331.9	360.8/10.2	1.0/0.6	98.2/2.7	35,013.6/7,080.4	47,254.9/5,475.1	132.7/4.4	0.3/0.4	28.6/0.7	9556.6/494.8	15,477.5/1,017.4	27.9/2.0	4.0/–	389.8/–	111,987.7/–	178,319.6/–	521.4/–
Java		–	–	0.2/0.4	7.8/0.4	854.6/94.2	18,956.3/3,208.7	9.3/1.7	0.1/0.2	1.7/0.5	62.3/16.8	3388.9/805.1	2.1/0.2	0.0/0.2	1.4/0.5	168.0/33.4	4024.7/564.2	1.7/0.5	0.3/–	10.9/–	1084.9/–	26,369.9/–	13.1/–
(test-method)		–	–	0.5/1.3	18.4/1.4	2631.4/200.2	11,459.0/2,003.0	22.6/1.8	0.1/0.3	2.8/0.4	114.2/26.7	5524.5/539.1	3.5/0.8	0.1/0.3	3.1/0.4	455.9/64.2	1829.2/57.9	3.8/1.1	0.7/–	24.3/–	3201.5/–	18,812.7/–	29.9/–
–	–	0.6/0.5	23.5/0.6	3408.0/273.4	17,396.6/702.3	27.3/2.7	0.1/0.3	3.3/0.5	144.5/34.2	1717.5/73.2	4.2/0.6	0.1/0.4	4.9/0.7	721.3/91.1	2489.7/110.4	5.4/1.7	0.8/–	31.7/–	4273.8/–	21,603.8/–	36.9/–
–	–	0.7/0.7	24.4/2.2	3519.3/289.1	17,671.1/705.3	27.9/2.6	0.1/0.3	3.3/0.8	148.8/34.6	1910.7/84.7	4.3/1.4	0.2/0.4	4.9/0.4	746.8/96.4	2774.4/108.2	5.4/0.8	1.0/–	32.6/–	4414.9/–	22,356.2/–	37.6/–
–	–	0.7/1.0	28.9/0.7	4476.6/352.7	20,455.8/577.5	34.0/3.2	0.1/0.3	3.9/1.0	182.5/37.9	2091.8/100.0	5.0/0.9	0.2/0.9	5.9/0.9	922.1/112.4	3105.3/129.6	6.5/0.9	1.0/–	38.7/–	5581.2/–	25,652.9/–	45.5/–
–	–	0.9/1.0	36.8/2.1	5623.0/413.0	16,075.6/468.9	34.5/3.3	0.2/0.5	8.8/0.7	282.4/55.3	2172.4/111.3	8.4/1.4	0.1/0.3	5.6/1.2	828.4/102.2	2235.5/78.4	5.3/1.0	1.2/–	51.2/–	6733.8/–	20,483.5/–	48.2/–
–	–	0.9/0.6	41.2/2.3	6438.4/443.9	13,324.8/3,148.0	38.1/2.9	0.3/0.6	10.9/0.6	356.0/60.1	2581.1/122.9	9.8/1.6	0.1/0.3	6.1/0.9	903.5/105.9	2149.4/73.5	5.8/1.5	1.3/–	58.2/–	7697.9/–	18,055.3/–	53.7/–
–	–	0.9/1.3	33.7/2.2	4257.9/734.8	10,055.2/2,334.9	29.7/2.2	0.3/0.4	9.0/1.3	265.0/50.0	2113.3/107.5	8.0/1.6	0.1/0.6	4.5/0.5	626.2/78.5	1322.8/202.3	4.2/1.1	1.3/–	47.2/–	5149.1/–	13,491.3/–	41.9/–
C		401	10,075	7.8/4.0	482.9/14.4	6746.2/166.5	4308.3/155.7	503.6/11.4	4.6/3.7	304.7/12.4	5805.9/165.7	3855.0/160.9	260.2/8.0	1.2/3.4	72.9/2.8	402.0/23.9	2871.9/63.9	28.9/2.6	13.6/–	860.5/–	12,954.1/–	11,035.2/–	792.7/–
1833	8555	4.4/3.6	235.3/6.7	3547.3/123.2	2874.9/63.0	216.2/7.5	3.7/3.6	217.0/6.7	4527.4/122.5	2966.2/78.3	172.6/6.8	0.8/0.8	48.8/1.1	226.6/9.1	2255.3/37.9	17.6/1.9	8.9/–	501.1/–	8301.3/–	8096.4/–	406.4/–
308	1882	0.9/0.7	13.6/0.6	70.2/4.4	517.1/10.0	16.5/1.9	0.6/0.7	9.4/0.5	50.2/3.6	460.9/10.1	10.2/1.4	0.3/0.7	3.5/0.5	7.4/1.2	368.5/9.5	2.3/0.8	1.8/–	26.5/–	127.8/–	1346.5/–	29.0/–
1483	8520	1.8/0.9	21.8/0.7	116.3/8.1	783.3/16.8	28.9/2.2	1.3/1.0	14.9/0.3	120.2/7.9	604.9/12.6	18.5/1.9	0.3/0.8	2.2/0.4	5.9/1.1	228.6/6.5	2.5/0.6	3.4/–	38.9/–	242.4/–	1616.8/–	49.9/–
837	2956	1.7/1.1	68.2/1.2	1049.5/36.5	1597.8/32.0	60.8/3.7	1.1/1.0	48.7/1.6	849.0/32.7	1499.7/28.5	31.6/2.6	0.5/0.9	20.6/0.6	87.8/4.4	1324.7/27.5	7.3/1.1	3.3/–	137.5/–	1986.3/–	4422.2/–	99.7/–
Sum 	37.3/–	2192.0/–	236,331.5/–	632,508.4/–	2265.9/–	17.4/–	973.6/–	59,747.6/–	176,757.0/–	917.2/–	6.2/–	363.8/–	36,348.9/–	92,569.7/–	265.9/–	60.9/–	3529.4/–	332,428.0/–	901,835.1/–	3449.0/–
(2) When there are sufficient testing resources available,  is recommended for CCCP, because of the higher fault detection rates it can deliver.

(3) If the system under test is large in scale and high in complexity, method coverage is recommended to be used for CCCP.

5.8. Threats to validity
To facilitate the investigation of potential threats and to support the replication of experiments, we have made the relevant materials (including source code, subject programs, test suites, and mutants) available on our project website: https://arxiv.org/abs/2007.00370/. Despite that, our study still face some threats to validity, listed as follows.


Table 13. Statistical effectiveness comparisons of APFD and APFD
 between CCCP with  and the other five RTCP techniques for all C programs.

Metric	Comparison	Statement	Branch	Method
APFD	vs. 
0/0.94	0/0.97	0/0.85
vs. 
0/0.66	0/0.71	0/0.77
vs. 
2.3E−149/0.57	0/0.60	6.3E−55/0.54
vs. 
8.4E−262/0.58	0/0.62	2.2E−253/0.59
vs. 
3.0E−262/0.53	2.0E−80/0.55	4.8E−103/0.56
APFD
vs. 
0/0.79	0/0.82	0/0.74
vs. 
0/0.60	0/0.65	0/0.62
vs. 
5.0E−68/0.55	4.0E−158/0.57	1.4E−10/0.52
vs. 
2.5E−135/0.56	4.8E−237/0.58	1.4E−59/0.54
vs. 
1.4E−40/0.53	1.5E−66/0.54	1.4E−22/0.53
5.8.1. Internal validity
The main threat to internal validity lies in the implementation of our experiment. First, the randomized computations may affect the performance of CCCP: To address this, we repeated the prioritization process 1000 times and used statistical tests to assess the strategies. Second, the data structures used in the prioritization algorithms, and the faults in the source code, may introduce noise when evaluating the effectiveness and efficiency: To minimize these threats, we used data structures that were as similar as possible, and carefully reviewed all source code before conducting the experiment. Third, although we used the APFD and APFD
 metrics, which have been extensively adopted to assess the performance of RTCP techniques, APFD only reflects the rate at which faults are detected, ignoring the time and space costs, and APFD
 assumes that all faults have the same fault severity. To address this threat, our future work will involve additional metrics that can measure other practical performance aspects of prioritization strategies.

5.8.2. External validity
All the programs used in the experiment were medium-sized, and written in C or Java, which means that the results may not be generalizable to programs written in other languages (such as C++ and C#) and of different sizes. To reduce this threat, other relevant programs will be adopted to evaluate the CCCP performance. Mutation testing has been argued to be an appropriate approach for assessing fault detection performance (Andrews et al., 2005, Do and Rothermel, 2005, Just et al., 2014). Mutation testing has also been used in recent RTCP research studies (Luo et al., 2016, Luo et al., 2019, Henard et al., 2016, Lu et al., 2016). However, Luo et al. (2018) has highlighted the differences between real faults and mutants, explaining that the relative performances of RTCP techniques on mutants may not translate to similar relative performances with real faults. To address this threat, additional studies will be conducted to investigate the performance of RTCP on programs with real regression faults in the future.

6. Related work
A considerable amount of research has been conducted into regression testing techniques with a goal of improving the testing performance. This includes test case prioritization (Rothermel et al., 1999, Miranda et al., 2018), reduction (Chen et al., 2017, Shi et al., 2015) and selection (Zhang, 2018, Gligoric et al., 2015a). This Related Work section focuses on test case prioritization, which aims to detect faults as early as possible through the reordering of regression test cases (Yoo and Harman, 2012, Khatibsyarbini et al., 2018).

Prioritization Strategies. The most widely investigated prioritization strategies are the total and additional techniques (Rothermel et al., 1999). Because existing greedy strategies may produce suboptimal results, Li et al. (2007) translated the RTCP problem into a search problem and proposed several search-based algorithms, including a hill-climbing and genetic one. Motivated by random tie-breaking, Jiang et al. (2009) applied adaptive random testing to RTCP and proposed a family of adaptive random test cases prioritization techniques that aim to select a test case with the greatest distance from already selected ones.

More recently, as the total strategy and the additional strategy can be seen as two extreme instances, Zhang et al. (2013a) proposed a basic and an extended model to unify the two strategies. Saha et al. (2015) proposed an RTCP approach based on information retrieval without dynamic profiling or static analysis. Many existing RTCP approaches use code coverage to schedule the test cases, but do not consider the likely distribution of faults. To address this limitation, instead of traditional code coverage, Wang et al. (2017) used the quality-aware code coverage calculated by code inspection techniques to guide prioritization process.

Coverage criteria. In terms of coverage criteria, structural coverage has been widely adopted in test case prioritization. In addition to statement (Rothermel et al., 1999), branch (Jiang et al., 2009), method (Zhang et al., 2013a, Wang et al., 2017), block (Li et al., 2007) and modified condition/decision coverage (Jones and Harrold, 2003), Elbaum et al. (2000) proposed a fault-exposing-potential (FEP) criterion based on the probability of the test case detecting a fault. Recently, Chi et al. (2018) used function call sequences, arguing that basic structural coverage may not be optimal for dynamic prioritization.

Empirical studies. A large number of empirical studies have been performed aiming to offer practical guidelines for using RTCP techniques.

In addition to studies on traditional dynamic test prioritization (Rothermel et al., 1999, Elbaum et al., 2000, Do et al., 2006, Do et al., 2010), recently, Lu et al. (2016) were the first to investigate how real-world software evolution impacts on the performance of prioritization strategies: They reported that source code changes have a low impact on the effectiveness of traditional dynamic techniques, but that the opposite was true when considering new tests in the process of evolution.

Citing a lack of comprehensive studies comparing static and dynamic test prioritization techniques, Luo et al., 2016, Luo et al., 2019 compared static RTCP techniques with dynamic ones. Henard et al. (2016) compared white-box and back-box RTCP techniques.

7. Conclusions and future work
In this paper, we have introduced a new coverage criterion that combines the concepts of code and combination coverage. Based on this, we proposed a new prioritization technique, code combinations coverage based prioritization (CCCP). Results from our empirical studies have demonstrated that CCCP with the lowest combination strength () can achieve better fault detection rates than four well-known, popular prioritization techniques (total, additional, adaptive random, and search-based test prioritization). CCCP was also found to have comparable testing efficiency to the additional test prioritization technique, while requiring much less time to prioritize test cases than the adaptive random and search-based techniques. The results also show that CCCP with a higher combination strength () can be more effective than all other prioritization techniques, in terms of both APFD and APFD
.

Our future work will include examining more real-life programs to further investigate the performance of CCCP, including the impact of combination strengths. In this paper, we have only applied the concept of code combinations coverage to the traditional greedy prioritization strategy. It will be very interesting to examine new prioritization techniques based on code combinations coverage adopting other prioritization strategies such as search-based strategy.