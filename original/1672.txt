Abstract
Community smells represent sub-optimal conditions appearing within software development communities (e.g., non-communicating sub-teams, deviant contributors, etc.) that may lead to the emergence of social debt and increase the overall project’s cost. Previous work has studied these smells under different perspectives, investigating their nature, diffuseness, and impact on technical aspects of source code. Furthermore, it has been shown that some socio-technical metrics like, for instance, the well-known socio-technical congruence, can potentially be employed to foresee their appearance. Yet, there is still a lack of knowledge of the actual predictive power of such socio-technical metrics. In this paper, we aim at tackling this problem by empirically investigating (i) the potential value of socio-technical metrics as predictors of community smells and (ii) what is the performance of within- and cross-project community smell prediction models based on socio-technical metrics. To this aim, we exploit a dataset composed of 60 open-source projects and consider four community smells such as Organizational Silo, Black Cloud, Lone Wolf, and Bottleneck. The key results of our work report that a within-project solution can reach F-Measure and AUC-ROC of 77% and 78%, respectively, while cross-project models still require improvements, being however able to reach an F-Measure of 62% and overcome a random baseline. Among the metrics investigated, socio-technical congruence, communicability, and turnover-related metrics are the most powerful predictors of the emergence of community smells.

Previous
Next 
Keywords
Community smells

Social debt

Empirical software engineering

1. Introduction
Software engineering is de-facto a social activity (Ralph et al., 2016, Hannemann et al., 2009) which involves often thousands if not more stakeholders arranged globally, and often separated by physical and cultural distance (i.e., different cultures and culture clashes (Sharp et al., 2000, Jaakkola, 2012, Gren, 2019)), expertise or power distance (Hofstede et al., 2008, Greenhoe, 2016, Packirisamy, 2017, Sanchez-Anguix et al., 2012), and more). Literature calls the sum of the negative and unforeseen costs and consequences of such conditions community smells, namely sub-optimal organizational and socio-technical situations that hamper or altogether impede the straightforward production, operation, and evolution of software (Palomba et al., 2018, Tamburri, 2019, Tamburri et al., 2019). Previous work established the interaction between community smells and their technical counterparts (i.e., code smells such as Spaghetti Code, God Class and more Palomba et al., 2014, Palomba et al., 2019, Palomba et al., 2015) as well as, more generally, technical debt in its various forms (Martini and Bosch, 2017, Martini et al., 2018). At the same time, community smells have been connected to all sorts of nasty phenomena, e.g., employee turnover (Tamburri et al., 2019), bad architecture decisions (Tamburri, 2019), and more, which often go beyond causing technical issues in software code but may well cause organizational turmoil or even decline (Tamburri et al., 2020) of software projects’ organizational stability, if not project failure. Therefore, predicting community smells before they manifest along software projects’ timelines may prove critical in anticipating sub-optimal organizational and socio-technical conditions before they become unmanageable. In summary, on the one hand, these conclusions from the state of the art indicate that community smells represent a first-class phenomenon to be considered when managing large-scale software systems from an organizational and socio-technical perspective.

On the other hand, previous experience reports and empirical evidence from the state of the art (Palomba et al., 2018, Tamburri, 2019, Tamburri et al., 2019) also remarked that predicting community smells at large and anticipating their existence is challenging, because of their socio-technical and evolutionary nature tied to the types and characteristics of the organizational-social structure around (Tamburri et al., 2013a, Tamburri et al., 2013b, Tamburri et al., 2019a). Yet, the early prediction of their emergence is highly desirable for both developers and project managers in order to take informed decisions and possibly re-organize the community structure to avoid them (Tamburri et al., 2019b, Tamburri et al., 2019a, Tamburri et al., 2015a, Soldani et al., 2018). In our previous work (Tamburri et al., 2019b), we investigated various aspects related to community smells, including their correlation with well-known socio-technical metrics (e.g., socio-technical congruence or turnover). Our results showed the existence of a number of correlations that lead us to believe that socio-technical metrics could be potentially useful for the prediction of community smells.


Download : Download high-res image (78KB)
Download : Download full-size image
To address our research goals, we conduct a large-scale empirical study involving 60 open-source software communities and measure how well a set of 40 socio-technical metrics can predict 4 community smells that have been shown to create harmful forms of both social and technical debt. First, we employed an information gain measure (Quinlan, 1986) to estimate the contribution given by each metric to the prediction and, in the second place, we built within- and cross-project community smell prediction models that exploit the considered socio-technical metrics. Our assumption when testing cross-project solutions is that similar working conditions in other smelly projects could be used to anticipate community smells. In so doing, we face a number of key aspects of machine learning modeling like, for instance, data normalization and balancing. Also, we verify the performance of five different machine learning algorithms for the task of community smell prediction.

The key findings of our study report that: (1) Socio-technical congruence, communicability, and turnover-related metrics are top-factors to consider when predicting the emergence of community problems; (2) Within-project models can effectively be employed for predicting community smells – in particular when Random Forest is used as classifier – since they reach a median F-Measure of 77%; and (3) A cross-project approach leads to a median F-Measure of 62%, being therefore a promising alternative despite some further improvements would be required.

To sum up, this paper offers three main contributions:

1.
The first, large-scale empirical exploration of predictive models for community smells—this exploration is entirely novel in the state of the art in software engineering research;

2.
The analysis of within- vs. cross-project community smells prediction, that provides insights into the different ways practitioners can exploit machine learning for early detection of community-related issues and whether working conditions ot other smelly projects could be exported and used to anticipate community smells on other projects;

3.
A benchmark dataset for researchers to proceed along this line of research and compare novel mechanisms with the devised models;

Structure of the paper. The rest of this paper is structured as follows. Section 2 outlines the related work. Section 3 outlines our research design while Sections 4 RQ, 5 RQ showcase our results. In Section 6 we discuss possible threats to validity of our results and explain how we mitigated them. Finally, we conclude the paper and report our future research agenda in Section 7.

2. Related work
In this section, we discuss the literature related to community smells as well as the research conducted on other socio-technical aspects.

2.1. Community smells and related research
The work we outlined in the previous pages addresses the application of machine-learning techniques to the prediction of community smells, that is, nasty organizational and socio-technical phenomena. Related work mostly resides in the domains of Machine-Learning and Big Data analytics for social-networks motif mining (Mursa et al., 2019, Russo, 2018, Choobdar et al., 2012) as well as organization networks’ analysis (Trucco et al., 2008, Whelan and Donnellan, 2005). For example, Trucco et al. (2008) and Ceci et al. (2011) use Bayesian models to elicit organization factors for risk engineering; however, the case-study presented by Trucco et al. is well beyond the state of the art in software engineering and provides reasonably large organization networks but reflecting mostly the domain of Supply-Chain Management. Conversely, previously in software engineering research, Russo (2018) has used learning techniques and motif mining for the purpose of profiling system call changes as part of software evolution. This latter work is closer to our own in terms of focusing on software artifacts and predicting anti-patterns in their key characteristics; however, Russo focuses purely on a specific technical aspect of software evolution, rather than the organization structure around it and the anti-patterns that might emerge as its operations unfold. From another perspective, but still focusing on the use of motif analysis for anti-patterns mining in organization networks, Argentieri et al. (2017) provide an overview of search-based techniques dedicated to cross-motif analysis and prediction; specifically, the authors look at combinations and mutual relations between motifs.

In comparison to the aforementioned works, we do not take the motif analysis angle at all; rather, we focus on state-of-the-art and practitioner-friendly metrics as predictors of community smells presence and severity. Our intent is to provide pre-trained models for practitioners to use in their own projects, e.g., as part of retrospectives, risk-analysis, decision-making, and more.

Beyond the above, from a rule-based perspective, community smells have seen intense research around the tool CodeFace, originally presented by Joblin et al. (2015a). More specifically, the aforementioned tool was augment and experimented with for the detection (Tamburri et al., 2019) and evaluation of the impact of community smells over code smells (Palomba et al., 2018). More specifically, Palomba et al. discover that community smells are the top factor when it comes to predicting specific code smells; these works provide fundamental motivations for our work as well as groundwork to establish the ground-truth behind our baseline dataset. Similar works on community smells have concentrated on establishing their impacts on other dimensions of software engineering (e.g., Architecture Debt Martini and Bosch, 2017, organization structure types Tamburri et al., 2019a, and more). With the contributions in this work we aim at providing a basic predictive mechanism for, on the one hand, practitioners to embed in their own DevOps pipelines, hopefully using the produced insights to avoid needless waste and, on the other hand, academics willing to improve beyond our results and current accuracy levels.

2.2. Research on other socio-technical aspects
The socio-technical nature of software engineering has been explored for years and from several perspectives. The seminal work by Nagappan et al. (2008) showed in practice the influence of organizational structure and other “human” aspects over software quality. This and similar works (e.g., Repenning et al., 0000 or Viana et al., 2012) bring evidence that motivates our study of social communities in organizations and the debt connected to them. On the one hand, while Nagappan et al. established the correlation between organizational structures and software quality, we aim to predict patterns of sub-optimality across said structures, e.g., to allow for preventive action by means of social networks analysis (Meneely et al., 2008) or predictive organizational rewiring. Finally, seminal studies on socio-technical congruence, first defined by Cataldo et al. (2008) can support the predictive modeling and analytics around community smells. On one hand, socio-technical congruence is the degree to which technical and social dependencies match, when coordination is needed. On the other hand, STC may require further elaboration as well as different degrees of granularity for it to play a key role in anticipating and correcting sub-optimal organizational behavior. From a more recent perspective, the social aspects playing a key role in either code community hosting platforms (e.g., GitHub) or closed-source software engineering have been investigated from a social coding perspective (e.g., see Trockman, 2018) as well as from the developer attraction and retention perspective (e.g., see Qiu et al., 2019). These and other recent theories around social software engineering factors and relations are fundamental to refine predictive models such as the ones proposed in this paper.

3. Empirical study setup
The goal of the study is to investigate if and to what extent the emergence of community smells can be predicted by socio-technical metrics, with the purpose of providing practitioners with an automated mechanism able to promptly pinpoint the possible presence of issues within the organization community of software development teams. The perspective is of both researchers and practitioners: the former are interested in understanding how much community smells can be predicted by lightweight socio-technical indicators, while the latter aim at finding methods to automatically identify and possibly avoid the emergence of community-related problems.

3.1. Research questions and methodological overview
Our study is driven by three main research questions that aim at exploring the problem of community smell prediction under three different perspectives. First, in our previous work (Tamburri et al., 2019b) we studied the correlation between a number of socio-technical metrics and the amount of community smells in open-source projects, finding that these metrics can foreseen community-related problems. As such, our first research question aims at understanding more closely the predictive power of socio-technical metrics for community smell prediction. Thus, we ask:


Download : Download high-res image (38KB)
Download : Download full-size image
In other words, 
 has a preliminary/descriptive nature: indeed, while in our previous study (Tamburri et al., 2019b) we only observed correlations, the first research question has the goal of taking a closer look into the potential of socio-technical metrics for the prediction of the emergence of community smells. In this sense, the idea is to have a preliminary investigation aimed at characterizing the amount of information provided by each metric before employing them in machine learning models able to predict the emergence of community smells in general and individual smells in particular. Furthermore, 
 serves as a way to corroborate the correlation findings we have previously discovered.

Once assessed which metrics can be actually used as predictors of community smells, we then proceed with the definition of a supervised learning technique. As part of the second research question, we train the devised model using data coming from previous history of individual projects, i.e., using a within-project strategy, with the aim of assessing how much information directly coming from the considered projects can be used for prediction of future community-related issues. Thus, we formulate the following RQ:


Download : Download high-res image (55KB)
Download : Download full-size image
As a final step, we evaluate whether and to what extent can we exploit socio-technical information of external projects to train a community smell prediction model, i.e., we assess the performance of a cross-project approach. In this case, our goal is to measure how effectively can new projects that lack of community smell and socio-technical data use external information to find potential problems within their development communities. Hence, we ask:


Download : Download high-res image (55KB)
Download : Download full-size image
In short, we address 
 by quantifying the relevance of each socio-technical metric for the prediction of community smells; in this respect, we employ an Information Gain measure (Quinlan, 1986), which is a statistical technique that can estimate the gain provided by each independent variable to the prediction of the dependent one, giving a ranking of the features based on their importance. In the context of 
 and 
 we follow well-established guidelines (Arisholm et al., 2010, Tantithamthavorn and Hassan, 2018) to devise within- and cross-project models, respectively. As such, we consider and address common modeling problems such as (1) data normalization, (2) data balancing, and (3) testing of different machine learning algorithms, properly configured with respect to their hyper-parameters. Finally, we interpret the results achieved by considering various evaluation metrics (e.g., F-Measure or AUC-ROC Baeza-Yates et al., 1999) that can provide a broad vision of the strengths and weaknesses of using supervised learning for community smell prediction. Sections 4, 5 present more details on the methodology adopted and discuss the results.

3.2. Context of the study and dataset preparation
The context of the study consists of a publicly available dataset that we built in our previous study (Tamburri et al., 2019b) and publicly available on Github.1 It contains labeled data related to four types of community smells identified over 60 open-source software communities, whose basic data are reported in Appendix. More specifically, all systems are hosted on Github2 and their selection was originally driven by two main factors: first, we focused on communities having at least 30 contributors and 1,000 commits—this was needed to actually observe community smells, which are problems usually occurring in large projects (Bird et al., 2008, Tamburri et al., 2015b, Tamburri et al., 2013b) ; similarly, the rationale for the selection of the 30/1000 rule reflects previous work in organizations and social networks research (Dasgupta et al., 2009) which points out that SNs which reflect a total of 1̃00 nodes exhibit the organizational and community structure behavior of so-called non small-world topology networks and therefore are considered “large”. Considering that in software organizational structures every single developer commits to and caters for about 2–3 software artifacts across the Developer Social Networks in our dataset, we fixed the total minimum of 30 contributors to characterize “large” software projects. Second, we only considered systems for which at least ten commits were performed during the last observed month, with the aim of mitigating threats due to outdated phenomena. As such, starting from all projects respecting these two conditions, we randomly picked 60 of them.

Once selected the objects, we identified and validated community smells. To this aim, we first restricted our focus to four community smells that have been shown by previous research (Tamburri et al., 2015b, Palomba et al., 2018) to be (1) among the most problematic community-related issues to deal with and (2) a potential threat to the emergence of technical debt.3 Specifically, these are:

1.
Organizational Silo Effect: This refers to the presence of siled areas of the developer community that do not communicate, except through one or two of their respective members;

2.
Black Cloud Effect: This reflects an information overload due to lack of structured communications or cooperation governance;

3.
Lone Wolf Effect: This smell appears in cases where the development community presents unsanctioned or defiant contributors who carry out their work with little consideration of their peers, their decisions and communication;

4.
Bottleneck or “Radio-silence” Effect: This is an instance of the “unique boundary spanner” (Wasserman and Faust, 1994) problem from social-networks analysis: one member interposes herself into every formal interaction across two or more sub-communities with little or no flexibility to introduce other parallel channels.

To identify them, we relied on our tool CodeFace4Smells  (Tamburri et al., 2019b), a fork of CodeFace (Joblin et al., 2015b) which was originally designed to extract coordination and communication graphs mapping the developer’s relations within a community. Our tool builds on top of CodeFace and applies a rule-based approach to identify instances of the four community smells described above. For example, the identification pattern for Lone Wolf is based on the detection of development collaborations between two community members that have intermittent communication counterparts or feature communication by means of an external “intruder”, i.e., not involved in the collaboration.

A basic example is given in Fig. 1. In this example two developers, “1” and “2”, are collaborating on some code, but they are not connected by any communication link other than developer “3”, who is not co-committing on a shared file. In this case, either developer “1” or developer “2” (or both) can develop a Lone Wolf community smell. The full list of identification rules as well as more examples are available in our online appendix (Palomba and Tamburri, 2019).


Download : Download high-res image (94KB)
Download : Download full-size image
Fig. 1. Lone Wolf Community Smell identification pattern—image taken from Tamburri et al. (2019b).

It is important to note that the dataset contains community smells for each release of the considered projects, meaning that they have been detected over their entire life span—this is a key detail for this study, as it enables the execution of a release-by-release strategy when training the devised within-project model (as described in Section 5). The distribution of the community smells per release is shown in Fig. 2: as visible, for three of them, i.e.,Organizational Silo, Lone Wolf, and Bottleneck, the median number of occurrences for each release is around 10, while Black Cloud is much lower (2). This aspect clearly affects the way a machine learning algorithm aiming at predicting them should be setup. Last, but not least, it is important to comment on the reliability of this dataset when used as ground-truth by a machine learning solution. Unlike other types of software engineering information that can be more easily verified (e.g., defects or design flaws Di Nucci et al., 2018a, Di Nucci et al., 2017, Pecorelli et al., 2019a, Pecorelli et al., 2019b), it is not possible to create a “tangible” oracle of community smells since (1) they involve teams and (2) are not tracked by organizations and cannot be without an automated solution (Tamburri et al., 2015b). As such, we are not able to compute common measures like precision and recall when evaluating the tool. Alternatively, to evaluate the identification rules adopted by CodeFace4Smells, in our previous works (Tamburri et al., 2019b, Tamburri et al., 2015b), we conducted surveys and/or semi-structured interviews with both the original industrial and open-source practitioners belonging to the communities we considered, showing them the results of the tool and asking for confirmation. More specifically, we first mined the community smells appearing in the 60 software projects also used in the context of this paper as well as the email addresses of the most active developers of those communities (i.e., the ones who committed at least 10 changes during the year before the release dates taken into account), ending up with 172 developers who were willing to participate in the study. Secondly, we inquired the developers about the existence and perception of the community smells considered. Finally, we performed a follow-up confirmatory study involving 35 developers (of those who participated already to the survey), in which we openly discussed about community smells and the performance of the tool. As an outcome, we discovered that the problem of community smells is highly recognized in practice, with them being considered as the main source of social debt. Furthermore, all the survey respondents and interviewees reported the validity and usefulness of CodeFace4Smells, without pointing out additional problematic situations occurred in their communities. In other words, according to developers, the community smells output by the tool are all true positives; as for false negatives, if they exist, these were not pointed out by original developers. This makes us confident of the high reliability of the tool.


Download : Download high-res image (71KB)
Download : Download full-size image
Fig. 2. Distribution of community smells in our dataset—image taken from Tamburri et al. (2019b).

Once identified the smelly community structures in our dataset, the final step required to conduct our study consists of the identification of the non-smelly ones, namely those community structures that are not affected by smells and that, therefore, can be used to complement the smelly ones and train a machine learner. For each release of the 60 projects, we consider as non-smelly all those parts of the developer’s social network that are not detected as smelly by CodeFace4Smells.

4. RQ
. on the predictive power of socio-technical metrics
This section describes methodology and results that address our first research question.

4.1. Research methodology
The first step of our empirical study consists of understanding the predictive power of socio-technical metrics when it comes to the prediction of community smells. Such an analysis derives from the results of our previous study (Tamburri et al., 2019b), where we discovered a number of correlations between socio-technical metrics and community smells: for this reason, in the context of this paper we take into account exactly the same set of metrics previously analyzed, which are briefly reported in Table 1. This set consists of 40 different metrics belonging to five main categories that capture community-related aspects under different perspectives, e.g., the turnover of developers rather than their overall closeness. The rationale behind the selection of those metrics is summarized in Table 2. Although the aforementioned rationales reflect a considerable amount of metrics, there exist as many as 350+ factors that may play a key role in predicting and managing sub-optimal organizational conditions, as well as software project success and failure as we ourselves highlighted in previous research (Tamburri et al., 2020). With this first work, we aim at laying foundations (i.e., by conducting a first analysis and providing a dataset) for future work in the area.


Table 1. Socio-technical metrics considered in our study. All of them have been shown to be correlated to community smells in our previous work Tamburri et al. (2019b). Note that the “Devs” or developers are defined as the total sum of members in the developer social network whereas the “core (either global, code, or ml) developers” are defined as the total sum of developers which are part of the core of the organizational structure in line with Wallerstein core–periphery network structure theory, which Joblin et al. (2016) have shown to be true for Software Communities as well; conversely, non-core developers reflect the total sum of every other developer not belonging to the structural core.

Category	Metric	Description
Developer social network metrics	devs	Number of developers present in the global Developers Social Network
ml.only.devs	Number of developers present only in the communication Developers Social Network
code.only.devs	Number of developers present only in the collaboration Developers Social Network
ml.code.devs	Number of developers present both in the collaboration and in the communication DSNs
perc.ml.only.devs	Percentage of developers present only in the communication Developers Social Network
perc.code.only.devs	Percentage of developers present only in the collaboration Developers Social Network
perc.ml.code.devs	Percentage of developers present both in the collaboration and in the communication DSNs
sponsored.devs	Number of sponsored developers (95% of their commits are done in working hours)
ratio.sponsored	Ratio of sponsored developers with respect to developers present in the collaboration DSN
Socio-technical metrics	st.congruence	Estimation of socio-technical congruence
communicability	Estimation of information communicability (decisions diffusion)
num.tz	Number of timezones involved in the software development
ratio.smelly.devs	Ratio of developers involved in at least one Community Smell
Core community members metrics	core.global.devs	Number of core developers of the global Developers Social Network
core.mail.devs	Number of core developers of the communication Developers Social Network
core.code.devs	Number of core developers of the collaboration Developers Social Network
sponsored.core.devs	Number of core sponsored developers
ratio.sponsored.core	Ratio of core sponsored developers with respect to core developers of the collaboration DSN
global.truck	Ratio of non-core developers of the global Developers Social Network
mail.truck	Ratio of non-core developers of the communication Developers Social Network
code.truck	Ratio of non-core developers of the collaboration Developers Social Network
mail.only.core.devs	Number of core developers present only in the communication DSN
code.only.core.devs	Number of core developers present only in the collaboration DSN
ml.code.core.devs	Number of core developers present both in the communication and in the collaboration DSNs
ratio.mail.only.core	Ratio of core developers present only in the communication DSN
ratio.code.only.core	Ratio of core developers present only in the collaboration DSN
ratio.ml.code.core	Ratio of core developers present both in the communication and in the collaboration DSNs
Turnover	global.turnover	Global developers turnover with respect to the previous temporal window
code.turnover	Collaboration developers turnover with respect to the previous temporal window
core.global.turnover	Core global developers turnover with respect to the previous temporal window
core.mail.turnover	Core communication developers turnover with respect to the previous temporal window
core.code.turnover	Core collaboration developers turnover with respect to the previous temporal window
ratio.smelly.quitters	Ratio of developers previously involved in any Community Smell that left the community
Social network analysis metrics	closeness.centr	SNA degree metric of the global DSN computed using closeness
betweenness.centr	SNA degree metric of the global DSN computed using betweenness
degree.centr	SNA degree metric of the global DSN computed using degree
global.mod	SNA modularity metric of the global DSN
mail.mod	SNA modularity metric of the communication Developers Social Network
code.mod	SNA modularity metric of the collaboration Developers Social Network
density	SNA density metric of the global Developers Social Network
As anticipated in Section 3, we then address 
 by running an information gain measure (Quinlan, 1986) able to quantify the gain provided by each feature to the prediction. More formally, let  be a supervised learning model, let 
 be the set of socio-technical metrics composing , an information gain measure (Quinlan, 1986) applies the following formula to compute a measure which defines the difference in entropy from before to after the set  is split on an attribute 
: (1)
where the function  indicates the entropy of the model that includes the predictor 
, while the function 
 measures the entropy of the model that does not include 
. As for entropy, this is computed using the Shannon’s entropy (Shannon, 1948), which computes the probability that the predictor 
 affects the dependent variable’s possible outcomes. It is computed as follow: (2)


Table 2. Metrics selection rationale; metrics and rationales follow the same clustering schema from Table 1.

Metric	Rationale
Developer social network metrics	These metrics are largely derived from the works of Meneely et al., 2008, Meneely and Williams, 2011 as well as Joblin et al. (2015a) who, respectively, (1) introduced the fundamental roles of Developer Social Networks in predicting failures (both at technical and organizational level) and (2) prototyped technologies for the verifiable investigation of fully-formed developer communities. Both research groups find independently that DSN metrics are not only relevant to specify the stability and predictive characteristics of organizational structures but also that they reflect relevant technical characteristics in code which reflect socio-technical issues in the organizational structure.
Socio-technical metrics	These metrics are derived from the state-of-the-art in social software engineering as reflected by the topics discussed in the “Cooperative and Human Aspects of Software Engineering (CHASE)“ and “Social Software Engineering (SSE)” workshop series investigating the social and organizational aspects emerging within software communities, and any relation thereto. Conversely, more mature and prominent sample research exists which highlights the relation between these metrics and sub-optimal conditions in the organizational structure, e.g., Kwan et al. (2011) who investigate build failures connected to sub-optimal coordination patterns or Tamburri (2019) who studies sub-optimal architectural decision patterns connected to incommunicability.
Core-community members metrics	Metrics in this cluster range between the truck-factor (whose relation to sub-optimal organizational conditions is highlighted by Avelino et al. (2017) as well as others Ricca et al. (2011)) to core–periphery numbering metrics, which are largely inspired by the work of Manteli et al., 2014a, Manteli et al., 2014b who investigate core–periphery organizational (anti-)patterns and their relation with community member descriptors as well as core–periphery mismatches.
Turnover metrics	These metrics are derived directly from the original committers of the studies reported in Palomba et al., 2018, Tamburri, 2019, Tamburri et al., 2019 who desired to investigate their turnover rates across several distributed software development sites across the world. Such industrial requirement to focus on Turnover as a key dimension was mapped to all aforementioned core-community members metrics.
Social-network analysis metrics	These metrics are derived from the state of the art in organizations’ research and reflect the most relevant centrality measures recurrently used in connection to sub-optimal organizational conditions such as prima-donna effects (e.g., see Dekker, 2014) or lack of boundary-spanning Leifer and Delbecq, 1978, Levina and Vaast, 2005, Du and Pan, 2013.
In simpler terms, the algorithm quantifies how much uncertainty in  is reduced after splitting  on predictor 
. In our work, we implement information gain through the Gain Ratio Feature Evaluation measure (Quinlan, 1986) available in the Weka toolkit (Hall et al., 2009), in combination with the weka.attributeSelection.Ranker search method. This ranks 
 in descending order based on the contribution provided by 
 to the decisions made by . More specifically, the output of the algorithm is represented by a ranked list in which the features having the higher expected reduction in entropy are placed at the top. With this procedure, we can evaluate the relevance of each socio-technical metric in the prediction model. Along with this analysis, we also assess whether a certain feature mainly contributes to the prediction of smelly or non-smelly social structures: this is done through the evaluateAttribute function of the Weka implementation of the algorithm. We run our analyses considering each project independently, thus obtaining 60 different ranks. We prefer this option since it allows us to both assess if certain metrics are more powerful on specific projects and evaluate the overall relevance of each metric. To analyze the resulting ranks and have statistically significant conclusions, we finally exploit the Scott–Knott Effect Size Difference (ESD) test (Tantithamthavorn et al., 2017). This represents an effect-size aware variant of the original Scott–Knott test (Scott and Knott, 1974) that has been recently recommended for software engineering research (Kabinna et al., 2016, Li et al., 2016, Tantithamthavorn et al., 2015) because it (i) uses hierarchical cluster analysis to partition the set of treatment means into statistically distinct groups according to their influence, (ii) corrects the non-normal distribution of an input dataset, and (iii) merges any two statistically distinct groups that have a negligible effect size into one group to avoid the generation of trivial groups. To measure the effect size, the tests uses the Cliff’s Delta (or ) (Grissom and Kim, 2005). We employ the publicly available ScottKnottESD implementation4 originally developed by Tantithamthavorn et al. (2017). Finally, it should be noted that all metrics we are using as predictors in our machine learning exercise are independent from the metrics used for the detection of the golden-set we are using in our dataset and, as such, there is no risk of over-fitting or misleading results given by the inter-dependence between dependent and independent variables.

4.2. Analysis of the results
An overview of the results for our first research question is presented in Table 3. As shown, all the considered socio-technical indicators have some predictive power and, indeed, the mean information gain is of at least 0.10. Interestingly, the standard deviation indicates that the results do not consistently vary from project to project, meaning that the metrics represent valid indicators independently from the specificities of a certain software development community.

Among them, socio-technical congruence and communicability are the ones providing the highest information gain for community smell prediction. This was somehow expected since these metrics reflect the coordination and communication level existing in an organization, thus covering the two key aspects of community smells. From a more technical viewpoint, the results obtained for these two metrics can be interpreted as follow. Since the information gain measures the extent to which a certain feature will impact the purity of the dataset, namely how much the feature will help discriminating the dependent variable classes of the model, we can claim that socio-technical congruence and communicability are the metrics that maximize the differences between smelly and non-smelly community structures, meaning that these are the characteristics that most impact the fact that a community structure will be smelly—these will likely be the first two features that a machine learning model will consider when performing predictions. It is worth noting that both the metrics mainly support the prediction of smelly communities, thus confirming that they seem to be good predictors for community smells. The third relevant factor is represented by the core global developers turnover with respect to the previous temporal window (core.global.turnover): its high relevance may indicate that the emergence of community-related problems can be accelerated/reduced when there is a high/low turnover of core developers. This is in line with findings previously shown in organizational and social science research (Dess and Shaw, 2001, Dalton et al., 1981, Glebbeek and Bax, 2004), and suggests that software communities can be, to some extent, subject of similar dynamics as other types of communities. The high relevance of turnover is also visible when considering the other metrics aiming at capturing this aspect: for example, core.mail.turnover, core.code.turnover, and ration.smelly.quitters have high information gain values (0.47, 0.37, and 0.37, respectively). Hence, on the one hand our results indicate that turnover metrics may potentially provide a notable amount of information bits to a machine learning model that would allow it to better discern community smelly patterns. On the other hand, we can also claim that turnover metrics have a potentially less predictive power than coordination and communication ones: in other words, our findings seem to suggest that keeping socio-technical congruence and communicability aspects under control is more important for the emergence of community smells than the fact of having developers entering/leaving the community—this is again an indication that the implementation of good governance mechanisms may increase the health of the community independently from the specific developers composing it (Dalton et al., 1981, Tamburri et al., 2019a).

According to our findings, also some typical social-network analysis metrics, like degree.centr (info gain  0.53) and density (0.41), are relevant factors to consider, meaning that even the structure of the community can impact the emergence of community-related problems. On the other hand, metrics capturing the truck factor, i.e., the number of team members that have to suddenly leave a project before it stalls, provide a lower information gain. As an example, the code.truck metric have an information gain of 0.16 with a standard deviation of 0.10, which indicates that in some cases it is almost irrelevant for predicting community smells. The low amount of information provided by truck factor metrics seem to confirm, again, that the emergence of community smells can be ruled by putting in place effective governance mechanisms that stimulate the coordination/communication among developers rather than having an excessive control on factors involving individual developers that may, in this case, leave the project at a certain point in time. From a statistical point of view, the discussion above is confirmed. The ranking provided by the Scott–Knott ESD test is exactly the same as the one of the Gain Ratio Feature Evaluation, meaning that the contribution given by the considered socio-technical metrics is statistically significant.


Table 3. Results outline for 
; column 1 identifies the analyzed metric, Column 2 and 3 provide Mean and Standard Deviation respectively while Column 4 elaborates on smelliness and finally Column 5 elaborates on results from the Scott–Knott test. The SK-ESD values reported refer to the percentage of datasets in which a certain feature appeared to be at the top rank.

Metric	Mean	St. Dev.	Class	SK-ESD
st.congruence	0.54	0.05	smelly	74
communicability	0.53	0.03	smelly	67
core.global.turnover	0.53	0.10	smelly	62
degree.centr	0.53	0.07	non-smelly	59
core.global.devs	0.48	0.03	non-smelly	39
perc.ml.only.devs	0.47	0.06	smelly	46
core.mail.turnover	0.47	0.03	non-smelly	46
devs	0.46	0.09	non-smelly	44
ml.code.devs	0.46	0.09	smelly	40
mail.truck	0.44	0.07	non-smelly	40
perc.ml.code.devs	0.43	0.06	non-smelly	38
density	0.41	0.04	smelly	35
code.only.core.devs	0.40	0.06	non-smelly	30
perc.code.only.devs	0.39	0.04	non-smelly	32
sponsored.core.devs	0.39	0.10	smelly	32
core.code.turnover	0.37	0.09	smelly	32
ratio.smelly.quitters	0.37	0.07	non-smelly	32
code.mod	0.37	0.07	non-smelly	31
ratio.mail.only.core	0.36	0.08	non-smelly	29
closeness.centr	0.36	0.06	smelly	27
ratio.sponsored.core	0.34	0.10	non-smelly	24
global.turnover	0.34	0.06	smelly	16
mail.mod	0.34	0.09	non-smelly	9
code.turnover	0.32	0.05	non-smelly	8
sponsored.devs	0.31	0.10	non-smelly	1
ratio.code.only.core	0.31	0.10	non-smelly	1
ratio.ml.code.core	0.30	0.07	non-smelly	1
global.mod	0.30	0.04	non-smelly	1
global.truck	0.26	0.03	smelly	1
betweenness.centr	0.24	0.03	non-smelly	1
core.code.devs	0.22	0.06	non-smelly	1
ratio.smelly.devs	0.22	0.04	smelly	1
ml.only.devs	0.20	0.06	non-smelly	1
code.only.devs	0.20	0.08	smelly	1
ratio.sponsored	0.17	0.06	non-smelly	1
mail.only.core.devs	0.17	0.07	non-smelly	1
code.truck	0.16	0.10	smelly	1
ml.code.core.devs	0.14	0.04	non-smelly	1
num.tz	0.13	0.09	non-smelly	1
core.mail.devs	0.10	0.09	smelly	1
To broaden the scope of the discussion, the results obtained from this research question tell us that community smells are a multifaceted phenomenon that is characterized by various aspects not only related to coordination and communication, but also to turnover and community structure. As such, we argue that more research on the topic should be conducted in order to provide an improved understanding of how these aspects influence the emergence of community smells.


Download : Download high-res image (103KB)
Download : Download full-size image
5. RQ
–RQ
. assessing community smell prediction models
This section overviews the methodological details and the results for 
 and 
.

5.1. Research methodology
To address both research questions, we need to devise community smell prediction models. This requires the design and definition of a number of steps, which we describe in the following.

Dependent Variable.
In our work, we have two types of response variables. In the first case, we consider a binary variable in the set {true, false}, which represents the presence/absence of a community smell. Thus, we first assess how well can a machine learning model predict the simple presence of a community smell, independently from its specific type. In the second case, we consider the problem of classifying the precise smell affecting a software community: as such, the response variable is a nominal value that can assume values in {organizational-silo, black-cloud, lone-wolf, bottleneck, none}, where none indicates non-smelly social structures.

Independent Variables.
The features used to predict community smells are the socio-technical metrics previously presented in Table 1. However, in this context we take into account the problem of multi-collinearity (O’brien, 2007), which appears when two or more variables of the model are highly correlated to each other, possibly leading the machine learner not to distinguish which of them should consider when predicting the dependent variable. We compute the Spearman’s rank correlation (Dodge, 2008) between all possible pairs of metrics to determine whether there are pairs strongly correlated (i.e., with a Spearman’s   0.8). If two independent variables are highly correlated, we exclude the one having the least predictive power, as measured in 
 through the Gain Ratio Feature Evaluation measure (Quinlan, 1986).

Machine-learning Algorithm.
To the best of our knowledge, this is the first work that aims at predicting the emergence of community smells. As such, still nothing is known on how different machine learning algorithms work in this context. For this reason, we experiment with five different classifiers, namely Random Forest, J48, Logistic Regression, Decision Table, and Naive Bayes. These algorithms make different assumptions on the underlying data as well as have different advantages and drawbacks in terms of execution speed and overfitting (Bishop, 2006), thus giving us the possibility to explore deeper the problem of community smell prediction. It is important to point out that before running them, we configure their hyper-parameters as recommended in literature (Tantithamthavorn et al., 2018); for this task, we exploit the Grid Search algorithm (Bergstra and Bengio, 2012).

Training Strategy.
In the context of 
 we aim at building and evaluating a within-project model. To this aim, we employ a release-by-release training approach, where the data of a release 
 is used to train a model that can predict the emergence of community smells on the release 
. The process is repeated for each pair of releases of the considered projects until the end of the observed history. Of course, we have to exclude first and last release of each project from the validation: the former has no previous data to use as training, while the latter has no future data to use as testing. We opt for this time-sensitive strategy since our data follows a temporal distribution: this implies that other widely-used validation approaches (e.g., 10-fold cross Zhang, 1993) do not represent valid alternatives, as they would potentially train the model with data coming from releases that are subsequent with respect to the data in the test set—thus leading to unrealistic and biased results (Krstajic et al., 2014). Furthermore, a release-by-release strategy can actually simulate a real-case scenario where a prediction model is updated as soon as new information is available.

When training the experimented machine learning techniques, we take into account the data imbalance problem (Chawla et al., 2002), which occurs when the number of data points available in the training set for a certain class (e.g., the number of smelly instances) is far less than the amount of data points available for another class (e.g., the number of non-smelly instances), possibly reducing the ability of machine learning algorithms to learn how to classify the minority class. This problem eventually occurs in our case, as, on average, only 12% of community structures of the considered releases are smelly. Hence, we apply the Synthetic Minority Over-sampling Technique (SMOTE) (Chawla et al., 2002). We use the standard implementation of the algorithm available in Weka: as such, the parameter K, which refers to the number of nearest neighbors that the algorithm should use while oversampling the training data, is equal to 5. When predicting the presence/absence of community smells, the algorithm creates synthetic instances of any kind of community smells. Instead, when predicting the smell type, it is repeated multiple times so that it can over-sample the instances of each community smell. More precisely, in the first case the algorithm has been run once per release—we used a release-by-release strategy, hence the training set should have been balanced at each release. In the second case we run it once per smell per release, i.e., we ensured that the same proportion of a certain community smell was available in all the releases considered. In both cases, the algorithm gives us a balanced training set, where all classes have a similar amount of instances. For the sake of completeness, it is important to remark that we iteratively apply SMOTE on each release 
 used as training set, leaving the test set intact, so respecting a real case scenario where the number of smelly and non-smelly structures is not balanced.

Turning the focus on 
, in this case we are interested in assessing a cross-project model. We employ a leave-one-out validation strategy (Krstajic et al., 2014): we use a project 
 as test set, while we train the experimented machine learning algorithms with data coming from all the remaining projects. This process is then repeated 60 times, so that we let each project be the test set once. In a cross-project scenario, there are three problems to consider. First, the distribution of the socio-technical metrics can substantially differs from project to project, meaning that a machine learner could potentially fail in learning because of the influence of outliers and/or other anomalies (Flach, 2012). For this reason, we scale and normalize the computed metrics using the normalize function available in the Weka toolkit. Second, cross-project models can suffer from data unbalance as well: so, also in this case we iteratively apply SMOTE to balance the training data—we run it once for each training set built. Finally, cross-project models are sensitive to the selection of the training data. Indeed, heterogeneous information coming from very different projects with respect to the tested one could have a negative influence on the predictive ability of the machine learner. While there is some ongoing research on this topic (Peters et al., 2013, Krishna et al., 2016, Krishna and Menzies, 2018), we could not find a mature enough and/or publicly available instrument that could allow us a proper project selection for training our learners. For this reason, it can be said that our work sets the lower-bound for cross-project community smell prediction. Should our results be already promising, this would imply that even better prediction performance could be obtained by practitioners if a more careful selection of the training data would be performed.

Evaluation Metrics.
We assess the goodness of the experimented models by computing well-known metrics such as precision, recall, F-Measure, AUC-ROC, and Matthew’s Correlation Coefficient (MCC) (Reich and Barai, 1999). These metrics allow us to study the performance of both within- and cross-project community smell prediction models under different perspectives.

5.2. Analysis of the results
As explained above, we experimented with five different machine learning algorithms such as Random Forest, J48, Logistic Regression, Decision Table, and Naive Bayes. Such an experiment revealed that Random Forest is the technique obtaining the best performance considering all the evaluation indicators and overcome J48, i.e., the second best performing technique, by up to 7% and 5% in terms of F-Measure and AUC-ROC, respectively. It is also worth remarking that the model built using Random Forest performed, overall, 11% better with respect to model built using the same classifier but using all features: this confirms that the feature selection process applied actually provided gains in terms of performance. Based on the observations above, we therefore decided to focus the discussion on the results achieved by Random Forest, while a comprehensive overview of the performance of the other classifiers is available in our online appendix (Palomba and Tamburri, 2019).

Within-project community smell prediction. As a preliminary methodological step, we verified possible multi-collinearities among the features of the model. This led to the removal of five variables, namely ratio.sponsored (it had a correlation of 0.86 with ratio.sponsored.core and was removed because it provides a lower information gain), num.tz (because of its correlation of 0.82 with code.only.devs), devs (because of the 0.92 correlation value with perc.ml.only.devs), ratio.mail.only.core (because of the correlation of 0.87 with ratio.smelly.quitters), and betweenness.centr (correlation of 0.83 withdegree.centr).

Fig. 3 depicts box plots describing the distribution of F-Measure, AUC-ROC, and MCC when running the community smell prediction model, trained using within-project data, over the considered software projects. The figure reports both the performance of the model used to predict a binary value reporting presence/absence of community smells (on the left-hand side) and the model exploited to predict the types of community smells appearing in next project releases (on the right-hand side). Looking at the figure, we can immediately point out that, overall, the performance of the model is high when considering all the evaluation metrics. Indeed, it reaches a median F-Measure is 77% when predicting the existence of community smells, while the F-Measure is equal to 63% when trying to identify the exact type of smell that will occur. The results are consistent among the other metrics. On the one hand, these results indicate that a within-project approach allows a pretty accurate prediction of community smell emergence. On the other hand, the AUC-ROC values – which measure how well the classifiers can separate the binary classes (Hanley and McNeil, 1982) – tell us that within-project models are also robust. As such, we can answer 
 by saying that a within-project solution can properly support developers and project managers when assessing the health status of a community. An interesting observation can be done when considering the MCC values of the binary model when compared to the one aiming at predicting community smell types. As shown in Fig. 3, we observe that, despite the higher average accuracy, the variability of the first is larger, meaning that there exist software projects where it is easier to predict the exact type of community smells rather than their simple presence. Differently from the other evaluation metrics, MCC is computed using the entire confusion matrix and, therefore, it takes into account the ability of a learner to classify false and true negatives. The results tell us that the binary model has sometimes more difficulties in classifying actual community smells (false negatives) and smell-free community structures (true negatives) with respect to the model that classifies community smell types. This can be explained by considering that the binary model does not aim at distinguishing the characteristics of the individual smells. As a consequence, it learns more general features that define a community smell structure: based on what we found, this sometimes biases the model, that cannot properly classify actual community smells and smell-free community structures. On the contrary, the finer-grained model is defined to learn the features characterizing single community smells, possibly helping the machine learner to use the right features when classifying true and false negatives.


Download : Download high-res image (143KB)
Download : Download full-size image
Fig. 4. Box plots reporting the performance of the investigated cross-project community smell prediction models in terms of F-Measure, AUC-ROC, and MCC.

Going deeper into the results, we noticed that predicting the existence of community smells in future releases of a system represents an easier task with respect to the prediction of the precise type of problem that will occur, as visible in the difference of 14% in terms of F-Measure. This was somehow expected (and reasonable) for various reasons. In the first place, a binary classification allows the machine learner to have more data points to learn and, perhaps more importantly, the task of learning the characteristics of just two classes is generally easier for machine learning algorithms (Sokolova and Lapalme, 2009). Secondly, more community smell types can be determined by the same set of socio-technical metrics, thus creating noise during the classification. This claim is supported by an additional analysis that we have done on the features used by Random Forest to predict the different types of community smells. In particular, we measured the Gini index (Gastwirth, 1972) – the entropy-based metric used by the classifier to decide which features it should use for prediction – obtained by the socio-technical metrics when used to predict the different types of community smell types. As a result, we discovered that socio-technical congruence and core.global.turnover are the main metrics used for predicting both Organizational Silo and Black Cloud, while communicability has a high impact on the decisions made by the classifier when identifying Lone Wolf and Bottleneck. As such, the false positive rate increases, thus reducing the accuracy.

Finally, we observed that the performance of both model types does not vary over different projects; looking at the shapes of the box plots, which are all narrowed toward the median of the distributions, we can conclude that the prediction accuracy is similar independently from the underlying development community.


Download : Download high-res image (130KB)
Download : Download full-size image
Cross-project community smell prediction. As previously done, we first controlled for multi-collinearity. In this case, with the step we removed seven variables, namely: ratio.sponsored (correlation of 0.93 with ratio.sponsored.core), num.tz (correlation of 0.81 with code.only.devs), devs (correlation of 0.85 with perc.ml.only.devs), ratio.mail.only. core (correlation of 0.97 withratio.smelly. quitters), betweenness.centr (correlation of 0.86 with degree.centr), perc.code.only.devs (correlation of 0.94 with code.only.core.devs), and core.mail.devs (correlation of 0.9 with ml.code.core.devs).

Fig. 4 reports the results of this analysis. Similarly to 
, we show the performance of cross-project models when used to predict a binary value on presence/absence of community smells (left-hand side of Fig. 4) and the exact type of community problem (right-hand side of Fig. 4).

The results are somehow similar to what reported for within-project models. Also in this case the binary model works better than the one classifying community smell types: the median F-Measure of the former reaches 62%, while the one of the latter is 58%. The difference is similar when considering AUC-ROC (60% vs 57%) and MCC (59% vs 58%). As clearly visible, however, the overall accuracy of cross-project models is notably lower than within-project ones. This is likely due to the heterogeneity of the data contained in the training set; this leads us to confirm previous findings in the field on the lower performance of cross-project prediction models (Di Nucci et al., 2017, Di Nucci et al., 2018b, Zimmermann et al., 2009, Rahman et al., 2012).

At the same time, the performance is still to be considered promising: even without selecting the data to use as training, the cross-project model obtains a prediction accuracy that is higher than the one of a random model. This statement is supported by an additional experiment we have performed, in which we compared the model with a simple classifier that randomly guesses the smelliness of a community—note that also in this case we applied SMOTE first to avoid having a too relaxed baseline. We found that the random model has an F-Measure close to 46%, being therefore worst than the cross-project model devised. Based on these findings, we can argue that more research on the topic could lead to potential additional benefits when it turns to the prediction of community smells using cross-project data.

When considering the prediction of community smell types, the performance drops substantially. In this case, the heterogeneity issue combined with the ability of certain socio-technical metrics to influence more community smells are at the basis of the reduced performance. Also in this case, we tried to understand how bad this performance is when compared with the one of a random model. We found that the devised cross-project model has an F-Measure 27% higher than the baseline (which reached a median F-Measure of 31%), confirming that it still represents a more viable approach for being used in practice. In conclusion, our model poses the ground for more research on effective mechanisms to make cross-project community smell prediction practical.

When comparing the results of within- and cross-project models, we can notice that the variability of the performance is larger in the first case—this is true for all considered evaluation metrics. This means that, while the performance of within-project models is generally higher, the latter seem to be more stable. From a practical standpoint, our findings tell us that a cross-project setting guarantees similar performance when employed in different contexts: this result is pretty promising in our view, since it implies that further improvement in the way the model is trained (e.g., proper selection of the projects to be used as training data) could not only lead to better performance, but also to the definition of models that can be pragmatically used by developers in unseen contexts.


Download : Download high-res image (160KB)
Download : Download full-size image
5.3. Cross-study findings
The results outlined in the previous section are in line with observations made in previous work (Tamburri et al., 2019). More specifically, in Tamburri et al. (2019), we observed that a restricted set of 18 socio-technical metrics from the same overview we adopt here, directly correlated – either positively or negatively – with community smells; beyond this contribution, the manuscript proceeded in highlighting all observable stability thresholds for at least some of the factors we also adopt in the scope of this work (e.g., a minimum value of 0.8 for the truck-number of the collaboration DSN was reported as a prerequisite for stability).

Among the aforementioned 18 factors, three key forces emerge, namely, socio-technical congruence, communicability, and truck-factor (as well as connected turnover metrics) and are reported – both in this work and in Tamburri et al. (2019) – as strongly correlated with the emergence and prediction of no less than 2 community smells. Said metrics are therefore set to become first-class citizens in terms of software metrics needed to track and govern software projects appropriately (e.g., as augmentations of platforms such as Bitergia or OpenHub). On the one hand, low numbers across the board on such metrics indicates that corrective action needs to be undertaken in the communication and/or collaboration structure across the project. On the other hand, high numbers on the aforementioned metrics may themselves represent conditions whose impact on software processes remains to be fathomed.

Concerning all remaining metrics, nothing definitive can be said. While on previous study we reported all metrics being connected to some form of socio-technical condition, the observations made in this study – although confirmative of previous observations – does not exclude any effect being enacted by the dimensions represented in those other metrics with respect to the forces at play, i.e., community smells. Further work into figuring out all necessary software people metrics to measure, predict, and manage social debt – and connected forces – is needed.

6. Threats to validity
There are a number of threats to validity that could have biased our results.

Construct Validity. The first threat in this category concerns with the correctness of the dataset exploited in the study. In this respect, we relied on a publicly available source that we built in the context of our research (Tamburri et al., 2019b), which contained labeled data of both socio-technical metrics and community smells.

Another potential threat is related to the selection of the independent variables used to build the experimented models. We exploited socio-technical metrics that have been shown to correlate with the occurrence of community smells (Tamburri et al., 2019b) and, therefore, our selection was based on previous findings. Nevertheless, we cannot rule out that other metrics, not considered in the study, could provide additional contribution to the performance of community smell prediction models. We plan to investigate this aspect further as part of our future research agenda.

Conclusion Validity. In the context of 
, we measured the predictive power of the considered socio-technical metrics by means of the Gain Ratio evaluation measure (Quinlan, 1986). The usage of this algorithm is recommended since it can quantify the amount on predictive uncertainly that is reduced using a specific feature (Flach, 2012), providing us with a mechanism able to rank features based on their importance that we could use to interpret the predictive power of the exploited socio-technical metrics. Finally, we backup our observations through the use of a statistical test, i.e., the Scott–Knott ESD (Tantithamthavorn et al., 2015), that confirmed the results given by the Gain Ratio evaluation measure.

As for 
 and 
, a first threat is related to the methodology applied to discard redundant features from the models built. We first relied on the Sperman’s correlation coefficient (Dodge, 2008) to identify pairs of features strongly correlated to each other and, secondly, we excluded one of them by considering the gain they provided to the model (computed using the information gain measure). In so doing, we could only take into account the intra-metric correlation between features, but not their complementarity: for instance, it would be possible that two features taken together offer higher gain compared to the gain obtained by a single feature. While some recent work has proposed heuristics to identify such complementarity relations (Singha and Shenoy, 2018), we could not find any open implementation that would have allowed us to apply them in our context. Hence, we leave to future research work the understanding of how this methodological choice impacts our findings. In the second place, it is worth discussing the implications of the data balancing method employed, i.e.,SMOTE (Chawla et al., 2002). The process it uses to create synthetic instances is stochastic and, therefore, the particular execution on our dataset could have led to inconclusive results. To address this potential threat, we conducted an additional analysis in which we ran SMOTE multiple times and verified the resulting performance. We observed that the performance are in line with those reported in Section 5, with a standard deviation of 0.03, overall. We could therefore conclude that the performance discussed in the paper were not obtained by chance.

As for the interpretation of the performance achieved by the models built. We mitigated this problem by considering more than one evaluation metric (e.g., F-measure, AUC-ROC, and MCC). Also, previous work has shown the importance of considering data pre-processing actions to properly set machine learning models (Tantithamthavorn et al., 2018, Tantithamthavorn and Hassan, 2018, Jiarpakdee et al., 2019, Pecorelli et al., 2019a). For this reason, in our research we considered the application of techniques to deal with data normalization, feature selection, data balancing, and hyper-parameters configuration.

In addition, the validation strategy may be object of discussion: when building the within-project model, we adopted a release-by-release strategy because our data follows a temporal order. Furthermore, it simulates a real-case scenario where a prediction model is updated as soon as new data from a previous release are available (Palomba et al., 2018). Hence, the selected validation strategy was the only one actually suitable in our context. When building a cross-project solution, the main threat is related to the way we train the model. In particular, we could not deal with the sensitivity of cross-project models to heterogeneous data and, as such, we use all the projects (but the tested one) within the training set. We are aware of this limitation but, unfortunately, there is still no instrument that allows an accurate selection of training data for cross-project models and/or that is publicly available, despite the ongoing research on the topic (Peters et al., 2013, Krishna et al., 2016, Krishna and Menzies, 2018). Nevertheless, we (i) argue that the reported results represent a lower-bound, yet promising ground for further studies on cross-project community smell prediction and (ii) encourage future research on the matter.

External Validity. With respect to the generalizability of the results, our study considers 60 open-source communities that are all active but different in terms of size, scope, number of contributors, and domain. Yet, we recognize that the performance of the experimented models may differ on other datasets. Hence, we encourage replications of our study targeting different communities as well as closed-source environments.

It is also important to comment the fact that recent work  (Jiarpakdee et al., 2020) has shown that the top-ranked metrics from the Scott–Knott ESD test may vary among instances of predictions. As a consequence, it is possible that the results of 
 represent a summary of the models, but not provide specific explanation to the predictions. Hence, the results may not be generalized to all projects. We recognize this limitation and, as part of our future research agenda, we plan to address it by means of explainable AI methods, which is a rising research field in the context of software engineering and that may be a useful instrument to provide more insights into the generalizability of the results of our study.

7. Conclusion
In this paper, we aimed at studying how well can community smells be predicted using a set of socio-technical metrics that previous work (Tamburri et al., 2019b) has shown to be correlated with the phenomenon. To this aim, we carried out an empirical investigation – involving 60 open-source projects – in which we (1) measured the actual predictive power of socio-technical metrics and (2) assessed the performance of within- and cross-project models for community smell prediction. As a side effect of our study, we also verified the capabilities of various machine learning algorithms. The main results of our study showed that socio-technical congruence, communicability, and turnover-related metrics are top-factors to monitor when predicting the emergence of community-related problems. Furthermore, within-project models can be effectively exploited by practitioners to predict community smells, especially when trained using Random Forest as classifier. Indeed, our experiment showed that it is possible to achieve an F-Measure of 77%. Finally, in case of lack of historical data, cross-project models represent a promising solution (F-Measure of 62%), yet they still need improvement, e.g., by means of the adoption of proper techniques for selecting training data.

These findings represent the main input for our future research agenda, which includes a replication of our study in an industrial context as well as the definition of ad hoc mechanisms able to improve the performance of community smell prediction models (e.g., though the use of explainable AI methods to improve the model explainability Jiarpakdee et al., 2020). Furthermore, we will consider the design of empirical studies aiming at understanding the impact of different feature selection methods, able to capture the complementarity relation among features (e.g., the adaptive heuristic proposed by Singha and Shenoy Singha and Shenoy, 2018), on the reported results. We also plan to investigate other features that can be used to boost the performance obtained: for instance, we plan to investigate whether and how pure technical-related metrics (e.g., the number of packages) can be used to inform our models and better predict the emergence of community-related issues. At the same time, the major limitation we perceive in terms of action ability of our results – to be addressed in the future – is that, on the one hand, we obtained a useful and usable ML model which is fully reproducible but, on the other hand, we do not provision the model on a DataOps pipeline that practitioners could use. We plan this DataOps-based increment of our ML modeling exercise as a future work.

