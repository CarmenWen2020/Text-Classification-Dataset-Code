Abstract
Researcher bias occurs when researchers influence the results of an empirical study based on their expectations, either consciously or unconsciously. Researcher bias might be due to the use of Questionable Research Practices (QRPs). In research fields like medicine, blinding techniques have been applied to counteract researcher bias. In this paper, we present two studies to increase our body of knowledge on researcher bias in Software Engineering (SE) experiments, including: (i) QRPs potentially leading to researcher bias; (ii) causes behind researcher bias; and (iii) possible actions to counteract researcher bias with a focus on, but not limited to, blinding techniques. The former is an interview study, intended as an exploratory study, with nine experts of the empirical SE community. The latter is a quantitative survey with 51 respondents, who were experts of the above-mentioned community. The findings from the exploratory study represented the starting point to design the survey. In particular, we defined the questionnaire of this survey to support the findings from the exploratory study. From the interview study, it emerged that some QRPs (e.g., post-hoc outlier criteria) are acceptable in certain cases. Also, it appears that researcher bias is perceived in SE and, to counteract researcher bias, a number of solutions have been highlighted. For example, duplicating the data analysis in SE experiments or fostering open data policies in SE conferences/journals. The findings from the interview study are mostly confirmed by those from the survey, and allowed us to delineate recommendations to counteract researcher bias in SE experiments. Some recommendations are intended for SE researchers, while others are purposeful for the boards of SE research venues.

Keywords
Researcher bias
Experimenter bias
Survey
Blinding

1. Introduction
In research, bias is defined as the combination of various design, data, analysis, and presentation factors tending to produce findings that should not be produced (Ioannidis, 2005). Researcher bias, or experimenter bias, occurs when the researcher (consciously or unconsciously) influences the results of an empirical study based on their expectations.

In some cases, researcher bias is due to the use of Questionable Research Practices (QRPs) to follow one’s agenda and achieve specific expectations—e.g., changing the procedure for excluding data after looking at the impact of doing so on the results. Another form of bias is publication bias, which occurs when studies are published based on their results—usually positive results are more likely to be published than negative ones (Dwan et al., 2008).

To counteract researcher bias, according to established guidelines in Software Engineering (SE), researchers should disclaim their stance regarding an outcome. For example, Wohlin et al. (2012) and  Sjøberg and Bergersen (2021) consider experimenter expectancies as a threat to validity in SE experiments.

In this paper, we present the results of two studies, an interview study (Romano et al., 2020) and a survey, to increase our body of knowledge about researcher bias in human- and technology-oriented SE experiments.1 The interview study, intended as an exploratory study, aimed to gather the opinions of a group of experts about themes related to researcher bias in SE experiments. To collect data, we used semi-structured interviews. In particular, we interviewed nine experts of the empirical SE field. The interviews were concerned with: QRPs potentially leading to researcher bias, causes behind researcher bias, and possible actions to counteract it. Regarding the possible actions, we focused on (but not limited to) two blinding techniques, namely: blind data extraction and blind data analysis. The former consists of hiding some information (e.g., treatment assignment) from the researchers who extract the data; while, the latter is the temporary and judicious removal of labels and/or alteration of values before someone analyzes the data (MacCoun and Perlmutter, 2015). Although extensively used in other research fields like medicine and physics (MacCoun and Perlmutter, 2015, Karanicolas et al., 2010), SE researchers have used these techniques only in few occasions (Fucci et al., 2016, Sigweni and Shepperd, 2015).

The findings from the interview study represented the starting point to design our survey. In particular, we built a series of statements based on the findings from the interview study and then gathered, through a questionnaire, the level of agreement of experts in conducting SE experiments about these statements. The goal of the survey was to support the findings from the interview-based one. This methodological approach was inspired by past work in the SE research field (e.g., Francese et al., 2017, Murphy-Hill et al., 2014, Falessi et al., 2018).

This paper extends the one by Romano et al. (2020), presenting the findings from the interview study on researcher bias in SE experiments, as follows:

–
It adds a new study, a survey with experts in the empirical SE field, aiming to support the findings from the interview study.

–
It extends the discussion of the results by taking into account both interview study and survey.

Paper Structure. In Section 2, we summarize work related to ours. In Section 3, we present the design of both interview study and survey. The findings emerging from these two studies are shown in Section 4. In Section 5, we discuss the results, offering recommendations based on both studies, as well as possible limitations. Finally, we draw conclusions in Section 6.

2. Background
This section considers current relevant literature focusing on QRPs and researcher bias. We also illustrate some countermeasures adopted to deal with researcher bias, including blinding techniques.

2.1. Questionable research practices and researcher bias
Cases of QRPs, exploiting the gray area of what is considered acceptable, have been mounting in medicine, natural sciences, and psychology (e.g., Fanelli et al., 2017, John et al., 2012). As for the SE research field, Jørgensen et al. (2016) documented the presence of researcher bias and publication bias in SE experiments. The authors conducted a quantitative questionnaire-based survey, with researchers from some SE sub-communities, comprising questions about QRPs potentially leading to researcher bias and publication bias. Three out of seven questions were on QRPs related to researcher bias, namely:

1.
Post-hoc hypotheses—defined as reporting the results of one (or more) hypothesis tests where at least one of the hypotheses is formulated after looking at the data.

2.
Post-hoc outlier criteria—defined as developing or changing the rules for excluding data (e.g., outlier removal) after looking at the impact of doing so on the results.

3.
Flexible reporting of measures and analysis—defined as using several variants of a measure or several tests and then reporting only the measures and tests that give the strongest results.

The authors gathered 34 responses and found that: (i) 67% of the respondents had followed the post-hoc hypotheses practice; (ii) 55% had followed the post-hoc outlier criteria practice; and (iii) 69% had followed the flexible reporting of measures and analysis practices. Jørgensen et al. (2016) also built a model—based on 150 randomly-sampled SE experiments—to estimate the proportion of correct results at different levels of researcher bias and publication bias. The model suggests that both researcher bias and publication bias affect SE experiments since 52% of the statistically significant tests do not match a situation with no or low researcher bias and publication bias.

Shepperd et al. (2014) in their meta-analysis of defect prediction techniques came to a conclusion similar to that by Jørgensen et al. (2016). The authors pointed out the presence of researcher bias in the studies included in the meta-analysis as the factor with the largest effect was the research group publishing the paper, while the effect of the prediction technique was small.

2.2. Countermeasures to researchers bias
Researchers have proposed solutions to counteract researcher bias (e.g., Pannucci and Wilkins, 2010, Nuzzo, 2015). We can group these solutions into: (i) rival theories; (ii) transparency; and (iii) blinding. The first category consists of considering alternative or competing hypotheses with respect to the ones being tested in the study. The researcher should devise experiments that can explicitly distinguish competing hypotheses and, if possible, develop experiments that can distinguish between alternative theories. It is ideal that the researcher collaborates with a team of rivals—i.e., other researchers that, while being skeptical about the hypotheses, collaborate towards developing alternative explanations.

Several approaches fall under the umbrella of the transparency category. The main example is open science—i.e., the practice of sharing research data, computer code, and lab packages for public scrutiny so attempting to reproduce results. In research fields like medicine or psychology, transparency is also achieved through pre-registration (also known as registered report). It consists of submitting a study proposal presenting the study rationale and planning for peer review before conducting the study. Once the proposal is accepted, the researchers can conduct the study and submit a paper with the obtained results for a second round of revision. The paper cannot be rejected due to the study results (e.g., negative results), while it can be rejected for other reasons (e.g., deviations from the pre-registered analysis procedure) (Nosek and Lakens, 2014).

Finally, blinding (also known as masking) means concealing research design elements (e.g., treatment assignment or research hypotheses) from individuals involved in an empirical study (e.g., participants, data collectors, or data analysts) (Miller and Stewart, 2011, Page and Persch, 2013). Research fields like medicine and physics (MacCoun and Perlmutter, 2015, Karanicolas et al., 2010) have been encouraging the use of blinding techniques to deal with research bias. As for the SE research field, Shepperd et al. (2014) have fostered researchers to use blinding techniques in their studies. However, few researchers have applied blinding techniques in SE studies so far, namely: Fucci et al. (2016) who used blind data extraction and analysis in a human-oriented experiment, and Sigweni and Shepperd (2015) who applied blind data analysis in a technology-oriented experiment.

To explain how blind data extraction and analysis work, we refer to the experiment by Fucci et al. (2016) as an example. The study goal was to assess the impact of Test-Driven Development (TDD) on (i) functional quality of developed programs, (ii) developers’ productivity, and (iii) number of tests written. To that end, the experiment compared a treatment group—i.e., a group of developers who applied TDD to implement some programs—to a control group—i.e., a group of developers who implemented the same programs as the other group but by following Test-Last Development (TLD). Once the experiment was carried out, the raw dataset (i.e., the programs implemented by the developers) was handed over to a researcher playing the role of data extractor. In particular, given the raw dataset, this researcher extracted the values of the metrics (e.g., the PROD metric that quantified developers’ productivity) so obtaining the dataset. The extraction of the metrics was done blindly because the data extractor was aware of neither the experimental goal, hypotheses, treatment assignment, nor design. Next, the dataset was forwarded to two data analysts who performed the analysis (both descriptive and inferential) blindly. This is because they worked on a sanitized dataset and did not know the experimental goal. To sanitize the dataset, the labels of the experimental groups were temporarily replaced (e.g., the TDD group became the A group, while the TLD group became the B group) and the dependent variables were temporarily anonymized (e.g., PROD was renamed as DV1). To correctly analyze the data, the analysts were provided with a minimal description of the dependent and independent variables (e.g., DV1 is a dependent variable assuming values between 0 and 1), as well as the experimental design in which some information was adequately hidden (e.g., the experimental groups were referred to as A and B). The hidden information was disclosed once the analysis was completed (e.g., group A was actually the TDD group).

As mentioned-before, Sigweni and Shepperd (2015) used blind data analysis in a technology-oriented experiment. In particular, they assessed four prediction methods for software effort estimation to demonstrate the practicality of blind data analysis in SE experiments. The analyst did not know the prediction methods to be assessed (i.e., the name of the prediction methods was replaced). Moreover, any analysis was based on absolute residuals. The authors concluded that blind data analysis is a very practical technique that supports more objective analyses of experimental results.

3. Interview study and survey
In this section, we describe the design of both interview study and survey.

3.1. Protocol
For the first step of our research (i.e., the interview study), we opted for interviews as a data collection means, rather than questionnaires, because: (i) they decrease the number of “don’t know” and “no answers”, as the interviewees can ask for clarifications if a question is not clear to them, and (ii) the interviewer can ask for clarifications/details if needed (Wohlin et al., 2012). Also, such a data collection means fits the exploratory intention of our study.

We recruited researchers in our research network, who were experts in conducting (human- and technology-oriented) SE experiments. Nine researchers (also referred to as the interviewees, from here onward) were available to be interviewed either face-to-face or by phone. Each interview session involved the same interviewer (i.e.,the second author) and one interviewee at a time. At the beginning of the interview session, we obtained the consent of the interviewee for audio-recording the session. Also, we informed the interviewee that the gathered data would be treated confidentially. Each interview lasted between 50 and 75 min. We used semi-structured interviews (Wohlin et al., 2012). That is, the questions listed in the interview script were not necessarily asked in order because, depending on how the conversation evolved, some questions were handled before others. Semi-structured interviews allow for improvisation and exploration of the investigated phenomenon. The interview script is roughly a checklist that the interviewer adopts to guide the discussion with the interviewee and make sure that relevant topics are covered (Wohlin et al., 2012). In Fig. 1, we show the interview script.

With the second step of our research (i.e., the survey), we aimed to support the findings from the interview study by gathering the level of agreement of experts in conducting SE experiments about a series of statements we built upon the findings of the interview study. In other words, we aimed to apply a kind of triangulation2 known as methodological triangulation (Thurmond, 2001). Unlike the interview study, the questionnaire-based one is quantitative since it is informed by quantifiable data (i.e., the level of agreement of experts in conducting SE experiments about some statements). We opted for questionnaires as a data collection means because it fits our research purpose—i.e., validating the findings from a past exploratory investigation (e.g., Falessi et al., 2018). Moreover, questionnaires require less effort than interviews and can reach a larger part of the population (Wohlin et al., 2012).


Download : Download high-res image (2MB)
Download : Download full-size image
Fig. 1. Interview Script.

We invited 317 empirical SE experts (or simply researchers, from here onwards) to fill in our (online) questionnaire. In particular, we invited researchers who had published papers in the ESEM3 proceedings in the last three years. We (all authors of this paper) analyzed this list of empirical SE experts to validate and extend it. Each author added researchers (not included in this list) considered as an active researcher on topics related to empirical SE. We focused on ESEM because this conference can be considered the major forum for researchers acting in the context of empirical SE. It is worth mentioning that we did not invite the researchers who had taken part in the interview study because they would be clearly favorable towards the statements we built based on their opinions.

To ask SE experts to participate in the survey, we sent them an invitation letter via email (see Fig. A.11). The letter reported the objective of the survey, the due date to fill in the questionnaire, and the link to the online questionnaire. We also informed the invited researchers that they could freely share the questionnaire with other empirical SE experts. The invitation letter was sent on November 5th 2020. The survey was open for 20 days. We received 64 answers (response rate of 20%), of which 51 answers from respondents reporting to have carried out an experiment in the past. This resulted in a sample size () of 51. Each answer was unique (i.e., the same researcher cannot send two answers) and anonymous.

The questionnaire started with a filter question4 in which we asked the researchers whether they had ever carried out an experiment (human- and/or technology-oriented). This is because our goal was to investigate researcher bias in SE experiments, and we were aware that some respondents could not be experts in conducting experiments (while regarding themselves as experts, for example, in conducting case studies). Respondents who had carried out at least an experiment in the past could continue with the questionnaire, while those who had never carried out an experiment ended the questionnaire immediately.

The first part of the questionnaire (i.e., Demographics) included demographic questions (e.g., the academic position of the respondent or the research outlet where the respondent published her experiments) to better characterize the study context. To increase the response rate, the demographic questions were not mandatory as some respondents could not be willing to share some information such as the research outlet where the respondents published their experiments.

The remaining part of the questionnaire aimed to support the findings from the interview study. To that end, we built a series of statements based on the findings from the interview study. To keep the questionnaire at a reasonable length, we prioritized the statements extracted from the interview study by relevance and included in the questionnaire only those statements we deemed more relevant as suggested in the literature (e.g., Kasunic, 2005). For each statement, respondents had to rate how much they agreed with that statement on a (Likert-type) scale from 1 (i.e., “Strongly disagree”) to 5 (i.e., “Strongly agree”). For example, one of the findings emerging from the interviews is that the post-hoc outlier criteria practice should be avoided because it potentially leads to researcher bias (see Section 4.2). Therefore, we asked the respondents their level of agreement with the following statement: “The post-hoc outlier criteria practice should be avoided because it potentially leads to researcher bias”. As shown in Fig. 2, we arranged these statements into three sections. The answers to these statements were mandatory.

To evaluate the comprehensibility of the questionnaire and reduce as much as possible sources of misunderstanding, we conducted a pilot with two junior researchers (who were not involved in this research and were not invited to participate in the actual survey). Based on pilot feedback, we made changes to improve the clarity of the questionnaire before the survey took place.


Download : Download high-res image (2MB)
Download : Download full-size image
Fig. 2. Statements, arranged by section, we included in the questionnaire.

It is worth remarking that, from here onwards, we refer to the researchers/participants who took part in the interview study as the interviewees, while we refer to those who took part in the survey as the respondents.


Table 1. Characterization of the interviewees.

ID	Institution region	Academic position	Main research interest	Experience as an experimenter	Last published experiment
R1	Southeastern Europe	Assistant professor	Defect prediction	5–10 (years)	 6 months
R2	Northern Europe	Ph.D. student	Human and social aspects of SE	1–5 (years)	 18 months
R3	Northern Europe	Full professor	Mining software repositories	11–20 (years)	 6 months
R4	Northern America	Associate professor	Agile software development	11–20 (years)	 6 months
R5	Central Europe	Assistant professor	Software maintenance and evolution	5–10 (years)	 3 years
R6	Southern Europe	Associate professor	Software economics and metrics	11–20 (years)	 1 year
R7	Southern Europe	Assistant professor	Project and process management	11–20 (years)	 1 year
R8	Southern Europe	Full professor	Collaborative software development	 20 (years)	 18 months
R9	Southern Europe	Full professor	Software economics and metrics	11–20 (years)	 6 months
3.2. Participants
In Table 1, we report some information about the interviewees—this information was gathered through the Warm-up part of the interview (see Fig. 1). To guarantee the anonymity of the interviewees, we refer to each of them through an ID (from R1 to R9). Each interviewee had experience in performing experiments and, at the time of the interview, had published at least one experiment in one of the following SE high-quality venues: ICSE,5 EMSE,6 TSE,7 and/or TOSEM.8 The participants were quite heterogeneous in terms of location of their institution, academic position, main research interest, years of experience as an experimenter,9 and date of the last published experiment. The interviewees were employed in academic institutions located in different regions throughout Europe and North America. At the time of the interview, three interviewees were full professors, two were associate professors, three were assistant professors, and one was a Ph.D. student. R8 (full professor in a Southern European institution) has more than 20 years of experience in conducting SE experiments and had published her last experiment less than 18 months before the interview. Other researchers (e.g., R3, R4, R6, R7, and R9) had more than 10 years of experience in conducting SE experiments with their last experiment published less than one year before the interview. With the exception of R2 (the interviewee in a more junior position), the interviewees had more than five years of experience in conducting SE experiments. Only in one case (R5), the last experiment was published more than 18 months before the interview (but less the 3 years before the interview). The main research interest of the interviewees spanned across different sub-fields of SE, from human aspects to mining software repositories.

As for the respondents, we report some information about them—this information was gathered through the Demographics part of the questionnaire—in Table 2. As this table shows, most respondents (27) shared the location of the institution which they worked for. These respondents worked for institutions located in 11 different countries. The most represented country was Brazil (with five responses). The respondents were, for the most part, senior researchers (21 full professors and 10 associate professors). Most respondents (40) had more than five years of experience in conducting experiments. More than half of the respondents (28) had published their last experiment less than six months before they filled in the questionnaire, while the remaining ones had published their last experiment within the last three years. The majority of the respondents usually published their experiments in conferences (32) and journals (15). As for the former, the most preferred venues were ESEM, ICSE, and ESEC/FSE.10 As for the journals, the most preferred venues were EMSE, IST,11 and TSE. The respondents have, for the majority, experience with human-oriented experiments only (22). Seventeen respondents have experience with both kinds of experiments, while 12 respondents have experience with technology-oriented experiments only.


Table 2. Characterization of the respondents.

Characteristic	Values (Frequencies)
Institution county	Not provided (24), Brazil (5), Germany (4), Netherlands (3), Sweden (3), Canada (2), Spain (2), United States (2), Afghanistan (1), Australia (1), Estonia (1), Italy (1), Serbia (1), United Kingdom (1)
Academic position	Full professor (21), assistant professors (10), associate professor (10), Ph.D. student (4), post-doc (4), industry researcher (2)
Experience as an experimenter	11-20 years (17), 6-10 years (16), 1–5 years (11),  20 years (7)
Last published experiment	 6 months (28),  3 years (23)
Kind of venue	Conference (32), journal (15), book chapter (1), others (2)
Kind of conducted experiments	Human-oriented experiment only (22), human- and technology-oriented experiment (17), technology-oriented experiment only (12)
3.3. Data analysis
After transcribing the recordings of the interviews, we (i.e.,the first, third, and fourth authors) analyzed the transcripts by using a thematic analysis approach called template analysis, which is known to be flexible and fast (King, 2004). Template analysis allows the investigators to develop a list of codes, each identifying a theme within the transcripts. The codes are arranged in a template—it usually is a hierarchical structure of codes—showing the relationships among themes, as defined by the investigators. In template analysis, the investigators start analyzing the transcripts by using an initial template. That is, they start attaching pre-defined codes, arranged in a template, to delimit portions of text in the transcripts related to the themes. As King (2004) suggests, the best starting point for developing an initial template is the interview script. Accordingly, we developed our initial hierarchical template (see the non-bold text in Fig. 3) from the interview script. As customary in template analysis, we revised the initial template during the analysis (King, 2004). In particular, we renamed the second-level code Presence of Researcher Bias as Presence of Researcher Bias and Clues because we found portions of text about clues suggesting the presence of researcher bias. We concluded the analysis when any portion of text relevant to the goal of our interview study was coded and we agreed on the obtained template. To ease the thematic analysis of the transcripts, we used ATLAS.ti12—a tool for supporting qualitative data analyses, including template analysis.

As for the survey, we performed an exploratory data analysis of the answers. In particular, we visualized the results—i.e., answers to the statements—by using stacked barplots. Each stacked barplot reported the absolute frequencies for each level of agreement about a statement.


Download : Download high-res image (328KB)
Download : Download full-size image
Fig. 3. Initial and final templates—we highlight in bold the text added to the initial template to obtain the final one.

4. Findings from the interview study and survey
In this section, we present the findings emerging from the interview study according to the main themes identified by the first-level codes (i.e., Experiment Planning, Questionable Research Practices, and Researcher Bias) of the final template shown in  Fig. 3. We also support these findings by reporting excerpts of the related transcripts. We then triangulate these findings with those from the survey. In particular, we show the level of agreement of the survey respondents about the statements we built upon the findings from the interview study.

4.1. Experiment planning
As Fig. 3 shows, we defined two sub-themes within this main theme—namely, the roles of researchers in SE experiments and how they cope with threats to validity in their experiments.

Researcher Roles. It emerged from the interviews that, when conducting an experiment, there is a division of roles among the researchers involved in the experiment. Each researcher covers one or more roles (e.g., one researcher is involved in the planning and execution of the experiment, another one extracts the metrics from the raw data, and so on). However, it seems that only one researcher takes care of data analysis (i.e., one researcher plays the data analyst role). An excerpt from the interview with R6 follows:


Download : Download high-res image (56KB)
Download : Download full-size image
As far as the survey results are concerned, most respondents (38), in their experience as experimenters, had more than one researcher involved in the data analysis (S1). In this case, we cannot support the finding from the interview study.

Threats to Validity. When we asked the interviewees to elaborate on the threats to validity, they provided a number of examples, but none of them mentioned researcher bias (accordingly, we could not define a corresponding statement in the questionnaire of the survey).

4.2. Questionable research practices
This theme includes three sub-themes (see Fig. 3): the participants’ perceptions of post-hoc hypotheses, post-hoc outlier criteria, and flexible reporting of measures and analyses (see Section 2.1).

Post-hoc Hypotheses. According to the interviewees, the post-hoc hypotheses practice should not lead to researcher bias as long as (i) the researchers clearly report that such hypotheses are formulated in retrospect, or (ii) it is possible to ground such hypotheses on prior work (thus, there is no need to make clear that such hypotheses are post-hoc). Regarding (i), R5 said:


Download : Download high-res image (78KB)
Download : Download full-size image
As for the point (ii), R3 told us:


Download : Download high-res image (35KB)
Download : Download full-size image
Furthermore, it seems that the post-hoc hypotheses practice could be a means to get new insights into the investigated phenomenon, which researchers had not thought about when the study was planned. On this matter, R4 said:


Download : Download high-res image (131KB)
Download : Download full-size image
On the other hand, the majority of the respondents (23) believed that formulating post-hoc hypotheses leads to researcher bias even when they are disclosed as being formulated in retrospect in the reporting of the experiment (see Fig. 4(a)). A higher number of respondents (27) believed that, even when grounded on prior work, post-hoc hypotheses still lead to researcher bias (see Fig. 4(b)). However, most respondents (42) saw post-hoc hypotheses as a mean to get new insights into the phenomenon under study (see Fig. 4(c)).

In case researchers apply the post-hoc outlier criteria practice, the interviewees agreed that they should declare the use of this practice in the paper by providing, for example, the following information: (i) the results before and after removing outliers; (ii) the reasons behind the outlier removal; and (iii) an interpretation of the results (e.g., why, after the outlier removal, a null hypothesis passes from non-rejected to rejected). On this matter, we report R4’s comment:


Download : Download high-res image (82KB)
Download : Download full-size image
As for the survey, the majority of the respondents (25) agreed that the post-hoc outlier criteria practice leads to researcher bias. However, 19 of them neither agreed nor disagreed with the statement reported in Fig. 5(a). Nevertheless, the respondents believed that disclosing additional information regarding the outlier removal does not lead to researcher bias (see Fig. 5(b)). In particular, the majority believed that what needs to be reported is: the results with and without the outliers (33); the reasons for different results once outliers are removed (30); and the reasons behind the outlier removal (29).

Flexible Reporting of Measures and Analysis. Based on interviewees’ experience, when researchers can choose among equivalent statistical hypothesis tests (e.g., paired t-test and Wilcoxon signed-rank test), the results (i.e., p-values) are not so different. R8’s thought on this point follows:


Download : Download high-res image (37KB)
Download : Download full-size image
Furthermore, according to R3, if a statistical hypothesis test revealed a significant difference (e.g., -value slightly less than ) that an equivalent test did not (e.g., -value greater than ), that difference would be probably negligible. In other words, the effect size would show the true impact of that difference, so having or not a significant difference would not matter:

Download : Download high-res image (73KB)
Download : Download full-size image
As for the practice of using several variants of a measure and then reporting only the variants that give the strongest results, it is perceived as a bad practice. The researchers should discuss any variant of that measure in the paper. In this respect, R4 said:

Download : Download high-res image (53KB)
Download : Download full-size image

Download : Download high-res image (265KB)
Download : Download full-size image
Fig. 5. Results regarding the post-hoc outlier criteria practice.

As for the respondents, most of them (37) disagreed that reporting the results of a statistical test, rather than those of an equivalent one, does not matter because the difference (estimated by using an effect size measure) would be probably negligible (see Fig. 6(a)). On the other hand, the majority of respondents (41) agreed that the flexible reporting of measures practice leads to researchers bias (see Fig. 6(b)).

4.3. Researcher bias
This theme has three sub-themes (see  Fig. 3): the presence of researcher bias in experiments and clues suggesting such a presence; causes of researcher bias; and strategies to cope with researcher bias.

These findings seem to be confirmed in the survey. In particular, as shown in Fig. 7(a), the presence of researcher bias in SE experiments appears to be independent of the experiment kind; and its presence seems to be perceived as widespread as in other research fields (see Fig. 7(b)). From their experience as reviewers of SE experiments, the majority of respondents (30) had suspected that researchers bias the results of their experiments (see Fig. 7(c)). Also, most respondents (43) agreed that researchers can unconsciously bias the results based on their expectations (see Fig. 7(d)). Finally, most respondents (29) agreed that researcher bias is one of the reasons for inconsistent results among similar studies—i.e., studies addressing the same constructs (see Fig. 7(e)).

Causes of Researcher Bias. Three causes of researcher bias emerged from the interviews. First, interviewees believed that negative-results papers are usually rejected. This would lead researchers to bias their results (e.g., transforming non-significant results into statistically significant ones). R2 said:


Download : Download high-res image (66KB)
Download : Download full-size image
Second, the pressure of publishing papers can lead researchers to (unconsciously or consciously) bias the results. R5 said:

Download : Download high-res image (99KB)
Download : Download full-size image
Third, it seems that revision processes of SE conferences/journals are focusing too much on the empirical assessment, rather than on the contributions of the ideas to the body of knowledge. Thus, researchers would be led to bias their studies by making the results more publishable. R5 told us:

Download : Download high-res image (98KB)
Download : Download full-size image
The three causes of researcher bias identified from the interview study were all endorsed by the larger part (between 28 and 37) of the respondents. In fact, for each cause the greater part of the respondents either strongly agreed or agreed (Fig. 8). This finding is slightly less pronounced on the statement concerned the revision processes of SE conferences/journals (S16).

Coping with Researcher Bias. The interviewees suggested seven strategies to cope with researcher bias. First, the use of pre-registration in SE conferences/journals (see Section 2.2). This should prevent negative-results papers from being rejected. Moreover, pre-registration increases both credibility of study results and study replicability (Nosek and Lakens, 2014). Accordingly, researchers should be less prone to bias the results of their studies. In this respect, R5 said:


Download : Download high-res image (50KB)
Download : Download full-size image
Second, fostering open data policies in SE conferences/journals. This means not only making the gathered data publicly available, but also the analysis scripts of the study. Such open data policies should allow reviewers (and any other researcher) to repeat the data analysis of that study so attributing credibility to study outcomes and increasing the replicability of the study. Therefore, researchers should be discouraged from biasing their studies. An excerpt from the interview with R1 follows:

Download : Download high-res image (36KB)
Download : Download full-size image
Third, duplicate data analysis. That is, two researchers analyze the same data with their own scripts without interacting with one another. Then they exchange the scripts and data to cross-check them. Finally, the results of the data analysis are compared. R5 told about this kind of data analysis (she was using at the time of the interview), which should mitigate the unconscious bias of researchers involved in the data analysis.

Download : Download high-res image (49KB)
Download : Download full-size image
Fourth, means for increasing the awareness of researcher bias in SE. For example, panels on researcher bias in SE, an ethical code for the SE research field to warn researchers against this kind of bias, or papers on researcher bias in SE studies. Therefore, by increasing the awareness of researcher bias, researchers should be warned against this kind of bias. On this matter, R6 said:

Download : Download high-res image (31KB)
Download : Download full-size image
Fifth, guidelines for reviewers in SE conferences/journals. These guidelines should instruct the reviewers not to judge papers based on the study results (i.e., positive/negative results). As a consequence, researchers would bias the study results less because having a paper reporting positive/negative results would be equally valid. On this point, R4 said:

Download : Download high-res image (36KB)
Download : Download full-size image
Sixth, ad-hoc research tracks in SE conferences (or ad-hoc issues in SE journals). For example, specific tracks for papers reporting negative results or specific tracks for studies having a not so strong empirical assessment. Such a kind of track should lead researchers not to bias their results to have more publishable results. On this point, R7 said:

Download : Download high-res image (26KB)
Download : Download full-size image
Seventh, replicated experiments because the more the results of a study are confirmed by replications, the lower the likelihood of researcher bias is. R8’s thought follows:

Download : Download high-res image (61KB)
Download : Download full-size image

Download : Download high-res image (380KB)
Download : Download full-size image
Fig. 8. Results regarding the causes behind researcher bias.

The majority of respondents (between 43 and 44) agreed that actions based on experiment replication (see Fig. 9(g)), as well as actions regarding data analysis (see Fig. 9(c)) and sharing of experimental material (see Fig. 9(b)), can mitigate researcher bias in SE experiments. A lower number of respondents (between 29 and 39) agreed that actions targeting community efforts can mitigate researcher bias. Among these actions, there are initiatives to increase the awareness about researcher bias (see Fig. 9(d)), peer-review guidelines (see Fig. 9(e)), and initiatives within conference and journal steering groups to set up experiment pre-registration (see Fig. 9(a)) and negative-results tracks and special issues (see Fig. 9(f)).

Besides the above-mentioned strategies to cope with researches bias, we asked the interviewees their thoughts on two further strategies, blind data extraction and blind data analysis, used alone and together. In the following subsections, we report the findings concerning the sub-themes for blind data extraction, blind data analysis, and both these strategies. We also triangulate these findings with those from the survey.

4.3.1. Blind data extraction
Two sub-themes were defined for this theme (see Fig. 3): usefulness and drawbacks of blind data extraction in SE experiments.

Usefulness of Blind Data Extraction in SE. It emerged from the interviews that blind data extraction could be a useful technique to mitigate researcher bias because, even when extracting the metrics, a researcher could favor a given treatment based on her expectations. In other words, if the data extractor (i.e., the person who is responsible for extracting the metrics from the raw dataset) is aware of research design elements (e.g., treatment assignment), then the likelihood of influencing the results towards a given treatment is higher. This is why having blinded extractors would lessen the likelihood of influencing the results. On this point, R3 said:


Download : Download high-res image (82KB)
Download : Download full-size image
As far as the survey is concerned, the majority of the respondents (30) agreed that blind data extraction can mitigate researcher bias, whereas only a few (four) disagreed with such a statement (see Fig. 10(a)).

Drawbacks of Blind Data Extraction. As for the drawbacks of blind data extraction, the interviewees pointed out that the implementation of blind data extraction requires at least two people: an individual (i.e., the study executor) responsible for executing the experiment and another individual (i.e., the data extractor) with the necessary skills to extract the metrics from the raw dataset. The latter has to be blinded to research design elements. This seems to be little feasible when both study executor and data extractor belong to the same research group—guessing or finding out about hidden information (e.g., research hypotheses) would be more likely when both executor and extractor belong to the same research group. Therefore, to implement blind data extraction, it is preferable to have: (i) a research collaboration between two research groups where the experimenter and the extractor are not part of the same group; or (ii) an external expert that takes care of the metric extraction. In this respect, R8 stated:


Download : Download high-res image (104KB)
Download : Download full-size image
It is worth noting that R5 had already used blind data extraction. In particular, she (and her colleagues) had involved some experts to extract metrics from a raw dataset:

Download : Download high-res image (58KB)
Download : Download full-size image
4.3.2. Blind data analysis
Two sub-themes were defined for this theme (see Fig. 3): usefulness and drawbacks of blind data analysis in SE experiments.

Usefulness of Blind Data Analysis. According to the interviewees, blind data analysis is a useful technique to mitigate researcher bias. This is because a blinded analyst (i.e., an analyst unaware of research design elements) would perform the data analysis more objectively than an analyst aware of research design elements. On this matter, R7 said:


Download : Download high-res image (40KB)
Download : Download full-size image
As for the respondents, the majority of them (32) agreed that blind data analysis is a useful technique to mitigate researcher bias. Only a few (four) disagreed with such a statement (see Fig. 10(b)).

Drawbacks of Blind Data Analysis. Similarly to blind data extraction, the drawback of blind data analysis is that at least two researchers are needed—the former conducts the study and sanitized the dataset, while the latter performs the data analysis on the sanitized dataset. Moreover, it is preferable (as for blind data extraction) that the researchers do not belong to the same research group. In this respect, R8 said:


Download : Download high-res image (21KB)
Download : Download full-size image
4.3.3. Blind data extraction and analysis.
We defined three sub-themes for this theme: effectiveness of blind data analysis and extraction in coping with researcher bias, strategies to foster the adoption of blind data analysis and extraction in SE experiments, and intention to use blind data analysis and extraction.

Effectiveness of Blind Data Extraction and Analysis. From the interview study, it emerged that researcher bias could arise even if blind data extraction and analysis are applied together. That is, using both blind data analysis and extraction is considered a way to mitigate researcher bias (rather than a way to remove it). In fact, researcher bias could arise not only during the metric extraction and analysis phases but also during the execution of the experiment itself. Below, we report R3’s answer when we asked if the combination of blind data extraction and blind data analysis was enough to cope with researcher bias:


Download : Download high-res image (24KB)
Download : Download full-size image
The respondents found that the combined use of blind data extraction and analysis can be considered an appropriate technique to mitigate researcher bias (see Fig. 10(c)). The majority of the respondents (34) agreed that blind data analysis is a useful technique to mitigate researcher bias, while four disagreed.

Fostering Blind Data Extraction and Analysis. The interviewees suggested a number of strategies to ease the adoption of blind data extraction and analysis in SE. The first strategy is a policy for conferences/journals similar to the double-blind peer-review one. That is, this policy would consist of requiring that any submitted experiment to that conference/journal had to use blind data extraction and analysis. However, this strategy is not always feasible, as the same interviewees observed, due to the following reasons: (i) the reviewers cannot make sure the authors of a paper have really used blind data extraction and analysis; (ii) researchers, who are not involved in research collaborations, would be harmed by this policy; and (iii) empirical evidence on the effectiveness of blind data extraction and analysis in SE studies is necessary to foster conferences/journals to adopt this policy. Regarding the point (i), R1 said:


Download : Download high-res image (16KB)
Download : Download full-size image
On the point (ii), R8 said:

Download : Download high-res image (48KB)
Download : Download full-size image
As for the last point, R4 said:

Download : Download high-res image (49KB)
Download : Download full-size image
The second strategy to foster the use of blind data extraction and analysis is a third-party service provider that takes care of metric extraction and data analysis blindly. For example, the researchers conduct the experiment and, when needed, sanitize the raw dataset (e.g., it removes any label to the treatments). Then they submit the raw dataset to this service provider, which extracts the metrics and then analyzes the data. After analyzing the data, the service provider sends the results to the researches. In this respect, R5 said:

Download : Download high-res image (72KB)
Download : Download full-size image
This strategy also has its drawbacks. As pointed out by R5, it is not easy to realize such a system. Also, the researchers should trust the service provider as well as the people that perform blindly the data extraction and analysis. Furthermore, it would most likely introduce extra costs. The third strategy consists of a guideline for applying blind data extraction and analysis in SE. R6 told us:

Download : Download high-res image (19KB)
Download : Download full-size image
Finally, empirical evidence on the effectiveness of blind data extraction and analysis in SE would foster the adoption of these blind techniques. In this respect, R4 said:

Download : Download high-res image (36KB)
Download : Download full-size image
Intention to Use Blind Data Extraction and Analysis. All interviewees stated they would take into account blind data extraction and analysis for their experiments. For example, R8 stated:


Download : Download high-res image (34KB)
Download : Download full-size image
When we asked whether the respondents would use blind data extraction and/or analysis in their next experiment (see Fig. 10(d)), the majority of the respondents were on the fence (25 for blind data extraction, 24 for blind data analysis). A lower number of respondents was willing to use blind data extraction (15) and blind data analysis (17) in their next experiment, while 11 respondents would not use blind data extraction and 10 will not use blind data analysis in their next experiment.

5. Discussion
In this section, we first discuss the results from both studies we presented in this paper and then the limitations of these studies.

5.1. Overall discussion
Studies on researcher bias and its mitigation have a longstanding tradition in the natural and medical sciences. For example, physicists employ sophisticated blinding techniques to their data tailored to specific types of investigation (Roodman, 2003); medical researchers use double-blind randomized clinical trials as the standard way to avoid bias (Doll, 1998). In the SE research field, the discourse on QRPs and RB mitigation started to appear in 2014–2015 in the work by Jørgensen et al. (2016) and Shepperd et al., 2014, Shepperd, 2015. In this section, we present the recommendations of our research. Some recommendations are intended for SE researchers while others are intended for the boards of SE research outlets. These recommendations are based on an introspection within our SE community and represent a first step towards the level of sophistication and awareness observed in other research fields.

The results of both interview study and survey support those by Jørgensen et al. (2016) and Shepperd et al. (2014)—i.e., researcher bias affects SE experiments. According to the respondents, the different kinds of experiments (i.e., human- and technology-oriented) seem to be equally affected by researcher bias. Also, it seems to be widely accepted that researcher bias is an unconscious phenomenon that needs to be addressed to improve the generation and solidification of scientific knowledge, and to avoid a methodological crisis (i.e., the impossibility to reproduce experimental results (Pashler and Wagenmakers, 2012)).

According to the interviewees, the formulation of post-hoc hypotheses should not be considered a QRP as long as the researcher explicitly mentions their use or it is possible to ground such hypotheses on prior work. On the contrary, the majority of the respondents consider post-hoc hypotheses to lead to researcher bias even when such hypotheses are disclosed and grounded on the literature. However, from both interview study and survey, it seems that this practice can be used to gain new insights into the investigated phenomenon (e.g., for further studies). Based on these results, we can delineate the following recommendation:

❧
Research hypotheses, generated after looking at the results of a study, need to be carefully disclosed by researchers. The investigation of such hypotheses can be the subject of follow-up studies.

This recommendation is also inline with those Jørgensen et al. (2016) delineated for SE researchers. In particular, the authors wrote: “make it clear whether a hypothesis was stated in advance or derived after looking at the data (exploratory hypothesis to be tested in follow-up studies)”.

According to the results from both interview study and survey, the post-hoc outlier removal practice is not always questionable. It is considered acceptable if the researchers provide the results after and before the outlier removal, justify the outlier removal, and discuss the causes behind possible differences. Existing guidelines for evaluating SE experiments (e.g., Kitchenham et al., 2002) require authors to provide a clear outlier dropout analysis, which is particularly relevant for researchers interested in integrating the results of similar experiments (e.g., meta-analysis). Accordingly, we can draw the following recommendation:

❧
Researchers should have dedicated sections to report why and how outliers are removed, and how the results are impacted. Make the results (and possibly the dataset), before the outlier removal, available.

Although we observed that the post-hoc outlier removal practice is not always considered questionable, the results from both studies suggest avoiding the use of this practice. In other words, researchers should still define the inclusion/exclusion outlier criteria in advance (Jørgensen et al., 2016). However, if a researcher faces a situation in which the use of the post-hoc outlier removal practice is reasonable, she should follow the above-mentioned recommendation.

The flexible reporting of measures is strongly perceived to lead to researcher bias in both studies. We make our the recommendation by Jørgensen et al. (2016) to report on all measures and extend it as follows:

❧
Researchers should disclose all measures in the paper and share the results for the measures they cannot include in the paper (e.g., for space reasons) by using an appendix or a replication package.

Both interviewees and respondents saw the potential of blinding (both when extracting and analyzing data) and, to some extent, were favorable to use it. Although useful for mitigating researcher bias, blind data extraction and analysis do not solve the problem. In fact, as the interviewees suggested, blind data extraction and analysis are more effective when the key roles (e.g., study executor and data extractor) are taken up by people that do not belong to the same research group. Our recommendation follows:

❧
Researchers should consider blind data extraction and analysis especially if they can involve external experts, or collaborate with other research groups to have external researchers, who take care of blind data extraction and analysis.

Involving external experts or collaborating with other research groups is not always possible. A simple form of blind data analysis can be achieved within the same research group by relabeling the experimental groups with non-identifying terms to hide the actual treatments from the data analyst (MacCoun and Perlmutter, 2015, Fucci et al., 2016). To mitigate researcher bias, the interviewees suggested to use duplicate data analysis—i.e., asking two or more people to analyze the data independently. This approach was largely endorsed by the respondents. Also, according to the respondents, more researchers are usually involved when analyzing the data, so making duplicate data analysis a feasible solution. Duplicate data analysis can be easily extended to data extraction, and can be applied in alternative (or in conjunction) with blind data extraction and analysis. Our recommendation follows:

❧
Researchers should consider simpler forms of blinding possibly together with duplicate data extraction and analysis if they cannot involve external experts or external researchers in the process of data extraction and analysis.

The interviewees suggested other strategies to mitigate researcher bias. A large part of respondents considered open data policies to be effective in mitigating researcher bias. Publicly-available datasets and analysis scripts foster external replications, which can help us understand how large is the role that researchers play when attempting at replicating experimental results. In the SE research, there seems to be a shortage of replication studies. A 2005 literature survey of 103 controlled experiments published in leading SE journals (Sjoeberg et al., 2005) reported that only 18% were replications. Out of these, the experimental results tend to be confirmed when the same team of researchers attempts to replicate the results. For example, this was the case for six out of the seven experiments categorized as differentiated replications. The lack of result replicability is usually attributed to the variations in the contextual factors of the experiments (e.g., programming language, participants’ experience) (Juristo and Vegas, 2011). However, to the best of our knowledge, only few studies directly attribute the different results to the fact that other researchers carried out the replication (Shepperd et al., 2014). Two other recommended strategies to mitigate researcher bias, both largely supported by the respondents, are: (i) experiment protocol pre-registration and (ii) negative-results conference tracks and journal issues. We can thus delineate the following recommendation:

❧
Editorial and program boards should explicitly promote and reward open data policies. When possible, they should establish pre-registration and negative-results tracks and special issues to limit publishing results hampered by researcher bias.

According to the interviewees, researcher bias could be triggered by specific reviewers’ behaviors. The respondents largely agreed that such behaviors are the reviewers tendency to reject negative-results papers and to focus too much on empirical assessment at the expenses of novel contributions to the body of knowledge. These behaviors, combined with the pressure to publish (perceived by the large majority of the respondents), lead researchers to bias their results to make them more publishable. We can thus delineate the following recommendation:

❧
Editorial and program boards should instruct reviewers to not judge the quality of a submission based on its results, either positive or negative. For submissions reporting interesting findings but with weak empirical assessment, boards should consider ad-hoc shepherding initiatives.

In several research fields, researcher bias seems to be the leading cause of a methodological crisis (e.g., Ioannidis et al., 2017, Dacrema et al., 2019). The sample of the empirical SE community we surveyed largely considered it to be the case also in the SE research field. We are concerned that the practitioners and the general public will consider the SE research field less credible due to the impact of researcher bias on the validity of SE research inquiries. Therefore, our last recommendation is:

❧
The SE research community needs to raise awareness on researcher bias, the problems it can cause, as well as initiatives for limiting it. This can be accomplished, for example, with special conference panels and town hall meetings.

Some of our recommendations have been already applied in fields where experiments with different degrees of control are the predominant research approach (e.g., medicine Doll, 1998). The forensic sciences employ a technique called sequential unmasking (Krane et al., 2008). Similar to data blinding, the approach aims at minimizing the influence of information (such as a suspect profile) when analyzing DNA collected from evidence. The approach also proposes a separation of tasks between individuals familiar with case information and the analyst from whom domain-irrelevant information is masked.

Fields focusing on collecting and analyzing qualitative data have developed other ways to address researcher bias, such as “Interview the interviewers” (Chenail, 2011). This approach allows the interviewer to identify a priori assumptions about the participants by becoming one of them and being interviewed by a third-party who does not have any specific expectations on the answers (e.g., a colleague not involved in the study). The interviewer records the interview and compares it with the script, self-reflecting on the parts that were included or left out. In the social sciences, there are two recommended approaches to do so, journaling (Miller, 1997) and inter-personal recalling (Kagan, 1972). Similar forms of self-reflection and peer-review are recommended as ways to reduce researcher bias in fields, such as anthropology, which make extensive use of ethnographies as research methods (LeCompte, 1987).

5.2. Limitations
The response rate (20%) of the survey might imply that only motivated researchers took part in the survey. This might have affected the results of the survey; however, motivated researchers are more likely to answer truthfully.

We left the online questionnaire open only for 20 days. This might have affected the response rate of the survey and thus the results. Despite we included in the questionnaire only the statements we deemed more relevant as suggested in the literature (e.g., Kasunic, 2005), the number of statements in the questionnaire might have had an effect on the response rate. On the other hand, reducing further the number of statements included in the questionnaire would have affected our capability of triangulating the results from the two studies.

The sampling method used in the interview study, as well as the one used in the survey, might have affected the results. Both interviewees and respondents might not have answered truthfully because scarcely motivated or afraid of being judged. To mitigate this threat in the interview study, the participation in the study was voluntary—volunteers are generally more motivated (Wohlin et al., 2012)—and we informed the interviewees that the gathered data would be treated confidentially. As for the survey, the answers to the questionnaire were anonymous.

Respondents of questionnaires might have difficulty comprehending statements or questions (e.g., because ambiguous, not clear, or not well formulated). To mitigate such a threat, we conducted a pilot study with two junior researchers. The use of unfamiliar terms in questionnaires might negatively influence questionnaire comprehensibility as well. We mitigated such a threat by including in the questionnaire explanations of terms that could be unfamiliar to the respondents.

Finally, since the recommendations delineated in Section 5.1 are based on evidence collected from interviewees and respondents within the SE community, we cannot claim they will apply to other research fields.

6. Conclusion
In this paper, we investigate researcher bias in SE experiments, including: (i) QRPs potentially leading to researcher bias; (ii) causes behind researcher bias; and (iii) possible actions to counteract researcher bias with a focus on, but not limited to, blind data extraction and analysis. To pursue such an objective, we adopted a two-step methodological approach comprising a qualitative interview study followed by a quantitative survey. The interview study is intended as an exploratory study. The findings from this survey represented the starting point to design the survey, which we conducted to support the findings from the interview study. The findings from the interview study are mostly confirmed by those from the survey—e.g., the post-hoc outlier removal practice is not always questionable for both interviewees and respondents. In few cases, the findings from the interview study are not confirmed—e.g., the interviewees did not find questionable the formulation of post-hoc hypotheses, while the respondents did. Both interviewees and respondents perceived the presence of researcher bias in se experiments. Therefore, researcher bias cannot be underestimated. To counteract it, we delineated a series of recommendations; some of them are intended for se researchers, while others are purposeful for the boards of SE research venues.