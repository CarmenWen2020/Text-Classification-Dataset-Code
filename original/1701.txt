Abstract
Context:
An excessive number of code smells make a software system hard to evolve and maintain. Machine learning methods, in addition to metric-based and heuristic-based methods, have been recently applied to detect code smells; however, current methods are considered far from mature.

Objective:
First, explore the feasibility of applying deep learning models to detect smells without extensive feature engineering. Second, investigate the possibility of applying transfer-learning in the context of detecting code smells.

Methods:
We train smell detection models based on Convolution Neural Networks and Recurrent Neural Networks as their principal hidden layers along with autoencoder models. For the first objective, we perform training and evaluation on C# samples, whereas for the second objective, we train the models from C# code and evaluate the models over Java code samples and vice-versa.

Results:
We find it feasible to detect smells using deep learning methods though the models’ performance is smell-specific. Our experiments show that transfer-learning is definitely feasible for implementation smells with performance comparable to that of direct-learning. This work opens up a new paradigm to detect code smells by transfer-learning especially for the programming languages where the comprehensive code smell detection tools are not available.

Previous
Next 
Keywords
Code smells

Smell detection tools

Deep learning

Transfer-learning

1. Introduction
The metaphor of code smells is used to indicate the presence of quality issues in source code (Fowler, 1999, Sharma and Spinellis, 2018). A large number of smells in a software system is associated with a high level of technical debt (Kruchten et al., 2012) hampering the system’s evolution. Given the practical significance of code smells, software engineering researchers have studied the concept in detail and explored various aspects associated with it including causes, impacts, and detection methods (Sharma and Spinellis, 2018).

A large body of work has been carried out to detect smells in source code. Traditionally, metric-based (Marinescu, 2005, Salehie et al., 2006) and rule/heuristic-based (Moha et al., 2010, Sharma et al., 2016) smell detection techniques are commonly used (Sharma and Spinellis, 2018, Rasool and Arshad, 2015). In recent years, smell detection techniques based on machine-learning (Maiga et al., 2012b, Czibula et al., 2015) have emerged as a potent alternative as they not only have the potential to bring human judgment in the smell detection but also provide the grounds for transferring results from one problem to another. Researchers have used Bayesian belief networks (Khomh et al., 2009, Khomh et al., 2011), support vector machines (Maiga et al., 2012a), and binary logistic regression (Bryton et al., 2010) to identify smells.

The resilience of machine learning models renders them appropriate for reuse beyond the bounds of tasks they may have been trained on. Transfer-learning refers to the technique where a learning algorithm exploits the commonalities between different learning tasks to enable knowledge transfer across tasks (Bengio et al., 2013). In this context, it would be plausible to explore the possibility of leveraging the availability of tools and data related to code smell detection in a programming language in order to train machine learning models that address the same problem on another language. The cross-application of a machine learning model could provide opportunities for detecting smells without actually developing a language-specific smell detection tool from scratch.

Despite the potential prospects, existing approaches for applying machine learning techniques for smell detection offer significant room for improvement. In a recent study, Nucci et al. (2018) note, after observing practices such as improper data handling in existing software engineering literature, that the problem of detecting smells still requires extensive research to attain a maturity.

Furthermore, machine learning techniques (such as Bayesian networks, support vector machines, and logistic regression) that have been applied so far require considerable pre-processing to generate features for the source code, a substantial effort that hinders their adoption in practice. Traditionally, researchers use machine-learning methods that require extracting feature-sets from source code. Typically, code metrics are used as the feature set for smell detection purposes. We perceive two shortcomings in such usage of machine-learning methods for detecting smells. First, availability of an external tool to compute metrics for the target programming language on which we would like to apply the machine learning model becomes the prerequisite. Second and more importantly, we are limiting the machine learning algorithm to use only the metrics that we are computing and feeding as feature-set. Therefore, the machine learning algorithm cannot observe any pattern that is not captured by the provided set of metrics.

In this context, deep learning models, specifically neural networks, offer a compelling alternative. The Convolution Neural Network (cnn) and the Recurrent Neural Network (rnn) are state-of-the-art supervised learning methods currently employed in many practical applications, including image recognition(Krizhevsky et al., 2012, Szegedy et al., 2015), speech recognition (Sainath et al., 2015), and natural language processing (Johnson and Zhang, 2015). These advanced models are capable of inferring features during training and can learn to classify samples based on these inferred features.

In this paper, we present experiments with deep learning models with two specific goals:

•
To investigate whether deep learning methods can effectively detect code smells. In particular, to employ architectures that include layers of cnns, rnns as well as autoencoders, inspect how different models perform on detecting diverse code smells and how model performance is affected by tweaking the learning hyper-parameters.

•
To investigate whether results on smell detection through deep learning are transferable; specifically, to explorewhether models trained for detecting smells on a programming language can be re-used to detect smells on another language.

Keeping these goals in mind, we define research questions and prepare an experimental setup to detect four smells viz. complex method, complex conditional, feature envy, and multifaceted abstraction using deep learning models in different configurations. We develop a set of tools and scripts to automate the experiment and collate the results. Based on the results, we derive conclusions to our addressed research questions.

The contributions of this paper are summarized below.

•
An extensive study that applies deep learning models in detecting code smells without carrying out extensive feature engineering and compares the performance of different methods; to the best of our knowledge this is the first study of this kind and scale.

•
An exploration that not only shows the feasibility of applying transfer-learning for identifying code smells but also compares the performance of deep learning models in the transfer-learning context. This exploration potentially will open a new paradigm to detect smells specifically for programming languages for which mature code smell detection tools are not available.

•
Openly available tools, scripts, and data used in this experiment1 to promote replication as well as incremental studies.

•
The study identifies and documents challenges that we perceived and opportunities that the exploration offered.

The rest of the paper is organized as follows. Section 2 sets up the stage by presenting background and related work. We define our research objective in Section 3 and research method in Section 4. Section 5 presents our findings, discussion, and further research opportunities. We present threats to validity of this work in Section 6 and conclude in Section 7.

2. Background and related work
In this section, we present the background about the topic of code smells as well as machine learning and elaborate on the related literature.

2.1. Code smells
Kent Beck coined the term “code smell” (Fowler, 1999) and defined it as “certain structures in the code that suggest (or sometimes scream) for refactoring”. Code smells indicate the presence of quality problems impacting many facets of quality (Sharma and Spinellis, 2018) of a software system (Fowler, 1999, Suryanarayana et al., 2014). The presence of an excessive number of smells in a software system makes it hard to maintain and evolve.

Smells are categorized as implementation (Fowler, 1999), design (Suryanarayana et al., 2014), and architecture smells (Garcia et al., 2009a) based on their scope, granularity, and impact. Implementation smells are typically confined to a limited scope and impact (e.g., a method). Examples of implementation smells are long method, complex method, long parameter list, and complex conditional (Fowler, 1999). Design smells occur at higher granularity, i.e., abstractions, and hence are confined to a class or a set of classes. God class, multifaceted abstraction, cyclic-dependency modularization, and rebellious hierarchy are examples of design smells (Suryanarayana et al., 2014). Along similar lines, architecture smells span across multiple components and have a system-wide impact. Some examples of architecture smells are god component (Lippert and Roock, 2006), feature concentration (de Andrade et al., 2014), and scattered functionality (Garcia et al., 2009b).

A plethora of work related to code smell detection exists in the software engineering literature. Researchers have proposed methods for detecting smells that can be largely divided into five categories (Sharma and Spinellis, 2018). Metric-based smell detection methods (Marinescu, 2005, Vidal et al., 2014, Salehie et al., 2006) take source code as input, prepare a source code representation, such as an Abstract Syntax Tree (ast), compute a set of source code metrics, and detect smells by applying appropriate thresholds (Marinescu, 2005). Rule/Heuristic-based smell detection methods  (Moha et al., 2010, Sharma et al., 2016, Arnaoudova et al., 2013, Tsantalis and Chatzigeorgiou, 2011) typically take source code representations and sometimes additional software metrics as input. They detect a set of smells when the defined rules/heuristics get satisfied. History-based smell detection methods use source code evolution information (Palomba et al., 2015, Fu and Shen, 2015). Such methods extract structural information of the code and how it has changed over a period of time. This information is used by a detection model to infer smells in the code. Optimization-based smell detection approaches (Sahin et al., 2014, Ouni et al., 2015, Kessentini et al., 2014) apply an optimization algorithm on computed software metrics and, in some cases, existing examples of smells to detect new smells in the source code. Further studies compare different detection methods (Palomba et al., 2018, Paiva et al., 2017).

2.2. Deep learning
Deep learning is a subfield of machine learning that allows computational models composed of multiple processing layers to learn representations of data with multiple levels of abstraction (LeCun et al., 2015, Goodfellow et al., 2016). Even though the idea of layered neural networks with internal “hidden” units was already introduced in the 80s (Rumelhart et al., 1986), a breakthrough in the field came in 2006 by Hinton et al. (2006) who introduced the idea of learning a hierarchy of features one level at a time. Ever since, and particularly during the course of the last decade, the field has taken off due to the advances in hardware, the release of benchmark datasets (Deng et al., 2009, Krizhevsky and Hinton, 2009, LeCun et al., 2010), and a growing research focus on optimization methods (Martens, 2010, Kingma and Ba, 2014). Although deep learning architectures often consist of tens or hundreds of successive layers, much shallower architectures may also fall under the category of deep learning, as long as at least one hidden layer exists between the input and the output layer.

Deep learning architectures are being used extensively for addressing a multitude of detection, classification, and prediction problems. Architectures involving layers of cnns are inspired by the hierarchical organization of the visual cortex in animals, which consists of alternating layers of simple and complex cells (Felleman and Van Essen, 1991, Hubel and Wiesel, 1962). cnns have been proven particularly effective for problems of optical recognition and are widely used for image classification and detection (Krizhevsky et al., 2012, Szegedy et al., 2015, LeCun et al., 1998), segmentation of regions of interest in biological images (Kraus et al., 2016), and face recognition (Lawrence et al., 1997, Parkhi et al., 2015). Besides recognition of directly interpretable visual features of an image, cnns have also been used for pattern recognition in signal spectrograms, with applications in speech recognition (Sainath et al., 2015). In these applications the input data are given in the form of matrices (2d arrays) for representing the 2d grid layout of pixels in an image. 1d representations of data have been used for applying 1d convolutions in sequential data such as textual patterns (Johnson and Zhang, 2015) or temporal event patterns (Lee et al., 2017, Abdeljaber et al., 2017). However, when it comes to sequential data, rnns (Rumelhart et al., 1986) have been proven superior due to their capability to dynamically “memorize” information provided in previous states and incorporate it to a current state. Long Short Term Memory (lstm) networks are a special kind of rnn that can connect information spanning long-term intervals, thus capturing long-term dependencies. lstms have been found to perform reasonably well within the context of representative applications that exhibit sequential patterns, such as speech recognition and music modeling (Greff et al., 2017, Graves et al., 2013). In addition, they have been established as state-of-the-art networks for a variety of natural language processing tasks; indicative applications include natural language generation (Wen et al., 2015), sentiment classification (Wang et al., 2016, Baziotis et al., 2017) and neural machine translation (Cho et al., 2014), among others. Finally, approaches for addressing problems of both visual and sequential nature, rely on the use of autoencoders (Rumelhart et al., 1985). Autoencoders have been used in the past for performing dimensionality reduction and data compression (Kramer, 1991, Hinton and Zemel, 1994). The basic idea of an autoencoder is that the input data is encoded into a compressed bottleneck-like representation which is in turn reconstructed back to an approximation of the input; the encoding–decoding process takes place in an unsupervised manner. Over the last decade, variants of autoencoders have been widely used as part of deep architectures for addressing problems of visual recognition (Vincent et al., 2008, Masci et al., 2011) and natural language processing (Socher et al., 2011, AP et al., 2014). One of the advantages of autoencoders is that they have been proven robust to cross-domain generalizations (Chen et al., 2012), thus providing solutions for domains where training data is imbalanced or scarce. In a similar vein, autoencoders have been used for discovering patterns that do not conform to some – otherwise homogeneous – data sample. Following the same rationale as in using linear dimensionality reduction methods such as Principal Components Analysis (pca) for outlier detection, autoencoders have been used as a non-linear alternative for discovering anomalies in extremely imbalanced data. Examples of problems where relevant methods have been used include image processing (Zhou and Paffenroth, 2017), and the identification of anomalies in spacecraft telemetry data (Sakurada and Yairi, 2014).


Table 1. Comparison of code smell detection techniques using machine learning.

Study	Machine learning method	Detected smells	Feature-set
Khomh et al. (2009)	Bayesian belief networks	Blob	Code metrics
Khomh et al. (2011)	Bayesian belief networks	Blob, functional decomposition, spaghetti code	Code metrics
Maiga et al., 2012b, Maiga et al., 2012a	Support vector machine	Blog, functional decomposition, spaghetti code, swiss army knife, Code metrics	
Bryton et al. (2010)	Binary logistic regression	Long method	Code metrics
Barbez et al. (2019)	cnn-based architecture	God class, feature envy	Code metrics
Arcelli Fontana et al. (2016)	16 machine learning algorithms	Data class, god class, feature envy, long method	Code metrics
Kim (2017)	Neural network	Large class, lazy class, data class, parallel inheritance hierarchy, god class, feature envy	Code metrics
Liu et al. (2019)	cnn-based, neural, lstm-based network	Feature envy, long method, large class, misplaced class	Code metrics and textual information
Hadj-Kacem and Bouassida (2018)	Neural network and autoencoder	God class, data class, feature envy, long method	Code metrics
This study	cnn, rnn, and autoencoder-based network	Complex method, complex conditional, feature envy, multifaceted abstraction	Tokenized source code
2.3. Machine learning techniques on source code
The emergence of online open-source repository hosting platforms such as GitHub in recent years has led to an explosion on the volumes of openly available source code along with metadata related to software development activities; this bulk of data is often referred to as “Big Code” (Allamanis et al., 2018). As an effect, software maintenance activities have started leveraging the wealth of openly available data, the availability of computational resources, and the recent advances in machine learning research. In this context, statistical regularities observed in source code have revealed the repetitive and predictable nature of programming languages, which has been compared to that of natural languages (Hindle et al., 2012, Ernst, 2017). To this end, problems of automation in natural language processing, such as identification of semantic similarity between texts, translation, text summarization, word prediction and language generation have been examined in the context of automating software development tasks. Relevant problems in software development include clone detection (White et al., 2016, Wei and Li, 2017), de-obfuscation (Vasilescu et al., 2017), language migration (Nguyen et al., 2013), source code summarization (Iyer et al., 2016), auto-correction (Pu et al., 2016, Gupta et al., 2017), auto-completion (Foster et al., 2012), generation (Oda et al., 2015, Ling et al., 2016, Yin and Neubig, 2017), and comprehension (Alexandru et al., 2017).

On a par with equivalent problems in natural language processing, the methods employed for automating several software engineering tasks are switching from traditional rule-based and probabilistic n-gram models to deep learning methods. The majority of the proposed deep learning solutions rely on the use of rnns which provide sophisticated mechanisms for capturing long term dependencies in sequential data, and specifically lstms (Hochreiter and Schmidhuber, 1997) that have been proved particularly effective for modeling natural language. Relevant methods have been applied on source code, aiming either to produce improved representations for encoding semantics of snippets (Alon et al., 2018), or as part of solutions to downstream tasks. Tasks that involve code edits have attracted particular interest, due to the practical implications induced by learning to automate pertinent maintenance activities. Employed towards this quest have been methods inspired from research on neural machine translation, such as the multi-layered lstms sequence-to-sequence model (Sutskever et al., 2014). Results produced by leveraging equivalent architectures, with representations of code snippets before and after applying some change, show the potential that the seq-to-seq model has towards learning meaningful repairs and refactorings (Chen et al., 2019, Tufano et al., 2019). Simpler types of networks that have produced promising results for semantic representations of code are based on the use of autoencoders  (White et al., 2016, Tufano et al., 2018).

Alternative approaches to mining source code have employed cnns in order to learn features from various representations of code. Li et al. (2017) have used single-dimension cnns to learn semantic and structural features of programs by working at the ast level of granularity and combining the learned features with traditional hand-crafted features to predict software defects. This method however incorporates hand-crafted features in the learning process and is not proven to yield transferable results. Similarly, a one-dimensional cnn-based architecture has been used by Allamanis et al. (2016) in order to detect patterns in source code and identify “interesting” locations where attention should be focused. The objective of the study is to predict short and descriptive names of source code snippets (e.g., a method body) given solely its tokens. cnns have also been used by Huo et al. (2016) in order to address the problem of bug localization. This approach leverages both the lexical information expressed in the natural language of a bug report and the structural information of source code in order to learn unified features. A more coarse-grain approach that also employs cnns has been proposed in the context of program comprehension (Ott et al., 2018) where the authors use imagery rather than script in order to discriminate between scripts written in two programming languages, namely Java and Python. Similarly, Ren et al. (2019) use a cnn-based neural network to identify self-admitted technical debt. Rantala and Mäntylä (2020) use NLP techniques to identify technical debt from comments. cnn-based models along with NLP techniques are used by Zampetti et al. (2020) to identify code patterns to help pay-back technical debt.

2.4. Machine learning on smell detection
In recent times, machine learning-based smell detection methods have attracted software engineering researchers. Machine learning is a subfield of artificial intelligence that trains solutions to problems rather than modeling them through hard-coded rules. In this approach, the rules that solve a problem are not set a-priori; rather, they are inferred in a data-driven manner. In supervised learning, a model is trained by being exposed to examples of instances of the problem along with their expected answers and statistical regularities are drawn. The representations that are learned from the data can in turn be applied and generalized to new, unseen data in a similar context.

Table 1 presents a comparison of existing attempts to detect smells using machine learning techniques. A typical machine learning smell detection method starts with a mathematical model representing the smell detection problem. Existing examples and source code models are used to train the model. The trained model is used to classify or predict the code fragments into smelly or non-smelly instances.Khomh et al., 2009, Khomh et al., 2011 use a Bayesian approach for the detection of three design smells. Their study forms a Bayesian graph using a set of metrics and determines the probability of a class being positive to a smell. Similarly, Maiga et al., 2012b, Maiga et al., 2012a employ support vector machine-based classifiers, trained using a set of  object-oriented metrics for each class to detect design smells (blob, feature concentration, spaghetti code, and swiss army knife). Furthermore, Bryton et al. (2010) detect long method smell instances by employing binary logistic regression. They use commonly used method metrics, such as Method Lines of Code (mloc) and cyclomatic complexity as regressors. Barbez et al. (2019) present an ensemble method that combine outcomes of multiple tools to detect god class and feature envy smells. They identify a set of key metrics for each smell and feed them to a cnn-based architecture. Arcelli Fontana et al. (2016) compare performance of various machine learning algorithms in detecting data class, god class, feature envy, and long method. Azadi et al. (2018) introduce WekaNose, a semi-automated tool that learns rules by identifying correlations between code smell instances and relevant metrics. Fontana and Zanoni (2017) use regression-based methods with extensively engineered features in order to classify code smell instances according to their severity. Overall, research on smell detection with machine learning techniques relies mostly on traditional methods with decision trees and support vector machines being the most commonly used algorithms (Azeem et al., 2019). Recent approaches that adapt deep learning architectures in the context of smell detection are limited. These presume substantial data engineering (Kim, 2017), to the extent of combining metrics relevant to different features exhibited in code (e.g., textual and structural features) (Liu et al., 2019), or hybrid methods that include a feature learning stage before feeding the data into the neural network (Hadj-Kacem and Bouassida, 2018).

The performance of a machine learning model primarily depends on the choice of suitable data representations that will adequately capture informative features for the task in hand. Another crucial factor for the performance of a model is the amount of available training data and the formation of the evaluation samples. As the proportion of positive and negative samples becomes more imbalanced, the classification task of the models becomes harder. Hence, a model would perform significantly better when classifying data from balanced datasets. Most of the above-mentioned approaches do not explicitly mention the ratio of positive and negative samples used for the evaluation. Fontana et al. (2016) carry out the evaluation using  positive and  negative samples for each smell which is considerably balanced compared to a realistic case. We further discuss this issue and demonstrate the effect of class imbalance in Section 5.

2.5. The need of applying deep learning for smell detection
Section 2.1 describes existing techniques for detecting code smells. Mainstream tools use rule-based and metric-based approaches to detect smells (Sharma and Spinellis, 2018). However, context plays an important role in deciding whether a reported smell is actually a quality issue for the development team, and existing tools do not consider the context while detecting the smells. For example, a method with a switch-case statement with ten cases, where each case instance has only a couple of simple statements will be detected as a complex conditional smell by the mainstream tools, because the associated rule (cyclomatic complexity greater than a threshold) will be triggered. However, the method’s developer might not consider it complex, because all case phrases share the same structure. When it comes to the tools’ validation, even manual annotation is often inadequate for ensuring the validity of the source code element rules. The main reason is that validation is typically carried out on open-source projects where the human validators are viewing the code snippets for the first time. Therefore, even though the tools are correct by the defined rules they still lack context-sensitivity. Deep learning, without specific feature set specification (such as metrics), could bring the code’s context under consideration when detecting smells.

The above discussion takes us to the next question: how to obtain or generate much needed training data for deep learning? The training data clearly need to be prepared considering the project’s context, because the aim is to make smell detection more context-sensitive. However, to the best of our knowledge, a large training dataset for training models of this scale is not available. In this study, we are using training data generated from existing tools as a first step towards assessing the extent to which smell detection is feasible via deep learning techniques. The study is a preliminary evaluation to verify the extent to which deep learning is suitable to detect smells that may involve context-sensitivity. This provides a stepping stone for future studies that will address more sophisticated problems such as custom context-sensitive smell detection. Such studies could replace our training data with manually annotated context-sensitive training data to achieve context-sensitive smell detection, thus overcoming the burden of hard coding custom rules into existing tools.

2.6. Challenges in applying deep learning on source code
Applying deep learning techniques on source code is non-trivial. In this section, we present challenges that we face in the process of applying deep learning techniques on source code.

2.6.1. Limits in analogies with other domains
Deep learning is advancing rapidly in domains that address problems of image, video, audio, text, and speech processing (LeCun et al., 2015). Consequently, these advances drive current trends in deep learning and inspire applications across disciplines. As such, studies that apply deep learning on source code rely heavily on results from these domains, and particularly that of text mining.

Based on prior observations that demonstrate similarity between source code and natural language (Hindle et al., 2012), the research community has largely addressed relevant problems on mining source code by adopting latest state-of-the-art natural language processing methods (Allamanis et al., 2016, Palomba et al., 2016, Iyer et al., 2016, Vasilescu et al., 2017, Yin and Neubig, 2017). However, besides similarities, there also exist major differences that need to be taken into consideration when designing such studies. First of all, source code, unlike natural language, is semantically fragile; minor syntactic changes can drastically change the meaning of code (Allamanis et al., 2018). As an effect, treating code as text by ignoring the underlying formal semantics carries the risk of not preserving the appropriate meaning. Besides formal semantics, the syntax of source code obviously presents substantial differences compared to the syntax found in text. As a result, methods that perform well on text are likely to under-perform on source code. Architectures involving cnn-1d layers, for instance, have been proven effective for matching subsequences of short lengths (Chollet, 2017), which are often found in natural language where the length of sentences is limited. This however does not necessarily apply on self-contained fragments of source code, such as method definitions, which tend to be longer. In order to address these shortcomings, currents research invests on developing appropriate representations for code (Allamanis et al., 2017, Alon et al., 2019, Alon et al., 2018).

Finally, even though good practices dictate naming conventions in coding, unlike natural language, there is no universal vocabulary of source code. This results to a diversity in the artificial vocabulary found in source code that may affect the quality of the models learned. Rare and complex identifiers constantly devised by developers result to limited repetition of terms, as well as patterns of locality, that are not common in natural language (Hellendoorn and Devanbu, 2017). The implications of these peculiarities in the quality of the resulting machine-learned models are acknowledged by the community, whilst latest research advances aim towards addressing these shortcomings (Karampatsis et al., 2020, Babii et al., 2019, Markovtsev et al., 2018) and painstakingly re-examining past results (Rahman et al., 2019).

Approaches that treat code as text mainly focus on the mining of sequential patterns of source code tokens. Other emerging approaches look into structural characteristics of the code with the objective of extracting visual patterns delineated on code (Ott et al., 2018). Even though there are features in source code, such as nesting, which demonstrate distinctive visual patterns, treating source code in terms of such patterns and ignoring the rich intertwined semantics carries the risk of oversimplifying the problem.

2.6.2. Lack of resources
Research employing deep learning techniques on software engineering data, including source code as well as other relevant artifacts, is still young. Consequently, results against traditional baseline techniques are very limited (Fu and Menzies, 2017, Hellendoorn and Devanbu, 2017) while the debate on whether deep neural networks are suitable for modeling source code is still open (Hellendoorn and Devanbu, 2017, Karampatsis and Sutton, 2019). Especially when it comes to processing solely source code artifacts, relevant studies are scarce and mostly address the problem of drawing out semantics related to the functionality of a piece of code (Allamanis et al., 2016, White et al., 2015, White et al., 2016, Mou et al., 2016, Piech et al., 2015). To the best of our knowledge, our study is the first to thoroughly investigate the application of deep learning techniques with the objective of examining characteristics of source code quality without making use of derived features. Therefore, a major challenge in studies of this kind is that there is no prior knowledge that would guide this investigation, a challenge reflected on all stages of the inquiry. At the level of designing an experiment, there exist no rules of thumb indicating a set up for a deep learning architecture that adequately models the fine-grained features required for the problem in hand. Furthermore, at the level of training a model, there is no prior baseline for hyper-parameters that would lead to an optimal solution. Finally, at the level of evaluating a trained model, there exist no benchmarks to compare against; there is no prior concrete indication on the expected outcomes in terms of reported metrics. Hence, a result that would appear sub-optimal in another domain such as natural language processing, may actually account for a significant advance in software quality assessment.

Besides challenges that relate to the know-how of applying deep learning techniques on source code, there are technical difficulties that arise due to the paucity of curated data in the field. The need for openly available data that can serve for replicating data-driven studies in software engineering has been long stressed (Robles, 2010). The release of curated data in the field is encouraged through badging artifact-evaluated papers as well as dedicated data showcase venues for publication. However, the software engineering domain is still far from providing benchmark datasets, whereas the available datasets are limited to curated collections of repositories with associated metadata that lack ground truth annotation that is essential for a multitude of supervised machine learning tasks. Therefore, unlike domains such as image processing and natural language processing where an abundance of annotated data exist (Krizhevsky and Hinton, 2009, Deng et al., 2009, LeCun et al., 2010, Maas et al., 2011), in the field of software engineering the lack of gold standards induces the inherent difficulty of collecting and curating data from scratch. The need for curating datasets of reference in software engineering studies has been recognized in the past (Dallmeier and Zimmermann, 2007), however, the progress in this front has not kept pace with the increasing volumes of data in the wild. Current efforts for overcoming this limitation include establishing benchmarks for evaluating results on semantic code search (Husain et al., 2019) and testing (Just et al., 2014), and the release of large-scale pretrained models for programming language vocabularies (Efstathiou and Spinellis, 2019), in an analogy to the natural language paradigm (Grave et al., 2018).

3. Research objectives
The goal of this research is to explore the plausibility of applying state-of-the-art deep learning methods to detect smells. Furthermore, within the same context, this work examines the feasibility of applying transfer-learning. Based on the stated goals, we define the following research questions that this work aims to explore.

RQ1
Is it possible to detect code smells using deep learning methods? If yes, which deep learning method performs superior?

We use cnn, rnn and autoencoder models in this exploration. For the cnn-based architecture, we provide input samples in 1d and 2d format to observe the difference in their capabilities due to the added dimension; we refer to them as cnn-1d and cnn-2d respectively. In the context of this research question, we define the following hypotheses.

RQ1.H1:It is feasible to detect code smells using deep learning methods.

The considered deep learning models have demonstrated high performance in the domain of image processing and natural language processing (Luong et al., 2015). We believe we can leverage these models in the presented context.

RQ1.H2: cnn-2d performs better than cnn-1d in the context of detecting smells.

The rationale behind this hypothesis is the added dimensionality in cnn-2d. The 2d model might observe inherent patterns when input data is presented in two dimensions that may possibly be hidden in one dimensional format. For instance, a 2-d variant could possibly identify the nesting depth of a method easier than its 1-d counterpart when detecting complex method smell.

RQ1.H3:rnn models perform better than cnn models in the context of detecting smells.

rnns are considered better for capturing sequential patterns and have the reputation to work well with text. Thus, taking into account the similarities that source code and natural language share, rnn models could prove superior to cnn models.

RQ1.H4: rnn and cnn variants of autoencoder model exhibit comparable performance to those of rnn and cnn-1d models.

An autoencoder model could be realized in various ways. In this work, we experiment with three variants of autoencoder models in which the models use fully connected neural network layers, lstm layers, and convolution layers respectively. We hypothesize that the performance of the autoencoder variants follow a pattern similar to that observed with rnn and cnn-1d models.

RQ2
Is it possible to detect code smells by applying transfer-learning techniques on similar languages? If yes, which deep learning model exhibits superior performance in detecting smells when applied in transfer-learning setting?

Transfer-learning is the capability of an algorithm to exploit the similarities between different learning tasks and provide a solution for a task by transferring knowledge acquired while solving another task. We would like to explore whether it is feasible to train a deep learning model from samples of C# and predict the smells using this trained model in samples of Java programming language and vice-versa. The feasibility exploration can be termed positive if the produced results are comparable with the results obtained from direct learning (RQ1). We derive the following hypotheses.

RQ2.H1: It is feasible to detect code smells by applying transfer-learning techniques on similar languages.

Given the high similarity in the syntax between the two programming languages considered in this study, we believe that we may train a model on samples of the one language and use it to classify smelly and non-smelly fragments on evaluation samples from the other.

RQ2.H2: The performance of transfer-learning is inferior compared to that of direct-learning.

Direct-learning in the context of our study refers to the case where training and evaluation samples belong to the same programming language. We expect that the performance of the models in transfer-learning could be inferior to that of direct-learning, given that in direct-learning both training and evaluation samples come from the same programming language, and hence are expected to exhibit homogeneous features.

4. Research method
This section describes the employed research method by first providing an overview and then elaborating on the data curation process. We also discuss the selection protocol of smells and architecture of the deep learning models.

4.1. Overview of the method
Fig. 1 provides an overview of the experiment. We download  C# and 1721 Java repositories from GitHub. We use the Designite and DesigniteJava smell detection tools to analyze C# and Java code respectively. We use CodeSplit, a set of utilities, to extract each method and class definition into separate files from C# and Java programs. Then the learning data generator uses the detected smells to bifurcate code fragments into positive or negative samples for a specified smell—positive samples contain the smell while the negative samples are free from that smell. Tokenizer takes a method or class definition and generates integer tokens for each token in the source code. As a preprocessing step we remove identical samples on the Tokenizer’s output, thus ensuring that the effects of code duplication on the evaluation of the resulting models are mitigated (Allamanis, 2019). The processed output of the Tokenizer is ready to feed to the neural networks.

4.2. Data curation
In this section, we elaborate on the process of generating training and evaluation samples along with the tools used in the process.

4.2.1. Downloading repositories
We use the following protocol to identify and download our subject systems.

•
We download repositories containing C# and Java code from GitHub. We use RepoReapers (Munaiah et al., 2017) to filter out low-quality repositories. RepoReapers analyzes GitHub repositories and provides scores for eight dimensions of their quality. These dimensions are architecture, community, continuous integration, documentation, history, license, issues, and unit tests.

•
We select all the repositories where at least seven out of eight RepoReapers’ dimensions have suitable scores for both C# and Java repositories. We consider a score suitable if it has a value greater than zero.

•
We ensure that RepoReapers results do not include forked repositories (Spinellis et al., 2020).

•
We discard repositories with fewer than ten stars and less than 1000 loc.

•
Following these criteria, we get a filtered list of  C# and 1721 Java repositories. We select a random subset of  Java repositories (by choosing a seed from the system clock for the random number generator) from the filtered Java repository list in order to mitigate the discrepancy between the volume of C# and Java code to be analyzed.

•
Finally, we download and analyze the selected  C# and  Java repositories.

4.2.2. Smell detection
We use Designite to detect smells in C# code. Designite(Sharma et al., 2016, Sharma, 2016) is a software design quality assessment tool for code written in C#. It supports detection of  implementation,  design, and seven architecture smells by analyzing source code properties at different granularities (method, class, and component). It also provides commonly used code metrics and other features such as trend analysis, code clone detection, and dependency structure matrix to help developers assess the software quality.2

Similar to the C# version, we have developed DesigniteJava(Sharma, 2018), which is an open-source tool for analyzing and detecting smells in a Java codebase. The tool supports detection of  design and ten implementation smells. Both the C# and Java versions implement the same rules to detect smells.

We use the console version of Designite (version 3.4.0) and DesigniteJava (version 1.5.0) to analyze C# and Java code respectively and detect the specified design and implementation smells in each of the downloaded repositories.

Manual validation: We conducted a manual validation to establish the accuracy of the used tools. We chose the well-known open-source repository DotNetOpenAuth3 for this purpose. The repository implements OpenAuth and OpenID protocol in C# and has a long development history (3500 commits at the time of writing this paper). It has been used by more than 19.6 thousand repositories and has attracted  stars. From this repository, we selected three projects DotNetOpenAuth.Core, DotNetOpenAuth.OpenId.RelyingParty.UI and OAuthClient. The selected projects contain 22,027 loc and  classes. We sought help from two volunteers to carry out manual validation—one volunteer works in a software development company (three years of industrial experience) and another volunteer is a computer science Ph.D. student with one year of industrial experience. None of the volunteers has worked on the analyzed repository before; however, they both have hands-on experience on complex industrial solutions and have a fair idea of software architecture and code smells.

We enforced the following protocol for the validation.

•
Each volunteer carried out the initial manual analysis individually without discussing it with the other volunteer.

•
Given their industry experience, they were familiar with the concept of code smell and were aware of commonly known smells. Each volunteer was presented the definition of each of the four considered design and implementation smells and interviewed to verify its correct understanding. We also provided additional material to accelerate their learning.

•
Both the individuals went through all source code files one by one and checked the existence of each smell following the corresponding definition.

•
While identifying smells, they were allowed to use ide features such as go to definition and list all references as well as metrics generated from other tools they wish to use.

•
While identifying smells, they were presented with method/class metrics.

•
Once both the volunteers completed the analysis, we computed Cohen’s Kappa (Cohen, 1960) to measure the mutual agreement between the volunteers’ findings. We obtained  as the value of Cohen’s Kappa.

•
Once both completed their individual analysis, they discussed their results, sorted out differences, and prepared a consolidated mutually agreed report of results.

•
Next, they used Designite and analyzed the consideredproject and obtained a list of design and implementation smells.

•
They compared the results obtained from the tool with their set of smells and tagged them as true-positive, false-positive, and false-negative.

Both volunteers manually scanned  classes for the two design smells and  methods for the two implementation smells. They found that the tool’s result matched their manual result except two instances of feature envy design smell. In both the cases, the tool was not counting the class members (i.e., methods and fields) belonging to another class when more than one member is referenced from the class under analysis. We fixed the problem in the tool before using it in our experiments; hence the tool shows perfect precision and recall for the smells considered in the experiment.

4.2.3. Splitting code fragments
CodeSplit is a set of two utility programs, one for each programming language, that split methods or classes written in C# and Java source code into individual files. Hence, given a C# or Java project, the utilities can parse the code correctly (using Roslyn for C# and Eclipse jdt for Java), and emit the individual method or class fragments into separate files following hierarchical structure (i.e., namespaces/packages become folders). CodeSplit for Java is an open-source project that can be found on GitHub (Sharma, 2019b). CodeSplit for C# can be downloaded freely online (Sharma, 2019a).

4.2.4. Generating training and evaluation data
The learning data generator requires information from two sources; a list of detected smells for each analyzed repository and a path to the folder where the code fragments corresponding to the repository are stored. The program takes a method (or class in case of design smells) at a time and checks whether the given smell has been detected in the method (or class) by Designite. If the method (or class) suffers from the smell, the program puts the code fragment into a “positive” folder corresponding to the smell otherwise into a “negative” folder.

4.2.5. Tokenizing learning data
Machine learning algorithms require the inputs to be given in a representation appropriate for extracting the features of interest, given the problem in hand. For a multitude of machine learning tasks it is a common practice to convert data into numerical representations before feeding them to a machine learning algorithm. In the context of this study, we need to convert source code into vectors of numbers honoring the language keywords and other semantics. Tokenizer (Spinellis, 2019) is an open-source tool that provides, among others, functionality for tokenizing source code elements into integers where different ranges of integers map to different types of elements in source code. Fig. 2 shows a small C# method and corresponding tokens generated by Tokenizer. Currently, it supports six programming languages, including C# and Java.

4.2.6. Data preparation
The stored samples are read into numpy arrays, preprocessed, and filtered. We first perform bare minimum preprocessing to clean the data—for both 1d and 2d samples—we scan all the samples for each smell and remove duplicates if any exist.

The data preparation steps are explained below.

•
We split the samples in the ratio of - for training; i.e., % of the samples are used for training a model while % samples are used for evaluation.

•
For the training samples, we perform the following steps.

–
We limit the maximum number of positive/negative samples to 5000. Therefore, for instance, if the number of negative samples is greater than 5000, we keep for the experiment exactly 5000 samples and drop the rest i.e., adopting an under-sampling technique (Pecorelli et al., 2020).

–
We perform model training using balanced samples, i.e., we balance the number of samples for training by choosing the smaller number between the positive and negative sample count; we discard the remaining training samples from the surplus.

•
For the evaluation samples, we perform the following steps.

–
The training and evaluation time depend on the number of samples. We limit the maximum number of positive/negative evaluation samples to 150,000 and 50,000 for implementation and design smells respectively to reduce the processing load. Even with these limits, all the experiments take  298 h to complete with the best hardware available to us. The upper limit of the samples is set way higher than the typical sample size in studies from the similar domain.

–
In the process of removing excess evaluation samples, we maintain the ratio between positive and negative samples for evaluation.

Table 2 presents data preparation process by providing number of samples in each step for all smells.

Each individual input instance, either a method in the case of implementation smells, or a class in the case of design smells, is stored in the appropriate data structure depending upon the model that will use it. In 1d representation, each individual input instance is represented by a flat 1d array of sequences of tokens, compatible for use with the rnn, cnn-1d and the autoencoder models. In the 2d representation, each input instance is represented by a 2d array of tokens, preserving the original statement-by-statement delineation of source code thus providing the grid-like input format that is required by cnn-2d models. All the individual samples are stored in a few files (where each file size is approximately  mb) to optimize the I/O operations due to a large number of files. We read all the samples into a numpy array and we filter out the outliers. In particular, we compute the mean input size and discard all the samples with length over one standard deviation away from the mean. This filtering helps us keep the training set in reasonable bounds and avoids waste of memory and processing resources. We pad the input array with zeros to the extent of the longest remaining input in order to create vectors of uniform length and bring the data in the appropriate format for using with the deep learning models. Finally, we shuffle the array of input samples along with its corresponding labels array.


Table 2. Number of samples in each step of preparing input data.

Initial samples	70–30 split	Applying max limit	Balancing
cm	Positive	Training	24,963	17,474	5,000	5,000
Evaluation	7,489	7,489	7,489
Negative	Training	464,866	325,406	5,000	5,000
Evaluation	139,460	139,460	139,460
cc	Positive	Training	6,186	4,330	4,330	4,330
Evaluation	1,856	1,856	1,856
Negative	Training	484,790	339,353	5,000	4,330
Evaluation	145,437	145,437	145,437
fe	Positive	Training	1,800	1,260	1,260	1,260
Evaluation	540	540	528
Negative	Training	170,439	119,307	5,000	1,260
Evaluation	51,132	50,000	50,000
ma	Positive	Training	293	205	205	205
Evaluation	88	88	85
Negative	Training	172,412	120,688	5,000	205
Evaluation	51,724	50,000	50,000
4.3. Selection of smells
Over the last two decades, the software engineering community has documented many smells associated with different granularities, scope, and domains (Sharma and Spinellis, 2018). A comprehensive taxonomy of well-established software smells can be found online.4 For this study, selection of smells is a crucial decision that needs to balance ambition with practicality. On the practicality front, the scope of the higher granularity smells, such as design and architecture smells, is wide, often spanning to multiple classes and components. It is essential to provide all the intertwined source code fragments to the deep learning model to make sure that the model captures the key deciding elements from the provided input source code. Hence, it is naturally difficult to detect them using deep learning approaches, unless extensive feature engineering is performed beforehand in order to attain an appropriate representation of the data. We started with implementation smells because they can be detected typically just by looking at a method. To address ambition, we would like to avoid very simple smells (such as long method) which can be easily detected by less sophisticated techniques.

We chose complex method (cm—i.e., the method has high cyclomatic complexity) and complex conditional (cc—i.e., a condition expression in a conditional statement such as if statement is complex). These two smells represent two dissimilar cases where neural networks have to spot specific features. To detect complex conditional, the neural networks must spot a specific range of tokens within only conditional blocks. On the other hand, detection of complex method requires looking at the entire method and the structural property within it (i.e., nesting depth of the method).

To expand the experiment’s ambition, we also select two design smells feature envy (fe—i.e., a method seems more interested in an abstraction other than the one it actually is in) and multifaceted abstraction (ma—i.e., a class has more than one responsibility assigned to it). The scope of these smells is larger (i.e., the whole class) and detection is not trivial since the neural network has to capture cohesion and coupling aspects. These smells not only allow us to compare the capabilities of neural networks in detecting implementation smells and design smells but also sets the stage for the future work to build on.

4.4. Architecture of deep learning models
In this section, we present the architecture of the neural network models that we use in this study. The Python implementation of the experiments using the Keras library can be found online (Sharma, 2021).

4.4.1. cnn model
Fig. 3 presents the architecture of the cnn model used to detect smells. This architecture is inspired by typical cnn architectures used in image classification (Krizhevsky et al., 2012) and consists of a feature extraction part followed by a classification part. The feature extraction part is composed of an ensemble of layers, specifically, convolution, batch normalization, and max pooling layers. This set of layers forms the architecture’s hidden layers. The convolution layer performs convolution operations based on the specified filter and kernel parameters and computes accordingly the network weights to the next layer, whereas the max pooling layer effectuates a reduction on the dimensionality of the feature space. Batch normalization (Ioffe and Szegedy, 2015) mitigates the effects of varied input distributions for each training mini-batch, thus optimizing training. In order to experiment with different configurations, we use one, two, and three hidden layers.

The output of the last max pooling layer is connected to a dropout layer. Dropout performs another type of regularization by ignoring some randomly selected nodes during training in order to prevent over-fitting (Srivastava et al., 2014). In our experiments we set the dropout rate for the layer to be equal to 0.1 which means that the nodes to be ignored are randomly selected with probability 0.1.

The output of the last dropout layer is fed into a densely connected classifier network that consists of a stack of two dense layers. These classifiers process 1d vectors, whereas the incoming output from the last hidden layer is a 3D tensor. The tensor corresponds to the height and width of an input sample and channel; in this case, the number of channels is one. For this reason, a flatten layer is used first, to transform the data in the appropriate format before feeding them to the first dense layer with  units and relu activation. This is followed by the second dense layer with one unit and sigmoid activation. This last dense layer comprises the output layer and contains a single neuron in order to make predictions on whether a given instance belongs to the positive or negative class in terms of the smell under investigation. The layer uses the sigmoid activation function in order to produce a probability within the range of  to .

We use dynamic batch size depending upon the size of samples to train. We divide the training sample size by  and use the result as the index to choose one of the items in the possible batch size array (, , , ). For instance, we use  as batch size when the training sample size is  and  when the training sample size is .


Download : Download high-res image (208KB)
Download : Download full-size image
Fig. 3. Architecture of the employed cnn models.

The hyper-parameters are set to different values in order to experiment with different configurations of the model. Table 3 lists all the different values chosen for the hyper-parameters. Filters is the number of convolutional filters employed, kernel size controls the size of the convolution window, and pooling window size governs the size of the down-sampling window during the pooling operation. We execute cnn models for  configurations that result from generating combinations of different values of hyper-parameters and number of repetitions of the set of hidden layers (). We label each configuration between  and  where configuration  refers to number of repetitions of the set of hidden layers  , number of filters  , kernel size  , and pooling window size  . Similarly, configuration  refers to number of repetitions of the set of hidden layers  , number of filters  , kernel size  , and pooling window size  . Both the 1d and 2d variants use the same architecture replacing the 2d version of Keras layers for their 1d counterparts.

We ensure the best attainable performance and avoid over-fitting by using early stopping.5 as a regularization method. This implies that even though the model is allowed to reach a predetermined maximum of  epochs during training, it may be forced to stop earlier. If there is no improvement in the validation loss of the trained model for five consecutive epochs (since patience, a parameter to early stopping mechanism, is set to five), the training is interrupted. Along with this, we also use model check point to restore the best weights of the trained model. We chose a maximum of  after carrying out a preliminary experiment which indicated that the majority of models would converge within this threshold. Among  total individual experiments for all four smells in RQ1 for cnn-1d, the models reached the maximum epoch only four times. In those cases, we stop the training and evaluate the model based on the weights at the last epoch.


Table 3. Chosen values of hyper-parameters for the cnn model.

Hyper-parameter	Values
Filters in convolution layer	{8, 16, 32, 64}
Kernel size in convolution layer	{5, 7, 11}
Pooling window size in max pooling layer	{2, 3, 4, 5}
Maximum epochs	{50}
For each experiment, we compute the following performance metrics: precision, recall, f1 score, and average precision score. We also record the actual epoch count where the models stopped training (due to early stopping). After we complete all the experiments with all the chosen hyper-parameters, we choose the best performing configuration and the corresponding number of epochs used by the experiment and retrain the model and record the final and best performance of the model.

4.4.2. rnn model
Fig. 4 presents the architecture of the employed rnn model which is inspired by state-of-the-art models in natural language modeling that employ an lstm network as a recurrent layer (Sundermeyer et al., 2012). The model consists of an embedding layer followed by the feature learning part — a hidden lstm layer. It is succeeded by the regularization (realized by a dropout layer) and classification (consisting of a dense layer) part.

The embedding layer maps discrete tokens into compact dense vector representations. One of the advantages of the lstm networks is that they can effectively handle sequences of varying lengths. To this end, in order to avoid the noise produced by the padded zeros in the input array, we set the mask_zero parameter to True in the Keras embedding layer. Thus the padding is ignored and only the meaningful part of the input data is taken into account. We set dropout and recurrent_dropout parameters of lstm layer to 0.1. The regular dropouts mask (or drop) network units at inputs and/or outputs whereas recurrent dropouts drop the connections between the recurrent units along with dropping units at inputs and/or outputs (Gal and Ghahramani, 2015). The output from the embedding layer is fed into the lstm layer, which in turn outputs to the dropout layer. As in the case of the cnn model, we experiment for different depths of the rnn model by repeating multiple instances of the hidden layer.


Download : Download high-res image (144KB)
Download : Download full-size image
Fig. 4. Architecture of the employed rnn models.

The dropout layer uses a dropout rate equal to 0.2, which we empirically found effective for preventing over-training, yet conservative enough for avoiding under-training. The dense layer, which comprises the classification output layer, is configured with one unit and sigmoid activation as in the case of the cnn model. Similarly to the cnn model, we use early stopping (with maximum epochs   and patience  ) and model check point callbacks. Also, we use the dynamic batch size selection as explained in the previous section.

We try different values for the model hyper-parameters; Table 4 lists the values selected for experimentation. The dimensionality of the embedding layer represents the size of each embedding vector; lstm units is the number of units in each lstm layer. We measure the performance of the rnn model in  configurations by forming the combinations produced by the different chosen values of hyper-parameters and the number of repetitions of the hidden lstm layer ().

As described earlier, we pick the best performing hyper-parameters and number of epochs and retrain the model to obtain the final and best performance of the model.


Table 4. Chosen values of hyper-parameters for the rnn model.

Hyper-parameter	Values
Dimensionality of embedding layer	{16, 32}
lstm units	{32, 64, 128}
Maximum epochs	{50}
4.4.3. Autoencoder model
Autoencoders are neural networks that can learn meaningful representations of the data in an unsupervised way. There exist diverse variants of autoencoders, however, in practice the purpose of all variants is to learn to reconstruct a representative copy of the given input. To this end, a bottleneck-like part between the input and the output layers encodes the input in a compressed representation which is in turn decompressed by a decoding part. The underlying principle is that the encoded representation captures salient features which are reflected in the reconstructed output and discards other, less important, thus providing dimensionality reduction and de-noising capabilities (Vincent et al., 2008).

A typical autoencoder model has essentially two sets oflayers – encoding and decoding layers – symmetrically built across the compression pipeline. The model produces an approximate, compressed representation of the input, which then attempts to reconstruct with some loss . In its simplest architectures an autoencoder consists of dense layers where the input is compressed by limiting the number of units in the intermediate hidden layers. Compression can also be implemented by imposing sparsity constraints on the hidden units that are being activated (Ng et al., 2011); this is effectuated by some regularization technique that adds a penalty term to the loss function. Besides autoencoder models implemented with dense layers, more complex architectures involve rnn and cnn hidden layers.

In the context of smell detection we experiment with a variety of autoencoder architectures, ranging from simple models built with dense layers, to more sophisticated models involving rnn and cnn hidden layers. We build the simple sparse autoencoder models with dense layers where we reduce the number of units in the intermediate layers and penalize the loss function through the L1-regularization procedure (Park and Hastie, 2007). We build more complex models by interpolating lstm or cnn layers with reduced dimensions between the input and the output. We use all variants of the autoencoders as classifiers of anomalies. We train the models to learn to represent patterns of non-smelly samples by using only negative (i.e., non-smelly) examples. We test the trained models on data that include both positive and negative samples. We use the reconstruction loss as a proxy for classifying an instance as smelly (Japkowicz et al., 1995, Hawkins et al., 2002, Williams et al., 2002). If for some instance the output of the model shows high loss, we accept that this example does not follow the pattern learnt by the model, which in turn implies classification of a positive instance of the smell under investigation.

As Fig. 5 shows, we have employed three variants of autoencoder models; the first variant uses dense, the second uses rnn, and the last variant mainly uses cnn-1d layers as the fundamental component that forms the model’s encoder and decoder layers. The convolution variant uses max pooling and upsampling layers with convolution layers in encoder and decoder respectively. Table 5 lists the hyper-parameters used for the autoencoder model. The number of units in the dense layer is the dimension of the output space of the layer, lstm units is the number of units in each lstm layer, and filters in the convolution layer is the number of convolutional filters applied. Kernel size controls the size of the convolution window, and pooling window size governs the size of down/up-sampling window during the pooling operation. For lstm layers, we set the values of dropout and recurrent_dropout to 0.1. The encoder and decoder layers are followed by a fully-connected dense layer. Once the training is complete, we find out the optimal performance of the trained autoencoder model by evaluating the performance at different values of the threshold.


Download : Download high-res image (279KB)
Download : Download full-size image
Fig. 5. Architecture of the employed Autoencoder models.


Table 5. Chosen values of hyper-parameters for the Autoencoder model.

Hyper-parameter	Values
Number of units (Dense)	{256, 512, 1024}
lstm units	{8, 16, 32}
Filters in convolution layer	{8, 16, 32, 64}
Kernel size in convolution layer	{5, 7, 11}
Pooling window size in max pooling	{2, 3, 4, 5}
and upsampling layer	
Epochs	{20}
4.5. Hardware specification
We perform all the experiments on the super-computing facility offered by grnet (Greek Research and Technology Network). The experiments were run on gpu nodes (8x NVidia V100). Each gpu incorporates  cuda cores. We requested  gpu node with  gb of memory for most of the experiments while submitting the job to the super computing facility. Some rnn experiments require more memory to perform the training; we requested  gb of memory for these.

5. Results and discussion
As elaborated in this section, we found that it is feasible to detect smells using deep learning models without extensive feature engineering. Our results also indicate that performance of deep learning models is highly smell-specific. Furthermore, we found that it is feasible to apply transfer-learning in the context of code smells detection. In the rest of the section, we discuss the results in detail.

5.1. Results of RQ1
RQ1
Is it possible to detect code smells using deep learning methods? If yes, which deep learning method performs superior?

5.1.1. Approach
We prepare the input samples as described in Section 4.2. Table 6 presents the number of positive and negative samples used for each smell for training and evaluation; cnn-1d, rnn, and ae use 1d samples and cnn-2d uses 2d samples. As mentioned earlier, we train our models with the same number of positive and negative samples (except in the case of ae where we use only negative samples to train the model). The one-dimensional sample counts are different from their two-dimensional counterparts because we apply additional constraint for outlier exclusion, on permissible height, in addition to the width.


Table 6. Number of positive (P) and negative (N) samples used for training and evaluation for RQ1.

cnn-1d, rnn, and ae	cnn-2d
Training	Evaluation	Training	Evaluation
p and n	p	n	p and n	p	n
cm	5000	7489	139,460	5000	5822	125,807
cc	4330	1856	145,437	3374	1446	129,933
fe	1260	528	50,000	1194	512	38,963
ma	205	85	50,000	189	82	39,071
5.1.2. Results
Fig. 6 presents the performance (i.e., f1 score) of the models for the considered smells for all the configurations that we experimented with. The results from each model show that performance of the models varies depending on the smell under analysis. Another observation from the trendlines shown in the plots is that performance of all the models remains more or less stable and unchanged for different configurations except for the rnn model with the complex method smell. This implies that despite the variability in the combinations of hyper-parameters that we experimented with, the effect on the particular models appears to be minor.

Table 7 presents the results of Mann–Whitney U test that we perform to ensure that each model performs differently than the other models. We also compute Hedges’ g (Becker, 2000) to figure out the effect size of the difference between each pair of deep learning models. Hedges’ g is similar to Cohen’s d (Yao and Shepperd, 2020) except the Hedges’ metric takes into account different sample sizes. The results in the Table show that almost all the model pairs are different and their effect size is significant.


Download : Download high-res image (723KB)
Download : Download full-size image
Fig. 6. Scatter plots of the performance (f1 score) exhibited by the considered deep learning models along with their corresponding trendline.

Fig. 7 presents the box plots comparing for each smell, the performance of all trained models, under all configurations. For all the analyzed smells, autoencoders outperform all of the other models. The f1 score values produced by all three variants of the autoencoder model are highly concentrated. The figure also shows that the variations in hyper-parameters do not affect the performance of the chosen autoencoder model. We also observe that the performance of individual model architectures vary from smell to smell; for instance, rnn shows small variance for feature envy smell but quite large for complex method smell.


Table 7. Results of Mann–Whitney U test and Hedges’ g effect size between the f1 values for all configurations of each considered model.

cnn-2d	rnn	ae
CM	cnn-1d	p  0.972, g  −0.015	p  0.001, g  1.60	p  4.33e−05, g  −7.12
cnn-2d	–	p  0.001975, g  1.62	p  4.336e−05, g  −7.52
rnn	–	–	p  7.7e−6, g  −2.09
CC	cnn-1d	e-16, g  −2.34	p  0.01, g  0.84	p  4.33e−05, g  −11.96
cnn-2d	–	p  2.58e−08, g  3.55	p  4.33e−05, g  −11.86
rnn	–	–	p  7.7e−6, g  −15.13
FE	cnn-1d	e-16, g  −1.56	p  0.002, g  0.8	p  4.31e−05, g  −8.03
cnn-2d	–	p  1.54e−06, g  2.03	p  4.33e−05, g  −4.57
rnn	–	–	p  7.7e−6, g  −24.08
MA	cnn-1d	p  4.77e−13, g  1.07	p  4.45e−06, g  1.32	p  6.07e−05, g  −4.97
cnn-2d	–	p  2.74e−06, g  4.23	p  4.33e−05, g  −63.98
rnn	–	–	p  2.62e−4, g  −27.94
RQ1.H1:It is feasible to detect code smells using deep learning methods.

Table 8 lists performance metrics (precision, recall, f1 score, mcc (Matthews Correlation Coefficient)) for the optimal configuration for each smell, comparing all four deep learning models. We present mcc also along with other accuracy metrics because mcc covers true negative instances as well which is not covered by the f1 score (Yao and Shepperd, 2020). The table also lists the hyper-parameters associated with the optimal configuration for each smell. Fig. 8 presents the performance (f1 score) of the deep learning models corresponding to each smell considered in this exploration. We use fully-connected neural network variant for ae in this experiment.

As regards implementation smells, for the complex method smell, even though autoencoders and rnn perform superior than the convolution models, the performance of all models under consideration is comparable. On the other hand, none of the models could identify complex conditional smell with a reasonable accuracy. This implies that the models could identify a smell that is exhibited through the structure of a method but could not successfully spot the smell characterized by micro-structure representing the conditional statements.


Download : Download high-res image (52KB)
Download : Download full-size image

Table 8. Performance (Precision, Recall, f1 score, mcc (Matthews Correlation Coefficient)) of all four models with configuration corresponding to the optimal performance. l: deep learning layers; f: number of filters; k: kernel size; mpw: maximum pooling window size; ed: embedding dimension; lstm: number of lstm units; e: number of epochs; u: number of units; t: threshold.

Smell	Performance	Configuration
p	r	f1	mcc	l	f	k	mpw	ed	lstm	e	u	t
cnn-1d	cm	0.46	0.60	0.52	0.54	2	32	5	4	–	–	15	–	–
cc	0.04	0.68	0.08	0.09	1	32	5	4	–	–	15	–	–
fe	0.03	0.69	0.06	0.07	1	8	11	2	–	–	31	–	–
ma	0.01	0.98	0.02	0.02	1	16	11	2	–	–	5	–	–
cnn-2d	cm	0.40	0.81	0.54	0.58	1	64	11	5	–	–	36	–	–
cc	0.07	0.60	0.13	0.14	2	64	7	2	–	–	22	–	–
fe	0.05	0.77	0.09	0.10	2	16	5	3	–	–	14	–	–
ma	0.01	0.92	0.02	0.02	2	64	11	2	–	–	6	–	–
rnn	cm	0.61	0.66	0.63	0.67	1	–	–	–	32	64	24	–	–
cc	0.04	0.65	0.08	0.10	1	–	–	–	32	64	3	–	–
fe	0.01	0.85	0.02	0.02	2	–	–	–	16	64	16	–	–
ma	0.00	0.07	0.01	0.01	2	–	–	–	16	128	11	–	–
ae	cm	0.60	0.68	0.64	0.67	1	–	–	–	–	–	20	32	319,000
cc	0.20	0.20	0.20	0.21	1	–	–	–	–	–	20	16	328,000
fe	0.18	0.24	0.21	0.22	2	–	–	–	–	–	20	16	325,000
ma	0.03	0.14	0.05	0.06	1	–	–	–	–	–	20	16	328,000

Download : Download high-res image (169KB)
Download : Download full-size image
Fig. 8. Comparative performance of the deep learning models for each considered smell.

Both of the design smells—feature envy and multifaceted abstraction—are non-trivial smells. Their detection requires analysis of method interactions to observe respectively coupling of a method with other classes, and incohesiveness of a class. For feature envy, autoencoders perform better than the other models; however, for multifaceted abstraction none of the employed deep learning models could capture the complex characteristics of the smell, implying that the token-level representation of the data may not be appropriate for capturing higher-level features required for detecting the smell.


Download : Download high-res image (81KB)
Download : Download full-size image
RQ1.H2: cnn-2d performs better than cnn-1d in the context of detecting smells.

Table 8 shows that the performance of cnn-1d and cnn-2d is comparable for complex method and feature envy smells. For complex conditional smell, cnn-2d does better than cnn-1d; probably due to a complex conditional statement contributes to a longer statement and cnn-2d could better identify it compared to cnn-1d using its 1-d form. Neither of the models could detect multifaceted abstraction smell instances. In summary, there is not sufficient evidence to conclude that cnn-2d is a superior model compared to cnn-1d.


Download : Download high-res image (51KB)
Download : Download full-size image
RQ1.H3: rnn models perform better than cnn models in the context of detecting smells.

Table 9 presents the comparison of rnn with cnn-1d and cnn-2d by comparing pairwise f1 measure differences in percentages, where the f1 score values are obtained by the optimal configuration in each case. Here, the performance difference in percentage is calculated by 
 
The rnn performs better for complex method smell against both convolution models. For complex conditional smell, cnn-2d performs better than both cnn-1d and rnn probably due to 2-d input samples could better represent the complex nature of conditional statements compared to its 1-d input form. However, the performance of rnn is lower for feature envy compared to both convolution models. Also, for complex conditional smell, rnn shows poorer performance compared to cnn-2d. To detect feature envy smell, it is required to identify complex relationships among methods and data members which rnn could not grasp.


Download : Download high-res image (66KB)
Download : Download full-size image
RQ1.H4: rnn and cnn variants of autoencoder model exhibit comparable performance to those of rnn and cnn-1d models.


Table 9. Performance (f1 score) comparison of rnn with cnn-1d and cnn-2d.

Smell	rnn vs. cnn-1d	rnn vs. cnn-2d
cm	16.71%	16.25%
cc	13.38%	−86.31%
fe	−153.57%	−159.62%
ma	−31.16%	9.65%
For complex method, rnn performs better than cnn-1d (refer to Fig. 8); however, within autoencoder model, cnn-1d is slightly better than rnn variant. Similarly, cnn-1d does better compared to rnn for feature envy smell but cnn-1d and rnn variants of autoencoder model show same performance. On the other hand, for complex conditional smell, rnn and cnn-1d both show similar performance in both configurations (see Fig. 9).


Download : Download high-res image (59KB)
Download : Download full-size image
5.1.3. Implications
This is the first attempt in the software engineering literature to show the feasibility of detecting smells using deep learning models from the tokenized source code without extensive feature engineering. It may motivate researchers and developers to explore this direction and build over it. For instance, context plays an important role in deciding whether a reported smell is actually a quality issue for the development team. One of the future works that the community may explore is to combine the models trained using samples classified by the existing smell detection tools with the developer’s feedback to identify more relevant smells considering the context.

Our results show that, even though both convolution methods perform superior for specific smells, their performance is comparable for each smell. This implies that we may use one-dimensional or two-dimensional cnn interchangeably without compromising the performance significantly.

Apart from experimenting with cnn and rnn-based models in various configurations, we also considered autoencoder. The autoencoder model treats a smells as a rare event; a simple autoencoder with one mid dense layer performs equally well with the more complex and deeper autoencoder configurations and better than the rnn and cnn based models. This observation provides grounds for further investigation, encouraging the software engineering community to propose simpler models for smell detection.

The comparative results on applying diverse deep learning models for detecting different types of smells suggest that a model is highly dependent on the kind of smells that it is trying to classify. This result could attract efforts from the software engineering community to develop smell-specific smell detection deep learning models.

5.2. Results of RQ2
RQ2
Is it possible to detect code smells by applying transfer-learning techniques on similar languages? If yes, which deep learning model exhibits superior performance in detecting smells when applied in transfer-learning setting?

5.2.1. Approach
In the case of direct-learning, the training and evaluation samples belong to the same programming language whereas in the transfer-learning case, the training and evaluation samples come from two similar but different programming languages. This research question examines the feasibility of applying transfer-learning i.e., train neural networks by using C# samples and employ the trained model to classify code fragments written in Java.

For the transfer-learning experiment (referred to as TL) we keep the training samples exactly the same as the ones we used in RQ1. For evaluation, we download repositories containing Java source code and preprocess the samples as described in Section 4.2. Similar to RQ1, evaluation is performed on a realistic scenario, i.e., we use all the positive and negative samples from the selected repositories and when enforcing maximum limit to samples we maintain the original ratio between positive and negative samples. This arrangement ensures that the models would perform as reported if employed in a real-world application. Table 10 shows the number of samples used for training and evaluation for this research question.


Table 10. Positive (P) and negative (N) number of samples used for training and evaluation for RQ2.

cnn-1d, rnn, and ae	cnn-2d
Training	Evaluation	Training	Evaluation
p and n	p	n	p and n	p	n
cm	5000	10,244	150,000	5000	5818	150,000
cc	4329	3,440	150,000	3374	2724	150,000
fe	1260	613	50,000	1194	682	50,000
ma	205	148	50,000	189	158	50,000

Download : Download high-res image (862KB)
Download : Download full-size image
Fig. 10. Scatter plots for each model and each considered smell comparing f1 score of direct-learning and transfer-learning along with corresponding trend-lines.

5.2.2. Results
As an overview, Fig. 10 shows the scatter plots for each deep learning model comparing the performance (f1 score) of both the direct-learning and transfer-learning for all the considered smells for all the configurations. These plots outline the performance exhibited by the models in both cases with trend lines distinguishing the compared series. The plots imply that the performance of the models are comparable in the transfer-learning and direct-learning cases. In the rest of the section, we report quantitative results on applying transfer learning between C# and Java.

RQ2.H1: It is feasible to detect code smells by applying transfer-learning techniques on similar languages.

Table 11 presents the performance of the models for all the considered smells demonstrating strong evidence on the feasibility of applying transfer-learning for smell detection. The performance pattern is in alignment to that in the direct-learning case; Spearman correlation between the performance produced by direct-learning and transfer-learning is 0.88 (with -value  ).


Download : Download high-res image (37KB)
Download : Download full-size image
Fig. 11 presents a comparison among the performance (i.e., f1 score) exhibited by all the considered deep learning models for each considered smell. Interestingly, cnn-2d performs superior to the rest of the models for all smells except feature envy; for feature envy smell, ae performs best. As indicated above, in general, the performance of the models follows a similar trend with the one observed in the case of direct-learning in RQ1.


Table 11. Performance of all four models with configuration corresponding to the optimal performance. l: deep learning layers; f: number of filters; k: kernel size; mpw: maximum pooling window size; ed: embedding dimension; lstm: number of lstm units; e: number of epochs; u: number of units; t: threshold.

Smell	Performance	Configuration
p	r	f1	l	f	k	mpw	ed	lstm	e	u	t
cnn-1d	cm	0.36	0.59	0.44	2	16	5	3	–	–	17	–	–
cc	0.08	0.18	0.11	2	32	7	2	–	–	9	–	–
fe	0.02	0.78	0.04	1	16	11	5	–	–	49	–	–
ma	0.01	0.78	0.01	1	64	11	5	–	–	5	–	–
cnn-2d	cm	0.36	0.82	0.50	1	16	11	4	–	–	30	–	–
cc	0.10	0.45	0.16	2	8	7	2	–	–	19	–	–
fe	0.03	0.37	0.06	2	16	7	3	–	–	24	–	–
ma	0.04	0.29	0.07	2	64	11	2	–	–	17	–	–
rnn	cm	0.31	0.57	0.40	1	–	–	–	16	32	5	–	–
cc	0.07	0.55	0.13	1	–	–	–	32	32	4	–	–
fe	0.03	0.64	0.06	1	–	–	–	32	128	10	–	–
ma	0.01	0.02	0.01	1	–	–	–	32	128	9	–	–
ae	cm	0.53	0.44	0.48	2	–	–	–	–	–	20	8	328,000
cc	0.09	0.23	0.13	1	–	–	–	–	–	20	8	328,000
fe	0.08	0.15	0.10	1	–	–	–	–	–	20	8	328,000
ma	0.03	0.20	0.06	1	–	–	–	–	–	20	8	328,000

Download : Download high-res image (194KB)
Download : Download full-size image
Fig. 11. Comparative performance of the deep learning models for each considered smell in transfer-learning settings.

RQ2.H2: The performance of transfer-learning is inferior compared to that of direct-learning.

Fig. 12 compares the performance of the models at their optimal configurations applied in transfer-learning and in direct-learning. We observe that, in the majority of cases, direct-learning performs better than the corresponding transfer-learning models. The exceptions are convolution models for complex conditional, rnn for feature envy, cnn-2d and ae for multifaceted abstraction smell, where transfer-learning shows better results.

We perform an additional experiment (referred to as TL) in which we reverse the direction of transfer-learning i.e., we train the models using Java samples and evaluate the trained models on C# samples. We download Java repositories and perform the data curation operations mentioned in Section 4.2 to compile a set of training and evaluation samples. Table 12presents the number of samples that are used for this experiment.


Download : Download high-res image (340KB)
Download : Download full-size image
Fig. 12. Comparison of performance of the deep learning models between direct-learning (DL) and transfer-learning (TL) settings.

We perform the experiment using the optimal configuration identified earlier and presented in Table 11. We present the obtained results in Table 13 for all the models with all the smells. We observe that the performance of the models in tl  experiment follows the similar pattern as we have seen in tl. We carry out Spearman correlation analysis between the performance of tl and tl  experiments. We found a strong correlation between the two with  (p-value  0.0002).

The ratio of positive and negative samples plays a significant role in the performance of a deep-learning model (Nucci et al., 2018, Pecorelli et al., 2020) and hence it is not easy to compare the performance of models trained with heterogeneous samples. We compute Normalized Performance Difference (npd) to compare the performance of models from direct-learning to transfer-learning. Normalized performance difference between methods i and j is given by the following equation. (1)
 

Here, f1  and f1  represent the performance (i.e., f1 score) and r  and r  refer to the ratio between negative and positive samples of each method. Table 14 presents the comparison of performance in the terms of both simple difference and normalized difference. Simple performance difference (pd) shows that transfer-learning performs inferior in the majority of instances; however, the normalized performance difference (npd) that scales the difference between performance proportionally indicates that transfer-learning does not have inferior performance compared to direct-learning.


Download : Download high-res image (36KB)
Download : Download full-size image
The above discussion leads to another interesting question: which deep learning model’s performance is the most or least sensitive to transfer-learning? We compute the npd between the performance pairs of direct-learning and transfer-learning for each considered model. Fig. 13 depicts the results; cnn-1d shows the highest difference in performance and hence cnn-1d is the most sensitive model to transfer-learning in this experiment. rnn on the other hand, shows the lowest difference in performance which renders the model the least sensitive and, consequently, the most robust for transfer-learning compared to the other models of this experiment.


Table 12. Number of positive (P) and negative (N) samples used for training (Java samples) and evaluation (C# samples).

cnn-1d and rnn	cnn-2d
Training	Evaluation	Training	Evaluation
p and n	p	n	p and n	p	n
cm	5000	7760	150,000	5000	7117	150,000
cc	5000	1843	150,000	5000	1669	150,000
fe	2183	496	50,000	1987	624	50,000
ma	545	82	50,000	483	105	50,000
5.2.3. Implications
Our results demonstrate that it is feasible to apply transfer-learning in the context of smell detection. Exploiting this approach can lead to a new category of smell detection tools, specifically for the programming languages where no mature smell detection tools are available.

5.3. Discussion
Although it is possible to detect some code smells using deep learning models, the presented method is by no means universal, and the outcome is sensitive to the training set composition and the training time. In the rest of the section, we elaborate on these observations emerging from the presented results.


Table 13. Performance of all four models with configuration corresponding to the optimal performance in tl  experiment. l: deep learning layers; f: number of filters; k: kernel size; mpw: maximum pooling window size; ed: embedding dimension; lstm: number of lstm units; e: number of epochs; u: number of units; t: threshold.

Smell	Performance	Configuration
p	r	f1	l	f	k	mpw	ed	lstm	e	u	t
cnn-1d	cm	0.06	0.87	0.11	2	16	5	3	–	–	17	–	–
cc	0.03	0.69	0.05	2	32	7	2	–	–	9	–	–
fe	0.02	0.83	0.04	1	16	11	5	–	–	49	–	–
ma	0.00	1.00	0.01	1	64	11	5	–	–	5	–	–
cnn-2d	cm	0.28	0.93	0.43	1	16	11	4	–	–	30	–	–
cc	0.01	0.93	0.03	2	8	7	2	–	–	19	–	–
fe	0.03	0.73	0.06	2	16	7	3	–	–	24	–	–
ma	0.01	0.18	0.01	2	64	11	2	–	–	17	–	–
rnn	cm	0.37	0.40	0.38	1	–	–	–	16	32	5	–	–
cc	0.05	0.02	0.03	1	–	–	–	32	32	4	–	–
fe	0.02	0.92	0.04	1	–	–	–	32	128	10	–	–
ma	0.00	0.11	0.01	1	–	–	–	32	128	9	–	–
ae	cm	0.42	0.43	0.43	2	–	–	–	–	–	20	8	328,000
cc	0.06	0.26	0.09	1	–	–	–	–	–	20	8	328,000
fe	0.04	0.35	0.07	1	–	–	–	–	–	20	8	328,000
ma	0.01	0.22	0.01	1	–	–	–	–	–	20	8	328,000

Table 14. Comparison of performance of transfer-learning with direct-learning. Here, dl and tl refer to performance of deep learning models in direct-learning and transfer-learning respectively. pd and npd refer to simple and normalized performance difference between direct-learning and transfer-learning respectively.


and

refer to the ratio of negative to positive samples for direct-learning and transfer-learning respectively.
5.3.1. Is there a silver-bullet?
In practical setting one would want to employ a universal model architecture that performs consistently well for all the considered smells; this would make the implementation of tools simpler.

rnn has the reputation to perform well with textual data and sequential patterns while cnn is considered good for imaging data and visual patterns. Given the similarity of source code and natural language, it is expected to obtain good performance from rnn. Our results show that rnn outperforms both cnn models in the cases of complex method; however, it does not live up to its reputation for other smells. ae is considered to be a good mechanism for learning to create copies of a given input where the key features are maintained; we observed that it works considerably well compared to other considered models. We have a uniform architecture for each model and we observed that the performance of the model differs significantly for different smells. It suggests that it is non-trivial, if not impossible, to propose a universal model architecture that works for all smells. Each smell exhibits diverse distinctive features and hence their detection mechanisms differ significantly. Therefore, given the nature of the problem, it is unlikely that one universal model architecture will be the silver-bullet for the detection of a wide range of smells with consistently good performance.

5.3.2. Performance comparison with baseline
A comparison with existing methods and tools is expected from a study proposing a new method. However, it is not feasible to compare the results presented in this paper with other attempts that use machine learning for smell detection  (Khomh et al., 2009, Khomh et al., 2011, Maiga et al., 2012b, Maiga et al., 2012a, Bryton et al., 2010, Barbez et al., 2019, Fontana et al., 2016) due to the following reasons. First, the replication packages of the related attempts are not available. Second, for most of the existing attempts, the ratio of positive and negative evaluation samples is not known; in the absence of this information, we cannot compare them with our results fairly since the ratio plays an important role in the performance of machine learning models. Furthermore, the existing approaches compute metrics and feed them to machine learning models as features. Models that only use metrics as the features can be as good as the metrics themselves. Metrics do not incorporate the context and hence the machine learning models based on the metrics do not exploit the power of machine learning because the models are used merely for selecting a threshold for the input metrics to classify smelly code from non-smelly code. This research attempts to move beyond the use of metrics as the only data source to detect the smells and bring more context sensitivity to the smell analysis. To the best of our knowledge, this is the first attempt to detect smells without using metrics as the features for the employed machine learning models. Due to this reason, it would be unfair to compare any machine learning model that uses metrics with the presented method. Also, it would be unfair to machine-learning based methods if we feed the raw tokenized source code as input because unlike deep learning methods, machine learning algorithms (such as Bayesian belief networks and support vector machines), treat each column of input as a specific feature. This assumption will not hold if tokenized source code is provided as input and the algorithms will perform very poorly.

The above discussion indicates two additional aspects. First, we, as software engineering community, need a manually validated gold-standard smells dataset for a wide range of smells. This would make bench-marking and comparison among different smell detection methods easy. Second, though this study shows that deep learning methods can grasp by themselves latent features that are necessary to identify smells and paves the way to make the smell detection more context sensitive, it is far from the level where it can be compared head-to-head with existing methods and surpass them in performance. In the future, we would like to explore combining source code in tokenized form with more refined features to help deep learning methods classify the smelly code with superior performance.


Table 15. Comparison of performance (Precision, Recall, and f1) with a random classifier (rc) following the training set frequencies or responding always indicating a smell.

Performance
Smell	Our results	rc (frequency)	rc (all smells)
p	r	f1	p	r	f1	p	r	f1
cnn-1d	cm	0.48	0.58	0.52	0.05	0.50	0.09	0.05	1	0.09
cc	0.04	0.70	0.07	0.01	0.50	0.02	0.01	1	0.02
fe	0.03	0.69	0.07	0.01	0.50	0.02	0.01	1	0.02
ma	0.01	0.98	0.01	0.00	0.50	0.00	0.00	1	0.01
cnn-2d	cm	0.38	0.83	0.53	0.04	0.50	0.08	0.04	1	0.08
cc	0.08	0.60	0.15	0.01	0.50	0.02	0.01	1	0.02
fe	0.04	0.78	0.07	0.01	0.50	0.02	0.01	1	0.02
ma	0.0	0.94	0.01	0.00	0.50	0.00	0.00	1	0.00
rnn	cm	0.72	0.55	0.63	0.05	0.50	0.09	0.05	1	0.09
cc	0.04	0.65	0.08	0.01	0.50	0.02	0.01	1	0.02
fe	0.01	0.87	0.03	0.01	0.50	0.02	0.01	1	0.02
ma	0.0	0.06	0.01	0.00	0.50	0.00	0.00	1	0.01
ae	cm	0.61	0.67	0.64	0.05	0.50	0.09	0.05	1	0.09
cc	0.21	0.20	0.21	0.01	0.50	0.02	0.01	1	0.02
fe	0.18	0.24	0.21	0.01	0.50	0.02	0.01	1	0.02
ma	0.03	0.12	0.04	0.00	0.50	0.00	0.00	1	0.01
We compare our results with the results obtained from two baseline random classifiers that do not really learn from the data but use only the distribution of smells in the training set to form their predictions. Table 15 presents the comparison. The first random classifier generates predictions by following the training set’s class distribution: that is, for every sample in the evaluation set it predicts whether it is a smell or not based on the frequency of smells in the training data. We did that for both balanced and imbalanced evaluation samples to mimic the learning process of the actual experiment. In the middle three columns, referred to as “rc (frequency)”, of the table we show the results for the balanced setting, as they were better than the results for the imbalanced setting. The second random classifier predicts that a smell is always present; this gives perfect recall, but low precision, as observed in the columns corresponding to “rc (all smells)” of the table. Overall, our models perform far better than a random classifier for all but multifaceted abstraction smell for both baseline variants.

5.3.3. Poor performance in detecting design smells
The presented neural networks perform very poorly when it comes to detecting the design smells feature envy and multifaceted abstraction. We infer the following two reasons for this under-performance. First, design smells such as feature envy and multifaceted abstraction are inherently difficult to spot unless a deeper semantic analysis is performed. Specifically, in the case of multifaceted abstraction, interactions among the methods of a class as well as the member fields have to be modeled in order to observe cohesion among the methods. This is a non-trivial aspect and the neural networks could not spot this aspect with the provided representation of the data. Therefore, we need to provide refined information in the form of engineered features along with the source code to help neural networks identify the inherent patterns. Second, the number of positive training samples were very low, thus significantly restricting our training set. The low number severely impacts the ability of neural networks to infer the responsible aspects that cause the smell. The future research attempts could address this limitation by increasing the number of repositories under analysis or by adopting techniques such as careful formation of artificial samples.

5.3.4. Variation in training-time
As observed in the results section, performance of the considered deep-learning models varies depending upon the smells. However, we also note that the models show considerable difference in the time consumed for training. We logged the time taken by each experiment for the comparison. Table 16 presents the average time taken by each model for each smell per epoch. The table shows that the rnn is consuming exorbitant amount of time compared to cnn and autoencoders. In the context of this study, this implies that if the performance of an rnn for a given task is comparable to that of a cnn or an autoencoder model, one should decline the rnn-based solution for significantly faster training time.


Table 16. Average training-time taken by the models to train a single epoch in seconds.

cnn-1d	cnn-2d	rnn	ae
cm	0.9	1.2	1155.5	3.8
cc	1.0	1.4	1575.9	3.3
fe	1.1	1.7	2284.6	3.5
ma	1.3	1.5	4997.7	2.6
5.3.5. Exploring other source code representations
In the recent times, we have witnessed a surge in the research towards alternative source code representations. The thriving progress on code mining research and specifically the increasing interest on problems pertinent to semantic code search has recently led to novel code-specific representations that incorporate structural features of a program (Allamanis et al., 2017, Alon et al., 2019, Alon et al., 2018). Such representations have been proven to perform well on the task they are tailored for, that is, learning semantics of specific source code fragments. Even though the afore-mentioned representations have been proven effective for capturing the semantics of programs, their generalizability to other code mining downstream tasks is questionable (Kang et al., 2019). In our study we aimed to address the problem of training models that capture qualitative characteristics in code. These are mostly manifest through syntactic features in the case of implementation smells and through class-specific method interaction in the case of design smells. Consequently, smell detection is agnostic to the semantics of the program under investigation. However, as a reference point we carried out an experiment using the current mainstream method for code representation, that is, code2vec.

We used the implementation provided by the authors of code2vec6 and modified it to suit our context. The originalcode2vec implementation is tightly coupled with the problem that its authors address and hence it was a non-trivial challenge to customize it to the needs of our classification problem. We changed the implementation to predict the presence or absence of smells by customizing the training of the code2vec model. During the training, we replaced the method names (as in the original implementation) with either true or false based on whether a smell is present or not. The trained model then predicted true or false indicating the presence or absence. The implementation we used can be found online.7

We preprocess and train the code2vec model using the same set of samples in the same number that we used for training all models, for the two implementation smells, i.e., complex method and complex conditional. Given that the code2vec model is designed to work at the method level, we did not use it for design smells that require class-level treatment. We run the model with the default parameters as proposed in the original implementation. The model performed mediocre with f1  0.22 (precision  0.16 and recall  0.35) for complex method and f1  0.06 (precision  0.03 and recall  0.26) for complex conditional smell. This performance is significantly lower than the performance shown by other models using simple source code tokenization. This confirms our speculation on the suitability of the afore-mentioned models for smell detection and agrees with the finding that a state-of-the-art model for semantic representation of code is not necessarily appropriate for downstream tasks.

5.4. Opportunities
This study encourages the research community to explore deep learning as a viable option for addressing the problem of smell detection. We showed that the solution is applicable in two programming languages, namely C# and Java. This result encourages further experimentation with additional programming languages of different paradigms.

We used the detection mechanisms of Designite for obtaining the ground truth to train our models. Relying on a specific tool does not alleviate the fact that smells are indeed detectable using deep learning methods—it rather provides grounds for generalization. A next step towards extending this work could be to investigate variations of smell definitions and diverse tool adaptions by accordingly fine-tuning training. To this end, we release the full pipeline of our deep learning toolkit and invite research in this direction. We are positive that this work will prove robust to extensions, given also the results that we obtained in the transfer-learning experiment.

We have shown that transfer-learning is feasible in the context of code smells. This result additionally introduces new, data-driven directions for automating smell detection which is particularly useful for programming languages for which smell detection tools are either not available or not matured.

Given that we did not consider the context and developers’ opinion on smells reported by deterministic tools, it would be interesting to combine these aspects either by considering the developers’ opinion (by manually tagging the samples) while segregating positive and negative samples or by designing models that take such opinions as an input to the model.

This work shows the feasibility of detecting implementation smells; however, complex smells such as multifaceted abstraction and feature envy require further exploration and present many open research challenges. Design and architecture smells typically span across multiple source files and abstractions. Furthermore, their detection involves identifying complex semantic features that makes design and architecture smell detection using machine learning methods difficult. The research community may build on the results presented in this study and further explore optimizations to the presented models, alternative models, or innovative model architectures to address the detection of complex design and architecture smells.

Smell samples used for training and evaluation are highly imbalanced naturally. We observed that in the best case it could be  negative samples per positive sample while it may go up to as skewed as  negative samples per positive sample (refer to Table 14). Compared to other deep-learning models, Autoencoders fit naturally in this context, because they are more robust to class imbalance. We anticipate that future research work would explore the potential of autoencoders in more detail.

Beyond smell detection, proposing an appropriate refactoring to remove a smell is a non-trivial challenge. There have been some attempts (Tsantalis et al., 2018, Biegel et al., 2011) to separate refactoring changes from bug fixes and feature additions. One may exploit the information produced from such tools and the power of deep learning methods to construct tools that propose suitable refactoring mechanism.

6. Threats to validity
Threats to the validity of our work mainly stem from possible faults in the employed tools, our assumption concerning similarity of both the programming languages, and generalizability and repeatability of the presented results.

6.1. Construct validity
Construct validity measures the degree to which tools and metrics actually measure the properties that they are supposed to measure. It concerns the appropriateness of observations and inferences made on the basis of measurements taken during the study.

In the context of using deep learning techniques for smell detection, we use Designite and DesigniteJava to detect smells in C# and Java code respectively and use these results as the ground truth. Relying on the outcome of two different tools may pose a threat to validity especially in the case of transfer-learning. To mitigate the risk, we make sure that both the tools use exactly the same set of metrics, thresholds, and heuristics to detect smells. Also, we ensure the smell detection similarity by employing automated as well as manual testing.

To address potential threats posed by representational discrepancies between the two languages we ensure that Tokenizer generates same tokens for same or similar language constructs. For instance, all the common reserved words are mapped to the same integer token for both the programming languages.

6.2. Internal validity
Internal validity refers to the validity of the research findings. It is primarily concerned with controlling the extraneous variables and external influences that may impact the outcome.

In the context of our investigation, exploring the feasibility of applying transfer-learning for smell detection, we assume that both programming languages are similar by paradigm, structure, and language constructs. It would be interesting to observe how two completely different programming languages (for example, Java and Python) can be combined in a transfer-learning experiment.

6.3. External validity
External validity concerns generalizability and repeatability of the produced results. The method presented in the study is programming language agnostic and thus can be repeated for any other programming language given the availability of appropriate tool-chain. To encourage the replication and building over this work, we have made all the tools, scripts, and data available online (Sharma, 2021).

7. Conclusions
The interest in machine learning-based techniques for processing source code has gained momentum in the recent years. Despite existing attempts, the community has identified the immaturity of the discipline for source code processing, especially when it comes to identifying quality aspects such as code smells. In this paper, we establish that deep learning methods can be used for smell detection. Specifically, we found that cnn, rnn, and autoencoder deep learning models can be used for code smell detection, though with varying performance. We did not find a clearly superior method between 1d and 2d convolution neural networks. Further, our results indicate that rnn performance is not consistently better than convolutional networks. Our experiment on applying transfer-learning proves the feasibility of practicing transfer-learning in the context of smell detection.

With the results presented in the paper we encourage software engineering researchers to build over our work as we identify ample opportunities for automating smell detection based on deep learning models. There are grounds for extending this work to a wider scope by including smells belonging to design and architecture granularities. Furthermore, there exist opportunities for further exploiting results and coupling with deep learning methods for identifying suitable refactoring candidates. From the practical side, the tool developers may induct the deep learning methods for effective smell detection and use transfer-learning to detect smells for programming languages where no appropriate code smell detection tools are available.