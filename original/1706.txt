Machine learning techniques are pervasive tools for emerging commercial applications and many dedicated machine learning computers on different scales have been deployed in embedded devices, servers, and data centers. Currently, most machine learning computer architectures still focus on optimizing performance and energy efficiency instead of programming productivity. However, with the fast development in silicon technology, programming productivity, including programming itself and software stack development, becomes the vital reason instead of performance and power efficiency that hinders the application of machine learning computers. In this article, we propose Cambricon-F, which is a series of homogeneous, sequential, multi-layer, layer-similar, and machine learning computers with same ISA. A Cambricon-F machine has a fractal von Neumann architecture to iteratively manage its components: it is with von Neumann architecture and its processing components (sub-nodes) are still Cambricon-F machines with von Neumann architecture and the same ISA. Since different Cambricon-F instances with different scales can share the same software stack on their common ISA, Cambricon-Fs can significantly improve the programming productivity. Moreover, we address four major challenges in Cambricon-F architecture design, which allow Cambricon-F to achieve a high efficiency. We implement two Cambricon-F instances at different scales, i.e., Cambricon-F100 and Cambricon-F1. Compared to GPU based machines (DGX-1 and 1080Ti), Cambricon-F instances achieve 2.82x, 5.14x better performance, 8.37x, 11.39x better efficiency on average, with 74.5, 93.8 percent smaller area costs, respectively. We further propose Cambricon-FR, which enhances the Cambricon-F machine learning computers to flexibly and efficiently support all the fractal operations with a reconfigurable fractal instruction set architecture. Compared to the Cambricon-F instances, Cambricon-FR machines achieve 1.96x, 2.49x better performance on average. Most importantly, Cambricon-FR computers are able to save the code length with a factor of 5.83, thus significantly improving the programming productivity.
SECTION 1Introduction
Machine learning techniques are pervasive tools for emerging commercial applications, including image recognition [1], [2], [3], speech recognition [4], [5], face cognition [6], [7], video analysis [8], [9], advertisement recommendation [10], and games [11], [12]. In recent years, many dedicated machine learning computers on different scales have been deployed in embedded devices, servers, and data centers. For example, Huawei Mate10 and P20 cellphones integrated Cambricon-1A machi ne learning processor core [13]. Apple iPhone X cellphones also integrated a machine learning subsystem to identify faces of users [14]. NVIDIA produced DGX-1 and DGX-2 machine learning computers based on NVIDIA GPU [15], [16]. Google announced a machine learning computer with 100 Petaflops peak performance based on TPU-3 chips [17]. Recently, IBM announced Summit, which is a machine learning supercomputer with 9216 POWER9 CPUs and 27648 NVIDIA V100 GPUs [18].

Currently, most machine learning computer architectures still focus on optimizing performance and energy efficiency instead of programming productivity. In Fig. 1, we try our best effort to summarize the power efficiencies of the most efficient machine learning accelerators proposed in the very year from 2012 to 2018. Obviously, the power efficiency keeps increasing at a dramatic speed, i.e., 3.2x each year. Neuflow achieves 230GOPS/W with IBM 45 nm technology in 2012 [19]. DianNao, a deep neural network accelerator proposed in 2014, improves the power efficiency by a factor of 4.05x. And in 2018, Conv-RAM achieves 28.1TOPS/W [20], i.e., 1213x improvement compared with those in 2012.


Fig. 1.
Power efficiency of recent proposed machine learning accelerators [19], [20], [21], [22], [23], [24], [25].

Show All

While energy efficiency of machine learning computers keeps increasing rapidly, programming productivity—including programming itself and software stack development—becomes the vital reason that hinders the deployment of machine learning techniques. Even if a machine learning computer has a high peak performance/energy efficiency, high-quality program and software stack are still essential to fulfill the actual performance and energy consumption requirements of machine learning applications.

Programming productivity is further compromised by different programming interfaces in a single machine learning computer. As illustrated in Fig. 2, a traditional machine learning computer often has many heterogeneous parallel components organized in a hierarchical way. While programming heterogeneous systems and parallel systems are already notoriously difficult, each layer in a traditional hierarchical machine learning computer may have a different programming interface, which further exacerbates the programming challenge. For example, a GPU-based machine learning computer, such as NVIDIA DGX-2 [16], contains heterogeneous chips, i.e., 2 CPUs (24 cores per CPU) and 16 V100 GPUs. Except that programming multiple GPUs requires manual work based on MPI or NCCL, programming a single GPU chip needs to use the CUDA language to manipulate thousands of GPU threads; programming CPUs needs to write C/C++ with parallel API support for tens of CPU threads. Moreover, even the software stack inside a single GPU is also quite complicated, which includes CUDA PTX for programming grids/blocks/threads in the GPU, and microcode for programming a stream processor [26]. Considering there have been so many different machine learning computers, the industry needs to put huge efforts on porting system software (including but not limited to libraries, algorithm primitives, programming frameworks, assemblers, and compiler backends) to machine learning computers. For instance, just in the Tensorflow alone, there are thousands of operators [27], and optimizing an operator (e.g., convolution) on a certain GPU can cost several months for a skilled developer. Porting an operator to a multi-GPU computer could be even more time-consuming. HuaWei and Cambricon have put hundreds of software developers to port programming frameworks to the machine learning subsystem in Mate10 cellphone [28].


Fig. 2.
A typical machine learning computer architecture.

Show All

In a nutshell, the programming productivity is greatly reduced by the heterogeneous, parallel, and layer-different nature of machine learning computer. Hence, we claim that an ideal computer for programmer should be homogeneous, sequential, and layer-similar, which allows simple sequential programming for machine learning system software and applications. Moreover, if all machine learning computers (even with extremely different scales) have the same ISA, then the burden of programmers can be further alleviated, since they do not need to implement and port machine learning system software again and again. Here the question is: Is it possible to develop a series of homogeneous, sequential, layer-similar, machine learning computers with the same ISA, which still have high efficiency?

To answer this question, we propose Cambricon-F, which can achieve easy-programming and high-efficiency for machine learning simultaneously. The key insight of Cambricon-F is to organize the components of a computer in a fractal way. Originally, the word “fractal” in math is used to describe complicated objects which exhibit similar patterns at different scales, known as expanding symmetry or evolving symmetry [29]. Without diving into the controversy in math, we borrow the concept of fractal for iterative decomposition with self-similar patterns to any scale, see Fig. 3 Top. Extended to computer domain, Cambricon-F is a series of homogeneous, sequential, multi-layer, layer-similar, machine learning computers with the same ISA. A Cambricon-F machine has a fractal von Neumann architecture to iteratively manage its components: it is with von Neumann architecture and its processing components (sub-nodes) are still Cambricon-F machines with von Neumann architecture and the same ISA. It features the fractal computing that iteratively decomposes an instruction on it into several instructions on low-layer sub-nodes. Hence, Cambricon-Fs with different scales can be used for different scenarios from embedded systems, desktops, data centers to supercomputers. As shown Fig. 3, a single-core accelerator, multi-core chip, multi-chip server, and multi-server system can be architected in a fractal way with the same ISA, for different scenarios in different scales. Thus, programmers only need to consider one sequential ISA to run the same code on any of such devices. Furthermore, we propose a reconfigurable FISA for fractal machine, which allows user-defined fractal instructions and user-specified executing procedures, to flexibly and efficiently support all the fractal operations. We further propose the Cambricon-FR machine learning computers to architectural support the reconfigurable FISA.

Fig. 3. - 
Top: A fractal graph example: Sierpinski carpet [30]. The graph is subdividing itself into smaller copies and continuing recursively. Botttom: Fractal computers, analogy to Sierpinski carpet.
Fig. 3.
Top: A fractal graph example: Sierpinski carpet [30]. The graph is subdividing itself into smaller copies and continuing recursively. Botttom: Fractal computers, analogy to Sierpinski carpet.

Show All

In this paper, we made the following major contributions.

We thoroughly find that common machine learning primitives can be considered as fractal operations, which can be decomposed into several smaller self-similar operations iteratively.

We proposed Cambricon-F, which is a series of homogeneous, sequential, multi-layer, layer-similar, machine learning computers with fractal von Neumann architecture and same ISA. By providing a sequential view to programmers, Cambricon-F can achieve easy-programming and high-efficiency simultaneously.

We summarize the four challenges in mapping different types of fractal operations onto Cambricon-F, including reduction operation mapping, fractal data management, communication congestion, and inter-instruction optimization. We propose a series of techniques to address the four major challenges.

We design and implement two Cambricon-F instances at different scale down to layout level and evaluate these Cambricon-F instances with quantitative experimental results. Compared to GPU based machines, with higher programming productivity (due to the same sequential ISA), Cambricon-F instances are also able to achieve better performance and efficiency.

We analyze the ineffectiveness in FISA and propose Cambricon-FR, which leverages a reconfigurable fractal instruction set architecture to efficiently and flexibly support all fractal operations.

We evaluate two Cambricon-FR instances, i.e., Cambricon-FR1 and Cambricon-FR100. Compared to GPU based machines, Cambricon-FR machines achieve 34.48x, 25.69x better performance on average. Most importantly, Cambricon-FR computers is able to save the code length with a factor of 5.83, thus significantly improving the programming productivity.

SECTION 2Fractal Operation and Machine Learning
In this section, we first analyze common machine learning techniques by decomposing them into computing primitives. Then we define the fractal operation, analyze three types of fractal operation with different computing dependencies, and demonstrate that all common machine learning computing primitives fall into the three types of fractal operation. We finally present the challenges in designing a fractal architecture that can effectively process all three types of fractal operations.

2.1 Machine Learning
Machine Learning Techniques. Machine learning techniques are usually computation&memory intensive and diverse in many aspects, such as processing flow, learning style, and training methodology. Fortunately, they are highly paralleled at different levels, and thus can be accelerated with heterogeneous machine learning computers, which equip dedicated devices, including GPU [31], [32], [33], FPGA [34], [35], [36], and even ASIC chips [23], [37], [38], [39], [40]. Here, we first decompose these techniques into computing primitives, then illustrate the mapping to fraction computing form.

Computing Primitives. We select six representative techniques and decompose the CPU execution time with typical dataset into their common primitives, see Table 1. Specifically, for the popularity of deep learning, we select the famous AlexNet [3] running with ImageNet [41] to represent convolutional neural networks (CNNs), a 3-layer multi-layer perceptron (MLP) to deep neural networks (DNNs). Others are k-means, k-NN, support vector machine (SVM), and learning vector quantization (LVQ). In line with previous works [23], [42], [43], [44], we decompose machine learning techniques into matrix and vector based operations. We aggregate operations such as vector multiplying matrix and matrix multiplying vector into matrix multiplying matrix, operations such as matrix adding/subtracting matrix, matrix multiplying scalar, and vector elementary arithmetics into element-wise operation. Hence we get seven major computing primitives after decomposition, including convolution (CONV), pooling (POOL), matrix multiplying matrix (MMM), element-wise operation (ELTW), sorting (SORT), and counting (COUNT). We still have CONV, POOL primitives instead of only using MMM for the convenience of analyzing and mapping emerging important deep learning algorithms. Note that IP is actually vector-multiplying-vector, which can also represent the fully connected layer in deep networks. It can be observed that these seven computing primitives characterize machine learning techniques mainly.

TABLE 1 Decomposing Execution Times of Typical Machine Learning Techniques into Common Primitives (IP: Inner Production; CONV: Convolution; POOL: Pooling; MMM: Matrix Multiplying Matrix; ELTW: Element-Wise Operation; SORT: Sorting; COUNT: Counting)

2.2 Fractal Operation
Fractal Operation. We say that an operation f(⋅) with an input tensor X is a fractal operation if there exists an operation g(⋅) allowing
f(X)=g(f(XA),f(XB),…),(1)
View Sourcewhere f(⋅) is the target operator, g(⋅) is the retrieving operator, X represents all operands of f(⋅), XA,XB,… are the subsets of X. Based on the relationship among XA,XB… and X, we can divide the fractal operations into three categories: independent, input dependent, and output dependent.

If XA,XB… are independent, non-overlapped to each other, each subset is independent that they can be computed locally, i.e., independent. In Fig. 4a, we use a vector adding operation as an example to present independent fractal operation. For clear illustration, we split X into two operands, i.e., x⃗ ,y⃗ —two input vectors for adding. As x⃗  and y⃗  can be divided into two independent pieces (xA→,xB→ and yA→,yB→), two vector adding operations can be achieved independently, i.e., zA→=xA→+yA→ and zB→=xB→+yB→. Each piece is working on independent part of the inputs and the final outputs just need assemble with no additional operation, i.e., z⃗ =[zA→,zB→]. Thus, g(⋅) is linear function g(x)=x.


Fig. 4.
Fractal operation dependency: (a) independent; (b) input dependent; (c) output dependent.

Show All

If XA,XB… are overlapped, each subset requires extra copies of some inputs that leads input redundancy in the fractal operation, i.e., input dependent. For example, a one-dimensional convolution as shown in Fig. 4b. Similarly, we use x⃗ ,y⃗  to represent two operands and x⃗ =[xA→,xB→]. We still divide the operation into two pieces, where each piece is working on independent part of outputs, i.e., z⃗ =[zA→,zB→]=x⃗ ⋆y⃗ =[xA→,xB→]⋆y⃗ . However, these two operations have overlapped inputs, where parts of xA→ and xB→ (xa→,xb→, respectively) are required additionally, i.e., zA→=[xA→,xb→]⋆y⃗  and zB→=[xa→,xB→]⋆y⃗ . But, there is still no additional operation for final outputs, i.e., g(x)=x.

In some cases, g(⋅) is introduced to reduce the results of pieces into the final results, i.e., output dependent. For example, as shown in Fig. 4c, an inner production operation (z=x⃗ ⋅y⃗ ) can be divided into smaller pieces where each piece still performs an inner production operation (zA=xA→⋅yA→ and zB=xB→⋅yB→); but to get the final results, the results of those pieces will be summed up, i.e., z=zA+zB. Thus, g(⋅) is the sum operation, g(⋅)=sum(⋅). Note that a fractal operation can be both output dependent and input dependent.

2.3 Fractal Computing for Machine Learning
We present how machine learning computing primitives can be accomplished in a fractal form (i.e., fractal computing) and analyze the challenges for designing corresponding architecture. Based on the above analysis, we can classify all machine learning primitives into three categories, see Table 2. Note that different decomposition can lead to different dependence. For example, CONV can divide the input features maps in channel dimension, where the final outputs rely on results from each divided pieces (thus output dependent), as shown in Fig. 5 (left); CONV can divide the input feature maps in height or width dimension, where each part of the output results only need inputs with some overlaps (thus input dependent), as shown in Fig. 5 (right).

TABLE 2 Computing Primitives Analysis


Fig. 5.
CONV decomposition. Left: dividing in channel dimension. Right: dividing in height dimension.

Show All

More importantly, to effectively process fractal operations, fractal architecture should be built hierarchically with a tree-like topology where several son nodes compose a father node iteratively, see example Cambricon-F architecture shown in Fig. 3. Obviously, independent operations are easily mapped to such fractal architecture and computed fractally. Also, input dependent can be transformed to independent with input redundancy. For the 1D convolution operation in Fig. 4b, each part only needs some more inputs from X then the fractal operation is independent. In Table 2, we present the analysis of decomposition of computing primitives in a fractal form. Additionally, we present the data redundancy if using independent decomposition instead of input dependent. For the output dependency operations, g(⋅) is inevitable no matter whether inputs are dependent or independent. Thus, it is totally feasible to perform machine learning computations in a fractal form. But for designing fractal architecture, we must solve the following challenges related to extra data redundancy and reduction operation g(⋅):

Reduction Operation. Reduction operation g(⋅) in output dependent operations are not naturally fitted in fractal operation as independent and input dependent operations. Thus, for efficiently processing g(⋅), we introduce lightweight computing unit (i.e., LFU) in each node locally. By aggregating data in son FFUs into a father LFU iteratively, such operations can be processed efficiently in father LFUs in Cambricon-F. We introduce that in detail in later Sections 3.1, 3.2, 3.3.

Data Redundancy. In fractal operation computing, input dependent operations can be computed as independent operations but with data redundancy. For that, the memory is hierarchically organized and the memory allocation leveraging the separable time order (Section 3.5).

Communication. Communication among different nodes would lead to enormous wire connections and consequently to be costly in terms of area, latency, and energy. For that, from our analysis, even the output dependent operations only require data movements from leaf to root node for reduction operations. Thus, it is unnecessary to have communication between any pair of nodes. In Cambricon-F, we organize the machine learning computations iteratively in a fractal form and limit the connections to father-son nodes only, thus reducing the wire congestion (Sections 3.3, 3.4).

In summary, after addressing the above concerns, the fractal architecture would be able to achieve at least comparable efficiency with traditional architecture for machine learning applications.

SECTION 3Cambricon-F Computers
In this section, we present the Cambricon-F computers from the architects’ perspective, including overall architecture, instruction set architecture, decoder, pipeline, memory hierarchy, and implementation details.

3.1 Fractal von Neumann Architecture
A Cambricon-F machine has a fractal von Neumann architecture, which is hierarchical architecture built iteratively, as illustrated in Fig. 6. At the top level (root node), programmers should only learn a simple von Neumann architecture that contains a memory component (Mem), a functional unit (FU), and a controller with a decoder inside to decode instructions. In the middle levels, each node is still with von Neumann architecture, containing a controller (it can be either hardware or software), a memory component, several processing units including local functional units (LFU) and several fractal functional units (FFU). Each FFU is a son node (Level i+1) of the current node (Level i) and has the same ISA and similar architecture. At the bottom level, each leaf node is an accelerator that finishes the most part of the computation. Therefore, a Cambricon-F machine is built with a fractal von Neumann architecture to iteratively manage its components.


Fig. 6.
A typical fractal von Neumann architecture: level 0 (top node)...level i node and its son node in level i+1...level N (leaf node).

Show All

The ISA of Cambricon-F is Fractal Instruction Set Architecture (FISA), where each fractal operation can be performed with one or more FISA instructions. FISA includes two different kinds of instructions: local instructions and fractal instructions. For a local instruction, the controller can directly issue it to an LFU, and the LFU will complete the local instruction. For a fractal instruction, the controller will translate into several instruction segments, where each instruction segment is solved by an FFU. Hence, programming Cambricon-F only needs to consider a single sequential ISA, while the heterogeneity can be implicitly solved through the collaboration between LFUs and FFUs, and the parallelism can be implicitly solved through the parallelism between FFUs. Since an Cambricon-F computer and its all descendant Cambricon-Fs/FFUs have the same ISA, a programmer does not need to consider the difference between different layers of a machine learning computer. Moreover, different Cambricon-F computers with different scales (either a machine learning supercomputer or a small machine learning subsystem in a cellphone) can use the same ISA, which allows a same binary code to run on platforms from cloud to end.

To efficiently process fractal operations, Cambricon-F adopts a hierarchical memory system. Cambricon-F manages the storage in two types: global memory and local memory. At the top level, Cambricon-F contains a larger memory for buffering input data, i.e., the global memory, which is also visible to programmers. Each node in Cambricon-F contains a local storage to buffer the data, which will become a “global memory“ shared among its son nodes. In such a manner, we manage all the memory in Cambricon-F hierarchically.

3.2 Instruction Set Architecture
Cambricon-F leverages a special instruction set architecture to achieve the fractal computing, i.e., Fractal Instruction Set Architecture. Formally, we give the definitions of FISA instruction and FISA:

FISA instruction. A FISA instruction, I, is a 3-tuple ⟨O,P,G⟩, where O is an operation, P is a finite set of operands, G is granularity indicator.

Fractal instruction. A FISA instruction, I⟨O,P,G⟩, is a fractal instruction, iff there exists a set of scale indicators G′1,G′2,…,G′n (G′i⪯G, ⪯ is the partial order defined on scale indicators) that I can be achieved through computing with I′1(G′1),I′2(G′2),…,I′n(G′n) and other FISA instructions iteratively.

An ISA set is a FISA set, iff it contains at least one fractal FISA instruction.

A machine M running FISA set is a fractal machine, iff there exists at least one fractal instruction that is fractal-executed on M.

The FISA design for Cambricon-F stays at a relatively higher level so as to improve the programming productivity with same sequential code, as in Table 3 where we show a subset of FISA. Primitives such as convolution and sorting can be directly expressed with FISA instructions. Operations of low operation intensity (e.g., Element-Wise Operations) are also supported in FISA for better programming versatility. Such instructions will be considered as a reduction operation by Cambricon-F and tend to execute on LFUs.

TABLE 3 Examples of Cambricon-F Instructions

3.3 Controller
The controller exists in each node in a Cambricon-F, serving to manage its son nodes working in a fractal manner. From a functionality perspective, the controller consists of three phases: a sequential decomposition phase, a demotion phase, and a parallel decomposition phase. Several specific modules are thusly designed in pipeline stages to accomplish the transformation procedure from input instructions to sub-level nodes, including FFUs and LFUs, see Fig. 7. Briefly, in sequential decomposition phase, input instructions are loaded into Inst Queue (IQ), which is later fetched by Sequential decomposer (SD). SD decomposes into a sequential executed instruction list regarding the hardware limitation. In demotion phase, reformed instructions in the list are decoded by Demotion Decoder (DD) into sub-level instructions. In parallel decomposition phase, sub-level instructions for fractal computing will be passed to FFUs through a Parallel decomposer (PD), for local computations to LFUs through a Reduction Controller (RC), for data movements to DMA Controller (DMAC) to access memory.


Fig. 7.
Pipeline partition in an Cambricon-F node.

Show All

Particularly, Demotion Decoder, the key component in controller, decode input upper instructions to sub-level instructions to be fractally computed. For each sub-level instruction SQ, DD checks operand dependencies to instructions running in the pipeline. DD will stall the pipeline if a read- after-write (RAW) dependency exists. DD also checks the storage requirements of operands, allocates memory space locally, and generates DMA instructions. DMA instructions will be sent to DMAC for data exchange between local memory and “shared memory” in upper level, e.g., loading sources or writing back results. DD then binds the new local addresses to operands in sub-level instructions which later sent to PD for fractal computing, RC for reduction operations.

Parallel decomposer subdivides sub-level instructions into multiple FISA instructions, i.e., fractal instructions, that are assigned FFUs. FFUs process fractal FISA instructions during the EX pipeline stage in parallel.

Reduction Controller aims to perform the reduction operation in output dependent fractal operations normally. However, reduction operation can be assigned to FFUs instead of LFUs for high efficiency, when RC predicts significantly reduced execution time on FFUs or founds LFUs unavailable. In cases, Reduction Controller send a commission to PD by writing the operation into Commission Register (CMR). PD will check if there is a commission in the register at the start of each FISA cycle, and append the commissioned operation.

3.4 Pipelining
Normally, Cambricon-F performs each instruction recursively. Top level (level 0) node decodes and sends fractal instructions to its FFUs where each FFU repeats the decoding/sending procedure until the leaf nodes for execution. Leaf nodes return computed results to their father nodes and that repeats until the top node. During such execution process, FFUs in leaf nodes, where the heavy computation tasks are performed, are idle when its upper-level nodes are decoding instructions recursively.

Thus, in order to increase the throughput of Cambricon-F, we pipeline the FISA instruction execution into five stages: Instruction Decoding (ID), Loading (LD), Execution (EX), Reduction (RD) and Writing Back (WB), see Fig. 7. Similar to pipeline in CPUs, an issued instruction will be decoded local instructions, fractal instructions, and DMA operations in the Controller (ID). Data is loaded from memory to local storage for FFUs and LFUs computation (EX stage) with DMA operations at LD stage. LFUs will start the reduction operations at RD stage or be bypassed if no reduction operation needed. The final or partial results will be written to memory from local storage at WB stage. Note that SD is executed asynchronically where SD keeps decompositing instructions from IQ to SQ.

As each node in Cambricon-F executes its 5-stage pipeline for its instructions, Cambricon-F will execute FISA with a recursive pipeline. In Fig. 8, we show the pipeline for a two-level Cambricon-F. In the EX stage of level 0, FFUs run their own pipeline, i.e., level 1 pipeline. As a result, the recursive pipeline of FISA has utilized every component of every hierarchy at almost any time, except the pipeline startup and emptying.


Fig. 8.
Fractal pipeline of FISA.

Show All

3.5 Memory Management
As each phase in the Controller may require memory allocation, memory management is challenging and more critical to the overall efficiency. Fortunately, we observe that memory blocks for parallel decomposition only live in EX and sometimes RD pipeline stage, and blocks for demotion live in the whole FISA cycle. But memory blocks for sequential decomposition may live across multiple FISA cycles since there could be multiple sub-level instructions decomposed from the FISA instruction.

As shown in Fig. 9, memory space is always allocated in the list order, which is consistent with the time order that Controller requests. For the memory allocation alive for multiple FISA instructions, i.e., those for sequential decomposition, we use the fourth memory space (static segment) which is shared by every pipeline stage for their different lifecycles. Memory allocation to static segment are double-ended for parity of instructions to avoid overlapped memory lifecycles of adjacent instructions. The design of the allocation list significantly reduces the complexity of memory management, and keep the space utilization efficiency.

Fig. 9. - 
Memory Management of Cambricon-F Controller. The memory space is divided into 4 segments (3 recycling and 1 static), managed as 5 stacks (2 stacks in the static segment).
Fig. 9.
Memory Management of Cambricon-F Controller. The memory space is divided into 4 segments (3 recycling and 1 static), managed as 5 stacks (2 stacks in the static segment).

Show All

Here, we do not manually release the allocated memory space, where new instruction will directly refill with new data. The reason is two-fold. First, results of instruction will be written back and the remain inputs or intermediate results are usually useless for later computations, as our FISA instructions work at a relatively higher level. Second, for seldom cases that following instructions may share the inputs/outputs, we implement a Tensor Transposition Table (TTT) to find out data that can be forwarded or reused, resulting in a similar behaviour as “pipeline forwarding” in common practices of processor pipelines.

3.6 Data Consistency and Coherence
In Cambricon-F, we apply many constraints to manage the data consistency and coherence problems. Since our system will decompose the operation into smaller non-overlapped segments for son nodes execution; thus, data may have many copies in different nodes. However, as presented in Section 3.5, our instruction generation will not allow write data to the read address space, thus ensuring data consistency naturally in most of the cases. Additionally, Tensor Transposition Table introduces the risk of data inconsistency as it forwards data from write address space to read address space. Instead of implementing costly consistency protocols, we set up a validity period for each record in TTT. TTT is split into banks as memory spaces does, where each bank only maintains the information of its corresponding memory segment. Whenever the segment is recycled, new data are allocated on, and old data may be overwritten thus never safe to forward again, the TTT invalidates all records. Thus, with such validity mechanism, the lifetime of records won't exceed the lifetime of the referenced data. To guarantee the data coherence, simultaneous memory writes into the same memory address are always prohibited. The destination addresses of instructions assigned to each FFU will always be different.

SECTION 4Programming and Execution
Programming. With all the effort to provide programmers with sequential programming experiences, Cambricon-F are able to run the same piece of code without any other work. In Fig. 10, we show a typical Cambricon-F inline assembly code using a k-Nearest Neighbor algorithm as a driving example. The principle of FISA is that the nodes perform their own duties and Do Not Interfere with how the child nodes work. The programmer of Cambricon-F, which acts as the “controller” beyond the top level node, also follows the principle. The programming of Cambricon-F has the following characteristics:

High level, arbitrary granularity. Each FISA instruction is corresponding to a complete machine learning primitive. The programmer does not interfere with how the operation is decomposed. High-level instructions bring higher operational intensity and help decrease data movements.


Fig. 10.
An cambricon-F program of k-NN.

Show All

Implicit data movement. Contrary to RISC, Cambricon-F does not provide explicit load-store instruction to the programmer. FISA hides the internal storage from the programmer by forcing all operands to be external. The programmer does not interfere with how the internal storage is used, so the program does not need to adapt to different internal storage sizes when applied to different Cambricon-F instances or nodes.

Hardware transparency. Note that there is no hardware information appeared in the code. The programmer of Cambricon-F only dedicates on defining the computation task, and do not interfere with the internal hardware behaviors.

For the next level nodes, the controller of the parent node acts as a programmer. The Do-Not-Interfere principle reduced the complexity of the programming, meanwhile, it also reduced the complexity of the controller.

Execution on Different Cambricon-F Instances. The execution model of Cambricon-F can be summarized as Single Task, Multiple Heritors (STMH). As shown in Fig. 11, a task is executed simultaneously on every hierarchies of Cambricon-F, where each hierarchy see a part of the task with different granularity. STMH defines how two adjacent hierarchies cooperate reducing the granularity to inherit the task from the higher hierarchy to the lower hierarchy. More specifically, the cooperating mechanics can be decoupled to two relations: the relation with parent node, and the relation between sibling nodes. Here, we define the paternity relation via Sequential decomposer, and the sibling relation via Parallel decomposer. Given the paternity and sibling relations, and under the assumption that leaf nodes can solve the assigned tasks directly, the execution of whole machine is clearly defined, regardless what configuration does the Cambricon-F instance have.

Fig. 11. - 
STMH execution model.
Fig. 11.
STMH execution model.

Show All

SECTION 5Methodology
Benchmarks. As shown in Table 4, we use seven different benchmarks in this paper. For the importance of deep learning, we select VGG-16 [45], a 16-layer CNN with 138 M parameters in total, and ResNet-152 [46], a very deep network with 152 layers, running with ImageNet [41] dataset as representative benchmarks. We also select four popular machine learning techniques, including k-NN, k-Means, LVQ, and SVM, as representative benchmarks. For these four machine learning techniques, we use a randomly generated data set, which contains 262 thousand 512-dimension samples within 128 categories, to emulate a computation-heavy scenario. Additionally, as MatMul is the most important operation in the machine learning domain, we also include MatMul running with randomly generated 32768-order square matrices as our benchmark.

TABLE 4 Benchmarks

GPUs. In this paper, we select two GPUs as our baseline, i.e., Nvidia DGX-1 [15] and Nvidia GeForce GTX-1080Ti. DGX-1 is a supercomputer with eight NVIDIA Tesla V100-SXM2 GPUs, where each has a 125TeraOps/sec peak performance. The bandwidth from the host to devices is measured as 84.24 GB/s in total. 1080Ti is a high-end graphics card with 10.6TeraOps/sec peak performance and 484 GB/s memory bandwidth. For DGX-1, we program the benchmarks under the framework TensorFlow 1.9 [27] with GPU support (CUDA 9.0 [47] and cuDNN 7 [48]), and optimize the computation graph via NVIDIA TensorRT 4 [48]. We use nvprof and nvidia-smi to measure its power and memory bandwidth usage.

Cambricon-F. We build two different size Cambricon-F instances that have similar characteristics as GPUs, i.e., Cambricon-F100 and Cambricon-F1, for a fair comparison to GPUs. Cambricon-F100 is a fractal machine learning supercomputer with a peak performance of 956 Top/s, similar to DGX-1 (125*8 = 1000 Tops/s). Cambricon-F100 is a five-level architecture of Server, Card, Chip, Fractal Multiprocessor (FMP), and Core in each level from top to bottom, see Table 5. At the top level (L0), Cambricon-F100 contains four Cambricon-F100 Computing Cards connected through PCI-E 3.0, a host CPU (Intel Xeon E5-4640 v4) serving as high-level controller and LFU, and 1TB host memory. The leaf node (L4) is a Cambricon-F accelerator serving as a computing Core, which has 256 KB eDRAM local storage, 16×16 MAC matrix running at 1 GHz, reaching peak performance of 477 GOPs/s. Cambricon-F1 is a Cambricon-F accelerating card at desktop scale with a peak performance of 14.9 Top/s, similar characteristics to 1080Ti (10.6 Top/s). Cambricon-F1 has a three-level architecture of Card, FMP, and Core in each level from top to bottom, see Table 5. Cambricon-F1 has one FMP on-chip and that has 32 cores inside.

TABLE 5 Specification of Cambricon-F Instances

To obtain the hardware characteristics, we implemented the Cambricon-F designs (up to chip level) in RTL and synthesize, place, and route using Synopsys toolchain under TSMC 45 nm technology. Fortunately, Cambricon-F is a fractal architecture built iteratively, we are able to estimate the hundreds millimeter square design using smaller pieces following bottom-up design philosophy. Due to the extreme long hardware emulation time and large design, we carefully build a simulator in C++ to get the performance. For energy costs, we dump data movements from our simulator and estimate memory costs with DESTINY [49], other parts are estimated based on our layout characteristics.

SECTION 6Experimental Results
We first present the main characteristics of Cambricon-F instances, then present the performance and energy results when comparing against GPUs and accelerators. The experimental results are shown in Fig. 12, where we adopt the Roofline Model [50] to illustrate the efficiency and bottleneck of the systems.

Fig. 12. - 
Roofline Cambricon-Fs compared to GPUs. (a) Cambricon-F1 and 1080Ti. (b) Cambricon-F100 and DGX-1.
Fig. 12.
Roofline Cambricon-Fs compared to GPUs. (a) Cambricon-F1 and 1080Ti. (b) Cambricon-F100 and DGX-1.

Show All

Hardware Characteristics. The layout of a Core, a FMP (same as a Cambricon-F1 Chip) and a Cambricon-F100 Chip are shown in Fig. 13. In Table 6, we present the detailed hardware characteristics of the chip in Cambricon-F100 and Cambricon-F1. Cambricon-F1 occupies 29.21 mm2 area, consuming a power of 4.94W, where each core has an area cost of 0.43 mm2, a power of 75.18 mW at 45nm. Cambricon-F100, which is a 8-chip server having 2048 cores in total, has an area of 415mm2 in total, consuming a power of 42.87W at 45 nm. It can be observed that Cambricon-F favors large memory.

TABLE 6 Cambricon-F Layout Characteristics


Fig. 13.
Layout of Cambricon-Fs. Left: Leaf Core. Mid: FMP(Cambricon-F1 Chip). Right: Cambricon-F100 Chip.

Show All

In Table 7, we also compare Cambricon-F chips with GPUs and accelerators. It can be observed that Cambricon-F1 chip has the highest power efficiency and area efficiency, 3.02 Tops/W and 0.51 Tops/mm2. Cambricon-F100 chip achieves the comparable area efficiency, but slightly lower power efficiency when compared against Google TPU [40]. While considering the entire card where 32 GB DRAM is included in each Cambricon-F Computing Card, Cambricon-F1 has a 40.57 percent more peak performance, but with 45.11 percent power cost of 1080Ti GPU card and Cambricon-F100 Computing Card has a 1.90x more peak performance with 67.34 percent power cost of a V100-SXM2 GPU card.

TABLE 7 Hardware Characteristics Comparison

Cambricon-F1 versus 1080Ti. As shown in Fig. 12a, Cambricon-F1 has attained a 5.14x performance and 87.3 percent lower traffic on average when compared to 1080Ti. An Cambricon-F1 Computing Card consumes an average of an 83.1 Watt power for all benchmarks, and 1080Ti consumes an average of 199.9 Watt. The attained performance of Cambricon-F1 is from 1.42x to 659x higher than 1080Ti. Note that Cambricon-F1 has a 40.6 percent higher peak performance and a 5.8 percent higher root bandwidth relatively to 1080Ti.

The main reason for that is because of the large on-chip storage. While in 1080Ti, the programmable nodes under the root memory, i.e., CUDA cores, have very limited local storage space (96 KB shared memory versus 8 MB L1 local storage); thus, the operational intensity is bounded. The operational intensity of all seven benchmarks on Cambricon-F1 has reached the ridge point of the roofline, indicating that the root bandwidth will not be the performance bottleneck of Cambricon-F1. Thus, Cambricon-F1 has attained 57.4-99.8, 88.9 percent on average of peak performance on all benchmarks.

Cambricon-F100 versus DGX-1. As shown in Fig. 12b, Cambricon-F100 has a 51.9 percent higher root memory bandwidth compared to DGX-1, while the peak performance of Cambricon-F100 is 4.4 percent lower than DGX-1. For power consumption, four Cambricon-F100 Computing Cards consume an average of 614.5 Watt at the total, and eight V100-SXM2 GPU cards consume an average of 1986.5 Watt. Overall, Cambricon-F100 have attained 1.74x-8.58x performance, 2.82x on average, compared to DGX-1.

On deep learning tasks, Cambricon-F100 improved the operational intensity by 37 and 33 percent for VGG-16 and ResNet-152, respectively, when compared to DGX-1. The operational intensity benefits from greater sub-problem scale, i.e., from larger batch size used. GPU performance does not always increase with batch size, which caused the best batch size choosing on GPU is smaller than on Cambricon-F. The broadcasting optimization of Cambricon-F improved operational intensity even further.

On machine learning tasks, DGX-1 achieves up to 85x higher operation intensity when compared Cambricon-F100. This difference is caused by the implicit management of intermediate memory in Cambricon-F. In Cambricon-F, programmers do not manipulate on memories except the main memory explicitly, Cambricon-F will write the intermediate result after each instruction back to the root once the tensor transposition mechanics failed to forward the data, which caused the traffic on root raised. For control intensive workloads as ML tasks in the benchmark, control flow always breaks the FISA pipeline and data forwarding, forcing the intermediate results written back to the root. k-NN and SVM have a relatively complete essential computation block. For k-NN, calculating distances between each pair of samples constituted ⩾ 95 percent of the total run-time, and for SVM, the kernel between each pair of samples, which is sufficiently operation-intensive, is calculated in each iteration. Thus, their operational intensity on Cambricon-F is less affected. k-Means and LVQ are also iterative algorithms as SVM is, but they do not have an operation-intensive computation block in each iteration, thus their operational intensity is more affected, which heavily limited the performance attainable. Moreover, the significantly smaller granularities of operations on these two benchmarks may be insufficient to hide the control latency of Cambricon-F nodes, resulting in an even worse performance on Cambricon-F100 compared to Cambricon-F1. With such a better operational intensity, DGX-1 has still shown a significant gap between attained performance and the roofline, since the bottleneck of GPU system is between graphic memories and chips. For k-Means and LVQ, GPU suffers from the control flow either and showing an even worse performance.

SECTION 7Cambricon-FR: Fractal Computers With a Reconfigurable FISA
In this section, we propose Cambricon-FR, which enhanced the Cambricon-F machine learning computers to flexibly and efficiently support all the operations with a reconfigurable fractal instruction set architecture (RFISA).

7.1 Ineffectiveness in FISA
While Cambricon-F is able to achieve high programming efficiency with maintained higher performance and energy efficiency, it still suffers from ineffectiveness when executing new operations that are not covered in the FISA set. In the FISA set (c.f., Section 3.2), several important operations from popular machine learning techniques are contained as FISA primitives directly for fast and efficient implementation. But other operations that are not FISA primitives can only be emulated through combining low-level operations and FISA primitives, which could be very ineffective in realization.

From the perspective of complexity, we can define two types of ineffectiveness in Cambricon-F, i.e., computation ineffectiveness (time complexity) and communication ineffectiveness (communication complexity). More precisely, regarding the computation ineffective, for a fractal operation which is effective on a fractal machine M, its obtained speedup ratio rc is irrelevant to the operation granularity G, where the rc is defined as the ratio of operation compute time on the leaf node of M and on the whole machine M, i.e., rc=TM/Tleaf node. For an operation is ineffective on a fractal machine, its achieved speedup ratio is asymptotically related to the granularity G, which means its computational complexity could become worse due to the non-direct support in FISA. For example, TopK, an operation not in FISA, could be very ineffective on Cambricon-F machines. As TopK can be support indirectly through the combination of Sort1D and Merge1D primitives in FISA, solving a TopK(n,K) is O(nlogn) complexity in time. However, running a leaf node, it becomes O(nlogK) complexity in time.

Regarding the communication ineffectiveness, for a fractal operation which is effective on a fractal machine M, its data traffic ratio rd is irrelevant to the operation granularity G, where the rd is defined as the ratio of data communications required to execute on the whole machine M and on the leaf node of M, i.e., rd=DM/Dleaf node where D denotes data traffics. For example, Conv3D, which is a convolutional operation with a pair of additional spatial dimensions D and KD, could be very communication ineffective if achieving indirectly using Cv2D primitive in FISA. While the time complexity remains the same, the lower-bound of data traffic of such communication ineffective realization could be O(KD) times higher than direct Conv3D primitive, as there will be at least KD−1 partial sum of results created in the root memory as intermediate data.

The two types of ineffectiveness at root are caused by the indirect support in FISA. For those ineffective operations, their execution on Cambricon-F machines are heavily limited by combining fragmented low-level instructions, which have to be executed sequentially. Therefore, to address the ineffectiveness issues in fractal machines, FISA should be able to support as many as possible operations. However, directly integrated all the possible operations is unrealistic and costly. Hence the question is, how to break the constraints of FISA and enable the fractal execution of any fractal operations on a fractal machine flexibly?

Our Solution. To address the ineffectiveness issue, we propose a reconfigurable FISA for fractal machines, which allows user-defined fractal instructions and user-specified executing procedures. Therefore, the fragmented low-level instructions are fused again in RFISA, enabling effective support for those ineffective operations. In order to realize a fractal machine for RFISA, several challenges must be addressed:

FISA. The RFISA needs to be flexible but still keep the programming productivity.

Architectural Support. The corresponding architecture should be able to support RFISA but still keep the homogeneous characteristic of control logic in each node, so as to maintain the fractal feature.

Programming Paradigm. A special designed programming model and a language is required, which should unify the programming of both Sequential Decomposer and Parallel Decomposer/Reduction Controller.

7.2 Reconfigurable Fractal Instruction Set Architecture
Fig. 14 compares the FISA, RFISA, and traditional ISAs (e.g., VLIW and RISC) in terms of the abstract level (which is decisive to productivity) and flexibility.

Fig. 14. - 
The comparison of FISA, RFISA, VLIW, and RISC in terms of abstraction and flexibility. The RFISA can achieve both good productivity and flexibility.
Fig. 14.
The comparison of FISA, RFISA, VLIW, and RISC in terms of abstraction and flexibility. The RFISA can achieve both good productivity and flexibility.

Show All

The topmost FISA can provide excellent productivity since it directly maps high-level primitives (e.g., convolution and sorting) to fractal instructions. However, other operations (which cannot be directly mapped to one single pre-defined fractal instruction) need to be programmed with multiple sequentially executed fractal instructions for flexibility. As the original operation is executed with multiple FISA instruction in sequence, all other nodes, except for the root node, are not aware of what the original operations is. This leads to the waste of computations and lots of chances for data reuse. Moreover, nowadays machine learning algorithms are evolving drastically with newly emerged high-level primitives. Once current fractal instructions in FISA cannot emulate a new operation, the only way to support that operation is to update the instruction set, design and manufacture new machines with an updated instruction set.

RFISA dominates the FISA and VLIW/RISC in Fig. 14, in terms of productivity and flexibility. In contrast to the FISA, RFISA does not provide native fractal instructions any more. Instead, a set of local instructions (same as in the FISA) are provided as the basic building blocks. Table 8 lists examples of local instructions. Based on such local instructions, for a new operation, programmers are able to build a corresponding new fractal instruction on demand with essential hardware supports (which will be elaborated in Section 7.3). Therefore, the desired operation is mapped to one single fractal instruction, instead of multiple sequentially executed fractal instructions in FISA, so as to improve the efficiency. In contrast to the VLIW and RISC instruction sets, which are more closer to the native computation ability of underlying hardware with low level of abstraction,1 RFISA can improve the productivity as high-level primitives can be expressed as one single fractal instruction. Moreover, the flexibility of RFISA is even better than VLIW/RISC for two aspects. First, the diversity of local instruction (i.e., four categories including data transfer, computational, logical and miscellaneous all operates with both vectors and scalars) offers the opportunity to compose various complicated operations. Second, from the programmers’ perspective, a reconfigurable and customized instruction set can extend existing instruction set without updating the hardware, which is very flexible for adapting to different applications.

TABLE 8 Examples of Local Instructions

In practice, the original fractal instructions provided in FISA can also be pre-configured to RFISA, so that a machine with RFISA can be fully compatible with the programs of FISA.

7.3 Cambricon-FR Architecture
In this section, we present the architecture support for RFISA, i.e., the Cambricon-FR machine learning computers. Overall, Cambricon-FR maintains almost the same architecture as Cambricon-F, except the controller modules where the controller in Cambricon-FR is reconfigurable. In Fig. 15, we show the two controllers in both Cambricon-F and Cambricon-FR. The controller in Cambricon-F contains a Sequential Decomposer, a Parallel Decomposer, a Demotion Decoder, and a Reduction Controller. Cambricon-FR replaces the SD, PD, and RC with a new module (a reconfigurable DEC). The DEC controls the execution of user-defined fractal instructions following the pre-loaded user-specified executing procedures in its storage. Therefore, with the key component of DEC, Cambricon-FR is flexible to support any fractal operation effectively.


Fig. 15.
Controller structure, Left: Cambricon-F. Right: Cambricon-FR.

Show All

DEC. Fig. 15 illustrates the detailed structure of DEC modules in Cambricon-FR and how it can be configured to perform the functionalities of SD and PD in Cambricon-F. The DEC contains a ROM, two stacks (PDSTACK and SDSTACK), and a controller (DECC). The ROM is used to store the user-specified executing procedures to be executed by DECC and the two data stacks are used to preserve the states when executing those procedures. The DEC deploys a double-thread executing model, where one thread performs the sequential decomposing (SD functionality) and the other performs the parallel decomposing (PD functionality). With a pre-defined thread priority, the DEC is able to perform one certain decomposition over the other. Additionally, since only the decomposing strategies are different in sequential decomposing and parallel decomposing, a user-specified executing procedure has many parts shared for the two threads, thus reducing the program size in ROM largely.

Dynamic Control. One possible situation in Cambricon-FR is that some instructions need data only decided in runtime, i.e., dynamic control. For example, when decomposing sparse data into equal pieces where each piece contains roughly same number of non-zero data, PopCnt (pop count) instruction will be applied to find the exact size of each piece. As such sparse information can only be obtained in runtime, PopCnt instruction needs dynamic control support. Cambricon-FR supports such dynamic control by allowing a memory address field of some RFISA instructions. And the DEC applies a scoreboard mechanism to replace the fields with contents from corresponding memory addresses before every sequential decomposition. Hence the sub-instructions after sequential decomposition can be directly sent to sub-nodes in Cambricon-FR without requirement of dynamic control support.

7.4 Programming
DeFracTalk Programming Model. To ease the burden of programming user-specified execution procedures for user-defined fractal instructions, we propose DeFracTalk (Decomposing Fractal Talk), a domain-specific programming model for programming the Cambricon-FR DEC. The DeFracTalk helps the programmers to define the execution flow of user-defined instructions but without bothering the programmers with too many details about the machine. In Fig. 16, we show the programming flow of DeFracTalk, where the programmers perform the left part light-weight tasks to tell the DEC how to perform the decomposing process. Specifically, the programmers declare the form of instruction with name and parameters where the DeFracTalk provides instruction instances with the declared name of parameters. The programmers define several decomposition options (opt) and the DeFracTalk tries its best to choose a decomposition option based on the hardware characteristics. The programmers define the range of pivot value which is used to decide the granularity of decomposed sub-instructions and the DeFracTalk finds the best pivot value based on the hardware characteristics. The programmers provide a piece of imperative code which defines how to write out the decomposed sub-instructions based on the pivot value and instruction arguments given by the DeFracTalk. Finally, the DeFracTalk is able to generate sub-instructions for DEC to decompose the user-defined instructions recursively.

Fig. 16. - 
DeFracTalk programming flow.
Fig. 16.
DeFracTalk programming flow.

Show All

The key feature of DeFracTalk is that it simplifies the programming and hardware design by providing an interaction interface to separate the definition and the specific decomposing process, see Fig. 16. For example, in the programming, programmers leave two key decisions, i.e., choosing the opt value and choosing the pivot value, to be made by DeFracTalk based on the machine characteristics. In such way, DeFracTalk achieves three the key advantages:

Independence of hardware configuration. As the decomposition is largely decided by DeFracTalk based on specific machine configuration information, programmers are able to define the same instruction but with different decomposition for different Cambricon-FR machines.

Independence of instruction granularity. As the two key parameters are decided by the DeFracTalk with regard to the machine characteristics, programmers are unaware of the detailed decomposition on internal nodes, thus providing an instruction granularity-independent programming experience.

Independence of SD or PD. As the DEC is configured to perform the functionalities of SD and PD, the two types of decomposition run in a double-thread mode. Thus, programmers are unaware of which decomposing process is running.

DeFracTalk Language. We propose a specialized programming language for DeFracTalk programming, i.e., the DeFracTaL(DeFracTalk Language). In order to support the future emerging fractal operations, we design the DeFracTaL strictly following the definition of fractal operation (given in Section 2.2) and DeFracTalk. We designed the syntax of DeFracTaL based on the C programming language. A part of syntax rules are listed below:

〈translation-unit〉 ::= 〈rfisa-def〉 〈translation-unit〉optional

〈rfisa-def〉 ::= 〈rfisa-opcode〉 〈rfisa-param-list〉 { 〈opt-def-list〉

〈pivot-decl〉 ::= 〈pivot〉 : [ 〈lower〉 , 〈upper〉 ]

〈opt-def〉 ::= 〈opt-specifier〉 〈pivot-decl〉optional 〈comp-stmt〉

〈stmt〉 ::= 〈var-decl〉 | 〈comp-stmt〉 | 〈frac-stmt〉 | 〈reduce-stmt〉 | 〈 sub-inst-stmt〉

〈frac-stmt〉 ::= frac 〈expr〉 : 〈expr〉 〈stmt〉

〈reduce-stmt〉 ::= reduce 〈stmt〉

〈sub-inst-stmt〉 ::= 〈rfisa-opcode〉 〈expr-list〉 ;

The rfisa-def defines the instruction name (rfisa-opcode), parameters (rfisa-param-list), and a list of options (opt-def) where each opt-def defines a decomposition option. In practice, there are opts that do not require a pivot to be defined, especially the last decomposition which generates all local instructions. opt-def contains a compound statement that corresponds to the imperative code piece defining the sub-instructions. There are three new statement types: frac-stmt, reduce-stmt and sub-inst-stmt, correspond to the target operator f(⋅), the retrieving operator g(⋅) as defined in Section 2.2, and the sub-instruction actually written out. With these correspondence, an operation can be defined in DeFracTaL as long as it meets the formal definition of fractal operation. In addition, DeFracTaL provided opt-specifier to specify whether the current defining opt is suitable in the context of specific decomposing phases, although DeFracTalk does not differentiate SD and PD. This is added for performance optimizing and simplicity of compiler design.

In Fig. 17, we show the programming in DeFracTaL using the definition of convolution instruction Cv2D as an example. The program declares the fractal instruction CV2D, the leaf decomposition (a local instruction conv in this example) and six opts decomposing the instruction along the dimension of batch, channel-out, height, width and channel-in. A special case is for channel-in which is separated into two opts for SD and PD, since the partial result is placed on the static segment when running in SD, but placed on the recycle segment when running in PD. In SD the local instruction add is serving as a sequential sub-instruction, while in PD it becomes a reduction instruction.

Fig. 17. - 
A DeFracTaL program defining Cv2D.
Fig. 17.
A DeFracTaL program defining Cv2D.

Show All

7.5 Evaluation
7.5.1 Methodology
Benchmarks. In Table 9, we show all benchmarks we used to evaluate Cambricon-FR instances. We select six fractal operations that are not natively supported on Cambricon-F as benchmarks, including 3D Convolution (Conv3D), Deconvolution (Deconv), Depthwise Convolution (DwiseConv), General Matrix Multiply (GEMM), Sparse Matrix Multiply Matrix (SPMM) and TopK, to evaluate the inefficiency issue. We also select five machine-learning applications which involve these operations, including C3D [51], FCN [52], Sparse AlexNet [3], MobileNet-V2 [53] and k-Nearest Neighbors, to evaluate the impact on overall applications.

TABLE 9 Benchmarks
Table 9- 
Benchmarks
GPUs. We use the same GPUs as baseline, i.e., Nvidia DGX-1 and Nvidia GeForce GTX-1080Ti, see Section 5. To evaluate both performance and programming productivity accurately on GPU systems, we write programs of benchmarks in plain CUDA C++ without calling computation libraries. On DGX-1, we write plain CUDA kernel functions utilizing TensorCore when possible. We report the maximum possible throughput as the performance metric by testing on various batch size setup. Lines of Source Code (SLoC) are used as the quantitative metrics, which are including minimum required codes to run the computation, excluding data preprocessing, any comments, blank lines or dead codes.

Cambricon-Fs and Cambricon-FRs. We also use the same configuration for Cambricon-Fs and Cambricon-FRs as described in Section 5. Cambricon-FR1 and Cambricon-FR100 have the same configuration as Cambricon-F1 and Cambricon-F100 respectively, despite the different controller structures. We write the program in FISA/RFISA instructions, and also DeFracTaL specifying new operations on Cambricon-FRs. Since programs can be ported without any adjustment between fractal machines, the codes on Cambricon-F1 and Cambricon-F100 are shared, so does Cambricon-FR1 and Cambricon-FR100. Therefore, we report SLoCs for Cambricon-F and Cambricon-FR, not the specific instances. We build C++ simulator to simulate these programs to obtain performance. The C++ simulator is event-driven, behavioral, modeled the execution on the hierarchical pipeline, which is sufficient to differentiate Cambricon-Fs and Cambricon-FRs.

7.5.2 Experimental Results
Performance. The performance comparison is shown in Fig. 18. For the desktop-scale fractal machines, Cambricon-FR1 runs 1.96x faster than Cambricon-F1 on average, and for the server-scale fractal machines, Cambricon-FR100 runs 2.49x faster than Cambricon-F100 on average. The result shows that RFISA breaks the obstruction lying between split FISA instructions, and achieved remarkable performance improvement on most of the benchmarks.

Fig. 18. - 
Speed up fractal machines compared to GPU. Top: Cambricon-F1 versus Cambricon-FR1. Bottom: Cambricon-F100 versus Cambricon-FR100.
Fig. 18.
Speed up fractal machines compared to GPU. Top: Cambricon-F1 versus Cambricon-FR1. Bottom: Cambricon-F100 versus Cambricon-FR100.

Show All

Depthwise convolution achieves a 5.72x higher performance on Cambricon-FR1 versus Cambricon-F1, and a 13.63x higher performance on Cambricon-FR100 versus Cambricon-F100. The reason is that without RFISA, depthwise convolution is built upon massive element-wise multiplication, which has a poor data locality; with a user-defined depthwise convolution instruction, the data locality exploited in the convolutional window can be preserved through every memory hierarchies. Since depthwise convolution is improved greatly, MobileNet-V2 which is mainly constructed from depthwise and pointwise convolutions also benefits a lot, resulting in a 2.90x/5.30x performance gain.

Conv3D achieves a 3.60x higher performance on Cambricon-FR100 versus Cambricon-F100, but there are only negligible improvements achieved on Cambricon-FR1 versus Cambricon-F1. As analysed in Section 7.1, Conv3D is communication ineffective on Cambricon-F and have a KD times higher lower-bound of communication. But on a smaller-scaled machine as Cambricon-FR1, the communication is already bounded by the memory capacity of the fractal nodes, thus the communication ineffectiveness has been hidden. Similar effects can be observed also on C3D and GEMM.

In contrast, TopK achieves an 18.88x higher performance on Cambricon-FR1 versus Cambricon-F1, but only 2.17x higher on Cambricon-FR100 versus Cambricon-F100. As analysed in Section 7.1, TopK is computation ineffective on Cambricon-F and have a worse time complexity. Cambricon-FR can reduce computation operations required in TopK dramatically, but for communications, the reduction is not as much. On a larger-scaled machine as Cambricon-FR100, the computation power is much higher but the bandwidth is limited, thus the effect on reducing computation operations cannot be clearly manifested.

Programming Productivity. Programming productivity can be measured from the perspective of algorithm programmers or system software developers (including compiler). Regarding the practitioners, they can only write one program for multiple Cambricon-F/Cambricon-FR machines. The required programs are reduced from N to 1, significantly improved programming productivity. And for programming a certain program, Cambricon-F/Cambricon-FR provides a similar programming experience as the programming framework (e.g., TensorFlow, PyTorch, etc.). In Fig. 19, we report the SLoC to quantitatively measure the programming productivity of algorithm programmers for one certain program. Compared to DGX-1 and 1080Ti, Cambricon-FR saves SLoC with a factor of 6.35, 6.06 on average, respectively, showing the significant improvement of programming productivity. And for N Cambrion-F/Cambricon-FR machines, the programming productivity can be further improved by N times, as non-fractal machines would have to write different programs for high efficiency.

Fig. 19. - 
Programming Productivity of Cambricon-F, Cambricon-FR and GPUs quantitated in SLoC.
Fig. 19.
Programming Productivity of Cambricon-F, Cambricon-FR and GPUs quantitated in SLoC.

Show All

Regarding the system software developers, they program to provide a programming environment to algorithm programmers. For Cambricon-FR machines, the DeFracTaL code is used to define new RFISA instructions, thus it can be treated as the system software programming. Since Cambricon-FR requires additional DeFracTaL codes, the SLoC of Cambricon-F is less than Cambricon-FR, with a factor of 4.70 on average. However, once the DeFracTaL codes are written, they can be reused between applications. If DeFracTaL codes are reused, they can improve the programming productivities for Cambricon-FR over Cambricon-F. The average length of DeFracTaL codes is 81.1 lines, and RFISA codes are shorter than FISA codes with a factor of 1.30, with the help of newly defined instructions.

SECTION 8Related Work
Machine Learning Accelerators. Due to the end of Moore's Law and Dennard Scaling, domain-specific accelerators designed for machine learning, especially DNNs, have become hot topics of computer architecture community in recent years. Many machine learning workloads have high intrinsic parallelism to be exploited by specific architecture. Most recent works are included in [57], [58], [59], [60], [61], [62], [63].

Yunji Chen et al. proposed the DianNao family of machine learning accelerators [22], [23], [37], [38], [39], which minimizes memory accesses to achieve both high performance and low power consumption. Yu-Hsin Chen et al. proposed Eyeriss [24] accelerator for deep CNNs which adopts a reconfigurable data path and running-length compression to skip zeros in the data, both to minimize memory access. Google's TPU [40] adopts a systolic array of PEs as its computing component to eliminate the requirement of local memory on PEs. Many previous works have shown that minimizing memory accesses is essential for machine learning accelerators, but have not quantized the effects of their efforts to reduce memory accesses.

Machine Learning Computers. Akhil Arunkumar et al. proposed MCM-GPU [64] to continue the scalability of monolithic GPU. By designing memory system and integration, MCM-GPU proposed a multi-chip module of GPUs with interconnections and caches showing that the performance of a multilayered GPU system can be comparable to a similarly sized, monolithic GPU. Both MCM-GPU and Cambricon-F provided a user-transparent extension to system scalability. Compared to Cambricon-F100 which also has a similar module—a computing card composing two chips, the control of MCM-GPU is fine-grained and heterogeneous while Cambricon-F100 remained homogeneous.

As the state-of-the-art GPU system, DGX-1 [15] was originally launched by NVIDIA in 2016 featuring eight NVIDIA Tesla P100 GPUs, then refreshed with new NVIDIA Tesla V100 GPUs which are particularly designed for deep learning acceleration. Compared to Cambricon-F100, the eight GPUs in DGX-1 are connected in a hybrid cube mesh network by NVLink, while the interconnection of Cambricon-F100 nodes is limited within parent-to-children paths, forming an H-tree topology. Building interconnection among sibling nodes for Cambricon-F may further improve performance, we left this exploration for future works.

ISA for Heterogeneous Systems. Recent research also addresses the programming productivity issue with new ISA. Venkat et al. [65] proposed Composite-ISA which constitutes a composite ISA superset with multi-ISA for heterogeneous multicores, while Cambricon-F uses a unified ISA for multi-systems with different scales.

SECTION 9Conclusion
In this paper, we propose Cambricon-F, machine learning computers with fractal von Neumann architecture and the same ISA, aiming to address the emerged critical issue that hinders the deployment of machine learning computers, i.e., programming productivity, including both programming itself and software stack development. We thoroughly analyze machine learning techniques for fractal computation and solve the three different types of fractal operation in our Cambricon-F architecture design. Cambricon-F features the fractal computing that iteratively decomposes an instruction on it into several instructions on low-layer sub-nodes. Thus, achieving easy-programming and high-efficiency simultaneously. Our results show that Cambricon-F achieves 5.14x, 2.82x better performance, 11.39x, 8.37x better efficiency on average, with 93.8, 74.5 percent smaller area costs when comparing against 1080Ti and V100 GPU, respectively. With the unified ISA and code for high programming productivity, Cambricon-F is also able to achieve better performance and efficiency. Further, we propose Cambricon-FR, featured with a reconfigurable FISA, to flexibly and efficiently support all fractal operations. Our results show that the two Cambricon-FR instances achieve 1.96x, 2.49x better performance on average when comparing against Cambricon-F instances. Cambricon-FRs are also able to save the line of codes with a factor 5.83 on average compared to selected GPUs, thus significantly improving the programming productivity.
