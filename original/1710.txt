As the demand for image applications with high resolution increases, the importance of the system for image processing is growing. Graphics processing units (GPUs) can increase computational capacity with massive parallelism, but are still subject to limited memory bandwidth. Near-data-processing (NDP) is expected to mitigate the performance and energy overhead caused as a result of data transfer by performing computations on the logic die of 3D-stacked memory. Although prior studies have demonstrated the advantages of NDP, a NDP solution focused on image processing has not yet been developed. This article proposes a GPU-based NDP architecture and well-matched optimization strategies considering both the characteristics of image applications and NDP constraints. First, data allocation to the processing unit is addressed to maintain the data locality and data access pattern. Second, a lightweight yet efficient NDP GPU architecture is proposed. By applying a prefetcher that leverages the pattern-aware data allocation, the number of active warps and the on-chip SRAM size of the NDP are significantly reduced. This enables the NDP constraints to be satisfied and a greater number of processing units to be integrated on a logic die. The evaluation results show that the proposed NDP GPU improves the performance by 1.85× and consumes 82.7 percent energy compared to the baseline NDP GPU.
SECTION 1Introduction
With the advent of recent ultra-high definition (UHD) image and video applications, the importance of a system optimized for image and video processing is increasing. Graphics processing units (GPUs) are known as an effective hardware to handle various image processing algorithms because of their massive parallelisms. However, recent applications with high resolution and frame rates have very large energy overhead and processing time because of the data movement between the processor and the off-chip memory [1]. Near-data-processing (NDP) [2], [3], [4], an emerging technology also termed processing-in-memory (PIM) [4], [5], [6], [7], [8], [9], [10], is a promising alternative for reducing the overhead caused by data movement. The cost-efficiency of NDP has not been good in the past because of the limitations of the DRAM technology. However, with the progress of the through-silicon-via (TSV) technology, three dimensional (3D) stacked memory has been developed including high-bandwidth memory (HBM) [11] and hybrid memory cube (HMC) [12]. This 3D stacked memory enables the coexistence of logic-die and dram-die, making NDP more realizable and appealing. Particularly, programmable GPU-based NDP is attractive, because it is capable of processing more various algorithms compared to hardware accelerators, and it enables the utilization of numerous image processing software development kits (SDKs) such as CUDA and openCL. Many recent studies have proposed taskscheduling methods for NDP-enabled GPU architecture [13], [14]. These studies were mainly done to accelerate specific applications such as deep neural network (DNN) [15] or 3D rendering [16]. Therefore, previous NDP studies may not be extensively applied to various image processing algorithms. NDP for image processing has not been sufficiently studied despite the strong demand for efficient image processing in many fields. A widely applicable programmable NDP architecture is required considering the common characteristics of image processing algorithms and NDP constraints.

The most naive approach to utilize the wide NDP bandwidth is to have a sufficient number of computing units. In the GPU, a large number of threads are context switched to hide latency and achieve parallelism. In this environment, a large sized register is needed to temporarily store the context, and the cache size also increases. Large size on-chip SRAM make it difficult to implement NDP because of the strict area constraints of the logic die. Meanwhile, scheduling a cooperative thread array (CTA) or thread block considering the data locality can also effectively increase the performance [17]. However, these studies do not preserve the algorithmic characteristics of image processing. In addition, they are not suitable for a lightweight NDP GPU because situations in which the number of threads is small are not considered.

In this paper, a GPU-based NDP architecture and optimization strategies are proposed. The two approaches are complementary together. The proposed optimization strategies allocate thread blocks to the processing unit while preserving the memory access pattern and data locality of the image processing algorithms. Taking advantage of this predictable allocation, an access pattern-aware prefetcher is added, and the hardware resources are greatly reduced. This allows one to easily satisfy the NDP constraints and provides an opportunity to integrate more processing units. The simulation results show that compared to a general GPU with massive resources, the proposed NDP GPU architecture has a higher competitiveness in energy efficiency and performance.

The main contributions of this paper are as follows.

A lightweight but efficient architecture suitable for NDP is proposed. The architecture is derived from the GPU architecture; however, it is optimized for image processing such that it requires significantly less hardware compared with a conventional GPU architecture. This allows more computing units to be placed on the area-constrained logic die, thereby making better use of NDP’s bandwidth.

Optimization strategies are presented for effectively utilizing the proposed NDP GPU. Thread blocks (or CTAs) are allocated according to the access pattern of the image processing application to maintain data locality and make the memory access predictable.

The performance of the proposed NDP GPU architecture is evaluated with various image processing applications in terms of processing time and energy using a cycle-accurate simulator. In addition, it is shown that the proposed NDP GPU satisfies various constraints that should be considered.

The remainder of this paper is organized as follows. Section 2 provides an analysis of the characteristics of image processing and the inefficiency of conventional GPU architectures. Section 3 proposes the NDP GPU architecture and optimization strategies specialized for image processing applications. Sections 4 and 5 present the simulation environment and the evaluation results, respectively. Section 6 introduces the related works. Finally, Section 7 concludes the paper.

SECTION 2Background and Motivation
2.1 Baseline GPU Architecture and Execution Model
The GPU consists of many streaming multiprocessors (SMs) and each SM executes multiple threads in a single instruction multiple thread (SIMT) fashion. The load-store (LDST) units inside an SM consist of the latter’s own private L1 cache, read-only texture memory, constant cache, and shared memory [18]. The L2 cache shared by multiple SMs is accessed through an interconnect network and connected to a device memory, which is an off-chip DRAM, through one or more memory controllers. Hereafter, a general GPU is termed a baseline GPU.

CUDA applications use the GPU through a kernel invocation. Each kernel consists of a group of threads termed a cooperative thread array (CTA) or a thread block. The CTA is the basic unit allocated to an SM, and no dependency is present between CTAs. A CTA is divided into subgroups termed warps. A single warp normally consists of 32 threads, and the threads included in the same warp execute the same instructions. In other words, the warp is the basic unit of instruction fetch and decoding. A GPU switches warps to hide memory access latency. Information regarding the switched warps is stored in the register. A previous study indicated that a baseline GPU allocates CTAs to SMs in a round-robin (RR) manner for processing load balancing [19]. The RR scheduling is completed as follows. The CTA scheduler verifies that a particular SM has sufficient resources to run the CTA. If so, the scheduler allocates the CTA to that SM. Otherwise, the next SM is checked in an RR order. The CTA scheduling is repeated until the kernel is terminated. Because the availability of SMs depends on the CTA running time of each SM, the mapping of SMs and CTAs is hardly predicted.

2.2 Inefficiencies of the GPU When Processing Image Applications on an NDP System
The GPU was originally the hardware for graphic processing, but various modules have been added for general applications. These modules make the GPU heavy, making it difficult to meet the area and power constraints of the NDP logic die. However, the predictable properties of image processing applications may enable the design of efficient and lightweight GPU architectures. Given these conditions, the inefficiencies of the conventional baseline GPU when targeting NDP and image processing applications are described as follows.

CTA Scheduling. Because CTAs do not complete jobs in the order of their allocation to SMs, the RR scheduling disperses the CTA executions to several SMs. This causes two important problems.

First, it is difficult to exploit data locality. Most image processing applications continuously read and process adjacent data blocks. The GPU coalesces global memory loads and stores issued by a warp. When the data at the boundary of the area processed by the CTA are loaded, adjacent data may be additionally loaded owing to coalescing. In this case, data locality can be increased by allocating a CTA that processes adjacent data blocks to the same SM. A filtering algorithm is one of the typical examples. Although there is an opportunity to exploit data locality between adjacent data blocks as described above, the baseline CTA scheduling algorithm allocates CTAs that process adjacent data blocks to different SMs, making it difficult to reuse the data within each SM. If adjacent CTAs are allocated to different SMs, the same data should be redundantly read from the L2 cache or the main memory. Such unnecessary memory access not only results in increases in the congestion of the memory hierarchy, but also consumes additional energy. Fig. 1 compares the number of load requests in the image processing benchmarks between the path from the SM to the L2 cache and the path from the SM to the DRAM. The number of load requests is considerably higher in the former than in the latter. This means that the SMs read the data redundantly.


Fig. 1.
Total memory traffic from SM to L2 cache and SM to DRAM.

Show All

Second, it is difficult to exploit the data access patterns of image processing applications. If RR scheduling is used, the order of CTA allocation to the SMs becomes different from the data access pattern. A predictable execution order falls into a disorder. Fig. 2a shows an example in which the stride between the image areas allocated to the SMs is changed by RR scheduling. In this example, it is assumed that one CTA is allocated to each SM with the access pattern of the raster scan order in the x-axis direction. The large rectangle represents the image, whereas the small boxes represent the divided image areas. The numbers and colors in the boxes represent the CTA index and the SM numbers to which a CTA is allocated, respectively. In the first block line, CTAs 0, 1, 2, and 3 are allocated to SMs 0, 1, 2 and 3, respectively. Thereafter, SM1 finishes its allocated job before SM0 does; thus, CTA4 is allocated to SM1, which is then available. This shows that the order of CTA allocation to SMs is unpredictable. The stride of the areas allocated to SM0 is changed to 6→2→3→2. In Fig. 2b, CTA allocation is performed by considering the application data access pattern. The stride of the areas allocated to each SM is constant at 1, and the data access pattern and CTA scheduling are consistent with each other. Such predictable data access provides an opportunity to drastically increase the efficiency of data loading from the memory.

Fig. 2. - 
Examples of CTA allocation to SM: (a) RR-based allocation and (b) pattern-aware allocation.
Fig. 2.
Examples of CTA allocation to SM: (a) RR-based allocation and (b) pattern-aware allocation.

Show All

Large Number of Threads. A GPU uses a large number of threads to hide long latency instructions such as a memory load. Many registers are used for fast context switching, which increases the area and leakage power. In addition, because thousands of threads share an L1 cache, the cache occupancy per thread is very low in the baseline GPU leading to frequent cache eviction. This causes a high miss rate. Fig. 3 shows the miss rate of the L1 cache in which various image processing benchmarks have a high miss rate of 0.69. This indicates that the high data locality of the image processing application is not efficiently utilized in the L1 cache.

Fig. 3. - 
L1 cache miss rate of image processing benchmarks on a GPU.
Fig. 3.
L1 cache miss rate of image processing benchmarks on a GPU.

Show All

L2 Cache. The L2 cache relieves the latency and bandwidth gaps between the L1 cache and the main memory. However, the bandwidth gap of the L1 cache is not large in the NDP environment where a high memory bandwidth is provided. Furthermore, a large-sized L2 cache makes it difficult to meet the NDP area and leakage power constraints.

SECTION 3Proposed GPU-Based NDP System
3.1 Overview
The proposed GPU-based NDP can not only reduce the energy consumption by decreasing the memory access overhead but also exploit the wide memory bandwidth. However, careful design efforts are required to consider the 3D stacked memory environment including the limited area and power constraints of the logic die. In this section, we propose an architecture that satisfies the NDP constraints. Optimization strategies are also provided to fully exploit the architecture.

NDP System Organization. Fig. 4 shows the NDP GPU architecture on the logic die of the NDP stack. The NDP GPU can be used alone or to assist the main GPU system such as server GPUs. This near memory has the advantages of a high bandwidth and low latency because the memory is accessed by the memory controller connected to the memory die by means of a TSV. Considering the physical constraints of the logic die, the number of SMs, the register size and the cache size are set to be 1/4, and the number of active threads of the NDP GPU is set to be 1/6 of those of a conventional GPU. A prefetcher that utilizes the data access pattern of the image processing application is included in the SM, and the memory hierarchy is simplified.

Fig. 4. - 
NDP stack and proposed NDP GPU architecture.
Fig. 4.
NDP stack and proposed NDP GPU architecture.

Show All

NDP Programming Model. The proposed NDP GPU uses a programming model based on the CUDA instruction set architecture. There are various methods of finding suitable kernels to offload to the NDP GPU [13], [14]; however, they are not the focus of this paper. Therefore, the programmer is allowed to specify the kernel to offload. Programmers perform programming according to simple but effective optimization guidelines to fully exploit the proposed NDP GPU. From this, CTA allocation is performed to fit the data access pattern. The proposed programming guidelines are easy to use and applicable to most image processing applications.

3.2 Access Pattern Aware CTA Allocation
To take advantage of the data access pattern in image processing applications, CTA-to-SM allocation is performed based on the data area information processed by CTAs. The allocation schemes are classified into three, as shown in Fig. 5. In the row-major and column-major CTA allocations of Figs. 5a and 5b, consecutive areas in the row or column direction are mapped to the same SM. In the tile-based CTA allocation, shown in Fig. 5c, the entire image is divided into the same number of tiles as the number of SMs, and each tile is mapped to a single SM. Table 1 shows the applicable allocation schemes according to the access pattern of the applications. In the raster scan or tensor-based application pattern, either the row or the column major allocation is chosen depending on the scan direction. Tile-based allocation is used for block-based scans. The proposed CTA allocation scheme guarantees that a single SM processes continuous data, which provides the opportunity to fully exploit the data locality and makes the data access pattern predictable. By exploiting the characteristics of the application, the NDP GPU can work well in the SMs with a small number of threads. The proposed CTA allocation is implemented as software through code modification (software CTA scheduling). There is an alternative to the proposed code modification method. In a more direct way, we can make the CTA scheduler assign CTAs to SM in a specific order (hardware CTA scheduling). The hardware CTA scheduling has the advantage in that the user does not need to modify the code. However, with a simple code modification, the performance can be improved even further by reducing the number of instructions executed. It is described in more detail in Guideline 1.

TABLE 1 CTA Allocation Scheme According to the Access Pattern of Application


Fig. 5.
CTA allocation scheme (a) row-major, (b) column-major, and (c) tile-based.

Show All

The remaining part of this section provides four guidelines for pattern-aware CTA allocation. Guideline 1 describes a software-based method to achieve CTA-to-SM mapping. Guidelines 2, 3, and 4 explain the programming techniques used to efficiently exploit the limited hardware resources of the NDP GPU. A pseudo code of the Convolution2D algorithm is provided for easy understanding.

Guideline 1. CTA Allocation to SMs. For software-based CTA allocation, only a single CTA is allocated to each SM. CTA processes its data to the desired pattern by adjusting the iteration range of the x, y, and z directions in the for-loop. Fig. 6 shows an example of software-based realization of row-major CTA allocation when four SMs are used. The left-hand side of Fig. 6 shows that the entire image is divided into four areas, which are allocated to four SMs. The right-hand side of Fig. 6 magnifies the image area allocated to SM1 and shows the order in which the data allocated to a single SM are processed. Each of the small squares represents the area processed at a time, with a width and a height of blockDim.x and blockDim.y, respectively. The area surrounded by the gray dotted line (image width × blockDim.x) is called a row block. First, the initial coordinate (init_x, init_y) of the area allocated to each SM is calculated (Step 1). This is denoted by a circle. In Step 2, the values representing the x- and y-coordinates of the pixels of the first block among the areas allocated to an SM are assigned to the variables x and y, respectively. Then, a convolution operation is performed on the pixels in the area of the block. After the convolution operation is completed for the pixels in the block, it moves to the next block. Step 3 shows the process of moving to the next target block area in the row-major direction. In Step 3, the value of x is changed to the value of the x-coordinate of the pixels in the next block by adding blockDim.x. Step 3 is repeated until the end of the row block is reached. When the end of the row block is reached, the convolution operation moves to the next row block. This is achieved in Step 4. In Step 4, to handle the next row block, the x- and y-coordinates of the first block of the next row block are assigned to the variables x and y, respectively. That is, the coordinates (x, y) become (init_x, y+blockDim.y). Step 3 and Step 4 are repeated until the execution is finished for the entire data area allocated to SM1.


Fig. 6.
Software-based row major pattern-aware CTA allocation example.

Show All

The steps to modify the original code are as follows. (i) When launching the kernel, set gridDim equal to the number of SMs so that only one CTA is assigned to each SM. (ii) Set the number of threads per CTA to 256 as specified in Guideline 3. (iii) Add a for-loop that allows the CTA to move along the access pattern of the application. Except for the three aforementioned aspects, the code in the part where the algorithm’s actual operation is performed requires little modification, making it easy to modify the code.

Fig. 7a shows the pseudo code for a CUDA kernel of the 2D convolution algorithm. The code needs to be modified to enable row-major CTA allocation, because the 2D convolution algorithm has a raster scan access pattern in the row direction. Fig. 7a shows the baseline 2D convolution code where one thread is allocated to each pixel in lines 2 and 3. The convolution filter operation is executed from lines 5 to 9. Fig. 7b shows the code modified for the row-major CTA allocation. The initial coordinate for Step 1 is calculated in lines 3 and 4. Line 7 sets the value of y to that of init_y before the initiation of the first for-loop for the processing of the row block. Before initiating the second for-loop for the CTA execution of the next row block, line 9 sets x to be init_x. This corresponds to Step 2. Next, lines 11 to 15 execute the convolution operation for a single CTA data area, and line 17 moves x to the second CTA area. Subsequently, Step 3 is completed. Step 3 is repeated until it finishes a single row block. Line 10 serves to determine whether the end of the row block has been reached. For Step 4, y is calculated in line 19 and x is reestablished in line 9 for the next row block. Here, lines 5 to 9 that perform the convolution operation in Fig. 7a are retained as lines 11 to 15 in Fig. 7b, which is the code after applying the proposed code modification method.

Fig. 7. - 
2D convolution pseudo code modification (a) baseline code, (b) code with pattern-aware CTA allocation, and (c) code with both pattern-aware CTA allocation and synchronization.
Fig. 7.
2D convolution pseudo code modification (a) baseline code, (b) code with pattern-aware CTA allocation, and (c) code with both pattern-aware CTA allocation and synchronization.

Show All

As explained earlier, the number of instructions can be reduced through software CTA scheduling. When executing the code in Fig. 7a, the hardware CTA scheduler allocates the CTA until the resources of the SMs are full. In this case, every time the CTA is newly allocated, an index calculation is required for indexes such as pixel_Pos x, y, which are related to blockIdx and threadIdx. On the other hand, when CTA scheduling is performed by software as shown in Fig. 7b, the index related to blockIdx and threadIdx is calculated only once to determine the initial position (init_x, init_y). Thereafter, only the process of adding blockDim to the initial position is related to the index calculation. In other words, software CTA scheduling reduces the number of instructions to be executed by not repeatedly executing the instructions required for the indexing calculation.

When the proposed method is used instead of RR scheduling, an imbalance may occur between the SMs. This imbalance induces underutilization, and, therefore, it is important to maintain the balance between SMs. An imbalance between the SMs may occur due to two factors. First, the data may not be evenly distributed among the SMs. Second, the time required for each SM to process the data may vary. However, these are not problems in the proposed CTA allocation. The proposed method targets image processing applications where image data is easily distributed among the SMs. In addition, the data processing time can be mainly divided into the loading time from memory and computation time. Among them, a difference in the data loading time can significantly affect the load balancing between the SMs. In the case of image processing, the arithmetic intensity is constant throughout the program. In this case, the time taken to the load data is almost constant. Also, because the computing power of the SMs is all the same, there is almost no difference between the time taken for each SM to perform the computations using the data. Therefore, the total time required for each SM to process data also has little difference. In the experiment, we observed a cycle difference of approximately 1 percent between the fastest SM and the slowest one.

Guideline 2. Syncthreads. Lack of data locality decreases cache efficiency, and therefore unprepared data increases stall. To avoid this, this paper puts an explicit synchronization for the L1 cache as in the case of shared memory. This helps to keep the execution speed similar between warps and prevent the decrease of the data locality. As is well known, a GPU already guarantees synchronization in the unit of CTA block. However, when the only one CTA is allocated to each SM as in the proposed environment, each CTA should process a relatively large data area. Without explicit synchronization for a certain size of data area, synchronization would occur only when the process for all the areas allocated to the individual SM is finished. If so, the data locality between warps would not be maintained because of their different execution speed in a CTA. In general GPU environment, instead of the explicit synchronization, a loose-round-robin (LRR) warp scheduling may be applied to make the execution speed of the warps similar. However, in contrast to RR, the LRR does not give a perfectly equivalent priority to the warps. Fig. 8 shows the accumulated number of instructions executed by the individual warps over time in a single SM in the case where only LRR warp scheduling is used without explicit syncthreads in the proposed NDP GPU environment described in Section 3.1. Fig. 8a represents the case where the NDP GPU has 16 SMs and each SM has 8 KB of L1 cache. In this case, the execution speed is fast in specific warps. The initially scheduled warps rapidly execute using the prefetched data. However, as the program execution continues, the memory traffic is increased, leading to an increase of memory latency. In this case, the prefetch requests by the warps scheduled later can fail to be met on the right time. These warps may not take the benefit of prefetching, and the speed gap between the warps is increased. In addition, due to the small cache size of the NDP GPU, the prefetched data requested for slow warps may be evicted by the prefetched data of early scheduled warps. Of course, the speed may be kept the same between the warps by the sufficient cache size or the reduced traffic. For example, Fig. 8b shows the case where the NDP GPU has 16 SMs, as in Fig. 8a, but each SM has a larger cache size of 32 KB. Thanks to the increased cache size, all the warps take advantage of prefetching, and thus the speed is maintained between the warps. Another example of Fig. 8c shows the case where the number of SMs in the NDP GPU is decreased to 8 and each SM has 8 KB of cache. In comparison with the case shown in Fig. 8a, as the memory traffic is decreased, all the warps well exploit the prefetched data and execute at a similar speed.


Fig. 8.
Accumulated number of instruction of warps in three NDP configurations without explicit synchronization (a) 16 SMs with 8KB L1 cache, (b) 16 SMs with 32KB L1 cache, and (c) 8 SMs with 8KB L1 cache.

Show All

The use of explicit synchronization may cause additional overhead due to temporal discontinuity in execution. This overhead is measured by comparing on- and off-synchronization in a perfect memory environment where there is no penalty due to data miss. In this ideal environment, synchronization increases the execution time by 5 to 10 percent. However, as shown in Fig. 8a, it is difficult to avoid the lack of data locality without explicit synchronization in the NDP GPU environment which adopts a prefetcher with a small cache size. Therefore, it is more effective to use explicit synchronization that allows to efficiently exploit the cache and thus decrease the memory access latency in spite of the synchronization overhead. In Fig. 7c, the code for synchronization is presented. It is almost identical to Fig. 7b, but the _syncthreads() command is added to line 17. The _syncthreads() command performs synchronization before going from Step 2 to Step 3.

Guideline 3. CTA Size Setting. The number of threads in the proposed NDP GPU architecture is limited to 256, and one CTA is allocated to each SM. Therefore, the CTA size (blockDim.x × blockDim.y) must be set up in accordance with the number of threads.

Guideline 4. Use of Shared Memory. The shared memory is user-managed, and shows a high speed because it is on-chip. However, when requesting data, both load and shared memory instructions must be used. Thus, the number of instructions is greater compared with general memory requests. In a GPU, the shared memory is effective when data is frequently reused or the access to global memory is not coalesced properly. As the proposed NDP architecture uses a small number of threads, it has little cache contention. Additionally, the prefetching in the proposed NDP architecture increases the hit rate of the cache. Therefore, if the coalescing problem is not significant, using the L1 cache rather than shared memory can improve the speed by reducing the number of instructions. For example, in a ConvolutionSeperable algorithm [20], the number of instructions decreases by 28 percent when the L1 cache is used instead of shared memory. Meanwhile, other LDST units including the texture unit and constant cache are read-only. A texture unit is dedicated hardware for interpolation and boundary handling, and thus has a great advantage in data reading. A constant cache is used to store a small number of constant variables. Therefore, it is recommended to retain the use of a texture unit or a constant cache, if there is any included in the conventional code.

3.3 NDP GPU Architecture
SM Structure. The SM in the NDP GPU has strict constraints in terms of power and area. This section suggests the number of warps and the size of the on-chip SRAM that are appropriate for the SM structure of the NDP GPU.

1) Pattern-aware prefetcher: In the NDP GPU, a stride-based prefetcher is mounted inside the SM to exploit the high bandwidth and increase the efficiency of the L1 cache. Because the data access pattern of the algorithm is preserved through the proposed CTA allocation, it does not require a history table unlike prior GPU prefetching [21], [22], [23], and accurate prefetching is provided with a simple stride-based prefetcher. First, stride at non-boundary (base stride) and stride at boundary (modified stride) are calculated in advance. Then, the current position in the image is calculated through the address and one of the base stride and the modified stride are applied. Fig. 9 shows the design of the proposed access pattern-aware prefetcher. The inputs are the start address of the image (image start address), size of the image (image size), CTA dimension (blockDim.x, blockDim.y, blockDim.z), and demand load address from the core. According to the main axis (x, y, z) of the access pattern, one of the blockDim.x, blockDim.y, blockDim.z of the CTA block is used as a base stride. For example, if the access pattern is a raster scan along the x-axis, blockDim.x is chosen. The position of the demand load address in the image is calculated by subtracting the image start address from the demand load address and dividing it with the image size. Base stride is added to the calculated position and compared with the image size to determine if the row or column will be changed. If the row or column is expected to change, modified stride = (base stride + image width × blockDim.y) is used instead of the base stride and added to the demand load address to calculate the prefetch address. Otherwise, base stride is added to the demand load address and used as a prefetch address. Same as the base stride, the modified stride is calculated when kernel is launched and saved in the prefetch information table.

Fig. 9. - 
Pattern-aware prefetcher design.
Fig. 9.
Pattern-aware prefetcher design.

Show All

2) The number of active warps: The access pattern-aware CTA allocation and prefetching make the necessary data readily available. Accordingly, as the stall decreases, the latency may be hidden with a smaller number of warps in comparison with the baseline GPU. Fig. 10 shows the average runtime of the benchmarks depending on the number of warps in the environment with a prefetcher. The runtime speed-up is saturated as the number of warps becomes 8 or higher. Therefore, the number of warps per SM is limited to 8 in the proposed NDP GPU.

Fig. 10. - 
Execution time according to the number of active warps.
Fig. 10.
Execution time according to the number of active warps.

Show All

3) Warp scheduler: The number of active warps is critical to hide the latency effectively under the condition of a small number of warps. Here, it is important to keep the execution speed similar among the warps. Therefore, an LRR warp scheduling scheme is employed. The problems described in guideline 2 of Section 3.2 are resolved by explicitly adding syncthreads.

4) On-chip SRAM size: The size of on-chip SRAM including register, L1 Data cache, and shared memory is reduced by using a small number of warps. The size of register needed for warp switching is also reduced. In addition, the valid size of cache and shared memory per thread substantially increases. Thus, their size can be reduced without performance degradation. The decrease in the on-chip SRAM size leads to the reduction of the leakage power as well as the area, allowing to easily satisfy the power and area constraints of the logic die. The register size and the L1 Data cache size are set to 8,192 and 8 KB, respectively.

Off-Chip L2 Cache. The proposed NDP GPU uses no L2 cache for the reasons described below. First, the difference between off-chip DRAM and the on-chip SRAM access energy is small in NDP GPU. On the contrary, in baseline GPU, the energy consumption for the access to DRAM is much larger than that for the SRAM access (difference of 1 or 2 orders of magnitude [24], [25]), because the access should be done through an off-chip link. Second, CTA allocation and the prefetcher in the proposed NDP GPU system helps to increase the hit rate of the L1 cache up to almost 100 percent. This reduces the need for L2 cache which is used to mitigate the miss penalty of L1 cache in the memory hierarchy. Third, the bandwidth gap between the main memory and the L2 cache is smaller in the NDP GPU than in the baseline GPU. The NDP GPU allows to exploit a wider main memory bandwidth in comparison with the baseline GPU, and thus the need for the L2 cache decreases. Finally, considering the area and power constraints of the logic die at which the NDP is located, the shared L2 cache with a large area and a high leakage power is a burden on the NDP. The removal of the L2 cache simplifies the memory hierarchy and decreases the power consumption.

The Number of SMs. In order to take advantage of the wide bandwidth of NDP, an appropriate number of SMs must be placed on the NDP GPU. However, area constraints of the logic die should be considered along with the performance and energy consumption to determine the number of SMs for NDP GPU. In evaluation, design sweeping is conducted varying the number of SMs. Considering the area and power consumption of the SM, the NDP GPU can be equipped with 16 SMs. Details are given in Section 5.5.

SECTION 4Methodology
Simulation Environment. A cycle-accurate simulator, GPGPU-Sim [26], was modified to verify the impact of the proposed NDP architecture. The four base configurations are provided as follows. Table 2 shows the detailed simulation configurations.

Base-8: This is the baseline NDP GPU, which has an L2 cache on the logic die and has 8SMs. The SM structure is the same as that of a conventional GPU.

TABLE 2 List of Evaluated NDP GPU Configurations

Base-8-LT: The number of SMs, the cache size, and the register size are the same as Base-8; however, the number of threads per SM is reduced to 256, which is the same as that of the proposed NDP.

IP-8-Base: This is the baseline image processing NDP, which has a reduced number of threads, reduced size of SRAM, and 8 SMs.

IP-8: This is the proposed image processing NDP. It combines the access pattern-aware CTA allocation, prefetcher, and IP-8-Base.

The NDP energy consumption was measured using GPUWattch [27] and CACTI 6.5 [28]. The NDP configurations have a bandwidth of 256 GB/s per stack. The 3D stacked DRAM was simulated using the parameters provided by [29]. According to [29], the datapath of a 3D stacked DRAM has the following three components - (i) the path from the row-buffer to the global sense amplifier (GSAs), (ii) the path from the GSAs to the DRAM I/Os, and (iii) the I/O channel between the DRAM and the GPU. Because NDP GPU is located on the logic die of a 3D stacked memory, it is assumed that datapath (iii) is not accessed when accessing the DRAM from the NDP GPU. In consideration of this, the energy consumed per bit access in NDP is 3.59 pJ/bit, which is the sum of the row activation energy and energy consumed in paths (i) and (ii). The power consumed for memory access was calculated by multiplying the memory bandwidth obtained by GPGPU-Sim.

Benchmarks. The benchmarks used for the performance evaluation are shown in Table 3. Eight commonly used image processing algorithms were tested. To evaluate the proposed pattern-aware CTA allocation, we modified the code according to the guidelines in Section 3.2. Conv2D, ConvSep, Denoising are filtering algorithms, which are the representative computer vision algorithms. Jacobi2D is a typical stencil algorithm and is often used in scientific computing. DownSample and Concat are also basic algorithms widely used in computer vision. Dct8×8 is a core module used in most image/video compression standards such as JPEG, H.264, and HEVC. MatMul is used for transformation. In addition, it is widely used in deep learning, which has been recently used in many fields. Benchmarks were divided into three categories according to their characteristics.

Type 1: Memory-intensive, with considerable data sharing among SMs — Conv2D and Jacobi2D.

TABLE 3 List of Benchmarks

Type 2: Memory-intensive, with little data sharing among SMs — ConvSep, DownSample, Concat and Dct8×8.

Type 3: Computation-intensive — Denoising and MatMul.

SECTION 5Evaluation
In this section, the proposed IP-NDP is evaluated in terms of performance, energy consumption, and memory traffic. Area overhead and power consumption of the proposed IP-NDP are analyzed considering the logic die constraint of 3D stacked memory and the performance per area is evaluated.

5.1 In-Depth Analysis
This section shows the effect of the proposed prefetching (PF) and pattern-aware CTA allocation (PACA). The following fifteen configurations in Table 4 were evaluated. To objectively evaluate the combination of the proposed PF and PACA and to see its impact, Base-8-PWS-PACA, Base-8-LT-PWS-PACA and IP-8-Base-PWS-PACA, which replace PF with a program counter based per warp stride prefetching (PWS), are also evaluated. The PWS was proposed for GPUs [21], [22], [23].

TABLE 4 List of Evaluated Configurations
Table 4- 
List of Evaluated Configurations
Fig. 11 shows the speed increase for various configurations normalized to Base-8. First, in the case of Base-8, the performance decreases when PACA is applied separately, and it improves when prefetching is applied either alone or together with the PACA. Base-8-PF shows a 1.02× speed increase compared with that of Base-8. There was only a slight improvement in the performance because the accuracy of the prefetch is not high without PACA. At the same time, because Base-8 has enough number of threads to hide the latency, the side effects caused by an inaccurate prefetch such as an early eviction are not significant. For Base-8-PACA, it shows a 0.55× speed up compared with that of Base-8. The decrease in performance is due to the decrease in resource utilization. PACA allocates only one CTA per SM, and because the number of threads per CTA is fixed at 256, only 256 threads per SM are used. Because the number of threads in Base-8 is 1536, using only 256 threads causes a decrease in utilization. Base-8-PF-PACA achieves a 1.00× speed increase compared to Base-8. Compared to IP-8, it only shows 0.85× speed up. Even though both prefetching and PACA are applied, the reason why it did not achieve the performance of IP-8 is due to the L2 cache. PACA makes the L2 cache unnecessary; however, because it still exists, memory latency increased due to congestion in the L2 cache. This delays the prefetching, which adversely affects the performance. Especially, for Type 3 benchmarks, Base-8-PF-PACA shows a 20 percent performance drop compared to Base-8. Type3 benchmarks are computation intensive and have considerable data sharing among SMs. Therefore, prefetching is not effective because the efficiency of the L2 cache is high, and the decrease in the thread utilization due to the PACA causes performance degradation.


Fig. 11.
Speedup when applying the proposed methods to each baseline configuration. All results are normalized to the Base-8.

Show All

Second, in the case of Base-8-LT, the performance increases even when prefetching and PACA are applied separately, and when both are applied together, the performance increases dramatically. Base-8-LT-PF achieves a 1.12× speed up compared to Base-8-LT. Unlike Base-8, which has 1536 threads, Base-8-LT has only 256 threads, making it difficult to hide latency through context switching. Therefore, although the accuracy is not very high, reducing the data load stall by increasing the cache hit rate through prefetching can help improve the performance by reducing the burden of latency hiding. In addition, because the effective cache size per thread of Base-8-LT is larger than that of Base-8, the side effects of incorrect prefetching are fewer than those of Base-8. Base-8-PACA shows 1.05× speed increase compared to Base-8-LT. Unlike that in Base-8-PACA, thread utilization in Base-8-LT-PACA does not decrease due to the PACA because the number of threads is originally 256. However, because the L2 cache exists in Base-8-LT, performance improvement obtained by the PACA is lower than that of Base-8-LT-PF. Base-8-LT-PF-PACA achieves a 1.91× increase in speed compared to Base-8-LT. In addition, compared with Base-8, Base-8-LT-PF-PACA achieves a 1.00× speed increase. This implies that the proposed method can effectively improve the performance in environments with a small number of threads. As in the case of Base-8-PF-PACA, it shows a 0.85× increase in performance compared to IP-8. As discussed above, the reason for this is that the L2 cache has become unnecessary yet remains and causes congestion.

Third, in the case of IP-8-Base, the performance improves when prefetching and PACA are applied separately, similar to that of Base-8-LT. In addition, IP-8 shows the highest performance among the tested configurations. IP-8-Base-PF shows a 1.01× performance improvement compared to IP-8-Base. This is a small value compared with that of Base-8-LT-PF, which shows a speed improvement of 1.12× compared to Base-8-LT. The reason is that IP-8-Base has a smaller cache size than Base-8-LT, and, therefore, it has a greater adverse effect when an incorrect prefetch is performed. However, ConvSep shows 1.36× performance improvement compared to Base-8, because the read miss rate of the L1 cache significantly decreases from 98.3 to 39.4 percent when prefetching is applied. IP-8-Base-PACA achieves a 1.13× speed up compared to IP-8-Base. This is a higher performance improvement than that when PACA is applied to Base-8-LT. Because IP-8-Base does not have an L2 cache unlike Base-8-LT, the effect of PACA was greater. In the case of IP-8, where both prefetching and PACA are applied to IP-8-Base, the speed is 2.07× and 1.17× faster than IP-8-Base and Base-8, respectively. As mentioned above, IP-8 shows the highest speed up among the tested configurations. Overall, the performance improvement is not significant when each contribution is applied separately, but it can be confirmed that when all of them are applied together, it shows a high performance improvement.

Finally, in all benchmarks, the proposed PF showed higher performance than PWS. Base-8-PF-PACA, Base-8-LT-PF-PACA, and IP-8 show 1.07×, 1.07×, and 1.16× speed increase compared with those of Base-8-PWS-PACA, Base-8-LT-PWS-PACA, and IP-8-Base-PWS-PACA, respectively. This is because the proposed method correctly calculated the stride at the image boundary, while PWS could not grasp the exact stride at the image boundary.

5.2 Performance Comparison With Previous Work
This section compares the proposed architecture with previous works. Previous works BCS [17] and [10] were evaluated. They were applied to Base-8 and Base-8-LT and evaluated. Instead of directly comparing to [10], we compared to a perfect L2 cache (PL) configuration. [10] proposed an L2 prefetching scheme, so comparison to a perfect L2 configuration is a very favorable comparison for [10], assuming 100 percent accuracy and coverage in the prefetching. For IP-8, the area per SM is less than that of Base-8, and more SMs can be mounted on the logic die. Therefore, in addition to IP-8, IP-16 with 16 SMs was also evaluated. To summarize, the following eight configurations were evaluated: baseline NDP (Base-8), Base-8 with [17] (Base-8-BCS), Base-8 with a perfect L2 (Base-8-PL), baseline NDP with a lower number of threads (Base-8-LT), Base-8-LT with [17] (Base-8-LT-BCS), Base-8-LT with a perfect L2 (Base-8-LT-PL), IP-NDP with 8SMs (IP-8), and IP-NDP with 16SMs (IP-16). When referring to IP-8 and IP-16 together, they are referred to as IP-NDP.

Fig. 12 shows the speed increase for the various configurations normalized to Base-8. Four observations from this result are explained as follows. First, the proposed IP-8 outperforms Base-8, Base-8-BCS, and Base-8-PL. IP-8 is 1.17×(up to 1.89×), 1.13×(up to 1.93×), and 1.02×(up to 1.25×) faster than Base-8, Base-8-BCS, and Base-8-PL, respectively. For Type 1 benchmarks, IP-8 achieves a 1.21×, 1.08×, 1.07× increase in speed compared with that of Base-8, Base-8-BCS, and Base-8-PL, respectively. Base-8-BCS shows a similar performance to Base-8-PL. This is because the Type 1 benchmarks have a high inter-CTA locality; therefore, the BCS works effectively. In the Type 2 benchmarks, IP-8 shows a 1.29×, and a 1.27× speed increase compared with those of Base-8 and Base-8-BCS, respectively, whereas it is only 1.05× faster than Base-8-PL. ConvSep shows the best performance improvement. The original ConvSep code uses shared memory. By changing the code that uses the shared memory to use the cache according to Guideline 4(Section 3.2), we can reduce the number of instructions, leading to a high performance improvement. The Type 2 benchmarks have low data sharing between the SMs. Therefore, Base-8-BCS, which exploits the inter-CTA locality, is not effective. In contrast, Base-8-PL shows a higher performance increase compared with Base-8. Because a perfect L2 cache reduces the memory latency, memory intensive Type 2 benchmarks can benefit from it. For Type 3 benchmarks, IP-NDP is 0.93×, 0.95×, and 0.92× faster than that of Base-8, Base-8-BCS and Base-8-PL, respectively. The computationally intensive Type3 benchmark has difficulty in properly using the advantage of IP-NDP. Nevertheless, it is impressive that eight warps of IP-NDP are sufficient for latency hiding if high accuracy prefetching is supported.

Fig. 12. - 
Speedup of Base-8, Base-8-BCS, Base-8-PL, Base-8-LT, Base-8-LT-BCS, Base-8-LT-PL, IP-8 and IP-16. All results are normalized to the Base-8.
Fig. 12.
Speedup of Base-8, Base-8-BCS, Base-8-PL, Base-8-LT, Base-8-LT-BCS, Base-8-LT-PL, IP-8 and IP-16. All results are normalized to the Base-8.

Show All

Second, when BCS and PL are applied to the LT environment, the performance improvement is substantially lower than that of IP-8. This confirms that the proposed method is more effective when the number of threads is small. IP-8 shows a 2.18×(up to 3.07×) increase in speed compared to Base-8-LT. In contrast, Base-8-LT-BCS only shows a 0.99×(up to 1.01×) speed increase compared to Base-8-LT. BCS is not effective throughout in all the benchmarks. In the situation in which the number of threads is reduced, it is difficult to effectively hide the latency by using locality without prefetching. Base-8-LT-PL achieves a 1.30×(up to 1.52×) speed up compared with that of Base-8-LT. A perfect L2 cache is more effective than BCS because it can reduce memory latency. Reducing memory latency alleviates the burden of latency hiding; thus, its effect on Base-8-LT, which has fewer threads than Base-8, is clear. However since it does not prefetch to the L1 cache, the performance improvement is limited because of the access time to the L2 cache.

Third, IP-16 outperforms the other configurations in all the benchmarks. IP-16 shows a 1.85× and 1.58× speed increase compared with those of Base-8 and IP-8, respectively. In the case of the Type 1 benchmarks, IP-16 improves its performance to 1.86×, which is nearly double that of IP-8. In the Type 1 benchmarks, the bandwidth can be utilized well by reducing the memory traffic. However, in the Type 2 benchmarks, the original inter-CTA locality was not sufficient to reduce memory traffic. Therefore, although the number of SMs doubled, the acceleration of IP-16 is only 1.36× faster than that of IP-8 because of the bottleneck of the memory bandwidth. For the Type 3 benchmark, because it is bottlenecked by the computation, the IP-16 achieves a 1.83× increase in performance compared with IP-8. Because the number of SMs increased by a factor of 2, the speed up of IP-16 in the computation intensive Type 3 benchmarks also nearly doubled.

Finally, whereas [10] shows only a 0.33× (up to 0.92×) increase in the performance of the perfect L2 cache, IP-8 shows a 1.02× (up to 1.25×), and a 1.67× (up to 2.66×) speed increase compared with that of Base-8-PL and Base-8-LT-PL, respectively. Moreover, IP-16 achieves a 1.61× (up to 2.14×), and 2.64× (up to 5.22×) speed increase compared with that of Base-8-PL and Base-8-LT-PL, respectively. Therefore, it is confirmed that the proposed method shows better performance than that of [10] and the performance improvement is much better if a reduced area is considered.

5.3 Cache Miss Rate and Memory Traffic
Fig. 13 shows the L1 data cache miss rate of the Base-8 and IP-8. Across all benchmarks, IP-8 shows a 77.5 percent reduction of cache miss rate compared to Base-8. In addition, the accuracy of the proposed prefetcher is 98.4 percent, which means there is nearly no unnecessary prefetch request.


Fig. 13.
L1 cache miss rate of the Base-8 and IP-8.

Show All

Fig. 14 shows the memory traffic between the SM and L2 cache (SM-L2 traffic) and the traffic between the SM and main memory (SM-Mem traffic) normalized to the SM-L2 traffic of Base-8. For Base-8, the SM-Mem traffic is equal to the traffic between the L2 cache and the main memory. For IP-8, traffic between the SM and L2 cache is not plotted becuase there is no L2 cache in IP-8. In addition, in IP-8, SM-L2 traffic is used as equal to SM-Mem traffic. IP-8 reduces SM-L2 traffic by an average of 32.4 percent compared to that of Base-8. On Type 1 benchmarks, IP-8 reduces SM-L2 traffic by an average of 61.4 percent compared to that of Base-8. As mentioned previously, Type 1 benchmarks share considerable data among SMs making the proposed CTA allocation scheme effective. For Type 2 benchmarks, IP-8 reduces SM-L2 traffic by an average of 4.3 percent compared to that of Base-8. Because Type 2 benchmarks already have limited data sharing among SMs, the effect of the proposed CTA allocation is also limited. In the case of SM-Mem traffic, IP-8 generated 18.0 percent more traffic than Base-8. The L2 cache of Base-8 prevents redundant data load among the SMs from going to the main memory. IP-8 prevents it by using pattern-aware CTA allocation, but redundant loads at the boundary of area allocated to SMs remain. In the case of MatMul, there are many data sharing between SMs because the same row and the same column of input matrices are repeatedly used to calculate the result. However, because the cause of data sharing is not coalescing, the traffic is not reduced by the proposed method. Nonetheless, the increased SM-Mem traffic of IP-8 compared to that of Base-8 is only 15.7 percent of the SM-L2 traffic of Base-8. Thus, it can be concluded that the proposed pattern-aware CTA allocation effectively reduces the total traffic within the memory hierarchy.

Fig. 14. - 
Memory traffic for the Base-8 and IP-8. All results are normalized to SM to L2 memory traffic of Base-8.
Fig. 14.
Memory traffic for the Base-8 and IP-8. All results are normalized to SM to L2 memory traffic of Base-8.

Show All

5.4 Energy Consumption
Fig. 15 shows the energy consumption of six configurations normalized to Base-8. Energy consumption of the perfect L2 was not measured because we could not compare the main memory access power of the perfect L2. The evaluation results show IP-NDP has the lowest energy consumption compared to that of all the benchmarks. IP-8 consumes 20.9 and 39.5 percent less energy compared to that of Base-8 and Base-8-LT, respectively. There are two main reasons for the minimum energy consumption of IP-8. First, IP-8’s fast execution increase its energy efficiency. Second, the SM of IP-8 consumes less static power than that of Base-8. SM of IP-8 has only 8,192 registers and a 8 KB L1 cache size, while the SM of Base-8 has 32,768 registers and a 32 KB L1 cache size. We estimated the static power of IP-8’s SM and assumed it consumes 75.2 percent of the static power compared to Base-8’s SM (details are provided in Section 5.5). For Denoising, IP-8 has only a 0.97× speed increase comapred to that of Base-8, but IP-8 consumed 8.6 percent less energy because of static power difference.

Fig. 15. - 
Energy consumption for Base-8, Base-8-BCS, Base-8-LT, Base-8-LT-BCS, IP-8, and IP-16. All results are normalized to the Base-8.
Fig. 15.
Energy consumption for Base-8, Base-8-BCS, Base-8-LT, Base-8-LT-BCS, IP-8, and IP-16. All results are normalized to the Base-8.

Show All

5.5 NDP Area and Power Analysis
In this section, area and power consumption of IP-NDP and baseline NDP are estimated and analyzed. The area was estimated based on fermi architecture [31], in which the area of the SM is known to be 16mm2 in a 40 nm process. The area of baseline NDP’s SM is estimated by scaling an area of fermi SM from 40 to a 22 nm process. In the case of IP-NDP, the area of the register and L1 data cache, which is a modified SRAM component is calculated using CACTI 6.5, and the remaining components are scaled to 22 nm and added. For a detailed configuration of register and cache refer to [32]. Calculated values are shown in Table 5. In addition, IP-NDP has a prefetcher. Pattern-aware prefetcher is the only unit added to the SM of IP-NDP. The pattern-aware prefetcher is composed of 1 mux, 3 adders, 1 divider, and 1 comparator. To store image’s start address, image size, base stride, and modified stride, 32, 20, 12, and 12 bits are required, respectively. This is negligible compared to the total SM area. An evaluation of a power consumption was carried out using GPUWattch and Cacti-6.5. SRAMs such as the registers, L1 cache, and shared memory are measured by Cacti 6.5. For the remainder, GPUWattch was used. Based on the obtained values, it was analyzed whether the proposed IP-NDP meets the area and power constraint conditions of the logic die.

TABLE 5 Estimated Area and Power Consumption of SM Components
Table 5- 
Estimated Area and Power Consumption of SM Components
Area. The area available on the logic die of HBM is known to be 50-60mm2 [33], [34]. For Base-NDP, considering the area of L2, only 4 SMs can be mounted on the logic die. Even if there is no L2 cache, only 12 SMs can be mounted. In case of IP-NDP, up to 22 SMs can be mounted considering that the SM area is 55 percent of that of Base-NDP and there is no L2 cache. Therefore, both IP-8 and IP-16 satisfy the area constraint.

Power. The power constraint of the logic die is known to be 55 W [33]. Fig. 16 shows the power consumption of Base-8, IP-8, and IP-16. Among the benchmarks, the maximum power consumption of IP-8 and IP-16 is 30.7 and 50.1 W, respectively. Thus, IP-8 and IP-16 both meet the power constraint of logic die. In addition, more SMs can be added considering power gating or a more powerful cooling system.

Fig. 16. - 
Power consumption of Base-8, IP-8, and IP-16.
Fig. 16.
Power consumption of Base-8, IP-8, and IP-16.

Show All

Finally, Fig. 17 shows the performance per area normalized to Base-8. The performance per area of IP-8 and IP-16 is 4.27× and 3.38× higher than that of Base-8, respectively. It can be confirmed that the proposed IP-NDP achieves high performance by efficiently using area.

Fig. 17. - 
Performance per area of Base-8, IP-8, and IP-16 normalized to Base-8.
Fig. 17.
Performance per area of Base-8, IP-8, and IP-16 normalized to Base-8.

Show All

SECTION 6Related Work
6.1 Near Data Processing
The PIM architecture during the early 1990s and early 2000s was implemented by integrating DRAM and the processor on the same DRAM die [5], [8], [35], [36]. The energy consumption required for data transfer could be reduced by placing a processor in the DRAM, but the cost of integrating the processor with the DRAM was too high. With advances in 3D stacked memory, NDP study [2], [6], [7], [9], [10], [13], [14], [16] proceeded to incorporate a computing unit into the logic die of the 3D stacked memory. Top-PIM [9] implemented the GPU SM in 3D-stacked memory and suggested a method for predicting NDP performance. However, the authors did not state whether the operation would be suitable or not for NDP, and they put it on the NDP without changing the SM structure. Xie et al. [16] proposed a PIM for graphic rendering and a rendering algorithm modified to be more suitable for PIM. They moved part of the texture units on the PIM. However, the PIM proposed in this paper was only suitable for rendering and not appropriate for processing in various image processing applications. Panda et al. [10] analyzed the memory access pattern and proposed a technique to improve the row buffer locality. In addition, they proposed a prefetching method which prefetched to the L2 cache. However, their work did not consider the CTA scheduling of the GPU. In contrast, this paper noted that the access pattern and locality of the algorithm are broken because of the base CTA scheduling of the GPU and proposed pattern-aware CTA allocation and prefetching to utilize it without modifying the base CTA scheduling. In addition, the proposed method outperforms the performance of [10] by accurately prefetching data to the L1 cache and not the L2 cache. Some researchers proposed an offloading method that allocated workloads to the host and NDP [13], [14]. Hsieh et al. [14] set the offloading criterion by determining the bandwidth benefit of the instruction when offloading to NDP. Moreover, they proposed user-transparent mapping to minimize the communication overhead between multiple NDP stacks. Pattnaik et al. [13] allocated workloads at the kernel level. They proposed a regression method to determine which kernel to offload to NDP. They focused on scheduling and offloading, not NDP architecture. Other studies proposed a PIM architecture for specific algorithms such as graph processing [2], [6], [7]. However, it is not easy to apply this architecture to image processing because the characteristics of image processing are different from those of graph processing.

6.2 GPU CTA Scheduling
Many researchers [17], [37], [38] have proposed an efficient CTA scheduling method in the GPU. Kayıran et al. [37] showed the optimal number of CTAs is different for each phase of the application, and the performance can be improved by dynamically limiting the number of CTAs to be executed. However, when the number of threads is small as in the proposed architecture, latency hiding does not propoerly work if the number of CTAs is reduced. [17], [38] proposed CTA schedulers that exploit the inter-CTA locality. Lee et al. [17] exploited inter-CTA locality by simply assigning two consecutive CTAs to one SM. However, simply allocating consecutive CTAs does not preserve the algorithm data access pattern; thus, performance improvement is limited as shown in the evalulation. Li et al. [19] classified the sources of inter-CTA locality and proposed the CTA clustering framework that can be used in real GPUs. Unlike [19], the proposed method allocates only one CTA per SM and the corresponding CTA processes the allocated area. The proposed scheme does not require complicated calculation for the CTA remapping function. In addition to space allocation, there is also a limited size cache and additional methods for better use of a smaller number of threads.

6.3 GPU Prefetching
Prior works [21], [22], [39] proposed prefetching methods for GPU. However, because the order in which the CTAs are allocated on the GPU is not known, the prefetching methods are limited to within the CTA. Jeon et al. [23] proposed CTA aware prefetching. The leading warps of each CTA are scheduled first and the stride is calculated to prefetch the next warp. However, it cannot prefetch the data for the leading warp of each CTA. Therefore, existing prefetching methods are effective when the size of CTA is sufficiently large to contain multiple warps. However, if the number of total threads is small, the number of CTAs decreases as the size of the CTA increases. If the number of CTAs decreases, the performance degrades because the initial overhead incurred when allocating CTAs is not hidden by other CTAs. Therefore, it is difficult to apply to the lightweight NDP GPU proposed in this paper.

SECTION 7Conclusion
In this paper, a software-based CTA allocation scheme as well as a GPU-based NDP architecture were proposed for image processing applications. Simply putting the GPU to a logic die in 3D memory can increase the image processing performance, but there may be considerable unnecessary memory access. Moreover, there is a performance limitation when the area constraints of the logic die are considered. To address this issue, pattern-aware CTA allocation is proposed for reducing the memory traffic, in which data are allocated to an SM according to the data access pattern of the image processing such that data sharing between the SMs is minimized. Meanwhile, by providing a prefetcher that utilizes pattern-aware CTA allocation, a lightweight NDP friendly architecture was achieved.

In future works, a compiler will be modified to automatically replace the code with an NDP-compatible code. Because the proposed programming technique is formal and regular, it is expected that the compiler can manage it in a manner that is transparent to the user.