Convolutional Neural Networks (CNNs) have been widely used in machine learning tasks. While delivering state-of-the-art accuracy, CNNs are known as both compute- and memory-intensive. This paper presents the SqueezeFlow accelerator architecture that exploits sparsity of CNN models for increased efficiency. Unlike prior accelerators that trade complexity for flexibility, SqueezeFlow exploits concise convolution rules to benefit from the reduction of computation and memory accesses as well as the acceleration of existing dense architectures without intrusive PE modifications. Specifically, SqueezeFlow employs a PT-OS-sparse dataflow that removes the ineffective computations while maintaining the regularity of CNN computations. We present a full design down to the layout at 65 nm, with an area of 4.80 mm 2 and power of 536.09 mW. The experiments show that SqueezeFlow achieves a speedup of 2:9× on VGG16 compared to the dense architectures, with an area and power overhead of only 8.8 and 15.3 percent, respectively. On three representative sparse CNNs, SqueezeFlow improves the performance and energy efficiency by 1:8× and 1:5× over the state-of-the-art sparse accelerators.
SECTION 1Introduction
Convolutional neural networks (CNNs) have achieved an unprecedented accuracy on many machine learning applications, from object recognition and detection to scene understanding [1], [2], [3], [4], [5]. CNNs are known to be both compute- and memory-intensive, making it difficult to efficiently handle large scale CNNs on CPUs or GPUs [6], [7], [8], [9], [10]. Hence, a number of customized accelerators [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34] have been proposed to deliver high computational throughput. However, with CNNs going larger and deeper to further yield higher accuracy, e.g., the amount of synaptic weights in [35], [36] reaches up to 10 billion, it remains a big challenge to efficiently process such networks even on state-of-the-art accelerators.

To deal with this problem, researchers have proposed many effective techniques to compress CNN models and reduce computation while maintaining comparably high accuracy with original CNNs. Previous studies [37], [38], [39], [40], [41] have demonstrated that a large fraction of input activations and weights can be pruned to zero without loss of accuracy. As shown in Fig. 1, more than 30 percent of the input activations and 50 percent of the weights in the convolutional layers (CVLs) of VGG16 are zero values, and more than 65 percent of the computations are unnecessary due to zero operands. Because zero values and their corresponding computations contribute nothing to the final results, the pruning techniques significantly shrink the amount of total computations in CNN inference.


Fig. 1.
The sparsity in the convolutional layers of VGG16.

Show All

However, such improvements are at the cost of regularity in CNN computations, making it hard to improve the performance and energy efficiency using existing accelerators which are good at processing regular and dense CNNs. The irregularity in sparse CNNs will inevitably introduce conditional branches for CNN computations to utilize the sparsity. Although enabling conditional branches is trivial for CPU-based solutions, it is hardly applicable for accelerators which are designed for fine-grained data or thread parallelism, rather than flexible data path control. The zeros are still fed into the accelerators to carry out unnecessary computations. Hence, dense accelerators hardly benefit from the sparsity in CNNs.

To overcome this problem, the prevailing method is to enable PEs with the flexibility to independently skip unnecessary computations [26], [27], [28], [29]. Eyeriss [26] exploits sparsity by saving computation energy for zero-valued input activations and compressing weights stored in DRAM. Cnvlutin [27] and Cambricon-X [28] use a compressed encoding of activations and weights, respectively, in their dataflow to remove the corresponding ineffective computations and memory accesses. SCNN [29] eliminates ineffective computations from both zero-valued weights and activations simultaneously. However, these solutions significantly increase the hardware complexity. Moreover, all of them incur performance degradation due to the unbalanced sparsity distribution among the PEs. It is observed that the attainable performance merely reaches no more than 50 percent of their nominal performance.

Unlike prior approaches that trade complexity for flexibility, this paper takes an alternative by smartly partitioning sparse CNNs. We propose concise convolution rules (CCR), to diminish the gap between sparse CNNs and dense CNN accelerators. CCR transforms a sparse convolution into multiple effective and ineffective sub-convolutions. The ineffective convolutions in which either the activations or weights are all zero values do not contribute to the final results and their computations can be eliminated, while the effective ones in which both the activations and weights are dense can be easily mapped to existing dense CNN accelerators. By doing so, sparse CNNs can be easily mapped to existing dense CNN accelerators, obviating the intrusive PE modifications. CCR advocates a novel approach to benefit from the reduction of computation and memory accesses as well as the acceleration of existing dense architectures.

Furthermore, we present SqueezeFlow, an efficient CNN inference accelerator that exploits sparsity in CNNs following the architectural implications of CCR. Unlike previous convolutional dataflow that depends on complex PE designs to remove ineffective computations, SqueezeFlow employs a PT-OS-sparse dataflow that delivers input activations and only nonzero weights to the PE array, ensuring that only effective multiplications are fed into the PE array. Since the ineffective computations are already removed before forwarding computations to PEs, SqueezeFlow dataflow supports sparse CNNs with comparably light-weighted PEs to that in dense CNN accelerators. SqueezeFlow also employs compressed encoding of both activations and weights to reduce DRAM accesses. Meanwhile, inter-PE propagations is enabled to further reduce the expensive on-chip memory accesses.

To evaluate SqueezeFlow, we present a concrete design down to the layout in a 65 nm CMOS technology. We empirically evaluate SqueezeFlow as well as previous accelerators on a set of representative CNN models. The experimental results show that SqueezeFlow achieves 2.9× speedup on VGG16 over a dense CNN accelerator with the same computational resources, with an area and power overhead of only 8.8 and 15.3 percent, respectively. Compared with the state-of-the-art sparse accelerators, SqueezeFlow improves the performance and energy efficiency by 1.8× and 1.5×, respectively. Furthermore, SqueezeFlow has considerable merit on the scalability to PE array scale and compatibility to both sparse and dense CNNs.

The rest of this paper is organized as follows: Section 2 reviews the related works. Section 3 delves into the details of CCR including three cases on sparse CNNs. Section 4 introduces the architectural implications of CCR. In Section 5, we describe the dataflow and the detailed architecture design of the SqueezeFlow accelerator. Section 6 describes the experimental methodology. In Section 7, we implement SqueezeFlow and compare the results to state-of-the-art dense and sparse CNN accelerators. Section 8 discusses the future work. Section 9 concludes this paper.

SECTION 2Related Works
In recent years, accelerating CNNs has been one of the hottest research tops in computer architecture. This section reviews the related works on CNN accelerators and presents the motivation of building a new accelerator for sparse CNNs.

2.1 Dense CNN Accelerators
There are many ASIC accelerators for dense CNNs. The DianNao family is a series of hardware accelerators dedicated for neural networks. DianNao [14] is the first member focusing on memory bandwidth utilization. DaDianNao [15] is proposed for efficiently processing large-scale neural networks with sufficient on-chip memory. ShiDianNao [16] is designed to completely eliminate off-chip memory accesses in embedded systems. Besides the DianNao family, there are many accelerators focusing on optimizing CNNs from computing perspective or memory perspective. Eyeriss [26] is an ASIC CNN accelerator that couples a compute grid with a NoC, enabling flexibility in scheduling CNN computation. FlexFlow [24] exploits the different parallelism schemes in CVLs to boost PE utilization. DNA [22] is a DNN accelerator that leverages Weight, Input, and Output Reuse within the same fabric. MAERI [30] is a recent DNN accelerator for mapping arbitrary dataflow that arises in DNNs due to its topology or mapping.

FPGAs have also been widely used to build CNN accelerators as they are flexible to accommodate different dataflows based on their reconfigurable substrate. Zhang et al. [18] focuses on the balance between computing resource and memory bandwidth for CVLs. Fused CNN [19] and Shen et al. [42] have explored CNN architectures that focus on cross-CVL optimizations over FPGAs. Qiu et al. [20] proposed a CNN accelerator to address the limited bandwidth problem by a dynamic-precision data quantization flow. Suda et al. [21] proposed a methodology to find the optimal accelerator design for any CNN model implementation.

2.2 Sparse CNN Accelerators
Sparsity has proven to be an effective approach to reduce computation and memory accesses in CNNs. Many works have investigated how to compress CNN models while maintaining the accuracy, including Sparse Coding [43], Auto Encoder/Decoder [38], Deep compression [44]. A state-of-the-art pruning technique is proposed by Han et al. [40] that learns only important connections in CNNs and prunes the unimportant connections. It shrinks the amount of synaptic weights by about 10× with negligible accuracy loss. Meanwhile, activation sparsity stems from zero-padding and the activation functions such as the rectified linear unit (ReLU).

The sparsity in CNNs will inevitably result in irregular workloads, which are difficult to accelerate by accelerators dedicated for dense and regular models. Lots of CNN accelerators have exploited sparsity to reduce energy and save computation time. Cnvlutin [27] stores sparse activations in compressed format and skips computation cycles for zero-valued activations to improve both performance and energy efficiency. Cambricon-X [28] exploits sparsity by compressing the pruned weights, skipping computation cycles for zero-valued weights. SCNN [29] leverages the sparsity in both weights and activations, exploiting an algorithmic dataflow that eliminates ineffective computations from both zero-valued weights and activations simultaneously. EIE [45] performs inference on the compressed fully-connected layers and accelerates the resulting sparse matrix-vector multiplication. Mao et al. [46] explores the granularity of sparsity in CNNs and shows that coarse-grained sparsity is more hardware-friendly and energy-efficient for sparse CNN accelerators. UCNN [47] exploits weight repetition to save energy and improve performance during CNN inference. These methods all enable PEs with the flexibility to independently skip unnecessary computations, thereby reaping the benefits of sparsity.

However, these solutions significantly increase the underlying hardware complexity. Moreover, they incur performance degradation due to the unbalanced sparsity distribution among the PEs. Because their PEs are usually tightly-coupled together to generate the results, early-finishing PEs has to stay idle while waiting for the laggards. This load imbalance results in severe PE under-utilization thus degrading the performance. For example, it is confirmed by the experimental results that SCNN can only reach up mo more than 50 percent of its nominal performance.

To tackle this problem, this paper presents a novel accelerator architecture, called SqueezeFlow, to exploit sparsity of CNN models for increased efficiency. SqueezeFlow exploits concise convolution rules to benefit from the reduction of computation and memory accesses as well as the acceleration of existing dense architectures without intrusive PE modifications. Hence, SqueezeFlow exploits sparsity while being free from the problems that prior solutions incurred.

SECTION 3Concise Convolution Rules
In this section, we present the concise convolution rules, which smartly partitions sparse convolutions into effective and ineffective sub-convolutions. We first introduce the formulated representation for convolutions to better clarify the rationale behind CCR. Then, we describe the formulated representation of CCR and the proof.

3.1 Formulated Representation for Convolution
2D Convolution. As a building block for CNNs, 2D convolution is the process of adding each element of the image to its local neighbors weighted by the kernel. Specifically, the input map (imap) I is convolved by a kernel K and generates the output map (omap) O, denoted as O=I∗K.

2D convolution has three modes to control the output shape, i.e., ’full’, 'same’ and ’valid’. In this paper, we focus on the ’full’ mode, since the results in other modes can be also obtained through that in ’full’ mode by cropping the omaps. Unless otherwise specified, the convolutions in the following texts are in full mode.

The computation of a 2D convolution is defined as:
O(x,y)=E=∑j=0S−1∑i=0R−1K(i,j)⋅I(x+i,y+j),−R<x<H,−S<y<W,H+R−1,F=W+S−1,(1)(2)(1)
View Sourcewhere H/W,R/S,E/F denote the height/width of the imaps, kernel and omaps, respectively. Note that the starting coordinate of O is not (0,0) but (1−R,1−S), while its ending coordinate is (H−1,W−1). We adopt such coordinate system to better formulate CCR.

Triplet Format for Sparse Matrix Representation. Before delving into the properties of 2D convolution, we first introduce the triplet format for a matrix representation. A matrix can be separated into multiple sub-matrices while recording their coordinates, i.e., the row and column index of the first element in each sub-matrix. Take a matrix K:(kij)4×4 as an example, it can be represented as follows,
K:{(0,0,K1),(2,0,K2),(0,2,K3),(3,0,K4)}or K=⨄i=14(ri,ci,Ki),(3)(2)
View Sourcewhere
r1=0,c1=0,r2=2,c2=0,r3=0,c3=2,r4=3,c4=0K1=[k00k10k01k11],K2=[k20k21],K3=⎡⎣⎢k02k12k22k03k13k23⎤⎦⎥,K4=[k30k31k32k33](4)(5)(3)
View Source

Similarly, the sub-matrices can be concatenated to generate the original one. Note that the sub-matrices may overlap with others. If this happens, the overlapped values should be accumulated when concatenated; otherwise, the sub-matrices should keep the original. The triple format provides a representation for sparse matrices since they are represented as combination of nonzero (dense) and zero matrices.

3.2 CCR on Sparse Kernels
CCR based on sparse kernels is defined as follows:

Property 1 (CCR-1).
ifK=⨄l=1p(rl,cl,Kl),  thenI∗K=⨄l=1p(αl,βl,I∗Kl)whereαl=1−rl−Rl,βl=1−cl−Sl,(6)(4)
View Source

where p denotes the number of sub-matrices decomposed from K, rl/cl denote the coordinates (row and column index) of sub-matrix Kl in K, Rl/Sl denote the height and width of sub-matrix Kl. The proof of CCR-1 is as follows:

Proof of CCR-1.
O(x,y)=∑j=0S−1∑i=0R−1K(i,j)⋅I(x+i,y+j)=∑l=1p∑j=clcl+Sl−1∑i=rlrl+Rl−1K(i,j)⋅I(x+i,y+j)=∑l=1p∑j=0Sl−1∑i=0Rl−1K(i+rl,j+cl)⋅I(x+rl+i,y+cl+j)=∑l=1p∑j=0Sl−1∑i=0Rl−1Kl(i,j)⋅I(x+rl+i,y+cl+j)=∑l=1pOl(x+rl,y+cl),(7)(8)(9)(10)(5)
View Sourcewhere Ol=I∗Kl.

According to (5), there exists an offset (−rl,−cl) between the coordinates in O and Ol. The starting coordinate of Ol, i.e., (1−Rl,1−Sl), corresponds to coordinate (1−Rl−rl,1−Sl−cl) in O, which is exactly the coordinate when concatenating the partial omaps. Therefore,
O=⨄l=1p(1−Rl−rl,1−Sl−cl,Ol),(6)
View SourceRight-click on figure for MathML and additional features.

CCR-1 transforms a convolution into multiple sub-convolutions, each with the original imap and a sub-kernel. Based on CCR-1, the computation and memory accesses of convolutions with sparse kernels are reduced. By decomposing a sparse kernel into nonzero and zero sub-kernels using the triplet representation, the convolution is transformed into effective sub-convolutions (with nonzero sub-kernels) and ineffective ones (with zero sub-kernels). Since the ineffective sub-convolutions do not contribute to the final results, their computation and memory accesses can be eliminated. Fig. 2 demonstrates an example. The kernel is decomposed into (0,0,K1),(0,1,K2),(2,0,K3),(2,1,K4), where K1 and K4 are zero sub-kernels and do not contribute to the final results. We only need to compute the effective sub-convolutions with K2,K3 and generate the sub-omap O2,O3, then concatenate them to the final results using the offsets calculated as:
α2=1−r2−R2=1−0−2=−1β2=1−c2−S2=1−1−2=−2α3=1−r3−R3=1−2−1=−2β3=1−c3−S3=1−0−1=0(11)(12)(13)(7)
View Source


Fig. 2.
An example of CCR on sparse kernels. The kernel K is decomposed to K2 and K3 to convolve I, respectively.

Show All

In this example, the computation volume quantified by multiply and accumulation (MAC) operations is significantly reduced. The original convolution contains 4×4×3×3=144 MACs, while the MAC operations aggregated by the effective sub-convolutions is 4×4×2×2+4×4×1×1=80, achieving a computation reduction up to 44.4 percent. Meanwhile, the memory footprint is also reduced in CNN inference since we only need to record the nonzero sub-kernels and their coordinates.

Most importantly, CCR-1 diminishes the gap between sparse and dense convolutions. The effective sub-convolutions derived from CCR-1 maintains the same imap size with the original one, which reveals that they can be easily mapped to existing dense accelerators at low hardware overhead. The architectural implications will be introduced in Section 4.

3.3 CCR on Sparse Input Maps
Similarly, CCR based on sparse imaps is defined as:

Property 2 (CCR-2).
ifI=⨄l=1q(ml,nl,Il),  thenI∗K=⨄l=1q(γl,θl,Il∗K)whereγl=1+ml−R,θl=1+nl−S,(14)(8)
View Source

where q denotes the number of sub-imaps decomposed from I, ml/nl denote the coordinates (row and column index) of sub-imap Il in I, R/S denote the height and width of kernel K.

The proof of CCR-2 is similar to the one of CCR-1 and is omitted due to space limitations. According to CCR-2, a convolution is transformed to multiple sub-convolutions, each with a sub-imap and the original kernel. The computations of convolutions with sparse imaps are reduced in a similar way as CCR-1. As exemplified in Fig. 3, the imap I is partitioned into seven sub-imaps, where I1,I2,I4,I6,I7 are zero sub-imaps that do not contribute to the final results. We only need to compute the effective sub-convolutions with I3,I5 and generate O3,O5, then concatenate them to generate the output map. The above example achieves a computation reduction up to 72.2 percent. The effective sub-convolutions maintain the same kernel size with the original one, which indicates that only limited hardware modifications to the previous accelerators are required to support the effective sub-convolutions.


Fig. 3.
An example of CCR on sparse input maps. The input map I is decomposed to I3 and I5 to be convolved with K, respectively.

Show All

3.4 CCR on Sparse Kernels and Input Maps
CCR-1 and CCR-2 simplify the computation of convolutions with sparse kernels or imaps, from which CCR based on both sparse kernels and sparse imaps is derived as follows:

Property 3 (CCR-3).
ifK=⨄i=1p(ri,ci,Ki),I=⨄j=1q(mj,nj,Ij)thenI∗K=⨄j=1q(γj,θj,Ij∗K)=⨄j=1q(γj,θj,⨄i=1p(αi,βi,Ij∗Ki))=⨄j=1p⨄i=1q(αi+γj+R−1,βi+θj+R−1,Ij∗Ki)=⨄j=1p⨄i=1q(mj−ri−Ri+1,nj−ci−Si+1,Ij∗Ki)(15)(16)(17)(18)(9)
View Source

According to CCR-3, every sub-imap is convolved by every sub-kernel and generates a sub-omap. The convolutions with either zero sub-imaps or zero sub-kernels can be eliminated. Fig. 4 demonstrates an example, the nonzero sub-imaps I1,I2 are convolved by the nonzero sub-kernels K1,K2 and generate four sub-omaps O11,O12,O21,O22, which are then concatenated to generate the final results. CCR-3 maximally removes the ineffective computations since it exploits the sparsity in both imaps and kernels. It achieves a reduction on MAC operations up to 72.2 percent in the above example.


Fig. 4.
An example of CCR on sparse kernels and input maps. The input map I is decomposed to I1 and I2, the kernel K is decomposed to K1 and K2. Each sub-imap is convolved with each sub-kernel, respectively.

Show All

Since CCR decomposes 2D convolutions into matrix multiplications, we would like to mention the essential differences between CCR and prior matrix computation optimizations, such as [48], [49], [50]. It is well known that 2D convolution can be performed using just one matrix multiplication, by converting one of the input matrices, e.g., the imap, to a Toeplitz matrix [51]. This involves replicating the data elements in imap multiple times across different matrix columns. After constructing the Toeplitz matrix, convolution can be implemented using prior matrix multiplication optimizations. The major downside of such representation is the space explosion when building the column matrix. For a convolution with a 2D k×k kernel matrix, the column matrix is k2 times larger than the original imap. The increased memory requirements will significantly increase the memory traffic, and lead to increased execution time. Therefore, this representation is rarely adopted by CNN accelerators which usually have limited on-chip local memories. Instead, the common method is using for-loops to perform 2D convolutions. In this scenario, a 2D convolution contains a set of matrix multiplications, where there exists plenty of data reuse opportunity. Prior matrix computation optimizations [48], [49], [50] only optimize for a single matrix multiplication, while CCR can be considered as a co-optimization of a set of matrix multiplications by exploiting data reuse. CCR eliminates the need for data replication on the input, which significantly shrinks the memory footprint. Because of the limited on-chip memory budget, using CCR approach to build CNN accelerators is easier to achieve high performance and energy efficiency.

3.5 Theoretical Reduction on Computation and Data Volume
Through CCR, we can calculate the theoretical reduction on computation and data volume. We take CVLs in VGG16 as an example. CCR-3 eliminates the most computations for all layers (79.7 percent on average) since it exploits the sparsity in both input maps and kernels. CCR-1 and CCR-2 remove 67.2 and 39.7 percent of the total computations, respectively. Meanwhile, CCR-1, CCR-2, and CCR-3 achieve 39.8, 11.5, 51.2 percent on data volume reduction, respectively.

SECTION 4Architectural Implications
This section introduces the architectural implications of CCR and presents how CCR bridges the gap between sparse CNNs and dense CNN accelerators. We start from a typical dense architecture and investigate how it evolves to support CCR without intrusive PE modifications.

4.1 Generic Dense Architecture
A generic architecture of state-of-the-art dense accelerators [16] can be modeled as Fig. 5a. It comprises a weight buffer (KB), input activation buffer (IB), output activation buffer (OB), a Processing Unit (PU), an accumulation unit (AccUnit) and a scatter crossbar. The PU is a 2D mesh of Px×Py Processing Elements (PEs). To process a convolution, a scalar of kernel and a matrix block of input map are fetched from their respective buffers and fed into the PU which computes a scalar matrix multiplication, i.e., every element in the matrix is multiplied by the scalar to form a Px×Py products. The products are delivered into AccUnit to accumulate with the corresponding partial sums fetched from OB, the result is then routed to OB by the crossbar. Because of the regular addressing pattern of OB, the output coordinates can be derived from the loop indices in a state machine (not shown in the figure). Note that the architecture we described is quite simple for the sake of investigating how it evolves to support CCR. Some unique features of prior solutions optimized for data movement are not enabled, more details can refer to [16].


Fig. 5.
Sparse architectures supporting CCR.

Show All

In this architecture, the input maps and kernels are stored in uncompressed format and they proceed in lock-step. Even if the scalar from the kernel is zero, it is still fed into the PU to carry out unnecessary computations. Thus, the dense architectures cannot benefit from the reduction of both computation and memory access.

4.2 Sparse Architectures Supporting CCR
We first demo a sparse architecture that supports CCR-1, denoted as SparseArch1. As stated in the analysis of CCR-1, if a kernel is sparse, we only need to record the nonzero values and their coordinates. Thus the kernels are stored in a compressed format, i.e., only the nonzero values and their coordinates appear in KB, as shown in Fig. 5b. The nonzero values are fed into the PU, while the corresponding coordinates are fetched into a Coordinate Computation Unit (CCU) to compute the coordinate of the output according to (4). The Px×Py products lies in a rectangle block in the output maps, the addressing pattern of OB is still regular and only one coordinate needs to be calculated, i.e., the coordinate of the first element. Then, the Px×Py products are delivered into the AccUnit indexed by the coordinates and are accumulated to the corresponding partial sums fetched from OB. Finally, the accumulated results are routed to OB by the crossbar. Since the sub-convolutions derived from CCR-1 remain the same input map size with the original convolution, the mapping of the sub-convolutions onto the architecture exhibits little difference with the mapping to the dense architecture [16].

Compared with the dense architecture, the coordinates are not derived from a state machine but a dedicated CCU. The CCU only needs to calculate one coordinate for Px×Py products and can be implemented at low overhead. Accordingly, SparseArch1 efficiently supports sparse convolutions, obviating intrusive PE modifications to the dense architecture.

The architecture supporting CCR-2 (SparseArch2) is derived following the similar rationale, as shown in Fig. 5c. To support CCR-3, the SparseArch3 architecture is more complicated with higher overhead than SparseArch1 and SparseArch2. Since the sub-convolutions derived from CCR-3 differ in both imap size and kernel size, the complexity significantly increases when mapping them to the same hardware. A feasible solution is that the imaps and kernels are all partitioned into sub-imaps and sub-kernels of 1×1 shape. The convolution with a 1×1 imap and 1×1 kernel thus equals to the scalar multiplication. As every scalar in I (a 1×1 imap) will be multiplied by every scalar in K (a 1×1 kernel), the original convolution then, is transformed to Cartesian product of vectors. The CCU has to calculate Px×Py coordinates for each Px×Py product since the coordinates of the products are randomly distributed, hence the complexity for its CCU is O(Px×Py). Notably, the randomness of the output coordinates will cause bank contention in AccUnit and the Crossbar since multiple partial sums may hash to the same buffer banks.

Table 1 summarizes the characteristics of these architectures. Clearly, SparseArch3 eliminates the most ineffective computations. However, the hardware design is the most complicated. Moreover, SparseArch3 has a poor compatibility with the dense models since the Cartesian Product cannot well match the dense convolutions. Although unable to remove the most ineffective computations, SparseArch1 is remarkably efficient and economic since it achieves a comparably high computation reduction rate at a lower overhead than SparseArch3. Therefore, we opted for SparseArch1 as the architecture to efficiently cope with not only dense but also sparse CNN models.

TABLE 1 The Sparse Architecture Design Specifications

SECTION 5SqueezeFlow Accelerator Architecture
This section presents a dataflow accelerator based on CCR-1, called SqueezeFlow. We first describe the PT-OS-sparse dataflow which employs CCR-1 as its inner core, followed by the full SqueezeFlow accelerator that employs such dataflow.

5.1 PT-OS-Sparse Dataflow
A complete CNN dataflow requires a deep nested loop structure, and defines how the loops are ordered, partitioned, and parallelized [26]. Similar to dense architectures, sparse architectures also employ specialized dataflows to optimize performance and energy efficiency. We present a specific dataflow PT-OS-sparse. This sparse dataflow enables input data reuse while minimizing the psum accumulation cost. The implementation of the PT-OS-sparse dataflow is inspired by the idea of CCR that decomposes sparse convolutions into dense sub-convolutions. The dense sub-convolutions are mapped to PEs using dense dataflows, which have been extensively in prior works.

Fig. 6 shows the loop nest in the PT-OS-sparse dataflow. Note that the output and input channel (K and C) dimension has already been factored into K/Tk and C/Tc output/input channel groups. Such loop tiling increases data reuse and improves energy efficiency. The pseudo code only shows the inner loops of the complete dataflow for brevity, the omitted outer loops can be spatially and/or temporally spread across the PEs, which have been extensively studied in prior works. We focus on the inner loops to demonstrate how this dataflow leverages CCR-1 to exploit sparsity.


Fig. 6.
PT-OS-sparse dataflow.

Show All

We employ a spatial tiling strategy to spread the work across an array of PEs so that each PE operates independently. The W×H element activation plane is partitioned into smaller Tw×Th element planar tiles (PT) that are distributed across the PEs. We employ an output-stationary (OS) computation order in which the psums are held stationary at the computation units for accumulation to minimize the psum accumulation cost. In the PT-OS-sparse dataflow, the sparse convolutions are decomposed into matrix scalar products, which is considered as convolutions with kernel size equal to one. Therefore, from the perspective of CCR-1, the kernels are partitioned into multiple sub-kernels of size 1×1.

The PT-OS-sparse dataflow contains looping over the K and C dimension (A,B), blocking in the W and H dimension (C,D), fetching scalar of weights (E) and the corresponding index (F), computing the coordinates of activations, fetching the matrix of activations according to the coordinates, computing the matrix scalar product in parallel (G,H). The Xcoord(), Ycoord() functions compute the x,y coordinate using the temporal loop indices w′,v, the spatial loop indices w, the RLC index w_idx, and the known kernel width and height.

For easier decoding, the weights are grouped into compressed blocks, with one Tc×Tk×R×S weights encoded into one compressed block, and we denote the length of each compressed R×S block as V, since the 2-dimension feature map will be compressed into 1-dimension vector. By contrast, the input activations are stored in uncompressed format, and the size of each block is Tc×W×H. At each access, the weight buffer delivers a scalar of nonzero filter weight along with its coordinate in the Tc×Tk×R×S region. Similarly, the input buffer delivers a matrix of Tw×Th input activations. The coordinates of this matrix block is computed using Xcoord and Ycoord functions. Then, the multiplier array computes the matrix scalar product to produce the Tw×Th partial sums, corresponding to the output block determined by the loop indices w′ and h′. Unlike prior Cartesian Product based dataflows such as SCNN, the multiplier outputs in PT-OS-sparse are still contiguous as they correspond to a matrix block in output feature maps, which significantly reduces the complexity when scattering the output psums into the output buffers (will be discussed in Section 5.4). Since we use output-stationary computation order, the output matrices are natively accumulated in the PE array. Meanwhile, the loop indices w,h and coordinate computations ensure that these output matrices are concatenated correctly to generate the final output results. When the computation for the output channel group has been completed, the output buffer will be compressed and written back to DRAM.

5.2 Overview of SqueezeFlow Architecture
Fig. 7 demonstrates the full SqueezeFlow accelerator architecture based on PT-OS-sparse dataflow. We call it SqueezeFlow because it is a dataflow architecture that “squeezes” dense CNNs from sparse CNNs. SqueezeFlow consists of an accelerator chip and off-chip memory (usually DRAM). The accelerator chip is primarily composed of Processing Unit (PU) and a global buffer (GLB), two decoders, an encoder, a Control Unit (CU), a Buffer Controller (BC), a Coordinate Computation Unit (CCU), and a Post-Processing Unit (PPU). The PU contains a PE array which are connected with each other via a network-on-chip (NoC) [52] and support high compute parallelism to perform the massive convolution operations. The PPU is responsible to apply the accumulation (the AccUnit is integrated into PPU), non-linear activation (e.g., ReLU), pooling, and dropout functions. The accelerator provides four levels of storage, including DRAM, GLB, inter-PE connections and Register Files in each PE. GLB is used to exploit input data reuse and hide DRAM access latency, or for the storage of intermediate data. The Encoder/Decoder is used to reduce DRAM accesses, so that the data transferred between on-chip GLB and off-chip DRAM are all in compressed format. By doing so, the off-chip memory accesses of the accelerator are largely reduced.


Fig. 7.
Complete SqueezeFlow accelerator architecture.

Show All

To accomplish a CVL computation, the activations and weights are brought from DRAM into GLB, both are in compressed format. Then the compressed weights are fed into the KB directly. The activations, however, are decompressed by the decoder and fed into IB. Then, the BC selects needed activations and weights from IB and KB respectively, based on the loaded instructions in the CU, and transfers those activations and weights to PU to perform sparse convolutions following PT-OS-sparse dataflow. The non-linear activation, pooling or dropout functions will be performed by PPU if necessary. When executing a CVL composed of multiple output feature maps with multiple input feature maps, the accelerator continuously performs the computations of an output feature map, and will not move to the next output feature map until the current map has been constructed. The rest of this section describes the design details of SqueezeFlow.

5.3 Processing Unit
Fig. 7 also presents the PU structure. The PU operates the weights and activations in the order defined by the PT-OS-sparse dataflow to produce the psums. The PU simultaneously read weights and input activations fetched from BC, and then distribute them to different PEs. In addition, the PU contains local storage structures in each PE, which is used to enable local propagation of input activations between PEs (see Section 5.5) to reduce the data transfer between PEs and the GLB. After performing the multiplication between activations and weights, the PU delivers the partial products into PPU. PPU collects these products and accumulate them to generate the final results.

Processing Elements. Fig. 7 also shows the PE micro-architecture. Each PE executes fixed multiply-accumulation operations. PEi,j, which is the PE at the i-th row and j-th column of the PU, has three inputs: one input for receiving the control signals; one input for reading weights (e.g., kernel values of CVLs) from KB; and one input for reading activations from IB/OB, or from other PEs depending on the control signal. The PEi,j has two outputs: one output for writing computation results to OB/IB; one output for propagating locally-stored activations to other PEs to efficiently reuse the data. When executing a CVL, each PE continuously accommodates a single output activation, and will not switch to another output activation until the current one has been computed. By distributing the nonzero kernel element to all the PEs, the PEs will compute different activations of the same output feature map simultaneously.

Inter-PE Propagation. CCR implies that each matrix scalar product requires data from a rectangular window of input maps, and the results also lie in a rectangular window of the output maps. In dense convolutions, such windows are significantly overlapping for adjacent activations, because of the sliding window characteristics of convolution. Although all required data are available from IB/OB, repeatedly reading them from the buffer to different PEs requires a high bandwidth. Therefore, inter-PE propagation is used to support efficient data reuse [16].

In sparse convolutions, inter-PE propagation can also be used to enable data reuse. However, because the overlapping pattern is randomized due to random distribution of zeros, only enabling the PEs to send locally-stored input activations to its left and lower neighbors (like dense architectures) is not efficient. We have to enable that the PEs can send the locally-stored input activation to any PE. Thus, we implement a Network-on-Chip to connect these PEs using fat tree architectures, which supports the data transfer among the PEs. Note that although the reuse pattern is randomized, it is deterministic given the overlapping pattern between the adjacent input activation blocks. Hence, the CU generates the control signals according to the overlapping pattern to determine how the NoC propagates the data among the PEs. Furthermore, unlike ShiDianNao that leverages FIFO structures, we only use a register in each PE for inter-PE propagation to reduce the complexity and area of PEs (see Section 5.5).

5.4 Coordinate Computation Unit
In dense accelerators, the addresses of the outputs are usually generated from a finite state machine (FSM), because of the regular data access patterns of dense convolutions. However, a simple FSM can hardly support address generation for sparse accelerators. Hence, we design a dedicated CCU for address generation. Since coordinate computation is tightly related to the compression techniques, we first introduce how we compress activations and weights using Run Length Compression (RLC). It should be noted that because the selection of specific format is orthogonal to the sparse architecture itself, we can also use other data compression techniques to construct SqueezeFlow, as long as its decoding can yield nonzero values and the corresponding coordinate.

Fig. 8 presents an example of RLC. It records the nonzero values and the count of zeros between adjacent nonzero values. There are two parts in RLC encoding, “Run” and “Length”, storing the nonzero values and the zero counts between adjacent nonzero elements, respectively. The weights are compressed in RLC format and stored in KB.

Fig. 8. - 
Run-length encoding for sparse kernels.
Fig. 8.
Run-length encoding for sparse kernels.

Show All

First, CCU calculates the original coordinates of nonzero weights in a recursive fashion, i.e., the coordinate of the current nonzero weight is computed using its distance to the prior nonzero element (the number of zeros) and the dividing by the W dimension. For example, for weights (…,Ri−1,Li−1,Ri,Li) store in KB, suppose that the coordinate of Ri−1 is known and denoted as (αi−1,βi−1), since there are Li−1 zeros between Ri−1 and Ri, the coordinate of Ri is derived as follows:
αi=αi−1+(βi−1+Li−1+1) / Wβi=(βi−1+Li−1+1) mod W(19)(10)
View Source

Second, the CCU computes the coordinates of the input activation matrix as we use an output-stationary dataflow. Given the coordinates of the output activation matrix (loop indices w and h) and the nonzero weight, the coordinates of the input activations is easily calculated according to the convolution characteristics. Because the coordinates of the input activations are still contiguous in W/H dimension according to CCR, CCU only computes the coordinate of the first input activation using the coordinates of weight and the output activations (loop indices w and h). The coordinates of the rest input activations are generated based on the first coordinate and W/H dimension.

5.5 Sparse CNN Mapping
In this subsection, we show in detail how SqueezeFlow supports to remove the ineffective computations in sparse CVLs. In Fig. 9, we consider a snapshot of the sparse convolution processing. Without losing any generality, we consider a small design having 2×2 PEs (PEi,j indicating the PE located at the ith row and jth column), and a CVL with 3×3 kernel size with four nonzero values, and 1×1 stride size. For the sake of brevity, we describe the first five cycles to analyze how the accelerator works cycle by cycle, as demonstrated in Fig. 10.


Fig. 9.
Algorithm-hardware mapping of convolution operations. The red marks indicate that the input activations are reused by different PEs.

Show All


Fig. 10.
Algorithm-hardware mapping of convolution operations. The red marks indicate that the psum has to initialized using data fetched from OB, otherwise using the value stored in the register as we use output-stationary dataflow. The blue marks indicate the inter-PE propagation.

Show All

Cycle #0: Each PE processes the partial products of the first block in the output feature map, i.e., O0,0, O0,1, O1,0 and O1,1. All four PEs share the same weight from KB, or the first nonzero element in the convolution kernel, i.e., R0. Since it is the first cycle, the coordinate of the prior nonzero weight is initialized as (0,−1). According to (10), the coordinate of R0 is calculated as follows:
xwt=αi−1+(βi−1+Li−1+1) / W=0ywt=(βi−1+Li−1+1) mod W=1(20)(11)
View SourceHence, the coordinate of R0 is (0,1). Then we calculate the coordinate of the first element in the input activation block using the coordinate of output activation and weight as follows:
xin=xout+xwt=0+0=0yin=yout+ywt=0+1=1,(21)(12)
View Sourcewhere xout/yout are the coordinates of the first output activation in the matrix block, i.e., (0,0). So the coordinate of the first element in the input activation block is I0,1. Because the coordinates of the input activations are still contiguous in W/H dimension, the coordinates of the rest input activations are easily calculated, i.e., I0,2, I1,1 and I1,2, which will be fetched into the four PEs. Each PE performs a multiplication between the received input activation and weight, and store the result in its local register, i.e., psum_new register. Then, the result will be accumulated with the data stored in psum_old register, which holds the psum value derived from previous activations and weights. The accumulated result will be stored locally in the psum_old register for future accumulation. In addition, each PE collects its received input activation in its input_reg for future inter-PE propagation.

Cycle #1: All four PEs share the second nonzero weight, i.e., R1. Similar to Cycle #0, CCU computes its coordinate as follows:
xwt=αi−1+(βi−1+Li−1+1) / W=0ywt=(βi−1+Li−1+1) mod W=2(22)(13)
View SourceThen, the coordinate of the first element in the input activation block is xin=0+0=0,yin=0+2=2. The four PEs respectively read their required input activations (I0,2, I0,3, I1,2 and I1,3). Because I0,2 and I1,2 are already stored in the input_reg of PE0,1 and PE1,1 respectively, PE0,0 and PE1,0 enable inter-PE propagation by reading the corresponding input activations from the input_regs of their right PEs, while PE0,1 and PE1,1 read their input activations from IB. Then the multiplication is performed in each PE and the results and input activations will be locally stored in the corresponding registers. So far each PE has processed the first row of the convolutional window, and will move to the second row of the convolutional window in the next cycle.

Cycle #2: Similar to Cycle #1, the coordinate of the third nonzero weight is calculated, i.e., (1,2). The four PEs respectively read their required input activations according to the index of the weight (I1,2, I1,3, I2,2 and I2,3). Because I1,3 is already stored in the input_reg of PE1,1, PE0,1 enables inter-PE propagation by reading the corresponding input activations from the input_regs of its bottom PEs. Meanwhile, the other PEs read their input activations from IB. The PEs perform the multiplications and the results and input activations will be locally stored in the corresponding registers.

Cycle #3: Similar to Cycle #1, all four PEs share the fourth nonzero weight, i.e., K2,0, and perform similar operations. So far each PE has processed the partial products of the first block in the output feature map, i.e., O0,0, O0,1, O1,0 and O1,1, and will move to the second block of the convolutional window in the next cycle. Hence, in this cycle, the partial products stored in psum_old registers will be written back to OB.

Cycle #4: The processing of the second block of the convolutional windows begins at this cycle, i.e., O0,2, O0,3, O1,2 and O1,3. All four PEs share the first nonzero weight from KB, i.e., K0,1 (the coordinate calculation is the same to Cycle #0). Then, the coordinate of the first element in the input activation block is xin=0+0=0,yin=2+1=3. The coordinates of the rest input activations is easily calculated, i.e., I0,4, I1,3 and I1,4. The following operations are similar and omitted for the sake of brevity.

So far we described the first five cycles when mapping sparse CVLs to SqueezeFlow. This example also clarifies that why FIFO structures are not used for inter-PE propagation. First, using FIFOs will significantly increase the area overhead of PEs. Second, because nonzero weights are randomly distributed, the data access of input activations is not so regular as in dense CNNs, using FIFOs does little help as it does not match the access patterns of sparse convolutions.

SECTION 6Experimental Methodology
Implementation. We implement SqueezeFlow in Synopsys design flow on TSMC 65 nm technology: simulating with Synopsys Verilog Compile Simulator (VCS), synthesizing with Synopsys Design Compiler (DC), analyzing power with Synopsys PrimeTime (PT), and placing them with Synopsys IC Compiler (ICC). We used CACTI 6.0 to estimate the energy cost of DRAM accesses [53]. We also resize and re-implement a dense accelerator, called DenseArch, following the rationale described in Section 4.

Baselines. We compare our design with the DenseArch and three representative sparse architectures: Cambricon-X [28], Cnvlutin [27] and SCNN [29]. We developed a custom-build cycle-level simulator to evaluate the performance and energy of the three baselines (Cambricon-X, Cnvlutin, SCNN) since we did not implement them with RTL configuration in Verilog. The simulator models their dataflow as well as the memory hierarchy and PE configurations. The simulator evaluates the performance by computing the number of cycles to process a given layer by the three accelerators. Meanwhile, the simulator collects the counts of MAC operations and memory accesses of different levels. These statistical data are used to build an energy model to estimate the energy consumption of the three baseline accelerators. Because of significant differences in dataflow, buffer sizing/organization, and implementation choices, our evaluated architectures cannot precisely represent the prior proposals.

Architectural Configurations. SqueezeFlow is equipped with a PU containing an 8×8 array of PEs. IB and OB SRAM size are 64 KB, while KB SRAM size is 84 KB, containing 64 KB to store nonzero values and additional 20 KB to store the indices of sparse kernels. We use 16-bit fixed-point arithmetic units as it has been proved to be effective in CNN accelerators [14], [15]. To make a fair comparison, we also resize the baseline architecture to be equipped with the same number of multipliers with SqueezeFlow. Additionally, the working frequency of SqueezeFlow and the baselines are kept the same at 0.9 GHz. The nominal DRAM bandwidth configuration is 34.2 GB/s as we use dual-channel DDR3-2133.

Benchmarks. We benchmark the performance using three representative CNNs: VGG16 [54], AlexNet [1], GoogLeNet [55]. Note that for GoogLeNet, we primarily focus on the CVLs in the inception modules. These models provide a wide range of shapes and sparsity that are suitable for testing the adaptability and flexibility of our accelerator.

SECTION 7Experimental Results
This section compares SqueezeFlow with the baselines in terms of area, performance, power, energy and scalability.

7.1 Layout Characteristics
Tables 2 and 3 presents the parameter settings and layout characteristics of the baseline DenseArch and SqueezeFlow (see Fig. 11). Under the same computing resources and buffer size, SqueezeFlow increases total area by about 8.8 percent over DenseArch, with 4.80 mm2 versus 4.41 mm2. Area compares across the two architectures as follows: 1) The buffers (IB, KB, OB) dominate the total area for both architectures, i.e., 76.2 and 73.3 percent for DenseArch and SqueezeFlow, respectively; 2) the area of PU remain almost the same for both architectures, since CCR enables to support sparse convolutions almost without intrusive PE modifications; 3) the main area overhead for SqueezeFlow stems from KB (increased 0.16 mm2 to store indices of sparse kernels), CP (increased 0.09 mm2 mainly because of CCU), DEC (Encoder/Decoder, 0.12 mm2). Overall, SqueezeFlow only incurs a slight area overhead in order to efficiently support sparse CNNs.


Fig. 11.
Layout of DenseArch and SqueezeFlow.

Show All

TABLE 2 Parameter Settings of DenseArch and SqueezeFlow

TABLE 3 Area and Power Comparison Between DenseArch and SqueezeFlow

The total power of SqueezeFlow is only 536.09 mW, which is 15.3 percent higher than DenseArch with 464.82 mW (averaged over the three benchmarks). The reason consists of two aspects. First, the additional hardware logic to exploit sparsity increases power by about 37.76 mW, including the CP (increases 21.33 mW as it has integrated a Coordinate Computation Unit) and DEC (16.43 mW). Second, the on-chip buffer increases the power by 20.64 mW. The larger buffer size to hold the indices and the relatively randomized accesses of these buffers leads to 8.3, 18.5, and 21.2 percent increment of the power for IB/OB/KB, respectively.

7.2 Performance
Fig. 12 summarizes the speedups delivered by the sparse architectures over the baseline DenseArch. SqueezeFlow consistently outperforms the baselines (except for SCNN on VGG16) and achieves an average speedup of 2.6×,1.8×,1.3×,1.2× over the baselines, respectively. The performance improvement of SqueezeFlow varies widely across the models. Specifically, SqueezeFlow improves the performance by 2.3–2.9× over DenseArch, 1.6–1.8× over Cnvlutin, 1.2–1.4× over Cambricon-X, 0.9–1.4× over SCNN. Although SCNN can theoretically achieve the highest performance for all models since it removes the most ineffective computations stemming from zeros in both kernels and imaps, it delivers a slight performance advantage over SqueezeFlow only on VGG16 but performed much worse than expected on AlexNet and GoogLeNet. The main reason is that SCNN incurs severe performance degradation from the unbalanced distribution of computations among the PEs. The results reveal that although SqueezeFlow is unable to remove all the ineffective computations, it is remarkably efficient across various models.

Fig. 12. - 
Speedup over the baseline dense architecture across the models.
Fig. 12.
Speedup over the baseline dense architecture across the models.

Show All

The performance results are better understood by looking at the performance breakdown of SqueezeFlow and SCNN across the CVLs as shown in Fig. 13. We take VGG16 as an example for further explanation. For bottom layers like C1 and C3, SqueezeFlow achieves a superior performance to SCNN. One reason is that the sparsity of kernels dominates these layers which well matches the advantages of SqueezeFlow, while input maps are not that sparse (only 5 percent sparsity in C1). As the sparsity increases with the layers going deeper, SCNN achieves an overwhelming performance over SqueezeFlow since imaps and kernels are both very sparse in the later layers. However, as the computation volume is mainly dominated by the bottom layers (larger input/output maps), SqueezeFlow achieves a comparable overall performance with SCNN due to the superior performance in the bottom layers.


Fig. 13.
Speedup over the baseline dense architecture across the models.

Show All

For the other two benchmarks, SqueezeFlow significantly outperforms SCNN because SCNN hardly reaches its nominal performance. Although SCNN considers both activation and weight sparsity, it incurs significant performance degradation mainly because of load unbalancing since SCNN exploits Cartesian Products to perform convolution. However, as SqueezeFlow uses matrix scalar product, the load balancing problem is natively addressed under PT-OS-sparse dataflow.

It should be noted that the stride is four in C1 of AlexNet. In this layer, we first compute the corresponding convolution with the same input feature maps and filter weights but stride equal to one. Then, we extract the valid data from it using the stride (=4) to generate the final result, which is similar to the operations in pooling layer. Such implementation will increase the computation volume as it introduces unnecessary computations, but it can be easily implemented under existing dataflow. Furthermore, as such layer type is not common in the benchmarks, it impacts little on the overall performance.

Impact of DRAM Bandwidth. As DRAM bandwidth affects the latency of off-chip memory accesses, we tested the performance of SqueezeFlow under different bandwidth configurations through simulation. We observed that the performance of SqueezeFlow begins to degrade when the DRAM bandwidth drops to 6 GB/s. Since the nominal DRAM bandwidth configuration is 34.2 GB/s, it provides ample bandwidth to absorb the off-chip traffic.

In summary, SqueezeFlow achieves almost the highest speedup on average across the models and provides a tremendous performance advantage over the baselines.

7.3 Energy
In Fig. 14, we report the energy comparison of the architectures which has been normalized to the energy of DenseArch. It should be noted that the energy consumption does not include main memory accesses which usually dominates the total energy consumption. On average, SqueezeFlow improves energy efficiency by 2.0×,1.5×,1.3×,1.5× over the baselines, respectively. The most striking result is that SqueezeFlow achieves an improvement of 2.23× over DenseArch on VGG16. The improvement of energy efficiency varies widely across the models depending on the sparsity of the models. Specifically, the improvement over SCNN ranges from 1.7× on AlexNet, to 1.2× on VGG16. The high energy efficiency of SqueezeFlow stems from: 1) the improvement of performance, SqueezeFlow achieves a high computation reduction rate and free from performance degradation caused by irregular computation distributions; 2) low hardware overhead, the architecture of SqueezeFlow is much simpler than SCNN, thus it is implemented with lower power.


Fig. 14.
Energy consumption normalized to the dense architecture.

Show All

We further show the energy breakdown of our accelerator and DenseArch in Fig. 15. It is clear that PU consumes the most energy in both DenseArch and SqueezeFlow. PU consumes over 50 percent of the total energy in DenseArch, which is consistent with the results reported in prior work [16]. Because of the high performance of SqueezeFlow, the energy consumption of all the components in SqueezeFlow is lower than that in DenseArch.


Fig. 15.
Energy breakdown of DenseArch and SqueezeFlow.

Show All

7.4 Scalability and Compatibility
We evaluate the scalability of the architectures from the sensitivity to the scale of PE array to study the hardware scale-out merit. Fig. 16 compares the performance of the architectures on VGG16. With the scaling up of PE array, SqueezeFlow maintains a stable high performance. Because the imap size is usually larger than PE array size, SqueezeFlow maintains a high PE utilization. Cnvlutin and Cambricon-X suffer from a slow performance degradation with the increasing of the PEs, while SCNN incurs a much more severe performance degradation. When PE array is larger than 9×9, SqueezeFlow surpasses SCNN for the performance on VGG16. The result highlights that SqueezeFlow has good scalability.

Fig. 16. - 
Speedup on VGG16 for different scales of PE array.
Fig. 16.
Speedup on VGG16 for different scales of PE array.

Show All

We also evaluate the compatibility of these architectures by testing their performance on both dense and sparse VGG16, as listed in Table 4. SqueezeFlow achieves a comparable performance with DenseArch although it is not dedicated for dense models. However, SCNN incurs 20 percent performance degradation compared to DenseArch, which reveals that SqueezeFlow has better compatibility than SCNN.

TABLE 4 Speedup on Dense and Sparse VGG16 Over DenseArch

SECTION 8Discussion
We have described the SqueezeFlow accelerator to exploit sparsity for increased efficiency. However, there are several limitations that prevent SqueezeFlow from optimal efficiency. The first limitation is that SqueezeFlow only removes the ineffective computations from zero weights, based on the observation that weight sparsity is usually higher than activation sparsity. It would be better if the accelerator can adaptively remove ineffective computations from zero weights or activations. The second limitation arises from the lack of efficient support for various convolution stride. SqueezeFlow handles this condition by introducing unnecessary computations. Although it is a common method and widely used in prior work, it would lead to better efficiency if there is dedicated support for various convolution strides, which might be a future work.

SECTION 9Conclusion
This paper first describes concise convolution rules which can smartly decomposes sparse convolutions into effective and ineffective sub-convolutions. The computations in ineffective sub-convolutions are eliminated while the effective ones can be easily mapped to existing dense CNN accelerators without intrusive PE modifications. Based on CCR, we propose SqueezeFlow accelerator architecture to exploit sparsity of CNNs. SqueezeFlow provides a tremendous performance and energy efficiency advantage over prior approaches. With a footprint of 4.80 mm2 and 536.09 mW, SqueezeFlow achieves a speedup of 2.9× on VGG16 over a comparably provisioned dense architecture. Compared with state-of-the-art sparse accelerators, SqueezeFlow improves the performance and energy efficiency by 1.8× and 1.5×, respectively.