Shrinking technology sizes of the CMOS circuits makes it possible to place more transistors on a single chip at each technology generation. On the other hand, circuits become more vulnerable to radiation effects due to lower supply and threshold voltage levels; thus, the number of transient faults in circuits tends to increase. Moreover, energy reduction techniques also negatively affect the reliability of circuits. Traditional high-level synthesis (HLS) methods usually consider only area and latency along with either energy or reliability. Especially the effect of using different voltage levels on reliability is completely ignored by previous studies. In this article, we present two new HLS methods for application-specific integrated circuit (ASIC) design under area and timing constraints with the objectives of low energy consumption and high reliability. For the mapping and scheduling steps of HLS, we propose integer linear programming (ILP) and genetic algorithm (GA)-based optimization methods. While ILP provides the optimum results, the CPU time increases exponentially with the number of application nodes. On the other hand, GA-based metaheuristic is faster and determines optimum or near-optimum results in shorter times than ILP. Additionally, we use a selective duplication method to further improve the overall reliability.
SECTION 1Introduction
Ever-increasing performance demand for the computer applications has resulted in shrinking of the technology sizes of the complementary metal-oxide-semiconductor (CMOS) circuits every 18 months over the past 50 years driven by the Moore's Law. Shrinking of the technology sizes made it possible to increase the number of transistors on chips, thus allowing the designers to embed more components in their designs than before. While the smaller transistor sizes reduce the cost of the integrated circuits as a result of having a smaller chip area, the increase in the circuit densities makes the design process more challenging. Furthermore, each technology generation also introduces new design problems that need to be tackled in digital systems. For example, when the technology sizes are reduced, circuits become more vulnerable to radiation effects due to lower supply and threshold voltage levels; therefore, the number of the transient faults in circuits increases [1]. A previous work shows how the soft error rates (SERs) of sequential circuits (SRAM and latches) and combinational logic with different sizes change with each technology generation [2]. While sequential elements still have high SER rates by keeping almost the same values, the SER of combinational logic increases dramatically. Therefore, tackling the soft error (SE) problem of combinational circuits has also become a major concern. Although combinational circuits can mask the transient errors to some extent, they cannot eliminate them completely without some extra precautions. Thus, new design methods for mitigating them before they are latched to memory elements are crucial.

While a reduced technology size makes the circuits more susceptible to transient faults, some energy reduction techniques also negatively affect their reliability. For example, when dynamic voltage scaling (DVS) is applied as an energy reduction method, a circuit consumes less energy under lower voltage levels; however, lowering the supply voltage also reduces the reliability of the circuit [3], [4]. Furthermore, when we consider the design of an application with large number of components, tackling all system requirements such as area, performance, energy consumption, and reliability becomes cumbersome. Therefore, a design automation tool is a must to ease the design process and to determine the best design in terms of the given objective function and the constraints. Generally, it is much more practical and efficient to tackle several constraints and optimization parameters at higher levels of abstraction for designing Application Specific Integrated Circuits (ASICs). High-level synthesis (HLS) process aims to integrate all system requirements on a higher level of abstraction and shields the designer from lower level design burdens [5].

Traditional HLS methods usually consider only area and latency together with either energy [6] or reliability [7]. To the best of our knowledge, there is no prior work that takes area and latency as constraints and energy and reliability as optimization parameters. Especially, the effect of different voltage levels on reliability is completely ignored by the previous studies. In this work, we aim to develop new HLS methods for ASIC design under area and timing constraints with objectives of low energy consumption and high reliability. In our work, we use different versions of the same resources in terms of area, performance, energy, and reliability. For this purpose, we implemented several adders and multiplier circuits to utilize in the design of the given application. For our optimization function with two parameters, we blend the energy and reliability values by assigning weights to each of them in order to be able to handle our multi-optimization problem. For the mapping and scheduling steps of the HLS, we use Integer Linear Programming (ILP) and Genetic Algorithm (GA)-based optimization methods. While the ILP-based method determines the optimum results, it takes too much time for some problems consisting of large number of variables. Therefore, we also propose a GA-based metaheuristic that determines optimum or near-optimum results in a reasonable amount of time.

The contributions of this paper are as follows:

We characterize a resource library with three adders and two multipliers under varying area, delay, energy, and reliability parameters. We list the same resource parameters under two voltage levels. We believe that our resource library will also be useful for future HLS studies.

We formulate the mapping and scheduling steps of our HLS design flow by means of an ILP-based method, which obtains the optimum results in short times for several applications. Although this method is not practically applicable for large-sized applications due to excessive solution times, it can be used as a base to compare the results of other optimization methods.

We present a GA-based metaheuristic method for solving the same problem. Our GA-based method obtains optimal or near-optimal results for most of the test instances in very short run-times, even for very large-sized applications.

We show that there is still a room for the reliability improvement after the mapping and scheduling steps are completed, and use a selective duplication method in this respect.

We illustrate the effectiveness of ILP and GA-based methods on several benchmarks in terms of energy and reliability by conducting rigorous experimental analysis.

The rest of the paper is organized as follows. Related work is presented in the next section. We explain SEs, voltage islands (VI), and effects of using different voltage levels on SEs in Section 3. In Section 4, we introduce our library characterization and the problem definition. We present our ILP-based method in Section 5, while we explain GA-based method and duplication-based post-processing in Section 6. We illustrate the effectiveness of the proposed methods by discussing the experimental results in Section 7. Finally, we conclude this paper in Section 8.

SECTION 2Related Work
There have been several HLS-related studies in the literature [8]. Earlier publications usually focus on latency and area as constraints and/or objective functions [5]. In this study, we incorporate energy and reliability metrics into the HLS process unlike the previous studies that only focus on one of these metrics along with area and latency. In the following subsections, the related studies are reviewed according to their field of concern.

2.1 Reliability-Aware HLS
Reliability was treated as a first-class citizen in a very old work under the fault-tolerance criteria for HLS designs [9]. This work aimed to design circuits under area and performance constraints to maximize the fault-tolerance by adding extra duplicated resources. Some other studies took advantage of the fact that the reliability of different implementations of the same function may be different due to their internal logic and masking capabilities. In addition, their area and latency values are also different. Optimization of a circuit by using different resources, especially reliability-aware resource allocation and binding in HLS, is known as an NP-hard problem, therefore a heuristic method as well as an ILP-based HLS method were proposed in [10] and [11], respectively. There have also been metaheuristic attempts for optimizing the reliability using different versions of a particular resource [12]. Some prior studies also presented HLS methods to design fault-tolerant data-paths in case of multi-cycle transient faults [13]. Authors of [14] presented a simulation-based method for combinational circuit synthesis considering soft errors. In [15], authors first perform the vulnerability analysis on control and data-path of the circuits; then, they propose ILP-based and heuristic methods to solve the reliability optimization problem under area and latency constraints. There are also metaheuristic attempts to solve the same problem such as the GA-based approach presented in [7]. The aforementioned optimization methods obtain very promising reliability results; however, none of them incorporates the energy consumption parameter into the HLS process. Considering the energy hungry applications of today, the energy parameter must be prioritized, which is our aim in this study.

2.2 Energy-Aware HLS
Dynamic voltage scaling has been the most commonly used energy consumption minimization method since it was introduced by [16]. Since the dynamic energy consumption decreases proportionally with the square of the voltage level, many commercial CPUs are implemented with this in mind, and new scheduling methods for varying voltage level assignments have been proposed for these new architectures [17], [18]. However, the target platform for these studies are either homogeneous or heterogeneous multiprocessor systems unlike our ASIC design platform. There are also prior studies in the context of HLS focusing on energy or power consumption [19], [20], [21], [22]. [19] presents a novel scheduling algorithm for minimizing energy, while [20] and [21] propose new methods to reduce the power consumption of circuits. There are even game theoretical scheduling algorithms using DVS for HLS [22]. The interested readers can find several methods about low power HLS design for nanoscale CMOS circuits in [23].

2.3 Energy- and Reliability-Aware Design
There has been some prior research that focused on both energy and reliability simultaneously on multiprocessor architectures [24], [25]. Some prior studies also explore energy and reliability trade-off in near-threshold voltage (NTV) computing for multiprocessors [26], [27]. However, none of the existing studies incorporate these metrics in the HLS steps. Since designers are obliged by the given processors and their target architectures which are fixed, it is not easy to transform multiprocessor-based design methods into ASIC designs. Additionally, ASIC design has another dimension in design space exploration, which is the area. To the best of our knowledge, there has not been any previous research in the HLS field that considered area, performance, reliability, and energy all together in a single study. Most of the prior research discussed the methods of increasing the system reliability under the area and latency constraints without considering the energy consumption of a system, or they suggested approaches to minimize the energy consumption of a system ignoring the effect of those approaches on the reliability.

SECTION 3Preliminaries
In the following subsections, we first discuss soft errors and reliability in digital systems. We then explain the concept of voltage islands, which is used to minimize the energy consumption. We finally present the effects of using different voltage levels on reliability of the circuits.

3.1 Soft Errors and Reliability
Function values in digital systems are generated as a result of switching in transistors. These physical elements can be affected by a number of external factors, which can cause undesirable switching situations that may lead to wrong results. If these non-persistent errors cause the data stored in memory to be erroneous even for a short period of time, all operations using that data will yield erroneous results until the data is updated. These errors in digital systems are called transient errors or soft errors (SEs).

A soft error is a signal fluctuation or an unexpected bit flip in semiconductor fabrics, which may occur due to radiation, alpha particles, or high-energy cosmic rays in the environment of the device containing the digital system. These errors generally do not corrupt the device; however, they may result in malfunctions. They usually occur when the energy accumulated in a transistor (Q) exceeds the critical energy (Qcritical). The threshold and supply voltage levels of transistors decrease in parallel to their sizes. Since voltage levels and Qcritical values are directly proportional [28], smaller Qcritical values can cause a bit flip at each technology generation. Fig. 1 shows the occurrence of transient errors in the silicon view and the transistor view.


Fig. 1.
Occurrence of SEs: silicon view (left) and transistor view (right) (adapted from [10]).

Show All

As the technology size decreases and the chip circuit densities increase, SERs increase significantly, particularly in combinational circuits. This increase in SERs negatively affects the reliability of a running system during its operation. Hence, it has become inevitable to consider the effects of transient errors during the design process.

The reliability of a system can be calculated with the formula given in Equation (1), where λ is SER, while t is the running time of the system.
R(t)=e−λt.(1)
View Source

From the equation it is evident that the higher the SER value the lower the reliability. One way to improve the reliability of a system is to back up its components (i.e., to create a replica). If two different versions of the system produce two different outputs, the result is incorrect. In such case, the system can be restarted or, alternatively, checkpoints can be added to the system to avoid the necessity for a complete restart. When a replica of a thread is created, the increased reliability value is calculated with the formula given in Equation (2), where Rs represents the total reliability after duplication, while Ri and Ri′ represent the reliability values of a system component and its replica, respectively.
Rs=Ri+Ri′−RiRi′.(2)
View SourceThere are a lot of studies that focus on using replicas or multiple spare circuit elements to increase the reliability of the systems with multiple circuit elements. These backups are usually selected to be the same as the original ones. However, circuit elements implemented in a different manner exhibit different behavior against transient errors. For instance, an adder circuit with a larger but faster operating area may have lower reliability values than a smaller but slower one. A previous study showed that the SER values of circuit elements implemented in different ways can also be different, and that by taking this into consideration during the design process the reliability of a system can be affected [10]. Nonetheless, in the design of integrated circuits, the energy consumption of a circuit is also an important criterion along with reliability, area, and performance constraints. We also incorporate duplication in our final design to further increase its reliability without increasing its area, which will be explained in Section 6.4.

3.2 Voltage Islands
Performance (runtime of an application) is the most important requirement that needs to be achieved for ASICs. Furthermore, while the energy consumption plays a significant role, especially in battery-powered systems, the reliability comes to the fore in critical applications such as rocket control circuits, satellites, and nuclear power plant control circuits. Therefore, when designing such systems, it should be ensured that they meet the given time and energy constraints, while maximizing the reliability. In this paper, we adopt the voltage islands method [29] in order to meet these requirements.

VI uses multi-supply voltages for different parts of the design, where each part is named as an island. It is a popular energy reduction method due to the fact that when digital circuits operate under a low voltage, their energy consumption decreases in proportion to the square of the voltage, while the worst-case execution time (WCET) only increases proportionally to the decrease in voltage. If the operation time requirement for a part of the digital circuit is sufficient, this part may be operated at a lower voltage to reduce the overall energy consumption. The effect of using different voltage levels on a circuit's energy and performance can be explained by the power consumption of a CMOS circuit. The dynamic power consumption of CMOS circuits is expressed in the Equation (3), where P is the power consumption, CL is the load capacitance, Ns is the number of switching cycles per second, v is the source voltage, and f is the operating frequency of the circuit.
P=CLNsv2f.(3)
View Source

If the source voltage of the circuit is reduced, the execution time of the circuit will also change proportionally according to the Equation (4), where k and α are constants varying based on the applied technology dimensions, and vt is the threshold voltage value.
t=CLv/k(v−vt)α.(4)
View Source

If the WCET of a digital circuit under high voltage (vh) is known, the WCET value under low voltage (vl) can be calculated from the Equation (5) derived from the Equation (4).
tvl=tvh(vlvh)(vh−vtvl−vt)2.(5)
View Source

Similarly, if the high-voltage energy consumption (Evh) of the circuit is known, the low-voltage energy consumption (Evl) can be calculated using the Equation (6).
Evl=Evh(vlvh)2.(6)
View Source

Although some previous studies have adopted VI as the main energy minimization method, they have only considered the negative effects of using multi-supply voltages on performance, while neglecting its negative impact on the reliability [30]. One of the most unique contributions of this study is the analysis of the effect of different voltage levels on the reliability of a system.

3.3 Effects of Multi-Supply Voltages on Reliability
Using multi-supply voltages is a very efficient technique when it comes to reducing the energy consumption. Nonetheless, when a digital circuit operates at a low voltage, it becomes more vulnerable to soft errors since Qcritical values of the transistors can be more easily exceeded, and as a result, the total reliability of the system decreases. In other words, lowering the operating voltage of a digital circuit (and therefore its frequency as well) will lead to both an increase in its execution time and a decrease in its reliability.

The fault rate of a system at frequency f (voltage v) is expressed by means of the Equation (7), where λ refers to the SER, and λ0 refers to the average error rate at frequency f.
λ(f)=λ0g(f).(7)
View SourceRight-click on figure for MathML and additional features.

Let the operating frequency at the highest operating voltage (vmax) be fmax=1. Transient errors generally occur when the critical voltage of the circuit is reached. This critical voltage is proportional to the system voltage. That is, when the system voltage is reduced, the critical threshold voltage will also decrease. Thus, at a low voltage, the circuit will be more sensitive to soft errors. Error rates according to the different voltage levels can be calculated by the Equation (8).
λ(f)=λ010d(1−f)(1−fmin).(8)
View Source

Here, the maximum error rate is expressed as λ(fmax)=λ010d, which is the minimum operating frequency. d>0 is a constant. The higher the value of d the higher the error rate in the circuit (i.e., the lower the value of d the more resistant the circuit to faults).

Using the Equation (8), the new reliability values can be calculated according to the changing energy levels and the execution time of a digital circuit. In this study, we consider the effect of multi-supply voltages on reliability, the energy consumption, and the latency of digital circuits.

SECTION 4Library Characterization and Problem Definition
4.1 Library Characterization
A function can be implemented in multiple ways in the hardware using different design methods, which produce several different versions of the same function. For example, an adder can be implemented as a ripple-carry adder, carry-lookahead adder, prefix adder etc. [31]. Different implementations of the same function may have different area, latency, and energy consumption values. Additionally, they can exhibit diverse behavior in terms of the error resilience when a soft error hits a part of the circuit. Some circuits can tolerate faults better than others since their transistor layouts and logic functions are different. This is due to the fact that each combinational circuit has fault masking capabilities to some extent. Therefore, different versions of the same function may have different reliability values in addition to the area, latency, and energy consumption. Combinational circuits can mask the effects of SEs to some degree, since their results are functions of logic values. Therefore, their reliability values may differ based on several factors such as the location of SE, the logic function and the duration of SE. For example, some prior studies propose value-aware reliability analysis [32] and soft error injection evaluation [33] to validate the reliability values of circuits.

The HLS methods proposed in this paper utilize different versions of the same resource in an attempt to find the optimum energy and reliability values under the given latency and area constraints for a particular application. In this respect, a resource library is characterized to be employed in this study. Note that our methods can accept any resource library that is built with different characterization techniques as long as the necessary parameters are provided.

We implemented three adders and two multipliers in Verilog and synthesized them using Cadence Genus synthesis tool [34]. We obtained the area, latency, and energy values for each resource under 1.2V supply and 0.5V threshold voltage levels. We then scaled each parameter. We used the reliability values estimated in [7] for these resources. Finally, we obtained the new latency, energy, and reliability values for 1.0V supply and 0.5V threshold voltage levels using the Equations (6), (5), and (8), respectively. The details of the resource library after the characterization are given in Table 1. In this table, A is the area of the resource measured in mm2. Lh and Ll represent the latency values of the corresponding resources under high and low voltage respectively, and they are measured in time steps. Similarly, Rh and Rl represent the reliability values, whereas Eh and El represent the energy consumption under high and low voltage measured in nanojoules (nJ), respectively.

TABLE 1 Resource Library

4.2 Problem Definition
The aim of this study is to propose HLS methods for the resource allocation and scheduling steps to maximize the reliability and minimize the energy consumption of the final design under the given latency and area constraints. HLS is an automated design process that converts a given behavioral description of an application into a synthesized hardware in terms an Register Transfer Level (RTL) netlist. A behavioral description can be written in a high-level language and it is converted to an internal data-flow representation in the form of directed acyclic graph (DAG) before the HLS process starts. Memory-oriented parts of the design have different characteristics than data-flow intensive operations [35]. Therefore, we restrict ourselves to only data-flow intensive operations in this study and we leave tackling all parts of the design as a future work.

In Fig. 2, we show the behavioral model for the differential equation solver, its data flow representation with the precedence constraints, and the final DAG adopted from [5], where each node represents an operation. The first and last dummy nodes, namely source and sink nodes, are added to the DAG as reference points to ease the implementation of the scheduling algorithms.


Fig. 2.
(a) An example design specification, (b) Data flow representation with precedence constraints, and (c) Directed Acyclic Graph (DAG) of the design specification. The symbols inside the nodes represent the operation type.

Show All

The goal of resource allocation in HLS is to assign a resource from the resource library to each node of the DAG while taking the area constraint into consideration, whereas a scheduling algorithm assigns the start times for each node of the DAG under latency constraints. The objective here is to minimize the total energy consumption and maximize the reliability of the final design. We will formally define the problem at hand in the next section.

There are several challenges to this problem, which make its optimization a very cumbersome task. First of all, we have a variety of possible resources (i.e., functional units) with different reliability, area, latency, and energy values that need to be taken into consideration simultaneously in the process of resource allocation and scheduling. Additionally, scheduling must take the task dependencies into account, so that dependent tasks will execute in the required order, and no precedence constraint is violated. Finally, different voltage levels assigned to different resources introduces further complication into the model. We propose ILP and GA-based methods in order to show the accuracy and execution times of different optimization methods. We elaborate on them in the following sections.

SECTION 5ILP Formulation
In this section, we present our ILP formulation of the problem, which maximizes the total reliability while minimizing the total energy consumption. The notations used in the ILP formulation of the problem are defined in Table 2.

TABLE 2 ILP Notations

ζi,j refers to the compatibility of Ti with Rj (e.g., an addition operation can only be assigned to an adder resource) and is formulated in Equation (9).
ζi,j={10If TTypei=RTypejotherwise.(9)
View Source

Assignedi,j,v is a Boolean variable which specifies if Rj is assigned to Ti under Vv (see Equation (10)).
Assignedi,j,v=⎧⎩⎨⎪⎪10if Ti is assigned to Rj under Vvotherwise.(10)
View Source

Only one resource should be assigned to each task under a single voltage level, while taking the compatibility into consideration. This is formulated in Equation (11).
While ζi,j=1∀i∈T:∑j∈R,v∈VAssignedi,j,v=1.(11)
View Source

Starti,s is a Boolean variable which specifies if Ti started at Csteps (see Equation (12)).
Starti,s={10if Ti started at Cstepsotherwise.(12)
View Source

A task may start at only one control step (see Equation (13)).
∀i∈T:∑s∈CstepsStarti,s=1.(13)
View Source

The delay of a task depends on the latency of the resource assigned to it and its voltage level (see Equation (14)).
∀i∈T:δi=∑r∈R,v∈VLr,v⋅Assignedi,r,v.(14)
View Source

For dependent tasks, precedence constraints must be considered. Inequality (15) ensures that the start time of a task that depends on a completion of another task must be greater than the end time of the precedent task.
∀(i,j)∈T: If PREC(i,j)=1∑s∈CstepsStartj,s⋅s≥∑s∈CstepsStarti,s⋅s+δi.(15)
View Source

The reliability of a task depends on the reliability of its assigned resource under the applied voltage level. This is formulated in Equation (16).
∀i∈T:ρi=∑r∈R,v∈VRelr,v⋅Assignedi,r,v,(16)
View Source

κi,s,r,v is a Boolean variable which specifies if Ti started at Csteps and if Rr has been assigned to it under Vv (see Equation (17)).
κi,s,r,v=⎧⎩⎨⎪⎪10If Ti started at Csteps and Rr is assignedto it under Vvotherwise.(17)
View Source

Each task can only start at one control step and only one resource can be assigned to it under a single voltage level. We ensure this using Equations (18) and (19).
∀i∈T:∑r∈R,s∈Csteps,v∈Vκi,s,r,v=1(18)
View Source
∀(i∈T,r∈R,s∈Csteps,v∈V):κi,s,r,v≥Assignedi,r,v+Starti,s−1.(19)
View Source

The total amount of energy consumed by a task depends on the energy consumption of its assigned resource under the applied voltage. This is formulated in Equation (20).
∀i∈T:ϵi=∑r∈R,v∈VEr,v⋅Assignedi,r,v.(20)
View SourceRight-click on figure for MathML and additional features.

To calculate the number of instances of each available resource used in the overall design, we have to determine the resources that are assigned to tasks starting at that control step. We only check the start times for each task at each control step since we assume that pipelined resources will be used in the design. NumRr,s,v represents the total number of instances of Rj at Csteps under Vv and is formulated in Equation (21).
∀(r∈R,s∈Csteps,v∈V):NumRr,s,v=∑i∈Tasksκi,s,r,v.(21)
View Source

Υr,v represents the total number of instances of each available resource under each voltage level that needs to be used in the overall circuit design, and it is the maximum of all NumRr,s,v (see Equation (22)).
∀(r∈R,v∈V):Υr,v=maxs∈CstepsNumRr,s,v.(22)
View Source

5.1 Constraints
The total area should not exceed the given area constraint. This is formulated in Inequality (23).
∑r∈R,v∈VΥr,v⋅Ar≤Λ.(23)
View Source

Latency constraint will be met if the start time of the last sink task (denoted as StartN and defined in Equation (24)) is less than or equal to the given maximum allowed latency. This is formulated in Inequality (25).
StartN=∑s∈CstepsStartN,s⋅s(24)
View SourceRight-click on figure for MathML and additional features.
StartN≤λ.(25)
View SourceRight-click on figure for MathML and additional features.

5.2 Objective Functions
The goal is to maximize the overall reliability of the circuit (Rtotal) while minimizing its total energy consumption (Etotal), formulated as the objective functions given by (26) and (27), respectively.
MaximizeRtotal=∑i∈Tasksρi(26)
View Source
MinimizeEtotal=∑i∈Tasksϵi.(27)
View Source

This bi-objective problem is formulated as a single objective function given in (28) by employing the scalarization technique in which we combine the weighted sum of energy and reliability values. The parameter α serves for the purpose of assigning weight to both reliability maximization and energy minimization. That is, through choosing different α values, we can prioritize either objective function to a certain degree, or assign an equal weight to both (by taking α=0.5).
Minimizeobj=α⋅(1−Rnorm)+(1−α)⋅(Enorm).(28)
View Source

Rnorm and Enorm are the values of the total reliability and the total energy consumption normalized to the range [0,1], and calculated by the Equations (29) and (30), respectively.
Rnorm=Rtotal−RminRmax−Rmin(29)
View Source
Enorm=Etotal−EminEmax−Emin.(30)
View Source

Rmin and Rmax are the minimum and maximum values the reliability of a given circuit can have. The minimum (or maximum) achievable reliability of a circuit can be calculated by assigning the least (or most) reliable resources in the resource library to every task. Similarly, we can calculate Emin and Emax, which are the minimum and maximum values of the energy amount a circuit can consume.

Although the proposed ILP method has a complex running time and takes too long to come up with a solution for complex benchmarks with a large number of tasks, it gives the optimal results for the problem at hand when α is either 0 or 1. Since the solutions obtained with ILP are generated by searching the entire space, they will be used for evaluating the quality of the GA-based method in terms of the CPU times and accuracy.

SECTION 6GA-Based Method
In the recent decades, genetic algorithms have evolved as an optimization technique that is more practical, time saving and efficient compared to other traditional optimization techniques. GAs are categorized as metaheuristic methods that obtain optimum or near-optimum solutions in a reasonable amount of time. They search the whole solution space randomly via the genetic operators; hence, they reduce the chance of getting trapped in local minima. In our work, we propose a GA-based method for concurrently finding the most reliable and the least energy consuming solution for our HLS problem. The proposed method follows three main stages of GAs: generation of the population, application of the genetic operators, and selection of the chromosomes based on the fitness values. These stages are explained in more detail in the following subsections.

6.1 Population Generation
The very first step in a genetic algorithm is to generate an initial population of solutions to the problem. Individuals of the population, also known as chromosomes, could be initialized either totally at random or heuristically to a certain extent. In our study, a part of the initial population is predetermined - this portion of individuals is 6 percent of the total population, whereas 94 percent of the population is created randomly. This approach will seed the starting population with a few good solutions that could help the algorithm find the best solution, as well as construct the rest with the random ones.

When we generate the 94 percent of the seed population, we assign resources and their voltage levels randomly. We choose the population size as 100, out of which six predetermined individuals are those with the highest and the lowest reliability, area, and latency values, respectively. We adopt the chromosome representation to symbolize a solution. Fig. 3 shows the chromosome representation of the DES graph given in Fig. 2, with the chosen voltage level under each resource (1 represents high while 0 represents the low voltage level).


Fig. 3.
A chromosome symbolization of a solution for the DES benchmark after resource assignment, with its Area (A), Latency (L), Reliability (R), and Energy (E) values not calculated yet.

Show All

A chromosome is an array of genes, where each gene's value represents its randomly assigned resource from our resource library in Table 1. The Area (A), Latency (L), Reliability (R), and Energy (E) of this solution are not calculated before the scheduling and resource assignment stages. This is denoted by question marks in their related cells.

6.2 Scheduling and Binding
The second stage of our GA is the calculation of the fitness value of each solution, with the goal of keeping the best individuals for the next generation. Consequently, the proposed GA first applies the HLS step to all solutions, and then calculates their fitness values (i.e., the reliability of each solution). The well-known List Scheduling (LS) [5] is performed in two steps: first, the mobility of each node/gene in the chromosome is determined, and then the resource binding is carried out. The mobility (m) of a node (i) is the time difference between the latest and earliest time steps that a node can be scheduled. The earliest time step (t(i)ASAP) and the latest time step (t(i)ALAP) are determined using the As Soon As Possible (ASAP) and As Late As Possible (ALAP) scheduling algorithms based on the resource assignment of the chromosome. We then use Equation (31) to calculate the mobility of each node.
m(i)=t(i)ALAP−t(i)ASAP.(31)
View SourceThe LS algorithm does not change the nodes with zero mobility (critical path nodes), even if more than one node on different critical paths need to use the same resource at the same time, which results in higher area values. We adjusted the LS results to allow the critical path nodes to share resources with other nodes; hence, the total area is decreased. The change was made by adding only one extra time step to the critical path. This approach slightly increases the latency, but at the same time it significantly reduces the area.

ASAP scheduling identifies the minimum latency that can be obtained with the assigned resources. In ASAP, every node is scheduled at the earliest time step possible. The overall minimum latency is the time step at which the last node of the chromosome is scheduled. ALAP scheduling does the opposite of ASAP as it returns the starting time steps for every node in the solution with the maximum possible latency. This is done by comparing the latency constraint with the latency returned from ASAP, and then using the higher of both.

The ASAP and ALAP scheduling for the chromosome given by Fig. 3 are shown in Fig. 4. In this figure, each dashed horizontal line shows the starting steps of some nodes. We do not draw each step in our scheduling figures to prevent overcrowding the illustration; instead, we only show the time steps if there is a node starting its execution in these time steps. To explain the GA operators on our running example, we assume the area (A¯) and latency (L¯) constraints as 30 units and 23 time-steps, respectively. The minimum latency returned from ASAP is 31, which is calculated by subtracting 1 from the starting step of the sink node (i.e., Lmin=t(n)ASAP−1). Then, after applying the ALAP scheduling under L=31 and determining the ALAP starting time step of each node, the algorithm calculates the mobility of each node as shown in Table 3.

TABLE 3 Node Mobilities for the Schedules in Fig. 4


Fig. 4.
ASAP and ALAP scheduling for the chromosome representation given in Fig. 3.

Show All

After calculating the mobility of the nodes, our algorithm applies the modified list scheduling, and subsequently returns A, L, R, and E values of the solution as shown in Fig. 5.


Fig. 5.
Final scheduling of the chromosome in Fig. 3.

Show All

6.3 Genetic Operators
The third stage of a genetic algorithm is to apply the genetic operators. We use crossover, mutation, and selection operators, which are the three most common ones.

6.3.1 Crossover
In the crossover operation, we select two chromosomes randomly from the population as the parents. We then swap a part of each parent chromosome to create two offsprings in an attempt to get better parts of each parent inherited by their children. The most commonly used crossover types are one-point, two-point, and uniform crossover. In order to identify the most suitable type for our problem, we applied all three of them on a study similar to ours mentioned in [7], and found that uniform crossover gives the best results for such problems. Uniform crossover swaps genes between both parents and makes two new children chromosomes. It assigns a random number u to every gene, where 0≤u≤1, and compares that number with the swapping probability ps. We chose ps to be 0.5 in order to give the equivalent swapping chance for all genes.

In Fig. 6, we demonstrate an example of applying the crossover operator on the solution in Fig. 3. This chromosome is the first parent for the crossover and the second parent is randomly chosen by the algorithm. After applying crossover on parents, two new children chromosomes are produced as demonstrated. It is evident that although the first child meets both latency and area constraints with better reliability value than both parents, the second child does not.


Fig. 6.
An example of uniform crossover.

Show All

6.3.2 Mutation
Mutation is an important and effective genetic operator to converge to the optimum solution. It modifies the randomly or heuristically selected genes to obtain a new chromosome. The goal of mutation is to diversify the population so that the chance of escaping from the local minima increases. However, if the mutation ratio becomes very high, then our GA-based search algorithm behaves like a random search. Thus, we set the mutation ratio to 10 percent of the total population in our method.

In order to find the most appropriate mutation operator for the problem at hand, we first developed a heuristic algorithm for the gene mutation to meet the area and latency constraints in cases where they were not satisfied. However, the heuristic eliminated several design alternatives, which resulted the metaheuristic in often being stuck in local minima. Therefore, we randomly selected the genes to assign new resources. We also assigned their voltage levels randomly.

While we are applying both crossover and mutation, we make sure the shared resources use the same voltage level to construct voltage islands [29]. Although it is possible to switch from one voltage level to another, this process also induces extra latency and power consumption. Instead, we simply place the resources running on low voltage level to one power island and the others to the high voltage island.

In Fig. 8, we illustrate how the mutation operation works. In this example, we show a chromosome to which two mutation operations are applied. Nodes 3 and 10 are randomly selected. Then, new resources are assigned to them also randomly. As a result, the total area and latency values are decreased.


Fig. 7.
The scheduling of first child of the crossover given in Fig. 6.

Show All


Fig. 8.
The mutation operator applied to the chromosome in Fig. 3.

Show All

6.3.3 Selection
Our algorithm selects the chromosomes randomly to apply crossover and mutation. After applying crossover and mutation, the total population doubles because of the newly added individuals. While 90 percent of the new population comes from the crossover, the remaining 10 percent is formed by the mutation. Since the total population size must be kept fixed, the algorithm splits the overall population into two groups based on the ordering of the fitness values. As clarified in Section 5.2, the objective function given in Equation (28) attempts to maximize the reliability and minimize the energy consumption for a single chromosome. We use this objective function in our fitness calculation. We then select half of the chromosomes from the top and half of the chromosomes from the bottom of the sorted population. This selection method raises the chance of finding the global minima since the diversity of solutions is increased.

Our three main steps are iterated for fixed number of times. We selected our iteration count experimentally as 1,000. We finally return the chromosome with the best fitness value as our solution.

6.4 Duplication-Based Post-Processing
After our GA-based method returns the final scheduled and bound solution, we take that solution with its latency, area, reliability and energy as an input to a subsequent process to enhance its reliability without violating the constraints. We employ a method similar to the duplication algorithm proposed in [7], which duplicates the nodes as much as it can using simple heuristic rules: it tests the potential of each node to have a duplicate resource from other resources that are previously allocated in the design. The nominated nodes are not on the critical path, so the latency of the solution is maintained. It also checks that the nominated resource is not scheduled for use at the same time step, so the area will not be increased either. For cases of multi-duplicable candidates, the precedence is given to the nodes with lower reliability values. The difference in our approach is that we also incorporate multi-supply voltages in the duplication process. That is, when we are selecting a resource for duplication we give priority to the one with lower voltage level. To calculate the reliability of the duplicated version, we use Equation (2).

The duplication process applied to the solution given in Fig. 7 is illustrated in Fig. 9. The shaded nodes are the added duplicate nodes. Furthermore, checkers are added at the end of each node and its duplicate to ascertain the similarity of the results. From Fig. 9, it is evident that the duplication process increased the total system reliability from 0.896 to 0.938 (4.2 percent higher). The algorithm simultaneously ensures that the duplicates do not add to the overall area or latency of the solution. Nevertheless, the overall energy consumption is increased. However, we try to decrease this energy increase as much as possible. For example, in Fig. 9, we have two voltage level options for resource M1 when we duplicate node 8. Although the version that uses low voltage has longer latency than its high voltage counterpart, we select M1 with low voltage since it does not violate the latency constraint and results in smaller energy increase.

Fig. 9. - 
The scheduling after the duplication of the solution from Fig. 7.
Fig. 9.
The scheduling after the duplication of the solution from Fig. 7.

Show All

SECTION 7Experimental Results
In this section, the effectiveness of our proposed methods is investigated through several sets of experiments. We selected four most commonly used benchmarks in literature: Differential Equation Solver (DES), Finite Impulse Response (FIR) filter, Auto-Regressive (AR) filter, and Elliptic Wave Filter (EWF). The benchmark features (the number of nodes and edges in their respective data flow graphs, as well as addition and multiplication operations) are briefly summarized in Table 4. More detailed specifications and data flow graphs for the benchmarks we used can be found in [10] and [11]. Due to the page limitations, we present the results of only two selected benchmarks for each set of experiments.

TABLE 4 Summary of Benchmarks
Table 4- 
Summary of Benchmarks
The resource library we used in our experiments is given in Table 1, where we list area, latency, reliability, and energy consumption values of each resource under low and high voltage levels. We measure latency in time steps (e.g., clock cycles), area in mm2, and energy consumption in nanojoules. It is worth noting that our proposed methods can be used with any resource library that has clearly defined area, latency, reliability, and energy consumption values for each resource under different voltage levels.

The experiments of the proposed methods which attempt to solve the bi-objective problem formulated in Equation (28) were performed using four benchmarks from Table 4 for varying area and latency constraints, as well as different α values.

The minimum latency constraints for each benchmark were obtained from ASAP scheduling algorithm by using the fastest resource for each type of the operations. Once the minimum circuit delay is obtained, the latency constraint can be increased gradually to test for less delay-sensitive cases, which may allow utilization of slower but more reliable resources in the design, and/or allow operation of certain resources at the low voltage level, as to reduce the overall energy consumption of the circuit. The minimum area constraints, on the other hand, were obtained by assigning a single resource with the smallest area for each different type of operation within a benchmark. Testing for different area constraints allows the algorithms to find solutions with lower latency and higher reliability values.

Since the objective function combines the weighted sum of energy and reliability values, by choosing different values of the parameter α (which assigns weight to reliability maximization and energy minimization) we can prioritize either objective function to a certain degree, or assign an equal weight to both. The experiments with varying area and latency constraints were performed for α values of 0.0, 0.5, and 1.0. The α value of 0.0 means that we give the maximum priority to the minimization of energy consumption, while disregarding the reliability of the circuit altogether. Similarly, the α value of 1.0 means that we give the maximum priority to maximizing reliability, without taking energy consumption into consideration at all. Finally, by taking α=0.5, we assign an equal weight to both objective functions, making the problem bi-objective in nature. Furthermore, in Section 7.3, we present more detailed results that demonstrate how reliability and energy consumption values change when the parameter α varies in steps of 0.1 (assigning varying weights to either reliability maximization or energy minimization).

The experiments were performed on a computer with the following configurations: Intel Core(TM)2 Duo CPU E8500, at 3.16 GHz, with 2 cores, 2 logical processors, and a total physical memory of 5,823 MB.

7.1 Comparison of ILP and GA Methods
In the first set of experiments, we compare ILP and GA-based methods on our benchmarks in terms of energy and reliability. Tables 5 and 6 show the reliability and energy consumption results for FIR and EWF benchmarks, respectively. The first column in this table specifies the value of the α. The second column indicates the latency (L) and area (A) constraints used in that particular test instance. The third and fourth columns give the reliability values from the solutions obtained by ILP and GA-based methods, respectively. Similarly, the sixth and seventh columns give the energy consumption values. Delta (Δ) represents the percentage change of the GA result relative to the ILP result, and the percentage changes in reliability and energy consumption results are given in the fifth and eighth columns respectively. The reliability Δ is calculated according to the percentage change increase formula, since a higher reliability value means a better solution, whereas the energy consumption Δ is calculated according to the percentage change decrease formula, as lower energy consumption is a more desirable outcome. Note that a negative Δ value means ILP performs better than GA, and vice versa.

TABLE 5 Results of FIR Benchmark
Table 5- 
Results of FIR Benchmark
TABLE 6 Results of EWF Benchmark
Table 6- 
Results of EWF Benchmark
For α values of 1.0 and 0.0 the GA-based method obtains optimum or near-optimum results in most of the cases. In these two cases, the solver is trying to either maximize the total reliability (for α=1.0) or minimize the overall energy consumption (for α=0.0). However, for the case of α=0.5, where we give an equal weight to both objective functions, we observe that GA may produce varying results for some benchmarks. In particular, from the average Δ values, we notice that GA-based method obtains solutions with relatively better reliability values compared to the ILP solutions for some benchmarks, but with a trade-off in higher percentage change in overall energy consumption. The most notable difference can be observed from the results of the AR (not reported here due to the page limit) and EWF benchmarks, which could indicate that this disparity in the obtained values grows as the number of benchmark nodes increase. Thus, we can conclude that GA-based method may produce unforeseeable results when trying to solve the bi-objective problem of giving an equal weight to both maximizing the total reliability or minimizing the overall energy consumption of a given circuit. This could be explained with the randomness of GAs in general.

7.2 Execution Time Analysis
While the ILP-based method determines the optimum results, it takes too much time for problems with a large number of variables. Scheduling and resource binding of DAGs under given constraints are known as NP-hard problems. Thus, the execution time of an ILP-based solution grows exponentially as the number of nodes increases, and becomes computationally impractical for more complex circuits. Therefore, the proposed GA-based method provides a faster solution with near-optimum results. Furthermore, increasing the number of voltage levels also results in even higher execution times, as the solution search space increases. Let n be the number of the nodes in a data-flow graph, r the number of resources in the resource library, and v the number of voltage levels. Since every resource has different reliability, latency, and energy consumption properties under different voltage levels, we can treat the instances of the same resource under different voltage levels as v different resources of the same type. Thus, we can assume that we have r×v resources in total. Consequently, in order to search through the whole solution space, we would have to inspect (rv)n possible resource allocations (as any resource can be used any number of times).

Fig. 10 demonstrates the comparison between the average execution times of ILP and GA methods for varying number of benchmark nodes under two voltage levels. As it is evident from the figure, while the execution time of the ILP method starts growing exponentially for benchmarks that have more than 20 nodes, the average execution time of the GA-based method remains to be around one second for any benchmark size, making it a practical method of choice for complex circuits with a large number of nodes.

Fig. 10. - 
Average execution times of ILP and GA methods for varying number of benchmark nodes.
Fig. 10.
Average execution times of ILP and GA methods for varying number of benchmark nodes.

Show All

7.3 Effect of Multiple Voltage Levels on Reliability
In the second set of experiments, we test the effect of using different voltage levels on reliability by changing α values in order to give different weights to reliability and energy. Figs. 11 and 12 demonstrate the changes in reliability and energy values for different values of α, for FIR and EWF benchmarks, respectively. The figures present more detailed results that in turn demonstrate how reliability and energy consumption values change when the parameter α varies in steps of 0.1 (assigning varying weights to either reliability maximization or energy minimization) for specific area (A) and latency (L) constraints. As α increases, we give more and more weight to maximizing the circuit reliability at the expense of higher energy consumption, and vice versa. Hence, with growing α, both reliability and energy consumption values should increase. From the (a) charts in the given figures that demonstrate the change in reliability values, we observe that the proposed GA-based method obtains the optimum or near-optimum results for all test cases. Nontheless, the GA solutions appear to result in a slightly higher overall energy consumption.

Fig. 11. - 
Changes over different $\alpha$α values for FIR benchmark ($A= 20, L=40$A=20,L=40).
Fig. 11.
Changes over different α values for FIR benchmark (A=20,L=40).

Show All

Fig. 12. - 
Changes over different $\alpha$α values for EWF benchmark ($A= 30, L=40$A=30,L=40).
Fig. 12.
Changes over different α values for EWF benchmark (A=30,L=40).

Show All

Overall, we observe that lowering the voltage level negatively affects the final reliability of a circuit. Furthermore, giving more than 50 percent weight to minimizing energy consumption generally results in unacceptably low reliability values of the final circuit design. Therefore, one should consider balancing the gain and loss in terms of the energy and reliability. For example, when we aim to optimize reliability we can define energy as a constraint or vice versa. Both our ILP and GA-based methods can easily be modified for assigning either reliability or energy as constraints since they offer a very flexible infrastructure for a user to adjust the area, latency, reliability, and energy parameters of the design at the same time.

7.4 Effects of Duplication
In the last set of experiments that shows the effect of duplication, we report the results for DES and AR benchmarks. Tables 7 and 8 show the reliability and energy consumption results of the duplicated-GA for DES and AR, respectively, for α value of 1.0. The symbols RΔ (reliability change) and EΔ (energy change) represent the percentage change of the GA, ILP, and Fully Duplicated (FD) results. While determining the FD version of the design, we assume that all the resources of the design obtained by the GA-based method are duplicated by the same resource. Therefore, the area and energy consumption of the FD version are 100 percent increased. We compare our duplication method with FD results to see if we can achieve similar reliability improvement with less area and less energy increase. In the given tables, the first column defines the latency and area constraints for which the tests were performed. The second and third columns represent the reliability and energy consumption results of the GA-duplication. The fourth and fifth columns indicate the reliability and energy percentage change of the GA results relative to the results after duplication. The sixth and seventh columns show the reliability and energy percentage change of the ILP results relative to the results of the duplicated GA method. In the last two columns, the results of the duplicated GA method were compared relative to the results of the FD solutions for which the reliability is calculated according to the Equation (2) using the resources obtained by GA, while the energy consumption and area are doubled.

TABLE 7 Duplicated GA Results of DES Compared to the Results of GA, ILP, and Fully Duplicated Methods
Table 7- 
Duplicated GA Results of DES Compared to the Results of GA, ILP, and Fully Duplicated Methods
TABLE 8 Duplicated GA Results of AR Compared to the Results of GA, ILP, and Fully Duplicated Methods
Table 8- 
Duplicated GA Results of AR Compared to the Results of GA, ILP, and Fully Duplicated Methods
From the results we observe that the GA duplication method improves the reliability results by around 5 percent on average over the original GA method, while increasing the energy with an average of 15 percent. Furthermore, the GA duplication results are better compared to the ILP results in terms of reliability; however, the energy consumption is increased. When we compare our GA duplication method against the FD counterpart, we observe that our reliability difference on four benchmarks is around 4 percent on average. On the other hand, FD method increases the energy consumption by an average of 70 percent and the area 100 percent. These results show that our duplication-based post processing is very effective when compared to the FD version, since it improves the reliability similarly to the FD method, but with half the area and much less energy consumption.

SECTION 8Conclusion
Constantly shrinking technology sizes have allowed for a significant increase in the number of transistors on chips, resulting in smaller integrated circuit areas and increased circuit densities. Higher circuit densities, in turn, have introduced new design problems in digital systems, such as more vulnerability of circuits to radiation effects due to lower supply and threshold voltage levels. Higher SERs of combinational logic in particular makes reliability-oriented HLS a priority. Nonetheless, achieving a high reliability of a circuit should not come at the expense of unlimited energy consumption.

This study incorporates both energy and reliability metrics into the optimization of the HLS process for ASIC design, unlike the previous studies that only focus on one of these metrics, while taking the area and latency constraints into consideration. Especially, the effect of using voltage islands on reliability is examined. Our goal was to develop new HLS methods for ASIC design under area and timing constraints with objectives of both low energy consumption and high reliability. We propose ILP and GA-based optimization methods, as well as a post-synthesis selective duplication method which further maximizes the reliability. While the ILP-based method determines the optimum results, it takes too much time for some problems; the execution time grows exponentially as the number of nodes increases, and it becomes computationally impractical for problems with a large number of variables. Therefore, we also propose a GA-based method that is faster and yields optimum or near-optimum results. We also characterize a resource library consisting of three adders and two multipliers with varying area, delay, energy, and reliability parameters under two voltage levels, which we believe will also be useful for future HLS studies.