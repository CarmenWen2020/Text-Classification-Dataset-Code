Numerous algorithmic optimization techniques have been proposed to alleviate the computational complexity of convolutional neural networks. Given the broad selection of AI accelerators, it is not obvious which approach benefits from which optimization most. The design space includes a large number of deployment settings (batch sizes, power modes, etc.) and unclear measurement methods. This research provides clarity into this design space, leveraging a novel benchmarking approach. We provide a theoretical evaluation of different CNNs and hardware platforms, focusing on understanding the impact of pruning and quantization as primary optimization techniques. We benchmark across a spectrum of FPGA, GPU, TPU, and VLIW processors for systematically pruned and quantized neural networks (ResNet50, GoogLeNetv1, MobileNetv1, a VGG derivative, a multilayer perceptron) over many deployment options, considering power, latency, and throughput at a specific accuracy. Our findings show that channel pruning is most effective and works across most hardware platforms, with speedups directly correlated to the reduction in compute load, while FPGAs benefit the most from quantization. Pruning and quantization are orthogonal, and yield optimal design points when combined. Further in-depth results can be found at our web portal, where we share all experimental data, provide data analytics, and invite the community to contribute.
SECTION 1Introduction
Over the last several years, convolutional neural networks (CNNs) are being adopted with a rising interest in their deployment within the embedded space to enable IoT devices with cognitive behaviour. The challenge in their deployment lies foremost in their compute and memory intensity, particularly within the space where chip area, power, memory and compute availability are at a premium. CNN inference requires often billions of operations and has a memory footprint ranging from several to hundreds of megabytes.

This has spawned a rise in algorithmic and architectural innovation, including topological transformations with pruning, and compression schemes such as quantization of datatypes. For example, as shown in Table 1, pruning can double the throughput and halve the latency for little accuracy drop. Similarly, extreme reduced precision neural networks (RPNNs), which take datatypes down to ternary or even binary representations, can bring significant orders of magnitude reductions in hardware cost [1].

TABLE 1 Benefits of Pruning on ResNet-50, Ultra96, Using Xilinx DPU (Deep Learning Processing Unit) Overlay

On the hardware side, architectural innovation is showcased by a broad spectrum of FPGA-based accelerators, new GPU editions such as the Turing and Ampere family, as well as by Google’s tensor processing unit (TPU) [2], and numerous start-up companies such as Habana, Graphcore, and GROC. Each of these architectures brings their own inherent benefit and it is very hard for system designers to identify the best design choices given the broad spectrum of CNN algorithms with numerous optimization possibilities and hardware architectures. The resulting design space is highly complex, and it is becoming increasingly difficult to predict which optimization technique combined with which architecture will deliver what performance for which particular compressed variant of a neural network. It is further complicated by different deployment parameters such as batch sizes, thread counts, stream sizes and hardware operating modes, as well as unclear methods for measuring key figures of merit.

In order to help drive clarity in this space, we systematically evaluate and compare pruned and quantized variants of the same CNNs on numerous FPGA accelerators, GPU, CPU, TPU and VLIW processors across many possible deployment settings for ImageNet, CIFAR-10 and MNIST classification. Regarding measuring methodology, we are leveraging a novel benchmark, QuTiBench [3]. This benchmark is unique in the way it supports algorithmic optimizations such as quantization and pruning, by always tying hardware performance and power directly back to the application accuracy.

Even with our hundreds of experimental data points and corresponding theoretical analysis, we can only partially penetrate this vast design space which exists today and is continuously evolving with new architectures being released all the time. Thus, we made all our experimental data as well as our theoretical analysis available through a web portal and we encourage future community contributions in the hope that, as a collective community, we can achieve a deeper understanding of optimal design choices. The contributions of this paper are as follows:

Theoretical evaluation of pruning and quantization on a spectrum of inference accelerators,

systematic experimental evaluation and comparison of pruning and quantization across these inference accelerators for different CNN topologies and machine learning tasks, across many possible deployment parameter settings using a well-defined measurement methodology,

detailed study on how different hardware architectures benefit from optimization methods such as pruning and quantization regarding throughput, latency, power, energy and utilization, and

a web portal to provide full access to all theoretical and experimental data to the community and enabling third party contributions. This can be found at: https://rcl-lab.github.io/QutibenchWeb/.

SECTION 2Background
In this section, we provide a brief background on the general scope of the machine learning design space with its most recent and relevant neural network topologies, popular optimization schemes and popular inference hardware architectures.

2.1 Design Space
Within the neural network inference domain, we are faced with an extremely complex design space. There are numerous machine learning tasks, and each of these can be trained with different datasets and different neural network topologies, and depending on these factors (as well as numerical representations, learning techniques and hyperparameter selection), the end solution can produce different results with the key figure of merit being accuracy. Regarding the implementations, there are numerous choices with different hardware platforms each of which can run different implementation alternatives and different deployment parameters including batch sizes and power modes. All of the implementation alternatives will deliver different performance characteristics.

This paper investigates three machine learning tasks: ImageNet, CIFAR-10 and MNIST classification. As CNN topologies, we utilize ResNet50 (RN50), MobileNetv1(MNv1), and GoogLeNetv1 (GN) for ImageNet, a VGG16 derivate named CNV [4] with six convolutional layers and three fully connected layers for CIFAR-10 and finally a multilayer perceptron (MLP) for MNIST classification with four layers [4].

2.2 CNNs and Their Popular Optimizations
To alleviate the computational burden of CNNs and maximize their performance, many optimization techniques have been introduced. Particularly successful techniques include pruning, compression, low rank approximations, Winograd, Strassen and FFTs, and quantization [5]. In this paper, we focus on pruning and quantization, as two of the most popular forms of optimizations, which are discussed in greater detail below.

2.2.1 Quantization and Numerical Representations
Transprecision computing is making strides in many application domains [6], [7], and is particularly effective for neural network inference. On smaller image classification tasks such as MNIST, SVHN and CIFAR-10, RPNNs achieve state of the art accuracy despite reduction in precision [8], [9], even for partial or full binarization of fully connected and convolutional layers. For more complex tasks and topologies such as ImageNet classification, similar results have been achieved [10], whereby an initial slight degradation in accuracy was compensated through increasing model size [11], [12], [13], [14], [15], new quantization schemes such as [16], and new training and optimization techniques [17], [18]. The resulting solutions can run significantly faster in hardware and might pose an attractive design trade-off even for small loss in accuracy.

The benefits of reducing precision is twofold: First, the compute cost per operator is greatly reduced which enables further performance scaling [4]. Second, memory footprints shrink and as a result the operational intensity (OI) of the application (the amount of compute that is required per memory access) increases and memory bottlenecks are reduced or removed.

In regards to quantization experiments, we include the following versions of CNN topologies with different precisions: GoogLeNetv1, and ResNet50 in half and full floating point precision and fixed point 8-bit integer (FP16, FP32, INT8), MobileNetv1 in INT8, and CNV and MLP in 4-bit and 2-bit fixed point integer (INT4, INT2) and FP16. Accuracy and corresponding compute requirements for all CNNs are given in Tables 3 and 4 respectively.

TABLE 2 Experimental Hardware Platforms

TABLE 3 Experimental CNNs and Their Accuracy

TABLE 4 Experimental CNNs and Their Requirements (Batch = 1)

Where possible, the FP32 models (specifically, ResNet50 and GoogLeNetV1) were chosen from publicly available sources. The FP32-based CNV and MLP required training from scratch and were trained in-house using Caffe. In all cases, quantization and pruning of models was performed using the available tools for the target platform. Specifically, the INT8 models attained for the DPU were quantized (with retraining) and pruned using the DECENT tool in DNNDK [19]. The FP16 models which were attained for the TX2 and NCS were directly quantized from FP32 using TensorRT1 and OpenVINO.2 The INT8 CNNs attained for the TPU were sourced from TensorFlow’s own quantized modelzoo.3 The INT4 and INT2 CNN models used for FINN and BISMO were trained in-house using the techniques described by Su et al. [20] using the code provided in the BNN-PYNQ Github repository with a few modifications in order to achieve a desired number of channels.4

2.2.2 Pruning
This is another popular optimization which has been shown to dramatically reduce memory and compute requirements, through either synaptic pruning or filter pruning. In the context of synaptic pruning, individual synaptic connections between neurons are removed according to a pruning rule, e.g., when the synapse weight is below a certain threshold. As visualized in Fig. 1, the resulting compute patterns become irregular and impact both memory and compute efficiency, unless hardware architectures offer support for sparse matrix representations to benefit from this [21]. In the context of convolutional layers, filter pruning (see Fig. 2) is more popular, as it yields regular compute patterns, which can be easily parallelized and stored efficiently, thereby benefitting a broader selection of platforms [5]. The basic technique computes a sensitivity of filters as the sum of magnitude of all included weights and removes all filters below a given threshold. Regarding the experiments included in this paper, we investigate three of the previously mentioned topologies pruned to different levels, specifically: ResNet50, CNV and MLP. Each pruning scale is given in a percentage such as 100 percent for the baseline. Unfortunately the definition of the percentage is not consistent. This is an artifact of the associated typically black-box tooling, each with their own definition of pruning scale. In the context of CNV and MLP it relates to the number of inner channels associated with inputs and outputs of hidden layers.


Fig. 1.
Synaptic pruning.

Show All


Fig. 2.
Filter pruning.

Show All

2.3 Hardware Architectures
There is a huge range in the types of hardware architectures used for machine learning applications, including CPUs, GPUs, FPGAs and specialized architectures. The field has spawned significant new research in computer architecture and created deep learning processing units (DPUs) which are specialized for this application domain and can be implemented with either ASICs or in FPGAs. Architectures can broadly be classified by the basic type of compute operation, memory bandwidth, level of parallelism, degree of specialization, and inherent precision support. Finally, we differentiate between feed-forward dataflow architectures, where the hardware mimics the CNN topology, and layer-by-layer compute architectures as they demonstrate very different performance characteristics.

CPUs are usually multicore and multi-threaded to support parallel processing, and incorporate vector processing units with implicitly managed memory hierarchies, and support single and double precision floating point operations. GPUs are vector processors that support smaller floating point formats (FP16) natively, most recently fixed point 8-bit integer formats, and have a mix of implicitly and explicitly managed memory with increasingly hardened features for machine learning workloads. For example, Nvidia’s Volta architecture, introduced in 2018, incorporates tensor cores as a new feature [22]. AMD’s Vega GPU [23] offers a new deep learning instruction set, with the goal of obtaining parity with NVIDIA’s high-end Tesla V100 datacenter GPUs. Both companies have low power GPUs: the AMD Vega mobile GPU [24] and NVIDIA Jetson TX2 [25]. The latter is evaluated in this study.

DPUs, such as Google’s TPU introduced in 2016 [26], work with tensors, have explicitly managed and specialized memory hierarchies and support integer and floating point operations. The first generation supported integer arithmetic with a massively parallel 8-bit matrix multiply engine. Newer generations [27] boast improved memory performance as well as support for floating point specifically aimed at training and also include a USB device,5 which we included in this effort. Intel is investigating several custom accelerators including Nervana, Habana and Movidius, which features in Intel’s ultra low power Neural Compute Stick (NCS). The current second generation, included in this effort, operates at about 1 Watt leveraging VLIW processing engines.

FPGAs are the most flexible of all target hardware platforms, and can be configured to support any numerical representation [1], [11], [28], [29], [30], even bit-serial hardware architectures which provide run-time configurable precision as for example shown in BISMO [31]. Furthermore, they can be fully customized towards specific neural network topologies in the form of feed-forward dataflow architectures, as for example shown in FINN [4], [28] and thereby achieve high performance and efficiency. Similar approaches have been taken by [32]. Because of this inherent flexibility, FPGAs are also particularly difficult to characterize. To ensure sufficient coverage, we include in this study layer-by-layer compute with Xilinx’s DPU, the FINN dataflow implementation and BISMO bit-serial accelerator as examples of fundamentally different implementations. It should be noted that although BISMO is a generic matrix multiply accelerator and does not provide system-level performance optimization for CNN inference, it is still relevant in that it can demonstrate potential performance gains for different precisions.

SECTION 3Experiments
In this section, we provide an overview of all experimental platforms, chosen workloads and conducted experiments. In total, we ran 540 experiments on 9 different hardware platforms including 5 different FPGA implementations, GPU, TPU, VLIW and CPU implementations, targetting specifically the embedded space. We leveraged 5 different topologies, and trained 42 CNNs for different pruning scales and for different precisions. For all of these combinations, we swept over batch sizes, thread counts, stream sizes, in all possible operating modes, measuring throughput, power, accuracy and latency. While we strive for a complete systematic evaluation of all combinations, we are constrained by what the various hardware platforms offer and many have limitations in regards to supported layer types, precisions and with that supported topologies. This is the number one reason why there is not a single network that can be run in all precisions on all hardware platforms. Instead many solutions are black box solutions that come with a supported modelzoo and CNNs, but different topology variants (with pruning scales and precisions) cannot be executed on these platforms. For example, the TPU failed to execute both the CNV and MLP, while the FPGA platforms did not support MobileNetv1 at the time of this study. Also, TPU, VLIW, and CPU cannot support precisions below INT8, for example INT2 or INT4, and the GPU does not support any datatype other than FP16 and FP32. Despite these limitations, we believe that the presented datapoints provide sufficient evidence to demonstrate the benefits of pruning and quantization on the various platforms.

Experimental hardware platforms are summarized in Table 2, and include a range of platforms, all popular for embedded applications. The table lists peak performance for different datatypes (INTx, FPx), memory bandwidth and capacity (MBW and MCAP) as well as thermal design power. For FPGA implementation, we use Xilinx’s commercially available DPU platforms implemented on the ZCU102, ZCU104 and Ultra96. Furthermore, we leverage FINN as an example of a fully synchronous dataflow architecture with arbitrary precision support,6 and BISMO. Note that BISMO uses only abouta quarter of the resources compared to FINN and as such is naturally not performance competitive. In regards to CPU implementation, we include an ARM Cortex A53 processor using gemmlowp [33] on the Ultra96 platform. Furthermore, we run experiments on the TPU and NCS, as introduced in the background section. Finally, we use Nvidia’s Jetson TX2 platform as a popular example of a GPU. The TX2 is manufactured at the same technology node as the selection of FPGAs. While the USB sticks are almost a separate category in regards to compute performance and power consumption (<2 W), we opted to include them as no comparable TPU or VLIW processor is available in the larger embedded category. With that, all platforms are roughly 1-10 TOPs in performance and 1-20 W in regards to power consumption. Note that we provide separate values for different datatype peak performance and the hardware performance scales with the reduced hardware cost of lower precision operations.

Regarding experimental workloads, Table 3 provides a complete overview of all CNNs that were leveraged in our experimentation (introduced in the background section), and Table 4 shows the corresponding compute and memory requirements in number of operations ([GOPS]), number of parameters or model size in millions of elements ([ME]) and OI in operations per byte read or written from memory ([OPS/B]). Please note, as training experiments are highly time-intensive with no necessarily guaranteed outcome, we only trained topologies at the given precision for the cases where the hardware platform could actually support it. Finally, Table 5 summarizes all included experiments.

TABLE 5 Overview of all Experiments

SECTION 4Benchmarking Methodology
In this paper, we make use of a novel benchmarking methodology, named QuTiBench [3], which is designed to expose the spectrum of possibilities and accurately reflect the capabilities of the different hardware platforms. In particular, it offers the following key characteristics. First, this benchmark provides scope for algorithmic optimization, which is the key focus of this paper, by coupling hardware performance with application level performance (accuracy). This allows for objective comparison between pruned and non-pruned variants and between floating point and reduced precision models. Results are compared via pareto graphs (accuracy versus latency, throughput and throughput/power) whereby optimal solutions can be found along the pareto frontier and the full spectrum of design tradeoffs is visualized. Examples of these can be found in Section 5. Second, the methodology includes a theoretical analysis of both hardware platforms (peak performance in TOPs or GOPs, external memory bandwidth in GBps and thermal design power in Watts), as well as algorithms in the form of compute and memory requirements for all CNN topologies. Combining algorithm requirements with hardware platform characteristics can be leveraged for performance predictions using roofline models [34]. In more detail, rooflines model the theoretical performance tperf of a hardware platform taking available off-chip memory bandwidth membw and peak compute performance pperf into account. tperf is given as a function of the operational intensity of an application, which in essence represents the ratio between instructions to be executed cmp, and total memory footprint mem, residing off-chip, in bytes. To be precise, tperf is calculated as the minimum of available compute performance pperf and memory bandwidth multiplied by an application’s operational intensity. This is expressed in the following formula:
tperf=Min(pperf,membw∗cmpmem).(1)
View Source

The visualization of a roofline graph is a 2D line graph in a log-log scale as shown in Fig. 3. The x-axis represents the operational intensity of applications, and the y-axis shows tperf. Given the function, the resulting curve resembles a roofline, hence the name, whereby the slope represents the memory bound area, and the flat top the compute bound area, where applications hit the absolute compute limit of a platform. When combining theoretical peak performance and memory bandwidth in roofline models with the operational intensity of CNNs, we can gain insights as to whether a neural network will be memory or compute bound and what is theoretically possible in regards to performance, see Fig. 3. Furthermore, we can leverage this model to provide performance estimates by calculating the specific operational intensity of a specific CNN, and using this in Equation (1). The resulting datapoint is indicated in Fig. 3 with the red circle. Note that for computing the operational intensity, we have to make assumptions for where weights, tensors, gradients, weight updates and state of a neural network are stored, and combine these with the size of the datatypes.


Fig. 3.
Performance predictions leveraging roofline analysis.

Show All

Finally, clear guidelines are given on how to measure power, latency and throughput on fundamentally different hardware platforms, and how to include deployment parameters such as batch sizes and operating modes in a fair manner. We have adopted these across all our experimentation. Our chosen metrics are explained in more detail in the following section.

4.1 Figures of Merit
In our experimentation, we made the following choices regarding figures of merit:

4.1.1 Throughput and Latency
We report compute throughput and latency as defined in [3] as this eliminates overheads such as data copy and focuses on compute performance only. The exact measurement points for compute throughput is illustrated in Fig. 4. Unfortunately, it is impossible to make measurements at exactly the same granularity across all the given platforms. For example for the Xilinx DPU, the primitive to execute the compute includes the movement of results at the end. The actual overlay is encrypted and manual probes cannot be inserted. Given that the data movement only contains the resulting probabilities, we believe this is negligible. In regards to throughput, we consider frames or inputs per second (fps) when comparing different algorithms with each other, as we are only interested in the application level performance when comparing to accuracy. We chose TOPs only when considering compute efficiency aspects. Finally, latency is reported as compute latency in milliseconds (msec).

Fig. 4. - 
System versus compute performance.
Fig. 4.
System versus compute performance.

Show All

4.1.2 Power and Energy
To represent power and energy cost, we only report platform power measured at the socket to ensure memory subsystems are taken into account. For the USB devices, we used a USB power meter (Innovateking, A3A3-B). While not as accurate as current sampling on the board, we typically found the numbers to be within 10 percent. Also it allowed us to adopt a consistent way of measuring across all platforms. We would also like to point out that idle power with these platforms can represent a significant percentage of the overall power budget and therefore cloud the observation. In particular two of the FPGA platforms (ZCU102 and ZCU104) are evaluation boards with many superfluous peripherals, which is reflected in high idle power (19.9 Watts) compared to the GPU (between 3.4 and 5.0 Watts depending on operating mode), while the additional dynamic power consumption is minimal. So when comparing total power, GPUs are more efficient, whereby if we were to bracket out board peripherals, FPGAs would be more efficient. Even though this somewhat obfuscates the results, we stick with full board power as this seems to be the most fair way of comparing. This is visualized in Fig. 5. Blue is the reading when power is applied to the board, orange when bitstream is loaded. Ubuntu once the OS has booted, and peripherals when USB hub with keyboard and mouse are connected. Network is full load. For the GPU: board, Ubuntu, peripheral and network are the same, and op. mode selects power mode and clock speed.


Fig. 5.
Breakdown of power.

Show All

4.1.3 Operating and Power Modes
Many of the chosen platforms offer different power or operating modes. We conduct all our experiments across the spectrum of all operating and power modes unless annotated otherwise. Specifically: The TPU can operate with a fast or a slow clock. The TX2 platform can run in either maxn, maxp or maxq modes. Maxn is the high performance mode with highest power consumption. Maxp is the most efficient mode, with lowest power but also lowest performance, and maxq prioritizes performance per Watt.

4.1.4 Batch Sizes, Thread Counts, and Stream Sizes
Many hardware platforms require increased batch sizes, thread counts or stream sizes in order to extract maximum performance out of a hardware platform and achieve high compute efficiency. However, this can result in substantially different latency as is shown in Fig. 6 whereby different hardware platforms show different behaviors.7 Different batch sizes, thread counts and stream sizes are represented as dots in the lines. In more detail: Layer-by-layer compute architectures including TPU, GPU, NCS, BISMO and A53 have a larger than linear increase in latency with regards to batch size because a sequence of images has to be buffered before each of the layers can be executed. This adds significant delays. Also, as the device utilization saturates with increasing batch size, peak performance is reached, and no further benefit can be achieved. The extreme variant is the NCS which has already achieved full throughput at batch=1. Therefore with increasing batch size latency rises but throughput remains constant. Dataflow architectures such as FINN have a fixed latency (measured from the first word of input both at the beginning and the end of the pipeline) independent of input stream size which is therefore determined by the length of the given pipeline. As such, when only one input is processed (stream size=1) the pipeline is underutilized and throughput is low. Full throughput is achieved when the stream size saturates the pipeline. As a result, the latency is flat over the stream sizes. Finally, the DPU, which deploys a vector of processing engines, increases latency with rising thread count, however at a much gentler slope than batch sizes for GPUs.


Fig. 6.
Latency versus throughput over batch sizes, thread counts, and stream sizes.

Show All

SECTION 5Evaluation
In this section, we evaluate the chosen hardware platforms in a wide range of possible configurations across all parameters for the given spectrum of potential workloads in regards to pruning and quantization across all figures of merit. We consider the following combinations:

Machine learning task: ImageNet, CIFAR-10, MNIST classification

Optimization: pruned, quantized, or both

Hardware platform: TX2, TPU, NCS, FINN on ZCU102 and ZCU104, BISMO on ZCU104, A53 on Ultra96, DPU on ZCU102 and ZCU104

Deployment parameters: operating mode and selection of batch and stream sizes and thread counts

We begin with a theoretical evaluation of how quantization and pruning can have potential impact. In regards to experimental evaluation, we systematically investigate for CNV, MLP and ResNet50, how pruning and quantization affect all hardware platforms, both individually, (Sections 5.2.2, 5.2.3, 5.2.4, and 5.2.5) and combined (Section 5.2.7), for the various figures of merit. Finally, we combine results, and evaluate both optimization techniques across all topologies and platforms, grouped by the different types of machine learning tasks to see whether pruning or quantization brings the most benefit. We conducted over 540 experiments and measured the results on running hardware. Regarding visualization, we chose box and whiskers graphs as they illustrate average, range and distribution of values across the various deployment options. For graphs which compare two figures of merit (accuracy-performance or latency-performance), we leverage charts with pareto frontiers to highlight optimal solutions within the design space. For example the frontier indicates what is the minimal throughput that can be achieved for a minimum accuracy requirement, given all possible combinations of CNN, hardware platform and optimization strategy. All data and results can be found on our web portal [35].

5.1 Theoretical Evaluation
For the purpose of theoretical evaluation, we leverage UCB’s roofline models as suggested by the QuTiBench benchmarking methodology [3] and explained in Section 4, and adjust both the OI according to the revised datatype and compute load in accordance with the resulting quantized and pruned networks, as well as the theoretical compute performance of a platform according to the datatype.

The following observations can be made. Reducing the precision increases the OI of an algorithm as less data needs to be moved off-chip. At the same time, the theoretical compute performance of the platform increases if the reduced precision datatype is natively supported. The resulting implementation is more likely to be compute bound and will improve by jumping to the reduced precision peak performance. For a ZCU104, from INT16 to INT4, this implies an improvement by a factor of 16x. These tradeoffs are depicted in Fig. 7.


Fig. 7.
Reducing precision scales performance and increases OI.

Show All

In regards to pruning, the characteristics of the hardware platform do not change, however both the required model size as well as the compute load are reduced. The graph in Fig. 8 visualizes this reduction in both compute and memory, which is quadratic in regard to pruning percentage for CNV and MLP. However, the resulting operational intensity of the algorithms remains roughly the same.8 This is detailed in Table 6. In regards to achievable performance, this means the following: The overall compute performance in regards to operations per second does not change as per roofline analysis, but the required amount of compute per input has reduced. Therefore the resulting throughput measured in fps should scale linearly with the reduction achieved through pruning in compute operations. Table 7 visualizes the reduction in compute as a function of the pruning percentage.

TABLE 6 Impact of Pruning on OI

TABLE 7 Compute for Pruned Variants


Fig. 8.
Compute and memory requirements when using filter pruning.

Show All

5.2 Experimental Evaluation
We first discuss the choice of data visualization with “box and whisker graphs” [36] to analyze figures of merits in isolation. Then we evaluate figures of merit in combination, specifically accuracy combined with throughput, and latency combined with throughput. Both highlight the optimal solutions along the pareto frontier in regards to the selected figures of merit. Not all datapoints are included for space constraints; all data can be found on our web portal.

5.2.1 Data Visualization
We chose box and whisker graphs as they allow to compress many experimental values and visualize overall behaviours. Fig. 9 illustrates this. Each figure shows all experimental data for one specific CNN topology. In more detail: We group all datapoints over all batch, stream sizes, thread counts and all deployment parameters per hardware platform and network into one bar. The boxes represent the first and second quartile and median, x is the average, and the whiskers show the outliers. There are always a group of bars per hardware platform and datatype/precision, and one bar per pruning factor. The size of the box visualizes the large variations of values (for example for GPUs). The benefits of quantization are visualized in comparison between 2 groups of box and whiskers, in particular in regards to averages but also minimum and maximum values. The benefits of pruning are shown by comparing the bars within 1 group. Finally, the first group for latency and throughput shows theoretical compute per input or the inverse to visualize the correlated relationship to the respective figure of merit. Note that we have normalized all values to the peak number achieved for each hardware platform in order to decrease the overall range of values. Absolute values can be found on our web portal [35].


Fig. 9.
Box and whiskers for different pruned and quantized versions of a given CNN topology.

Show All

5.2.2 Latency
As can be seen in Figs. 10 and 11 and as expected by our theoretical analysis, all platforms benefit from pruning in equal measure and directly related to the remaining compute in the network, as annotated in the figures through the red arrows. This applies to all tested topologies (however for ResNet50, we could only use the Ultra96-INT8 platform, as this was the only one which would execute the pruned CNNs.) Quantization benefits, highlighted by the blue arrows in Figs. 10 and 11 are shown for the hardware platforms with the supported datatypes; for example, A53 derives no benefit. TX2 benefits from going from FP32 to FP16 almost by 2x for larger batch sizes, less so for batch=1. FINN derives the greatest benefit (3.7x) due to the greater than linear reduction in hardware cost when going from INT4 to INT2, however for smaller networks (as we increase the pruning factor), the speedup converges to 1.3x, as we increasingly encounter hardware overheads. BISMO improves by up to 2x for larger batch sizes in reduction from INT4 to INT2. It is worth noting that quantization is beneficial independent of whether we have a layer-by-layer compute architecture or a full dataflow. This applies to all tested topologies. Furthermore, there is a significant difference in regards to latency variation to be observed between dataflow architectures (FINN), which shows no jitter, and all others. This is as predicted by the theoretical analysis.


Fig. 10.
Latency - pruning and quantization of MLP.

Show All

Fig. 11. - 
Latency - pruning and quantization of RN50.
Fig. 11.
Latency - pruning and quantization of RN50.

Show All

Finally, it can be observed that quantization and pruning can be effectively combined, and the overall speedup and latency reduction is an almost direct combination of the individual gains. We have visualized this with the green arrows in Fig. 10, and discuss this further in Section 5.2.6.

5.2.3 Performance
Similar to latency, direct speedup can be derived from both quantization and pruning. This is visualized in Figs. 12, 13, and 14 for CNV, MLP and ResNet50. The speed-up is mostly correlated to the amount of compute per input required (which has been included in the figures as the first column and is annotated in red. Quantization brings the greatest benefit for FINN; 2x on average for all pruned versions and all networks (annotated in blue). TX2 performance also improves between FP32 and FP16 but only by a factor of 1.42x for CNV and much less for MLP. We assume this is because MLP is, according to the theoretical analysis, memory bound given the operational intensity shown in Fig. 2. The DPU on Ultra96 gets a significant but not quite linear improvement. Specifically, by taking the pruning scale factor for ResNet-50 from 100 to 30 percent the speed-up is 1.81x. Furthermore, it can again be observed that pruning and quantization can be effectively combined (green annotation). A more detailled discussion of this can be found in Section 5.2.6.


Fig. 12.
Throughput (pruning and quantization of CNV).

Show All


Fig. 13.
Throughput - pruning and quantization of MLP.

Show All

5.2.4 Power
The effects of pruning and quantization on overall board power are naturally much more modest, as there is significant overhead in regards to board peripherals with idle power being a substantial component, as shown in Fig. 5. This applies in particular for FPGA devices (FINN and BISMO). But also the USB device shows only modest improvements with more pruning and quantization. The GPU platform derives the greatest benefits from pruning, with quantization bringing only limited benefits in regards to power (see Fig. 15, blue and red annotations). Also note that power measurements are done at the socket and come with a 10 percent error margin, which may explain some of the erratic measurements.


Fig. 14.
Throughput - pruning and quantization of ResNet50.

Show All


Fig. 15.
Power - pruning and quantization of CNV.

Show All

5.2.5 Efficiency
For NCS, the power efficiency seems to be linear, while TX2, FINN, BISMO and A53 improve more in line with the reduction in compute relating to pruning which is the square of the pruning scale. In addition, quantization for Tx2, provides almost a 2x improvement going from FP32 to FP16, while for FPGAs this is even more pronounced, where the improvement is greater than 2x, and for BISMO it is almost exactly 2x. Furthermore, for A53 below INT8 compute, there is naturally no difference as the compute is carried out in INT8 carrier datatypes using the gemmlowp library. For MLPs, thus the greatest benefit can be seen for the FPGA designs. We show CNV results in Fig. 16 and MLP in Fig. 17; other topologies are available on the web portal. Finally, as with the other figures of merit, it is shown that pruning and quantization can be effectively combined (annotated in green), which is discussed in more detail in Section 5.2.6.

Fig. 16. - 
Performance/Power - pruning and quantization of CNV.
Fig. 16.
Performance/Power - pruning and quantization of CNV.

Show All

Fig. 17. - 
Performance/Power - pruning and quantization of MLP.
Fig. 17.
Performance/Power - pruning and quantization of MLP.

Show All

5.2.6 Orthogonality of Pruning and Quantization
As previously mentioned, quantization and pruning can be effectively combined. In this subsection we quantify this in more detail. In order to do so, we must first introduce the nomenclature shown in Fig. 18. For each topology and each platform, we provide datapoints (latency and throughput) for the original variant (A), a quantized variant (B), a pruned variant (C) and a quantized and pruned variant (D). p1,q1,p2,q2 symbolize the improvements achieved with first degree and second degree optimization, whereby p1,p1 are the first degree optimizations, and p2,q2 the second degree optimizations. p stands for pruning and q stands for quantization. If D is better than C and B (lower for latency, and higher for throughput), then the optimizations are complimentary. If p2/p1=1 and q2/q1=1, then the optimizations are perfectly orthogonal.

Fig. 18. - 
Orthogonality of pruning and quantization.
Fig. 18.
Orthogonality of pruning and quantization.

Show All

In Table 8 we show some examples for the CNV and MLP topologies. We only included examples where optimizations were effective and both pruning and quantization on the same CNN can be applied.9 As can be seen, the measured results show that D is always greater than B and C and the ratios p2/p1 and q2/q1 are, apart from a couple exceptions, close to 100 percent, showing that they are orthogonal and can be effectively combined.

TABLE 8 Orthogonal Optimizations

5.2.7 Overall Design Space
The remaining questions are, first, for the same level of accuracy, do we derive more benefit from pruning or using reduced precision for ImageNet, CIFAR-10 and MNIST classification? Results are shown in Figs. 19, 20 and 21. The optimal solutions can be identified along the pareto frontier in the graph between accuracy and performance (or latency). Second, in embedded applications, latency is typically critical, and it is essential to understand what performance can be achieved within a given latency constraint. This can potentially limit the exploitation of batch- or thread-level parallelism. As such it is important to understand to what extend pruning and quantization can help and affect overall behaviour.

Accuracy Versus Performance. We first examine the trade-off between accuracy and performance in regards to fps. When looking at the design space of platforms and optimization techniques, we can observe that overall, combining pruning and quantization delivers the best result. Pruning seems to be more effective than quantization across a broader spectrum of platforms and topologies, however reducing precision is more effective for MLPs. For example, FINN-INT2-25 percent outperforms the FINN-INT4-12.5 percent. In this scenario, FINN also outperforms other architectures by far. This is due to the memory bound nature of the topology for FP16 and FP32 which throttles the compute performance on NCS and TX2. For INT4 and INT2, the model can remain on-chip for FINN while still offering competitive accuracy for this dataset and topology. We also make several other observations from these graphs. First, the USB devices are far lower in overall performance as expected from the theoretical analysis. Second, the bit-serial implementations do not feature on the pareto frontier, which is expected, as these implementations are not performance optimized. Third, FPGAs outperform others with regards to throughput in particular where quantization is available.

For ImageNet classification, as shown in Fig. 21, we have a few more topologies to compare with, and we can see that they have a massive impact in regards to performance and accuracy and in fact outweigh benefits from optimization techniques. Quantization from FP32 to FP16 has in essence no impact on accuracy and FP16 variants bring massive performance benefits. Pruning impacts accuracy when dropping below 50 percent, but up to then the performance benefits come basically for free. The NCS is far below competitive devices in regards to performance overall. The TPU offers GoogLeNetv1 performance on par with the TX2 whereby the INT8 accuracy is higher than TX2’s FP16 and FP32, most likely due to superior image preprocessing in TensorFlow. The TPU provides a pareto optimal design point with MobileNetv1, clearly showing that MobileNetv1 is an exceptional topology with regard to compute cost-accuracy compromise and outperforms GoogLeNetv1 variants on other devices. For the Ultra96, one can observe that pruning brings performance benefits at small accuracy reduction, above 80 percent, however overall the Ultra96 is too low in performance to offer optimal solutions. Finally, the ZCU102-DPU provides pareto optimal implementations with both GoogLeNet and ResNet variants. Unfortunately, in regards to ImageNet classification, we were not able to carry out a more systematic evaluation as many models were not supported by the various hardware platforms. Many more comparisons could be conducted by correlating power and latency with accuracy, or latency with throughput. This will be addressed in future work.

Fig. 19. - 
MNIST classification design space.
Fig. 19.
MNIST classification design space.

Show All

Fig. 20. - 
CIFAR-10 classification design space.
Fig. 20.
CIFAR-10 classification design space.

Show All

Fig. 21. - 
ImageNet classification design space.
Fig. 21.
ImageNet classification design space.

Show All

Latency Versus Performance. Finally, we consider the compromise between latency and performance in Figs. 22 and 23. More datapoints are visualized in the web portal, where the charts are also interactive and easier to parse. As detailed in Section 4, many hardware platforms require increased batch sizes, thread counts or stream sizes in order to extract maximum performance out of a hardware platform and achieve high compute efficiency. However, this can result in substantially different latency as is visualized in the following graphs, whereby we had to split up hardware platforms into different charts due to the large difference in ranges. As shown in previous results and explained in Section 4, FINN delivers lowest latency with no jitter for increasing stream sizes, whereby the performance in regards to fps will saturate when the pipeline is fully utilized. This is reflected in the right part of Fig. 22, where the FINN results almost overlap with the actual y-axis. This is true for all platforms. Shown here is only a subset, namely CIFAR-10 and ImageNet classification. For the layer-by-layer compute platforms such as NCS, BISMO, A53, and TX2, one can observe that the latency always increases with larger batch size and thread counts respectively, because a sequence of images has to be buffered before each of the layers can be executed which adds significant delays. The compute performance flattens out once peak performance has been reached. The extreme variant is the NCS which has already achieved full throughput at batch=1. Therefore with increasing batch size only latency rises but throughput stays constant. The DPU implementations reach performance saturation for very small thread counts. Finally, for the TPU we have only 2 data points: one for slow clock and one for fast clock operation, hence the unusual shape in the diagram.

Fig. 22. - 
CIFAR-10 classification design space: latency versus throughput.
Fig. 22.
CIFAR-10 classification design space: latency versus throughput.

Show All

Fig. 23. - 
ImageNet classification design space: latency versus throughput.
Fig. 23.
ImageNet classification design space: latency versus throughput.

Show All

SECTION 6Related Work
Numerous benchmarking efforts in this space have emerged, however we believe we are the only ones investigating the effects of algorithmic optimizations in the form of pruning and quantization across a spectrum of machine learning tasks. DeepBench [37] is one of earliest attempt, but is exclusively focused on microbenchmarks and does not take application level performance in regards to accuracy into account. Efforts such as [38] focus on benchmarking individual devices. Fathom [39], TPC [40], and ParaDnn [41] benchmark a representative workload with significant algorithmic breadth. They do not address the opportunities relating to numerical representations or pruning. MLPerf [42] is the most promising industry-wide effort in establishing a fair benchmark for CNN inference and training. However, MLPerf is not aimed at comparing optimization techniques and as such does not include a systematic evaluation or theoretical analysis which can be useful to provide early predictions for system designers without having to run experiments. The Collective Knowledge Framework [43] in conjunction with the ASPLOS Request Tournament [44] was one of the first places to introduce pareto graphs for full design space visualizations and collects submitted data points similarly to DAWNBench [45] through a web portal. Neither include the systematic evaluation leveraged in this paper.

SECTION 7Conclusion and Future Work
A broad spectrum of customized and heterogeneous compute accelerators have emerged to address the enormous compute and memory requirements of machine learning algorithms. Simultaneously, many algorithmic optimizations such as pruning and quantization have been adopted to lower the algorithmic burden. It is very hard to understand a priori which hardware platform benefits from which optimization strategy the most. For this purpose, we carried out a broad benchmarking exercise, leveraging a novel technique [3], that explores both pruning and quantization, independently and combined, in regard to a broad spectrum of CNNs and hardware platforms. We conducted over 540 experiments, measured the results on running hardware and provided all measurement data plus data analytics through a web portal [35].

The key findings are as follows:

Pruning brings performance, latency and power benefits across all platforms for all tested topologies in direct correlation to the reduction in GOPs/input.

Quantization brings benefits on all platforms and topologies which provide native support for the reduced precision datatypes. As a result, FPGAs can benefit the most from this technique, as arbitrary datatype operators can be implemented. In this case, quantization can be more effective than pruning. Compute performance scales directly with the precision reduction, in particular for MLP where the reduction in model size helps lower the inherent memory bottleneck. The chosen GPU, TPU, NCS, and ARM processor can only support a subset of INT8, FP16, and FP32, and when moving from FP32 to FP16 on these platforms where possible, similar speedup is achieved.

Power improvements are limited with FPGAs as idle power dominates overall power consumption, whereas for all other devices we see improvements in particular with pruning, less so with quantization.

We provide the experimental proof that quantization and pruning are orthogonal to each other and that the overall speedup and latency reduction is a direct combination of the individual gains.

Combined pruning and quantization delivers the best result in the overall design space as the accuracy seems to be resilient to combined optimization. This might not necessarily be intuitive as we expect there is a limited amount of redundancy within the CNN.

Among the chosen test platforms, FPGAs outperform, in particular for MLPs, in regards to latency and throughput in particular when quantization yields acceptable accuracy.

Layer-by-layer compute approaches result in much higher latency and latency variation, as a lot of buffering is required in order to achieve high compute efficiency.

Finally, CNN topologies have a massive impact on compute performance and accuracy and can outweigh benefits from optimization techniques. For example, a MobileNetv1 implementation on the TPU led to a pareto optimal design point.

The design space in this field is extremely complex. We encourage the hardware accelerator community to adopt the same benchmarking methodology such that more clarity and understanding can be derived. We specifically encourage others to contribute to our web portal. In the future, we plan to broaden the scope of machine learning tasks and topologies and include novel accelerator approaches and optimization techniques as they become available. Finally, we intend to carry out microbenchmarks, as defined by the benchmarking methodology, levels 1 and 2, which can be leveraged as an ablation study using different layer types with different precisions and pruning scales, to provide more accurate performance predictions compared to the purely theoretical analysis.