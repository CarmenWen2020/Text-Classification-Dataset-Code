Recurrent Neural Networks (RNNs) spend most of their execution time performing matrix-vector multiplication (MV-mul). Because the matrices in RNNs have poor reusability and the ever-increasing size of the matrices becomes too large to fit in the on-chip storage of mobile/IoT devices, the performance and energy efficiency of MV-mul is determined by those of main-memory DRAM. Therefore, computing MV-mul within DRAM draws much attention. However, previous studies lacked consideration for the matrix sparsity, the power constraints of DRAM devices, and concurrency in accessing DRAM from processors while performing MV-mul. We propose a main-memory architecture called MViD, which performs MV-mul by placing MAC units inside DRAM banks. For higher computational efficiency, we use a sparse matrix format and exploit quantization. Because of the limited power budget for DRAM devices, we implement the MAC units only on a portion of the DRAM banks. We architect MViD to slow down or pause MV-mul for concurrently processing memory requests from processors while satisfying the limited power budget. Our results show that MViD provides 7.2× higher throughput compared to the baseline system with four DRAM ranks (performing MV-mul in a chip-multiprocessor) while running inference of Deep Speech 2 with a memory-intensive workload.
SECTION 1Introduction
On EDGE devices such as mobile or IoT devices, Neural Networks (NNs) are frequently used for applications interacting with users. For example, Natural Language Processing (NLP) is widely used for devices such as Siri, Google Assistant, and Amazon Echo, and expected to become a key application in mobile in the near future. Recurrent Neural Network (RNN) [1] is crucial for NLP [2], [3]. Currently, edge devices typically rely on datacenters in performing a majority of RNN inference [4], [5], [6]. However, due to latency and energy burdens in transferring raw data for RNN inference over the networks, there is significant merit to perform RNN inference within the edge devices.

RNNs have many variants (e.g., LSTM [7] and GRU [8]), all of which spend most of the execution time for matrix-vector multiplication (MV-mul). A characteristic of MV-mul is that matrix elements, which correspond to weights in RNN, are used only once per vector, resulting in a limited degree of data reuse. The on-chip storage of edge devices, sized up to several megabytes (MB) [9], is typically too small to store all the weights whose sizes are reaching tens to hundreds of MB. Therefore, for each MV-mul, the matrix elements of RNN must be read from the main-memory DRAM. This is in sharp contrast to the characteristics of CNN, which has a high degree of data reuse for an input feature map while processing numerous weight channels [10].

When a matrix does not fit in on-chip memory, its MV-mul performance depends on DRAM bandwidth [4], [5], [11], which is much lower than the on-chip storage bandwidth of a processor. Also, in energy efficiency, the data transfer energy between a DRAM device and a processor dominates that of other operation types (e.g., multiply-accumulate (MAC) operations) [12]. Thus, the performance and energy efficiency of RNN inference is determined by the bandwidth and energy efficiency (J/b) of main-memory DRAM.

To solve the memory bottleneck and reduce J/b of DRAM accesses, there have been many near-data processing studies [13], [14], [15], [16]. They reduced off-chip memory traffic by adding operation units or accelerators near main memory, either within a DRAM die or a logic layer of a 3D-stacked memory. However, they do not fully utilize the internal DRAM bandwidth or do not consider the power constraint of DRAM. Also, they cannot perform acceleration operations and other memory requests from processors simultaneously, so they hardly function as the main memory.

In this paper, we propose the MViD (MV-mul in DRAM) architecture, which can handle both MV-mul operations and memory requests from processors (processor requests) under the maximum power budget of DRAM devices which is determined by normal DRAM operations. MViD places MAC units near the datapath I/O within DRAM banks to improve the performance of MV-mul by utilizing abundant DRAM internal bandwidth. In particular, we analyze the operational limit due to the DRAM power constraint and confirm that MAC units can be added only to half of the banks for the current mobile DRAM standard, LPDDR4 [17].

To further reduce the total amount of computation, MViD performs a sparse MV-mul operation exploiting sparsity and quantization. Processing sparse MV-mul is actively studied in various ways depending on the characteristics of the matrix [11], [18], [19], [20], and its effectiveness depends on finding an appropriate sparse data format by understanding the characteristics of the matrix. To find the optimum data format for MViD, we explore its design space by adjusting sparsity and quantization bits [11] of RNN weight matrices without accuracy loss. Also, we explore possible mappings of a weight matrix to a DRAM page.

MViD can act as the main memory by allowing other memory requests from a processor to be processed during MV-mul. MViD resolves the power and row-buffer conflicts that occur when MV-mul and processor-side requests are processed simultaneously by controlling the pace of MV-mul operation. For this, we implement MV-bank (bank performing MV-mul) control logic, which can slow down or pause the MV-mul operation. Also, MViD minimizes command/address path utilization overhead by leveraging the existing DRAM interface. MViD further improves the performance of MV-mul by placing the non-MV-mul workload data of the processor to the DRAM banks that do not perform MV-mul.

In this paper, we make the following contributions:

We propose MViD, which adds MAC units inside DRAM to solve the memory bottleneck of RNN inference in edge devices and to improve energy efficiency.

MViD deploys MAC units by carefully considering various constraints (internal bandwidth, power limit, and off-chip bandwidth) that could occur when adding MAC units in the current mobile DRAM standard. For the first time to the best of our knowledge, MViD can perform processor requests simultaneously with MV-mul; therefore, MViD functions as the main memory.

Through sparse matrix formatting, quantization, and bank partitioning, MViD improves the throughput of inference in Deep Speech 2, up to 7.2× compared to the baseline system without MViD. MViD also guarantees that MV-mul operations do not hoist processor requests.

SECTION 2Background and Motivation
2.1 Energy-Efficient RNN Mobile Inference
RNN inference is mostly performed in datacenters yet [4], [5]; however, to reduce the latency and improve the energy efficiency of serving RNNs, there is a strong demand for conducting RNN inference closer to service requests, such as in mobile and IoT devices. These devices typically do not have large on-chip memory due to power and cost issues.

RNNs have several variants, such as LSTM (long short-term memory) and GRU (gated recurrent unit), depending on the existence of certain gate types. RNN operation consists mainly of matrix-vector multiplication (MV-mul), element-wise sum/multiply, and activation function such as tanh or sigmoid, but MV-mul dominates the execution time. For example, a typical configuration of Deep Speech 2 (DS2) [2], a popular RNN benchmark performing end-to-end speech recognition, is composed of two convolutional layers and five GRU layers where the size of most GRU weight matrices is 1600 × 1600. When running on an Intel Skylake-based server [21], the FLOP and execution time of MV-mul take 86.8 and 88.6 percent of the total (see Fig. 1). The profiling results in the latest GPUs [22] are also similar.


Fig. 1.
The FLOP and execution time breakdown of Deep Speech 2 [2] in an Intel Skylake-based server [21].

Show All

Single GRU layer has six matrices (three for processing an input vector and the other three for processing the vector computed in the previous time step), and the size of each matrix is mostly 1600 × 1600 with DS2. Assuming that the size of each element is 16 bits [11], the size of a GRU layer is 30.7 MB. When we perform whole RNN inference, it executes five GRU layers and needs 145 MB (the size of three matrices of the first GRU layer is slightly smaller than the other ones) of storage for weight matrices, which is too large to fit in the on-chip memory (mostly up to several MBs) of mobile/IoT devices. In MV-mul, each vector element is reused by the number of rows within the weight matrix, whereas each weight element is used only once. The on-chip memory size of processors grows over time, but the aggregate size of weight matrices increases as well for better recognition quality (see Fig. 2). Therefore, the weight elements, which cannot be reused for RNN inference, must be fetched from main-memory DRAM devices.1


Fig. 2.
The aggregate size of weight matrices and word error rate (WER) of representative ASR models over time [2], [3], [23], [24], [25], [26], [27]. Not all models use the same data set.

Show All

From an energy efficiency perspective (i.e., J/b), reading data (several to dozens of bits) from a DRAM device requires orders of magnitude more energy than performing a multiply-accumulate (MAC) operation or reading the same amount of data from an SRAM with few KBs of capacity [10], [12]. From a performance perspective, the bandwidth of off-chip DRAM devices is much lower than that of on-chip memory in a processor, making the execution time of MV-mul limited by off-chip DRAM bandwidth. Therefore, it is critical to reduce the energy consumption and increase the bandwidth of main-memory DRAM accesses in improving the energy efficiency and performance of RNN inference.

2.2 How to Improve the Energy Efficiency and Bandwidth of DRAM Accesses in MV-mul
The best way to improve the energy efficiency of multiplying a large matrix (located at main-memory DRAM) with a vector is to perform all or most of MV-mul within DRAM dies. If a processor performs MV-mul by fetching weight elements from a separate main-memory DRAM device, inter-die communication consumes most of operating cost (data transfer energy) and capital cost (large silicon/packaging area, such as pins, pads, and TSVs) regardless of the ways interconnecting processor and DRAM dies, such as conventional package, silicon interposer (2.5D [28]), and face-to-face die stacking (3D) using TSVs [29].

If we perform MV-mul within a DRAM device but close to its inter-die I/O, we can save inter-die communication energy. However, this does not provide any performance gain unless we utilize multiple DRAM dies within a memory rank [13]. A way to achieve an additional performance benefit in MV-mul is to exploit a structural unit within a DRAM device, bank. A DRAM die has several to few dozens of banks, where each bank can operate independently and concurrently. Therefore, by conducting MV-mul in the edge of bank's datapath I/O, we can improve both performance (multiplied by the number of banks) and energy efficiency (by not dissipating energy for moving weight matrices across banks, which is a significant portion of DRAM read energy according to [30]).

Performing MV-mul on each DRAM bank saves the data transfer energy of weight elements; but, if the vector elements are from a processor, they should traverse farther through inter-die I/O and inter-bank datapath to reach the DRAM bank. Hence, one might regard that performing MV-mul on a bank provides no energy saving. However, as described in Section 2.1, 1) vector elements are reused by the number of matrix rows and 2) a vector is much smaller than a weight matrix (by the number of matrix rows), so we can save most of the energy for fetching a vector by providing a small storage per bank to store the vector elements.

Still, there exists an important feasibility issue in performing MV-mul at all the banks of a DRAM device. DRAM banks can operate in parallel; however, their operations are limited by various timing constraints (more details in Section 2.3). This limitation is because all banks share one or few inter-die I/O paths (called channels), which determine the maximum rate of meaningful activity (related to the data store, retrieval, and retention, which are the primary purposes of DRAM as main memory). Therefore, a DRAM die has a power limitation, which determines the design of its power delivery networks, such as the number of power and ground pins and the density of on-chip power/ground wires.

The cost of DRAM is very sensitive to this power limitation because a DRAM device uses few (typically two to three) metal layers, and hence populating more power/ground wires and pins would significantly increase fabrication cost and die size. If all the DRAM banks perform bursty data read operations for MV-mul, a DRAM device can dissipate power exceeding the aforementioned power limit. For example, on the mobile LPDDR4 DRAM device we use as a baseline, concurrently reading data from all DRAM banks to their global-I/O (GIO) sense amplifiers (SAs) would dissipate 1.7× of its power limitation. As an edge device executes applications other than RNNs as well, it is a huge waste of precious DRAM resources to design a DRAM device with a higher power budget only for MV-mul. Therefore, it is desired to perform MV-mul under the maximum power budget determined by normal DRAM operations.

2.3 LPDDR4 DRAM Organization and Operations
It is necessary to understand how a DRAM device is organized and operates to analyze its power constraint. An LPDDR4 DRAM device [31], which is the baseline memory used in this paper, is composed of two memory channels, each consisting of multiple banks. Multiple LPDDR4 dies can be connected (e.g., through die stacking) to form a multi-rank package with two channels. Each bank consists of a 2D array of DRAM cells; each DRAM page (often called row) is controlled by a wordline (WL), and each column is connected to local bitlines (BLs). The sequence of a DRAM read is as follows. 1) Activation (ACT): the cells of a selected DRAM page in a bank driven through a WL (typically 16K cells for LPDDR4) share charges with the corresponding BLs, and bitline sense amplifiers (BLSAs) detect the small voltage difference of BLs due to charge sharing and amplify it. 2) Read (RD): a portion of column bits (typically 256 bits for LPDDR4) of data latched in the BLSA are transferred to GIO SAs through the global I/O. The data is amplified again in the GIO SA and transferred to the I/O multiplexer (mux) through inter-bank datalines and leaves the DRAM die.

Reading the data in another column of an activated page needs only RD. To access data at a page other than the currently activated page in a bank, we should precharge BLs and BLSAs (PRE) prior to ACT and RD. The minimum time interval from ACT to PRE is tRAS, the time to restore data to the cells. Each bank can operate independently, but multiple banks cannot perform RD or WR (data retrieval) at the same time because banks share inter-bank datalines and command/address lines. Therefore, RD can be issued at a minimum tCCD interval to prevent a conflict in the inter-bank datalines. The time interval between two ACTs within a channel is limited to tRRD (row to row activation delay). ACT can be performed simultaneously on multiple banks, but only four ACTs within tFAW (four activation window) can be issued due to the power limit. If each channel of DRAM performs four ACTs in tFAW and multiple RDs at tCCD interval, DRAM reaches its peak power consumption level.

As explained in Section 2.2, if we perform a DRAM read but omit inter-bank and inter-die datapath by supporting the data to the MAC units next to GIO SAs, we can save energy. According to the detailed analysis later explained in Section 4.1, we can conduct up to four RDs simultaneously when performing MV-mul within DRAM banks without exceeding the peak power consumption level.

SECTION 3MV-mul in DRAM
3.1 Exploiting Quantization and Sparsity in RNN's Matrix Elements
Under this DRAM power constraint, to further reduce the total amount of computation, we exploit a characteristic of RNN that a significant portion of its weight elements is redundant. As mentioned in [11], even if we turn a majority of the weight elements of an RNN into zeros, the impact of this pruning on the RNN inference is negligible. We verify this by conducting an experiment on DS2 [2], whose network model size is specified in Section 2.1 with LibriSpeech dataset [32], following the pruning strategy explained in [33]. [33] prunes down the values below certain thresholds into zeros, after every few training epochs. From the experiment, we observed no significant surge in word/character error rates with the sparsity (the portion of zero values within a matrix) of the matrices being 25, 50, and 75 percent.

When we compare a sparse matrix format (specifying non-zero elements (NZs) with their positions) with a dense matrix format (listing all the matrix elements), if sparsity is higher than (position bits)(data bits + position bits), the sparse matrix format represents a matrix with fewer bits. Because reading weight matrices dominates the energy and the execution time of MV-mul, it is beneficial to exploit the sparse matrix formats for matrices whose sparsity values are above this threshold. There are several formats for representing a sparse matrix, exemplified by Compressed Sparse Row/Column (CSR/CSC), Coordinate (COO), and Diagonal (DIA) [34].

The distribution of NZs in a matrix determines the size efficiency (bytes per NZ) of a specific format. For RNN applications such as DS2, the NZs of the weight matrices are relatively uniform-randomly distributed (see Fig. 3). In this case, there is no significant difference in the bytes per NZ between sparse formats except for the formats favoring special distributions. In this paper, we propose delta encoding, a variant of CSR.

Fig. 3. - 
Distribution of the distance between two adjacent non-zero values (NZs) in the matrices of GRU layers (75 percent sparsity) in Deep Speech 2 [2]. More than 98 percent of NZs have a distance of less than 15.
Fig. 3.
Distribution of the distance between two adjacent non-zero values (NZs) in the matrices of GRU layers (75 percent sparsity) in Deep Speech 2 [2]. More than 98 percent of NZs have a distance of less than 15.

Show All

CSR stores the absolute column indices of NZs and the number of NZs per row, together with the NZ values. By contrast, our delta encoding stores the column distance from the previous NZ as an index to represent the matrix with fewer index bits in storing (data, index) pairs (see Fig. 4). Also, instead of storing the number of NZs per row, we reserve the maximum value which can be represented by an index to specify the end of a row. When the index indicates the end of a row, the corresponding data field stores the absolute address of the row. For example, if we dedicate four bits per index, we define 0xf (the maximum distance) as the end of a row and use the values from 0 x 0 to 0xe as the distance between two NZs. If the distance is greater than 0xe, we place one or more dummies (whose data value in the (data, index) pair is zero).

Fig. 4. - 
(a) An exemplary sparse matrix and (b) its representation using the delta encoding format. dN denotes the N-th non-zero value of a matrix in row-major order. Each row in the representation table includes up to one row of the sparse matrix.
Fig. 4.
(a) An exemplary sparse matrix and (b) its representation using the delta encoding format. dN denotes the N-th non-zero value of a matrix in row-major order. Each row in the representation table includes up to one row of the sparse matrix.

Show All

In addition to exploiting sparsity through pruning, we also apply quantization for a further reduction in size representing the matrix. When we applied both pruning and quantization to DS2, we observed that reducing the precision of data till 12 bits incurs no noticeable degradation in inference accuracy, which is consistent with the results reported in [11].

In our sparse matrix format, the frequency of dummy values added depends on the size of index bits and the sparsity of a matrix. We study an optimal index bit minimizing the total size of a matrix that is formatted at each sparsity value using the GRU layers of DS2. The experiment shows that using (12 bits, 4 bits) for (data, index) pairs is optimal for the sparsity range of 65 to 80 percent, where the portion of dummy pairs is less than 5 percent (see Fig. 5). As a result, we assume that a (data, index) pair is (12 bits, 4 bits), and matrices are 1600 × 1600 with a sparsity of 75 percent by default in this paper.

Fig. 5. - 
The storage requirement for a matrix with different index bits when using our sparse matrix encoding format (delta encoding). The storage requirement is compared to that of a dense matrix without pruning.
Fig. 5.
The storage requirement for a matrix with different index bits when using our sparse matrix encoding format (delta encoding). The storage requirement is compared to that of a dense matrix without pruning.

Show All

To verify the effectiveness of our delta encoding format on the sparse matrices, we compared the size of sparse matrices encoded with various sparse formats. From the experiment, the delta encoding format reduces the size of matrices by 32.7 percent compared to the CSR format for the DS2 dataset with the sparsity of 75 percent (see Fig. 6).


Fig. 6.
The relative size of the tested matrices encoded with various sparse matrix formats normalized to the size of the dense format [34]. We set the size of the dense format as the baseline per matrix. The matrices are categorized into two types: ones synthesized with the given sparsity values under a uniform random distribution and the other collected from the real dataset (a GRU layer of DS2), which has a sparsity of 75 percent. Our delta encoding format gives the smallest size per NZ among all the tested formats in our target sparsity ranges.

Show All

By applying quantization and sparse formatting, the total size of weight matrices in the DS2 decreases, but it is still 36 MB, which would not fit on-chip caches of the mobile devices. Therefore, it is effective to perform MV-mul in DRAM; even if the size of matrices and the (data, index) pairs would vary as ASR applications evolve, we believe that the qualitative observation and analysis of our proposed MV-mul accelerator would stay unchanged.

3.2 The Operation Sequence of MV-mul in DRAM
To perform MV-mul within DRAM banks, we should locate a weight matrix and an input vector within banks capable of MV-mul, called MV-banks. As we can conduct four RDs simultaneously within banks due to the power constraint, each channel in a DRAM device has four MV-banks. So the weight matrix is divided into four sub-matrices and distributed to the MV-banks. As the input vector is reused and much smaller than the matrix, we duplicate the input vector to each MV-bank equipped with a dedicated SRAM (iv-SRAM). An alternative is to use another DRAM bank or another row in the same DRAM bank for storing the input vector. However, when processing sparse matrix-vector multiplication, accesses to the input vector are not sequential (consecutive). Therefore, it is required to access a larger size of DRAM data than is needed for SRAM because the granularity of a DRAM read (256 bits) is much coarser than that of an SRAM read (12 bits); it is not efficient in terms of throughput and energy.

The power and area overheads of iv-SRAM depend on the number of entries and the size of each entry. The input vector is stored in a dense format, making iv-SRAM sized to hold 1,600 entries, each storing 12-bit value. Its area is equivalent to 2.07 MB of DRAM cells (1.61 percent of a 128 MB DRAM bank), and it dissipates just 1.9 percent of the total power for MV-mul. A more detailed power, area, and timing analysis is described in Section 4.1. Each MV-bank also has a separate SRAM to hold an output vector (ov-SRAM), where each element is 24-bit long. In our case of ov-SRAM with 400 entries, the area corresponds to 0.3 MB DRAM cells, and its power consumption is negligible because ov-SRAM has low utilization.

The main operation sequence of MV-mul is to 1) write an input vector to the iv-SRAM in MV-banks, 2) perform MV-mul in each MV-bank by sequentially performing the inner-product computation, and 3) read the generated output vector from the ov-SRAMs. We assume that the matrix elements are already stored in each MV-bank in the form of our sparse matrix format. The detailed operation sequence is as follows. First, writing the input vector is processed through broadcasting, because all iv-SRAMs in a DRAM device need the same input vector. The input vector from the processor is sent to each bank and stored in iv-SRAM simultaneously.

The core inner-product computation sequence in each MV-bank is as follows. Data reading reads data from an activated DRAM page. An LPDDR4 device stores 256 bits in GIO SAs in one read. For the size of a weight element being 16 bits, there are 16 elements per DRAM RD. Index decoding process is required as the weight element is encoded using a sparse format. The index refers to the relative column distance from the previous NZ location minus one. Therefore, we can find the absolute address needed for fetching a proper input vector element by accumulating the index values. The index of the first element represents the index of the first NZ data. Moreover, a 4-bit compare logic is required to identify the row end index (index value 0xf). Input vector fetching is performed using the absolute addresses, each pointing to one of the 16 weight elements. iv-SRAM serves 16 weight elements for decoded addresses within tCCD. MAC executing process conducts inner product through MAC units populated within an MV-bank. Because 16 weight elements and vector elements are fetched every tCCD, 16 MAC operations must be performed. We designed and compared various MAC units with different numbers of pipeline stages (more details in Section 4.1). We identified that populating 16 MAC units having a cycle time of tCCD is better than populating fewer MAC units with a shorter cycle time (through aggressive pipelining) in power and area perspectives. Therefore, we place 16 MAC units next to the GIO SAs of an MV-bank. Output storing process stores the result of the MAC operation in the ov-SRAM only when the row end index exists at the decoded index through the index decoder. The output store method depends on how the matrix elements are mapped to DRAM pages, as described below.

When MV-mul is finished in all the participating MV-banks, the results in ov-SRAM are transferred to the processor. The activation function is done by the processor; because the output vector is small in size, it gives little benefit of implementing the function within a DRAM die.

There are multiple ways to map a weight matrix represented with our delta encoding format to DRAM pages. As mapping changes, organization and operation sequence change accordingly. Considering that, we choose a mapping that we call Single-Row per Read (SR), where elements from a single DRAM read (256 bits) belong to one matrix row (see Fig. 7).


Fig. 7.
Mapping of the weight matrix to DRAM reads and pipeline stages for Single-Row per Read.

Show All

The elements fetched on GIO SAs belong to one matrix row so that index decoding can be done sequentially using one accumulator or through parallel prefix sum. It is not possible to sequentially decode 16 indices within tCCD using one accumulator, so we use a 6-level parallel prefix sum unit. The column indices from the parallel prefix sum unit are used to fetch the input vector values from the iv-SRAM. Each MAC unit accumulates the product of an input vector element and a weight element until the DRAM read includes the end of a matrix row. If the current DRAM read includes the end of a row, the partial sums stored in all the MAC units of an MV-bank are accumulated through an adder tree, which takes 5 ns (same as tCCD).

SR is inefficient in that on average a half of weight elements in a DRAM read (8 for LPDDR4) are padded with zero values per matrix row (performance degradation about 2 percent compared to an ideal configuration where MAC units are fully occupied in a 1,600 × 1,600 matrix). So we may consider other mapping types where the elements from a single DRAM read might belong to different matrix rows. However, from the perspective of implementation, the latter mapping types require higher hardware complexity. Hence, we choose SR as a mapping of our delta encoding format.

In utilizing this MViD architecture, we should consider how a processor controls MViD. For a DS2 matrix we use for evaluation, 1.25 MB of matrix data distributed in each bank must be read and multiplied with an input vector. If all the DRAM RD commands for operations are transferred from the memory controller (MC), it will cause a huge CA bus utilization overhead. Therefore, a single MV-mul command, including the size of the initial address and the weight matrix is passed to the MV-bank to perform all MV-mul operations.

MV-banks process MV-mul through taking commands that write/read the vector to/from the input/output SRAM (WR-iv, RD-ov). To define these two commands, we used the reserved-for-future-use (RFU) commands in the LPDDR4 specification [31]. WR-iv command broadcasts the same input vector to all MV-banks. The input vector broadcasted through inter-bank datalines is simultaneously written to the iv-SRAMs in the MV-banks. Once the broadcast is over, MV-mul can be started. Therefore, there is no separate DRAM command for starting MV-mul. RD-ov should be processed after the entire MV-mul operation is completed. Because conventional DRAM interfaces do not have a mechanism to notify the end of a specific operation to MC, the primary device in the bus interface, the MC periodically sends RD-ov to check if MV-mul is completed (polling). This requires the MV-bank control unit (MCU) checking the status of the MV-bank; MCU is located close to the inter-die I/O within a DRAM device. If MV-mul is over, MCU returns a predefined value for RD-ov. Then, the MC issues RD-ov against to retrieve the output vector values.

3.3 Concurrently Serving Requests from Processors and Performing MV-mul in DRAM
A computer system that performs RNN inference can simultaneously run other applications. Therefore, the performance of the other applications can be drastically reduced if MV-mul operations within DRAM block the other memory requests. For example, in a system stacking four LPDDR-3200 dies each with eight MV-banks, the minimum time required to read 1.25 MB of data and perform MV-mul is 6.125 μs if all MAC units are used for one MV-mul. If DRAM devices do not accept another memory request during the MV-mul operation, the processor would wait for the main memory response at least for 6.125 μs. Therefore, a DRAM device with MV-banks must provide a mechanism to service other memory requests while performing MV-mul.

Careful coordination is required for DRAM to serve both MV-mul and memory requests from a processor (processor requests) concurrently. Because performing MV-mul in MV-banks dissipates the maximum DRAM power, this power limit must be considered to serve processor requests. Moreover, row-buffer conflicts, which occur due to a request to DRAM row different from the active row, must be considered if a processor request heads to an MV-bank that is actively performing MV-mul.

Method for Achieving Concurrency: We achieve the coordination by sending a single MV-mul command (composed of WR-iv and RD-ov explained in Section 3.2) to the DRAM and by controlling the progress of MV-mul inside the DRAM. This coordination requires control generator units (CGUs) to convert a single MV-mul command into a sequence of fine-grained sub-commands. A CGU is located at each MV-bank and generates DRAM ACT, PRE, and RD commands to read weight elements, compute vector indices, and perform MAC operations, obeying DRAM's timing constraints.

CGUs must slow down or pause the progress of MV-mul within MV-banks to serve processor requests. When a processor request reaches a non-MV-bank, the DRAM device should secure enough power budget. This can be achieved by slowing down the progress of MV-banks. According to our analysis (whose methodology explained in Section 4.1), slowing down all MV-banks into half of their original rate provides a sufficient power budget to process the processor requests with half of the rate a DRAM device can serve the requests. It can be achieved by increasing both tCCD and tRRD twice of their original values. By contrast, when a processor request reaches an MV-bank, we need to slow down all MV-banks and also pause the target MV-bank to deal with the row-buffer conflict.

Implementing Commands for slow-Down and Pause: If a MC sends separate slow-down and pause commands prior to all memory requests, this would cause a huge CA bus utilization overhead. We alleviate this overhead by embedding the slow-down and pause information in existing DRAM commands.

First, the transmission of any normal DRAM command (i.e., RD, WR, ACT, and PRE) can be a signal to notify slow-down to MV-banks as all MV-banks must be slowed down to process normal DRAM commands to non-MV-banks. Therefore, if MV-banks are not in a slow-down state when MCU receives any normal DRAM command, it broadcasts the slow-down command (SD) to the CGUs of all MV-banks and then sends the original DRAM command to the target bank. The timing of the original DRAM command should be increased by 3 tCK due to command decoding (1 tCK) and SD broadcast (2 tCK). This timing adjustment is only needed when none of the MV-banks are in a slow-down or pause state.

Second, PRE can be augmented to notify the target MV-bank to pause. To process a request from a processor on an MV-bank actively processing MV-mul, we must precharge the currently active DRAM row to resolve this row-buffer conflict. Therefore, we define a pause PRE command (p-PRE) by modifying PRE and use it to pause an MV-bank prior to PRE. The timing (latency) of p-PRE should be different from that of PRE (tRP) because a MC does not exactly know what operation is being performed in the MV-bank when it sends p-PRE. For example, if p-PRE is sent while the target MV-bank is activating a row for MV-mul, the precharge operation must be delayed by up to tRAS because the MV-bank must restore the row being activated. Therefore, in order not to violate the internal DRAM timing constraints, the timing of p-PRE must be at least tRAS+tRP (= tRC).

The worst case for p-PRE is when the target MV-bank just started PRE for MV-mul. There are two options for CGU in this case: one is to stop MV-mul and to directly enter the pause state when PRE is finished, and the other is to perform the planned ACT followed by RD for MV-mul and then PRE for pause. The former favors a processor request as the time of p-PRE can be tRC. The latter favors the progress of MV-mul, at the cost of a higher timing value of p-PRE being tRC+tRP. We prefer the progress of MV-mul and set the timing of p-PRE to tRC+tRP, leaving the quantitative comparison of these two options as future work.

After all processor requests are processed, MV-banks must resume or return to the original processing throughput (speed-up). Because PRE is issued at the end of serving requests from a processor, we leverage PRE once again to notify speed-up (s-PRE) and resume (r-PRE). MC checks the state of each MV-bank and sends s-PRE or r-PRE command to MCU when speed-up or resume is needed. When MCU receives s-PRE, it sends the speed-up signal to the CGUs of all MV-banks and then sends PRE to the target bank. When MCU receives r-PRE, it is passed to the CGU of the target MV-bank, and the CGU resumes MV-mul after precharge. In LPDDR4, PRE has three unused CA bits; we use these to distinguish PRE from p-PRE, s-PRE, and r-PRE.

Policies for Slow-Down and Pause: If we slow down or pause MV-banks every time MC receives a request, the throughput of MV-mul would be deteriorated significantly. To prevent such a throughput drop, MC checks the memory request queue at every time interval (tIV). When the aggregate number of pending requests over multiple tIVs surpasses a certain threshold (nTH), MV-mul is slowed down or paused. In determining whether to slow down or pause, MC checks the following metrics: an aggregated number of memory requests to normal banks (num_req_nonMV), and an aggregated number of memory requests to each MV-bank (num_req_MV[n]). The policy that we propose is:

num_req_nonMV ≧ nTH → slow down all MV-banks.

num_req_MV[n] ≧ nTH → pause the n-th MV-bank and slow down the other MV-banks.

When a pause happens, we also slow down all MV-banks that are not paused due to the power budget (check a detailed analysis in Section 4.1). Also, once an MV-bank enters pause or slow-down state, MV-mul resumes or speeds up after the MV-bank processes all requests that can be processed in that state. Thus, processor requests heading to the MV-banks that are already paused have no additional delay due to the p-PRE command.

We use the same nTH for both slow-downs and pauses, making pauses less frequent than slow-downs when memory requests are evenly distributed among banks (which are common cases). This design choice is reasonable as a pause is more costly than a slow-down as a sequence of PRE and ACT should follow a pause to resolve a row-buffer conflict.

We accumulate num_req_nonMV and num_req_MV[n] over multiple tIVs, not resetting them at the end of every tIV. This accumulation prevents a MC from waiting until MV-mul is finished when few requests (less than nTH) from a processor form dependency on future requests (forming a deadlock). The frequency of slow-down and pause can be controlled by changing nTH and tIV. Large nTH and tIV help MV-mul finish earlier at the cost of a longer tail latency for requests from a processor, and vice versa for small nTH and tIV. We can adaptively control both nTH and tIV values to either favor MV-mul over processor requests or vice versa.

3.4 Put It All Together: MViD Architecture
Putting it all together, we propose MViD architecture, as illustrated in Fig. 8, whose design decisions are made through a series of design space exploration made in the previous sections. We populate an MCU close to the inter-die I/O within a DRAM device, which orchestrates the whole MV-mul process. 16 MAC units are placed near GIO SAs in four banks, as only up to four concurrent RD operations are permitted under the power constraint of the baseline LPDDR4 DRAM. Each of these MV-banks has a CGU, which takes commands and addresses that come to the MV-bank from the MCU and converts them to the fine-grained sub-commands. An iv-SRAM and an ov-SRAM are placed per MV-bank to store the input/output vectors. We used a parallel prefix sum logic for index decoding, and an adder tree to aggregate the output from the 16 MAC units.

Fig. 8. - 
The MViD architecture.
Fig. 8.
The MViD architecture.

Show All

The core inner-product computation part of MViD consists of five pipeline stages: data reading, index decoding, input data fetching, MAC executing, and output storing. First, 256-bit data are read from the DRAM bank and latched to GIO SAs, followed by the index decoding stage, which comes with parallel prefix sum and fetching stage, which loads 16 input elements from iv-SRAM. Then, MAC operations are executed, and the results are added up by an adder tree per weight matrix row, forming an output value, which is stored into an ov-SRAM. MCU and CGU deal with requests from a processor through slowing down or pausing MV-mul.

We augment LPDDR4 DRAM commands with p-PRE, s-PRE, r-PRE, WR-iv, and RD-ov to slow down or pause MV-banks, to upload an input vector followed by starting an MV-mul operation, and to check if MV-mul is over and then retrieve the output vector from ov-SRAMs. A CGU generates a repeated sequence of ACT, multiple RDs, and PRE for the aforementioned core computation. By contrast, an MCU manages slow-down and pause of MV-banks.

Because MV-mul operates within DRAM devices, not on a processor, the data transfers of the input/output vectors are controlled in a similar way to DMA between the processor and MViD. Data can be transferred to MViD by giving start address, data length, and operation type to the register allocated for MViD. The MC uses the commands described above to transfer the vector to multiple MV-banks and gather the output vector. When MV-mul is over, the MC sends an interrupt to the processor to indicate the end of MV-mul and transfers data to the processor. Besides, there is a cache coherence issue between the processor and MViD. We can deal with this by either 1) flushing cache lines that hold weight matrices prior to every MV-mul or 2) marking that memory region as uncacheable, which was also proposed by [14].

3.5 Additional Optimization Schemes
MViD architecture can be augmented with a bank partitioning scheme to enhance performance further. The progress of MV-mul is delayed by slow-downs and pauses of the MV-banks, and pauses are caused by the necessity of serving processor requests to the specific MV-banks. By exploiting the bank partitioning method [35], it is possible to store data of the non-MV-mul workloads from a processor in non-MV-banks, thereby preventing MV-banks from serving processor requests. Because no MV-bank is paused, MV-mul and processor requests can be served only experiencing slow-downs, leading to additional performance enhancement. In the perspective of memory capacity, only non-MV-banks can be used as storage by applying bank partitioning.

SECTION 4Evaluation
4.1 Power/Area/Timing Analysis
We analyzed the power, area, and timing for each major component of MViD (LPDDR4-3200 DRAM device, MAC unit, input decoder, adder tree, and iv-/ov-SRAM). We calculated the power and energy of DRAM based on the IDD specification of LPDDR4-3200.2 We designed a MAC unit, input decoder, and adder tree as Verilog, and synthesized them with 20nm DRAM process.3 We added 100 percent area redundancy to the result considering PnR (place and route). We set the cycle time constraint of MAC unit and SR components to 5 ns (200 MHz) so that it can operate within tCCD. We used CACTI [37] to model iv-SRAM with 1,600 of 12-bit entries and ov-SRAM with 400 of 24-bit entries.

Power and Energy. We define the maximum power of one DRAM channel as the power when bursty data reads occur consecutively with the maximum number (four) of ACTs every tFAW. The calculated maximum power is higher than the power of IDD5 (all bank refresh) and IDD7 (ACT-RD burst), which are the power-hungry IDD values in the LPDDR4 specification. We calculated the power of DRAM read for MV-mul (read only up to GIO SA) by referring to FGDRAM [30], and scaled it considering DRAM page size, the number of banks, and data path length. Within the maximum power, we can perform MV-mul simultaneously up to four banks per channel (see Fig. 9). Most of the MV-mul power of MViD is consumed for internal bank RD, and the power consumption of MAC, SRAM, and other components account for 2.7, 1.9, and 0.4 percent of the total power for MV-mul operation.


Fig. 9.
The breakdown of LPDDR4 peak power and MViD power consumption; LPDDR4 consumes maximum power with consecutive RDs and concurrent ACTs obeying the tFAW constraint.

Show All

We cannot process DRAM requests from a processor with original DRAM timing values because there is a limited power budget for normal DRAM commands even if MV-banks are slowed down or partially paused. When four MV-banks are slowed down to half the frequency, four MV-banks consume 300.8 mW, so that the available power budget is 337.0 mW. By doubling tCCD and tRRD, the maximum power required for normal DRAM commands is 326.2 mW, which is operable within the given power budget. When two MV-banks are paused, and the other two MV-banks are slowed down to half the frequency, these four MV-banks consume 163.4 mW, resulting in a power budget of 474.4 mW. When tRRD is set to double, the maximum power required for normal DRAM commands is 441.7 mW, which is also operable within a given power budget.

The energy for DRAM RD is the largest among the energy required for each component to perform MV-mul in MViD, as summarized in Table 1. The energy required for internal bank RD is 2.54 pJ/b, so 650.24 pJ is required for 256-bit read operation. The energy per MAC operation is 1.31 pJ/op, and 20.98 pJ is required for every DRAM RD because 16 MACs operate for processing 256-bit data. Each MAC operation requires access to an iv-SRAM, 1.17 pJ/acc × 16 acc = 18.72 pJ being required for 256-bit data. The input decoder is activated once for each DRAM RD, so 1.23 pJ/read is used. Moreover, the ov-SRAM and the adder tree require 0.53 pJ/acc and 1.39 pJ/op, respectively, which occupies a small fraction of the total energy because only one write is required per matrix row.

TABLE 1 Energy/Area/Timing of MViD Components
Table 1- 
Energy/Area/Timing of MViD Components
Area and Timing. The area of a MAC is 6,318 μm2, and the area of an iv-SRAM and an ov-SRAM is 30,048 and 3,931 μm2, respectively. The areas of the input decoder and the adder tree are 5,067 and 6,071 μm2. In the 8 Gb LPDDR4 die, the area overhead of the MViD components required for half of the entire bank is equivalent to 3.69 percent of the die size. MAC and SR components can operate once per tCCD (5 ns). Doubling the frequency of a MAC (400 MHz) can reduce the number of MACs by half, but it is not used because the area increases by more than twice in the synthesis result. Due to DRAM internal frequency constraints, we use iv-SRAM4 with 1.25 ns of cycle time to access 16 times within tCCD. Also, the cycle time of ov-SRAM is 2.5 ns because it requires only one access within tCCD.

4.2 Performance/Energy Evaluation
We simulated a chip-multiprocessor system to evaluate the performance of MViD. We modified McSimA+ [39] with the default parameters summarized in Table 2 for simulation. We measured the performance of DS2 by applying the effect of MViD on MV-mul based on DS2 data in Fig. 1 measured on a real machine. MViD can perform MV-mul in four MV-banks per rank.5

TABLE 2 Default Simulation Parameters

In our evaluation, we used the SPEC CPU2017 [40] benchmark suite for multi-programmed workloads running with MV-mul of DS2. Although SPEC CPU2017 is not a representative mobile workload, it is good enough to understand how MViD is affected by the memory intensity of host workloads. Using Simpoint [41], we extracted a representative simulation point of each application, consisting of 100M instructions. Each multi-programmed workload consists of two applications, mix-high (mcf, lbm) composed of memory-intensive applications, mix-low (exchange2, imagick) composed of memory non-intensive applications, and mix-med (xz, xalancbmk) composed of applications in-between.

We quantified the performance and energy efficiency of MViD by comparing it with the baseline configuration equipped with LPDDR4 DRAM in various conditions, through which we made the following key observations.

First, MViD provides a significant benefit in both performance (speed-up) and energy efficiency (energy-delay product (EDP)) across a wide range of matrix sparsity in MV-mul. Fig. 10 shows the speed-up and relative EDP of a single-rank MViD when only DS2 is performed in the evaluated system. MV-mul of DS2 is performed in either a processor (Base) or MV-banks (MViD) using the dense (-Dense) or sparse (-Sparse) format. We used matrices from a pre-trained model of DS2 [42]. The size of a matrix is independent of sparsity in Dense, and so of the performance. By contrast, as sparsity increases, the size of the matrix of Sparse decreases leading to better speed-up. Sparse outperforms Dense for a sparsity higher than 25 percent. Executing DS2 in MViD performs about 2.4× on average better than DS2 in Base across a wide range of sparsity because the performance of MV-mul is mostly proportional to memory access bandwidth. Considering energy efficiency, even if the power consumption of MViD is larger than that of Base, EDP on DS2 is improved about 6.5× on average better than Base as the execution time of MV-mul is reduced significantly.


Fig. 10.
The impacts of MViD on DS2 by changing sparsity in (a) performance and (b) energy-delay product (EDP), when DS2 is performed alone. We set the result of Base-Dense as the baseline.

Show All

Second, MViD still provides a substantial performance gain even when applications from non-MV-mul workloads are executed concurrently. Fig. 11a shows the relative DS2 throughput and the relative aggregate IPC of non-MV-mul workloads while changing the number (1, 2, and 4) of ranks. The RNN weight matrix of DS2 has a sparsity of 75 percent. We set nTH and tIV to 4 and 4 tCK, respectively. For each workload, we set the result of Base with one-rank as the baseline.


Fig. 11.
The impacts of MViD when DS2 is running solely or with non-MV-mul workloads (mix-low, mix-med, and mix-high). MV-mul of DS2 is performed in either a host processor (Base) or MV-banks (MViD). For each workload setup, we measure the relative throughput of DS2 and the relative IPC of the non-MV-mul workload, normalized to the throughput of DS2 and the non-MV-mul workload each running alone on the processor with one rank. (a) We change the number of ranks (1R, 2R, and 4R) and (b) we change nTH while running DS and mix-high on an one-rank MViD.

Show All

When using one DRAM rank, MViD performs 2.7×, 2.4×, and 2.2× better than Base in DS2 throughput when running with mix-low, mix-med, and mix-high. DS2 performs slower because MV-banks are slowed down or paused due to the requests from a processor. When running with memory non-intensive workloads, MV-banks do not experience frequent disturbance by the requests from a processor, and hence provide the performance gain similar to the case of running DS2 only. As memory intensity of the non-MV-mul workloads increases, the throughput of DS2 in MViD drops; however it is still higher than Base where both MV-mul in DS2 and non-MV-mul workloads compete the same limited off-chip DRAM bandwidth. MViD performs similar or better than Base for the non-MV-mul workloads. On mix-high, mix-med, and mix-low, the aggregate IPC of MViD is 32, 13, and 4 percent higher than that of Base. The increase in the aggregate IPC is different because compared to Base because it takes longer for the aggregate number of pending requests in a MC to exceed nTH.

As we populate more DRAM ranks, DS2 throughput of MViD increases significantly, whereas that of Base stays largely unchanged. This is because the aggregate bandwidth on MV-banks of MViD is proportional to the number of ranks, but the off-chip bandwidth of Base is not changed. For example, DS2 throughput of MViD is 5.7× higher than that of Base while running with mix-med. DS2 throughput of Base increases slightly as more ranks are populated because more banks lead to lower row-buffer conflict rates [35]. This reduction in row-buffer conflict rates makes a large impact on the aggregate IPC of the non-MV-mul workloads for Base. For MViD, populating more DRAM banks also increases the aggregate IPC of the non-MV-mul workloads, but not as sensitive as for Base. As the number of banks increases, the memory requests are distributed across the banks. Therefore, it takes longer for a specific MV-bank to have at least nTH requests because fewer requests reach a specific MV-bank for the same time interval. It makes a longer tail latency of non-MV-mul workloads compared to that with fewer DRAM banks.

Third, adjusting nTH is effective in trading MV-mul throughput with the performance of non-MV-mul workloads. Fig. 11b shows the relative throughput of DS2 and the relative aggregate IPC of a non-MV-mul workload (mix-high) as we change nTH on one-rank MViD. We set tIV as 4 tCK because tIV works in a way similar to nTH. When nTH is 1, any memory request heading to MV-banks that are not paused is delayed by tRC+tRP because a p-PRE command should be issued. Still, the aggregate IPC of the non-MV-mul workload of MViD is 35 percent better than that of Base because non-MV-mul workloads can utilize more aggregate off-chip memory bandwidth when MV-mul is performed on MViD instead of a processor.

As nTH increases, the aggregate IPC of the non-MV-mul workload is decreased. More requests must be queued up in MC to pause an MV-bank; even if an MV-bank is paused less frequently, the increased queuing delay leads to a longer tail latency in serving requests from the host processor. By contrast, the throughput of MV-mul is increased because MV-banks experience interference less often.

Fourth, bank partitioning ( MViD-P ) improves the performance of both MV-mul and non-MV-mul workloads. Bank partitioning eliminates row-buffer conflicts for MV-banks, so MV-mul operation is only slowed down but not paused. Even if the non-MV-mul workloads experience slightly more frequent row-buffer conflicts as they use fewer DRAM banks (lower bank-level parallelism), the cost of row-buffer conflicts to a non-MV-bank is not as high as that to an MV-bank as the latter often leads to a pause experiencing the delay of p-PRE (tRC+tRP). Therefore, DS2 throughput increases by 34 percent and the aggregate IPC of the non-MV-mul workloads (mix-high) increases by 9.5 percent on the one-rank MViD-P, respectively (see Fig. 12). With MViD-P on the four-rank, the aggregate IPC of mix-high on MViD exceeds that of Base whereas MViD provides 7.2× higher DS2 throughput compared to Base.


Fig. 12.
The impacts of bank partitioning (MViD-P) when DS2 is running with non-MV-mul workloads (mix-high). The configuration is the same as in Fig. 11a.

Show All

SECTION 5Discussion
Workload Variation. MViD is robust to changes in the matrix size of the workload. Our proposed MViD is optimized for the 1600 × 1600 matrices used by a representative end-to-end ASR application. As the model evolves, the size of the matrix used by the RNN may change, and hence, it is desired for MViD to maintain its performance benefit across a wide range of matrix sizes. We experimented with how the performance of MV-mul changes according to the matrix size (see Fig. 13). Even if the matrix size is reduced to 1/4 (when the number of rows and columns is halved each), the performance of MV-mul is 3.2 times higher than that of Base (only 5 percent less than the peak speedup of MV-mul). When the size of the matrix is larger than 1600 × 1600, the matrix is processed after being divided into sub-matrices, and a decrease in performance is no more than 5 percent as compared with the maximum speedup.

Load Balancing. The progress of MV-mul differs between MV-banks because each MV-bank experiences a different number of pauses due to interference by processor requests, whose distribution is both application and memory-address-mapping specific. This is a critical issue because MV-mul from all MV-banks should finish before the next phase (possibly another MV-mul) starts (forming a strong dependency). Inter-MV-bank load balancing could alleviate this problem. For this, there are two factors to consider, the NZ distribution between MV-banks and the pauses caused by processor requests. However, when we analyzed the weight matrices of various RNN models, the difference in the number of NZ values across MV-banks was within 2 percent. Also, the memory address mapping makes the processor requests quite evenly distributed over DRAM banks, resulting in the difference between the number of pauses of each MV-bank within 1 percent. Therefore, we did not devise a particular inter-MV-bank load balancing scheme.

Fig. 13. - 
The impacts of one-rank MViD on MV-mul by changing the size of the weight matrix. We set the result of Base with one-rank as the baseline.
Fig. 13.
The impacts of one-rank MViD on MV-mul by changing the size of the weight matrix. We set the result of Base with one-rank as the baseline.

Show All

SECTION 6Related Work
Near-Data Processing. There has been a large body of research that places processing units closer to the main memory. NDA [14] improved computation throughput using a 3D-stacked accelerator die on top of commodity DRAM dies. Chameleon [13] integrated NDA into the separate data buffer chip of LRDIMM (Load-Reduced DIMM). However, these did not specifically target NNs.

TETRIS [15] implemented processing units on a logic die of 3D-stacked memory for better throughput and energy efficiency. TETRIS also reduced memory accesses by performing simple accumulation closer to DRAM banks. However, they did not fully utilize the internal DRAM bandwidth. McDRAM [16] is closest to MViD in that it placed 2,048 MAC units within a DRAM die for accelerating MLPs and RNNs. However, McDRAM did not consider the sparsity of the matrices, the power constraints of the DRAM devices, and concurrency in accessing DRAM from processors while performing MV-mul. CHoNDA [43] considered concurrent host access during near-data processing. However, CHoNDA lacked a detailed analysis of the power generated by processing data within the DRAM.

Computational Optimization of RNN. Persistent RNN [44] alleviated the memory bandwidth bottleneck of LSTM by storing a portion of weight elements of an LSTM layer in GPU's on-chip memory. Sparse Persistent RNN [45] extended [44], covering sparse matrices as well. It compresses a sparse matrix into a densely packed matrix. However, both are difficult to apply to mobile environments with limited on-chip memory capacity. Zhang et al. [6] reduced the amount of off-chip memory transfers by exploiting inter- and intra-cell parallelism of LSTM in a mobile GPU with limited on-chip memory, which is orthogonal to MViD.

NN Accelerators. Fowers et al. [4] exploited pipeline parallelism to utilize processing units effectively and to reduce the service latency of few batches by distributing RNN weights across on-chip memory of FPGAs. However, they assumed all data can fit in on-chip and did not consider the sparsity of the weight matrices. EIE [18] proposed a hardware model capable of sparse matrix-vector multiplication exploiting data pruning for MLP and fully-connected layers. ESE [11] proposed an optimized architecture for FPGA by applying pruning and quantization for LSTM models. Cambricon-X [19] suggested an accelerator architecture for a wide range of DNN models by supporting both pruned sparse matrices and dense matrices. Although EIE, ESE, and Cambricon-X are similar to MViD in that they accelerate sparse matrices, they suffer from the off-chip memory bandwidth bottleneck when MLP/RNN models do not fit in on-chip memory with limited capacity.

SECTION 7Conclusion
We have proposed MViD, a near-data processing architecture that accelerates matrix-vector multiplication (MV-mul) in RNNs by performing MV-mul with multiply-accumulate (MAC) units inside main-memory DRAM. MViD maximizes computational and energy efficiency by using a sparse matrix format and reducing weight precision through quantization. MViD populates the MAC units only on a portion of the DRAM banks considering the limited power budget of DRAM. We proposed an optimized sparse matrix format for MViD called delta encoding. To process requests from a processor concurrently with MV-mul, we allow MViD to slow-down or pause the progress of MV-mul. Furthermore, we solve the performance overhead caused by a pause in MV-mul through bank partitioning. Our evaluation shows that MViD improves the throughput of Deep Speech 2, an MV-mul dominant application, by up to 4.9× and 7.2× compared to the baseline system with four DRAM ranks when running Deep Speech 2 alone and with memory-intensive applications, respectively.