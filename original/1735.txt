Convolutional neural networks (CNNs) are a vital approach in machine learning. However, their high complexity and energy consumption make them challenging to embed in mobile applications at the edge requiring real-time processes such as smart phones. In order to meet the real-time constraint of edge devices, recently proposed custom hardware CNN accelerators have exploited parallel processing elements (PEs) to increase throughput. However, this straightforward parallelization of PEs and high memory bandwidth require high data movement, leading to large energy consumption. As a result, only a certain number of PEs can be instantiated when designing bandwidth-limited custom accelerators targeting edge devices. While most bandwidth-limited designs claim a peak performance of a few hundred giga operations per second, their average runtime performance is substantially lower than their roofline when applied to state-of-the-art CNNs such as AlexNet, VGGNet and ResNet, as a result of low resource utilization and arithmetic intensity. In this work, we propose a zero-activation-skipping convolutional accelerator (ZASCA) that avoids noncontributory multiplications with zero-valued activations. ZASCA employs a dataflow that minimizes the gap between its average and peak performances while maximizing its arithmetic intensity for both sparse and dense representations of activations, targeting the bandwidth-limited edge computing scenario. More precisely, ZASCA achieves a performance efficiency of up to 94 percent over a set of state-of-the-art CNNs for image classification with dense representation where the performance efficiency is the ratio between the average runtime performance and the peak performance. Using its zero-skipping feature, ZASCA can further improve the performance efficiency of the state-of-the-art CNNs by up to 1.9× depending on the sparsity degree of activations. The implementation results in 65-nm TSMC CMOS technology show that, compared to the most energy-efficient accelerator, ZASCA can process convolutions from 5.5× to 17.5× faster, and is between 2.1× and 4.5× more energy efficient while occupying 2.1× less silicon area.
SECTION 1Introduction
Deep neural networks (DNNs), especially convolutional neural networks (CNNs) [1], have received tremendous attention due to their ability to surpass human-level accuracy on a wide range of complex tasks such as recognition, classification and detection [2]. Depending on their size and complexity, these networks achieve different degrees of classification/recognition accuracy. A CNN is a stack of multiple convolutional layers followed by fully-connected layers: convolutional layers extract high level abstractions and features of raw data, whereas fully-connected networks are used to learn non-linear combinations of the extracted features. In 2012, a CNN called AlexNet [3] was introduced: it is constituted of 5 convolutional layers followed by 3 fully-connected layers and achieves 42.9 percent misclassification rate (MCR) on the ImageNet dataset. AlexNet contains 2.3M weights and 58.6M weights in its convolutional and fully-connected layers, respectively, performing 1332M operations (i.e., 666M multiply-accumulates (MACs)) in its convolutional layers, and 117.2M operations (i.e., 58.6M MACs) in its fully-connected layers. VGGNet-16 [4] is another well-known CNN, containing 13 convolutional layers with 14.7M weights and 3 fully-connected layers with 124M weights. VGGNet-16 performs 30.6G operations in its convolutional layers and 248M operations in its fully-connected layers, achieving 27 percent MCR on ImageNet. Recently, ResNets [5] achieved a lower complexity and a better MCR by using residual connections. For instance, ResNet-18 achieves a similar MCR to VGGNet-16 (i.e., 27.88 percent on ImageNet) while performing 3.6G and 1M operations in its 17 convolutional layers with 11M weights and 1 fully-connected layer with 1M weights, respectively. Moreover, ResNet-50, containing 49 convolutional layers with 23.5M weights and 1 fully-connected layer with 2M weights, achieved a better MCR (i.e., 22.85 percent on ImageNet) by going even deeper. ResNet-50 respectively performs 7G and 4M operations within the two types of layers. All these CNNs have won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [6].

Regardless of the fact that in almost all the aforementioned CNNs the majority of weights is found in fully-connected layers, the number of operations are dominated by convolutions. As a result, the processing time of CNNs is also dominated by the convolutional processes. This issue can easily be addressed by exploiting parallel processing elements (PEs) to increase throughput. However, a straightforward parallelization requires high data movement and bandwidth, leading to high energy consumption [7].

During the past few years, many convolutional accelerators with different dataflows have been introduced in literature [8], [9], [10], [11], [12], targeting edge devices. While these ASIC architectures can successfully reduce the number of memory access to the off-chip memory and meet the latency constraints of small CNNs such as AlexNet within the edge computing scenario, they fail to employ the full potential of their architectures due to the limited data bandwidth of edge devices, resulting in a low performance efficiency. In fact, their bandwidth bottleneck and network topology (i.e., dataflow) prevent them from providing the required parallelism for their PEs right after each access to the off-chip memory. Therefore, a great deal of time is wasted reading data from memories. As a result, there is a huge gap between their peak performance and average runtime performance. The peak performance is defined as the maximum achievable number of operations per second, roughly computed as
peak performance=2×#PE×f,(1)
View Sourcewhere #PE and f denote the number of PEs and nominal frequency, respectively, and where each PE performs a single MAC at each clock cycle. The runtime performance is also defined as the number of operations per second obtained when performing computations on a specific dataset.

Fig. 1 visually illustrates the above limitation of state-of-the-art accelerators (i.e., Eyeriss [8], EEPS [13] and DSIP [14]) using their roofline model that ties together their performance and arithmetic intensity. The performance of an accelerator is measured as the number of operations per second while the arithmetic intensity is defined as the number of operations performed on a byte of accessed data from the off-chip memory [15].


Fig. 1.
The roofline model of Eyeriss, EEPS and DSIP tested on AlexNet and VGG-16. The square points and dashed lines denote the average runtime performance and the roofline (i.e., the peak performance), respectively.

Show All

Fig. 1 shows that Eyeriss has the highest arithmetic intensity when performing the convolutions of AlexNet and VGG-16 and compared to EEPS and DSIP. However, there is a huge gap between its peak performance (i.e., the roofline denoted by a red dashed line) and runtime performance. We measure the gap as the ratio between the average runtime performance and the peak performance and refer to it as performance efficiency. Fig. 2 shows the performance efficiency of the aforementioned convolutional accelerators. The performance efficiency of Eyeriss varies from 26 to 55 percent when running AlexNet and VGG-16: from 45 to 74 percent of its potential, hardware resources and energy are wasted. On the other hand, EEPS and DSIP have a higher performance efficiency while having a poor arithmetic intensity. More precisely, these accelerators have only focused on maximizing one implementation aspect: either arithmetic intensity (e.g., Eyeriss) or performance efficiency (e.g., EEPS and DSIP) while an accelerator maximizing both aspects is missing in literature.


Fig. 2.
The performance efficiency of Eyeriss, EEPS and DSIP tested on AlexNet and VGG-16. Note that the performance efficiency of Eyeriss, EEPS and DSIP is taken from their original papers.

Show All

As a first attempt to increase the performance efficiency and the arithmetic intensity, we presented a new dataflow, called fully-connected inspired dataflow (FID), and its architecture in [16] for filter sizes fixed to 3×3. However, FID and its architecture suffer from lack of reconfigurability and are only restricted to VGG-like networks where their filter sizes are fixed to 3×3 for all layers. It is worth mentioning that reconfigurability is of paramount importance in designing a convolutional accelerator as filter sizes of each layer can be different from others. For instance, AlexNet consists of 5 layers of convolutions with filter sizes 11×11, 5×5 and 3×3.

Another key motivation for our work is exploiting the intrinsic sparsity among activations incurred by using the rectified linear unit (ReLU) as the non-linear function in order to avoid unnecessary MAC operations. More precisely, ReLUs dynamically clamp all negatively-valued activations to zero at runtime. As a result, the sparsity degree of each layer highly relies on the network and dataset being used. Fig. 3 shows the average sparsity degree of activations for AlexNet, VGG-16, ResNet-18 and ResNet-50. The sparsity degree of activations (i.e., the fraction of zeros in activations) suggests that noncontributory multiplications/additions can be avoided to speed up the convolutional computations. Moreover, reading zero-valued activations can be skipped during DRAM accesses as memory accesses directly contribute in total energy consumption of systems [17], [18]. It is worth mentioning that the sparsity among activations is an intrinsic occurrence in neural networks using the ReLU as their non-linear function. Therefore, there is no approximation involved in their computations and skipping the noncontributory computations does not incur any accuracy degradation.


Fig. 3.
The average sparsity degree of convolutional layers for AlexNet, VGG-16, ResNet-18 and ResNet-50.

Show All

Several recent convolutional accelerators specialized for the inference computation in the cloud have also investigated the sparsity among activations and weights to skip unnecessary computations. Cnvlutin [19] and Cambricon-X [20] exploit the sparsity among activations and weights mainly to speed up the convolutional computations, respectively. SCNN [21] introduced a dataflow to exploit the sparsity among both activations and weights to reduce computation cycles and energy consumption. Despite the speedup that these accelerators achieve over their baseline, their power consumption is far beyond the power budget of edge systems which is limited to a few hundred mW [22]. More precisely, these accelerators exploit the straightforward parallelism and require high data bandwidth. However, the data bandwidth of most of the embedded and mobile devices is limited by the bandwidth of the off-chip memory. For instance, most of the low-power DDRs (LPDDRs) such as LPDDR4 provide the bit-width of up to 64 bits [23]. As a result, the bandwidth of state-of-the-art convolutional accelerators for mobile devices is limited to 64 bits [8], [9], [13], [24]. Therefore, accelerating convolutions while skipping the noncontributory computations within the edge computing scenario is nontrivial and missing.

Motivated by the aforementioned statements, in this paper, we present a zero-activation-skipping convolutional accelerator (ZASCA) that enables further improvements in the arithmetic intensity and the performance efficiency over non-zero-skipping accelerators for edge devices. To this end, we first extend and generalize FID to support all type of filter sizes used in state -of-the-art CNNs. We then propose a zero- skipping paradigm based on the generalized FID (GFID) and its architecture (i.e., ZASCA). ZASCA is optimized to achieve high arithmetic intensity, performance efficiency and energy efficiency, while keeping the power consumption below the budget of mobile/embedded devices. Finally, we evaluate the performance of ZASCA on state-of-the-art CNN models (i.e., AlexNet, VGGNet-16, ResNet-18 and ResNet-50) and show that ZASCA performs the convolutional computations of these CNNs with an 83 percent minimum performance efficiency when using dense representation (i.e., no zero-activation-skipping). Using its zero-skipping feature, ZASCA can further improve the performance efficiency by a factor of 1.9×. Finally, we show that the roofline model of ZASCA surpasses the roofline models of the state-of-the-art accelerators in terms of both performance efficiency and arithmetic intensity.

SECTION 2Preliminaries
Inspired by the organization of the animal visual cortex, it was shown that the connectivity of neurons in convolutional layers can be mathematically described by a convolution operation [25]. All neurons in a convolutional layer share a set of weights, also referred to as a filter. The main computational kernel of a convolutional layer involves high-dimensional convolutions. The convolutional layers take input pixels, which are also called input activation maps, arranged in 3 dimensions (i.e., height Hin, width Win and channel Cin), and generate output pixels, which are also called output activation maps, arranged in 3 dimensions (i.e., height Hout, width Wout and channel Cout). This transformation is a result of the convolution between the input activation maps and a set of Cout 3D filters. More precisely, every single 2D Hout×Wout plane of the output activation maps is a result of the convolution between the 3D input activation maps with a set of 3D filters. In fact, a summation of multiple plane-wise 2D convolutions forms a 3D convolution. At the end, the results of 3D convolutions are also added to 1D bias. In summary, the convolutional processes with the input activation maps, the output activation maps, the filters and the bias matrices denoted as X, Y, W and B, respectively, can be expressed as
Y(z,t,q)=B(q)+∑k=1Cin∑j=1Hf∑i=1WfX(zS+j,tS+i,k)×W(j,i,k,q),
View Source
Hout=(Hin−Hf+S)/S,
View Source
Wout=(Win−Wf+S)/S,(2)
View Sourcewhere 1≤z≤Hout, 1≤t≤Wout and 1≤q≤Cout. The stride S represents the number of activation map pixels of which the filter is shifted after each convolution. Contrary to the fully-connected layers, convolutional computations are dominated by numerous MACs according to Eq. (2), leading to a high degree of computational complexity. The parameters used in the convolutional process are summarized in Table 1.

TABLE 1 Convolutional Layer Computation Parameters

2.1 Fully-Connected Inspired Dataflow for Convolutional Computations
In [16], FID was introduced. It can be used to efficiently perform the computations of convolutional layers with filter parameter Wf fixed to 3. Let us note that 2D convolution is the weighted summation of each pixel of an input image with its neighboring pixels, and consider an input image as a matrix X8×2, a filter as a matrix W1×3 and an output as a matrix Y6×2, such that
X=[X1X9X2X10……X8X16],W=W1W2W3,
View Source
Y=[Y1Y7Y2Y8……Y6Y12].
View SourceConsidering each output pixel assigned to a neuron, Table 2 shows the convolutional process of this example in a way similar to the fully-connected layer computations, where input pixels are read sequentially at each clock cycle (CC) and the neurons share the same input pixels. This example considers Cin=1, Cout=1, Hin=2, Win=8, Hf=1, Wf=3 and S=1. Similar to the fully-connected dataflow, each neuron loads a different weight at each time step, subsequently accumulating the weighted input pixels. The number of time steps required to perform the convolutional computations is also equal to the number of input pixels, Hin×Win. When passed to the next neuron belonging to the same row of the output activation map, the weights need to be shifted by one position. However, weight passing between neurons of different rows requires a shift of Wf positions, as can be observed between clock cycles #6 and #9 for W1 denoted in red in Table 2.

TABLE 2 The FID for Convolutional Computations

As shown in Table 2, three PEs (i.e., neurons) are sufficient to perform the convolutions. In fact, there are only 3 active neurons at each time step. Each PE thus receives its input at clock cycle 3×i+1, 3×i+2 and 3×i+3. Their outputs are also valid after 3 clock cycles in the given example. So far, we only considered a case with Hf=1. In case of Hf=3, the procedure in Table 2 has to be repeated 2 times more: the first iteration with W1, W2 and W3, the second with W4, W5 and W6, and the final one with W7, W8 and W9. Similarly, for higher values of Cin, the process has to be to repeated Cin times. Therefore, a memory is required to store the partial values generated by the 3 neurons for each output pixel. In general, N output pixels can be computed using 3 neurons (i.e., PEs) and 3 separate N/3-element SRAM memories working in parallel. The unit generating the N output pixels of an output activation map is referred to as a convolutional element (CE).

SECTION 3Generalized Fully-Connected Inspired Dataflow (GFID)
Let us define a generalized form of the FID by unrolling it over time as a matrix M:
M=⎡⎣⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢W1W2⋮WWf0⋮00⋮0W1W2⋮WWf0⋮⋯0⋮00W1W2⋮WWf⋮0⋯⋮⋱⋯0⋮0W1W2⋮WWf⎤⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥,}S.(3)
View Sourcewhere each column of the matrix M can contain only Wf non-zero elements at most. The shift amount within each row of the output activation map is equal to S, denoted with a horizontal dashed line in the matrix M. The number of columns of the matrix M indicates the N output pixels that belong to the same row of the output activation map, while the number of rows of M denotes the required number of clock cycles. We use the GFID matrix M to represent different filter sizes used in the state-of-the-art CNNs. State-of-the-art networks such as LeNet, ZFNet, AlexNet, GoogleNet, VGGNet, NIN and ResNet are all constructed based on a combination of filter sizes of 11×11 with S=4, 7×7 with S=2, 5×5 with S=1, 5×5 with S=2, 3×3 with S=1, and 1×1 with S=1. All of the aforementioned filter sizes and others can be easily represented using the matrix M. For instance, we represent two filter sizes of 3×3 with S=1 and 7×7 with S=2 using the GFID below.

In Section 2.1, we showed that 3 PEs are sufficient to perform the convolutions for filter size of 3×3 with S=1. Therefore, a CE containing only 3 neurons can perform the convolutional computations. Considering a convolution of a row of a filter map with its corresponding input pixels, N+2 clock cycles are required to generate N output pixels which belong to the same row of the output activation map. For instance, in the given example in Table 2, 8 clock cycles are required to generate the output pixels of the first row of the output activation map (i.e., the first 6 output pixels). This example can also be expressed using the GFID matrix M as follows:
M8×6=⎡⎣⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢W1W2W3000000W1W2W3000000W1W2W3000000W1W2W3000000W1W2W3000000W1W2W3⎤⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥.(4)
View SourceThe matrix M also confirms that there are only 3 active neurons at each time steps, highlighted in red. Considering filters with Wf=7 and S=2, the shift amounts within each row of the output activation map is equal to 2 as shown in the following matrix M:
M15×5=⎡⎣⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢W1W2W3W4W5W6W70000000000W1W2W3W4W5W6W70000000000W1W2W3W4W5W6W70000000000W1W2W3W4W5W6W70000000000W1W2W3W4W5W6W7⎤⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥.(5)
View SourceWhile the higher stride value linearly decreases the number of pixels in the output activation maps, it also reduces the number of neurons required to perform the convolutional computations. For instance, the above matrix M shows that there are only 4 active neurons at each time step, while the width of the filter Wf=7. According to the matrix M, 15 clock cycles are required to generate 5 output pixels in the given example.

3.1 Utilization Factor for Different Filter Sizes
The number of clock cycles required to perform the convolutions using GFID is equal to the number of input pixels. Considering Cin=1 and Hf=1, in order to generate N pixels of an output activation map, S×N+Wf−S clock cycles are required to perform the convolutions according to Eq. (2). Let us define the number of required PEs in the CE as T. The number of pixels computed by each neuron is equal to N/T when N is a multiple of T. Each neuron also requires Wf clock cycles to generate an output pixel. Therefore, the utilization factor of GFID can be expressed as
UF=NT×WfS×N+Wf−S×100.(6)
View SourceIn Section 1, we discussed the importance of high performance efficiency. The utilization factor of PEs in a convolutional accelerator is also linearly proportional to its performance efficiency. Any increase in the utilization factor of PEs exploited in the CE results in a higher performance efficiency. Considering the fact that Wf and S are usually small, a high UF is achieved for a large value of N. In other words, the maximum achievable utilization factor can be obtained as
UFmax=limN→∞UF=WfT×S×100.(7)
View SourceRight-click on figure for MathML and additional features.

Eq. (7) suggests that the highest performance efficiency is obtained when N≫(Wf−S). Table 3 shows the maximum utilization factors for filters with different filter sizes and stride values. Please note that Table 3 only shows the commonly used filter sizes while other filter sizes can be still represented using the GFID.

TABLE 3 The Maximum Utilization Factor for Different Filter Sizes
Table 3- 
The Maximum Utilization Factor for Different Filter Sizes
SECTION 4Generalized Hardware Architecture
In Section 3, we showed that different filter sizes require different number of PEs per CE. In order to perform the computations while achieving a high performance efficiency, the number of PEs per CE has to be reconfigurable. In other words, K instantiated PEs have to dynamically adapt to act as a multiple of T PEs to achieve the maximum possible utilization factor. The closed form solution for this strategy is
K=LCM(Ti),  i∈{1,3,4,5},(8)
View Sourcewhere LCM denotes the least common multiple. Using this approach, 60 PEs are required to achieve the maximum possible utilization factor for all the network sizes listed in Table 3. Depending on the required T, the 60 PEs can dynamically behave as a set of T PEs. For instance, they can act as 60, 20, 15 and 12 parallel CEs for T equal to 1, 3, 4 and 5, respectively, where each CE also contains 1, 3, 4 and 5 PEs. However, using 60 reconfigurable PEs is not trivial and results in a complex architecture.

Observing commonly-used CNNs, we find out that T=1 and T=3 are the dominant minimum numbers of PEs. More precisely, the two filters with Wf=5 and Wf=7 have the least impact on the overall performance efficiency of CNNs, since they are used in only one layer of CNNs. For instance, the first layer of ResNets is fixed to the receptive field of 7×7 and stride of S=2. The filter sizes of the remaining layers are either fixed to 3×3 (for ResNet-18 and ResNet-34) or a combination of 1×1 and 3×3 (for ResNet-50, ResNet-101 and ResNet-152) [5]. Therefore, we use K=6 PEs inside the reconfigurable CE: the reason is twofold. First of all, 6 PEs can be easily used as 2 and 6 CEs containing 3 and 1 PEs for T=3 and T=1, which are the dominant minimum numbers of PEs for most of CNNs introduced in literature. Second, they can perform the computations for T=4 and T=5 with a minimum level of complexity for the address generator unit. In this case, with K larger than what strictly necessary, the number of clock cycles required to perform the convolutional computations remains the same. However, the utilization factors of PEs for these cases decreases. It is worth mentioning that while we only discussed about commonly-used filter sizes in CNNs, other filter sizes can be implemented using this approach.

Fig. 4 shows the architecture of the CE. It consists of two main sub-blocks: the weight generator and K=6 PEs working in parallel. All the PEs share the same input activation pixel while their weights are different. Each PE takes an input activation map and its corresponding weight according to the proposed GFID and performs the accumulation-multiplication for the first row of the first input filter, i.e., W1, W2, …, WWf. This process takes Wf clock cycles and the computed partial value is stored in a memory of L elements. Afterwards, the PE starts the processing of another output activation pixel, using the same weights. The convolutional computations of the first row of the first input filter require S×N+Wf−S clock cycles, as discussed in Section 3.1. Upon reaching this point, the partial value of the first output activation pixel is read from the memory and the computations of the second row of the first input filter are performed for S×N+Wf−S clock cycles. In general, this procedure is repeated for Hf times until the computations of the first filter are finished (i.e., upon completion of Hf×(S×N+Wf−S) clock cycles). At this point, the computation of the second of the Cin filters starts. Upon completion of Cin×Hf×(S×N+Wf−S) clock cycles, the output value of each PE is passed through the ReLU and the result is stored in the off-chip memory. The ReLU as the non-linear function is nowadays a common choice in CNNs since ReLUs achieve a better accuracy performance when compared to other non-linear functions [3], [4], [5], [19], [21], [26], [27]. While we use the ReLU as the non-linear function in this paper, other non-linear functions can also be adopted in PEs.


Fig. 4.
The reconfigurable CE.

Show All

4.1 Reconfigurable Weight Generator Unit
The weight generator unit provides each neuron an appropriate weight according to the proposed GFID. The weight generator unit consists of 6 register sets where each set contains 11 registers. The appropriate weight for each neuron is provided by selecting among these shift registers. For instance, we show the reconfigurability of the weight generator unit for the filter sizes of Wf=3 with S=1 and Wf=7 with S=2 as case-studies below.

4.1.1 Filters with Wf=3 and S=1
As discussed in Section 4, in case of Wf=3 and S=1, the reconfigurable CE containing 6 neurons can function as two CEs of 3 neurons each. Fig. 5a shows the weight generator unit and its working path highlighted in black when using Wf=3 and S=1. It is worth noting that CEs are separated using a dashed line. Each CE loads the weights of the first row of the first filter (i.e., W1, W2 and W3) through the input ports denoted as In #1 and In #4 in Fig. 5a. These weights then loop through the first register of each set to provide one clock cycle delay for each neuron according to (3). Considering Eq. (6), the utilization factor of each neuron for this case can be computed as
UF=NN+2,(9)
View Sourcewhich approaches 100 percent for large values of N.


Fig. 5.
Involved hardware resources and paths in case of convolution computations for (a) Wf=3 and S=1, (b) Wf=7 and S=2.

Show All

4.1.2 Filters with Wf=7 and S=2
The convolutional computations for filters with Wf=5 and S=1 are performed in a way similar to the convolutional computations of the filters with Wf=3 and S=1, with the difference that 4 neurons are active at each time step. Therefore, 6 PEs are used to compute the convolutions for Wf=7 and S=2, while only 4 neurons are sufficient. As a result, the reconfigurable CE functions as a single CE containing 6 PEs (see Fig. 5b). The CE loads the weights of the first row of the first filter (i.e., W1, W2, …, and W7) through the input port In #1 and they loop through the black paths in Fig. 5b. In this scheme, the first two registers of each register set are used to provide the required two delays for each PE, as shown in Section 3. It is worth mentioning that while 12 registers are used in this case, only 7 of them contain the weights. The utilization factor for this configuration is computed as follows:
UF=7N12N+30.(10)
View SourceSince 4 PEs are sufficient to perform the computations of this case, using 6 neurons highly affects the utilization factor and results in 53 percent for large values of N. However, the final impact of this case when considering the computations of the whole system is negligible due to the fact that this configuration is usually used for one layer in the state-of-the-art CNNs such as ResNets and GoogleNets.

4.1.3 Other Filters
Similar to the filter sizes of Wf=3 with S=1 and Wf=7 with S=2, any other filter size can also be implemented using the proposed CE. Table 4 lists the maximum achievable UF for commonly-used filter sizes in literature using the proposed architecture. Comparing Table 4 with Table 3 shows that using K=6 PEs in parallel reduces the maximum achievable UF for the filter sizes of Wf=5 with S=1 and Wf=7 with S=2 from 100 and 88 percent to 83 and 53 percent, respectively. However, the final impact of these cases when considering the computations of the whole system is negligible, as these configurations are normally used for a few layers only in the state-of-the-art CNNs.

TABLE 4 The Maximum Utilization Factor for Different Filter Sizes Using the Proposed CE

SECTION 5Zero-Activation-Skipping Convolutional Accelerator
So far, we have introduced a reconfigurable CE that supports different filter sizes with high utilization factors. We also showed that the weight generator unit provides an appropriate shift amount for each PE using shift registers. In fact, the values of input weights (i.e., the values of each row of each input filter) are circulated through the shift registers for Wf clock cycles as discussed in Section 4. The size of the shift register (i.e., the number of registers involved in the computation) varies depending on the filter size being used. For instance, 3 registers are used for the convolutional computations for Wf=3 and S=1. More generally, the number of registers required for the computations, which are hereafter referred to as Nr, can be obtained by multiplication between the required stride value and the number of PEs used in each CE. Table 5 summarizes the number of involved registers in the convolutional computations given Wf and S for commonly-used filter sizes while our architecture is not limited to them.

TABLE 5 The Number of Registers Used in the Convolutional Process for Different Filter Sizes

Table 5 and Fig. 5 show that the position of weights in the registers are unchanged after Nr clock cycles. This observation suggests that if any multiple of Nr activations in consecutive order is equal to zero, we can skip their computations using the proposed CE. Table 6a illustrates an example for Wf=3 with S=1 when 6 consecutive activations are zero (denoted in gray). According to Table 6a, the output values Y5, Y6, Y7 and Y8 are zeros due to the multiplications with zero-valued activations. Moreover, the output values Y3 and Y4 are valid at clock cycles #4 since their remaining computations are noncontributory due to the multiplications with zero-valued activations. As a result, the dataflow changes to a simpler one as shown in Table 6b and the noncontributory computations can be skipped without any impact on the flow of weights through the registers. Similar approach can be used for other filter sizes depending on their Nr value and the number of consecutive zero-valued activations. Therefore, noncontributory computations can be avoided using the proposed method and CE. However, in order to take advantage of zero-valued activations, an encoder/decoder is required to specify the number of zero-valued activations to the CE. Moreover, using the encoder/decoder allows us to compress the activations in order to reduce memory accesses to the off-chip memory.

TABLE 6 The Complete Convolutional Process (a) without and (b) with Skipping Zero-Valued Activations

The encoder can be easily realized using counters and comparators to keep track of zero-valued activations as shown in Fig. 6. The final output activations, which are stored in the internal SRAM inside the CE, are first passed to the ReLU and then to the encoder at each clock cycle. It is worth noting that we use 15 bits for representation of the value of input/output activations. The encoder starts counting consecutive zero-valued activations right after detecting the first zero-valued activation. The encoder passes its incoming input directly to its output port along with a single flag bit while counting the number of consecutive zero-valued activations. As soon as the encoder detects that the number of consecutive zero-valued activations is a multiple of Nr denoted as n×Nr, it outputs the value of n using 15 bits along with a bit to distinguish between the zero and non-zero activations. More precisely, the encoded word denotes that the value of current activation and n upcoming activations are zero when the flag bit is high. Otherwise, it only denotes the value of non-zero activation. It is worth mentioning that in case of detecting a multiple of Nr by the encoder, the value of n is overwritten into the memory element storing the second zero-valued activation of the corresponding sequence of zero-valued activations. Its appropriate address is also generated by the controller of the system. For instance, Table 7 illustrates the encoding procedure given the sequence {128,0,0,0,0,0,0,0,0,512,768} and Nr=3. Upon the completion of the encoding procedure, the off-chip memory contains the consecutive encoded/compressed values as {0040,0000,8002,0000,0100,0180} in hexadecimal format. The number of memory accesses to the off-chip memory for the writing procedure using the encoded values is the same as the dense model.


Fig. 6.
The encoder.

Show All

TABLE 7 An Example of Encoding Procedure Given Nr=3 N r = 3

The decoder performs the inverse computations. It uses the flag bit to either skip the noncontributory computations or perform the computations of non-zero-valued activations. In case of receiving a low flag bit, it passes the non-zero value of activation to the CE. Otherwise, it first passes zero to the CE to store all the intermediate output values of neurons into their corresponding SRAM. It then passes the value of n to the controller of the system to update the writing addresses of the internal SRAMs in the CE. This unit is simply implemented using multiplexers.

5.1 Exploiting Parallel CEs
While the proposed reconfigurable CE can efficiently perform convolutional computations, using a single CE results in a long latency and numerous memory accesses. To address this issue, p CEs are instantiated in parallel to generate p out of Cout output activation maps in parallel. Since the reconfigurable CE itself can function as up to 6 parallel CE, the upper bound for the maximum number of CE is 6p in ZASCA. Therefore, the computational latency of ZASCA is effectively reduced by a p factor when compared to a single reconfigurable CE. Moreover, the memory accesses are reduced as well, since the input pixels are shared among the parallel CE, while each CE is fed by a different set of weights.

Exploiting parallel CE requires an input bandwidth of (1+6×P)×16 bits (6×p×16 for weights and 16 for input pixels). However, most of the embedded and mobile devices cannot provide such a high bandwidth. In fact, their bandwidth is limited by the bandwidth of the off-chip memory. For instance, most of the low-power DDRs (LPDDRs) such as LPDDR4 provide the bit-width of up to 64 bits [23]. As a result, the bandwidth of state-of-the-art convolutional accelerators for mobile devices is limited to 64 bits [8], [9], [13], [24]. To overcome this problem, ZASCA makes use of pipelining. As discussed in Section 3, each input pixel is read at each clock cycle while Wf weights are read only for the first Wf clock cycles when performing the convolutional process of the first row of the first input filter. The parameter Wf is also a small value compared to the processing time of convolutions for the first row of the first input filter (i.e., Wf≪(S×N+Wf−S)). More precisely, the input bandwidth from the (Wf)th clock cycle to the (S×N+Wf−S)th clock cycle is occupied by input pixels only. Therefore, we can fill out this available bandwidth by pipelining the CEs with up to ⌊(S×N+Wf−S)/Wf⌋ stages, while the additional latency overhead is negligible compared to the overall latency of the system.

5.2 Processing Time and Memory Accesses of ZASCA
Earlier in Section 4 we showed that in convolutional processes, a single CE computes N out of Hout×Wout pixels of one of Cout output activation maps within Cin×Hf×(S×N+Wf−S) clock cycles. We also showed that the total number of weight passing occurrences for the computation of a single convolutional layer is equal to Hout−1, which causes additional (Wf−1)×(Hout−1) clock cycles for the computations of each row of the input filters. Considering p parallel CEs, the number of required clock cycles is expressed as
CC=Wout×HoutN×(S×N+Wf−S)×Hf×Cin×⌈Coutp⌉+(Wf−1)×(Hout−1)×Hf×Cin×⌈Coutp⌉.(11)
View Source

Eq. (11) suggests that the number of required clock cycles for convolutional computations is independent of N for large values of N (i.e., S×N≫(Wf−S)) when not considering the weight passing overheads. In Section 5.1, we showed that input pixels are shared among all CEs and each pixel is read at each clock cycle. This means that the number of memory accesses by input activation maps (MAimaps) is equal to the number of clock cycles required to complete the convolution. On the other hand, the weights are read in the first Wf clock cycles out of a total of (S×N+Wf−S). As a result, the number of required memory accesses by filters to compute N out of Hout×Wout pixels of one out of Cout output activation map is equal to Cin×Hf×Wf. In general, the number of memory accesses by filters (MAfilters) can be computed as follows:
MAfilters=Hf×Wf×Cin×⌈Wout×HoutN⌉×Cout.(12)
View SourceFinally, the total number of memory accesses (MA) is a summation of memory accesses by filters, input activation maps and output activation maps, where the number of memory accesses by output activation maps (MAomaps) is equal to Wout×Hout×Cout. It is worth mentioning that so far, we have computed the latency and memory access of ZASCA performing on dense activations. Considering ZASCA performing on sparse activations, the latency and number of memory accesses by input activation maps (MAimaps) are linearly reduced depending on the sparsity degree of activations. More precisely, the latency and number of memory accesses by input activation maps are equal to (1 - Sp) × CC, where Sp denotes the sparsity degree.

SECTION 6Implementation Results
In this paper, we optimize ZASCA for a low-latency, low-memory access implementation while keeping its power consumption below the power budget of mobile devices, limited to a few hundred mW [22]. Fig. 7 shows the architecture of ZASCA, which consists of three main sub-blocks: CEs, pipelining stages and encoder/decoder units. ZASCA contains 32 reconfigurable CEs, each of which with 6 PEs. Each PE is also associated with L=64 24-bit memory. The pipelining stages provide the required amount of shifts depending on the value of Wf using shift registers and multiplexers, as discussed in Section 5.1. The encoder/decoder units also enable zero-activation-skipping convolutional computations.


Fig. 7.
The architecture of ZASCA and its layout.

Show All

The p and N parameters do not only affect latency and number of memory accesses, but also impact power and area costs. Therefore, it is possible to obtain different trade-offs between processing time, throughput and implementation costs depending on p and N. Since the reconfigurable CE functions differently based on Wf and S, the effective values of N and p vary for each case. Table 8 shows the effective values of N and p for commonly-used filter sizes in literature while our architecture is not limited to them. The effective values of N and p, denoted as Neff and peff respectively, have to be used in all the equations reported in this paper that rely on these two values.

TABLE 8 The Effective Value of N N and p p in ZASCA Depending on Wf W f and S S

6.1 Methodology
ZASCA was described in Verilog and placed and routed via Cadence Virtuoso Layout Suite using TSMC 65 nm GP CMOS technology. The resulting layout is shown in Fig. 7. Artisan dual-port memory compiler was also used for the implementation of internal SRAMs inside the CEs. ZASCA works at a nominal frequency of 200 MHz for convolutional processes. Table 9 lists a summary of the main characteristics of ZASCA. For evaluation purposes, we use four state-of-the-art CNNs: AlexNet, VGG-16, ResNet-18 and ResNet-50 using [28], [29] in order to perform image classification on ImageNet dataset [6]. We then evaluate ZASCA on the aforementioned CNNs and show the implementation results compared to the state-of-the-art Eyeriss, EEPS and DSIP accelerators for edge devices. Eyeriss was fabricated in TSMC 65 nm CMOS technology with a silicon area of 12.52 mm2 (1,852 kgates) and tested on AlexNet and VGGNet-16 [8], [24]. Since both Eyeriss and ZASCA were implemented in TSMC 65 nm CMOS technology and use 16-bit fixed-point representations, a direct comparison of these two implementations is fair. EEPS was fabricated in a 40-nm Low-Power (LP) technology with a silicon area of 2.4 mm2 and tested only on AlexNet. EEPS uses 16-bit MAC units for the convolutions while modulating precision, frequency and voltage to reduce its power/energy consumption. For a fair comparison with EEPS, we perform a technology scaling according to [14] only when comparing in terms of power/energy consumption. DSIP was fabricated in a 65-nm CMOS technology with a silicon area of 10.56 mm2 and tested only on AlexNet. Since both DSIP and ZASCA were implemented in a 65 nm CMOS technology and use 16-bit fixed-point representations, a direct comparison of these two implementations is also fair. It is worth mentioning that the implementation results of the state-of-the-art Eyeriss, EEPS and DSIP accelerators were obtained from their original papers for comparison purposes throughout this paper.

TABLE 9 The Main Characteristics of ZASCA

6.2 Sparsity Degree of Input Activations
As discussed in Section 1, the sparsity among activations is an intrinsic occurrence in CNNs using the ReLU which dynamically clamps all negatively-valued activations to zero. Fig. 8 shows the breakdown of sparsity degree of activations in convolutional layers of AlexNet, VGG-16, ResNet-18 and ResNet-50 when running on ZASCA. More specifically, we only considered the zero-valued activations that can be skipped using ZASCA in our measurement of the sparsity degree (see Section 5). The sparsity degrees were measured over the validation set of ImageNet dataset. The sparsity degree of each layer is linearly proportional to the amount of speedup. For instance, 70 percent sparsity degree denotes that 70 percent of MAC operations are avoided on ZASCA and the computation time is reduced by a factor of 2.3×.

Fig. 8. - 
The breakdown of sparsity degree of CNNs when running on ZASCA.
Fig. 8.
The breakdown of sparsity degree of CNNs when running on ZASCA.

Show All

6.3 Performance
We hereafter denote ZASCA performing convolutions on dense activations as ZASCAD and ZASCA performing convolutions on sparse activations as ZASCAS. It is worth mentioning that the accuracy of the models using the dense representation is the same as those using the sparse representation for activations. Fig. 9 shows the performance of ZASCAD and ZASCAS when running AlexNet, VGG-16, ResNet-18 and ResNet-50 in terms of giga operations per second compared to Eyeriss, EEPS and DSIP. It is worth noting that Eyeriss was tested on AlexNet and VGG-16 while EEPS and DSIP was only tested on AlexNet. Fig. 9 shows that ZASCAD outperforms Eyeriss in terms of runtime performance by factors of 1.4× and 3.1× when running AlexNet and VGG-16, respectively. Skipping noncontributory computations further improves the runtime performance ranging from 2× to 5.8× compared to Eyeriss. ZASCAD yields a slightly better runtime performance compared to EEPS while EEPS contains 64 more MAC units than ZASCA. Exploiting the sparsity among the activations, ZASCAS performs the convolutions of AlexNet 1.5× faster than EEPS. Finally, ZASCAD and ZASCAS outperforms DSIP in terms of runtime performance by factors of 2.7× and 4×, respectively.


Fig. 9.
The runtime performance of ZASCAD, ZASCAS, Eyeriss, EEPS and DSIP. Note that the runtime performance of Eyeriss, EEPS and DSIP is taken from their original papers.

Show All

As discussed in Section 5, all the PEs in ZASCA share the same input activation pixel and the complete calculations of each output pixel are performed serially, allowing all the PEs to easily skip the noncontributory computations with zero-valued input activation pixels. However, Eyeriss, EEPS and DSIP rely on a 2D array of PEs and perform the computations of each output pixel in a semi-parallel fashion. More precisely, the computations of each output pixel are split into different parts where a certain number of these parts is performed in parallel. The results of all the parts are then added to make output pixels. In this way, PEs computing each part take different activation pixels. In case of skipping the computations with zero-valued input activation pixels of each part, the runtime performance is still limited by the part containing the least number of zero-valued input activation pixels. Note that having all the parts with the same number of zero-valued input activation pixels is unlikely to happen when exploiting the aforementioned approach. Moreover, using batch sizes greater than 1 exacerbates the problem since skipping the noncontributory computations is possible only when all the batches contain zero-valued input activation pixels at the same time.

Eyeriss and DSIP use batch sizes greater than 1 to obtain a lower number of memory accesses and to achieve a higher energy efficiency. Using high batch sizes allows more filter reuse and maximizes the utilization of all available PEs, resulting in lower memory accesses and a higher energy efficiency compared to the use of batch size of 1, as discussed in [7]. However, using this method results in a higher computational latency. Increasing the number of PEs is another way to improve the runtime performance at similar or better energy efficiency in existing dataflows [7]. The choices of large batch sizes and large number of PEs aim at better exploiting bandwidth and hardware resources, but these are ineffective design choices when deploying to a bandwidth-constrained edge computing device with a limited power budget. As a result, the number of PEs in Eyeriss and DSIP is less than 200 with the maximum batch size of 4.

Moreover, Eyeriss, EEPS and DSIP use an on-chip SRAM to store and reuse input activations in order to provide the required parallelism for its processing units and reduce memory accesses to the off-chip memory. However, such a technique results in a long latency due to the bandwidth bottleneck of the off-chip memory. Therefore, Eyeriss and DSIP suffer from a high computational latency. On the other hand, GFID enables ZASCA to perform the convolutional processes right after fetching the first data from the off-chip memory, which results in a low computational latency. Table 10 summarizes the total latency (including the reading/writing latency) of ZASCAD, ZASCAS, Eyeriss, EEPS and DSIP. Eyeriss performs convolutional computations of AlexNet and VGGNet-16 in 115.3 ms and 4.3 s while using a batch size of 4 and 3, respectively. DSIP performs the convolutions of AlexNet withing 226.6 ms when using a batch size of 4. ZASCAD outperforms Eyeriss and DSIP in terms of latency by factors ranging from 5.5× and 10.9×. Moreover, ZASCAS further speeds up the computations by a factor of 1.7× on average compared to ZASCAD.

TABLE 10 The Total Latency of ZASCAD, ZASCAS, Eyeriss, EEPS and DSIP

Introduction of ResNet-18 was one of the first attempts on the software side to reduce the computational complexity of VGGNet-16 while keeping its accuracy performance almost intact. In fact, ResNet-18 requires 8.5× less operations compared to VGGNet-16. In this work, we showed that ZASCAD also reflects a similar improvement factor (i.e., 7.5×) in the total execution time compared to that of VGGNet-16 on the hardware side when running ResNet-18. Moreover, using the sparse representation for activations enables ZASCAS to achieve the performance needed for real-time applications (i.e., the minimum throughput of 30 frames per second) when running ResNet-18.

6.4 Power and Energy Consumption
Fig. 10 reports average power consumption of ZASCAD, ZASCAS, DSIP, EEPS and Eyeriss across AlexNet, VGG-16, ResNet-18 and ResNet-50. Power consumptions of ZASCAD and ZASCAS was obtained based on post-synthesis layout by measuring actual switching activities over 100 images randomly selected from the validation set. ZASCAD, ZASCAS, Eyeriss and EEPS dissipate power ranging from 248 to 305 mW, meeting the power budget of edge devices. However, DSIP dissipates lower power due to its smaller number of PEs when compared to the aforementioned accelerators.


Fig. 10.
The power consumption of ZASCAD, ZASCAS, Eyeriss, EEPS and DSIP. Note that the runtime performance of Eyeriss, EEPS and DSIP is taken from their original papers.

Show All

Fig. 11 illustrates average energy consumption of ZASCAD and ZASCAS compared to EEPS, DSIP and Eyeriss. When performing the convolutional process of AlexNet on dense activations, DSIP achieves the highest energy efficiency when compared to ZASCAD, EEPS and Eyeriss. However, ZASCAS outperforms DSIP by a factor of 1.4×. Considering the inference computation of VGG-16, ZASCAD is 2.7× energy efficient than Eyeriss while ZASCAS is 1.7× energy efficient than ZASCAD on average. Moreover, ZASCAS achieves the highest energy efficiency among the aforementioned accelerators when performing the convolutions of ResNet-50.


Fig. 11.
The energy efficiency of ZASCAD, ZASCAS, Eyeriss, EEPS and DSIP. Note that the energy efficiency of Eyeriss, EEPS and DSIP is taken from their original papers.

Show All

6.5 Memory Accesses
Memory accesses to the off-chip DRAM play a crucial role in the system energy efficiency as discussed in [8]. ZASCA offers two techniques to reduce the number of memory access. First, we showed that exploiting parallel tiles reduces the number of accesses to input activations by a factor of p (see Section 5.2). Second, ZASCA exploits sparsity among activations to compress them using an encoder. As a result, the number of memory accesses to input activations is reduced depending on their sparsity degree. Fig. 12 shows the memory accesses to the off-chip DRAM for ZASCAD, ZASCAS, DSIP, EEPS and Eyeriss. As discussed earlier, Eyeriss and DSIP use high batch sizes to obtain a lower number of memory accesses while this approach incurs a high latency. However, ZASCAD obtains competitive memory accesses while achieving up to 10.9× lower latency compared to Eyeriss and DSIP. On the other hand, ZASCAS further reduces memory access by a factor of 1.38× on average over ZASCAD by compressing activations, resulting in the lowest number of off-chip memory accesses when compared with the state-of-the-art accelerators.


Fig. 12.
The memory accesses of ZASCAD, ZASCAS, Eyeriss, EEPS and DSIP. Note that the memory accesses of Eyeriss, EEPS and DSIP is taken from their original papers.

Show All

6.6 Roofline Model
The roofline model is commonly used to visually model the performance of accelerators by showing their inherent hardware limitations [15]. This model is obtained by tying arithmetic intensity and the performance of a given accelerator, where the arithmetic intensity is the number of operations performed per byte of data accessed from the off-chip memory for each batch. It is worth noting that the performance of architectures is bounded by their peak performance and peak memory bandwidth. Since the memory bandwidth of convolutional accelerator is fixed, we have only considered the peak performance bound in the roofline model. The roofline model of ZASCA along with the state-of-the-art accelerators is depicted in Fig. 13, where the peak performance bound of ZASCA and the state-of-the-art accelerators are denoted using a solid line and dashed horizontal lines, respectively. The roofline model shows that Eyeriss achieves the highest arithmetic intensity among accelerators performing convolutions on dense activations. This is mainly because Eyeriss compresses sparse activations to reduce memory accesses while still performing the convolutions on dense activations. However, the main drawback of Eyeriss is its poor performance. In fact, there is a huge gap between its runtime performance and its peak performance bound as denoted using an arrow in Fig. 13. The gap between the runtime performance and the peak performance bound is computed using the performance efficiency as shown in Fig. 14. On the other hand, ZASCAD has a competitive arithmetic intensity compared to Eyeriss while achieving the highest performance efficiency among the state-of-the-art accelerators. Performing the computations on sparse activations enables ZASCA to yields the highest arithmetic intensity and performance efficiency. In fact, ZASCAS surpasses the peak performance bound of ZASCA by skipping noncontributory computations.


Fig. 13.
The roofline model of ZASCAD, ZASCAS, Eyeriss, EEPS and DSIP tested on AlexNet, VGG-16 and ResNet-50. The circles and solid/dashed lines denote the average runtime performance and the peak performance bound, respectively.

Show All


Fig. 14.
The performance efficiency of ZASCAD, ZASCAS, Eyeriss, EEPS and DSIP. Note that the performance efficiency of Eyeriss, EEPS and DSIP is taken from their original papers.

Show All

SECTION 7Related Work
During the past few years, numerous works have been conducted towards ASIC implementations of DNNs. However, most of them were only tested on either small datasets or outdated CNNs which require order of magnitudes lower parameters and computations [22], [30], [31], [32], [33], [34]. Recently, Google released a custom DNN processor tensor processing unit (TPU) [35]. TPU is a programmable and reconfigurable processor that can perform both fully-connected and convolutional computations. However, its power consumption exceeds the power budgets of embedded devices [16].

Despite the existence of many works on accelerating sparse multiplications [18], [36], [37], [38], [39], [40], only few works have focused on exploiting sparsity to accelerate convolutional computations in CNNs, such as Cnvlutin [19], Cambricon-X [20] and SCNN [21]. These architectures are mainly designed to accelerate computations on the cloud where unlimited data bandwidth is provided. Cnvlutin relies on DaDianNao's architecture [30] as its baseline and achieves up to a 1.55× speedup by exploiting sparsity among activations. Cambricon-X is a convolutional accelerator containing 528 operators working at 1 GHz that can perform convolutions of both dense and sparse models. As a result, it can achieve a theoretical peak performance of 528 G-ops/s when performing on dense models. Exploiting weight sparsity, it can skip ineffectual multiplications and speed up the computations by up to a factor of 2.51× over its dense model, yielding 544 G-ops/s at most [20]. Therefore, the highest achievable performance efficiency of this accelerator is 41 and 103 percent when performing on dense and sparse models, respectively. However, ZASCA outperforms Cambricon-X in terms of performance efficiency by yielding the peak performance efficiency of 94 and 168 percent when performing on dense and sparse activations, respectively. Finally, SCNN employs sparsity among both activations and weights to reduce energy, latency and data transfer time. Using this approach, SCNN managed to improve performance and energy by factors of 2.7× and 2.3× compared to its dense baseline, respectively. Despite the improvements that these accelerators provide over their baseline, they fail to meet the power and memory bandwidth constraint of edge devices, which is limited to a few hundered mW [22]. However, ZASCA uses a substantially different dataflow from the above designs and aims to accelerate convolutions for low-power mobile devices at the edge. Moreover, since no quantitative runtime result was provided, a direct comparison cannot be made with these works.

Recently, a few works have focused on minimizing energy by modulating precision, frequency and supply voltage of their accelerator for each convolutional layer [9], [11], [13]. In [9], a precision-scalable convolutional accelerator (i.e., Envision), fabricated in 28 nm UTBB FD-SOI technology, was introduced. This architecture dynamically adapts itself depending on the required precision for each layer, instead of using a fixed precision. More precisely, it exploits a reconfigurable MAC which is able to perform a 16-bit, two 8-bit and four 4-bit multiplications/accumulations, depending on the required precision. As a result, using a dynamic fixed-point technique allows to change frequency and supply voltage over time which results in a lower power/energy consumption. Envision contains 256 reconfigurable MACs. This accelerator performs the convolutional computations of AlexNet to 21.3 ms (62.6 G-ops/s), and those of VGG-16 to 598.8 ms (51.3 G-ops/s), while its performance efficiency is respectively limited to 38 and 32 percent on average. Similar to Eyeriss, the low performance efficiency of Envision results in a large gate count of 1950 kgates. More precisely, Envision requires more parallelism to meet the timing constraint of real-time edge devices, resulting in a large silicon area. Despite the differences in the technology nodes and applied circuit-level techniques, ZASCAS outperforms Envision in terms of gate count (1.9× smaller), latency (1.5× and 2.4× lower), throughput (1.5× and 2.4× faster) and performance efficiency (3.2× and 5× better).

In [12], a configurable accelerator framework (i.e., CAF) was introduced. CAF was fabricated in 28 nm UTBB FD-SOI technology with a silicon area of 34.9 mm2 and contains various types of accelerators including 8 convolutional accelerator cores (the total of 288 MACs) for computer vision applications. This architecture uses 16-bit fixed-point representations and performs the convolutional computations of AlexNet to 17.1 ms while consuming 61 mW at 0.575 V and its performance efficiency is respectively limited to 67 percent. Despite the advanced technology nodes used in CAF, ZASCAS outperforms this architecture in terms of latency (1.2× lower), throughput (1.2× faster) and performance efficiency (1.8× better).

As discussed in Section 6, ZASCAS outperforms Eyeriss [8] in terms of gate count (1.8× smaller), latency (5.5× and 17.5× lower), throughput (2× and 5.8× faster), performance efficiency (2.2× and 6.2× better) and energy efficiency (2.1× and 4.5× more efficient) while having roughly the same number of memory accesses per batch. It is worth noting that a direct comparison of ZASCA with the works published in [9], [12] does not constitute a fair comparison, since they dynamically modulate precision, frequency and supply voltage and use advanced technology nodes, which allows them to instantiate more PEs while still having a low-power/energy consumption. However, the introduced performance efficiency metric can be used for a fair comparison as it reflects the performance of the accelerators independent of their technology nodes, precisions and optimization techniques as shown in Fig. 14. It is worth mentioning that the circuit-level techniques used in [9], [11], [13] can also be used in ZASCA to further improve its energy/power consumption.

When considering beyond standard CNNs, bottleneck and depthwise separable convolutions are commonly used nowadays due to computational considerations. The bottleneck convolutions were first introduced and used in very deep residual networks such as ResNet-50 [5]. In the bottleneck architectures, each standard convolutional layer is usually sandwiched between two convolutional layers with the filter size of 1 × 1. The first convolution is used to reduce the dimensionality of inputs (i.e., the number of channels) and the last one to restore it. In this way, a very deep network can be achieved while using inexpensive 1 × 1 filters [26]. In this work, we showed that ZASCA can successfully map the bottleneck architecture used in ResNet-50 while skipping its noncontributory computations with zero-valued input activation pixels. Using depthwise separable convolutions, which is a building block of MobilNets [27], is another way of reducing complexity of standard convolutions. In this approach, each standard convolution is reformed into a depthwise convolution followed by a pointwise convolution. In the depthwise convolution, each input channel is associated with a separate 2D filter and the result of convolution between each input channel and its filter constructs a single output channel. On the other hand, the pointwise convolution is referred to the standard convolution with the filter size of 1 × 1. Since each input activation channel contributes only to one output activation channel in the depthwise convolution, this convolutional process requires parallel access to all the input activation channels and all the filters to perform the computations in parallel. As a result, ZASCA cannot fully exploit its available resources for this type of convolution. More precisely, ZASCA can only perform the depthwise convolutional computations of two output channels in parallel due to its limited data bandwidth. However, the final impact of depthwise convolutional processes is negligible as the computational complexity of depthwise separable convolutions is dominated by the pointwise convolutions [27]. Note that ZASCA can efficiently perform the pointwise convolutional computations (see Section 4.1.3).

SECTION 8Conclusion
In this paper, we proposed ZASCA: an accelerator performing convolutions using either dense or sparse activations. ZASCA exploits a dataflow (i.e., GFID) that allows to perform the convolutional computations while reading input data under the bandwidth bottleneck of edge devices, which maximizes its performance efficiency. Such a dataflow also enables ZASCA to avoid noncontributory multiplications/accumulations with zero-valued activations to further speedup the computations. Compared to the state-of-the-art accelerator for mobile devices, ZASCA enhances the latency, throughput, energy efficiency and performance efficiency by up to 17.5×, 5.8×, 4.5× and 6.2×, respectively.