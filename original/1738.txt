The exploding complexity and computation efficiency requirements of applications are stimulating a strong demand for hardware acceleration with heterogeneous platforms such as FPGAs. However, a high-quality FPGA design is very hard to create and optimize as it requires FPGA expertise and a long design iteration time. In contrast, software applications are typically developed in a short development cycle, with high-level languages like Python, which have much higher levels of abstraction than all existing hardware design flows. To close this gap between hardware design flows and software applications, and simplify FPGA programming, we create PyLog, a high-level, algorithm-centric Python-based programming and synthesis flow for FPGA. PyLog is powered by a set of compiler optimization passes and a type inference system to generate high-quality hardware design. It abstracts away the implementation details, and allows designers to focus on algorithm specification. PyLog takes in Python functions, generates PyLog intermediate representation (PyLog IR), performs several optimization passes, including pragma insertion, design space exploration, and memory customization, etc., and creates complete FPGA system designs. PyLog also has a runtime that allows users to run the PyLog code directly on the target FPGA platform without any extra code development. The whole design flow is automated. Evaluation shows that PyLog significantly improves FPGA design productivity and generates highly efficient FPGA designs that outperform highly optimized CPU implementation and state-of-the-art FPGA implementation by 3.17× and 1.24× on average.
SECTION 1Introduction
The last decade has witnessed an explosive growth of new applications in terms of quantity, diversity, and demands for computing capability and energy efficiency. As an example, deep learning algorithms, which have been shown to be successful in many domains, are driving the revolutionary changes in computer system design. According to Dean et al. [1], the number of machine learning papers on arXiv [2] doubles in less than two years, which has outpaced Moore's Law. The rapid growth of diverse applications poses immense challenges to many aspects of computing systems, including compiler, architecture, storage, etc.

These challenges have motivated new computing systems for the new decade. The FPGA-based computing platform is an emerging platform that provides reconfigurability, along with high performance, low latency, and high energy efficiency. FPGA's unique computing capability makes it a promising platform to tackle the rising computation challenges. FPGA accelerators have been deployed in both cloud servers and edge devices at scale. However, as FPGAs are getting used in increasing number of emerging applications and scenarios at a rapid pace, programming FPGA and optimizing FPGA design gradually become the main barriers in FPGA development.

The most widely-adopted FPGA development flow today starts with programming FPGA at the register transfer level (RTL) in hardware description languages (HDL) such as Verilog and VHDL. Then designers use FPGA synthesis tools from FPGA vendors to synthesize RTL designs into FPGA bitstreams, which are used to configure FPGA. Programming FPGA at this level requires rich expertise in digital circuit design and the FPGA architecture. Besides, programming at this level is non-intuitive, error-prone, and hard to reuse code compared with modern programming languages, leading to long development, optimization, and verification cycles.

High-level synthesis (HLS) aims to simplify FPGA programming. Elevating the abstraction level of FPGA programming to that of C/C++/OpenCL [3], [4], [5], [6], HLS tools enable FPGA designers to express their algorithms in more familiar high-level languages. Developers are expected to use HLS pragmas or directives to guide the HLS tools to optimize and generate desired RTL design. Compared with RTL design flow, HLS allows FPGA developers to develop, optimize, verify, and reuse their designs at a higher level, thereby greatly improving productivity. However, as C/C++/OpenCL are initially designed for general-purpose processors and start with an inherent sequential execution model inside each kernel/function definition of these languages, they are essentially different from the FPGA's fine-grained parallel processing nature (OpenCL model can describe parallel work-items but it is at thread-granularity and not very well supported in current HLS tools). The existing HLS tools are also designed in a way that accommodates the sequential execution model of input languages.

As a compromise between the HLS programming model and the FPGA execution model, HLS users often manually annotate their code with HLS pragmas or directives to give HLS compiler hints on parallelism and desirable synthesis approaches. These pragmas have a significant impact on the performance and energy efficiency of the synthesis output. Oftentimes code transformation and rewriting are also needed to improve the outcome. In the end, the quality of synthesized designs from HLS highly depends on how the code is written and how the HLS pragmas are added to the code. This requires a considerable amount of engineering time to iteratively adjust the source code and pragmas used. Complicated applications or thorough optimizations may lead to long source code that is difficult to read and maintain. For example, the HLS C code for convolution kernel from CHaiDNN [7] library has nearly 8,000 lines, which is much longer than the well-optimized convolution code for CPU or GPU.

Apart from the difficulties in creating and optimizing designs with current FPGA programming flows, the wide gap between application programming and FPGA programming is another challenge in FPGA design. Applications are typically developed with languages (e.g., Python) at a much higher level of abstraction, where the application programming models and styles focus more on describing the algorithm itself, instead of low-level implementation details. In the current HLS flow, when there is a need to accelerate the application, FPGA developers typically need to first lower the abstraction level of the application, re-implement the application in plain C code, and use it as the starting point for HLS. This lowering step is time-consuming and error-prone, and it also makes the FPGA design cycle longer.

These challenges in current FPGA design flows urge us to further elevate the abstraction level of FPGA programming. Among the existing programming languages, Python is one of the most popular and widely-used languages. It has been well adopted in various domains such as machine learning, scientific computing, data analysis, education, etc. Python is also easy to learn. We propose PyLog, an algorithm-centric Python-based programming and synthesis flow for FPGAs. PyLog uses general Python compatible syntax, and it provides a set of handy low-level and high-level built-in operators that are capable of describing most of the common computation patterns in a natural way.

The PyLog compiler takes Python code as input, and compiles the code into optimized HLS-synthesizable C code with HLS pragmas. Fig. 1 shows the FPGA design flow with PyLog. We design PyLog in a way that the Python language allows the developers to focus on algorithm and computation flow description without much implementation details, while the PyLog compiler takes over the traditional FPGA developers’ burden of exploring possible implementations and optimizations. The functional nature of the Python language also preserves some algorithm-level design information that is helpful for PyLog analysis and transformation. In the PyLog flow, algorithm and implementation are separated as much as possible. The goal of this design is to relieve FPGA programmers from manual design tuning while giving the PyLog compiler maximum information about computation patterns and therefore maximum freedom of design optimization. The whole PyLog flow is developed in Python. PyLog has been open-sourced to enable future research in this field.1

Fig. 1. - 
FPGA design flow with PyLog.
Fig. 1.
FPGA design flow with PyLog.

Show All

The key contributions of this work are:

We design and implement PyLog, a high-level Python-based programming and synthesis flow for FPGAs, which greatly simplifies FPGA programming. The expressiveness of Python allows developers to achieve high design quality with much fewer lines of code compared with previous C/C++ based high-level synthesis flows.

PyLog compiler is an ahead-of-time compiler and it is capable of doing various types of program analysis and optimizations. It features PyLog intermediate representation, type inference engine, and a set of compiler optimizations, which are all designed to create highly efficient FPGA accelerator systems.

PyLog also provides a set of high-level operators that ease algorithm description. These operators are general enough to describe computation patterns across different domains, and their implementation can be configured and optimized by PyLog to meet different design requirements.

PyLog automatically generates optimized designs from high-level algorithm specifications, based on hardware resource constraints. Evaluation shows that PyLog generates highly efficient FPGA designs that outperform highly optimized CPU implementations and state-of-the-art FPGA implementations by 3.17× and 1.24× on average.

The rest of the paper is organized as follows. Section 2 reviews related works in literature. Then, the paper presents PyLog programming model in detail in Section 3, and elaborates on PyLog compiler design in Section 4. Section 5 evaluates PyLog with several real-world workloads. Finally, Section 6 concludes the paper.

SECTION 2Related Works
Recently there are growing interests in the research community on high-level programming languages either for accelerators or domain-specific applications. Halide [8] is one popular domain-specific language (DSL) for image processing applications. It provides a programming paradigm that separates algorithm specification and implementation details. Users first specify algorithm with high-level operators, then specify optimization and implementation details by creating “schedules” with Halide APIs. This paradigm is also used in other frameworks like TVM [9] and HeteroCL [10]. Delite [11] is another high-level programming framework that provides high-level pattern-based operators to simplify programming. PyLog also supports pattern-based operators such as the “map” operator and the stencil operator, which will be discussed in more detail in later sections. SYCL[12]/DPC++[13] is a framework built on top of OpenCL that provides an abstraction layer to simplify heterogeneous computing. Both host and accelerators are programmed within the same framework at C/C++ level. PyLog also unifies host programming and accelerator programming within the same language and framework, and also supports CPU+FPGA type of heterogeneous computing. The differences are that PyLog uses Python, and it hides the underlying hardware-related details. This hardware-agnostic abstraction provided by PyLog allows users to focus on algorithm specification and can further simplify the programming of host-accelerator systems.

HeteroCL [10] is one recent work that builds on the TVM framework [9]. HeteroCL is built as an API library of the Python Language, and its compiler is a runtime compiler. When HeteroCL code runs, it makes API calls to construct computation patterns and computing schedules from Python-syntax statements and generates synthesizable C code for Merlin HLS compiler [14]. For example, HeteroCL exposes APIs to perform code transformations such as loop pipelining, loop unrolling, quantization, etc. Although both HeteroCL and PyLog use Python syntax, their approaches are very different. HeteroCL is more like a Python library that is used to describe computation flow, instead of an implementation of a subset or extension of the Python language.

On the contrary, PyLog directly implements a compiler that supports a subset of the Python language. PyLog code is compiled by the ahead-of-time Python language compiler in PyLog and the code that programmers write is the direct input to the compiler. PyLog's ahead-of-time compilation setting has several advantages. First of all, this enables maximum flexibility of language design, and makes it very easy to be extended. There is no constraints or limitation on the language design. Second, PyLog is designed to be compatible with standard Python grammar, and Python programmers can immediately start to code in PyLog. The existing Python code can be simply synthesized without much modification. Another difference between HeteroCL and PyLog is that, with HeteroCL, programmers still need to manually apply transformations and optimizations using HeteroCL APIs, while in PyLog, the compiler is responsible for implementation and optimization by default. Programmers can use pragmas to force the compiler to generate a specific implementation of the PyLog code, if they choose to do so.

Dahlia [15] is another high-level programming language that compiles to HLS C code. Dahlia uses Scala-like syntax and it uses a type system to enforce design constraints in the HLS code so that unrolling and memory partitioning factors match. Similar to HLS flow, Dahlia requires users to specify design pragmas. PyLog doesn't require manual annotation of HLS pragmas. On the contrary, PyLog compiler automatically inserts pragma to optimize the design.

There are also several works that use Python and other high-level languages like Scala and Haskell as hardware-description language (HDL) [16], [17], [18], [19]. The biggest difference between PyLog and these works is that PyLog flow is a high-level synthesis flow, and it does not require hardware knowledge to program in PyLog. These previous high-level HDLs elevate the syntax of hardware description languages, but users still need hardware expertise to use these HDLs.

SECTION 3PyLog Programming Model
3.1 Overview
PyLog presents a unified programming model for host and accelerator logic with consistent syntax and semantics. This seamless host-accelerator programming model enables agile system design, convenient functional simulation, and flexible design space exploration.

Listing 1 shows a simple example of PyLog program that describes both host and accelerator. This example contains two top-level functions, preprocess and compute. Function compute is decorated with a Python decorator @pylog, which make it a PyLog kernel function, and it will be synthesized into a hardware accelerator on FPGA by PyLog. With @pylog decorator, programmers can easily specify accelerator function in the existing Python code.

As shown in the example, both host and accelerator are programmed with Python at the same abstraction level. The host and accelerator interact with each other seamlessly in a natural way. PyLog closes the gap between the abstract levels of host programming and FPGA accelerator programming, and enables efficient system-level host-accelerator co-design.

Listing 1. A Simple PyLog Example
Fig. 2 illustrates the overall PyLog flow and the target FPGA system architecture. When PyLog user runs PyLog program with a standard Python interpreter, the @pylog decorator calls the PyLog compiler to compile the decorated PyLog kernel function into HLS C code. Then, system generator synthesizes hardware logic based on the generated HLS C code and integrates all the system components to create a complete FPGA design. The generated FPGA bitstream is used to configure the hardware resource and interconnects on FPGA. The rest of PyLog program is interpreted by the standard Python interpreter, and this part is the host code that runs on the host CPU. When the decorated function is called, PyLog runtime is invoked to program FPGA, allocate and populate memory and invoke FPGA to accelerate the computation. PyLog runtime uses PYNQ [20] and XRT, which are runtime libraries from Xilinx. The lower half of Fig. 2 shows an example of PCIe-based FPGA platform. Note that PyLog can support both PCIe-based high-performance FPGAs and low-power SoCs and MPSoCs (MultiProcessor System on a Chip). CPU and FPGA interact through memory-mapped I/O ports as well as configuration registers on FPGA side.


Fig. 2.
The PyLog flow and system architecture example.

Show All

PyLog requires the arguments of the PyLog functions to be NumPy arrays or NumPy scalars. From these NumPy object inputs, PyLog collects type and shape information of the arguments to the top function, which is the initial information for the type inference engine in PyLog.

The PyLog compiler is available as a Python library. To run FPGA synthesis or run FPGA accelerator, users simply run the whole program with standard Python interpreter. When the decorated Python function compute is called (line 13 of Listing 1), PyLog compiler is invoked to look for the synthesized design for the decorated function. If the function has not been synthesized before, PyLog will compile the decorated compute function, generate compute FPGA IP, integrate with other IPs into a complete FPGA design, and finally synthesize FPGA hardware design and get FPGA bitstream and configuration files. If there exists a synthesized design for the decorated function for the target FPGA platform but the design has not been deployed in the target FPGA, then PyLog will program the FPGA, allocate memory, populate memory space with input data, and call FPGA accelerator and collect computing results. To run the FPGA accelerated programs, users do not need to write any extra FPGA specific code, and they will not notice any difference between the ways of running the code on CPU and on FPGA. All the underlying CPU-FPGA interactions are taken care of by the PyLog runtime. PyLog synthesis flow and execution flow share the same piece of input code, which is very similar to a regular Python code, except the decorator @pylog. Both synthesis flow and execution flow are fully automated.

PyLog users can also pass a “mode” string to @pylog decorator to configure PyLog mode. For example, @pylog(mode=‘cgen’) runs only HLS C code generation flow. The list of possible PyLog modes is shown in Table 1.

TABLE 1 PyLog's Built-in Modes
Table 1- 
PyLog's Built-in Modes
PyLog uses Python syntax and it is friendly to both software and hardware developers. PyLog has a built-in type inference engine that can infer the types of objects in the program. PyLog supports basic Python operations and expressions as well as several high-level operators, which makes decription of computation flow intuitive, efficient, and natural (Section 3.2). Besides, PyLog allows programmers to nest operators to express complicated computation patterns. Offset slices and nested operators significantly simplify code and greatly increase the expressiveness of PyLog (Section 3.3). External HLS IP libraries are also supported in PyLog to allow users to reuse existing HLS designs (Section 3.4). In addition to the expressive high-level PyLog operators and external IPs, PyLog also supports data type customization and computation customization (Section 3.5). In addition, PyLog naturally allows users to simulate accelerator behavior in Python (Section 3.6). These capabilities make PyLog expressive and flexible enough to describe many different computation patterns across application domains. The rest of this section will describe all the features in detail.

3.2 High-Level Operators
In addition to the frequently used standard Python keywords and built-in functions, PyLog provides a set of high-level operators that describe common computation patterns. These high-level operators allow user to express computation at high level without specifying implementation details. Each high-level operation can have multiple valid implementations. Fig. 3 shows an example of generating multiple valid implementations from a PyLog map operation. PyLog can generate multiple versions of HLS C implementations in different styles, which corresponds to different hardware structures, e.g., shift registers, systolic arrays, etc.


Fig. 3.
Different implementations generated from the same PyLog code.

Show All

Array Operators. PyLog supports a set of commonly used NumPy-style binary array operators. Listing 2 shows two examples of array operations. The first example (line 2) assumes that a and b are multi-dimensional arrays with the same shape. PyLog can infer the types and shapes of the operands (a and b) as well as the target (c), and further infer the binary operations “+” and “=” (assignment) to be array operators. PyLog generates the optimized parallel for loops to implement this array operation. PyLog also supports indexing and slicing that adds flexibility to expressions as in NumPy and native Python.

PyLog slicing style is the same as that in Python. Slice expression “start:end:step” means a sequence of indices with “start” as the first index, “start+step” as the second index, etc. The index continues until it reaches end (but not including end). Note that this is consistent with Python slice's left-inclusive and right-exclusive definition. Any of start, end or step can be left out. If step is left out, its value is 1 by default. If start is left out, it means that the slice starts with 0. If end is missing, it means that the end value equals to the size of that dimension. Using -1 for end means that the end is the size of dimension minus one. the PyLog compiler can infer the actual range of dimensions and indices according to the shape of the array under consideration. For example, a[::2] means a sub-array consisting of every other element in a.

Line 5 in the Listing 2 shows an example of using slicing and indexing to apply array operations to sub-arrays of the original arrays. In this example, elements in every other row from row 2 to row 64 of z (a total of 32 rows, note that row 66 is excluded) receive the product of variable x and all elements in y that have index 6 in the second dimension and all the indices except the last one in the third dimension. The compatibility between the input and output arrays of this operation is checked by PyLog's type system. Also, the alignment between the input and output elements is automatically set by PyLog. For example, assume that x is a scalar variable. If z is a 100×8 array and y is a 32×16×9 array, there will be 32×8=256 elements in z and the same number of elements in y involved. z[2,0] will receive x * y[0,6,0], and z[4,4] will receive x * y[1,6,4]. In general, for all the 256 y elements involved, multiplication of x and y[m,6,n] will be assigned to z[2*(m+1),n].

Note that x can be a scalar or a vector, which will make the meaning of “*” different (vector element-wise multiplication versus vector linear scaling). Again, PyLog is able to infer the types and shapes of all the arrays with indexing and slicing, and determine the exact meaning of each operation. It will also check whether the operations are valid by comparing the shapes of sub-arrays. Array operators plus the slicing expressions support succinct and clear specification of linear algebra algorithms such as convolution and matrix multiplication.

Listing 2. Array Operation Examples
map Operator. PyLog supports a built-in map operator, which is an extended version of the Python map function. Similar to the map function in Python, map operator in PyLog can be used as map(f,o1,…,on) to repeatedly apply a function f to the n iterable objects o1,o2,…,on where all objects must have the same shape. By default, the PyLog map operator behaves the same way as the Python map operator. In this case, f is defined with n formal parameters p1,p2,…pn, each of which refers to an element of the corresponding object that f is to be applied to in the map operator. For example, in Line 2 of Listing 3, x (p1) refers to an element of vec_a (o1) and y (p2) refers to an element of vec_b (o2). The application of f to these two iterable objects result in iteratively invoking f with pairs of vec_a and vec_b elements as the actual parameters. That is, in the ith iteration of the map implementation, f takes the ith element of vec_a and the ith element of vec_b, add them together, and assign the sum to the ith element of out. In many cases, function f is a lambda function (anonymous function), as shown in Listing 3. In the same example (Line 2 of Listing 3), the lambda function is an addition function whose output is produced by adding the values of two input parameters together.

Listing 3. PyLog map and dot Examples
Beyond the basic features, PyLog map further supports an extension to allow the function f to access any number of elements in the iterable objects by specifying an offset or an offset slice expression that specifies a collection of offsets. The offsets are defined based on the iteration indices. For example, in Line 5 of Listing 3, in the ith iteration of map, the lambda function accesses vec[i-1] (specified as (x[-1]), vec[i] (x[0]), and vec[i+1] (x[1]) in the function body. Thus this example is a 1D convolution with a 3-element filter of w0, w1, and w2. Fig. 4 visualizes a few map examples. Fig. 4a shows a general case of 2D map where offsets are used to access neighbor elements. Figs. 4b and 4c show examples of using PyLog map to describe 1D and 2D convolution respectively. Note that this offset extension can naturally describe stencil operations. PyLog compiles stencil code described with map operator and connects to SODA [21] to generate highly efficient stencil accelerators.

Fig. 4. - 
PyLog map operator examples. (a) 2D stencil, (b) 1D convolution, (c) 2D convolution.
Fig. 4.
PyLog map operator examples. (a) 2D stencil, (b) 1D convolution, (c) 2D convolution.

Show All

dot Operator. In PyLog, dot is defined as an element-wise multiplication followed by a sum reduction. In other words, dot(a, b) is equivalent to sum(a*b), where a*b is an element-wise multiplication and the sum operator calculates the sum of all elements in the iterable object. For example, Line 8 of Listing 3 performs a dot product between row i of matrix and in_vec and assign the output value to the ith element of out_vec. The dot operator is introduced in PyLog to simplify programming and expose more optimization opportunities to the compiler. This operation is frequently used in many different applications, e.g., image filtering, matrix multiplication, stencils, etc.

Custom Operators. PyLog allows users to define custom operators as PyLog functions, inside a PyLog decorated top function. Similar to other operators, PyLog can infer the types and shapes of operands and output of custom functions by propagating type information from input to output through the whole function. These user-defined functions can be reused to simplify programming. These custom functions will be synthesized into HLS C functions.

3.3 Offset Slicing and Operator Chaining
The PyLog map offsets and offset slice expressions can be used for accessing higher dimensional formal parameter arrays in map operator. For each dimension, the offset can be a single number (e.g., 1, which means an input element in that dimension with offset 1 is accessed in an iteration), a slice (e.g., -1:2, which means three elements in that dimension with offsets -1, 0, and 1 are accessed in an iteration), or the entire dimension (:, which means all elements in that dimension are accessed in an iteration). For example, in Line 11 of Listing 3, the map operator applies the lambda function to all the elements in 2D arrays ma and mb. In iteration (i, j) of map, the offset expression x[0,:] means that the function accesses all elements of the ith row of ma and y[:,0] means that the function accesses all elements of the jth column of mb. That is, the map and lambda functions, chained together in Line 11, perform a dot product between the ith row of ma (x[0,:]) and the jth column of mb (y[:, 0]) and assigned the dot product value to out[i,j]. The whole expression completes the matrix multiplication.

Note that the map offset slices for accessing formal parameter arrays should not be confused with the slicing of PyLog arrays. Assume that x is a formal parameter of a lambda and vec is a PyLog array, offset slice x[-1:2] specifies three accesses to the formal parameter with offsets -1, 0, 1 whereas vec[1:-1] produces a slice that consists of all elements of vec except the first and the last elements (i.e., all internal elements of vec).

The PyLog offset extension and operator chaining enable Python developers to intuitively and succinctly express common computation patterns in various application domains. For example, in Fig. 4c, img is the input to a 2D convolution, and w is a 3×3 convolution filter. The slice [1:-1,1:-1] applied to img extracts out the sub-array excluding the outermost elements at the edges. The lambda function is applied to each element in the extracted array, img[1:-1,1:-1]. The parameter to the lambda function is a, which corresponds to each element in the array. The offset slicing expression applied to a, a[-1:2,-1:2], expresses a sub-array consisting of the current element a as well as neighbor elements around the current element a.

The dot operator multiplies the 3×3 square around the current element a with the convolution weight w (which is also a 3×3 square) element-wisely, and does a summation reduction to get the single convolution output at the current element a. This dot operation is repeatedly applied to each element position in img[1:-1,1:-1], and this completes the whole 2D convolution. Note that PyLog can infer the shape of each object involved in the computation and users do not need to give any hints about object shapes. Details of PyLog type inference will be presented in Section 4.

Listing 4. Variants of 2D Convolution in PyLog
In addition to the basic 2D convolution, variants of more general convolution can also be expressed easily in PyLog, as shown in Listing 4. Dilated convolution with dilation value of 2 can be described (line 2) by simply replacing [-1:2,-1:2] with [-2:3:2,-2:3:2] in the basic example above. Slice “-2:3:2” means the sequence of indices of “{-2, 0, 2}”, therefore a[-2:3:2,-2:3:2] represents the shape of a dilated convolution filter. Similarly, convolution with non-unit stride can also be described by specifying step in the index slices of img (line 5).

Note that these high-level operators only describe the arithmetic relations between array objects without specifying any actual implementation details. For example, the map operator describes the repeated applications of a function to all the elements in an array, but it does not prescribe any ordering when iterating through these elements. In traditional HLS, the similar computation would be expressed with nested for loops, which actually implies an iteration order. The HLS quality heavily depends on how the computation is expressed, which is a key reason why it is hard to create optimal hardware design with HLS flow. In the PyLog flow, the actual order of iterating in map operation is left for the PyLog compiler to decide. This approach not only simplifies programming, but also enables better design optimization and code generation. Given the high-level operators, PyLog compiler gets full information about the computation pattern. It knows exactly where data dependencies are in the code, which allows it to perform aggressive loop transformation and other code optimizations. How PyLog performs code optimization will be discussed in Section 4.

3.4 HLS C Library Integration
In addition to the high-level operators and Python modules described above, PyLog also supports integration of external HLS C library functions. This allows users to leverage the existing highly optimized HLS libraries. We develop an extensible library of HLS C operators that implement widely used NumPy functions. The interface of these operators is compatible with NumPy functions. PyLog users can call these HLS C operators in the same way as NumPy function calls. A few examples are shown in Table 2.

TABLE 2 Examples of NumPy Operators in HLS C

In spite of the similar interface, the specific implementation of our operator library is very different from NumPy. These PyLog external operators are developed in HLS C language and are highly optimized for hardware. These operators are implemented as HLS C code templates and are highly parameterized and configurable, and can be configured by PyLog according to data types and shapes, as well as design goals. Similar to the other PyLog operators and function calls, PyLog type inference engine will also do type inference and type checking for these external operators, to figure out the configurations of these operators and ensure the arguments and return of these operators to be valid. Type inference and checking is done based on the inference rules customized for each of these operators. Taking operator matmul(A,B,C) that performs matrix multiplication C=A*B as an example, PyLog type engine checks whether the shapes of A, B, and C have a pattern of (m,k), (k,n) and (m,n) or not. If yes, PyLog will configure the operator based on the inferred type and shape and instantiate the operator template and generate the corresponding HLS C implementation. If the dimensions are illegal, PyLog compiler will stop and output error messages accordingly.

Beside functionality parameters, these external operators also have configurable performance parameters, which configure the implementation of the operator. PyLog tunes these parameters to balance the performance and resource usage of the entire design based on the design goals. The performance parameters are listed in the second column of Table 2. Each operator can also be configured as pipelined or non-pipelined according to design needs.

3.5 Bitwidth and Compute Customization
In FPGA designs, integers and fixed-point data types are widely used to improve computation efficiency. PyLog allows users to specify integer and fixed-point data types with arbitrary precision. Listing 5 shows a few examples. The PyLog type system supports the propagation and compatibility checking of user-defined data types.

Listing 5. PyLog Arbitrary Precision Type Examples
Aside from the internal optimization passes, PyLog also allows users who want to have more control to customize computation and memory in the code. Table 3 summarizes the computation customization types in PyLog. for loops can be customized with unroll or pipeline pragmas. Operator map can be customized with reorder or tiling pragmas, which will be applied to the for loops generated from map operation. Loop reordering and tiling are safe in map operation since there is no loop-carried dependences in map operation.

TABLE 3 Computation Customization in PyLog

3.6 Functional Simulation Support
With the unified and seamless host-accelerator programming model provided by PyLog, programmers are not only able to program both host and accelerator efficiently, but also simulate the functionality of both host and accelerator easily. PyLog provides a pysim mode that allows the PyLog code to be interpreted by the standard Python interpreter. In pysim mode, all the PyLog specific operations and customizations will be replaced with native Python code. pysim can be used to help debugging and improve development efficiency.

3.7 Python Support and Limitations
PyLog supports commonly used Python features as well as PyLog-specific high-level operators. The supported features are summarized in Table 4. Please note that the Python features supported by PyLog is a subset of the complete Python. This is because that some Python features do not have well-defined and clear meaning for hardware synthesis, due to the language constraints in the backend HLS-C hardware design tools. Representative examples include object-oriented programming, functions with variable lengths of input arguments, loops with variable bounds, dynamic array allocations, etc. We are working to support a wider set of Python features by defining their behaviors in hardware synthesis.

TABLE 4 Supported Language Features
Table 4- 
Supported Language Features
To summarize, Table 5 compares the features of existing high-level FPGA design languages and flows with PyLog.

TABLE 5 High-Level FPGA Design Flow Comparison
Table 5- 
High-Level FPGA Design Flow Comparison
SECTION 4Compilation and Synthesis Flow
PyLog flow is a fully automated FPGA programming and synthesis flow. It consists of three parts, PyLog compiler, PyLog system generator, and PyLog runtime.

The PyLog compiler is a source-to-source compiler that translates PyLog source code to optimized HLS C code which can be synthesized by high-level synthesis (HLS) tools. The current supported HLS tool is Xilinx Vivado HLS [3] and Merlin compiler [14]. However, the analysis and optimization used in PyLog are not restricted to these HLS tools. The code generator of PyLog can be extended to support other HLS tools from other vendors without much difficulty. The compilation steps of PyLog compiler can be categorized into four stages: (1) front-end analysis and PyLog intermediate representation (PLIR) generation, (2) type inference and checking, (3) code optimization, and (4) HLS C code generation.

The PyLog system generator calls FPGA vendor's tools to synthesize generated HLS C code, integrates the FPGA application kernel with other system components, and generates FPGA configuration bitstream. The PyLog runtime configures and invokes FPGA to accelerate computation in user's application. This runtime is built on top of the Xilinx PYNQ library [20], which provides Python-based APIs to interact with the FPGA.

4.1 Front-End Analysis and PLIR Generation
When a @pylog decorated function is called with NumPy arrays and scalars as input arguments, PyLog compilation begins. In the first step, PyLog collects information about the data types and shapes of the input arguments to the top function. This information is passed to PyLog sub-modules. The source code of the top function is parsed by Python built-in abstract syntax tree (AST) module ast, which outputs the AST of the input PyLog code. AST is a low-level representation of the input code, which only represents the code syntax structure without semantics information. AST is the starting point for the following PyLog specific compilation steps. ast is the only module from standard Python interpreter that PyLog depends on. The following compilation steps in PyLog do not depend on the standard Python interpreter.

In the first stage, PyLog front-end traverses the PyLog AST, analyzes code structure, collects code information, and generates PyLog intermediate representation (PLIR). PLIR is a high-level tree-based data structure that is distilled from the AST and is more concise. It can be considered as a syntax tree at higher level. Each node in the PLIR corresponds to a sub-tree of the AST and contains the attributes that carry information about the code objects. PLIR describes the PyLog code structure, including high-level operations (e.g., map, dot, etc.), low-level controls (e.g., for, if else, etc.), functions, external IPs and configurations, etc. PLIR works as the hardware-agnostic IR for lowering high-level operations down to low-level controls. This lowering step is the implementation process of high-level operations.

Table 6 shows the categories of PyLog IR nodes. In general, the name of each node type is PL followed by the descriptive name. PLIR nodes include nodes representing high-level computation patterns and code structures, e.g., PLMap, PLDot, etc., as well as nodes representing lower-level generic statements and operations, e.g., PLFor (for operation), PLBinOp (binary operation), etc. Compared with the nodes in the PyLog AST, each node in PLIR is coarser in granularity, and the attributes of PLIR nodes carry more information. Each PLIR node has multiple attributes that either point to the other PLIR nodes or store the related information about this node. Essentially, the PLIR generation can be considered as a process where the PyLog front-end analyzer aggregates the sub-trees and structure information in the AST to form PLIR nodes.

TABLE 6 PLIR Node Categories
Table 6- 
PLIR Node Categories
In Table 6, low-level operations and expressions are the nodes that represent basic arithmetic operations, indices, basic expressions, etc. PLIR high-level operations are the nodes that represent high-level primitive operations in PyLog, e.g., map, dot, etc, and they are specific to PyLog. Code structures are the nodes that represent control flow and structure of the code, e.g., loops, branches, function definition, function calls, etc. The data fields of a PLIR node can point to other PLIR nodes; therefore, the whole PLIR is a tree that represents the code at high level.

4.2 Type Inference and Type Checking
One of the biggest challenges in compiling Python code is that Python is a dynamically typed language and there is no explicit type declaration in the Python code, which makes it hard for compiler to understand the actual semantics of some operations. For example, consider a simple expression “a + b”. Since a and b can be scalars or multi-dimensional arrays, this expression can mean scalar addition, or vector element-wise addition. The corresponding C code for this expression will be very different in these two cases. Without a context of types and shapes of a and b, it is impossible to know the actual meaning of this expression. To solve this problem, we implement a type inference engine in PyLog that infers the types and shapes of each code object in the PyLog program. With PyLog type inference support, PyLog users do not need to provide explicit type annotations or hints in PyLog program.

Algorithm 1. Type Inference and Checking
Input: Set of all variables S; set of input variables Sin⊂S; set of output variables Sout⊂S; Type mappings T defined on Sin∪Sout, PLIR with root Nroot.

output: Type mappings T defined on S, or TypeError.

Styped←Sin∪Sout

for each node n∈ PostOrderTraversal(Nroot) do

if n∈Styped then

np← Parent(n)

if np∉Styped then

T(np)← TypeRule(n→np,T(n))

Styped←Styped∪{np}

else if T(np)≠ TypeRule(n→np,T(n)) then

return TypeError

for each nc∈ Children(n) do

if nc∉Styped then

T(nc)← TypeRule(n→nc,T(n))

Styped←Styped∪{nc}

else if T(nc)≠ TypeRule(n→nc,T(n)) do

return TypeError

if S⊂Styped then return T

else return TypeError

In PyLog compiler, the type information of an object includes the types of data elements in the object as well as the number of dimensions of the object. The PyLog compiler uses PLType(ty, dim) to denote types, where ty is the type of data elements and dim is the number of dimensions. For simplicity, we use notation Tdt to represent PLType(t, d). For example, the type of a three-dimensional array consisting of floating-point numbers can be represented as PLType(float, 3), or T3float. Algorithm 1 shows the pseudo-code of the type inference algorithm in PyLog. Type inference starts with type and shape information of function arguments in the PyLog top function (which is carried by the input NumPy objects), and type information propagates across the whole PyLog code. Type inference is performed on PLIR and the result type and shape information is annotated to PLIR nodes. Type information propagation happens by performing type inference at each PLIR nodes, which is done according to the type inference rules.

Table 7 lists a few examples of type inference rules. As an example, when inferring types of objects in a map operation (the last example in Table 7), type inference engine first retrieves the type of operands Tnt1 from current context, which stores the types and shapes of visible variables at current point in the code. Then, the type engine is able to tell that the type of the argument to function f is T0t1, because map operator iterates through each element in the operand and f takes one element as input. Next, the type engine infers types of objects inside f and gets the return type of f, Tmt2. Finally, as a map operator, its return value is the aggregated results of f's return values, so the type is Tm+nt2. Fig. 5 demonstrates the type inference steps of chained operations.

TABLE 7 Type Inference Rules Examples

The shapes of objects are also inferred in the similar way, and happens at the same time as type inference. After type inference, the semantic of operations and expressions in the PyLog code is determined. When inconsistency is detected by the type system, a compilation error message is generated for the user. This makes debugging more user friendly than pure interpreter-based Python implementations. PLIR with type information is then ready for optimization and code generation.

4.3 Compiler Optimization
PLIR characterizes the high-level computation patterns in the input PyLog code, making it easier for PyLog optimizer to optimize the computation flow. Before the optimization stage, all the compiler analysis and transformation are independent of actual implementation. In this optimization stage, the compiler starts to consider and optimize the implementation for better performance. PyLog optimization consists of three steps, high-level operators lowering, loop transformation, and HLS pragma insertion.

High-Level Operator Lowering. In this first step, PyLog optimizer traverses through PLIR, and replaces high-level operators in PLIR with functional equivalent groups of generic low-level operators. For example, vector operators and map operators will be replaced with a group of for loop nodes that represent nested for loops. The information about the type of original high-level operators (e.g., map, dot, etc.) are kept and annotated to the generated for nodes. This information is useful in the following optimizations.

Loop Transformation. For each high-level operator, PyLog is capable of generating multiple styles of nested for loops, including plain sequential version, loop reordering version, loop tiling version, and mixed optimization version where loop reordering and tiling are combined. The type of loop transformation used depends on the operator type and the available hardware resources. For example, loop reordering and loop tiling are safe and possible in map operation since there is no loop-carried dependence in map operation. Note that each of these versions also has one or more tunable parameters.

Algorithm 2. HLS Pragma Insertion Algorithm
Input: Original loop structure L, improvement threshold T, total available area Atotal.

Output: Loop structure L with HLS pragmas configured.

lat,area← Evaluate(L) ⊳ lat: latency

for each loop L∈ PostOrderTraversal(L) do

if L has attribute unroll or pipeline then

continue

else

A←{unroll,pipeline,unroll.pipeline}

for each action∈A do

L.action()

latency′,area′← Evaluate(L.action())

if latency−latency′<T or area′>Atotal then

Undo L.action()

continue

else break

HLS Pragma Insertion. After PyLog optimizer replaces the high-level operators with nested for loops, the PyLog optimizer traverses the whole PLIR tree again and identifies the for loops in the code, then, it generates a loop structure tree that represents all the for loops in the code and their structure information. Each node in the tree is a PLOptLoop object that represents a for loop and its information. Each PLOptLoop object has actions of unroll(n) and pipeline(). Fig. 6 shows an example of for loop structure and its tree representation. With this representation, the loop structure in the code becomes very clear. Then, PyLog optimizer starts to insert HLS pragmas according to the optimization algorithm. Algorithm 2 shows the pseudo-code for one of the optimization heuristics. In this algorithm, the algorithm traverses the loop structure in the post-order. That is, the optimizer starts with the leaf nodes in the loop structure, which corresponds to the innermost for loops in the code. Then it moves to the parents of the traversed nodes. For each node, determined by the type of for loop (whether it belongs to map nodes or dot nodes or regular for loops), the optimizer tries a set of candidate HLS pragmas (or actions). In each trial, it evaluates the area and latency changes after applying that pragma. If the benefits is higher than a threshold, it accepts the change, otherwise it discards the change. Then it continues and moves on to next action or next node. Note that right now we are using a basic heuristic to guide the optimization. Other more sophisticated optimization/search algorithms can be used and plugged into the PyLog optimizer and guide the optimization. We leave this as a part of future work.

Fig. 5. - 
An example of type inference on chained high-level operators.
Fig. 5.
An example of type inference on chained high-level operators.

Show All


Fig. 6.
An example for loop structure and its tree representation. for loops of different types are in different colors.

Show All

Performance and Resource Models. We use the performance and resource models proposed in [22] to estimate the latency and resource utilization of the design points of each source code version. Based on these estimates, the compiler identifies the optimal design points. These optimal design points become the candidates of global design optimization. We use an example in Section 5.3 to further demonstrate the optimization mechanism.

4.4 Global Design Optimization
The optimizations presented in Section 4.3 focus on fine-grained operation-level and loop-level optimization, while in this section, we discuss system-level optimization in PyLog flow. Operation-level and loop-level optimizations identify the top implementation candidates for each of the high-level operators in PyLog. At the system-level optimization stage, PyLog optimizer finalizes the low-level design choices based on the global constraints and optimization targets. We formulate this global optimization problem as an integer linear programming (ILP) problem, solve the problem with an ILP solver, and finally get the optimal design choices.

At the operation-level optimization stage, for each high-level operator pi, the optimizer identifies the set of top m design candidates for this operator {p(1)i,p(2)i,…,p(m)i}, as well as the latency and resource estimates of these candidates. These candidates are the ones that achieve shortest latency among all designs with same amount of resource usage. Let's denote the latency and resource estimates with mappings L:p(j)i→Z+ and A:p(j)i→Z+ respectively. Note that for each type of FPGA resource, there will be one estimate function. To simplify the notations, here we only write down the formula for one type of resource. The same formula applies to the other types of FPGA resources. The goal of this global optimization stage is to identify the optimal choice of design candidates for each of the high-level operators so that the overall latency of the whole design can be minimized, while the resource usage meets the FPGA resource constraints. This optimization problem can be formulated as ILP problem as follows.

We define a binary indicator variable x(j)i∈{0,1} to denote whether or not we choose the jth candidate of the ith operator, i.e., p(j)i. Since only one candidate will be chosen for a specific operator, we have the constraint ∑jx(j)i=1 for each operator pi. Given available resource Amax on FPGA, the optimization problem is
minx(j)i∈{0,1}∑i,jx(j)iL(p(j)i)(1)
View Source
subject to∑jx(j)i=1,∀i,(2)
View SourceRight-click on figure for MathML and additional features.
∑i,jx(j)iA(p(j)i)≤Amax.(3)
View SourceRight-click on figure for MathML and additional features.Please note that the summation here takes the control flows in the program into account. For example, if a high-level operator is called inside a sequential for loop, all the dynamic instances of this operator will be summed up. This formulated ILP problem is then sovled by an external ILP solver, and the solution corresponds to the optimal choices of high-level operators design candidates.

4.5 C Code Generation and System Generation
After optimization, all the high-level operators in PLIR have been replaced by low-level operators. PyLog code generator traverses through the optimized PLIR and generates C abstract syntax tree (AST), which is then translated into actual C code.

The PyLog system generator calls FPGA synthesis tools to synthesize generated HLS C code into FPGA IP block (Vivado HLS or Merlin compiler), and integrate the IP with all the other system components to create the complete FPGA design (Vitis or Merlin compiler). The whole system generation flow is fully automated.

4.6 PyLog Runtime
When the PyLog kernel function is called, PyLog automatically invokes FPGA to accelerate the program. First, it programs FPGA, then it allocates and populates arrays in CPU-FPGA shared memory space. Second, it invokes FPGA accelerator, and waits for FPGA to finish. Finally, PyLog collects computing results from FPGA and returns the results to the kernel function caller in the host program. This runtime is built on top of Xilinx PYNQ library [20].

SECTION 5Evaluation
This section evaluates PyLog flow in several different aspects, namely portability, language expressiveness, and performance.

5.1 Portability
PyLog flow is designed to be generic enough to support different FPGA platforms, from small low-power SoCs to large high-performance FPGA systems. The whole PyLog flow, including code generation, hardware generation, and PyLog runtime, can be used with a wide range of FPGA platforms. Table 10 lists the FPGA platforms that are currently supported by PyLog flow. When targeting a specific FPGA platform, one simply passes the platform name to the @pylog decorator, and no other code change is needed. For example, @pylog(mode=‘deploy’, board=‘aws_f1’) will execute the PyLog code using Amazon AWS F1 instance FPGAs. Exactly the same PyLog code with @pylog(mode=‘deploy’, board=‘pynq’) applied instead will run the program with PYNQ FPGA, assuming FPGA bitstreams have been generated with mode=‘hwgen’. Even though we use Xilinx FPGAs to demonstrate the capabilities of PyLog, it is possible to extend PyLog to support FPGA and HLS tools from other vendors, e.g., Intel. We leave this extension as a part of the future work. In Section 5.4 we list the evaluation results on both AWS F1 instance and ZedBoard. On both platforms, PyLog-generated accelerators are able to outperform optimized CPU performance and even manually designed FPGA accelerators. This demonstrates the performance portability of PyLog.

TABLE 8 Accelerator Performance Evaluation on AWS F1 Instance
Table 8- 
Accelerator Performance Evaluation on AWS F1 Instance
TABLE 9 Accelerator Performance Evaluation on ZedBoard
Table 9- 
Accelerator Performance Evaluation on ZedBoard
TABLE 10 Current Supported FPGA Platforms in PyLog
Table 10- 
Current Supported FPGA Platforms in PyLog
5.2 Expressiveness
To evaluate the expressiveness of PyLog, we compare the number of lines of code between HLS C code and PyLog code. To make comparison fair, for PyLog versions, we only use PyLog built-in high-level operators to express the benchmarks but not using other pre-built functions or libraries. The HLS C versions are HLS C code generated by PyLog from the corresponding PyLog version. This guarantees the FPGA designs of two versions are equal. Fig. 8 shows the results. With PyLog, on average only 30% length of code is needed to express computation, compared to the Vivado HLS flow.


Fig. 7.
Design space of various code versions of 2D array addition. (“ic12”: interchange loop 1 and loop 2; “tile2”: tile loop 2).

Show All


Fig. 8.
Length of HLS C code and PyLog code.

Show All

5.3 Design Space Exploration and Search
We evaluate the effectiveness of our compiler optimizations by profiling the latency and resource utilization of real-world workloads. Fig. 7 shows all the valid design points of a 2D array addition example, as well as the optimal design points. These design points are identified by PyLog compiler automatically. First of all, PyLog identifies the four valid versions of implementation, that is, “ic12”, “ic12+tile2”, “ic12+tile2+tile1”, and “ic12+tile2+tile1+ic23”. Here “ic12” corresponds to the version that interchanges loop1 and loop2, while “ic12+tile2” is the version that tiles loop2 after interchanging loop1 and loop2. Second, PyLog explores all the valid design points for each of these valid code versions. These design points correspond to different ways of inserting HLS pragmas. Fig. 7 uses circles with different colors to mark the design points from different code versions. The design points on the pareto curve are also marked in the figure. These are the optimal design points for the corresponding resource utilization. As we can see from Fig. 7, these optimal design points come from different code versions. Our compiler is able to identify the optimal design points from all the code versions. These design points on the pareto curve becomes the design candidates for the global design optimization, which is then solved by ILP solver.

5.4 Accelerator Performance
We evaluate PyLog performance with real-world benchmarks on both Amazon EC2 F1 f1.2xlarge instance [28] and ZedBoard. The benchmarks are from different domains and have varied computation patterns, including linear algebra, data analytics, stencil, sparse operations, etc. Amazon EC2 F1 f1.2xlarge instance is a cloud computing platform with 8-core Intel Xeon E5-2686 v4 CPU and a Xilinx Virtex UltraScale+ XCVU9P FPGA. ZedBoard is a low-power Xilinx ZYNQ-7000 based System-on-Chip (SoC) platform. The results from two platforms are presented in Tables 8 and 9 respectively. For ZedBoard results, the speedup numbers are the performance of PyLog generated accelerators over the CPU performance on ZedBoard. Based on the evaluation results on both AWS F1 and ZedBoard, we can see that PyLog is able to generate performant accelerators for both large and small FPGAs.

Table 8 shows the evaluation results on AWS F1 instance. The table lists FPGA resource utilization (look-up tables, registers, BRAMs and DSPs), design frequency (f (MHz)), design power (P (W)), and kernel execution time (T (ms)) of PyLog generated designs. Resource utilization, frequency, and power numbers are collected from Vivado post-implementation reports. TCPU is the execution time on AWS F1 CPU. The CPU baselines are optimized CPU implementations from [10] and other sources. We enable multi-threading whenever possible, as long as a NumPy operation has multi-thread implementation. THCL is the execution time on HeteroCL [10] generated accelerators. The HeteroCL accelerators are generated from optimized HeteroCL implementations that are available online. TPyLog is the execution time on PyLog generated accelerators. SpMV and histogram benchmarks do not have HeteroCL implementations available yet so we do not compare with HeteroCL versions for these two benchmarks. The stencil benchmarks (Jacobi-2D, Seidel, and Gaussian Filter) are compiled to generate Vivado HLS C code with external HLS library SODA [21]. Since HeteroCL uses the Merlin compiler as their general backend in the evaluation [10], we also compile the benchmarks to Merlin C code to allow further optimizations from Merlin so that we can fairly compare performance results from PyLog and HeteroCL. In terms of compilation time, PyLog HLS C code generation only takes seconds and therefore PyLog compilation time is negligible compared to Vitis synthesis that takes hours. The input data for the benchmarks are chosen to be same as in [10]. We leave the study on performance impact of data size as a part of the future work.

The last two columns in Table 8 show the speedup achieved by PyLog accelerator over CPU implementation and HeteroCL implementation respectively. On average, PyLog accelerators achieve around 3.17× and 1.24× speedup over CPU baseline and HeteroCL accelerators. PyLog can generate accelerators with better or similar performance compared with HeteroCL in most of benchmarks. Note that we uses PyLog generic backend to compile GEMM benchmark while HeteroCL uses HLS C based manually optimized systolic array backend for GEMM [30], which is highly optimized. This is the main reason for the performance gap in GEMM benchmark. After we add support for systolic array backends this gap will be filled. This is left as future work. The main sources of speedup achieved by PyLog are as follows. First, the high-level operators expose parallel processing opportunities and the compiler is able to insert HLS pragma with better insight. Second, PyLog compiles Python code directly and has native support for imperative programming. This enables users to have fine-grained control in hardware generation. Third, PyLog incorporates external highly optimized HLS libraries and it tunes the design parameters of these libraries to achieve good balance of performance and resource utilization. If we compare the resource utilization of accelerators generated by PyLog and HeteroCL [10], we can see the resource utilization numbers correlate to performance numbers. Benchmarks achieving better performance also utilize more resources. When comparing frequency, PyLog generated accelerators can meet 250 MHz constraint in all benchmarks except Gaussian Filter, the design of which has routing problems that lead to lower frequency. In general, PyLog is able to utilize available resources and achieve good performance with high frequency in most benchmarks.

SECTION 6Conclusion
In order to improve FPGA development efficiency and simplify FPGA programming, we built PyLog, an algorithm-centric Python-based FPGA programming and synthesis flow. PyLog flow compiles Python functions into optimized HLS C code, and generates a complete system including FPGA accelerator as well as host-side runtime environment. The built-in PyLog high-level operators and PyLog optimizer automate design implementation and optimization, which reduces the burden of FPGA developers. Evaluation results show that PyLog is expressive to describe different types of applications with fewer lines of code and it can accelerates high-level software applications effectively and achieve significant speedup compared to CPU baselines and even some of manual FPGA designs.