Currently, huge amounts of data are produced by edge device. Considering the heavy burden of network bandwidth and the service delay requirements of delay-sensitive applications, processing the data at network edge is a great choice. However, edge devices such as smart wearables, connected and autonomous vehicles usually have several limitations on computational capacity and energy which will influence the quality of service. As an effective and efficient strategy, offloading is widely used to address this issue. But when facing device heterogeneity problem and task complexity increase, service quality degradation and resource utility decrease often occur due to unreasonable task distribution. Since conventional simplex offloading strategies show limited performance in complex environment, we are motivated to design a dynamic regional resource scheduling framework which is able to work effectively taking different indexes into consideration. Thus, in this article we first propose a double offloading framework to simulate the offloading process in real edge scenario which consists of different edge servers and devices. Then we formulate the offloading as a Markov Decision Process (MDP) and utilize a deep reinforcement learning (DRL) algorithm named asynchronous advantage actor-critic (A3C) as the offloading decision making strategy to balance the workload of edge servers and finally reduce the overhead in terms of energy and time. Comparison experiments for local computing and wide-used DRL algorithm DQN are conducted in a comprehensive benchmark and the results show that our work performs much better on self-adjusting and overhead reduction.
SECTION 1Introduction
The past several years have seen the explosive growth of modern mobile devices due to a series of obvious advantages. First, they have access to a wealth of data suitable for processing locally or transmitting with low latency, which in return can greatly improve the user experience on the device. Then, processing massive data under edge network can significantly relieve the heavy burden of backbone bandwidth. At the same time, user's privacy can be better-protected through storing the data in network edge devices instead of uploading to a large cloud center. These features make it attractive and feasible to deploy diverse applications in certain edge scenarios that require extreme low delay, such as autonomous driving and remote surgery. But we can't ignore the fact that most edge terminals suffer from constrained computational resources and limited battery life, which poses a significant challenge to improve the quality of service one step further[1]. Derived from cloud computing era, the cooperation between cloud and edge is necessary at present. Computation offloading has been envisioned as a useful approach to overcome this challenge in cloud computing. Traditionally, tasks occupying large computational resources are offloaded to remote centralized clouds like Amazon EC2 and Microsoft Azure for processing. Then the results can be transmitted back to end devices. This approach actually enables those of weak computational capacity to process resource-hungry tasks. However, this approach still faces severe delay problems and gives heavy bandwidth stress to the backbone network. Moreover, transmission cost rises with the increase of distance. To address these problems, mobile device can offload tasks to cloudlets [2] or edge servers [3]. In this way, long distance offloading can be avoided and the transmission delay can be kept low. Different offloading strategies also help edge system make an optimal decision about resource allocation. But situation becomes complicated when users’ uncertainties are considered. It will result in the random production of tasks with diverse sizes. Besides, dynamic available resources of an edge server and wireless channels will also make the offloading more difficult. What's worse, resource management becomes more intractable when mobile devices are numerous. According to the report of Cisco Visual Networking Index, there will be more than 11.6 billion mobile devices by 2020 [4]. Hence, novel strategy for edge computing offloading is urgently needed.

Reinforcement learning has shown great performance in tackling these decision-making problems that are dynamic, scenario-driven or requiring self-awareness capability. With two characteristics: trial-and-error search and delayed reward, Alpha Zero beats the top human players in different aspects such as Chinese chess and GO through self-learning from the very beginning [5], [6]. So is it possible to utilize reinforcement learning to solve the dynamic offloading problem under edge conditions? The answer is definitely yes. Simple combination of reinforcement learning and computation offloading has been proposed in the work [7]. One step further, more comprehensive strategies with deep reinforcement learning are developed for the joint consideration of energy, radio resource, utility, mobility and computational complexity [8], [9], [10]. These explorations are of great inspiration that contribute a lot to the study of edge computing, but they have missed the key point of task offloading under edge scenario and their algorithms need improvement. Specifically, Q-learning is only capable in the field where its actions and states are discrete. The input and result must be in low dimension that constrain the task diversity. Obviously, it's contradictory in real situation. Then, despite Deep Q Network (DQN) is able to process high dimensional sequential data, it's also constrained by its simple discrete output and slow convergence speed. Last and most important, hierarchical system design for massive amount of mobile devices is missing in these models. Current edge systems are not practical enough. These motivate us to devise a novel framework and optimize the offloading strategy via more appropriate algorithm.

Thus, in this paper we first propose a double offloading framework subject to the dynamics of task randomness and device amount. Specifically, ESs can offload tasks between each other directed by a offloading policy. As illustrated in Fig. 1, user equipment (UE) in the range of a Pico Base Station (BS) chooses to offload computation-intensive tasks to the Edge Server (ES) or process normal tasks locally according to our time and energy cost criteria. When some UE incur task explosion, busy ES can offload a part of tasks to the idle ES which effectively avoids the task block and resource waste. Then, we formulate the decision making procedure as a MDP. In this process, action space and state space represent offloading decision set and task queue state respectively. Giving immediate reward and transition probability, an optimal offloading decision result is generated considering different task distribution. To obtain better convergence time and more accurate decision making results, which is usually in very large when UE numbers are massive, we design our offloading strategy based on deep reinforcement learning algorithm A3C. We name this system A3C-DO and experiments have shown the outperformance of our system.

Fig. 1. - 
Framework of edge computing scenario.
Fig. 1.
Framework of edge computing scenario.

Show All

SECTION 2Related Work
Resource management plays an important role in both cloud system and edge system. The effective distribution and deployment of storage, network bandwidth, computing resource can bring significant improvement for energy saving and latency reduction. Offloading aims at improving QoS by properly distributing computational load. Real scenarios are complex and dynamic which means offloading algorithm and computational architecture should have enough scalability. But it's tough for most models to handle problems derived from different scenarios. Models and offloading strategies have their capacity range. We will introduce several kinds of offloading models. At first, we discuss the single user systems with deterministic task model and stochastic task model. Subsequently, we introduce the management for radio resource and MEC server resource considering multi-user. Finally, management of the MEC systems with heterogeneous servers are discussed as the extension where server distribution and resource offloading are considered.

In single-user edge systems, offloading happens between sole user and sole edge server or several edge server. Deterministic tasks or stochastic tasks will lead to different task models and solutions. Focusing on the energy consumption and executing latency of UEs, Wang et al. [11] aimed to reach the goal by optimizing transmitting power and rate. The variable substitution technique was applied to solve the optimization problem, a nonconvex problem, which was recast as a convex problem. Sequently, the above problems were extensively studied under multi-cloud server condition. Mahmoodi et al. [12] considered an energy and cost optimization problem under a complicated scenario where the tasks were arbitrary dependent rather than sequential. The task relation graph was differentiated by a parallel parameter and a sequential parameter. And the independent task sequences were able be executed currently. Based on these tricks, a comprehensive offloading scheme was designed to jointly minimize the energy consumption and latency. Stochastic task models are more complex. For the purpose of balancing the power consumption and the application execution time under a threshold, Huang et al. [13] designed an adaptive offloading algorithm making use of Lyapunov function. The low complexity characterized the algorithm. The energy consumption and execution time comparison under different network scenarios (WI-FI and 3G) were presented. In [14], some dynamic problems were jointly considered include whether to use local CPU resources or cloud resources, how to distribute the tasks, what was the appropriate CPU clock speed and how to control the network interface. An algorithm named DREAM was proposed to solve the problems mentioned above by using Lyapunov optimization. Trace-driven simulations were realizing in 3G/LTE/WiFi condition. A DREAM architecture was also designed and applied in an Android platform. Some experiments were conduct to elucidate the advantages of this architecture in energy saving and delay decreasing.

As for multi-user systems, cooperative management involves radio resources and computational resources. Centralized systems and distributed systems require different offloading schemes. In centralized system, MEC server is the management center which handles the computation requests and distributes the resources among the mobile devices. In [15], systems with different cloud resources and mobile access technologies were jointly considered. For the system using TDMA with adequate computational resources, a convex optimization problem was formulated and an offloading priority function was proved useful. Under finite cloud capacity condition, the solution was derived from the strategy in infinite condition adding a cloud capacity upper bound. Mao et al. [16] presented an algorithm to balance the power consumption and the delay in multi-user MEC system with the requirement of task buffer stability. Resources distribution strategy was designed considering network condition and the influence of various parameters was expounded. In distributed systems, game theory and decomposition techniques are often useful. Ma et al. [17] concentrated on the multiuser-multichannel offloading problem. Because centralized optimization was proved NP-hard, a game theoretic was utilized for the distributed optimization. The game model derived from the distributed computation offloading decision was proved always existing a Nash equilibrium. Corresponding algorithm was proposed to obtain the max converging time. Further research was extended in multichannel wireless contention environment. Guo et al. [18] considered the computation offloading problem under the constrains of task workload, maximum execution delay and transmitting cost. They designed a distributed energy-efficient resource scheduling algorithm in which dynamic voltage and frequency scaling technique were applied to adjust the computation of local device or cloud and finally reduced the energy cost.

In edge systems with heterogeneous edge servers, server selection and server cooperation is very beneficial to the whole systems. In [19], the local cloud and Internet cloud resources were jointly considered to maximize the probability of a task successfully executed within the constrain. When the given threshold is exceeded, tasks were offloaded to the Internet cloud. The proposed scheduling policy utilized a priority queue, a N-dimension Markov chain, to consider the delay requirements of applications which gives priority to the delay-sensitive applications in the local cloud. In [20], resource sharing was further studied considering the geographical elements. In the Geo-distributed mobile cloud computing scenario, devices had access to the cloud resources that are geographically close. In order to increase the resource utilization, the resources were leased from resource-rich service providers to the resource-deficient service providers. Remote cooperation and local cooperation were described. Then, the resource cooperation problem was formulated in the coalition game theory framework. Two strategies were presented to simulate the cooperation.

It's noteworthy that the device heterogeneity in edge scenario leads to the difficulty of effective offloading. Single classical method tends to show limited performance when facing dynamic offloading problems with multiple constrains. The utilization of reinforcement learning has greatly solved this dilemma. The work in [7] built a simple offloading model based on Q-Learning in which one edge server received a few tasks from one device. Although optimal offloading decisions are made with the help of Q-function subject to device capacity and energy, the model is not robust enough. Sequently, powerful Deep Q-Network is applied widely into the study of edge computing offloading. Duc Van Le and Chen-Khong Tham designed an algorithm that used DQN to decide the optimal offloading decision jointly considering task loss probability and user mobility [8]. Also adopting Deep Q-Network based algorithm, the works in [9] and [10] payed more attention to the tasks distribution considering radio resources and their models were more complete. In [9], optimization problem was divided into offloading part and allocation part to reduce computational complexity and an original action selection method was proposed to improve efficiency. Despite these works have proved the feasibility of using deep reinforcement learning for offloading optimizing, their models are unable to reveal the scenario adaptation capacity, which is our most interested point. In comparison, we devise a two-layer architecture that can guarantee the offloading from UE to edge server and among edge servers. Through this mechanism, we can properly allocate regional resources in a higher level which not only improves efficiency of dynamic task offloading but also avoids resource idleness. Then, we utilize a deep reinforcement learning algorithm that is more appropriate under edge condition. Compared with Q-learning that can only play a role in processing uncomplicated problems with a few states, our algorithm is able to optimize sequential decision-making problem in larger state space. Compared with DQN-based strategies, our strategy requires less computational resources and has better convergence speed. The implementation of DQN-based strategy is dependent on powerful GPU. Our strategy can be realized in devices with only multi-core CPU which is more friendly to edge devices with limited computational resources.

SECTION 3System Model
We first introduce our system model. We consider a two-layer system model which consists of various UEs and ESs. On the bottom of this structure is UE in which a task will be produced with a certain probability at the beginning of every time slot. We set this probability to simulate different computation densities of different UEs and areas. With the help of our offloading strategy, users can offload the computation to the ES. The communication served in this area is provided by the Pico BS. A UE will maintain a task queue for those tasks processed locally. Each ES will also maintain a task queue based on first-come first-service (FCFS) principle. It's certain that waiting time will influence the efficiency of tasks processing and surplus computational resources of other ESs are available. So an ES can offload the tasks with long waiting time to other ESs through a Macro BS and execute the other tasks in the queue. After that, results will be sent back to the user. To precisely simulate the transmission and computation process of the edge system, we devise appropriate communication model and computation model which we will introduce next in details. For the convenience of reading, we summarize the notations used in this paper in Table 1.

TABLE 1 Notations

3.1 Communication Model
We first introduce the communication model. Traditional wireless cellular networks are typically deployed as homogeneous networks using a macro-centric planning process which often leads to poor flexibility in terminal numbers, QoS requirements, base station configuration and so on [21]. This motivates us to consider heterogeneous networks for the management of the uplink and downlink of UEs and ESs deploying both macro base stations and pico base stations. The placement of pico station just needs to consider the coverage issues and traffic density roughly. Its low transmitting power and small size also brings flexibility. We set a pico base station for the communication between UEs and an ES inside an area. Macro base station is larger and more powerful. The location selection requires more strict consideration. We set a macro base station to support the communication of edge servers. According to [22], we can compute the uplink rate of a UE n that offloads the computation to the edge server s with (1). And the formula to compute the uplink rate between edge servers is the same.
r(s,n)=Wlog2{1+Ps,nHs,nw0¯+∑i≠s,j≠nPi,jHi,j}.(1)
View SourceRight-click on figure for MathML and additional features.Here W is the channel bandwidth and Ps,n is the transmission power of UE n offloading tasks to the server s. Hs,n denotes the channel gain of UE n offloading tasks to the server s caused by the path loss and shadowing attenuation and w0¯ represents the background noise.

3.2 Computational Model
We let Ts,n:=(Bs,n,Ds,n,τs,n) denotes the computation task of UE n in the region served by edge server s which can be computed locally or offloaded to the edge server s. Here Bs,n represents the size of computational input data (i.e., the byte size of a program) and Ds,n represents the entire number of computing cycles required to accomplish task Ts,n which is assumed the same whether executed locally or offloaded. Then τs,n denotes the upper bound of tolerable delay of task Ts,n that is very important for guaranteeing QoS under heterogeneous network [23]. This will be a key reference criteria for the decision of offloading. Based on [1], [24], we can acquire the necessary information to calculate Bs,n and Ds,n. It's assumed that the task can not be partitioned to be offloaded partly which means each UE should process an integral task or offload it all to an edge server. We use As,n={(a1,1,…,a1,|n|),…,(a|s|,1,…,a|s|,|n|)|ai,j∈{0,1}} to define the computational offloading decision vector in which ai,j=1 denotes UE j offloads its task to edge server i and 0 denotes local computing.

3.2.1 Local Computing
For local computing, UE n in the region served by edge server s processes its task Ts,n in the mobile device. Considering the continuity of task generation, we set a buffer Qus,n={Tu1,Tu2,…,TuN|TuN∈{Ts,n|as,n=0}} as the task queue of UE n. When a task is pushed into the queue, it might has to wait for processing due to the FCFS principle. We use tlefts,n to denote the waiting time of task Ts,n for pre-existing task processing. Experimentally, we update tlefts,n at the beginning of each time slot by removing finished tasks and adding tasks with higher priority than Ts,n. It's uncomplicated to design this function in programming. But the detailed and formulated description in the paper seems quiet unfriendly which, at the same time, is not crucial for explaining the logical structure of our system. So we concisely set the pre-processing time cost for Ts,n as tlefts,n. We define fls,n as the computational capacity (i.e., computing cycles per second) of UE n. Different UEs have different computational capacities. Then, the computation time of the task Ts,n is shown as
tls,n=Ds,nfls,n+tlefts,n,(2)
View Sourceand the energy cost of processing this task is given by
els,n=γ−1Ds,n,(3)
View Sourcewhere γ denotes the computing cycles per unit energy consumption and γ=665 Mcycle/J according to the practical experiment in [25].

By combining the time cost (2) and energy cost (3), we can give the total cost of local computing as
Cls,nsubject to =λts,ntls,n+λes,nels,nλts,n+λes,n=1,λts,n,λes,n∈[0,1],(4)
View Sourcewhere λts,n represents the weight of time cost and λes,n denotes the weight of energy cost. In order to improving the flexibility of the model, we set a constrain for λts,n and λes,n so that we can change the weight of time cost and energy cost depending on different requirements of UE. Practically, appropriate weights about time cost and energy cost can be determined by applying multiple criteria decision making and multi-attribute utility theory [26].

3.2.2 Offloading Computing
For computation offloading, UE n has to upload the task Ts,n to the pico base station through wireless access and the base station forwards the task to the edge server s. According to the communication model proposed before, the required time and energy for transmitting the task can be computed respectively as
tups,n=Bs,nrs,n,(5)
View Sourceand
eups,n=Ps,nBs,nrs,n,(6)
View Sourcewhere Bs,n denotes the size of input data, rs,n stands for the uplink rate in the wireless channel and Ps,n is the transmission power mentioned above. After transmitting, edge server s will process the task Ts,n. We use fos to represent the computational capability of edge server s (computing cycles per second). The execution time of the task Ts,n offloaded from UE n can be denoted as
tps,n=Ds,nfos.(7)
View SourceEdge server s receives different tasks from the UEs covered by the pico station. We design a server task queue Qs={Ts1,Ts2,…,Tsi,…,TsN|Tsi∈{Ts,n|as,n=1}} to allocate the tasks based on the FCFS principle in which i represents the location label of Ts,n. In the task queue, a task has to wait until the tasks before it has been finished. The waiting time consists of two parts: the remaining tasks in the queue before this time slot and the offloaded tasks which arrive earlier. As mentioned above, We set tlefts,n to represent the waiting time of task Ts,n, which is described as Tsi in the server task queue Qs, for pre-existing task processing. Then, the total waiting time twaits,n can be calculated as
twaits,n=∑j=1i−1as,jtps,j+tlefts,n,(8)
View SourceRight-click on figure for MathML and additional features.where ∑i−1j=1as,jtps,j represents the processing time of tasks before task Ts,n in the server task queue Qs. Combining (5), (7) and (8), we give the total time cost of offloading computing as
tos,n=tups,n+tps,n+twaits,n.(9)
View SourceAccording to (6) and (9), we can calculate the total cost of the offloading computing of task Ts,n including time and energy as
Coffs,n=λts,ntos,n+λes,neups,n,(10)
View Sourcewhere the two coefficients are subject to λts,n+λes,n=1 and λts,n,λes,n∈[0,1].

3.2.3 offloading Between Servers
Practically, the number of UE in an area has a strong connection with the quality of service. Offloading overfull tasks to a edge server will lead to severe block in the task queue. Long waiting time of some tasks actually offsets the time bonus obtained by offloading. This problem seems to be unavoidable in a UE-ES system. However, the communication between different UE-ES systems can bring a solution to this problem. When edge server s is constrained by queuing block, we can find a resource-surplus edge server s’ as the next offloading target. According to the practical average tolerant service delay, tasks beyond the time threshold ηs will be offloaded to edge server s’. Before offloading tasks to the other ES, we check the tasks of there queues. Through comparison of their longest waiting time, we choose the ES with minimum workload as the target. Therefore we can respectively give the tasks transmission time from s to s’ and the execution time cost as
tup′s,n=Bs,nrs,s′,(11)
View Sourceand
tp′s,n=Ds,nfos′.(12)
View SourceConsidering that edge server s’ may receive tasks from different edge servers, we design to maintain a buffer queue Qtemps′ for those tasks and they are sorted in the queue based on their arriving order. We push the tasks from other edge server into the task queue behind local tasks for they have lower priority comparing with local tasks. They will be executed after local tasks. So for the task Ts,n offloaded from edge server s, we consider the waiting time as
twait′s,n=∑j=1i−1as′,jtps′,j+tlefts′,n,(13)
View Sourcewhere tlefts′,n is the waiting time of Ts,n for pre-existing task processing when Ts,n arrived. Combining (11), (12) and (13) we give the total time cost of the task that offloaded to other edge server as
to′s,n=tup′s,n+tp′s,n+twait′s,n.(14)
View SourceThen, we can determine the entire cost for offloading twice in terms of time and energy as:
Coff′s,n=λts,nto′s,n+λes,neups,n,(15)
View SourceRight-click on figure for MathML and additional features.where the two coefficients are subjected to λts,n+λes,n=1,λts,n,λes,n∈[0,1].

Normally, we need to consider the time cost for data downlink. But many studies such as [7], [24] choose to overlook the influence of downlink transmission because of two main reasons. First, the result size of applications like objection detection is much smaller comparing with the size of input data. These applications collect images and graphs for analyzing, returning the type description of objects and probability. Besides, with the development radio communication technology, high-layer edge servers at the base station are resource-rich and able to provide high-speed downlink rate that is nearly ten times of the uplink rate of UEs. Therefore, we ignore the downlink delay and just consider the bottleneck optimization in data transmission. With the system model introduced above, we will devise a reinforcement learning based strategy for multi-user offloading decision-making policy in mobile edge computing.

SECTION 4Computation Offloading via Parallel Actor-Critic Reinforcement Learning
4.1 Offloading Problem Formulation
The motivation of using reinforcement learning for offloading decision making is to improve the flexibility of our system under different circumstances. Although the number of UEs and edge server may change, the main procedure of tasks processing is fixed. Tasks emerges randomly in UEs and part of them are offloaded to the edge server via our policy. Edge server processes these tasks and return the results back to UEs. The state of an edge server in current time slot is only affected by its state in the last time slot (e.g., the remaining tasks in the queue). Essentially, the property behind this is called Markov Property and this procedure can be formulated as a MDP. Generally, we can denote a MDP problem with a tuple M={ST,A,P,R}, where ST represents the state space and A denotes the action space, Past,st′=P[STt+1=st′|STt=st,At=a] indicates the transition probability from state st to st’ after action a. And Rast=E[Rt+1|STt=st,At=a] is the immediate reward brought by taking action a in state st. Target of the model is to find a reward-maximizing policy π(a|st)=P[At=a|STt=st] that instructs the optimal action choosing under different states.

State and Action: To correctly describe the state ST of UEs and edge server, we need to obtain the execution information of their tasks in the queue. Besides, we have to figure out whether the edge serve queue has reached the burden threshold and choosing which server as the second offloading target. So we give the state ST as
ST={st=(Qu,Qs,ρ,DT)},(16)
View Sourcewhere Qu and Qs represents the UE's queue state, edge server's queue state. Parameter ρ is a signal that indicates whether the burden of edge server has reached the threshold and DT denotes the second offloading target edge server. As mentioned in Section 3, the UE's task queue is defined as the set of tasks that will be executed locally. Qu={Qus,1,Qus,2,…,Qus,n} is the combined state of all UEs connecting with edge server s. Qs={Ts1,Ts2,…,TsN|Tsi∈{Ts,n|as,n=1}} denotes the tasks that are currently stored in the queue of edge server s.

We define the action space as
A={As={A1,A2,…,An}},(17)
View Sourcewhere the action space A is the composite offloading decision results of UEs in different regions. As={a=(as,1,as,2,…,as,n)|as,i∈{0,1}} denotes the offloading decision of UEs in the coverage of edge server s. The value 1 means offloading the task to edge server s and 0 means executing locally.

Reward Function: The target of our system model is to minimize the processing delay or energy cost depending on various situations through making optimal offloading decision. Specifically, the time constrain of a tasks has to be assured whether offloading or not. For the UEs only considering energy saving, offloading is the first choice. At the same time, we have to maintain an appropriate device utility since the edge server will be overwhelmed if all UEs offloads their tasks and the idle state of UE is also a waste of computation resource. Based on the consideration above, we devise the reward function Rs,a given an action As and the state edge server s as follows:
Rs,a=Rts,a+Rcs,a,(18)
View SourceRight-click on figure for MathML and additional features.where Rts,a is the time reward function and Rcs,a is the total cost reward function.

The value of Rts,a is calculated as
Rts,a=β∑i=1[τs,i−(1−as,i)tls,i−as,i(1−ρ)tos,i−as,iρto′s,i].(19)
View Source

We define Rts,a as the reward when a task is successfully finished without exceeding the given time limit τs,n. Through such a design, even the task is executed locally this action will be rewarded as long as the time requirement is kept. Common tasks are encouraged to execute locally or the edge server will suffer much heavy burden. The coefficient β takes the responsibility of changing the influential level of time reward. If the system finished the task with time improvement, the reward will be positive. If the task timed out, the action will face the punishment of negative reward.

The total cost reward Rcs,a is calculated as
Rcs,a=∑i=1as,i{Cli,j−(1−ρ)Coffi,j−ρCoff′i,j},(20)
View Sourcewhere if the action has obtained cost saving comparing with the execution locally, the reward will be positive that encourages this action. However, if the cost becomes larger, the punishment will response via negative reward.

4.2 A3C-Based Offloading Algorithm
When facing problems in real scenario, the state space and action space tend to be very large so that it's nearly impossible to obtain the accurate state value or action value via normal reinforcement learning. Combining deep learning and reinforcement learning, DRL algorithms(Deep Reinforcement Leaning) such as Deep Q-Network and DDPG becomes useful for handling the problems of large state space and action space. These methods store mass experience data as a data chunk and choose a part randomly for learning and updating. This mechanism is called experience replay, which is the key point of these algorithms. However, experience replay exists several drawbacks due to its characteristics. Experience replay will occupy more memory and computation resource each time the agent interacts with environment. Besides, experience replay limits the algorithms that they can only utilize the data generated through old policy to update [27]. Normally, edge terminals just have relatively sufficient computational resources which means resource consumption is also an important aspect for implementation. So methods of high computational consumption are not appropriate under edge condition, which motivates us to seek a better solution, Asynchronous Advantage Actor-Critic (A3C).

A3C algorithm is based on the actor-critic network which combines value function V(st,θv) and policy function π(a|st;θ) and θ is the weight. In A3C-based learning scheme, the user acts as an agent to taking the offloading action An in state st according to the policy π(a|st;θ). The Critic network generates the value estimation of these actions and the Actor network utilizes the value to optimize the policy which aims to maximize the future discounted reward over a long time running. Specifically, the Critic network uses one n-step temporal-difference error to estimate the value of the action which is calculated as
Δθv=∂(R−V(sti;θv))2/∂θv,(21)
View SourceRight-click on figure for MathML and additional features.where R is the immediate reward after taking an action at a state. Actually, the value-based Critic network realizes single step updating via bootstrapping which effectively improves the learning efficiency of the Actor network. The Actor network updates the parameters θ to ascend the gradients of policy π(a|st;θ) based on the value obtained from the Critic network. The gradients accumulation of parameters θ can be calculated as
Δθ=(R−V(sti;θ))∇θlogπ(Ai|sti;θ).(22)
View SourceRight-click on figure for MathML and additional features.In this formulation, we use a baseline to reduce variance. The baseline function is only related to the state so adding this function into the policy function will not influence the policy gradients. We choose state value function V(sti;θ) as the baseline function since that the advantage function can be described as: A(sti;θ)=R−V(sti;θ).

Addition to the Actor-Critic network, the asynchronous interaction between parallel agents and environments improves the performance of A3C. Without using experience replay, A3C executes several agents synchronously to enable them to obtain different states in parallel. The parallelism mechanism decorrelates agents’ data into a more stationary process so the samples are eligible for training. After interacting with the environments, each agent updates the global network with their own new parameters. In the next learning process, each agent can utilize the global improvements for its individual interaction. Furthermore, there exists another advantage of using this algorithm. The training process of traditional deep reinforcement learning algorithms are of high dependence on powerful GPU. Normal edge devices are often unable to provide powerful graph processing capacity. But A3C algorithm can be realized on a single machine with a standard multi-core CPU. Each thread can control an agent to update parameters in parallel. The implementation of this algorithm is relatively friendly to edge devices.

The specific description of A3C-based offloading decision making algorithm is presented in Algorithm 1.

SECTION 5Performance Evaluation
In this section, we utilize Tensorflow to evaluate the performance of our algorithm. The performance comparison with several existed algorithms is shown considering total reward, energy cost, service delay and total cost.

SECTION Algorithm 1.A3C-Based Offloading Decision Making Algorithm
Assume global shared parameter vectors θ and θv and global shared counter T=0

Assume thread-specific parameter vector θ′ and θ′v

Initialize thread step counter t←1

repeat

Reset gradients: dθ←0 and dθv←0.

Synchronize thread-specific parameters θ′=θ and θ′v=θv

tstart=t

Get state stt

repeat

Perform at according to policy π(At|stt;θ′)

Receive reward rt and new state stt+1

t←t+1

T←T+1

until terminal st or t−tstart==tmax

if stt is terminal state then

R=0

else

R=V(stt,θ′v)

end if

for i∈t−1,…,tstart do

R←ri+γR

Accumulate gradients wrt θ′:

dθ←dθ+∇θ′logπ(Ai|sti;θ′)(R−V(sti;θ′v))

Accumulate gradients wrt θ′v:

dθv←dθv+∂(R−V(sti;θ′v))2/∂θ′v

end for

Perform asynchronous update of θ using dθ and of θv using dθv

until T>Tmax

5.1 Experiment Settings and Metrics
First, We consider a edge offloading system consists of Ns=3 edge servers and Nu=12 UEs. Each pico base-station has a coverage range of 50 m and provides communication service for one ES and 4 UEs. We determine the number settings based on two considerations. First, under the computational-limited edge scenario, the density of users won't be large. So the number settings for both edge server and UE are reasonable under the service of pico base station. Second, the number of edge server and UE is alterable. But the experimental workload will increase exponentially without obtaining extra research value. The system consisting of three edge server and their controlled UE is functionally capable to represent the interaction of smart devices under edge computing scenario. For experimental convenience, we locate the base-station in the center of the region and the ES is close to the base-station so that we can ignore the transmission delay between base-station and ES. For transmission model, we refer to the parameters in [18]. We set the channel bandwidth W=20 MHz, the transmission power Ps,n=1W and the background noise w0¯=50 dBm. We set the channel gain Hs,n=dμs,n where ds,n is the distance between UE n and the base-station, and μ=4 is the path loss factor.

For the computation task, object detection, such as [28], is a typical application which relies on edge devices to collect images and process locally or offload to edge servers. These tasks are computation intensive and have tolerance requirements. So we set a high computing load-input data ratio Ds,n/Bs,n=10 Mcycles/KB to describe the complexity of these tasks. The computing cycles per task are randomly assigned from the set [50,100] GHz to simulate task variety. The computing capacity of local UE fls,n is assigned from the set {5,6,8,9} Gcycles/s for the consideration of equipment difference. The computing capacity fs assigned for edge server is selected from the set {50,70,90} Gcycles/s. For the weight coefficients of energy cost and time cost, λes,n+λts,n=1. We set λes,n=λts,n=0.5 since energy cost and time cost have the same influence in our model. The simulation parameters are summarized in Table 2.

TABLE 2 Simulation Parameters
Table 2- 
Simulation Parameters
In order to accurately analyzing the effectiveness and efficiency of our algorithm in the complex edge scenario, we compare our method with DQN-base method and non-offloading method considering total reward, energy cost and time cost. The improvement on convergence speed is obvious which is crucial for those delay-sensitive applications. In addition, we also study the influence of task density on time consumption, energy consumption and total cost which shows the necessity of double offloading mechanism between ESs.

5.2 Comparison of Total Reward on Strategies
In this subsection, we evaluate the comprehensive performance of four different strategies including Local Computing (LC), DQN with Double Offloading (DQN-DO), A3C with Double Offloading (A3C-DO) and A3C with Single Offloading (A3C-SO). We chose total reward as the metrics of strategies performance since the the progress on energy consumption and time cost will directly lead to the increase of reward. Larger reward means better performance.

The task density we set for each UE is average 10 tasks/100s. We collect the whole tasks created in 1,000 seconds and train them using Tensorflow for total 5,000 episodes. The results of local computing will not change with episodes for it's linear. The total reward for local computing is −11,367 which is much worse than other results. The convergence curve of the other three strategies has been plotted in Fig. 2 and we can obtain some useful information through the comparison. At first, the result of local computing is kind of disappointing because according to the task size and local computing capacity, most tasks can be finished within the tolerant threshold. Through analyzing the task sequence we find the reason. The appearance of task is random so that some tasks are created in a short time range. Once a task isn't finished in time, subsequent tasks will be influenced which result in more time violations. Then the reward becomes really bad. This has proved that local computing with resource-limited UE can't provide satisfying quality of service in complex situation. Then, it's obvious that our two A3C-based strategies have much better reward. The result means that our strategies can make better decisions for the same task sequences. Besides, they show faster convergence speed compared with DQN. Specifically, A3C-SO's reward can get close to its top value with 1,000 episodes but DQN needs about 2,000 episodes. The observation proves our strategies are able to make nearly optimal decisions in a shorter time which is crucial for delay-sensitive applications. As for the reward difference of A3C-SO and A3C-DO, we will leave the analysis of this problem in the later subsection for analyzing the influence of task density .

Fig. 2. - 
Comparison of total reward for different strategies.
Fig. 2.
Comparison of total reward for different strategies.

Show All

5.3 Influence of Task Density on Total Cost
In this subsection, we analyze the influence of different task densities on the strategy's total cost. We run our experiment on seven task densities that are respectively 10, 12, 14, 16, 18, 20 and 22 tasks each 100 seconds. The density variation from 10 to 22 has involved both low-workload condition and high-workload condition. The performance comparison between different task densities is helpful to reveal the stability of experimental frameworks, which is the most important part of our design. In Fig. 3, we plot the convergence curves of total cost on task density 10. As expected, A3C-based strategies still reveal brilliant convergence speed compared with DQN and the total cost of two A3Cs are nearly half of the DQN's. The small total cost differences between A3C-SO and A3C-DO denote that complex hierarchy will increase the processing burden at a low task density. However, the advantage of a complex hierarchy begins to take effect as the task density becomes larger. In Fig. 4, we plot the average total cost value of the last 500 episodes in seven task densities considering DQN-DO, A3C-DO, and A3C-SO. The average value is used to cover the random disturbance caused by the algorithm itself. At density 10, 12 and 14, the total cost of A3C-DO and A3C-SO are very close. They are also lower than the total cost of DQN. Obvious difference begins to emerge at density 16 and becomes larger at 18, 20 and 22. In the latter three situations, the total cost of A3C-SO even exceeds DQN's. But the performance of A3C-DO is very stable, only augmenting in a small scale due to the increase of total tasks. The observation demonstrates that edge offloading system with only one single powerful edge server just provide stable services in a narrow task density range. When tasks become large, single offloading structure is inefficient. Though A3C-DO performs a little worse than A3C-SO in low density situations, which can be actually ignored due to the task randomness, it keeps the stability when task density increases. Even DQN-DO performs better than A3C-SO in total cost. The results prove that our double offloading structure is capable to effectively do the offloading with low cost under diverse situations. The analysis on time cost and energy cost is present in the next subsection.


Fig. 3.
Total cost comparison on density 10.

Show All

Fig. 4. - 
Total cost comparison on different task densities.
Fig. 4.
Total cost comparison on different task densities.

Show All

5.4 Influence of Task Density on Energy and Time Cost
In this subsection, we further analyze the influence of task density on total cost which consists of energy cost and time cost. In Figs. 5 and 6, we plot the comparison of energy cost and time cost on seven task densities. At density 10, 12 and 14, the energy cost of A3C-SO and A3C-DO are close and they nearly half of the DQN's energy cost. The results denote that in two A3C strategies, more tasks are offloaded to the ES than DQN because computing locally will consume more energy than uploading. Higher time cost of DQN and its lower reward also prove this reason. At density 16 and 18, DQN offload more tasks so its growth of energy cost and time cost is not obvious. Correspondingly, the energy cost of two A3Cs grow a lot. At density 18, the energy cost of two A3Cs are close but A3C-SO's time cost rises sharply. Double offloading structure is obviously responsible for this result since similar energy cost means they have similar local burden and similar local time cost. Without the help of double offloading, tasks in ES are blocked which results in the time cost explosion. At density 20 and 22, the energy cost and time cost of A3C-SO keep rising but DQN and A3C-DO perform much better. With the same processing capabilities, the cost growth must come from task waiting time and local computing energy cost. In a edge computing system consists of several ESs and UEs, it's common that random tasks distribution leads to different load on different devices. Double offloading structure provides a bridge to transform computational resources from idle part to busy part which effectively improve the resource utility of the whole system and reduce total cost.


Fig. 5.
Energy cost comparison on different task densities.

Show All


Fig. 6.
Time cost comparison on different task densities.

Show All

5.5 Self-Adjusting Ability and Random Exploration
In this subsection, we analyze the self-adjusting ability and random exploration scheme of our framework. As shown in Fig. 7, a severe fluctuation occurred after episode 4,000. The reward went through a sharp decline but recovered in a few episodes and reached a higher reward. To further explore what happened, we plot the time cost curve and energy cost curve in Figs. 8 and 9 respectively. It's clear that the time cost rose suddenly and declined subsequently. The energy cost keeps going down which means tasks are offloaded to the ES. Combining these two curves and the random exploration scheme of our framework, we can figure out the process. At the step of action choosing which is the offloading decision making process, prior decision action under current policy has higher choosing probability. However, it doesn't mean other actions are useless. Random Exploration enables the algorithm to find an action that may be better at subsequent states. But the other side of the coin is that this action may lead to a worse decision for current states. The spine of these curves has proved it. The exciting thing is our framework finds back the right decision direction in a short episodes. Moreover, the performance of A3C-DO is getting closer to A3C-SO's considering reward and energy cost. A better result is triggered by a bad action choice. The experiment is interesting but it truly demonstrates the self-adjusting ability of our framework and the meaning of random exploration.


Fig. 7.
Total reward with flaw in A3C-DO.

Show All


Fig. 8.
Time cost with flaw in A3C-DO.

Show All

Fig. 9. - 
Energy cost with flaw in A3C-DO.
Fig. 9.
Energy cost with flaw in A3C-DO.

Show All

SECTION 6Conclusion and Future Works
In this paper, we mainly concentrate on the scheduling of limited regional resource in edge scenarios. For effectively utilizing idle resources in a region, we propose a double offloading framework that links different edge servers to alleviate the partial load aggravation. Then, we design an algorithm based on deep reinforcement learning to efficiently find an appropriate solution to offload the tasks of user equipment. The simulation results demonstrate that our method is able to take good advantage of system resources to make better offloading decisions with lower energy cost and time cost under the same conditions. The reward curves at tasks density 10 shows that our A3C-based algorithms have better decision results and brilliant convergence speed compared with the wide-used DQN. The cost comparison between A3C-DO and DQN-DO has highlighted the improvements of our method in energy and delay. Besides, by comparing A3C-DO and A3C-SO in several task densities, we prove that the double offloading framework can effectively schedule the tasks distribution among the whole system to reduce the partial overhead in terms of energy and delay. Moreover, we have also shown the stability and self-adjusting ability when task density increases or bad decision is made.

However, there still exists a challenge that need to be overcome in the future work. We explain it here to shed a little light on the research on this direction. In this framework based on DRL, the decision action of each UE is discrete. When the number of UE increases, their action space will grow exponentially. Each neuron network node of the output layer represents one action so that it's impossible to build such a neuron network when the action space is discretely large. The next research direction we focus on is to approximate the discrete action space using continuous action space. To overcome this challenge, a proper relation mapping between discrete action space and continuous action space is the most important.