Edge computing is a new paradigm to provide strong computing capability at the edge of pervasive radio access networks close to users. A critical research challenge of edge computing is to design an efficient offloading strategy to decide which tasks can be offloaded to edge servers with limited resources. Although many research efforts attempt to address this challenge, they need centralized control, which is not practical because users are rational individuals with interests to maximize their benefits. In this article, we study to design a decentralized algorithm for computation offloading, so that users can independently choose their offloading decisions. Game theory has been applied in the algorithm design. Different from existing work, we address the challenge that users may refuse to expose their information about network bandwidth and preference. Therefore, it requires that our solution should make the offloading decision without such knowledge. We formulate the problem as a partially observable Markov decision process (POMDP), which is solved by a policy gradient deep reinforcement learning (DRL) based approach. Extensive simulation results show that our proposal significantly outperforms existing solutions.
SECTION 1Introduction
As MOBILE phones are gaining enormous popularity, more and more mobile applications, such as face recognition, natural language processing and augmented reality, are emerging and attracting great attention [1], [2], [3]. These mobile applications are typically resource-hungry, demanding intensive computation and high energy consumption, which can hardly be supported by mobile phones with limited computation resources and battery life. To overcome this limitation, a novel computing paradigm, called edge computing, has been proposed as a promising solution [4]. A number of modest-size computing servers have been deployed at the edge of pervasive radio access networks close to users, so that users can offload their computing tasks to these servers with low latency.

Although the edge-based computation offloading approach can significantly augment computation capability of users, developing a comprehensive and reliable edge computing system remains challenging. Edge servers have limited hardware resources. If too many users choose to offload their tasks simultaneously, it would exceed the capacity of edge servers, leading to long task response time. Therefore, it is critical to design an efficient offloading strategy to decide which tasks should be offloaded to edge servers. This problem has been recognized as one of the most critical challenges for edge computing, but most existing work needs centralized control to achieve global optimal performance [5], [6]. Unfortunately, it is not practical to force all users to act according to centralized control because they are individuals with rational choices in computation offloading.

Game theory is a powerful framework to analyze the interactions among multiple players who act in their own interests. It can be used to design decentralized mechanisms, such that no player has the incentive to deviate unilaterally. Thanks to its great promises, game theory has been applied for designing offloading algorithm for edge computing by recent research efforts. For example, Chen et al. [7], [8] have designed a decentralized computation offloading game for mobile cloud computing. Jošilo et al. [9] have proposed selfish decentralized computation offloading in dense wireless networks where each user can offload its computation to multiple wireless base stations. However, existing work can hardly be applied in practice because of two weaknesses. First, they consider a discrete action model that allows users to choose a limited number of actions. Although this model works well in scenarios with a few users, it cannot handle large-scale problems. A straightforward approach is to add more actions in the problem formulation, but it leads to higher algorithm complexity. Second, existing work has a strong assumption that all users should share their information, e.g., quality of network connection and preference on energy efficiency, so that they can make the best offloading decisions. However, users may be unwilling to expose such personal information due to privacy and security concerns.

In this paper, we study to conquer the above weaknesses by designing an algorithm based on game theory enhanced by deep reinforcement learning (DRL). Specifically, we consider a number of users who can connect an edge server via multiple access points (e.g., base stations or WiFi routers). Each user can arbitrarily divide its task into smaller subtasks and choose to offload a portion of them to the edge server. A challenge arises because of partial offloading. It makes the model more flexible, but users should choose their actions from a continuous space, which is different from discrete models used by existing work that considers simple offloading decisions, e.g., offloading the whole task or not [10].

We first study a simple scenario that users share their information, e.g., network bandwidth and preference, and design an algorithm that can achieve Nash equilibrium. Based on the insight provided by this algorithm, we then extend our work for scenarios without information sharing. The problem is formulated as a multi-agent partially observable Markov decision process (POMDP). To address the challenges of network dynamics and continuous decision space, we propose a decentralized approach based on deep reinforcement learning (D-DRL) with policy gradient and differential neural computer (DNC). Our approach can effectively learn the optimal offloading policy under high network dynamics in a continuous decision space directly from computation offloading game history without any prior knowledge about system models. It has merits over model-based computation offloading game strategies in that it is model-free and provides a general solution to computation offloading problems. Thus, it can be applied to complex and unpredictable situations where it is difficult to obtain precise system models. Moreover, DNC, which is first used in policy gradient DRL, is capable of remembering past information and inferring the hidden states of observations automatically. By incorporating the DNC into our framework, not only the policy optimization process will be accelerated significantly, but also the users can learn policy when the network is time-varying and uncertain.

The main contributions of this paper are summarized as follows:

We study the task offloading problem in edge computing and formulate it as a decentralized computation offloading game in each time slot by taking into account both communication and computation cost. We solve this problem by proposing an algorithm that can achieve the Nash equilibrium.

We study the offloading problem without information sharing and formulate it as a multi-agent POMDP. An algorithm based on DRL and DNC has been proposed to solve this challenging problem.

Simulation results demonstrate the effectiveness of the proposed scheme by comparing it with state-of-the-art.

The remainder of this paper is organized as follows. In Section 2, we discuss the related works. Section 3 presents the problem description. Section 4 gives the algorithm design for scenarios with information sharing. Section 5 provides the detailed multi-agent reinforcement learning approach for scenarios without information sharing. Finally, Section 6 evaluates the system performance by simulation and Section 8 concludes the paper.

SECTION 2Related Work
The edge computing paradigm has attracted considerable attention in both academia and industry over the past several years. Nokia introduced the very first real-world edge computing platform in 2013 [11], in which the computing platform called radio application cloud servers is fully integrated with the Flexi Multiradio base stations. Saguna also introduced their fully virtualized edge computing platform Open-RAN, which can provide an open environment for running third-party edge computing applications [12]. Edge computing has been applied into various scenarios [13], [14].

Many existing work has studied the computation offloading problem from the perspective of a single user. Redenko et al. [15] have shown that computation offloading can save energy according to their experimental results. In [16], an optimization scheme for energy-efficient application execution has been proposed on the cloud-assisted mobile application platform. Xian et al. [17] have proposed an adaptive timeout scheme for computation offloading to improve the energy saving. There are some works that have investigated the computation offloading problem in the multi-user case. Rodrigues et al. [6] have proposed a hybrid method for minimizing service delay in edge computing through virtual machine migration and transmission power control. In [20], an iterative algorithm has been proposed to perform the joint optimization of radio and computational resources for multi-cell edge computing under the budget constraints of latency and power. You et al. [21] have studied a centralized offloading framework for a multi-user edge computing system based on TDMA and OFDMA aiming to minimize the user's energy consumption. Lin et al. [4] have provided a comprehensive survey on computation offloading toward edge computing.

However, all above works need centralized control, ignoring the interactions among multiple users when they independently determine their computation offloading strategies. Some recent works [7], [8], [22], [23], [24], [25] have modeled users as self-interested game players and proposed decentralized schemes to solve the multi-user computation offloading problems. However, they mainly focus on the computation offloading problems under relatively static environment. In real network environment, due to the time-varying wireless networks, the utility of each user is dynamically changing, and thus the solution of the Nash equilibrium in the static game model may not be reached.

Xiao et al. [26] have proposed the multi-user computation offloading problem in time-variant wireless networks, and each user needs to compete the computational resource. A Q-learning based approach has been proposed to achieve the Nash equilibrium of the dynamic computation offloading game. However, the users’ decision space is discrete in their model and the proposed approach has high complexity in solving large-scale problems.

It is challenging to achieve Nash equilibrium in the stochastic games in the decentralized and dynamic environment. Multi-agent Nash Q-Learning [27] has been proposed for discrete stochastic game. Lillicrap et al. [28] have proposed the DDPG approach for the multi-agent Markov decision process, where the environment is fully observable. Zhan et al. [29] has proposed a learning-based incentive mechanism for federated learning, which achieves the Nash equilibrium based on deep reinforcement learning. In contrast to the previous research, our work in this paper formally addresses the problem of partial computation offloading, dynamic environment and incomplete information sharing in edge computing. This is a non-trivial problem due to that each user could only obtain partial observation and thus could not derive the optimal decision.

SECTION 3Problem Description
As shown in Fig. 1, we consider a set of N={1,2,…,N} users, each of which has a computation-intensive and delay-sensitive task to be executed. There are multiple wireless base stations, through which users can offload their computation tasks to nearby edge servers deployed by the network operator. Following the model in [21], we let Cn denote the size of input data of user n. The user n can offload a part of its computation task to the edge server at time k, which is denoted by xkn, and 0≤xkn≤Cn. For example, if user n processes all the computation task locally, then xkn=0. On the other hand, we have xkn=Cn if the user offloads all its input to the edge server. The network bandwidth assigned to the user n at time k is denoted by bkn∈[B––n,B¯¯¯¯n], where B––n and B¯¯¯¯n are the minimum and maximum network bandwidth, respectively. This offloading model can be applied in many applications scenarios, such as malware detection [30]. Malware detection systems explore the feature of the runtime behavior of thousands of applications on mobile devices and involve logging data at each application execution. The application traces have to be scanned in real-time based on the latest malware signature files downloaded from the security database. By offloading the detection tasks to secure servers, edge-based malware detection can reduce the computation of mobile devices. In this scenario, the users can offload any part of the traces to the edge server [21].


Fig. 1.
Multi-user computation offloading with N users in an edge computing environment.

Show All

The edge server applies algorithms such as deep learning to process the input data. The total amount of input data offloaded from all users to the edge server is ∑Nn=1xkn. Let R denote the total amount of computational resources available at the edge server, which has been normalized to the processing capability of users. The edge server allocates its computation resources to users according to their uploaded data amount, so that user n obtains a portion of xkn∑Nm=1xkmR [26], [31]. We use Fkn to denote the computational capacity of user n in the time slot k. Based on the transmission delay and the processing delay of edge server, the task execution delay of user n in time slot k denoted by tkn, is given by
tkn=max{xknbkn+∑m=1NxknR,Cn−xknFkn}.
View Source

The transmission cost of user n depends on its transmission bandwidth bkn. We let pn be the transmission cost of unit data for user n. Based on the processing speed of the edge server and the energy cost of the users, the instant utility of user n at time k is defined as
un(xkn,xk−n)=αnxknR∑Nm=1xkm−(pnxknbkn+νn(Cn−xkn)),(1)
View Sourcewhere xk−n=(xk1,…,xkn−1,xkn+1,…,xkN) are the decisions of all users except user n. The coefficient αn is the weight of obtained computational resources at the edge server. To provide rich modeling flexibility and meet user-specific demands, different users could choose different weight parameters in the decision making. For example, when the battery of a user is at a full state, the user would like to put larger weight on the decision making. The νn is the energy consumption of processing unit input data by user n. Then, the computational cost for user n to process the local data is νn(Cn−xkn) [21].

Given other users’ decisions xk−n, the user n would like to make a decision xkn∈[0,Cn] to maximize its utility in terms of energy consumption and processing time, i.e.,
OPUser:maxs.t.un(xkn,xk−n),xkn∈[0,Cn].(2)
View Source

Game theory is a powerful framework to analyze the interactions among multiple users who act in their own interests and design optimal computation offloading scheme, such that no user has the incentive to deviate unilaterally. Our objective is to design an algorithm to achieve the Nash equilibrium that is defined as follows.

3Definition 1 (Nash equilibrium).
The strategy set xk,∗=[xk,∗n]n∈N constitutes a Nash equilibrium of the decentralized computation offloading game in time slot k if the following condition is satisfied:
un(xk,∗n,xk,∗−n)≥un(xkn,xk,∗−n).(3)
View Source

The Nash equilibrium has a nice self-stability property such that the user at the equilibrium can achieve a mutually satisfactory solution and no one has the incentive to deviate. This property is very important to the decentralized computation offloading problem, since the users may act in their own interests.

SECTION 4Algorithm With Information Sharing
In this section, we study to design an algorithm for the cases that all mobile users share their complete information including weight αn, network bandwidth bkn and so on. To proceed, we first introduce an important concept called best response [32].

4Definition 2.
Given the strategies xk−n of the other users at time k, user n's strategy xk,∗n is the best response if
un(xk,∗n,xk−n)≥un(xkn,xk−n).(4)
View Source

According to (3) and (4), all users play the best offloading strategies towards each other at the Nash equilibrium. Based on the concept of best response, we have the following observation.

4Lemma 1.
Given xk−n and the fact that the unit communication cost pnbn is larger than the unit computational cost νn, the optimal computation offloading strategy of the nth user is
xk,∗n=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪0,Cn,αn∑m≠nNxkmpnbkn−νnR−−−−−−−−⎷−∑m≠nNxkm,ifΔ1,ifΔ2,otherwise,(5)
View SourceRight-click on figure for MathML and additional features.where
Δ2:Cn2Δ1:αnRpnbkn−νn<∑m≠nNxkm,and+(∑m≠nNxkm)2+(2Cn−αnRpnbkn−νn)∑m≠nNxkm<0,
View SourceRight-click on figure for MathML and additional features.

4Proof.
According to Eqn. (1), the first-order derivative of un with respect to xkn is
∂un∂xkn=αnR∑Nm=1xkm−αnRxkn(∑Nm=1xkm)2−pnbkn+νn,(6)
View SourceRight-click on figure for MathML and additional features.and the second-order derivative of un with respect to xkn is
∂2un∂xkn2=αn−2R∑Nm≠nxkm(∑Nm=1xkm)3<0.(7)
View SourceRight-click on figure for MathML and additional features.Then the utility function un is a strictly concave function in xkn. Therefore, given any R>0 and any feasible strategy profile of xk−n of the other users, the optimal computation offloading strategy of user n is unique, if it exits. Setting the first-order derivative of un with respect to xkn to 0, we have
∂un∂xkn=αnR∑Nm=1xkm−αnRxkn(∑Nm=1xkm)2−pnbkn+νn=0.(8)
View SourceBy solving (8), we obtain
x~kn=αn∑m≠nNxkmpnbkn−νnR−−−−−−−−−−⎷−∑m≠nNxkm.(9)
View SourceRight-click on figure for MathML and additional features.If 0<x~kn<Cn, we have xk,∗n=x~kn. If αn∑m≠nNxkmpnbkn−νnR−−−−−−−−⎷−∑m≠nNxkm<0, i.e., Δ1 holds, we have xk,∗n=0. Otherwise, i.e., Δ2 holds, we have xk,∗n=Cn.

Based on above Lemma 1, we have the following theorem for users to determine their decisions at Nash equilibrium.

4Theorem 1.
With information sharing, the computation offloading strategy of user n at the unique Nash equilibrium satisfies
xk,∗n=(N−1)R∑m=1Npmbkm−νmαm(1−(N−1)pnbkn−νnαn∑m=1Npmbkm−νmαm),(10)
View Sourceif
((N−1)R−Cn)∑m≠nNpmbkm−νmαm(N−1)(N−2)R+Cn<pnbkn−νnαn<∑m≠nNpmbkm−νmαmN−2.(11)
View SourceThe utility of user n is
un(xk,∗n,xk,∗−n)=αnxk,∗nR∑m=1Nxk,∗m−(pnxk,∗nbkn+νn(Cn−xk,∗n)),
View Sourceand the computation delay is
tkn=max⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪xk,∗nbkn+∑m=1Nxk,∗mR,Cn−xk,∗nFkn⎫⎭⎬⎪⎪⎪⎪⎪⎪⎪⎪.
View Source

4Proof.
According Eqn. (5), for 0<xk,∗n<Cn, we have
∑m=1Nxk,∗m=αn∑m≠nNxk,∗mpnbkn−νnR−−−−−−−−−−⎷.(12)
View SourceSet Ξ=∑m=1Nxk,∗m, by Eqn. (12), we have
xk,∗n=Ξ−Ξ2(pnbkn−νn)Rαn,(13)
View Sourceand therefore
Ξ=NΞ−Ξ2∑m=1Npmbkm−νmRαm.(14)
View SourceSince xk,∗n∈(0,Cn), hence Ξ>0 has a unique solution for Eqn. (14). Based on Eqn. (14), we obtain the unique solution for Ξ as
Ξ=(N−1)R∑m=1Npmbkm−νmαm.(15)
View SourcePlugging the unique Ξ into Eqn. (13), we have the unique solution for user n as
xk,∗n=(N−1)R∑m=1Npmbkm−νmαm(1−(N−1)pnbkn−νnαn∑m=1Npmbkm−νmαm).(16)
View SourceTherefore, the computation offloading game has a unique Nash equilibrium. Since xk,∗n∈(0,Cn), based on Eqn. (16), we can obtain
((N−1)R−Cn)∑m≠nNpmbkm−νmαm(N−1)(N−2)R+Cn<pnbkn−νnαn<∑m≠nNpmbkm−νmαm(N−2).
View SourceRight-click on figure for MathML and additional features.This completes the proof. If (11) holds, we have xk,∗n∈(0,Cn), which satisfies (10). By plugging Eqn. (10), we can obtain the utility un(xk,∗n,xk,∗−n) and computation delay tkn of user n under Nash equilibrium.

4Remark 1.
According to Theorem 1, the offloading strategy of each user at Nash equilibrium is determined by the computation resource of the edge server, the transmission cost of each user, the radio bandwidth of each user, the local computational cost of each user, and user's preference for delay and energy consumption.

4Corollary 1.
With information sharing, if battery power of the nth user is not enough, it will offload less computation to the edge server.

4Proof.
As mentioned in Section 3, if user n's battery is not enough, in order to save energy, user n would like to put more weight on energy consumption in the computation offloading decision making (i.e., lower αn). According to Eqn. (10), we can derive that the first-order derivative of xk,∗n with respect to αn is
∂xk,∗n∂αn=(N−2)pnbkn−νnαn∑m=1Npmbkm−νmαmαn2(∑m=1Npmbkm−νmαm)3+∑m≠nNpmbkm−νmαm−(N−2)pnbkn−νnαnαn2(∑m=1Npmbkm−νmαm)3.
View SourceSince xk,∗n>0, we can derive 1−(N−1)pnbkn−νnαn∑m=1Npmbkm−νmαm>0, i.e., ∑m≠nNpmbkm−νmαm−(N−2)pnbkn−νnαn>0. Then, we can obtain ∂xk,∗n∂αn>0. Therefore, when user's battery is at a low level, it will reduce the weight αn and offload less computation to the edge server.

According to the above analysis, we propose Algorithm 1 that can achieve the Nash equilibrium with complete information sharing. As shown in Algorithm 1, in each time slot, each user first shares its information with other users. After receiving this private information from others, each user decides its offloading strategy according to the optimal computation offloading strategy that can be obtained through Theorem 1.

SECTION Algorithm 1.Algorithm With Complete Information Sharing
for each time slot k do

   for each user n∈N do

      Publish its private information, i.e., un(xkn,xk−n), pn, bkn, νn, \alpha _n.

     Collect information from other users.

 Calculate the optimal computation offloading strategy according to Theorem 1.

   end for

end for

SECTION 5Algorithm Without Information Sharing
In scenarios with complete information sharing, each user requires the knowledge of other users’ information such as the radio bandwidth b_n^k, the decision making weights \alpha _n and so on. However, it is unrealistic to obtain these information in practice because users may refuse to expose these parameters due to the consideration of privacy protection. Furthermore, the physical parameters of users are time-variant, making it challenging for one user to estimate other users’ properties accurately.

In this section, we study the offloading algorithm when the properties of other users are unobservable. Specifically, we formulate the dynamic decentralized computation offloading game as a multi-agent partially observable Markov decision process and design a novel dynamic computation offloading algorithm D-DRL for users based on multi-agent DRL approach. With this algorithm, each user can determine the approximately optimal computation offloading strategy directly from game history without any prior information about other users.

5.1 Partially Observable Markov Decision Process
As demonstrated in Fig. 2, a multi-agent POMDP can be represented by \mathcal {M}_p=\langle \mathcal {S}, \mathcal {A}, \mathcal {P}, \mathcal {R}, \mathcal {O}, \mathcal {T}\rangle, which consists of a state space \mathcal {S}=\lbrace \mathcal {S}_n=\lbrace s_n^k|\forall k \in \mathbb {N}\rbrace \rbrace _{n\in \mathcal {N}}, an action space \mathcal {A}=\lbrace \mathcal {A}_n\rbrace _{n\in \mathcal {N}}, a state transition probability function set \mathcal {P}=\lbrace P_n:\mathcal {S}_n\times \mathcal {A}\times \mathcal {S}_n\rightarrow [0,1]\rbrace _{n\in \mathcal {N}}, a reward space \mathcal {R}=\lbrace \mathcal {R}_n\rbrace _{n\in \mathcal {N}}, an observation space \mathcal {O}, and an observation function set \mathcal {T}=\lbrace T_n:\mathcal {S}_n\times \mathcal {O}_n\rightarrow [0,1]\rbrace _{n\in \mathcal {N}}. In the POMDP, the state space \mathcal {S} is partially known and the decisions are made from observation space \mathcal {O}. Users can only observe the past strategy set \lbrace \boldsymbol {x}^{k-L},\boldsymbol {x}^{k-L+1},\ldots, \boldsymbol {x}^{k-1}\rbrace and their own properties. We establish the POMDP for this game as follows.


Fig. 2.
Partially observable Markov decision process.

Show All

Observation Space: There is an observation space \mathcal {O}=\lbrace \mathcal {O}_n\rbrace _{n\in \mathcal {N}}, where \mathcal {O}_n=\lbrace \boldsymbol {o}_n^k|\forall k \in \mathbb {N}\rbrace and \boldsymbol {o}_n^k = [\boldsymbol {x}_{-n}^{k-L},b_n^{k-L+1},\ldots, \boldsymbol {x}_{-n}^{k-1},b_n^{k}]^T. Note that \boldsymbol {x}_{-n}^{k-L} is randomly generated when k\leq L. At the beginning of time slot k, the observation of user n consists of its radio bandwidth at previous L-1 time slots and current time, and the size of input data uploaded by other users at previous L time slots.

Action Space: We have \mathcal {A}=\lbrace \mathcal {A}_n\rbrace _{n\in \mathcal {N}}, where \mathcal {A}_n=\lbrace x_n^k|\forall k \in \mathbb {N}\rbrace. At the beginning of time slot k, the action of user n is the size of input data x_n^k uploaded to the edge server.

Observation Transition: After all users take actions at time k, the observation of user n will transit into \boldsymbol {o}_n^{k+1} satisfying \boldsymbol {o}_n^{k+1}\sim \int _{\mathcal {S}_n}T_n(\cdot |s_n)P_n(s_n|s_n^k,\lbrace x_m^k\rbrace _{m\in \mathcal {N}})\mathrm {d}s_n. It is noteworthy that transition from b_n^k into b_n^{k+1} is a stochastic process. Hence, the whole observation transition process is stochastic.

Reward Space: There is a reward space \mathcal {R}=\lbrace \mathcal {R}_n\rbrace _{n\in \mathcal {N}}, where \mathcal {R}_n=\lbrace r_n^k|\forall k \in \mathbb {N}\rbrace and r_n^k=u_n(x_n^k,\boldsymbol {x}_{-n}^k). After all users take actions at time k, each user will calculate its reward according to its utility function and then start next game.

Multi-Agent Learning Objective: We represent the computation offloading policy of user n parameterized by \boldsymbol {\theta }_n as \pi _{\boldsymbol {\theta }_n}, which is defined as \pi _{\boldsymbol {\theta }_n}: \mathcal {O}_n\times \mathcal {A}_n\rightarrow [0,1]. Then, the policy optimization problem for user n is derived as follows \begin{align*} \boldsymbol {\theta }_n^{*} &= \arg \max _{\boldsymbol {\theta }_n}~L_n(\pi _{\boldsymbol {\theta }_n})\\ &= \arg \max _{\boldsymbol {\theta }_n}~\mathbb {E}\Big [V^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n^0\big)|\rho _n^0\Big ]\\ &= \arg \max _{\boldsymbol {\theta }_n}~\mathbb {E}\Big [Q^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n^0,x_n^0\big)|\rho _n^0,\pi _{\boldsymbol {\theta }_n}\Big ], \tag{17}\end{align*}
View Sourcewhere \begin{equation*} V^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n\big)=\mathbb {E}\big [R_n^k|\boldsymbol {o}_n^k=\boldsymbol {o}_n, \Pi, \mathcal {P},\mathcal {T}\big ], \qquad\;\; \tag{18}\end{equation*}
View Source\begin{align*} Q^{\pi _{\boldsymbol {\theta }_n}}(\boldsymbol {o}_n,x_n)=\mathbb {E}\big [&R_n^k|\boldsymbol {o}_n^k=\boldsymbol {o}_n,\\ &x_n^k=x_n,\Pi, \mathcal {P},\mathcal {T}\big ], \tag{19}\end{align*}
View Source\begin{equation*} R_n^k=\sum _{l=k}^{K}\gamma ^{l-k}r_n^k. \qquad\qquad\qquad\;\; \tag{20}\end{equation*}
View SourceIn above equations, \Pi =\lbrace \pi _{\boldsymbol {\theta }_n}\rbrace _{n\in \mathcal {N}} represents the set of all users’ policies, V^{\pi _{\boldsymbol {\theta }_n}} is the value function for observation, Q^{\pi _{\boldsymbol {\theta }_n}}(\boldsymbol {o}_n,x_n) is the value function for observation and action, \rho _n^0 is the initial observation probability distribution of user n, R_n^k is the discounted expected future reward for user n at time slot k, and \gamma \in [0,1] is the discount factor.

Therefore, after formulating the dynamic decentralized computation offloading game as a multi-agent POMDP, the computation offloading strategies of users can be optimized through a multi-agent policy gradient DRL approach under non-cooperative scenarios.

5.2 Algorithm Design
5.2.1 Overview
As shown in Fig. 3, there are N users, each of which has a module called DRL controller that determines the size of input data uploaded to the edge sever. The design of the DRL controller is illustrated in the bottom part of Fig. 3, where there are an actor network, a critic network, a replay buffer, a utility calculation module, a memory module, a value function optimizer, and a policy optimizer. Specifically, the actor network outputs actions according to observation directly through a multiple fully-connected neural network. The critic network first maps the observation into a feature vector, and then reads features from its memory. By combining these features, the final estimated value of observation can be derived and its memory will be renewed through write operations. The replay buffer will store the past D time slots game records for actor and critic update, after which the buffer will be cleared up.


Fig. 3.
Dynamic decentralized computation offloading with D-DRL.

Show All

5.2.2 Actor and Critic Networks Design
In [33], MADDPG was proposed for multi-agent reinforcement learning, which first extended actor-critic framework into multi-agent continuous policy optimization. However, they assumed that agents could share their observations and thus designed a centralized critic for agents, which differed from our problem. Considering that the users are unable to share their observations, we design a decentralized actor and decentralized critic for each user. Denote the actor network for approximating the policy of user n as \pi _{\boldsymbol {\theta }_n} and the critic network for approximating the value function of user n as V_{\boldsymbol {\omega }_n}, where \boldsymbol {\theta }_n and \boldsymbol {\omega }_n represent the parameters of actor network and critic network, respectively.

More precisely, the actor network \pi _{\boldsymbol {\theta }_n} is designed as a multi-layer fully-connected neural network, which determines the size of input data x_n^k uploaded by user n from its real-time observation \boldsymbol {o}_n^k. The critic network V_{\boldsymbol {\omega }_n} is much more complex, which consists of connected-layer and DNC [34]. DNC is a special recurrent neural network with internal memory module, which is capable of learning and remembering the past hidden states of inputs. With DNC, the observation value estimated by V_{\boldsymbol {\omega }_n} depends on the entire observation history. Previous works such as [35] and [36] have founded that recurrent neural networks are effective for addressing POMDP problems. We also demonstrate that critic network with DNC makes the user achieve faster and better convergence to equilibrium through compared experiments.

5.2.3 Policy Optimization Method
We optimize the continuous policy for each user through policy gradient method, for which the objective function is defined in (17). According to the policy gradient theorem proven in [37] and the trust region policy optimization theory proposed in [38], the policy gradient can be calculated as \begin{align*} \nabla _{\boldsymbol {\theta }_n}L_n&=\mathbb {E}_{\pi _{\boldsymbol {\theta }_n},\rho _n^1(\boldsymbol {o}_n)}\Big [\nabla _{\boldsymbol {\theta }_n}\log \pi _{\boldsymbol {\theta }_n}\big (\boldsymbol {o}_n,x_n\big)Q^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n,x_n\big)\Big ]\\ &=\mathbb {E}_{\pi _{\boldsymbol {\theta }_n},\rho _n^1(\boldsymbol {o}_n)}\Big [\nabla _{\boldsymbol {\theta }_n}\log \pi _{\boldsymbol {\theta }_n}\big (\boldsymbol {o}_n,x_n\big)A^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n,x_n\big)\Big ]\\ &\approx \mathbb {E}_{\pi _{\boldsymbol {\hat{\theta }}_n},\rho _n^1(\boldsymbol {o}_n)}\Big [f_n\nabla _{\boldsymbol {\theta }_n}\log \pi _{\boldsymbol {\theta }_n}\big (\boldsymbol {o}_n,x_n\big)A^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n,x_n\big)\Big ], \tag{21}\end{align*}
View Sourcewhere f_n=\frac{\pi _{\boldsymbol {\theta }_n}(\boldsymbol {o}_n|x_n)}{\pi _{\boldsymbol {\hat{\theta }}_n}(\boldsymbol {o}_n|x_n)}, A^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n,x_n\big)=Q^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n,x_n\big)-V^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n\big) is the advantage function for observation and action, \boldsymbol {\hat{\theta }}_n represents the parameter of policy for sampling, and \rho _n^1(\boldsymbol {o}_n) is the observation distribution induced by the POMDP. To accelerate the convergence of policy optimization, we adopt the proximal policy optimization (PPO) method proposed in [39], which clips the policy gradient as \begin{equation*} \nabla _{\boldsymbol {\theta }_n}L_n\approx \mathbb {E}_{\pi _{\boldsymbol {\hat{\theta }}_n},\rho _n^1(\boldsymbol {o}_n)}\Big [\nabla _{\boldsymbol {\theta }_n}\log \pi _{\boldsymbol {\theta }_n}\big (\boldsymbol {o}_n,x_n\big)C(\boldsymbol {o}_n,x_n)\Big ], \tag{22}\end{equation*}
View SourceRight-click on figure for MathML and additional features.where \begin{equation*} C(\boldsymbol {o}_n,x_n)=\min \big [f_nA^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n,x_n\big), \eta (f_n)A^{\pi _{\boldsymbol {\theta }_n}}\big (\boldsymbol {o}_n,x_n\big)\big ], \tag{23}\end{equation*}
View Source\begin{equation*} \eta (x)= \left\lbrace \begin{array}{ll}1+\varepsilon, &x>1+\varepsilon \\ x, &1-\varepsilon \leq x \leq 1+\varepsilon \\ 1-\varepsilon, &x<1-\varepsilon \end{array}\right., \tag{24}\end{equation*}
View Sourceand \varepsilon is an adjustable parameter.

5.2.4 Updating Laws of Actor and Critic
We define the loss function for updating the critic network of user n as \begin{equation*} J_n(\boldsymbol {\omega }_n) = \mathbb {E}_{\boldsymbol {o}_n\sim \rho _n^1(\boldsymbol {o}_n)}\Big [-V_{\boldsymbol {\omega }_n}(\boldsymbol {o}_n) +\mathbb {E}_{\boldsymbol {o}_n^{\prime }, x_n}\big [r+V_{\boldsymbol {\omega }_n}(\boldsymbol {o}_n^{\prime })\big ]\Big ]^2, \tag{25}\end{equation*}
View Sourcewhere \boldsymbol {o}_n^{\prime } is the next time observation of \boldsymbol {o}_n for user n. During training process, the estimated gradient about \boldsymbol {\omega }_n is calculated as \begin{equation*} \nabla _{\boldsymbol {\omega }_n}\hat{J}_n = \frac{1}{D}\sum _{k=0}^{D-1}\Big [V_{\boldsymbol {\omega }_n}\big (\boldsymbol {o}_n^k\big)-Y_n^k)\Big ]\frac{dV_{\boldsymbol {\omega }_n}(\boldsymbol {o}_n)}{d\boldsymbol {\omega }_n}, \tag{26}\end{equation*}
View Sourcewhere \begin{equation*} Y_n^k =R_n^k-\gamma ^{D-k}R_n(D)+\gamma ^{D-k}V_{\boldsymbol {\omega }_n}\big (\boldsymbol {o}_n(D)\big), \tag{27}\end{equation*}
View Sourceand D is the size of mini-batch for updating critic network. Then, the critic network V_{\boldsymbol {\omega }_n} is updated through mini-batch stochastic gradient descent method as follows \begin{equation*} \boldsymbol {\omega }_n \leftarrow \boldsymbol {\omega }_n - l_{n,1}\nabla _{\boldsymbol {\omega }_n}\hat{J}_n, \tag{28}\end{equation*}
View Sourcewhere l_{n,1} is the critic learning rate for user n.

Moreover, the estimated gradient about \boldsymbol {\theta }_n is calculated as \begin{equation*} \nabla _{\boldsymbol {\theta }_n}\hat{L}_n=\frac{1}{D}\sum _{k=0}^{D-1}\nabla _{\boldsymbol {\theta }_n}\log \pi _{\boldsymbol {\theta }_n}(\boldsymbol {o}_n^k,x_n^k)C(\boldsymbol {o}_n^k,x_n^k), \tag{29}\end{equation*}
View Sourcewhere D is the size of mini-batch for updating actor network. And we update the actor network \pi _{\boldsymbol {\theta }_n} by mini-batch stochastic gradient ascent as follows \begin{equation*} \boldsymbol {\theta }_n \leftarrow \boldsymbol {\theta }_n + l_{n,2}\nabla _{\boldsymbol {\theta }_n}\hat{L}_n, \tag{30}\end{equation*}
View Sourcewhere l_{n,2} is the actor learning rate for user n.

Algorithm 2. D-DRL: Algorithm Without Information Sharing
for user n\in \mathcal {N} do

   Initialize \gamma, l_{n,1}, l_{n,2}, \boldsymbol {\theta }_n, \boldsymbol {\omega }_n, and \boldsymbol {o}_n^0.

end for

for time slot k in 1,2,\ldots do

   for user n\in \mathcal {N} do

      Observe b_n^k and update its observation \boldsymbol {o}_n^{k-1} into \boldsymbol {o}_n^k.

      Store \lbrace \boldsymbol {o}_n^{k-1},x_n^{k-1},\boldsymbol {o}_n^{k},r_n^{k-1}\rbrace into \mathcal {D}_n.

      Input \boldsymbol {o}_n^k into actor network \pi _{\boldsymbol {\theta }_n} and determine the size of input data x_n^k uploaded to the edge server.

      Calculate its reward r_n^k=u_n(x_n^k,\boldsymbol {x}_{-n}^k) by (1).

   end for

   if k \% D == 0 then

      for m in 1,2,\ldots, M do

         for user n\in \mathcal {N} do

            Calculate \nabla _{\boldsymbol {\theta }_n}\hat{L}_n and \nabla _{\boldsymbol {\omega }_n}\hat{J}_n via (26) and (29).

            Update \boldsymbol {\theta }_n and \boldsymbol {\omega }_n through (28) and (30).

         end for

      end for

      Clear the replay buffer \mathcal {D}_n.

   end if

end for

5.2.5 Algorithm Details
The pseudo codes are shown in Algorithm 2, where each user initializes their observations and the parameters of their actor and critic networks (Line 1-3).

At each time slot k, each user observes its bandwidth and updates its observation (Line 6). Then, each user stores its old observation, strategy, reward and new observation into its experience replay buffer (Line 7). Next, each user takes its current observation as the input of its actor network and then upload a portion of its input data to the edge sever according to the output of the actor network (Line 8). After all users upload their data, each user calculates its instant utility (reward) (Line 9).

Each user updates its actor and critic networks in every D time slots (Line 13). They take the D records in their replay buffers as one mini-batch and estimate the gradients for updating actor and critic networks (Line 14). After that, each user optimizes its actor and critic networks by mini-batch stochastic gradient ascent and descent methods for M times, respectively (Line 15). Finally, they clear their replay buffer (Line 18).

SECTION 6Performance Evaluation
6.1 Simulation Settings
Extensive simulations are conducted to evaluate the performance of the proposed algorithms. We consider a set of users, each of which has an amount of input data C_n \;\backsim\; N(1, 0.1)Mb. These data should be processed in real time and deleted periodically to avoid the storage overflow. We set the user's unit transmission cost as p_n=1 J / s, the weight as \alpha _n=1, the user's computation capacity in time slot k as F_n^k = 0.01 Mb/s and computation capacity of edge server as R=16Mb / s by default. Any radio bandwidth that satisfies the constrain in Theorem 1 is reasonable. In our simulation, we set the minimum and maximum network bandwidth as \frac{1}{12.5}Mbps and \frac{1}{4.5}Mbps which satisfies the constrain.

The parameters of the DRL controller are selected through fine-tuning. Specifically, the actor network has two hidden fully-connected layers, each of which contains 200 nodes and 50 nodes, respectively. The critic network also has two hidden fully-connected layers, each of which has 200 nodes and 50 nodes, respectively. The DNC module has three read heads, one write head, and a memory matrix whose size is 10 \times 32. We set D=20, M=5 and L=5 by default. We compared our work with five baseline approaches.1 In the simulation, the Nash equilibrium (NE) is calculated by Algorithm 1 with complete information sharing. Moreover, as for real world scenario, each user could not get the information of others. Then, the learning-based offloading algorithm has been designed without knowing any prior information to derive the NE.

MAA2C [40]: this is a modified A2C, which is an implementation of A2C for multi-agent environment, called “MAA2C” in this paper.

MAPPO [41]: this is a modified PPO, which is an implementation of PPO for very large scale competitive multi-agent training, called “MAPPO” in this paper.

Greedy: it is a heuristic algorithm that greedily chooses a policy with maximum reward from the replay buffer.

Random: users randomly select the size of input data to offload to the edge server.

Edge computing by all users (EdgeAll for short): users offload all their computation to the edge server.

6.2 Simulation Results
We first study the convergence of the proposed D-DRL algorithm when there are 4 users. In this simulation, we set b_1^k=\frac{1}{11.5}, b_2^k=\frac{1}{10.5}, b_3^k=\frac{1}{9.5}, b_4^k=\frac{1}{8.5}. As shown in Fig. 5, both offloading strategies (i.e., x_{n}^{k}) and user utility converge to the NE at about 8000 time slots. The offloading strategies of the four users under Nash equilibrium are 0.165, 0.26, 0.485, and 0.43, respectively.

We then study the influence of network bandwidth by changing user 4's bandwidth (i.e., b_{4}^k) from \frac{1}{12.5} to \frac{1}{4.5}. The bandwidth of other 3 users are set as b_1^k=\frac{1}{11.5}, b_2^k=\frac{1}{10.5}, and b_3^k=\frac{1}{9.5}. As shown in Fig. 4, user 4 increases its size of offloading data and thus obtains a higher utility, under a higher radio bandwidth. For example, user 4 offloads less than 20 percent of input data to the edge when b_{4}^k=\frac{1}{12.5}, leading to a low utility of 0.2. However, when its bandwidth has been improved to b_{4}^k=\frac{1}{4.5}, a higher utility of 6.2 can be obtained by offloading about 81 percent of input data. This is because the unit transmission cost decreases under a larger bandwidth, which motivates users to upload more data to the edge server. In Fig. 4c, we also observe that the user computation delay decreases as the growth of the radio bandwidth. For example, the delay of user 4 decreases by 76.5 percent as the radio bandwidth b_4^k changes from \frac{1}{12.5} to \frac{1}{4.5}. This is because user 4 uploads more data to the edge server during radio bandwidth increasing, leaving less data to be computed locally, thus will reduce the delay of local computation. Fig. 4d shows the energy consumption of each user. We can observe that, user 4 consumes more energy to upload more data to the edge server, in order to decrease the computation delay and thus increase the utility. Since other users offload less data to the edge server with the increase of user 4's radio bandwidth, their energy consumption will reduce. In addition, other users have a performance degradation during the bandwidth increasing of user 4. For example, user 3 has a size of data offloading decreasing by 30 percent, the utility decreasing by 60 percent, and computation delay increasing by 18 percent. This is because other users’ competitiveness declined and resulting in computation resource reducing. Based on this analysis, we can derive that when other user's bandwidth increases, user n must upload less data to the edge server.

Fig. 4. - 
Performance of D-DRL with four users when varying the radio bandwidth of user 4.
Fig. 4.
Performance of D-DRL with four users when varying the radio bandwidth of user 4.

Show All

Fig. 5. - 
Convergence of D-DRL.
Fig. 5.
Convergence of D-DRL.

Show All

We study the affect of weight \alpha _{n} by changing \alpha _{4} from 1 to 1.8. The weights of other users are set to \alpha _1=1, \alpha _2=1, and \alpha _3=1. Meanwhile, we fix the bandwidth as b_1^k=\frac{1}{11.5}, b_2^k=\frac{1}{10.5}, b_3^k=\frac{1}{9.5}, and b_4^k=\frac{1}{8.5}. As shown in Fig. 6a, we observe that the user 4 with larger weight offloads more data to the edge server, which coincides the statement in Corollary 1. That is because user 4's battery is in high level, as shown in Fig. 6d, it can spend more power to offload more data to the edge server, in order to reduce the computation delay. For example, for user 4, the size of input data offloaded to the edge server increases by 90 percent, while the computation delay decreases by 66 percent, if the weight changes from 1.0 to 1.8.

Fig. 6. - 
Performance of D-DRL with four users when varying the weight of user 4.
Fig. 6.
Performance of D-DRL with four users when varying the weight of user 4.

Show All

We compare the performance of five different algorithms under the time-variant network environment, where b_1^k=\frac{1}{11.5}, b_2^k=\frac{1}{10.5}, b_3^k=\frac{1}{9.5} and b_4^k is randomly chosen among \lbrace \frac{1}{4.5}, \frac{1}{6.5}, \frac{1}{8.5}, \frac{1}{10.5}, \frac{1}{12.5}\rbrace. As shown in Fig. 7, our proposed algorithm D-DRL significantly exceeds others, by having the highest average utility and the fastest convergence speed. For instance, D-DRL increases the average utility of the users by 2 percent as compared with MAPPO, by 14 percent as compared with MAA2C, by 32.5 percent as compared with Greedy and by 410 percent as compared with Random. Meanwhile, our proposal takes about 8000 time slots to converge to the stable state, and almost no shock. However, MAPPO takes about 14000 time slots and MAA2C takes about 18000 time slots. At the same time, both of them have shocks.

Fig. 7. - 
Average performance with $N=4$N=4 users in time-variant network environment.
Fig. 7.
Average performance with N=4 users in time-variant network environment.

Show All

Fig. 8 shows the impact of number of users. In this simulation, in order to show the simulation results more clearly, we set R=24 Mb. The average utility of users decreases with the number of users sharing the edge server. That is, although the computation capability of the edge server remains unchanged, it leads to more competitions among users. Therefore, users need to decrease the size of computation to offload to the edge server, resulting in a decline of average utility, as shown in Fig. 8a. At the same time, we can observe that D-DRL has the maximum average utility. When N is greater than 4, MAA2C could not converge to the Nash equilibrium. Fig 8b shows the convergence speed of the three strategies, we see that D-DRL converges to the stable state with the fastest speed.

Fig. 8. - 
Performance of decentralized computation offloading when varying the number of users.
Fig. 8.
Performance of decentralized computation offloading when varying the number of users.

Show All

SECTION 7Discussions
7.1 Time Complexity Analysis
During the D-DRL execution, given the observation information as input, each user utilizes its own actor network \pi _{\boldsymbol {\theta }_n} to generate an action, and thus the computational complexity is merely based on a fully-connected deep neural network. According to [42], the time complexity of a fully-connected deep neural network is determined by the number of multiplication operations, which is O(\sum _{f=1}^F \epsilon _f \cdot \epsilon _{f-1}), and \epsilon _f is the number of neural units in fully-connected layer f. In our design, we use two fully-connected hidden layers in the actor network \pi _{\boldsymbol {\theta }_n} . Meanwhile, since modern mobile devices are becoming stronger and stronger,2 they can afford the computational overhead incurred by such a kind of actor network.

7.2 Task Offloading Scenario
As for the big data analytics, for example, there are large amounts of pictures to be analyzed. In this case, the volume of pictures can be approximated as a continuous value. Hence, this picture analysis task can be arbitrarily divided into pieces and chosen a portion of them to offload to the edge server. Our model fits this scenario very well. Even for a single picture analysis task, the task is consisted of a large amount of CPU instructions which also can be regarded as a continuous value. Therefore, our model also matches it.

SECTION 8Conclusion
In this paper, we consider the dynamic computation offloading decision making problem among users for edge computing under dynamic environment and propose a computation offloading game formulation. We show that the game always admits a unique Nash equilibrium under the certain conditions. We also design a decentralized computation offloading algorithm called “D-DRL” which first combines policy gradient DRL-based approach with DNC that can achieve the optimal offloading strategy. Simulation results demonstrate that the proposed algorithm is much more efficient as compared with the baseline approaches.