Semi-supervised anomaly detection is an approach to identify anomalies by learning the distribution of normal data. Backpropagation neural networks (i.e., BP-NNs) based approaches have recently drawn attention because of their good generalization capability. In a typical situation, BP-NN-based models are iteratively optimized in server machines with input data gathered from the edge devices. However, (1) the iterative optimization often requires significant efforts to follow changes in the distribution of normal data (i.e., concept drift), and (2) data transfers between edge and server impose additional latency and energy consumption. To address these issues, we propose ONLAD and its IP core, named ONLAD Core. ONLAD is highly optimized to perform fast sequential learning to follow concept drift in less than one millisecond. ONLAD Core realizes on-device learning for edge devices at low power consumption, which realizes standalone execution where data transfers between edge and server are not required. Experiments show that ONLAD has favorable anomaly detection capability in an environment that simulates concept drift. Evaluations of ONLAD Core confirm that the training latency is 1.95x~6.58x faster than the other software implementations. Also, the runtime power consumption of ONLAD Core implemented on PYNQ-Z1 board, a small FPGA/CPU SoC platform, is 5.0x~25.4x lower than them.
SECTION 1Introduction
Anomaly detection is an approach to identify rare data instances (i.e., anomalies) that have different patterns or come from different distributions from that of the majority (i.e., the normal class) [1]. There are mainly three approaches in anomaly detection: (1) supervised anomaly detection, (2) semi-supervised anomaly detection, and (3) unsupervised anomaly detection.

A typical strategy of supervised anomaly detection is to build a binary-classification model for the normal class versus the anomaly class [1]. It requires labeled normal and anomaly data to train a model, however, anomaly instances are basically much rarer than normal ones, which imposes the class-imbalanced problem [2]. Several works have addressed this issue by undersampling the majority data or oversampling the minority data [3], [4], or assigning more costs on misclassified data to make the classifier concentrate the minority classes [5].

Semi-supervised anomaly detection, one of the main topics of this paper, assumes that all the training data belong to the normal class [1]. A typical strategy of semi-supervised anomaly detection is to learn the distribution of normal data and then to identify data samples distant from the distribution as anomalies. Semi-supervised approaches do not require anomalies to train a model, which makes them applicable to a wide range of real-world tasks. Various approaches have been proposed, such as nearest-neighbor based techniques [6], [7], clustering approaches [8], [9], and one-class classification approaches [10], [11].

Unsupervised anomaly detection does not require labeled training data [1], thus its constraint is the least restrictive. Many semi-supervised methods can be used in an unsupervised manner by using unlabeled data to train a model because most unlabeled data belong to the normal class. Sometimes, unsupervised anomaly detection and semi-supervised anomaly detection are not distinguished explicitly.

In this paper, we focus on semi-supervised anomaly detection. Recently, neural network-based approaches [12], [13], [14] have been drawing attention because in many cases they achieve relatively higher generalization performance than the traditional approaches for a wide range of real-world data such as images, natural languages, and audio data. Although there are some variants of neural networks, backpropagation neural networks (i.e., BP-NNs) are currently widely used.

Fig. 1 illustrates a typical application of BP-NN-based semi-supervised anomaly detection models. The system shown in the figure is designed for edge devices that implement their own models to detect anomalies of incoming real-world data. In this system, the edge devices are supposed to perform only inference computations (e.g., calculating anomaly scores), and training computations are offloaded to server machines. The models are iteratively trained in the server machines with a large amount of input data gathered from the edge devices. Once the training loop completes, parameters of the edge devices are updated with the optimized ones. However, there are two issues with this approach: (1) BP-NNs’ iterative optimization approach often takes a considerable computation time, which makes it difficult to follow time-series changes in the distribution of normal data (i.e., concept drift). (2) Data transfers to the server machines may impose several problems on the edge devices such as additional latency and energy consumption for communication.

As mentioned before, learning the distribution of normal data is a key feature of semi-supervised anomaly detection approaches. However, the distribution may change over time. This phenomenon is referred to as concept drift. Concept drift is a serious problem when there are frequent changes in the surrounding environment of data [15] or behavioral state changes in data sources [16]. A semi-supervised anomaly detection model should learn new normal data to follow the changes, however, BP-NNs’ iterative optimization approach often introduces a considerable delay, which widens a gap between the latest true distribution of normal data and the one learned by the model [17]. This gap makes identifying anomalies more difficult gradually.


Fig. 1.
Typical application of BP-NN-based semi-supervised anomaly detection models.

Show All

Usually, edge devices that implement machine learning models are specialized only for prediction computations because the backpropagation method often requires a large amount of computational power. This is why training computations of BP-NNs are typically offloaded to server machines with high computational power. In this case, data transfers to the server machines are inevitable, which imposes additional energy consumption for communication and potential risk of data breaches on the edge devices.

One practical solution to these two issues is the on-device sequential learning approach illustrated in Fig. 2. In this approach, incoming input data are sequentially learned on edge devices themselves. This approach allows the edge devices to sequentially follow changes in the distribution of normal data and makes possible standalone execution where no data transfers are required. However, it poses challenges in regard to how to construct such a sequential learning algorithm and how to implement it on edge devices with limited resources.


Fig. 2.
On-device sequential learning approach.

Show All

To deal with the underlying challenges, we propose an ON-device sequential Learning semi-supervised Anomaly Detector called ONLAD and its IP core, named ONLAD Core.1 The algorithm of ONLAD is designed to perform fast sequential learning to follow concept drift in less than one millisecond. ONLAD Core realizes on-device learning for resource-limited edge devices at low power consumption.

In this paper, we make the following contributions:

ONLAD leverages OS-ELM [19], a lightweight neural network that can perform fast sequential learning, as a core component. In Section 3.1, we theoretically analyze the training algorithm of OS-ELM and demonstrate that the computational cost significantly reduces without degrading the training results when the batch size equals 1.

In Section 3.2, we propose a computationally lightweight forgetting mechanism for OS-ELM based on FP-ELM, a state-of-the-art OS-ELM variant with a dynamic forgetting mechanism. Since a key feature of semi-supervised anomaly detection is to learn the distribution of normal data, OS-ELM should be able to forget past learned normal data when the distribution changes. The proposed method provides such a function for OS-ELM with a tiny additional computational cost.

In Section 3.3, we propose ONLAD, a new sequential learning semi-supervised anomaly detector that combines OS-ELM and an autoencoder [20], a neural network-based dimensionality reduction model. This combination, together with the other proposed techniques to reduce the computational cost, realizes fast sequential learning semi-supervised anomaly detection. Experiments using several public datasets in Section 5 show that ONLAD has comparable generalization capability to that of BP-NN based models in the context of anomaly detection. They also confirm that ONLAD outperforms BP-NN based models in terms of anomaly detection capability especially in an environment that simulates concept drift.

In Section 4, we describe the design and implementation of ONLAD Core. Evaluations of ONLAD Core in Section 6 show that ONLAD Core can perform training and prediction computations approximately in less than one millisecond. In comparison with software counterparts, the training latency of ONLAD Core is faster by 1.95x∼6.58x, while the prediction latency is faster by 2.29x∼4.73x on average. They also confirm that the proposed forgetting mechanism is faster than the baseline algorithm, FP-ELM, by 3.21x on average. In addition, our evaluations show that ONLAD Core can be implemented on PYNQ-Z1 board, a small FPGA/CPU SoC platform, in practical model sizes. It is also demonstrated that the runtime power consumption of PYNQ-Z1 board that implements ONLAD Core is 5.0x∼25.4x lower than the other software counterparts when training computations are continuously executed.

The rest of this paper is organized as follows: Section 2 provides a brief review of the basic technologies behind ONLAD. We propose ONLAD in Section 3. Section 4 describes the design and implementation of ONLAD Core. ONLAD is evaluated in terms of anomaly detection capability in Section 5. ONLAD Core is also evaluated in terms of latency, FPGA resource utilization, and power consumption in Section 6. Related works are described in Section 7. Section 8 concludes this paper.

SECTION 2Preliminaries
This section provides a brief introduction of the base technologies behind ONLAD: (1) Extreme Learning Machine (ELM), (2) Online Sequential Extreme Learning Machine (OS-ELM), and (3) autoencoders.

2.1 ELM
ELM [21] illustrated in Fig. 3 is a kind of single hidden layer feedforward network (i.e., SLFN) that consists of an input layer, a hidden layer, and an output layer. Suppose an n-dimensional input chunk x∈Rk×n of batch size =k is given; an m-dimensional output chunk y∈Rk×m is computed as follows.
y=G(x⋅α+b)β,(1)
View Sourcewhere α∈Rn×N~ denotes an input weight connecting the input layer and the hidden layer, and β∈RN~×m an output weight connecting the hidden layer and the output layer. b∈RN~ denotes a bias vector of the hidden layer, and G an activation function applied to the hidden layer output.


Fig. 3.
Extreme learning machine.

Show All

If an SLFN can approximate an m-dimensional target chunk t∈Rk×m with zero error, it implies that there exists β which satisfies the following equation.
G(x⋅α+b)β=t.(2)
View SourceLet H∈Rk×N~ be the hidden layer output G(x⋅α+b); then the optimal output weight β^ is computed as follows.
β^=H†t,(3)
View Sourcewhere H† is the pseudo inverse of H. H† can be calculated with matrix decomposition algorithms such as Singular Value Decomposition (SVD) [22]. In particular, if HTH or HHT is non-singular, H† can be calculated in an efficient way with H†=(HTH)−1HT or H†=HT(HHT)−1.

The whole training process is completed simply by replacing β with β^. α and b do not change once they have been initialized with random values; the conversion from x to H is random projection.

ELM does not use iterative optimization that BP-NNs use, but rather one-shot optimization, which makes the whole training process faster. ELM can compute the optimal output weight faster than BP-NNs [21]. It is categorized as a batch learning algorithm, wherein all the training data are assumed to be available in advance. In other words, ELM must be retrained with the whole dataset, including the past training data, in order to learn new instances.

2.2 OS-ELM
OS-ELM [19] is an ELM variant that can perform sequential learning instead of batch learning. Suppose the ith training chunk {xi∈Rki×n,ti∈Rki×m} of batch size =ki is given; we need to find β that minimizes the following error.
∥∥∥∥∥⎡⎣⎢⎢H0⋮Hi⎤⎦⎥⎥βi−⎡⎣⎢⎢t0⋮ti⎤⎦⎥⎥∥∥∥∥∥,(4)
View Sourcewhere Hi is defined as Hi≡G(xi⋅α+b). The optimal output weight is sequentially computed as follows.
Piβi=Pi−1−Pi−1HTi(I+HiPi−1HTi)−1HiPi−1=βi−1+PiHTi(ti−Hiβi−1),(5)
View SourceP0 and β0 are computed as follows.
P0β0=(HT0H0)−1=P0HT0t0.(6)
View SourceThe number of initial training samples k0 should be greater than that of hidden nodes N~ to make HT0H0 nonsingular.

As shown in Equation (5), OS-ELM sequentially finds the optimal output weight for the new training chunk without memory or retraining using past training data, unlike ELM. OS-ELM can also find the optimal solution faster than BP-NNs [19].

2.3 Autoencoders
An autoencoder [20] illustrated in Fig. 4 is a neural network-based unsupervised learning model for finding a well-characterized dimensionality reduced form x~∈Rk×n~ of an input chunk x∈Rk×n (n~<n). Generally, the output of an intermediate layer is regarded as x~. ELM and OS-ELM have only one intermediate layer; therefore, the hidden layer output H is regarded as x~. Basically, the number of hidden nodes n~ is constrained to be less than that of input nodes n. Such autoencoders are specially referred to as undercomplete autoencoders. However, sometimes they take the opposite setting (i.e., n<n~) where they are referred to as overcomplete autoencoders. Although overcomplete autoencoders cannot perform dimensionality reduction, they can obtain well-characterized representations for classification problems by applying regularization conditions or noise [23] to their loss functions.


Fig. 4.
Autoencoder.

Show All

In the training process, input data are also used as targets (i.e., t=x); therefore, an autoencoder is trained to correctly reconstruct input data as output data. It is empirically known that x~ tends to become well-characterized when the error between input data and reconstructed output data converges [20]. Labeled data are not required during the whole training process; this is why an autoencoder is categorized as an unsupervised learning model.

Autoencoders have been attracting attention in the field of semi-supervised anomaly detection [13], [24], too. In this context, an autoencoder is trained only with normal data; therefore, its output tends to have a relatively large reconstruction error in the case of an anomaly. Thus, anomalies can be detected by setting a threshold for the errors. This approach is categorized as a semi-supervised anomaly detection method since only normal data are used as training data.

Principal Component Analysis (PCA), another non-statistical dimensionality reduction algorithm, is often compared with autoencoders. Sakurada et al. showed that autoencoder-based models can detect subtle anomalies that PCA fails to pick up [13]. Moreover, autoencoders can perform nonlinear transformations without costly computations that kernel PCA [25] requires.

SECTION 3ONLAD
As mentioned in the introduction, ONLAD leverages OS-ELM as its core component. In this section, we provide a theoretical analysis of OS-ELM and demonstrate that the computational cost of the training algorithm significantly reduces when batch size = 1 without any deterioration of the training results. Then, we propose a computationally lightweight forgetting mechanism to deal with concept drift. Finally, we formulate the algorithm of ONLAD.

3.1 Analysis of OS-ELM
The training algorithm of OS-ELM (i.e., Equation (5)) mainly consists of (1) matrix products and (2) matrix inversions. Suppose the computational iterations of a matrix product A∈Rp×q⋅B∈Rq×r are pqr and those of a matrix inversion C−1∈Rr×r are r3; the total computational iterations of these two operations in Equation (5) are calculated as follows.
IprodIinv=4kN~2+k(2k+2m+n)N~=k3,
View Sourcewhere Iprod denotes the total computational iterations of the matrix products, while Iinv denotes those of the matrix inversions. n, N~, and m are the numbers of input, hidden, and output nodes of OS-ELM, respectively. k denotes the batch size. For instance, the computational iterations of HiPi−1HTi are calculated by dividing the computing process into two steps: (1) Hi∈Rk×N~⋅Pi−1∈RN~×N~ and (2) HiPi−1∈Rk×N~⋅HTi∈RN~×k. In this case, these computational iterations are calculated as kN~2 and k2N~, respectively.

Let Ik be the total computational iterations of matrix products and matrix inversions in Equation (5) when batch size = k. Accordingly, the following equations can be derived.
Ik=Iprod+Iinv=4kN~2+k(2k+2m+n)N~+k3=k(4N~2+(2k+2m+n)N~+k2)≥k(4N~2+(2+2m+n)N~+1)=kI1.
View Source

Finally, Ik≥kI1 is obtained. This inequality shows that the training algorithm becomes computationally more efficient when batch size = 1, rather than when batch size = k (>1). Please note that this insight does not always make sense especially for software implementations because this computational model does not take into account the software-specific overheads such as memory allocation and function calls. However, bare-metal implementations, including ONLAD Core, receive benefits from this insight since they are free from such overheads. Moreover, when k = 1, the computational cost of the matrix inversion (I+HiPi−1HTi)−1 in Equation (5) is significantly reduced, as the size of the target matrix I+HiPi−1HTi is k×k. In this case, the following training algorithm is derived from Equation (5).
Piβi=Pi−1−Pi−1hTihiPi−11+hiPi−1hTi=βi−1+PihTi(ti−hiβi−1),(7)
View Sourcewhere h∈RN~ denotes the special case of H∈Rk×N~ when k = 1. Thanks to the above trick, OS-ELM can perform training without any costly matrix inversions, which helps to reduce not only the computational cost but also the hardware resources needed for ONLAD Core. It also makes it easier to parallelize the training algorithm, because there are no matrix inversions with a low degree of parallelism in Equation (7). Furthermore, the training results of OS-ELM are not affected even when batch size = 1, because OS-ELM gives the same output weight when training is performed N times with batch size = k or Nk times with batch size = 1. This is a notable difference from BP-NNs; their training results get better or worse depending on the batch size. On the basis of the above discussion, the batch size of OS-ELM used in ONLAD is always set to 1.

3.2 Lightweight Forgetting Mechanism for OS-ELM
In certain real environments, the distribution of normal data may change as time goes by. In this case, ONLAD should have a function to adaptively forget past learned normal data with a tiny additional computational cost. To deal with this challenge, we propose a computationally lightweight forgetting mechanism based on Forgetting Parameters Extreme Learning Machine (FP-ELM) [26], a state-of-the-art OS-ELM variant with a dynamic forgetting mechanism.

3.2.1 Review of FP-ELM
This section provides a brief review of FP-ELM. The training algorithm of FP-ELM is formulated as follows.
Kiβi=α2iKi−1+HTiHi=βi−1+(λI+Ki)−1⋅(HTi(ti−Hiβi−1)−λ(1−α2i)βi−1).(8)
View SourceEspecially, K0 and β0 are computed as follows.
K0β0=HT0H0=(λI+HT0H0)−1HT0t0,(9)
View Sourcewhere λ is the L2 regularization parameter for β. λ limits ∥β∥2 so that it does not become too large to prevent overfitting. 0<αi≤1 is the forgetting factor that controls the weight (i.e., the significance) of each past training chunk. Suppose the latest training step is i; then wk, the weight of the kth training chunk, is gradually decreased from one step to the next, as shown below.
wk={∏ij=k+1αj,1,(0≤k≤i−1)(k=i).(10)
View SourcePlease note that αi is a variable parameter that can be adaptively updated according to the information in the arriving input data or output error values.

3.2.2 Proposed Forgetting Mechanism
FP-ELM can control the weights of past training chunks. However, it cannot remove the matrix inversion (λI+Pi)−1 in Equation (8) even when the batch equals 1, because the size of the target matrix λI+Pi is N~×N~, where N~ denotes the number of hidden nodes. To address this issue, we modify FP-ELM so that it can remove the matrix inversion when batch size = 1.

First, the following equations are derived by disabling the L2 regularization trick (i.e., let λ = 0) in Equation (8).
Kiβi=α2iKi−1+HTiHi=βi−1+K−1iHTi(ti−Hiβi−1).(11)
View SourceNext, the update formula of K−1i is derived with the Woodbury formula [27].2
K−1i=(α2iKi−1+HTiHi)−1=(1α2iK−1i−1)−(1α2iK−1i−1)HTi⋅(I+Hi(1α2iK−1i−1)HTi)−1Hi(1α2iK−1i−1).(12)
View SourceFinally, the training algorithm is obtained by defining Pi≡K−1i.
Piβi=(1α2iPi−1)−(1α2iPi−1)HTi⋅(I+Hi(1α2iPi−1)HTi)−1Hi(1α2iPi−1)=βi−1+PiHTi(ti−Hiβi−1),(13)
View SourceP0 and β0 are computed with the same algorithm as Equation (6). The proposed forgetting mechanism eliminates the matrix inversion in Equation (13) when batch size = 1 because the size of the target matrix I+Hi(1α2iPi−1)HTi is k×k, where k denotes the batch size. Equation (13) becomes equal to the original training algorithm of OS-ELM when 1α2iPi is replaced with Pi. Thus, the proposed method provides a forgetting function with a tiny additional computational cost to the original training algorithm of OS-ELM. However, it may suffer from overfitting, since the L2 regularization trick is disabled. The trade-off is quantitatively evaluated in Section 5.

Algorithm 1. Example of Using ONLAD
α←random(), b←random()

H0←G(x0∈Rk0×n⋅α+b) {k0≫N~}

P0←(HT0H0)−1, β0←P0HT0t0

i←1

for until {xi∈Rn,0<αi≤1} exists do

hi←G(xi⋅α+b)

if ϵ>1+hi(1α2iPi−1)hTi then

print(“Singular matrix encountered.”)

i←i+1

continue

end if

score←L(xi,hiβi−1)

if score>θ then

print(“Anomaly detected.”)

end if

Pi−1←1α2iPi−1

Pi←Pi−1−Pi−1hTihiPi−11+hiPi−1hTi

βi←βi−1+PihTi(xi−hiβi−1)

i←i+1

end for

3.3 Algorithm
ONLAD leverages OS-ELM of batch size = 1 in conjunction with the proposed forgetting mechanism. The following equations are derived by combining Equations (7) and (13).
Piβi=(1α2iPi−1)−(1α2iPi−1)hTihi(1α2iPi−1)1+hi(1α2iPi−1)hTi=βi−1+PihTi(ti−hiβi−1).(14)
View SourceONLAD is built on an OS-ELM-based autoencoder to construct a semi-supervised anomaly detector; ti=xi holds in Equation (14). The training algorithm of ONLAD is as follows.
Piβi=(1α2iPi−1)−(1α2iPi−1)hTihi(1α2iPi−1)1+hi(1α2iPi−1)hTi=βi−1+PihTi(xi−hiβi−1),(15)
View SourceP0 and β0 are computed as follows (there are no changes from Equation (6)).
P0β0=(HT0H0)−1=P0HT0t0.(16)
View SourceAs indicated in Equation (15), ONLAD performs training and forgetting operations at the same time.

The prediction algorithm is formulated as follows.
score=L(x,G(x⋅α+b)β),(17)
View Sourcewhere L denotes a loss function, and score is an anomaly score of x.

3.4 Stability of OS-ELM Training
OS-ELM has a training stability issue: if I+HiPi−1HTi in Equation (5) is close to a singular matrix, the training becomes unstable regardless of the batch size [19]. In the context of ONLAD, the problem occurs when 1+hi(1α2iPi−1)hTi in Equation (15) is close to 0. Thus, ONLAD should stop the training when ϵ>1+hi(1α2iPi−1)hTi, where ϵ denotes a small positive value.

3.5 Example of ONLAD in Practical Use
The following is an example of ONLAD (shown in Algorithm 1) intended for practical use. First, α and b are initialized with random values; then β0 and P0 are computed with Equation (16). Please note that the number of initial training samples k0 should be larger than that of hidden nodes N~ to make HT0H0 nonsingular. At the ith training step in the following loop, the inequality ϵ>1+hi(1α2iPi−1)hTi is evaluated, then the rest of the lines are skipped if it is true. If it is false, then an anomaly score of xi is computed with Equation (17). xi is judged to be an anomaly if the score is greater than a user-defined threshold θ; otherwise ONLAD judges xi to be a normal sample. Finally, sequential learning is performed with Equation (15).

SECTION 4ONLAD Core
This section describes the design and implementation of ONLAD Core, an IP core of ONLAD. To demonstrate that ONLAD Core can be implemented on edge devices with limited resources, we use PYNQ-Z1 board, a low-cost SoC platform where an FPGA is integrated. Fig. 5 displays the board, and its specifications are shown in Table 1. We develop ONLAD Core with Vivado HLS v2018.3 and implement it on PYNQ-Z1 board using Vivado v2018.3. The clock frequency of ONLAD Core is set to 100.0 MHz.


Fig. 5.
PYNQ-Z1 board.

Show All

TABLE 1 Specifications of PYNQ-Z1 Board

4.1 Overview of Board-Level Implementation
First, we provide a brief overview of our board-level implementation. Fig. 6 shows the block diagram. The Processing System (PS) part is mainly responsible for preprocessing of input data and triggering a Direct Memory Access (DMA) controller. The DMA controller converts preprocessed input data in DRAM to AXI4-Stream format packets and transfers them to ONLAD Core. It also converts output packets of ONLAD Core back to AXI4-Memory-Mapped format data, and transfers them to DRAM. On the other hand, the Programmable Logic (PL) part implements ONLAD Core. ONLAD Core performs training or prediction computations according to the information in the header of input packets (the details are to be described later).


Fig. 6.
Block diagram of board-level implementation.

Show All

4.2 Details of ONLAD Core
Fig. 7 illustrates the block diagram of ONLAD Core, with its four important sub-modules: (1) Parameter Buffer, (2) Input Buffer, (3) Train Module, and (4) Predict Module. The rest of this section explains these sub-modules one by one.

Fig. 7. - 
Block diagram of ONLAD Core.
Fig. 7.
Block diagram of ONLAD Core.

Show All

4.2.1 Parameter Buffer
Parameter Buffer manages the parameters of ONLAD Core (i.e., α, β, P, and b). All the parameters are implemented with BRAMs; hence, more BRAM instances are consumed as the sizes of the parameters increase. Specifically, the total number of matrix elements of Parameter Buffer (denoted as Sparam) is calculated as follows.
Sparameter=N~2+(2n+1)N~.(18)
View SourcePlease note that n=m is applied in Equation (18), since ONLAD is an autoencoder. Equation (18) shows that the utilization of the BRAM instances of this module is proportional to the square of the number of hidden nodes N~2 and is also proportional to the number of input nodes n.

4.2.2 Input Buffer
Input Buffer stores a single input vector preprocessed in the PS part and, like Parameter Buffer, is implemented with BRAMs. The total number of matrix elements of Input Buffer (denoted as Sinput) is calculated as follows.
Sinput=n.(19)
View SourceEquation (19) shows that the utilization of the BRAM instances of this module is proportional to the number of input nodes n. This module is shared with Train Module and Predict Module so that they can read the input vector.

4.2.3 Train Module
Train Module executes the training algorithm (i.e., Equation (15)) in order to update the parameters in Parameter Buffer. Fig. 8 shows the processing flow. Each processing block is sequentially executed. According to the discussion in Section 3.4, Train Module is designed to interrupt the computation when O3<ϵ holds. In our implementation, ϵ is set to 1e−4. The output signal of Success indicates whether the inequality is satisfied or not (1/0 means satisfied/not satisfied). All the matrix operations, including matrix products, matrix adds, matrix subs, and element-wise multiplies are implemented with arithmetic units of 32-bit fixed-point precision using DSPs. To save hardware resources, these matrix operations are designed to use a specific number of arithmetic units regardless of the number of input and hidden nodes. The matrices shown in the processing flow (i.e., O1∼8 and hi) are implemented with BRAMs.


Fig. 8.
Processing flow of train module.

Show All

The total number of matrix elements of Train Module (denoted as Strain) is calculated as follows.
Strain=2N~2+4N~+2n+1.(20)
View SourceEquation (20) shows that the utilization of the BRAM instances of this module is proportional to the square of the number of hidden nodes N~2 and is also proportional to the number of input nodes n.

Itrain below denotes the total computational iterations needed to finish the processing flow, calculated in the manner described in Section 3.1.
Itrain=4N~2+(3n+1)N~.(21)
View SourceThe computational cost is proportional to the square of the number of hidden nodes N~2, and is also proportional to the number of input nodes n.

4.2.4 Predict Module
Predict Module executes the prediction algorithm (i.e., Equation (17)) to output anomaly scores. Fig. 9 shows the processing flow. Predict Module follows the design methodology of Train Module.


Fig. 9.
Processing flow of predict module.

Show All

The total number of matrix elements of Predict Module (denoted as Spredict) is calculated as follows.
Spredict=N~+n.(22)
View SourceEquation (22) shows that the utilization of the BRAM instances of this module is proportional to the numbers of hidden nodes N~ and input nodes n.

Ipredict below denotes the total computational iterations to finish the processing flow.
Ipredict=2nN~.(23)
View SourceThe computational cost is proportional to the numbers of hidden nodes N~ and input nodes n.

4.2.5 Implementation of Matrix Operations
In ONLAD Core, matrix operations, such as matrix product, matrix add, matrix sub, and element-wise multiply, are implemented as a dedicated circuit. These matrix operations are designed with C-level language and synthesized with Vivado HLS. Loop unrolling and loop pipelining directives are used in the innermost loops of these operations for parallelization. In this design, unrolling factor is set to 2, so that they are parallelized with two arithmetic units.

4.3 Instructions of ONLAD Core
ONLAD Core is designed to execute the following instructions: (1) update_params, (2) update_input, (3) update_ff, (4) do_training, and (5) do_prediction. The packet format of each instruction is detailed in Fig. 10. An input packet is of 64 bits long. The first 3-bit field (i.e., Mode Field) specifies an instruction to be executed on ONLAD Core. The following 61-bit field (i.e., Data Field) is reserved for several uses according to the instruction. An output packet is of 32 bits long and embeds an output result of Train Module or Predict Module.


Fig. 10.
Packet formats.

Show All

In the rest of this section, we describe how the sub-modules of ONLAD Core work according to each instruction.

4.3.1 update_params
This instruction updates Parameter Buffer. The packet format is shown in the first row of Fig. 10. The target parameter is specified in Mode Field of an input packet. Index in Data Field embeds an index of the target parameter, and Value an update value. The target parameter is updated as below.
target[Index]←Value.(24)
View SourcePlease note that all the parameters are managed as row-major flattened 1-D arrays in Parameter Buffer.

4.3.2 update_input
This instruction updates Input Buffer. The packet format (the second row of Fig. 10) is almost the same as update_params instruction except for Mode Field.
x[Index]←Value.(25)
View SourceInput Buffer is updated with the above formula. Please note that n input packets are required to create an n-dimensional input vector.

4.3.3 update_ff
This instruction updates the forgetting factor αi managed in Train Module. The packet format of this instruction is shown in the third row of Fig. 10. Value in Data Field embeds an update value. αi is updated as follows.
αi←Value.(26)
View Source

4.3.4 do_training
This instruction executes training computations with Train Module. Train Module first reads the latest parameters (i.e., βi−1 and Pi−1) from Parameter Buffer, and an input vector from Input Buffer. Then, it executes the training algorithm and updates Parameter Buffer with the new parameters (i.e., βi and Pi).

The packet format is shown in the fourth row of Fig. 10. An input packet of this instruction is just a trigger to perform training. An output packet of this instruction embeds an evaluation result (denoted as Success) of the inequality described in Section 4.2.3 (1/0 means satisfied/not satisfied).

4.3.5 do_prediction
This instruction executes prediction computations with Predict Module. Predict Module reads the latest output weight if it is updated, and an input vector in the same way as Train Module. Predict Module then executes the prediction algorithm and outputs an anomaly score of the input vector.

The packet format is shown in the last row of Fig. 10. An input packet of this instruction is also just a trigger for prediction. An output packet of this instruction embeds an output anomaly score (denoted as Score) computed by ONLAD Core.

SECTION 5Evaluations of Anomaly Detection Capability
In this section, the anomaly detection capability of ONLAD is evaluated in comparison with other models. A common server machine (OS: Ubuntu 18.04, CPU: Intel Core i7 6700 3.4 GHz, GPU: Nvidia GTX 1070 8 GB, DRAM: DDR4 16 GB, and Storage: SSD 512 GB) is used as the experimental machine in this section and Section 6.

5.1 Experimental Setup
ONLAD is compared with the following models: (1) FPELM-AE, (2) NN-AE, and (3) DNN-AE. FPELM-AE is an FP-ELM-based autoencoder. This model is used to quantitatively evaluate the effect of disabling the L2 regularization trick in ONLAD. NN-AE is a 3-layer BP-NN-based autoencoder, and DNN-AE is a BP-NN-based deep autoencoder consisting of five layers. These models are used to compare OS-ELM-based autoencoders (i.e., FPELM-AE and ONLAD) with BP-NN-based ones. All the models, including ONLAD, were implemented with TensorFlow v1.13.1 [31].

For a comprehensive evaluation, two testbeds: (1) Offline Testbed and (2) Online Testbed are conducted. Offline Testbed simulates an environment where all training and test data are available in advance and no concept drift occurs. This is a standard experimental setup to evaluate semi-supervised anomaly detection models. The purpose of Offline Testbed is to measure the generalization capability of ONLAD in the context of anomaly detection. This testbed is not used to evaluate the proposed forgetting mechanism (i.e., αi is always fixed to 1), since no concept drift occurs in this testbed. On the other hand, Online Testbed simulates an environment where at first only a small part of a dataset is given and the rest arrives as time goes by. Online Testbed assumes that concept drift occurs. The purpose of this testbed is to evaluate the robustness of the proposed forgetting mechanism against concept drift in comparison with the other models.

Several public classification datasets listed in Table 2 are used to construct Offline Testbed and Online Testbed. All data samples are normalized within [0, 1] by using min-max normalization. Hyperparameters of each model are explored within the ranges detailed in Table 3.3

TABLE 2 Datasets

TABLE 3 Search Ranges of Hyperparameters

5.2 Experimental Method
This section describes the experimental methods of Offline Testbed and Online Testbed, respectively.

Algorithm 2. Offline Testbed
Xtrain≡[X(0)train,X(1)train,…,X(c−1)train]

Xtest≡[X(0)test,X(1)test,…,X(c−1)test]

average_auc←0

for i←0 to c−1 do

Xnormal_train←X(i)train

Xnormal_test←X(i)test

Xanomaly←X(j≠i)test

num_anomalies←len(Xnormal_test)×0.1

Xanomaly←sample(Xanomaly,num_anomalies)

model.train(Xnormal_train)

scores←model.predict(concat([Xnormal_test,Xanomaly]))

average_auc←average_auc+calc_auc(scores)

model.reset()

end for

average_auc←average_aucc

Algorithm 2 shows the experimental method of Offline Testbed. In this testbed, a dataset is divided into training data Xtrain (80 percent) and test data Xtest (20 percent), respectively. Suppose we have a dataset that consists of c classes in total; training data of class i are used as normal data for training (denoted as Xnormal_train) and test data of class i are as normal data for testing (denoted as Xnormal_test). Test data of class j≠i are used as anomaly data (denoted as Xanomaly). The number of samples in Xanomaly is limited up to 10 percent of that of Xnormal_test to simulate a practical situation; anomaly data are much rarer than normal data in most cases. A model is trained with Xnormal_train (NN-AE and DNN-AE are trained with batch size = B for E epochs). Once the training procedure is finished, the model is evaluated with a test set that mixes Xnormal_test and Xanomaly, then an AUC (Area Under Curve) score is calculated. AUC is one of the most widely used metrics for evaluating the accuracy of anomaly detection models independently of particular anomaly score thresholds. The above process is repeated until i<c, then all the c AUC scores are averaged. The output score is recorded as a result of a single trial; the final AUC scores reported in Table 4 are averages over 50 trials. 10-fold cross-validation is conducted for hyperparameter tuning.

TABLE 4 AUC Scores on Offline Testbed

Algorithm 3 shows the experimental method of Online Testbed. In this testbed, a dataset is divided into initial data Xinit (10 percent), test data Xtest (45 percent), and validation data Xvalid (45 percent). Xinit represents for data samples that exit in the begining. Xtest and Xvalid represent for data samples that sequentially arrive as time goes by. Xtest is used to measure the final AUC scores, while Xvalid is only for hyperparameter tuning. Both are further divided into normal data Xnormal (90 percent) and anomaly data Xanomaly (10 percent). In the first step, a list (denoted as indices) consisting of integers 0∼c−1 is constructed and randomly shuffled. The output indicates the normal class of each concept; e.g., supposing that indices=[2,0,1], the normal class of the 0/1/2th concept is 2/0/1. The ith concept Xconcept[i] mixes normal data of class indices[i] and anomaly data of class j≠indices[i]. The number of anomaly samples per one concept is limited to 10 percent of that of normal samples. A model is trained with initial data of the first normal class X(indices[0])init(NN-AE and DNN-AE are trained with batch size = B for E epochs). Then, the model computes an anomaly score for each data sample continuously given from Xconcept[0]∼Xconcept[c−1]. Every time an anomaly score is computed, the model is trained with the data sample (all the models, including NN-AE and DNN-AE, are trained with batch size = 1 to sequentially follow the transition of the normal class). After all the data samples are fed to the model, an AUC score is calculated with the anomaly scores. This AUC score is recorded as a result of a single trial; the final AUC scores reported in Table 5 are averages over 50 trials. Hyperparameter tuning is conducted with the same algorithm for 10 trials by replacing Xtest with Xvalid in Algorithm 3.

Algorithm 3. Online Testbed
Xinit≡[X(0)init,X(1)init,…,X(c−1)init]

Xtest≡[X(0)test,X(1)test,…,X(c−1)test]

Xnormal,Xanomaly←split(Xtest,“9 : 1”)

indices←[0,1,…,c−1]

shuffle(indices)

Xconcept←[]

for i← to c−1 do

concept←[X(indices[i])normal]

num_anomalies←len(X(indices[i])normal)×0.1

concept.append(sample(X(j≠indices[i])anomaly,num_anomalies))

Xconcept.append(shuffle(concat(concept)))

end for

model.train(X(indices[0])init)

scores←[]

for i← to c−1 do

for all x in Xconcept[i] do

score←model.predict(x)

scores.append(score)

model.train(x)

end for

end for

auc←calc_auc(scores)

TABLE 5 AUC Scores on Online Testbed

5.3 Experimental Results
The experimental results for Offline Testbed are shown in Table 4. The hyperparameter settings are also listed in Table 6. Here, NN-AE and DNN-AE achieve slightly higher AUC scores than those of ONLAD by approximately 0.01∼0.03 point on almost all the datasets. This result implies that BP-NN-based autoencoders have slightly higher generalization capability than that of OS-ELM-based ones in the context of anomaly detection. However, NN-AE and DNN-AE have to be iteratively trained for some epochs in order to achieve their best performance (here, they were trained for 5∼20 epochs). In contrast, ONLAD always finds the optimal output weight in only one epoch. Also, ONLAD achieves its best AUC scores with an equal or smaller size compared with NN-AE and DNN-AE for all the datasets, which helps to reduce the computational cost and save on hardware resources required to implement ONLAD Core. In addition, the differences between the AUC scores of ONLAD and FPELM-AE are within 0.001∼0.004 point; ONLAD keeps favorable generalization performance even when the L2 regularization trick is disabled. In summary, ONLAD has comparable generalization capability to that of the BP-NN-based models in much smaller training epochs with an equal or smaller model size.

TABLE 6 Hyperparameter Settings on Offline Testbed

The experimental results for Online Testbed are shown in Table 5. The hyperparameter settings are also listed in Table 7. Here, another model, named ONLAD-NF (ONLAD-No-Forgetting-mechanism) is introduced in order to examine the effectiveness of the proposed forgetting mechanism. ONLAD-NF is the special case of ONLAD, where the forgetting mechanism is disabled by setting αi to 1. The hyperparameter settings of ONLAD-NF are the same as those of ONLAD, except for αi. As shown in the table, ONLAD-NF suffers from significantly lower AUC scores than ONLAD. The reason is quite obvious; ONLAD-NF does not have any functions to forget past learned data, therefore it gradually becomes more difficult to detect anomalies every time concept drift happens. NN-AE and DNN-AE, on the other hand, achieve much higher AUC scores than ONLAD-NF because BP-NNs have the catastrophic forgetting nature [35], which works as a kind of forgetting mechanism. However, BP-NNs do not have any numerical parameters to analytically control the progress of forgetting, unlike ONLAD. For this reason, ONLAD stably achieves more favorable AUC scores. Additionally, ONLAD and FPELM-AE have similar AUC scores on most of the datasets, as with the results on Offline Testbed. This result shows that the proposed forgetting mechanism is not significantly affected by the L2 regularization trick on these datasets. In summary, ONLAD achieves much higher AUC scores than those of NN-AE and DNN-AE by approximately 0.10∼0.18 point on three datasets out of the five ones. It also achieves comparable AUC scores to those of the BP-NN-based models on the other two datasets.

TABLE 7 Hyperparameter Settings on Online Testbed

SECTION 6Evaluations of Performance and Cost
In this section, ONLAD Core is evaluated in terms of latency, FPGA resource utilization, and power consumption in comparison with software implementations.

6.1 Experimental Setup
ONLAD Core is evaluated in comparison with the following software implementations: (1) NN-AE-CPU, (2) DNN-AE-CPU, (3) NN-AE-GPU, (4) DNN-AE-GPU, (5) FPELM-AE-CPU, and (6) FPELM-AE-GPU. {*}-CPU is executed only with a CPU, while {*}-GPU is executed with a GPU in cooperation with a CPU. All of these implementations are developed with Tensorflow v1.13.1. Here, Tensorflow v1.13.1 is built with AVX2 (Advanced Vector eXtensions 2) instructions and -O3 option to accelerate CPU computations. It is also built with CUDA [36] v10.0 to enable GPGPU execution.

The hyperparameter settings of the above implementations are detailed in Table 8. p(x), αi, λ and E have been omitted from the table because these parameters are unrelated to any of the evaluation metrics (i.e., latency, FPGA resource utilization, and power consumption). The batch size (i.e., B) of NN-AE-{*} and DNN-AE-{*} is fixed to 1, as with ONLAD Core and FPELM-AE-{*} in order to conduct fair comparisons of latency and power consumption.

TABLE 8 Hyperparameter Settings in Section 6

6.2 Latency
6.2.1 Training/Prediction Latency
Here, we refer to “training latency” as the elapsed time from when a model receives an input sample until the training algorithm is computed. “Prediction latency” is the elapsed time from when a model receives an input data sample until an anomaly score is calculated.

Figs. 11 and 12 show the training and prediction latency times of each implementation versus the numbers of input and hidden nodes (all the reported times are averages over 50,000 trials). To measure practical latency times, the exploration range of the number of input nodes is set to {128, 256, 512, 1,024}, while that of the number of hidden nodes is set to {16, 32, 64} on the basis of the hyperparameter settings of ONLAD in Section 5.


Fig. 11.
Comparison of training latency.

Show All


Fig. 12.
Comparison of prediction latency.

Show All

As shown in the figures, the latency times of the software implementations remain almost constant as the number of input nodes increases. This outcome shows that most of their execution times are occupied with software overheads to invoke training and prediction tasks.7 The GPU-based implementations especially suffer from high latency times because of the communication cost between a GPU and a CPU in addition to the software overheads. In contrast, ONLAD Core is free from these overheads. Consequently, ONLAD Core achieves 1.95x, 2.45x, 2.56x, 3.38x, 4.51x, and 6.58x speedups on average over NN-AE-CPU, DNN-AE-CPU, FPELM-AE-CPU, NN-AE-GPU, DNN-AE-GPU, and FPELM-AE-GPU in terms of training latency, and 2.29x, 2.37x, 2.36x, 4.38x, 4.73x, and 4.57x speedups on average over them in terms of prediction latency. ONLAD Core can perform fast sequential learning and prediction to follow concept drift approximately in less than one millisecond.

However, please note that ONLAD Core may become slower than the others when there are many input nodes since the computational cost of Train/Predict Module is proportional to the number of input nodes, as shown in Equations (21) and (23). Hence, ONLAD Core has difficulty achieving speedups beyond 1.0x over the software implementations when there are thousands of input nodes.

Moreover, the computational cost of Train Module is proportional to the square of the number of hidden nodes, too. However, contrary to expectations, Fig. 14 shows that the latency times are almost proportional to the number of hidden nodes. This is because (3n+1)N~≫4N~2 holds as long as n≫N~ in Equation (21). In other words, the computational cost of Train Module stays almost proportional to the number of hidden nodes as long as the number of input nodes is much greater than that of hidden nodes. The practicality of this condition is empirically demonstrated; the best hyperparameter settings of ONLAD Core satisfy n≫N~ as shown in Tables 6 and 7. Hence, in practical situations, the computational cost of ONLAD Core does not excessively increase even when the number of hidden nodes is increased.

6.2.2 Computational Cost of Proposed Forgetting Mechanism
Here, the proposed forgetting mechanism of ONLAD Core and the baseline algorithm (i.e., FP-ELM) are compared in terms of computational cost. Since the forgetting operation of ONLAD Core or FP-ELM is unified into the training algorithm, we use training latency times to compare them. Also, to make a fair comparison of their computational costs, we compare a CPU implementation of ONLAD Core and FPELM-AE-CPU, both of which are implemented with the same library (i.e., Tensorflow).

Fig. 13 shows the experimental results, where the exploration ranges of input and hidden nodes are set to 8x larger than those of Figs. 11 and 12, in order to increase the ratio of computation time of the models and make a clear comparison of their computational costs. Consequently, our forgetting mechanism is faster than FPELM-AE-CPU by 3.21x on average. The computational cost of our forgetting mechanism is O(N~2) as shown in Equation (21), however, that of FP-ELM is O(N~3) since the matrix size of the matrix inversion of FP-ELM is N~×N~; the gap of their computation times gradually widens as the number of hidden nodes increases.


Fig. 13.
Comparison of training latency of proposed forgetting mechanism and FP-ELM.

Show All


Fig. 14.
Relationship between training latency of ONLAD Core and hidden nodes.

Show All

Fig. 15. - 
Training latency versus input node size (hidden nodes = 512).
Fig. 15.
Training latency versus input node size (hidden nodes = 512).

Show All

Fig. 16. - 
Prediction latency versus input node size (hidden nodes = 512).
Fig. 16.
Prediction latency versus input node size (hidden nodes = 512).

Show All

6.3 FPGA Resource Utilization
This section evaluates FPGA resource utilization of ONLAD Core by varying the numbers of input and hidden nodes. The exploration range of the number of input nodes is chosen to be {128, 256, 512, 1,024}, and that of the number of hidden nodes is to {16, 32, 64} on the basis of the results in the previous section. For ease of analysis, we use pre-synthesis resource utilization reports produced by Vivado HLS as experimental results.

Table 9 shows the experimental results. The DSP utilization remains almost constant even as the numbers of input and hidden nodes increase. This is a reasonable outcome since the DSP slices are consumed only for Train Module and Predict Module, and both of them are designed to use a specific number of arithmetic units regardless of the number of input and hidden nodes, as mentioned in Sections 4.2.3 and 4.2.4.

TABLE 9 FPGA Resource Utilization of ONLAD Core (Pre-Synthesis Results)

However, ONLAD Core consumes more BRAM instances as the model size increases. Sonlad below denotes the total number of matrix elements of the entire ONLAD Core.
Sonlad=Sparameter+Sinput+Strain+Spredict=5N~2+(5n+4)N~+2n+1.(27)
View SourceEquation (27) shows that the utilization of the BRAM instances of ONLAD Core is linearly increased as the number of input nodes n increases. The experimental results shown in Table 9 are consistent with Equation (27); the BRAM utilization is proportional to the number of input nodes.

Equation (27) also shows that the utilization of the BRAM instances is proportional to the square of the number of hidden nodes N~2, too. However, the BRAM utilization ratios of ONLAD Core are almost proportional to the number of hidden nodes, as shown in Fig. 17. The same logic as in the previous section can explain this outcome; (5n+4)N~≫5N~2 holds as long as n≫N~ in Equation (27). The practicality of the condition n≫N~ is also as described in the previous section. Hence, in practical situations, the BRAM utilization of ONLAD Core is suppressed and does not excessively increase even if the number of hidden nodes increases. Consequently, except for the largest setting (n,N~)=(1,024,64), all the utilization rates of ONLAD Core are under the limit.

Fig. 17. - 
Relationship between BRAM utilization and hidden nodes.
Fig. 17.
Relationship between BRAM utilization and hidden nodes.

Show All

6.4 Power Consumption
This section evaluates the runtime power consumption of our board-level implementation in comparison with the other software implementations. We use an ordinary watt-hour meter to measure the power consumption of PYNQ-Z1 board. For the software implementations, s-tui and nvidia-smi are used. s-tui [37] is an open-source CPU monitoring tool; we use it to measure the power consumption of the CPU (i.e., Intel Core i7 6700 3.4 GHz) equipped in the experimental machine. On the other hand, nvidia-smi is a GPU monitoring utility provided by Nvidia. We use it to measure the power consumption of the GPU (i.e., Nvidia GTX 1070 8 GB) equipped in the experimental machine.

The input and hidden nodes of all the implementations are commonly set to 512 and 64. This setting has been confirmed to consume the largest amount of resources in Table 9. The resource utilization report of ONLAD Core is shown in Table 10.

TABLE 10 FPGA Resource Utilization of ONLAD Core (Post-Synthesis Result)

Fig. 18 shows the power consumption of each implementation when training computations are continuously executed. As shown in the figure, our implementation consumes 3.1 W, 5.0x∼25.4x lower than the others. The reported power consumption of our implementation includes not only that of ONLAD Core, but also that of other components such as a dual-core ARM CPU. Hence, the power consumption of ONLAD Core itself is even lower than 3.1 W.


Fig. 18.
Comparison of power consumption.

Show All

SECTION 7Related Work
7.1 On-Device Learning
Data play an important role in machine learning, although sometimes they can be privacy-sensitive. Here, on-device prediction/learning is a way to ensure data privacy because it does not require user data transfers with external server machines. Ravi et al. proposed ProjectionNet [38] to make existing BP-NN-based models smaller and reduce the memory they take up on user devices without significantly degrading accuracy. This is done by leveraging an Locality Sensitive Hashing (LSH) based projection method and a distillation training framework. Konečný et al. proposed a federated learning framework [39], which utilizes user devices as computational nodes to train a global model. In this framework, user devices are supposed to perform training only with their local data; then, the updated weights are aggregated into the global model. Zhu et al. and Park et al. studied federated learning approaches with edge devices on wireless sensor networks [40], [41]. They explored their essential building blocks and pointed out underlying challenges. Our approach shares a common idea that edge devices themselves perform training, although the aim is not to create a global model; our work tries to create a locally personalized model for the target edge device.

7.2 Anomaly Detection With OS-ELM
Since sequential learning approaches are capable of learning input data online, they have been utilized for anomaly detection where real-time adaptation and prediction are often required. OS-ELM is no exception; several studies have been reported on anomaly detection using OS-ELM. Nizar et al. proposed an OS-ELM-based irregular behavior detection system of electricity customers to prevent non-technical losses such as power theft and illegal connections [42]. They compared their system with SVM based ones and showed its superiority. Singh et al. proposed an OS-ELM-based network traffic Intrusion Detection System (IDS). They showed that the system can perform training on a huge amount of traffic data even with limited memory space [43]. Bosman et al. proposed a decentralized anomaly detection system for wireless sensor networks [44]. On the other hand, we utilize OS-ELM for semi-supervised anomaly detection in conjunction with an autoencoder. As far as we know, we propose the combination as the first work.

7.3 OS-ELM Variants With Forgetting Mechanisms
Over the past several years, several OS-ELM variants with forgetting mechanisms have been proposed. Zhao et al. were the first to study a forgetting mechanism for OS-ELM, called FOS-ELM [45]. FOS-ELM takes a sliding-window approach, where the latest s training chunks are taken into account (s is a fixed parameter of window size). On the other hand, λDFFOS-ELM [46] and FP-ELM [26] introduce variable forgetting factors to forget old training chunks gradually. They adaptively update the forgetting factors according to the information in arriving input data or output error values. Our approach is based on FP-ELM, though it is modified to provide the forgetting mechanism with a tiny additional computational cost to the original algorithm of OS-ELM.

7.4 Hardware Implementations of OS-ELM
Several papers on hardware implementations of ELM [47], [48], [49], [50] have been reported since 2012. However, implementations of OS-ELM have just started to be reported. Tsukada et al. provided a theoretical analysis for hardware implementations of OS-ELM to significantly reduce the computational cost [18]. Villora et al. and Safaei et al. proposed fast and efficient FPGA-based implementations of OS-ELM for embedded systems [51], [52]. In this paper, we propose an IP core that implements the proposed OS-ELM-based semi-supervised anomaly detection approach. This IP core can be implemented on edge devices of limited resources and works at low power consumption.

7.5 Neural Network Based Hardware Implementations for Anomaly Detection
In this section, we compare several NN-based anomaly detection hardware implementations in Table 11. Akin et al. proposed an FPGA based condition monitoring system, whose prediction time is less than 2 msec, for induction motors [53]. The proposed system employs a supervised anomaly detection approach using a 3-layer binary-classification model; it requires both anomaly data and normal data for training. Wess et al. proposed an electrocardiogram anomaly detection approach based on FPGA [54]. The proposed system consists of (1) feature extraction, (2) dimensional reduction, and (3) classification, in which (3) is implemented as a dedicated circuit on FPGA. They reported that the prediction latency is approximately less than 100 cycles, although their approach is also based on a classification model as well as [53]. In contrast to the above implementations, ONLAD Core adopts a semi-supervised approach, where only normal data are required for training.

TABLE 11 Comparison of NN-Based Hardware Implementations for Anomaly Detection

Moss et al. proposed an FPGA based anomaly detector for radio frequency signals [55]. The proposed IP core realizes semi-supervised anomaly detection using a BP-NN based autoencoder, which is a similar approach to our work. Also, its prediction latency is as fast as 105 nsec. However, the model size (i.e., weight parameters) is 100x smaller than ONLAD Core, and the FPGA platform is much larger than ours. Besides, their IP core does not support training computations. Alrawashdeh et al. proposed a Deep Belief Network (DBN) based IP core that supports training as with ONLAD Core for anomaly detection [56]. They proposed a cost-efficient training model for the contrastive divergence algorithm of DBN and reported that the performance of the IP core achieves 37 Gops/W. However, the model adopts a classification based approach as with [53] and [54]. On the other hand, ONLAD Core supports training, and at the same time it adopts a semi-supervised anomaly detection approach, which makes it more applicable to a wide range of real-world applications.

7.6 Design Tools for Hardware Implementation of Neural Networks
The PYNQ-Z1 board used in this work provides the PYNQ library [57] which allows the developers to design CPU-FPGA co-architecture with Python codes, although it is not specialized for implementing neural networks. fpgaConvNet [58] is an automated design framework for Convolutional Neural Network (CNN) based classification models on FPGA platforms. This framework adopts a synchronous dataflow model where the design space of performance and cost is explored, while taking into account platform-specific constraints. DnnWeaver [59] is a design tool that generates synthesizable DNN accelerators from high-level configurations in Caffe. The DnnWeaver compiler tiles, schedules, and batches DNN operations to maximize data reuse and utilize target FPGA's memory. Zhao et al. proposed a high-level design framework for Binarized Neural Networks (BNNs) [60]. Since the main arithmetics of BNNs are simple bitwise logic operations instead of costly floating-point operations, the computational cost and FPGA resources required to implement the accelerator can be significantly reduced compared with conventional CNNs. GUINNESS [61] is a GUI based design tool for implementing BNNs on Xilinx SoC platforms. In this tool, the designers do not need to write any RTL codes or scripts, which enables software designers to develop prototypes of BNN-based accelerators without knowledge of hardware.

SECTION 8Conclusion
8.1 Summary
In this work, we proposed ONLAD which realizes fast sequential learning semi-supervised anomaly detection by constructing an autoencoder with OS-ELM. We showed that the computational cost of OS-ELM is significantly reduced when the batch size is fixed to 1, which contributes to speedup of ONLAD. Also, we proposed a computationally lightweight forgetting mechanism for OS-ELM, based on FP-ELM. It enables ONLAD to follow concept drift at a low computational cost. In addition, we proposed ONLAD Core in order to realize on-device execution of ONLAD on resource-limited edge devices at low power consumption. Since ONLAD Core does not need to offload training computations to external remote server machines, it enables standalone execution where no data transfers to server machines are required.

Experimental results using public datasets showed that ONLAD has comparable generalization capability to that of BP-NN-based models in the context of anomaly detection. We also confirmed that ONLAD has favorable anomaly detection capability especially in an environment that simulates concept drift.

Evaluations of ONLAD Core confirmed that it can perform training and prediction computations faster than software implementations of BP-NNs and FP-ELM by 1.95x∼6.58x and 2.29x∼4.73x on average. They also comfirmed that the proposed forgetting mechanism is faster than FP-ELM by 3.21x on average. In addition, our evaluations showed that ONLAD Core can be implemented on PYNQ-Z1 board in practical model sizes. We demonstrated that the runtime power consumption of PYNQ-Z1 board that implements ONLAD Core is 5.0x∼25.4x lower in comparison with the other software implementations when training computations are continuously executed.

8.2 Future Directions
BP-NNs are known to achieve higher generalization performance to some extent by stacking more layers. Although the original OS-ELM algorithm is limited to have only one hidden layer, Multi-Layer Online Sequential Extreme Learning Machine (ML-OSELM) [62] proposed by Mirza et al. provides a multi-layer framework for OS-ELM. According to [62], ML-OSELM outperforms OS-ELM on well-known open classification datasets by 0.15∼2.58% in terms of test accuracy. Thus, anomaly detection capability of ONLAD can be further improved by replacing OS-ELM in ONLAD with ML-OSELM. We plan to work with the multi-layer version of ONLAD and ONLAD Core.

In real world, there are some systems that have multiple action modes such as air conditioners, robot arms, and gas turbines. In the context of anomaly detection, such systems are often formulated as mixture models which consist of multiple sub-distributions of normal data. Recently, a mixture model framework that utilizes multiple OS-ELM instances was proposed in [63]. We plan to apply this framework to ONLAD and ONLAD Core.