–Google deployed several TPU generations since teaching us lessons that changed our views semiconductor technology advances unequally compiler compatibility trumps binary compatibility especially for VLIW domain specific architectures DSA target total cost of ownership vs initial cost support multi tenancy deep neural networks DNN grow .X annually DNN advances evolve workloads some inference tasks require floating point inference DSAs need air cooling apps limit latency not batch size and backwards ML compatibility helps deploy DNNs quickly. These lessons molded TPUvi an inference DSA deployed since . I. TPUv addresses the harder task of training . First training parallelization is harder. Each inference is independent so a simple cluster of servers with DSA chips can scale out. A training run iterates over millions of examples coordinating across parallel resources because it must produce a single consistent set of weights for the model. Second computation is harder. Backpropagation requires derivatives for every computation in a model. It includes higher precision activation functions some of which are transcendental and multiplication by transposed weight matrices. Third training needs more memory. Weight updates access intermediate values from forward and back propagation vastly upping storage requirements temporary storage can be X weight storage. Fourth it must be more programmable. Training algorithms and models are continually changing so a DSA restricted to current best practice algorithms during design could rapidly become obsolete. Finally short integers can work for inference but sufficiently capturing the sum of many small weight updates during training normally needs floating point arithmetic. The TPUv block diagram can be transformed into TPUv via a sequence of changes showing the more general needs of training over inference. The split on chip SRAM made sense when buffering data between sequential fixed function units of TPUv but undivided on chip memory is better for training. The read only weights for inference allow optimizations that don’t work for training which writes weights. The first change is to merge Activation Storage and the Accumulators into a single Vector Memory see Figure . In TPUv a more programmable Vector Unit replaced the fixed function datapath of the Activation Pipeline of TPUv containing pooling and activation units . Bfloat is a better match to DNNs than I fp so the MXU was changed to become the first hardware to support it with K MAC units ½ the side of the systolic array so ¼ of the size . The MXU was then attached to the Vector Unit as a matrix co processor. Read only weights make no sense for training—whose goal is setting them—and significant buffer space is needed for temporary per step variables. DRAM backs Vector Memory so that the pair form a compiler controlled memory hierarchy. In package HBM DRAM increases bandwidth X over DDR keeping the TPUv core well utilized. TPUv fetches its own bit VLIW instructions from a local memory rather than the host CPU supplying them. Introduction to TPUs Commercial Domain Specific Architectures DSAs for Deep Neural Networks DNNs are well established . This paper revisits and expands on that work with a fourth generation DSA. It shows the evolution of an architecture family as production experience has informed new designs beyond the familiar issues of sluggish CPUs and a diminishing Moore’s Law . Figure . TPUv block diagram left vs TPUv/v. Figure shows the block diagrams of the three TPUs deployed in Google datacenters starting in and Table gives their key features. Let’s review the first three TPUs. TPUv Google’s first DNN DSA handles inference also called serving . The high bandwidth loop red to the left of Figure shows the main data and computation path that crunches DNN layers. The green Activation Storage and Accumulators SRAM blocks buffer the blue computation blocks of the Matrix Multiply Unit MXU and the Activation Pipeline. The systolic array MXU has K bit integer Multiply Accumulate MAC units. DDR DRAM feeds the loop at much lower bandwidth with model parameters also called weights and TPUv connects to the host CPU over PCIe for exchanging model inputs and outputs at even lower bandwidth. The host CPU also sends TPUv instructions over PCIe. Compared to contemporary GPUs and CPUs its performance/TDP perf/Watt on production workloads was X higher . This paper is part of the Industry Track of ISCA s program. ¥ % Feature TPUv TPUv TPUv TPUvi NVIDIA T Peak TFLOPS / Chip b int bf bf bf/b int i fp / b int First deployed GA date Q Q Q Q Q DNN Target Inference only Training Inf. Training Inf. Inference only Inference only Network links x Gbits/s / Chip x x x Max chips / supercomputer Chip Clock Rate MHz / Turbo Idle Power Watts Chip TDP Watts Chip / System / / / / / Die Size mm Transistors B Chip Technology nm nm nm nm nm Memory size on /off chip MB / GB MB / GB MB / GB MB / GB MB / GB Memory GB/s / Chip if ECC is disabled MXU Size / Core x x x x x Cores / Chip Chips / CPUHost Table . Key characteristics of DSAs. The underlines show changes over the prior TPU generation from left to right. System TDP includes power for the DSA memory system plus its share of the server host power e.g. add host TDP/ for DSAs per host. ● Document the unequal improvement in logic wires SRAM and DRAM from nm to nm—including an update of Horowitz’s operation energy table from nm to nm—and show how these changes led to four systolic floating point matrix units for TPUvi in versus one systolic integer matrix unit for TPUv in . ● Explain the difference between designing for performance per TCO vs per CapEx leading to HBM and a low TDP for TPUvi and show how TPUv’s headroom led to application scaleup after the paper . ● Explain backwards ML compatibility including why inference can need floating point and how it spurred the TPUvi and TPUv designs § . Backwards ML compatible training also tailors DNNs to TPUvi § . ● Measure production inference applications to show that DSAs normally run multiple DNNs concurrently requiring Google inference DSAs to support multi tenancy. ● Discuss how DNN advances change the production inference workload. The workload keeps MLP and CNN from but adds BERT and RNN succeeds LSTM. ● Document the growth of production DNNs in memory size and computation by ~.x annually since which encourages designing DSAs with headroom. ● Show that Google’s TCO and TDP for DNN DSAs are strongly correlated R . likely due to the end of Dennard scaling. TDP offers a good proxy for DSA TCO. ● Document that the SLO limit is P time for inference applications list typical batch sizes and show how large on chip SRAM helps P performance. ● Explain why TPUvi architects chose compiler compatibility over binary compatibility for its VLIW ISA. ● Describe Google’s latest inference accelerator in production since March and evaluate its performance/ Training needs large scale so another enhancement is to add a custom chip to chip interconnect fabric ICI enabling TPUv supercomputers of up to chips . Unlike TPUv TPUv has two TensorCores per chip. Global wires on a chip don’t scale with shrinking feature size see lesson below so their relative delay increases. Two smaller cores per chip prevent the excessive latencies of a single large full chip core. We stopped at two because we believe it is easier to compile programs efficiently for two brawny cores than for numerous wimpy cores. TPUv is a “midlife kicker ” a mild redesign of TPUv in the same technology that has X the number of MXUs and HBM capacity and increases the clock rate memory bandwidth and ICI bandwidth by .X. A TPUv supercomputer also scales up to chips. TPUv matches the contemporary Volta GPU when both use bit floating point bfloat vs I fp . However Volta needs to use I fp when training Google production workloads making TPUv ~X faster. Several applications scale to chips at %–% of perfect linear speedup . This paper introduces TPUvi—i stands for inference— forged by the hard earned lessons from building and deploying TPUs over years. Section § distills ten of these insights. Had we known about them in with more time we would have designed these TPUs differently especially TPUv. § shows how these lessons shaped TPUvi. To avoid repetition § only lists changes from TPUv. § and § compare TPUvi performance/TDP to TPUv for production apps and to NVIDIA T for MLPerf Inference .–.. § describes industry inference DSAs. They often run afoul of the lessons so none meet Google’s needs. A discussion and conclusion end the paper. But first are highlights of this paper’s contributions TDP vs. TPUv and NVIDIA’s T inference GPU using production apps and MLPerf Inference benchmarks . .. Operation Int Int BFloat I FP I FP Int Int ✕ BFloat I FP I FP KB SRAM SRAM KB SRAM MB SRAM GeoMean DRAM DDR/ HBM GDDR The first three lessons apply to any DSA and perhaps even to GPUs and CPUs. For typographic clarity the lessons are tagged with circled numbers e.g. ②. ① Logic wires SRAM DRAM improve unequally Horowitz’s insights on operation energy inspired many DSA designs . Table updates it to nm showing an average gain of .X from nm but the change is uneven ● SRAM access improved only .X–.X in part because SRAM density is scaling slower than in the past. Comparing nm to nm SRAM capacity per mm is ~X less dense than ideal scaling would suggest . ● DRAM access improved .X due to packaging innovations. High Bandwidth Memory HBM places short stacks of DRAM dies close to DSAs over wide buses. ● While not in Table energy per unit length of wire improved X . Poor wire delay scaling led TPUv/v to use smaller cores from larger core on TPUv. Logic improves much faster than wires and SRAM so logic is relatively “free.” HBM is more energy efficient than GDDR or DDR DRAM. HBM also has the lowest cost per GB/s of bandwidth. Picojoules per Operation nm nm / . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Circa nm Circa nm . ② Leverage prior compiler optimizations Since the s the fortunes of a new architecture have been bound to the quality of its compilers. Indeed compiler problems likely sank the Itanium’s VLIW architecture yet many DSAs rely on VLIW see § including TPUs. Architects wish for great compilers to be developed on simulators yet much of the progress occurs after hardware is available since compiler writers can measure actual time taken by code. Thus reaching an architecture’s full potential quickly is much easier if it can leverage prior compiler optimizations rather than starting from scratch. DSAs are not exceptions to this rule they are paragons. TPUs rely on the XLA Accelerated Linear Algebra compiler which started in and NVIDIA GPUs have used the CUDA compiler since . Figure shows the gains over months from MLPerf Training benchmark version . to .. CUDA compilation improved the GPU by .X. Perhaps because it is less mature XLA raised the TPU by .X. In contrast C compilers improve generalpurpose code %–% annually . Good compilers are critical to a DSA’s success see § . The XLA compiler developed for TPUv was enhanced for TPUv and TPUv. Its intermediate language has changed little since TPUv. Table . Energy per Operation nm vs nm. Memory is pJ per bit access. Figure . DSA gains per chip for MLPerf Training . to . over months for the same compilers. The unverified TPUv MLPerf . scores for Mask R CNN and Transformer are from all other results are from . . Ten Lessons Learned Since We list the most important lessons even if depending on their experience some readers find them unsurprising. We note however that architects of recent commercial ML accelerators ignored some of these lessons see § . ③ Design for performance per TCO vs per CapEx Capital Expense CapEx is the price for an item . Operation Expense OpEx is the cost of operation including electricity consumed and power provisioning. Standard accounting amortizes computer CapEx over years so for years TCO CapEx ✕ OpEx. Google and most companies care more about performance/TCO of production apps perf/TCO than raw performance or Horowitz’s MB SRAM power is a single bank SRAM. Most would use multiple banks which explains the . reduction in MB SRAM going from to . It is omitted from the geomean. pJ for DDR/ DRAM is only the I/O . HBM and GDDR also list only the I/O energy . performance/CapEx perf/CapEx of benchmarks . While TCO is the main variable that Google optimizes for during product design CapEx still influences some business decisions that are outside the scope of this paper. Our experience is that CPUs and GPUs typically aim to maximize performance of benchmarks versus purchase price at the time of their announcement. Since price depends on volume and contract negotiations some architects use performance/mm as a proxy for perf/CapEx . Alas performance/mm can look good even if it’s bad for perf/TCO e.g. increasing the clock rate of the Alibaba HanGuang .x . GHz costs .x more power dropping counters that can help tune DNN performance or speeding up memory by turning off error correction ECC §.B . Moreover benchmarks are inherently backward looking since they don’t normally factor in growth that can be important for perf/TCO over three years see Table ⑧. In contrast TPUv’s headroom enabled app improvements since publication . Developers maintained SLOs yet increased operations X for MLP and X for CNN ⑧. A DSA should aim for good Perf/TCO over its full lifetime and not only at its birth. Alas TCO is confidential. Figure plots TCO versus system TDP for all TPUs plus the NVIDIA T. Power involves everything in a rack including a top of rack switch. The trendline shows nearly perfectly correlation as the correlation coefficient R is .. R is still . for CPUs GPUs and TPUs §.D . Use TDP if TCO is unavailable. without Dennard scaling using more transistors and die area to build faster processors likely raises both power and cost. The next lessons are focused on DNN DSAs. ④ Support Backwards ML Compatibility Some DNNs have time to market constraints as there can be economic value for timeliness. This perspective led to a principle of backwards ML compatibility. The goal is the same for a new CPU it should get exactly the same results including the same exception behavior and correlated performance in this case for training and serving across TPU generations starting with TPUv . Any new inference TPU would at least need to offer the same numerics to get the same results and the same exception behavior bfloat and I fp would be required. If a new TPU doesn t support the same operations or numeric types then the compiler can’t give a backwards ML compatibility guarantee. Floating point addition is not associative meaning the order of operations can prevent high level operations such as matrix multiply or convolution from giving bit identical results. One can t constrain the compiler to a particular fixed order of operations while still allowing performance optimization. This subtle complication implies using the same compiler that is generating code for all targets in a similar way since the compiler sometimes changes the order of evaluation. Identical order means a new TPU should be similar to prior TPUs from the compiler’s perspective. Performance should correlate if it trains well it should serve well. To deploy immediately developers don t want a change to a DNN that reduces training step time to result in poor inference latency since it could violate the DNN SLO ⑩. Moreover lesson ⑩ shows that backwards ML compatible training can pre tune DNNs to the serving hardware. ⑤ Inference DSAs need air cooling for global scale The W TPUv and W TPUv were air cooled but the W TPUv uses liquid cooling. Liquid cooling requires placing TPUs in several adjacent racks to amortize the cooling infrastructure. That placement restriction is not a problem for training supercomputers which already consist of several adjacent racks. Moreover the downsides to limiting training to a few datacenters that have more space are small since widespread deployment is unnecessary. Not so for user facing inference as low user latency requires a worldwide footprint. Some strategic datacenters are already packed so finding room for several adjacent racks is hard. To ease deployment inference DSAs should be air cooled. Figure . TCO vs system TDP for T and TPUv/v/v/vi. An R of . means R is nearly perfect at . out of .. System TDP is capacity planning power and not average power since capacity is more expensive see §.D for details . Two TCO factors come directly from power the costs of electricity used and of provisioning power—power distribution and cooling—which is twice as much as electricity . Much of the rest is computer CapEx. Chip CapEx is not directly tied to power yet the correlation coefficient R is still .. As Moore’s Law slows and ⑥ Some inference apps need floating point arithmetic Quantization for DNNs aims to retain inference time model quality using integers even though all training is done in Like our TCO ignores financing lumps capacity power with “datacenter infrastructure” and uses year amortization. Datacenter space is amortized over years so its cost is low. Some are air cooled for inference but most are liquid cooled. floating point. Quantized arithmetic grants area and power savings but it can trade those for reduced quality delayed deployment and some apps don’t work well when quantized see Figure and NMT from MLPerf Inference . in § . TPUv required quantization—since it supported only integer arithmetic—which proved a problem for some datacenter applications. Early in TPUv development application developers said a % drop in quality was acceptable but they changed their minds by the time the hardware arrived perhaps because DNN overall quality improved so that % added to a % error was relatively small but % added to a % error was relatively large. This change meant it could take months of extra development to restore the quality score using integers that experts achieved in floating point during training. For example the quality for the top scores for the ImageNet competition improved % from to . This lesson teaches that DSAs may offer quantization but unlike TPUv they should not require it. Also see §.H about quantization aware training. Of the DNNs in Table only RNN was quantized and the primary benefit was halving the memory footprint. From MLPerf D Unet and DLRM have been post training quantized to bits with accuracy loss .% versus FP and the accuracy loss is % for BERT. problematic software updates. Table shows % of our production inference workload needs multi tenancy. Application developers demand fast switching time between models e.g. us which cannot be met by loading weights from the CPU host over the PCIe bus ms so DSAs need local memory. To keep all weights in on chip SRAM we’d need to load MB e.g MLP in us or an external memory bandwidth of GB/s faster than current inference chips § . Moreover Table predicts that DNNs will likely grow. Multi tenancy suggests fast DRAM for DSAs since all weights can’t fit in SRAM. Alas DNN DSA designers often ignore multi tenancy. Indeed multi tenancy is not mentioned in the TPUv paper . It was lucky that the smallest available DDR DRAM held GB allowing TPUv software to add multi tenancy. Name MLP MLP CNN CNN RNN RNN BERT BERT Avg. Max Avg. Number of % Use MultiSize Size Programs / tenancy MB MB StdDev Range Yes ± % % N.A. Yes ±. No % % Yes ± Yes ± % % No Yes ± % % N.A. Yes ±. Table . The average and maximum size includes multi tenancy. Next is the number of DNNs sharing the DSA. Last is Google inference workload mix in July vs February showing % of the inference workload per DNN weighted by the TCO of the TPUv/v/v system. MLP is RankBrain CNN is AlphaZero CNN is an internal model for image classification BERT is DeepRank . MLPerf will likely soon add a multi tenancy requirement. Model Annual Memory Increase Annual FLOPS Increase CNN . . MLP . . CNN . . MLP . . Figure . Quantization error for segmentation . The left image identifies the outline of objects in the right photo. The Int outline is fuzzy around the girl’s head so it includes bystanders and it doesn’t isolate the arm of the person in red on the right. The unreliable outline crops the photo incorrectly. Table . Annual increase in production applications of . The final lessons are about the DNN apps themselves which one would like to know before designing hardware to run them—whether it is a DSA GPU or CPU. ⑧ DNNs grow ~.x/year in memory and compute Unlike benchmarks programmers continuously improve production applications which usually increases memory size and computation requirements. Table tracks the average annual increase in memory size and computation for the four original production inference apps that still run on TPUv/v/v. At a ~.x annual increase our production DNNs grow as fast as Moore’s Law like early PC software. This rate suggests architects should provide headroom so that DSAs can remain useful over their full lifetimes. ⑦ Production inference normally needs multi tenancy Like CPUs DSAs should support multi tenancy. Sharing can lower costs and reduce latency if applications use many models. For example translation DNNs need many language pairs and speech DNNs must handle several dialects. Multi tenancy also supports multiple batch sizes to balance throughput and latency . Another reason is simply good software engineering practices. Examples include trying new features on a fraction of customers or slowly deploying a new release to reduce the chances of ⑨ DNN workloads evolve with DNN breakthroughs Table has the DNNs that are ~% of Google’s inference workload. MLP and CNN from remain popular although some apps switched from MLPs to BERT DNNs % which explains the MLP drop % to % . BERT appeared in yet it’s already % of the workload. To improve quality a transformer encoder plus LSTM decoder RNN and a Wave RNN RNN replaced LSTMs % . This lesson teaches the importance of programmability and flexibility for inference DSAs to track DNN progress. deliver better fleet wide perf/TCO if we could afford to design both inference and training optimized chips. The “ birds with stone” moment came when we realized we could get two chips from a single design by having a single core chip for inference like TPUv and a dual core chip for training like TPUv as long as both chips used the same core scaled versions of the same uncore and were developed by the same team in a unified codebase. We could further improve TCO ③ of the inference chip by reducing interchip communication bandwidth and MXU logic layout density which lowered power consumption and maximum power density and enabled air cooling ⑤. Thus Google deployed the single core TPUvi for inference and the dual core TPUv which scales to chips for training. Google previewed TPUv as part of the MLPerf Training . in July where it was .X faster than TPUv and matched the performance of the NVIDIA Ampere GPU . Compiler compatibility not binary compatibility. Given that TPUv and TPUv shared a bit VLIW instruction bundle length conventional architecture wisdom would be for TPUvi and TPUv to try to maintain backwards binary compatibility. We chose to be compiler compatible instead of binary compatible for a few reasons ● The original argument for VLIW was enabling more hardware resources over time by recompiling the apps with the compiler controlling the new instruction level parallelism which binary compatibility restricts . ● Many engineers built Itanium compilers including some in the XLA team where they learned the drawbacks of binary compatibility for a VLIW compiler and hardware. ● The XLA compiler accepts JAX and PyTorch as well as TensorFlow so TPUs could rely on one compiler versus having an interface that works for many compilers. ● TPU software is maintained and distributed in source code rather than in binary code. XLA divides the compiling task into producing HighLevel Operations HLO that are machine independent and Low Level Operations LLO that are machine dependent. Optimizations at the HLO level apply to all platforms. If a new TPU restricts the needed compiler changes to LLOs e.g. wider VLIW it maintains compiler compatibility. Like NVIDIA GPU/CUDA TPU/XLA illustrates hardware/ software co design in a commercial setting see §.G . Increased on chip SRAM storage with common memory CMEM . The first concern of a DSA after its compiler is the memory system . Limiting memory costs may improve perf/CapEx but could hurt perf/TCO ③. Despite aiming at inference multi tenancy ⑦ the rapid growth of DNNs ⑧ and the superior energy efficiency of HBM ① led to TPUvi to keep using HBM like TPUv. Nevertheless SRAM is x more energy efficient than DRAM Table and there are large data structures that ⑩ Inference SLO limit is P latency not batch size list latency time limits for serving models for production apps. However recent DSA papers have redefined the latency limit in terms of batch size often set at . Table shows the P time SLO for production apps and the MLPerf Inference . benchmarks §.E . It also shows the largest batch size of recent TPUs that meet the SLO. Clearly datacenter applications limit latency not batch size. Future DSAs should take advantage of larger batch sizes. Our production workloads compared to MLPerf average ~X larger batch sizes despite ~X stricter latency constraints. Backwards ML compatibility gives performance portability from training to inference ④ so Google’s internal models are pre tuned. By contrast the MLPerf inference models were trained on GPUs and so are less well tuned for TPUs. Production MLPerf . DNN ms batch DNN ms batch DNN ms batch MLP RNN Resnet MLP RNN SSD CNN BERT GNMT CNN BERT Table . Latency limit in ms and batch size picked for TPUvi. . How the Lessons Shaped TPUvi’s Design Given the importance of leveraging prior compiler optimizations ② and backwards ML compatibility ④—plus the benefits of reusing earlier hardware designs—TPUvi was going to follow TPUv or brawny cores per chip a large systolic MXU array and vector unit per core compiler controlled vector memory and compiler controlled DMA access to HBM. To avoid repetition and to leave room for insights this paper concentrates on the differences from TPUv those interested in more details should see . After TPUv we reconsidered our strategy of building a single chip that is optimized for training yet also used for inference. Given that design resources are limited it was hard to design TPUv and TPUv concurrently. NVIDIA also releases training and inference chips sequentially for example P came six months after the Pascal P GPU and T followed Volta by one year. Even so we could likely don’t fit in Vector Memory § . Figures and show the new MB Common Memory CMEM of TPUvi. This expanded memory hierarchy reduces the number of accesses to the slowest and least energy efficient memory see § . We picked MB as the knee of the curve between good performance and a reasonable chip size as the amortized chip cost is a significant fraction of TCO ③. Figure shows that the resulting CMEM is % of the die area. Since TPUvi is aimed at inference its die size is closer to TPUv’s die size than to TPUv’s size Table . Four dimensional tensor DMA. Memory system architecture is critical to any DNN accelerator and should be designed to maximize performance of common case workloads while being flexible enough for future models ⑧ ⑨ . TPUvi contains tensor DMA engines that are distributed throughout the chip’s uncore to mitigate the impact of interconnect latency and wire scaling challenges ①. The tensor DMA engines function as coprocessors that fully decode and execute TensorCore DMA instructions. Feedback from the XLA team on the usability and performance of the TPUv/v’s two dimensional single strided DMAs motivated the development of a new four dimensional triple strided tensor DMA architecture for TPUvi ②. It is compiler compatible but not binary compatible with past TPU chips. The new D tensor DMA design supports arbitrary steps per stride and positive/negative stride distances in each dimension. The inner vector is the TPUvi memory system’s native B word size which matches the lane bit vector unit inherited from TPUv/v and also facilitates efficient HBM access and interconnect design as described below. The striding parameters are independently programmable for the source side and destination side of the DMA. This feature offloads work from the TensorCore by enabling in memory B granular D tensor copies reshapes scatters gathers and memsets that can be used for transfers between any pair of architectural memories on the same chip as well as across different chips. The chip side of any host DMA similarly supports D operations. We can emulate more than four dimensions using multiple separate DMAs thereby reducing ISA encoding space and DMA complexity and cost. Software is sensitive to DMA bandwidth with latency being a secondary concern as long as the compiler is able to issue large DMAs or keep enough small DMAs in flight. The DMA bandwidth is designed to be independent of the chosen striding parameters because predictable performance is a key goal for effective compiler optimizations ②. To maximize predictable performance and simplify hardware and software TPUvi unifies the DMA architecture across local on chip remote chip to chip and host host to chip and chip to host transfers to simplify scaling of applications from a single chip to a complete system. It retains the essence of the TPUv/v relaxed DMA ordering model that is built around explicit software based synchronization . Unrolled DMA reads and writes are completely unordered both within a DMA and across DMAs. All on chip memories can each be concurrently accessed using DMAs as well as loads/stores while off chip HBM can only be accessed using DMAs. If there are concurrent overlapping address patterns between DMAs and/or load/store instructions explicit core DMA synchronization must be used to avoid memory level Figure . TPUvi chip block diagram. Architectural memories are HBM Common Memory CMEM Vector Memory VMEM Scalar Memory SMEM and Instruction Memory IMEM . The data path is the Matrix Multiply Unit MXU Vector Processing Unit VPU Cross Lane Unit XLU and TensorCore Sequencer TCS . The uncore everything not in blue includes the On Chip Interconnect OCI ICI Router ICR ICI Link Stack LST HBM Controller HBMC Unified Host Interface UHI and Chip Manager MGR . Figure . TPUvi chip floorplan. The die is mm see Table . CMEM is % of the area. OCI blocks are stretched to fill space in the abutted floorplan because the die dimensions and overall layout are dominated by the TensorCore CMEM and SerDes locations. The TensorCore and CMEM block arrangements are derived from the TPUv floorplan. hazards. TPUvi has support for synchronizing partial completion progress of DMAs with the TensorCore to help hide DMA ramp up and ramp down latency in case it becomes useful for future compiler optimizations ② larger DNNs ⑨ and/or tighter workload latency constraints ⑩. Custom on chip interconnect OCI . Rapidly growing and evolving DNN workloads ⑧ ⑨ have driven the TPU uncore towards greater flexibility each generation. Each component of past TPUs designs were connected point to point Figure . As memory bandwidth increases and the number of components grows a point to point approach becomes too expensive requiring significant routing resources and die area. It also requires up front choices about which communication patterns to support. For example in TPUv a TensorCore can only access half of HBM as a local memory it must go through the ICI to access the other half of HBM. This split imposes limits on how software can use the chip in the future ⑧. In TPUvi we added a shared On Chip Interconnect OCI that connects all components on the die and we can scale its topology based on the components that are present. OCI was particularly important with the addition of CMEM the choice of how to allocate and transfer data between HBM CMEM and VMEM continues to evolve ⑧. We designed for much wider datapaths compared with a typical SoC—B native access size instead of B cache lines. HBM bandwidth per core also increased by .X over TPUv and we expect similarly significant increases in the future . To handle this scale we were inspired by NUMA memory systems—take advantage of spatial locality of accesses to minimize latency and bisection bandwidth—but rather than putting the NUMA boundary between cores we put it between parts inside the same core. While the entire B memory word is accessible to any component the larger high bandwidth memories HBM CMEM and VMEM are each physically partitioned into four B wide groups to optimize HBM accesses. The corresponding group for each memory is connected to a segment of the OCI with minimal overlap to other groups effectively creating four non overlapping networks with each serving GB/s of HBM bandwidth rather than a single network serving all GB/s. This grouping provides locality that reduces latency and wiring resources and simplifies interconnect arbitration. This four way split is essential as wiring resources don’t scale as well as logic ①. Arithmetic improvements. Another big decision is the arithmetic unit. The danger of requiring quantization ⑥ and the importance of backwards ML compatibility ④ meant retaining bfloat and fp from TPUv despite aiming at inference. As we also wanted applications quantized for TPUv to port easily to TPUvi TPUvi also supports int. Our XLA colleagues suggested that they could handle twice as many MXUs in TPUvi as they did for TPUv ②. Logic improved the most in the more advanced technology node ① so we could afford more MXUs. Equally important the new CMEM could feed them § and §.A . The VLIW instruction needed extra fields to handle the four MXUs and the CMEM scratchpad memory which were easy to add given no need for binary compatibility. The TPUvi instruction is % wider than TPUv. We also wanted to reduce the latency through the systolic array of the MXU while minimizing area and power. Rather than sequentially adding each floating point multiplication result to the previous partial sum with a series of two input adders TPUvi first sums groups of four multiplication results together and then adds them to the previous partial sum with a series of two input adders. This optimized addition cuts the critical path through the systolic array to ¼ the latency of the baseline approach. Once we decided to adopt a four input sum we recognized the opportunity to optimize that component by building a custom four input floating point adder that eliminates the rounding and normalization logic for the intermediate results. Although the new results are not numerically equivalent eliminating rounding steps increases accuracy over the old summation logic. Fortunately the differences from a four versus two input adder are small enough to not affect ML results meaningfully ④. Moreover the four input adder saved % area and % power relative to a series of two input adders. It also reduced overall MXU peak power by % which directly impacts the TDP and cooling system design ⑤ because the MXUs are the most power dense components of the chip. Clock Rate and TDP. Air cooling for inference ⑤ and reducing TCO ③ led to a clock rate of . GHz and chip TDP of W once again closer to TPUv W than to TPUv W . ICI scaling. To provide headroom for future DNN growth ⑧ TPUvi has ICI links so that the chips per board can access nearby chip memory quickly Figure via model partitioning . TPUv uses ICI links. Apps may use it as the software stack matures and as DNNs grow ⑧. Figure . TPUvi board with chips that are connected by ICI. Workload analysis features. Building upon lessons from TPUv TPUvi includes extensive tracing and performance counter hardware features particularly in the uncore. They are used by the software stack to measure and analyze system level bottlenecks in user workloads and guide continuous compiler level and application level optimizations Figure . These features increase design time and area but are worthwhile because we aim for Perf/ TCO not Perf/CapEx ③. The features enable significant system level performance improvements and boost developer productivity over the lifetime of the product as DNN workloads grow and evolve see Table ⑦ ⑧ ⑨. scenarios Server has to meet a P latency constraint ⑩ Table and Offline that batch processes inference tasks without an SLO. The T used int on ResNet and SSD but used fp on NMT since NVIDIA couldn’t get int to work for this DNN ⑥. NVIDIA’s latest MLPerf Inference code for T was run in Google datacenters. §.B explains why T MLPerf Inference speed slows in Google datacenters. TPUs ran all the benchmarks using bf yet TPUvi averages .–.X as fast as T. TPUvi falls to .–.X for perf/TDP ③ although NMT is .X perf/TDP as both DSAs compute in floating point. We also measured performance per average power instead of TDP. TPUvi was . .X T for NMT .X for ResNet and . .X for SSD with a geomean of .X. SSD depends on NonMax Suppression which involves many gathers and other slow operations of high memory intensity. GPU’s coalescing memory likely runs faster than in TPU’s HBM. Backwards ML compatibility makes TPUvi good for Google even if its int perf/TDP isn’t much larger than T. Figure . Performance top and performance/system Watt ③ for production apps ⑨ relative to TPUv for the other TPUs. . TPUvi Performance Analysis Figure compares performance and perf/TDP of TPUs relative to TPUv for the production inference apps ⑨. TPUv and TPUvi are both ~.X faster with TPUv .X TPUv. The larger hotter TPUv/v dies have two cores while the smaller cooler TPUvi has one making TPUvi a win in perf/TCO ③ and for deployment ⑤. Thus TPUvi shines for perf/TDP at .X vs TPUv. .X is a combination of .X FLOPS T vs T .X SRAM capacity vs MB .X DRAM bandwidth vs GB/s .X TDP vs W and microarchitectural changes such as X MXU count per core to improve utilization. The technology upgrade to nm improves energy efficiency and transistor density to enable bigger FLOPS and SRAM. Among these perf/TDP factors CMEM gained ~.X Figure nm contributed ~.X and others contributed the remaining ~.X. The “accelerator wall” paper predicts perf/TDP across DSA generations from the log of the increase in transistors from new semiconductor nodes. Yet TPUvi delivers .X the TPUv perf/TDP using .X the transistors. Figure compares performance and perf/ system TDP of TPUv and TPUvi relative to the NVIDIA T see § using the MLPerf Inference .–. benchmarks. §.C discusses comparing it to A. MLPerf has two datacenter Figure . Performance top and performance/system TDP ③ relative to T for TPUv/vi in our datacenter §.B . Note that the T uses int for ResNet and SSD and fp for NMT. The TPUs use bf for all three to maintain backwards ML compatibility ④ with TPUv/v. MLPerf Inference . omits NMT so we use MLPerf Inference . code for it and . code for ResNet and SSD in Figures and . These results are unofficial as they have not been verified by MLPerf. . Performance In More Depth CMEM Figure shows the benefit of adding CMEM to TPUvi using the MLPerf Inference .–. benchmarks. The average benefit for offline is only .X but .X for server as the relationship between latency and the latency limited server performance is nonlinear at some utilization points. To understand CMEM’s impact on the production applications ⑨ Figure compares the performance of doubling the HBM bandwidth with CMEM disabled to the standard HBM bandwidth with CMEM enabled. Compiler flags disabled CMEM and running on TPUv with one core disabled doubled HBM bandwidth for one core. doubling HBM bandwidth. For apps the CMEM gain is even higher at . –.X providing much greater benefits. The roofline model in Figure helps explain what is going on. It divides applications into compute bound or memory bound using operational intensity to determine where an application is relative to its roofline. The ridgepoint is the operational intensity FLOPS per byte of memory accessed that is at the dividing line between memory bound and compute bound. While the roofline model normally is checking to see if an application is compute bound or DRAM bound it can be used recursively to see if it is CMEM bound. The CMEM read bandwidth is GB/s its write bandwidth is GB/s and unlike HBM it can read and write simultaneously. Figure shows the traditional DRAM roofline HBM along with the higher rooflines for CMEM read and CMEM write for the eight production applications. The vertical points show the performance gain per app with CMEM on vs. CMEM off. BERT .X. Its HBM footprint is MB so it could fit in CMEM. However XLA allocates all parameters in HBM to reduce context switch time and then prefetches into CMEM §.F so it only saves % of HBM traffic. The operational intensity goes from without CMEM to with CMEM still below the HBM ridgepoint of . The working set causes the large speedup of .. BERT includes a Gather op for embedding. It does indexed memory access from a large buffer ~MiB and this random access pattern causes poor HBM performance. As CMEM is larger than MiB the input table is placed in CMEM which has a much higher random access bandwidth. As a result the Gather op is significantly faster ~x yielding an overall .x model performance gain. BERT .X. Its HBM footprint is MB larger than CMEM. Prefetching works well with CMEM saving % of HBM traffic. The large speedup is again explained by the embedding table ~MiB for the Gather op which accounts for % of step time without CMEM. When the table is in CMEM the Gather op runs ~x faster. RNN .X. Its HBM footprint is MB larger than CMEM. Nevertheless CMEM filters out an impressive % of HBM traffic. This model runs many iterations of GRU layers and as intermediate tensors are placed in CMEM it can avoid costly HBM traffic. As this filtered traffic goes to CMEM the model is now CMEM bound. CMEM size. Figure explores the impact of a smaller CMEM than MB on the apps and the MLPerf server benchmarks. The app average performance is % at MB % at MB % at MB and % at MB. For MLPerf it is % at MB % at MB % at MB and % at MB. Reducing CMEM %–% might have little impact on current DNNs but given our perf/TCO orientation ③ and the growth of DNNs ⑧ we won’t know for sure for a few years if CMEM was too large. Figure . Unverified MLPerf Inference impact of turning on CMEM vs no CMEM. Figure . Performance of X HBM bandwidth with no CMEM relative to CMEM with standard HBM bandwidth. Figure . Roofline model showing apps without CMEM low point vs with CMEM high point . Operational intensity OI here is operations divided by memory accesses to HBM or to CMEM. If OI were relative to HBM only CMEM would increase OI and move the points to the right as well as up. For of the applications the speedups over TPUvi without CMEM are similar both in the range of . to .X. Our interpretation is that the main benefit of CMEM is the much higher memory bandwidth for these applications adding CMEM is cheaper lower power and easier than perf/TCO ③ as architects want good perf/TCO for a DSA’s entire lifetime including its early years. Now that we’ve explained TPUvi and its perf/TDP let’s see how other commercial inference DSAs measure up. . Related Work Table shows all five entries for datacenter inference chips for the MLPerf Inference . and . server benchmarks. The T is a low power inference chip containing symmetric multiprocessors and a relatively fast GDDR DRAM at GB/second if ECC is off . Its standard clock rate is . GHz with a Turbo mode at . Ghz. It supports floating point and integer numerics. The primary omission that prevents backwards ML compatibility ④ with TPUv used for training is the lack of bf support in T. This paper compares TPUvi to it in § since of these five chips T comes the closest to meeting Google’s needs. Habana Goya is a moderate power inference chip with VLIW SIMD vector cores running at . GHz attached to a relatively slow DDR memory. Its perf/TDP is half of T for the SSD benchmark. It has int and fp but omits bfloat. Given no backwards ML compatibility when training with TPUs ④ and lessons ⑦ and ⑧ about the importance of multi tenancy and how fast DNNs grow Goya would not be a good choice for Google’s datacenters. The Intel Nervana NNP I Spring Hill is a low power inference chip with VLIW vector cores with K int/fp MACs each running at . GHz . They use a relatively slow LPDDR memory. Its perf/TDP is .X of T for the single ResNet benchmark. Like the Goya it omits bfloat and has relatively slow DRAM memory so we would again worry about backwards ML compatibility ④ plus multi tenancy ⑦ and DNN growth ⑧ for the NNP I. Zebra is a soft core that runs on the Xilinx Alveo FPGA . It offers moderate power at . GHz with the relatively slow DDR DRAM. Its perf/TDP is X worse than T for ResNet. It has the same deployment concerns for Google as Goya whose perf/TDP is ~X higher plus Zebra is designed exclusively for CNNs ⑨. Finally Alibaba HanGuang is a core chip with moderate to high power at . Ghz. Each core has a Tensor Engine a Pooling Engine and a Memory Engine and it executes CISC like instructions. Not only is its perf/TDP .X better than T for ResNet its unnormalized performance is ~X faster. It also has MB of SRAM .X larger than the next closest. While the lack of bfloat would be an issue ④ the Achilles Heel of the HanGuang is that it has no DRAM whatsoever. mentions the use of a PCIe switch to gang multiple chips together to increase capacity but using chips to match the multi tenancy ⑦ sizes of Table would be expensive and it’s unclear how to support DNNs rapid growth ⑧. Figure . Percent of MB speed as CMEM varies – MB for the apps and MLPerf Inference . . server code. perf/TDP TDP Cores vs T W T . Goya . Nervana . Zebra . HanGuang . GHz . . . . MiB Type SRAM DRAM GDDR ≈ DDR LPDDR DDR none Table . Five DSAs that ran the MLPerf Inference . Goya Nervana HanGuang or . T Zebra server scenarios. Goya ran only SSD and the last three ran only ResNet. Performance is relative to T for MLPerf Inference .. TDP in this table is per chip rather than per system since the latter was unavailable. All have –GB of DRAM except HanGuang which has none. All results are from . Figure . T and TPUv running unverified MLPerf Inference benchmarks ./. at Google with memory ECC off and on. NVIDIA’s MLPerf score is % for T and % for TPUvi is unverified MLPerf in our datacenter. For the minute case the T was idle initially. It then ran MLPerf with ECC off and on for minute at the fastest clock rate. T offers inline ECC which uses memory bandwidth. For the minute case the machines are not idle beforehand. TPUvi’s speed is unchanged whether ECC is on or off or how long it runs. This reminds us of the Activation Storage in TPUv Figure . Our initial buffer allocation scheme used most of its MB. We eventually used an integer linear programming solver to effectively triple memory space. The large memory allowed our relatively weak initial compiler to satisfy the apps in TPUv’s early years. This example demonstrates the interplay of compiler quality ② and mm chip has B transistors and uses Watts so its TDP is .X higher than T. MLPerf . inference shows A runs ResNet server .X–.X faster than T and SSD server is .X–.X faster. The perf/TDP of A is within ±% of T. As T is more like TPUvi it is the best current candidate to compare to see § . . Discussion We start with benchmarks and measurements. A. Utilization of peak FLOPS versus roofline Table shows the average fraction of peak FLOPS/s for four TPUs. One reason is that we increased the number of MXUs over time as that was a good way to leverage the new technology node ①. Despite MXUs per core in TPUvi the MXUs occupy only % of the die Figure . A more useful metric than fraction of peak FLOPS is fraction of roofline limited by memory bandwidth or FLOPS/s depending on the DNN arithmetic intensity. Table shows it generally increasing over time. Without CMEM TPUvi FLOPS/s would drop to % and roofline to %. TPU TPUv TPUv TPUv TPUvi MXUs/Chip x x x x MXUs % Die Area % % % % FLOPS/s Utilization % % % % HBM Roofline Util. % % % % Table . Average utilization of peak performance and of roofline for our eight production applications. D. Correlation of TCO and TDP Figure shows R is . for the DNN DSAs when comparing TCO versus system TDP in Google datacenters. R is still . R . for chips of CPUs GPUs and TPUs across several generations. By definition a correlation coefficient R ranges from . to .. If R . statisticians say the correlation is perfectly linear. There is a strong linear correlation if R is . moderate between . and . and . is weak. Turning R into R shows the percent that explains the variability of the dependent variable. An R of . means TDP explains % of the variability of TCO for the five DSAs and an R of . explains % of TCO variability for all processors. As it is close to how companies decide what to build or buy we hope future DSA papers will report perf/system TDP over a chip’s lifetime. B. Turbo mode and Perf/CapEx versus Perf/TCO warns that the cooling system is critical to T performance as the clock can slow when temperature rises since Turbo mode has a .X faster clock. MLPerf Inference runs are required to last at least one minute which is near the time it takes to heat up a chip and its heatsink fully ③. Our experiments found that an idle T at the lowest clock rate ran at ℃ see . If we run MLPerf at . GHz the temperature rises to ℃ in seconds. Thereafter the chip stays at ℃ with the clock speed varying between . and . GHz. Others found different variations in temperature presumably due to running programs with different operational intensity than MLPerf. Given our datacenter environment plus our need for ECC—optional for MLPerf Inference . and .—Figure shows the impact of running MLPerf Inference ./. for minutes with ECC enabled T performance drops %–% below its MLPerf Inference ./. level. With ECC on TPUvi starts at ~℃ and rises only ~℃ over minutes. Google purposely provisions enough power and cooling in datacenters to keep TPUvi latency constant and includes extra memory for ECC so that it can always be on. E. Power Savings vs TCO and P Latency Trying to maximize average perf/TDP such as temporarily running faster can lead to worse tail latencies . TPUs omit techniques like Turbo mode and caches that surely help P latency but probably not P latency. Most OpEx cost is for provisioning power and not for electricity use so saving power already provisioned for doesn’t improve TCO as much as one might hope. The last three topics are about TPUvi software. F. Multi tenancy and on chip SRAM The XLA compiler allocates weights in DRAM and prefetches them into CMEM for execution to reduce the context switching time for multi tenancy ⑦ if weights are allocated to CMEM they must be reloaded before the task can continue. The software stack must also reload CMEM for apps that don’t prefetch. BERT would take about ~ microseconds—MB ÷ GB/s—to load the weights from HBM which is on the borderline of acceptability. G. Compiler vs. Binary Compatibility in GPUs and TPUs Most CPUs require backwards binary compatibility § a problem for VLIW DSAs § . TPUs don’t. The nearest software approach to TPUvi is NVIDIA’s PTX virtual instruction set. NVIDIA promises PTX compatibility across GPU generations so some tools and programmers use PTX instead of CUDA. XLA’s Low Level Operations LLO are the closest analogy to PTX see § . Only a few programmers write in LLO as there is no guarantee that LLO code will work for future TPUs and most TPU developers can get good performance without having to resort to LLO coding. C. Benchmarking TPUvi and T versus NVIDIA A § compares TPUvi to T but some asked if the paper should use the A instead. Our experience with TPUv was that a large power hungry expensive chip was a mismatch to inference ③ so the paper uses T. The A The TPU ASIC die area is only a small fraction of the total die area in the package. Four Hi HBM stacks in TPUv total mm which swamps the TPU ASIC area mm in Table . H. Quantization Aware Training The quantization downsides in ⑥ are for post training quantization. Another approach is quantization aware training which uses integers during training allowing the switch to serving without a separate quantization step. Basically the developer gets backwards ML compatibility while using integer data types. The challenge is motivating the use of quantization aware training. Some developers might like the better memory footprint or performance for DSAs whose integer arithmetic is faster than floating point. The issue may come down to how much harder training is in integers versus floating point and for how many applications does it work. Dennard scaling is finished so TCO is becoming even more strongly correlated with power dissipation. Google’s TCO and system TDP have a correlation coefficient R of . for DNN DSAs and . for a collection of CPUs GPUs and TPUs. This paper also updated Horowitz s influential energy operation table to a modern technology with current models of SRAM and DRAM. With recent processes logic is still becoming denser and faster yet SRAM is scaling very weakly ①— nm SRAM looks to be only % denser than nm —so the energy cost of memory accesses dominates perf/TDP even more ③. Hence contrary to the ML developer community’s convention of trying to minimize FLOPS—which leads to memory access intensive DNNs—in a datacenter context reduced precision FLOPS can be relatively free in comparison to memory references. With Moore s Law diminishing and Dennard scaling dead hardware/software/DNN co design is the best chance for DNN DSAs to keep vaulting accelerator walls . . Conclusion Once Google developed and deployed its first generation inference TPU in datacenters Google’s creative ML application developers and economic realities caused us to change the inference and training system roadmaps. Faced with widespread adoption across Google’s products an explosion in the number of ML application developers and needs for high developer velocity enabling automated roll out of new models multiple times per day we changed Google’s roadmap plans. In the process we learned ten lessons about DSAs and DNNs in general and about DNN DSAs specifically that shaped the design of TPUvi ① Logic improves more quickly than wires and SRAM ⇒ TPUvi has MXUs per core vs for TPUv and for TPUv/v. ② Leverage existing compiler optimizations ⇒ TPUvi evolved from TPUv instead of being a brand new ISA. ③ Design for perf/TCO instead of perf/CapEx ⇒ TDP is low CMEM/HBM are fast and the die is not big. ④ Backwards ML compatibility enables rapid deployment of trained DNNs ⇒TPUvi supports bf and avoids arithmetic problems by looking like TPUv from the XLA compiler’s perspective. ⑤ Inference DSAs need air cooling for global scale ⇒ Its design and . GHz clock lowers its TDP to W. ⑥ Some inference apps need floating point arithmetic ⇒ It supports bf and int so quantization is optional. ⑦ Production inference normally needs multi tenancy ⇒ TPUvi’s HBM capacity can support multiple tenants. ⑧ DNNs grow ~.x annually in memory and compute ⇒ To support DNN growth TPUvi has MXUs fast onand off chip memory and ICI to link adjacent TPUs. ⑨ DNN workloads evolve with DNN breakthroughs ⇒ Its programmability and software stack help pace DNNs. ⑩ The inference SLO is P latency not batch size ⇒ Backwards ML compatible training tailors DNNs to TPUvi yielding batch sizes of – that raise throughput and meet SLOs. Applications do not restrict batch size. Acknowledgment The authors built or analyzed the systems that involved major contributions from many others. Thanks go to the hardware and software teams for making TPUs possible. We also thank David Culler Jeff Dean Urs Hölzle Christos Kozyrakis Hank Levy Alex Ramirez Partha Ranganathan and Sophia Shao for feedback on this paper. The people on these teams include Gaurav Agrawal Catherine Ahlschlager Ahmet Akyildiz Ashby Armistead Sandeep Bhatia Rich Bonderson Oliver Bowen Roger Carpenter Andrew Casper Clifford Chao Dehao Chen Chiachen Chou William Chwee Xiangyu Dong Houle Gan Rakesh Gautam Peter Gavin Arnd Geis Ben Gelb Russ Gibbons Sandeep Giri Vinayak Gokhale Pareesa Golnari Rajendra Gottipati Nils Graef Jesse Guss Benjamin Gwin David Haskell Blake Hechtman Matthew Hedlund Jian Ho Doug Hogberg Jerry Huang Michael Hsu Adam Hutchin Mike Hutton Berkin Ilbeyi Srikrishna Iyer Arpith Jacob Indira Jayaram Chetan Kale Pankaj Kanwar Srinidhi Kestur Teju Khubchandani Woon Seong Kwon Namhoon Kim Andy Koch Alan Kulawik Poorna Kumar Alice Kuo Steve Lacy Joshua Lang Chester Li Avinash Lingamneni Derek Lockhart Stephen Longfield Fong Lou Tao Liu Kyle Lucke Adriana Maggiore David Majnemer Seth Merriman Rolf Mueller David Munday Mandar Munishwar Hithesh Murthy Lifeng Nai Spoorthy Nanjaiah Andrew Noonan Alexander Nguyen Vinh Nguyen Tayo Oguntebi Virag Parekh Jose Baiocchi Paredes Sang Keun Park Tejas Parikh Omkar Pathak Ram Babu Penugonda Andy Phelps Vaishali Raghuraman Guru Rajamani Andrew Ranck Paul Rodman Bjarke Roune Ohad Russo Amit Sabne Amir Salek Kirk Sanders Julian Schrittwieser Chris Severn Boone Severson Hamid Shojaei Jaideep Singh Tej Soni Jaswanth Sreeram Dan Steinberg Jim Stichnot Qian Sun Mercedes Tan Hua Tang Horia Toma Alex Thomson Ani Udipi Dimitris Vardoulakis Sandeep Venishetti Jack Webber Monica Wong Chan Hsin Jung Yang Mingyao Yang Xiaoming Yu Lu Yuan Sara Zebian Feini Zhang and Ce Zheng. . 