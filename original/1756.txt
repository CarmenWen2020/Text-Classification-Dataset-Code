Abstract—Although DRAM capacity and bandwidth have increased sharply by the advances in technology and standards, its
latency and energy per access have remained almost constant in
recent generations. The main portion of DRAM power/energy is
dissipated by Read, Write, and Refresh operations, all initiated
by a Precharge phase. Precharge phase not only imposes a
large amount of energy consumption, but also increases the
delay of closing a row in a memory block to open another one.
By reduction of row-hit rate in recent workloads, especially in
multi-core systems, precharge rate increases which exacerbates
DRAM power dissipation and access latency. This work proposes
a novel DRAM structure, called Precharge-Free DRAM (PFDRAM), that eliminates the Precharge phase of DRAM. PFDRAM uses the charge on bitlines from the previous Activation
phase, as the starting point for the next Activation. The difference
between PF-DRAM and conventional DRAM structure is limited
to precharge and equalizer circuitry and simple modifications in
sense amplifier, which are all limited to subarray level. PF-DRAM
is compatible with the mainstream JEDEC memory standards
like DDRx and HBM, with minimum modifications in memory
controller. Furthermore, almost all of the previously proposed
power/energy reduction techniques in DRAM are still applicable
to PF-DRAM for further improvement. Our experimental results
on a 8 GB memory system running SPEC CPU2017 and PARSEC 2.1 workloads show an average of 35.3% memory power
consumption reduction (up to 54.2%) achieved by the system
using PF-DRAM with respect to the system using conventional
DRAM. Moreover, the overall performance is improved by 8.6%,
in average (up to 24.3%). According to our analysis, all such
improvements are achieved for less than 9% area overhead.
Index Terms—DRAM, Power Consumption, Memory Access
Latency, Sense Amplifier
I. INTRODUCTION
By increasing the demand for memory space of computer
systems, from high-performance computers to low-power embedded devices, memory subsystem contributes for a large
amount of power/energy consumption of the entire system.
Recent evaluations reveal that memory subsystem accounts for
25%-57% of system power consumption [29], [34], [56].
Among different units of a memory subsystem, DRAM
chips consume a major portion of power/energy [22], [50].
A DRAM memory chip is composed of three components:
1- memory cell array which stores the data in a matrix of
storage cells, 2- peripherals, to access memory cells, and 3-
input/output (I/O) interface which translates external signals
to activate peripheral circuits [1], [21].
Three main operations that manipulate memory cell array
are: Read, Write, and Refresh. These operations are initiated
by activating a row of cells, which is the most power-hungry
operation in DRAM [2], [12], [13], [56]. To maximize the ratio
of memory cell arrays size to the peripherals and I/O interface
area, and to boost performance of DRAM, the commands
are issued to a bunch of DRAM memory chips for parallel
Read/Write/Refresh operations. Thus, a large number of cells,
e.g. 64 K cells in DDR4 [18], are activated at the same time,
while many of activated cells may not be read or written during
the access, hence, imposing extra power dissipation [56].
Since DRAM dissipates a considerable amount of dynamic
power/energy, many previous studies propose different techniques to reduce its power/energy consumption. Power gating
or voltage scaling of some memory cell arrays [7], [12], reducing read/write size to decrease signal activities [13], [14], [26],
[33], [56], [59], [61], reducing data transmission energy by
coding and compression techniques [27], [28], [38], [47] and
encoding data to efficiently utilize memory space and reduce
power consumption [5], [15], [35], [40], reducing refreshing
energy by lowering refresh rate or intelligently refreshing with
respect to operation condition [6], [9], [23], [24], [30], [31],
[37], [42], [57], optimizing peripherals structure [48], [52],
and boosting row-hit [17], [20], [32], [43], [49], [51], [54],
[60] are the main categories of such techniques.
In all generations of DRAMs, from initial prototypes to
recent products, any Activation requires to precharge some bitlines beforehand [19], [21], [48]. Precharge phase is mandatory
to set a starting point for sense amplifiers (SAs). Two bitlines
are paired and connected to an SA; the SA determines the
stored value of the connected cell to one of the two bitlines by
detecting the tiny voltage perturbation on that bitline compared
to other bitline as the reference voltage. At the end of sensing
phase, one of the bitlines is discharged to 0 while the other
one is charged to VDD.
The older generations of DRAMs precharge bitlines to VDD
before sensing and utilize a dummy cell to determine the value
stored in the target cell [21]. The newer generations precharge
the bitlines to VDD/2 in Precharge phase, with no need to
a dummy cell during access [19], [21]. Charging a large
number of bitline pairs to VDD/2 (VDD in earlier products) in
Precharge phase, and then charging one bitline in each pair to
VDD (having one of them charged to VDD in earlier products)
and discharging the other to 0 in Activation phase, dissipate
considerable amount of energy. In conventional DRAMs, the
amount of charge and discharge on each bitline pair is the
same, regardless of the current and previous values on bitlines.
This work proposes a Precharge-Free DRAM (called PFDRAM) structure which uses the charge (0 or VDD) remained
from the previous operation on bitline pairs, to determine the

"$.*&&&UI"OOVBM*OUFSOBUJPOBM4ZNQPTJVNPO$PNQVUFS"SDIJUFDUVSF	*4$"

¥*&&&
%0**4$"
2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) | 978-1-6654-3333-4/21/$31.00 ©2021 IEEE | DOI: 10.1109/ISCA52012.2021.00019
stored value in the corresponding cell of each bitline. In PFDRAM, if the voltage on a cell is equal to the voltage of the
connected bitline (i.e. both are 0 or VDD), no voltage change
occurs on bitlines pair and almost no power is dissipated by
the pair during this access. A flip on bitlines value occurs only
if the voltage of the cell differs from the bitlines voltage.
Knowing that the similarity of stored values in main memory, due to data dependency, narrow-width values, and highfrequent instructions, can be up to 90% in recent data words
[47], it is highly expected that two consecutive accessed data
on a bitline be equal. Thus PF-DRAM can significantly reduce
bitlines discharge/charge rate during Read/Write/Precharge
operations. Note that even for completely random stored
values in DRAM, the probability of bitline flips will be
50% in consecutive accesses, and PF-DRAM can still reduce
dynamic power consumption of DRAM. Our simulations using
SPEC CPU2017 and PARSEC 2.1 benchmark suites show that
the average power consumption of a PF-DRAM main memory
reduces by 35.3% (up to 54.2%) in single- and multi-core
systems, in comparison with a conventional DRAM structure.
Moreover, PF-DRAM reduces average memory access latency by 19.2%, and hence boosting the overall system performance, more precisely the Instructions Per Cycle (IPC), by
8.6%, in average.
Another benefit of PF-DRAM that reduces design complexity and cost is the elimination of half-VDD (VDD/2) power
source used to precharge bitlines [21] in conventional DRAMs.
All above-mentioned benefits of PF-DRAM are achieved
for less than 8.8% area overhead due to small changes in
the SA with no modification in DRAM cell arrays. Note that
our proposal is orthogonal to almost all previously proposed
DRAM power reduction techniques; therefore, PF-DRAM can
be used together with such previous solutions for further
power/energy saving.
II. DRAM BACKGROUND
High density (up to 32 Gb/chip), high bandwidth (up to
25.6 GB/s in DDR4), and low cost, made DRAMs the best
option as main memory in High-Performance Computers [19],
Graphic Processing Units [13], and embedded System-onChips [53]. These privileges are achieved by grouping numerous tiny memory cells in a hierarchical organization and
interleaving memory requests among different groups.
Typical contemporary JEDEC standard [8] DRAMs are controlled by one or more Memory Controllers (MCs), residing
on processor die, that manage all complex Access & Hold
operations in DRAM. Each MC is connected to a Channel
which is composed of one or more Ranks that share datapath
and control commands, each of which split into many banks
(e.g. 16 banks per rank in DDR4 and HBM). All of the
banks in a rank receive the same commands and operate
simultaneously to improve memory-level parallelism within a
rank [13], [21], [26].
Each bank is made of multiple MATs (memory arrays),
arranged in subarrays, as shown in Fig. 1. Each MAT contains
some memory cells in a 2-dimensional arrangement (e.g.
Dd Dd
Dd Dd
ĂŶŬ
ŝƚůŝŶĞ
t>ŶͲϭ
t>Ŷ
ŝƚůŝŶĞ
^ΘW
^ĞŶƐĞ
ŵƉůŝĨŝĞƌ;^Ϳ
^ͲŶĂďůĞ
^ͲŶĂďůĞ
Dϵ
Dϳ Dϴ
Dϱ Dϲ
Dϰ
s
sͬϮ
WƌĞĐŚĂƌŐĞ
Θ
ƋƵĂůŝǌĞƌ
;WͿ
>ŽĐĂůĂƚĂůŝŶĞ
ŽůƵŵŶ^ĞůĞĐƚ>ŝŶĞ
DĂƐƚĞƌĂƚĂůŝŶĞ
>ŽĐĂůtŽƌĚůŝŶĞ;>t>Ϳ 'ůŽďĂůtŽƌĚůŝŶĞ;'t>Ϳ
ŽůƵŵŶ^ĞůĞĐƚ
>ŝŶĞ
/ͬK
/ͬK
ŽůƵŵŶ
^ǁŝƚĐŚ
/ͬK
ďŝƚůŝŶĞ ďŝƚůŝŶĞ
Dϭ
DϮ Dϯ
DϭϬ Dϭϭ
DĞŵ͘Ğůů
^ƵďĂƌƌĂǇ
>ŽĐĂůĂƚĂůŝŶĞ
ŽůƵŵŶĞĐŽĚĞƌ
'ůŽďĂůZŽǁĞĐŽĚĞƌ
ϱϭϮĞůůƐ
ϱϭϮĞůůƐ
Fig. 1: DRAM bank structure
512×512). When the Global Row Decoder activates a Global
Wordline (GWL) in a bank, all Local Wordlines (LWLs)
connected to the GWL are activated and connect the cells
capacitors to the corresponding bitlines. Even/odd LWLs connect the accessed cells to bitline/bitline or vice versa. During
Activation phase, as the first step for Read/Write/Refresh, one
page size (e.g. 64 K) of cells are connected to bitlines.
Various structures for DRAM cells were proposed; however,
the dominant DRAM cell structure is 1T-1C (the memory cell
shown in Fig. 1). In this structure, an NMOS access transistor
connects the cell capacitor to a bitline. In conventional DRAM
memories, all bitlines in a subarray must be precharged to
VDD/2 during Precharge phase, prior to the Activation. By
sharing the stored charge in the bitcells capacitors (Ccell) and
bitlines parasitic capacitors (CBL), a tiny voltage perturbation
(VSconv. ) occurs on the bitlines, which can be given by:
VSconv. = VDD
2 . Ccell
CBL + Ccell
(1)
By the end of Charge Sharing phase, the developed voltage
on bitlines is VDD/2±VSconv. (see Fig. 2a). Due to long bitline,
about 1536 F in a 512×512 6 F2 DRAM (F stands for feature
size): 1- the parasitic capacitance of bitlines is relatively larger
than bitcells capacitors (e.g. 144 fF vs. 24 fF [12]), thus,
VSconv. is relatively small, about 100 mV to 200 mV, and 2-
the ambient noise on bitlines is high. Therefore, differential
sensing of bitlines is inevitable to cancel noises and detecting
small signals on bitlines for a reliable memory access. To this
goal, each pair of bitlines are connected to a differential SA
(composed of M4 to M9 in Fig. 1). One bitline (bitline in
Fig. 1) in each bitline pair is used as the reference voltage
(VDD/2) for SA to sense voltage perturbation on other bitline.
To further reduce noise, transposed folded-bitlines are used
in contemporary DRAM structures [21]. In folded-bitlines,
bitlines of each pair are twisted to minimize noise difference
between them. Note that, due to less sensitivity of older
generations of DRAMs to the noise, open-bitline structure
were popular because of its smaller cells (6 F2 vs. 8 F2).
By activating the SA, using SA-Enable and SA-Enable, the
bitline with lower voltage is discharged to 0 and the other
bitline is charged to VDD by the positive feedback of cross
coupled NOT gates in the SA.
                       
wordline
Precharge
SA Enable
Column Select
bitline
I/O
tRCD tRP
tCL
Charge Sharing
Sensing
& Restore
Precharge
tRC tRAS
Row Open
PRE ACT Column Access PRE
bitline
DDR4 Timing:
tRCD=14ns
tCL=14ns
tRP=14ns
tRAS=32ns
0 0.1 0.2 0.3 0.4
DRAM device power (W)
Precharge and Activation
(background and active)
Refresh
 I/O and R/W Peripherals
(a)
(b)
Fig. 2: Internal DRAM operation and timing (a), DRAM device power
dissipation breakdown under typical load (b)
Accesses to memory cells are destructive. By holding the
wordline active during SA activation, the destroyed value of
a bitcell is restored. The required time for stabilization of
SA is called tRCD (Row to Column latency), which includes
wordline activation and charge sharing latency as well. After
tRCD duration, Read/Write operation can be performed by
activating one of the Column Select Lines from each MAT
and connecting the SA output to Master dataline. The Write is
similar to Read operation, but the data propagates in opposite
direction. A Refresh is similar to Read except that, after
restoration, the row is closed with no column access. The
latency of accessing a column and transferring the data to/from
Local dataline is called tCL (Column Delay). The time from
when the Row Activate command is issued until the data at I/O
is available is called tRAS (Row Address Strobe); it includes
activation of wordline, charge sharing, sensing, restoration of
connected cells and column select steps (see Fig. 2a).
To prevent voltage drop when charging/discharging bitcell
capacitors to/from VDD by the NMOS access transistor,
the controlling voltage of wordlines should be higher than
VDD+VT h, where VT h is the NMOS access transistor threshold voltage. This voltage is called High-Level VDD or VDH
and is generated by a charge pumping level shifter [21]. In
DDR4, this voltage is supplied by 2.5 V input pins (VPP [4]).
Precharging bitlines is done by connecting them to each
other and to VDD/2 voltage source at the same time, to
precharge and equalize bitline and bitline by the Precharge
and Equalizer (PE) circuits (see Fig. 1). This operation imposes tRP (Row Precharge) delay before any activation which
makes the minimum DRAM access cycle time (tRC) equal
to tRCD+tCL+tRP. In earlier generations of DRAMs, bitlines
were precharged to VDD [21]. However, in current DRAM
structures, bitlines are precharged to VDD/2 to utilize charge
sharing between bitlines and reduce their voltage swing. By
halving the voltage swing, the energy dissipation of bitlines
during Precharge is almost halved in comparison with a fullVDD/2
VDD
GND
VDD/2
VDD
GND
123456789
bitline bitline
'0' '0' '1' '1' '0'
10
Activation
Precharge
Time
Fig. 3: A bitline pair activities during ’0’→ ’0’, ’0’→ ’1’, ’1’→ ’1’, and
’1’→’0’ sequences in a conventional DRAM
VDD precharging [21]. However, it requires an additional
regulator to generate VDD/2.
Since one page (say 8 KB) is accessed during each Activation and only one block (say 64 B) is read/written, in
open-row policy, the cells stay connected to bitlines while the
SAs (called row-buffer as well) remain activated, hoping that
subsequent accesses can be served with the previously latched
data in them (row-hit). In this case the request is served only
with the tCL latency. Otherwise, the row should be closed by
a Precharge command for the next access. In the cases that the
locality of references is low, it is preferred to close the row
right after the access is served, in order to avoid tRP latency
in the next access (close-row policy) [48].
III. PROPOSED DRAM STRUCTURE
A. Motivation
As Fig. 2b illustrates, Precharge and Activation operations,
are the main sources of power dissipation in DRAMs [2], [13],
[21], [62]. By reducing dissipated I/O energy in 3D stacked
memories, e.g. HBMs, the Precharge/Activation/Refresh energy share is increased even more [13], [36]. In conventional
DRAMs, access to bitcells for Read/Write/Refresh, requires
precharge of all bitlines of the corresponding subarrays to
VDD/2 (unless a Precharge-All command was used before).
The light/dark bands in Fig. 3 show Precharge/Activation
phases of a pair of bitlines connected to an SA. The upward
arrows show bitlines charging, which draw current from VDD.
The drawn energy from VDD power source, to precharge
one bitline (Epre) parasitic capacitor (CBL) to VDD/2, can
be calculated using Eq. (2), where Qpre = CBL. VDD
2 is the
amount of charge drawn from the power source that pulls up
bitline voltage by VDD
2 , and VDD is the DRAM chip operating
voltage (e.g. 1.2 V in DDR4). Since VDD/2 voltage source
is generated from input VDD power rail, using half-VDD
internal regulator, almost half of Precharge energy is dissipated
in push-pull output transistors of the VDD/2 regulator [21].
By sharing the opposite charges on bitlines in each pair
through connecting them together during the Precharge phase,
the drawn energy from VDD power source is reduced by a
factor of β (about 0.54, according to our simulations). β can
be slightly changed by modifying the width of Precharge and
Equalizer transistors, with energy/performance compensation.
Epre = β.Qpre.VDD = β.CBL.
VDD
2
.VDD = β.CBL.
VDD
2
2
(2)

The blue arrows in Fig. 3 indicate transitions on bitlines that
dissipate Epre. This figure shows all possible transitions on a
bitline over time (’0’→’0’, ’0’→’1’, ’1’→’1’, and ’1’→’0’).
During Activation, one bitline in each pair charges to VDD
and the other is discharged to 0 by the SA. If VSconv.>0, the
bitline connected to the cell and the cell are charged to VDD,
otherwise, the bitline is charged to VDD. As Eq. (3) shows, the
dissipated energy in both cases is the same. The red arrows in
Fig. 3 show the transitions on bitline and bitline that dissipate
Activation energy in conventional DRAMs (Eactconv. ).
Eactconv. = Qact.VDD
=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
(CBL + Ccell).( VDD
2 − VSconv. ).VDD , VSconv.>0
=(CBL+Ccell).( VDD
2 −VDD
2 . Ccell
CBL+Ccell ).VDD
CBL. VDD
2 .VDD , VSconv.<0
= CBL.
VDD
2
2
(3)
As Fig. 3 illustrates, even if the accessed value on a bitline
does not change in consecutive Activations (’0’→ ’0’ and
’1’→’1’), one charge and one discharge occur on the bitline
and its dual (bitline). For instance, the bitline voltage, after
the stabilization in time period 1, is 0. In time period 2,
bitline is precharged to VDD/2 and dissipates Epre. Then it
is discharged to 0 again in time period 3, while Eactconv. is
dissipated by bitline in time period 3. The same condition
(inverse for bitline and bitline) occurs in time period 5 to 7.
Thus, regardless of the previous and current values on bitlines,
the access energy (Eaccconv. ) can be calculated by Eq. (4).
Eaccconv. = Epre + Eactconv. = 1
2
(1 + β).CBL.VDD2 (4)
Precharging bitlines not only dissipates energy, but also extends the latency of closing a row. About one third of tRC is
devoted to tRP (see Fig. 2a). tRP has not improve over time in
comparison with capacity and bandwidth. In fact the Precharge
operation latency has increased from third generation to fourth
generation (from 12.5ns to 14.1ns) [11].
If stored values in DRAM cells are random, the chance of
reading the same value on a bitline in two successive accesses
is 50%. In this condition, half of Eaccconv. is dissipated for extra charge/discharge of bitlines. Based on this fact we propose
a new bitline configuration and a special SA which eliminate
the need for precharging bitlines. In this configuration, each
pair of bitlines flips only if the current accessed value differs
from the previous one.
In real-world applications not only the content of DRAM
are not random, but also high data similarity exists between
them [47]. The reason for such similarity of data in DRAM,
and thus low probability of bitlines flip during successive Activations are: 1- narrow-width values [45], 2- large number of
’0’ in stored data [15], 3- data dependency, 4- locality of references, and 5- frequent instructions and low hamming distance
in machine codes. Our simulations with SPEC CPU2017 and
PARSEC 2.1 on 1-core, 2-core, 4-core, and 8-core systems,
show the average probability of bitlines flip in successive
Activations is 9.1%, 11.0%, 11.9%, and 13.6%, respectively.

Fig. 4: PF-DRAM bitlines configuration and its internal signals during cell
access (durations lengths are illustrative, not real)
B. Precharge-Free DRAM
The proposed DRAM, called Precharge-Free DRAM (PFDRAM), eliminates the Precharge phase and imposes unnecessary bitlines-flip during Activation phase. In PF-DRAM, each
bitline keeps its previous state till the value in the accessed cell
differs from the previous value on the corresponding bitline.
Bitline configuration of PF-DRAM is presented in Fig. 4.
Here, bitlines in each pair are named Bitline Left (BLL)
and Bitline Right (BLR) (instead of bitline and bitline),
since they do not carry opposite values like in conventional
DRAMs. In PF-DRAM, the bitlines are decoupled from the SA
input/output, SA Left (SAL) and SA Right (SAR) nodes, using
two decoupling transistors, M4 and M5. Bitline decoupling
(isolation) is widely used for sharing SAs among bitline pairs,
to reduce SAs area overhead, or to hide tRP [21], [48].
To differentially determine a tiny perturbation on a bitline
during cell access, both bitlines in each pair should have the
same voltage before row activation. In conventional DRAMs,
bitlines must be precharged to a specific value (e.g. VDD/2),
while in PF-DRAM they keep their voltage from the previous
Activation, that can be VDD or 0. Cell access is initiated by
activating a wordline, for instance WLn in Fig. 4 in time
period 1. The access transistors are activated by VDH, like
in conventional DRAMs, to prevent voltage drop.
Before wordline activation starts in time period 1, BLL
and BLR are connected to SAL and SAR, respectively, by
activating Decoupler Right (DCR) and Decoupler Left (DCL)
signals. The SA is disabled in this time period (SA Enable
                                                                                                        
(SAEN)=0. SAEN is short for SA-Enable and SA-Enable
here); thus, SAL and SAR follow the voltage of BLL and
BLR. To balance the voltages of BLL, BLR, SAL and SAR,
they are shunted to each other by holding Bitline Equalizer
(BLEQ) signal high before time period 1. BLEQ, DCR, and
DCL signals are activated by VDH as well, similar to that
in conventional DRAMs. Since SAEN remains disabled in
Charge Sharing phase, SAL and SAR follow the voltage of
BLL and BLR, respectively, in time period 1 (see SAL, SAR,
BLL, and BLR voltage in the time period 1 in Fig.4). By
wordline activation, two scenarios may be considered:
Scenario I. Stored value in the accessed cell is equal to the
value on corresponding bitline, i.e. both are 0 or VDD. Here,
voltage perturbation due to charge sharing (VSPF ) is 0.
Scenario II. Stored value in the cell and connected bitline
differ. In this scenario, VSPF can be calculated by Eq. (5). In
this case, the amplitude of VSPF is twice the amplitude of
VSconv. (Eq. (1)). Thanks to higher voltage difference between
cell and connected bitline. The BLL waveform in time period 1
shows four possible voltage changes on the connected bitline
to the accessed cell. Since BLR is float in this time period, it
preserves its previous voltage, which is the same as BLL at
the beginning of time period 1.
VSPF = VDD. Ccell
CBL + Ccell
(5)
In conventional DRAMs, the starting voltage for sensing
in the SA is VDD/2 and VDD/2±Vs on SA nodes. Thus,
regardless of the cell being connected to bitline or bitline,
differential amplification of bitlines voltages determines the
output data. In conventional DRAMs, it is not acceptable to
have the same voltage on SAL and SAR when the SA is
activated. Since, in this condition, the SA output is determined
by ambient noise or random tiny mismatch between the SA
transistors strength, due to process variation or aging [44],
[46].
Accessing BLL or BLR, and four different possible starting
points on SAL (BLL) and SAR (BLR) leads to eight different
states that must be distinguished by PF-DRAM sensing circuit.
These states are shown in Table I.
Consider the case where the accessed cell is connected to
BLL. If SAL=SAR=0 at the starting point, it shows that both
cell and BLL contain ’0’. In this condition, the stabilization
point for SAL and SAR should be 0 and VDD, respectively.
If SAL=VSPF and SAR=0, it shows that the connected cell
to BLL contains ’1’. Thus, the correct stabilized voltages of
SAL and SAR should be VDD and 0, respectively, for correct
sensing. Similar logic is applicable to other rows of the table.
To achieve the expected stabilization points under different
conditions, the SA is imbalanced using SA Imbalancer circuit.
This circuit is composed of two tri-state NOT gates, activated by Boost Right (BOOSTR) and Boost Left (BOOSTL)
signals. When the charge sharing is completed between the
cell, BLL/BLR (BLL in Fig. 4), and the SA node (SAL in
Fig. 4), by the end of time period 1, BOOSTL/BOOSTR is
activated together with SAEN in time period 2. By activating
BOOSTL/BOOSTR, the activated tri-state NOT gate drive
SAR/SAL in parallel with one of the cross coupled NOT gates
in the SA. This constructs an SA which is more sensitive to
SAL/SAR voltage than SAR/SAL voltage. Thus, in Scenario
I, when SAL and SAR voltages are the same (VDD or 0),
the boosted side is dominant to determine the stabilization
point. For instance, consider the first line in Table I. Since
the accessed cell is connected to BLL, BOOSTL should be
activated. Thus, two PMOS transistors, one in the activated
tri-state NOT gate and the other in one of the cross coupled
NOT gates of the SA, start to pull SAR to VDD. This is
while, only one PMOS transistor in the sibling NOT gate in
the SA tries to pull SAL to VDD. In such a race condition,
two parallel PMOS transistors win and the stabilization point
is SAL=0 and SAR=VDD. The same explanation is applicable
to the case SAL=SAR=VDD for NMOS transistors. As Fig. 4
shows, SAL preserves its value in time period 2, while SAR
flips, when VSPF =0 (fine dashed red line and bold dashed black
line).
Together with activation of the SA, DCR and DCL are
disabled to disconnect the SA from bitlines. Since the parasitic
capacitance of SAL and SAR are much smaller than that of
the bitlines, the decoupled SA stabilizes much faster and more
power efficient than the time it should sense and drive bitlines
at the same time [48].
In Scenario II, if the voltage of the bitline connected to
the cell grows/drops to VSPF /VDD−VSPF , while the voltage of
reference bitline is 0/VDD, the single NOT gate in the SA is
the winner of the race with two parallel NOT gates. This is
because the input voltage of the single NOT gate is 0/VDD
and its drive current is higher than that of two NOT gates in
parallel with the input voltage of VSPF /VDD−VSPF . Thus, the
SA node with voltage VSPF /VDD−VSPF is pulled to VDD/0.
See SAL voltage change during time period 2 in Fig.4 (fine
dashed black line and bold dashed red line).
The controllable imbalance SA can detect all of the eight
possible starting point conditions during a DRAM cell access, by two control signals of BOOSTL and BOOSTR.
BOOSTL/BOOSTR exactly follows SAEN signal, while
BOOSTR/BOOSTL is connected to 0.
In conventional DRAMs, accessed cell is restored by driving
the connected bitline to the same value in the cell, while the
other bitline takes the opposite value. In PF-DRAM, both
bitlines are pulled to the same value stored in the accessed
cell, in Restoration phase. This is done by simultaneously
TABLE I: Possible starting points and expected stabilization points of the SA
for correct sensing in PF-DRAM
Starting Point Stabilized
SAL SAR BLL/BLR Access SAL SAR
0 0 BLL 0 VDD
VSPF 0 BLL VDD 0
VDD−VSPF VDD BLL 0 VDD
VDD VDD BLL VDD 0
0 0 BLR VDD 0
0 VSPF BLR 0 VDD
VDD VDD−VSPF BLR VDD 0
VDD VDD BLR 0 VDD

connecting BLL and BLR to SAL/SAR, by activating M3 and
M4 or M2 and M5 if BLL or BLR is connected to the cell,
respectively. For this aim, Voltage Updater Right (VUR) and
DCL or Voltage Updater Left (VUL) and DCR are activated
in time period 3. The decision on which pair activation can
simply be taken based on selection of odd or even wordlines.
With this configuration BLL and BLR are always
charged/discharged to same value and their voltage changes
only when the stored charge on accessed DRAM cell differs
from the charge remained on bitlines from previous Activation.
Since the boosted side of the SA drives the bitlines in
Restoration phase, and M2 to M5 are designed weaker than SA
transistors, connecting the bitlines to the SA cannot change its
state and only generates a small glitch in the connected SA
node at the beginning of time period 3 (if the value on bitlines
and the SA node vary). More explanation is given in Section V.
The Activation energy (on bitlines) for cell data restoration
is dissipated only when the previous value on bitline is ’0’ and
the current connected DRAM cell to that bitline contains ’1’ (0
to VDD transition). This energy can be calculated by Eq. (6),
where P0 →1 is the probability of ’0’ to ’1’ transition on a
bitline during Activation. The cell is restored in time period 3,
only if its value has been destroyed.
Eacc.PF = P0→1.Qact.V DD
= P0→1.((CBL + Ccell).(VDD − VSPF ) + CBL.VDD).VDD
= P0→1.((CBL+Ccell).(VDD−VDD. Ccell
CBL+Ccell
+VDD.CBL).VDD
= 2 × P0→1.CBL.VDD2 (6)
The Column Select Line (see Fig. 1) can be activated in
parallel with charge/discharge of bitlines at the beginning
of time period 3 (like the technique in [48]). This reduces
tRCD in comparison to that in conventional DRAMs, since in
conventional DRAMs, large parasitic capacitance of bitlines
should first be charged/discharged to start tCL.
The row activation policy of PF-DRAM is open-row.
Thus, time period 4 in Fig. 4 is extended as long as no
Read/Write/Refresh occurs or all accesses hit the open row.
In PF-DRAM, in the case of closing a row, only SAL
and SAR are needed to be equalized, which is performed
by deactivating SAEN and BOOSTL/BOOSTR, and activating
(holding active) DCL, DCR, and holding BLEQ active. This
operation equalizes the voltage of internal nodes in the SA to
the same value on the floating bitlines in time period 5. Since
the parasitic capacitance of SAL and SAR are much smaller
than that of bitlines, the equalization latency (less than 100ps)
and consumed energy are negligible. After equalizing the SA
internal nodes, PF-DRAM is ready for the next row activation
with no need for precharge of bitlines.
PF-DRAM is not based on data similarity in DRAMs. Even
if the stored values in DRAM cells are completely random,
the probability of ’0’→ ’1’ transition on bitlines, during an
Activation, is 25%. In this case, Eacc.(PF ) = 0.64×Eaccconv. .
This is while a high degree of data similarity (low probability
of ’0’→’1’ transition on bitlines) exists in accessed data.
To the best of our knowledge, PF-DRAM is the first DRAM
design with no need to Precharge phase. The main privileges
of PF-DRAM are as follows:
• In PF-DRAM, a bitline flips only when the previous and
recent connected cells to that bitline contain different
values. While in conventional DRAMs, bitlines have the
same amount of activity regardless of accessed values.
• tRP is removed from access latency in PF-DRAM.
• In PF-DRAM, tRCD is reduced by removing the load of
charge/discharge of bitlines capacitors from SA.
• VDD/2 voltage source is removed in PF-DRAM.
• Since PF-DRAM is proposed based on conventional
DRAMs, the modifications to popular DRAM structure
is minimal and cell arrays remain unchanged.
• PF-DRAM can be applied to all JEDEC DRAMs and
work in parallel with previously proposed power/energy
reduction techniques presented for DRAMs.
C. Peak Power and Refresh Rate
The extreme condition for PF-DRAM power consumption is
when the value on each bitline flips in consecutive accesses.
It may occur if an application (test or Trojan) intentionally
stores inverse values in DRAM rows and access them with a
specific stride such that each access leads to a row miss. In
this condition, the peak dissipated energy in the bitlines by
PF-DRAM will be 29% higher than a conventional DRAM
(see Eq. (6) and Eq. (4)). Thus, peak power constraints of PFDRAM must be considered for the rare worst-cases to reduce
voltage drop on power rails and substrate coupling noise.
The worst-case DRAM non-selected cells leakage, occurs
when a non-selected bitcell capacitor is charged to VDD and
corresponding bitline is connected to 0. In this condition, the
P-N junction leakage to the substrate and sub-threshold access
transistor leakage discharge the cell in parallel. This situation
is the same for conventional DRAM with open-row policy as
well, thus, PF-DRAM does not affect the refresh rate.
IV. EXPERIMENTAL SETUP
We evaluated PF-DRAM in single-core, 2-core, 4-core,
and 8-core systems (each core with out-of-order X86 (64-
bit) architecture equipped with private L1 and shared L2
caches) summarized in Table II. At system-level, we utilized
gem5 full-system simulator [10] to extract memory footprint
and DRAM energy dissipation during execution of 16 randomly selected workloads from SPEC CPU2017 [3] and five
randomly selected workloads from PARSEC 2.1 benchmark
suites. One billion instructions from each workload have
been fast-forwarded for warm-up phase and the next 500
million instructions were simulated in detailed mode. The
SPEC CPU2017 workloads are executed in System-Emulation
mode of gem5 simulator. For n-core evaluation, 16 workloads
are randomly divided to the sets each with n workloads
and executed on a n-core system. PARSEC 2.1 multi-thread
workloads are executed in Full-System mode.
To calculate the probability of ’0’→’1’ transitions (P0→1)
on bitlines, we extracted the trace of read/written data from/to
DRAM memory in a compressed debug file and analyzed it.

TABLE II: Simulation environment and configuration
Architecture-level parameters
Processor Single/Multi (2, 4, and 8)-core, out-of-order,
4 issue, @ 3 GHz, X86, 64-bit
L1-cache private for each core 32 KB, 4-way set
associative, 64 B line size, 2 cycle latency, for
instruction and data
L2-cache shared between the cores 2 MB, 8-way set
associative, 64 B line size, 8 cycle latency
Main memory 8 GB DDR4-2400, 16 Chip per DIMM,
2 Channel, 2 Rank/channel, 16 Bank, 8 KB rowbuffer size, Open-row, 64 ms Refresh interval
tRCD, tCL, tRP 14.2/10.8 ns, 14.2 ns, 14.2 ns/0.8 ns
Conv. DRAM/PF-DRAM
Workloads
Integer suite
(SPEC-CPU2017)
deepsjeng, leela, mcf, omnetpp, perlbench,
x264, xalancbmk, xz
Floating-point suite
(SPEC-CPU2017)
bwaves, cam4, imagick, lbm, nab, namd,
povray, wrf
Multi-thread suite
(PARSEC2.1)
bodytrack, canneal, ferret, fluidanimate, stream
Circuit-level parameters
Technology 16 nm Predictive Technology Model
Operating Voltage Chip power supply (VDD) = 1.2 V
Activating power supply (VPP) = 2.5 V
MAT size 512×512 cell
CBL, Ccell 144 fF, 24 fF
Transistors W/L SA 10/1, M1∼M5 2/1
Process Variation σ = 20%, m = Vth, Capacitance, Resistance
An 8 GB DDR4-2400 DRAM is considered for main memory, arranged in 8 chip per DIMM (Dual In-line Memory Module), 4 ranks per 2 channels, 8 devices per rank, and 16 banks
per rank. The row-buffer size is 8 KB. Page interleaving is in
the order of row, bank, rank, memory channel, and column
from the most to least significant address bits [48]. tRCD,
tCL, and tRP all take 17 clock (14.16 ns) in the conventional
DRAM. While tRCD and tRP are reduced to 13 clock (for
wordline activation, charge sharing and SA stabilization) and
1 clock (10.8 ns and 0.8 ns), respectively, in PF-DRAM.
For circuit-level simulation, we used 16 nm Multi-Gate technology model in HSPICE circuit simulator. The width/length
(W/L) ratio of the SA transistors and M1 to M5 are 10/1 and
2/1, respectively [16]. MAT size is considered 512×512 cells,
a common organization for the current DRAMs [12].
Activating power source (VPP) voltage is 2.5 V. The drawn
current from VPP in different operations of DRAM (IPPx) [2]
is the same for PF-DRAM and conventional DRAM.
V. EVALUATION
In this section, first the correct functionality of PF-DRAM
and its robustness are evaluated with circuit-level simulation,
after which P1→0, memory access latency and its effect
on Instruction Per Cycle (IPC) are extracted. At the end,
power/energy consumption improvement of PF-DRAM is evaluated in comparison with the conventional DRAM structure.
Almost all of the previously proposed DRAM power/energy
reduction techniques can be applied in parallel with PF-DRAM
for further power/energy reduction (see Section VI).
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Cell Voltage (V)
BL=VDD, Cell=VDD
BL=VDD, Cell=0
BL=0, Cell=VDD
BL=0, Cell=0
0.0
0.2
0.4
0.6
0.8
1.0
1.2
BLL Voltage (V) 0.0
0.2
0.4
0.6
0.8
1.0
1.2
BLR Voltage (V) 0.0
0.2
0.4
0.6
0.8
1.0
1.2
SAL Voltage (V) 0.0
0.2
0.4
0.6
0.8
1.0
1.2
0 2 4 6 8 10 12 14
SAR Voltage (V)
Time (ns)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
2.98 3.00 3.02 3.04
0.0
0.2
0.4
0.6
0.8
1.0
1.2
2.98 3.00 3.02 3.04
Fig. 5: Accessed cell, bitlines, and the SA internal nodes voltage waveform
during Activation phase
A. PF-DRAM Functionality
For accurate power and delay evaluation of bitlines, we utilized Lumped-element model, with 512 segments on bitlines.
The total capacitance of bitline and cell is 144 fF and 24 fF,
respectively, in our evaluation.
Fig. 5 shows the accessed cell, bitlines, and the SA internal
nodes voltage waveform during Activation phase. The simulation results are presented for connecting the cell to BLL. As
the figure shows BLL, BLR, and cell voltage only changes if
the stored charge in the cell and connected bitline (BLL) are
different. The cell voltage changes by about 1020 mV during
sharing its charge with the bitline, when bitline contains the
opposite value, and restores again when the SA stabilized and
connected to the bitlines (at 3.5 ns).
To evaluate noise effects of charges on SAL and SAR from
the previous Activation (before 0.5 ns), we considered opposite
values (with respect to connected bitline) on both of them. By
connecting the cell and the SA to bitlines at time 0.5 ns, SAL
and SAR follow the voltage on BLL and BLR, respectively,
with a small delay. This delay is due to RC propagation latency
of long bitlines. For the worst-case evaluation, the cell is
connected to the furthermost point of bitline to the SA.
By activating the SA, based on the voltage on SAL and
SAR, the SA is stabilized in less than 100 ps. The race between
SAL and SAR leads to a glitch on the SA internal nodes, which
is smaller than 433 mV in the worst-case.
As simulation results show, by connecting SAL and SAR to
BLL and BLR at 14 ns, the voltage of BLL and BLR slightly
drops/increases due to sharing the charge between the SA
nodes and them. However, since SAL, SAR, BLL and BLR

D E
F G
H I
Fig. 6: Conventional DRAM (a and b) and PF-DRAM (c, d, e, and f) SA
stabilization at the presence of PV
have the same voltage, this glitch does not affect the next cell
access. We even observed in the case that SAL and SAR were
not equalized, the SA works correctly. The generated glitch on
the SA node which updates bitlines (BLL in this case at 3.5 ns)
is less than 160 mV in worst-case. This glitch diminishes by
charging/discharging the bitlines through SA transistors.
B. PF-DRAM Robustness
1) Process Variation: We considered transistors threshold
voltage drift due to the process variation (PV) [25], as an effective factor on the SA robustness. The Gaussian distribution
for PV modeling is considered, which randomly drifts DRAM
transistors (access transistors and the SA transistors) threshold
voltage, cells capacitance and bitlines parasitic resistance and
capacitance with a mean value of m (nominal value of each parameter) and standard deviation of σ (Vth(m, σ)). The Monte
Carlo simulation method with 2000 iterations is considered to
extract the number of faulty cell accesses for PF-DRAM and
the conventional DRAM.
An extreme value of σ = 20% (σ is about 15% in recent
technology) is considered to trigger more faults. The complete
bitline decoupling scheme [21], [48], [52] is considered for the
conventional DRAM for a fair comparison.
Fig. 6a and b show the SA output (SAL node in this
case) of the conventional DRAM with PV, when the accessed
cell charge is 0 and VDD, respectively. With the considered
extreme PV, 13 and 10 faulty cell accesses occurred in 4000
simulation rounds (0.58%). Fig. 6c and d show PF-DRAM SA
stabilization when bitlines charge is 0 and accessed cell charge
is 0/VDD. In this case, the number of faulty accesses is 9/4
out of 4000 simulation rounds (0.33%). When bitlines initial
charge is VDD (Fig. 6e and f), the number of faulty accesses
become 7/7 (0.35%). Thus, the average fault rate of PF-DRAM
is 0.34% for accessing random values stored in DRAM.
The higher robustness of PF-DRAM to PV in comparison
with conventional DRAMs is due to the redundancy in SA
imbalancer circuit, amplifying the output of the SA in parallel.
By increasing the SA area and number of transistors, process variation effect especially random fluctuations decreases
[41]. Our evaluations show that by duplicating conventional
DRAM’s SAs, the PV effect decreases on them as well.
2) Noise Immunity: PF-DRAM uses differential sensing
scheme of bitlines, like conventional DRAMs, to detect tiny
perturbation on bitlines during charge sharing phase. We evaluated the noise margin (smallest noise which leads to a faulty
access) of PF-DRAM and conventional DRAM cell accesses
by sweeping the voltage of a noise voltage source located
between bitlines and SA. Our results show the noise margin
of the conventional DRAM is 85 mV for reading ’0’ and ’1’.
This value is 85 mV and 90 mV for PF-DRAM when the
accessed cell and bitlines carry the same and different charges,
respectively. As results show, PF-DRAM noise margin is even
slightly higher than the conventional DRAM.
PF-DRAM is as robust as the conventional DRAM against
various internal DRAM noises like Wordline Drive Noise
and Power-Supply Voltage Bounce. In PF-DRAM, the only
interference which may be more error prone in the worstcase, include bitline-to-bitline and bitline-to-wordline coupling
noises, which are larger than the conventional DRAM due to
the larger bitlines swing amplitude.
In worst-case scenario, in which all bitlines in a MAT are
raised from VSPF (close to 0) to VDD, a pulse is generated
on inactive wordlines. This pulse is larger than that in the
conventional DRAMs, as the voltage swing on bitlines in
a conventional DRAM is about VDD/2. However, since the
restoration phase (BL update in Fig. 4) can be performed in
parallel with data access, BL can be updated gradually using
weaker updater transistors (M2-M5 in Fig. 4). By decreasing
the speed of voltage change on bitlines, the generated bitlineto-wordline noise can be controlled. On the other hand, since
PF-DRAM reduces voltage swing probability on bitlines, the
generated noise on wordlines (selected and non-selected) is
expected to be low.
The bitline-to-bitline interference is divided in two categories: during charge sharing and during bitlines update. The
ratio of noise-to-signal due to bitline-to-bitline interference
during charge sharing, is the same as the conventional DRAM
(larger Vsignal, larger Vnoise). On the other hand, since
in PF-DRAM the bitlines are decoupled from SAs during
amplification, PF-DRAM is robust against bitline-to-bitline
updating noise.
C. DRAM Bitline-Flip Probability
The average DRAM row-hit rate and bitlines-flip probability
by execution of SPEC CPU2017 and PARSEC2.1 is presented
in Fig. 7. By changing the system configuration from singlecore to 8-core, the row-hit rate decreases from 73.8% to 12.4%
in SPEC CPU2017. This is due to the drastic reduction in
locality of references of memory access patterns, especially
when different workloads are executed in parallel. The rowhit rate reduction in PARSEC 2.1 is less by increasing the

Fig. 7: DRAM Row-hit rate and average bitline-flip probability for SPEC-CPU2017 and PARSEC2.1 workloads
number of cores from 2 to 8 (from 77.5% to 41.6%), which
shows higher locality of references in multi-thread executions
of PARSEC 2.1 workloads in comparison with multi-program
execution of SPEC CPU2017 workloads.
The average of DRAM bitlines-flip probability for
SPEC CPU2017 benchmark suite executed on a single-core
system is 9.1%. The namd and x264 workloads have the highest and lowest bitline-flip rate of 19.5% and 3.9%, respectively.
By raising the number of simultaneously executing workloads (threads) from 1 to 2, 3, and 8, bitline-flip probability increases by 1.4%, 3.8%, and 6.9% for SPEC CPU2017. This is
due to concurrent accesses to main memory by different workloads that reduce dependency in the accessed data. However,
because of high rate of narrow-width values, large number of
’0’ bits, and low hamming distance between frequently-used
instruction opcodes, bitline-flip probability does not increase
much. The average bitline-flip probability in PARSEC 2.1
increase with a lower rate than that for SPEC CPU2017 when
increasing the number of executing threads in each workload.
This is because the data similarity is higher in multi-thread
execution of a single workload than simultaneous execution
of diverse workloads.
D. Performance Improvement
The locality of references has decreased in recent workloads, and consequently the row-hit rate is also reduced.
Furthermore, by rising the number of simultaneous executing
cores/threads, which is the main trend in recent years, row-hit
rate reduces considerably [56].
Closing a row, due to a row-miss in open-row policy or a
refresh command, is performed by deactivating the open row
and precharging bitlines in conventional DRAMs. In close-row
policy, however, a Precharge operation is performed after each
DRAM access, which imposes extra power overhead, but can
reduce latency when row-hit rate is low.
Fig. 8 shows average memory access latency and its effect
on IPC of different workloads with different system configurations for PF-DRAM and conventional DRAMs. The average
single-core memory access latency of SPEC CPU2017 benchmark suite is improved by 2.1 ns (7.8%). The improvement
grows to 3.9 ns, 8.3 ns, and 28.8 ns (11.5%, 15.2%, 26.3%),
for 2, 4, and 8-core execution, respectively. Memory access
latency reduction also improves when the number of threads in
PARSEC 2.1 is increased. bodytrack shows the highest memory access latency improvement of 17.1% in PARSEC 2.1.
The Relative IPC improvement of PF-DRAM compared
to the conventional DRAM structure grows when row-hit
rate decreases, in general. The effect of memory access
latency reduction on IPC improvement is more visible in
multi-core/thread configurations and the workloads with higher
memory access rates, like cam4, mcf, omnetpp, and x264 with
the memory access (read/write) rate of over 30%. Furthermore,
the IPC of workloads with lower cache hit-rates benefit more
from PF-DRAM. The highest IPC improvement rate of 15.1%
for PF-DRAM is observed in SPEC CPU2017 executing on
the 8-core configuration.
E. Power/Energy Consumption
Five major components in DRAM power consumption are
the dissipated power for: Activation, Read/Write, I/O Termination, Refresh, and dissipated power in background [2]. In
short, DRAM’s consumed power during workload simulation
is calculated based on power consumption of different operations, extracted from data-sheets, and derating the power
of each component based on the command scheduling in the
system (gem5 power model).
PF-DRAM reduces the current drawn from VDD power
source for Activation (IDD0) and Refresh (IDD5B). See [2] for
more details on DDR4 power calculation. IDD0 and IDD5B
are constant for conventional DRAMs, while they depend to
bitliens-flip rate of each workload and are calculated based on
the average P0→1 on bitlines of DRAM MATs. The ratio of
power reduction for a workload (RWL) is calculated by Eq. (7).
RWL = EaccPF
Eaccconv.
(7)
IDD0 and IDD5B contain background power (IDD3N when
any bank is active) as well, thus, since PF-DRAM only reduces
dynamic power consumption, IDD0 and IDD5B of PF-DRAM
for each workload (IDD0PF (WL) and IDD5BPF (WL)) can be
calculated by Eq. (8) and Eq. (9), respectively.
IDD0PF (WL) = RWL.(IDD0 − IDD3N ) + IDD3N (8)
IDD5BPF (WL) = RWL.(IDD5B − IDD3N ) + IDD3N (9)
By updating the new values for IDD0 and IDD5B in gem5
power model, the power consumption of DRAM is calculated
by re-execution of the workload. Fig. 9 depicts components
of conventional DRAM and PF-DRAM power consumption
values, and overall power reduction by PF-DRAM. When the
number of executing cores/threads increases, DRAM power
consumption grows due to more memory accesses, in general.

Fig. 8: Average memory latency and relative IPC improvement for SPEC-CPU2017 and PARSEC2.1 workloads
Fig. 9: Different components of power consumption for SPEC-CPU2017 and PARSEC-2.1 workloads
The average Precharge+Activation power consumption in
a conventional DRAM during execution of SPEC CPU2017
workloads on single-core, 2-core, 4-core, and 8-core system
configurations is 243 mW, 574 mW, 1224 mW, and 2825 mW,
respectively. These values reduces to 37 mW, 109 mW,
301 mW, and 877 mW, respectively, in PF-DRAM, which show
huge average reduction of 6.4x, 5.2x, 4.0x, and 3.2x of bitlines
dissipated power, respectively.
Having more cores/threads decreases row-hit rate, which
increases the share of Precharge/Activation operations in overall DRAM power consumption. Such a scenario boosts PFDRAM power reduction, while it also increases bitlines-flip.
For PARSEC 2.1 benchmark suite, bitlines-flip rate is affected less by increasing the number of parallel executing
threads. Thus, Activation power reduction gain does not
change much for this benchmark for different number of
threads. This is due to similarity in stored data of workload.
Dissipated power due to Refresh is almost constant for a
conventional DRAM during execution of different workloads
running on various number of cores. However, PF-DRAM
Refresh power consumption depends to the stored values in
DRAM. Higher similarity of values in DRAM, decreases
refresh power. In the best case, refresh power decreases by
17.2x for x264 workload. The average refresh power reduction
for all workloads and all configurations is about 6.9x.
The dissipated energy by Read/Write peripherals and I/O
terminations is almost the same for PF-DRAM and conventional DRAM. However, since IPC improves in PF-DRAM, the
dissipated power for Read/Write and I/O termination increases
due to the higher main memory access rate.
The overall power reduction of PF-DRAM for single-core,
2-core, 4-core, and 8-core executions of SPEC CPU2017 is
38.5%, 42.4%, 43.8%, and 44.1%, respectively. The lowest
and highest improvements are 21.5% and 54.2% for cam4 and
bwaves workloads, respectively.
The average power reduction for all configurations executing PARSEC 2.1 workloads is 28.8%. The lowest improvement
is for fluidanimate workload which is 16.7%, 12.6%, and
14.2% for 2-core, 4-core, and 8-core executions, respectively.
The higher row-hit rate and bit-flip probability of this workload
are the main reasons for the less power reduction.
The average power–delay product (PDP) of PF-DRAM
improvement is more than its power reduction because of
simultaneous improvement of IPC and power consumption.
The average PDP of PF-DRAM decreases by 43.0% and
31.3% in average for SPEC CPU2017 and PARSEC 2.1,
comparing with conventional DRAM.
F. Area Overhead
The largest portion of DRAM area is covered by DRAM
cells. The share of the SAs area in conventional DRAMs
is about 8% [48], [58]. The added SA Imbalancer circuit
in PF-DRAM is similar to the SA. M1 to M3 are already
available in conventional DRAMs for precharging and bitlines
equalization. The area overhead of decoupling transistors M4
and M5 are about 0.8% [48]. Therefore, the overall area
overhead of PF-DRAM is about 8.8% with respect to the
conventional DRAM. It is worth mentioning that the used
transistors in the SA Imbalancer have the smallest W/L ratio,
while the W/L ratio of the SA transistors is at least 10/1. Thus,
the occupied area by the SA Imbalancer can reduces much less
than the SA area (8%), which further reduces the overall area
overhead of PF-DRAM.
G. Architectural Optimization
Regarding the PF-DRAM circuit-level improvements in
comparison with the conventional DRAMs, the following
architecture to system level optimizations can be considered
to intensify PF-DRAM effectiveness:

• Data arrangement: By arranging the data of an application, considering DRAM banks structure, the probability
of bitline-flip can be minimized.
• OS optimization: Optimizing memory paging in the OS
to reduce bitline-flip probability.
• Memory access scheduling: By eliminating the Precharge
phase from internal states of DRAM banks, MC can be
simplified (less states and shorter reordering queue).
• DRAM page size reduction: Regarding the fact that rowhit rate is decreasing in recent workloads (especially
in multi-thread and multi-program systems), and since
opening a new row in PF-DRAM is less costly than
the conventional DRAMs, PF-DRAM page size can be
reduced for further energy gain.
• Lower required time for refreshing: Refresh time is
reduced, thus the memory is more available to serve CPU
memory access requests.
VI. RELATED WORK
Most previous proposals on DRAM power/energy reduction
are orthogonal to PF-DRAM. This is because PF-DRAM
eliminates Precharge phase of subarrays, while other memory
access/refresh operations exist in conventional DRAMs. Thus,
these techniques can be applied to PF-DRAM as well, for
further power/energy reduction. In this section we briefly
enumerate these techniques and then discuss the techniques
which can affect PF-DRAM efficiency in more detail.
State-preserving and non-state-preserving low-power
DRAMs [7], [12]: By utilizing multi-banking feature of
DRAMs, these techniques put some of DRAM banks in lowvoltage or gate their power. Data arrangement, application behaviour, address mapping, and number of independent memory
banks are effective on the efficiency of these techniques.
Activating smaller sections of memory [13], [14], [26], [33],
[56], [59], [61]: By partitioning the memory with granularity
of wordlines/bitlines up to memory banks, or hierarchical
organization of DRAM banks, a smaller part of memory can
be activated in each access to reduce power dissipation.
Exploiting data similarity/correlation for low-power data
transmission [27], [28], [38], [47]: By reduction of toggling
in chip I/O, the dissipated power in I/O drivers and termination
resistors can be reduced.
DRAM refresh energy reduction [6], [9], [23], [24], [30],
[31], [37], [42], [57]: Deciding on refresh rate based on the
extreme tail cell of DRAM, increases power consumption
and results in performance degradation. Thus, some previous
proposals categorize different rows with different retention
times and perform different refresh rate on them.
The techniques which are not completely orthogonal to PFDRAM are the techniques based on 1) Sensing Structures Optimization, 2) Stored Data Encoding, and 3) Row-Hit Boosting.
Sensing Structures Optimization: The evaluations in [48]
show precharging bitlines before the next Activation, dominates deactivation time. Thus, Row-Buffer Decoupling (RBD)
technique [48] decouples bitlines from the SAs right after a
row is opened and the DRAM cell values are restored (tRCD).
Thus, by early precharging of bitlines before closing a row,
tRP period can be overlapped by the time the opened row is
accessed. The early precharging policy [48] not only does not
decrease the dissipated energy per bit access, but also incurs
extra bitlines precharges in write operations. Closed yet Open
DRAM [52] simultaneously performs Read and Precharge
operations by bitline isolation as well; however, Precharge
latency of SA is decreased in comparison with RBD technique.
All previously proposed DRAM access/refresh circuits need
a Precharge phase of bitlines to either VDD/2 or VDD.
Stored Data Encoding: Power reduction techniques in this
category exploit possible lossless data encoding/compression
to increase effective DRAM capacity [5], [15], [35], [40].
Most of such techniques are variations of Frequent Pattern
Compression (FPC) and Zero-Content compression schemes.
Since a large portion of DRAM cells contain ’0’, simple zeroaware data compression techniques can compress application
data by 10%-90%, for different workloads [15]. However,
these techniques suffer from high data access cost. The main
drawback of these techniques is that the decompression latency
is added to DRAM latency. Furthermore, caching and virtual
to physical page mapping need an extra layer of indirection,
with further added complexity and memory latency [39], [55].
PF-DRAM is not dependant on data similarity and reduces
DRAM power with even fully-random data. However, data
similarity boosts power consumption gains in PF-DRAM.
Since data compression can decrease data similarity in DRAM,
by applying data compression the efficiency of PF-DRAM
might reduce for some workloads.
Row-Hit Boosting: Higher row-hit rates lead to lower DRAM
access latency and power consumption. Thus, some proposals
focus on reducing row-conflict rate, by predicting DRAM
access patterns [51]. The technique presented in [51] uses
scoreboarding to detect the rate of locality of references and
decide to close a row or keep it open. Breaking a large rowbuffer into multiple smaller row-buffers improves row-hit as
well with a considerable hardware overhead [17]. Utilizing
multi-row buffers and caching high-frequently accessed rows
(hot rows) increase row-hit, by adding row-buffers in the
base layer of the stacked DRAM [32], [60] and conventional
DRAM [54], or by exploiting larger I/O buffers [20], [43].
There are some studies that increase row-hit by memory
address redistribution, using traffic reshaping or changing the
mapping of physical address bits to the index of rank, bank,
row, and column [49]. Open-row policy exploits row-hit to
reduce DRAM access power consumption; however, it may
imposes performance loss for memory access patterns with
low locality of references, especially in multi-core systems.
VII. CONCLUSIONS
DRAM latency and power consumption play a significant role in overall system performance and dissipated
power/energy. The main power-hungry and sluggish operations
in DRAMs are: Precharge and Activation. In Precharge phase,
all the bitlines in a subarray are precharged to VDD/2 to set a
starting point for the sense amplifiers that differentially detect

a tiny signal on bitlines associated to each cell. In conventional
DRAMs, regardless of the previous voltage on bitlines, the
same amount of energy is dissipated in Precharge and Activation phases due to the extra charge/discharge phases.
In this paper, we proposed a DRAM structure, called
Precharge-Free DRAM (PF-DRAM), where the bitlines flip
only when the accessed cell value differs from the previous
value during the last cell access via those bitlines. Hence, PFDRAM performs better than the conventional DRAM in terms
of power consumption and performance.
The experimental results show an average of 35.3% reduction in DRAM power consumption achieved by PFDRAM compared to the conventional DRAM. Furthermore,
PF-DRAM improved the overall system performance by up
to 24.3%. All of these improvements are achieved for a
reasonably low area overhead of about 8.8%.