Abstract‚ÄîDeep Neural Networks (DNN) are used in
a variety of applications and services. With the evolving
nature of DNNs, the race to build optimal hardware
(both in datacenter and edge) continues. General purpose multi-core CPUs offer unique attractive advantages for DNN inference at both datacenter [60] and
edge [71]. Most of the CPU pipeline design complexity
is targeted towards optimizing general-purpose single
thread performance, and is overkill for relatively simpler, but still hugely important, data parallel DNN inference workloads. Addressing this disparity efficiently
can enable both raw performance scaling and overall
performance/Watt improvements for multi-core CPU
DNN inference.
We present REDUCT, where we build innovative
solutions that bypass traditional CPU resources which
impact DNN inference power and limit its performance. Fundamentally, REDUCT‚Äôs ‚ÄúKeep it close‚Äù policy enables consecutive pieces of work to be executed
close to each other. REDUCT enables instruction delivery/decode close to execution and instruction execution
close to data. Simple ISA extensions encode the fixediteration count loop-y workload behavior enabling an
effective bypass of many power-hungry front-end stages
of the wide Out-of-Order (OoO) CPU pipeline. Per
core performance scales efficiently by distributing lightweight tensor compute near all caches in a multi-level
cache hierarchy. This maximizes the cumulative utilization of the existing architectural bandwidth resources
in the system and minimizes movement of data.
Across a number of DNN models, REDUCT achieves
a 2.3√ó increase in convolution performance/Watt with
a 2√ó to 3.94√ó scaling in raw performance. Similarly,
REDUCT achieves a 1.8√ó increase in inner-product
performance/Watt with 2.8√ó scaling in performance.
REDUCT performance/power scaling is achieved with
no increase to cache capacity or bandwidth and a mere
2.63% increase in area. Crucially, REDUCT operates
entirely within the CPU programming and memory
model, simplifying software development, while achieving performance similar to or better than state-ofthe-art Domain Specific Accelerators (DSA) for DNN
inference, providing fresh design choices in the AI era.
I. Introduction
New data-centric paradigms of compute have made
machine learning (ML) and Deep Neural Networks (DNN)
pervasive in all fields of human endeavor. The race to
build hardware for DNN execution continues with custom Domain Specific Accelerators (DSA), programmable
FPGAs, and general-purpose GPUs and CPUs all throwing their hats in the ring [63] [39]. The goodness of
these hardware platforms is judged on multiple parameters including performance (throughput/latency), power,
area, cost, deployment form-factor, software ecosystem
and programmability. While each hardware platform has
innate goodness on a subset of these parameters, none are
currently optimal on all of them. Industry and academic
research and development continues for each of these
hardware platforms to improve their offerings on all fronts.
CPU vendors continue to invest on both the hardware
and software fronts towards improving DNN inference.
Both Intel and Apple have announced new compute directly targeting matrix operations through Advanced Matrix Extension (AMX) [12], [27]. In fact, despite having
a dedicated Neural Accelerator, Apple A13 Bionic added
AMX units in the CPU Lightning cores [27] by extending
the ARMv8.4 ISA. Furthermore, additional support for
multiple data formats like int8 [2] and BF16 [11] have been
added to the CPU to stay abreast of DNN algorithmic
research. High performance libraries from CPU vendors
are continuously being developed and tuned to extract
maximum performance from new CPU compute. For example, Intel‚Äôs MKL-DNN library achieved up to 198X
performance improvement [3] on state-of-the-art CPU
configurations. Smart algorithms [35] and co-processor
solutions like Centaur [47] push the CPU frontier further.
CPUs currently constitute a significant share of the
compute resources employed in industry for DNN inference. The wide prevalence of CPUs already in datacenters
(for example, at Facebook) provisioned for peak load
levels in conjunction with diurnal load cycles, leads to
the abundant availability of free CPU compute for DNN
inference [60], [71]. The programmable general-purpose
nature of CPUs with their rich and mature ecosystem of
tools and programming models enable quick development
and deployment [73]. CPUs also offer latency benefits (see
[44], [74] and MLPerf Inference Results [17]), since they
don‚Äôt rely on a driver-offload for DNN tasks, enabling a
close interaction between DNN and non-DNN tasks in realtime inference. DNN tasks with limited parallelism, like
Recurrent NNs, fit more naturally to CPUs with higher
clock frequencies [76].

"$.*&&&UI"OOVBM*OUFSOBUJPOBM4ZNQPTJVNPO$PNQVUFS"SDIJUFDUVSF	*4$"

¬•*&&&
%0**4$"
2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) | 978-1-6654-3333-4/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ISCA52012.2021.00022
Main Memory (DRAM/HBM)
Core
L1 $
L2 $
L3 $
Performance 
High Compute Eciency
High Reuse  High L1 HitRate
 Spare L2, L3 Bandwidth
Power
Dominated by
OOO Core Pipeline
Main Memory (DRAM/HBM)
L1 $
L2 $
L3 $
Core
Performance 
Low Compute Eciency
 Low ROI on Using Small L1
 Spare L2, L3 Bandwidth
Power
Primarily in OOO Core Pipeline
& Cross-Cache Data Movement
COMPUTE
SOURCES OF
BANDWIDTH
SOURCES
OF POWER
(a) High Ops/Byte Matrix-Matrix Primives (e.g. Convoluon) (b) Low Ops/Byte Matrix-Vector Primives (e.g. InnerProduct)
+ +
Fig. 1. Modern CPUs with multi-level memory hierarchies and how inference primitives use them
Wide and deep OoO CPU pipelines, designed to expose Instruction Level Parallelism (ILP) for single thread
performance draw significant power. All instructions are
unrolled in hardware and flow through everyone of these
power hungry pipeline stages. All compute is placed
‚Äúmonolithically‚Äù atop a serially accessed multi-level cache
hierarchy designed to minimize average load latency. All
loads and stores must go through the L1 cache, thus
restricting it to be the primary source of bandwidth.
There is a stark disparity in CPU requirements for
single-thread performance with limited ILP and DNNinference primitives like convolution or inner-product
that are heavily data-parallel. DNN performance is governed by raw compute throughput and a required data
throughput (bandwidth) to feed it. Generational compute
scaling in CPUs is achieved via both intra-core scaling [2],
[29] and multi-core scaling. The required bandwidth to
feed the compute however, depends on primitives themselves; the higher the compute intensity (Ops/Byte) the
lower the bandwidth required, and vice-versa. As DNN
usages and topologies evolve, there is an increasing heterogeneity in their Ops/Byte (Compute/Bandwidth) requirements (see Section II).
The monolithic core-centric CPU organization results in
sub-optimal performance, resource utilization and power
when executing DNN primitives with diverse Ops/Byte
requirements. Matrix-Matrix primitives (like convolution)
illustrated in Figure 1(a) typically have high Ops/Byte.
High reuse and hit-rate in the L1 cache delivers sufficient
bandwidth for current compute resources. However, due to
serialized accesses through the hierarchy, further compute
scaling will hit a L1 cache bandwidth limit, despite L2
and L3 cache bandwidths being heavily underutilized.
Matrix-Vector primitives (like inner-product), shown in
Figure 1(b) have much lower Ops/Byte. Low hit-rates and
bandwidth from small L1 caches delivers insufficient bandwidth for current compute resources. Serializing through
the hierarchy unnecessarily introduces a L1 bottleneck
despite larger L2 and L3 caches (with better hit-rates)
being heavily underutilized in bandwidth. CPU execution
unrolls every instruction instance through all pipeline
stages. In particular wide fetch, decode, rename (RAT)
and dispatch (RS) stages designed to expose ILP are
extremely power hungry, raking up as much as 59% of
total power consumed (see Section II-B4). Much of the
hardware complexity in these stages isn‚Äôt useful for highly
repetitive data-parallel, fixed iteration count DNN kernels.
We present REDUCT, where we employ a ‚ÄúKeep it
close‚Äù policy that enables consecutive pieces of work to be
executed close to each other. REDUCT enables instruction
delivery/decode close to execution and instruction execution close to data. REDUCT proposes simple ‚ÄúREDUCT
Support Extensions‚Äù (rSX) ISA extensions to encode the
structured loop-y workload behavior. This allows an effective bypass of many power-hungry stages of the wide OoO
CPU pipeline into light-weight Tensor Functional Units
(TFU). The core does fewer fetches and decodes, with a
bulk offload of decoded tensor work (load/store/compute)
to the TFUs, keeping instruction delivery/decode close
to execution. The TFUs are distributed near all caches
in a multi-level cache hierarchy, unshackling the binds of
serialization through the hierarchy. This allows bypassing
inner cache-levels as required, keeping instruction execution close to data. REDUCT‚Äôs near-cache approach maximizes the cumulative utilization of existing architectural
bandwidth resources in the system demanding no increase
in cache capacity or bandwidth. Crucially, REDUCT operates entirely within the CPU programming and memory
model, supporting existing virtual memory and hardware
coherence along with hardware memory consistency and
distribution of work to all compute by leveraging Simultaneous Multi Thread (SMT) CPU capabilities.
We make the following key contributions in this paper
that address the disparity in requirements for single thread
performance vs DNN workloads.
‚Ä¢ We present REDUCT, where we distribute lightweight Tensor Functional Units, near each level of
cache. REDUCT maximizes efficient utilization of the
existing cumulative bandwidth in the system and
minimizes data movement. REDUCT adds minimal
additional hardware with no increase in cache capacity or bandwidth to the CPU, while scaling performance to levels matching state-of-the-art DNN DSAs.
   
‚Ä¢ We develop simple ‚ÄúREDUCT Support Extensions‚Äù
to the ISA that condense and encode multiple loops
of fixed iteration count information. This allows many
unnecessary power-hungry stages of the OOO CPU
pipeline to be bypassed with all work (unrolling and
execution) performed in close proximity to the data,
drastically improving achieved performance/Watt.
‚Ä¢ To our knowledge, REDUCT is the first practical
implementation of near-memory processing that uses
only the existing hardware and software architectural
interfaces (like existing on-die cache ports and SMT
hardware contexts) and thus requires no change to
the CPU programming or memory model.
Evaluated across multiple DNN models, REDUCT
achieves a 2.3√ó improvement in convolution performance/Watt with a 2√ó to 3.94√ó scaling in raw performance. Similarly, REDUCT achieves a 1.8√ó increase in
inner-product performance/Watt with 2.8√ó performance.
With full support for the existing CPU programming
model, no increase in cache capacity or bandwidth and
a mere 2.63% additional area, REDUCT enables unprecedented CPU efficiency gains and TCO savings for
datacenters while matching performance levels of state-ofthe-art DNN DSAs.
II. Characterization and Opportunity
We first perform an in-depth power and performance
characterization of multiple primitives common to DNN
inference on state-of-the-art CPU configurations. The goal
is to derive insights that can lead to efficient performance
and performance/Watt scaling.
A. Programming and Execution Model
DNN models are specified in frameworks like TensorFlow [30], Caffe [51], PyTorch [22], MXNet [4], OpenVino [21] etc. These frameworks provide developers with
easy-to-use APIs to describe topologies and model parameters while abstracting away the underlying hardware. The frameworks in-turn leverage highly-optimized,
platform-specific libraries like Intel oneDNN [19] and
MKL-DNN [14] [26] [13], AMD‚Äôs BLIS or libFLAME [9],
ARM‚Äôs compute libraries [5] and Nvidia‚Äôs cuDNN [18]
to extract maximum performance from the underlying
hardware. The inner-most loops of DNN primitives are
typically implemented in a vectorized, highly optimized
(often JITed [42]) manner for optimal performance and
maximum data reuse (see Figure 2). The outer-most loops
in the primitives (computing different sets of outputs like
in the example in Figure 2) are parallelized using well
established threading run-times (OpenMP [20], TBB [16])
to distribute work across compute cores in the target
hardware (see Figure 3).
parallel for : // over Outputs
Kernel example: Compung subset of
Outputs
Load Weight R1  [A1+1]
Load Weight R2  [A2+1]
Load Weight R3  [A3+1]
Load Weight R4  [A4+1]
Load Input R5  [A5+2+3]
MAC R{6++} += R1, R5
MAC R{7++} +=R2, R5
MAC R{8++} +=R3, R5
MAC R{9++} += R4, R5
Loop
Loop
Store Outputs (R6, R7‚Ä¶)
end parallel for // Weight Reuse
// Input Reuse
// Output Reuse/Accumulaon
Input Reuse
Weight Reuse
Output
Accumulaon
Input  Weight = Output
Peak reuse opportunies
in compung outputs
Output Channels
Fig. 2. 1x1 Convolution kernel example
B. Performance and Power Analysis
We evaluate state-of-the-art oneDNN [19] primitives 1.
We focus on int8 data types since several seminal studies
[43] have shown that 8 bit (or lower) precision is sufficient
for inference accuracy. The use of lower precisions reduces
dependence on expensive off-chip DRAM bandwidth enabling us to focus on cache bandwidths that are on-die.
The compute intensity or Ops/Byte property of any
DNN primitive plays a fundamental role in determining its
bandwidth requirements (and hence compute efficiency)
from the system. This metric needs to be evaluated at
multiple levels of abstraction:
‚Ä¢ Algorithm: The theoretical peak Ops/Byte is based
on the work being performed if we had an ‚Äúinfinite‚Äù
register file (RF) holding data. For example, in 1x1
convolution (Figure 2), every weight element is
reused across all input plane elements in the same
input channel, for different output plane elements.
Similarly, input and output elements also have peak
possible reuse opportunities.
‚Ä¢ Kernel: The software implementation extracts reuse
out of the finite RF of the compute core depending on
both the RF size and the data-flow implemented. RF
usage determines the minimum number of loads and
stores to be executed. For the kernel in Figure 2, the
1Our evaluation methodology, system configuration and workloads
are described in detail in Section IV.
KERNEL
parallel for i..k: //outputs
Load, compute, store
instructions in loops
to compute outputs
end parallel for
for i..j for j‚Äô..k SMT0
Core 0
L2
L3
L1
‚Ä¶
SMT0
Core N
Fig. 3. Program flow in the baseline multi-core processor
           
number of weight loads (reused across inputs in the
inner-most loop) and the number of iterations of the
innermost loop (reusing weights to compute different
outputs) are governed by the RF size.
‚Ä¢ Hardware: On-die cache hit-rates determine bandwidth delivery to the core and cross-cache data movement when kernels are executed on the underlying
hardware. We define Data Movement Overheads introduced by the hardware as the ratio of cumulative
cross-cache data movement (fills and evictions) to
the stores and loads from/to the compute core‚Äôs RF
(determined by the kernel).
We analyze and summarize these metrics in Table I for the
convolution and FC primitives with key highlights in red.
1) Convolution Characterization: We evaluate the optimized oneDNN implementation that fuses convolution
and ReLU (non-linear function on output) primitives.
Table I details our characterization of the high Ops/Byte
matrix-matrix convolution primitive across all convolutional layers of ResNet-50 [46] and shows several interesting insights (highlighted in red). First, the kernels employ
output-stationary data-flows (maximizing output reuse
in RF), with very low store bandwidth (Stores/MACInstr). Input and weight reuse variability across layers is
subsumed within the RF resulting in a fairly steady 0.5
Loads/MAC-Instr requirement across all layers. Second,
high average L1 hit-rates (86%) result in high compute
efficiency (120 MACs/cycle out of a peak 128). Fills and
evictions at the L1 cache still add an average 20% overhead
in data movement. conv1 layer of ResNet50 has the lowest
L1 hit-rate, adding 69% data movement overhead at L1,
with performance as low as 100 MACs/cycle.
Performance Opportunity: High L1 hit-rates mean that
we use only about 60% of available L1 bandwidth (0.5
loads/MAC-Inst, two MAC-Inst/core/cycle vs two loads/-
cycle/core at L1). Furthermore, the L2 and L3 bandwidths
are still hugely under-utilized due to cache-hierarchy seTABLE I
ResNet-50 Convolution and Transformer Inner Product
Characterization
ResNet50
Convolution
Transformer
InnerProduct
Metric Avg. Min Max Avg. Min Max
Ops/Byte: Based on Algorithm
Input 1021 32 4608 1727 1024 33708
Weight 2245 100 25600 1 1 1
Output 998 64 4608 2057 1024 33708
Memory Transactions/Op-Instr: Based on Kernel
Loads 0.49 0.39 0.59 1.35 1.19 1.41
Stores 0.058 0.003 0.25 7E-04 3E-05 9E-04
Hardware: On-Die Cache Hit-Rate
L1 $ 86% 57% 98% 23% 15% 26%
L2 $ 88% 51% 99% 72% 0% 100%
L3 $ 99.4% 97.6% 99.9% 99% 64% 100%
Hardware: Data Movement Overhead
L1-L2 20% 1% 69% 109% 81% 121%
L2-L3 2% 0.3% 5.8% 47% 27% 99%
Total 22% 2% 71% 156% 147% 181%
Hardware: Performance (Peak=128 MACs/Cycle/Core)
Ops/Cyc 120.4 100.0 127 12.99 8.81 14.02
rialization. Placing tensor compute near each of these
caches, would enable further scaling of performance without any increase in overall on-die capacity or bandwidth.
More re-use directly from these caches also reduces data
movement to the L1 caches. The 0.5 loads/MAC-instr requirement and peak bandwidths of each cache determines
the peak compute required near each cache.
2) Inner-Product Characterization: Inner-Product
primitives involve matrix-vector operations and have
lower peak Ops/Byte compared to convolutions. These
primitives are prominent in Recurrent DNN models
and Sequence-to-Sequence models (eg. Transformer [70])
which are heavily used in applications like natural
language processing. Table I also details our innerproduct characterization for all layers in Transformer.
This primitive has a poor 23% L1 hit-rate, which coupled
with a high 1.35 Load/MAC-instr bandwidth requirement
results in low compute efficiency achieving only 13
MACs/cycle. Furthermore, we see up to a whopping
156% overhead in cross-cache data movement.
Performance Opportunity: While L1 hit-rates are low,
hit-rates in the larger L2 (1MB) and L3 (1.375MB per
core) are significantly better. Tensor compute placed directly near these caches, bypassing the small L1 entirely,
would leverage the higher hit-rates for higher bandwidth
to feed the compute. Along with eliminating all data
movement to an under-sized L1, we would see better performance, with no increase in cache capacity or bandwidth.
3) Pooling/Concat: We also evaluate Pooling (dimensionality reduction) and Concat primitives and they
mainly involve data movement with low data reuse. Models like DenseNet-169 [50] pass lower level features (outputs) directly to later layers as inputs, using the Concat
primitive to prepare data. Execution near L2 and/or L3
caches would reduce this data movement cost.
4) Power Analysis: Figure 4 shows the contribution to
total power from various clusters in the CPU. CPUs unroll
every instance of every instruction of all loops into every
stage of the CPU pipeline. All instructions go through
fetch and decode, allocation and dispatch. Register allocation and renaming (RAT) and OOO dispatch (from
RS) are extremely expensive in power for wide and deep
OOO cores. For compute bound ResNet-50, these stages
contribute to 59.7% of total power! For bandwidth bound
59.65%
48.18%
28.69%
5.34%
6.30%
14.12%
5.35%
32.36%
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
ResNet50
Transformer
Power Stackup on a Modern CPU
FE+RS+RAT MACs Cache Access Data Movement Overhead
Opportunity to save power
Fig. 4. Stackup of power consumption in convolution dominated
ResNet50 and inner product dominated Transformer

TABLE II
Characterization Summary of DNN Primitives
Primitive Observations Data
Movement
Overhead
Opportunity
Convolution Bandwidth over-provisioned w.r.t. compute,
Under-utilization of L2/L3 Bandwidth
Mostly at
L1-L2
Perform tensor compute near all caches
(L1,L2,L3)
Inner-product Compute over-provisioned w.r.t. bandwidth,
Poor hitrate at 32KB L1
High at L1-L2
and L2-L3
Place tensor compute near large caches
(L2 and L3)
Pooling/Concat Low data reuse
Mostly data movement
High at L1-L2
and L2-L3
Execute near outer cache levels
(L3/L2)
Power dominated by unrolled wide OOO Fetch, Alloc and Dispatch Exploit structured/loopy kernel to encode multiple loops
inner-product primitives in Transformer, they contribute
to 48.2% of total power with cache access and data
movement adding another 46.5%.
Power Opportunity: With structured loop-y fixed iteration count DNN kernels, loop unrolling should happen in
a ‚Äúlean‚Äù scheduler, close to the tensor compute. Using a
‚Äúmacro‚Äù-ISA that encodes this loop information, the CPU
can offload multiple loops of decoded (but not unrolled)
work to the ‚Äúlean‚Äù, low-cost near-cache compute effectively
bypassing power-hungry stages of the legacy CPU pipeline
for most of the execution.
5) Summary: Table II concludes this section by summarizing our main observations for performance and power.
Optimally executing primitives near caches best suited
to their requirements can provide performance, power
and resource utilization benefits. Furthermore, we should
leverage the structured nature of the workloads to bypass
power-hungry front-end stages of the legacy CPU pipeline
- preferably unrolling and scheduling pre-decoded instructions near the execution units.
III. REDUCT
We present REDUCT2, which places light-weight ‚ÄúTensor Functional Units‚Äù (TFU) near all on-die caches in
the system as depicted in Figure 5. In combination
with ‚ÄúREDUCT Support Extensions‚Äù (rSX) to the ISA,
the goal is to efficiently leverage existing system resources (maximizing or minimizing use as required) for
performance and power benefits. Crucially, REDUCT
also retains the existing CPU programming and memory models which can significantly speed up development
and deployment efforts. We now detail the architectural,
micro-architectural and programming model aspects of
REDUCT.
A. REDUCT: Architectural Support
1) REDUCT Support Extensions (rSX): A close examination of state-of-the-art oneDNN kernels implementing
DNN primitives shines light on opportunities to encode
multiple loops of information succinctly. Such optimizations would enable minimizing and bypassing the powerhungry front-end stages of the CPU pipeline with unrolling
and dispatch of work proximal to the execution units.
2Reduct has multiple meanings: 1) to reduce; 2) to bring back to
memory; 3) to simplify - all relevant to this work
Fig. 5. An overview of REDUCT design
Depicted in Figure 6, is the meta-data information each
instruction requires to encode its loop behavior in the
kernel. First, we need to know the number of loops and
their iteration counts. The ISA can set a limit on the
maximum number of loops encoded, and we find that
supporting four loops is sufficient for all these kernels
(capturing load, compute and store operations). Each
instruction needs to know the set of loops it resides within.
In the example, weight loads are only executed in the outer
loop. Second, loads and stores need a base address as well
as an address stride for each loop it resides in. These can be
computed since DNN primitive implementation employs
structured data layouts to maximize cache port width
and capacity. Finally, data dependence is still through
registers. Hence, we can require stride values per loop for
destination register ids as well. In the example, iterations
of the innermost loop reuses weights (being loaded in the
parallel for : // over Outputs
Kernel example: Compung subset of Outputs
LoadrSX Weight R1  [A1+1]
LoadrSX Weight R2  [A2+1]
LoadrSX WeightR3  [A3+1]
LoadrSX Weight R4  [A4+1]
LoadrSX Input R5  [A5+2+3]
MACrSX R{6++} += R1, R5
MACrSX R{7++} += R2, R5
MACrSX R{8++} += R3, R5
MACrSX R{9++} += R4,R5
Loop
Loop
end parallel for
Store Outputs (R6, R7‚Ä¶)
Meta-data Required to Encode Loop Informaon
‚Ä¢ Total # of loops: ISA will set a limit
‚Ä¢ Loop[s] Iteraon count
‚Ä¢ ‚ÄúValid‚Äù loops/instr: Default = All loops
‚Ä¢ Example: Input Load, MAC = All loops
‚Ä¢ Example: Weight Loads = Only outer loop
‚Ä¢ rSX bit: Mark ‚ÄúLoop Encoded‚Äù Instr
‚Ä¢ Base Address: Loads/Stores
‚Ä¢ Address stride: Loads/Stores
‚Ä¢ Dst. Reg. Stride: Default = 0
‚Ä¢ Example:  = 4 ,  = 0
Stores are done in separate loops.
Not included in this example
Fig. 6. Requirements for encoding all loop information
            
Instructions (rSX enabled Kernel) Comments
¬ô TFULoopStart Flushes TFU Code Register in the core
¬ô TFULoopCount 2 Sets total expected loops to 2
(Loop Iteration Calculation into R0) Baseline ISA. Executed in the core.
¬ô TFULoopIteration 1, R0 //eg. = 64 Sets iteration count of outer loop (loop1) to calc. value
(Loop Iteration Calculation into R0) Baseline ISA. Executed in the core.
¬ô TFULoopIteration 2, R0 //eg. = 4 Sets iteration count of inner loop (loop2) to calculated value
LoadrSX Weight R1 ‚Üê [A1+Œî1] rSX instruction to be executed in TFU near a cache
¬ô TFULoopDisable 2 Disables outer loop for previously allocated rSX Instr.
(BaseAddress Calculation into R0) Baseline ISA. Executed in the core.
¬ô TFUBaseAddress R0 Sets BaseAddress for previously allocated rSX Instr.
(Stride Calculation into R0) Baseline ISA. Executed in the core.
¬ô TFUStride 1, R0 Sets stride for loop1 of previously allocated rSX Instr to calc. value
‚Ä¶
MACrSX R{6+Œ¥+Œ∏} += R1, R5 rSX instruction to be executed in TFU near a cache
Fig. 7. REDUCT : New rSX instructions and execution semantics
outer loop) to compute different output elements that need
to be stored in different registers. Here, the destination
register id stride is determined by the number of outputs
computed in the innermost loop (4).
Figure 7 illustrates the new rSX instructions and their
semantics. Kernel instructions are tagged with a rSX-bit
(denoting near-cache TFU execution) and are decoded and
allocated into new TFU Code Registers in the core. Our
examination of primitives across multiple DNN models
shows 32 registers to be sufficient. However, if a kernel has more than 32 instructions (and/or 4 loops) it
would need to be split into smaller kernels that fit within
these constraints. New rSX instructions (TFULoopCount,
TFULoopIteration, TFULoopDisable, TFUBaseAddres,
TFUStride, and TFURegStride) populate their respective
meta-data loop information for the instructions tagged
with rSX-bit. The meta-data information can be calculated using regular ISA (similar to the way the baseline
kernels currently do it). The new TFULoopStart instruction flushes the TFU Code Registers for new rSX-tagged
instructions and the TFULoopEnd instruction dispatches
the TFU Code Registers to the near-cache TFU for unrolled execution. We conservatively estimate each TFU
Code Register holds 8B of information (opcode, up to 3
registers (or a register and base address),4 loop iteration
counts (with a valid bit) and 4 address and register
strides). The entire offload takes 32 cycles (using an 8B
offload bus width) and this time is amortized by the
hundreds of cycles of unrolled execution in the TFU.
2) Tensor Functional Units (TFU): Figure 8 depicts
the Tensor Functional Units (TFU), with 32 TFU Code
Registers. A lean ‚ÄúUnrolling Scheduler‚Äù populates two 8-
entry in-order Issue Queues - one for all compute opcodes
and another for loads and stores. The design simplifies
scheduling (lower power!) and allows hoisting of loads over
compute (to hide load latency) while maintaining strict
load/store ordering within the TFU. Loads and stores
directly access the cache each TFU is placed proximal to -
bypassing any inner levels. Snoops into inner cache levels,
as required, are handled through added coherency support
to the cache (Section III-B2). A small Translation Cache
assists in memory management (Section III-B1).
Fig. 8. Schematic of the Tensor Functional Unit (TFU)
Both kernel characterization and performance analysis
show that a 48-entry ‚Äúdeep‚Äù TFU Data Register File
per TFU is sufficient - with no register renaming (lower
power again!) required. The loads/MAC-instr requirement
of the workload and the near cache bandwidth bounds the
peak compute ‚Äúwidth‚Äù (the number of 64B MAC execution
units) of the TFU. We evaluate the performance, power
and energy implications of different compute widths in
Section V. TFU area analysis is included in Section IV.
3) Leveraging SMT: Modern server-class CPUs support
2-way (Intel, AMD) and 4-way SMT (IBM, Sun SPARC).
However, since all compute is shared across SMT threads,
DNN frameworks disable SMT or use only one thread
per core for the primitives, relying on multi-core compute
scaling instead. With REDUCT, each TFU is essentially
a lean compute engine directly accessing one cache level
in the hierarchy. As shown in Figure 9, we leverage
SMT to bind each TFU exclusively to one of the logical
SMT threads in the physical core. Therefore, each TFU
is part of a fully capable, OS-visible hardware context.
DNN frameworks can then distribute work across TFUs
using existing threading runtimes. This also enables fine
grained control over which caches and TFUs to use for
a primitive(Section III-C3). rSX ISA, TFU design and
SMT usage allow REDUCT to maintain the CPU memory
model (Section III-B4).
KERNEL
parallel for i..k: //outputs
Load, compute, store
rSX instrs. in loops
to compute outputs
end parallel for
parallel for i..j
Core 0
L2
L3
L1
‚Ä¶
Similar distribution to other cores & SMT threads
TFU
TFU
TFU SMT0
SMT1
i..i‚Äô i‚Äô..i‚Äô‚Äô i‚Äò‚Äô.. jSMT2
rSX Instructions
Offloaded to TFUs
non-rSX Instructions
Executed in existing
Core functional units
rSX
rSX
rSX
Fig. 9. REDUCT program flow leveraging SMT to bind each TFU
to an OS-visible thread and dispatch of rSX to TFUs

B. REDUCT: Micro-Architectural Support
1) Virtual Memory: The TFUs need virtual memory
to physical translation since CPU caches are physically
tagged. oneDNN optimizations use special layouts, customized to feed compute, with a high spatial locality
of tensor accesses [28] (core TLB hit-rates > 98%). We
find that a small 8-entry local Translation Cache (TC)
per TFU, holding recently observed virtual to physical
mappings, can achieve a 95% hit-rate. Each TFU needs
to snoop the local core TLB (which is close by since the
TFUs are near caches physically co-located with the core)
to populate the TC. With the high hit-rates, we observe
no performance impact of TC misses or TLB snoops.
2) Coherence Support: Since REDUCT operates in
the cache coherent address space, hardware coherence uses
much of the existing framework and is supported through
minor additions to the L2/L3 caches and controllers.
The near-L2 TFU adds the requirement of a core-valid
bit at the L2 (denoting ownership by the local near-L2
TFU) and the ability to generate a Request for Ownership (RFO) for its stores. Similarly the near-L3 TFU
requires the directory entries at L3 to distinguish owners
between the existing cores and the local near-L3 TFUs
through additional core-valid bits per near-L3 TFU. The
L3 controller must also generate RFOs for stores generated
by its local near-L3 TFU. Snoops that may be required
due to these RFOs use the existing coherence framework.
Since oneDNN uses output stationary kernels, each TFU
operates on its own set of output elements. Therefore, we
see a negligible increase in snoop traffic, limited to cases
when different output elements share a 64B cacheline.
3) Distributed L3 Caches: L3 caches in modern
CPUs are distributed and shared across multiple-cores.
Each near-L3 TFU per core is placed near the shared
L3 slice that is co-located with the core. While L1 and
L2 caches are private per core, and can have duplication
of data across them, the shared L3 doesn‚Äôt support data
duplication across slices. However, REDUCT requires such
duplication for its near-L3 TFUs. For example, they may
all need the same weight element to compute their respective outputs. Traversing the L3 interconnect for every
address not available locally would cripple near-L3 TFU
performance and add significant extra data movement
overhead. We enhance existing class-of-service technologies like Intel‚Äôs CAT [10] to partition a portion of the setassociative cache (a small subset of its total ways) in each
L3 bank as a local cache for the attached TFU. Section V
includes performance sensitivity to the reserved local cache
capacity for each near-L3 TFU.
4) Memory Ordering: Within a TFU, strict loads/-
store ordering is maintained (TSO). Within any thread,
non-TFU operations past any TFU bulk-offload are not
allocated till the TFU bulk-offload operations are over.
A bulk-offload of hundreds of cycles of work to the TFU
(through rSX-ISA) amortizes any performance cost of this
serialization. Non-TFU load/store ordering continues to
be maintained by the core. Note that since TFUs are on
different SMT threads, we do not need to guarantee any
ordering in execution of loads and stores across TFUs. To
our best knowledge, REDUCT is the only near-memory
proposal to fully support the hardware memory consistency model of the CPU.
5) Context switching and Exceptions: Under current DNN usage models (example - micro-services in datacenters) context switching or exceptions are extremely
unlikely to occur. However, TFUs signal exceptions like
existing functional/compute units in the physical cores.
TFU context (the TFU Code Registers and TFU Data
Registers) must be included in any context save/restore.
C. REDUCT: Programming Model Support
1) Generating rSX Code: High performance libraries
already implement primitives using optimized code that
is JITed [42]. The kernel instructions must now be tagged
with the rSX-bit. Note that this is no different than incorporating any other ISA enhancement (example AVX256 to
AVX512). The JITer is already used to compute various
loop variables. Note that the programmer need not be
aware of the exact TFU and cache level where these
rSX instructions will eventually be executed. Optimizing
compilers to generate rSX code from native languages
without JITing is possible but not explored in our work.
2) Exposing REDUCT Capability at Each Cache Level:
Presence of TFUs in each cache level and their corresponding compute width is exposed through the cpuid interface
that is supported by all modern processors.
3) Optimal TFU selection for primitives: As we characterize and summarize in Table II, and with further analysis
in Section V, each primitive has an optimal set of TFUs
for power-performance efficiency. We leverage the use of
SMT (binding a TFU to a logical thread) and use existing
OpenMP APIs to set the affinity of a primitive to a
subset of cores. Specifically, we use KMP SET AFFINITY
in the LLVM OpenMP Runtime [24] to achieve this. DNN
frameworks (TensorFlow, Caffe etc) would do the same
before invoking oneDNN primitives.
4) Distribution of work across TFUs: Since caches
across the multi-level hierarchy have different bandwidths,
their corresponding TFU will have different compute
‚Äúwidths‚Äù as well. For compute bound primitives like convolution, we need to divide work across TFUs proportionate
to their compute strength for optimal performance. With
high cache hit-rates and predictable performance, such
a ‚Äústatic‚Äù division of work is sufficient for these workloads. Towards this, we introduce a simple new schedule
kind called static asymmetric in the LLVM OpenMP
runtime. For example, for three TFUs with compute
strengths in a 2:2:1 ratio, a static equal division of work
would result in unequal thread completion times with the
third (weakest) TFU determining final runtime. With a
static asymmetric distribution, in the same 2:2:1 ratio, all
threads optimally complete at the same time.

TABLE III
Minimal software stack to support REDUCT
Level in Software Stack REDUCT Support
DNN Frameworks
(TensorFlow, Caffe, PyTorch)
Set KMP THREAD AFFINITY appropriately for each primitive
High Performance Library
(oneDNN)
Use rSX extensions for tensor load,
store, compute instructions
Threading Runtime
(OpenMP)
Asymmetric static scheduling
when appropriate (e.g.
Convolution)
Table III summarizes the intercepts in the overall software stack by REDUCT. REDUCT operates entirely
within the CPU programming and memory model.
IV. Evaluation Methodology
Simulation Framework: We use a modified version
of Sniper [34] for our cycle-accurate multi-core simulations. We model a state-of-the-art Intel 28 core datacenter
processor [2] but with 4-way SMT whose parameters
are listed in Table IV. This baseline supports a peak
128 (2*64) MACs/cycle/core of compute (similar to Intel
DL-Boost [64]) with per-core on-die cache bandwidth
including two 64B read ports at L1, two 64B read/write
ports at L2 and one 64B read/write port at L3.
The simulation framework has been thoroughly validated against silicon by executing all the evaluated
oneDNN [14] primitives and verifying the performance
trends using a Cascade Lake system running Ubuntu
16.04 with oneDNN v1.0 library. CACTI [57], [67] and
McPAT [54] are used to quantify cache and overall energy
impact. TABLE IV
Simulator Parameters
28 Cores 2.6GHz, 4-way SMT, 320-entry ROB,
128 (2*64) MACs/cyc/core
L1 cache
Private, 32kB, 8-way set associative, LRU, 8-enrty
MSHR, 2x 64B read ports, 1x 64B write ports, Data
access=4 cycles, Tag lookup=1 cycle
L2 cache
Private, 1MB, 16-way set associative, LRU, 48-enrty
MSHR, 2x 64B read/write ports , Data access=8
cycles, Tag lookup=2 cycle
L3 cache
Distributed, Non-inclusive, 1.375 MB/slice, 11-way
set associative, RRIP, 48-enrty MSHR, 1x 64B read-
/write port per slice, Data access latency=10 cycles
Tag directory MESIF, 10 cycle latency
4x7 Mesh
NoC XY Routing, 22 cycle latency max.
REDUCT: We model all of REDUCT in Sniper allowing sweeps of peak compute at each TFU at different
cache levels. The last SMT thread is tied to L1 TFU but
not used. We use prefix ‚ÄúR‚Äù (for REDUCT ) or ‚ÄúM‚Äù (for
traditional monolithic core) followed by a number which
indicates the peak number of MACs/cycle/core that the
configuration has. Further, the notation also attaches an
explicit distribution of compute resources across the cache
TABLE V
Notation for REDUCT configurations
Name MACs/Cycle/Core Distribution
R128 128 same as M128
R256 256 128@L1, 64@L2, 64@L3
R320 320 128@L1, 128@L2, 64@L3
R512 512 256@L1, 128@L2, 128@L3
R640 640 256@L1, 256@L2, 128@L3
TABLE VI
Area Breakdown for TFU (in mm2)
Registers MACs TC,Queues,Control
0.15 0.17 0.06
Total Bytes: 3184
Total Area: 0.38mm2
levels as indicated in Table V. Mxxx configurations have
MAC units that support xxx MACs/cycle/core. Table V
specifies the hardware resources and the distribution for
the REDUCT configurations.
Area Requirements of REDUCT: We synthesize a
Verilog implementation of a TFU instance that is capable
of 256 (4*64) MACs/cycle/TFU to support the R640 configuration. Synthesis is done using TSMC 28nm library,
setting a target clock of 1 GHz, using Synopsys Design
Compiler [25]. The total area per TFU is 0.38mm2 and the
detailed breakup area is given in Table VI. An additional
2KB/core of storage is required for new core-valid bits
in L2 and L3 for full coherence. Total area overhead is
the sum of this coherence storage and the area required
for three TFUs/core. Projecting the total overhead to the
14nm technology [68], [1] in which the Intel Xeon server
chips are built, we conservatively estimate the overall
overhead due to REDUCT to be a mere 2.63% of a single
Xeon core. From observing die-plots( [69], [23]), REDUCT
takes 35% less than the AVX-512 unit already present in
the cores, while delivering a higher peak MACs/cycle/core
cumulative compute.
Workloads and Software Stack: We evaluate six
DNN topologies end-to-end including ResNet-50 [46],
DenseNet-169 [50], MobileNet [48], ResNext-101 [72],
Transformer [70] and TwoStream [40]. Of these, Transformer comprises solely of inner-product layers while the
others have mostly convolution layers. We use the latest
open source oneDNN v1.0 and the Intel C++ compiler
19.0 to ensure we are evaluating state-of-the-art software
implementations. Note that these are full system (including DRAM), multi-core evaluations. Since we study int8
inference, most model weights fit in caches. Coupled with
high reuse, overall impact on performance and power from
DRAM is very low. These workloads are parallelized using
the OpenMP multi-threading framework using our new
static asymmetric scheduling.
V. Results
We will first present performance and data movement
impact of REDUCT near-cache compute for DNN primitives in the ResNet-50 and Transformer models. We then
do a detailed power, performance and energy study for
REDUCT on ResNet-50 and Transformer. This is followed by results for all six DNN topologies evaluated. We
then summarize the performance and performance/Watt
goodness of REDUCT and conclude by comparing it with
available MLPerf data for GPUs and DSAs.

117
177
250
299
393
465
26%
40%
62% 58%
92%
89%
23%
23%
12% 9% 11% 9%
0%
20%
40%
60%
80%
100%
0
128
256
384
512
640
M128 M256/
M320/
M512/
M640
R256 R320 R512 R640
Aggregate Cache Bandwidth Ulizaon and
Data Movement Overhead
Achieved MACs/Cycle/Core
MACs/Cycle/Core
Aggregate B/W Ulizaon
Data Movement Overhead
37.1x
20.1x
0x
5x
10x
15x
20x
25x
30x
35x
40x
Peak PSX-ISA
Compressibility
Compressibility
rSX-ISA
13
29.1
20.2
27.7
43.4
0%
40%
80%
120%
160%
200%
0
8
16
24
32
40
48
M128/
M256
REDUCT
@L2
REDUCT
@L3
REDUCT
@L3
(8ways)
REDUCT
@L2 + L3
(8ways)
Achieved MACs/Cycle/Core
Data Movement Overhead
19.1x
10.1x
0x
4x
8x
12x
16x
20x
Peak PSX-ISA
Compressibility
3.4x perf @1.9x
lower data
movement
compared to M128
rSX-ISA
Fig. 10. Impact of REDUCT on ResNet50 convolutional layers (left) and Transformer inner-product layers (right)
A. Convolution
Figure 10 (left) shows the impact of REDUCT on
ResNet50 convolutional layers and reveals multiple interesting observations. Traditional scaling of compute
plateaus at an average ‚àº185 MACs/cycle/core from M256
onwards, since L1 bandwidth saturates, despite using
only 40% of total available on-die bandwidth. However,
REDUCT scales well in performance at all points, achieving between 2x to 3.94x performance over baseline, with
up to 90% cumulative bandwidth utilization in the higher
peak compute points. REDUCT R256 also has 41% higher
performance than M256, achieving near peak performance.
M256 needs peak bandwidth (100% hit-rate) from L1 to
achieve performance. In contrast, the R256 configuration
doesn‚Äôt require peak bandwidth from any of its caches and
is hence able to get higher performance.
We define compressibility as the reduction in the number of dynamic instructions that go through the core‚Äôs
FE+RAT with the usage of rSX-ISA, essentially a measure
of bulk-offload opportunity. rSX-ISA achieves an average
20√ó compressibility, which will translate to power savings.
Peak compressibility of the data parallel loops is actually
37√ó, but the new rSX instructions used to populate loop
meta-data reduces the compressibility. Finally, REDUCT
reduces data movement overheads from 26% down to 9%,
mainly going down at the L1-L2 interface.
0x conv1 res2a_branch1 res2a_branch2a res2a_branch2c res2b_branch2a res2b_branch2b res2b_branch2c res2c_branch2a res2c_branch2b res2c_branch2c res3a_branch1 res3a_branch2a res3a_branch2b res3a_branch2c res3b_branch2a res3b_branch2b res3b_branch2c res3c_branch2a res3c_branch2b res3c_branch2c res3d_branch2a res3d_branch2b res3d_branch2c res4a_branch1 res4a_branch2a res4a_branch2b res4a_branch2c res4b_branch2a res4b_branch2b res4b_branch2c res4c_branch2a res4c_branch2b res4c_branch2c res4d_branch2a res4d_branch2b res4d_branch2c res4e_branch2a res4e_branch2b res4e_branch2c res4f_branch2a res4f_branch2b res4f_branch2c res5a_branch1 res5a_branch2a res5a_branch2b res5a_branch2c res5b_branch2a res5b_branch2b res5b_branch2c res5c_branch2a res5c_branch2b res5c_branch2c
20x
40x
60x
80x
100x
Convoluon Layers in ResNet50
Compressibility
PSX ISA Compressibliity
0
128
256
384
512
640
Achieved MACs/Cycle/Core
P640 Achieved MACs/Cycle/Core P256 Achieved MACs/Cycle/Core
conv1 res5c_branch2c
R640 Achieved MACs/Cycle/Core R256 Achieved MACs/Cycle/Core
rSX ISA Compressibility
Fig. 11. Per ResNet-50 convolution layer REDUCT performance for
R256 and R640 configurations. Compressibility using rSX ISA is at
the bottom
Figure 11 shows per-layer performance and compressibility for the 53 convolution layers in ResNet-50 for the
R256 and R640 configurations. Sparing a few layers at the
start like conv1 and last few layers like res5c branch2c,
R256 configuration achieves peak performance across layers. The initial layers suffer from low cache hit-rates due
to model loading. The last few layers have lower total
Ops/Byte and the near-L3 TFUs see low hit-rates in their
256KB local partitioned cache (2 out of 11 ways) with
high cross-L3 traffic. Increasing the reserved near-L3 ways
from 2 to 8 ways improves performance for these layers
by 40%-60%. Compressibility increases with increase in
the input channel dimension (due to higher accumulation
required per output) and is generally lower for 1x1 kernels
compared to 3x3 kernels (due to lower input reuse). R640
scales performance over R256 by a factor of 1.86x for a
resource scaling of 2.5x.
B. Inner Product
Figure 10 (right) plots the average impact of REDUCT
configurations on performance, data-movement for the
106 Transformer inner-product layers on the right. Since
these are low Ops/Byte bandwidth bound primitives, we
only use the R256 REDUCT configuration but schedule
threads selectively at different cache levels. By merely
executing the inner-product directly near the large 1MB
L2, with better hit-rate and therefore bandwidth delivery,
REDUCT achieves 2.2√ó more performance and a 2.1√ó
reduction in data movement overheads. All L1 traffic has
been eliminated. Execution only near-L3, with a 2-way
256KB local cache, also reduces data movement. However, as we can now expect, provisioning more capacity
to the near-L3 compute increases performance to match
REDUCT near-L2. Executing the primitive at both L2
and L3 improves performance by a huge 3.3√ó over what
can be achieved by the baseline. This is achieved with no
increase in on-die cache bandwidth while simultaneously
reducing data movement overhead by 2√ó. Finally, the new
rSX-ISA achieves 10√ó compression, which will directly
translate to power and energy savings.

C. Pooling and Concat Layers
For brevity, we summarize the main observations from
evaluations of the pooling and concat primitives. Executing the res5c ResNet-50 pooling layer solely near L3
reduces data movement overhead by 95% (103% down
to 8%). Similarly, Concat layers in DenseNet-169 see an
average 150% data movement overhead, which is brought
down by 70%-95% (down to 25%-5% overhead) via near
L2 or near L3 execution.
D. Detailed Energy Analysis
Figure 13 represents a detailed energy analysis of the
ResNet-50 convolutional layers and Transformer innerproduct layers on the baseline M128 and R256 configurations. All the energy components are shown relative to
the total energy cost of running on a M128 configuration.
We deconstruct the impact of the near-cache and rSX
ISA components of the REDUCT proposal separately and
when put together.
In ResNet50, the FE and OOO stages of the CPU
pipeline dominate overall energy (60%). M256, even while
having twice the resources as M128, is iso-energy with
M128 as long as near-cache capabilities are not included to
them. Adding near-cache compute reduces inter cache data
movement costs between L1 and L2 but also increase direct
accesses to the larger (costlier in power) L2/L3 caches.
Therefore, near-cache compute ends up iso-energy with
baseline. The rSX ISA proposal achieves an average 20.1√ó
compression, which translates to a 17√ó reduction in energy
from the FE and OOO stages of the pipeline. REDUCT
R256 configuration therefore operates at 42% of baseline
energy (translating to a 13% decrease in power, along with
a 2x increase in performance).
The bandwidth bound, low Ops/Byte inner-product
layers in Transformer present a different story: data movement reduction (primarily by eliminating at L1) brings a
18.6% reduction in energy. Despite lower data reuse compared to convolution, the rSX ISA proposal achieves a 10√ó
compression, bypassing the FE and OOO pipeline stages
and providing a 42.8% energy reduction. Put together, we
0%
20%
40%
60%
80%
100%
M128/
M256
R256,
no rSX ISA
M256,
rSX ISA
REDUCT
(R256,
rSX ISA)
M128/
M256
R256,
no rSX ISA
M256,
rSX ISA
REDUCT
(R256,
rSX ISA,
ResNet50 Transformer
Energy Relave to M128
FE+RS+RAT MACs Cache Access Data Movement
Minor reducon due
to data movement
reducon
rSX-ISA eliminates
core overhead
Both near-$
and rSX ISA
reduce energy
rSX ISA is the
reason for
reduced energy
rSX-ISA eliminates
core overhead
Data Movement reducon
is the major contribuon
8 Ways@L3)
Fig. 13. Stackup of energy consumption
see a 61.5% reduction in overall energy consumption (at
1.5% lower in power and 2.77√ó better performance).
E. End-to-End Performance, Power and Energy
Figure 12 shows performance, energy and power impact of two REDUCT configs (R256 and R640 respectively) over six DNN-inference topologies. Inner-product
heavy Transformer is bandwidth bound. It achieves 2.78√ó
better performance at iso-power (65% lower energy) for
both REDUCT configs. The rest of the DNN models
have mostly convolution layers. With the exception of
DenseNet-169, these topologies see around 2√ó performance with 2√ó compute (R256 vs M128), at 12%-14%
lower power (60% lower energy). This increases to around
3.95√ó higher performance with 5√ó more compute (R640
vs M128) at 65% higher power (and 60% lower energy).
DenseNet-169 has a number of Concat layers which take
nearly 20% of total runtime. REDUCT reduces power for
this data shuffling primitive but doesn‚Äôt impact performance. This results in slightly lower performance improvement for DenseNet-169 with REDUCT, but at slightly
better power compared to the other convolution heavy
DNN topologies. 2.42x 3.70x
3.88x
3.95x
4.03x
2.78x
-57.4% -56.3% -57.2% -57.3% -60.6%
-64.6%
3.0%
61.7% 66.0% 68.4% 58.6%
-1.5%
-80%
-60%
-40%
-20%
0%
20%
40%
60%
80%
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
DenseNet
ResNeXt101
MobileNet
ResNet50
TwoStreamNN
Transformer
Dierence in Energy, Power
Speedup
Speedup - Energy Change - Power Change -
1.66x
2.00x
2.05x
2.07x
2.19x
2.78x
-57.7% -57.6%
-60.7% -57.3%
-60.7%
-64.6%
-30.0% -14.6% -13.1% -12.3%
-13.9%
-1.5%
-80%
-60%
-40%
-20%
0%
20%
40%
60%
80%
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
DenseNet
ResNeXt101
MobileNet
ResNet50
TwoStreamNN
Transformer
Dierence in Energy, Power
Speedup
Speedup - Energy Change - Power Change -
Fig. 12. Overall perf. improvement, energy and power difference for six DNN topologies using REDUCT. R256 (left) and R640 (right)
configurations are shown relative to M128. Primary Y-Axis plots performance and secondary Y-Axis plots power and energy change.

0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
Performance Improvement w.r.to
M128 Baseline
Power Consumpon w.r.to M128 Baseline
rSX ISA, no Near$
REDUCT
Compute - MACs/Cycle/Core: 128 256 320 512 640
M128 M128 + rSX ISA
R256 + rSX ISA
Monolithic
R640 + rSX ISA
Fig. 14. ResNet-50: Overall performance and power summary
F. Performance Per Watt Benefits
Figure 14 succinctly summarizes the goodness of
REDUCT on ResNet-50. On the X and Y-axes, the figure shows performance and power of different configurations normalized to the M128 baseline. Performance/Watt
(PPW) improvements across configurations are captured
as the ratio of respective Y and X-coordinates. Monolithic performance scaling (shown in red) saturates at
M256 with no PPW improvements over M128. Moving
to rSX-ISA (blue) improves PPW by 2.3√ó over any
equivalent monolithic configuration. REDUCT (green)
leverages near-cache compute on top of the rSX ISA,
enabling performance to scale by 2√ó to 3.94√ó depending
on the available TDP (13% lower power to 68% higher
power), while maintaining the PPW gains that the rSX
ISA brings. We observed that for inner-product heavy
topologies like Transformer, REDUCT achieves a 1.8√ó
increase in inner-product performance/Watt with 2.8√ó
improvement performance.
G. Comparison of REDUCT to DSAs and GPUs
We compare REDUCT with other platforms that are
used for DNN inference by using state-of-the-art, vendorprovided, publicly available throughput results on ResNet50 v1.5 from MLPerf Inference results [17], [62]. Figure 15
shows the number of queries processed by a variety of
accelerators (Habana Labs [7] , Intel Nervana [15], TPU
v3 [52]), GPUs(using a Titan RTX T496X2 card), and
Cascade Lake based Intel Xeon Platinum 9200 processor
(similar to our CPU baseline). We normalize the data to
per node (for accelerators and GPUs) or per socket (for
CPUs) and use the best results for each platform.
Incredibly, REDUCT achieves comparable (or even
somewhat better) performance to state-of-the-art DSAs
(Intel Nervana, Google TPU v3). Note, this performance
is achieved with a mere 2.63% additional area, maximizing
usage of existing cache capacity/bandwidth, while supporting the CPU programming/memory model and singlethread goodness. In contrast, discrete DSAs add much
larger area (separate compute/caches) and power budgets
to the system. We don‚Äôt make any performance/Watt com16.56
14.45
8.17
5.28
2.98
5.23
6.25 8.20
10.20
0
2
4
6
8
10
12
14
16
18
R256 R320 R512 R640
NVIDIA
Titan
RTX
Habana TPU Intel
Nervana
Intel
Xeon
REDUCT*
Throughput
(x1000 queries/sec)
1.75x
3.42x
Fig. 15. MLPerf inference throughput of GPUs, accelerators, and
CPUs compared with throughput of REDUCT (projected‚àó)
parisons due to the absence of these metrics in MLPerf. We
have shown significant (1.8√ó-2.3√ó) improvement in CPU
performance/Watt with REDUCT.
VI. REDUCT on low-power, edge CPUs
We have verified the performance/power benefit of
REDUCT across a range of compute widths, caches sizes
and bandwidths, including lower compute (16/32 MAC/-
cycle/core) and cache bandwidths typical to lower power
edge CPUs. Edge CPUs can pose two additional challenges. First, they typically have shallower cache hierarchies, often with multiple cores sharing an L2 cache. With
REDUCT, we would still have a TFU per core at the
L2 cache, but with lower compute strength. The total
compute strength across all TFUs near shared-L2 should
be proportional to the L2 bandwidth. Second, these cores
typically have no SMT capabilities, hence a core would
simultaneously schedule to all its TFUs, breaking TSO
memory ordering. However, relaxed consistency is fine for
ML workloads [6]. For low power and low cost edge SOCs,
where budget, latency and battery life requirements reduce
the RoI of adding specialized DSA hardware with driverbased offload, we believe REDUCT represents an excellent
CPU solution on all cost, performance and energy metrics.
VII. Related Work
Near data processing is an active research topic spanning DRAM to on-die caches, with recent emphasis on the
site of processing[59] as well as practical approaches[58] [47]
to enable it. Moving processing to in/near memories like
DRAM([65],[66],[37]), 3D-stacked HBM[8] or HMC[61]([32],
[75],[36],[53],[49],[33]) and even SSDs[56] is one direction.
Compute Cache[31], Neural Cache[38], and Duality Cache
[41] are all recent works that propose converting the CPU
on-die caches into compute units capable of bit-serial /bitparallel operations in the SRAM sub-arrays. While these
works show huge gains, they involve changes to highly
optimized SRAM sub-arrays, and can degrade or complicate signal integrity, density and floorplan. Livia [55]
proposed a data-centric architecture that moves tasks to

cache levels ‚Äúclosest‚Äù to the data source but the architecture has several limitations. Livia is meant for irregular
workloads and can track only one data element‚Äôs presence across hierarchy while moving the task. Inference
workloads need multiple tensors and accesses to them
are very regular. Livia also proposes a new programming
model which requires application rewrite. REDUCT takes
a light-weight, compute-near-cache approach, reusing existing micro-architectural interfaces and extracting large
improvements in performance and power, while supporting
the existing programming model.
From the power perspective, REDUCT is different from
solutions like Revolver [45] in using explicit instructions
to encode repetitive loops, requiring no learning, and by
bypassing all of front-end, RS and RAT pipeline stages.
VIII. Summary
As the amount of data created and analyzed grows
exponentially, OEMs are willing to harness all forms of
available compute to tackle their computational needs.
While CPUs are the dominant platform-of-choice for DNN
inference in datacenters, our in-depth analysis reveals
significant opportunities for more efficient CPU resource
utilization leading to drastic improvements in performance/Watt and the ability to scale performance without
increasing cache capacity or bandwidth. Using a combination of simple ISA enhancements and light-weight tensor
compute near all caches in the CPU, REDUCT raises the
bar for CPU-based DNN-inference performance and performance/Watt while maintaining the same programming
and memory models. REDUCT represents a fundamental
re-imagination of the general-purpose CPU, better suited
for the AI era.