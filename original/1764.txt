Abstractâ€”Near-Memory Processing (NMP) systems that integrate accelerators within DIMM (Dual-Inline Memory Module)
buffer chips potentially provide high performance with relatively
low design and manufacturing costs. However, an inevitable
communication bottleneck arises when considering the main
memory bus among peer DIMMs and the host CPU. This
communication bottleneck roots in the bus-based nature and
the limited point-to-point communication pattern of the main
memory system. The aggregated memory bandwidth of DIMMbased NMP scales with the number of DIMMs. When the
number of DIMMs in a channel scales up, the per-DIMM
point-to-point communication bandwidth scales down, whereas
the computation resources and local memory bandwidth per
DIMM stay the same. For many important sparse data-intensive
workloads like graph applications and sparse tensor algebra,
we identify that communication among DIMMs and the host
CPU easily dominates their processing procedure in previous
DIMM-based NMP systems, which severely bottlenecks their
performance.
To tackle this challenge, we propose that inter-DIMM broadcast should be implemented and utilized in the main memory
system of DIMM-based NMP. On the hardware side, the main
memory bus naturally scales out with broadcast, where perDIMM effective bandwidth of broadcast remains the same as
the number of DIMMs grows. On the software side, many sparse
applications can be implemented in a form such that broadcasts
dominate their communication. Based on these ideas, we design
ABC-DIMM, which Alleviates the Bottleneck of Communication
in DIMM-based NMP, consisting of integral broadcast mechanisms and Broadcast-Process programming framework, with
minimized modifications to commodity software-hardware stack.
Our evaluation shows that ABC-DIMM offers an 8.33Ã— geo-mean
speedup over a 16-core CPU baseline, and outperforms two NMP
baselines by 2.59Ã— and 2.93Ã— on average.
Index Termsâ€”near-memory processing, inter-DIMM broadcast, Broadcast-Process framework, sparse applications
I. INTRODUCTION
Emerging data-intensive workloads [1] such as graph applications [2] [3] desire both high capacity and high data
transfer rate to meet strict Quality-of-Service requirements
[4] [5]. The memory bandwidth of a main-memory system,
however, cannot easily satisfy this ever-increasing demand
under tight pin count and power constraints [6] [7]. Therefore,
mounting pressure has been put on the memory system of
modern computer architectures [8].
âˆ—Corresponding author: Leibo Liu (liulb@tsinghua.edu.cn)
To tackle this challenge, various near-memory processing
architectures have been proposed [9] [10] [11] [12] [13] [14].
They carry out computation close to data and thereby avoid
the data transmission bottleneck of the main memory bus
[15] [16] [17] [18] [19] [20]. Among them an appealing
choice is DIMM-based NMP [21] [22] [23] [4] [1], which
integrates a computation unit with the memory buffer chip
on a Dual-Inline Memory Module (DIMM). By performing
computation and memory access on each DIMM in parallel,
the aggregate bandwidth boosts as the number of DIMMs
grows [21]. Such an architecture not only has the potential of
providing high capacity and high bandwidth, but also features
relatively low cost. Besides, it can be easily integrated into
commodity systems, striking a good balance among various
design considerations [21] [23].
However, the inevitable communication among peer DIMMs through the main memory bus severely limits the
scalability of DIMM-based NMP. The large-scale parallel
computation of multiple DIMMs, which is the basis of DIMMbased NMPâ€™s high performance, results in inevitable communication among peer DIMMs and the host CPU through the
main memory bus. For many important sparse data-intensive
workloads like graph applications and Sparse Matrix-Vector
multiplication (SpMV), we identify that the communication
easily dominates their processing procedure in the DIMMbased NMP system and severely bottlenecks their performance.
The key drawback of DIMM-based NMP lies in the busbased nature and the limited point-to-point communication
pattern of the main memory system. On the one hand, the
aggregate bandwidth of a memory bus does not change as
the number of DIMMs goes up within a channel, leading
to fast per-DIMM communication bandwidth degradation. On
the other hand, per-DIMM computation capacity and local
memory bandwidth stay still. This rapidly widening gap soon
bottlenecks the overall performance, making the acceleration
of many widely adopted applications impractical in such a
system.
To tackle this challenge, we propose that inter-DIMM
broadcast should be implemented and utilized in the main
memory system of DIMM-based NMP. On the hardware side,
the main memory bus naturally scales out with broadcast,

"$.*&&&UI"OOVBM*OUFSOBUJPOBM4ZNQPTJVNPO$PNQVUFS"SDIJUFDUVSF	*4$"

Â¥*&&&
%0**4$"
2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) | 978-1-6654-3333-4/21/$31.00 Â©2021 IEEE | DOI: 10.1109/ISCA52012.2021.00027
where per-DIMM effective bandwidth of broadcast remains
the same as the number of DIMMs grows. On the software side, many sparse applications can be implemented in
a form such that broadcasts dominate their communication
[24]. Based on these ideas, we design ABC-DIMM, a hardware/software holistic solution consisting of integral mechanisms for intra- and inter-channel broadcast as well as the
high-level Broadcast-Process programming framework, which
work together to Alleviate the Bottleneck of Communication.
Concretely, the intra-channel broadcast mechanism consists
of broadcast-write and read-broadcast: the former broadcasts
data from the host CPU Memory Controller (MC) to all the
DIMMs in the channel, while the latter broadcasts data from
one DIMM to all the other DIMMs and the MC.
The philosophy of ABC-DIMMâ€™s implementation is to
make the design as simple and low-cost as possible. This is
rather challenging, since current computer architecture designs
have taken no consideration for inter-DIMM broadcasts. In this
paper, we have managed to constrain modifications within the
host CPU MC, the DIMM buffer chip and the Operating System (OS) with a full-stack design. First, we design new memory commands to support our proposed broadcast mechanisms.
Second, to make them work with unmodified DRAM chips,
we implement these commands by translating them to regular
commands on the DIMM buffer chip. Third, we interface the
new memory commands to CPU with minimized architecture
changes by introducing a simple address mapping mechanism
to the MC. Fourth, considering the timing constraints of the
new commands, we add small changes to MC Finite State Machines (FSMs). Fifth, to enhance system robustness, we equip
ABC-DIMM with advanced DDR features such as ECC (Error
Checking and Correction) and C/A (Command/Address) parity
check. Finally, we provide friendly and efficient API designs to
hide low-level resource management and correctness guarantee
details from users.
In short, this paper makes the following major contributions:
â€¢ We identify communication through the memory bus as
a major bottleneck for many data-intensive applications
to scale out in DIMM-based NMP. The bus-based nature
of main memory systems makes point-to-point communication unscalable.
â€¢ We propose that inter-DIMM broadcast is a promising
method to scale out communication in the main memory
system. Based on this idea, we design ABC-DIMM, a
solution consisting of integral broadcast mechanisms as
well as the Broadcast-Process programming framework
to co-optimize the overall system performance.
â€¢ We provide a full-stack implementation of ABC-DIMM
that minimizes system modifications. Our evaluation
shows that ABC-DIMM offers an 8.33Ã— geo-mean
speedup over a 16-core CPU baseline, and outperforms
two NMP baselines by 2.59Ã— and 2.93Ã— on average.
II. BACKGROUND
A. DRAM Basics
Memory system organization. As illustrated in Figure 1(a),
the main memory system consists of independent channels. In
Fig. 1. (a) Main memory system architecture (b) NMP unit within buffer
chip
each channel, DIMMs are connected to the host CPU memory
controller through a data bus and a command/address bus.
Each DIMM is further organized as a hierarchy of ranks and
banks, with each bank composed of multiple rows and a row
buffer.
DDR Commands. When the host CPU wants to access
some data from a certain row in a bank, it sends an activation
(ACT) command with the row address through the C/A bus
to the bank, informing it to load all the data in the row to the
row buffer. Subsequently, it sends a read (RD) or write (WR)
command with the offset address of the request data in the
row to launch data access. With an RD command, the desired
data will be moved from the row buffer on to the data bus.
Conversely with a WR command, the CPU will write the data
to the data bus. Then the data will be moved from the bus to
the row buffer, replacing the old data at the specified offset
address. When the CPU no longer needs to access the data in
the buffer or wishes to access another row, it has to inform
the bank with a precharge (PRE) command to move the data
in the buffer back to their belonging row before it proceeds to
subsequent operations.
Apart from the commands mentioned above, the CPU also
uses Mode Register Set (MRS) command to modify the mode
register values of DRAM devices for work mode changes [25]
[26].
DDR Timing Constraints. The time required for a memory
operation and between operations under specific conditions
is explicitly defined and has to be strictly obeyed when the
host memory controller issues DDR Commands [27]. An MC
is generally designed with a complex finite state machine to
record the system state and to calculate out timing constraints
to be obeyed for pending commands. Following are the timing
parameters required for reading this paper: tCL is the interval
between the issue of an RD command and the start of data
transmission on the data bus. tCWL is the gap between the
issue of a WR command and the beginning of the write data
burst. tRTRS, short for rank-to-rank switching time, is the
minimum gap between issues of memory access commands
to different ranks. It essentially exists for the environment
change of the data bus, concretely including the time for
resynchronization and termination setups [27] [28].

Fig. 2. Chameleon-MapReduce Framework [21] [29] [30]
B. NMP Basics
Baseline Architecture. With the increase of the number of
ranks in a channel, data transmission through the bus becomes
more difficult due to load capacitance growth. Thus buffer
chips are designed and placed on DIMMs for better signal
integrity. DIMMs with both data and C/A buffers are called
Load-Reduced DIMMs (LRDIMMs) [21] [27].
Interestingly, LRDIMMs also provide chances for efficient
near-memory processing. By integrating processors with buffer
chips and having multiple LRDIMMs in each channel work in
parallel, we are able to obtain an aggregate bandwidth much
larger than that of the main memory [21]. The performance can
be further boosted by utilizing rank-level parallelism, achieved
by enabling the on-DIMM processor to access more than one
local rank at the same time [4]. The baseline architecture in
this paper is shown in Figure 1(b).
Chameleon-MapReduce Framework. As has been mentioned, the high performance of DIMM-based NMP essentially
depends on high DIMM-level parallelism. This inevitably introduces parallel programming issues like communication, task
partitioning and scheduling. Chameleon [21], the pioneer of
DIMM-based NMP, inherits MapReduce [30] framework from
NDA [29], its 3D-stacked predecessor, to provide guidelines
for NMP-styled parallel programming. As shown in Figure 2,
the Chameleon-MapReduce framework divides the execution
flow into the NMP phase and the CPU-Processing phase. In the
beginning, CPU splits the input data among DIMMs, which
then calculate out intermediate results during the NMP phase.
In the CPU-Processing phase, CPU collects the intermediate
results from each DIMM and reduces them to the final output.
For an iterative workload, the output will become the input of
the next iteration [29]. Apart from Chameleon-MapReduce,
Memory Channel Network (MCN) [22] also provides a communication model for NMP programming. In short, it enables
convenient inter-DIMM and CPU-DIMM communication by
virtualizing them as homogeneous Ethernet links [22].
However, neither of the solutions above attempts to tackle
the communication bottleneck inherent in the main memory
NMP systems, which will be discussed in the next section.
III. MOTIVATIONAL ANALYSIS
ABC-DIMM is motivated by holistic observations from both
hardware and software perspectives.
A. Hardware Perspective
1) Bandwidth Scalability Comparison: Inter-DIMM broadcast solves the bandwidth scalability problem of point-to-point
communication in current main memory systems.
Fig. 3. Bandwidth scalability comparison under single-channel configuration
The main memory bus is a time-division multiplexing
communication medium in essence. With point-to-point communication, CPU can only transfer data to or from one DIMM
at a time. Suppose that there are n DIMMs in a single memory
channel and each DIMM has equal concurrent communication demands. When a DIMM possesses the channel bus, it
experiences the full memory channel bandwidth. However,
waiting for this possession takes up (nâˆ’1) times of its actual
communication time, resulting in a per-DIMM bandwidth of
merely 1/n of the full channel bandwidth, as illustrated in
Figure 3. To make matters worse, this per-DIMM bandwidth is
further halved for inter-DIMM communication, since data are
loaded first to the CPU cache from the source DIMM and then
written to the destination DIMM, transmitted twice through
the memory bus. As many data-intensive and communicationintensive workloads require increasingly more DIMMs for
larger memory capacity [23] [3], the deteriorating bandwidth
will seriously bottleneck their performance.
Inter-DIMM broadcast solves this problem by expanding the
effective channel bandwidth. With inter-DIMM broadcast, the
host CPU can broadcast data to all the DIMMs, and a DIMM
can broadcast data to all the other DIMMs and the host CPU,
thus effectively multiplying the memory bus bandwidth by n
compared with point-to-point communication. In this way, the
bandwidth reduction effect caused by waiting time is perfectly
offset and the per-DIMM bandwidth becomes invariant with
the number of DIMMs.
2) Physical Implementability of Broadcast: Physically implementing inter-DIMM broadcast on the main memory bus is
intuitively simple. When a DIMM/CPU transfers data through
the bus, it physically changes the voltage level of the whole
bus. To this end, it has to charge the termination capacitors
of every DIMM on the bus, even if it merely desires to
communicate with one of them [27]. When the charging
process completes, every DIMM sees the transmitted data at
its local termination. In other words, broadcast phenomenon
already physically exists with currently supported point-topoint communication, only that it has long been seen as a
curse rather than a blessing, causing troubles like energy waste
[21] [27]. Thus, implementing inter-DIMM broadcast is only
an idea that takes advantage of an already existing physical
mechanism, turning it from waste to treasure. Consequently,
inter-DIMM broadcast triggers negligible power increase on
the main memory bus. Considering that DIMM-based NMP

already supports parallel operation of multiple DIMMs in a
channel, the total power increase for broadcast in a DIMMbased NMP system is also trivial.
Of course, concerns may still exist for signal integrity.
After all, signal integrity in current systems is optimized for
point-to-point communication rather than broadcast [26]. Utilizing identical parameter settings may not best suit broadcast
implementation. Fortunately, current DDR standards already
provide a very general signal integrity optimization solution,
including the Dynamic On-Die Termination (ODT) mechanism
and the Calibration mechanism [26] [25]. The former allows
the DIMMs to dynamically change its termination impedance
for different DDR commands based on a pre-configured set
of parameters to achieve satisfactory signal integrity [26]. The
latter calibrates or trains these parameters during the system
startup, so as to fit the needs of variant system environments.
The Calibration mechanism also trains other signal-integrityrelevant parameters like timing compensations for optimal signal sampling [26]. By incrementally adding broadcast-specific
parameter settings and corresponding calibration features to
current mechanisms, acceptable signal integrity can be realized
for inter-DIMM broadcast.
Apart from physical implementability, the remaining problem is to fit broadcast mechanisms into current computer architectures with as minor costs as possible. This is a challenging
task, and also our major consideration in the system design of
ABC-DIMM.
B. Software Perspective
Many data-intensive applications are severely bottlenecked
by communication in the DIMM-based NMP. By implementing them in a broadcast-dominant way [24], such a bottleneck
can be significantly reduced with the help of broadcast. Let
us use PageRank as an illustrative example.
PageRank, together with most graph applications and SpMV
itself, can be interpreted as a specific form of generalized
sparse matrix-vector multiplication defined on a specific semiring [3] [2]. The only difference between generalized SpMVs
and normal ones lies in that multiplications and additions are
substituted respectively with user-defined Process and Reduce
operators [3]. Therefore, although the discussion in this section
is limited to PageRank for simplicity, the insights it reveals are
general enough to apply to other workloads.
1) PageRank Algorithm: For graph applications like PageRank, the matrix and vector involved is generally the permutation of their adjacency matrix and their vertex value vector.
A non-zero at (i, j) in the adjacency matrix indicates an edge
from source vertex vi to destination vertex vj . For weighted
graphs, the value of the non-zero element corresponds to the
weight of the edge [3].
In PageRank, specifically, each vertex in the graph represents a web page and each edge indicates an existent link
from the source page to the destination page. The goal of
the algorithm is to calculate the rank value of each page,
which reveals its relative importance. The calculation process
is iterative: new rank values are calculated out by carrying out
a generalized matrix vector multiplication in each iteration
Fig. 4. An example graph, its adjacency matrix and the process of PageRank
on the graph in an iteration
Fig. 5. Chameleon-MapReduce style Implementation of PageRank
until rank values converge [3] [31]. In an iteration, the old
rank value of each vertex is first multiplied by the inverse
of the vertexâ€™s out-degree as a pre-process and used as the
input vector element of the generalized SpMV. The Process
operator selects input vector elements based on the adjacent
matrix, and the Reduce operator is defined as normal addition
[3]. Figure 4 shows an iteration of the process. More formally,
a post-process involving a simple constant multiplication and
addition is required for each vertex, but this is omitted here for
simplicity. Generally, the pre-process and post-process cost is
negligible compared with the generalized SpMV.
2) Chameleon-MapReduce Implementation: Figure 5
shows how PageRank is implemented in the form of
Fig. 6. Broadcast-Process style Implementation of PageRank
Fig. 7. Performance variations with #DIMMs, normalized to the 1-DIMM
performance of Chameleon-MapReduce style Implementation

Fig. 8. Communication time ratio and bandwidth utilization of ChameleonMapReduce style implementation varying with #DIMMs
Fig. 9. Communication time ratios of Broadcast-Process style implementations varying with #DIMMs
Chameleon-MapReduce. For simplicity, we only consider
single-channel implementations in this section, and the
pre-process stage in each iteration is not discussed. The
matrix is sliced by column and distributed in each DIMM
in advance. For each iteration, the CPU first splits the old
rank value vector to each DIMM. The computing unit on
each DIMM then carries out a partial generalized SpMV and
calculates out an intermediate result of the output vector.
Finally, the CPU collects the intermediate results from each
DIMM and derives the new rank value vector by adding them
up. Obviously, this implementation cannot take advantage of
broadcast, since no identical data are transferred.
Figure 7 displays the performance results of ChameleonMapReduce-styled PageRank on WK graph under singlechannel configuration based on our simulation. The setup
of evaluations in this section is detailed in Section V-A.
From the figure, we can see that this scheme shows low
scalability. With #DIMMs below 4, the speedup grows slowly
to 1.87Ã—, and then starts to drop after that. Figure 8 shows
the CPU Processing time ratio and the bandwidth utilization
during the CPU Processing phase. It is obvious that CPU
Processing time severely bottlenecks the overall performance.
With bandwidth utilization close to 80%, it is also quite clear
that the CPU Processing bottleneck is essentially a CPUDIMM communication bottleneck.
3) Broadcast-Process Implementation: The communication
bottleneck among DIMMs of MapReduce-based PageRank
casts a shadow over DIMM-based systems. However, by refactoring MapReduce-based implementations into Broadcastfriendly ones, the communication bottleneck can be eliminated
by the aforementioned inter-DIMM broadcast.
PageRank can be implemented in a Broadcast-Process style
[24], as is illustrated in Figure 6. The matrix is sliced by row
and distributed in each DIMM in advance. In the beginning
of each iteration, each DIMM possesses a unique part of the
old rank value vector. To initialize the iteration, they first
broadcast their vector part to every other DIMM through
CPU, so that every DIMM possesses a complete version of
the old vector after the communication phase. Then they are
able to calculate out their corresponding part of the new rank
vector with the complete old vector and the partial matrix
in hand. Obviously, broadcasts dominate the communication
phase. Another interesting feature of this implementation is
that all of the computation workloads are offloaded to the
DIMMs, and the CPU only takes charge of communication.
Figure 7 and 9 show the performance results and the
communication time ratios of this implementation style without hardware inter-DIMM broadcast support. Similar to
Chameleon-MapReduce, communication bottlenecks the overall performance, but the results here are even worse. This is
consistent with our analysis from the hardware perspective
in Figure 3, since the Broadcast-Process implementation is
dominated by inter-DIMM communication rather than CPUDIMM communication in Chameleon-MapReduce.
With hardware inter-DIMM broadcast support, however, the
communication cost will be greatly reduced. The performance
and communication ratios are shown in Figure 7 and 9. From
Figure 9, we find that the communication bottleneck has been
removed, giving rise to almost linear scalability illustrated in
Figure 7. These results provide a sound motivation for ABCDIMM.
IV. ABC-DIMM
In this section, we detail the software-hardware co-design
of ABC-DIMM. Section IV-A proposes Broadcast-Process
framework which guides programmers to refactor targeted
workloads into broadcast-friendly implementations. To enable
inter-DIMM broadcast on the hardware side, Section IV-B
illustrates integral broadcast mechanisms for both intra- and
inter-channel settings. Based on these mechanisms, we detail
hardware architecture in Section IV-C, with the goal of minimizing modifications on commodity CPU-memory architectures. Section IV-D demonstrates user-friendly broadcast APIs
to encapsulate resource management and correctness guarantee
details.
A. Broadcast-Process Framework
Broadcast-Process framework aims to restructure a pointto-point communication-intensive workload into a broadcastintensive one. Compared to MapReduce where tasks are
distributed by splitting the input data (Figure 5), BroadcastProcess creates tasks by partitioning the outputs (Figure 6).
In this way, instead of gathering output data in MapReduce,
communication among tasks are fulfilled by broadcasting input
data.
Figure 10 summarizes the execution flow of BroadcastProcess framework, which is divided into a parallel NMP
computation phase and a broadcast communication phase.
For an iterative workload like PageRank, a new iteration begins

Fig. 10. Broadcast-Process Framework
Fig. 11. Intra-Channel Broadcast Mechanism
when CPU detects that all the DIMMs have completed their
work of the last one and reached the synchronization barrier.
It then proceeds to the broadcast phase, where partial results
are broadcast from each DIMM to every other DIMM, so that
every one gets a complete copy of the last iterationâ€™s results.
When broadcasts are done, the NMP phase starts and new
results are calculated by each DIMM with local data. As for
a non-iterative workload like SpMV, the broadcast phase can
be viewed as a data layout initialization stage: since input
data may be originally distributed in a way that is NMPunfriendly or not ready for broadcast, during initialization CPU
has to collect them on cache with normal access first and
then broadcast them to DIMMs with NMP-friendly address
mapping.
Broadcast-Process framework is general enough for a large
number of workloads. Intuitively, many workloads can be
tailored into the divide-conquer paradigm [30]. Since the
divided output can be calculated out on each DIMM with a
whole copy of input data at hand, the DIMM-based NMP
system thereby conquers the problem with our BroadcastProcess framework.
B. Inter-DIMM Broadcast Mechanism
The inter-DIMM broadcast consists of an intra-channel
mechanism and an inter-channel mechanism. The former
mechanism is implemented by tweaking DDR, whereas the
latter one is realized with the assistance of software on CPU.
1) Intra-Channel Broadcast Mechanism: The intra-channel
broadcast mechanism is composed of broadcast-write and
read-broadcast, whose dataflows are illustrated in Figure
11(a) and 11(b).
Broadcast-write is a straight-forward mechanism. Corresponding to traditional DDR writes, it writes data from CPU
MC to every relevant rank in the channel. Instead of allowing
the MC to appoint specific broadcast addresses for each rank,
the data are stored in identical locations for all ranks. Such a
design trades flexibility for performance. Rather than issuing a
lot of commands for distinct addresses across multiple ranks,
which consumes much C/A bandwidth and severely degrades
broadcast performance, our design only issues one command
for each broadcast, so that broadcast bandwidth can be fully
utilized.
Broadcast-write is, in essence, a CPU-DIMM broadcast
mechanism rather than inter-DIMM broadcast. With only
broadcast-write at hand, the CPU has to load the data from
the source DIMM to its cache before it broadcast-writes the
data, resulting in a halved broadcast bandwidth in effect. To
avoid this overhead, the read-broadcast is designed.
Read-broadcast corresponds to traditional DDR reads. But it
is actually carried out with a read and a concurrent broadcastwrite. On the read-broadcast request from CPU, the source
DIMM moves the data to the bus. The other DIMMs, on
the contrary, treat this request as a write command, and
simultaneously store the data from the bus to their local ranks.
In this way, data are broadcast on read, thus avoiding the
superfluous broadcast-write. Besides, similar to the broadcastwrite, every destination rank involved in read-broadcast stores
the transmitted data in identical locations as with the source
rank, so that read-broadcast can be implemented with one
command and full broadcast bandwidth can be achieved.
2) Inter-Channel Broadcast Mechanism: The inter-channel
broadcast can be easily implemented by software with
broadcast-write and read-broadcast support. As displayed in
Figure 12, when a data block requires broadcast in a multichannel system, the CPU can first read-broadcast the block
in the local channel and load it on the cache, and finally
broadcast-write the cached block in every other channel one
by one.
3) Multi-core Broadcast Phase Implementation: To fully
evacuate the potential of Broadcast-Process framework under
multi-channel configurations, an optimized multi-core implementation of the communication phase is required. In our
design, data are sliced into blocks. Each core broadcasts a data
block to each channel in a round robin fashion, as shown in
Figure 12(b). For simplicity, only one block from one DIMM
in each channel is considered in the figure, and only the
design for iterative algorithms is displayed. For non-iterative
workloads, the read-broadcast is replaced with normal read,
with round-robin broadcast-writes covering every channel.
With this scheme, block-wise access optimizes buffer hit rate,
the round robin fashion minimizes channel access contention,
and thus high bandwidth utilization can be achieved.
Based on the implementation above, the broadcast percentage can approach 100% for an iterative algorithm, whose data
layout initialization can be amortized across iterations. For

Fig. 12. (a) Inter-Channel Broadcast Mechanism (b) an implementation of Broadcast-Process Communication Phase under multi-channel configuration
non-iterative algorithms, data layout initialization counts. Thus
their broadcast percentage is N
N+1 Ã— 100% under N-channel
configuration. Here one broadcast is counted as one memory
access.
C. Hardware Implementation
In this subsection, we provide the detailed hardware architecture implementation based on the aforementioned intraDIMM broadcast mechanisms, as illustrated in Figure 13.
The philosophy of ABC-DIMM is to make the design as
simple and low-cost as possible, and we manage to constrain
our design within the host memory controller and the DIMM
buffer chip. This also helps us constrain our design on the
rank level, and avoid the complex impacts from DRAM
chip internal mechanisms like address remapping, which are
restricted within chip level and not exposed to rank-level
access [33] [34]. For the support of broadcast mechanisms,
new DDR commands are designed, the signal formats of
which are introduced in Section IV-C1. To make them work
with unmodified DRAM chips, we translate these commands
to regular DDR commands on the DIMM buffer chip, as
described in Section IV-C2. In Section IV-C3, we modify the
address mapping unit and the finite state machines (FSMs)
of the host MC, aiming to interface new commands to CPU
with minimized costs. In Section IV-C4, we present the control
flow and associated control commands of NMP computation.
Finally in Section IV-C5, the ECC and C/A parity check
support in ABC-DIMM is briefly discussed.
1) Broadcast Commands and their encodings: ABCDIMM introduces four new memory commands for broadcast
mechanisms: ACTB, PREB, WRB and RDB.
WRB corresponds to the broadcast-write mechanism, while
RDB corresponds to the read-broadcast mechanism. However,
it is not enough to realize high-performance broadcasts with
only these two commands at hand. Specifically, when we deal
with a broadcast-write in a 16-rank channel and unfortunately
meet row miss in every one of the ranks, host MC will
have to issue and complete 16 precharge commands and
16 activation commands before only one WRB command is
issued. Although precharges and activations in different ranks
can be parallelized, the delay introduced by 32 command
issues can still cause significant performance degradation.
Thus, a broadcast version of PRE and ACT commands is
required, corresponding to PREB and ACTB, respectively.
A crucial problem on introducing the new commands is how
to encode them with current C/A signals of DDR4. As shown
in Figure 14(a), DDR4 standards have already used the limited
bits to its extreme [26]. To tackle this problem, we propose that
the Chip ID bits, referred to as C in Figure 14(a), can be used
for the new commands. These 3 bits are utilized to appoint a
specific layer of a stacked DRAM device, which theoretically
may have a height of 2, 4 or 8 [25] [26] [35]. However, 8-
layer devices are rarely produced at present, meaning that for
most systems, we can safely adopt the design illustrated in
Figure 14(b). Thus, the highest chip ID bit indicates broadcast
commands in ABC-DIMM. A new mode register in the buffer
chip can be introduced to enable or disable the broadcast
commands for better compatibility.
This introduced mode register can be further modified to
realize broadcast command masking, by disabling broadcast
for specific ranks or only enabling them to respond to broadcast commands with CS (Chip Select) signals within a specific
set. The latter mechanism can be used to support multi-NMPworkload environment. For non-broadcast DIMMs with no
such register, broadcast commands naturally mask them out
since the CS signals will not trigger them. CS signals can be
appropriately selected for masking through OS-level address
mapping.
2) Broadcast-enabling DIMM Buffer chip: The DIMM
buffer chip plays a key role in realizing the functions of the
new commands. As illustrated in Figure 13(b), when broadcast
commands arrive at the buffer chip , they are first translated
to regular DDR commands by C/A Translators before sent to
the DRAM devices. As shown in Figure 13(c), the translation
of WRB, PREB and ACTB is straight forward. When the
C/A Translator of a specific rank detects a WRB command,
it simply transforms the command to a regular WR by setting
the highest Chip ID as 0 and the corresponding CS signal as 1.
For PREB and ACTB, the C/A Translator carries out exactly
the same behavior. This direct translation, however, means
that a precharge command may be sent to an idle bank that
has already been precharged. Fortunately, this will not cause
any problem since the idle bank will automatically ignore the
redundant precharge command according to JEDEC standards
[35].
The translation of RDB is much more complex. According
to the read-broadcast mechanism, RDB reads out data from a
specific rank and broadcasts the data to all the other ranks as
well as the host MC. For the source rank of the data, RDB
should act as a regular RD, as shown in Figure 13(c). For
other ranks to store the data, RDB should act as a WR.

Fig. 13. Hardware Architecture: (a) Overall single-channel architecture (b) Modified buffer chip architecture (c) C/A translation rule table for C/A Translators
(d) Modified Address mapping scheme for host MC (e) Modified FSM and Scheduler architecture of host MC [32]
Fig. 14. (a) DDR4 C/A signal format [25] [26] (b) Format Definition for
New Commands; (c)Timing diagram of WRB (d)Timing diagram of RDB
However, a serious problem arises if the timing parameters
of DRAM chips are considered. Unlike a regular WR, the time
interval between the data burst of RDB and the command issue
is tCL rather than tCWL. Fortunately, tCL is generally larger
than tCWL [27] [35]. To enable destination ranks to receive
the data at the right time, we only need to translate the RDB to
WR and delay for an interval of tCL âˆ’ tW L before issuing
it to the ranks. The translation rule and timing diagram of
RDB are shown in Figure 13(c) and Figure 14(d) respectively.
Even if tCL happens to be smaller than tCWL under very rare
occasions, we can still match the timing by delaying the RD
command for the source rank instead.
3) Broadcast-enabling host MC: The host MC is in charge
of exposing broadcast mechanisms to CPU and maintaining
appropriate DDR timing.
CPU detects broadcast commands by using the highest 2
bits of the physical address as a broadcast flag. As displayed
in Figure 13(d), if the highest 2 bits are both set, the address
mapping unit will identify the incoming write / read transaction as a broadcast-write / read-broadcast transaction and map
the address in an NMP-friendly way.
To maintain appropriate DDR timing for broadcast commands, the architecture of FSMs and Schedulers also require
modifications, as illustrated in Figure 13(e). The design is
based on the architecture in [32]. A broadcast queue, as well as
a corresponding scheduler, is added. In this design, the queue
stores pending transactions to each rank; the scheduler decodes
the transactions into specific DDR commands and takes care
of timing constraints, with tRTRS considered by the Arbiter.
Obviously, the introduction of broadcast does not affect the
design of Rank/BG/Bank FSMs at all, since our design is
constrained within the DIMM buffer chip without changing
the structure of a rank. Therefore, broadcast-relevant timing
constraints can be considered as a simple pile up of the
timing constraints of the separate regular commands involved
in the broadcast, if not considering rank-to-rank switching
time. tRTRS requires to be considered when a subsequent
read, write or broadcast command accesses a rank that is not
accessed by the most recent command of these types. Under
other occasions, it either does not exist, or is overlapped by
other constraints that have been taken care of by schedulers.
Finally, scheduling is also important in MC design. For
broadcast scheduling, existent mechanisms for normal access
also apply, even in a multi-program environment. Considering
that an NMP DIMM is generally occupied by a single task,
broadcast scheduling is actually simpler than normal access
in a multi-program environment. For the single-program environment in our evaluation, the conventional FR-FCFS strategy
is adopted.
4) Control Flow for NMP: With above mechanisms, we are able to implement the whole control flow of the Broadcast-Process Framework. In the beginning, the host CPU broadcasts the data with new broadcast commands, then informs the on-DIMM compute units to enter the NMP phase. Subsequently, the
CPU detects the completion of the NMP phase by polling

each compute unit or with interrupts triggered by the units.
It then collects data from each DIMM, checking the status of each compute unit and deciding whether to proceed. In ABC-DIMM, we use an SIMD unit as the compute core, which is optimized and capable of efficiently
utilizing on-DIMM local memory bandwidth.
As with previous works [22] [21], we also provide a
1-cacheline-sized scratchpad in each compute unit for the
CPU to read, as an optional method for the CPU to collect
information from each DIMM. Besides, the compute unit is
allowed to interrupt the host CPU with the EVENT N signal
reserved by DDR4 standards [22] [21].
To facilitate host CPUâ€™s control over DIMM compute units,
we introduce MRSB command for ABC-DIMM. This command is used to change the values of specific registers in the
DIMM compute unit, so as to inform the compute unit to carry
out certain behavior. As shown in Figure 14, it is encoded as
regular MRS with C[2]=1. On the host MC side, the write
request with highest 2 address bits equal to 10 are identified
as register write transactions and loaded into the register write
queue in Figure 13(e). The opcode of the command is derived
from the lower bits of the request data. Since this command
does not actually involve memory operations, it can be issued
any time when the C/A bus is idle. After issued to the buffer
chip, it is then delivered to the compute unit and changed
the value of the target register, as is specified by the opcode.
This command can be a non-broadcast one that only operates
on a single DIMM, and it can also operate on every DIMM
as a broadcast-styled command. In our implementation of the
Broadcast-Process framework, we use this command to inform
the DIMMs to enter the NMP phase.
5) ECC and C/A Parity Check: ABC-DIMM supports ECC,
an important feature for reliability. In ABC-DIMM, the inbuffer-chip compute unit is equipped with an ECC-enabled
MC. During the NMP phase, ECC is carried out by the
compute unit MC just as by the host MC, and will inform
the host through ALER N signal on uncorrectable errors. For
broadcast-write, ECC check bits are generated by the host
MC and broadcast to the corresponding DRAM chip of each
rank. For read-broadcast, a â€™Delayedâ€™ ECC scheme is adopted.
During the execution of a read-broadcast command, ECC is
carried out only by the host MC. Since the broadcast data are
meant to be further used by compute units, the ECC of the
data transferred to each DIMM is postponed until the data are
read by the compute units in the NMP phase. Obviously, this
scheme works well with correctable errors. For uncorrectable
errors, ECC on the host side is enough to inform the system
software in time.
ABC-DIMM is also compatible with C/A parity check,
with broadcast C/A checked in parallel by every related
rank. The major challenge here lies in possible unrecoverable
data overwrite caused by occasions like partial failure. For
instance, when failure occurs in the destination ranks of a
read-broadcast, host MC is informed through ALERT N, but
the source rank is not, and the on-going commands in this
rank cannot be stopped in time. A subsequent on-going write
command may overwrite the read-broadcast source data before
redo is carried out, and thus the read-broadcast cannot be
correctly redone. Furthermore, considering the effects of a
failure on subsequent on-going broadcast commands, things
can be even more complex. In ABC-DIMM, possible unrecoverable data overwrite can be eliminated with software-level
restrictions, which are comfortably followed in the BroadcastProcess framework implementation with no performance overhead, as shown in Section IV-B3 and Figure 12(b). Concretely,
for CPU, a physical address allocated for broadcast can only
be used in one of the following three ways before release: (1)
to be exclusively used as the source of read-broadcasts; (2) to
be exclusively used as the destination of read-broadcasts with
an identical source physical address; (3) to be exclusively used
for broadcast-writes. No non-broadcast access is allowed for a
broadcast physical address. With these, safe redo is guaranteed
for C/A parity check failure handling.
D. Inter-DIMM broadcast API Implementation
The preceding hardware implementation provides a low
level interface to software - the flag bits in the physical
address. Utilizing this interface on the user level is not an easy
task. In this subsection, we will discuss the implementation of
high-level APIs.
Broadcast memory space mapping and management. To
enable user applications to access broadcast-flagged physical
address, the OS needs to establish corresponding virtual-tophysical address mapping with page tables, so as to correctly set the broadcast flag bits. For processors that support
cache locking [36], such a feature can be utilized to prevent
undesirable cacheline evictions for certain memory space,
like those of in-cache buffers used in the Broadcast-Process
framework implementation. Finally, to avoid disturbing the
channel interleaving mechanism [27] [29] [7] adopted in
conventional address mapping, the physical addresses in the
same locations across relevant ranks and all channels should
be simultaneously allocated for broadcast at a time.
Safe and efficient software broadcast implementation.
Taking caches into consideration, the safety or correctness of
broadcast operations is not guaranteed by our hardware implementation. For instance, supposing we access a broadcastflagged address with a simple store instruction, with the
expectation of realizing broadcast-write, the cache system,
however, will first check if the cacheline of this address exists
in the cache. If so, no memory access will be triggered. If
not, the system will load the data, triggering an opposite readbroadcast operation.
To tackle this problem, special software implementations are
required for safe and efficient broadcast. First, for broadcastwrite, we utilize AVX instructions that store data to memory
with a non-temporal memory hint, like mm256 stream pd()
[37], which bypass the cache system and directly write the data
to the memory controller. Second, for read-broadcast, normal
AVX load intrinsics like mm256 load pd() [37] are used to
trigger read-broadcast. Then mm clflush() [37] is called to
invalidate the cacheline to enable future broadcast.

V. EVALUATION
In this section, we report the evaluation results of ABCDIMM and answer the following questions.
(1) Does ABC-DIMM perform better and scale better than
DIMM-based NMP baselines?
(2) How do the broadcast mechanisms contribute to the
performance and scalability of ABC-DIMM?
(3) How does the number of channels influence the scalability
of ABC-DIMM and baseline systems?
(4) How do the features of benchmark applications and
datasets influence the performance and scalability of our
evaluated systems?
A. Methodology
To answer the questions above, we evaluate ABC-DIMM
against 3 baselines - that is, a 16-core CPU baseline, a
Chameloen-MapReduce-style NMP baseline [21] (chameleonmr), and a Broadcast-Process-style MCN-alike NMP baseline
[22] (mcn-bc).
Configuration. Table I summarizes the configurations of the
CPU baseline system and the NMP system. We adopt identical
configurations for ABC-DIMM and NMP baselines, except
that ABC-DIMM is broadcast-enabled while chameleon-mr
and mcn-bc feature CPU-DIMM and inter-DIMM point-topoint communication respectively. This is for fair comparison
and to focus our study on communication. For area and power
assessment, we derive the estimation for the SIMD unit based
on RTL synthesis of Synopsys DC with the TSMC 28nm
technology. The area and power of the on-chip memory under
28nm technology are estimated using Destiny [38]. Then we
convert these 28nm-technology results to 14nm with scaling
factors shown in [39].
Apart from broadcast mechanisms, the major difference
among NMP baselines and ABC-DIMM lies in their programming frameworks. Chameleon-mr adopts the ChameleonMapReduce framework, while mcn-bc and ABC-DIMM use
the Broadcast-Process framework.
Simulation. We integrate Ramulator [40] with zsim [41]
[42], a fast and accurate simulator, to evaluate the performance
of the CPU-baseline and the host CPU of the NMP system.
For the evaluation of ABC-DIMMâ€™s host CPU, we modifies the
Ramulator and integrates it with our proposed broadcast mechanisms as well as the corresponding timing constraints. To
evaluate the performance of the on-DIMM compute units, we
use an in-house simulator for the SIMD unit, which integrates
Ramulator for cycle-accurate on-DIMM local memory access
simulation. The CPU codes for baselines are all compiled with
-O3 flag.
Benchmarks. We evaluate four algorithms for comparison.
PageRank (PR) has been previously discussed. SpMV calculates the multiplication of a sparse matrix and a dense vector.
Single Source Shortest Path (SSSP) searches for the shortest
paths between a specific source vertex and other vertices.
Average Teenage Followers (AT) [43] calculates the number of
teenage followers of each user and the average on users over
a certain age. In the implementation of average calculation
TABLE I
SYSTEM CONFIGURATION
CPU-baseline processor configuration
Processor 16-core OoO, 3.2GHz
L1 Size(L1D: 64KB, L1I: 32KB);
Associativity (L1D: 8, L1I: 4); LRU
L2 256KB; Associativity (8); LRU
LLC 16MB; Associativity (16); LRU
NMP host processor configuration
Processor 4-core OoO, 3.2GHz
L1 Size(L1D: 64KB, L1I: 32KB);
Associativity (L1D: 8, L1I: 4); LRU
L2 256KB; Associativity (8); LRU
LLC 4MB; Associativity (16); LRU
Memory system configuration
DDR4-2133, 4Gb, x8,
4 channels Ã— 8 DIMMs Ã— 2 ranks, FR-FCFS
32-entry RD/WR queue, Open policy
DRAM timing parameters
tBL=4,tCCDS=4,tCCDL=6,tRTRS=2,tCL=16,
tRCD=16,tRP=16,tCWL=11,tRAS=36,tRC=52,
tRTP=8,tWTRS=3,tWTRL=8,tWR=16
DIMM-side Compute Unit
On-Chip 768KB Scratchpad
eDRAM 256KB Stream Buffer
memory Area: 0.154mm2 Power: 225.0mW
SIMD Unit 1024-bit array, Frequency: 1.2GHz
Area: 0.386mm2 Power: 295.6mW
TABLE II
GRAPH AND SPARSE MATRIX DATASETS USED FOR EVALUATION
Graph #Vertexes #Edges
(Sparse Matrix) (#Columns) (#Non-zeros)
livejournal (LJ) [44] 4.85M 68.99M
pokec (PK) [44] 1.63M 30.62M
wikipedia (WK) [45] 3.57M 45.03M
wb-edu (EDU) [45] 9.85M 57.16M
NLR [45] 4.16M 24.98M
on ABC-DIMM and mcn-bc, specifically, Broadcast-Process
Framework is used to calculate out the local average on each
DIMM. Then the CPU collects these local results and derives
the global average. The cost of this final step is generally
negligible. The former two applications are iterative, while
the latter two are not.
The datasets we use are listed in Table II. They are typical
datasets from various sources. Among them, LJ and PK are
obtained from social networks [44]; WK and EDU are web
page link datasets [45]; and NLR is obtained from numerical
analysis applications [45].
B. Performance Comparison
Taking advantage of broadcast mechanisms, ABC-DIMM
displays higher performance compared with chameleon-mr
and mcn-bc.

Fig. 15. Performance of ABC-DIMM, chameleon-mr and mcn-bc normalized to the performance of 16-core CPU
Fig. 16. Communication-computation ratios of ABC-DIMM, chameleon-mr and mcn-bc under 32-DIMM configuration
Figure 15 shows the speedups of ABC-DIMM and NMP
baselines over the 16-core CPU with a 4-channel memory
system for the evaluated benchmarks. The performance results
under 1-channel-1-DIMM and 4-channel-32-DIMM configurations are displayed for the three architectures. The former
configuration involves minimum communication and reveals
the computing power of a single DIMM. The latter involves
maximum resources and shows how well the evaluated systems evacuate the system potential. The best performance is
separately displayed for the two NMP baselines. As shown in
Section V-C2, chameleon-mr and mcn-bc scales poorly under
4-channel configurations and does not work best with the
maximum number of DIMMs.
Figure 16 shows the communication-computation ratios for
evaluated architectures and benchmarks under 32-DIMM configuration. The communication-computation ratio is defined as
the ratio between NMP processing time and communication
time, which includes broadcast time for ABC-DIMM and mcnbc, CPU processing time for chameleon-mr, and initialization
time for all. This figure helps us get a first glimpse on the
roots of performance variations.
Based on the two figures above, we made the following key
observations. First, ABC-DIMM outperforms chameleon-mr
and mcn-bc by great advantage. Under 4-channel-32-DIMM
configuration, The relative performance of ABC-DIMM over
the two baselines are 4.87Ã— and 7.03Ã— respectively, as is
revealed in Figure 15. Its speedups over the CPU range
from 4.98Ã— to 11.98Ã—, and achieve 8.33Ã— on average. Even
compared with the best performance of chameleon-mr and
mcn-bc, ABC-DIMM still maintains geo-mean speedups of
2.59Ã— and 2.93Ã—.
Second, inter-DIMM broadcast is the key in ABC-DIMMâ€™s
high performance. From Figure 15, we can see that chameleonmr, mcn-bc, and ABC-DIMM achieve almost identical performance under 1-channel-1-DIMM configuration. This partly
shows that local computing power and processing tasks on
DIMMs are not the critical factors in performance differences
of the three architectures. Figure 16 further confirms the key
role communication plays. As is revealed, the communicationcomputation ratios of chameleon-mr and mcn-bc range from
6.53 to 16.99, achieving 8.68 and 12.86 on average respectively. ABC-DIMM, however, has a much more ideal geomean ratio that is as low as 0.96, thanks to the broadcast
mechanisms. From these figures, we can safely draw the
conclusion that the communication bottleneck severely limiting the baseline performance is greatly reduced or even
eliminated by broadcast, thus enabling ABC-DIMM to fully
utilize the parallelization of 32 DIMMs and achieve impressive
performance.
Third, the features of applications and datasets exert evident
influence on performance. Concretely, NMP systems, including baselines and ABC-DIMM, perform best on SSSP. Unlike
PageRank and AT, SSSP not only computes on vertex values,
but also on edge weights as well. This increases sequential
local memory access on each DIMM, which is scalabilityfriendly and increases NMP computation time. In this way, the
communication portion is relatively lowered and scalability is
increased, thus leading to enhanced performance. However,
SpMV, which also involves weight access, features lower
performance. This is mainly due to its communication pattern,
which will be discussed in the next subsection.
Compared with application features, the dataset features
show even greater influence on speedups. From Figure 15, it
is obvious that much lower performance is achieved by NMP
baselines on EDU. Specifically, chameleon-mr and mcn-bc
display very little speedups, and the relative speedup between
ABC-DIMM and NMP baselines reaches its top on EDU under
32-DIMM configuration. We can easily find the direct reason

Fig. 17. Speedups of ABC-DIMM, chameleon-mr and mcn-bc over 16-core
CPU with varying #DIMMs under 1-channel configuration
Fig. 18. Communication-Computation ratios of ABC-DIMM, chameleon-mr
and mcn-bc with varying #DIMMs under 1-channel configuration
in Figure 16, which shows that communication overheads are
much more significant on this dataset. Fundamentally, this is
determined by the high sparsity of EDU, which has a very
low #edge-#vertex ratio. NLR, however, seems contradictory
to this deduction. It shows higher communication overheads
than EDU, but with better speedups. In fact, NLR still lowers
the NMP baseline performance, but its poor locality feature
disturbs the CPU cache and lowers the CPU baseline performance, therefore resulting in relatively higher speedups. In
short, the sparser the dataset, the more serious the communication bottleneck, and the more important broadcast is for
NMP systems.
C. Scalability Comparison
In this section, we will take a closer look at how ABCDIMM and baseline NMP systems scale with the number of
DIMMs. For simplicity, we will only discuss on PageRank
and SpMV workloads, as well as LJ and EDU datasets.
1) Single Channel: Figure 17 shows the varying speedups
of ABC-DIMM and NMP baselines over the 16-core CPU with
varying #DIMMs under single-channel configuration. Corresponding communication-computation ratio variations are
displayed in Figure 18.
Based on these figures, we can draw conclusions much
similar to those in the previous subsection. First, ABC-DIMM
scales much better than NMP baselines, showing almost linear
scalability. Although chameleon-mr scales slightly better than
mcn-bc, both of their performance stops scaling up before
#DIMMs reaches 5, and even begins to sink thereafter. Second, broadcast is the key of ABC-DIMMâ€™s scalability. As is
revealed in Figure 18, when the communication-computation
ratios of the baselines soar with the increase of DIMMs,
the ratio of ABC-DIMM grows only slightly thanks to the
broadcast mechanisms. Third, the sparsity feature of EDU
dataset has strong influence on performance. NMP baselines
Fig. 19. Speedups of ABC-DIMM, chameleon-mr and mcn-bc over 16-core
CPU with varying #DIMMs under 4-channel configuration
Fig. 20. Communication-Computation ratios of ABC-DIMM, chameleon-mr
and mcn-bc with varying #DIMMs per channel under 4-channel configuration
exhibit very poor scalability on EDU, whose performance
quickly degrades after #DIMMs reaches 4. With the help of
broadcast, ABC-DIMM scales out, though at a lower rate than
on LJ.
Consistent with the observation in the previous subsection,
SpMV, though involving weight access, features lower performance than PageRank under 8-DIMM configuration for ABCDIMM. The fundamental reason lies in the different broadcast
strategies adopted in PageRank and in SpMV. During the
communication phase in PageRank, the broadcast of data
in a single memory channel can be implemented with a
simple read-broadcast, and does not involve any broadcastwrite operation. In SpMV, however, broadcast takes place in
the data layout initialization phase. Input data are originally
stored in a way not ready for broadcast. The host CPU has
to load the data to its cache first, and then broadcast them
to all the DIMMs in the channel with broadcast-writes. This
process incurs double communication traffic compared with
that in PageRank, and thus offsets SpMVâ€™s advantage on
weight access. We can confirm this effect in Figure 18, which
displays that the communication-computation ratios of ABCDIMM SpMV are significantly larger when compared to those
of PageRank.
2) Multiple Channels: Since scalability curves are very
similar in form for different channels, we only discuss on 4-
channel configuration in this subsection for simplicity. Figure
19 and Figure 20 display the speedup and communicationcomputation ratio variations under 4-channel configuration.
The x-axis shows the number of DIMMs per channel.
Under multiple-channel configuration, major observations
in single-channel evaluation still hold. The most significant
difference lies in the reduced scalability. Under 4-channel
configuration, ABC-DIMM no longer scales out linearly on
LJ, and its performance increase on EDU is also slower. This
scalability is, however, still better than chameleon-mr and mcn-

bc. Their performance almost fails to scale out and begins
to drop ever after #DIMMs per channel exceeds 2, not only
on EDU, but also on LJ as well. The underlying reason is
quite simple: with the increase in #channels, so is in #DIMMs,
which results in lower per-DIMM computation workloads and
relatively higher communication portions, as is revealed in
Figure 20. Therefore, the increase in #channels only worsens
the communication bottleneck, making broadcast an even more
important mechanism for DIMM-based NMP.
Another difference lies in the performance of SpMV. Although ABC-DIMM still performs better on PageRank, the
gap between PageRank and SpMV is relatively lowered under
4-channel configuration. The cost of data load from memory to cache is amortized across channels, thus resulting in
reduced communication overheads and relatively enhanced
performance for SpMV.
VI. RELATED WORK
Aside from those NMP works discussed in Section II, ABCDIMM can trace back to other works relevant to broadcast
and memory, including those on cache coherence [46] [47]
[48], on distributed shared memory systems [49] [50] [51],
on single-chip 3D-stacked NMP [52] [24], etc. Compared
with them, our work is different in the following aspects.
First, we target at alleviating the communication bottleneck
of DIMM-based NMP. In this aspect, the most similar works
are [52] [24], which also target at communication reduction
for NMP. However, their broadcast/multicast is implemented
with network-on-chip and targeted at single-chip 3D-stacked
NMP. Second, we propose inter-DIMM broadcast on the DDR
main memory bus and the Broadcast-Process framework as
a solution for the bottleneck. Although bus-style broadcast
has been used for communication optimization before [49],
to our best knowledge, no prior work has proposed broadcast
on the DDR main memory bus. Nor have the costs and
benefits of such broadcast been examined with NMP taken into
consideration. The major challenge here lies in compatibility
with DDR standards and lowering architectural costs, and we
present a full-stack design to tackle this challenge.
VII. CONCLUSION
In this paper, we identify communication through the main
memory bus as a major bottleneck in the DIMM-based NMP
system for many applications. To tackle this challenge, we
propose ABC-DIMM, which alleviates the bottleneck with
integral inter-DIMM broadcast mechanisms and BroadcastProcess programming framework. We further provide a fullstack implementation that minimizes modifications to the
computer system. Our evaluation shows that compared to a
CPU baseline and two NMP baselines, ABC-DIMM achieves
8.33Ã—, 2.59Ã— and 2.93Ã— speedups on average.