Abstract‚ÄîMulti-core architectures have enabled data centers
to increasingly co-locate multiple jobs to improve resource
utilization and lower the operational cost. Unfortunately, naively
co-locating multiple jobs may lead to only a modest increase in
system throughput. Worse, some users may observe proportionally higher performance degradation compared to other users
co-located on the same physical multi-core system. SATORI is a
novel strategy to partition multi-core architectural resources to
achieve two conflicting goals simultaneously: increasing system
throughput and achieving fairness among the co-located jobs.
I. INTRODUCTION
Background and Problem. Co-locating workloads on chip
multiprocessors (CMPs) is an attractive approach for cloud
computing service providers as it enables them to improve
resource utilization and lower the capital and operational costs
of running their data centers [11], [19], [28], [54], [59], [60],
[64], [67], [79], [91]. Co-location of workloads is likely to
yield even higher benefits in the future as data centers are
continuously observing an increase in the number of throughputoriented workloads [8], [23]‚Äì[25], [80] ‚Äì these workloads are
naturally most suitable for co-location because such jobs are
often long-running and do not have strict latency requirements.
While co-location of workloads can potentially increase
throughput, it can lead to unfairness where some users or
workloads may observe proportionally higher performance
degradation than others, compared to their no-interference (i.e.,
co-location free) execution. Consequently, some prior research
works have attempted to optimize for throughput [40], [41],
[52], [71], [81], [86], [94], and some other works have focused
on achieving high fairness [46], [66], [90]. However, achieving
both the goals (throughput and fairness) simultaneously for
co-located workloads continues to be challenging because
throughput and fairness are fundamentally conflicting goals
(system providers often want higher throughput and fairness
is preferred by users) ‚Äî optimizing one often compromises
the other. Next, we discuss why existing approaches, while
useful, do not achieve the full potential toward providing a
near-optimal solution in a multi-objective setting.
Challenges, Existing Approaches and Gaps. The difficulty
in achieving optimality stems from two significant challenges:
(1) complexity of the effects of ‚Äúmultiple‚Äù shared architectural
*A part of this work was performed during an unprecedented time ‚Äì around
the peak of the COVID-19 pandemic. We were able to conduct this scientific
study only because first-line responders and essential workers worked tirelessly
to keep our community safe and functioning. We are grateful for their effort.
This paper is dedicated to the memories of all the first-line responders and
essential workers who sacrificed their lives trying to keep ours safe.
resources (e.g., cache, memory, etc.) at runtime towards
the optimization goal (e.g., system throughput), and (2) cooptimizing for conflicting goals simultaneously at runtime
(e.g., throughput and fairness). An unfair system, even with
high throughput, is undesirable since it leads to some users
experiencing a disproportionate slowdown, incorrect resource
usage charging, delay of scientific discovery, and bad user
experience (indirect monetary loss). Fairness of the system can
be quantified by widely-used fairness index (e.g., Jain‚Äôs Fairness
Index [35]), which can capture the degree of ‚Äúsimilarity‚Äù in
the performance degradation of co-located jobs compared to
co-location-free execution (Sec. II).
Consider a relatively simple scenario where multiple jobs are
co-located on a CMP machine and share only ‚Äúone‚Äù resource
(e.g., last-level cache), and the aim is to improve one single goal
(e.g., system throughput). Prior works have provided practical
solutions to the problem of this nature [7], [38], [46], [53],
[81], [90], [94]. For example, a recent solution dCAT [90]
changes last-level cache (LLC) ways allocation dynamically
among co-located workloads to achieve high throughput. The
core intuition behind these solutions is to assess the relative
‚Äúutility‚Äù of an additional share of a resource on a particular
job‚Äôs performance. Such an approach classifies some jobs as
‚Äúdonors‚Äù and others as ‚Äúreceivers‚Äù to improve the throughput.
Adding more shared resources (e.g., memory bandwidth)
makes the problem even more challenging, as previous works
such as CoPart, Heracles, and PARTIES have demonstrated [12],
[52], [66]. These works have taken two different approaches
to optimize the partitioning of multiple shared resources. In
the first approach, partitioning of individual resources is done
separately, but the decisions are communicated to reach a global
optimal resource partition. For example, CoPart [66] maintains
two separate finite state machines (FSM), one for shared cache
and one for memory bandwidth. These FSMs are not joint or
linked but are aware of each other‚Äôs decisions. In the second
approach, partitioning of individual resources is performed
in one dimension at a time (similar to a gradient descent
method). For example, Heracles [52] and PARTIES [12] (the
most recent resource partitioning strategy) perform resource
partitioning in a gradient descent style where partitioning of
one resource is explored first before adjusting the allocations
for other resources. CoPart is simple, but may not scale well
to multiple resources, unlike PARTIES and Heracles, that can
scale well to multiple resources.
However, a source of sub-optimality in both the approaches
is that these do not explore partitioning multiple resources
simultaneously. Joint exploration of multiple resources simultaneously is necessary since benefits of increasing one resource

"$.*&&&UI"OOVBM*OUFSOBUJPOBM4ZNQPTJVNPO$PNQVUFS"SDIJUFDUVSF	*4$"

¬•*&&&
%0**4$"
2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) | 978-1-6654-3333-4/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ISCA52012.2021.00031
allocation for a given job may not be realized fully until the
allocation for other resources are appropriately resized, but
this leads to search space explosion (Sec. II).
Finally, while optimizing for one goal is hard enough,
adding one more compounds the challenge further. This is
especially true if the second goal conflicts with the first goal
(e.g., improving both throughput and fairness). Prior works,
when optimizing for fairness or throughput, often resort to
targeting one goal and settle with best-effort improvement for
the other. For example, CoPart [66] focuses on improving
fairness and demonstrates that the throughput is not hurt
compared to the baseline (unmanaged partitioning of the
resources). This is because effectively controlling and finding
a balance between conflicting goals online is challenging on a
real system (experimental evidence in Sec. II).
SATORI: Key Contributions and Evaluation. SATORI is the
first solution that actively controls both system throughput and
fairness goals simultaneously, when multiple workloads are
co-located on a CMP machine and share multiple architectural
resources. SATORI proposes new solutions to existing gaps of
optimality in current approaches.
SATORI develops a new approach grounded in Bayesian Optimization (BO) theory to intelligently explore multi-resource
partitioning configuration space. In particular, SATORI‚Äôs BO
based approach enables us to design a practically-feasible
technique that can work on a real system in an online fashion.
The key intuition behind using BO is to build simple and
just-accurate-enough models for finding near-optimal solutions,
without requiring offline profiling, instrumentation, offline deep
learning/reinforcement based training or building complex
performance models which may incur high-overhead and
may not be portable to new situations depending upon the
nature of the offline training dataset [16], [45], [47], [48],
[54], [75], [85]. As our evaluation also confirms, tolerating a
slight inaccuracy of the model allows SATORI to achieve nearoptimal configurations in an online fashion. SATORI enables
fast and efficient navigation of large configuration space to
find the optimal resource partition configuration (Sec. III-A). It
performs joint exploration of multiple resources simultaneously
- removing the limitation of existing approaches that maintain
one finite state machine for each shared resource or explore
one resource dimension at a time.
One of the vital novel insights we discover is that it is
beneficial to trade-off one goal for another temporarily to
achieve higher returns on both the goals over long-term
(as discussed in Sec. III-C). To exploit this observation,
SATORI intelligently prioritizes one goal over the other on
a short-term basis but then strikes a balance between the two
goals on a long-term basis ‚Äì to the best of our knowledge,
SATORI is the first solution to observe and exploit this finding.
Unfortunately, dynamic re-prioritization of goals causes the
objective function, which is being optimized, to change over
time, and hence, it becomes challenging and ineffective to
apply BO. To address this challenge, SATORI implements a
new enhancement that dynamically and separately monitors
the system-level throughput and job-level fairness indicators
to construct a new overall combined BO objective function
targeting multiple goals. This objective function successfully
changes the prioritization of the goals, while maintaining the
expected behavior of the traditional BO by bounding the
magnitude and period of dynamic re-prioritization of goals
(Sec. III-B). The inherent design of SATORI makes it suitable
for additional concurrent optimization goals.
SATORI evaluation demonstrates its effectiveness across a
range of scenarios and parallel workloads. For example, in
terms of throughput, SATORI outperforms most recent workload
co-location techniques including dCAT, CoPart, and PARTIES
by 19%, 17%, and 14%, respectively while achieving higher
fairness at the same time by 25%, 17%, and 14%, respectively.
SATORI performs within 8% of the offline, practically-infeasible
oracle scheme. SATORI can be complementary and potentially
combined with recent targeted approaches based on economics,
resource multiplexing, and machine learning (including [43],
[59], [60], [62], [64]).
SATORI is available at https://github.com/rohanbasuroy/satori
II. CHARACTERIZING CHALLENGES AND OPPORTUNITIES
IN MULTI-GOAL SHARED RESOURCE PARTITIONING
New CMP servers consist of multiple resources that can be
shared across workloads (e.g., cores, LLC, memory bandwidth,
power). Chip manufactures provide a mechanism to partition
these resources among workloads in a dynamically configurable
way (e.g., Intel‚Äôs CAT, MBA, RAPL [1], [15], [33]). Similar
to previous works, we leverage these capabilities to partition
resources among concurrently co-located workloads. Before
analyzing our characterization study, we define the terms used
in this paper.
Resource partitioning configuration (or simply, a configuration) refers to shares of different jobs of shared architectural
resources. For example, consider jobs A and B, which share
two resources ‚Äì cores (6 cores) and LLC (4 cache ways per
set). A configuration is defined as one permutation of resource
allocation of all available resources to all co-located jobs. Thus,
a possible configuration is {3 (50%) cores to job A, 3 (50%)
cores to job B, 1 (25%) cache way per set to job A, and 3
(75%) cache ways per set to job B}.
System throughput and fairness objectives can be expressed
using multiple metrics as described below. These metrics offer
different advantages and have been widely-used in various
past works to maximize the system throughput and fairness by
optimizing the chosen metric [4], [27], [37], [66], [90].
System throughput can be expressed using multiple metrics.
For example, it can be expressed as the geometric mean of
the speedups of the co-located jobs as compared to their
baseline isolation performance at a given time during their
run:  N
i=1 si
 1
N
, where N is the number of co-located jobs
and si is the speedup of the i
th job. It can also be expressed
as harmonic mean of speed ups, or as the sum of instructions
per second [13], [20], [69], [88].
Fairness measures the degree of ‚Äúsimilarity‚Äù in the performances of co-located jobs. It can also be expressed using
multiple metrics. For example, we use Jain‚Äôs Fairness Index [35]
as our metric of fairness: 1
1+CoV2 , where CoV is the coefficient
of variation (standard deviation as a fraction of the mean) of
the speedups (from baseline isolation performances) of the
co-located jobs. Thus, if the standard deviation of job speedups
is small (i.e., the speedups are similar and hence, the jobs are
treated highly fairly), the CoV is small, and the fairness index
is close to 1. On the other hand, if it is large, the CoV is
  
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Optimal Throughput Configuration
blackscholes
freqmine
canneal
streamcluster
fluidanimate
0
25
50
75
100
Cores (%)
0
25
50
75
100
LLC Ways (%)
0 5 10 15 20 25 30 35 40 45 50
Time (s)
0
25
50
75
100
Mem. B/w (%)
Fig. 1. The configuration which achieves optimal throughput
changes significantly and frequently over time, for all shared
architectural resources.
0 25 50 75 0 25 50 75 0 25 50 75 100
 Cores (%) LLC Ways (%) Mem. B/w (%)
Fairness
Oracle
Throughput
Oracle
blackscholes
freqmine
canneal
streamcluster
fluidanimate
Fig. 2. The optimal configurations for throughput and fairness
differ significantly at any point.
large, and the fairness is close to 0. Another fairness metric is:
1 ‚àí CoV, which evaluates to 1 in the case of perfect fairness
and can also be negative in cases of unfairness [21], [73].
Next, we note that finding optimal configuration requires
searching a large configuration space. Unfortunately, the
configuration search space grows prohibitively as the number
of co-located jobs and shared resources increase. Formally,
suppose there are Nres resources, Mjobs co-located jobs, and
the rth resource has Uunits(r) units of the resource, then
the size of the configuration space can be expressed as
Sconf = Nres
r=1 Uunits(r)‚àí1
Mjobs‚àí1

.
For example, if three co-located jobs share just two resources
(e.g., number of cores and memory bandwidth), each resource
with 10 units, then the total number of possible configurations
is 1, 296. When the number of co-located jobs increases to
four, the size of the configuration space grows to 7, 056.
Simply adding one more shared resource increases the size
of the configuration space to 5,92,704. Worse, the optimal
configuration does not remain the same over time, even for
the same set of co-located jobs. To better understand the
magnitude of this challenge, we tracked how the optimal
resource configuration changes over time for a five-job mix
sharing three shared architectural resources (i.e., cores, lastlevel cache, memory bandwidth). Optimal resource partitioning
is obtained via an exhaustive offline search of all possible
configurations to maximize the system throughput.
Fig. 1 shows the optimal configuration for throughput over
time for a job mix consisting of 5 jobs from the PARSEC
benchmark suite [6] (evaluation methodology details in Sec. IV).
We observe that the configuration to achieve optimal throughput
can change by more than 20% during the course of the run.
In essence, this is because jobs may have different program
phases, and each phase may have different sensitivity toward
architectural resources. Unfortunately, exploring the full search
space to find the optimal resource partitioning configuration
requires several hours of exhaustive search. We found that the
optimal configuration for fairness also varies dynamically and
significantly (results not shown for brevity).
Observation 1. Search space of architectural resource
partitioning configurations grows prohibitively large as the
number of shared architectural resources and co-located jobs
increase. Also, the optimal resource partitioning configuration
is challenging to find and frequently changes over time.
Next, Fig. 2 demonstrates the differences in the optimal
configuration for the two conflicting goals at the same point
during the execution. These configurations are significantly
different for the two goals (up to 40% difference). Employing
throughput optimized configuration achieves only 67% of the
fairness obtained using fairness optimized resource partition.
Similarly, fairness optimized resource partition achieves only
59% of the throughput obtained with throughput optimized
resource partition.
One may hypothesize that the ‚Äúaverage‚Äù of the optimal
resource partitioning configurations for both goals might
perform reasonably well for both the goals and might be a good
compromise. Note that obtaining this ‚Äúaverage‚Äù optimal configuration is also practically infeasible since it requires obtaining
several hours of exhaustive offline search to obtain the optimal
configuration for each goal separately. Even hypothetically
assuming that optimal configuration is available instantaneously
using oracle knowledge, such a derived ‚Äúaverage‚Äù optimal
configuration performs poorly in both aspects. It achieves only
59% of the oracle throughput and 72% of the oracle fairness.
Observation 2. Optimal resource partitioning configurations
for conflicting goals (e.g., system throughput vs fairness)
can be very different from each other. Finding a resource
partitioning configuration that strikes a balance between the
conflicting goals is challenging.
An alternative intuitive method to strike a balance between
throughput and fairness is to optimize for throughput half of
the time and optimize for fairness the other half, although
hypothetically assuming that optimal resource partitioning
configuration is available instantaneously. Our results revealed
that this approach also leads to significantly sub-optimal results
(achieving only 72% of the oracle throughput and 81% of
the oracle fairness). But, slight improvement over previous
‚Äúaverage‚Äù optimal resource partitioning method led us to pose
a fundamental question: do we have an equal opportunity
(or difficulty) in finding the balance between throughput and
fairness over time? In other words, is it easier to obtain higher
throughput at some point with small temporary sacrifice in
fairness. At a later point, when it becomes easier to obtain
higher fairness with small temporary sacrifice in throughput
such that there is a net gain in at least one goal when both the
points are considered together. Another intuition behind this
exploration is the following: since the optimal configurations
for both throughput and fairness changes over time, their
respective contributions toward throughput and fairness may
not remain static either, and this should open up an opportunity
  
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Throughput Fairness
Period Œît1
(a)
0
2
4
6
8
Diff. in Perf. of
Ca and Cb (%)
Period Œît2
(b)
0
2
4
6
8
Diff. in Perf. of
Cc and Cd (%)
Period Œît3
(c)
0
2
4
6
8
Diff. in Perf. of
Ce and Cf (%)
Period Œît4
(d)
0
2
4
6
8
Diff. in Perf. of
Cg and Ch (%)
Fig. 3. There exists an opportunity to re-balance the conflicting
goals (throughput and fairness) over time and one does not
need to put ‚Äúequal emphasis‚Äù on both the goals at all times.
for ‚Äútemporarily‚Äù trading one goal for another.
Fig. 3 (a) shows the differences in throughput obtained with
two sample configurations at point Œît1 and the corresponding
difference in fairness for the same two sample configurations
at point Œît1. Fig. 3 (b) quantifies the same metrics but at a
different point Œît2 for two different sample configurations.
Note that this example uses the same five-job mix as Fig. 1
using the same methodology. The choice of configurations
Ca, Cb, Cc, and Cd and periods Œît1 and Œît2 is made for
simplicity to provide evidence for the following observation
(there are many other similar configurations).
We observe that at both points (Œît1 and Œît2), the percentage
difference in throughput between the two configurations is the
same. However, the corresponding difference in fairness for
the same two configurations is in different directions (at Œît1
it is lower and at Œît2 it is higher). This suggests that if one
prioritizes throughput at Œît1 and prioritizes fairness at Œît2,
there is a net gain in fairness without sacrificing throughput.
Fig. 3(c) and (d) show a similar example where there is a net
gain in throughput without sacrificing fairness, by selectively
prioritizing one goal over the other at different times.
Observation 3. There is an opportunity to re-balance
conflicting goals over time, i.e., prioritizing one goal over
the other for a short duration and vice versa during another
period yields higher benefits.
SATORI bases its design on the above insights and findings.
III. SATORI: DESIGN AND IMPLEMENTATION
SATORI leverages Bayesian Optimization (BO) to efficiently
navigate the search space to find near-optimal partitioning
configurations (Sec. III-A discusses the details of the solution).
As a high-level summary, BO builds a low-overhead proxy
model to predict the performance of different configuration
samples and uses an acquisition function, which directs BO
toward samples to evaluate (principled, intelligent exploration
of the search space). BO‚Äôs underlying proxy model is not wholly
accurate by design but becomes iteratively more accurate and
suitable for online use. Employing a proxy model underneath
and navigating the configuration space with the help of an
acquisition function, allows SATORI to adjust partitions for
multiple resources simultaneously.
Next, SATORI carefully constructs the objective function that
incorporates methods to achieve multiple goals (i.e., throughput
and fairness) at the same time. SATORI‚Äôs objective function
is also carefully designed to support one of the key ideas
of SATORI: ‚Äúprioritize‚Äù one goal over another temporarily to
exploit the opportunity of re-balancing conflicting goals over
time (i.e., configurable control). Finally, SATORI‚Äôs objective
function is designed to be extensible and can include additional
goals (e.g., energy-efficiency). The BO exploration process
optimizes this objective function to find the near-optimal
configuration.
Notably, SATORI does not require any programmer hints,
recompilation, new architecture support, or instrumentation.
SATORI can be deployed readily on platforms where hardware
support for architectural resource partitioning is available.
A. How does SATORI employ Bayesian Optimization for
efficient search space exploration?
BO is a theoretically-grounded method for solving the
problem of maximizing a black-box objective function, f. BO
does not need to know the relationship between the input
and the objective function. BO incorporates a prior belief of
the objective function, and by querying it multiple times, it
develops a posterior that better approximates f. Traditionally,
the optimization aims to figure out the input x‚àó that maximizes
the objective function f(x) with minimum possible queries.
Mathematically, BO can be expressed as following (where X
is the search space of interest):
x‚àó = argmax x‚ààX
f(x) (1)
In the context of SATORI, the input, x, to the objective
function, f, is a resource partition configuration which allocates
different units of each resource to different co-located jobs. All
such possible resource partitioning configurations comprise
the search space X and are referred to as sample points in the
space. The objective function, f, is unknown because SATORI
does not know the relationship between the input configuration
and the objective function (i.e., the function that expresses the
relationship between a given configuration and corresponding
outcome in terms throughput and fairness).
However, SATORI can evaluate the value of the objective
function, f, given an input x (resource partition configuration)
by running the system with the given configuration and
observing the performance of all co-located jobs. The value of
the objective function indicates the quality of the figure of merit
(e.g., throughput, or fairness). However, unlike traditional cases
where the optimization problem is to maximize the objective
function [3], [31], [32], [57], [58], [72], SATORI requires careful
construction of the objective function due to the dynamically
changing nature of the objective (dynamic prioritization among
multiple goals). This aspect of SATORI and the corresponding
solution is discussed later in Sec. III-B. SATORI uses BO to
find the optimal value of the objective function with just a
few function evaluations (i.e., running a minimum number of
configurations).
To identify the global optimum of the objective function,
BO intelligently explores the search space by evaluating the
objective function with different input samples (configurations).
As it explores the sample space, it builds a stochastic model
of the unknown objective function and keeps updating it, and
uses this knowledge to identify which samples to evaluate
next such that it reaches near the global optimum. The two
key components that enable this are: (1) the proxy model,
and (2) the acquisition function. BO uses a proxy model,
M(x), to stochastically estimate the performance of different
configurations in the space. This stochastic model is improved
iteratively during the space exploration process, as more

Sampled
Configurations
True Objective
Function f(x)
Predictive
Model‚Äôs Mean
Confidence Interval of
the Predictive Model
Acquisition
Function a(x)
Exploration
Exploitation
Exploration
A BC
Application
#Cores
#LLC Ways
Mem. B/w
A configuration
(denoted as x) refers to
different applications‚Äôs
share of different
resources
Resources
Fig. 4. Visual depiction of Bayesian optimization (BO) components including configuration, true objective function, proxy
model, and acquisition function.
Algorithm 1 SATORI BO Engine Algorithm.
1: Input: Initial resource configurations (Sinit).
2: Run the system with Sinit & record throughput and fairness.
3: Record baseline (isolation) performances.
4: while TRUE do
5: Generate objective function (details in Sec. III-B and III-C).
6: Update the proxy model M(x).
7: Compute the acquisition function a(x).
8: Optimize a(x) and find the next sample to evaluate.
9: Run the system with the selected configuration x.
10: Record throughput and fairness for x. 11: & add these values to the existing set.
12: if End of a job or Start of new job or Every reset interval
13: Reset baseline (rerecord isolation performances).
points are sampled. Fig. 4 shows a snapshot of the state of a
BO execution during exploration where 6 points are already
sampled, and BO has estimates for the rest of the points as
predicted by the proxy model. We note that the proxy model
predicts a range of values for non-sampled points - it consists
of both predicted mean and variance. The variance captures
the model‚Äôs ‚Äúuncertainty‚Äù at a given point. SATORI chooses
Gaussian Process (GP) as the proxy model, which means that
each sample point (configuration), the uncertainty follows a
Gaussian (Normal) distribution defined by the mean (Œº) and the
standard deviation or uncertainty (œÉ). SATORI uses the Matern ¬¥
covariance kernel ( 5
2 ) for its GP proxy model [42], [77].
Next, BO needs to steer itself toward the global optimum by
sampling the ‚Äúmost promising points‚Äù. BO uses the acquisition
function to move in the right direction during the space exploration process. That is, an acquisition function a(x) dictates
which configuration points to sample next such that BO moves
closer to the best performing samples in the space. As shown in
Fig. 4, acquisition function returns different values for different
non-sampled points at a given stage; mostly, the acquisition
function wants the BO to explore different neighborhoods. It
strikes a balance between ‚Äúexploration‚Äù (exploring the areas
of the configuration space with significant uncertainty) and
‚Äúexploitation‚Äù (exploiting the vicinity of the neighborhood
where the predicted mean value of the proxy model is high).
SATORI chooses the Expected Improvement (EI) method as the
acquisition function that provides a reasonable balance between
exploration vs. exploitation at a low evaluation cost [77]. Note
that as more points are sampled, the proxy model is updated,
and the acquisition function is re-evaluated after each sample.
This iterative process of SATORI is summarized in Algorithm 1.
Here Sinit refers to the initial resource configuration in which
all available resources are divided equally among the workloads.
B. Constructing SATORI‚Äôs Objective Function
Unlike the traditional BO implementation, where evaluating
the objective function returns a single value (e.g., runtime)
to be maximized or minimized, SATORI needs to balance
multiple goals (throughput and fairness). But, to make decisions
about which configurations to evaluate next, SATORI still needs
to assess the ‚Äúgoodness‚Äù of a configuration when evaluated
(i.e., the system is run for a short duration under a given
configuration).
To this end, SATORI designs an objective function (i.e., a
performance score that is assigned to a configuration at the end
of the period when the system is run under a given resource
partition configuration). This function guides SATORI to search
in the right direction in the large configuration space. To achieve
that effectively, the objective function needs to treat both the
goals equally and be comparable. Unfortunately, in general,
different goals are calculated using different formulas and have
different ranges. For instance, when fairness is calculated as
1 ‚àí CoV, where CoV is the coefficient of variance of the
speedups of the jobs compared to their respective baseline
isolation performances, the fairness metric has no lower bound.
So we normalize them to obtain the same range of 1 to 0. Also,
SATORI needs to be configurable where different weights can
be given to different goals. Therefore, the objective function
can be expressed as:
f(x) = 
K
i=1
Wi √ó Goali(x) = WT √ó T(x) + WF √ó F(x)
(2)
Here, WT is the weight on throughput T(x) and WF is
the weight on fairness F(x) of the evaluated configuration x.
However, note that this objective function is configurable. It can
be reduced to work with just one goal (e.g., throughput), and
it can be extended to work with K different goals. While, this
objective function works if weights are configured at the start
and never changed during the runtime, recall that we need to do
dynamic re-balancing of weights to exploit the opportunity of
temporary prioritization (discussed in detail in Sec. III-C). This
means that the BO engine needs to allow the weights WT and
WF to change over time. However, traditional BO cannot deal
with this as it only optimizes over a single non-dynamicallychanging objective function, as shown in Fig. 5. Traditional
BO optimizes its static objective function as it collects more
configuration samples (3 samples at t1 to 4 samples at t2).
However, SATORI‚Äôs objective function changes between two
already sampled points (e.g., Ca and Cb). The example shown
in the figure has equal weights (0.5) on throughput and fairness
at t1, and the objective function is constructed accordingly
(same as traditional BO in the example). However, at t2, the
weights change to place 0.75 weight on throughput and 0.25 on
fairness, and the objective function needs to be reconstructed
accordingly.
In traditional BO, when the objective function changes, the
BO engine has to re-sample already sampled configurations to
reconstruct the proxy model for the objective function. However,
this incurs a high overhead because all the configurations
are required to be re-sampled in every iteration, which is
prohibitively infeasible in a real online system.
To combat this problem, SATORI employs a novel technique
that maintains a separate record of configuration-dependent

Objective Function
Configuration
Time t Time t
New Sampled Configuration
Traditional Bayesian Optimization
SATORI‚Äôs Bayesian Optimization
2
Throughput ( T ) Fairness ( F )
f(x) = 0.5 T + 0.5 F
Objective Function
Configuration
f‚Äô(x) = 0.75 T + 0.25 F
New Sampled Configuration
Time t1 Time t2
1
Cb Cc
f(x) f(x)
Ca Ca Cb Cd Cc
Fig. 5. SATORI maintains separate throughput and fairness
performances and constructs a fresh objective function every
iteration based on dynamic weights.
performances of every single conflicting goal. The benefits of
this strategy are multi-fold: (1) The reduction in the overhead
of objective function reconstruction is significant. Instead of
re-sampling the configurations, this enables us to use existing
goal-specific records to perform a software-based reconstruction
of the proxy model. While this has slightly higher overhead
than simply updating proxy model like in traditional BO,
our evaluation (Sec. V) shows that this overhead does not
interfere with the execution of co-located jobs, and in fact, the
performance gain outweighs the overhead significantly. (2) This
approach makes SATORI portable, customizable, and extensible
to multiple objectives without much user-based coding effort
since the goals are weighed independently of each other (Eq. 2).
Next, we discuss how we dynamically re-balance the weights
set on throughput and fairness.
C. Dynamic Prioritization of Goals in SATORI
SATORI delivers better performance by dynamically setting
different priorities on throughput and fairness. However, it still
has to ensure that, over the long-term, both throughput and
fairness achieve equal priorities. As discussed in Sec. III-B,
this is achieved in the form of ‚Äúweights‚Äù. On average, both
throughput and fairness should receive an equal weight of 0.5.
To do so, SATORI uses an equalization period (TE) over which
the weights received by throughput and fairness have to be 0.5
on average. Moreover, SATORI also uses a shorter prioritization
period (TP ) over which it can prioritize one goal over the
other. Therefore, both throughput and fairness have two weight
components: equalization and prioritization.
As the purpose of the equalization weight is to ‚Äúequalize‚Äù,
it is set according to the imbalance between the throughput
and fairness weights so far in the current equalization period.
For instance, if throughput has received more weight so far,
then its equalization weight should be decreased as the end of
the equalization period approaches. Therefore, SATORI assigns
the weights as following:
WT E = 1
2
te ‚àíte
i=1
WTi & WF E = 1
2
te ‚àíte
i=1
WFi (3)
Near the start of ŒîTE Near the end of ŒîTE ŒîTE
ŒîTP ŒîTP
Throughput
Fairness
ŒîT
ŒîF
Throughput
Fairness
ŒîT
ŒîF
Update throughput (fairness)
weight according to WTP (WFP)
with less effect of WTE (WFE)
WTP
WFP
WTE
WFE
Update throughput (fairness)
weight according to WTP (WFP)
with dominant effect of WTE (WFE)
WTP
WFP
WTE
WFE
Fig. 6. The prioritization weights depend on improvement in
throughput and fairness performances during the prioritization
period. The equalization weights are more dominant toward
the end of the equalization period.
Here, WT E (WF E) is the equalization throughput (fairness)
weight, te is the amount of time elapsed in the equalization
period, and WTi (WFi ) is the throughput (fairness) weight
during the i
th iteration (note that time is discretized here (0.1s
intervals as configured in the current implementation) and
hence, is represented as a summation). Thus, the equalization
weight is set based on the amount of time elapsed and the
previous weights set during the current equalization period.
On the other end, the prioritization weight is set based
on the percent change in throughput and fairness during
the prioritization period. Intuitively, the prioritization weight
of throughput (fairness) should be adjusted based on how
well the fairness (throughput) goal was achieved in the
previous prioritization period. If fairness is given higher priority
during the previous period and if it successfully utilized this
opportunity, throughput should get this opportunity during the
next interval, and vice versa. Therefore, SATORI assigns the
prioritization weights as following:
WT P = 1
4 +
1
2
ŒîF
ŒîT + ŒîF
& WF P = 1
4 +
1
2
ŒîT
ŒîT + ŒîF
(4)
Here, WT P (WF P ) is the prioritization throughput (fairness)
weight, ŒîT (ŒîF ) is the % improvement in throughput (fairness)
from the beginning of the prioritization period (TP ) until its end.
Evidently, for example, if the % improvement in fairness ŒîF
is higher during the previous prioritization period, then the new
prioritization weight for throughput WT P will be higher. Note
that the constants in Eq. 4 help SATORI limit the prioritization
weight between 0.25 and 0.75 so as to not allow weights to
be 0 and 1 which can be completely disadvantageous to a
single goal and also cause a high degree of unstable weight
fluctuations from one sampling interval to another. We also
experimented with favoring the well-performing objective in
next period and found that, while reasonably effective, it can
underperform the chosen design by approximately 5%.
To summarize, Fig. 6 shows how the equalization weight
depends on how far along SATORI is within the equalization
period. In contrast, the prioritization weight depends on the
improvement in throughput and fairness during the prioritization period. The next task is to combine the equalization
and prioritization weights to generate the final throughput
(WT ) and fairness (WF ) weights. To combine the two, the
key requirement is to gradually increase the importance of
equalization weight as the end of the equalization period
approaches to ensure the average of equal weights is maintained.
This ensures that neither fairness nor throughput is favored

more than the other in the long run, as demonstrated by the
following equations:
WT = te
TE
WT E +

1 ‚àí te
TE

WT P (5)
WF = te
TE
WF E +

1 ‚àí te
TE

WF P (6)
The influence of the equalization component of the weight
is linearly increased as the end of the equalization period
approaches. We choose a linear increase as opposed to a
stronger form of increase (e.g., exponential) to ensure that
the equalization component does not completely overpower
the weight near the end, which would result in temporary
prioritization opportunities not being adequately exploited. To
be aware of phase changes and workload mix changes, SATORI
resets its baseline every equalization period by measuring the
isolated performances of the co-located workloads. Be it a
phase change or a change in the workload mixes, SATORI
requires no further initialization. It adaptively configures itself
to find the optimal configuration. Note that SATORI does not
disqualify previously sampled configurations from re-evaluation
(e.g., upon phase or workload-mix change), and reacts to the
program phase changes as our evaluation confirms (Sec. V).
Implications of dynamically changing objective function of
SATORI. SATORI constructs a new objective function to exploit
the temporal opportunities for maximizing competing goals
(dynamic prioritization). It does so by dynamically changing
the weight factors. However, this naturally leads us to ask:
what does this do to our BO process? This strategy can make
different choices than what a traditional BO-based approach
would have made. This is because ‚Äúmoving the goal post‚Äù can
affect SATORI‚Äôs belief about the search space and subsequent
decisions. The key is to ensure that the expected deviation
is bounded. This is why SATORI sets the upper and lower
bound on the weight factors as 0.75 and 0.25, respectively,
and keeps the equalization and prioritization periods bounded.
Our evaluation (Sec. V) shows that this is effective in keeping
the BO process controlled and near the traditionally expected
behavior, while maximizing the objective function and hence,
achieving better performance. We also point out that SATORI
predicts a promising configuration to evaluate, but the ‚Äúpredicted expected improvement‚Äù of the configuration itself is not
directly used. Instead, the configuration is indeed evaluated and
compared against previous configuration evaluations. Therefore,
the tweaked perception of the underlying BO proxy model
does not affect the function and correctness. In fact, our
evaluation (Sec. V) also confirms that the variation in observed
performance for SATORI is similar to SATORI without dynamic
prioritization, while achieving a higher average performance
level. Also, the reached performance level remains stable,
similar to SATORI without dynamic prioritization unless the
phase related characteristics of the workloads change. Finally,
we also note that SATORI needs to be run for a whole set of
co-located workloads instead of one SATORI for each possible
pair of workloads in a given set.
IV. EXPERIMENTAL METHODOLOGY
Testbed and Implementation. The experiments are conducted
on a Intel Xeon Scalable platform with 14nm Skylake
TABLE I. PARSEC benchmarks [6] used in this study.
Blackscholes Option pricing with Black-Scholes Partial Differential Eq.
Canneal Simulated cache-aware annealing to optimize chip design
Fluidanimate Fluid dynamics for animation with Smoothed
Freqmine Frequent itemset mining
Streamcluster Online clustering of an input stream
Swaptions Pricing of a portfolio of swaptions
TABLE II. CloudSuite benchmarks [22] used in this study.
Data Analytics Naive Bayes classifier on Wikipedia entries
Graph Analytics Page ranking on Twitter data
In-memory Analytics In-memory filtering of movie ratings
Media Streaming Nginx server to stream videos
Web Search Web search algorithm implementation
processors. The server has 10 physical cores with 2-way
hyperthreading, 85W thermal design power, turbo-boosting,
and shared last level cache. SATORI logic is implemented as a
lightweight Python-based monitoring service leveraging pqos
to measure performance, but does not require user intervention,
profiling, or additional hardware support and deployable as a
user-space system software. SATORI uses taskset for setting
core affinity, Intel‚Äôs Cache Allocation Technology (CAT) for
LLC cache ways assignments, and Intel‚Äôs Memory Bandwidth
Allocation (MBA) feature for memory bandwidth partitioning
via setting Model Specific Registers (MSRs). The effect of
SATORI‚Äôs monitoring and decision making is included in all
the results and characterized in the overhead (Sec. V).
The instructions per second (IPS) of each workload is
collected with pqos tool, sampled at a rate of 10 Hz. Methodological pitfalls associate with IPS due to synchronization are
avoided for multi-threaded workloads [2], [20], [83]. Fixedwork methodology [29], [41], [66], [90] (using the same
number of instructions/runs similar to other partitioning realsystem partitioning works [41], [66], [90]) is employed and
we have confirmed that these methodological choices do
not negatively affect our results or conclusions. The default
metrics used are Jain‚Äôs fairness index and sum of instructions
per second as these have been used by other competing
techniques [66], [90]. Our evaluation confirmed that SATORI
provides similar improvements over competing techniques
for other commonly-used objectives metrics. This is because
the core ideas and insights behind SATORI design is not
metric-dependent and do not favor workloads with specific
behavior. SATORI is implemented using Skopt and Hyperopt [5],
which are distributed hyperparameter optimization libraries. An
online evaluation of the isolated performance of each of the
workloads is performed as a baseline for the measurement of
throughput and fairness. Prioritization and equalization periods
are configured to 1 second and 10 seconds, respectively, by
default, but other values provide similar trends (Sec. V).
Workloads. SATORI is evaluated using a set of representative parallel, throughput-oriented workloads chosen from
the PARSEC-3.0 benchmark suite [6] (listed in Table I).
Additionally, to demonstrate the effectiveness of SATORI on
a diverse set of workloads, SATORI is also evaluated on the
benchmarks from the CloudSuite benchmark suite [22] and the
Exascale Computing Project (ECP) benchmark suite [56], [70]
(Table II and III). Unless otherwise mentioned, the presented
results correspond to the PARSEC suite.
SATORI is evaluated by co-locating combinations of different
applications in a job-mix. 5 workloads out of the 7 PARSEC
workloads in Table I are chosen in a mix ‚Äì resulting in

TABLE III. ECP benchmarks [56], [70] used in this study.
miniFE Unstructured finite element solver
XSBench Computational kernel of Monte Carlo neuronic
SWFFT Fast Fourier transform for HACC (cosmology code)
AMG Parallel algebraic multigrid solver for linear systems
Hypre Scalable linear solvers and multigrid methods
a total of 21 (7
5
 = 21) different workload configurations.
Relatively high degree of co-location is chosen as it presents
more challenging situation for SATORI for optimization. 3 and
2 workloads out of the 5 CloudSuite workloads (Table II) and 5
ECP workloads, respectively, are chosen (Table III) ‚Äì resulting
in a 10 different workload configurations for each benchmark
suite. SATORI uses the underlying BO scheme to update its
resource allocation configuration every 0.1 seconds.
Competing Resource Partitioning Policies. We compare the
performance of SATORI with the following policies:
‚Ä¢ Random Search (Random) samples a configuration
stochastically from all possible configurations using a
uniform distribution without repetition. The sampled
configuration is updated every 0.1 second.
‚Ä¢ dCAT improves throughput by dynamically partitioning a
single shared resource (last-level cache) [90].
‚Ä¢ CoPart primarily focuses on improving fairness by
partitioning two resources ‚Äì last-level cache and memory
bandwidth using two separate FSMs for each resource [66].
‚Ä¢ PARTIES primarily targets achieving Quality of Service
(QoS) of co-located multiple latency-critical workloads [12].
We note that PARTIES is designed for meeting QoS targets
of co-located latency-critical jobs. A consequential caveat
is that PARTIES should not be necessarily expected to
perform for the situation it was not designed for. However,
we included PARTIES in our evaluation because the core
idea behind PARTIES is partitioning shared resources
using a gradient descent method ‚Äì and, gradient descent
method can be applied for resource partitioning among
throughput-oriented workloads via iteratively optimizing
for throughput to find the optimal resource partitioning
configuration, although it was originally applied in the
context for latency-critical workloads (our evaluation
confirms the promise of gradient descent method in this
new context). However, this method requires adjustment to
simultaneously optimize for throughput and fairness. We
have modified PARTIES to maximize both throughput and
fairness, giving equal priority to both.
‚Ä¢ Brute-Force Search (Oracle). Oracle refers to the
brute-force, offline search strategy, which samples all
possible configurations and selects the one which maximizes
a given goal or a combination of goals. It is calculated
every 0.1 seconds to account for the phase changes
(Section II). This Oracle strategy has three different variants:
Throughput Oracle. This solely maximizes throughput and
ignores the fairness goal (i.e., WT = 1 and WF = 0).
Fairness Oracle. This solely maximizes fairness and
ignores the throughput goal (i.e., WT = 0 and WF = 1).
Balanced Oracle. This puts an equal priority on maximizing
Random
dCAT
CoPART
PARTIES
SATORI
Throughput
SATORI
Throughput
Oracle
60
70
80
90
100
110
120
Throughput (% of
Balanced Oracle)
59
73 75 78
92 95
109
(a) Throughput
Random
dCAT
CoPART
PARTIES
SATORI
Fairness
SATORI
Fairness
Oracle
20
40
60
80
100
120
Fairness (% of
Balanced Oracle)
32
67 75 78
92 96
108
(b) Fairness
Fig. 7. SATORI achieves better throughput and fairness than
other techniques (averaged across different job mixes, PARSEC
benchmarks).
for throughput and fairness (i.e., WT = 0.5 and WF = 0.5).
This strategy acts as the ceiling that SATORI aims to
touch when maximizing for both throughput and fairness.
Therefore, all results are presented as % of Balanced Oracle
(i.e., % distance from the theoretical optimal).
‚Ä¢ Throughput and Fairness SATORI. SATORI dynamically
changes WT and WF during runtime to maintain a balance
between the conflicting goals of maximizing throughput and
fairness. Throughput SATORI refers to a specific variant of
SATORI with WT = 1 and WF = 0 (i.e., maximize only
throughput). Fairness SATORI refers to a specific variant
of SATORI with WT = 0 and WF = 1 (i.e., maximize
only fairness). These variants are evaluated and analyzed to
quantify the limits of SATORI when optimizing a single goal.
V. EVALUATION: RESULTS AND ANALYSIS
SATORI actively achieves both throughput and fairness goals simultaneously. SATORI provides near-optimal
throughput and fairness with significant improvement over
competing techniques. Fig. 7 quantifies the performance of all
considered techniques (on average for 21 job mixes consisting
of 5 jobs as percentage of the performance of Balanced Oracle).
We make the following three observations:
First, SATORI outperforms competing techniques, performing
14% points better than the next best technique PARTIES for
throughput and fairness. Notably, SATORI achieves 92% of the
throughput and fairness of the Balanced Oracle.
Second, the throughput of SATORI with throughput as the
only goal (Throughput SATORI) is higher than the throughput
obtained by SATORI (which focuses on both goals), and close
to the ideal Throughput Oracle. This result shows that SATORI
can be configured to maximize a single goal (achieve high
throughput, close to the Throughput Oracle). Fig. 7(b) shows
similar trends for fairness.
Finally, as expected, Random policy performs the worst,
followed by dCAT, CoPart, and PARTIES. This confirms our
intuition that controlling multiple resources actively achieves
better performance (CoPart>dCAT). Also, our results show
that a gradient descent approach (PARTIES) is quite useful for
improving the throughput and fairness metrics.
Next, we dig deeper to quantify the effectiveness of SATORI
for individual job mixes. We plot the throughput and fairness
for all of the 21 job mixes individually in Fig 8. We observe
that SATORI provides better performance than competing
techniques consistently for all job mixes for both throughput
and fairness. In some cases, SATORI can have up to 20%
points better throughput and 10% points better fairness than
 
0 2 4 6 8 10 12 14 16 18 20
Job Mix ID
60
70
80
90
100
Throughput
(% of Balanced Oracle)
Random
dCAT
CoPART
PARTIES
SATORI
(a) Throughput
0 2 4 6 8 10 12 14 16 18 20
Job Mix ID
40
60
80
100
Fairness
(% of Balanced Oracle)
Random
dCAT
CoPART
PARTIES
SATORI
(b) Fairness
Fig. 8. SATORI achieves better throughput and fairness than
other techniques for every job mix (PARSEC benchmarks). The
results are sorted in ascending order of SATORI‚Äôs performance.
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Job Mix ID
60
70
80
90
100
Speedup of Worst
Performing Job (% of
Balanced Oracle)
Random dCAT CoPART PARTIES SATORI
(a) Worst Case Speedup Per Job Mix
Random
dCAT
CoPART
PARTIES
SATORI
60
70
80
90
100
Speedup of Worst
Performing Job (% of
Balanced Oracle)
64
70
75 75
87
(b) Average of Mixes
Fig. 9. The worst performing job in a mix performs better
with SATORI than other techniques across (a) all 21 job mixes
and (b) on average.
the next best technique PARTIES, but never worse than
competing techniques. In fact, Fig. 9 shows that the worstperforming job in a mix performs much better with SATORI
than with other techniques for every single job mix, on average
achieving 87% of the Balanced Oracle performance. This
result shows that benefits of SATORI are not dependent on
specific characteristics found in specific workloads or job mixes
(e.g., CPU-intensive, memory-intensive, cache-sensitive, etc.).
SATORI works effectively across a variety of workload mixes.
We analyzed the reasons for variation in SATORI‚Äôs benefits
across different job mixes. The mix consisting of canneal,
freqmine, streamcluster, swaptions, and vips (among PARSEC
benchmark suite mixes) has the highest throughput gain (job
mix 20) and replacing freqmine with fluidanimate results in
the lowest throughput gain (job mix 0). The reason is the high
compute-resource (number of cores) sensitivity of fluidanimate
(animation of fluid particles). Another example, Job mix 17
(canneal, freqmine, streamcluster, swaptions, vips) has high
throughput gain, however replacing swaptions and streamcluster
with blackscholes and fluidanimate results in low throughput
gain (job mix 3) as blackscholes and fluidanimate both contend
for same resource ‚Äì memory bandwidth.
To further test the effectiveness of SATORI across different
types of workloads, we evaluated different job mixes for
CloudSuite and ECP benchmarks (Fig. 10 and 11). We again
observe similar performance trends ‚Äì SATORI significantly
outperforms all competing techniques and consistently
outperforms across all job mixes for these benchmarks
too. CloudSuite and ECP benchmarks were chosen because
they represent a diverse set of workloads beyond PARSEC
benchmarks ‚Äì for example, data analytics kernels, graph
algorithm, and scientific computing applications (e.g., linear
solvers and physical simulation kernels). Our results show
that SATORI provides consistent improvement both in terms of
throughput and fairness for all job mixes for both CloudSuite
and ECP benchmark suites.
0 2 4 6 8
Job Mix ID
70
80
90
100
Throughput
(% of Balanced Oracle)
Random
dCAT
CoPART
PARTIES
SATORI
(a) Throughput
0 2 4 6 8
Job Mix ID
70
80
90
100
Fairness
(% of Balanced Oracle)
Random
dCAT
CoPART
PARTIES
SATORI
(b) Fairness
Fig. 10. SATORI achieves better throughput and fairness than
other techniques for every job mix (CloudSuite benchmarks).
The results are sorted by SATORI‚Äôs performance.
0 2 4 6 8
Job Mix ID
60
70
80
90
100
Throughput
(% of Balanced Oracle)
Random
dCAT
CoPART
PARTIES
SATORI
(a) Throughput
0 2 4 6 8
Job Mix ID
50
60
70
80
90
100
Throughput
(% of Balanced Oracle)
Random
dCAT
CoPART
PARTIES
SATORI
(b) Fairness
Fig. 11. SATORI achieves better throughput and fairness than
other techniques for every mix of two out of five co-located ECP
benchmarks. The results are sorted by SATORI‚Äôs performance.
The aggregate results indicate that in terms of throughput,
SATORI outperforms the next best technique (PARTIES) by
9% and 15% for CloudSuite and ECP benchmark suites,
respectively (Fig. 12 and Fig. 13). In terms of fairness, SATORI
outperforms the next best technique (PARTIES) by 5% and
15% for CloudSuite and ECP benchmark suites, respectively.
To understand the performance behavior better, we looked at
different job-mixes. SATORI achieves the lowest performance
for mix 0 of miniFE and SWFFT (ECP benchmark suite). The
main reason for this is that miniFE has intensive compute
(high IPC and FLOP rate) and last-level cache (high L1 missrate) requirements, which makes it difficult to co-locate with
SWFFT which has an equally high LLC requirement. On the
other hand, SATORI achieves the best performance for mix
9 of AMG and Hypre. Both of them have similar resource
requirements for all resources, which while making co-location
difficult, also makes the search space easier to navigate for
SATORI, resulting in near-optimal results. For all job mixes
representing a wide variety of characteristics, SATORI performs
better than competing techniques by more than 10% points in
terms of both throughput and fairness in most cases.
Next, we investigate the source of SATORI‚Äôs benefits. In
particular, we tested whether SATORI can outperform dCAT
and CoPart only because it partitions more resources (dCAT
partitions the LLC ways and CoPart partitions the memory
bandwidth and LLC ways). Interestingly, our results revealed
that SATORI‚Äôs approach is more effective than dCAT and
CoPart even when partitioning only one or two resources.
When SATORI only partitions the LLC ways, it performs 4%
points better than dCAT on throughput and 5% points better
on fairness. Further, when SATORI only partitions the LLC
ways and memory bandwidth, it performs 7% points better
than CoPart on throughput and 4% points better on fairness.
Therefore, the higher benefits of SATORI are not merely a
result of operating in a multi-resource design space. Note that

Random
dCAT
CoPART
PARTIES
SATORI
60
70
80
90
100
Throughput (% of
Balanced Oracle)
67
77 78 82
91
(a) Throughput
Random
dCAT
CoPART
PARTIES
SATORI
60
70
80
90
100
Fairness (% of
Balanced Oracle)
69
77 78 82
87
(b) Fairness
Fig. 12. SATORI achieves better throughput and fairness
(averaged across different job mixes, CloudSuite benchmarks). Random dCAT CoPART PARTIES SATORI
60
70
80
90
100
Throughput (% of
Balanced Oracle)
59
70 71 75
90
(a) Throughput
Random
dCAT
CoPART
PARTIES
SATORI
60
70
80
90
100
Fairness (% of
Balanced Oracle)
54
70 70 74
89
(b) Fairness
Fig. 13. SATORI achieves better throughput and fairness
(averaged across different job mixes, ECP benchmarks).
SATORI and PARTIES partition the same set of resources.
Why does SATORI work so well? Next, we look at the internal
functioning of SATORI to provide insight into its performance
gain. Fig. 14(a) provides a closer look at how SATORI
dynamically re-balances the prioritization and equalization
components of fairness and throughput weights. The two blue
regions combined indicate the overall throughput weight, and
the two yellow regions combined indicated the overall fairness
weight. We observe that overall throughput and fairness weights
can deviate by up to 50% from the equal weights mark of 0.5
due to temporary prioritization of one goal over the other.
However, the equalization component ensures that for the
equalization period (a configurable parameter, set to 10 seconds
in this example), the weights are 0.5 on average. This is why
the equalization component gets more dominant as we approach
the end of the equalization window of different lengths (10s,
20s, 30s, etc.).
To understand the effectiveness of the dynamic weight
prioritization strategy, we compare SATORI against SATORI
implemented with static 0.5 weights for fairness and throughput
(Fig. 14(b)). We observe that dynamic prioritization of weights
provides up to 10% additional benefit over the static weights
strategy for both fairness and throughput. More importantly,
dynamic weight prioritization opportunities and benefits are
present in all 21 job mixes and lead to improvements in both the
goals (i.e., not limited to improving only one goal: throughput
or fairness). This indicates that SATORI successfully leverages
short-term prioritization of one goal over the other to improve
performance significantly.
Next, Fig. 15 helps us explain that SATORI outperforms
competing techniques by choosing better configurations. To this
end, we quantify the proximity of the configuration obtained by
SATORI to the configuration obtained by the Balanced Oracle to
demonstrate its near-optimality and comparison with PARTIES.
We calculate the distance between the configuration allocated by
0 5 10 15 20 25 30 35 40 45 50
Time (s)
0.0
0.5
1.0
Weight
Prioritization Throughput Weight
Prioritization Fairness Weight
Equalization Throughput Weight
Equalization Fairness Weight
(a) Weight Re-balancing During Runtime
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Job Mix ID
0.0
2.5
5.0
7.5
10.0
Perf. Improvement over
Constant Weights (%)
Throughput Fairness
(b) Impact of Dynamic Weights
Fig. 14. (a) SATORI dynamically re-balances equalization and
prioritization throughput and fairness weights. (b) Dynamic
weight re-balancing improves performance. Random dCAT CoPART PARTIES SATORI
3.5
4.0
4.5
5.0
5.5
6.0
6.5
7.0
Configuration Distance
(from Balanced Oracle)
(a) Config. Distance
0 10 20 30 40 50
Time (s)
0
5
10
Distance From Balanced
Oracle (change in
background color
means phasal change)
SATORI PARTIES
(b) SATORI and PARTIES Config. Distance
Fig. 15. (a) The configurations set by SATORI are the closest
to the Balanced Oracle. (b) SATORI dynamically changes
configurations better than PARTIES as the phase changes.
different techniques and the configuration allocated by Balanced
Oracle. In this example, the configuration has 15 dimensions
(5 jobs √ó 3 resources), so calculating the distance between
two configurations is the same as calculating the Euclidean
distance between two vectors of 15 dimensions. Note that the
maximum possible distance is 13 in this scenario. Fig. 15(a)
shows that SATORI indeed achieves the closest proximity to
the Balanced Oracle (averaged over the runtime of a job mix
and across different job mixes), with other techniques having
at least 1.3√ó more distance of SATORI. Fig. 15(b) shows that
SATORI is able to find better configuration than PARTIES as the
phase changes ‚Äì distance from Oracle for SATORI is lower than
PARTIES, although the optimal configuration changes over
time and changes frequently. SATORI outperforms because of
both the capabilities: better search compared to gradient descent
and dynamic prioritization ‚Äì the magnitude from individual
contribution can range from 2% - 10% depending upon the
application, workload mix and temporal phase.
SATORI is not sensitive to the prioritization period and
the equalization period within a wide range, and does not
require investment in tuning efforts to yield benefits. Fig. 16
shows the effect of these factors on SATORI‚Äôs performance. It
shows the sensitivity of SATORI‚Äôs performance to prioritization
and equalization periods (the two most essential and tunable
parameters of SATORI). While the sensitivity is not high, we
observe that generally, the throughput and fairness performance
degrades as the period length increases. This is because a

0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
2 4 6 8 10
Prioritization Period (s)
70
75
80
85
90
95
100
Performance (%
of Balanced Oracle)
Throughput
Fairness
10 20 30 40 50
Equalization Period (s)
Fig. 16. SATORI has low sensitivity to the tuning of the
prioritization period and the equalization period. It does not
require large tuning efforts to achieve near-optimal results.
longer prioritization period provides less opportunity to make
precise dynamic decisions based on very short phases. That
is, if the period is too long, the decisions are made based on
performance during longer phases. And, a longer equalization
period provides less opportunity to equalize the two goals,
thus causing the performances to degrade. However, this only
happens for a very long prioritization period (> 5 seconds)
and equalization period (> 30 seconds). Overall, SATORI
does not require a large amount of tuning effort to set these
parameters. Parameter values chosen in a reasonable range
perform similarly.
SATORI‚Äôs design of the new objective function (moving goal
post) helps it extract higher performance (throughput and
fairness), but does not force the underlying BO model to
behave unexpectedly. Recall that SATORI constructs a new
objective function that changes the objective function itself
over time (dynamic prioritization of goals). Using an example,
we show that this feature, although non-conventional, keeps
the underlying BO engine operations in the neighborhood of
the expected BO operations, but yield higher performance. The
job mix used for illustration includes a group of five PARSEC
benchmarks (blackscholes, canneal, fluidanimate, freqmine,
streamcluster) and covers the initial portion of a long-running
experiment. Similar observations are drawn from other job
mixes, but not shown for brevity.
First, Fig. 17(a) shows the value of the objective function
over time. SATORI achieves higher value for the objective function over time compared to the SATORI without prioritization.
This is expected since the goal of SATORI is to indeed maximize
the objective function ‚Äì and that yields better performance.
However, a natural inquiry is: does this increase the variance in
decision that the underlying BO makes? More specifically, does
the estimation of SATORI about the proxy model change more
drastically than the estimate of SATORI without prioritization?
If the difference in the estimations is varying erratically over
time, then it can potentially lead to corresponding increased
variability in the observed performance.
Fig. 17(b) shows the percentage change in the values of the
proxy models (mean of absolute difference of estimate of each
configurations from one iteration to the next) in SATORI and
SATORI without dynamic prioritization. Fig. 17(b) confirms
that the range of percentage change in the proxy model value is
similar in SATORI and SATORI without dynamic prioritization.
The cross-overs between the two curves occur because the
exploration and exploitation phase of SATORI and SATORI
without dynamic prioritization are not in synchronization. The
final inquiry is: does SATORI result in more variation in the
observed performance due to its new objective function design
(changing weight factors)? Fig. 17(a) earlier showed that new
objective function design helps it maximize the objective
function better. Our results (Fig. 18) demonstrate that the
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
0 10 20 30 40 50
Time (s)
(a)
0.6
0.7
0.8
0.9
Objective Function
Value
0 10 20 30 40 50
Time (s)
(b)
0.0
2.5
5.0
% Change in Proxy
Model Value from One
Iteration to the Next
SATORI SATORI without Dynamic Prioritization
Fig. 17. New objective function construction in SATORI
improves the quality of the solution by obtaining (a) higher
objective function values, but without (b) unexpected changes
in the underlying BO operations.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
0 10 20 30 40 50
Time (s)
75
80
85
90
95
100
105
Performance (%
of Balanced Oracle)
Throughput
SATORI SATORI without Dynamic Prioritization
0 10 20 30 40 50
Time (s)
Fairness
Fig. 18. Variation in the observed performance for both goals
is similar for SATORI and SATORI without prioritization.
variation in the observed performance for SATORI is similar to
that of SATORI without dynamic prioritization. SATORI curve
is always above the SATORI without dynamic prioritization
curve and closer to the Oracle, but the changes over time are
similar in nature and within the expected region ‚Äì confirming
that bounding the weight factor is also helpful (Sec. III-B).
Finally, Fig. 19 shows a sample snapshot to demonstrate that
prioritizing the weaker goal empirically has better potential to
improve the benefits for both goals over a time period, instead
of prioritizing the stronger goal (as discussed in Sec. III).
SATORI‚Äôs relative improvement increases with higher colocation degree (scalability). As the co-location degree increases from three to seven applications, the %-point difference
between SATORI and PARTIES monotonically increases for
both throughput and fairness goals (% point difference is
8%, 11%, 13%, 13% and 15% for 3, 4, 5, 6, and 7 colocated applications, respectively). This is because as the colocation degree increases, finding the optimum becomes more
challenging due to the larger search space with more number
of local maxima, and gradient-descent methods tend to get
stuck in a local maximum more than SATORI.
SATORI is practical for real-systems and incurs low overhead. First, we note that SATORI‚Äôs overhead is not in the critical
path of job mix execution as jobs continue to execute while
SATORI is making its decisions. Jobs continue to execute using
their previous resource allocation configuration until SATORI
generates a new decision and implements it. SATORI‚Äôs most
time-consuming component is its BO engine, which includes
a GP model update and acquisition function evaluation. We
measured that in a 100ms sampling interval, all BO related
tasks take only 1.2ms on average.
Further, we note that SATORI is optimized not to be active
and incurring computation related overhead at all times. It is
invoked only when the performance of a specific job changes
significantly or the job mix changes. This ensures that 1.2 ms
overhead is incurred only when needed. Further optimizations
such as avoiding frequent updates to the GP model after the
optimal configuration detection saves GP model overheads that
can potentially increase the computational time. In terms of

0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
0 10 20 30 40 50
Time (s)
75
80
85
90
95
100
105
Performance (%
of Balanced Oracle)
Throughput
SATORI (weaker goal prioritized)
SATORI (stronger goal prioritized)
0 10 20 30 40 50
Time (s)
Fairness
Fig. 19. Prioritizing the weaker performing goal helps SATORI
to reach higher levels of throughput and fairness.
computation interference, our experiments reveal that SATORI
only executes about 1% of the instructions executed by the
job mix on an average. Note that all our performance results
related to SATORI presented in this paper automatically include
this overhead. BO can be sensitive to the initial configuration
set used at the search start; final outcome quality may vary by
1-3%. SATORI mitigates this challenge by building a reasonable
set of ‚Äúgood‚Äù configurations (e.g., equal resource partitions,
less imbalance in partition share across resources for a job)
instead of starting from random configurations.
VI. RELATED WORK
Besides multiple state-of-the-art techniques used in our evaluation, most prior works have focused on managing/partitioning
one resource to meet one goal such as improving utilization,
energy consumption, etc. [8]‚Äì[10], [17], [19], [30], [34], [36],
[40], [41], [55], [63], [65], [67], [71], [76], [78], [81], [84],
[87], [89], [90], [93]. Some techniques can be customized to
meet different goals, but not all goals simultaneously. Many
recent works have leveraged the new hardware capabilities of
LLC ways and memory bandwidth partitioning, power-capping,
etc., to use multiple resources to meet a single goal [12], [18],
[39], [44], [49], [50], [52], [61], [66], [68], [74], [94].
Our recent work designed for co-locating latency critical
jobs, CLITE [68], also takes a BO-based approach to partition
shared multi-core resources. CLITE outperforms PARTIES
when co-locating multiple latency critical jobs. But, when
applied in SATORI‚Äôs context of multiple throughput-oriented
co-located workloads with two competing objectives (fairness
and throughput), CLITE performs similar to PARTIES and
underperforms SATORI by a similar margin as PARTIES. This is
expected since both CLITE and PARTIES are not designed for
this problem context; they do not actively and simultaneously
control competing objectives, and do not employ the insight
of dynamic prioritization of goals.
REF technique, proposed by Zahedi et al. [92], is the
first proposal to demonstrate the use of Cobb-Douglas utility
for modeling performance, and uses game theory to fairly
allocate resources among multiple applications. REF primarily
focuses on improving fairness and does not actively maximize
throughput (unlike SATORI), but it provides multiple desirable
properties including sharing incentives, envy freeness, and
Pareto efficiency. In contrast to SATORI, REF requires prior
information or profiling about the performance of each application under all possible resource allocations using Cobb-Douglas
utility function ‚Äì while promising, it is often not practical in
typical cloud computing scenarios. A game-theoretic approach
is more suitable where the set of co-located applications is fixed
and tenants need assurance about envy freeness and sharing
incentive [51], [92].
Wang et al. [82] proposed, ReBudget, a market-based
resource allocation scheme for allocating multi-core resources.
ReBudget proposes a novel iterative bidding-pricing procedure
where users solve an optimization problem using hill-climbing
to maximize their local utility and bid for resources. Then,
the market determines their allocations based on multiple
rounds of bidding. Extensive simulation-based evaluation,
leveraging hardware-support for utility calculation, demonstrates the value of iterative bidding ideas. However, quickly
achieving market equilibrium with multiple applications and
more than two shared architectural resources is challenging. In
contrast, SATORI employs a BO-based approach that works on
commodity hardware and exploits the dynamic prioritization
of competing goals: fairness and throughput. Combining gametheoretic approaches such as REF and ReBudget with SATORI
can provide further opportunities for improving fairness and
throughput.
Ghodsi et al. [26] proposed dominant resource fairness
(DRF) to maintain fairness among applications by providing an
equal share of the most dominant resource to each application.
Chowdhury et al. [14] proposed HUG to improve DRF by
considering elastic and correlated demands among network
links. However, they still assume that the demand vector is
supplied by the user, and they do not consider correlated
utility ‚Äì which is critical in the case for multi-core resource
partitioning (e.g., memory bandwidth allocation can affect
the utility of allocated cache ways) and SATORI‚Äôs BO-based
approach captures these effects. Also, these approaches require
a user to define a demand vector of resource share, which
may not be readily available or be determined for co-located
applications on a CMP. SATORI assumes no input from the
user. SATORI does not make such static decisions and instead
adapts to the dynamically changing characteristics to utilize
the shared resources more efficiently.
VII. CONCLUSION
Previous works do not actively and simultaneously control
partitioning knobs while achieving multiple goals. SATORI
is the only technique to actively and simultaneously control
multiple CMP architectural resources to achieve multiple
goals. SATORI can effectively handle computing cores, LLC
ways, memory bandwidth, and power-cap resources to achieve
an optimal balance among throughput and fairness ‚Äì which
also complements recently published approaches for energyefficiency under co-location based on economics, collaborative
filtering and machine learning [43], [59], [60], [62], [64]