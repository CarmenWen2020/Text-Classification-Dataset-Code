Abstractâ€”Recent advances in Deep Neural Networks (DNNs)
have led to active development of specialized DNN accelerators,
many of which feature a large number of processing elements laid
out spatially, together with a multi-level memory hierarchy and
flexible interconnect. While DNN accelerators can take advantage
of data reuse and achieve high peak throughput, they also expose
a large number of runtime parameters to the programmers who
need to explicitly manage how computation is scheduled both
spatially and temporally. In fact, different scheduling choices can
lead to wide variations in performance and efficiency, motivating
the need for a fast and efficient search strategy to navigate the
vast scheduling space.
To address this challenge, we present CoSA, a constrainedoptimization-based approach for scheduling DNN accelerators.
As opposed to existing approaches that either rely on designersâ€™
heuristics or iterative methods to navigate the search space, CoSA
expresses scheduling decisions as a constrained-optimization
problem that can be deterministically solved using mathematical optimization techniques. Specifically, CoSA leverages the
regularities in DNN operators and hardware to formulate the
DNN scheduling space into a mixed-integer programming (MIP)
problem with algorithmic and architectural constraints, which
can be solved to automatically generate a highly efficient schedule
in one shot. We demonstrate that CoSA-generated schedules significantly outperform state-of-the-art approaches by a geometric
mean of up to 2.5Ã— across a wide range of DNN networks while
improving the time-to-solution by 90Ã—.
Index Termsâ€”scheduling, accelerator, neural networks, compiler optimizations
I. INTRODUCTION
Deep neural networks (DNNs) have gained major interest in
recent years due to their robust ability to learn based on large
amounts of data. DNN-based approaches have been applied
to computer vision [34], [43], [57], machine translation [64],
[68], audio synthesis [66], recommendation models [31], [46],
autonomous driving [11] and many other fields. Motivated by
the high computational requirements of DNNs, there have been
exciting developments in both research and commercial spaces
in building specialized DNN accelerators for both edge [1],
[16], [17], [26], [50], [61], [63], [72] and cloud applications [5],
[19], [27], [36], [39], [69].
State-of-the-art DNN accelerators typically incorporate large
arrays of processing elements to boost parallelism, together with
a deep multi-level memory hierarchy and a flexible networkon-chip (NoC) to improve data reuse. While these architectural
structures can improve the performance and energy efficiency of
DNN execution, they also expose a large number of scheduling
parameters to programmers who must decide when and where
each piece of computation and data movement is mapped
onto the accelerators both spatially and temporally. Here, we
use schedule to describe how a DNN layer is partitioned
spatially and temporally to execute on specialized accelerators.
Given a target DNN layer and a specific hardware architecture,
there could be millions, or even billions, of valid schedules
with a wide range of performance and energy efficiency [49].
Considering the vast range of DNN layer dimensions and
hardware architectures, there is a significant demand for a
generalized framework to quickly produce efficient scheduling
options for accelerators of varying hardware configurations.
Achieving high performance on a spatially distributed
architecture requires several factors to be carefully considered,
including tiling for good hardware utilization, pipelining
data movement with compute, and maximizing data re-use.
Previous scheduling frameworks have attempted to reflect these
considerations by formulating an analytical cost model, pruning
the scheduling space with known hardware constraints, and
then exhaustively searching for the best candidate based on
their cost models [14], [23], [49], [71]. However, navigating
the scheduling space in such a brute-force fashion can easily
become intractable for larger DNN layers and more complex
hardware architectures. Other notable efforts have employed
feedback-driven approaches, such as black-box tuning, beam
search, and other machine learning algorithms with iterative
sampling [3], [15], [38]. However, these schedulers typically
require massive training datasets and large-scale simulations to
learn performance models, making it infeasible to extend them
to other types of hardware accelerators, especially those still
under development. Hence, there is a clear need for efficient
scheduling mechanisms to quickly navigate the search space
and produce performant scheduling options.
In this work, we demonstrate CoSA, a constrainedoptimization-based approach to schedule DNN accelerators. In
contrast to prior work that either requires exhaustive bruteforce-based or expensive feedback-driven approaches, CoSA
expresses the DNN accelerator scheduling as a constrainedoptimization problem that can be deterministically solved using

"$.*&&&UI"OOVBM*OUFSOBUJPOBM4ZNQPTJVNPO$PNQVUFS"SDIJUFDUVSF	*4$"

Â¥*&&&
%0**4$"
2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) | 978-1-6654-3333-4/21/$31.00 Â©2021 IEEE | DOI: 10.1109/ISCA52012.2021.00050
Fig. 1: Execution latency histogram of 40K valid scheduling choices
for a ResNet-50 layer on a spatial accelerator.
todayâ€™s mathematical optimization libraries in one pass. In
particular, CoSA leverages the regularities in both DNN layers
and spatial hardware accelerators where the algorithmic and
hardware parameters can be clearly defined as scheduling
constraints. Specifically, CoSA formulates the DNN scheduling
problem as a prime-factor allocation problem that determines 1)
tiling sizes for different memory levels, 2) relative loop ordering
to exploit reuse, and 3) how computation should be executed
spatially and temporally. CoSA constructs the scheduling
constraints by exposing both the algorithmic behaviors, e.g.,
layer dimensions, and hardware parameters, e.g., memory
and network hierarchies. Together with clearly defined and
composable objective functions, CoSA can solve the DNN
scheduling problem in one shot without expensive iterative
search. Our evaluation demonstrates that CoSA-generated
schedules outperform state-of-the-art approaches by 2.5Ã—
across different DNN network layers, while requiring 90Ã—
less scheduling time as it does not require iterative search.
In summary, this work makes the following contributions:
â€¢ We formulate DNN accelerator scheduling as a
constrained-optimization problem that can be solved in a
single pass. To the best of our knowledge, CoSA is the
first constrained-optimization-based approach to tackle
major DNN scheduling decisions in one shot.
â€¢ We take a communication-oriented approach in the CoSA
formulation that highlights the importance of data transfer
across different on-chip memories and exposes the cost
through clearly defined objective functions.
â€¢ We demonstrate that CoSA can quickly generate highperformance schedules outperforming state-of-the-art approaches for different DNN layers across different hardware architectures.
II. BACKGROUND AND MOTIVATION
In this section, we discuss the complexity of DNN scheduling
space and the state-of-the-art schedulers to navigate the space.
A. DNN Scheduling Space
Scheduling is a crucial decision-making process for the
compilers to effectively assign workload to compute resources.
With the emergence of numerous DNN accelerators with
diverse architectures, there is a need for a fast, performant,
and explainable approach to scheduling. Our work focuses
Scheduler Search Algorithm
Brute-force Approaches:
Timeloop [49] Brute-force & Random
dMazeRunner [23] Brute-force
Triton [65] Brute-force over powers of two
Interstellar [71] Brute-force
Marvel [14] Decoupled Brute-force
Feedback-based Approaches:
AutoTVM [15] ML-based Iteration
Halide [56] Beamsearch [3], OpenTuner [6], [45]
FlexFlow [38] MCMC
Gamma [40] Genetic Algorithm
Mind Mapping [35] Gradient-based Search
Constrained Optimization Approaches:
Polly+Pluto [12], [13], [30]
Tensor Comprehension [67] Polyhedral Transformations
Tiramisu [8]
CoSA Mixed-Integer Programming (MIP)
TABLE I: State-of-the-art DNN accelerator schedulers.
on operator-level scheduling, which aims to optimize the
performance of each operator, i.e. DNN layer, on specific
hardware. Operator-level scheduling typically comprises three
key loop optimizations: loop tiling, loop permutation, and
spatial mapping. Loop tiling describes which loops are mapped
to which memory hierarchy and the corresponding tile sizes.
Loop permutation determines the relative order of the loops,
while spatial mapping binds one or more loop dimensions
to spatial hardware resources, such as parallel processing
elements, instead of mapping them to temporal (i.e. sequential)
execution. Each optimization can have a significant impact
on the performance, and all three optimizations need to be
considered together to achieve the best performance.
Consider scheduling a 3Ã—3 convolution layer in
ResNet50 [34] with 256 input and output channels,
and an output dimension of 14Ã—14, on an accelerator with
five levels of memory. If we split each individual loop bound
into its prime factors and assign each one to a memory level,
we would have billions of schedules to consider. Among the
randomly sampled schedules from all possible loop tilings,
half of them fail to satisfy the buffer capacity constraints
(e.g. a schedule is invalid if it requires a 4KB buffer, though
the available buffer size is only 2KB.). Fig. 1 shows the
performance distribution of the valid schedules. We observe a
wide performance difference among the valid schedules, with
the best one outperforming the worst one by 7.2Ã—. In addition,
we observe clusters of schedules that have similar latencies in
the Fig. 1, revealing structure in the solution space.
B. State-of-the-art Schedulers
Given that the scheduling space for a DNN layer can have
billions of valid schedules, finding a good schedule through
exhaustive search can become an intractable problem. Table I
shows some recent efforts to tackle this complexity.
1) Brute-force Approaches: Recent efforts combine exhaustive search with heuristics to manually prune the scheduling

 
 

	 







	

	
		
	

		
	
 
 
		
	

	

	

	







   
   
   


 
  	



  
 
  	
 
 	
   
	 
   
	
	

	


	

Fig. 2: DNN scheduling problem formulation with CoSA. CoSA takes 1) DNN layer dimensions
and 2) DNN accelerator parameters and expresses the scheduling problem into a constrained
optimization problem to produce a performant schedule in one shot.
1 //DRAM level
2 for q2 = [0 : 2) :
3 // Global Buffer level
4 for p2 = [0 : 7) :
5 for q1 = [0 : 7) :
6 for n0 = [0 : 3) :
7 spatial_for r0 = [0 : 3) :
8 spatial_for k1 = [0 : 2) :
9 // Input Buffer level
10 spatial_for k0 = [0 : 2) :
11 // Weight Buffer level
12 for c1 = [0 : 2) :
13 for p1 = [0 : 2) :
14 // Accumulation Buffer level
15 for s0 = [0 : 3) :
16 for p0 = [0 : 2) :
17 spatial_for c0 = [0 : 8) :
18 // Register
19 for q0 = [0 : 2) :
Listing 1: An example schedule using
the loop nest representation for a DNN
layer of dimension R = S = 3, P =
Q = 28, C = 8, K = 4, N = 3. Same
variable prefix indicates tiles from the
same problem dimension.
space [14], [23], [49], [65], [71]. To lower the cost of exhaustive
search, schedulers in this category typically use a lightweight
analytical model to estimate latency, throughput, and power
consumption to compare all valid mappings of a given layer to
find the best schedule. The disadvantages of this approach
are two-fold. First, such a brute-force search tends to be
exceedingly expensive for complex hardware architectures,
making it infeasible to find a good schedule quickly. Second,
the generated schedules often do not perform optimally since
analytical models may fail to consider the communication
latency across the spatial hardware.
2) Feedback-based Approaches: Other recent efforts use
feedback-driven approaches along with machine learning or
other statistical methods [3], [15], [35], [38], [40], [56] to
improve the accuracy of the cost model and search for the
solution using black-box or gradient-based search. Although
such approaches can potentially learn the distribution of the
scheduling space, they typically require a large amount of
training data due to their feedback-driven nature. As a result,
these approaches are mainly applicable to post-silicon hardware
where performing a large-scale measurement is possible but
are not feasible for hardware under development.
3) Constrained-optimization Approaches: Constrainedoptimization problems, in which objective functions are
maximized or minimized subject to given sets of constraints,
have demonstrated the ability to solve many complex largescale problems in a reasonable time. Such methods have been
widely used in architecture and systems research for instruction
scheduling [20], [47], [48], high-level synthesis [22], memory
partitioning [7], [37] [21], algorithm selection [33], [73], and
program synthesis [4], [10], [52], [53], [62].
In particular, polyhedral transformation has leveraged
constrained-optimization-based approach for auto-vectorization
and loop tiling [2], [9], [13], [30], [42], [51]. Prior work
targets general-purpose CPUs and GPUs that run with finegrained instructions and hardware-managed cache, as opposed
to the software-managed spatial accelerators that we target. In
addition, existing polyhedral-based approaches [8], [9], [13]
lack direct support for tile-size optimization. Instead, they take
the tile size as input and apply a transformation based on the
given tile size. Due to this limitation, the tile size decision
cannot be co-optimized with other loop transformations, e.g.
loop permutation, in one pass, leading to sub-optimal schedules.
To address the drawbacks of existing approaches and
leverage the regularities from the DNN workloads and the
accelerator design for optimization, CoSA employs constrained
optimization to tackle the DNN scheduling problem in one
pass. CoSA presents a unique domain-specific representation
for DNN scheduling that better captures the utilization and
communication cost and encodes different loop transformations,
i.e., tiling size, loop permutation, and spatial mapping decisions,
in one formulation. This unified representation enables us to
solve for all three optimizations in one pass and produce
efficient schedules for a complex accelerator system with a
multi-level memory hierarchy.
III. THE COSA FRAMEWORK
To navigate the large scheduling space of DNN accelerators, we develop CoSA, a constrained-optimization-based
DNN scheduler to automatically generate high-performance
schedules for spatially distributed accelerators. CoSA not only
deterministically solves for a good schedule in one pass without
the need for exhaustive search or iterative sampling, but can
also be easily applied to different network layers and hardware
architectures. This section discusses the CoSA framework and
how CoSA formulates the DNN scheduling problem with
mixed-integer programming (MIP).
A. CoSA Overview
CoSA optimizes operator-level schedules for mapping DNN
layers onto spatial DNN accelerators. Specifically, CoSA formulates the scheduling problem as a constrained-optimization
                                     
Fig. 3: Performance comparison of schedules with different loop
permutations for a convolution operator with the layer dimensions of
R = S = 3, P = Q = 8, C = 32, K = 1024. The leftmost schedule
(CKP) refers to a relative ordering where the input channel dimension
(C) is the outermost loop and the output height dimension (P) is the
innermost loop. Since this layer is weight-heavy, loop permutations
that emphasize weight reuse, e.g., PCK and PKC, are more efficient.
problem with variables representing the schedule, constraints
representing DNN dimensions and hardware parameters, and
objective functions representing goals, such as maximizing
buffer utilization or achieving better parallelism. Fig. 2 shows
the target problem space of CoSA. CoSA takes the specifications of the DNN layers and the underlying spatial accelerator
as input constraints and generates a valid and high-performance
schedule based on the objective functions in one pass.
1) Target Workload: The work targets the DNN operators
that can be expressed by a nested loop with 7 variables as loop
bounds: R, S, P, Q, C, K, N. R and S refer to the convolution
kernel width and height, P and Q refer to the output width
and height, C refers to the input channel size, K refers to
the output channel size, and N refers to the batch size, as
illustrated in Fig. 2. The convolution operation computes the
dot product of the filter size R Ã— S Ã— C of inputs and weights
to generate one point in the output. Matrix multiplications can
be expressed in this scheme as well.
2) Target Architecture: CoSA targets spatial architectures
with an array of processing elements (PEs) connected via an
on-chip network and with multiple levels of memory hierarchy,
a commonly adopted architecture template in todayâ€™s DNN
accelerator designs [18], [19], [28], [29], [36], [44], [54], [55],
[60], [69], [71].
3) Target Scheduling Decisions: CoSA-generated schedules
describe how a specified DNN layer is executed on a given
spatial architecture. Listing 1 shows an example of a schedule.
Here, we use a loop-nest representation [49] to explicitly
describe how the computation of a convolution layer is mapped
to levels of memory hierarchies. We highlight three aspects
of the schedule: 1) loop tiling, which describes which loops
are mapped to which memory level and the values of the
loop bounds; 2) loop permutation, which handles the relative
ordering between loops in the same memory hierarchy; and 3)
spatial mapping, which defines which loops are mapped to
parallel spatial resources (shown as spatial_for loops in
Listing 1). All three factors play a key role in the efficiency
of the scheduling choice. Next, we highlight the implications
of loop permutation and spatial mapping, both of which are
less explored than the well-studied loop tiling.
Fig. 3 illustrates the impact of loop permutation for a
Fig. 4: Performance comparison of schedules with different spatial
mappings for a convolution operator with the layer dimensions of
R = S = 1, P = Q = 16, C = 256, K = 1024. Factors in s list
are for spatial mapping, and factors in t list are for temporal mapping.
For example, s:P4C4,t:K4 represents a mapping where a factor 4
of the P dimension and a factor 4 of the C dimension are mapped
to spatial execution in a system with 16 PEs, leaving Kâ€™s factor 4 to
temporal mapping.
convolution layer on a given hardware design. All the schedules
use the same loop tiling and spatial mapping except the loop
ordering at the global-buffer level, as indicated in the labels of
the X-axis, where CKP means the input channel dimension (C)
is the outermost loop, and the output height dimension (P) is
the innermost loop. In this case, selecting P as the outermost
loop, i.e. PCK and PKC, can lead to a 1.7Ã— speedup for this
layer, motivating the need to consider the implications of loop
permutation in the scheduling problem.
Fig. 4 shows the impact of spatial mapping on DNN
execution. We notice that there is a 4.3Ã— gap between best
(rightmost) and worst (leftmost) schedules for the layer in
consideration. The fundamental reason for the differences is the
different communication traffic generated by different spatial
mapping options. The best schedule, i.e., the rightmost schedule
in the figure (s:P2C4K2, t:P2K2), is obtained when factors
P = 2, C = 4, K = 2 are mapped to the spatial loops, which
cannot be achieved by simply choosing either model or data
parallelism in the spatial partition. As a result, a systematic
evaluation of different spatial mapping choices is required to
find a good schedule.
The rest of the section discusses how CoSA formulates the
scheduling variables, constraints, and objectives to solve the
DNN scheduling problem.
B. CoSA Variables and Constants
This section discusses the variables and constants, summarized in Table II, used in CoSA formulation.
CoSA Variables CoSA Constants Indices
X
binary matrix
to represent
a schedule
A layer dimension to
data tensor mapping
i memory level
j layer dimension
B memory level to
data tensor mapping
n prime factor index
k mapping choice
z permutation level
v data tensor
TABLE II: CoSA Notations.

DNN Layer: R = 3, S = 1, P = 1, Q = 1, C = 1, K = 4, N = 3
âˆ’â†’ Prime Factors: = [[3],[1],[1],[1],[1],[2,2][3]]
Idx Perm Schedule
j Layer Dim. R=3 ... K=4 N=3
n Prime Factors 3 ... 2 2 3
k s / t Mapping s t s t s t s t
i
Memory Levels
Register ...
... ...
InputBuf ... âœ“
GlobalBuf
O0
O1 âœ“
O2 âœ“
...
OZ âœ“
TABLE III: Example binary matrix X representing a schedule. A
checkmark in s, t indicates spatial or temporal mapping. A checkmark
in O0, ..., OZ indicates the rank for loop permutation. In this schedule,
the loop tile of size 3 from problem dimension N is allocated within
the GlobalBuf at the innermost loop level, assigned for temporal
execution. Both loop tiles from K are mapped to spatial resources.
1) Variable Representation: We devise a mathematical
representation for the DNN schedules and formulate the
scheduling problem as a prime-factor allocation problem. Given
a layer specification, we first factorize each loop bound into its
prime_f actors. If the loop bound themselves are large prime
number, we can pad them and then factorize. We assign each
prime factor to a scheduling configuration that is composed
of a combination of three decisions: 1) the mapped memory
level, 2) the permutation order, and 3) the spatial mapping.
Each prime factor has exactly one scheduling configuration.
Here, we use a binary matrix X to represent the prime factor
allocation, i.e., the scheduling space, shown in Table III. The
four dimensions of X are: 1) the layer dimension variables
(indexed by j), 2) the prime factors of the loop bounds (indexed
by n), 3) whether it is a spatial or temporal mapping (indexed
by k), and 4) the memory and the permutation levels (indexed
by i). With the prime factor decomposition, CoSAâ€™s encoding
can represent all possible schedules and guarantees that the
optimization solves for the full search space.
Table III shows an example binary matrix X that represents
the schedule shown in Listing 1. First, CoSA performs the
tiling optimizations by assigning the prime factors to different
memory levels. For example, dimension K is split into two tiles,
where the inner tile of size 2 is allocated to the input buffer, and
the outer tile of size 2 is allocated in the global buffer. Second,
mapping a prime factor to spatial execution is indicated by
whether the factor is mapped to a spatial column s or a temporal
column t in the table. In this example, both prime factors for
K are spatially mapped. Finally, for loop permutation, we add
rank indices O0, O1, ..., OZ to the memory level of interest,
where only one prime factor can be mapped to each rank. The
lowest-ranked factor is allocated to the innermost loop, while
the highest-ranked factor is allocated to the outermost loop.
In the example shown in Table III, the problem dimension N
is mapped at the O1 level in the global buffer for temporal
mapping, which means the factor N = 3 will be assigned
rank 1 in the global-buffer level. Without other factors in the
Related Idx
W IA OA v
R âœ“ -
j
S âœ“ -
P âœ“ âœ“
Q âœ“ âœ“
C âœ“ âœ“
K âœ“ âœ“
N âœ“ âœ“
Related Idx
W IA OA v
Register âœ“ âœ“ âœ“
i
AccBuf âœ“
WBuf âœ“
InputBuf âœ“
GlobalBuf âœ“ âœ“
DRAM âœ“ âœ“ âœ“
TABLE IV: Constant binary matrices A (left) and B (right). A
encodes how different layer dimensions associate with data tensors. B
encodes which data tensor can be stored in which memory hierarchy.
global-buffer level, factor N = 3 with the smallest rank will
become the innermost loop in permutation. For the ranking of
permutation, we reserve enough slots for all prime factors at
all memory levels. Not all the slots need to be filled since a
prime factor can only be allocated to one memory level.
2) Constant Parameters: In addition to the loop-related
variables, we have intrinsic relations across different components in the architecture and layer specifications which must be
encoded by constant parameters. CoSA uses two constant binary
matrices to encode the unique relations in the DNN scheduling
space, shown in Tabel IV. The first binary constant matrix, A,
encodes the association between layer dimensions (i.e., rows of
the matrix) and data tensors (i.e., columns of the matrix). For
each input (IA), weight (W), and output (OA) tensor, matrix
A indicates which layer dimensions, i.e., R, S, P, Q, C, K, N,
should be used to calculate the data transaction size as well as
multicast and reduction traffic on the accelerators.
In addition, we introduce another binary matrix B to
represent which memory hierarchy can be used to store which
data tensor. DNN accelerators typically deploy a multi-level
memory hierarchy, where each memory level can be used
to store different types of data tensors. For example, matrix
B shown in Table IV represents an architecture that has
dedicated input and weight buffers for input activation and
weight, respectively, while providing a shared global buffer to
store input and output activations.
C. CoSA Constraints
This section discusses the constraints derived from the target
accelerator architecture that must be satisfied in CoSA and
shows how to express them with CoSA variables and constants.
1) Buffer Capacity Constraint: To generate a valid schedule
in a software-managed memory system, a key constraint is to
ensure that the size of data to be sent to the buffer does not
exceed the buffer capacity. The hardware memory hierarchy
can be represented by the binary constant matrix B discussed
earlier. For each memory buffer, based on the tensor-dimension
correlation matrix A, we calculate the tiling size of each tensor
by multiplying the relevant prime factors together indicated by
X. Both spatial and temporal factors should be included in the
buffer utilization. Let Nj be the number of prime factors for
the layer dimension j. Then the utilization of the buffer level

I can be expressed as:
I
âˆ’1
i=0
6

, Nj
j=0,n=0

1
k=0
prime_f actorj,n, X(j,n),i,kAj,vBI,v = 1
1, otherwise
(1)
We then set the upper bound of the buffer utilization to
the capacity of different buffer sizes, represented using MI,v.
However, a problem with this utilization constraint is that
it involves products of the decision variables X, making it
nonlinear and infeasible to solve with standard constraint
solvers. To address this limitation, we take the logarithm of
both sides of the constraints to obtain a linear expression for
the utilization and encode the if-else statement as:
UI,v = 
Iâˆ’1
i=0
6

, Nj
j=0,n=0

1
k=0
log(prime_f actorj,n)Aj,vBI,vX(j,n),i,k
â‰¤ log(MI,v), âˆ€I
(2)
To encode different precisions for different data tensors, we
add the logarithm of the datatype sizes precisionv to UI,v.
2) Spatial Resource Constraint: Another set of CoSA
constraints is from the limited number of spatial resources. At
the chip level, there is a limited number of PEs. At the PE level,
there is a limited number of multiply-and-accumulate (MAC)
units. In CoSA, once a factor is assigned to spatial mapping
in the configuration, it needs to satisfy: 1) each problem factor
can only be mapped to either spatial or temporal execution,
2) factors that map to spatial execution do not exceed the
resource limit in the architecture. These two constraints can
be expressed in the equations below:

1
k=0
X(j,n),i,k == 1, âˆ€(j, n), i (3)
6

, Nj
j=0,n=0
log(prime_f actorj,n)X(j,n),I,0 â‰¤ log(SI ), âˆ€I (4)
where SI is the number of available spatial resources at the
level I.
D. Objective Functions
In this section, we describe the objective functions for CoSA.
Each objective can be either used individually to optimize a
single aspect of performance, e.g., utilization, compute, and
communication, or combined with others.
1) Utilization-Driven Objective: High on-chip buffer utilization improves data-reuse opportunity. As demonstrated in the
prior work [25], communication lower bounds can be achieved
when the tiling block size is optimized for buffer utilization in
a system with one-level cache. In this work, we formulate a
utilization objective that aims to maximize the buffer utilization
of all tensors, so the overall communication is minimized. We
use the same formulation for the buffer utilization as in III-C1
and maximize the following linear utilization function:
U til Ë† = 
Iâˆ’1
i=0

2
v=0
Ui,v (5)
Here, maximizing the sum of utilization for all buffer
levels and all tensors in the logarithm form is equivalent to
maximizing the geometric mean of the buffer utilization. Users
can also attach weights to the different buffer levels or different
data tensors if they want to optimize for the utilization of a
specific level of the memory.
2) Compute-Driven Objective: The total number of compute
cycles is another factor that affects the quality of schedules. In
this formulation, we multiply all the temporal factors for the
estimated compute cycles in each PE. Intuitively, this objective
allows the constraint solver to exploit the parallelism in the
system by mapping more iterations to the spatial resources
than to temporal iterations. The objective can be expressed as
a linear function again with logarithm taken:
Comp Ë† = 
I
i=0
6

, Nj
j=0,n=0
log(prime_f actorj,n)X(j,n),i,1 (6)
3) Traffic-Driven Objective: Communication latency is a key
contributing factor to the performance of spatial architecture.
CoSA also includes a traffic-driven objective to capture the
communication cost. Specifically, communication traffic can
be decomposed into three terms: 1) data size per transfer, 2)
spatial factors of multicast and unicast traffic, and 3) temporal
iterations. Multiplying these three factors will get the total
amount of traffic in the network. Next, we discuss how we
capture each of these factors using CoSAâ€™s representation.
First, similar to the buffer utilization expression, data size
per transfer can computed using the allocated prime factors
in matrix X, together with the dimension-tensor correlation
matrix A, as shown in the equation below:
Dv = 
Iâˆ’1
i=0
6

, Nj
j=0,n=0

1
k=0
log(prime_f actorj,n)Aj,vX(j,n),i,k
(7)
Second, spatial factors would incur different multicast, unicast, and reduction patterns. The dimension-tensor correlation
matrix A discussed in Sec III-B2 can be used to indicate
different traffic patters. Specifically, depending on whether
the spatial dimension, indicated by the binary matrix X, is
related to the specific tensor in consideration, represented by
the constant matrix A, different traffic patterns, e.g., multicast
vs. unicast or reduction vs. unicast, would occur.
Fig. 5 shows how the intrinsic tensor-dimension correlation
matrix A can be used to calculate different traffic patterns for
different variables. For example, as shown in Fig. 5a, if the
dimension P is mapped spatially, AP,W = 0 implies multicast
traffic for weight tensor W. Since weight is not related to
P, when we send weights from global buffer to PEs, the
weight traffic will be multicasted to the destination PEs. If the
dimension C is mapped spatially, AC,W = 1 (Fig. 5b) implies
unicast traffic for weight tensor W as weight is related to C.
Similarly, if the dimension C is mapped spatially, AC,OA = 0
(Fig. 5c) implies reduction traffic for output tensor OA, where
partially sum needs to be reduced across C before sending
back to GB. If the dimension P is mapped spatially, AP,OA = 1
    

   
   
   
 

   
   
   
   
   

  
   
   

   
   
   
   

   
   
  	
  
	
  
	 

 	


	
	

	
	

	
	

	
	 
Fig. 5: Different traffic patterns based on the constant matrix A.
The two figures (top) show how the constant A encodes the traffic
types (multicast, unicast, reduction) for different data tensors from the
global buffer to PEs. The figures on the bottom show its implication
on output tensor reduction traffics.
(Fig. 5d) would indicate unicast traffic for output tensor OA,
as each traffic contributes to different regions of the output.
CoSA formulates this relationship in the following equation:
Lv =
6

, Nj
j=0,n=0
log(prime_f actorj,n)X(j,n),I,0Aj,v (8)
The third term, temporal iteration is used to calculate the
number of data transfers at the NoC level. We introduce a traffic
iteration factor Y that is a function of X at the permutation
level, A, and B. Y indicates if the outer NoC loop bound
should be used for different variables. With Y, we ensure that,
for each variable, if a relevant factor term is seen inside the
current loop level, the current loop levelâ€™s factor should be
used to compute the traffic iteration regardless of whether it is
related to the data tensor of the variable of interest. This is a
term that drives the reuse optimization. Mathematically, Y is
constrained as:
Yv,z â‰¥
6

, Nj
j=0,n=0
X(j,n),z,1Aj,vBI,v, âˆ€z, âˆ€v
Yv,z â‰¥ Yv,zâˆ’1, âˆ€z > 0, âˆ€v
(9)
Where z represents the position index for permutation and
Z equals the total valid levels for permutation. The traffic
iteration term can thus be expressed as:
Tv =
Z
âˆ’1
z=0
6

,Nj
j=0,n=0
log(prime_f actorj,n)Yv,zX(j,n),z,1 (10)
This turns the linear objective into quadratic as we multiply
Y with X to indicate whether there is a factor at the current
permutation level.
After we calculate each individual term, we can combine
them together for each tensor that contributes to the total traffic
in the network. Similar to the logarithmic transformation we
did earlier, instead of multiplying these three terms together,
we take the logarithm on both sides to get a linear expression
of the traffic, as shown in the equation below:
T raf Ë† = 
2
v=0
(Dv + Lv + Tv) (11)
4) Overall Objective: One can construct a composite objective comprised of a linear combination of U til Ë† , Comp Ë† ,
and T raf Ë† , where we want to minimize the compute and
communication latency while maximizing the on-chip buffer
utilization:
OË† = âˆ’wU U til Ë† + wC Comp Ë† + wT T raf Ë† (12)
where wU , wT , wC are user-selected parameters controlling
the importance of each objective. For a system with doublebuffering optimization, wT can be set to map the traffic sizes
to the cycles for memory accesses. This brings wT T raf Ë† to
be of the same importance as wC Comp Ë† in the optimization.
Another formulation of the overall objective function to balance
the memory access and compute cycles is to minimize the
difference of the two terms: DË† = wT T raf Ë† âˆ’ wC Comp Ë† . The
weights of different objectives can be determined by using a set
of micro-benchmarks that characterize the compute, memory,
and communication latencies of the target architecture.
E. Limitation of CoSA
CoSA leverages the regularity from both the problem and the
architecture space, where it assumes a dense CNN workload
and does not exploit the sparsity of the data. It also best targets
hardware systems with deterministic behavior and explicitly
managed scratchpads. This is because, in systems with nondeterministic behaviors, it can be challenging to construct optimization objectives that capture the impact of such behaviors.
However, CoSA can be augmented with an iterative search on
the objective functions and their corresponding hyperparameters
to approximate the unknown hardware performance model and
directly prune off the invalid points from the search space.
IV. METHODOLOGY
This section discusses the evaluation platforms we use
followed by the experimental setup for CoSA evaluation.
A. Evaluation Platforms
We evaluate the schedules generated by CoSA on two
platforms: 1) Timeloop for cycle performance and energy
consumption, and 2) our cycle-exact NoC simulator for overall
latency performance. The latter more accurately captures the
communication overhead and concurrent hardware behaviors
on a spatial architecture.
Timeloop provides microarchitecture and technologyspecific energy models for estimating the performance and
energy on DNN accelerators. Timeloop reports the performance in terms of the maximum cycles required for each
processing element to complete the workload and to perform
                           
Arithmetic : Storage : Network :
MACs 64 / PE Registers 64B / PE Dimension 4Ã—4
Weight/Input 8bit Accum. Buffer 3KB / PE Router Wormhole
Precision Weight Buffer 32KB / PE Flit Size 64b
Partial-Sum 24bit Input Buffer 8KB / PE Routing X-Y
Precision Global Buffer 128KB Multicast Yes
TABLE V: The baseline DNN accelerator architecture.
memory accesses, assuming perfect latency hiding with double
buffering. The energy consumption in Timeloop is calculated
by multiplying the access count on each hardware component
with the energy per access and summing the products up. The
access count is inferred from the schedule and the energy per
access is provided by an energy reference table in Timeloop.
NoC Simulator augments the Timeloop analytical compute
model for PEs with a synthesizable NoC implementation to
reflect the communication cost. Communication is one of the
key contributing factors for latency in a NoC-based system,
especially for the communication bound schedules.
The NoC simulator is transaction-based and cycle-exact for
modeling the on-chip traffic. Leveraging the synthesizable
SystemC router design from Matchlib [41] that supports
unicast and multicast requests, we construct a resizable 2-
D mesh network and implement an X-Y routing scheme.
The simulator captures both computation and communication
latencies by concurrently modeling data transfers in the NoC,
the PE executions, and off-chip DRAM accesses based on the
DRAMSim2 model [58], where the impact of traffic congestion
on the NoC can also be manifested.
B. Baseline Schedulers
We evaluate CoSA with respect to two other scheduling
schemes: 1) a Random scheduler that searches for five
different valid schedules, from which we choose the one with
the best result for the target metric, and 2) the Timeloop
Hybrid mapper in Timeloop [49] that randomly selects a
tiling factorization, prunes superfluous permutations, and then
linearly explores the pruned subspace of mappings before it
proceeds to the next random factorization. For this mapper,
we keep the default termination condition where each thread
self-terminates after visiting 500 consecutive mappings that
are valid yet sub-optimal. The mapper is run with 32 threads,
each of which independently searches the scheduling space
until its termination condition is met. Once all threads have
terminated, Timeloop returns the best schedule obtained from
all 16,000+ valid schedules.
C. Experiment Setup
Mixed-Integer Program (MIP) Solver: CoSA uses
Gurobi [32], a general-purpose optimization solver for MIP
and other constrained programming, as the solver. We specify
the CoSA variables, constraints, and objective functions before
we invoke the solver. The solver takes at most seconds to return
a schedule for DNN layers.
DNN workloads: We measure the performance of CoSAgenerated schedules over a wide range of DNN workloads
targeting different DNN tasks with diverse layer dimensions,
including: ResNet-50 [34], ResNeXt-50 (32x4d) [70], and
Deepbench [24] (OCR and Face Recognition). The precision
used for the benchmarks is 8-bit for the input and weights,
and 24-bit for the partial sums. We do not pad the dimensions
to be multiples of 2, as it incurs more overhead and outweighs
the benefits it provides to allow more scheduling options.
Baseline architecture: We consider a spatial-array architecture like Simba [59] as our baseline. Detailed specifications
of the hardware constructs are summarized in Table V. We
demonstrate that the CoSA framework is general to be applied
for different architecture parameters while delivering highperformance scheduling options in one shot.
V. EVALUATION
In this section, we demonstrate the improved time-to-solution,
performance, and energy of CoSA compared to baseline
schedulers, across different evaluation platforms and different
DNN architectures on a diverse set of DNN layers.
A. Time to Solution
We compare the average time for CoSA and the baseline
schedulers to generate the schedule of each layer from the
four target DNN workloads. Table VI shows that CoSAâ€™s
optimization-driven approach offers more than 90Ã— (4.2s vs.
379.9s) time-to-solution advantage over the Timeloop Hybrid
search strategy. Timeloop Hybrid search sampled 67 million
schedules per layer and evaluated more than 16 thousand valid
ones among them, leading to a long runtime. With Random
search, a random sampling of 20K samples in 4.6 seconds
resulted in only five valid schedules, further demonstrating the
need to have a constraint-based strategy to prune the invalid
search space directly. In the following section, we show that
CoSA not only shortens the time-to-solution but also generates
high-quality schedules.
CoSA Random (5Ã—) Timeloop Hybrid
Avg. Runtime / Layer 4.2s 4.6s 379.9s
Avg. Samples / Layer 1 20K 67M
Avg. Evaluations / Layer 1 5 16K+
TABLE VI: Time-to-solution Comparison. CoSA outputs only one
valid schedule per layer. CoSAâ€™s runtime is 1.1Ã— and 90Ã— shorter
than the Random and Timeloop Hybrid search, respectively.

Fig. 6: Speedup of different schedules relative to Random search on the baseline 4Ã—4 NoC architecture. X-axis labels follow the naming
convention R_P_C_K_Stride where S = R and Q = P in all workloads. CoSA achieves 5.2Ã— and 1.5Ã— higher geomean speedup across
four DNN workloads compared to the Random and Timeloop Hybrid search.
Fig. 7: Improvements in total network energy reported by the
Timeloop energy model. Energy estimations are normalized to results
from Random search and are evaluated on the baseline 4Ã—4 NoC.
B. Evaluation on Timeloop Performance and Energy Models
We compare the performance of the Random search, the
Timeloop Hybrid mapper, and the CoSA scheduler for four
different DNN workloads. The evaluations are based on our
baseline architecture described in Table V and the Timeloop
evaluation platform mentioned in Section IV-A.
1) Performance: Fig. 6 shows the speedup reported by
Timeloop for different scheduling schemes relative to Random
search. Fig. 6 demonstrates that the CoSA-generated schedules
are not only valid but also outperform the ones generated
by both Random search and Timeloop Hybrid search. The
geometric mean of the speedups of CoSA schedules relative to
the Random and Timeloop Hybrid search ones are 5.2Ã— and
1.5Ã— respectively across four DNNs.
In the few layers where Timeloop Hybrid search slightly
outperforms CoSA, we find a higher iteration count at the
Fig. 8: Objective function breakdown for ResNet-50 layer
3_7_512_512_1. The goal is to minimize the total objective in Eq. 12.
CoSA achieves the lowest values for all objective functions on this
layer among all approaches.
DRAM level in Timeloop Hybrid schedules, which helps to
reduce the size of each DRAM transaction and balance the
pipeline. Fine tuning the weights of the objective functions
could be used to further improve the CoSA-generated schedules.
A more exhaustive Timeloop Hybrid search (32K valid
schedules) results in an improvement of only 7.5% in latency
while increasing runtime by 2Ã—. We find that even with 2Ã—
more valid samples evaluated, Timeloop Hybrid search still
cannot generate schedules that are of similar efficiency to
CoSA.
2) Energy: We use the Timeloop energy model to evaluate
the energy of different schedules. Because energy cost is highly
correlated with the access count on each hardware component,
our traffic objective in CoSA is used for the schedule optimization targeting energy efficiency. Fig. 7 demonstrates that CoSA,
using no simulation feedback, can generate schedules 22%
more energy-efficient than the best Timeloop Hybrid solutions

(a) 8 Ã— 8 PEs (b) Larger Buffers
Fig. 9: Speedup relative to Random search reported by Timeloop model on different hardware architectures. CoSAâ€™s performance generalizes
across different hardware architectures with different computing and on-chip storage resources.
Fig. 10: Speedup reported by NoC simulator relative to Random search on the baseline 4Ã—4 NoC architecture. CoSA achieves 3.3Ã— and
2.5Ã— higher geomean speedup across four DNN workloads compared to the Random and Timeloop Hybrid search on the more communication
sensitive NoC simulator.
selected from 16,000+ valid schedules optimizing the energy.
3) Objective Breakdown: A detailed breakdown of the
CoSA objective function on ResNet50 layer 3_7_512_512_1
is included in Fig.8. Our overall objective function aims to
capture an optimization heuristic to maximize the utilization and
minimize the compute and traffic costs at the same time with
a weighted sum of the three. Fig.8 shows that CoSA achieves
the lowest total objective among all approaches, and optimizes
all three sub-objectives simultaneously. This observation on
the objective values aligns with our empirical results in Fig. 6,
where CoSA schedule runs 7Ã— faster than the ones generated
by Random and Timeloop Hybrid search.
4) Different HW Architectures: We further explore the performance of CoSA with different DNN architecture parameters
such as different PE array sizes and different SRAM buffer
sizes. We apply the same weights for the evaluation on the same
architecture and customize the objective weights in Eqn.12
using a micro-benchmark for different architectures. Fig.9
shows the geomean speedup of CoSA across all networks
on two different hardware architectures.
PE Array Dimension. We scale the number of PEs up by
4Ã— and increase both the on-chip communication and DRAM
bandwidth by 2Ã— correspondingly. Both of these modifications
significantly impact the compute and communication patterns of
DNN layer executions. With a larger spatial array of arithmetic
units, this case study presents a scheduling problem where
decisions about spatial and temporal mapping can be especially
crucial to attaining high performance. Fig. 9a shows that CoSA
achieves 4.4Ã— and 1.1Ã— speedup compared to Random and
Timeloop Hybrid search respectively across four networks. This
shows that the performance of our scheduler can scale and
generalize to NoCs with more PEs, which tend to be more

Fig. 11: Speedup relative to TVM reported on K80 GPU.
affected by communication costs.
SRAM Size. We also increase the sizes of the local
and global buffers to demonstrate that CoSA can achieve
consistently good schedules across different architectures. The
sizes of local buffers, i.e. accumulation, weight, and input
buffers, are doubled and the global buffer size increased 8Ã—.
Modified memory capacities, at the PE and global buffer level,
are likely to impact the optimal strategy for data re-use and
NoC communication traffic reduction. With CoSA, we show
5.7Ã— speedup over Random and 1.4Ã— speedup over Timeloop
Hybrid search in Fig. 9b, demonstrating CoSAâ€™s capability
across different architectures.
C. Evaluation on NoC Simulator
To further compare the quality of schedules generated
by different scheduling schemes, we evaluate them on our
NoC simulation platform. The NoC simulation platform more
accurately captures the communication overhead from the onchip network as compared to the Timeloop models.
Fig. 10 shows the speedup relative to the Random baseline.
We observe that CoSA-generated schedules outperform the
baseline schedules for all four DNN workloads, with the
greatest performance gains occurring for convolutional layers,
e.g. DeepBench layers. Intriguingly, for these same layers,
Timeloop Hybrid scheduler actually under-performs Random
search as its internal analytical model does not accurately
capture the communication traffic in the network. On the
other hand, there is no significant difference between the
performance of FC layers among different schedules, as the
FC layers are heavily memory-bound with low PE utilization.
The DRAM access time dominates in these layers even with
the best schedules with respect to reuse of buffered data.
Overall, CoSA achieves a geometric average of up to 3.3Ã—
speedup relative to the best Random search solutions and 2.5Ã—
relative to Timeloop Hybrid search schedules across the four
networks. Furthermore, unlike the iterative nature of Random
and Timeloop Hybrid search schedules, CoSA schedules are
consistently performant with the one-shot solution.
D. Evaluation on GPU
To show the potential use of CoSA for general-purpose
hardware, we also formulate GPU scheduling as a constrainedoptimization problem using CoSA. We evaluate the performance of CoSA on GPU and compare it against TVM [15].
Target GPU. We target NVIDIA K80 GPU with 2496 CUDA
cores and a 1.5MB L2 cache. This GPU has a 48KB shared
memory and 64KB local registers, shared by a maximum of
1024 threads in each CUDA thread block. The thread block is
a programming abstraction that represents a group of threads
that can be run serially or in parallel in CUDA. The maximum
dimension of a thread block is (1024, 1024, 64). Violating these
constraints in the CUDA kernel results in invalid schedules.
Constraints. CoSA expresses the hardware constraints for
GPU thread groups and shared/local memory similarly to how
we specify the spatial resource and buffer capacity constraints
in Section III-C. Each thread group can be seen as a spatial
level with a specific size. The product of all three thread group
sizes is enforced to be smaller than 1024. The share memory
utilization is calculated as buffer capacity constraints, and the
register utilization is calculated by multiplying the total number
of threads with the inner loop register utilization.
Objective Functions. In CoSA, we compute the compute
objective by discounting the total compute cycles with the total
number of threads for GPU, to reflect the performance gain
from thread-level parallelism. We then adjust the weights of
the other objectives using a micro-benchmark.
We run TVM with the XGBoost tuner for 50 trials per
layer as the baseline. CoSA generates valid schedules in one
shot with a time-to-solution 2, 500Ã— shorter than TVM (0.02s
vs. 50s per layer). The CoSA-generated schedules achieve
1.10Ã— geomean speedup compared to the TVM schedules on
ResNet50 as shown in Fig.11.
VI. CONCLUSION
In this paper, we present CoSA, an optimization-driven
approach to DNN scheduling. Harnessing the regularities from
DNN workloads and target accelerator designs, we formulate
scheduling into a constrained optimization problem that can be
solved directly without incurring the high cost of iterative
scheduling. We devise a single mathematical formulation
to simultaneously solve for all three key optimizations in
scheduling: loop tiling, loop permutation, and spatial mapping.
Comparing our results to schedules generated from the stateof-the-art work, our approach achieves up to 2.5Ã— speedup
and 22% better energy-efficiency, with 90Ã— shorter time-tosolution.