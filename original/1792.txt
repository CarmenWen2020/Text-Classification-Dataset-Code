Abstract‚ÄîCryogenic computing, which runs a computer device
at an extremely low temperature, is highly promising thanks
to the significant reduction of the wire latency and leakage
current. A recently proposed cryogenic DRAM design achieved
the promising performance improvement, but it also reveals that
it must reduce the DRAM‚Äôs dynamic power to overcome the huge
cooling cost at 77 K. Therefore, researchers now target to reduce
the cryogenic DRAM‚Äôs refresh power by utilizing its significantly
increased retention time driven by the reduced leakage current.
To achieve the goal, however, architects should first answer many
fundamental questions regarding the reliability and then design
a refresh-free, but still robust cryogenic DRAM by utilizing the
analysis result.
In this work, we propose a near refresh-free, but robust
cryogenic DRAM (NRFC-DRAM), which can almost eliminate
its refresh overhead while ensuring reliable operations at 77 K.
For the purpose, we first evaluate various DRAM samples of
multiple vendors by conducting a thorough analysis to accurately
estimate the cryogenic DRAM‚Äôs retention time and reliability.
Our analysis identifies a new critical challenge such that reducing
DRAM‚Äôs refresh rate can make the memory highly unreliable
because normal memory operations can now appear as rowhammer attacks at 77 K. Therefore, NRFC-DRAM requires a
cost-effective, cryogenic-friendly protection mechanism against
the new row-hammer-like ‚Äúfaults‚Äù at 77 K.
To resolve the challenge, we present CryoGuard, our cryogenicfriendly row-hammer protection method to ensure the NRFCDRAM‚Äôs reliable operations at 77 K. With CryoGuard applied,
NRFC-DRAM reduces the overall power consumption by 25.9 %
even with its cooling cost included, whereas the existing cryogenic
DRAM fails to reduce the power consumption.
Index Terms‚ÄîDRAM, Low-power design, Temperature-aware
design, Emerging technologies
I. INTRODUCTION
Cryogenic computing, which runs a computer device at an
extremely low temperature such as 77 K (or ‚àí196 ¬∞C), is a
highly promising way to improve its power efficiency and
performance thanks to the significantly reduced wire latency
and leakage current [5], [34]. To realize the potentials, computer architects have developed temperature-aware architecture
modeling tools, used them to design various architecture
units for the target low temperature, and clearly showed their
performance and cost benefits [4], [24], [28].
As one of the promising components, a recent study proposed a low-power DRAM designed for 77 K (CLP-DRAM)
by decreasing its supply and threshold voltages [24]. CLPDRAM minimizes its cooling-cost-critical dynamic power and
‚àóCorresponding author.
thus achieves ‚àº10 % of overall power reduction. However,
when memory-intensive workloads are deployed, CLP-DRAM
becomes difficult to reduce the overall power due to the
quickly increased cooling cost. Therefore, architects are in dire
need of developing a much more power-efficient cryogenic
DRAM rather than applying only a naive voltage scaling.
From this perspective, researchers now focus on reducing
the cryogenic DRAM‚Äôs power-hungry refresh operations. As
the leakage current is nearly eliminated at 77 K, it slows down
the DRAM cell‚Äôs charge-loss process (or increases its data
retention time), as shown in recent studies [13], [38]. That is,
architects can correspondingly reduce the cryogenic DRAM‚Äôs
refresh rate and further reduce a significant amount of its
power consumption. Our analysis confirms the clear potential
of nearly eliminated refreshes, which can additionally reduce
the CLP-DRAM‚Äôs power consumption by 30 %.
However, before reducing the refresh rate, architects should
first answer the fundamental questions regarding the reliability
of the refresh-free DRAM. In our analysis, we find that the
DRAM row-hammer failure can be a critical challenge in
developing a near refresh-free DRAM to achieve the ideal
power efficiency. The row-hammer failure (or row-hammer
error) is data corruption in a DRAM row incurred by repetitive
accesses to a neighboring row [18]. As naively lowering a
DRAM‚Äôs refresh rate increases the memory accesses within
a refresh period, it can correspondingly increase the vulnerability to the row-hammer failure. Therefore, it is essential
to accurately identify the DRAM‚Äôs data corruption behaviors
with the decreased refresh rate at 77 K.
To resolve the challenge, this paper proposes a near refreshfree, but robust cryogenic DRAM (NRFC-DRAM), which
can almost eliminate its refresh overhead while ensuring the
reliability at 77 K. For this purpose, we first thoroughly analyze the possible refresh-rate reduction and the row-hammer
error behaviors of modern DRAMs at low temperatures. We
build FPGA-based custom memory controllers to adjust the
DRAM refresh rate, and then measure the modern DRAMs‚Äô
data retention time and row-hammer failure distributions at
low temperatures down to 150 K. In these experiments, we
rigorously test 18 DRAM modules with three DRAM vendors,
six chip types, and various data and access patterns. Our
experiment shows that although the worst-case retention time
of cryogenic DRAM can be prolonged to a few seconds (at
least 20 times longer than 64 ms), its row-hammer threshold

"$.*&&&UI"OOVBM*OUFSOBUJPOBM4ZNQPTJVNPO$PNQVUFS"SDIJUFDUVSF	*4$"

¬•*&&&
%0**4$"
2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) | 978-1-6654-3333-4/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ISCA52012.2021.00056
0% 20% 40% 60% 80% 100%
RT-DRAM
CLP-DRAM
CLP-DRAM‚Ä¶
Power (normalized to RT-DRAM)
DRAM static power DRAM access power DRAM refresh power
Cooling power for refresh Cooling power for others
CLP-DRAM
without refresh0%
Fig. 1: Power breakdown of room-temperature DRAM (RTDRAM), CLP-DRAM [24], and CLP-DRAM without refresh
does not increase much or even decreases at low temperatures.
As a result, we observe that NRFC-DRAM becomes much
more vulnerable to the row-hammer, and even the normal
workloads (rather than the malicious attacks) can incur the
failure. Therefore, we clearly reveal that protecting cryogenic
DRAM from row-hammer failure is not only a ‚Äúsecurity‚Äù
issue but also a ‚Äúreliability‚Äù challenge, which should be
resolved to ensure the correct functionality of NRFC-DRAM.
Next, to ensure the reliability of NRFC-DRAM, we analyze
major row-hammer protection mechanisms and provide the
guidelines for designing a cryogenic-friendly row-hammer
protection method. We first observe that the counter-based
protection methods provide the highest reliability as other
major row-hammer defenses cannot perfectly prevent the rowhammer failure incurred by ordinary workloads. Second, we
also observe that it is important to minimize the protection overhead of the counter-based approach because NRFCDRAM suffers from the huge counter cost when the roomtemperature-optimized method is naively applied. In summary,
a cryogenic row-hammer protection method should take the
counter-based approach and minimize the counter overhead
with cryogenic-friendly optimizations.
By following the guidelines, we present CryoGuard, our
cryogenic-friendly counter-based protection, which reduces
its overhead as follows. First, CryoGuard uses a uniformly
distributed counter structure instead of a room-temperatureoptimized structure, which reduces the area overhead by
44.1 %. Second, CryoGuard takes the cryogenic-friendly 3TeDRAM cell counters, which reduces the area overhead of
SRAM-based counters by twice. Third, CryoGuard utilizes an
existing error correction code (ECC) to detect and correct the
single-bit errors, which further reduces the area overhead by
79.5 %. By combining the three methods, CryoGuard guarantees the reliable operation of NRFC-DRAM while reducing
its counter area overhead by 14.2 %, compared to that of the
room-temperature protection. As a result, we provide a reliable
and robust NRFC-DRAM which achieves 25.9 % of overall
power reduction.
In summary, our work makes the following contributions:
‚Ä¢ Cryogenic DRAM analysis and finding: To the best
of our knowledge, this is the first work to analyze data
corruption behaviors of a near refresh-free cryogenic
DRAM and find its critical reliability challenge.
‚Ä¢ Cryogenic-friendly reliability method: We present a
reliable, cost-effective, and cryogenic-friendly error protection scheme along with our design guidelines. We also
firstly show that the row-hammer mitigation designs at
77 K should be totally different from those at 300 K.
‚Ä¢ Near refresh-free and robust cryogenic DRAM: We
propose a novel DRAM design, which can nearly eliminate its refresh overhead while ensuring its reliability
at 77 K. Our robust NRFC-DRAM leads to 25.9 % of
overall power reduction with the protection overhead
comparable to that of the room-temperature method.
II. BACKGROUND AND MOTIVATION
A. Fast and power-efficient cryogenic DRAM
Cryogenic computing, which is to operate a computer at
very low temperatures, is highly promising for performance
and power efficiency thanks to the benefits of low temperatures. For example, the wire resistivity is significantly reduced
at low temperatures [5], which improves the data transfer
speed of the wire. Also, as the subthreshold leakage current
of MOSFET becomes negligible at low temperatures [17],
cryogenic computing enables an aggressive voltage scaling to
significantly reduce the power consumption. Therefore, with
these advantages, architects can build cryogenic computers to
greatly improve both performance and power efficiency.
Among the two representative ultra-low temperatures, 77 K
and 4 K, we target 77 K due to its cost-effectiveness. 77 K
computing, which can be easily achieved with liquid nitrogen
(i.e., LN), can utilize CMOS technology with a manageable
cooling-power cost (Eq. (3) in Section VI-A3). On the other
hand, 4 K computing accompanies tremendous cooling-power
cost, which is over 400 times the device power [11]. For that
reason, the major target of 4 K computing is not classical
computing but superconductor-based [19], [25] or quantum
computing [2], [37]. Therefore, classical computer architects
have focused more on 77 K, rather than 4 K [4], [13], [24],
[28], [38].
For cryogenic computing, DRAM is one of the most promising candidates due to the following reasons. First, the low wire
resistance greatly reduces the DRAM access latency because
DRAM mainly relies on long wires to transfer data between
the memory cells and the chip interfaces. Second, the almost
eliminated leakage current significantly reduces the DRAM
power as DRAM consumes a huge amount of static power
[29]. To demonstrate these advantages, previous work [24] proposed a cryogenic-optimal DRAM design (i.e., CLP-DRAM).
With the carefully chosen DRAM circuit-level design and
operating voltages, the CLP-DRAM achieves 34.7 % shorter
access latency and 90.8 % lower power consumption than the
conventional DRAM.
However, although CLP-DRAM has cryogenic-optimized
MOSFET and circuit designs, it can still fail to achieve power
efficiency. Fig. 1 shows the power breakdown of DRAMs
when the memory-intensive workloads are deployed, as explained in Section VI. Following the previous work [24] and
the physics literature [12], we estimate the cooling cost for
77 K as 9.65 times the device power consumption. As the figure shows, CLP-DRAM in our scenario consumes even 1.2 %
more power than the conventional room-temperature DRAM
(RT-DRAM) due to the huge cooling power. Therefore, to

Row decoder
DRAM
subarray
Sense amplifier
‚Ä¶
‚Ä¶
Bitlines
Wordlines
(a) DRAM subarray
Wordline
Bitline
Access transistor
Storage capacitor
(b) DRAM cell with natural leakage
Leakage
Wordline (victim row)
Bitline
Access transistor
Storage capacitor
(c) DRAM cell with row-hammer leakage
Wordline (aggressor row)
Frequent
activation
Fig. 2: DRAM background for (a) the subarray structure and
the data loss due to (b) natural and (c) row-hammer leakage
definitely realize cryogenic DRAM‚Äôs benefit, we should find
new opportunities to further reduce the power consumption.
B. Opportunity for refresh power reduction
In this work, we focus on the DRAM refresh operation as a
great opportunity to save a large amount of power consumption
at 77 K. As shown in Fig. 2b, DRAM cells store data in
the capacitors as electric charges. However, the charges are
gradually leaked away and eventually lost after a certain time,
or data retention time. To prevent the data loss, DRAM chips
periodically refresh the cells with a period shorter than the
worst-case (i.e., shortest) retention time.
In the case of the cryogenic DRAM, the refresh period
can be greatly extended thanks to the nearly eliminated
subthreshold leakage current of the access transistors. As the
subthreshold current, which dominates the overall leakage
current, exponentially decreases with the temperature [17],
cryogenic DRAM‚Äôs overall retention time is greatly prolonged
[13], [38]. Thus, the cryogenic DRAM can consume much less
power than the RT-DRAM by extending the refresh period.
However, the previous work [24] designed CLP-DRAM
without considering the refresh-related benefit at 77 K. The
previous work conservatively assumed that CLP-DRAM‚Äôs
refresh period is the same as the period of RT-DRAM. As
a result, CLP-DRAM consumes 30.3 % of its device power
for the refresh operation, and thus the refresh correspondingly
incurs the same ratio of cooling power as shown in Fig. 1
and Eq. (3).
Therefore, in this paper, we target to build a near refreshfree cryogenic DRAM (NRFC-DRAM) to minimize its refresh
power overhead. As shown in Fig. 1, CLP-DRAM without
refresh shows the clear potential of eliminating refresh operations at 77 K. In other words, we can further reduce a
considerable amount of CLP-DRAM‚Äôs power by reducing the
refresh rate at cryogenic temperature.
C. Challenges for designing a near refresh-free DRAM
Despite a huge opportunity of saving power, naively extending the refresh period can make a DRAM device more
vulnerable to another type of the data loss, known as the rowhammer failure. As depicted in Fig. 2c, when we repeatedly
access a DRAM row (i.e., aggressor row), it incurs extra
charge leakage to the adjacent rows (i.e., victim rows). If
the number of activation exceeds a certain threshold (i.e., the
0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150
64ms (ordinary)
1280ms (20 times)
6400ms (100 times)
‹¥‹™th at room temperature
Highest number of row activation within a refresh period (thousands)
Fig. 3: The highest row-activation counts at room and cryogenic temperatures while executing SPEC2006 workloads [10]
row-hammer threshold or RHth) within a refresh period, the
victim rows lose their data. Several works demonstrated the
row-hammer failure and proposed defense mechanisms against
the failure at 300 K [8], [9], [16], [18], [30].
In this context, near-refresh free DRAM can be more
vulnerable to the row-hammer failure as the number of row
activations within a refresh period significantly increases. For
example, when we run SPEC2006 workloads [10] at the room
temperature (Fig. 3), the highest number of row activations
within a refresh period (64 ms) is negligible compared to
the smallest RHth among reported values (i.e., 10,000 [16]).
However, when we assume the 20‚àº100 times longer refresh
period with the same RHth at 77 K, the highest row activation
count exceeds the RHth at 300 K. This result shows that if
RHth does not significantly increase at 77 K, NRFC-DRAM
becomes vulnerable to the row-hammer failure. Thus, we
should carefully analyze the data corruption behaviors of
NRFC-DRAM.
To achieve the goal, it is necessary to accurately quantify
the possible refresh-rate reduction and the row-hammer failure
behaviors at low temperatures. However, there has been no
study to characterize the row-hammer failure at such low
temperatures. Also, even though previous studies measured
the DRAM retention time at low temperatures [13], [38],
they did not cover the up-to-date DDR4 DRAMs in their
experiments.
Also, after the row-hammer vulnerability of an NRFCDRAM is analyzed, architects should provide a cryogenicfriendly protection method against the possible failure. However, to the best of our knowledge, there has been no design
principle for the protection method targeting 77 K. To
ensure both robustness and efficiency, the cryogenic-friendly
protection method should guarantee the highest level of reliability as well as the minimized protection overhead.
In this paper, we resolve the challenges as follows. We
first perform rigorous DRAM measurements to analyze both
the data retention time and the row-hammer failure at low
temperatures (Section III). Next, we provide the guidelines
for designing a cryogenic-friendly error protection method
by analyzing the major row-hammer defense mechanisms
(Section IV). Finally, by following the guidelines, we present
our cryogenic-friendly reliability method, which realizes a
robust and low-power NRFC-DRAM design (Section V).
III. DRAM CHARACTERIZATION AT LOW TEMPERATURES
In this section, we analyze how the temperature affects the
row-hammer failure as well as the worst-case data retention
time. For rigorous analysis, we build custom FPGA-based
memory controllers and measure 18 DRAM modules with

TABLE I: Evaluation setup and configuration
Experimental setup and configuration
FPGA board Xilinx VC707 Xilinx ZCU104
DRAM type DDR3 DDR4
DRAM vendor
(# samples)
Vendor-A 2GB (√ó3)
Vendor-B 2GB (√ó3)
Vendor-C 1GB (√ó3)
Vendor-A 4GB (√ó3)
Vendor-B 4GB (√ó3)
Vendor-C 8GB (√ó3)
Data pattern
(data retention time)
All-ones
Checkered
All-ones
Checkered
Data pattern
(row-hammer failure)
Row-stripe
Checkered
Row-stripe
Checkered
Access pattern
(row-hammer failure)
Single-sided
Double-sided Double-sided
Temperature range 150 K‚àº320 K 150 K‚àº340 K
LN
FPGA DIMM DRAM
Copper
Plate
LN Evaporator
Thermocouple
Probe
Temperature
Data Logger
Fig. 4: Experimental setup to measure the DRAM‚Äôs rowhammer and retention characteristics at low temperatures
three DRAM vendors, six chip types, and various (or worst)
data and access patterns as summarized in Table I.
A. Experimental methodology
1) Experimental setup: Fig. 4 shows our experimental setup
to measure the data retention and row-hammer characteristics
at low temperatures. First, we use Xilinx VC707 and Xilinx ZCU104 FPGA boards, which have DDR3 and DDR4
SODIMM sockets, respectively. With these boards, we control
the DRAM refresh operations by modifying Xilinx Memory
Interface Generator (MIG). Next, to achieve stable low-target
temperatures, we utilize a liquid nitrogen (i.e., LN) evaporator.
With the high heat capacity of the LN evaporator, we control
the DRAM temperature from 150 K to 340 K by adjusting the
amount of LN. We attach temperature sensors to every DRAM
chip‚Äôs package and observe negligible temperature differences
among the chips. With the experimental setup, we measure
the retention time and the row-hammer threshold of 18 DDR3
and DDR4 DRAM modules from three vendors (Vendor-A,
Vendor-B, and Vendor-C) at various temperatures (Table I).
Note that we intentionally anonymize the vendors to avoid
giving a wrong impression.
2) Measurement process: Measurement processes 1 and 2
show the detailed steps to measure the retention time and
row-hammer threshold at the target temperature, respectively.
We measure the data retention time of each DRAM cell
as follows. We first write data to the entire DRAM rows
according to the specified data pattern. As DRAM‚Äôs data
retention characteristics are data-dependent [14], [15], [21],
[22], [32], we use the all-ones and checkered patterns, which
Measurement process 1 Data retention time
Input: Data pattern, T arget retention time
1: Write Data pattern to all DRAM rows
2: Disable the DRAM refresh for T arget retention time
3: Enable the DRAM refresh
4: Read all DRAM rows and count the flipped bits
Measurement process 2 Row-hammer threshold
Input: Data pattern, Access pattern, T arget RHth
1: for victim in all DRAM rows do
2: Write Data pattern.adjacent to victim ¬± 1
3: Write Data pattern.victim to victim
4: Disable the DRAM refresh
5: if Access pattern is Single-sided then
6: Activate one of victim ¬± 1 for T arget RHth times
7: else if Access pattern is Double-sided then
8: Activate both victim ¬± 1 for 1
2T arget RHth times
9: end if
10: Enable the DRAM refresh
11: Read victim and count the flipped bits
12: end for
are the representative patterns for measuring the retention
time in the previous work [26]. We then disable the refresh
operation for the target retention time. After that, we enable
the refresh operation again and check the entire DRAM to
count the flipped bits. By repeating this process for a range
of target retention time, we get the cumulative distribution of
DRAM cell‚Äôs data retention time.
On the other hand, we measure the row-hammer threshold
(RHth) of each DRAM cell as follows. We first choose a
victim row and write a data pattern to the adjacent rows (i.e.,
victim¬±1) and the victim row. For the data pattern, we use the
row-stripe and checkered patterns, which incurred the highest
number of flipped bits in [16]. Next, we disable the DRAM
refresh and activate adjacent rows by the target RHth times. In
our profiling, we repetitively access the same DRAM row with
the maximum speed and thus conservatively count the bit-flips
in the worst thermal condition. When we access the adjacent
rows, we test both the single-sided and double-sided access
patterns [16] for rigorous analysis. After that, we enable the
DRAM refresh and count the flipped bits in the victim row.
By repeating this process to test all DRAM rows for a range
of the target RHth, we get the cumulative distribution of the
DRAM cell‚Äôs RHth at the target temperature. Note that we
set the target RHth in a coarse-grained manner to observe the
failure at the wide range.
B. Result analysis and key observations
In this section, we present our measurements for the data
retention time and row-hammer threshold at low temperatures
and emphasize our key observations. Even though we test
three DRAM modules for each chip type, we show the worstcase result to clearly describe our observations. We show the
results from only the representative temperatures (e.g., the
highest and the lowest), but we observe the same trend in
other temperatures.
1) Refresh period: First, we confirm that the DRAM refresh
period can be dramatically prolonged at 150 K or lower
temperatures. Fig. 5 shows the retention time distribution of

320K 150K
All-one pattern Checkered pattern
1.E+0
1.E+2
1.E+4
1.E+6
1.E+8
1 4 16 64 256
# flipped bits
Retention time (s)
1.E+0
1.E+2
1.E+4
1.E+6
1.E+8
1 4 16 64 256
# flipped bits
Retention time (s)
(a)
1.E+0
1.E+2
1.E+4
1.E+6
1.E+8
1 4 16 64 256
# flipped bits
Retention time (s)
1.E+0
1.E+2
1.E+4
1.E+6
1.E+8
1 4 16 64 256
# flipped bits
Retention time (s)
(b)
1.E+0
1.E+2
1.E+4
1.E+6
1.E+8
1 4 16 64 256 1024
# flipped bits
Retention time (s)
1.E+0
1.E+2
1.E+4
1.E+6
1.E+8
1 4 16 64 256 1024
# flipped bits
Retention time (s)
(c)
Fig. 5: Retention time distribution of DDR3 DRAMs; (a)
Vendor-A (b) Vendor-B (c) Vendor-C
DDR3 DRAMs from three vendors, measured with the allones and checkered data patterns. The overall retention time
is extended with the temperature reduction, similar to the
previously reported results [13], [38]. In addition, the shortest
retention time in all measurements at 150 K is longer than
one second, except for the result in Fig. 5b. Even though one
flipped bit is observed at one second in the Vendor-B‚Äôs DRAM
at 150 K, such an outlier bit can be easily mitigated by either
redundancy or ECC [38]. Note that our measurement well
matches the previous experiment [38] in that most DRAM
modules show no fails within one second.
The up-to-date DDR4 DRAMs show a more clear extension
of their retention time at low temperatures. Fig. 6 shows
the retention time distribution of DDR4 DRAMs for various
vendors and temperatures ranging from 210 K to 340 K. The
figure clearly shows that the shortest retention time of the
DDR4 DRAMs is significantly prolonged at 210 K, at least five
seconds regardless of the vendors. Note that the measured data
with a retention time longer than eight seconds are omitted
in Fig. 6. At temperatures lower than 210 K, we expect that
the shortest retention time will be slightly more extended or
remain the same due to the temperature dependency of the
leakage mechanism [28].
Thus, we confirm that it is possible to nearly eliminate the
periodic refreshes by extending cryogenic DRAM‚Äôs refresh
period from 64 ms to a few seconds. We summarize our key
observation from the retention time measurement as follows.
Observation 1. The refresh period can be prolonged
to a few seconds at 150 K or lower temperatures.
Thus, we can significantly reduce CLP-DRAM‚Äôs refresh power consumption.
340K 300K 260K 210K
All-one pattern Checkered pattern
1.E+0
1.E+2
1.E+4
1.E+6
64 256 1024 4096
# flipped bits
Retention time (ms)
1.E+0
1.E+2
1.E+4
1.E+6
64 256 1024 4096
# flipped bits
Retention time (ms)
(a)
1.E+0
1.E+2
1.E+4
1.E+6
64 256 1024 4096
# flipped bits
Retention time (ms)
1.E+0
1.E+2
1.E+4
1.E+6
64 256 1024 4096
# flipped bits
Retention time (ms)
(b)
1.E+0
1.E+2
1.E+4
1.E+6
64 256 1024 4096
# flipped bits
Retention time (ms)
1.E+0
1.E+2
1.E+4
1.E+6
64 256 1024 4096
# flipped bits
Retention time (ms)
(c)
Fig. 6: Retention time distribution of DDR4 DRAMs; (a)
Vendor-A (b) Vendor-B (c) Vendor-C
2) Row-hammer error behavior: Next, we observe that
the row-hammer threshold only marginally increases or even
decreases at low temperatures, including 150 K. Fig. 7 and
Fig. 8 show the RHth distributions of DRAMs measured with
the single-sided and double-sided access patterns, respectively.
For the DDR3 DRAMs measured with the single-sided access
pattern, Fig. 7 shows that fewer bits are flipped at low
temperatures, regardless of the DRAM vendor and the data
pattern. Specifically, the flipped bits at 150 K are always fewer
than those at 300 K in all cases in Fig. 7.
However, we find that the reduction of the smallest (i.e.,
worst-case) RHth can be marginal at 150 K. For the DRAM
from Vendor-C in Fig. 7, even though the number of flipped
bits is reduced by 10‚àº100 times at 150 K, its smallest RHth
increases by only 66 %. Furthermore, for the DRAMs from
Vendor-A and B, the changes of the smallest RHth are
marginal compared to the improvement observed in VendorC‚Äôs DRAM. From the conservative perspective, it indicates
that the RHth of the DRAM modules would not increase much
even at the cryogenic temperature. Note that the nearly constant RHth can make a refresh-free DRAM more vulnerable to
the row-hammer-like failure because the increased number of
DRAM accesses within a refresh period has a higher chance
to exceed the threshold.
For more rigorous analysis, we further perform double-sided
row-hammer experiments for the two DRAM vendors (VendorA and B) with up-to-date DDR4 DRAMs (Fig. 8). We choose
Vendor-A and B because their DDR3 DRAMs show the worstcase behavior in Fig. 7. Our DDR4 DRAM experiments reveal
the novel trend that the number of row-hammer errors can
even increase at low temperatures. For example, we find that
the smallest RHth value at 150 K (8,100) is slightly lower than
that of 300 K (11,600). In summary, the row-hammer threshold

300K 150K
Row-stripe pattern Checkered pattern
1.E+0
1.E+2
1.E+4
1.E+6
300 500 700 900
# flipped bits
Target RHth (thousand)
1.E+0
1.E+2
1.E+4
1.E+6
300 500 700 900
# flipped bits
Target RHth (thousand)
(a)
1.E+0
1.E+2
1.E+4
1.E+6
200 400 600 800 1000
# flipped bits
Target RHth (thousand)
1.E+0
1.E+2
1.E+4
1.E+6
200 400 600 800 1000
# flipped bits
Target RHth (thousand)
(b)
1.E+0
1.E+2
1.E+4
1.E+6
300 900 1500 2100
# flipped bits
Target RHth (thousand)
1.E+0
1.E+2
1.E+4
1.E+6
300 900 1500 2100
# flipped bits
Target RHth (thousand)
(c)
Fig. 7: Row-hammer errors in DDR3 DRAMs with the singlesided access pattern; (a) Vendor-A (b) Vendor-B (c) Vendor-C
of DRAM does not increase much or even decreases at low
temperatures. Note that we obtain 11,600 as the smallest RHth
of DDR4 DRAMs at 300 K, which is similar to the value
reported by the state-of-the-art measurement, 10,000 [16].
We expect that the measured row-hammer-related trend at
150 K will be maintained at lower temperatures, including
77 K, mainly due to the temperature dependency of the rowhammer-induced leakage and threshold voltage (i.e., Vth). For
the row-hammer-induced leakage, its two major mechanisms
(i.e., trap-assisted carrier injection, wordline coupling) are
mitigated at lower temperatures [34], [39]. As a result, the
number of flipped bits can be slightly reduced at 77 K, similar
to our DDR3 measurements. On the other hand, the Vth of
DRAM access transistors increases at low temperatures [13],
and thus the noise margin (i.e., the voltage difference between
Vdd and Vth) correspondingly decreases. Note that DDR4
DRAM is more vulnerable to such noise margin reduction
because DDR4 DRAM operates at a lower Vdd than DDR3
DRAM, which matches our results. We emphasize our key
observation in the row-hammer error behavior as follows.
Observation 2. The row-hammer threshold does not
increase much or even decreases at cryogenic temperatures. Thus, naively extending the refresh period of
CLP-DRAM can incur much more row-hammer errors
due to the almost same RHth.
3) NRFC-DRAM‚Äôs row-hammer vulnerability: Based on
our measurements, we reveal that cryogenic DRAM with
the extended refresh period (i.e., NRFC-DRAM) becomes
more vulnerable to the row-hammer, and even the ordinary
workloads can incur the failure. In the following analysis, we
set NRFC-DRAM‚Äôs refresh period and row-hammer threshold
DDR4, 150K DDR4, 300K DDR3, 150K DDR3, 300K
Row-stripe pattern Checkered pattern
1.E+0
1.E+3
1.E+6
1.E+9
8 32 128 512 2048
# flipped bits
Target RHth (thousands)
1.E+0
1.E+3
1.E+6
1.E+9
8 32 128 512 2048
# flipped bits
Target RHth (thousands)
(a)
1.E+0
1.E+3
1.E+6
1.E+9
8 32 128 512 2048
# flipped bits
Target RHth (thousands)
1.E+0
1.E+3
1.E+6
1.E+9
8 32 128 512 2048
# flipped bits
Target RHth (thousands)
(b)
Fig. 8: Row-hammer errors in DDR3/DDR4 DRAMs with the
double-sided access pattern; (a) Vendor-A (b) Vendor-B The ratio of rows (cumulative)
70%
80%
90%
100%
The number of row activation (normalized to RHth) 300K
77K
99.95%
@77K
100%
@300K
Fig. 9: Cumulative distribution of row activation counts during
a refresh period
based on our previous observations. Based on our measurements and Observation 1, we conservatively set the refresh
period at 77 K to 1.28 s, which is 20 times longer than
64 ms. On the other hand, we set the RHth at 77 K as
8,100, which is the smallest value observed in our 150 K
experiment (Observation 2). For the room temperature DRAM,
we conservatively set the RHth at 300 K as 10,000, reported
by the state-of-the-art measurement [16].
Fig. 9 shows the cumulative distribution of the activation
counts for each DRAM row during a refresh period. We
derive the CDF from the gem5 [3] simulations by running
17 SPEC2006 workloads [10]. We normalize the number of
activations to the RHth at each temperature (i.e., 10,000 for
300 K, 8,100 for 77 K). At 300 K, no DRAM row is activated
more than RHth/4 times due to the short refresh period. In
other words, even the memory-intensive workloads do not
trigger the row-hammer failure at 300 K.
However, at 77 K, the same workloads can incur the rowhammer failure due to the significantly extended refresh period
and the marginally changed RHth. As shown in Fig. 9, 0.05 %
of DRAM rows are activated more than RHth times at 77 K.
Therefore, even the general-purpose applications can incur
the row-hammer failure. That is, preventing the row-hammer
failure becomes a critical reliability issue beyond a security
issue. We emphasize our last observation as follows.
Observation 3. NRFC-DRAM is much more vulnerable
to the row-hammer, and even ordinary workloads can
trigger the failure. That is, row-hammer is not only a
simple ‚Äúsecurity‚Äù issue anymore; it becomes a serious
‚Äúreliability‚Äù problem to be resolved.

IV. GUIDELINES FOR DESIGNING CRYOGENIC-FRIENDLY
ROW-HAMMER PROTECTION METHOD
As we identify the critical reliability challenge of NRFCDRAM, it becomes crucial to develop a cryogenic-friendly
row-hammer protection method. First, the protection scheme
should perfectly prevent the row-hammer failure while running
ordinary workloads. In addition, the power and area overhead
of the protection method should be low enough to realize a
robust NRFC-DRAM design. In this section, we provide the
guidelines to achieve the goals by analyzing the major rowhammer mitigation mechanisms.
A. Guideline 1. Utilize the counter-based protection method
We first suggest utilizing the counter-based approach to
ensure the reliable NRFC-DRAM operation at 77 K. To draw
the guideline, we first survey the major row-hammer defenses
against the malicious attack. In our discussion, we focus on
whether the defenses can perfectly prevent the row-hammer
failure or not while running the ordinary workloads.
In-DRAM defense: The in-DRAM defense is implemented
in some modern DRAMs [9] and autonomously refreshes the
victim candidates. However, as DRAM is a passive device,
such autonomous operations should occur only in its free
time (i.e., time for refreshes). That is, its protection capability
is tightly bounded by the refresh rate. Therefore, the inDRAM defense is difficult to guarantee the reliability of
NRFC-DRAM due to its low refresh rate. For example, the
in-DRAM defense cannot prevent the failure if a memoryintensive workload accesses a DRAM row more than RHth
between back-to-back refresh commands. Therefore, we need
a protection method whose capability is independent of the
refresh rate.
Software-based defense: The software-based defense utilizes software-accessible profiling to detect the malicious rowhammer attack without extra hardware. For example, ANVIL
[1] examines performance counters, identifies the abnormal
behaviors of the row-hammer attack, and prevents the DRAM
failure. However, they cannot ensure the reliability of NRFCDRAM due to the difficulties in identifying non-malicious applications which induce the unintentional row-hammer failure.
Therefore, we need a protection method that can precisely
identify the row-hammer failure.
Probability-based defense: The probability-based defense
utilizes the probabilistic mechanism to detect and prevent
a row-hammer attack. For example, PARA [18] refreshes a
DRAM row with a certain probability whenever its adjacent
rows get activated. Even with a very low refresh probability
(e.g., 0.1 %) per row activation, PARA prevents the rowhammer failure with a high probability, because the repetitive
adjacent-row activations are necessary for the row-hammer
attack. However, every probability-based defense cannot avoid
a slight chance to allow the row-hammer failure due to its
probabilistic nature, and thus it cannot ensure the reliability
of NRFC-DRAM running normal workloads.
Counter-based defense: The counter-based defense utilizes
hardware counters to track the number of activations for
Operating
temperature
0 4 8 12 16
300K
77K
Counter overhead of TWiCe (MB)
Fig. 10: Counter capacity overhead of TWiCe for 256 GB
DRAM at 300 K and 77 K
each DRAM row. In principle, the counter-based method
prevents the failure by refreshing the adjacent rows before the
activation count reaches the target RHth. That is, the counterbased approach‚Äôs reliability is independent of the refresh
period. Also, the counter-based defense can prevent every
row-hammer failure induced by ordinary applications. As a
result, with the accurately profiled smallest RHth, the counterbased protection can perfectly prevent row-hammer failures in
NRFC-DRAM. Note that the counter-based defense can ensure
both the reliability for ordinary applications and the security
against malicious attacks.
In summary, we confirm that the majority of existing rowhammer mitigations can fail to ensure the NRFC-DRAM‚Äôs
reliability, except for the counter-based approach. Therefore,
we should utilize the counter-based method for a robust
NRFC-DRAM design.
B. Guideline 2. Minimize the counter overhead at 77 K
We now emphasize the importance of reducing the counterbased protection‚Äôs area and power overhead at 77 K. To draw
the guideline, we perform an analysis with two representative counter-based mechanisms: a bounded time-window
mechanism (i.e., TWiCe [23]) and multi-row counter-based
mechanism (i.e., CAT [35]). In our analysis, we first explain
their key ideas for reducing the number of counters. Then, we
evaluate their overhead at 77 K in terms of the counter area
and power consumption. We analyze the counter overhead of
a system with 32 dual-rank 8 GB DDR4 DRAMs (256 GB;
1024 banks) running SPEC2006 workloads [10]. The detailed
power and area calculation methodologies are summarized in
Section VI-A.
1) Bounded time-window mechanism (i.e., TWiCe):
Key mechanism. The key idea of TWiCe is that the required
number of counters is bounded on a refresh period. As the
number of possible DRAM access is limited by the refresh cycle (Tref) and the mandatory time interval between consecutive
row activations (TRC), only a few DRAM rows can be activated
more than RHth times within a refresh period. By using the
fact, TWiCe prevents the row-hammer failure with quite a
few counters. Instead of allocating counters to every DRAM
row, TWiCe tracks only a few frequently activated rows whose
activation counts can reach RHth within a remaining refresh
cycle. According to the analysis in [23], the total number of
counters for TWiCe (Ncounters) is calculated as Eq. (1). At
300 K, the area overhead of TWiCe is only 2.71 kB per 1 GB
DRAM bank, which is regarded as a manageable overhead.
Ncounters = Tref
8192 ¬∑ TRC
¬∑ (1 +
8192

n=1
8192
n √ó RHth
) (1)
 
Counter
tree
DRAM
rows
Internal node Counter DRAM row group Hot row
(a) (b) (c)
Fig. 11: Dynamic counter allocation of CAT; (a) initial state,
(b) state after a counter allocation, (c) final state at 77 K
0
1
2
3
4
5
16 32 64 128 256
Power
(normalized)
Number of counters per bank
DRAM, dynamic
Counter, dynamic
Counter, static
48.05
(a)
0
1
2
3
4
5
256 512 1024 2048 4096
Power
(normalized)
Number of counters per bank
DRAM, dynamic
Counter, dynamic
Counter, static
(b)
Fig. 12: Power consumption of CAT to defend the rowhammer failure at (a) 300 K and (b) 77 K, normalized to the
power consumption of the 64-counters CAT at 300 K
Overhead at 77 K. However, the time-window mechanisms
suffer from the huge counter area overhead in NRFC-DRAM.
As shown in Eq. (1), the counter overhead linearly increases
with the refresh period. As the refresh period is extended by
at least 20 times at 77 K (i.e., Observation 1), the overhead
of time-window mechanisms significantly increases. For example, Fig. 10 shows the counter storage overhead of TWiCe
for 256 GB DRAM at 300 K and 77 K, respectively. TWiCe
requires only 694 kB of counters at 300 K, but the counter
overhead at 77 K exceeds 16 MB, which is even comparable
to the L3 cache capacity of commodity CPUs. Note that the
MB-scale hardware cost is significant because the counter
table should be located nearby the memory controller to avoid
refresh-rate-bounded in-DRAM defense at 77 K. Therefore,
the time-window mechanisms cannot be directly applied to
NRFC-DRAM at 77 K due to the huge counter capacity
requirement.
2) Multi-row counter-based mechanism (i.e., CAT):
Key mechanism. The key idea of multi-row counter mechanisms is that most DRAM rows are activated much fewer
than RHth within a refresh period. For example, Fig. 9 shows
that 99 % of DRAM rows are activated fewer than RHth
128 times
during 64 ms. In this case, we can allocate a counter for
a group of 128 contiguous rows with the hope that their
aggregated number of activations is still lower than RHth.
However, if a counter exceeds RHth, every row in the group
should be refreshed, and it incurs a huge refresh overhead.
To reduce the additional refreshes, CAT utilizes a dynamic
counter allocation method illustrated in Fig. 11a and Fig. 11b.
The hot row indicates the aggressor row, which is activated
more than RHth times during a refresh period. CAT first
uniformly divides the rows into a few coarse-grained groups
(Fig. 11a). For every row activation, CAT increases the corresponding counter by one. When a counter for the group
exceeds a certain threshold, CAT allocates a new counter to
split the corresponding group (Fig. 11b). As a result, CAT
(a) (b)
1
2
7
3 6
5 4
(c) (d)
Internal node
Counter
DRAM row group
Hot row
Warm row
Refreshed rows
Fig. 13: Example scenario that CAT incurs more extra refreshes than SCA at 77 K. (a) CAT at 300 K, (b) SCA at 300 K,
(c) CAT at 77 K, and (d) SCA at 77 K
tracks the hot rows more precisely with fewer additional
counters and thus reduces extra refreshes for the row-hammer
defense.
Overhead at 77 K. However, the protection overhead
of multi-row counter mechanisms becomes much larger in
NRFC-DRAM. As the refresh period gets extended at 77 K,
the number of row activations within a refresh period (1.28 s)
also drastically increases. Thus, even for normal applications,
the number of hot rows significantly increases at 77 K, as
shown in Fig. 11c. As a result, most counters quickly exceed
the RHth and incur the huge counter overhead.
Figs. 12a and 12b show the power consumption of CAT with
a different number of counters at 300 K and 77 K, respectively.
All data are normalized to the power consumption of roomtemperature CAT with 64 counters. As shown in Fig. 12a,
CAT with 64 counters minimizes the power consumption at
300 K. As a result, CAT requires 256 kB (4 Byte/counter ¬∑
64 counter/bank ¬∑ 1024 bank/system) of counter capacity to
minimize the power consumption. However, due to the increased number of hot rows at 77 K, the power-optimized CAT
requires 1024 counters per bank (Fig. 12b). In other words,
CAT requires 4 MB (4 ¬∑ 1024 ¬∑ 1024) of counters for 256 GB
DRAM, which is 16 times higher than the overhead at 300 K.
In summary, we confirm that naively using the existing
counter-based defenses incurs huge counter overhead at 77 K.
Thus, we should minimize the area and power overhead by
carefully designing a cryogenic-friendly protection method.
V. CRYOGENIC-FRIENDLY ROW-HAMMER PROTECTION
Following the guidelines from the previous section, we
present CryoGuard, our cryogenic-friendly protection method
based on the counter mechanism. To minimize the counter
overhead, we apply cryogenic-friendly optimizations to the
existing counter-based defense. In the following sections, we
explain how each optimization reduces the counter overhead.
A. Cryogenic-friendly counter allocation
Starting from CAT, we first improve both the power and
area efficiency by eliminating the overhead of the dynamic
counter allocation. We set CAT as our baseline because CAT
has relatively low overhead among defenses in Section IV.
CAT‚Äôs tree-based counter allocation efficiently works at
room temperature [35]. However, its dynamic scheme incurs

0% 20% 40% 60% 80% 100%
CAT-1024
SCA-1024
Area (normalized to CAT-1024)
(a)
0% 20% 40% 60% 80% 100%
CAT-1024
SCA-1024
Power (normalized to CAT-1024)
Counter, static Counter, dynamic DRAM, dynamic
(b)
Fig. 14: Area and power comparison for CAT and SCA
additional refreshes and counter overhead at cryogenic temperatures due to the row-hammer characteristics we observed
at 77 K (Observation 3). Therefore, we remove the dynamic
scheme and revert to the simple static counter assignment
(SCA), which allocates counters to evenly divided groups of
DRAM rows.1 SCA has the following advantages over CAT
at 77 K.
Fewer extra refreshes. At room temperatures, CAT incurs
fewer extra refreshes than SCA by focusing on a few hot rows.
Fig. 13a and Fig. 13b show the counter allocation of CAT
and SCA, respectively, in a scenario where only one hot row
incurs row-hammer failure. In such a scenario, CAT allocates
more counters to narrow down the hot row, while SCA equally
divides the rows. As a result, although both schemes have one
counter (‚ù∂ and ‚ù∑) triggered by the hot row, CAT incurs fewer
refreshes as ‚ù∂ covers fewer rows than ‚ù∑.
However, SCA is more effective than CAT for 77 K NRFCDRAM whose rows are more activated during the prolonged
refresh period. Fig. 13c and Fig. 13d show the counter allocation of CAT and SCA at 77 K in NRFC-DRAM, respectively.
In the figures, NRFC-DRAM has a lot of warm rows (i.e.,
the rows whose activation count is high but does not exceed
RHth) due to the increased row activations. In such a scenario,
CAT has two disadvantages incurring extra refreshes.
First, CAT‚Äôs counter values tend to be higher than the
actual number of row activations for their coverages due to
its counter-splitting mechanism. When the counter located at
‚ë§ splits, two child counters (i.e., ‚ù∏ and ‚ùª) need to inherit
the value of ‚ë§, which is the sum of row activation counts for
both row groups. As a result, even though ‚ùª has no hot row
in the coverage, it is likely to exceed the RHth due to the high
starting value inherited from ‚ë§.
Second, CAT suffers from additional refreshes at 77 K
because it tends to have a lot of large row groups with warm
rows (e.g., ‚ùº). As CAT uses most of the free counters to
narrow down the hot rows (e.g., ‚ù∏), it is highly likely to
have no free counter to be allocated for splitting large row
groups. As a result, if a large row group‚Äôs counter exceeds
the threshold due to the warm rows, CAT incurs a huge extra
overhead to refresh all rows allocated in the large row group.
On the other hand, SCA does not suffer from such penalties
and incurs fewer extra refreshes. For example, only one SCA
counter in Fig. 13d (‚ùπ) incurs extra refreshes. As a result, SCA
1SCA is also proposed by the authors of CAT [35], before they propose
the dynamic counter allocation scheme.
0% 20% 40% 60% 80% 100%
SCA-1024
SCA-1024 (eDRAM)
Area (normalized to SCA-1024 (SRAM))
(a)
0% 20% 40% 60% 80% 100%
SCA-1024
SCA-1024 (eDRAM)
Power (normalized to SCA-1024 (SRAM))
Counter, static Counter, dynamic Counter, refresh DRAM, dynamic
(b)
Fig. 15: Area and power comparison between SRAM and
eDRAM implementations for SCA
incurs 69.5 % fewer refresh operations than CAT at 77 K in
our simulation.
Lower area overhead. SCA occupies a smaller area than
CAT because it does not require the memory for the internal
nodes in CAT‚Äôs counter-tree structure (i.e., the white circles
in Fig. 11). By replacing CAT to SCA with the same number
of counters, the counter area overhead is reduced by 44.1 %
(Fig. 14(a)).
Lower power consumption. SCA‚Äôs fewer refreshes and
lower area overhead reduce the power consumption for the
row-hammer defense. Fig. 14(b) shows the row-hammerrelated power consumption of CAT and SCA at 77 K. The
69.5 % fewer extra refreshes reduce the DRAM dynamic
power by 69.5 %. Moreover, the 44.1 % smaller counter area
reduces the counter dynamic power by 24.1 %. Thus, SCA
reduces the row-hammer-related power by 33.1 % in total.
In summary, SCA outperforms CAT at 77 K in terms of
the number of extra refreshes, area overhead, and power
consumption. Therefore, we select SCA as our cryogenicfriendly counter allocation scheme.
B. Counter with cryogenic-friendly 3T-eDRAM
Next, we utilize a cryogenic-optimal memory cell technology to reduce both area and power overhead for the
row-hammer defense. At room temperatures, the counter is
commonly made from SRAM. However, at cryogenic temperatures, denser memory technologies become feasible because
their leakage current is almost eliminated at low temperatures.
For example, the previous work [28] implemented capacitysensitive L2 and L3 caches with 3T-eDRAM to double the
capacity without increasing the area. In a similar manner, we
utilize 3T-eDRAM to reduce the counter area overhead while
maintaining the capacity.
Fig. 15 shows the two advantages of the eDRAM-based
counter: lower area overhead and lower power consumption.
Lower area overhead. According to the previous work [7],
a 3T-eDRAM cell is 2.13 times smaller than an SRAM cell. In
addition, the total area (i.e., including the decoder, wordline,
bitline, and sense amplifier) of 3T-eDRAM is at least 50 %
smaller than the same-capacity SRAM [6]. Therefore, we
assume that the 3T-eDRAM-based counter occupies a twice
smaller area than the same-capacity SRAM counter (Fig. 15a).
Lower power consumption. According to the previous
works [6], [28], 3T-eDRAM consumes less power than the

A, 1bit A, 2+bits B, 1bit B, 2+bits
1.E+0
1.E+3
1.E+6
1.E+9
8 32 128
# flipped
words
Target RHth (thousands)
1.E+0
1.E+3
1.E+6
1.E+9
8 32 128
# flipped
words
Target RHth (thousands)
Row-stripe pattern Checkered pattern
Fig. 16: The number of DRAM words with at least one (1bit)
and two (2+bits) row-hammer failures in Vendor-A/B‚Äôs DDR4
DRAMs
0% 20% 40% 60% 80% 100%
SCA-1024 (eDRAM)
SCA-256 (eDRAM)
Area (normalized to SCA-1024 (eDRAM))
with ECC
(a)
0% 20% 40% 60% 80% 100%
SCA-1024 (eDRAM)
SCA-256 (eDRAM)
Power (normalized to SCA-1024 (eDRAM))
Counter, static Counter, dynamic Counter, refresh DRAM, dynamic
with ECC
(b)
Fig. 17: Area and power comparison for eDRAM-based SCA
with/without ECC awareness
same-capacity SRAM. First, 3T-eDRAM consumes less dynamic energy due to its smaller area and capacitance [6].
Second, thanks to the significantly increased retention time of
eDRAM cells, the refresh power overhead becomes manageable at 77 K [28]. Fig. 15b clearly shows the same trend, where
the dynamic power is reduced by 22.4 % with the manageable
eDRAM refresh power consumption (4.65 %). As a result, 3TeDRAM reduces row-hammer-related power consumption by
15.7 % in total.
In summary, cryogenic-friendly 3T-eDRAM can reduce the
counter area to half with lower power consumption. Therefore,
we select the 3T-eDRAM counter for our protection method.
C. ECC-assisted protection with cryogenic-friendly counter
Lastly, we further reduce the row-hammer protection overhead by utilizing the error correction codes (ECC). During
our row-hammer measurements at 150 K (Section III), we
observe that most DRAM words have at most one flipped
bit. Therefore, if we prevent single-bit errors by utilizing
DRAM ECC (i.e., SECDED [33]), we can configure our
counter-based defenses with a threshold for two (or more)
bits failures (i.e., two-bit row-hammer threshold; RHth,2). As
the RHth,2 is expected to be higher than the single-bit rowhammer threshold (RHth,1), we can further reduce the counter
area and power consumption while still ensuring reliability.
Note that ECC cannot solely prevent row-hammer failures
even at the room temperature [8].
To achieve the goal, we perform further row-hammer experiments to derive the RHth,2 at 77 K. Fig. 16 shows the
number of DRAM words with more than one (1bit) and two
(2+bits) row-hammer errors at 150 K, which is measured with
the same methodology as the DDR4 experiment in Fig. 8. In
our analysis, we observe that the lowest RHth,2 at 150K is
20,000, which is significantly higher than the RHth,1, 8,100.
TABLE II: Evaluation setup
Row-hammer defense specification
Design RTCAT
CryoTWiCe
CryoCAT
CryoSCA
CryoCounter
CryoEACAT
CryoGuard
RHth 10,000 8,100 8,100 8,100 8,100 20,000 20,000
ECCawareness No No No No No Yes Yes
Counter
organization CAT TWiCe CAT SCA SCA CAT SCA
Counter
cell type SRAM SRAM SRAM SRAM 3TeDRAM SRAM 3TeDRAM
# counter
per bank 64 4,096 1,024 1,024 1,024 512 256
DRAM specification
Design RT-DRAM CLP-DRAM NRFC-DRAM
Refresh
period 64ms 64ms 1280ms
Access
energy 2nJ/access 0.51nJ/access 0.51nJ/access
Static
power 171mW 1.29mW 1.29mW
Common specification
CPU Based on the Intel i7-6700
Memory 8GB DDR4-2400
Based on the two-bit threshold at 77 K (i.e., 20,000), we resize
the SCA eDRAM-based counter with the same optimization
as shown in Fig. 12. Compared to the counter without ECC
support, the number of counters per bank is changed from
1024 to 256. As a result, the area and power consumption
are reduced by 79.5 % and 48.6 %, respectively, as shown in
Fig. 17. Therefore, we adopt the ECC-assisted counter design
for our cryogenic-friendly defense scheme.
In summary, by applying every optimization to the baseline
counter-based approach, we propose the cryogenic-friendly
protection method, or CryoGuard. CryoGuard minimizes the
counter area and power consumption with the three optimizations (i.e., SCA, 3T-eDRAM counter, and ECC-assisted
protection).
VI. EVALUATION
In this section, we evaluate CryoGuard in terms of power
and area efficiency. We first introduce our evaluation methodology (Section VI-A) and evaluate the area overhead (Section VI-B) and power consumption (Section VI-C).
A. Evaluation methodology
1) Area overhead: We compare CryoGuard‚Äôs counter area
with CAT configured for the room-temperature operation
(i.e., RT-CAT). We choose RT-CAT as our baseline because we optimize CryoGuard starting from CAT. We also
compare CryoGuard with two state-of-the-art counter-based
schemes (i.e., TWiCe and CAT) configured for the cryogenictemperature operation (i.e., Cryo-TWiCe and Cryo-CAT). Finally, we evaluate our semi-optimized schemes (SCA (CryoSCA), SCA with eDRAM-based counter (Cryo-Counter), and
ECC-assisted CAT (Cryo-EACAT)) to show the effectiveness
of each optimization step. We summarize the specifications in
Table II.
We calculate the area of SRAM-based counters with CACTI
[29]. For 3T-eDRAM counters, we derive their area by scaling the area of same-capacity SRAM to half, following the
previous work [7]. We also include the difference in the unitcounter area among the different protection schemes.

1
4
16
64
256
1024
4096
16384
0.25
1
4
16
64
256
RTCAT
CryoTWiCe
CryoCAT
CryoSCA
CryoCounter
CryoEACAT
CryoGuard
# counters
per bank
Area (normalized
to RT-CAT)
Row-hammer defenses
Normalized area
# counters per bank
Fig. 18: Area overhead and the required number of counters
per DRAM bank for each protection scheme
2) Power evaluation methodology: We evaluate the power
consumption of near refresh-free cryogenic DRAM equipped
with CryoGuard (i.e., NRFC-DRAM with CryoGuard) by
comparing it with other memory designs: (1) conventional
room-temperature DRAM (RT-DRAM), (2) Cryogenic LowPower DRAM (CLP-DRAM) [24], (3) NRFC-DRAM, (4)
NRFC-DRAM with Cryo-TWiCe, and (5) NRFC-DRAM with
Cryo-CAT. We calculate the static and dynamic power of RTand CLP-DRAM from the previous work [24] for consistent
evaluation. Note that NRFC-DRAM is the same as CLPDRAM except for its longer refresh period (1.28 s instead of
64 ms). Also, note that the first three designs are equipped with
the ideal row-hammer protection, which consumes no power.
We summarize their specifications in Table II.
As the working scenario in our evaluation, we derive the
memory-access traces from gem5 [3] simulations, which run
17 SPEC2006 workloads [10] on Intel i7-6700 and 8 GB
DDR4-2400 DRAM. Note that the scenario is a memoryintensive situation where eight CPU cores access a single
DRAM module simultaneously. This situation is conservative
because the higher memory intensity induces more dynamic
and extra DRAM refresh power consumptions.
To evaluate the power consumption of various protection
schemes, we calculate the power for extra DRAM refreshes
and counters. First, we obtain the number of extra refreshes
from our trace-based TWiCe, CAT, and SCA simulators. Next,
we utilize CACTI [29] to calculate the SRAM power and
extract the 3T-eDRAM power from the previous work [28].
3) Cooling cost model: For the fair evaluation of cryogenic
DRAM, we need to include the cooling cost for maintaining
77 K. First, we ignore the one-time costs (e.g., the costs for
coolers and liquid nitrogen) because they are much cheaper
than the recurring cost (i.e., cooling electricity power) [27],
[28]. Thus, we focus on the cooling power consumption only,
which dominates the total power consumption at 77 K.
Pcooling = Pdevice ¬∑ CO (2)
Ptotal = Pdevice+Pcooling = (1+CO)¬∑Pdevice = 10.65¬∑Pdevice (3)
Based on the fundamental physics [20], Eq. (2) states the
linear relationship between the power consumption of coolers
(Pcooling) and the power consumed (and dissipated) by the
target devices (Pdevice). The cooling overhead (CO) is the
coefficient, which depends on both the target temperature and
the efficiency of the coolers. In our evaluation, we model the
CO of 77 K coolers as 9.65W/W, based on the real data from
235 coolers surveyed by previous works [12], [36]. As a result,
the total power consumption of our 77 K system (Ptotal) is the
0% 50% 100%
RT-DRAM
CLP-DRAM
NRFC-DRAM
NRFC-DRAM with Cryo-TWiCe
NRFC-DRAM with Cryo-CAT
NRFC-DRAM with CryoGuard
Power (normalized to RT-DRAM)
DRAM static power DRAM access power DRAM refresh power
Counter power RH defense refresh power Cooling power
Fig. 19: Total power consumption of the RT-DRAM, CLPDRAM, and NRFC-DRAM with row-hammer defenses
power consumed by the target devices (Pdevice) multiplied by
10.65 (Eq. (3)). On the other hand, we ignore the cooling
cost for the room-temperature system, so we conservatively
underestimate the power efficiency of the cryogenic DRAM.
B. Area overhead evaluation
Fig. 18 shows the number of counters per bank and area
overhead of row-hammer defenses when each defense is
configured to minimize power consumption. The area is normalized to the area of RT-CAT. Compared to the naive cryogenic defenses (i.e., Cryo-TWiCe and Cryo-CAT), CryoGuard
achieves a significant area reduction (66.5 and 17.4 times,
respectively). CryoGuard‚Äôs area overhead is even comparable
to RT-CAT.
The naive cryogenic defenses (Cryo-TWiCe and Cryo-CAT)
suffer from significant area overhead. Cryo-TWiCe and CryoCAT have 57.01 and 14.95 times higher area overhead than
RT-CAT. The huge area overhead results from the 20 times
longer refresh period with the almost constant RHth.
With the cryogenic-friendly counter allocation, Cryo-SCA
reduces the counter area by 1.79 times compared to Cryo-CAT.
As the tree-based allocation becomes ineffective at 77 K, the
uniform counter allocation becomes more efficient.
In Cryo-Counter, the area overhead is further reduced to
4.18 times of RT-CAT area by implementing the counters with
the cryogenic-friendly 3T-eDRAM. Note that the half-sized
3T-eDRAM is only feasible at low temperatures because its
short retention time is prolonged at 77 K [28].
Finally, by using ECC support, CryoGuard reduces area
overhead by 4.88 times further. CryoGuard‚Äôs area is even
14.2 % lower than the area of RT-CAT. CryoGuard‚Äôs main
idea is to configure the counter only for the two-bit errors
while utilizing the DRAM ECC support (i.e., SECDED). As
most commodity server-class DRAMs already support ECC,
CryoGuard requires no additional cost for its implementation.
Note that the ECC-only design (Cryo-EACAT) has 8.36
times higher overhead than RT-CAT. It clearly indicates that
both the counter-area optimizations (i.e., SCA, 3T-eDRAM)
and ECC support are necessary to build NRFC-DRAM with
the manageable counter overhead.
C. Power evaluation
Fig. 19 shows the total power consumption (including the
cooling cost) of various memory designs. The values are
normalized to RT-DRAM‚Äôs power.

128.%
10%
80%
60%
64.%
512.%
40%
320%
RTCAT
CryoTWiCe
CryoCAT
CryoGuard
RTCAT
CryoTWiCe
CryoCAT
CryoGuard
RTCAT
CryoTWiCe
CryoCAT
CryoGuard
0.5x RHth 0.75x RHth 1x RHth
3T-eDRAM
capacity
SRAM
capacity
SRAM 3T-eDRAM
Fig. 20: Counter capacity of Cryo-TWiCe, Cryo-CAT, and
CryoGuard with lower row-hammer threshold
In RT-DRAM, the dynamic power occupies 38.1 % of the
total power and incurs huge cooling power consumption at
77 K. Since the dynamic power does not decrease at the cryogenic temperature, RT-DRAM at 77 K consumes at least 4.06
times higher power than the RT-DRAM at 300 K, according
to Eq. (3). This challenge motivated the previous work [24]
to design cryogenic low-power DRAM (CLP-DRAM).
However, even with the drastically reduced DRAM power
(9.5 %), CLP-DRAM cannot achieve power efficiency due to
the huge cooling cost. To achieve power efficiency, we focus
on the refresh power, which occupies 30.3 % of CLP-DRAM‚Äôs
power consumption (as shown in Fig. 1).
By extending the refresh period to 1.28 s, NRFC-DRAM
shows reduced power consumption (70.8 %). In fact, such a
power reduction is achievable only with the ideal row-hammer
defense. However, the naive cryogenic row-hammer defenses,
Cryo-TWiCe and Cryo-CAT, consume 29.0 % and 5.8 % of
additional power, respectively.
On the other hand, CryoGuard consumes only 3.3 % of
additional power. The total power consumption of NRFCDRAM with CryoGuard is similar to the ideal NRFC-DRAM.
By using CryoGuard, we can build near refresh-free DRAM
with negligible area overhead and 25.9 % of total power
reduction.
D. RHth sensitivity analysis
In this paper, we set RHth at 300 K and 77 K as 10,000 and
8,100, the smallest values in the previous work [16] and our
measurement, respectively. However, the RHth at 300 K and
77 K can be lower than our setup due to the RHth scaling in
the future DRAM [16] and the slight temperature dependence
of the RHth. Therefore, we should clarify the effectiveness of
CryoGuard for various RHth values lower than our setup.
Fig. 20 and Fig. 21 show the counter capacity and the
refresh-related power consumption of the four schemes for
various RHth values. For example, for the 0.5√ó RHth configuration, we set RHth of RT-CAT, Cryo-TWiCe, Cryo-CAT, and
CryoGuard as 5,000, 4,050, 4,050, and 10,000, respectively.
We assume the system with 256 GB DRAM, as the same as the
analysis in Section IV-B. Note that we scale the two vertical
axes in Fig. 20 to match their corresponding die area (e.g.,
64 kB SRAM has the same area as 128 kB 3T-eDRAM).
First, RHth of both 300 K and 77 K can be reduced in the
future generation of DRAM. Even in those cases (i.e., 0.5√ó
RHth, 0.75√ó RHth), CryoGuard always achieves the lowest
capacity and refresh-related power consumption. For example,
at 0.5√ó of RHth, the counter overhead of Cryo-TWiCe and
0%
4%
8%
12%
RTCAT
CryoTWiCe
CryoCAT
CryoGuard
RTCAT
CryoTWiCe
CryoCAT
CryoGuard
RTCAT
CryoTWiCe
CryoCAT
CryoGuard
0.5x RHth 0.75x RHth 1x RHth
Periodic refresh Defensive refresh Counter
101.1% 100.9% 100.6%
Refresh-related power
(normalized to RT-DRAM)
Fig. 21: Power consumption of Cryo-TWiCe, Cryo-CAT, and
CryoGuard with lower row-hammer threshold
Cryo-CAT is 32 MB and 8 MB, which are 64 times and 16
times larger than that of RT-CAT, respectively. On the other
hand, CryoGuard shows the lowest counter capacity with the
minimum power consumption for every RHth value.
In addition, only 77 K RHth can be lower than our setup due
to the slight temperature dependence of RHth (Section III-B2).
Even in that case, the CryoGuard‚Äôs overhead (i.e., CryoGuard
at 0.5√ó RHth and 0.75√ó RHth) is still comparable to that
of 1√ó RHth RT-CAT. For example, CryoGuard‚Äôs counter
overhead for 0.5√ó RHth is only twice larger than RT-CAT
overhead at 1√ó RHth. Therefore, we believe that our CryoGuard is always effective regardless of the RHth scaling in
the future DRAM and its temperature dependence.
VII. RELATED WORK
In this section, we discuss the prior works for row-hammer
characterization, row-hammer defense, and 77 K memory.
Row-hammer characterization: S.Kim et al. [16] measured the row-hammer threshold for various DRAMs. Park
et al. [31] and Yang et al. [39] explained the temperature dependence of the row-hammer threshold. However, no previous
work shows the row-hammer threshold and error distribution
at the cryogenic temperatures.
Row-hammer defense: Kim et al. [18] firstly reported the
row-hammer failure and proposed PARA. Seyed et al. [35]
adopted an adaptive tree, and Lee et al. [23] applied the timewindow to optimize counter‚Äôs area and power consumption.
However, the previous works only focused on 300 K, not 77 K.
77 K cryogenic memory: Lee et al. [24], Min et al. [28],
and Byun et al. [4] proposed the cryogenic-optimal DRAM,
cache, and pipeline designs, respectively, which improve the
server performance and power efficiency. Rambus [13], [38]
measured DRAM‚Äôs cell retention time at 77 K. However, they
did not evaluate the DRAM refresh-power reduction at 77 K.
They also did not consider the row-hammer characteristics
(i.e., threshold and error distribution) at 77 K, which make
DRAM extremely vulnerable at the temperature. To the best
of our knowledge, this work is the first study to characterize
the row-hammer error behaviors at cryogenic temperatures.
VIII. CONCLUSION
In this paper, we proposed the near refresh-free, but robust
cryogenic DRAM by carefully utilizing the increased retention
time at 77 K. To achieve the goal, we first analyzed DRAM‚Äôs
error behaviors and revealed critical reliability challenges to
lower the refresh rate. We resolved the challenges by developing a cryogenic-friendly protection method, which combines

an existing ECC and evenly distributed eDRAM counters.
Our evaluation clearly showed that cryogenic DRAM‚Äôs power
efficiency could significantly improve while ensuring a reliable
operation at 77 K.