Abstract‚ÄîPersonalized recommendation systems have become
a major AI application in modern data centers. The main
challenges in processing personalized recommendation inferences
are the large memory footprint and high bandwidth requirement of embedding layers. To overcome the capacity limit
and bandwidth congestion of on-chip memory, near memory
processing (NMP) can be a promising solution. Recent work on
accelerating personalized recommendations proposes a DIMMbased NMP design to solve the bandwidth problem and increases
memory capacity. The performance of NMP is determined by
the internal bandwidth and the prior DIMM-based approach
utilizes more DIMMs to achieve higher operation throughput.
However, extending the number of DIMMs could eventually lead
to significant power consumption due to inefficient scaling. We
propose SPACE, a novel heterogeneous memory architecture,
which is efficient in terms of performance and energy. SPACE
exploits a compute-capable 3D-stacked DRAM with DIMMs for
personalized recommendations. Prior to designing the proposed
system, we give a quantitative analysis of the user/item interactions and define the two localities: gather locality and reduction
locality. In gather operations, we find only a small proportion
of items are highly-accessed by users, and we call this gather
locality. Also, we define reduction locality as the reusability of
the gathered items in reduction operations. Based on the gather
locality, SPACE allocates highly-accessed embedding items to
the 3D-stacked DRAM to achieve the maximum bandwidth.
Subsequently, by exploiting reduction locality, we utilize the
remaining space of the 3D-stacked DRAM to store and reuse
repeated partial sums, thereby minimizing the required number
of element-wise reduction operations. As a result, the evaluation
shows that SPACE achieves 3.2√ó performance improvement
and 56% energy saving over the previous DIMM-based NMPs
leveraging 3D-stacked DRAM with a 1/8 size of DIMMs. Also,
compared to the state-of-the-art DRAM cache designs with the
same NMP configuration, SPACE achieves an average 32.7% of
performance improvement.
Keywords‚ÄîRecommendation System, Embedding Layer, Locality, Heterogeneous Memory, Near Memory Processing
I. INTRODUCTION
As recommendation systems are continuously expanding
across e-commerce, social networking, and search engines [1],
[4], [5], [7], [8], personalized recommendation inferences consume the majority of resources in the AI data center [25]. To
leverage personalized interaction inputs between individuals
and millions of items, deep learning-based recommendation
models employ embedding layers [15], [20], [40], with lookup
(gather) and pooling (reduction) operations per embedding
table. Unfortunately, in production-scale data centers with
orders of magnitude more embeddings, their performance is
dominated by the embedding operations, which have sparse
1-Stack
1-Stack
2-Stack
1-DIMM
2-DIMM
4-DIMM
8-DIMM
1-Rank
2-Rank
4-Rank
8-Rank
0
2
4
6
8
10
12
0 32 64 96 128
Relative Operation Throughput
(Norm. to Single TensorDIMM)
Memory Capacity (GB)
HMC-Opt
HBM-Opt
TensorDIMM
RecNMP
DLRM-large
Capacity Limit
Fig. 1: Bandwidth and capacity scalability of memory, when applying
NMP for embedding layers. The vertical line labeled DLRM-large
represents the size of 64 embedding tables with vectors of 512B
each [30]. 3D-stacked DRAMs (HMC-Opt and HBM-Opt) have
limited capacity whereas DIMM-based DRAMs (TensorDIMM and
RecNMP) have low scale-up.
and irregular memory access patterns [24], [25], [27], [30],
[32]. These memory-bound embedding operations lead to the
major performance bottleneck in conventional data center
platforms.
As a promising technique for accelerating memory-intensive
workloads such as personalized recommendations, nearmemory processing (NMP) architectures have been proposed
in a variety of hardware platforms and programming models [10], [16], [28], [30], [32], [39], [57]. Among the prior
work, utilizing 3D-stacked DRAM is an attractive approach
since multiple DRAM dies can provide high bandwidth while a
logic die supports compute functionality [10], [28], [39], [57].
However, 3D-stacked DRAMs [38], [45], [50], [53] are limited
to several GBs and might not be big enough to store the entire
embedding tables of the production recommendation models.
Recently, TensorDIMM [32] and RecNMP [30] proposed
to use specialized DIMM based NMP designs in order to
scale memory space up to several tens of GBs. However,
due to the relatively low bandwidth of commodity DIMMs,
the DIMM-based NMP designs require multiple DIMMs to
achieve performance comparable to the 3D-stacked DRAMs.
Figure 1 illustrates the memory capacity and throughput of
embedding operation using DIMM-based NMP solutions and
NMP-optimized 3D-stack DRAMs, respectively. In the case
of 3D-stacked DRAMs (e.g., HBM-Opt and HMC-Opt), they
provide higher bandwidth but have smaller memory capacity
compared to the DIMM-based solutions. So, as the embedding
size increases, page faults or lacks of physical memory occur

"$.*&&&UI"OOVBM*OUFSOBUJPOBM4ZNQPTJVNPO$PNQVUFS"SDIJUFDUVSF	*4$"

¬•*&&&
%0**4$"
2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) | 978-1-6654-3333-4/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ISCA52012.2021.00059
more frequently. Meanwhile, TensorDIMM [32] can store embedding tables in DIMMs with high capacity and scalability.
However, the throughput increases at a lower rate due to the
relatively low bandwidth of DIMMs. Also, if TensorDIMM
extends to have multiple DIMMs for performance improvement, the sclae-out of DIMMs entails the inefficient growth
of capacity. To overcome the performance limit of a single
DIMM, Liu Ke et al. proposed RecNMP [30] using NMP designs in each rank to exploit rank-level parallelism. Rank-level
scaling can provide an additional performance improvement
with the limited number of DIMM slots. However, it still has
a limit due to the low scalability of the commodity DIMM
structure. In addition, the state-of-the-art DIMM technologies
utilize DRAM chips with higher density while increasing the
capacity of a single DIMM up to 512GB [33]. However, this
also cannot contribute to the overall performance improvement
due to the bandwidth limit.
To tackle these memory challenges, we propose sparse
locality-aware (SPACE) architecture that utilizes heterogeneous memory to take advantage of both high throughput
of 3D-stacked DRAM and large data storage capacity of
DIMMs. We perform a detailed analysis on the characterization of embedding operations (Section III) and present a
data management technique to efficiently allocate embedding
tables (Section IV). Based on our evaluation, we identify the
following two localities for each embedding operation:
1) Gather locality: Embedding gather operations perform
embedding table lookups for the previously interacted
items. In gather operations, we observe a phenomenon
called ‚Äùlong-tail,‚Äù which implies that a large proportion
of accesses for an embedding table come from a small
number of items. We call this gather locality that are
generated by a few popular items on the recommendation system.
2) Reduction locality: After the gather operations, embedding reduction operations perform element-wise summations of item vectors gathered from the embedding table.
During reduction operations, we find there are frequently
occurred combinations of items among the users. Based
on this observation, we define reduction locality that is
due to the frequent reuse of the reduced items.
We design a heterogeneous memory allocation framework
for SPACE based on the observed gather and reduction localities. To optimize memory space with gather locality, SPACE
allocates a small number of frequently accessed item embeddings to high-bandwidth 3D-stacked DRAM and the majority
of rarely accessed embeddings to high-capacity DIMMs. The
proposed item allocation technique leverages the maximum
throughput of the entire memory system while minimizing the
use of 3D-stacked DRAM. Subsequently, to optimize memory
bandwidth with reduction locality, we exploit partial sums,
which are reduction of multiple items with high reusability
among the users. Partial sum access skips multiple item accesses and element-wise operations, leading to faster and more
energy-efficient embeddings. Although storing more partial
sums helps to improve performance, it requires additional
memory space and needs to be carefully designed considering
tradeoffs. The proposed SPACE allocates embedding items
to 3D-stacked DRAM based on gather locality first and then
stores partial sums to the remaining space.
To the best of our knowledge, SPACE is the first study to
leverage processing in heterogeneous memory for personalized
recommendations. Also, we present an analysis of embedding
behaviors and specialized hardware and software co-design. In
summary, SPACE makes the following contributions:
‚Ä¢ We achieve both the bandwidth and capacity scaling using
the heterogeneous memory system. SPACE takes the
bandwidth advantage from utilizing a 3D-stacked DRAM
and capacity advantage from high-density DIMMs.
‚Ä¢ We analyze the locality of embedding gather and reduction operations across eight categories of recommendation datasets and demonstrate that the locality of embedding operations exhibits very similar pattern. Our analysis
on results shows a new insight that social phenomena are
occurring in user and item interactions.
‚Ä¢ We propose the embedding-specific design, including the
data management framework and hardware for skipping
reduction operations. SPACE achieves a performance gain
of 55% compared to NMP with a large 3D-stacked
DRAM and 32.7% compared to the previously proposed
heterogeneous model [17] when adopting the same nearmemory processors.
II. RECOMMENDATION SYSTEM AND NEAR MEMORY
PROCESSING
This section describes the DNN architecture for personalized recommendation models and the characteristic of each
layer. We specifically address the embedding layer that dominates the execution time of recommendation models in production environments. We also present the limitation of prior
DIMM-based NMP studies in terms of performance scaling
and energy consumption.
A. Characteristics of Personalized Recommendation Models
Emerging deep learning-based personalized recommendation systems suggest predicting item ranking based on personal characteristics and previous interactions. For an accurate
prediction of interactions between many users and items, personalized recommendation systems employ embedding layers.
For the personalized recommendations in production environments, we use Facebook‚Äôs deep learning recommendation
model (DLRM) in this research. DLRM consists of an embedding layer and two fully-connected layers (bottom-FC and
top-FC) [40].
Figure 2 shows a simplified deep learning-based recommendation model. In order to suggest an item to a single user,
DLRM receives a set of user characteristics and the user‚Äôs past
interactions as input vectors. The user characteristics are dense
inputs containing personal information (e.g. age, gender, etc.).
On the other hand, the user‚Äôs past interactions are sparse inputs
indicating the preferred item indices. Dense inputs are handled

User Characteristics User‚Äôs Past Interactions
Concatenate
Embedding Layer
Bottom-FC Layer
‚Ä¶
Compute Intensive
Memory Intensive
Top-FC Layer
Gather [Item 1, Item 2, Item 4, ... ]
Reduction
Item 1 + Item 2
+ Item 4 + ‚Ä¶
Category 1 Category N
Fig. 2: Architecture of deep learning recommendation model
by the bottom-FC layers, and sparse inputs are processed by
embedding layers.
Embedding layers is designed to extract features of user‚Äôs
preferred items from the embedding table. The gather operations collect the item vectors that are looked up by sparse
input. Next, the reduction operation merges the item vectors
by element-wise summation. Compared to the fully-connected
layer where the weight vector and the input vector are multiplied and added, the embedding layer shows relative simple
element-wise operations but the access pattern is extremely
irregular. In addition, the embedding table consists of millions
of item vectors but only tens of vectors are used for an
inference.
Since embedding layers and fully-connected layers have
different operation characteristics, they require different system behaviors when executed. Fully-connected layers with
a highly regular computational pattern have the advantages
of leveraging CPU or GPU with fast on-chip memory and
multi-core processors. On the other hand, embedding layers
have a large footprint and memory-oriented operation, causing memory challenges in conventional server environments.
Because the size of embedding table exceeds the size of the
on-chip memory, on-chip memory conflicts frequently occur
and introduce frequent off-chip data traffics. These memory
problems of embedding layers contribute to the slowdown in
recommendation systems.
B. Near Memory Processing for Recommendation System
In order to provide efficient memory systems, for the
production-scale embedding layer, recent studies proposed
the near memory processing designs [30], [32] that execute
embedding operations in memory instead of the host processors. The NMP architectures not only solve the on-chip
memory conflict problem but also accelerate the embedding
layer operation. Due to the fact that the embedding tables
require a large memory capacity from tens of MBs to several
GBs, previous researches place the processing module inside
the commodity DIMMs.
Figure 3 shows how previous work implement the DIMMbased NMP design and accelerate embedding operations.
TensorDIMM [32] focuses on mitigating the PCI-e transfer
overhead caused by the large size of embedding tables in the
TensorDIMM
S S
RecNMP
S S S S S S Processing Unit
S SRAM Cache
DRAM Array
DRAM Array
Storing Embedding
Legend
Embedding
M
Embedding Layers
Embedding
N
DLRM-1
DLRM-2
Gather
Reduction
Embedding
Operation
Fig. 3: Previous near-memory processing approaches for accelerating
embedding operations
discrete GPU system. Also, to provide sufficient memory capacity, it stores the embedding table in DIMMs instead of the
limited GPU memory. In TensorDIMM, DIMMs provide the
requested embedding vectors to the NMP module for gather
and reduction operations. For the top-FC layers, TensorDIMM
transfers the reduction values to the GPU boards running the
recommendation models. Meanwhile, RecNMP, which targets
CPU servers, provides NMP modules per rank with rank-level
parallelism. Placing the NMP in the ranks can linearly improve
performance proportional to the number of ranks in a single
DIMM. In addition, the NMP module of RecNMP has an
SRAM cache to exploit the data locality.
C. Analysis of DIMM-based Near Memory Processing
To accelerate the embedding operations, the aforementioned
NMP approaches suggest to comprise multiple DIMMs and
exploit DIMM-level or rank-level parallelism. Also, as shown
in Figure 2, the bottom-FC and embedding layers exist independently and two layers can execute in parallel using different
devices. For example, memory-intensive embedding operations are performed on the NMP modules whereas a computeintensive bottom-FC is performed on the CPU cores. However,
since the results from two layers need to be combined and
transferred to the top-FC, it is essential to reduce the waiting
time between devices. Given these environments, we consider
two design perspectives as the number of DIMMs increases.
Performance aspect: If the process bottleneck is the embedding layers, then a performance improvement can be achieved
by reducing the execution time of embedding operations with
more DIMMs. However, no performance improvement occurs
when the bottleneck is bottom-FC layers.
Energy aspect: Since the NMP reduces execution delay of the
embedding layers, the energy consumption does not proportionally increase with having more DIMMs. On the other hand,
more DIMMs has a little or no impact on the execution delay
of FC layers due to the low memory utilization. Thus, the
energy consumption is proportional to the number of DIMMs.
We perform experiments varying the number of DIMMs
with an eight-core CPU system. In our experiment, single
DLRM is executed on each core as described in Section V. By
comparing with NMP models exploiting different parallelisms,
we scale out by using a DIMM, that is a 1-rank DIMM with an
NMP module. Table I shows the neural network configuration
we used.
Figure 4 shows the execution time for embedding layers,
with various numbers of embedding tables and DIMMs. In-

TABLE I: Network Configuration of DLRMs
DLRM network configuration
Batch size
Bottom-FC layers
Top-FC layers
Embedding pooling
Embedding vector
Embedding size
Number of embedding tables
64
128-64-32
128-32-1
80
512B
1,000,000
8 / 16 / 32 / 64
creasing the number of embedding tables makes delay on
embedding layers than bottom-FC layers. For example, when
processing embedding operations with using a single DIMM,
the embedding layers with more than 16 embedding tables
spend longer delay than the bottom-FC layers. In order to
make the embedding layer operation faster than bottom-FC
layers, more DIMMs are required. In the case of Emb-64, even
eight DIMMs cannot achieve faster execution delay compared
to the bottom-FC layers. Utilizing eight DIMMs for eight
cores might be a very expensive memory system configuration
considering that AMD‚Äôs EPYC server consists of 64 cores (up
to 128 threads) and 16 DIMMs (up to 64 ranks) [9].
0
2
4
6
8
DIMM √ó1
DIMM √ó2
DIMM √ó4
DIMM √ó8
DIMM √ó1
DIMM √ó2
DIMM √ó4
DIMM √ó8
DIMM √ó1
DIMM √ó2
DIMM √ó4
DIMM √ó8
DIMM √ó1
DIMM √ó2
DIMM √ó4
DIMM √ó8
Emb-8 Emb-16 Emb-32 Emb-64
Embedding execution time
(Norm. to Bottom-FC)
Bottom-FC
Fig. 4: Embedding layer execution time compared to bottom fully
connected layers
Although the large memory configuration with more
DIMMs is a feasible solution, DIMM scaling also imposes
additional energy consumption. Figure 5 shows the energy
consumed in DIMMs by the NMP operations (embedding
layers) and CPU operations (bottom-FC + top-FC layers).
When the number of embedding tables increases, the amount
of computation would be enlarged as well. Both of the NMP
and CPU operations exhibit growth in energy consumption.
Meanwhile, increasing the number of DIMMs has different
results in NMP and CPU operations. More number of DIMMs
in NMP operation does not heavily increase the amount of
energy consumption, since using more DIMMs reduces the
execution delay owing to the parallel processing of NMP. On
the other hand, the operation on CPU experiences dramatic
increase of energy consumption as the number of DIMMs
increases. This is due to that increasing the number of DIMMs
does not successfully reduce the execution time of the CPU
operations and only increases the power consumption. As
a result, expanding the DIMMs to accelerate more NMP
operations significantly increases energy consumption in CPU
operations.
0
30
60
90
120
DIMM √ó1
DIMM √ó2
DIMM √ó4
DIMM √ó8
DIMM √ó1
DIMM √ó2
DIMM √ó4
DIMM √ó8
DIMM √ó1
DIMM √ó2
DIMM √ó4
DIMM √ó8
DIMM √ó1
DIMM √ó2
DIMM √ó4
DIMM √ó8
Emb-8 Emb-16 Emb-32 Emb-64
Relative energy
consumption of DIMMs
Embedding
Bottom-FC + Top-FC
Fig. 5: Energy consumption of embedding layer and bottom-FC and
top-FC layers
III. SPARSE EMBEDDING OPERATING IN HETEROGENEOUS
MEMORY
In this section, we first present two localities in embedding
operations. Based on the observation, we find an efficient
allocation ratio between HBM and DIMMs, and the reuse technique. Further, we show the main challenges in applying the
proposed technique to the previously studied heterogeneous
memory systems.
A. Locality of Sparse Embedding Operations
We evaluate both embedding gather and reduction operations across various categories of real-world recommendation
datasets [2], [6], [26], [41], [44], [55], and found that there
exist high localities in each embedding operations (gather and
reduction) in practical online services.
User 1 Pooling [2, 3, 8]
User 2 Pooling [2, 7, 8]
User 3 Pooling [2, 4, 8]
User 4 Pooling [4, 5, 8]
8
2
5
3
7
4
6
1
(a) Pooling graph
0
1
2
3
4
5
Item 8
Item 2
Item 4
Item 3
Item 5
Item 7
Item 1
Item 6
Item Access Count
Item sorted by access
Frequently-used
Gather
(b) Item frequency
0
1
2
3
4
5
Item 8
Item 2
Item 4
Item 3
Item 5
Item 7
Item 1
Item 6
User ID
Item sorted by access
Repeated
Reduction
(c) Item combination
Fig. 6: The example is given with a small-sized example for easy
understanding. (a) illustrates a pooling graph represented with nodes
and edges, and (b) shows an embedding gather locality based on
item access frequency. (c) shows embedding reduction frequency and
locality of each item.
In production-scale personalized inferences, millions of
active users access multiple item indices in each embedding
table. To model complex embedding table accesses and reduction patterns across users, we apply the graph representation [12]. Figure 6 (a) shows an example network of
interactions among four users and eight items in embedding
operations. Each item index of the embedding table is expressed as a node, and the pooling that represents the reduction
pattern of gathered items for each user, creates edges between
item nodes. For example, User 1 having item indices of [2, 3,
8] connects Node 2, 3, and 8. In the same way, User 2,3, and
4 also create edges between the corresponding nodes.

(a) Google Maps (b) Amazon eBooks (c) LastFM Musics (d) MovieLens (e) Animation (f) Steam Game
Fig. 7: Gather locality in different categories of datasets
(a) Google Maps (b) Amazon eBooks (c) LastFM Music (d) MovieLens (e) Animation (f) Steam Game
Fig. 8: Reduction locality in different categories of datasets
Figure 6 (b) shows the number of accesses to each item
index sorted in a descending order; item 8 and 2 are the most
frequently accessed two items. In Figure 6 (a), these items are
connected to the most of the edges and become the dominant
item nodes that exhibit the power-law characteristics. We call
this popularity as gather locality. Figure 6 (c) represents the
result of converting the pooling graph (Figure 6 (a)) to
the user-item graph. We find that the item combination with
Item 8 and 2 are accessed by User 1, 2, and 3 in common.
We define this combination of repeatedly accessed items as
reduction locality. To verify gather and reduction locality
across multiple applications, we evaluate the item frequency
and repeated combination with production-scale interaction
datasets. Figure 7 and 8 show the localities of embedding
operations in real-world datasets, analyzed in the same way
as shown in Figure 6 (b) and (c).
Gather locality: Figure 7 shows the power-law distribution
in different datasets. In fact, the distribution implies that
most of the users are only interested in a small portion
of the popular items. This characteristic is called a longtail phenomenon [12], and means only a few items occupy
more than 80% of the total access count; this can be easily
encountered in the online business market [14], [42], [43],
[54], [56]. Taken together, we discover that the same long-tail
phenomenon occurs in the embedding gather operations.
Reduction locality: As shown in Figure 8, user-item matrices
show the gradation pattern in different categories. These
patterns imply that many popular items are likely to be favored
by a user simultaneously, and preferences for those items may
also be common to the other users. In a view that people
generally have similar preferences, reduction locality has the
similar intuition to the collaborative filtering [48].
B. Utilizing Locality in Heterogeneous Memory
For the proposed heterogeneous memory architecture with
processing capability, we introduce two techniques based on
the embedding localities. One is embedding table distribution
and allocation across heterogeneous memory, and the other is
reducing operation utilizing partial sums of embedding items.
The first technique, heterogeneous memory allocation, determines how much portion of the embedding tables are stored
in HBM and DIMMs, that can directly affect the performance
of near-memory processing. To achieve high performance, we
exploit the gather locality in our allocation method first. Since
there are a relatively small number of highly-accessed items
based on our gather locality analysis, we utilize HBM to store
these items, thereby providing higher bandwidth. Whereas,
the items with low gather locality requires a large capacity
and shows a relatively low utilization rate. Therefore, we use
DIMMs to store those items.
0.0
0.5
1.0
1.5
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Relative throughput of the
memory requests served
(Norm. to HBM-only)
Ratio of items stored on HBM. Rest of the items are stored on DIMM
DIMM HBM Overall
Fig. 9: Memory bandwidth according to the heterogeneous memory
allocation (case study of LastFM)
In Figure 9, we show the number of memory requests
successfully served by varying the number of embedding items
in HBM, which is sorted based on the access rates. The y-axis
shows the relative throughput of requests compared to the case
when HBM is solely used. The throughput of HBM increases
rapidly when only a small number of highly accessed items are
stored and reaches its maximum throughput when storing 5%
of the total items. At the maximum, the result indicates that
51.4% more requests are served compared to the HBM-only
case. This is because a large number of low-locality items are
served by DIMMs, so there are less channel and bank conflicts
in HBM. DIMMs offer the additional throughput by storing
items with low gather-locality. To achieve the maximum

bandwidth, both the HBM and DIMMs serve items based on
the available bandwidth. In our experimental environment, the
item serving ratio between the HBM and DIMMs is close to
4:1 in the entire dataset.
0%
10%
20%
30%
40%
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Rate of items
served as psums
HBM allocation rate of psums sorted by frequency
psum2 psum3
Fig. 10: Rate of items served as psums according to the storing partial
sums (case study of LastFM)
The second technique that is based on reduction locality,
uses partial sums to accelerate the embedding layer. We
find that preparing partial sums (psums) from element-wise
addition beforehand can remove the repeated gather operations
and element-wise summations. For example, assuming that we
reduce with 3 items and HBM already stores the partial sum of
3 items, the processing module can skip 3 gather operations
and 2 element-wise additions by only using the partial sum
(psum). Figure 10 shows the ratio of item served as psums,
with varying capacity for storing psums. We experiment with
psums of two items (psum2) and three items (psum3). Since
the capacity of possible partial sums exceeds the size of
the embedding table, we only allow storing psums up to
the total number of items; the x-axis shows the ratio of the
number of partial sums over the number of items. When the
number of stored psum2 reaches its maximum, psum2 serve
37.2% of the total reduction requests. Since using psum2
removes one reduction and replaces two table accesses for
gather with single psum access, it reduces 18.6% of elementwise summations and memory accesses. On the other hand,
storing psum3 with addition of three items only serves 13%
of the embedding operations, because all combination of
psum3 is significantly large compared to the embedding size.
Accordingly, our proposed system only stores psum2 in HBM.
Table II shows experimental results of using each localities
in the entire dataset. When utilizing gather locality to store
21.3% of the total items in HBM, HBM and DIMMs serve
item requests with 4:1 ratio and the system provides the maximum throughput. We find that the overall memory bandwidth
is improved by 60.3% on average compared to an HBM-only
system. In addition, psum2 serve 19.7% of items and reduce
the element-wise summations and memory accesses by 9.8%
on average.
TABLE II: Experimental results of gather/reduction locality
Locality Experiments Results
Gather Avg. item allocation rate of HBM 21.3%
Avg. throughput improvement 60.3%
Reduction Avg. operation rate of psum2 19.7%
C. Challenges in Previous Heterogeneous Memory
Previous studies on heterogeneous memory employ the
combination of HBM and DIMMs [17], [19], [23], [31], [34],
[37], [46], [47], [51], [52]. They can be categorized into (i)
using HBM as hardware-managed cache for DIMMs [17],
[23], [34], [47] and (ii) flat addressable memory approaches
across HBM and DIMMs as main memory [18], [46], [51].
We evaluate these two different approaches for near-memory
processing and show the challenges in applying our localityaware techniques to the other heterogeneous memory architectures. The system configuration of each heterogeneous
memory architecture is summarized in Section V.
0.0
1.0
2.0
3.0
cds
kds
tvm
map
lfm
mvl
ani
stm
Avg.
cds
kds
tvm
map
lfm
mvl
ani
stm
Avg.
cds
kds
tvm
map
lfm
mvl
ani
stm
Avg.
BEAR CAMEO PoM
Memory Bandwidth
HBM demand DIMM demand HBM overhead DIMM overhead
Fig. 11: Memory bandwidth consumption in the previous heterogeneous architectures applying the proposed allocation ratio
0%
5%
10%
15%
20%
0.0
0.2
0.4
0.6
cds
kds
tvm
map
lfm
mvl
ani
stm
Avg.
cds
kds
tvm
map
lfm
mvl
ani
stm
Avg.
cds
kds
tvm
map
lfm
mvl
ani
stm
Avg.
Store all Store 1/2 Store 1/4
Reuse Rate
Banddwidth Overhead
Overhead Reuse rate
Store all Store 1/2 Store 1/4
Fig. 12: Bandwidth overhead and reuse rate of psums according to
three different psum storing configurations
To lessen any kind of additional software overhead, the
previous heterogeneous memory systems adopt the hardwarebased management that migrates data between the memories
during execution time. When the embedding items are moved
to the other memory structure, the internal bandwidth of the
data movement becomes the major performance bottleneck.
Therefore, near memory processing can be slowed down due to
the bandwidth congestion due to the internal data movement.
Figure 11 shows the bandwidth consumption in the previously
proposed heterogeneous memory for gather operations when
storing the embeddings according to our allocation policy.
BEAR [17] uses HBM as a DRAM cache and is considered
as a bandwidth-efficient model, and it shows 16% of overhead
for data movement between HBM and DIMM. On the other
hand, PoM [51] using the coarse-grained data management
consumes a large bandwidth overhead, 76% on average. In
summary, the existing architectures suffer from excessive
bandwidth consumption due to the dynamic data transfer
between HBM and DIMM.
To verify the effectiveness of caching psums, we implement
the direct-mapped DRAM cache introduced by BEAR [17] to
store psums. We only allow to store psum2 from the reduction
operations. Since there are many possible combinations of

SPACE::Prepare_Emb(Emb emb)
{
Align_by_locality(emb)
HBM_emb √Ö emb.high_item
DIMM_emb √Ö emb.low_item
HBM_emb √Ö emb.high_psum
Alloc_n_Map(HBM, DIMM)
}
Main {
SPACE::Prepare_Emb(emb0)
for(inference) {
FC(),‚Ä¶
SPACE::Pooling(emb0)
Conc(),‚Ä¶
}
}
(a)
Processing
Unit
Main Memory
DRAM Cache
Extension
Core
L1/L2
cache
Memory Controller
Last-level Cache
Core
L1/L2
cache
Mapping
Pooling Kernel
‚Ä¶
SPACE
Instruction
‚Ä¶
Request
Existed Hardware
Extended Hardware
Extended Software
Other Operation Execution Path
Pooling Operation Execution Path
Embedding Table Allocation Path
Legend
High Locality Item, Partial Sum
Low Locality Item
Offloading
Driver
Allocation
Host Side Memory Side
Alined Allocator
Embedding
Software Hardware
(b)
Fig. 13: (a) Framework of SPACE (b) SPACE architecture overview
psum2 when pooling the items, we limit the number of partial
sums to store in the DRAM cache. Figure 12 shows the
bandwidth overhead to store psums and the reuse rate of
psums. We arrange the pooling according to the locality of
the item and generate psums by pairing items in order. When
storing the all the psums (store all), 40.6% of bandwidth
overhead occurs and 8.9% of total operations reuse psums on
average. Storing only quarter of the psums (store 1/4) reduces
the bandwidth overhead to average 8.6%, but aggravates the
reuse rate of psums to average 2.8%. Considering that the ideal
reuse rate is average 19.7%, the dynamic caching of psums
shows a relatively low reuse rate. Since the replacement of
items and psums can significantly degrade the performance,
we propose static data allocation in Section IV.
IV. SPACE ARCHITECTURE
In this section, we introduce SPACE, which adds HBMbased NMP to a conventional system for embedding layer processing. SPACE leverages the bandwidth of the heterogeneous
memory system through the hardware and software co-design.
The software supports the overall data management, allowing
SPACE to operate NMP modules with no modifications to a
host processor or DIMMs.
A. Overview
SPACE manages embedding tables based on the localities
of the embedding layer described in Section III-A. With the
analysis of gather locality, SPACE exclusively allocates highlocality items to an HBM and low-locality items to DIMMs. To
apply the psum reuse method for reduction locality, SPACE
stores the psums in the remaining space of the HBM. The
item allocation and psum storage managed by the software
are performed prior to the inference. The reason to perform
data allocation beforehand in a static way is that managing
and storing data dynamically introduce additional bandwidth
overhead and degrade the performance when processing embedding layers (Section III-C).
Figure 13 (a) shows the framework of SPACE. Prior to
the inference, SPACE profiles the locality of the items and
rearrange the embedding tables. Beginning the inference, the
allocator distributes and allocates items of the embedding table
to the HBM and DIMMs. The distribution ratio is proportional
to the bandwidth ratio of the HBM and DIMMs. After allocating the items, the framework generates the high-locality
psums and stores them in the HBM. When the inference
starts, the software driver delivers the mapping information
of the allocated embedding table to the processing units for
NMP. The detailed algorithms for the item allocation and psum
storage are described in Section IV-B. During inference, the
pooling kernel (embedding layer) is offloaded to the memory
system, for computations in the HBM-side processing units.
The detailed mechanism for the execution of pooling kernels
is described in Section IV-C.
As shown in Figure 13 (b), we extend the software and hardware of a conventional system for embedding layer processing.
In the proposed architecture, the allocator stores the items
and psums and the offloading driver dispatches NMP kernel.
This software extension enables NMP processing modifying
the memory controller and no other hardware modifications
are needed. Then, the extended memory controller decodes
the kernel to determine the memory addresses of the items
and psums and converts them into SPACE instructions. The
address translation of the items and psums is performed inside
of the memory controller instead of the host-side memory
management unit. When executing instructions, the processing
unit receives high-locality items and psums from the HBM
and low-locality items from the DIMMs and proceeds to the
operations of embedding layers.
B. Preprocessing for Memory Allocation
The preprocessing rearranges each embedding table according to the locality of item and determines the group of items
and the number of psums to be stored in HBM. After that,
memory allocation framework creates the HBM embedding
and DIMM embedding using the prepocessing information and
allocates them in heterogeneous memory.
The preprocessing process consists of 4 steps described in
Algorithm 1. STEP 1 first determines the embedding size to be
stored in HBM. Similar to buddy memory allocation, the initial
size of the HBM embedding is selected as the smallest power
of 2 MB, but larger than the embedding table. This is because
configuring HBM embeddings can be small fragmentation of
HBM embeddings by splitting into halves later. STEP 2 is the
process of generating an aligned embedding table. In this step,

Algorithm 1 Locality-aware pre-processing pseudo code
1: procedure RESERVE EMBEDDING SIZE  STEP 1
2: HBM Embedding = 2 MB
3: While Raw Emb < HBM Embedding
4: HBM Embedding *=2
5: end procedure
6:
7: procedure ALIGN EMBEDDING TABLE  STEP 2
8: access count (Raw Emb)
9: Aligned Emb = sort by access (Raw Emb)
10: end procedure
11:
12: procedure SET ITEM-LINE  STEP 3
13: Item Access = 0, Item-line = 0
14: While Item Access/Total Access < HBM Bandwidth/Total Bandwidth
15: Item Access += Access of Aligned Emb[Item-line]
16: Item-line += 1
17: end procedure
18:
19: procedure SET PSUM-LINE  STEP 4
20: Remaing HBM Embedding
21: = HBM Embedding - Item-line*Vector Size
22: Number of Psum = 0, Psum-line = 0
23: While Number of Psum*Vector Size < Remaining HBM Embedding
24: Number of Psum += Psum-line
25: Psum-line += 1
26: end procedure
the framework ranks items of the embedding table based on
the number of accesses per item. The accesses are counted
either based on the training set of the embedding layer or the
previous reference set. STEP 3 is the process of calculating the
item-line for classifying the items to be stored in the HBM and
DIMMs. The item-line is the borderline value that indicates
the distribution of item vectors between the HBM and DIMMs.
The access rate to an item with an index lower than the itemline should be equal to the HBM bandwidth ratio of the total
memory bandwidth. At STEP 4, the number of psums that can
be stored in the remaining space of the HBM embedding is
calculated. The information of the psums to be stored in the
HBM embedding is represented by the psum-line value. If the
two items have an index lower than the psum-line, the psum
of these items is present in HBM.
Aligned Embedding
HBM Embedding
Item-line
Psum-line
DIMM Embedding
Item Psum
Item
(a) HBM and DIMM embedding
Emb-1 512MB Emb-1
Emb-2 512MB
Emb-3 256MB
Emb-4 512MB
Emb-1
Emb-1
Emb-1
Emb-3
Emb-3 Emb-2 Emb-4
Emb-2
Emb-2
HBM Embeddings 1GB Memory Space
(b) Buddy table allocation
Fig. 14: Allocation framework of SPACE. (a) illustrates the generating HBM and DIMM embedding. (b) shows the example of the
buddy table allocation when incomming four HBM embeddings.
After the preprocessing steps, memory allocation process is
performed. Figure 14 shows the memory allocation framework
of SPACE. Through the item-line, psum-line, and aligned
embedding table obtained from the preprocessing, memory
allocation framework generates the HBM and DIMM embedding as shown in Figure 14 (a). The item-line is used
to distribute the items of aligned embedding table between
the HBM embedding and DIMM embedding. In the HBM
embedding, items below the item-line value are stored first.
Then, for the remaining space, the HBM stores all the psums
that can be created from the items within the psum-line value.
The psums are stored in the ascending order of the item
index. For example, if the psum-line is 3, the HBM stores
psums in the HBM embedding in the following order: psum
of (item0, item1), psum of (item0, item2), psum of (item1,
item2). Finally, the DIMM embedding stores items that are
beyond the item-line value.
To efficiently store the HBM embedding in the HBM with
limited capacity, SPACE employs the buddy table allocator.
The buddy table allocator has similar behaviors to the buddy
allocator. However, the difference is in the allocation method
when there is no free space for storing the incoming embeddings. The buddy table allocator does not replace the new
embedding table with another, but instead, it replaces the other
embedding psum region. Figure 14 (b) shows the example of
allocating a pre-processed HBM embeddings in 1 GB of HBM
space. There is no capacity problem when allocating two HBM
embeddings with 512 MB (Emb-1 and Emb-2). However, if
the HBM tries to store an additional 256 MB HBM embedding
(Emb-3), the space is run out and the HBM may have to
replace it with one of the HBM embedding with 512 MB.
Instead, the buddy table allocator replaces it with the psum
region of the other HBM embedding (Emb-1) that contains
more than 256 MB of psum. On the other hand, if there is
not enough large psum region to replace with incoming HBM
embedding (Emb-4), the incoming HBM embedding reduce
the psum region. In this case, Emb-4 is stored only half of
HBM embedding with 256 MB. The allocation framework
tries to store as many items as possible rather than psums.
C. Hardware Designs for In-Memory Processing
SPACE offloads embedding kernels to the memory controller by the help of an offloading driver. Then, the additional
logic in memory controller decodes the embedding kernel
and translates it into SPACE instructions. Since the SPACE
instructions is designed to find the physical memory address,
SPACE does not require from host-side memory management
unit. During the address translation, SPACE uses the value of
item-line and psum-line set inside the memory controller to
acquire the address information where the items are stored.
Figure 15 illustrates the process of encoding instructions
from the embedding kernel and the instruction format of
SPACE. During the allocation time, the memory controller
sets the base address for each embedding tables and saves
it in the DIMM mapping table (D.map table) and the HBM
mapping table (H.map table). Meanwhile, the item-line and
psum-line are also set in the item table and psum table.
Figure 15 describes the operation flow of executing embedding
layers. First, the memory controller separates the offloaded
    
Pooling
Kernel
Item
Table
Psum
Table
1
0
Itm-line > Item Index?
( if Item-line = 3 )
1
0
Psum-line > Item Index?
( if Psum-line = 2 )
Coalescer
[ 4 ]
[ 0, 1, 2 ]
[ 0, 1 ]
[ 2 ]
Item Indices
[0, 1, 2, 4 ]
[ ps0 , 2 ]
D.Map
Table
H.Map
Table
Instruction
Encoder
DIMM
Base Addr.
DIMM CMD
Generator
HBM
Base Addr.
Embedding ID
Emb-1
C.SLS
Add.I
Add.M
Read C/A DQ SPACE Instruction Format
Add.M Vector Size HBM Addr Register ID
OpCode Vector Size Source Destination
Add.I Vector Size Vector Pos Register ID
OpCode Vector Size Position Destination
Part of Vector
Immediate value
Fig. 15: Extended memory controller design and instruction format
kernel into embedding ID (Emb-1) and item indices ([0, 1,
2, 4]). The embedding ID is used to get the item-line and
psum-line information of the embedding and the base address
of embedding. The memory controller classifies and merges
the item indices according to the item-line and psum-line.
In this example, the item-line and psum-line are set to 3
and 2, respectively. Item index [4] that is larger than the
value of the item-line is classified as the item to be stored
in DIMMs. On the other hand, since item indices [0, 1, 2]
are within the item-line, the controller can figure out that the
items are stored in the HBM. After that, the value of psumline is used to decide whether items are stored as psums
in the HBM. As the stored order of psums is based on the
item indices, the coalescer can calculate the psum index using
the item indices. In this example, since the psum of item
index [0] and item index [1] is in front of psum vectors,
the coalescer translates the two item indices to the single
psum index [ps0]. Finally, the instruction encoder converts the
item index (index [2]) and the psum index [ps0] into SPACE
instruction. When adding the vectors of psum and item [ps0, 2]
stored in the HBM, the memory controller launches memory
operand addition instructions (Add.M) to the logic die. On
the other hand, when the item vector with index [4] is stored
in DIMMs, the memory controller generates memory requests
and receives item‚Äôs value. By using the item value received
from the DIMMs, the memory controller creates an immediate
operand addition instruction (Add.I) and passes it to the logic
die of the HBM. When converting the instructions, the location
of item or psum vectors can be found by using the item or
psum indices and the base address in H.map and D.map table.
The processing units in the logic die receive item vectors
from the HBM and accumulate the item vectors in the internal
register file when generating Add.M instruction. Otherwise,
Add.I instructions use the item vectors recieved from DIMMs
as immediate values and accumulate it inside HBM. Since
SPACE utilizes two different operands in its instruction, we
design a dedicated instruction format for each of Add.M
and Add.I instructions. Add.M instruction uses the physical
address of HBM as a source operands. On the other hand, the
Add.I instruction uses immediate values that is part of a vector
and their positions to support additions. The reason to partition
the vector is that the vector size can be varying according to the
recommendation model but SPACE use fixed size instruction
format. These SPACE instructions are sent to the HBM-side
NMP modules for embedding layer operations. We do not
discuss the detailed hardware design of NMP modules, since
the NMP modules of SPACE consist of vector logic units,
which are similar to the previous works [30], [32].
V. EXPERIMENTAL METHODOLOGY
A. Datasets for Personalized Recommendation
We use the various datasets from web service vendors and
show that the localities of embedding operation appear in realword recommendation systems. Table III shows the productscale datasets used in our experiments. To evaluate the utility
of localities according to the embedding table size, the datasets
consist of various sizes of embedding tables with thousands to
millions of items. Also, we categorize the datasets as large (L)
and small (S) embeddings according to the size of embedding
tables.
TABLE III: User/item interaction datasets
Category Dataset Name # of items
Large
Amazon - CDs & Vinyl [41] cds 1,944,316
Amazon - Kindle Store [41] kds 2,409,260
Amazon - TV & Movie [41] tvm 3,826,084
Google Maps [44] map 2,980,097
Small
LastFM [6] lfm 160,113
MovieLens [26] mvl 59,047
Anime [2] ani 11,200
Steam Game [55] stm 5,155
Table IV shows the recommendation system workloads used
in the experiments. We combine four datasets as a single
workload for modeling the multi-embedding recommendation
systems. Using mixed workloads, we can experiment with
varying load conditions on the server.
TABLE IV: Workload configurations
Workload Dataset Configuration Class
CDS cds-cds-cds-cds 4L
KDS kds-kds-kds-kds 4L
TVM tvm-tvm-tvm-tvm 4L
MAP map-map-map-map 4L
LFM lfm-lfm-lfm-lfm 4S
MVL mvl-mvl-mvl-mvl 4S
ANI ani-ani-ani-ani 4S
STM stm-stm-stm-stm 4S
MIX1 cds-kds-tvm-map 4L
MIX2 kds-tvm-map-stm 3L + 1S
MIX3 cds-kds-ani-stm 2L + 2S
MIX4 tvm-map-lfm-mvl 2L + 2S
MIX5 cds-lfm-mvl-ani 1L + 3S
MIX6 lfm-mvl-ani-stm 4S
To farily evaluate our dataset profiling techniques in Section IV, we divide the data set for the inference set and
profiling set in a 1:1 ratio. We conduct a sampling of 10,000
inferences and profillings from the inference and profiling sets.
B. System Configuration
We implemented the SPACE hardware in Gem5 [13] and
DRAMsim3 [36], a cycle-level CPU and memory simulator.

We evaluate the host-side operation, such as fully connected
layers using the cycle-accurate model of Gem5 with DRAMsim3. On the other hand, for the embedding operation, we use
a stand-alone simulation of DRAMsim3 executing recommendation system written by python. We implement all of NMP
architectures on the DRAMsim3 and the embedding kernel
transferred to the NMP architecture.
For heterogeneous memory architecture, 3D-stacked memory and DIMM-based memory are configured as one memory
simulation. Table V shows the system configuration of our
study. We use HBM as 3D-stacked DRAM, and DDR4-3200
as DIMM-based DRAM.
TABLE V: System configuration
Processors
Cores
Frequency
Last level cache
8 OoO cores
3 GHz
L3 cache, 16MB, 16-way
3D-Stacked DRAM
Memory component
Channel
Rank / Bank
Bus frequency
Device width
nCL-nRCD-nRP
HBM 4-hi 1 stack
8 channels, 512 MB per channels
1 ranks / 16 banks per channels
1GHz (DDR 2GHz)
128b
14-14-14 (cycles)
DIMM-Based DRAM
Memory component
Channel
Rank / Bank
Bus frequency
Device width
nCL-nRCD-nRP
DDR4
2 Channel, 16 GB per channel
1 ranks / 8 banks per channel
1.6 GHz (DDR 3.2 GHz)
4b
22-22-22 (cycles)
We model several heterogeneous memory architectures for
performance evaluation using HBM and DIMMs. We adopt
our proposed technique in other heterogeneous memory architectures. These architectures that we use in experiments are
described below.
BEAR [17]: A DRAM cache model with fine-grained data
management. BEAR mitigates the bandwidth overhead of data
management.
CAMEO [18]: A flat addressable model with fine-grained data
management. CAMEO is the bandwidth-efficient model of flat
addressable heterogeneous memory.
PoM [51]: A flat addressable model with coarse-grained data
management. As a fundamental model of flat addressable
heterogeneous memory, several previous works design the
heterogeneous memory system based on PoM [31], [46].
VI. EVALUATION RESULTS
We analyze the performance impact of the proposed SPACE
and its hardware/software overheads. We also compare SPACE
with the other heterogeneous memory systems assuming that
they can also process embedding layers in HBM-side. Furthermore, we compare the performance and energy consumption
of SPACE with the prior DIMM-based NMP approaches.
A. Analysis of SPACE
Performance impact of SPACE: Figure 16 and 17 show
the performance improvements of SPACE over the NMP with
infinite-sized HBM. SPACE A only utilizes the allocation
techniques, while SPACE employs both the allocation and
psum techniques. Figure 16 shows an average performance
gain of 52% for SPACE A and 66% for SPACE when using
single HBM stack. Meanwhile, Figure 17 shows speedup of
20% for SPACE A and 32% on average for SPACE when
HBM is configured with two stacks. If HBM oppupies a
small portion of memory system bandwidth, SPACE achieves
relatively high performance improvement by leveraging the
bandwidth of DIMMs, as shown in Figure 16. In the opposite
case, the performance improvement of SPACE demonstrated in
Figure 17 comes from the bandwidth reduction from exploiting
psum rather than utilizing the bandwidth of DIMMs.
52%
66%
0%
30%
60%
90%
120%
CDV KDS TVM MAP LFM MVL ANI STM MIX1 MIX2 MIX3 MIX4 MIX5 MIX6 Avg.
Performance
Improvement
SPACE_A SPACE
Fig. 16: Performance improvement of SPACE with 1-stack HBM
compared to infinitely large 1-stack HBM.
20%
31%
0%
20%
40%
60%
CDV KDS TVM MAP LFM MVL ANI STM MIX1 MIX2 MIX3 MIX4 MIX5 MIX6 Avg.
Performance
Improvement
SPACE_A SPACE
Fig. 17: Performance improvement of SPACE with 2-stack HBM
compared to infinitely large 2-stack HBM.
Hardware overhead: To evaluate the overhead of an NMP
processor in SPACE, we generate RTL models of the processor
using Synopsys Application-Specific Instruction-set Processor
designer (ASIP) and estimate the area and power consumption
of the RTL models with Synopsys Design Compiler [3]. In
particular, we use 45 nm CMOS technology and 250 MHz
clock frequency to calculate the area and power consumption,
following the experimental methods of RecNMP [30]. Also,
we use CACTI [35] to measure the power consumption of the
SRAM structure used in SPACE with 22 nm technology.
Table VI shows the area/power overhead of the hostside/memory-side design. In host side, extension logic (index
classifier, coalescer, and encoder) and tables (H.Map, D.Map,
Item, and Psum) are appended to memory controllers. Since
the extension logic consists of simple integer adder, multiplier,
and comparator, it takes only 0.03 mm2 in area and 31.7 mW
in power. In addition, the tables have a low overhead of 0.01
mm2 in area and and 35 mW in power. This is because the
tables require only 2 KB SRAM (4 table √ó 64 entry √ó 64 bit)
when supporting the 64 embedding tables. In memory side, we
evaluate the area/power in single memory channel. Compared
to the lightweight NMP modules (0.34 mm2) that perform
element-wise summation of the same scale [30], SPACE has
a similar area overhead (0.33 mm2). Since, SPACE has an
embedding processing unit for each HBM channel, the design
overhead increases linearly by 342.5 mW in power and 0.33
mm2 in area, according to the number of channels growth.

0.0
0.5
1.0
1.5
2.0
CDV KDS TVM MAP LFM MVL ANI STM MIX1 MIX2 MIX3 MIX4 MIX5 MIX6 Avg.
Speedup
BEAR CAMEO PoM SPACE_A SPACE
Fig. 18: Relative performance of SPACE models compared to using HBM as DRAM cache (BEAR) and HBM as additional memory space
with flat address (CAMEO and PoM).
0.0
3.0
6.0
9.0
12.0
15.0
CDV KDS TVM MAP LFM MVL ANI STM MIX1 MIX2 MIX3 MIX4 MIX5 MIX6 Avg.
Speedup
TensorDIMM√ó2 TensorDIMM√ó4 RecNMP√ó2 RecNMP√ó4 HBM_only√ó1 HBM_only√ó2 SPACE√ó1√ó2 SPACE√ó1√ó4 SPACE√ó2√ó2
Fig. 19: Relative performance of NMP models normalized to TensorDIMM√ó2. In DIMM-based and HBM-based NMP models, √óN means
the models are configured with N DIMMs or N HBM stacks, respectively. √óM√óN denotes that SPACE consists of M HBM stacks and N
DIMMs.
0.0
0.5
1.0
1.5
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
TensorDIMM√ó2
RecNMP√ó2
HBM_only√ó1
SPACE√ó1√ó2
CDV KDS TVM MAP LFM MVL ANI STM MIX1 MIX2 MIX3 MIX4 MIX5 MIX6 Avg.
Energy Consumption
(Norm. to TensorDIMM)
HBM DIMM SRAM Processing
Fig. 20: The breakdown of energy consumption in NMP models.
TABLE VI: Hardware design overhead
Location Hardware component Area Dynamic power
Host side Extension logic 0.03 mm2 31.7 mW
2KB SRAM 0.01 mm2 35 mW
Memory side NMP per channel 0.33 mm2 342.5 mW
Software overhead: Since the SPACE framework requires
pre-processing (Section IV-B), we measured its software overhead for 10 million user-item interactions on the Intel Xeon
E5-2690v4-based sever. The runtime takes 0.99 s and 0.02 s
for gather and reduction pre-processing, and their energy consumption is 79.05 J and 2.45 J, respectively. When considering
that recommendation systems are re-trained in a short period
(e.g., daily in Facebook servers [27]), our pre-processing
scheme can maintain valid states. Also, the pre-processing is
required only once for production-scale inferences, we expect
the software overhead of the pre-processing is acceptable.
B. Comparison with Other Heterogeneous Memory
To evaluate our heterogeneous architecture specialized for
embedding tables, we compare SPACE with the prior works
on heterogeneous memory [17], [18], [51]. Since the previous
techniques do not support NMP, we adopt the processing units
of SPACE to the other heterogeneous memory architectures.
We set BEAR [17] as baseline, which is the most bandwidthefficient heterogeneous memory model using HBM as DRAM
cache. Note that the rest of the prior schemes and SPACE
are flat addressable. These models exploit different granularity
in data management. The evaluated heterogeneous memory
architectures are described in Section V.
Figure 18 shows relative performance of the heterogeneous
memory when processing embedding layers, normalized to
BEAR. Except SPACE, the flat addressable memory models
show performance degradation due to the bandwidth overhead
of data management, described in Section III-C. The performance of PoM applying coarse-grained data management
is only 68% performance of BEAR. On the other hand,
SPACE A achieves performance improvement of 23.7% using
the item allocation technique. Additionally, SPACE further
improves SPACE A by 7.8%, achieving speedup of 31.5%
over the baseline by exploiting the reduce locality.
C. Comparison with Other NMP Models
We experiment different NMP models with various memory
configurations to evaluate the impact of scale-out. We set
DIMMs as single-rank DIMMs and use two (√ó2) or four (√ó4)
DIMMs to exploit DIMM-level parallelism in TensorDIMM
and RecNMP 1 . In addition, we use single- (√ó1) and twostack (√ó2) HBM with NMP (HBM only) which has unlimited
1RecNMP has DIMM-level and rank-level parallelism. The performance
improvement in each parallelism is similar. However, adopting DIMM-level
and rank-level parallelism simultaneously makes its memory capacity larger
than the other NMP models. For that reason, we only applied DIMM-level
parallelism in RecNMP to compare it with TensorDIMM using same memory
capacity.

capacity. To confirm the performance scaling of SPACE, we
vary the number of HBM stacks and DIMMs. Figure 19 shows
the relative performance of SPACE and the other NMP models
normalized to TensorDIMM√ó2 when executing embedding
layers. RecNMP achieves speed up of 10% and 18% over
TensorDIMM when using two and four DIMMs by utilizing
an SRAM cache in each rank. Using single-stack HBM
(HBM only√ó1) can achieve 2√ó speedup compared to TensorDIMM√ó2. Furthermore, as a heterogeneous memory approach,
SPACE√ó1√ó2 improves the performance by 3.2√ó. SPACE can
achieve even higher performance by scaling of HBM stacks
and DIMMs. Specifically, SPACE shows 4.3√ó speedup when
the number of DIMMs is doubled (SPACE√ó1√ó4) and 5.9√ó
speedup with two-stacks HBM (SPACE√ó2√ó2).
Figure 20 shows the relative energy consumption of NMP
models when executing embedding layers normalized to TensorDIMM√ó2. RecNMP√ó2 saves 9% of energy in DIMM by
accelerating the embedding layers but consumes additional 6%
of energy in its SRAM caches. In contrast, HBM only√ó1 reduces the energy consumption by 81%. SPACE√ó1√ó2 consumes
more energy compared to HBM only√ó1 since it employs two
DIMMs. Still, SPACE achieves a high energy reduction of
57% on average by reducing the execution time.
VII. RELATED WORK
Since sparse embedding operations pose unique memory
challenges [25], [27], recent work aims to optimize the memory footprint and bandwidth of personalized recommendation
systems in the data center [21], [22], [29], [30], [32], [49].
Characterization of embedding locality: Although many
previous studies of user-item interactions have been explored [14], [54], [56], analysis of embedding access patterns
based on the interactions has received relatively less attention.
Our locality characterization of embedding operations comes
from the analysis using the real-world interaction dataset when
compared to prior works [25], [29], [30], [32].
Solution of embedding operation: To solve the memory
capacity problem, in the case of BANDANA [21], they implement a DRAM cache solution for non-volatile memory
(NVM). And the mixed dimension embedding [22], a software
compression technique, dramatically compresses the embedding table 16x without loss of accuracy.
Embedding processing in memory: Prior works have
proposed DIMM-based near memory processing architectures [11], [30], [32] to accelerate embedding layers. They
optimize the processing method based on the operational
characteristics of embedding layers. In particular, Fafnir [11]
proposed reduction techniques to minimize data access between NMPs. However, it still has the limitation from DIMMbased NMPs which cannot achieve both the bandwidth and
capacity scalability.
VIII. CONCLUSION
We propose SPACE, a high-bandwidth near memory
processing solution for personalized recommendation. Our
SPACE architecture consists of a 3D-stacked DRAM as a
compute-capable cache for DIMMs. We perform a characterization of embedding locality and reveal its common locality for optimizing SPACE. Using a heterogeneous memory
system, SPACE leverages the high throughput of 3D-stacked
DRAM and large data storage of DIMMs. SPACE utilizes a
3D-stacked DRAM as a compute-capable cache with locality
optimization, achieving an average 2.9-3.4√ó performance improvement and 56% energy saving compared to the previous
DIMM-based NMPs by adding 3D-stacked DRAM with the
1/8 size of DIMM in the memory system.