Abstract‚ÄîWith offloading of data to the cloud, ensuring privacy and securing data has become more important. However,
encrypting data alone is insufficient as the memory address
itself can leak sensitive information. In this work, we exploit
packetized memory interface to provide secure memory access
and support oblivious computation in a system with multiple
memory modules interconnected with a multi-hop, memorycentric network. While the memory address can be encrypted
with a packetized memory interface, simply encrypting the
address does not provide full oblivious computation since
coarse-grain memory access patterns can be leaked. In this
work, we first propose a scalable encryption microarchitecture
with source-based routing where the packet is only encrypted
once at source and latency overhead in intermediate routers is
minimized. We then define secure routing in memory-centric
networks to enable oblivious computation such that memory
access patterns across the memory modules are completely
obfuscated. We explore different naive secure routing algorithms to ensure oblivious computation but they come with high
performance overhead. To minimize performance overhead,
we propose ghost packets that replace dummy packets with
existing network traffic. We also propose Ghost routing that
batches multiple ghost packets together to minimize bandwidth
loss from naive secure routing while exploiting random routing.
Keywords-Oblivious Computation; Routing; Memory-centric
Network
I. INTRODUCTION
As the computing paradigm shifts to cloud computing, it
allows significant convenience and cost-efficiency; however,
it comes at the cost of giving up full physical control of the
data. Physical attacks can leak valuable information, even
if the software and the OS are secure - thus, providing a
secure hardware system has become an important challenge.
There have been prior work on secure processors [1]‚Äì[4]
that encrypt data that leaves the processors. However, secure
processors do not protect against the vulnerability exposed
by the address trace sent to the memory [5]. In modern
DRAM, memory address is transmitted in plaintext and
anyone with physical access can access the memory address
trace. By observing the memory addresses, the attacker can
obtain sensitive information from user-level programs [6], and
database accesses [5]. To overcome this limitation, Oblivious
RAM (ORAM) [7] approaches have been proposed where
the memory access pattern is provably hidden by generating
accesses to random memory locations. Prior work [8]‚Äì[10]
have significantly reduced the complexity of implementing
ORAM but they can still result in 10√ó additional bandwidth.
One fundamental limitation of prior work is the modern
DRAM (i.e., DDR) interface where the memory address
cannot be encrypted. However, alternative memory interfaces
based on ‚Äúpackets‚Äù instead of memory commands have
been proposed such as hybrid memory cubes (HMC) [11],
GenZ [12] and PCIe-based CXL [13] that provides high-speed
interconnect between CPU-to-Memory. These interfaces
enable processor-memory communication through high-level
messages (e.g., read request) instead of DRAM commands.
In this work, we exploit the packetized interface to provide
oblivious computation. If an observable trace is independent
of input so that an attacker that monitors the memory accesses
cannot observe any behavior of the particular program, the
system is defined as oblivious system.
Prior work [14], [15] also leveraged packetized interface
for secure memory access but were limited to systems with
a single memory module (or multiple memory modules but
all of the modules were directly connected to the host CPU).
These approaches do not necessarily scale to multi-hop
networks. In addition, the packet‚Äôs final destination leaks
coarse-grain access pattern as the final hop is always headed
to the destination. If we assume page-based interleaving
for different modules, this reveals page access patterns.
Recent work have shown how page fault (or page access)
sequences [16]‚Äì[18] and reuse distance (i.e., the number of
page faults between a page is reused) [19] can leak secret
information. These attacks can be extended to a multi-node
network even without creating an intentional page fault if
the pages are allocated across the different nodes.
In this work, we address the challenges of oblivious computation in a system consisting of a multi-hop network with
multiple memory modules that are interconnected through
a memory-centric network [20], [21] where each memory
module also serves as a ‚Äúrouter‚Äù node. By exploiting the
packetized interface, we encrypt the entire packet, including
both the data and address. Even if both the data and address
are encrypted with the packetized interface, it does not
guarantee oblivious computation [7] since coarse-grain access
(or traffic) pattern is revealed with multiple memory modules
‚Äì i.e., which memory modules are being frequently accessed.
To enable oblivious computation while minimizing performance impact, we propose a scalable microarchitecture that
supports end-to-end encryption while minimizing per-hop
latency. In a multiple-hop network, encrypting/decrypting

"$.*&&&UI"OOVBM*OUFSOBUJPOBM4ZNQPTJVNPO$PNQVUFS"SDIJUFDUVSF	*4$"

¬•*&&&
%0**4$"
2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) | 978-1-6654-3333-4/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ISCA52012.2021.00077

	
Figure 1: High-level block diagram of multiple memory modules that are interconnected. Logic near the memory provides
computation capability to support hardware security.
packets at each hop increases overall packet latency; however,
the encrypted address and data are not needed until the destination. As a result, only the routing information is transmitted
in plaintext to avoid the need for encryption/decryption in
intermediate nodes while the rest of the packet, including the
rest of the packet header is transmitted in ciphertext. We also
propose a router microarchitecture pipeline where the packets
are eagerly forwarded in the intermediate routers to the next
node while decryption is done in parallel to minimize per-hop
latency overhead. While we leverage the commonly used
AES counter mode [22] with precomputation [23], out-oforder packet arrival complicates decryption at the destination.
We show how random sequence of buffer index can be
transmitted with the packet to overcome this limitation.
To completely obfuscate the access pattern, we propose
secure routing in memory networks to enable oblivious
computation. Naive implementation of secure routing has
high performance overhead since a memory access requires
packets to be sent to all N nodes in the system ‚Äì resulting
in additional bandwidth consumed for the network channels
as well as the network injection channels. In this work, we
propose Ghost routing where we exploit existing network
traffic to minimize performance overhead while providing
secure routing. Ghost routing consists of two components ‚Äì
injecting ghost packets into the network as needed to ensure
oblivious traffic pattern while batching packets to minimize
the impact on network injection bandwidth constraints.
In particular, the contributions of this work include the
following.
‚Ä¢ We propose oblivious computation in a multi-module,
memory-centric network by exploiting packetized memory interface and multi-hop network routing.
‚Ä¢ We propose distributed, scalable encryption microarchitecture that minimizes per-hop latency while ensuring
proper decryption with out-of-order packet arrival at
destination.
‚Ä¢ We propose Ghost routing that provides secure routing to achieve oblivious computation. Ghost routing
exploits existing traffic and ghost packets to minimize
performance overhead while still obfuscating the traffic
pattern.
Addr
Data
Cmd
dest R/W ‚Ä¶ Payload (addr/data)
packet header
(a) (b)
Route info dest R/W ‚Ä¶ Payload (addr/data)
packet header
ciphertext
plaintext
(c)
Figure 2: High-level memory interface for (a) DDR with
address, data, and DRAM commands where only data is
encrypted, (b) packetized DRAM interface with entire packet
encrypted, and (c) this work where routing information is
plaintext and the remaining packet is encrypted.
II. BACKGROUND & THREAT MODEL
A. Memory Network & Packetized Interface
Advances in technology and 3D integration have enabled
die-stacking connected using Through-Silicon Vias (TSVs).
The 3D stacked DRAM die is stacked on top of a logic die
(Figure 1) that contains a vault 1 memory controller, a highspeed signaling I/O logic, and a switch that interconnects
the I/Os to the different memory controllers. The memory
module can be a building block of a network to interconnect
with other memory modules and CPUs to create a memory
network [20]. In this work, we assume a system based on
memory modules with a packetized interface.
Prior work [5], [10], [24], [25] for ORAM assumed a
conventional DRAM interface (Figure 2(a)) where the data
is encrypted but the memory address and DRAM commands
(i.e., precharge, activate, etc.) are transmitted as plaintext.
In comparison, the packetized interface enables the communication between the CPU and memory modules through
‚Äúpackets‚Äù ‚Äì i.e., messages or packets are communicated to
memory modules. With a packetized interface, the entire
packet, including both the address and the data, can be
encrypted (Figure 2(b)). The logic die in the memory modules
can be used for decryption (and encryption). In this work,
we propose to not encrypt the entire packet but expose the
routing information as plaintext (Figure 2(c)), to minimize
the encryption/decryption overhead at each router.
B. Threat Model
In this work, we assume the attacker has physical access to
the system and carry out off-chip side-channel attack [26].
Similar to prior work on tamper-resistant processors [1]‚Äì
[3] we assume that the processors are secure as well as
the logic layer of the memory modules. However, the offchip components are insecure ‚Äì in particular, the channels
between the modules. The attacker can learn the physical
topology of the system and can observe the traffic between
1A vault refers to the vertically aligned memory partition.
   
Obliviousness properties Single Module [14], [15] Multiple Modules (This work)
Fine-grain Coarse-grain
1 Which data is being accessed? encrypt address
2 How old when it was last accessed? encrypt with OTP OTP Ghost packets
3 Whether the same data is being accessed (linkability)? encrypt with OTP OTP Ghost packets
4 Access pattern encrypt with OTP OTP Ghost routing, Ghost packets
5 Access is a read or a write? make packet size identical batching
Table I: Different properties of ORAM security and how this work achieves obliviousness.
	
	








	



 
 
 


 




	
 

	



	

 	


	
	



Figure 3: High-level block diagram of memory attestation.
Attestation logic is in separate logic near memory modules.
the different nodes in the system. We assume an attacker
can probe channels between the modules and inject or replay
in-flight packets, but cannot modify legitimate packets.
For the 3D stacked memory, we assume the memory
layers are untrusted, thus they are vulnerable to other types
of attacks, including row-hammer attacks [27], cold boot
attacks [28], beaming neutrons on memory cells [29]. To
protect against these attacks, additional data encryption [30]
and integrity verification [31] can be added locally within the
secure logic layer. An attacker is able to check thermal and
power activity of memory modules to detect which modules
or banks are accessed. However, these attacks are addressed
in prior work [32], [33], and are outside of this work‚Äôs
scope. This work also does not address timing channel-based
attacks but we expect previously proposed approaches [34],
[35] can be extended to this work. We also assume the CPU
module and the logic layer in the memory modules are part
of the trusted computing base and remote attestation can
be done, similar to how it is done for Intel SGX [36] and
ARM PSA [37] to determine if the modules are authentic
or not. The only difference is that in addition to the CPU
vendors, the memory module vendors need to support remote
attestation.
III. MEMORY ADDRESS ENCRYPTION
A. Oblivious Computation Overview
We use a similar definition of ORAM security as in prior
work [7]‚Äì[9] but exploit the packetized memory interface
to provide oblivious computation [14], [15]. However, prior
work were limited to a ‚Äúsingle-node‚Äù system ‚Äì i.e., memory
modules were directly connected to the CPUs and packets did
not need to traverse multiple hops. In comparison, we extend
oblivious computation to a multi-hop network where routing
is necessary. Table I summarizes different ORAM properties
and compares this work with prior work [14], [15]. With a
single module, encrypting address with One-time Pad (OTP)
is sufficient to hide which data is being accessed, including
linkability. When scaling to a multi-hop system, both finegrain (i.e., which data is being accessed) and coarse-grain
(i.e., which memory module is being accessed) information
need to be obfuscated. Address encryption obfuscates finegrain accesses but coarse-grain access patterns can be leaked
based on the routing. We also leverage OTP precomputation
and describe how to scale OTP without increasing perhop router latency with source-based routing. We introduce
Ghost routing with ghost packets to hide linkability and
access patterns. Read/write accesses are hidden by making all
packets the same size [14], [15] but we minimize performance
impact across the network through batching.
B. Initial Attestation and Key Exchange
To enable the secure communication through the memory
network, the memory modules, in addition to the CPUs, need
to be trusted. To build trust between the memory modules for
secure communication and the CPUs, we propose attestation
of the memory modules, similar to CPU attestation [38],
to ensure that the memory modules are vendor authorized
modules and up-to-date. Attestation requests are generated by
the application, and sent by CPU as normal memory request
packets through the memory network (Figure 3-1 ).The only
change necessary is to support a new type of command for
the attestation requests between the CPU and the memory
modules. Once the memory module receives the attestation
request, local attestation on the secure logic firmware 2 is
done and signed with its hardware-embedded attestation key
(Figure 3-2 ). Results of local attestation are sent back to
the CPU (Figure 3-3 ) for verifying the results. Since it
is infeasible to have all of the memory vendor‚Äôs certificate
built into the CPU hardware, the attestation software needs
to verify the results with the memory vendors, through the
CPU vendor (Figure 3-4 ,5 ). Once the attestation results
are returned (Figure 3-6 ,7 ), trust between the CPU and
the memory modules is established.
Each module has a public-private key pair generated at
boot time that is stored in tamper-resistant on-chip storage.
During the initial attestation process, each memory module
also sends a certificate containing its public key to the CPU
2The logic in the logic layer is mostly ‚Äúforwarding‚Äù hardwired logic;
thus, to drop packets or store packets would require a hardware trojan to
manipulate packets. Some routers have programmable routing tables but
in this work, we leverage source routing where the output ports for each
hop are pre-determined when injected into the network. Thus, if the boot
sequence (network discovery, etc.) is secure, then the chances for these
other attacks are limited.
                





 	  
  	  





	 	

 
 







	 	 











Figure 4: Memory access latency pipeline for (a) baseline, (b)
pre-computed OTP with per-hop encryption, and (c) source
routing and eager forwarding.
and the CPU collects all of the modules‚Äô public key and
broadcasts its public keys to memory modules. The other
modules‚Äô public keys are used to encrypt the secret key
that is used in OTP key generation. The secret key, shared
between each pair of modules, is generated by the module
with a smaller identification number (ID) and sent to the
other module after encrypting with the other module‚Äôs public
key. This ensures that all processor-memory pairs share a
secret key without the possibility of a man-in-the-middle
attack.
C. Encryption with AES Counter Mode
In this work, we leverage counter mode encryption [22],
[39] that uses a block cipher (e.g., AES). For encryption,
a counter value and the secret key are used as inputs to
AES with the resulting bitstream representing the one-time
pad (OTP). With an incrementing counter, OTP can be precomputed [23] to hide the latency. The pre-computed OTP is
XOR‚Äôed with a plaintext to generate a ciphertext. To avoid
replay attack, we assume a nonce is exchanged between
each pair of nodes and used in the encryption. A separate
random number is generated for each source-destination pair
to ensure the same OTP is not re-used. Since we use a 64-bit
nonce in this work, for a nonce used between nodes i and
j, both i and j generate a 32-bit random number and are
exchanged in plaintext. The two numbers are concatenated to
generate the 64-bit nonce (RAND(i, j)) (Figure 5(a)). The
nonce is regenerated when the system boots up or whenever
a source-destination counter is saturated. For decryption, the
same OTP is calculated using the same counter value and
XOR‚Äôed with the ciphertext to restore the plaintext.
IV. ROUTER ARCHITECTURE
A. Router Pipeline & Microarchitecture
The latency overhead with a baseline approach of perhop encryption/decryption is shown in Figure 4(a) with a
router pipeline diagram. If encryption with the block ciphers
(e.g., AES) is done at each hop, the latency overhead can
be accumulated. Pre-computed OTP only adds an XOR to
the pipeline (Figure 4(b)) which reduces latency overhead at
each router. However, XOR latency can accumulate across
multiple hops but more importantly, per-hop encryption
causes significant energy overhead since it uses more OTPs
per packet. To avoid the per-hop overhead, we leverage
source routing and expose the routing information as
plaintext while the rest of the packet is encrypted. This
ensures that packet encryption/decryption is only done once
per packet ‚Äì encryption at the source and decryption at the
destination. In addition, the XOR operation can be done in
parallel with the router pipeline, as shown in (Figure 4(c))
through eager forwarding ‚Äì i.e., since the packet header
is in plaintext, the router pipeline can proceed while XOR
operation occurs in parallel.
We exploit pre-computed OTP stored in an OTP buffer
per source-destination pair (Figure 5). If packets between
a source-destination pair are sent and received in order, a
simple FIFO OTP buffer is sufficient. In a system with
a single memory module, this guarantees OTP counter
synchronization at both sides [14], [15]. However, packets
can arrive out-of-order in a multi-hop network if nondeterministic routing is used. A simple method of OTP
synchronization is to send the counter value as plaintext since
encryption of the counter value itself is not necessary [39] but
this introduces additional decryption latency at the destination.
Thus, we propose to transmit the OTP buffer index which
allows the packet to look-up the OTP value from the
destination OTP buffer. Since the buffer index value would
increase by 1 (modulo the OTP buffer size) for each sourcedestination communication, an adversary can observe if a
particular destination is receiving more packets by observing
the index pattern. Thus, we generate a random permutation
sequence of OTP buffer index at the source for each sourcedestination pair and each index is used only once to avoid any
collision. Once the random sequence is used up, the random
sequence is generated again at the source. When a packet
arrives at each router, the packet is decrypted by XOR‚Äôing
the ciphertext with the appropriate OTP. If the decrypted
packet header matches the current node ID, then the packet
is assumed to be destined for this particular memory module
and the packet is sent to the appropriate memory vault for
memory access. If the packet was not destined for the current
router, it will not be properly decrypted since it will not have
a valid OTP ‚Äì thus, the packet will be ‚Äúignored.‚Äù 3
B. Packetized Interface for Read and Write
With a packetized interface, a read or a write can be
easily determined by the packet size ‚Äì i.e., read requests
are often single-flit packets while write request consists of
multiple flits. To hide whether the access is a read or a write,
packets can be padded to make the packets match the size
of the large packet [14], [15]. However, the performance
(bandwidth) impact from padding is more significant in a
3False positives can occur where a packet is decoded with the wrong OTP.
However, a false positive does not impact the correctness of the system
since the packet is still forwarded to the final destination. In addition, the
probability of a false positive is relatively small ‚Äì O( 1
2n ) where n is the
number of bits in the OTP ‚Äì smaller than the bit error rate of the channels.
               

 	

 

 
 	
	
" 	
 	
!






 
 	 
! 	 
" 	

 	

!
#



	 
 
	



 

!

 





 
$ 	 
% 	 
"""
 	
 
$ 	 
% 	 
"""
 	
 
$ 	 
% 	 
"""
 	
	
 
$ 	 
% 	 
"""
 	
#
 
$ 	 
% 	 
"""
 	
 
$ 	 
% 	 
"""
 	

(a) (b)
Figure 5: High-level block diagram of (a) OTP buffer and (b) the proposed encryption/decryption architecture.
multi-hop network since each packet traverses multiple hops.
To minimize performance and bandwidth loss, we propose
batching multiple requests together to reduce the padding
overhead. Batching can be done on a per-destination basis
and if there are no requests to batch, packets can be padded.
However, because of the randomized routing that we exploit
(Section V), the destination of the short packets do not need
to be the same: as long as the packet destination shares one
of the intermediate nodes, batching can be done to minimize
bandwidth overhead.
V. SECURE ROUTING
In this section, we explore secure routing to achieve
oblivious computation; however, since routing algorithms [41]
impact network performance, we present how Ghost routing
and ghost packets can minimize performance degradation. In
this work, we define injection bandwidth as the channel
bandwidth between endpoint nodes (i.e., CPU) into the
network and the network bandwidth as the network channels
bandwidth between the router nodes. While the need for more
network bandwidth is a common problem in any network,
prior work [20] has shown how the injection bandwidth
of memory-centric networks can be a greater bottleneck
with the larger number of memory modules (compared to
the endpoint nodes). In particular, we explore how Ghost
routing and ghost packets can help reduce the performance
degradation by reducing the injection bandwidth overhead.
A. Secure Routing Algorithms
Prior ORAM-based solutions [8], [40] obtain obliviousness
through random shuffling. For each access, a page or block is
remapped and written back to a random location during the
write back phase. However, remapping-based approaches
require additional storage at the processor side to keep the
remapping information. In addition, to hide an access to
a newly assigned location during the write back phase,
dummy accesses to different locations are involved. This
increases both injection and network bandwidth and leads to
performance degradation. In this section, we first describe
naive secure routing algorithms that ensure obliviousness
without remapping or relocation and do not require additional
storage overhead for storing remapping information (Table II);
however, they result in high performance degradation.
Minimal Routing: A na¬®ƒ±ve implementation of coarse-grained
ORAM can be leveraged to define minimal routing with N ‚àí1
dummy packets injected for every real packet in a N-node
network. With minimal routing, each dummy request should
generate a dummy reply because the module that replies
to the request may reveal which module received the real
packet. While this approach completely hides memory access
patterns without sacrificing the packet‚Äôs zero-load latency, the
injection channel bandwidth becomes the bottleneck since
O(M ¬∑ N) dummy packets are injected for M real packets.
The network throughput also suffers because of dummy
traffic: with average hop count havg, network bandwidth
overhead is O(M ¬∑ N ¬∑ havg).
Sequential Routing: In a network, the destination of a
single packet can be effectively hidden without injecting
N ‚àí 1 packets, but with a single packet routed through all
memory nodes. An example of sequential is described in
Figure 6. For a packet, the actual destination might be the last
node(R12 in Figure 6(a)) or any intermediate routers. Even
if the packet‚Äôs destination was an intermediate router, the
packet is still forwarded after visiting the destination node
until it reaches the end of the network. The coarse-grain
memory access pattern is completely hidden but there is
performance impact both in terms of latency and bandwidth.
In particular, the latency of memory access is significantly
increased - in the worst case, increased by N√ó where N is
the network size. The network throughput also suffers since
each packet consumes more network bandwidth. However,
because dummy packets are not injected, injection bandwidth
does not sacrifice and dummy replies are not required.
Broadcast Routing: The performance of sequential routing
can be improved by broadcasting packets with a tree-based
routing (Figure 6(b)). At intermediate routers, packets are
copied and broadcast down multiple paths as necessary. This
improves the packet latency but does not reduce the network
bandwidth consumed for each memory access. In addition, in
broadcast, each broadcast packet should generate a dummy
                  
Baseline Network
ORAM [40] Path ORAM [8] Minimal
Routing
Sequential,
Broadcast
Routing
Ghost Routing
Average Cost Worst Cost
Security No guarantees Oblivious Oblivious Oblivious Oblivious Oblivious Oblivious
Injection
Bandwidth O(M) O(M) O(M ¬∑ log(N ¬∑ C)) O(M ¬∑ N) O(M) O(M) O(M ¬∑ N)
Network
Bandwidth O(M ¬∑ havg) O(M ¬∑ havg) O(M ¬∑ log(N ¬∑ C) ¬∑ havg) O(M ¬∑ havg) O(M ¬∑ N) O(M ¬∑ havg) O(M ¬∑ N ¬∑ havg)
Storage
Overhead N/A O(N ¬∑ ClogN) O(N ¬∑ Clog(N ¬∑ C)) N/A N/A N/A N/A
Table II: Cost comparison of alternative oblivious computation in a memory network. M : The number of real packets,
N : The number of modules in the network, C : Capacity of a memory node in number of words.
   	 

   
   
	 
  
   	 

   
   
	 
  
(a) (b)
Figure 6: Routing examples of (a) sequential and (b) broadcast
algorithms.
Algorithm 1 Ghost Packet Injection Algorithm.
1: // N : network size, dest : packet destination
2: while ‚àÉ packet to inject do
3: rand[] = random permutation sequence of length N;
4: for i=0; i < rand[].length; i++ do
5: dest = rand[i];
6: if ‚àÉ real packet to dest then
7: inject real packet
8: else
9: inject dummy packet
10: end if
11: end for
12: end while
reply at its final node to hide whether the packet is a real
packet or a copied packet.
These secure routing algorithms enable oblivious computation and completely hide coarse-grain access patterns but
significantly degrade performance. Table II provides a qualitative comparison on the overhead of different approaches
to enable oblivious computation. In the following section,
we describe how to improve minimal routing with existing
network traffic while still providing oblivious computation
through ghost packets and Ghost routing.
B. Ghost Packets
In this work, we propose to exploit existing network traffic
to minimize performance overhead. To hide the coarse-grain
access patterns, traffic pattern across the different memory
modules need to be random such that a specific access
pattern can not be distinguished from another traffic patterns
#'"
$""
$'"
%""
%'"
&""
&'"




$'('
! ]

   % 	  
Figure 7: Statistics of memory channel utilization for
SPEC2006 workloads [42] in NUMA system. Measured with
a dual-socket Intel Xeon Gold 6132 machine with 8GB √ó 6
memory channels.
(Section V-E). This also implies that if traffic pattern itself is
random, no additional traffic is needed to obfuscate the access
pattern but simple random permutation of injection order
is sufficient to ensure oblivious computation. In general,
memory access across multiple channels are often loadbalanced (often using sophisticated hashing algorithms [43])
to maximize memory bandwidth utilization. Figure 7 shows
statistics of memory channel utilization measured on a 6-
channel NUMA system. While the memory utilization differs
based on the workload, the utilization across the memory
channels for a given workload are very similar, as shown
by the tight distribution of the 25th and 75th bandwidth ‚Äì
thus, we exploit this behavior to minimize the amount of
dummy packets. For most workloads, because of hashing used
in memory controllers, memory traffic across the different
channels is well-balanced and additional traffic needed to
obfuscate traffic can be kept minimal.
In this work, we define a ghost packet as a packet
that an adversary cannot distinguish whether it is a real
packet or a dummy packet. Algorithm 1 describes our
proposed ghost packet injection algorithm. We use random
permutation sequence of length N to shuffle the sequence
of an observable trace ‚Äìthus, guaranteeing both properties
2 and 3 (Table I) can be met. The random permutation
sequence also guarantees that traffic is spatially random but
also accessed in random order. Request packets are buffered
in separate queues based on the destination and injection
is done based on random permutation sequence. Based on
the permutation sequence, if the queue has a real packet to
inject, the packet is injected; otherwise, a dummy packet is
injected. Once the permutation sequence has been used up,
another random permutation sequence is generated and the
process is repeated if there are remaining packets to inject.
                                     
 	 	 	 	
	 	 	 	
	 	 	 	
	 	 	 	
	
	

   	 

   
   
	 
  
(a) (b)
Figure 8: (a) Valiant‚Äôs routing and (b) randomized Valiant‚Äôs
routing where destination (R15) is visited before intermediate
node (R6).
Algorithm 1 injects dummy packets, similar to minimal
routing; however, minimal routing always injects a fixed
number of dummy packets. In Algorithm 1, the amount of
injected dummy traffic is not deterministic. Thus, an attacker
cannot distinguish real and dummy packets from observed
traffic. Dummy packets have been proposed before [34] but
to the best of our knowledge, they were only applicable for
a point-to-point communication (e.g., memory channel) and
this is one of the first work to extend it to a network. Within
the router, dummy and real packets are handled identically
as as both packets are simply forwarded to the destination,
at the hardware-level. The only difference is that dummy
packets will contain random bits of data and thus, when
decoded with OTP, the node ID will not match the current
node and the packet will be ignored.
C. Ghost Routing
In this work we define Ghost routing as any routing
algorithm that leverages ghost packets. Ghost routing can
also be classified as either Ghost minimal or Ghost nonminimal routing. Ghost minimal routing is defined as minimal
routing algorithm with ghost packets. Although existing
traffic effectively reduce the number of injected dummy
packets in Ghost minimal routing compared to minimal
routing, each ghost packet still needs to be padded to match
the size of large packet (or write packets) (Section IV-B).
To reduce the bandwidth overhead caused by padded ghost
packets, we propose Ghost non-minimal routing.
Ghost non-minimal routing is inspired by Valiant‚Äôs routing [44] where a packet is first routed to an intermediate
node before being sent to destination for load-balancing. For
example, a packet is first routed from the source (or the
CPU) to an intermediate random node (R6) and then, to
the destination (R15) in Figure 8(a). With Ghost routing,
we extend Valiant‚Äôs routing algorithm such that the ordering
of destination and intermediate node is randomized. Thus,
with Ghost non-minimal routing, packet can be first routed
to R15 (destination) and then to R6 (random intermediate
node), as shown in Figure 8(b). Even if a packet‚Äôs destination
is reached, the packet is still forwarded towards the ‚Äúfinal‚Äù
node as specified by the routing information header.
While this routing improves network load-balancing, it
can also be used to inject a single packet for two destinations
Algorithm 2 Ghost Routing Injection Algorithm.
1: // intm, final: intermediate node and final node
2: // N : number of nodes in the network
3: while ‚àÉ packet to inject do
4: // All possible permutation of 2 nodes
5: rand[] = (0,1), (0, 2), ..., (N-1, N-2).random shuffle();
6: for i=0; i < rand[].length; i++ do
7: intm, f inal = rand[i];
8: if (‚àÉ real packet to intm) and (‚àÉ real packet to
f inal) then
9: batch two real packets (intm, f inal)
10: else if (‚àÉ real packet to intm) or (‚àÉ real packet
to f inal) then
11: batch a real and a dummy packet (intm, f inal)
12: else
13: batch two dummy packets (intm, f inal)
14: end if
15: end for
16: end while
with Ghost routing. Since dummy packets have no content
and are only injected to obfuscate the traffic, they can
be combined with real packets or other dummy packets.
Although we batch multiple ghost packets in a single packet,
overall network traffic needs to be uniform random and the
injection order also needs to be randomized to guarantee
obliviousness (Section V-E). For obliviousness, we modify
Algorithm 1 to support Ghost non-minimal routing. Unlike
Ghost minimal routing which follows random permutation of
set of nodes and is N in length, the permutation sequence in
Ghost non-minimal routing consist of 2-tuples of nodes to
specify destination and intermediate nodes. Thus, the length
of rand[] is N P2 .
Algorithm 2 describes the modified random injection
algorithm for Ghost non-minimal routing. Unlike Valiant‚Äôs
routing which selects an intermediate node randomly, an
intermediate node in Ghost non-minimal routing is set by line
5 of Algorithm 2 to keep security guarantee. In Ghost nonminimal routing, a real packet can be injected if there is any
real packet to either f inal or intm nodes ‚Äì thus increasing
the chance of injecting a real packet compared to minimal
routing. Note that for a given request, there is no difference
between a packet sent as intm or f inal in terms of oblivious
property. If there are packets for both f inal and intm, two
real packets can be batched as a single packet and injected
at the same time. If there is no packet for both f inal and
intm, a dummy packet that routes through intm and f inal
is injected. Batching enabled with Ghost non-minimal routing
results in dummy packets being reduced by approximately
a factor of 2√ó. The permutation sequence of Ghost nonminimal routing is longer than the sequence for minimal
routing (Algorithm 1) and might appear to generate more
                   
dummy packets. However, there is negligible performance
impact from the length of the sequence since real packets
are always leveraged and to send the same sequence of
real packets, the permutation sequence of Algorithm 1 and
Algorithm 2 are effectively similar.
D. Return (Reply) Traffic with Ghost Routing
While Ghost routing obfuscates forward traffic or the
traffic from the CPU to the memory modules, there is also a
reverse or reply traffic from the memory modules back to
the CPU that can reveal information. Return traffic is not a
concern in ORAM based systems [14], [40] and in Ghost
minimal routing since all dummy requests generate dummy
replies. However, Ghost non-minimal routing and a multinode network differs since reply traffic will originate from
a particular memory module and thus, reveal which memory
module was accessed. To hide which memory module is
accessed, the reply packet follows the same path back to the
CPUs.
Regardless of whether the destination is the intermediate
node or the final node, the reply packet is still generated
at the final node.‚Äì e.g., in Figure 8(b), R15, generates the
reply data for the request. As the reply packet routes through
the same path back to the CPU, the packet is encrypted
again or replaced in the intermediate node. Since the final
node of the request was R6, R6 generates a dummy packet
that is routed through R15 and back to the CPU. When the
reply packet routes through the intermediate node (R15), the
packet would ‚Äúpick-up‚Äù the proper reply data and forward it
back to the CPU. When the data is not ready for a ‚Äúpick-up‚Äù
(e.g., congestion in the memory module), the reply packet is
still sent back to the source CPU and a separate request is
injected into the network to obtain the data. When the final
node (e.g., R6) injects the reply packet for a read request,
the packet injected into the network is a large packet with
the data payload. However, the payload for data will simply
contain random data as the proper data in the packet will be
replaced when packet routes through the memory module
providing the data (e.g., R15). If the final node was the
destination node, the reply packet will contains the real data
when it routes through the intermediate node. To hide whether
the data is replaced or not, the reply traffic is encrypted at
both the destination and the intermediate nodes ‚Äì for this
case, a reply packet is encrypted twice and then, needs to
be decrypted twice at the source to recover the data.
E. Security Discussion
We define the system as oblivious if for a given sequence
of memory accesses (e.g., p), sequence of access resulting
from the routing algorithm ROUT E(p) is independent of p.
Thus, for two arbitrary sequence of memory accesses(e.g., p
and p
), if |ROUT E(p)| == |ROUT E(p
)|, two are indistinguishable where |ROUT E(p)| is the length of memory
access sequence. We define an observable trace ROUT E(p)
as follows.
ROUTE(p):= {{intm1, f inal1}, {intm2, f inal2}, ...,
{intmt, f inalt}}
intmi := intermediate node of i ‚àí th packet
finali := f inal node of i ‚àí th packet
We provide a proof sketch based on two observations to
prove the obliviousness of the system.
Observation 1. Resulting network traffic from Ghost Routing
is always uniformly random, thus independent of input p.
Observation 2. Ghost routing injects packets in random
order, thus the resulting sequence order is independent of
input p.
Theorem 1. For a given observable trace ROUT E(p), if
ROUT E(p) is independent of p, the routing algorithm
ROUT E() is oblivious.
Proof: According to Algorithm 1, a real packet is
injected if its destination matches rand[i] at line 5. Although
there is no real packet for the destination, a dummy packet
is injected to make deterministic uniform random traffic at
line 9. In Ghost non-minimal routing(Algorithm 2), if a real
packet to both intm and dest does not exist, a dummy packet
is injected. If traffic was biased and no additional traffic
was injected, the only secure routing would be sequential
routing (Section V-A). For Ghost routing, additional packets
are injected to the network (Algorithm 1 line 9 for Ghost
minimal and Algorithm 2 line 13 for Ghost non-minimal) to
create an oblivious traffic pattern. Thus, in the worst-case, if
only one memory module is accessed, additional or dummy
packets are needed for all other memory modules to create
a random distribution. In Ghost non-minimal routing, an
adversary counts an observable packet {intmi, f inali} as
two network traffic: (src, intmi) and (src, f inali). In both
cases, resulting traffic distribution from proposed algorithms
follow the original distribution of rand[] which is always
uniform random. Thus, Observation 1 applies.
Algorithm 1 (line 3) generates a new random permutation
sequence as it consumes all. The order of all resulting
sequences follows the order of random permutation sequence,
which is random. In Algorithm 2 for Ghost non-minimal
routing, line 5 randomly shuffles the tuples. Thus Observation
2 applies. With these observations from Ghost routing, the
resulting traffic is uniformly random and resulting sequence
of packets is also randomized ‚Äì the observable trace is
independent of the original sequence p, thus the system
is oblivious.
F. Cost Analysis of Ghost Routing
In the worst-case traffic when only a single memory
module is accessed (i.e., hotspot traffic), Ghost routing injects
O(N) dummy packets for each real packet. This causes
O(M ¬∑ N) injection bandwidth and O(M ¬∑ N ¬∑havg) network
bandwidth overhead for M packets. However, if memory
traffic is well distributed over different memory channels (e.g.,
  
Parameter Value
Core
32 Out-of-Order cores
@ 4 GHz
Issue width: 4,
ROB size: 64
L1 I/D cache 64 KB, 4-way,
1-cycle latency
L2 cache Private 256 KB, 16-way,
10-cycle latency
Cache coherence Directory-based MOESI
Cache line size 128 B
Directory Full directory, 1-cycle,
4 per processor
Table III: Processor configurations.
Parameter Value
Organization 8 layers √ó
16 vaults
Size 4 GB
per module
scheduler FR-FCFS
DRAM timing
tCK=1.25ns,
tRP=11,
tCCD=4,
tRCD=11,
tCL=11,
tWR=12,
tRAS=22
Table IV: Memory configurations.
Workload Problem Size L2 MPKI
Barnes 16K particles 0.39
FFT 64K data points 2.26
FMM 16K particles 0.63
LU 512 √ó 512 matrix 0.42
Ocean 258 √ó 258 grids 7.61
Radix 32M integers 12.88
Raytrace teapot 0.81
Water-Sp 256 molecules 0.17
x264 640x360 32 frames 4.35
streamcluster 8192 points/block 3.46
canneal 100,000 elements 3.63
blackscholes 16K options 0.22
Table V: Evaluated Workloads.
Figure 9: Probability distribution of the number of injected
dummy packets.
Figure 7), sending N ‚àí 1 dummy packets for a real packet
rarely occurs. For random input traffic, we can compute
the average-case cost of Ghost routing. Figure 9 shows the
probability distribution (P(X = i)) of the number of injected
dummy packets(i) before a real packet injected in the Ghost
routing, measured in synthetic random traffic. From this
distribution, the expected number of dummy packets for
each real packet can be calculated by the following equation.
E[num of dummy packets] =
N
‚àí1
i=1
P(X = i) ¬∑ i
We observe that the expected value of number of dummy
packets is approximately 1, regardless of the value of N,
at high traffic injection rate. Thus, the cost of injecting a
real packet in Ghost routing is empirically O(1). For M
packets, the total injection bandwidth is O(M) and network
bandwidth is O(M ¬∑ havg). At lower traffic injection rate,
the cost can be higher but as we shown in Section VII, the
impact on performance from Ghost routing is much lower at
such lower traffic rate.
VI. EVALUATION
A. Methodology
In this work, we use a detailed cycle-accurate simulator
with the core model from McSimA+ [45], the cache model
from GEMS [46], and Booksim [47] network simulator. The
simulator was modified such that the memory scheduling
is done within the logic layer of the memory module. The
encryption and decryption logic, including the OTP precomputation was modeled in the memory module nodes and the
CPUs. The configuration for the processors and memories are
Figure 10: Performance impact of encryption uarch.
shown in Table III. We used RW:CLH:VL:CB:CLL:LY:BY
memory address mapping scheme (RW:Row, VL:Vault,
CB:Cube, LY:Layer, CLH:Column High, CLL:Column Low,
BY:Byte Offset). In our evaluation, we focused on the 4CPU16MEM system with the dragonfly topology [20] which
was identified as an efficient topology for memory-centric
networks. We used SPLASH-2 benchmark suite and Parsec3.0 suite [48], [49] with the input size given in Table V.
We evaluated system energy consumption using McPAT [50]
for the CPU, CACTI-3DD [51] for the memory‚Äôs DRAM,
and the interconnect energy model was based on [20]. We
assume 1.25 GHz router frequency and 3.2ns SerDes latency
(1.6ns for serialization and 1.6ns for deserialization).
B. Results
Encryption: We first compare the impact of our proposed
encryption microarchitecture ‚Äì i.e., the impact of finegrain oblivious computation (Table I) and impact of the
different router pipeline implementation. Figure 4 compares
the router pipeline for per-hop encryption/decryption (Figure 4(a)) (PER-HOP AES), pre-computed OTP (Figure 4(b))
(PER-HOP OTP), source routing (Figure 4(c)) (SOURCE
OTP) to an non-secure BASELINE without any encryption or
decryption (Figure 10). We assume that the XOR introduces
another pipeline stage in the router. PER-HOP AES results in
significant performance degradation ‚Äì up to 2.29√ó, compared
to non-secure router pipeline microarchitecture. By using precomputed OTP, there is still some degradation ‚Äì on average
9.3% degradation. However, by leveraging source routing
with end-to-end encryption, the performance overhead from
the additional latency is nearly negligible as the performance
nearly matches the performance of a system without any
support for encryption.
 
%#," # $
)%#  ($!$ $%#&$%#   (153
	 
5+62 4+40 8+42 3+56
/012345678
/
4
0/
04
/
0
1
2
3
!#*'#
%
% )
!#*& %
		
 	
  !$%  !$% ! , 
Figure 11: Comparison of proposed secure routing algorithm across different workloads.
Routing Algorithms: We compare different routing algorithms to provide coarse-grain oblivious computation.
Results from different secure routing algorithms are shown
in Figure 11 and Figure 12. We compare SEQUENTIAL,
BROADCAST, MINIMAL, Ghost minimal and Ghost
non-minimal. All routing algorithms evaluated are secure
routing and results are normalized to the non-secure, minimal
(dimension-order) routing that does not provide oblivious
computation. To hide whether the memory access is a read
or write, packets are padded.
The result shows that MINIMAL which injects N ‚àí 1
dummy packets for a real packet significantly degrades the
overall performance by up to 9.5√ó. Performance degradation
from dummy packets is even greater in memory-intensive
workload (e.g., FFT, Ocean, Radix, x264) and results in
high energy overhead (3.95√ó on average), because of the
large amount of extra memory (i.e. network) traffic. This can
be significantly improved by ghost packets, which leverage
existing traffic. Ghost packet is effective in memory-intensive
workload since there is a higher probability of reusing existing
memory traffic, thus less amount of dummy packets are
injected for these workloads. On average, Ghost minimal
injects 6.9√ó dummy packets while MINIMAL injects 15√ó
dummy packets in a 16-node network (Figure 13).
Ghost non-minimal results in 22.1% performance
overhead, 3.32√ó energy overhead, and 3.39√ó dummy packets
are injected on average. Ghost non-minimal has a
higher chance of injecting a real packet compared to Ghost
minimal, thus added dummy packets are greatly reduced.
In Ghost minimal (Algorithm 1), a real packet is injected
if there is a packet in buffer for f inal node; however, in
Ghost non-minimal (Algorithm 2), a real packet can
be injected if there is a real packet for either intm or f inal
‚Äì thus, it reduces the number of dummy packets.
Compared to Ghost minimal and Ghost
non-minimal, BROADCAST and SEQUENTIAL show
significant performance degradation because of the additional
network bandwidth consumed. Although BROADCAST
is expected to outperform SEQUENTIAL, BROADCAST
shows 41.3% performance overhead and 50.8% energy
overhead while SEQUENTIAL shows 53.1% performance
overhead and 63.2% energy overhead. This is because
BROADCAST optimizes the packet latency, but not the
network traffic. BROADCAST has lower average-hop count
compared to SEQUENTIAL. The difference in packet
latency results in the performance of two routing algorithms.
 % $
/&-/ .&0, 0&.*
)
*
+
,
-
.
/
" '!  ! 
  $"    # 
		
 	
  !" !"'
Figure 12: Normalized energy of proposed secure routing
algorithm across different workloads.  
	
#
%
$#
$%
	
	  "
"  		!   	

Figure 13: Amount of extra packets added, normalized to the
number of regular packets.
The secondary y-axis on Figure 11 also shows average
packet latency for each routing algorithm. On average,
SEQUENTIAL has 315%, BROADCAST has 44.2%, and
Ghost non-minimal has 78.4% overhead compared to
unsecured baseline. For SEQUENTIAL and BROADCAST,
the request packet continues routing to the final node and
the reply packet (with dummy data) which generated at the
final node follows the path back to pick-up the real payload
that leads higher overhead in latency.
The effective secure routing algorithm will likely differs
depending on the topology and the bandwidth. Figure 14
shows average runtime overhead of proposed routing algorithms across different network topologies: mesh (MESH),
flattened butterfly (FBFLY) [52] and dragonfly (DFLY) [53].
For high-radix topologies with non-minimal routing, Ghost
non-minimal routing might provide more benefit since Valiant
routing is critical in these topologies to load-balance all
channels. Moreover, it results in performance degradation
across all of the topologies and GHOST non-minimal
results in the smallest performance degradation because of the
additional packets needed for MINIMAL and BROADCAST.
Network Statistics: Figure 15 shows channel utilization
during the execution of a memory-intensive workload FFT
and a compute-intensive workload Water-sp. The results are
separated into injection bandwidth (or the channels directly
connected to the CPU) and the network bandwidth (or the
router-to-router channels). Channel utilization is measured
as active cycles divided by total execution cycles. The result
shows that a significant amount of bandwidth is used by
dummy packets in MINIMAL. In particular, for the memory-
                        
 # %# $$
!# "$ "     $   $ 



 
!
"


	
	
 
 


	 	
Figure 14: Average performance slowdown for different secure
routing algorithms across different topologies.
+
+&-
+&/
+&0
+&1
,
!%!
 
"$
+
+&,
+&-
+&.
!%!
 
"$
		

'(

	



		

'(

	


	

		

'(

	



		

'(

	


	
 '( '(
	!#! !# #!
Figure 15: (a) Injection channel and (b) network channel
utilization from real and dummy packets.
intensive workload, MINIMAL saturates injection bandwidth
and this explains the significant performance overhead (up to
√ó9.53). As the injected amount of dummy packets reduces,
channel utilization is also reduced. In SEQUENTIAL and
BROADCAST, there are no dummy packets injected at the
CPU thus bandwidth is only used for processing real packets.
However, network bandwidth usage increases compared to
other routing algorithms since each packet traverse more
hops in the network (or utilize more network channels) to
route through all nodes. For BROADCAST, a real packet
is copied using a tree-based broadcast (Figure 6(b)) and
generates dummy packets. These copied packets consume a
significant amount of network bandwidth that causes network
bandwidth to saturate. For workloads that are not memoryintensive (e.g., Water-sp, blackscholes), both injection and
network channels are underutilized. This explains why the
performance impact of a secure routing was relatively small
in those cases (Figure 11).
Synthetic Traffic Pattern Comparisons: To demonstrate
the extreme traffic cases in a network, we compare the
performance of different routing algorithms using synthetic
traffic patterns, including random traffic and hotspot traffic
and measured the performance as the network load is varied.
Hotspot traffic models traffic where only a single memory
module is accessed, while random traffic assumes every node
in the network is randomly accessed.
Figure 16 shows the performance improvement (or
speedup) of the proposed routing algorithms, compared
to MINIMAL (secure) routing algorithm. The synthetic
workloads were compared using a ‚Äúbatch‚Äù-mode [47] where
each CPU injected 100k packets and the memory modules
generated a reply packet. When the traffic pattern is random
and the network load is low (i.e., 0.0 ‚àº 0.4), the performance
difference between routing algorithms is negligible since most
of the network channels are underutilized and thus, extra
packets in the network has minimal impact. As the network
Figure 16: Speedup over MINIMAL routing algorithm with (a)
random traffic and (b) hotspot synthetic traffic patterns.
load in the random traffic increases, network bandwidth
becomes a bottleneck and Ghost routing algorithms provide
significant benefits. In particular, when network traffic is
random and the load is high, Ghost non-minimal is
recommended since it leverages existing traffic and batches
ghost packets to further reduce the performance overhead over
Ghost minimal. Speedup of Ghost non-minimal
increases as the network load increases as well. If network load is high, there are higher chances to inject a
real packet rather than a dummy packet. Unlike Ghost
non-minimal, SEQUENTIAL and BROADCAST does not
benefit from the random traffic characteristics since all nodes
have to be traversed, regardless. Between SEQUENTIAL and
BROADCAST, BROADCAST performs better because of low
latency. In SEQUENTIAL, the average hop count for the
packet is N and traverses more hop count than any other
routing algorithm. Thus, ‚Äúreal‚Äù packets in SEQUENTIAL
consume more network channel bandwidth and results in the
worst routing algorithm for the random traffic.
In comparison, when the traffic pattern is hotspot,
SEQUENTIAL or BROADCAST provides better performance
regardless of the network load. For this type of traffic, Ghost
minimal and Ghost non-minimal cannot leverage
existing (or ‚Äúreal‚Äù) traffic to use, but have to generate dummy
packets to obfuscate the original hotspot traffic. As a result, Ghost minimal and Ghost non-minimal inject
N ‚àí 1 dummy per packet and results in a performance that
is very similar to MINIMAL, resulting in high performance
degradation, compared to SEQUENTIAL or BROADCAST.
However, as discussed earlier Section V-B, hotspot traffic
does not occur frequently since memory bandwidth is often
maximized by accessing all memory modules. It remains to
be seen if an adaptive ghost routing can improve overall
performance if a traffic pattern like hotspot traffic appears
more frequently in the system.
VII. DISCUSSION
Comparison to other approaches: In Table VI, we compare
the performance of ORAM-based solutions with Ghost
non-minimal. On average, Path-ORAM results in over
12.7√ó (up to 22.5√ó) overhead compared to the minimal
routing without any support for obliviousness while Ghost
non-minimal results in 22.1% overhead in performance
                                      
Workload Path-ORAM Network
ORAM Constant GHOST
barnes 1960.9% 130.0% 83% 1.1%
fft 1134.5% 82.5% 115% 41.4%
fmm 368.9% 36.6% 58.7% 30.3%
lu 286.6% 74.0% 17.6% 14.5%
ocean 2027.1% 119.3% 115% 41.1%
radix 2153.4% 164.9% 133% 30.1%
raytrace 1271.1% 84.9% 69.2% 16.4%
water-sp 190.6% 12.8% 12.2% 2.6%
GMEAN 1174.14% 81.5% 75.4% 22.1%
Table VI: Performance overhead of Path-ORAM [8], NetworkORAM [40], Constant rate injection [14] and Ghost (nonminimal) System compared to non-secure baseline system.
(up to 41.4%). As Ghost non-minimal encrypts the
address of each packet, fine-grained access in each memory module is hidden. For fair comparison, we compare
Ghost non-minimal with Network ORAM [40] which
implements coarse-grain ORAM computation model. In the
simulation of Network ORAM, our evaluation shows that it
can result in 81.5% overhead on average. We also compare
against an alternative approach where packets are transmitted
constantly every t cycles (constant(t)) ‚Äì a scaled version
based on InvisMem [14] where each network channel sends
constant traffic. Results show that constant(t) can
significantly degrade performance, by over 2√ó for some
workloads. While the performance degradation was small in
prior work [14], the performance degradation accumulates
across multiple hops in the memory network with multiple
hops. For example, the memory access latency increased
by 2.3√ó for Radix compared with the baseline because of
the additional per-hop latency ‚Äì both the additional perhop encryption/decryption latency as well as the latency of
waiting for the next slot.
Scalability: The overhead of ORAM based on Path-ORAM
is O(log(M)) where M is the memory capacity and is
not directly dependent on the number of memory modules.
In comparison, the Ghost routing can be proportional to
the number of memory modules or O(M/k), where k
is the amount of memory capacity per memory module.
With memory capacity increasing per memory module, both
through advances in technology as well as integrating more
memory chips within a single memory module, k continues
to increase which does not benefit Path-ORAM. For example,
for a 4GB Path-ORAM which required 25 levels, scaling
to 64 GB requires 29 levels. If the average hop count in
the network is havg, Freecursive implementation for a 64GB
would have a network overhead of approximately 29 √ó havg.
In comparison, Ghost routing introduces an overhead of up
to 2 √ó havg assuming k = 4GB.
Per-Hop Encryption: In this work, we proposed end-toend encryption where the packet is encrypted at the source
with OTP and then, decrypted at the destination with
the appropriate OTP. An alternative approach is per-hop
encryption or uses separate OTP-based encryption across each
channel in the network. This approach simplifies the OTP
management since packets will always arrive in order across
a single channel. However, packets still need to be decrypted
and then, re-encrypted before being transmitted to the next
node and increase per-hop latency. Our evaluation shows that
this approach results in approximately 9.3% performance
degradation, compared with the insecure baseline, whereas
the end-to-end encryption that we used has negligible
degradation. More importantly, leveraging only per-hop
encryption does not hide coarse-grain access patterns since
the destination is always the final node. Thus, the secure
routing is still necessary to obfuscate the coarse-grain access
patterns. In such cases, per-hop encryption adds additional
overhead in terms of OTP buffer depth and cost of OTP
generation because of non-minimal routing. On average, if the
average hop count in the network is havg, per-hop encryption
requires havg√ó more OTPs per packet.
Cost Overhead: The main components of added cost from
the proposed architecture are the counters and the OTP
buffer. For the OTP buffer, the depth impacts performance
‚Äì for example, the depth of the OTP buffer is necessary to
hide the OTP key computation time, and if the buffer is
empty, the packet will need to stall until the OTP key is
generated. For the 16-node memory network system that
we evaluated, 10 buffer entries were sufficient to ensure
that the OTP buffers are not emptied and the amount of
additional storage necessary for the OTP buffers at each
node is approximately 5kB (with a 128bit OTP key). In
addition, a counter is needed for each source-destination
communication pair. Since we assume there is no memoryto-memory traffic, each CPU needs m counters while each
memory module needs n counters, assuming there are n
CPUs and m memory modules. In our evaluation, because
of coherence traffic, there is also CPU to CPU traffic ‚Äì thus,
additional n ‚àí 1 counters are needed at each CPU. With
64-bits for each counter, the amount of storage required for
the counters is 152B for each CPU and 32B for each memory
module. Each CPU stores n + m ‚àí 1 public keys and each
memory module stores n public keys and its private key. With
1024-bit keys, CPUs need 2.4kB and memory modules need
512B approximately. Additional storage necessary to store
random permutation is approximately 120bytes per CPU.
Possible Attacks: In our threat model, the information
that is leaked is the coarse-grain memory access pattern.
Prior work [16] introduced an attack based on page-level
memory access patterns where secrets were extracted by
observing page fault sequences ‚Äì one example of coarsegrain memory access patterns. Similar approaches can be
extended to memory network or memory module granularity.
An attacker can observe a sequence of memory accesses to
identify a secret. For instance, an application that reads one
element from a big array where the index is input (secret).
If the array spans across different memory modules, we can

infer the input secret based on the observed memory trace.
Otherwise, an attacker may exploit a known rootkit [54] to
configure memory mapping as intended.
Related Work: Exploiting packetized CPU-memory interface to obfuscate memory access patterns has been recently
proposed (ObfusMem [15], InvisiMem [14]). This work has
similar approaches to these prior work, including encrypting
both the data and the address and OTP-based encryption [39].
However, the biggest difference is that these prior work [14],
[15] focus on a system with a single CPU module and a
single memory module (or multiple memory modules directly
connected to the CPU module) while this work addresses a
system with a network of memory modules.
Path-ORAM [8] and other implementation have been
proposed to reduce the complexity of ORAM [7] and make
it more practical. Phantom [5] implemented a prototype of
a secure processor and oblivious memory system on real
hardware. Recursive ORAM [55] uses additional ORAMs
to store the position map and provides high scalability
but reduces performance. Oblivious Network-RAM [40] is
closely related to this work as the assumptions made in
oblivious NRAM are very similar to this work. Oblivious
NRAM assume an attacker cannot observe the access pattern
within a ‚Äúmemory bank‚Äù (corresponding to the memory
modules in this work) but can observe which memory bank
the CPU is communicating with, and ORAM is extended
to hide the access pattern. However, similar to ORAM and
Path-ORAM, random initialization and random shuffling
is required for obliviousness. A page or block should be
remapped after an access. This requires client(e.g., processor)
storage to keep remapping information. In addition, with use
of remapping in a multiprocessor system, coherence protocol
for position map is required to be shared among processors.
In comparison, our approach does not require remapping
but exploits existing network traffic and bandwidth available.
SecureDIMM [56] proposed a secure DIMM architecture
that replaced the DRAM buffer with secure buffer that can
perform ORAM operations. The secure buffer achieves inmodule obliviousness, but still lacks explanation of coarsegrain access obfuscation. ForkPath [57] proposed batching
ORAM requests by path merging to reduce redundant
memory accesses in a Path-ORAM and Shadow Block [58]
proposed data duplication to facilitate early access of intended
block in an ORAM access. Both work reduced overhead from
redundant ORAM accesses but still requires memory block
remapping that leads same problems discussed above.
VIII. CONCLUSION
In this work, we proposed secure routing to ensure
oblivious computation in a multi-node, memory network
system that exploits packetized memory interface. In particular, we proposed Ghost routing that minimizes the
performance degradation by leveraging existing network
traffic and batching ghost packets. We also describe the
router pipeline and microarchitecture necessary to support
secure routing which includes eager forwarding of packets to
minimize overall network latency. Our evaluation shows that
Ghost routing provides oblivious computation with only 22%
performance overhead, compared to the insecure baseline
system.