Abstract‚ÄîThe revolution of machine learning poses an unprecedented demand for computation resources, urging more
transistors on a single monolithic chip, which is not sustainable
in the Post-Moore era. The multichip integration with small
functional dies, called chiplets, can reduce the manufacturing
cost, improve the fabrication yield, and achieve die-level reuse for
different system scales. DNN workload mapping and hardware
design space exploration on such multichip systems are critical,
but missing in the current stage.
This work provides a hierarchical and analytical framework
to describe the DNN mapping on a multichip accelerator and
analyze the communication overhead. Based on this framework,
we propose an automatic tool called NN-Baton with a pre-design
flow and a post-design flow. The pre-design flow aims to guide the
chiplet granularity exploration with given area and performance
budgets for the target workload. The post-design flow focuses
on the workload orchestration on different computation levels -
package, chiplet, and core - in the hierarchy. Compared to Simba,
NN-Baton generates mapping strategies that save 22.5%‚àº44%
energy under the same computation and memory configurations.
The architecture exploration demonstrates that area is a decisive
factor for the chiplet granularity. For a 2048-MAC system under
a 2 mm2 chiplet area constraint, the 4-chiplet implementation
with 4 cores and 16 lanes of 8-size vector-MAC is always the
top-pick computation allocation across several benchmarks. In
contrast, the optimal memory allocation policy in the hierarchy
typically depends on the neural network models.
Index Terms‚ÄîAccelerator, chiplet, MCM, neural network,
deep learning, scheduling, design space exploration
I. INTRODUCTION
Applications across the cloud and edge platforms are booming with the flourishing of deep neural networks (DNNs), such
as image recognition [20, 31, 57], object detection [16, 52],
image caption [12, 63], and natural language processing [21,
29]. Given that state-of-the-art DNN models feature intensive
computation and storage [69], a significant number of DNN
accelerators have been recently proposed from academia to
industry, including FPGAs [9, 65, 73], ASIC accelerators [26,
30, 45, 48], and SoCs in the mobile device and datacenter
infrastructure [37, 38, 59].
The area of DNN hardware tends to be increasingly
larger [3, 24, 25, 46] to provide high computing throughput
and on-chip memory capability. However, facing the end of
Moore‚Äôs Law [33], transistor cost reduction slows and the
fabrication yield challenge grows. Consequently, designing
such a large monolithic chip leads to pricey fabrication and
prolonged development timelines [34].
In the past decades, overcoming the well-known ‚Äúpower
wall‚Äù by multi-core processors [43] has achieved orders-ofmagnitude efficiency improvements. Moreover, to conquer the
‚Äúmemory wall‚Äù [67], the novel Non-von Neumann architecture
is a promising solution and has been actively studied [54, 70,
75]. Nowadays, a new wall, namely ‚Äúarea wall‚Äù, has been
established, which means that we fail to obtain high integration
via a large chip cost-efficiently due to the decline of fabrication
yield and the increase of cost per transistor in the advanced
process. This challenge motivates us to employ the multichip-module (MCM) package with several small functional
dies, called chiplets, to provide a large-scale system, including
CPUs [4, 58, 64], GPUs [2, 72], FPGAs [18] and DNN
accelerators [23, 55, 78].
On-package integration enjoys lower manufacturing costs,
higher fabrication yield, and better flexibility than the monolithic chip. Nevertheless, this novel platform for DNN accelerators raises several brand-new challenges for the computer
architecture community. Simba [55] is a pioneering multichip
DNN accelerator with up to 36 chiplets on the package,
and its main contribution is the solution to the natural nonuniformity between on-chip and on-package bandwidth and
latency. Beyond that, we clarify the following two challenges.
The first challenge is to map the DNN workload on the
multichip accelerator during the hardware‚Äôs deployment. Recent works concerning DNN mapping mostly focus on a
single chip [15, 32, 71] rather than an on-package system
with DRAM and multiple accelerator chiplets. According
to Simba‚Äôs prototype, a multichip accelerator involves more
complex communication, especially the newly added die-to-die
communication, which consumes higher energy and provides
lower bandwidth than the on-chip interconnection [66]. Therefore, the neural network workload for chiplets needs to be
aggregated as locally as possible. Fragmented data are likely
to bring redundant transfer and communication congestion.
The multichip system also shows hierarchical parallelism with
thousands of processing elements. As a result, the workload
mapping requires to be layered and ruled.
The second challenge is to explore the chiplet granularity
during the development of an accelerator. Given a required

"$.*&&&UI"OOVBM*OUFSOBUJPOBM4ZNQPTJVNPO$PNQVUFS"SDIJUFDUVSF	*4$"

¬•*&&&
%0**4$"
2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) | 978-1-6654-3333-4/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ISCA52012.2021.00083
performance, we expect to select the appropriate chiplet scale
and resource allocation policy: the number of MAC (Multiplyand-Accumulate) units and on-chip memory sizes of each level
in the hierarchy. A design with fewer chiplets leads to a larger
area but simplifies the NoP (Network-on-Package) topology
with less inter-chip communication overhead. On the contrary,
a large-scale system with abundant chiplets in small sizes
(e.g., the 6√ó6 Simba system) results in complex networking
but saves the development cost (e.g., only 6mm2 each Simba
chiplet). An automatic tool for the design space exploration
appropriate for the multichip accelerator is urgently in need.
To address the challenges outlined above, this paper aims
to (1) describe and analyze the DNN mapping on a multichip
accelerator and (2) explore the design space for the optimal
chiplet granularity.
Therefore, we first establish a universal and concise hardware model with three levels: package, chiplet, and core
that are available for the subsequent analysis. Each hierarchy includes the corresponding computation and memory
components. This paper targets the layer-wise mapping on
the general-scale multichip accelerator, so we employ the
directional ring network on package interconnecting 1-to-8
chiplets rather than an intricate network for tens of chiplets.
Next, we propose an output-centric hierarchical description
with spatial and temporal primitives. The spatial primitive is
used to partition a large workload to several parallel modules,
such as chiplets and cores. In contrast, the temporal primitive
is adopted to depict the loop unrolling strategy, aiming to maximize the buffer locality and the data reuse efficiency. Based
on the description, we put forward a quantitative and analytical
methodology named C3P (Critical-Capacity Critical-Position)
to evaluate the hardware and generate the optimal mapping
strategy for each layer.
Finally, we propose an automatic tool named NN-Baton. It
provides a pre-design flow to guide the chiplet granularity and
conduct the multichip accelerator implementation, including
the computation and memory resource allocation on each
parallel level (package, chiplet, and core). It also contains
a post-design flow with the spatial and temporal partition
strategies to map each layer on the accelerator.
In summary, this paper makes the following contributions:
‚Ä¢ A Universal Multichip Accelerator: It consists of three
parallel hierarchies: package, chiplet and core to enable
the efficient workload mapping and design exploration.
‚Ä¢ A Hierarchical and Analytical Framework: The framework contains a chiplet-friendly output-centric description with temporal and spatial primitives and a C3P
methodology to evaluate the communication overhead.
‚Ä¢ An Automatic Tool: Based on the output-centric description and the C3P methodology, we develop NN-Baton,
a promising automatic tool to guide the development of
multichip accelerator and the DNN workload deployment.
‚Ä¢ We demonstrate the effectiveness of NN-Baton using
several typical models. NN-Baton saves 22.5%‚àº44%
energy compared to Simba in a 4-chiplet system. Additionally, we present how NN-Baton effectually provides
the optimal accelerators with the given number of MAC
units and area constraint for the target models.
To the best of our knowledge, this is the first work to
propose a complete framework with an automatic tool to facilitate the DNN workload mapping and the chiplet granularity
exploration for multichip accelerators.
II. BACKGROUND
This section aims to present the basic core concepts of the
DNN workload and chiplet technique, which helps readers to
understand the motivations and principles of this work.
A. DNN Workload
DNNs are composed of various components, including convolution, activation function, batch normalization, pooling, and
fully-connected layers. The convolution layer, used for feature
extraction, is particularly computation-intensive. Figure 1 provides an overview of the convolutional computation with a
seven-dimensional loop nest. This paper focuses on the batch
size of one to minimize the inference latency in some deployment scenarios like self-driving and datacenter [19, 27, 35].
We define a complete output cube of HO√óWO√óCO as a
layer workload consuming a 3D input cube and a 4D weight
tensor. Generally, due to the on-chip resource limitation, a
DNN accelerator needs to execute numerous iterations to form
the whole workload. When the convolution stride is smaller
than the kernel size, calculating adjacent planar convolution
windows generates an input overlap (or called halo) region,
potentially leading to redundant memory access.
In order to reuse data in the on-chip buffer, DNN accelerators require proper loop tiling and loop unrolling that are
critical optimization goals in many recent works [6, 15, 32, 41,
50, 71, 76]. Loop tiling is used to partition a large loop into
several tiny ones aiming to fit the buffer size and exploit the
memory locality. Loop unrolling targets the order in the loop
nest and it decides the dataflow in a computation array, such
as input stationary (IS) [50], weight stationary (WS) [47], and
output-stationary (OS) [10, 44].
CO
CI
1
CI
NÁÅÖ
WI
HI
0
CO
‚Ä¶ Kx
KY
WO
Input overlap
HO
(halo)
NÁÅÖ
Input Cube Output Cube
1
2
3
4
5
6
7
8
9
10
for n = [0 : N):
for co = [0 : CO):
for wo = [0 : WO):
for ho = [0 : HO):
for ci = [0 : CI):
for ky = [0 : KY):
for kx = [0 : KX):
O[n,co,ho,wo]+=
I[n,ci,ho+ky,wo+kx]
*W[co,ci,ky,kx]
Fig. 1: Overview of a convolution layer.
B. Chiplet Basis
The revolution of big data and machine learning is posing
an unprecedented demand for computation resources, which
requires more and more transistors to be integrated on the
chip. However, due to the slowdown of Dennard scaling and
Moore‚Äôs law [11], it becomes challenging and cost-inefficient
to design such a sophisticated SoC with ever-growing area
requirement. Tesla FSD with 72 TOPS performance targets at

the self-driving scenario and requires a 260 mm2 area [3]. The
area of Hanguang is as large as 709 mm2 fabricated in a 12
nm process technology to provide 825 TOPS performance in
datacenter [25]. The larger design consumes higher manufacturing costs and leads to a lower fabrication yield.
Chiplet technique becomes one of the promising methods
to tackle this problem. Several small functional dies known as
chiplet can be assembled to build a large-scale system within
a single package using the MCM technique. Chiplets are
connected via on-package links in a silicon interposer [39, 64]
or an organic substrate [58, 78]. Compared to the reticlelimited large monolithic die, the chiplet-based system enjoys
a lower manufacturing & testing cost and a higher fabrication
yield [4]. In addition, the chiplet-based system can mix silicon
dies from several heterogeneous process nodes to improve cost
efficiency [58]. However, the on-package die-to-die communication induces a considerable gap with the traditional fully onchip implementation and has become an active research topic
in recent years [40, 56, 66]. These works target at bridging
the gap of bandwidth and energy between the die-to-die and
on-chip interconnection.
In the field of DNN accelerators, Simba [55, 78, 79] is a
pioneering work fabricated in a chiplet-based system. Unlike
previous works using a small-scale 2D parallel array [1, 5, 7,
10, 28, 36, 50], a multichip DNN accelerator contains several
computation and memory levels, leading to the complicated
workload mapping and energy breakdown. A multichip accelerator tends to provide a large-scale system with multiple
arrays in a chiplet and multiple chiplets in a package. Table I
shows an overview of the energy breakdown in a multichip
accelerator. In contrast with small-scale designs [7, 44, 74], a
large L2 memory is introduced to cache data for several arrays.
The die-to-die communication has become a significant energy
portion, ranking only the second to the DRAM access. In
practice, each 1-bit transfer consumes 3.34 pJ energy because
the data need to go through a pair of die-to-die PHYs.
Therefore, when we map a workload to several chiplets,
the data locality should be taken into consideration to avoid
unnecessary inter-chip communication. The data transfer on
the package behaves differently from that on the chip. Onpackage transfer exploits a 2D-mesh NoP [64] or a shared IO
die [58] with higher complexity and overhead than the onchip communication. Naturally, the inter-chip communication
raises another problem: how to decide the chiplet granularity
in a multichip system? With the required number of MAC
units, distributing them to different numbers of chiplets brings
a trade-off between communication overhead and chiplet area.
These problems motivate us to develop a systematic tool that
helps architects explore superior designs and helps developers
map DNN workloads efficiently.
III. BASELINE ARCHITECTURE: MULTICHIP
HIERARCHICAL MODEL
To analyze the workload mapping, we first introduce a
universal hardware model shown in Figure 2. Later, we analyze
the inefficiencies of the dataflow in Simba and provide a
TABLE I: The energy overhead and characters of typical
operations in a 16 nm multichip hardware system. The dieto-die energy is from GRS [66].
Operation Energy
(pJ/bit)
Relative
cost
Feature
DRAM access 8.75 364.58√ó
As a slaver attached to a standard
high-speed bus and transfer data
to the chiplet through DDR PHY
Die-to-die
communication
1.17 53.75√ó Go through a pair of D2D PHYs
to transfer data from chip to chip
L2 access
(32KB SRAM)
0.81 33.75√ó SRAM multicast or unicast
data, which helps to save
time and energy L1 access
(1KB SRAM)
0.3 12.5√ó
Register readmodify-write 0.104 4.3√ó Frequently accessed in WS dataflow
8bit MAC 0.024 1√ó Energy is decided by utilization
hierarchical output-centric dataflow that is typically more
friendly to multichip accelerators.
A. Hierarchical Parallel Model
1) Core Architecture: We define an accelerator core as a PE
array connected with several local buffers for activations (AL1), weights (W-L1) and outputs (O-L1). The core architecture
is a Simba-like design that contains L lanes of P-size parallel
vector MAC units with weight-stationary dataflow. The outputchannel and input-channel are mapped along the L and P
dimensions of the PE array. The A-L1 and W-L1 buffers are
generated with double SRAMs to overlap the data loading
and computation time. As for the O-L1 buffer, we implement
it with registers to complete the read-modify-write operation
in one cycle. Besides, during once workload allocation, an
HOC√óWOC√óL output tile is assigned to the core. With the
given HOC√óWOC√óL, we can figure out the local memory
footprint requirements of various buffers.
2) Chiplet Architecture: We model the chiplet-level architecture with NC cores, a shared activation buffer (A-L2),
and a global output buffer (O-L2). All cores and buffers are
interconnected via a central bus attached to interfaces for
the off-chip memory and remote chiplet communication. We
employ the ground-referenced-signaling (GRS) as the die-todie (D2D) interface model that consumes 1.17 pJ/bit in 16 nm
process technology [66]. The central bus can multicast data
from the A-L2 buffer to the lower-level cores, conducive to the
activation-shared workload partition. The O-L2 is responsible
for collecting the final results of all cores in the once workload
assignment and then writes back to DRAM. The function of AL2 is to reuse activations of halo regions when adjacent planar
tiles are assigned to several cores. For weights, we organize
all W-L1 into a buffer pool and reallocate them in different
sharing modes. For example, when core-1 and core-2 require
the same weights, their W-L1 buffers are merged into a shared
group and broadcast the data to two PE arrays. Otherwise, two
cores are distributed with the private W-L1 buffers, and each
PE array can only access the local W-L1 space. This policy

Cross Bar (an abstraction IO die)
DRAM
2
DRAM
0
DRAM
3
DRAM
1
Chiplet 2
Chiplet 3
Chiplet 0
Chiplet 1
Ring
interconnect
Memory bus
on package
Package Architecture
Core Architecture
‚Ä¶
Vector MACs (P)
‚Ä¶
W-L1
(SRAM)
W-L1
(SRAM)
W-L1
(SRAM)
W-L1
(SRAM)
W-L1
(SRAM) ‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
A-L1 Buffer (SRAM)
Arbitrated Crossbar (from local or remote W-L1)
+ + + + +
To other cores
From other
cores
To
O-L2
‚Ä¶ O-L1
(Reg) O-L1
(Reg) O-L1
(Reg) O-L1
(Reg) O-L1
(Reg)
Post
Process
Acc Acc Acc Acc Acc
Load from bus From A-L2
Lanes (L)
Chiplet Architecture
Core 0 Core 2 Core 4 Core 6 A-L2
Buffer
Core 1 Core 3 Core 5 Core 7 O-L2
Buffer
Shared Data Bus
DDR I/F + DMA D2D I/F (Out) D2D I/F (In)
Computation Storage Interconnect
Package-Level: mapping along HO & CO dimension
NP Chiplet
parallelism DRAM Crossbar (DRAM)
and ring bus (chiplet)
Chiplet-Level: mapping along HO & WO & CO dimension
NC Core
parallelism A-L2, O-L2 Shared data bus
(multicast / unicast)
Core-Level: mapping along CI & CO dimension
PÁÅ§L MAC
parallelism
A-L1, O-L1,
W-L1
Array with multiple
lanes (broadcast)
Fig. 2: Overview of the three-level architecture for the multichip accelerator.
refrains from duplicating weights and makes full use of the
on-chip W-L1 buffers.
3) Package Architecture: We integrate NP chiplets via a
simple directional ring network on the package to simplify
the inter-chip communication. Chiplets on the package are
attached to NP global DRAMs via a crossbar that allows
chiplets to access the whole off-chip memory space. Based on
the directional ring network, we introduce a rotating transfer
shown in Figure 3 for sharing data among several chiplets.
Generally, each input feature map is shared by all filters
for various output channels. Therefore, when chiplets are
mapped along the output channel dimension, they require the
same input activations and distinct weights. In this case, each
chiplet is allocated with 1/NP input channels to process the
corresponding computation (Figure 3(a)). Subsequently, each
chiplet writes through its buffered activations from the local
A-L1 buffer to the adjacent chiplet via the ring interconnection (Figure 3(c)). Such operations repeat for NP times to
complete the accumulation of all input channels. Likewise,
when chiplets process diverse output feature map tiles, they
transfer weights and keep activations stationary on the chip
(Figure 3(b), (d)). This rotating transfer strategy provides a
straightforward solution for chiplets to spatially share data and
avoids duplication.
B. Dataflow
As shown in Figure 4(c)‚àº(d), Simba adopts a WS-based
weight-centric dataflow and performs the convolution computation in a systolic-like fashion on the NoC and NoP [55, 78].
The weight-centric dataflow means that the spatial mapping
(parallel for) centers around the weight dimension (e.g., CI
and CO). Simba splits input channels (CI) along rows and
splits output channels (CO) along columns. However, the
weight-centric dataflow fails to leverage the planar dimension
(HO and WO) of the activations, leaving the hidden overhead
of reloading the halo regions. Besides, partial sums are accumulated from the top to the bottom row across cores and chips,
which leads to higher communication overhead derived from
partial sums with a wider bit-width (24 bits compared to 8 bits
for inputs and weights). Additionally, to transfer 8-bit inputs
and weights and 24-bit partial sums using a unified interface,
Simba employs a high-cost SerDes IP in the PE router.
(a) Allocated along the output channel
with shared input (channel dimension).
(b) Allocated along the output planar
with shared weights (filter dimension).
Activation Tile 0
0:31
Activation Tile 1
32:63
Activation Tile 2
64:95
Activation Tile 3
96:127
96:127
0:31
32:63
64:95
64:95
96:127
0:31
32:63
32:63
64:95
96:127
0:31
Chiplet-0 Weight
Input
Chiplet-1 Weight
Input
Chiplet-2 Input
Weight
Chiplet-3 Input
Weight
Chiplet-0 0:15 48:63 32:47 16:31
Weight Filter 0:31
Input
Chiplet-1 16:31 0:15 48:63 32:47
Weight Filter 32:63
Input
Chiplet-2 32:47 16:31 0:15 48:63
Filter 64:95
Input
Weight
Chiplet-3 32:47 16:31 0:15
Filter 96:127
Input
Weight
48:63
------
D2D PHY (I)
A-L2
------
On-chip Bus
D2D PHY (O)
D2D PHY (I)
A-L2
------
On-chip Bus
D2D PHY (O)
D2D PHY (I)
A-L2
------
On-chip Bus
D2D PHY (O)
D2D PHY (I)
A-L2
On-chip Bus
D2D PHY (O)
A-L1 A-L1 A-L1 A-L1
0:15 16:31 32:47 48:63
YH
(c) Write through the local data to A-L1 of the adjacent chiplet in a rotation fashion.
double
write through buffer
YH
hi
PH
ch
YH
YH
hi
PH
ch
YH
YH
hi
PH
ch
YH
Y
i
H
hi c
Chiplet-0 Chiplet-1 Chiplet-2 Chiplet-3
------
D2D PHY (I)
DRAM
------
On-chip Bus
D2D PHY (O)
D2D PHY (I)
DRAM
------
On-chip Bus
D2D PHY (O)
D2D PHY (I)
DRAM
------
On-chip Bus
D2D PHY (O)
D2D PHY (I)
DRAM
On-chip Bus
D2D PHY (O)
W-L1 W-L1 W-L1 W-L1
0:31 32:63 64:95 96:127
YH
(d) Write through the local data to W-L1 of the adjacent chiplet in a rotation fashion.
double
write through buffer HY
h
PH
ch
YH
HY P
i hc
YH
HY
hi
PH
ch
YH
i
HY
hi c
Chiplet-0 Chiplet-1 Chiplet-2 Chiplet-3
Time Time
Fig. 3: The rotating transfer for data (activations and weights)
sharing via the directional ring network on the package.
(a)‚àº(b) present the dataflow for activation and weight sharing.
(c)‚àº(d) show the hardware mechanism of the rotating transfer
using the write-through manner.
In consequence, as shown in Figure 4(a)‚àº(b), we apply an
OS-based output-centric dataflow to the package and chiplet
level by mapping the workload along the output dimension
(CO, HO, and WO). Each core does not export results until all
the partial sums have been accumulated and then re-quantized
to 8-bit data for the next layer. In this way, only the 8-bit inputs
and weights are transferred through the simplified interfaces
with lower communication overhead. Based on the outputcentric dataflow, We further introduce (1) the spatial primitive
to represent the workload partition on the package and chiplet
(similar to parallel for in Simba), (2) the temporal primitive
to describe the loop unrolling strategy (similar to for in
Simba), and (3) the rotating primitive to describe the data
sharing among chiplets. The spatial-temporal pair generates

CO
HO
WO
(a) The output stationary (OS) variant of
multiple output channels and activations
// Package-Level
(HOt, WOt, COt) = temporal(spatial(HO, WO, CO))
// Chiplet-Level
(HOC, WOC, L) = temporal(spatial(HOt, WOt, COt))
// Core WS-Dataflow
rotating 0 ‚Üí Np:
lane-mapping 0 ‚Üí L:
vector-mapping 0 ‚Üí P:
for fy = [0 : FY):
for fx = [0 : FX):
for hoc = [0 : HOC):
for woc = [0 : WOC):
1
2
3
4
5
6
7
8
9
10
11
12
(b) Output-centric spatial-temporal
dataflow representation
// Package-Level
for ho2 = [0 : HO2):
for xo2 = [0 : XO2):
parallel_for co2 = [0 : CO2):
parallel_for ci2 = [0 : CI2):
// Chiplet-Level
for ho1 = [0 : HO1):
for wo1 = [0 : WO1):
parallel_for co1 = [0 : CO1):
parallel_for ci1 = [0 : CI1):
// Core WS-Dataflow
lane-mapping 0 ‚Üí L:
vector-mapping 0 ‚Üí P:
for fy = [0 : FY):
for fx = [0 : FX):
for hoc = [0 : HOC):
for woc = [0 : WOC):
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
(d) Weight-centric Simba baseline
dataflow representation
(c) The weight stationary of multiple
input - output channels
CI
‚Ä¶
Kx
0
0
0
1
0
3 KY CO
‚Ä¶
CO
Fig. 4: (a)‚àº(b) show the employed OS dataflow and the proposed output-centric spatial-temporal dataflow representation.
(c)‚àº(d) present the Simba baseline weight-centric dataflow.
Some necessary notations can refer to Figure 5(f).
a single workload for chiplets or cores each time. It provides
excellent convenience for describing the workload mapping
and evaluating the memory access overhead, which will be
introduced in Section IV. Note that the rotating transfer occurs
in the A-L1 or W-L1 buffers (Figure 3(c)‚àº(d)), so we place
the rotating primitive inside the core-level block to complete
the computation of an output tile.
IV. HIERARCHICAL AND ANALYTICAL FRAMEWORK FOR
MULTICHIP DNN ACCELERATORS
This section first introduces a framework with the outputcentric dataflow description and the C3P evaluation methodology. In addition, we analyze the preferred choice of the
partition pattern when we apply the spatial primitive. Based
on the framework, we then present a detailed characterization
of the proposed NN-Baton.
A. The Hierarchical Output-Centric Dataflow Description
Loop transformation is the core of dataflow analysis, which
has been frequently studied in these years [32, 41, 49, 71].
However, they need to analyze intricate tiling and unrolling
strategy for the whole seven-dimensional loop nest. In the following, we intend to use the spatial and temporal primitives
to describe the loop tiling and loop unrolling, respectively.
1) Spatial Primitive: It refers to the spatial parallelism
mapping on the computation units (similar to parallel for in
Simba). Figure 5 shows the spatial partition strategies, two
for the package level, and three for the chiplet level. In the
package level, an output cube is spatially divided into NP
parts (Figure 5(a)‚àº(b)) along the channel or plane dimension.
The P-type partition is more likely to be applied in the large
feature map layer to parallelize the plane dimension with
shared weights. Given that the large feature map layers usually
Workload Description
total workload HO ÁÅ§ WO ÁÅ§ CO
single chiplet
workload HOt ÁÅ§ WOt ÁÅ§ COt
single core
workload HOC ÁÅ§ WOC ÁÅ§ L
(f) Conventions
(a) Total workload partition
(NP=4) along plane (P-type)
CO
Chiplet-0
Chiplet-1
Chiplet-2
Chiplet-3
WO
COt
(c) Chiplet workload partition
(NC=4) along channel (C-type) HOt
WOt
WOC
L
HOC
Core-0
Core-1
Core-2
Core-3
(b) Total workload partition
(NP=4) along channel (C-type)
CO WO
HO
HO
(d) Chiplet workload partition
(NC=4) along plane (P-type)
COt
HOt
WOt
(e) Chiplet workload partition (NC=4)
along hybrid dimensions (H-type)
COt
HOt
WOt
Fig. 5: The visualization of loop transformation using spatial
and temporal primitives on the package and chiplet. Cubes in
the figure represent output cubes. The bold red lines denote
the spatial partition for several chiplets or cores, and dotted
lines denote the temporal partition for a single workload for
a chiplet or core each time.
show up early in a model with a small number of weights,
chiplets share weights cost-efficiently via ring network using
the rotating transfer. Similarly, The C-type partition is adaptive
to the wide layers with intensive weights and fewer activations
that can be shared among chiplets at a low cost. Additionally,
for the limited on-chip memory capacity, each time a chiplet is
delivered with a single chiplet workload of HOt√óWOt√óCOt.
Owing to the lower on-chip communication cost, we introduce a supplemental partition, H-type that allows to partition
along both dimension simultaneously (Figure 5(c)‚àº(e)). The
chiplet-level spatial primitive provides higher flexibility than
that on the package. At this point, two-level spatial primitives
provide six loop tiling combinations in total. With a particular
partition combination in a layer, the organization of W-L1
buffers, the central bus mode for data sharing, and the transfer
path for die-to-die sharing are then reconfigured.
2) Temporal Primitives: It refers to the temporal unrolling
mapping in sequence (similar to for in Simba). Temporal
primitives target the loop unrolling following the spatial
partition of macro workloads. The optimal strategy usually
depends on the layer characteristics. Thanks to the outputcentric dataflow, we reduce the unrolling search space from
the seven-dimensional loop nest to only two dimensions: the
channel-priority (C dimension in the inner-loop) unrolling
and the plane-priority (H-W dimension in the inner-loop)
unrolling. Figure 6(a) illustrates the alternative choices with
two temporal primitives in the hierarchy for generating four
possibilities in total. The channel-priority unrolling is friendly
to the weights reuse if the W-L1 buffers can hold the total
weights for the single chiplet/core workload computation.
On the contrary, activation reuse potentially gains benefit
from the plane-priority unrolling. Combined with six spatial
partition manners, our dataflow space provides twenty-four
loop transformation solutions in our hierarchical framework
that facilitates the analysis in Section IV-B.

// Package-Level
(HOt, WOt, COt) = temporal(spatial(HO, WO, CO))
for c2 = [0 : C2):
for h2 = [0 : H2): // H2 * HOt = HO
for w2 = [0 : W2): // W2 * WOt = WO
for c2 = [0 : C2): // C2 * COt = CO
// Chiplet-Level
(HOC, WOC, L) = temporal(spatial(HOt, WOt, COt))
for c1 = [0 : C1):
for h1 = [0 : H1): // H1 * HOc = HOt
for w1 = [0 : W1): // W1 * WOc = WOt
for c1 = [0 : C1): // C1 * LOt = COt
1
2
3
4
5
6
7
8
9
10
11
12
(a) Loop order in the temporal primitive
for h2 = [0 : H2):
for w2 = [0 : W2):
for c2 = [0 : C2):
for h1 = [0 : H1):
for w1 = [0 : W1):
for c1 = [0 : C1):
1
2
3
4
5
6
for c2 = [0 : C2):
for h2 = [0 : H2):
for w2 = [0 : W2):
for h1 = [0 : H1):
for w1 = [0 : W1):
for c1 = [0 : C1):
1
2
3
4
5
6
(d) Example-2 for the W-L1 analysis
Reuse Region Critical Position
(c) Example-1 for the W-L1 analysis
for h2 = [0 : H2):
for w2 = [0 : W2):
for c2 = [0 : C2):
for h1 = [0 : H1):
for w1 = [0 : W1):
for c1 = [0 : C1):
1
2
3
4
5
6
for c2 = [0 : C2):
for h2 = [0 : H2):
for w2 = [0 : W2):
for h1 = [0 : H1):
for w1 = [0 : W1):
for c1 = [0 : C1):
1
2
3
4
5
6
(f) Example-4 for the A-L1 analysis
(e) Example-3 for the A-L1 analysis
3. Traverse the reuse region
before the next critical
position in the outer loops
4. Penalty factor
2. Compare buffer size
to the critical capacity
1. Search a new
critical position
Satisfy the capacity
Inadequate
capacity
END
At the outmost loop
(b) The flow diagram of the C3P loop analysis
Fig. 6: (a) presents loop unrolling (plane-priority or channelpriority) inside the temporal primitives. (b) provides the C3P
flow diagram to calculate the total memory access for a
specific temporal combination. (c)‚àº(f) show examples for the
memory access analysis of W-L1 and A-L1 buffers.
B. The Analytical Methodology for Evaluation
The DNN workload mapping strategy and the buffer sizes
of each level in the memory hierarchy are two principal factors
for memory access. To concisely evaluate the memory access
overhead, we formulate a Critical-Capacity Critical-Position
(C3P) methodology to understand the relation between the
buffer size and workload mapping. The critical positions (Cp)
denote the loops that decide the data reuse for a buffer, while
the critical capacity (Cc) refers to the buffer sizes that allow
reusing data for the outer-loop.
Figure 6(c)‚àº(d) show examples of W-L1 analysis. We
divide temporal loops into two categories: the relevant and
irrelevant loops for weights. The relevant loops decide the
critical positions for W-L1 while the irrelevant loops between
critical positions (or the boundary) form several reuse regions.
After that, we leverage the flow diagram shown in Figure 6(b)
to analyze the example-1. Firstly, the loop C1 is the first
critical position Cp1, and the corresponding Cc1 is the capacity to reuse data for the outer-loop, W1-H1. Then, we can
obtain the value of the first critical point: Cc1 = C1 √ó filters,
where filters refers to the volume of weights used for a single
core workload. W-L1 with less than Cc1 size will encounter
H1√óW1 ‚àí 1 access penalties reloading from the off-chip
memory. Later, we search the next Cp in the outer-loops and
perform the same procedures. Just note that the second critical
capacity can be calculated as Cc2 = C2 √ó C1 √ó filters. The
example-2 in Figure 6(d) is similar, but the minimal capacity
without penalty only depends on Cp1 because Cp2 is at the
boundary of the loop nest. Given that several W-L1 buffers
can be merged and shared for multiple cores, the actual buffer
size of W-L1 may vary with layers in different sharing modes.
Figure 6(e)‚àº(f) present the example for the A-L1 buffer.
The example-3 shows another loop case where the first loop
is a reuse region. The lower degree of these six loops is the PE
array computation for the HOC√óWOC√óL workload. Therefore, we additionally supplement Cp0 and Cc0 to complete the
framework. Besides, different from the analysis of W-L1, the
critical capacity for a specific input feature map is determined
by the workload size, kernel size, and stride. The example-4
in Figure 6(f) is a bad case for A-L1 because Cc1 does not
contribute to any data reuse. Only when the buffer size is larger
than Cc2 does the buffer locality come into effect. The A-L2
analysis is quite similar to A-L1 with the additional operation
to sum up the entire on-chip workload and then calculate the
corresponding input feature size.
In summary, illustrated in Equation(1)‚àº(2), the total memory access of the j-level memory (A(j)
tot) comprises two parts:
the intrinsic access (A0) decided by the layer configuration and
the total penalty access. Nj denotes the set of critical positions
ranging from the outmost critical position to the jth one (i.e.
Ccj , Ccj+1, ...). It is observed that buffers with large enough
capacities contribute to no access penalty at high energy and
area overhead costs. The A(j)
tot can be calculated as:
A(j)
tot = A0 √ó (1 +
k‚ààNj
Pk) (1)
where Pk is defined as Equation (2). Rk denotes the reuse
region between the kth and (k+1)th critical position. Besides,
LC means the loop count. With the aforementioned C3P
methodology, we can evaluate the overhead of each memory
level in the chiplet architecture.
Pk =
‚éß
‚é®
‚é©

i‚ààRk
LCi, buf < Cck
1, buf  Cck
(2)
C. Feature Map Partition Pattern Analysis
During the previous analysis, we combined the H and
W dimensions in the dataflow description and ignored the
(more tiles) (less tiles) (more tiles) (less tiles)
300
350
400
450
500
550
600
650
64 256 1024 4096 16384
Extra memory access due to
feature overlap (%)
Single tile size (# of elements of a tile)
1:1 Partition
1:4 Partition
0
10
20
30
40
50
60
70
64 256 1024 4096 16384 65536
Single tile size (# of elements of a tile)
1:1 Partition
1:4 Partition
(a) Conv1 layer in ResNet-50
(kernel size=7, stride=2, output size=256)
(b) Conv1 layer in VGG-16
(kernel size=3, stride=1, output size=512)
64ÁÅÖ256
128ÁÅÖ128
4ÁÅÖ16
8ÁÅÖ8
128ÁÅÖ512
256ÁÅÖ256
4ÁÅÖ16
8ÁÅÖ8
height-width ratio height-width ratio
Fig. 7: The redundant memory access with 1:4 and 1:1
partitions patterns in two convolution layers derived from the
input feature overlap when workloads are allocated to several
chiplets or cores. The input resolution of model is 512√ó512.
   
partition pattern. Previous studies for the single-chip accelerator adopted various partition patterns, including stripe [28],
rectangle [7], and square [36]. However, with the same number of elements, the spatial partition pattern for H and W
dimensions can significantly impact memory access, especially
in a multichip system. In a multichip system for large-scale
computation (e.g., 512√ó512 inputs), an inappropriate pattern
can lead to unexpected energy overhead originated from halo
regions. Consequently, the partition pattern selection should be
taken into consideration in the workload mapping procedure.
Figure 7 shows the redundant memory access results from
the planar partition with different patterns in two convolution
layers. The first convolution layer in ResNet-50 features 7√ó7
kernel size and 2√ó2 stride, leading to halo regions of five
elements on each side and up to 650% memory access
increase. It is observed that the square pattern enjoys less
redundant access compared to the rectangle (stripe) one, but
the gap between them tends to be smaller when the tile size
is getting larger. Compared to the 7√ó7 convolution, the 3√ó3
convolution in VGG-16 presents lower extra access while they
have the same variation trend.
The conclusions in Figure 7 motivate us to employ a square
pattern as far as possible to reduce redundant memory access,
especially in the temporal partition. As shown in Figure 5,
the temporal primitive generates numerous small tiles, and
thus the square pattern is a preferred choice in this scenario.
We observe that the package-level spatial primitive generates
only NP tiles (1‚àº8 in this paper), and theoretically, both
choices seem acceptable. However, in the multichip system,
to provide enough bandwidth for four chiplets, four DRAMs
are integrated into the system. An appropriate data layout
is indispensable to avoid memory access conflict and ensure
processing efficiency. Figure 8(a) shows a massive central halo
region when we use the square pattern, indicating that the
central data need to be accessed by four chiplets. In contrast,
the halo data are accessed by two chiplets at most in the
rectangle pattern (Figure 8(b)). As for the on-chip spatial
partition, the control logic can be more flexible so that the
preferred pattern generally depends on the specific design and
the shape of a single chiplet workload.
CI
WI
HI
CI
WI
(b) Rectangle pattern
Chiplet-0
Chiplet-1
Chiplet-2
Chiplet-3
(a) Square pattern
Chiplet-2 Chiplet-3
Chiplet-0 Chiplet-1
halo
region
HI
Fig. 8: The visualization of the halo regions using two partition
patterns (square and rectangle). The overlap regions potentially
lead to the DRAM access conflict.
C3P Evaluation Engine
(Cost Extracted from Tech. Lib.)
Area Overhead
Mapping Analysis Engine (mem, PE, PHY)
Hardware Design
Space Exploration
Partition Pattern
Loop Transformation Runtime
Energy Cost
(mem/PHY access, MAC)
Design Proposal
# of chiplets: 2
# of cores: 8
# of lanes: 16
Vec.MAC size: 16
A-L1 size: 32KB
W-L1 size: 144KB
¬∑¬∑¬∑¬∑¬∑¬∑
NN Model
Description
(Parsed from the
Pytorch Model)
Mapping Strategy
Layer1: package-level:
spatial (P)
temporal (C-Priority)
Layer1: chiplet-level:
spatial (P)
temporal (P-Priority)
Layer2: ¬∑¬∑¬∑¬∑¬∑¬∑
Specific HW
Description
Input NN-Baton Tool (Implemented in Python)
Output Report
Pre-Design Flow
Post-Design Flow
Resource
Options
+ Constraint
Fig. 9: An overview of the NN-Baton diagram composed
of three key components: hardware design space exploration,
mapping analysis, and cost analysis modules.
D. The Proposed NN-Baton Automatic Tool
Figure 9 provides a high-level description of the proposed
NN-Baton automatic tool. NN-Baton can generate the workload mapping strategy and the chiplet granularity in a predesign flow and a post-design flow, respectively.
The pre-design flow helps architects decide the chiplet granularity and choose an appropriate hardware resource scheme
for computation and memory with the given neural network
workloads and hardware constraints. The constraints primarily
include the number of MAC units and the area budgets.
Provided with mapping strategies and configured with a target
process technology, the C3P evaluation engine estimates the
area and energy of different possible hardware design choices.
The runtime estimation is decided by the total number of MAC
units and the utilization. The hardware with too high channelwise parallelism (i.e., L, the number of lanes) is improper for
the thin layer, leading to the under-utilization of computing
resources. Finally, the optimal proposal is reported in the
output, followed by the C3P evaluation engine.
The post-design can be viewed as a sub-process of the
pre-design flow with a specific hardware configuration. This
flow produces a detailed mapping strategy for deploying the
model on hardware with spatial and temporal primitives.
The spatial primitives contain the partition dimension and
the partition pattern, while temporal primitives contain the
loop order and loop counts. The reported information can be
potentially used for the optimization of the hardware compiler.
V. EXPERIMENTAL SETUP
A. Hardware Configurations
We model a multichip DNN accelerator based on the
description of Section III with 8-bit arithmetic and 24-bit reserved width for partial sums. To evaluate the area and energy

overhead, we use standard cells and memory models from
ARM and synthesize a computation core in the UMC 28 nm
process technology using Synopsys Design Compiler [60].
Scaled to 16 nm technology to match the GRS macro, the
area and power of an 8-bit MAC are respectively 135.1 um2
and 0.024 pJ/op running at 500MHz frequency.
We select the appropriate multiplexer width and number
of banks with a given SRAM configuration for the optimal
area and power from the memory model library. As demonstrated by several preliminary evaluations, the area and power
approximately satisfy a linear relationship with the SRAM
size presented in Figure 10, which allows us to extend the
exploration space of memory search using linear regression.
The average GRS energy in our evaluation is 1.17 pJ/bit
with a 0.38 mm2 area [66]. The DRAM power is estimated to
be 8.75 pJ/bit [22, 71]. The total area of a chiplet includes
SRAM, RF, MAC units, and the off-chip PHY and ignores the
controller and other IP modules.
B. Workloads
From state-of-the-art DNN models [20, 31, 53, 57], we
can extract some representative layers as follows: activationintensive layers (activations<weights), weight-intensive layers
(activations>weights), large kernel-size layer (7√ó7), pointwise layer (1√ó1), and other common layers (3√ó3). We also
select AlexNet, VGG-16, ResNet-50 and DarkNet-19 with the
input resolutions of 224√ó224 (for classification tasks) and
512√ó512 (for detection tasks). AlexNet contains convolution
layer of diverse kernel sizes, ranging from 3√ó3 to 11√ó11
while the other three models mainly contain 3√ó3 and 1√ó1
layers. ResNet-50 and DarkNet-19 are wide models with up
to 2048 channels. The feature map size in ResNet-50 reduces
earlier than that in VGG-16 and DarkNet-19. Consequently,
0
2
4
6
8
10
12
14
16
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 50 100 150 200 250
Area (um2) x 10000
Write / Read Energy (pJ/op/bit)
Single-Port SRAM Size (KB)
0
20
40
60
80
100
120
0
0.01
0.02
0.03
0.04
0.05
0.06
0 200 400 600 800
Area (um2) x 10
Write / Read Energy (pJ/op/bit)
Register File Size (B)
Write
Read
Area
Write
Read
Area
Fig. 10: The linear relationship between the memory (SRAM
and Register File) size and overhead (scaled to 16 nm). The
energy of RF refers to one read-modify-write operation.
TABLE II: The design space of computation resources and
memory footprint for the experimental setup.
Design Space Exploration
Computation Resources Memory Footprint
Vector-MAC (P) 2, 4, 8, 16 O-L1 Size (B) 48 - 144
# of Lanes (L) 2, 4, 8, 16 A-L1 Size (KB) 1 - 128
# of Cores (NC ) 1, 2, 4, 8, 16 W-L1 Size (KB) 2 - 256
# of Chiplets (NP ) 1, 2, 4, 8 A-L2 Size (KB) 32 - 256
compared to ResNet-50, their peak memory requirements
(show up in Layer-1‚àº2) for activations are four times as many.
C. NN-Baton Implementation
We implement NN-Baton based on Figure 9 in Python. The
resource options from Table II establish the whole exploration
space for our evaluations. The O-L2 buffer size is set to match
the volume of the final elements of a single chiplet workload.
The model description is parsed from the Pytorch model
using the torch.jit function [51]. The mapping analysis
engine adopts exhaustive search to evaluate hundreds of cases,
including partition patterns with different height-width ratios
and loop transformation of various spatial-temporal combinations introduced in Section IV. Based on the extracted area
per unit and energy per operation in Section V-A, the C3P
evaluation engine models the corresponding area and energy of
the accelerator. Runtime depends on the total MAC units and
the computation resource utilization. We establish a simulator
to obtain the runtime for a specific workload. In the next
section, we employ the pre-design and post-design flow to
discuss the workload mapping and design space exploration
for the multichip accelerator.
VI. CASE STUDIES
A. Workload Mapping on the Multichip Accelerator
1) Analysis of Mapping Diverse Layers: We first configure
the hardware model with 4 chiplets, 8 cores, 8 lanes of 8-
size vector MAC, 1.5KB O-L1, 800B A-L1, 18KB W-L1
and 64KB A-L2. With the given hardware, we employ NNBaton to explore the workload mapping for five distinct layers
using different spatial partition strategies. We use the VGG16 conv1, VGG-16 conv12, ResNet-50 conv1, ResNet-50
res2a branch2a and res2a branch2b as the activation-intensive
layer, weight-intensive layer, large kernel-size layer, pointwise layer and common layer, respectively. From the results in
Figure 11, we observe that the hybrid partition in the chipletlevel ((C, H) or (P, H)) provides the overall lower energy
overhead. In the activation-intensive and large kernel layer, the
feature map size and the halo region tend to be large, so P-type
partition provides fewer redundant memory access derived
from the halo regions. However, in the weight-intensive and
especially the point-wise layer, the channel dimension has
become the principal bottleneck in the computation, prompting
the C-type partition to be a preferable solution. Because we
use ResNet-50 res2a branch2b as the common layer where
the burden of activation is a little less than that of weight, the
results show the C-type partition is more advantageous than
the others. The comparison between two rows in the figure
demonstrates the similar trends and validates the stability of
NN-Baton for different input sizes.
As for the energy breakdown, we observe that the impact of
spatial partition manner for the die-to-die overhead is distinct,
which is a primary cause of higher total energy. For example,
in the activation-intensive layer, the C-type partition on the
package leads to a large number of activations transfer among
chiplets while the situation is inverse in the weight-intensive

0
2
4
6
8
(C, C) (C, P) (C, H) (P, C) (P, P) (P, H) 0
0.5
1
1.5
2
2.5
(C, C) (C, P) (C, H) (P, C) (P, P) (P, H) 0
0.5
1
1.5
2
(C, C) (C, P) (C, H) (P, C) (P, P) (P, H)
0
5
10
15
(C, C) (C, P) (C, H) (P, C) (P, P) (P, H) 0
1
2
3
4
5
(C, C) (C, P) (C, H) (P, C) (P, P) (P, H) 0
0.5
1
1.5
2
2.5
3
(C, C) (C, P) (C, H) (P, C) (P, P) (P, H)
0
0.5
1
1.5
(C, P) (C, H) (P, C) (P, P) (P, H)
Energy (mJ) 0
1
2
3
4
5
6
(C, P) (C, H) (P, C) (P, P) (P, H)
Energy (mJ)
0
0.5
1
1.5
2
(C, P) (C, H) (P, C) (P, P) (P, H)
0
2
4
6
8
(C, P) (C, H) (P, C) (P, P) (P, H)
(a) Activation-intensive (b) Weight-intensive (c) Large kernel size (d) Point-wise (e) Common layer
Die-to-Die DRAM W-L1 A-L2 A-L1 Output MAC
224ÁÅÖ224 Input 512ÁÅÖ512 Input
Fig. 11: The energy estimations with breakdown of different spatial partition strategies with two input resolutions are presented
in two rows. Five types of layers are listed and we choose the best temporal strategy for each result. In the x-axis, two items
denote the spatial partition on the package and chiplet level. C, P, and H denote channel, plane, and hybrid described in
Section IV, respectively. We remove the (C, C) option in (a) and (c) due to the mismatch with their small output channels.
0
0.2
0.4
0.6
0.8
1
SimbaOurs SimbaOurs SimbaOurs SimbaOurs SimbaOurs
Normalized Energy
Activationintensive Weightintensive Large
kernel
Pointwise
Common
layer
(a) 224x224 Input Resolution
Die-to-Die DRAM W-L1 A-L2 A-L1 O-L1 MAC
0
0.2
0.4
0.6
0.8
1
SimbaOurs SimbaOurs SimbaOurs SimbaOurs SimbaOurs
Normalized Energy
Activationintensive Weightintensive Large
kernel
Pointwise
Common
layer
(b) 512x512 Input Resolution
Fig. 12: The normalized energy breakdown of the Simba
baseline dataflow and the dataflow generated by NN-Baton
in five distinct layers.
layer. The W-L1 energy consumption is always low in the
evaluations due to the weight-stationary dataflow in each core.
Improper loop transformation for W-L1 primarily leads to
extra access to the DRAM.
The diverse preference of different spatial primitives motivates us to apply an optimal solution to different layers
properly. Therefore, according to each layer‚Äôs parameter characteristics, NN-Baton provides a distinct mapping strategy
layer-wise to minimize the overall energy cost.
2) Comparison with Simba: We model a 4-chiplet Simba
prototype with the configurations (memory sizes and computation resources) and basic dataflow introduced in previous
works [55, 78]. For comparability, the multichip accelerator
model for NN-Baton is configured with the same memory and
computation resources as Simba. Considering that we target
the workload mapping optimization in this experiment, like in
NN-Baton, we omit the controller and RISC-V overhead of
Simba and primarily count the memory write/read operations
coupled with the die-to-die communication.
Figure 12 shows the normalized energy in five distinct layers
of two input resolutions, and NN-Baton provides overall low
energy. We observe significant advantages of NN-Baton in the
activation-intensive and large kernel-size layers, especially in
the 512√ó512 resolution case. The advantages are based on the
output-centric dataflow and the C-type spatial partition in NNBaton, beneficial to the activation buffers and DRAM access.
These layers contain numerous activations and large size of
halo regions, and consequently, the output-centric dataflow
can prevent the 24-bit partial sums from transferring on the
NoC and NoP and avoid the excessively fragmented H-W
dimensions. On the contrary, in layers with smaller feature
sizes, such as the weight-intensive and point-wise layers, both
perform similarly. Besides, we observe that Simba‚Äôs die-to-die
overhead is always slightly higher than ours due to the massive
transfer for partial sums on the package.
Figure 13 provides model-level comparisons with Simba.
The results present 22.5%‚àº44% lower energy cost in three
classical models with two input resolutions. According to the
previous analysis, Simba baseline dataflow is weak in the
layers with large feature maps and halo regions, so the results
of 512√ó512 are always inferior to those of 224√ó224. The
feature map size reduces later in VGG-16 and DarkNet-19 than
ResNet-50, so NN-Baton saves more energy in the VGG-16
and DarkNet-19 benchmarks.
These two experiments demonstrate the outperformance
of output-centric dataflow compared to the weight-centric

0
0.2
0.4
0.6
0.8
1
SimbaOurs SimbaOurs SimbaOurs SimbaOurs SimbaOurs SimbaOurs
Normalized Energy
ResNet-50
Simba Ours
VGG-16 DarkNet-19 ResNet-50 VGG-16 DarkNet-19
(224ÁÅ§224) (512ÁÅ§512)
22.5% 24% 26.8% 28.9% 35.5% 44%
Fig. 13: The comparisons between Simba and NN-Baton using
three classical DNN models with 224√ó224 and 512√ó512
inputs. The estimation calculates the CONV and FC layers. We
reorganize FC layers into point-wise layers for the evaluation.
manner. This is because output-centric dataflow manages to
aggregate data more locally to reduce the energy overhead.
B. Design Space Exploration for Multichip Accelerators
1) Analysis of the chiplet granularity: One of the most
promising explorations for the multichip accelerator is to decide the granularity of a chiplet. Given a required performance,
we need to figure out the optimal number of chiplets that
significantly impact the manufacturing costs. With 2048 MAC
units in total, there are up to 63 possibilities with different
combinations of the number of chiplets per package, cores per
chiplet, lanes per core, and MAC units per lane. We assemble
the memory hierarchy with buffer sizes proportional to the
computation resources in this experiment.
Figure 14 shows the optimal hardware implementation using
1-to-8 chiplets with the best case of the other three computation dimensions when they are deployed to four typical
models. We observe that without any area constraint, the
energy consumption is generally higher with more chiplets.
This is straightforward because on-chip communication generally shows a lower cost than inter-chip communication.
With regard to the exceptions in VGG-16 and ResNet-50,
the smaller search space of one-chiplet schemes (only three
options) limits the exploration for a better optimum. However,
with a chiplet area constraint of 2 mm2, no implementation
meets the constraint using one chiplet, and the 4-chiplet implementations provide overall low energy-delay-product (EDP).
Encouragingly, even though we select the best case out of
20 implementations in 4-chiplet design, the 4-4-16-8 scheme
is always the top-pick one, indicating that this is the optimum computation resource configuration under 2 mm2 area
constraint. The 8-chiplet design shows relatively high energy
cost and more runtime because assembling 256 MAC units
per chiplets is so scattered that it creates much die-to-die
communication overhead.
It is observed that in a multichip accelerator, the chiplet
area is a significant driven force to push a design distributed
in several dies. Employing the chiplet-based solution sacrifices
the performance and energy cost but obtains lower cost and
enables the die reuse. Such trade-off is quite common in
Energy w/ area constraint
224ÁÅÖ224 Input 512ÁÅÖ512 Input
(a) AlexNet
0
1
2
0
1
2
3
(1,16,16,8)
(1,8,16,16)
(2,8,16,8)
(2,16,16,4)
(4,4,16,8)
(4,16,16,2)
(8,8,16,2)
(8,4,16,4)
Runtime (ms) (area-constraint)
Energy (mJ)
2
3
4
5
6
0
2
4
6
8
(1,16,16,8)
(1,8,16,16)
(2,8,16,8)
(2,16,16,4)
(4,4,16,8)
(4,16,16,2)
(8,8,16,2)
(8,4,16,4)
Runtime (ms) (area-constraint)
Energy (mJ)
8
9
10
0
5
10
15
(1,16,16,8)
(1,8,16,16)
(2,8,16,8)
(2,16,16,4)
(4,4,16,8)
(4,16,16,2)
(8,8,16,2)
(8,4,16,4)
Runtime (ms) (area-constraint)
Energy (mJ)
36
38
40
42
44
46
0
10
20
30
40
50
60
(1,16,16,8)
(1,8,16,16)
(2,8,16,8)
(2,16,16,4)
(4,4,16,8)
(4,16,16,2)
(8,8,16,2)
(8,4,16,4)
Runtime (ms) (area-constraint)
Energy (mJ)
(b) VGG-16
2.4
2.6
2.8
3
0
2
4
6
8
10
(1,16,16,8)
(1,8,16,16)
(2,8,16,8)
(2,16,16,4)
(4,4,16,8)
(4,16,16,2)
(8,8,16,2)
(8,4,16,4)
Runtime (ms) (area-constraint)
Energy (mJ)
8
10
12
14
0
10
20
30
40
(1,16,16,8)
(1,8,16,16)
(2,8,16,8)
(2,16,16,4)
(4,4,16,8)
(4,16,16,2)
(8,8,16,2)
(8,4,16,4)
Runtime (ms) (area-constraint)
Energy (mJ)
(c) ResNet-50
3
4
5
0
1
2
3
4
5
(1,8,16,16)
INVALID
(2,8,16,8)
(2,16,16,4)
(4,4,16,8)
(4,16,16,2)
(8,8,16,2)
(8,4,16,4)
Runtime (ms) (area-constraint)
Energy (mJ)
6
7
8
9
0
4
8
12
16
(1,16,16,8)
(1,8,16,16)
(2,8,16,8)
(2,16,16,4)
(4,4,16,8)
(4,16,16,2)
(8,8,16,2)
(8,4,16,4)
Runtime (ms) (area-constraint)
Energy (mJ)
(d) DarkNet-19
Energy w/o area constraint Runtime
Fig. 14: Different hardware implementations with 2048 MAC
units in total. The energy and runtime are estimated with the
optimal workload mapping strategy. The grey bars are the
best implementations without any area constraint in a specific
model. The colored bars are the prime implementations under
2 mm2 chiplet area constraint. Four groups in a plot refers to
the 1, 2, 4, and 8-chiplets implementations. The four-element
tuple at the x-axis represents (chiplet, core, lane, vector-size).
The bar with a red dotted box enjoys the lowest EDP.
a chiplet-based system, and NN-Baton provides a possible
solution for this problem.
2) Design Space Exploration: We employ the NN-Baton
pre-design flow to implement the multichip accelerator design
space exploration for three models using the parameters listed
in Table II. The energy and runtime in each layer are based
on the optimal mapping strategy for a specific hardware
implementation. Given the area and performance budgets, NNBaton provides every possible design with the recommended
computation and memory allocation. Besides, the search can
skip some invalid cases to speed up the space sweeping, such
as the A-L1 size smaller than A-L2 or the total MAC units
less than the required quantities.
Figure 15 presents the design space exploration with 5800
valid points of over 100,000 sweeping for the whole model.
Four colors represent 1, 2, 4, and 8-chiplet implementations,
respectively. The grey trend-line divides the results into two
zones: the right points are some redundant designs with un-

VGG-16 (512ÁÅÖ512 Input) ResNet-50 (512ÁÅÖ512 Input) DarkNet-19 (224ÁÅÖ224 Input)
Area Per Chiplet (mm¬≤) Area Per Chiplet (mm¬≤) Area Per Chiplet (mm¬≤)
Normalized EDP
0
0.2
0.4
0.6
0.8
1.0
23456 23456 23456
global optimum
optimum under
area constraint
1 2 4 8
the number of
chiplets per package
area constraint area constraint area constraint
Fig. 15: The design space explorations for the multichip accelerators of 4096 MAC units. With no RISC-V and fewer interface
macros in our model than Simba, we apply a chiplet area constraint of 3 mm2. In the three plots, we zoom in the region of
the optimum point under area constraint. Points below the grey trend-lines refer to the more proper designs.
necessary memories, while the left points are potential implementations with appropriate memory allocation. We observe
that four colored points appear layered in the plot without
area constraint: the 1-chiplet designs (yellow) gather in the
lower right region, and the other three gather to the upper
left direction in succession. This observation matches the
conclusion in Figure 14 that the designs with fewer chiplets
sacrifice the area to provide a lower EDP.
Surprisingly, in three benchmarks, the optimal implementations under area constraint are all the configurations of 2-
chiplet, 8-core, 16-lane, and 16-vector-size, as the same in
Figure 14. We believe that the optimal resource allocation for
computing highly depends on the area constraint. However,
the memory allocations in these three recommended schemes
are distinct. The benchmarks with 512√ó512 input resolution
prefer larger activation buffers (32KB A-L1) than the 224√ó224
benchmark (4KB A-L1). Also, the W-L1 size in DarkNet of
224√ó224 input (72KB W-L1) is smaller than the other two
benchmarks (144KB W-L1) even though the peak storage of
weights in DarkNet (4.5MB) is larger than that of VGG or
ResNet (2.25MB). Because the feature sizes in these weightintensive layers are quite small, the on-chip memory can buffer
all the activations without any weight access penalty even if
buf < CC. In conclusion, the computation resource allocation
depends more on the area constraint while memory allocation
is sensitive to the target model.
VII. RELATED WORKS
A. DNN Workload Mapping
DNN mapping is a necessary procedure when we expect
to deploy a specific model on the accelerator efficiently. The
loop transformation is required to achieve efficient cache
locality for the seven-dimensional loop nest. TeraDeep [17]
is a prior work using loop tilling in convolution layers.
Later, a series of studies proposed various dataflow for the
PE array computation, including output stationary [10, 44],
weight stationary [13, 47], input stationary [50], row stationary [6]. For the loop transformation analysis, Zhang et
al. [76] and Ma et al. [42] provided reliable frameworks for
the FPGA platform. These loop transformation strategies are
all expressed in a loop-nest manner to reveal the scheduling
policy [8]. Besides, some variants for domain-specific architectures are proposed recently. TETRIS leveraged a vault-centric
partition for NN accelerators with 3D-stacked DRAM [14].
Tangram [15] employed a cascaded loop nest of two layers
to describe the layer-pipeline. MAESTRO [32] used a datacentric framework to concisely depict the data movement and
reuse behavior in the spatial PE array. These works motivate
us that a specific architecture needs the corresponding dataflow
description. Currently, there is a lack of a proper description
of the chiplet-based system.
B. Hardware Design Space Exploration and Chiplet Design
Hardware design space exploration (DSE) guides the hardware architect to decide the chiplet granularity and distribute
resources with the given applications. Caffeine [77] proposed
a complete framework with analysis and DSE engines to
generate FPGA design using HLS, and AutoDNNchip [68]
took a further study on the chip design. Interstellar [71]
explored the dataflow and memory hierarchy for the DNN
accelerator. MAGNet [61] primarily explored the design space
for an NVDLA-like [47] computation core and proposed a
novel output stationary - local weight stationary dataflow.
Simba [55, 62, 78, 79] is a pioneering work to implement
a DNN accelerator using chiplet technology. It provided the
basic architecture and dataflow for the multichip accelerator
design. Simba also took an in-depth study on the non-uniform
workload partitioning on the NoC and NoP. However, they
did not provide a complete and analytical framework for the
workload mapping and the guidance to the chiplet granularity.
When we push DNN accelerators to the chiplet dimension, the
granularity of each chiplet is principal for the designers. The
existing DSE works mainly focus on traditional platforms like
FPGA and monolithic chip. DSE for the multichip accelerators
provides a promising future for the architecture community.
VIII. CONCLUSION
This work filled the gap in the field of DNN workload
mapping and DSE for the multichip accelerators. Based on
a universal and concise model, we presented a hierarchical
and analytical framework with the output-centric dataflow
description and the analytical C3P methodology for evaluation. We then proposed an automatic tool called NN-Baton to

conduct the orchestration of the DNN workload and guide the
chiplet granularity decision. We evaluated NN-Baton relative
to Simba and showed a 22.5%‚àº44% energy reduction in
three classical models with 224√ó224 and 512√ó512 input
resolutions. Besides, we provided a case study about the
impact of spatial partition and demonstrated the necessity to
employ the preferable spatial primitives in different layers.
Finally, we explored the chiplet granularity decision with a
required number of MAC units. The results demonstrated
that the area constraint is a decisive factor for the optimal
computation allocation, while the memory resource allocation
typically depends on the NN models.