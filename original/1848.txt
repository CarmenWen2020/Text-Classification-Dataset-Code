Abstractâ€”Brain-computer interfaces (BCIs) offer avenues to
treat neurological disorders, shed light on brain function, and
interface the brain with the digital world. Their wider adoption
rests, however, on achieving adequate real-time performance,
meeting stringent power constraints, and adhering to FDAmandated safety requirements for chronic implantation. BCIs
have, to date, been designed as custom ASICs for specific diseases
or for specific tasks in specific brain regions. General-purpose
architectures that can be used to treat multiple diseases and
enable various computational tasks are needed for wider BCI
adoption, but the conventional wisdom is that such systems
cannot meet necessary performance and power constraints.
We present HALO (Hardware Architecture for LOw-power
BCIs), a general-purpose architecture for implantable BCIs.
HALO enables tasks such as treatment of disorders (e.g.,
epilepsy, movement disorders), and records/processes data for
studies that advance our understanding of the brain. We use
electrophysiological data from the motor cortex of a non-human
primate to determine how to decompose HALOâ€™s computational
capabilities into hardware building blocks. We simplify, prune,
and share these building blocks to judiciously use available hardware resources while enabling many modes of brain-computer
interaction. The result is a configurable heterogeneous array of
hardware processing elements (PEs). The PEs are configured
by a low-power RISC-V micro-controller into signal processing
pipelines that meet the target performance and power constraints
necessary to deploy HALO widely and safely.
I. INTRODUCTION
Brain-computer interfaces (BCIs) can treat neurological diseases, shed light on our understanding of the brain, and enable
new brain-computer interactions [27, 42, 54, 77]. Researchers
have already demonstrated BCIs that can control prostheses,
treat neurological disorders (e.g., epilepsy, Parkinsonâ€™s disease,
anxiety, and schizophrenia), and navigate augmented realities [33, 38, 46, 52, 54, 55, 81, 82, 112, 115, 121, 122].
Many BCIs are realized as headsets or electrodes placed
on the scalp, and use electromagnetic signals emanating over
* Joint first authors who have contributed to this work equally. Authors
are listed in alphabetical order of last name.
the skull from biological neurons to deduce brain activity [42, 54, 77]. While these devices do not require surgical deployment, the signals they collect are noisy and lowresolution, making them less ideal as a source of control
signal for forward-looking BCI applications [34, 41, 74, 86].
In these cases, a better alternativeâ€”and the focus of our
studyâ€”is to surgically embed BCIs directly on, around [111],
and in the brain tissue [15]. Such proximity enables implantable BCIs to record from and stimulate large numbers of
neurons with high signal fidelity, spatial resolution, and in real
time [104]. Consequently, implantable BCIs are already being
used by over 160K patients worldwide [71] and are actively
being developed by companies like Kernel [6], Longeviti [7],
Neuropace [12], Medtronic [14], and Neuralink [74]. The
question of how to build low-power hardware for on-board
processing is critical to the success of these devices.
BCIs targeting large numbers of neurons have thus far been
realized with custom ASICs that treat only certain diseases
or perform specific tasks in specific brain regions. Flexible
hardware that supports multiple tasks and treats multiple
diseases is needed for wider BCI adoption. However, the few
programmable BCIs that have been built to date process only
a limited number of neurons while meeting the low-power
requirements necessary for implantation in the brain [3, 4].
Table I shows such limitations for a set of cutting-edge commercial and research BCIs from Medtronic [3, 4], Neuropace
[106], and others [23, 56, 84].
In response, we architect BCI hardware sufficiently flexible
to treat multiple disorders and enable many brain interactions, yet also adequately low-power for safe and chronic
implantation in the brain. Our approach, HALO (Hardware
Architecture for LOw-power BCIs), balances the flexibility
of general-purpose processing with the power-efficiency of
specialized hardware. Table I shows that HALO offers:
Safety: FDA, FCC, and IEEE guidelines state that implantable
BCIs must not dissipate more than 15-40mW of power,
391
2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)
978-1-7281-4661-4/20/$31.00 Â©2020 IEEE
DOI 10.1109/ISCA45697.2020.00041
depending on the target brain region [37, 48, 67, 89, 102, 116].
Dissipating more power can heat surrounding brain tissue by
more than 1 â—¦C, causing cellular damage [13, 60, 116, 125].
Flexibility: Different brain regions use different neural circuits
and require different processing algorithms. For example, neuropsychiatric disorders can manifest in the dorsal and orbital
prefrontal cortices, the amygdala, hypothalamus, and ventral
striatum (among others) [20, 24, 61, 92]. Patients with one
neurological disorder often suffer from others; e.g., patients
diagnosed with epilepsy are 8Ã— likelier to develop Alzheimerâ€™s
and 3Ã— likelier to develop Parkinsonâ€™s diseases [57, 96].
Table I shows that commercial devices generally target one
disease, but HALO can be configured to treat any of the
diseases targeted by existing BCIs.1 HALO is also extensible,
enabling support for emerging neural processing algorithms
undergoing active research [24, 32, 39, 80, 90].
Performance: Many BCIs are closed-loop. For example, BCIs
for epilepsy process neuronal signals to predict seizures,
and then electrically stimulate neurons via on-board neurostimulators to mitigate the severity of these seizures. To
be effective, the time between seizure onset and stimulation
must be within tens of milliseconds [66, 106, 117]. Because
such neurological disorders are influenced by structural and
functional networks across brain centers [25, 29, 62], BCIs
must read/stimulate many channels (sensors that interact with
biological neurons) at high resolution and sampling frequency.
It is difficult to build low-power hardware that can process
large data streams in real time. As shown in Table I, Medtronic
and Neuropace devices read/stimulate a handful of channels
with low frequency, while Kassiri et. al. [56] and NURIP [84]
support tens of channels. Emerging Neuralink devices [74]
support thousands of channels, but consume 750mW and are
not safe for chronic implantation. As programs like DARPA
NESD [1] target recording/stimulating millions of channels,
these challenges will be exacerbated. HALO is inspired by
previous approaches, particularly efforts like NURIP, which
implements sophisticated seizure prediction pipelines, and
other similar studies (see Â§VII), but offers higher bandwidth
brain communication in real time at 15mW.
Research contributions: To realize HALO, we first identify
a list of BCI tasks to support in Â§III. This list includes disease
treatment, signal processing, secure transmission of neuronal
data (e.g., compression and encryption of extracellular voltage
streams), and subsumes the capabilities of many cutting-edge
BCI devices. Since BCIs are an active area of research, this
list is not exhaustive. Nevertheless, it offers a viable design
path for us to identify a broader set of tasks needed for a
flexible BCI platform, select functional units with which to
realize them, and then consider how best to integrate them.
To tame a large design space of possible architecture and
integration options, we devise a hardware-software co-design
1We currently envision configuring one task at a time, but HALO can be
easily extended to run as many parallel BCI tasks as can fit within the power
budget. We will investigate this further in future work.
Medtronic Neuropace Aziz Chen Kassiri Neuralink NURIP HALO
[10] [106] [23] [37] [56] [74] [84]
Tasks Supported
Spike Detection Ã— Ã— Ã— Ã— Ã— Ã— Ã—
Compression Ã— Ã—  Ã—Ã— Ã— Ã—
Seizure Prediction Ã—  Ã—   Ã— 
Movement Intent  Ã— Ã—Ã— Ã— Ã— Ã—
Encryption Ã— Ã— Ã— Ã— Ã— Ã— Ã—
Technical Capabilities
Programmable  Limited Ã— Limited  Ã— Limited
Read Channels 4 8 256 4 24 3072 32 96
Stimulation Channels 4 8 0 0 24 0 32 16
Sample Frequency (Hz) 250 250 5K 200 7.2K 18.6K 256 30K
Sample Resolution (bits) 10 10 8 10 - 10 16 16
Safety (<15mW)   Ã—  Ã— 
TABLE I: Medtronic devices support movement intent to
mitigate dystonia, essential tremors, and Parkinsonâ€™s disease.
These devices are flexible but not power-efficient, and have
limited channel counts and reduced spatial/temporal resolution. Furthermore, they are single-task. For example, Aziz
delta compresses and exfiltrates data to enable offline analysis.
Neuropace, NURIP and Chen et. al. implement sophisticated
algorithms and are sufficiently low power for safe use, but
have restricted programmability. Neuralink supports higher
bandwidth communication with the brain but at the cost of
750mW, precluding real-world use in patients for now. HALO
supports the functionality of all other BCIs while offering high
spatial and temporal sensor resolution within 15mW.
Technique Direction Section
Kernel PE Decomposition SWâ†’ HW IV-A
PE Reuse Generalization SWâ†’ HW IV-A
PE Locality Refactoring SWâ† HW IV-A
Spatial Reprogramming SWâ† HW IV-B
Counter Saturation SWâ†” HW IV-B
NoC Route Selection SWâ†’ HW IV-D
TABLE II: Overview of hardware-software co-design techniques used in HALO. Each line indicates design influence
and the paper section that discusses details of the technique.
approach to systematically architect HALO. Standard lowpower design dictates that we realize one accelerator per BCI
task (e.g., compression, seizure prediction, etc.) in the form
of a dedicated ASIC. We refer to this as a monolithic ASIC
design. We show, however, that monolithic ASICs exceed the
15mW power budget permissible for safe BCIs in many cases.
We consequently take an alternate approach, and refactor
the underlying algorithm of the original BCI tasks into distinct
pieces that realize different phases of the algorithm. We refer
to these pieces as kernels, and show that they facilitate design
of ultra-low-power hardware processing elements (or PEs) via
novel hardware-software co-design approaches summarized in
Table II. We round out the design with a low-power RISCV micro-controller to configure PEs into processing pipelines
and support computation for which there are currently no
PEs. The result is an unconventional style of heterogeneity,
where a family of accelerators operates in unrelated clock
domains with ultra-low-power asynchronous circuit-switched
communication. The design has the following properties:
392                    
(a) Each PE operates in its own clock domain at the minimum
frequency to sustain target performance. This reduces power
consumption versus monolithic ASICs. Research questions
(see Â§IV-A) involve identification of kernel boundaries within
BCI tasks â€“ sometimes they are naturally available, but often
they require refactoring of the original algorithm.
(b) PEs enable hardware-software co-design. PE decomposition allows identification of kernels that are common across
BCI tasks. Adding support for generalized PE reuse via
architectural configurability saves area and power (Â§IV-A). PEs
also enable going beyond recent work on spatial programming
[79, 83, 85, 126] and permitting hardware-directed software
refactoring of BCI algorithms (Â§IV-B), decomposition of PEs
into even more power efficient mini-PEs by leveraging data
locality properties of the source algorithm (Â§IV-B), and novel
counter saturation techniques (Â§IV-B). These innovations are
orthogonal and complementary to well-known techniques like
pipelining and reduced precision, which we also use.
(c) PEs enable a simple communication fabric. PE decomposition creates static data-flow routes that allow use of an
ultra-low-power circuit-switched network for inter-PE communication, and eschew the need for conventional power-hungry
packet-switched on-chip networks. The network is made up
of programmable switches that permit the doctor/technician to
configure the PEs to realize different BCI tasks at runtime.
The network is extensible and can accommodate additional
PEs for future BCI tasks.
We evaluate HALO with electrophysiological data collected
in vivo from a non-human primateâ€™s arm and leg motor cortex,
the brain regions responsible for arm and leg movement.
Every task, from closed-loop seizure/tremor mitigation, to
spike detection and extracellular voltage stream compression,
fits under 15mW [89]. HALO achieves 4-57Ã— and 2Ã— lower
power dissipation than software alternatives and monolithic
ASICs respectively.
Overall, this work goes beyond single accelerator design,
and offers a blueprint for ultra-low-power multi-accelerator
SoC design. It also offers a suite of algorithm-to-hardware codesign techniques that are complementary to circuit-level optimizations explored in related work on low-power embedded
systems in wearables and implantable BCIs [59, 64, 84, 93].
Longer-term directions: This work is a first step in a longerterm research project involving several generations of HALO
tape-outs with progressively more complete implementation.
At the time of writing this paper, we have performed functional
validation, validation of synthesized design, and physical synthesis (multi-corner/multi-mode) to achieve timing and power
closure with a margin for all HALO components. In addition,
we have taped out a modified RISC-V core with all the hooks
necessary to interface with the rest of the architecture. We will
add components one step at a time in incremental tape-outs to
manage design risk. Figure 1 shows a layout diagram of the
first chip tape-out, which we have submitted for fabrication in
a 28nm technology.
Fig. 1: On the left is a circuit diagram of our first HALO tapeout in a 28nm technology. On the right is an implantable BCI.
BCIs have form factors under 1cm2 and are placed on brain
tissue, where their sensors probe millimeters into the tissue.
The devices can be packaged in a hermetically-fused silica
capsule or titanium capsule.
II. BACKGROUND
On the right in Figure 1, we show an example implantable
BCI. Building blocks beyond processing hardware include:
1 Sensors: BCIs use sensors ranging from single electrodes
for individual neurons to arrays of hundreds of microelectrodes, which record and stimulate 5-10 neurons individually
and several hundred in total [22, 23, 58]. Going forward, sensors will record from an ever-increasing number of biological
neurons. For example, widely-used Utah arrays already integrate up to 256 microelectrode channels [16, 22]. Although not
immediately practical, approaches like Neuralinkâ€™s â€œthreadsâ€
and DARPA NESD performers are targeting thousands to
millions of channels [1, 74].
2 Analog front-end: The analog data recorded from the
sensors must be amplified and digitized via analog-to-digital
converters (ADCs). Different BCIs use ADCs with different
sample resolution and frequency, but 8-16 bits per sample at
20-50 KHz are common [68, 74, 88, 89].
3 Communication links: BCIs use RF links that vary from
the low MHz to GHz range, with 2.4GHz being typical [119].
Since RF deposition heats up brain tissue, the FCC and FDA
limit the specific absorption rate to 1.6W/kg over 1g of tissue,
and 1W/kg over 10g of tissue, respectively [13, 94, 110, 125].
Therefore we aim for power budgets of 15mW, while also
minimizing radio transmission power.
4 Power sources: BCIs are typically powered by single-use
non-rechargeable batteries, rechargeable batteries, or inductive
power transfer. All must be judicious with power. Nonrechargeable batteries require service lifetimes of 12-15 years,
as they require surgery for replacement [3, 26]. Rechargeable
batteries and inductive powering both require transcutaneous
wireless powering [43, 45, 67, 119], and must reduce the
transferred power so as to prevent excessive heating.
HALO must be compatible with any variant of 1 -4 and
must meet two goals to be widely usable: none of the BCI tasks
should exceed 15mW, and RF transmission bandwidth should
be minimized to mitigate power deposited in brain tissue.
393      
III. LIST OF SUPPORTED BCI TASKS
We support many BCI tasks, some of which treat neurological disorders via closed-loop operation, and others that reduce
radio transmission via compression of neuronal activity:
1 Seizure prediction: Implantable BCIs used to treat epilepsy
predict seizure onset from neuronal firing patterns and, if a
seizure is predicted, electrically stimulate biological neurons
in certain brain regions [14, 106]. Electrical stimulation breaks
feedback loops in the neural circuits responsible for seizures,
thereby mitigating seizure severity [14, 37, 56, 106]. Seizure
prediction is an active area of research in the neuroscience
community. State-of-the-art seizure prediction algorithms use
FFTs, cross-correlation, and bandpass filters over a linear
model. We implement an algorithm that combines all three
complementary approaches in HALO [99]. As such, it is
an exemplar of other closed-loop BCI algorithms used to
treat major depressive disorder, psychosis, and obsessivecompulsive disorder [66, 117].
2 Movement intent: For individuals with essential tremor,
Parkinsonâ€™s disease, and other movement disorders, therapeutic stimulation of the motor cortex can relieve symptoms [49].
Implantable BCIs can continually stimulate the brain, but this
wastes energy when the affected limb is unused, and can lead
to medical side effects [2, 49]. A better option is to stimulate
brain tissue when neuronal firing indicates use of the affected
limb [49]. Similarly, for paralyzed individuals, neuronal signals can be decoded to determine how to control prostheses [35, 107, 114]. Such approaches have been demonstrated
on non-human primates [35], and require millisecond latency
processing between detection of movement and stimulation of
the brain [113]. State-of-the-art algorithms exploit the fact that
movement intent is correlated with drops in neuronal firing in
the 14-25Hz band in the motor cortex region, which can be
detected using an FFT [49, 108].
3 Compression: Compression reduces radio transmission, and
is useful for high-bandwidth brain interaction [23, 28, 118].
One may consider using lossy compression, but the brain is not
understood well enough to identify what portions of the electrophysiological data can be safely discarded [98]. Apart from
some specific and well-understood forms of lossy compression
â€“ including spike detection, which we discuss subsequently â€“
lossless compression is more widely used and palatable to the
neuroscience community today [91]. In HALO, we support
several lossless compression schemes, as their compression
ratios and power consumption can vary depending on brain
region and patient activity. We support well-known LZ4 [8]
and LZMA [9] compression, as well as a custom-built discrete
wavelet transform (DWT) [44] compression. In Â§VI, we show
that compression ratios vary by as much as 40% depending
on compression algorithm and target brain region.
4 Spike detection: This is the first step in spike sorting
pipelines that extract activity of specific neurons from the
recorded signal. Spike sorting is performed on an external
system, but we include spike detection on the BCI as it sends
only the parts of the signal that contain a detected spike,
effectively compressing transmitted data. Due to the relative
rarity of spikes, spike detection lowers signal transmission
bandwidth by orders of magnitude [44], reducing both device
power and power deposited on the brain tissue by the radio.
Spike detection is typically implemented using a non-linear
energy operator (NEO) or using DWT [44].
5 Encryption: Although current state-of-art devices do not
currently support encryption, we foresee it to be necessary for
future BCIs in order to protect patient data during exfiltration
off the device. HIPAA, NIST, and NSA require using AES
with an encryption key of at least 128 bits [5, 11, 109].
IV. HARDWARE-SOFTWARE CO-DESIGN
Â§III describes five tasks, two of which are realizable in multiple ways. Compression can be achieved with LZ4, LZMA,
and DWT, while spike detection can be achieved with NEO
and DWT. Hence, HALO can be configured by a doctor/technician at runtime into one of eight distinct pipelines.
With conventional monolithic ASICs, we would implement
8 ASICs, one per task. Instead, HALO supports these tasks
via PEs that realize distinct processing kernels, as shown in
Figure 2. Each PE operates at a frequency catered to its
specific computational needs and includes processing logic,
private memory, and an adapter to communicate over the
interconnect. While there may be benefits to giving PEs access
to a global memory via caches, we leave this for future studies.
Decomposing BCI tasks into PEs lets us determine dataflow and enables an ultra-low-power on-chip circuit-switched
network for asynchronous inter-PE communication rather than
a conventional power-hungry packet-switched network. Many
of our PEs, like LZ and FFT, require computational resources
that scale with the number of sensor channels, increasing
power/area usage. To address this, we implement a standalone
interleaver that buffers and rearranges data so that these PEs
can be time-multiplexed to operate on a single channel at
a time. We complete HALO by integrating a low-power
micro-controller, which assembles the PEs into pipelines that
realize the BCI tasks via programmable circuit switches in the
network and runs algorithms for which there are no PEs yet.
HALOâ€™s PE-centric approach eschews the need for a global
clock or phase-locked loops. Furthermore, bundling computational kernels within PEs with private memory means that
HALO is naturally modular and extensible. As we learn more
about the brainâ€™s function, we may want to support more BCI
tasks and our architecture will naturally permit insertion of
additional PEs for emerging neuroscientific algorithms.
A. PE Decomposition
The process of decomposing BCI algorithms into PEs
varies in complexity, depending on how clearly separated the
algorithmic phases are. Importantly, PE decomposition must
not change algorithmic functionality; i.e., there should be no
change in algorithmic accuracy, compression ratio, etc.
394     
Fig. 2: ADCs digitize the analog neuronal signals and feed them into processing logic, which consists of low-power hardware
PEs and a RISC-V micro-controller. PEs are configured into pipelines to realize BCI tasks, ranging from compression (in blue)
to spike detection (in green). Optional PEs (e.g., AES encryption) are shown in square brackets. PEs operating in parallel (e.g.,
FFT, XCOR, and BBF in the seizure prediction pipeline) are shown in curly brackets. All BCI tasks are under 15mW.
Kernel PE decomposition: Some BCI tasks consist of distinct
computational kernels naturally amenable to PE decomposition. For example, seizure prediction combines kernels for
Fourier transform (FFT), cross-correlation (XCOR), Butterworth Bandpass Filtering (BBF), and a support vector machine
(SVM). We realize each as a PE as shown in Figure 2. As
FFT, XCOR, and BBF have no data dependencies, they can
operate in parallel. This approach saves power because XCOR
contains complex computation (e.g., divisions, square roots)
that scales quadratically with channel count. In contrast, BBF
is a simple filter with minimal arithmetic that scales linearly
with channel count. Separating XCOR and BBF into separate
PEs ensures that BBFâ€™s filtering logic is clocked over an order
of magnitude slower than the logic for cross-correlation.
PE reuse generalization: Many BCI tasks use popular computational kernels in slightly different variants. To exploit this,
we take inspiration from functional unit sharing from CPU
microarchitecture to develop configurable PEs that can be
shared among BCI tasks. Consider, for example, movement
intent, which can be decomposed into FFT, followed by logic
that checks whether the FFT output is in a particular spectral
range. We create a threshold PE (or THR) to determine when a
PEs output is within a specified numerical range (see Figure 2)
and enable sharing of the FFT between movement intent and
seizure prediction tasks. The FFT PE is configurable because
movement intent requires 14-25-point FFTs to detect drops
in signal power, while seizure prediction requires 1024-point
FFTs [49, 99, 108]. We find the increased complexity of
configurable PEs to be worth the cost (see Â§VI), as it enables
reuse and provides a more versatile platform.
As another example, consider spike detection, which can be
implemented with either nonlinear-energy operator (NEO) or
DWT [44]. In either case, the output is fed to the THR PE,
permitting reuse of THR with movement intent. Moreover, the
DWT PE can be shared with one of the compression pipelines,
as shown in Figure 2. Like the FFT PE, the DWT PE must
be configurable to permit sharing, because spike detection
requires recursive applications of DWT (usually three, four,
or five times [44]), while compression requires only one.
Figure 2 shows that we also share logic for the LempelZiv pattern search in a single PE for the LZMA and LZ4
compression tasks. To enable sharing of LZ, we perform intraPE optimizations, as discussed in the next section.
Algorithm 1 LZMA pseudocode
1: function LZMA COMPRESS BLOCK(input)
2: output = list(lzma header);
3: while data = input.get() do
4: best match = f ind best match(data);
5: P robmatch = count(tablematch, best match)
6: /count total(tablematch);
7: r1 = range encode(P robmatch);
8: output.push back(r1);
9: increment counter(tablematch, best match);
10: end while
11: return output;
12: end function
Major refactoring: PE decomposition can, in many cases,
be more complicated and require significant refactoring of
the original algorithm. Consider, for example, LZMA and
DWTMA compression. Both algorithms use Markov (MA)
chains to calculate the probability of the current input value
based on observed history, which is used to pick more efficient
encoding of the input signal. We found that using the combined
MA PE overshoots the 15mW power budget. To solve this
problem, we refactored the original algorithm to make it
more amenable for PE decomposition. To separate algorithmic
phases, we realize that data locality (i.e. following routines that
manipulate major data structures) is a good indicator of kernel
boundaries within programs. This observation is tied to the
fact that PEs in HALO have only local memories and cannot
share large amounts of data. Locality refactoring highlights
how design decisions about the architecture (i.e., use of PElocal memories) guided refactoring of our algorithms.
Algorithm 1 and Figure 3 demonstrates how we use this
insight to change LZMA. The second half of this algorithm
can be separated into probability calculations and frequency
information updates centered around the maintenance of the
core MA data structure, the frequency table (in green), as well
as efficient encoding (in blue). Figure 3 shows a simplified
block diagram of MA hardware before and after algorithmic
395
 
 
		


		











Fig. 3: (Left) Hardware for the initial version of MA, with
input from LZ (see Algorithm 1). Hardware corresponding
to the lines of code from the algorithm are indicated by the
numbers in the diagram. The first and last steps in MA share
the symbol table. Output is produced by the second block,
after which control passes to the third block to maintain data
structures. (Right) After refactoring, MA can be split into PEs
that realize the new MA (at the top), and RC (at the bottom).
refactoring. The key memory structures, the frequency table
and the encoder state, are again shown in green and blue
respectively, matching their color coding in Algorithm 1.
We then refactor the algorithm to bring together phases that
operate on the same data structures, allowing us to separate the
PEs since they can now operate independently with minimal
data movement. This permits clocking each component at
significantly lower frequency, leading to power savings of 2Ã—.
B. Processing Element Optimizations
Once PEs are identified, they offer additional opportunities
for hardware-algorithm co-design. Many of the optimizations
do not change the functional behavior of the algorithm, but
others do modify the output.
Unchanged PE output: Some of the PEs (e.g., XCOR, LZ)
process data in blocks instead of samples. These PEs must wait
for all inputs in the block to arrive. When all inputs arrive,
computation occurs in a burst, and an output is produced. This
type of bursty computation is problematic as it requires either
large buffers to sink the bursts, or high PE frequency to meet
data rates while sustaining periods of bursty activity. Neither
option is ideal from the perspective of saving power.
In response, we note that recent work on spatial programming [79, 83, 85, 126] devised techniques to tolerate such
bursty activity. While the original work focuses on hardware
support to overcome this problem, we extend this work to
spatially reprogram the original algorithm and co-design it
with the hardware to achieve power improvements.
As an example, consider the XCOR PE. The original
algorithm, shown in Algorithm 2, performs computation at the
Algorithm 2 XCOR naive implementation
1: function XCOR(input, output)
2: // channel[][] stores input in appropriate channel location
3: channel[channel num][sample num] = input
4: // Calculate correlation
5: if channel.f illed() then
6: for each i, j âˆˆ channels do
7: data i = 0
8: for each data âˆˆ channel[i] do
9: data i+ = data
10: end for
11: data j = 0
12: for k âˆˆ [LAG, SIZE] do
13: data j+ = channel[j][k]
14: end for
15: avg i = data i/SIZE
16: avg j = data j/SIZE
17: output.push back(avg i, avg j)
18: end for
19: return output;
20: end if
21: end function
end once all data has been filled into the block. We refactor the
algorithm to process inputs as early as they are available. The
final form in Algorithm 3 reduces the amount of computation
needed in the final step, as well as the number of buffers
needed to store the inputs. This translates to a power savings of
2.2Ã— over the original algorithm. This technique also extends
to other PEs like LZ to achieve 1.5Ã— power reduction.
Additionally, HALO benefits from application of known
architectural techniques to save power. These include pipelining, parallelizing computation with additional hardware, and
more. For example, we use pipelining optimizations to reduce
frequencies for PEs like XCOR, NEO, BBF, and SVM.
Consider, for example, the XCOR PE. XCOR calculates the
cross-correlation of pairs of channels using three inner loops
for computing means, sums, and square roots, each of which is
individually amenable to pipelining. We note that pipelining a
circuit does not reduce the power it dissipates per se. However,
reducing the critical path of a circuit enables us to utilize the
available timing slack for gate downsizing and voltage scaling,
which can enable substantial power savings. Pipelining XCOR
in this manner saves 1.4Ã— power.
Finally, LZ and MA PEs require initialization of data structures at the beginning of every compressed block. We found
that dedicated circuits are necessary to meet the 15mW budget.
These circuits use only combinational logic. For example,
the circuit used in MA contains a few inverters and AND
gates per input bit. Using these circuits instead of standalone
initialization phase reduces PE power consumption by 1.8Ã—.
Modified PE output: Although initialization circuits decrease
the direct power/performance cost of starting a new compression block, there is also an indirect cost of using uninitialized
internal structures, which leads to lower compression rates.
This presents a problem with respect to the choice of block
size. On one hand, large block sizes lead to better estimates
396                        
of frequencies, and therefore better compression ratios, which
ultimately saves radio transmission power. On the other hand,
small block sizes allow the use of smaller data types and
reduce the memory footprint and power of the MA PE.
A traditional approach would aim to balance power/compression ratio for an ideal design. However, such an approach
does not find a design point that fits within the constrained
power budget. Instead we observe that the frequencies of
values within a block remain largely unchanged after they have
stabilized. Consequently, we allow the frequency counters to
saturate and set block size independently of counter bit width.
Overall, counter saturation modification allows HALO to
benefit both from reduced memory footprint of 16 bit counters,
and better compression ratio of larger blocks. It is also yet
another example of co-design where software modification can
complement hardware design. Like our other software refactoring techniques, counter saturation ensures that no data is lost.
In other words, compression ratio may decrease (marginally,
as we find) but the data can still be correctly decoded because
the frequencies used to guide encoding schemes do not affect
the accuracy of what is encoded. We explore the effect of
changing the block size on power and compression ratio in
Â§VI-D.
HALO also benefits from application of known architectural techniques that trade data precision for power improvement. Where possible, we use fixed-point rather than floating
point computation. We also reduce bit width for fixed-point
integers. Although some of the signal processing algorithms
that we study use 32-bit integers in the original studies [99],
such high-resolution representation is often unnecessary and
can be reduced to save power without significantly impacting
accuracy. Knowing the limits of signal data, we replace
floating point arithmetic with fixed point arithmetic in the BBF
PE and achieve an order of magnitude reduction in power,
with only < 0.1% increase in relative error. When using fixed
point, reducing RCâ€™s 32-bit integers to 16-bit integers saved
Algorithm 3 XCOR spatial programming refactoring
1: function XCOR(input, output)
2: // channel[][] stores input in appropriate channel location
3: channel[channel num][sample num] = input
4: // data[] stores sums of input received so far
5: data[count]+ = input
6: // data lag[] stores sums of input till LAG
7: if count 2 == LAG then
8: data lag[count] = data[count]
9: end if
10: // Finish correlation computation
11: if channel.f illed() then
12: for each i, j âˆˆ channels do
13: avg i = data[i]/SIZE
14: avg j = (data[j] âˆ’ data lag[j])/SIZE
15: output.push back(avg i, avg j)
16: end for
17: return output
18: end if
19: end function
PE power by 1.6Ã— with no change in accuracy.
C. Processing Element Summary
Table III lists HALOâ€™s PEs. We offer partial PE parameterization to enable sharing (see Â§IV-A) and to personalize the
algorithm to patients. As an example, recent studies show that
it is possible to modify the number of weights and values in
the SVM PE to improve seizure prediction accuracy [19, 100].
To permit such personalization, we expose as many as 5000
weights while remaining within the power budget.
PE parameters influence the total PE memory capacities. Table III quantifies the upper bound of these sizes. For example,
the LZ PEâ€™s memory size is determined by finding matches of
the current byte sequence in its history. The doctor/technician
can reduce history size via the micro-controller if desired. In
such cases, we power-gate unused memory banks.
D. On-Chip Network
We clock each PE at the lowest frequency needed to
meet data processing rates, and synthesize with established
synchronous design flows (see Sec. V). Local (intra-PE)
synchronization is based on per-PE pausable clock generators
and clock control units [123]. The clock generators use ring
oscillators with a delay line which is extracted from the critical
path. The ring oscillator is designed so that its frequency
variation tracks the critical path [73]. One might expect ring
oscillator-based clocking to inhibit circuit performance due to
increased clock uncertainty. This is not the case in HALO
because of the low operating frequency of PEs with respect to
the achievable performance of our target process node. All PEs
were synthesized with hundreds of pico-seconds of positive
timing slack, minimizing â€“ if not completely negating â€“ the
impact of increase in clock uncertainty.
While running PEs in separate clock domains saves power, it
can potentially complicate design of inter-PE communication.
Prior work on globally asynchronous locally synchronous
(GALS) architectures [36, 63, 72] encountered these issues for
packet-switched on-chip networks. Unfortunately, we cannot
re-purpose their solutions as our analysis with the DSENT
tool [105] estimates that a simple packet-switched mesh
network consumes over 50mW, well over our 15mW power
budget. Instead, we co-design inter-PE communication with
the BCI algorithms. The decomposition of BCI tasks into
kernels creates static and well-defined data-flows between PEs.
NoC route selection allows replacement of a packet-switched
network to a far lower-power circuit-switched network built
on an asynchronous communication fabric [30, 75, 76]. It also
enables HALO to accommodate new algorithms in the future
by simply plugging in new PEs.
Our network uses asynchronous SEND-ACK communication over an 8-bit data bus. The receiver ACKs once it has received input and is ready to receive new data. An interconnect
wrapper provides a FIFO interface for the input and output
of each PE. Configurable switches assemble the interconnect
so that it realizes our target pipelines. Routing is similar to
FPGAs (i.e., we fix the routes in the network but allow the
397
PE Functionality Parameters
LZ Lempel-Ziv match length-offset pair search. Hashes four input bytes to index into first
array of hash-chain, which records position of previous instance of the same data.
Indexes second array of hash-chain using this value and find distance to previous
occurrence of data.
History length, H [256-4096B]
First array size is 8KB
Second array size is 2Ã—H bytes
Max memory size is 24KB
LIC Encodes LZ output with linear integer coding. 256-byte array stores literals (bytes with
no previous matches). Literals are output on matches and identified with headers/lengths.
N/A
MA Receives data to encode from LZ and DWT. Maintains counters for each input type
(literal, length, offset in LZ and predict, updates in DWT) in a Fenwick tree. Counter
lookups and increments are O(logN). Emits counter values to RC, for each input.
History length, H [256-4096B]
Literal counter 256 bytes; length/offset
counters 2Ã—H bytes; max memory 16.25 KB
RC Encodes data using range encoding with the probability information from MA N/A
DWT Discrete Wavelet Transform, used in spike detection [44] and compression. Levels [1-5]
NEO Non-linear energy operator, which estimates the energy content of a signal using
techniques described in prior work [44].
N/A
FFT Fast Fourier Transform of channels. FFT points [up to 1024]
XCOR Accepts list of channel numbers (i.e., channel map) for which pair-wise cross-correlation
is calculated. Uses input parameter LAG to control the delay between the two channels.
LAG [0-64]
User-defined channel map
BBF Butterworth bandpass filter identifies frequency bands correlated to seizures. Frequencies up to ADC Nyquist limit
SVM Uses outputs of FFT, BBF, and XCOR to predict seizure onset. Multiplies input values
and weights to perform classification.
Up to 5000 32-bit user-defined integer weights
THR Emits a set bit if input is below threshold User-defined threshold value (32-bit)
GATE Passes one input stream based on the value of the second input line (provided by THR). N/A
AES AES-128 bit encryption in ECB mode Encryption key [128-bit]
TABLE III: Description of PEs, their key data structures, and interactions with one another. End-users can parameterize key
attributes (e.g., the history length of LZ can be made between 256 and 4096 bytes), and show impact on memory and logic.
links to be configurable), but simplified because only a small
set of connectivity patterns need to be supported, and because
we route data buses rather than independent bits. Switches are
implemented with programmable muxes/demuxes.
We use per-PE FIFO buffers as logical adapters to transfer
data from the network into the form expected by the PE. The
adapter also modifies the output created by the PE to match the
fixed width interface of the interconnect. HALOâ€™s interconnect
sends messages in streams of bytes, bits, and tokens (packets
of multiple values). Naturally, when configuring PEs, the
programmer must ensure that the output interface of a PE
matches the input interface of its target PE. In practice,
this gives HALO a wide configuration space and allows
doctors/technicians to construct many pipelines at runtime.
E. RISC-V Micro-controller
We use a low-power micro-controller on HALO to configure the PEs and support computation not currently within PEs.
We use RISC-V though any micro-controller is suitable.
1 Pipeline configuration: The micro-controller assembles PEs
into pipelines by configuring the programmable switches in
software. We use instructions to write to general purpose IO
pins that set the switches dynamically. The output interface of
source PEs must match the input interface of target PEs.
2 PE configuration: The micro-controller configures the PE
parameters from Table III. Each PE maintains parameter variables in internal memory accessible by the micro-controller.
3 Closed-loop support: The micro-controller can configure
interconnect switches so that it can receive and operate on the
result of any PE. This is particularly useful for closed-loop
recording/stimulation scenarios. For example, when a seizure
is predicted, the micro-controller can set the microelectrode
array to stimulate neurons. Stimulation logic is suitable for
software execution because it occurs rarely and requires more
complex decision-making (i.e., personalization of length, frequency, and amplitude of stimulation pulses to the patient) than
might be appropriate for a hardware implementation. With
HALO, we can stimulate as many as 16 channels under the
power budget, whereas commercial BCIs today only stimulate
4-8 channels [10, 21, 69, 78, 124].
4 Safe operation: HALO realizes ultra-low power Vdd comparator circuits, running at low frequencies, to identify power
overshoot. On overshoot, this circuit interrupts the microcontroller, allowing it to shut off PEs to reduce overall power.
The micro-controller must be used with care as it consumes
more power than the PEs. It is, however, well-suited for
low-intensity tasks at low data rates. HALO runs the microcontroller at a low frequency (25MHz) with a small amount
of memory (64Kb). Even with scarce compute and memory
resources, micro-controllers can perform complex communication and control services and boot real-time OSes [65].
V. METHODOLOGY
A. Target Design
HALO can operate with all sensor, ADC, amplifier, and
radio technologies. For our evaluation, we assume a microelectrode array with 96 channels, each of which records the
activity of groups of neurons (i.e., 5-10) in their vicinity. We
allow 2Ã— more simultaneous stimulation channels (16) than
commercial designs [10, 21, 69, 78, 124]. This translates to a
0.48mW upper bound for chronic stimulation [10], which is
used in the movement intent and seizure prediction pipelines.
Additionally, we assume that each sample is encoded in 16
bits at a frequency of 30KHz, on par with recent work
on BCIs [18, 88]. This results in a real-time data rate of
~46Mbps. Finally, we assume a radio with an operating energy
of 200pJ/bit, similar to current implantable BCIs [70]. We
consider a strict power budget of 15mW from the range seen in
398    
state of the art BCIs [37, 48, 67, 89, 102, 116]. Commercial
ADCs achieve 1mW per 1Msps sampling rate [97]. In line
with this, we dedicate 3mW power to ADCs and amplifiers.
All of HALOâ€™s processing pipelines, including the radio, must
therefore consume no more than 12mW of power.
We use LZ and MA PEs with 4KB of history, 256-entry
byte arrays for the literals in LIC, and 16-bit divides in
RC. We use a 5000-weight SVM PE, and a 1024-point FFT.
Finally, we integrate a 2-stage in-order 32-bit Ibex RISC-V
core (formerly known as Zero-Riscy [40]) with the RV32EC
ISA â€” an embedded (or reduced) version of RV32I with
16 general-purpose registers, and a â€œcompressionâ€ feature to
reduce memory requirements for storing programs (this feature
is used commonly for low-power embedded devices). We fully
synthesize and test the RISC-V core using our commercial
synthesis flow (see Â§ V-B).
B. Hardware Evaluations
We design and test all of HALOâ€™s components using
a commercial 28nm fully-depleted silicon-on-insulator (FDSOI) CMOS process. Synthesis and power analysis is performed using the latest generation of CadenceÂ® synthesis tools
with standard cell libraries from STMicroelectronics. Memories were generated using foundry-supplied memory macros.
Relying on commercial IP (instead of academic or predictive
tools) means that our power numbers are more representative
of real fabricated chips. We run multi-corner, physically-aware
synthesis to cover all process and environmental variation
corners. To err on the conservative side, we present results for
the worst variation corner. Since our design is power-limited,
we define this corner at TrFF, VddMAX, and RCBEST, at 1V
Vdd. While scaling voltage down can further reduce power,
we focus on the improvements due to the architectural design
rather than circuit-level optimizations. Adhering to HALOâ€™s
strict thermal constraints, our standard cell and macro libraries
have been characterized at (or interpolated to) 40 â—¦C.
We compare HALO PEs against the power expended by
running software versions of our PEs on the RISC-V microcontroller. To do this, we combine our hardware evaluation
flow with a custom memory profiler that determines the runtime memory requirements of our target software. We simulate
our software kernels in behavioral RTL to quantify these memory requirements along with the minimum required frequency
necessary to meet the real-time performance requirements of
the kernel. Subsequently, we synthesize the RISC-V core with
the minimum frequency as a constraint and re-simulate the
gate-level RTL to extract annotated switching activity factors
for all gates. We then use the netlist and annotated activity
factors to extract accurate power numbers for logic, and
introduce memory activity factors into the memory compiler.
Note that the same set of steps is used to measure PE power.
We use our floorplan data to estimate an upper bound on
interconnect/switch power. As reported in prior work [63],
such interconnects require relatively few gates (e.g., 0.55 kilogate equivalents) and have < 1% impact on power. We use the
floorplan to assess input/output adapter overhead, upper bound
Spike Det (NEO)
Spike Det (DWT)
Compr (LZ4)
Compr (LZMA)
Compr (DWTMA)
Move Intent
Seizure Pred
Encrypt (Raw)
101
102
Total Power (mW)
64 Core 64 Core
8 Core
32 Core
64 core 16 Core
64 Core
16 Core
12mW budget
RISC-V
HALO
Monolithic-ASIC
HALO-no-NoC
Fig. 4: Power (in log-scale) of PEs, control logic and radios
for HALO versus RISC-V and monolithic ASICs. To meet
the 15mW device power budget, these components (without
ADCs and amplifiers) need to be under 12mW (the red line).
We compare HALO against the lowest-power RISC-V and
HALO-no-NoC, which shows how much power would be
saved if HALOâ€™s configurability were sacrificed.
on routing distance, and wire capacitance. The upper bound
on total interconnect and switch power is < 300Î¼W. We base
these estimates on our experience with designing/fabricating
multiple generations of FPGAs in many technologies, including 28nm.
C. In-Vivo Electrophysiology from Non-Human Primate
We use electrophysiological data collected from the brain of
a non-human primate. Microelectrode arrays were implanted
in two locations in the motor cortex, corresponding to the
left upper and lower limbs. The arrays were connected to a
CereplexWTM head stage [18] for communication and data
transmission to a CerebusTM data acquisition system and signal
processor [17]. Multiple antennas were used to accommodate
free movement of the animal. We use recordings of brain
activity while the animal performed tasks such as walking on
a treadmill, reaching for a treat, and overcoming a moving
styrofoam obstacle. All research protocols were approved
and monitored by Brown Universityâ€™s Institutional Animal
Care and Use Committee, and all research was performed in
accordance with relevant guidelines and regulations.
VI. EVALUATION
A. Power Analysis of Frequently-Used Tasks
Figure 4 compares HALOâ€™s power versus ASICs and software alternatives on RISC-V. We focus on the power consumed
by the processing logic and radio, rather than the amplifier array and ADCs. Software tasks can execute on microcontroller cores in both single-core and multi-core designs,
where we divide the 96 channel data streams and operate
on them in parallel. We study 1-64 RISC-V core counts, in
powers of two and report the best configuration per task. We
399
PE Freq
(MHz)
Logic (mW) Mem (mW) Total
(mW)
Area
Leak Dyn Leak Dyn (KGE)
LZ 129 0.055 1.455 0.095 1.466 3.071 55
LIC 22.5 0.057 0.267 0.006 0.046 0.376 25
MA 92 0.127 2.148 0.067 0.997 3.339 66
RC 90 0.029 0.763 0 0 0.792 12
DWT 3 0.004 0.002 0 0 0.006 2
NEO 3 0.012 0.003 0 0 0.015 5
FFT 15.7 0.057 0.509 0.085 0.356 1.007 22
XCOR 85 0.07 4.182 0.307 0.053 4.612 81
BBF 6 0.066 0.034 0 0 0.1 23
SVM 3 0.018 0.018 0.081 0.033 0.15 8
THR 16 0.002 0.011 0 0 0.013 1
GATE 5 0.003 0.006 0.067 0.054 0.13 17
AES 5 0.053 0.059 0 0 0.112 34
Tasks
Compr (LZ4) 0.112 1.722 0.101 1.512 3.447 80
Compr (LZMA) 0.211 4.366 0.122 2.463 7.162 133
Compr (DWTMA) 0.16 2.913 0.0123 0.33 3.415 80
Seizure Prediction 0.216 4.760 0.54 0.496 6.012 111
Spike Det (NEO) 0.017 0.02 0.067 0.054 0.158 24
Spike Det (DWT) 0.009 0.019 0.067 0.054 0.149 20
Movement Intent 0.062 0.526 0.152 0.41 1.15 40
Encrypt (Raw) 0.053 0.059 0 0 0.112 34
RISC-V Controlest 25 0.341 0.137 0.248 1.080 1.800 70
TABLE IV: PE power (leakage/dynamic for logic/memory
separated), frequency, and area (in kilo-gate equivalents or
KGEs). All numbers assume data processing rates of 46Mbps.
also show an idealized version of HALO where the on-chip
interconnect is removed to quantify the power penalty for the
configurability that the network offers. Both HALO variants
use the optimizations from Â§IV-B. HALO uses less power
than monolithic ASICs and RISC-V approaches. Moreover, the
inclusion of the low-power circuit-switched network consumes
marginally more power than HALO-no-NoC.
B. Power Analysis of Processing Elements
Per-PE power consumption is detailed in Table IV. For each
PE, we quantify the operating frequency needed to process
the full stream of neuronal data at 46Mbps. We separate logic
and memory power into static and dynamic components, and
also show overall area. As the majority of frequently-used
tasks use multiple PEs to form a processing pipeline, we also
present combined numbers for all PEs that form pipelines. The
full system power consumption combines the reported power
consumption with that of auxiliary circuits such as ADCs,
radio, interconnect, etc. We also consider the power needed to
run a single RISC-V core to handle low frequency tasks such
as neuronal stimulation and the communication interface.
Table IV shows that the compression and seizure prediction
pipelines consume the most power, but are still within the
power budget. In general, the higher the operating frequency,
the higher the dynamic power. Furthermore, as expected, PEs
that use more memory (e.g., LZ, XCOR, MA, etc.) also expend
more dynamic and static power on the memory component.
Figure 5 quantifies the PEs in Table IV per processing
pipeline. On the left, we show the total power expended
by the processing pipeline including radio and interconnect,
discounting ADCs and amplifiers, and separate the processing
contributions into the various PEs (in blue), and on the RISC-V
core (in green) that is used to perform tasks like microelectrode
array stimulation control for movement intent and seizure preSpike Det (NEO)
Spike Det (DWT)
Compr (LZ4)
Compr (LZMA)
Compr (DWTMA)
Move Intent
Seizure Pred
Encrypt (Raw)
0
20
40
60
80
100
Percentage
Logic Leak
Logic Dyn
Mem Leak
Mem Dyn
Spike Det (NEO)
Spike Det (DWT)
Compr (LZ4)
Compr (LZMA)
Compr (DWTMA)
Move Intent
Seizure Pred
Encrypt (Raw)
0
2
4
6
8
10
12
14
Power (mW)
NEO
GATE+THR
DWT
GATE+THR
LZ
LIC
LZ
MA
RC
DWT
MA
RC
FFT
GATE+THR
FFT
XCOR BBF
SVM
GATE+THR
AES
12mW
Component
NoC+interleaver
Stimulation
Radio
Control
Fig. 5: (Left) Power of each task pipelineâ€™s PEs, stimulation,
network and interleaver and control. Control refers to microcontroller power to set up the pipeline. Stimulation refers to
power for chronic neurostimulation. (Right) Percentages of
total power for compute logic and memory, separated into
dynamic and leakage components.
diction. All tasks remain comfortably under the 12mW power
budget for the processing pipeline and radio. As expected,
spike detection tasks expend low power because they are
simple (NEO, DWT, GATE, and THR require few hardware
resources), and use low radio bandwidth. On the other hand,
encrypting the raw data results in higher power consumption
for the radio as it has to transmit the entire raw data stream
from the 96 channels. The dedicated compression schemes
(i.e., LZ4 and LZMA) consume roughly 9-11mW, with varying
amounts going on logic versus the radio. LZMA has a higher
compression factor (see Â§VI-C) and hence requires lower radio
power. However, it requires more computation to achieve this
compression ratio, which is why its logic power is higher.
Finally, power consumed by the network and the interleaver
remains negligible in comparison to the PEs.
The graph on the right in Figure 5 sheds light on the
breakdown of processing pipeline power in terms of dynamic
versus leakage components, for the logic and memory parts
of the PEs. These numbers vary substantially, with techniques
that use little computation (e.g., spike detection) expending
the bulk of their power on memory. Most other tasks require
a balance of power across compute and memory. In cases
where memory accesses are more frequent (e.g., compression
algorithms), dynamic memory power outweighs leakage.
C. Impact of Co-Design Decisions
Figure 6 shows the impact of the hardware-algorithm codesign discussed in Â§IV-B on the XCOR PE (on the left)
and LZ, MA, and RC PEs for LZMA (on the right). Since
these optimizations impact only the power of PEs, we elide a
discussion of their impact on radios, ADCs, amplifiers and the
RISC-V core. The graph on the left shows the improvement
400
XCOR-initial
+spt-prg
+opt
0
5
10
15
PE Power (mW)
XCOR LZ MA-unsplit MA RC LZMA-initial +spt-prg +MA-RC split +opt
0
5
10
15
20
PE Power (mW)
Fig. 6: (Left) Impact of spatial refactoring and other optimizations on XCOR power in seizure prediction. (Right)
Impact of spatial refactoring, PE decomposition and other
microarchitectural optimizations on LZMA power.
in XCOR while the graph on the right shows improvements
in LZMA. All must be under the 12mW target budget. For
XCOR, the unoptimized version exceeds the target budget.
Spatial reprogramming saves 50% power. Optimizations such
as pipelining further lower the power.
Figure 6 shows that unoptimized LZMA uses 20mW and
exceeds the 12mW power budget. Spatial reprogramming
saves 1.5Ã— power. To reduce power further, we use locality
refactoring to split the original MA into separate MA and RC
PEs. This reduces power to 11.2mW, which is then further
dropped using other optimizations such as pipelining.
D. More Design Space Studies
We introduced PE parameterization, blocking, and interleaving in Â§IV. The graph on the left in Figure 7 plots the
compression per mW of the three BCI task pipelines as a
function of history length. The compression per mW of both
LZ4 and LZMA peak at history length of 4096 bytes. Longer
histories enable better compression, but the gains drop after
a window size of 4096 bytes. Apart from the LZMA-8192
configuration, all configurations are within the 12mW power
budget. Thus, we advocate using a 4096 byte history length.
The graph on the right in Figure 7 plots compression per
mW as interleaving depth is varied. For block based algorithms
like LZ4 and LZMA, memory interleaving greatly reduces
hardware resources within the PE at the cost of a smaller, less
power hungry memory buffer. As interleaving depth increases,
so too does the cost of memory. Non-block based compression
like DWTMA does not face this trade-off.
Figure 8 plots the compression ratio per mW as a function
of compression block size. LZMA and DWTMA benefit from
larger block sizes, which enable better estimation of input frequencies until a log block size of 22 (4MB). Beyond this, the
compression per mW drops slightly. This drop occurs because
the saturating counters in Â§IV-B introduce inaccuracies in input
frequency estimations. As LZ4 encoding does not depend on
block size, it remains unaffected.
Finally, Figure 9 shows an example of how HALO can
adapt to the needs of different brain regions. We separate
compression/power results of LZ4, LZMA and DWTMA
1024
2048
4096
8192
History Length (Bytes)
0.20
0.22
0.24
0.26
0.28
0.30
0.32
Compression Ratio / mW
1
2
4
8
16
32
64
128
256
512
1024
Interleaving Depth (Samples)
0.10
0.15
0.20
0.25
0.30
Compression Ratio / mW
LZ4 LZMA DWTMA HALO design point
Fig. 7: (Left) Longer LZ history enables better matches but
requires more memory. A history of 4KB best balances power
and performance. All configurations except 8KB use <12mW.
(Right) LZ match benefits only slightly from channel correlation, so larger interleaving compresses better and reduces
radio bandwidth. 128 sample interleaving is a good balance.
DWTMA is mostly unaffected by interleaving for â‰¥ 2 samples.
16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Log Block Size (Bytes)
0.200
0.225
0.250
0.275
0.300
0.325
Compression Ratio / mW
LZ4 LZMA DWTMA HALO design point
Fig. 8: Impact of blocking on compression ratio and power.
The x-axis is log (block size), the y-axis is compression ratio
per mW. LZ4 compression ratio does not drastically vary with
the block size because it uses LIC encoding, which does suffer
a penalty for smaller blocks. For LZMA and DWT, block
size affects the compression ratio. As the block size increases,
frequency estimations improve, improving compression ratio
per unit mW from block log size 16 (64kB) to 22 (4MB).
After 22 (4MB), blocks become too large and the saturated
counters slowly degrade estimations of the frequencies.
for the arm and leg regions in the motor cortex. LZMAâ€™s
compression ratio is superior to LZ4 and DWTMA, but LZ4
power is lower. If radio-deposited energy is the dominating
concern, we advocate using LZMA. Figure 9 shows that the
power budget targets are met regardless of brain region.
VII. CONCLUSIONS & DISCUSSION
Modularity: This work performs an initial exploration of
workloads that are important for neuroscience, but the list
of tasks can be expanded. Future BCIs will implement other
workloads, with different pipelines targeting different research
and medical objectives. Because of its modular design, HALO
401
Arm Leg 1
2
3
4
Compression Ratio
Arm Leg 0
2
4
6
8
10
12
Power (Radio + PE) (mW)
LZ4 LZMA DWTMA
Fig. 9: (Left) Compression ratio for LZ4, LZMA, and
DWTMA, separated by experiments performed on the arm and
leg regions of the motor cortex. The error bars show variance
of compression ratios between different experimental trials.
(Right) LZ4, DWTMA, and LZMA power (excluding ADCs
and amplifiers), separated by arm and leg regions.
is able to support such workloads seamlessly. In the near-term,
we are further enhancing HALOâ€™s seizure prediction algorithm by implementing kernels for calculation of approximate
entropy, Hann functions, and Hjorth parameters [47, 51, 87].
We have also discovered that compression based on the
Burrows-Wheeler transform (e.g., Bzip2) may be particularly
effective for certain classes of neural data. Implementing
a monolithic ASIC for Bzip2 will be overly complex and
power-hungry, but HALOâ€™s modularity offers a lower-power
alternative. In particular, since Bzip2 uses range coding, we
simply need to implement the Burrows-Wheeler transform, but
can reuse the MA and RC PEs.
Distributed designs: Structural and functional networks
across brain centers influence the manifestation of several
neurological disorders. For example, the occurrence of seizures
and their inter-ictal state (i.e., the period of time between
successive convulsions) is a function of the activity in many
parts of the brain including the hippocampus, the lateral septal
nuclei and anterior hypothalamus, as well as the upper brainstem, intralaminar thalamus, and fronto-parietal association
cortex [25, 29, 62, 101, 103]. In such cases, we envision
the need for multiple HALO devices on different brain subcenters, with one device determining the onset of a seizure,
and another device used to stimulate tissue on another brain
region, thereby mitigating (and perhaps even eliminating) the
â€œspreadâ€ of seizures across sub-centers. Distributed designs
[31, 89, 95] require particularly power-efficient and flexible
platforms, making HALO a compelling starting point.
Related work: HALO is partly inspired by prior work on
embedded systems for wearables. While HALO is significantly more power-constrained because it is implanted in the
brain, many wearables also target signal processing and data
exfiltration [59, 64, 120]. Like our work on HALO, some of
the recent work in this space â€“ e.g., applications like smart
watches that can be used to monitor fitness, GPS, heart rate,
hand gestures, etc. â€“ studies the question of how to make these
designs more general and flexible [50, 53, 65]. While HALO
meets a different set of constraints, we expect the lessons from
this work to apply to some of these domains too.
Implications on accelerator SoCs: While HALO focuses
on implantable BCIs, the question of how to design multiaccelerator SoCs in ultra-power-constrained environments is
a more general problem facing the systems community. This
work offers one way of systematically traversing the design
space by using software engineering techniques to make
hardware more amenable for efficient implementation. While
the details of how we decompose individual algorithms into
constituent pieces, identify shared pieces among algorithms,
prune these pieces to a canonical set, and then implement
these pieces into distinct hardware blocks that can operate
at their individual target frequency may vary across domains,
their generality offers lessons for domains beyond BCIs.