Abstract—Neural personalized recommendation is the cornerstone of a wide collection of cloud services and products,
constituting significant compute demand of cloud infrastructure.
Thus, improving the execution efficiency of recommendation
directly translates into infrastructure capacity saving. In this
paper, we propose DeepRecSched, a recommendation inference
scheduler that maximizes latency-bounded throughput by taking
into account characteristics of inference query size and arrival
patterns, model architectures, and underlying hardware systems.
By carefully optimizing task versus data-level parallelism, DeepRecSched improves system throughput on server class CPUs
by 2× across eight industry-representative models. Next, we
deploy and evaluate this optimization in an at-scale production datacenter which reduces end-to-end tail latency across
a wide variety of recommendation models by 30%. Finally,
DeepRecSched demonstrates the role and impact of specialized
AI hardware in optimizing system level performance (QPS) and
power efficiency (QPS/watt) of recommendation inference.
In order to enable the design space exploration of customized
recommendation systems shown in this paper, we design and
validate an end-to-end modeling infrastructure, DeepRecInfra.
DeepRecInfra enables studies over a variety of recommendation
use cases, taking into account at-scale effects, such as query
arrival patterns and recommendation query sizes, observed
from a production datacenter, as well as industry-representative
models and tail latency targets.
I. INTRODUCTION
Recommendation algorithms are used pervasively to improve and personalize user experience across a variety of webservices. Search engines use recommendation algorithms to
order results, social networks to suggest posts, e-commerce
websites to suggest purchases, and video streaming services
to recommend movies. As their sophistication increases with
more and better quality data, recommendation algorithms have
evolved from simple rule-based or nearest neighbor-based
designs [1] to deep learning approaches [2]–[7].
Deep learning-based personalized recommendation algorithms enable a plethora of use cases [8]. For example, Facebook’s recommendation use cases require more than 10× the
datacenter inference capacity compared to common computer
vision and natural language processing tasks [9]. As a result,
over 80% of machine learning inference cycles at Facebook’s
datacenter fleets are devoted to recommendation and ranking
inference [10]. Similar capacity demands can be found at
Google [11], Amazon [8], [12], and Alibaba [5], [6]. And
NCF
RM1
DIN
RM2
WND
RM3
MT-WNDDIEN
DeepSpeech2
ResNet50
Skylake Roofline
RMC1
RMC3
NCF
DIEN
MT-WND
WND
RMC2
DIN
Fig. 1: State-of-the-art recommendation models span diverse
performance characteristics compared to CNNs and RNNs.
Based on their use case, recommendation models have unique
architectures introducing model-level heterogeneity.
yet, despite their importance and the significant research on
optimizing deep learning based AI workloads [13]–[17] from
the systems and architecture community, relatively little attention has been devoted to solutions for recommendation [18].
In fact, deep learning-based recommendation inference poses
unique challenges that demand unique solutions.
First, recommendation models exhibit unique compute,
memory, and data reuse characteristics. Figure 1(a) compares
the compute intensity of industry-representative recommendation models1 [2]–[7], [10] to state-of-the-art convolutional
(CNN) [19] and recurrent (RNN) neural networks [20].
Compared to CNNs and RNNs, recommendation models,
highlighted in the shaded yellow region, tend to be memory intensive as opposed to compute intensive. Furthermore,
recommendation models exhibit higher storage requirements
(GBs) and irregular memory accesses [10]. This is because
recommendation models operate over not only continuous but
also categorical input features. Compared to the continuous
features (i.e., vectors, matrices, images), categorical features
are processed by inherently different operations. This unique
characteristic of recommendation models exposes new system
design opportunities to enable efficient inference.
Next, depending on the use case, major components of a
recommendation model can be sized differently [21]. This
1Section III describes the eight recommendation models in detail.
982
2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)
978-1-7281-4661-4/20/$31.00 ©2020 IEEE
DOI 10.1109/ISCA45697.2020.00084
introduces model-level heterogeneity across the state-of-theart deep learning-based recommendation models. By focusing
on memory access breakdown, Figure 1(b) shows diversity
among recommendation models themselves. For instance,
dense feature processing that incurs regular memory accesses
dominate for Google’s WnD [4], [7], NCF [2], Facebook’s
DLRM-RMC3 [10], and Alibaba’s DIEN [6]. In contrast,
categorical, sparse feature processing that incurs irregular
memory accesses dominate for other recommendation models
such as Facebook’s DLRM-RMC1/RMC2 [10] and Alibaba’s
DIN [5]. These diverse characteristics of recommendation
models expose system optimization design opportunities.
Finally, recommendation models are deployed across webservices that require solutions to consider effects of executing
at-scale in datacenters. For instance, it is commonly known
that requests for web-based services follow Poisson and lognormal distributions for arrival and working set size respectively [22]. Similar characteristics are observed for arrival
rates of recommendation queries. However, query working set
sizes for recommendation follow a distinct distribution with
heavier tail effects. This difference in query size distribution
leads to varying optimization strategies for at-scale inference.
Optimizations based on production query size distributions,
compared to log-normal, improve system throughput by up to
1.7× for at-scale recommendation inference.
To enable design optimizations for the diverse collection of
industry-relevant recommendation models, this paper presents
DeepRecInfra – an end-to-end infrastructure that enables
researchers to study at-scale effects of query size and arrival patterns. First, we perform an in-depth characterization
of eight state-of-the-art recommendation models that cover
commercial video recommendation, e-commerce, and social
media [2], [4]–[7], [10]. Next, we profile recommendation
services in a production datacenter to instrument an inference
load generator for modeling recommendation queries.
Built on top of the performance characterization of the recommendation models and dynamic query arrival patterns (rate
and size), we propose a hill-climbing based scheduler – DeepRecSched – that splits queries into mini-batches based on the
query size and arrival pattern, the recommendation model, and
the underlying hardware platform. DeepRecSched maximizes
system load under a strict tail-latency target by trading off
request versus batch-level parallelism. Since it is also important to consider the role of hardware accelerators for at-scale
AI infrastructure efficiency, DeepRecSched also evaluates the
impact of specialized hardware for neural recommendation by
emulating its behavior running on state-of-art GPUs.
The important contributions of this work are:
1) This paper describes a new end-to-end infrastructure,
DeepRecInfra, that enables system design and optimization across a diverse set of recommendation models.
DeepRecInfra integrates query arrival patterns and size
distributions, observed in a production datacenter. We
highlight the importance of the unique query arrival and
size characteristics for at-scale recommendation inference
and identify a new performance optimization opportunity
Dense
Features
User
Item
Query
N
N
N
Dense
feature
DNNstack
Embedding
 Table
Lookup
Embedding
 Table
Lookup
Sparse feature
pooling
Predictor
DNN-stack N
Neural Personalized
 Recommendation Model
 Recommendation Inputs
Feature
Interaction
Depth
Batch
size
Width
Width
Categorical Inputs Categorical Inputs
Continupus
inputsLookups per table Lookups per table
Number of
 tables Operator
(e.g.,
Concat,
Sum,
Attention,
RNN)
Operator
(e.g.,
Concat,
Sum)
Categorical
Features
Categorical
Features
Depth
CTRs
Fig. 2: General architecture of personalized recommendation
models. Configuring the key parameters (red) yields different
implementations of industry-representative models.
by exploiting request batching (Section III).
2) We propose DeepRecSched– the first batch-scheduler
that (a) partitions work across CPUs and accelerators
(GPU), (b) trades off batch- (data) and request- (task)
parallelism. DeepRecSched is tailor-designed to take into
account the dynamic query arrival patterns (rate and
size), recommendation model architectures, and servicelevel latency targets (Section IV). Evaluated with DeepRecInfra, DeepRecSched doubles system throughput of
server class CPUs under strict latency targets. In addition,
we implement and evaluate the proposed design on a
production datacenter with live recommendation query
traffic, showing a 1.3× reduction in the tail latency.
3) We demonstrate that GPU accelerators can be appealing
for recommendation inference. Given not all queries are
equal in recommendation inference, this paper shows that
the latency and throughput tradeoff between CPU and
GPU execution varies across different models, system
loads, and latency targets, highlighting the importance of
DeepRecSched’s dynamism to determine optimal configurations. We also show that, for recommendation inference, power efficiency is not always optimal in the face
of GPUs, as compared to CPUs (Section VI).
Systems research for personalized recommendation is still
a nascent field. To enable follow-on work, we have open
sourced the proposed DeepRecInfra infrastructure2. This
includes the industry-representative neural recommendation
models, and at-scale query arrival rates and size distributions
presented in this paper.
II. NEURAL RECOMMENDATION MODELS
Recommendation is the task of personalizing recommending
content based on a user’s preferences. Recommendation is
used across many services including search, video and movie
content, e-commerce, and advertisements. However, accurately
modeling preferences based on previous interactions can be
challenging because users only interact with a small subset of
all possible items. As a result, unlike inputs to traditional deep
neural networks (DNNs), inputs to recommendation models
include both dense and sparse features.
2http://vlsiarch.eecs.harvard.edu/research/recommendation
983
Model Company Domain Dense-FC Predict-FC Embeddings
Tables Lookup Pooling
NCF [2] - Movies - 256-256-128 4 1 Concat
Wide&Deep [4] Google Play Store - 1024-512-256 Tens 1 Concat
MT-Wide&Deep [7] Youtube Video - N x (1024-512-256) Tens 1 Concat
DLRM-RMC1 [10] Facebook Social Media 256-128-32 256-64-1 ≤ 10 ∼ 80 Sum
DLRM-RMC2 [10] Facebook Social Media 256-128-32 512-128-1 ≤ 40 ∼ 80 Sum
DLRM-RMC3 [10] Facebook Social Media 2560-512-32 512-128-1 ≤ 10 ∼ 20 Sum
DIN [5] Alibaba E-commerce - 200-80-2 Tens Hundreds Attention+FC
DIEN [6] Alibaba E-commerce - 200-80-2 Tens Tens Attention+RNN
TABLE I: Architectural features of state-of-the-art personalized recommendation models.
A. Salient Components in Neural Recommendation Models
To accurately model user preference, state-of-the-art recommendation models use deep learning solutions. Figure 2
depicts a generalized architecture of DNN-based recommendation models with dense and sparse features as inputs.
Features. Dense features describe continuous inputs that
are processed with MLP layers i.e., fully-connected layers –
similar to classic DNN approaches. On the other hand, sparse
features represent categorical inputs, such as the collection of
products a user has previously purchased. Since the number of
interactions for a categorical feature is often small compared
to the feature’s cardinality (all available products), the binary
vector representing such interactions ends up very sparse.
Embedding Tables. Each sparse feature has a corresponding embedding table that is composed of a collection of latent
embedding vectors. The number of vectors, or rows in the
table, is determined by the number of categories in the given
feature – this can vary from tens to billions. The number of
elements in each vector is determined by the number of latent
features for the category representation. This latent dimension
is typically 16, 32, or 64. In total, embedding tables often
require up to tens of GBs of storage.
Embedding Table Access. While embedding tables themselves are dense data structures, embedding operations incur sparse, irregular memory accesses. Each sparse input
is encoded either as one-hot or multi-hot encoded vectors,
which are used to index specific rows of an embedding table.
The resulting embedding vectors are combined with a sparse
feature pooling operation such as concatenation or sum.
Feature Interaction. The outputs of the dense and sparse
features are combined before being processed by subsequent
predictor-DNN stacks. Typical operations for feature interaction include concatenation, sum, and averaging.
Product Ranking. The output of the predictor-DNN stacks
is the click through rate (CTR) probability for a single useritem pair. To serve relevant content to users, the CTR of
thousands of potential items are evaluated for each user. All
CTR’s are then ranked and the top-N choices are presented
to the user. As a result, deploying recommendation models
requires running the models with non-unit batch sizes.
III. DEEPRECINFRA: AT-SCALE RECOMMENDATION
To better understand the distinct characteristics of and
design system solutions for neural recommendation models,
we developed, DeepRecInfra, to model and evaluate at-scale
recommendation inference. DeepRecInfra is implemented as
a highly extensible framework enabling us to consider a
variety of recommendation use cases. In particular, DeepRecInfra consists of three important components: (1) a suite
of industry-representative models, (2) industry-representative
tail latency targets, and (3) real-time query serving based on
arrival rates and working set size distributions profiled from
recommendation running in a production datacenter.
A. Industry-scale recommendation models
Recent publications from Google, Facebook, and Alibaba
present notable differences across their recommendation models [2], [5]–[7], [10]. The generalized recommendation model
architecture shown in Figure 2 can be customized by configuring formative parameters in order to realize these different
implementations. To capture the diversity, DeepRecInfra composes a collection of eight state-of-the-art recommendation
models. We describe the unique aspects of each model below
and summarize their distinguishing parameters in Table I.
• Neural Collaborative Filtering (NCF) generalizes matrix factorization (MF) techniques proposed via the Netflix
Prize [23] [24] with MLPs and non-linearities. NCF implements one-hot encoded sparse features, four embedding
tables, and MF based sparse pooling.
• Wide and Deep (WnD) considers both sparse and dense
input features and is deployed in Google’s Play Store [4].
Dense input features are directly concatenated with the output of one-hot encoded embedding lookups and a relatively
large Predict-FC stack produces output CTRs.
• Multi-Task Wide and Deep (MT-WnD) extends WnD
by evaluating multiple output objectives including CTR,
comment rate, likes, and ratings using a separate PredictFC stack for each objective. Leveraging multi-objective
modeling, MT-WnD enables a finer grained and improved
user experience [25].
• Deep Learning Recommendation Model (DLRM) is a
set of models from Facebook that differs from the aforementioned examples with its large number of embedding
lookups [3]. Based on the configurations shown in [10]
varying the number of lookups per table and size of FC
layers yield three different architectures, DLRM-RMC1,
DLRM-RMC2, and DLRM-RMC3.
• Deep Interest Network (DIN) uses attention to model user
interests. DIN does not consider dense input features but
has tens of embedding tables of varying sizes. Smaller embedding tables process one-hot encoded inputs while larger
ones (up to 109 rows) process multi-hot encoded inputs with
hundreds of lookups. Outputs of these embedding operations
984
Production
query sizes
Lognormal
Heavy tail of
 query size
75th percentile (p75)
of query sizes
Fig. 3: Queries for personalized recommendation models follow a unique distribution not captured by traditional workload
distributions (i.e. normal, log-normal) considered for webservices. The heavy tail of query sizes found in production
recommendation services leads to unique design optimizations.
are combined as a weighted sum by a local activation unit
(i.e., attention) and then concatenated [5].
• Deep Interest Evolution Network (DIEN) captures evolving user interests over time by augmenting DIN with gated
recurrent units (GRUs) [6]. Inputs to the model are one-hot
encoded sparse features. Embedding vectors are processed
by attention-based multi-layer GRUs.
B. Service level requirement on tail latency
Personalized recommendation models are used in many
Internet services deployed at a global scale. They must service
a large number of requests across the datacenter while meeting
strict latency targets set by the Service Level Agreements
(SLAs) of various use cases. Thus, recommendation systems
are optimized for latency-bounded throughput measured as
the queries per second (QPS) that can be processed under
a p95 tail-latency requirement. Across different applications
(e.g., search, social-media, e-commerce) we find these latency
targets vary significantly, which can result in distinct system
design decisions. In this paper, we use the published targets
and profiled model runtime to set the tail-latency target. Details
on these tail latency targets are available in Section V.
C. Real-Time Query Serving for Recommendation Inference
DeepRecInfra takes into account two important dimensions
of real-time query serving: arrival rate and working set sizes.
Query Arrival Pattern: Arrival times for queries for
datacenter services are determined by the inter-arrival time
between consecutive requests. This inter-arrival time can be
modeled using a variety of distributions including uniform,
normal or Poisson distributions [22], [26]–[29]. Previous work
has shown that, these distributions can lead to different system
design optimizations [22], [29]. Following web-services, by
profiling the statistical distribution of recommendation services
in a production datacenter, we find that query arrival rates
follow a Poisson distribution [22], [26]–[28], [30].
Query Working Set Size Pattern: Not all recommendation
queries are created equal. The size of queries for recommendation inference relates to the number of items to be
ranked for a given user. This translates to the amount of
work per inference. Given the potential number of items to
Model 1
Skylake
Model 2
Broadwell
Datacenter
Datacenter
Individual
nodes Individual
nodes
Fig. 4: Performance distribution of recommendation inference
at datacenter scale to individual machines. Individual machines
follow inference distributions, excluding network and geographic effects, at the datacenter scale to within ∼ 9%.
be served depends heavily on users’ interaction with the webservice, query sizes vary. While new hardware and software
optimizations are typically evaluated across an array of fixed
size batches, this is not the same as optimizing systems
where batch-sizes vary dynamically (i.e., recommendation). To
maximize efficiency of systems with dynamic batch-sizes, is
it important to optimize for the particular working set size
distribution (see Section IV).
Related work on designing system solutions for web services typically assumes working set sizes of queries follow a
fixed, normal, or log-normal distribution [22]. However, Figure 3 illustrates that query sizes for recommendation exhibit
a heavier tail compared to canonical log-normal distributions.
Thus, while DeepRecInfra’s load generator supports a variety
of distributions, the remainder of this paper uses the query
size distribution found in a production datacenter (Figure 3).
D. Subsampling datacenter fleet with single-node servers
To serve potentially billions of users across the world,
recommendation models are typically run across thousands of
machines. However, it may not always be possible to deploy
design optimizations across a production-scale datacenter. We
show a handful of machines can be used to study and optimize
tail performance of recommendation inference. Figure 4 shows
the cumulative distribution of two different recommendation
models running on server-class Intel Skylake and Broadwell
machines. We find that the datacenter scale performance
(black) is within 10% of the distribution measured on a
handful of machines (red). Thus, tail-latency trends for recommendation inference across a subset of machines can be
representative of larger scale systems.
E. Putting it Altogether
To study at-scale characteristics of recommendation, it is
important to use representative infrastructure. This includes
representative models, query arrival rates, and query working set size distributions. Thus, we developed DeepRecInfra,
shown in Figure 5, by incorporating an extensible load generator to model query arrival rate and size patterns for recommendation use cases. This enables efficient and representative
design space exploration catered to at-scale recommendation.
985
Query
arrival pattern
Query working
set size
Recommendation Load
Generator CPU core
…
Accel queries
System
throughput
(QPS)
CTR
Inference CPU core Queries
Measured tail latency
DeepRecInfra DeepRecSched
SLA target
Accelerator Poisson Production
Accel. query
size
threshold
CPU queries Per-request
batch-size
Rec. models (i.e., DLRM, WND,
NCF, MTWnD, DIN, DIEN)
<
Fig. 5: DeepRecInfra implements an extensible framework that considers industry-representative recommendation models, application level tail latency targets, and real-time query serving (rate and size). Built upon DeepRecInfra, DeepRecSched optimizes
system throughput (QPS) under strict latency targets by optimzing per-request batch-size (request versus batch parallelism)
and accelerator query size threshold (parallelizing queries across specialized hardware).
DeepRecInfra is designed to enable future research for
at-scale recommendation inference. First, model architecture
parameters are specified at the command line. This includes
parameters such as the width/depth of DNN layers, number
of continuous and categorical input features, number of rows
and columns in embedding tables, number of sparse IDs per
lookup, and interaction operations between continuous and
categorical input features. Next, given the example implementations, additional recommendation models can be added to the
infrastructure. Finally, to model various at-scale effects, users
can configure the degree of model co-location, tail latency
targets, and query arrival rates and sizes.
IV. DEEPRECSCHED DESIGN
In this section, we present the design, implementation, and
evaluation of the proposed design – DeepRecSched– a recommendation inference scheduler that optimizes latency-bounded
throughput for at-scale execution. Central to DeepRecSched is
the observation that working set sizes for recommendation
queries follow a unique distribution with a heavy tail. Intuitively, large queries limit the throughput (QPS) a system
can handle given a strict latency target. DeepRecSched is
tailor-designed to address this bottleneck with two design
optimizations. First is exploiting batch (data) versus request
(task) level parallelism. This is accomplished by splitting large
queries into multiple requests of smaller batch size; requests
are processed by parallel cores. This requires carefully balancing batch-level and SIMD-level parallelism, cache contention,
and the potential increase in queuing delay from a larger
number of smaller-sized requests. Second, large queries are
offloaded to specialized AI hardware in order to accelerate
at-scale recommendation inference. The decision to offload
queries onto specialized AI hardware must carefully balance
data communication costs, and parallelism across both server
class CPU cores and the accelerator for each model.
DeepRecSched optimizes system throughput across the
large and complex design space of recommendation use cases
encompassed by DeepRecInfra (i.e., models, latency targets,
query serving, hardware platforms). This is accomplished by
tuning the per-core batch-size (i.e., balancing batch versus
request-level parallelism on CPUs) and accelerator query size
threshold (i.e., offloading larger queries to specialized hardware). In order to tune these parameters across the diverse set
of recommendation use cases, DeepRecSched implements a
hill-climbing based scheduling policy. The scheduler provides
an effective solution, given our observation of the convexity
of batch-size and accelerator partitioning problem, over more
complex control theoretic approaches, such as PID [31], [32].
We motivate the need for automated solutions given the
apparent model diversity (Section IV-A), optimize batch versus
request parallelism (Section IV-B), and modulate the query
offloading degree for specialized hardware (Section IV-C).
Figure 5 illustrates the proposed DeepRecSched design in
the context of DeepRecInfra.
A. Model diversity demands flexible optimization
The apparent diversity of the industry-representative recommendation models leads to varying, unique performance
bottlenecks. Figure 6 compares the performance characteristics
of recommendation models running on a server class Intel
Broadwell, shown as fractions of time spent on Caffe2 operators for a fixed batch size of 64. As expected, inference runtime
for models with high degrees of dense feature processing (i.e.,
DLRM-RMC3, NCF, WND, MT-WND) is dominated by the
MLP layers. On the other hand, inference runtime for models
dominated by sparse feature processing (i.e., DLRM-RMC1
and DLRM-RMC2) is dominated by embedding table lookups.
Interestingly, inference runtime for attention based recommendation models is dominated by neither FC nor embedding
table operations. For instance, inference run time for DIN is
split between concatenation, embedding table, sum, and FC
operations. This is a result of the attention units, which (1)
concatenate user and item embedding vectors, (2) perform a
small FC operation, and (3) use the output of the FC operation
to weight the original user embedding vector. Similarly, the
execution time of DIEN is dominated by recurrent layers. This
is a result of fewer embedding table lookups whose outputs
are processed by a series of relatively large attention layers.
This design space is further expanded considering the
heterogeneity of CPUs found in production datacenters [9].
Recent work shows recommendation models are run on a
986
Embedding-dominated MLP-dominated Attention-dominated
Fig. 6: Operator breakdown of state-of-the-art personalized
recommendation models with a batch-size of 64. The large
diversity in bottlenecks leads to varying design optimizations.
BDW
SKL-AVX2
SKL-AVX512
BDW
SKL-AVX2
SKL-AVX512
BDW
SKL-AVX2
SKL-AVX512
WND DIN DLRM-RMC2
Wider
SIMD
Larger L2 capacity
Clock frequency
Fig. 7: Performance of WnD, DIN, and DLRM-RMC3 on
Broadwell and Skylake, using AVX-2 and AVX-256 support.
Performance variation across hardware platforms is due to
difference in micro-architectural features such as SIMD-width,
cache capacity, and clock frequency.
variety of server class CPUs such as Intel Broadwell and
Skylake [10]. While, Broadwell implements CPUs running at
2.4GHz with AVX-256 SIMD units and inclusive L2/L3 cache
hierarchies, Skylake cores run at 2.0GHz with AVX-512 units
and exclusive caches with a larger effective cache capacity.
Figure 7 shows the impact of CPU micro-architecture on
neural recommendation inference performance. We show the
performance of WnD, DIN, and DLRM-RMC2 on Broadwell
(BDW), as well as Skylake using both AVX-256 (SKLAVX2) and AVX-512 (SKL-AVX512) instructions. Given the
fixed operating frequency and cache hierarchy between SKLAVX2 and SKL-AVX512, the 3.0× performance difference
for WnD can be attributed to the better utilization of the
SIMD units. Similarly, given the fixed SIMD width, the 1.3×
performance difference between BDW and SKL-AVX2 is a
result of the larger L2 caches that help accelerate the Concat
operator with highly regular memory access pattern. Finally,
the performance difference between BDW and SKL-AVX2
instructions on DLRM-RMC2 is attributed to a 20% difference
in core frequency accelerating the embedding table operations.
Given the variety of operator and system bottlenecks, an
important design feature of DeepRecSched is to automatically
optimize request- versus batch-level parallelism and leverage
parallelism with specialized hardware.
Hill
climbing
Batch size
256
Batch size
64
Batch size
256
DLRM-RMC3 at
Med latency
target
DLRM-RMC3 at
Low latency
target
Batch
size
128
DLRM-RMC3
DIEN
DLRM-RMC1
Batch size
512
Request parallelism Batch parallelism
Fig. 8: Optimal request vs. batch parallelism varies based
on the use case. Optimal batch-size varies across latency
targets for DLRM-RMC (top) and models (bottom) i.e.,
i.e., DLRM-RMC2 (embedding-dominated), DLRM-RMC3
(MLP-dominated), DIEN (attention-dominated).
B. Optimal batch size varies
While all queries can be processed by a single core,
splitting queries across cores to exploit hardware parallelism,
is often advantageous. Thus, DeepRecSched splits queries into
individual requests. However, this sacrifices parallelism within
a request with a decreased batch size.
The optimal batch size that maximizes the system QPS
throughput varies based on (1) tail latency targets and (2)
recommendation models. Figure8 shows the achievable system
throughput (QPS) as we vary the per-core batch-size. Recall
that small batch-sizes (request parallelism) parallelizes a single
query across multiple cores while larger batch-sizes (batch
parallelism) processes a query on a single core. Figure 8
(top) illustrates that, for DLRM-RMC3, the optimal batch size
increases from 128 to 256 as the tail latency target is relaxed
from 66ms (low) to 100ms (medium). (See Section V for more
details on tail-latency targets.) Furthermore, Figure 8(bottom)
shows that the optimal batch size for DIEN (attention-based),
DLRM-RMC3 (FC heavy), and DLRM-RMC1 (embedding
table heavy) is 64, 128, and 256, respectively.
Note that the design space is further expanded when optimizing across the heterogeneous hardware platforms [9].
Following Figure 7, micro-architectural features across these
servers can impact the optimum tradeoff between request- and
batch-level parallelism. For example, higher batch sizes are
typically required to exploit the benefits of the wider SIMD
units in Intel Skylake [10]. Next, while inclusive cache (i.e.,
Broadwell) hierarchies simplify cache coherence protocols,
they are more susceptible to cache contention and performance
degradation from parallel cores [33], [34]. In the context of
recommendation, this can be achieved by trading off request
for batch parallelism. Section VI provides a more detailed
analysis into the implication of hardware heterogeneity on
987
1 1
1 1
1 1
1
1
47 23 15 1024 1024
1024
1024
1024 1024
1024
1024
14 37 101 508 101
max batch size
GPUs begin to outperform CPUs
min
batch
size
compute
time
data loading
time
Fig. 9: GPU speedup over CPU for representative recommendation models. The batch-size at which GPUs start to outperform
CPUs and their speedup at large batch-sizes varies across models.
trading off request- versus batch-level parallelism.
C. Leverage parallelism with specialized hardware
In addition to balancing request- versus batch-level parallelism on general purpose CPUs, in the presence of specialized
AI hardware, DeepRecSched improves system throughput by
offloading queries that can best leverage parallelism in the
available specialized hardware. We evaluate the role of accelerators with state-of-the-art GPUs.
Figure 9(top) illustrates the speedup of GPUs over a CPU
(single threaded) across the recommendation models at various
batch sizes. For each model, we illustrate the relative performance of GPUs over CPUs at a unit batch-size, the batch-size
required to outperform CPU-only hardware platforms, and a
large batch-size of 1024. Given the higher compute intensity
and memory bandwidth, GPUs provide significant performance benefits at higher batch sizes — especially for compute
intensive models. However, across the different classes of
recommendation models, there is large variation in (1) speedup
at large batch sizes (i.e. 1024) and (2) batch size required to
outperform CPU-only hardware platforms vary widely. This
is due to the overhead of transferring inputs from the CPU
to the GPU, which consumes a significant fraction of time.
For instance, as shown in Figure 9(bottom), across all batch
sizes, data loading time consumes on average 60∼80% of the
end-to-end inference time on the GPU for all models.
In addition to considering performance characteristics of
recommendation models on standalone systems, it is important
to analyze the impact of the dynamic query working set
size distributions. Figure 10 illustrates the execution time
breakdown for queries smaller than the p75th size versus larger
queries. Despite the long tail, the collection of small queries
constitute over half the CPU execution time. 25% of large
queries contribute to nearly 50% of total execution time. This
unique query size distribution with a long tail makes GPUs an
interesting accelerator target. Figure 10 shows that, across all
models, GPU can effectively accelerate the execution time of
large queries. While offloading the large queries can reduce
execution time, the amount of speedup varies based on the
model architecture. The optimal threshold for offloading varies
across models, motivating a design that can automatically tune
the offloading decision for recommendation inference.
Trading off processing queries on CPUs versus GPUs requires careful optimization. Intuitively, offloading queries to
the GPU incurs significant data transfer overheads. To amortize
this cost, GPUs often require larger batch sizes to exhibit
speedup over CPUs, as shown in Figure 9 [35]. Consequently,
DeepRecSched improves system throughput by offloading the
largest queries for recommendation inference to the GPU.
This can be accomplished by tuning the query-size threshold.
Queries larger than this threshold are offloaded to the GPU
while smaller ones are processed on CPU cores.
Figure 11 illustrates the impact of query-size threshold (xaxis) on the achievable QPS (y-axis) across a variety of recommendation models. The optimal threshold varies across the
three recommendation models, DLRM-RMC3, DLRM-RMC1,
and DIEN. In fact, we find that the threshold not only varies
across model architectures, but also across tail latency targets.
Note, the optimal query size thresholds take into account the
dynamic working set distribution found in Figure 3. Compared
to the batch-size at which the GPU demonstrates speedup over
the CPU (Figure 9), the optimal query-size threshold for both
DLRM-RMC1 and DIEN are higher — 47 vs. 320 for DLRMRMC1 and 101 vs. 512 for DIEN. Thus, systems that optimize
recommendation inference by offloading work to specialized
AI accelerators must consider the dynamic working set sizes
— a salient feature of DeepRecSched.
D. DeepRecSched Design Summary
Recall that production datacenters run a variety of recommendation models that evolve over time across heterogeneous hardware platforms with varying SLA targets [9].
Thus, automated solutions are needed to optimize batch- and
request-level parallelism, and offloading inference queries to
specialized hardware. We tailor-design DeepRecSched for atscale recommendation inference. For example, given the convexity of the batch-size and accelerator offloading optimization
problem seen in Figure 8 and Figure 11, DeepRecSched implements a hill-climbing based algorithm. Compared to more
complex control-theoretic approaches, such as PID controllers,
hill-climbing offers a simple, scalable, and effective solution.
In fact, we demonstrate the efficacy of DeepRecSched, by not
only evaluating it using DeepRecInfra but also deploying it in
a real production datacenter fleet (see Section VI for details).
988
RMC1 RMC2 RMC3 NCF WND MT-WND DIN DIEN
Smaller
than p75
query size
Larger than p75
query size
CPU optimal for
smaller than p75 query size
6x speedup of
 large queries on GPU
CPU GPU CPU GPU CPU GPU CPU GPU CPU GPU CPU GPU CPU GPU CPU GPU
Fig. 10: Aggregated execution time over the query set based
on the size distribution for CPU and GPU. GPUs readily
accelerate larger queries; however, the optimal inflection point
and speedup differ across models.
Query
thres.
256
Query
thres.
320
Query
thres.
512 DLRM-RMC3
DIEN
DLRM-RMC1
All GPU All CPU
Fig. 11: The optimal query size threshold, and thus fraction of
queries processed by the GPU, varies across recommendation
models i.e., DLRM-RMC2 (embedding-dominated), DLRMRMC3 (MLP-dominated), DIEN (attention-dominated).
Given a particular recommendation model, hardware platform, and tail latency target, DeepRecSched first tunes the
tradeoff between batch- versus request-level parallelism. Starting with a unit batch-size, DeepRecSched gradually increases
the per-core batch-size in order to optimize system throughput.
Note that DeepRecSched is designed to perform the hillclimbing based optimization during the initial warm up period
of launching a service. DeepRecSched then tunes the querysize threshold for offloading queries to specialized hardware.
Starting with a unit query-size threshold (i.e., all queries are
processed on the accelerator), DeepRecSched again applies
hill-climbing to gradually increase the threshold until the
achievable QPS degrades. As what Section VI later shows,
by automatically tuning the per-request batch size and GPU
query-size threshold, DeepRecSched optimizes infrastructure
efficiency of at-scale recommendation across a variety of
different model architectures, tail latency targets, query-size
distributions, and the underlying hardware.
V. METHODOLOGY
We implement and evaluate DeepRecSched with DeepRecInfra across a variety of different hardware systems and
platforms. We then compare the performance and power
efficiency results with a production-scale baseline.
DeepRecInfra comprises three notable components:
• Model Implementation: We implement all the recommendation models (Table I) in Caffe2 with Intel MKL as the
backend library for CPUs [36] and CUDA/cuDNN 10.1
Model Runtime Bottleneck SLA target
DLRM-RMC1 Embedding dominated 100ms
DLRM-RMC2 Embedding dominated 400ms
DLRM-RMC3 MLP dominated 100ms
NCF MLP dominated 5ms
WND MLP dominated 25ms
MT-WND MLP dominated 25ms
DIN Embedding + Attention dominated 100ms
DIEN Attention-based GRU dominated 35ms
TABLE II: Summarizing performance implications of different personalized recommendation and latency targets used to
illustrate design space tradeoffs for DeepRecSched.
for GPUs [37]. All CPU experiments are conducted with
a single Caffe2 worker and Intel MKL thread.
• SLA Latency Targets: Table II presents the tail latency
targets for each of the recommendation models [4]–[7], [10].
For instance, the Google Play store imposes an SLA target of
tens of milliseconds on WnD [4], [11]. On the other hand,
Facebook’s social media platform requires DLRM-RMC1,
DLRM-RMC2, and DLRM-RMC3 have an SLA target
of hundreds of milliseconds [10]. Alibaba’s e-commerce
platform requires DIN and DIEN have an SLA target of tens
of milliseconds [5], [6]. To explore the design tradeoffs over
a range of latency targets, we consider three latency targets
for each model — Low, Medium, and High — where Low
and High tail latency targets are set to be 50% lower and
50% higher than that of Medium, respectively.
• Real-Time Query Patterns: Query patterns in DeepRecInfra are configurable on two axes: arrival rate and size. The
arrival pattern is fit to a Poisson distribution whereas sizes
are drawn from the production distribution (Figure 3).
Experimental System Setup. To consider the implications
of hardware heterogeneity found in datacenter [9], [10], [38],
we evaluate DeepRecSched with two generations of dualsocket server-class Intel CPUs: Broadwell and Skylake. Broadwell comprises 28 cores running at 2.4GHz with AVX-2 SIMD
units and implements an inclusive L2/L3 cache hierarchy. Its
TDP is of 120W. Skylake comprises of 40 cores running
at 2.0GHz with AVX-512 SIMD units and implements an
exclusive L2/L3 cache hierarchy. Its TDP is of 125W.
To consider the implications of AI hardware accelerators,
we extend the design space to take into account a GPU
accelerator model based on real empirical characterization.
The accelerator performance model is constructed with the
performance profiles of each recommendation model across
the range of query sizes over a real-hardware GPU — serverclass NVIDIA GTX 1080Ti with 3584 CUDA cores, 11GB of
DDR5 memory, and optimized cuDNN backend library (see
Figure 9). This includes both data loading and model computation [39]–[44], capturing the performance-critical components
of the end-to-end recommendation inference.
Production-scale baseline. We compare DeepRecSched to
the baseline that implements a fixed batch size configuration.
This fixed batch size configuration is typically set by splitting
the largest query evenly across all available cores on the
underlying hardware platform. Given the maximum query
989
DLRM-RMC1 DLRM-RMC2 DLRM-RMC3 NCF WND MTWND DIN DIEN GeoMean
Embedding-dominated MLP-dominated Attention-dominated Aggregated
Fig. 12: Compared to a static scheduler based on production recommendation services, the top figure shows performance,
measured in system throughout (QPS) across a range of latency targets, while the bottom shows power efficiency (QPS/Watt),
for DeepRecSched-CPU and DeepRecSched-GPU.
size of 1000 (Figure 3), the static batch size configuration
is determined as 25 for a server-class 40-core Intel Skylake.
VI. DEEPRECSCHED EVALUATION
This section presents the performance and power efficiency improvements of DeepRecSched running on CPUs
(DeepRecSched-CPU) and GPUs (DeepRecSched-GPU) over
the baseline across a vast and complex design space, including
all eight state-of-the-art recommendation models using DeepRecInfra. Next, we detail the benefits of DeepRecSched by
diving into (1) request- versus batch-level parallelism, (2)
a case study of demonstrating the optimizations in a real
production datacenter, and (3) parallelization opportunities of
offloading requests to specialized hardware.
Performance. Figure 12(top) compares the throughput performance of DeepRecSched-CPU and DeepRecSched-GPU
versus a baseline static scheduler across the three tail latency
configurations, all normalized to the measured QPS at the low
tail latency case of the baseline. Overall, DeepRecSched-CPU
achieves 1.7×, 2.1×, and 2.7× higher QPS across all models
for the low, medium, and high tail latency targets, respectively.
DeepRecSched-CPU is able to increase the overall system
throughput by optimizing batch size configuration. Furthermore, DeepRecSched-GPU increases performance improvement to 4.0×, 5.1×, and 5.8× at the low, medium, and high tail
latency targets, respectively. Thus, parallelizing requests across
general-purpose CPUs and specialized hardware provides additional performance improvement for recommendation.
Power efficiency. Figure 12(bottom) compares the QPSper-watt power efficiency of DeepRecSched-CPU and
DeepRecSched-GPU by again normalizing the measured
QPS/Watt to the low tail latency case of the baseline static
scheduler. Given higher performance under the TDP power
budget as the baseline, DeepRecSched-CPU achieves 1.7×,
2.1×, and 2.7× higher QPS/Watt for all models under the
low, medium, and high tail latency targets, respectively. Aggregated across all models, DeepRecSched-GPU improves the
power efficiency improvement to 2×, 2.6×, and 2.9× for
each latency target. Compared to the performance improvement, DeepRecSched-GPU provides marginal improvement in
power efficiency due to the overhead of GPU acceleration.
In fact, while DeepRecSched-GPU improves system QPS
across all recommendation models and latency targets, compared to DeepRecSched-CPU, it does not globally improve
QPS/Watt. In particular, the power efficiency improvement of
DeepRecSched-GPU is more pronounced for compute intensive models (i.e., WND, MT-WND, NCF). On the other hand,
for memory intensive models (i.e., DLRM-RMC1, DIN), the
power overhead for offloading recommendation inference to
GPUs outweighs the performance gain, degrading the overall
power efficiency. Thus, judicious optimization of offloading
queries across CPUs and specialized AI hardware can improve
infrastructure efficiency for recommendation at-scale.
A. Balance of Request and Batch Parallelism
Here we take a deep into how DeepRecSched-CPU improves QPS by balancing request- versus batch-level parallelism across varying (1) latency targets, (2) query size
distributions, (3) models, and (4) hardware platforms.
Optimizing across SLA targets. Figure 13(a) illustrates the
tradeoff between request- and batch-level parallelism across
varying tail latency targets for DLRM-RMC1. Under lower,
stricter tail latency targets, QPS is optimized at lower batch
sizes — favoring request level parallelism. On the other
hand, at more relaxed tail latency targets, DeepRecSched-CPU
finds the optimal configuration to be at a higher batch size
— favoring batch-level parallelism. For instance, using the
production working set size distribution, the optimal batch-size
at target tail latencies of 60ms and 120ms are 128 and 1024
respectively. Intuitively, this is a result of achieving overall
higher system throughput with larger batch-sizes at more
relaxed latency targets. As shown in Figure 12(top), optimizing
this per-request batch size yields DeepRecSched-CPU’s QPS
improvements over the static baseline across latency targets.
Optimizing across query size distributions Figure 13(a)
also shows the optimal batch size, for DLRM-RMC1, varies
990
Batch-level
 parallelism
Request
 parallelism
Skylake
Broadwell
Batch size = 1024,
QPS = 670
Batch size = 256,
QPS = 1100
Batch parallel
Request
parallel
Compute
Intensive
Memory
intensive
Low
SLA target Medium
SLA target
High
SLA target
Lognormal
batch size
DLRM-RMC1
DLRM-RMC2
DIN
Fig. 13: Exploiting the unique characteristics of at-scale recommendation yields efficiency improvements given the optimal
batch size varies across SLA targets and query size distributions (left), models (middle), and hardware platforms (right).
across query working set size distributions (lognormal and
the production distribution). The optimal batch-size across all
tail latency targets is strictly lower for lognormal than the
query size distribution found in production recommendation
use cases. This is a result of, as shown in Figure 3, query
sizes in production recommendation use cases following a
distribution with a heavier tail. In fact, applying optimal
batch-size configuration based on the lognormal query size
distribution to the production distribution degrades the performance of DeepRecSched-CPU by 1.2×, 1.4×, and 1.7× at
low, medium, and high tail-latencies for DLRM-RMC1. Thus,
built ontop of DeepRecInfra, DeepRecSched-CPU carefully
optimizes request verus batch-level parallelism for recommendation inference in production datacenters.
Optimizing across recommendation models. Figure 13(b)
illustrates that the optimal batch size varies across recommendation models with distinct compute and memory characteristics. Here, we consider two compute intensive models (e.g.,
DLRM-RMC3, WnD) and two memory intensive models (e.g.,
DLRM-RMC1, DIN). We find that compute intensive models
are typically optimized with lower batch-sizes as compared
to memory intensive models. For example, at the high SLA
targets, DLRM-RMC3 and WnD have an optimal batch size of
256 and 128, respectively. On the other hand, DLRM-RMC1
and DIN are optimized at a larger batch size of 1024. This is
a result of the compute intensive models being accelerated by
the data-parallel SIMD units (i.e., AVX-512 in Intel Skylake,
AVX-256 in Intel Broadwell). Thus, throughput for compute
intensive models is maximized by fully utilizing the dataparallel SIMD units and parallelizing queries into multiple
requests across the chip-multiprocessor cores.
While higher throughput is achieved at smaller batchsizes for compute intensive models, memory intensive models
require larger batch sizes. This is because the primary performance bottleneck of models with heavy embedding table
accesses lies in the DRAM bandwidth utilization. In order
to saturate, and fully utilize, the per-core memory bandwidth, memory intensive recommendation models must be
run with higher batch-sizes. Thus, in addition to request level
parallelism, memory bandwidth utilization can be improved
significantly by running recommendation inference at a higher
batch size. By exploiting characteristics of the models to optimize the per-request batch size, DeepRecSched-CPU achieves
higher QPS across the various recommendation models.
Optimizing across hardware platforms. Figure 13(c)
shows the optimal batch size, for DLRM-RMC3, varies across
server architectures (Intel Broadwell and Skylake machines).
The optimal batch size, across all tail-latency targets, is strictly
higher on Intel Broadwell compared to Skylake. For example,
at a latency target of 175ms, the optimal batch-size on Intel
Broadwell and Skylake is 1024 and 256, respectively. This is
a result of the varying cache hierarchies on the two platforms.
In particular, Intel Broadwell implements an inclusive L2/L3
cache hierarchy while Intel Skylake implements an exclusive
L2/L3 cache hierarchy. As a result, Intel Broadwell suffers
from higher cache contention with more active cores leading
to performance degradation. For example, at a latency target of
175ms and per-request batch sizes of 16 (request-parallel) and
1024 (batch-parallel), Intel Broadwell has an L2 cache miss
rate of 55% and 40% respectively. To compensate for this performance penalty, DeepRecSched-CPU runs recommendation
models with higher batch-sizes — fewer request and active
cores per query — on Intel Broadwell.
Overall, DeepRecSched enables a fine balance between
request vs. batch-level parallelism across not only varying tail
latency targets, query size distributions, and recommendation
models, but also the underlying hardware platforms.
B. Tail Latency Reduction for At-Scale Production Execution
Following the evaluations using DeepRecInfra, we deploy
the proposed design and demonstrate that the optimizations
translate to higher performance in a real production datacenter.
Figure 14 illustrates the impact of varying the batch-size on
the measured tail latency of recommendation models running
in a production datacenter. Experiments are conducted using
production A/B tests with a portion of real-time datacenter
traffic to consider end-to-end system effects including loadbalancing and networking. The A/B tests run on a cluster
of hundreds of server-class Intel CPUs running a wide collection of recommendation models used in the production
datacenter fleet. The baseline configuration is a fixed batchsize, deployed in a real production datacenter fleet, set coarsely
optimizing for a large collection of models. Optimizing the
batch- versus request-parallelism at a finer granularity, by
taking into account particular model architectures and hardware platforms, enables further performance gains. To enable
this finer granularity optimization and account for the diurnal
production traffic asf well as intra-day query variability, we
deploy and evaluate DeepRecSched over the course of 24
hours. Compared to the baseline configuration, the optimal
991
Opt Baseline Opt Baseline
Reduce
p95 by
1.39x Reduce
p99 by
Hill 1.31x
climbing
Hill
climbing
Fig. 14: Exploiting the request vs. batch-level parallelism optimization demonstrated by DeepRecSched in a real production
datacenter improves performance of at-scale recommendation
services. Across models and servers, optimizing batch size
reduces p95 and p99 latency by 1.39× (left) and 1.31× (right).
batch size provides a 1.39× and 1.31× reduction in p95
and p99 tail latencies, respectively. This reduction in the tail
latency can be used to increase system throughput (QPS) of
the cluster of machines.
C. Leverage Parallelism with Specialized Hardware
In addition to trading off request vs. batch-level parallelism,
DeepRecSched-GPU leverages additional parallelism by offloading recommendation inference queries to GPUs.
Performance improvements. GPUs are often treated as
throughput-oriented accelerators. However, in the context of
personalized recommendation, we find that GPUs can unlock
lower tail latency targets unacheivable by CPUs. Figure 15(a)
illustrates the performance impact of scheduling requests
across both CPUs and GPUs. While the lowest achievable taillatency targets for DLRM-RMC1 on CPUs is 57ms, GPUs can
achieve a tail-latency target of as low as 41ms (1.4× reduction). This is a result of recommendation models exhibiting
high compute and memory intensity, as well as the heavy tail
of query sizes in production use cases (Figure 3).
Next, in addition to achieving lower tail latencies, parallelization across both the CPU and the specialized hardware increases system throughput. For instance, Figure 15(a)
shows that across all tail-latency targets, DeepRecSched-GPU
achieves higher QPS than DeepRecSched-CPU. This is as a
result of the execution of the larger queries on GPUs, enabling
higher system throughput. Interestingly, the percent of work
processed by the GPU decreases with higher tail latency targets. This is due that, at a low latency target, DeepRecSchedGPU optimizes system throughput by setting a low query size
threshold and offloads a large fraction of queries to the GPU.
Under a more relaxed tail-latency constraint, more inference
queries can be processed by the CMPs. This leads to a higher
query size threshold for DeepRecSched-GPU. At a tail latency
target of 120ms, the optimal query size threshold is 324 and the
percent of work processed by the GPU falls to 18%. As shown
in Figure 12(top), optimizing the query size threshold yields
DeepRecSched-GPU’s system throughput improvements over
the static baseline and DeepRecSched-CPU across the different
tail latency targets and recommendation models.
Infrastructure efficiency implications. While GPUs can
enable lower latency and higher QPS, power efficiency is not
CPU
% work processed
by GPU
GPU
GPU
CPU
CPU
optimal
GPU
optimal
Fig. 15: (Top) System throughput increases by scheduling
queries across both CPUs and GPUs. The percent of work
processed by the GPU decreases at higher tail latency targets.
(Bottom) While QPS strictly improves, the optimal configuration based on QPS/Watt, varies based on tail latency targets.
always optimized with GPUs as the specialized AI accelerator. For instance, Figure 15(b) shows the QPS/Watt of both
DeepRecSched-CPU and DeepRecSched-GPU for DLRMRMC1, across a variety of tail latency targets. At low tail latency targets, QPS/Watt is maximized by DeepRecSched-GPU
— parallelizing queries across both CPUs and GPUs. However, under more relaxed tail-latency targets, we find QPS/Watt
is optimized by processing queries on CPUs only. Despite the
additional power overhead of the GPU, DeepRecSched-GPU
does not provide commensurate system throughput benefits
over DeepRecSched-CPU at higher tail latencies.
More generally, power efficiency is co-optimized by considering both the tail latency target and the recommendation
model. For instance, Figure 12(b) illustrates the power efficiency for the collection of recommendation models across
different tail latency targets. We find that DeepRecSchedGPU achieves higher QPS/Watt across all latency targets for
compute-intensive models (i.e., NCF, WnD, MT-WnD) — the
performance improvement of specialized hardware outweighs
the increase in power footprint. Similarly, for DLRM-RMC2
and DIEN, DeepRecSched-GPU provides marginal power efficiency improvements. On the other hand, the optimal configuration for maximizing power efficiency of DLRM-RMC1
and DLRM-RMC3 varies based on the tail latency target.
As a result, Figure 12(b) shows that in order to maximize
infrastructure efficiency, it is important to consider model
architecture and tail latency targets.
D. Datacenter provisioning implications.
In addition to the scheduling optimizations offered by
DeepRecSched, the analysis can be applied to study provisioning strategies for datacenters running the wide collection
of recommendation models. Figure 16 considers the ratio of
CPU to GPUs, in order to minimize total power consumption,
as we vary tail latency targets (left) and GPU power efficiency
(right). Here, all models serve an equal amount of traffic
(QPS); the tradeoffs will vary based on the distribution of
992
Target latency
150ms
Baseline
NVIDIA 1080 Ti
GPU
GPU unlock
lower-latency
use cases
Higher ratio of CPU
under relaxed latency targets
Fig. 16: Evaluating datacenter provisioning strategies for recommendation in order optimize overall power footprint. (Left)
Increasing tail-latency reduces the fraction of GPUs. (Right)
Improving GPU TDP, such as with more power efficient
accelerators, increases the fraction of GPUs.
models deployed. Figure 16 shows higher ratios of GPUs are
optimal under lower latency targets. Intuitively, this follows
Figure 15 as GPUs enable lower latency recommendation use
cases by accelerating the large queries. However, under more
relaxed tail latency (i.e., SLA) targets it is optimal deploy
higher ratios of CPUs for recommendation inference. Note, tail
latency targets vary across applications as shown in Table II.
In addition to the impact of varying SLA targets, accelerator power efficiency also impacts datacenter provisioning.
Figure 16 (right) considers the impact of varying the power
efficiency of the NVIDIA GTX 1080 Ti GPU. Intuitively,
improving power efficiency makes accelerators more appealing for recommendation inference. Thus, designing efficient
GPUs and accelerators may enable specialized hardware for
recommendation inference at the datacenter scale.
VII. RELATED WORK
While the system and computer architecture community has
devoted significant efforts to characterize and optimize deep
neural network (DNN) inference efficiency, relatively little
work has explored running recommendation at-scale.
DNN accelerator designs. Currently-available benchmarks
for DNNs primarily focus on FC, CNNs, and RNNs [35], [45]–
[47]. Building upon the performance bottlenecks, a variety
of software and hardware solutions have been proposed to
optimize traditional DNNs [13]–[15], [17], [48]–[68]. While
the benchmarks and accelerator designs consider a variety of
DNN use cases and systems, prior solutions do not apply to
the wide collection of state-of-the-art recommendation models
presented in this paper. For example, recent characterization of
Facebook’s DLRM implementation demonstrates that DNNs
for recommendation have unique compute and memory characteristics [3], [10]. These implementations are included, as
DLRM-RMC 1-3, within DeepRecInfra. In addition, MLPerf,
an industry-academic benchmark suite for machine learning,
provides NCF as a training benchmark [28]. (MLPerf is developing a more representative recommendation benchmark for
the next submission round [69], [70]). In addition, an important
aspect of the end-to-end infrastructure presented in the paper
is accounting for at-scale request characteristics (arrival rate
and size), particularly important for recommendation.
Optimizations for personalized recommendation. A few
recent works explore design optimization opportunities for recommendation models. For instance, TensorDimm proposes a
near memory processing solution for recommendation models
similar to DLRM-RMC 1-3 and NCF [18]. Ginart et al. and Shi
et al. [71], [72] compresses embedding tables in recommendation models while maintaining the model accuracy. In contrast,
this paper optimizes the at-scale inference performance of a
wider collection of recommendation models by considering
the effect of inference query characteristics.
Machine learning at-scale. Finally, prior work has examined the performance characteristics and optimizations for
ML running on at-scale, warehouse scale machines. Sirius
and DjiNN-and-Tonic explore the implications of ML in
warehouse-scale computers [26], [73]. However, the unique
properties of recommendation inference and query patterns
have not been the focus of the prior work. Li et al. [22] exploit
task and data-level parallelism to meet SLA targets of latency
critical applications i.e., Microsoft’s Bing search. Furthermore, recent work has open-sourced benchmarks for studying
the performance implication of at-scale execution of latency
critical datacenter workloads and cloud micro-services [27],
[30]. This paper provides an end-to-end infrastructure (DeepRecInfra) and design solutions (DeepRecSched) specialized
for recommendation inference. DeepRecInfra models realtime query patterns, representative of the distinct working set
size distribution. The unique characteristics provide significant
performance improvement for at-scale recommendation.
VIII. CONCLUSION
Given the growing ubiquity of web-based services that use
recommendation algorithms, such as search, social-media, ecommerce, and video streaming, neural personalized recommendation comprises the majority of AI inference capacity and
cycles in production datacenter. We propose DeepRecInfra,
an extensible infrastructure to study at-scale recommendation
inference comprising eight state-of-the-art recommendation
models, SLA targets, and query patterns. Built upon this
framework, DeepRecSched exploits the unique characteristics
of at-scale recommendation inference in order to optimize
system throughput, under strict tail latency targets, by 2×. In
a real production datacenter, DeepRecSched achieves similar
performance benefits. Finally, through judicious optimizations,
DeepRecSched can leverage additional parallelism by offloading queries across CPUs and specialized AI hardware to
achieve higher system throughput and infrastructure efficiency.