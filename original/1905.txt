Abstract‚ÄîAs the models and the datasets to train deep learning
(DL) models scale, system architects are faced with new challenges, one of which is the memory capacity bottleneck, where the
limited physical memory inside the accelerator device constrains
the algorithm that can be studied. We propose a memory-centric
deep learning system that can transparently expand the memory
capacity available to the accelerators while also providing fast
inter-device communication for parallel training. Our proposal
aggregates a pool of memory modules locally within the deviceside interconnect, which are decoupled from the host interface
and function as a vehicle for transparent memory capacity
expansion. Compared to conventional systems, our proposal
achieves an average 2.8√ó speedup on eight DL applications and
increases the system-wide memory capacity to tens of TBs.
Index Terms‚ÄîSystem architecture, HPC, machine learning
I. INTRODUCTION
Deep learning (DL) models and its training datasets are
scaling at a phenomenal rate, so the progress in DL is primarily
limited by how fast the deep neural network (DNN) model
can be evaluated and how large of a memory you can utilize
for training. DL practitioners are therefore seeking efficient
parallel training solutions, increasingly adopting a dense system node design, which houses several PCIe-attached coprocessor devices [1]‚Äì[3] to increase the node-level throughput. As the system-level performance is contingent upon
how the DL algorithm is parallelized across the devices and
how effectively they communicate with each other, leading
vendors in this space are employing a custom device-side interconnection network that utilizes proprietary high-bandwidth
signaling solutions (e.g., NVIDIA‚Äôs NVLINK which provides
100s of GB/sec of bandwidth) for fast communication and
synchronization [3], [4]. Such device-centric deep learning
system architecture (DC-DLA) is becoming mainstream for DL
(Figure 1(a)) and we are seeing an increasing number of HPC
systems that employ a standalone, device-side interconnection
network for efficient DL parallelization [3], [5]‚Äì[7].
As researchers seek to deploy deeper and larger DNN
topologies however, end-users are faced upon a memory
‚Äúcapacity‚Äù wall, where the limited on-device physical memory
(based on on-package 3D stack memory [8] in NVIDIA‚Äôs
V100 [1], Google‚Äôs TPUv2 [2], and Intel-Nervana‚Äôs Lake
Crest accelerator [3]) constrains the algorithm that can be
trained [9]‚Äì[12]. Increasing the capacity of these on-package
stacked DRAM however is challenging due to wireability
of the silicon interposers, chip pinout required to drive the
added DRAM stacks, and technology limits on how many
DRAM stacks you can vertically integrate. Consequently,
recent solutions have proposed to use the device (GPU/TPU)
memory as an application level cache with respect to the
host CPU memory [9], [10], [13]‚Äì[16], effectively virtualizing
DNN memory usage across the host and device memory
via PCIe (Section II-B). The effectiveness of these prior
solutions however is sensitive to the host-device communication bandwidth as it determines the latency incurred in
migrating DNN data in/out of these two memory regions. The
left-axis in Figure 2 shows the execution time of state-ofthe-art convolutional neural networks (CNNs) on successive
versions of a single, high-end DL accelerator, which has been
reduced by a factor of 20√ó to 34√ó over five years. During
these time periods, the signaling circuitry and the overall
communication bandwidth offered by the latest PCIe and
InfiniBand has improved only by a factor of 1√ó (PCIe gen3)
and 3.5√ó (i.e., IB-FDR to IB-HDR), respectively. This has led
to a steadily increasing performance overhead of host-device
memory virtualization via PCIe (right-axis in Figure 2), where
the growing performance gap between the device computing
power and host-device (PCIe) communication bandwidth aggravates system-level performance. Virtualizing DNN memory
over a multi-GPU/TPU system incurs even higher performance
overheads because the effective host‚Äìdevice communication
bandwidth allocated per device gets proportionally reduced
to the number of intra-node devices. Therefore, the overall
system can experience a significant performance slowdown
due to the additional latency incurred during host‚Äìdevice
memory copies (Section V). Overall, current trends point to
an urgent need for a system architectural solution that satisfies
the dual requirements of (a) fast inter-device communication
for parallel training, and (b) high-performance memory virtualization over a large memory pool to enable memory hungry
DNNs to be trainable over accelerator devices.
In this paper, we make a case for a memory-centric deep
learning system architecture (MC-DLA) that aggregates a pool
of capacity-optimized memory modules within the deviceside interconnect for transparent memory capacity expansion
(Figure 1(b)). While our proposal is reminiscent of prior
disaggregated memory proposals [17], [18], the CPU-centric
memory disaggregation solutions suffer from the same performance bottlenecks of DC-DLA because of its reliance on PCIe.
In our proposal, the pool of memory modules (henceforth
referred to as memory-nodes) are completely decoupled from
the legacy, host-device interface (e.g., PCIe) and are stationed
148
2018 51st Annual IEEE/ACM International Symposium on Microarchitecture
978-1-5386-6240-3/18/$31.00 ¬©2018 IEEE
DOI 10.1109/MICRO.2018.00021
 

 

 

 


 

	 	
 

!
 

 

 

 


 

	 	
 

!

 
 
 
 
 
 
 

  "
 
 
 !
	 	 


 
"



 

 
 
	!
Fig. 1: (a) A device-centric deep learning system architecture, and
(b) a memory-centric deep learning system architecture.
locally within the device-side interconnect. We propose to
interconnect the accelerators and the memory-nodes using the
high-bandwidth, low-latency signaling links (e.g., NVLINK)
and utilize the memory-nodes as a backing store to the
accelerators. This allows the memory-nodes to function as a
vehicle for transparent memory capacity expansion, allowing
researchers to train DL algorithms that are much larger,
deeper, and more complex. Because the accelerators access the
memory-nodes via the high-bandwidth links, the performance
overhead of virtualizing memory can be substantially reduced.
At the same time, MC-DLA connects the accelerators and the
memory-nodes in a manner that maximizes inter-device communication bandwidth so that DC-DLA‚Äôs high-efficiency in
conducting collective communication operations is maintained.
Overall, this paper makes the following contributions:
‚Ä¢ To the best of our knowledge, our work is the first that
highlights the importance of device-side interconnects in
training scaled up DL algorithms, presenting a quantitative analysis on parallel training in the context of HPC
systems with multiple accelerator (GPU/TPU) devices.
‚Ä¢ This work identifies key system-level performance bottlenecks on DC-DLA and motivates the need for a new
system architecture that balances fast communication and
user productivity in training large DNN algorithms.
‚Ä¢ We propose and evaluate a system architecture called
MC-DLA that provides transparent memory capacity expansion while also enabling fast inter-device communication. Compared to DC-DLA designs, our proposal
achieves an average 2.8√ó performance improvement
while expanding the system-wide memory capacity exposed to the accelerators to tens of TBs.
II. BACKGROUND AND MOTIVATION
A. DL Training versus Inference
DNNs require training to be ready for inference. Training is
a three-step process that involves learning the optimal values
of the DNN weights using the backpropagation algorithm [19].
First, a serialized, layer-wise computation process called forward propagation is taken from the first (input) layer to the
last (output) layer in a serialized fashion (the blue arrows
from bottom to top in Figure 3). A given layer applies a
set of mathematical operation (e.g., convolution, activation,
recurrence, etc) to the input feature maps (X) and derives
the output feature maps (Y), which is forwarded to the next


 

	



 

	


!"
'&
#
 $
%

!"
'&
#
 $
%

!"
'&
#
 $
%

!"
'&
#
 $
%

'$  $ 
 #$






	
 



	
 %"
Fig. 2: Execution time of running state-of-the-art CNN models across
five recent generation of a single DL accelerator device (left-axis)
and the performance overhead incurred due to memory virtualization
(right-axis). See Section IV for evaluation methodology.
layer to be used as its input feature maps. At the end of
forward propagation, a prediction of the input is given, which
is compared against the ground truth. The defined loss function
quantifies the magnitude of the error between the current
prediction and the ground truth, which is encapsulated in a
value called gradients of the loss function with respect to
the last layer‚Äôs output. Then, backpropagation is performed in
the opposite direction of forward propagation (the red arrows
from top to bottom) again in a layer-wise manner, where the
incoming gradients (dY) are used to derive the output gradients
(dX) to be sent to the previous layer to be used as its input
gradients. Using the dY and dX, each layer derives its own
weight gradients (dW) to adjust its own layer‚Äôs weights (W)
so that the loss value is incrementally reduced, improving the
performance of the DNN model.
B. Virtualizing Memory for Deep Learning
The chain-rule based backpropagation algorithm requires
a given layer‚Äôs input feature maps values (X) to derive the
gradient values of the layer‚Äôs weights (dW) [19]. Consequently,
the overall memory allocation size of DNN training scales
proportionally to the network depth (i.e., memory cost of
O(N) to train a DNN with N layers). End-users must therefore
carefully tune their network topology (i.e., the number of
layers in a DNN and the inter-layer connections) and the
training batch size to make sure the overall memory requirement fits within the physical memory capacity, which can
severely limit user productivity. Given recent research trends
where DL practitioners are seeking to deploy ever larger
and deeper network algorithms (e.g., the memory allocations
required for training can easily exceed 100s of GBs [20]‚Äì[26]),
tackling this memory capacity bottleneck while minimizing
performance overheads becomes vital in enabling researchers
to keep studying scaled up DL algorithms. Prior work on
virtualizing memory usage of DNNs [9], [10], [13]‚Äì[16] have
proposed to utilize both host and device memory concurrently
for allocating data structures for DNN training. By leveraging
the user-level DNN topology graph as means to extract a
compile-time data dependency information (which is encapsulated as a direct acyclic graph (DAG) data structure) of
the memory-hungry data structures, e.g., feature maps (X)
and/or weights (W), DNN virtual memory can leverage this
149
   
 
               
Fig. 3: (a) Data-parallel and (b) model-parallel training. Blue and
red arrows inside each device represent a given layer‚Äôs computation
during forward and backward propagation, respectively. The blue and
red arrows that crosses the boundaries of two devices represent the
per layer inter-device communication and synchronization operations.
As shown, model-parallel training incurs much more frequent synchronization than data-parallel training [31], [32].
data dependency information to derive the DNN data reuse
distance to schedule performance-aware data copy operations
via memory-overlaying across host and device memory via
PCIe [27]‚Äì[29]. Existing DL frameworks [10], [15], [30]
therefore opt to leverage this DAG to schedule software-level
DMA initiated data transfers between host and device, overlapping it with the DNN forward and backward propagation,
to maximally utilize the PCIe communication bandwidth and
minimize the performance overheads of data migration. By
only keeping soon-to-be used DNN data inside the device
memory, the memory allocation size of training a network
with N layers can be reduced from O(N) to O(1), enhancing
DL practitioners‚Äô ability to train scaled-up algorithms.
C. Parallelization of DL Algorithms
As the DNN algorithm gets more complex and deeper [20]‚Äì
[22], the need for distributed multi-node systems, each with
multiple accelerator devices, have significantly increased to
provide high computing horsepower for DL practitioners. Consequently, efficient parallelization of DL algorithms and fast
communication among the devices become vital for maximally
exploiting the HPC systems based on these dense multi-device
nodes. Note that the scope of this paper is on developing an
efficient intra-node system architecture, so as in conventional
designs, we assume that inter-node communication is handled
using MPI via Ethernet or InfiniBand.
Parallel DL Training. The most popular parallel training
strategies employed by DL frameworks are data-parallel and
model-parallel training (Figure 3). Data-parallel training is a
parallelization scheme that allocates the same network model
across all the workers, but each worker is assigned with
a different batch of the overall training dataset. In modelparallel training however, all workers work on an identical
batch of the training dataset (i.e., the problem size is fixed
at batch size N), but each are allocated with different portions of the network model. The parallel tasks distributed
across the workers must periodically synchronize to have a
consistent DNN model trained within each worker, preventing
both data-parallel and model-parallel training from achieving
perfect scaling. Model-parallel training generally incurs much
frequent synchronization than data-parallel approaches as the
input feature maps (X) and gradients (dX, dW) must be
aggregated across layer boundaries due to the nature of its
       
       


	
       
       
	 	
       
       
 
 
Fig. 4: Key collective communication primitives for parallel training.
parallelization algorithm. Data-parallel training, on the other
hand, only requires the accumulation of dW during backpropagation and is therefore assumed to be more amenable for
achieving close to linear speedup. However, not all networks
or layers can be data-parallelized easily, especially for DNNs
with large models [24], [33], so both model and data-parallel
training are considered important in quantifying the robustness
of the system interconnect design of HPC systems.
Communication. As implied through previous discussions, minimizing communication overheads is key in highperformance parallel training. Consequently, maximally utilizing the communication bandwidth provisioned across accelerators within and across compute nodes is crucial. Key
collective communication primitives for parallel training are
all-gather (X), all-reduce (dX and dW), and broadcast (dW),
which are shown in Figure 4. Prior work [34], [35] has
demonstrated that the ring-algorithm based collective communication can provide optimal link bandwidth utilization for
the aforementioned collective operations. Leading system vendors in this space are therefore employing a topology-aware,
ring-algorithm based collective communication library (e.g.,
NVIDIA‚Äôs NCCL [36], IBM‚Äôs PowerAI DDL [30], and Baidu‚Äôs
AllReduce [37]). These libraries cast the underlying system
interconnect as multiple ring networks and orchestrate the
DL communication operations based on the ring-algorithm for
maximizing bandwidth utilization while minimizing latency.
Device-side Interconnects for DL. For efficient communication and synchronization across accelerator devices,
recent HPC systems for DL are employing proprietary, highbandwidth device-side interconnection networks that provide
100s of GB/sec of inter-device communication bandwidth.
Intel-Nervana‚Äôs Lake Crest accelerator [3] employs 12 highbandwidth signaling links (20√ó that of what PCIe provides)
that can tightly couple the DL accelerator devices with each
other. NVIDIA‚Äôs DGX system [5] is equipped with 8 Volta
V100 [1] GPUs where each V100 comes with 6 highbandwidth NVLINKs (bi-directional bandwidth of 50 GB/sec
per link, aggregate channel bandwidth of 300 GB/sec per
GPU), which are used to form a cube-mesh topology across
eight V100s (Figure 5). By casting the cube-mesh topology as
three ring interconnects, the eight GPUs communicate through
these high-bandwidth ring networks using the NCCL library,
which helps achieve optimal bandwidth utilization and minimize latency. While such device-centric deep learning system
architecture (DC-DLA) solution has advantages in terms of
inter-device synchronizations, communicating with the hostside CPU can only be done using the legacy PCIe link, which
150








Fig. 5: The cube-mesh, device-side interconnect employed in
NVIDIA‚Äôs DGX system. The black, gray, and dotted arrows form
three ring networks for collective communication operations [5], [7].
can cause severe performance bottlenecks for devices utilizing
the host CPU memory for memory virtualization.
One might expect that a system architecture that is optimized in the other end of the spectrum, where the high bandwidth links are all or partially used to access CPU memory,
could achieve the best of both system-level communication
(that is, providing decent communication bandwidth using the
now singular or duo ring networks) and high-performance
virtual memory (having high enough bandwidth to read/write
to/from CPU memory). Such host-centric DL system architecture (HC-DLA) however falls short on several aspects. First, an
HC-DLA system is simply not a feasible option to begin with
for the vast majority of the current, x86-based HPC systems
because these proprietary high-bandwidth signaling links for
device-side interconnects are incompatible with x86 CPUs.
HC-DLA systems that do enable high-bandwidth CPU-GPU
communications (e.g., IBM Power + NVIDIA GPUs) still face
the following challenges. First, allocating a subset of the highbandwidth links to connect to the host CPU leaves smaller
inter-device bandwidth available for device communication,
which could potentially slowdown the effective node-level
throughput for algorithms sensitive to inter-device communications. More importantly, however, having just a single highbandwidth link per each accelerator device directly connected
to the host CPU can leave little memory bandwidth available
to the CPU itself, leading to a highly unbalanced system
design. As we detail in the next section, virtualizing the
memory usage of DL algorithms require the DMA engine
to fully utilize the communication link bandwidth in order to
effectively hide the latency of copying data in and out of device
memory. This means that a singular high-bandwidth link of 25
GB/sec per device would amount to a total of 100 GB/sec of
worst-case host-side memory bandwidth consumption when
accounting for the four PCIe-attached devices connected to
a single CPU socket. As a point of reference, the maximum
memory bandwidth available for a high-end Intel Xeon CPU
and the IBM Power9 is ‚Äúonly‚Äù 80 GB/sec [38] and 120
GB/sec [39] per socket, respectively, due to CPU‚Äôs latencyoriented design (rather than the throughput-oriented GPUs or
Google‚Äôs TPUs [2] that require high bandwidth rather than low
latency). As quantitatively discussed in Section V, HC-DLA
can consume an average 92% of host-side memory bandwidth
for certain workloads, leaving only 8% of memory bandwidth
available for the host CPU itself. While we explore such
design point in this paper, we argue that such unbalanced
system architecture is less practical for future DL systems
as it severely lacks design flexibility; that is, the amount of
read/write throughput the system designer can provision for
host-device memory virtualization is limited by the maximum
memory bandwidth available per each CPU socket, regardless
of how much device-side high-bandwidth links are available,
which can cause severe bottlenecks for future algorithms that
are much larger, deeper, and more complex.
III. MEMORY-CENTRIC HPC SYSTEM ARCHITECTURE
FOR DEEP LEARNING
In this paper, we propose a new architectural solution
for future HPC systems optimized for deep learning. Our
goal is to develop a DL system architecture that enables
fast inter-device communication for parallel training while
at the same time provisioning high-bandwidth communication
channels to a pool of capacity-optimized memory modules
(which we refer to as memory-nodes in the rest of this paper)
for high-performance virtual memory. We argue that HPC
system architectures for DL training should be designed in
a memory-centric manner as the memory ‚Äúcapacity‚Äù wall
poses one of the biggest challenges in training deep and large
learning algorithms [9]‚Äì[11]. Prior work on disaggregated
memory [17], [18] can similarly expand the pool of memory
exposed to the system through a separate memory-blade accessed over PCIe or the NIC. Similar to DC-DLA however,
the growing performance gap between (GPU/TPU) device
computing power and host-device communication (Figure 2)
renders the CPU-centric, PCIe-based memory disaggregation
solutions impractical for deep learning training as the latency
to access the added memory pool will become bottlenecked
by PCIe. Consequently, the memory-nodes in our memorycentric DL system architecture (MC-DLA) are stationed locally
inside the device-side interconnection network, eliminating all
its ties with the host PCIe interface. This section details the
design of the memory-node architecture and its application for
our proposed MC-DLA system that leverages these memorynodes as building blocks to achieve the aforementioned design
goals. As the scope of this paper is on studying the intra-node
system architecture, we refer to the PCIe-attached accelerator
devices (e.g., GPUs or TPUs) as device-nodes in the rest of
this paper because both the memory-nodes and device-nodes
function as separate nodes inside the device-side interconnect
(Figure 1(b)). MC-DLA is applicable for both GPUs and
TPUs as our proposal concerns an efficient system architecture
design for DL accelerators (i.e., the device-nodes). For ease
of explanation, we assume the device-nodes are based on
GPUs and use terminologies defined in NVIDIA‚Äôs CUDA
hardware/software interface in the remainder of this section.
A. Memory Node Architecture
The key objective of our memory-node design is to unlock
the high-bandwidth communication channels of the deviceside interconnect for high-performance virtual memory. Figure 6 illustrates the design of our memory-node architecture,
which contains N high-bandwidth links for communicating
with the device-side interconnection network. The N links are
logically partitioned into M groups (M‚â§N) and each group
of (N/M) links are used exclusively by a designated devicenode for DNN memory virtualization. A protocol engine that
151















 














 



 





















 

















	
 

	

Fig. 6: Memory-node architecture.
is compatible with the device-side interconnect is used to
provide a maximum bandwidth of B GB/sec per link, so a
device-node assigned with a group of (N/M) links can utilize
the DMA engine to read (write) data from (to) the memory
DIMMs with (N/M)√óB GB/sec of throughput. The DMA
engine forwards a device-node‚Äôs data transfer request to the
memory controller which has an array of commodity memory
DIMMs it manages. An ASIC that handles encryption or compression can optionally be added to the memory-node. This
paper assumes that the memory DIMMs are populated with
capacity and density optimized commodity memory solutions:
from 8‚Äì16 GB DDR4 RDIMMs (registered DIMMs) to 32‚Äì
128 GB LRDIMMs (load-reduced DIMMs). To narrow down
the design space we explore, the rest of this paper assumes that
the board housing a single memory-node is sized equivalent
to a high-end PCIe accelerator board to be compatible with
existing device-side interconnects and minimize the design
costs of the server chassis enclosure. A memory-node built
out of a mezzanine board sized equivalent to Volta V100‚Äôs
(14 cm √ó 8 cm) can house ten DDR4 DIMMs, providing a
maximum of 170 GB/sec (PC4-17000) to 256 GB/sec (PC4-
25600) of memory bandwidth with an overall memory capacity
expansion of 80 GB to 1.3 TB per memory-node.
B. System Architecture
As our work is the first that highlights the importance of
device-side interconnects in training scaled-up DL algorithms,
system architects are given a wide design space under our
proposal. A full design space exploration is beyond the scope
of this paper, so this section presents three system interconnect
design points that incorporate our memory-nodes and discuss
their trade-offs in terms of link bandwidth utilization and
overall performance. To narrow down our design options, we
assume that the number of device-nodes and memory-nodes
are identical and that all device-nodes and memory-nodes have
N high-bandwidth communication links to interface with the
other nodes in the network (each link providing B GB/sec of
uni-directional communication bandwidth, Figure 6). We use
the system configuration of NVIDIA‚Äôs DGX system (N=6 highbandwidth links per device, each link providing B=25 GB/sec
communication bandwidth) as a running example to describe
the design intuitions behind MC-DLA.
System Interconnect. The design objective of the MC-DLA
device-side interconnect is to balance communication, memory
virtualization, and overall design complexity. A straightforward and an intuitive interconnect design that can utilize
our memory-nodes as a backing store to the device-nodes

 

	

 
 
	
	
	 

 

	

 

	
	
	
(a)
	 	


	 	 





 
	 	


	 	






(b)


	
	



 
	
	  
 	




	
	

 	 

(c)
Fig. 7: MC-DLA system interconnect containing 8 device-nodes and
8 memory-nodes: (a) a derivative interconnect design based on the
cube-mesh topology of Figure 5, where the device-nodes (D0‚àí7)
communicate with the memory-nodes (M0‚àí7) under a star topology,
(b) the memory-nodes folded inward, and (c) the proposed ringbased system interconnect. Nodes that are part of the same ring are
interconnected using the same color-coded arrows (links).
is shown in Figure 7(a). Here, the communication links that
constitute one of the (N/2)=3 ring networks in Figure 5 (e.g.,
the singular ring network constructed using the 8 bi-directional
black arrows) are rearranged to construct a ring network using
the 8 memory-nodes and 8 device-nodes. Each device-node is
now provided with the ability to access its designated memorynode using two high-bandwidth links (50 GB/sec communication bandwidth between Dn‚ÜîMn), significantly reducing the
latency to migrate data to/from the backing store. There are
two significant limitations with this design however, as (1)
the 3 rings used for inter-device collective communication are
constructed in a highly unbalanced fashion (i.e., 2 rings are
constructed with a maximum 8 hop count while the remaining
ring incurs a maximum 24 hop count1), rendering the overall
communication latency be bottlenecked by the longest ring,
and (2) the 8 light-gray/dotted bi-directional links are neither
being utilized for communication nor for memory virtualization, failing to maximally utilize available communication
resources2. Figure 7(b) is an alternative design point that
1In Figure 7(a), each memory-node is visited twice when traversing the
black-arrowed ring network, e.g., ¬∑¬∑¬∑ M0‚ÜíD0‚ÜíM0‚ÜíM7‚ÜíD7‚ÜíM7 ¬∑¬∑¬∑ 2The light-gray/dotted arrows form the 4th ring with only the 8 memorynodes, without any device-nodes (i.e., all device-nodes are already fully utilizing the N=6 links). For parallel DL training, the messages to be communicated
across the devices are generated by the device-nodes (stored inside GPU
memory) and never inside the memory-nodes, so the 4th ring does not help
improve the performance of communication nor memory virtualization.
152

	



	 




 
 



 













 












(a)

















 


 

	

	





(b)
Fig. 8: (a) Ring-based MC-DLA system interconnect optimized for
packaging and design complexity (e.g., equal length inter-node links),
and (b) the physical design of MC-DLA where each half of the ring
is connected via the enclosure backplane.
better balances the 3 ring networks‚Äô performance (the 3 rings
containing the device-nodes are constructed with a maximum
8, 12, and 20 hop count, respectively), but it similarly suffers
from the aforementioned limitations: unbalanced ring design
and underutilization of communication resources.
To holistically address these challenges, we arrive at our
ring based MC-DLA interconnect that maintains competitive
collective communication performance of DC-DLA while, at
the same time, significantly improving and maximizing the
communication bandwidth available for memory virtualization. Figure 7(c) illustrates our proposed ring-based MC-DLA
interconnect design (N/2=3 rings overall), where a given
device-node is provided with a pair of high-bandwidth links
to two memory nodes located on its left and right side of
any given ring. The key advantage of our ring-based MC-DLA
architecture is twofold. First, each device-node is now able
to utilize the two memory-nodes located on its (logical) left
and right side of a given ring, maximally utilizing the N=6
links for virtualizing memory (3√ó higher bandwidth than in
Figure 7(a,b)). This allows MC-DLA to achieve (number of
rings)*(link bandwidth to left and right nodes) = (N/2)*(2*B)
= 150 GB/sec of communication bandwidth, a significant
improvement over the legacy PCIe. Second, the communication bandwidth to the memory-nodes can linearly scale,
proportional to the signaling technology used to implement
these high-bandwidth links, as opposed to the PCIe-based
DC-DLA or HC-DLA design, whose maximal communication bandwidth is capped at the maximum CPU socket-level
memory bandwidth. For instance, both DC-DLA and HC-DLA,
regardless of whether the host-device interface is designed
using NVLINK or the next-generation PCIe, can only provide

	

	

	
  
     
     
         





	
 

	



  

 


Fig. 9: Latency incurred when performing collective communication
primitives, as a function of the number of nodes inside the ring
network (normalized to a ring with 2 nodes). Each link has 50
GB/sec of bi-directional bandwidth and the nodes communicate with
a message size of 4 KB with a target synchronization size at 8 MB.
up to the maximum per CPU socket-level memory bandwidth,
which is approximately 80 GB/sec and 120 GB/sec for highend Intel Xeon and IBM Power CPUs, respectively. The eight
accelerator devices in our MC-DLA has (150 GB/sec per device
√ó 8 devices) = 1200 GB/sec of communication bandwidth
to its neighboring memory-nodes, the number of which will
proportionally grow as a function of the link bandwidth of B
GB/sec. Figure 8 is an illustration of our ring-based MC-DLA
re-designed to be optimized for packaging costs, easing its
adoption for real-world HPC systems.
One might be concerned that the latency incurred for
collective communications will increase as MC-DLA adds 8
additional (memory) nodes, effectively doubling the number
of nodes inside the ring. For reasonably large messages, our
ring-based MC-DLA with 16 nodes incur negligible latency
overheads for all-gather, broadcast, and all-reduce (Figure 9).
When the communication size is small, MC-DLA does incur
higher latency than DC-DLA, but in such scenario the communication latency is not a performance limiter to begin with
(Amdahl‚Äôs law). We demonstrate in Section V that the impact
of this latency overhead is negligible on system performance.
Software Interface. MC-DLA builds upon the memoryoverlaying based DNN memory virtualization solutions [9],
[10], which assume the following: (1) the high-level DL framework analyzes the neural network DAG structure at compiletime and derives the data-dependencies of memory-hungry
DNN data, (2) this information is utilized by the runtime
memory manager to schedule performance-aware, softwaremanaged memory overlaying operations (i.e., DMA-initiated
cudaMemcpyAsync) across host-device memory to expand
the reach of memory available for training. The MC-DLA
design introduces another tier of memory region in addition to
the host and device memory ‚Äì the capacity-optimized memory
inside the memory-nodes, which we refer to as deviceremote
memory in the rest of this paper. We propose to utilize
deviceremote memory to supplant the role of host memory
for stashing DNN data with long reuse distance. In other
words, memory virtualization is implemented using the local
device (devicelocal) memory and deviceremote memory
without having the CPU memory involved. To allow the
runtime memory manager to (de)allocate data structures inside
153
TABLE I: Software API extensions for MC-DLA.
API Arguments Semantics
cudaMallocRemote &src, size malloc size bytes to deviceremote
memory and return ptr to src
cudaFreeRemote &src free memory that is allocated under
deviceremote memory
cudaMemcpyAsync &src, &dst, copy size bytes from src to dst,
size, but direction now includes
direction LocalToRemote and RemoteToLocal
deviceremote memory and initiate DMA data transfers in/out
of this memory region, we introduce three extensions to the
CUDA runtime APIs (libcudart.so) for deviceremote
memory (de)allocation and memory copy (Table I). Using
these APIs, existing DL frameworks can seamlessly exploit
the additional pool of memory inside our memory-nodes.
System Software Support. MC-DLA requires the device
driver to be able to (de)allocate memory in deviceremote
memory and be able to map that address space to userlevel programs. Under our design, any given memory-node
is logically partitioned into two groups and all the resources
within a group (e.g., DMA engine, memory controller, and
memory DIMMs) are exclusively assigned to a single devicenode for servicing its requests (Figure 8). As these resources
are not to be shared by any two device-nodes by design,
the device driver manages both its client device-node and the
each half of the left and right side memory-nodes‚Äôs physical
memory under a single device memory address space. Consequently, the devicelocal physical memory lives at the bottom
of this single device memory address space and each half
of the two deviceremote physical memory is concatenated
and mapped into the higher address space (Figure 10). From
the driver‚Äôs perspective, the device-node augmented with its
share of memory-nodes can be thought of as a single PCIe
device but with a larger memory capacity (e.g., Maxwell M40
containing 12 GB versus Volta V100 with 16 GB), hence
existing system software APIs (e.g., mmap) can be used asis to map the enlarged device memory address region to the
user-level space. The current design of MC-DLA can add up
to 1.3 TB √ó 8 = 10.4 TB of additional physical memory
(Section V-C), well fitting under the addressing capabilities
of current GPUs (e.g., 49-bit virtual addressing (512 TB) and
47-bit physical memory addressing (128 TB)) [40]. The added
memory capacity to each device-node is informed to the device
driver at boot-time so that the driver takes it into consideration when (de)allocating memory. Allocating pages in both
devicelocal and deviceremote memories can be done using
existing device-side page-tables and the page-table walker,
but our page allocation/placement policy is designed in a
bandwidth-aware (BW_AWARE) manner in order to maximally
exploit the high-bandwidth communication channels to the left
and right memory nodes. Consider a cudaMallocRemote
call with D Bytes of memory allocation requested to the
driver. Rather than having the entire D Bytes of data be
allocated under a single memory-node (which we refer to as
LOCAL allocation policy3), our proposal splits the requested
malloc size into two equal sized chunks (aligned in page
3The LOCAL allocation policy is named after the local NUMA zone page
allocation policy of libNUMA in Linux and is not intended to imply that
allocation is done inside devicelocal memory.
&,"

+&)  "%+
%&&&#"%
+'&
	
&,"
 &,"
 +'&


 

'!+

 
 

+'&
	
'!+

 
 

'!+

 
 
 	'"'!( %"!*'!&)#%!"
 	&!!!*'
 	  "%+"'"!&,%$(&'(&!cudaMallocRemote
Fig. 10: The LOCAL and BW_AWARE page allocation policy employed in MC-DLA. BW_AWARE allows the device-node D1 to read
(write) data from (to) the left and right memory-nodes concurrently,
reducing the overall latency by half compared to LOCAL.
granularity) and maps the pages within each chunk to the
left and right memory-node‚Äôs share of the memory address
space in a round-robin fashion. This allows the device-node
to utilize all N high-bandwidth links to read/write data from
the two memory-nodes, maximally utilizing the N√óB GB/sec
of memory bandwidth for memory virtualization.
IV. METHODOLOGY
Device and Memory Nodes. We developed an in-house
system-level simulator for evaluating MC-DLA. The high-level
architecture design of our DL accelerator resembles that of
Eyeriss [41] or DaDianNao [42] in that our device architecture
also employs a spatial array of processing elements (PEs),
each of which contains (1) a multitude of MAC operators for
handling vector operations, (2) local SRAM buffers (doublebuffered to overlap computation with data fetches) to leverage
data locality, and (3) a high-bandwidth on-package memory (e.g., HBM [8]) for local devicelocal allocations. The
baseline device-node has been configured as summarized in
Table II but we also evaluate MC-DLA‚Äôs sensitivity to alternative configurations in Section V-B. Our model is designed
to optimize generic GEMM (general matrix multiplication)
operations so that it handles not only convolutional layers, but
also recurrent layers, fully-connected layers, activation layers,
and etc. Based on our analysis, an output-stationary dataflow
(i.e., output feature maps are stationed locally on-chip) as
discussed by Chen et al. [41] provides a good balance in
terms of MAC utilization and energy-efficiency across all of
the layers we evaluate, hence our device accelerator employs
the output-stationary dataflow rather than the row-stationary
dataflow. It is worth pointing out that the scope of this paper is
on studying HPC system architectures for DL training, rather
than the development of a high-efficiency accelerator device.
Therefore, our proposal is equally effective for alternative DL
accelerator designs and DNN dataflows. Note that a single
154
TABLE II: Device-/memory-node configuration parameter.
Device-node
Number of PEs 1024
MACs per PE 125
PE operating frequency 1 GHz
Local SRAM buffer size per PE 32 KB
Memory bandwidth 900 GB/sec
Memory access latency 100 cycles
Number of high-bandwidth links (N) 6
Communication bandwidth per link (B) 25 GB/sec
Memory-node
Memory bandwidth 256 GB/sec
Memory access latency 100 cycles
Number of high-bandwidth links (N) 6
Communication bandwidth per link (B) 25 GB/sec
iteration of training can take hundreds of milliseconds even on
a real high-end GPU card, so being able to perform simulation
in tractable amount of time is crucial. We therefore model
the device-node‚Äôs HBM memory and the memory DIMMs
inside the memory-nodes as having fixed memory bandwidth
and latency, rather than resorting to a cycle-level DRAM
simulator [43]‚Äì[45]. We believe our methodology provides
accurate estimations without losing fidelity due to the following two reasons: (1) as DNN computation and memory
accesses have high data locality with highly deterministic
dataflow, existing designs [42], [46], [47] primarily employ
a lightweight FSM or microcontrollers to orchestrate on and
off-chip data movements in coarse-granular data sizes and
(2) all inter-node (e.g., host‚Äìdevicelocal, devicelocal‚Äì
deviceremote) data copy operations are conducted as coarsegrained, bulk data transfers using DMAs (Section II-B) with
high data locality, rendering the system-level performance
being less sensitive to the underlying behavior of the DRAM
microarchitecture (e.g., bank conflicts).
System Architecture. We assume an 8 device-node system
configuration in all of our experiments (Figure 1). The baseline
DC-DLA system architecture is modeled after NVIDIA‚Äôs DGX
system [5] and IBM‚Äôs PowerAI DDL system [30]. Both
of these HPC systems employ the cube-mesh device-side
interconnect, which is flattened into multiple ring networks
(three rings in our evaluation, N=6 links per device node,
Section III-B) to maximally utilize inter-node link bandwidth
and minimize the latency incurred in conducting inter-device
communications [34], [35]. DC-DLA uses PCIe (gen3) to
communicate with the host memory for memory virtualization.
The HC-DLA system architecture is modeled after IBMNVIDIA‚Äôs Power9 Summit [48], which assumes the following:
(1) among the N high-bandwidth links available to each devicenode, HC-DLA allocates half of them to be connected to the
CPU memory for reads and writes, trading off fast memory
virtualization over communication in a balanced manner, (2)
four device-nodes are connected to a single CPU socket (i.e.,
8 devices sharing two sockets), and (3) the maximum CPU
socket memory bandwidth is large enough to fully service the
aggregate CPU memory bandwidth usage of the four devicenodes that are connected to that CPU socket. Consequently,
this hypothetical CPU in HC-DLA has 300 GB/sec of per
socket CPU memory bandwidth (3√ó to 4√ó overprovisioned
than real systems [38], [39]) which allows half of the N(=6)
TABLE III: Evaluated benchmarks.
Network Application # of layers
AlexNet Image recognition 8
GoogLeNet Image recognition 58
VGG-E Image recognition 19
ResNet Image recognition 34
Network Application Timesteps
RNN-GEMV Speech recognition 50
RNN-LSTM-1 Machine translation 25
RNN-LSTM-2 Language modeling 25
RNN-GRU Speech recognition 187
high-bandwidth links to be used to read/write CPU memory
(i.e., 4√óB√ó3 = 300 GB/sec). As HC-DLA can consume up to
100% of the provisioned CPU memory bandwidth (we discuss
the maximum and average CPU memory bandwidth usage of
all our system design points in Section V-A), such high CPU
memory bandwidth usage could potentially incur destructive
interference on CPU‚Äôs role [49] in the overall DL training
process (e.g., running the DL framework software, interacting
with the backing storage HDD/SSD to fetch training data, etc),
slowing down the overall training time. For a conservative
evaluation, we assume that HC-DLA‚Äôs CPU memory bandwidth usage has no effect on system performance. We omit
the results of HC-DLA designs that partitions high-bandwidth
links in an asymmetric manner as these design points were
shown to be less robust than the studied, balanced HC-DLA.
An oracular version of DC-DLA was also established by
having an infinitely sized on-package, devicelocal memory
available inside each device-node, obviating the need for CPUGPU data migration. We explore such (unbuildable) system
design point to evaluate the effectiveness of MC-DLA. The
memory-nodes in MC-DLA are configured to house ten DDR4
DIMMs providing a maximum of 256 GB/sec of memory
bandwidth to the neighboring device-nodes.
Benchmarks. We study a diverse set of eight DNN applications (Table III) that encompasses not only convolutional
neural networks (CNNs) but also recurrent neural networks
(RNNs). We choose four CNN topologies that show stateof-the-art performance in ImageNet [50], namely AlexNet,
GoogLeNet, VGG-E, and ResNet. The four RNN applications have been chosen from Baidu‚Äôs DeepBench application
suite [23], which includes one GEMV-based vanilla RNN
topology, two LSTM-based, and one GRU-based RNNs. We
use a batch size of 512 for all our evaluations and study both
data-parallel and model-parallel training (Figure 3) for partitioning the DL algorithm across the eight device-nodes. For
model-parallel training, we employ the model-parallelization
strategy as employed by Krizhevsky et al. [51].
Memory-overlaying for DNN Virtual Memory. We implemented the runtime memory management policy as described
in [9], [10], [30], [52], which leverages the network DAG
to analyze inter-layer data dependency to schedule memoryoverlaying operations for virtual memory. Under our implementation, the device memory is utilized as an applicationlevel cache with respect to the host memory. Concretely, the
runtime memory manager pushes all layer‚Äôs feature maps to
the backing store after its last reuse during forward propaga155


	




 
 
























































",) %%)  ()     




	 
%#&*))!%$ -$ '%$!.)!%$ #%'-+!')*"!.)!%$
(a)


	




 
 
























































",) %%)  ()     




	 
%#&*))!%$ -$ '%$!.)!%$ #%'-+!')*"!.)!%$
(b)
Fig. 11: Breakdown of latencies incurred during: (a) data-parallel training, (b) model-parallel training. Figures are normalized to the highest
stacked bar chart. Note that the sum of these three latency categories does not directly translate into the system-level performance because
DL frameworks try to overlap computation time with synchronization and memory virtualization.
tion and prefetches them back to the local device memory during backpropagation4. While some of these local‚Üîremote data
migration operations might not be necessary for DNNs that fit
within the memory capacity limits, following prior work [9],
[14], [52], [55], we employ such memory management policy
to maximally stress the system interconnect. In other words,
the 8 DNN applications we study are used as microbenchmarks to stress test the system interconnect and evaluate its
robustness in providing a performant virtual memory system
without compromising communication performance.
V. EVALUATION
This section evaluates six system design points, the baseline
DC-DLA, a hypothetical HC-DLA design (Section IV), one
star-topology based MC-DLA (Figure 7(b), MC-DLA(S)),
two ring-based MC-DLA design points (with LOCAL and
BW_AWARE page allocation policy, denoted as MC-DLA(L)
and MC-DLA(B), respectively), and an oracular DC-DLA with
infinite memory size (DC-DLA(O)). All average values are
based on harmonic means.
A. Identifying System Bottlenecks
Convolutional layers are generally compute-limited (e.g.,
sliding window based dataflow manifests high data locality)
and its feature maps, rather than weights, dominate memory
allocation during training. Conversely, fully-connected layers
and recurrent layers are memory bandwidth-limited where its
4We employ one exception to this rule: for layers that have short computation time (e.g., activation layers, pooling layers, ...), the memory manager
chooses to re-compute the feature maps during backpropagation rather than
migrating these data to the backing store. Such optimization minimizes
the number of memory overlaying operations and is currently employed in
MXNet [53], [54]. We adopt such optimization for a conservative evaluation
and make sure the system performance is not unnecessarily degraded.
weights take up a larger fraction of the memory allocation
than feature maps. Consequently, data-parallel training of
CNNs are generally insensitive to the underlying system‚Äôs
ability to provide fast inter-device communication because the
synchronization data size (i.e., size of the weight gradients,
dW) is relatively much smaller than its feature map size.
Memory virtualization can therefore become a performance
bottleneck for data-parallel training of CNNs. RNNs however
have a relatively larger dW size for synchronization hence both
fast communication and high-bandwidth memory virtualization is required for data-parallel RNN training. Model-parallel
training, as discussed in Section II-C, incurs much frequent
(and larger) synchronization operations than its data-parallel
counterparts, so a high-bandwidth device-side interconnect is
crucial for scalable DL training.
In this context, to clearly illustrate the system-level performance bottlenecks, we derive the latencies incurred in
performing the (a) computations required for forward and
backward propagation, (b) inter-device synchronization, and
(c) memory-overlaying for memory virtualization, the three of
which are stacked altogether in a single bar chart as shown
in Figure 11. Overall, DC-DLA spends the least amount of
time on synchronization thanks to its high-bandwidth deviceside interconnection network. Memory virtualization however
causes a significant performance bottleneck for DC-DLA on 14
out of the 16 training examples because the PCIe links only
provide a small fraction of what the high-bandwidth links can
service. Thanks to the high-bandwidth links allocated to access
CPU memory, HC-DLA can significantly reduce the latency
incurred in memory virtualization (average 88% reduction).
This however comes at a cost where: (1) the smaller interdevice communication bandwidth now leads to an increase
in synchronization time (average 90% increase), and (2) the
156



 
 




	

'%
!!%

$%

 



'%
!!%

$%

 



'%
!!%

$%

 





 

 


 	





& !"#
&%"#
 '!% !%"#
Fig. 12: CPU memory bandwidth usage under different DLA designs.
multiple devices in HC-DLA that are utilizing CPU memory
for virtual memory are now consuming a significant fraction
of CPU memory bandwidth as shown in Figure 12. For DL
training, CPUs play the role of (1) running the DL framework
software, and (2) getting the training datasets ready to be fed
into the accelerator devices (e.g., reading and batching input
datasets from the HDD/SSD storage devices, preprocessing the
input batches, and uploading the preprocessed input batches
to the CPU memory) [49]. While it is possible that such high
host-device interaction can cause destructive interference and
cause performance slowdown on HC-DLA, for a conservative
analysis, we do not take such behavior into account when
evaluating overall performance in Section V-B. The three
MC-DLA designs are able to achieve the best of both DC-DLA
and HC-DLA as the system interconnect successfully reduces
the latency incurred in virtualizing memory while not having
to incur noticeable overhead in inter-device communications.
Additionally, because memory virtualization is provided using
deviceremote memory, there are no CPU memory bandwidth
consumption whatsoever, enabling a system design that scales
independently to its ties with the host interface.
B. Performance
Figure 13 summarizes the performance of MC-DLA compared to DC-DLA, HC-DLA, and the oracular DC-DLA. The
HC-DLA design provides an average 32% and 38% speedup
over DC-DLA for data-parallel and model-parallel training,
respectively. This is due to HC-DLA‚Äôs ability to balance fast
communication and memory virtualization, which DC-DLA
fails in achieving due to its asymmetric partitioning of communication bandwidths (i.e., more than 10√ó difference in
bandwidth provisioned for inter-device communication and
memory virtualization). HC-DLA however is only able to
leverage half of its high-bandwidth links for communication
and virtual memory, failing to maximally benefit from the
device-side interconnect. Our proposed MC-DLA(B) design
fully unlocks the N high-bandwidth links for both communication and memory virtualization, leading to an average
3.5√ó and 2.1√ó speedup over DC-DLA for data-parallel and
model-parallel training, respectively (average 2.8√ó). Moreover, MC-DLA(B) reaches 84%‚Äì99% of the performance
of an unbuildable, oracular DC-DLA (average 95%). While
MC-DLA(S) does much better than DC-DLA or HC-DLA, its
suboptimal utilization of high-bandwidth links leaves significant performance left on the table (maximum 24%, average
14% performance loss than MC-DLA(B)). It is worth pointing

 

	



 

	
 


 

 

 

 

 


(a)

 

	



 

	
 


 

 

 

 

 


(b)
Fig. 13: Performance improvements offered by MC-DLA for: (a) dataparallel training, (b) model-parallel training.




	


%$
 $

#$





"

%$
 $

#$





"

%$
 $

#$





"

%$
 $

#$





"

$	 
$  
$ 
$	

$!"
 !"
Fig. 14: MC-DLA performance sensitivity to input batch size.
out that the relatively sub-optimal, but simpler MC-DLA(L)
design achieves 96% of the performance of MC-DLA(B).
Although MC-DLA(L) is only provisioned with half the
memory virtualization bandwidth of MC-DLA(B), the highbandwidth communication channels for synchronization are
equally provided for both designs thanks to its ring-based
system interconnect. While MC-DLA(L) and MC-DLA(B)
provides similar benefits over the 8 applications we study in
this paper, we believe MC-DLA(B) to be a more robust and
scalable design option as it can maximally utilize the interconnect bandwidth with reasonable design costs (Section III-B).
Sensitivity. Figure 14 shows MC-DLA(B)‚Äôs performance
sensitivity to the input batch size, demonstrating MC-DLA‚Äôs
robustness (an average 2.17√ó speedup over DC-DLA across all
batch sizes). We also studied DC-DLA with the next generation
PCIe (gen4) which doubles the PCIe link bandwidth and
improves DC-DLA‚Äôs memory virtualization performance. Such
design point improves DC-DLA‚Äôs performance by 38%, narrowing the performance gap between DC-DLA and MC-DLA to
2.1√ó (as opposed to 2.8√ó), but comes at a cost of significant
CPU memory bandwidth consumption (proportional to the
increase in PCIe link bandwidth). System designs with (1)
157
TABLE IV: Memory-node power consumption (DDR4-2400).
Single DIMM Memory-node
DDR4 modules TDP (W) TDP (W) GB/W
8 GB RDIMM [59] 2.9 29 2.8
16 GB RDIMM [60] 6.6 66 2.4
32 GB LRDIMM [61] 8.7 87 3.7
64 GB LRDIMM [62] 10.2 102 6.3
128 GB LRDIMM [63] 12.7 127 10.1
a faster device-node configuration such as TPUv2 and (2)
scaled-up node configuration such as DGX-2 (2 PFLOPS and
2.4 TB/s of device-side interconnect bandwidth5) have also
been explored which leads to MC-DLA with an average 3.2√ó
and 2.9√ó speedup over DC-DLA, respectively. Rhu et al. [56]
proposed to leverage CNN activation sparsity to compress
and reduce local-device communication traffic to alleviate the
PCIe bottleneck. This technique provides an average 2.6√ó
reduction in PCIe traffic, narrowing the performance gap
between DC-DLA and MC-DLA to 2.3√ó for the 4 CNN
applications we study. Overall, MC-DLA exhibited robustness
across various sensitivity studies we conducted (e.g., different
chip configurations, input batch sizes, and etc.) as it guarantees high-performance memory virtualization and inter-device
communication by design.
C. Power Efficiency
MC-DLA utilizes existing accelerators as-is, so the major
power overhead comes from the memory-nodes added to the
ring network. NVIDIA‚Äôs DGX system (i.e., DC-DLA) has a
TDP of 3, 200 W, where the eight V100 GPUs consume
75% of the system power (i.e., 300 W √ó 8). Table IV
summarizes our estimation of a single memory-node‚Äôs power
consumption using publicly available power measurements
of DDR4 DIMMs [57] and Micron‚Äôs DDR4 system power
calculator [58]. For power-limited environments, memorynodes with 8 GB RDIMM would be most appropriate which
incurs an additional (29 √ó 8) = 232 W power consumption
(7% increase over DGX-1V). For consumers more so focused
on capacity expansion, the 128 GB LRDIMM based memorynode would provide high value (1.3 TB of memory under
127 W, highest GB/W). System-wide power consumption will
increase by 31% (i.e., 127 √ó 8 = 1, 016 W), but such option
would drastically increase the pool of memory by 10.4 TBs.
Microsoft‚Äôs custom-built HGX-1 [6], a 4U server chassis
featuring 8 Pascal GPUs, can have a TDP up to 9, 600 W,
so we believe the design overheads of MC-DLA is reasonable given its unique value proposition. Overall, MC-DLA
achieves (2.8√ó/1.31)=2.1√ó to (2.8√ó/1.07)=2.6√ó increase in
performance per watt while substantially enhancing the pool
of memory exposed to the device-nodes.
D. Scalability
Although the image classification problem [11] is gradually
gaining less traction from the DL algorithm community, there
is still on-going research in parallelizing and distributing CNN
5By provisioning even higher compute and communication bandwidth than
the baseline system, the benefits of MC-DLA is even more pronounced as
DC-DLA becomes completely bottlenecked by memory virtualization.
training to 1000s of GPUs/TPUs to reduce training time
and achieve performance scalability. Recent advances in this
domain of research [64]‚Äì[67] employ data-parallel training
with extremely large batch sizes (e.g., 32K in [65]) to reduce
the intra-/inter-node communication overheads and achieve
near perfect performance scaling. As the memory usage of
these existing CNN algorithms are optimized to fit within the
physical GPU memory constraints, training is done without
any CPU-GPU data migration involved. Using our simulation
infrastructure, we observe similar (perfect) performance scalability with DC-DLA when memory virtualization is disabled
(i.e., close to 4√ó and 8√ó reduction in training time when the
4 CNN applications in Table III are data-parallelized across
4/8 GPUs). However, when memory virtualization is enabled
and the feature maps are migrated in/out of local-remote
memory, the performance improvements achieved with 4/8
GPUs under DC-DLA is only 1.3√ó/2.7√ó because of the hostdevice communication bottleneck6. Performance scalability is
regained using MC-DLA thanks to its ability to perfectly hide
data migration overhead (Figure 11).
E. User Productivity
As state-of-the-art CNN algorithms for image classification [20], [22] reach super-human performance, the DL research community has shifted towards more challenging tasks
such as video understanding (e.g., video classification and
captioning [25], [68], [69], video question and answering [26],
[70], [71]). Given an input video stream, the goal is to capture
the context of the scenes, objects, and activities and be able
to express how these relate to each other in a complete
sentence. State-of-the-art video understanding algorithms are
commonly implemented as a mixture of CNNs, LSTMs, and
memory networks [72], [73], but training these algorithms
end-to-end under current HPC systems becomes practically
impossible because of the memory capacity bottleneck. DL
practitioners are therefore forced to compromise their learning
algorithm (e.g., freezing subset of the algorithm without endto-end training, reducing the number of input video frames and
recurrent timesteps per training iteration, cropping video frame
sizes, . . . ) so that the overall memory footprint fits within
the physical GPU memory. With the advent of large-scale
video training datasets such as YouTube-8M [74], providing
sufficient amount of memory that enhances user productivity
will become vital. Aside from being able to train DNNs
that are deeper and larger, MC-DLA can open up a wider
range of complex learning algorithms (e.g., end-to-end training
of aforementioned video-to-text algorithms employing larger
CNNs/LSTMs) that are currently impossible to train due to
memory capacity limits, propelling continued innovation in
this active research space.
6Although host-device data migration for these CNN workloads is arguably
unnecessary, following prior work [9], [14], [52], [55], we use existing
workloads to study performance scalability as there are no publicly available
DNN algorithms that exceed the memory capacity limits of current systems
(i.e., you cannot train a DNN algorithm unless its memory requirement fits
within the physical memory size limits, the chicken-and-egg problem).
158
 

!"

"
 " 
	!
Fig. 15: Scale-out, datacenter-level device-side interconnect plane.
Figure assumes that a given system node houses 8 nodes and each
device-/memory-node is provided with N=3 high-bandwidth links.
The device-side switch allows each of the 8 nodes to communicate
with any of the other nodes inside the system node, enabling it
to be casted into any interconnection topology (e.g., the ring-based
MC-DLA interconnect, Figure 7(c)).
VI. FUTURE RESEARCH DIRECTION
To the best of our knowledge, our work is the first in the
literature that highlights the growing significance of deviceside interconnects in training DL algorithms across multiple
devices. Due to space limitations and the wide design space,
this paper focused on intra-node system architectural issues,
assuming inter-node communication is handled using MPI
over Ethernet/InfiniBand. NVIDIA recently announced the
NVSwitch [75] technology which is an NVLINK-compatible
switch, enabling system vendors opportunities to scale-up/out
device-side interconnection networks, for instance (1) incorporating a larger number of GPUs within a system node [7] or (2)
tightly integrating thousands of GPUs across hundreds of system nodes (Figure 15), similar to Microsoft‚Äôs BrainWave [76].
The introduction of these device-side switching technologies
for accelerators that enable scale-out device-side interconnects
emphasizes the importance of device-side interconnection networks moving forward and opens up interesting research opportunities. Exploring our memory-node architecture as part of
these scale-out device-side interconnects with 100s of devicenodes and memory-nodes across a distributed network is part
of our next future work.
VII. RELATED WORK
Memory disaggregation [17], [18] expands the CPU memory hierarchy to include a remote level provided by a separate
memory blade connected over PCIe, which helps increase the
pool of CPU accessible memory. Kim et al. [77], [78] proposed
to interconnect multiple CPUs/GPUs by leveraging the packet
routing capabilities of HMCs [79], effectively composing a
memory network that provides flexible processor bandwidth
utilization. The scope of [77], [78] is significantly different
than what our work focuses on, but more importantly, our
proposal is not tied with a particular memory technology
whereas [77], [78] assumes a 3D stacked memory with routing
capabilities embedded inside the logic layers. Slowdown on
Moore‚Äôs law and Denard scaling have driven researchers to
pursue ‚Äúchiplet‚Äù based processor designs [80]‚Äì[82], where a
large SoC is decomposed into multiple smaller (but higher
yield) chiplets and are re-assembled as a single package.
One can envision combining the concept of chiplet-based
GPUs with the notion of memory networks [77] as means
to tightly integrate GPUs and HMCs within a package for
memory capacity expansion. However, the maximum number
of GPUs as well as HMCs that can be integrated inside a
single package is bounded by various technology constraints,
load distribution, and ease of programmability (e.g., recent
MCM-GPU assumes only up-to 4 GPUs integrated within a
single package). The focus of MC-DLA is on efficient parallelization and workload partitioning in a system-level context
as opposed to these prior chiplet-context studies focusing on
package-level or board-level integrations. A large body of prior
work has explored the design of a single accelerator device
architecture for deep learning inference [42], [46], [83]‚Äì[95]
with an increased interest on leveraging DNN sparsity for
further energy-efficiency improvements [47], [96]‚Äì[105]. Park
et al. [106] proposed a scale-out acceleration platform for
training machine learning algorithms using an FPGA-based
16-node distributed system. These prior studies are orthogonal
to our MC-DLA proposal and can be adopted further for
additional enhancements.
VIII. CONCLUSION
As the models and datasets to train DL models scale, system
vendors are employing a custom device-side interconnection
network for fast communication and synchronization across
accelerator devices. This paper is the first to describe the
growing significance of device-side interconnects for training
scaled up DL algorithms and highlights the importance of
balancing inter-device communication and fast memory virtualization. We make a case for a memory-centric DL system
and presented a scalable, programmable, and energy-efficient
HPC platform for DL training, which provides an average
2.8√ó speedup over DC-DLA while drastically expanding the
pool of memory accessible to accelerators to 10s of TBs.