Abstract—Intelligent Personal Assistants (IPAs) with the capability of natural language processing (NLP) are increasingly
popular in today’s mobile devices. Recurrent neural networks
(RNNs), especially one of their forms – Long-Short Term Memory
networks (LSTMs), are becoming the core machine learning
technique applied in the NLP-based IPAs. With the continuously
improved performance of mobile GPUs, local processing has
become a promising solution to the large data transmission and
privacy issues induced by the cloud-centric computations of IPAs.
However, LSTMs exhibit quite inefficient memory access pattern
when executed on mobile GPUs due to the redundant data
movements and limited off-chip bandwidth. In this study, we
aim to explore the memory friendly LSTM on mobile GPUs by
hierarchically reducing the off-chip memory accesses. To address
the redundant data movements, we propose inter-cell level optimizations that intelligently parallelize the originally sequentially
executed LSTM cells (basic units in RNNs, corresponding to
neurons in CNNs) to improve the data locality across cells with
negligible accuracy loss. To relax the pressure on limited offchip memory bandwidth, we propose intra-cell level optimizations
that dynamically skip the loads and computations of rows in
the weight matrices with trivial contribution to the outputs. We
also introduce a light-weighted module to the GPUs architecture
for the runtime row skipping in weight matrices. Moreover,
our techniques are equipped with thresholds which provide
a unique tunning space for performance-accuracy trade-offs
directly guided by the user preferences. The experimental results
show our optimizations achieves substantial improvements on
both performance and power with user-imperceptible accuracy
loss. And our optimizations exhibit the strong scalability with
the increasing input data set. Our user study also shows that our
designed system delivers the excellent user experience.
Index Terms—Approximate Computing, GPGPU/GPU, Mobile
and Embedded Architectures
This research is supported by NSF grants CCF-1619243, CCF1537085(CAREER), CCF-1537062. This research is partially supported by
National Natural Science Foundation of China under grants No.61772350,
Beijing Noval Z181100006218093.
I. INTRODUCTION
Intelligent Personal Assistants (IPAs) with the capability of
natural language processing (NLP), such as Apple’s Siri [1],
Google Assistant [2], and Amazon Alexa [3], have already
been heavily used by mobile users, and are expected to
become the major workloads in future mobile devices [4].
The core technique to support NLP in IPA applications is
artificial neural networks, such as convolution neural networks
(CNNs) and recurrent neural networks (RNNs). CNNs have
shown the strong ability in achieving high accuracy for image
and speech recognition. With the increasing requirements on
personalizations, the NLP-based IPAs are expected to understand more complicated requests from humans and perform
like a person [5], [6]. These features require capturing the
relationship among input samples, which is largely ignored
in CNNs. RNNs, especially one of their forms – Long-Short
Term Memory networks (LSTMs) [7], show great promise in
exploring affiliations among serial samples (details in Section
II). Thus, LSTMs are becoming the major technique for NLP
in IPAs. Many companies, e.g. Google, Apple, Microsoft,
Amazon and Baidu, leverage this advantage of LSTMs in
processing the context information for their NLP-based IPA
applications, e.g. semantics classification [8], automatic speech
recognition [9], [10] and question answering [11].
Traditionally, the computations of NN-based IPA applications are performed in the cloud, which faces large data
transmission and privacy issues. With the continuously improved performance of modern mobile devices, localizing
these applications becomes promising and has attracted major
attentions [12]–[15]. Especially, mobile GPUs have been improved significantly to support parallel computing like neural
networks [16]–[21]. They become one of the best candidates
to host the computations required by NN-based IPAs such as
LSTMs.
162
2018 51st Annual IEEE/ACM International Symposium on Microarchitecture
978-1-5386-6240-3/18/$31.00 ©2018 IEEE
DOI 10.1109/MICRO.2018.00022
However, we observe that LSTMs exhibit quite inefficient
memory access patterns and face two serious memory bottlenecks when executed on mobile GPUs. (1) Redundant data
movements: as a natural feature of LSTMs, some weight
matrices are shared by all the cells (basic units in RNNs,
corresponding to neurons in CNNs) in one LSTM layer. And
all cells have to be processed in-order in each layer due to
the context link (i.e., the data dependence) between every
two adjacent cells. Given the limited mobile GPUs on-chip
storage, this sequential execution causes redundant loads from
the off-chip memory for the shared weight matrices across
cells. (2) Limited off-chip bandwidth: the relatively large
working set per LSTM cell also causes severe pressure to
the off-chip memory bandwidth. Both these two bottlenecks
significantly extend LSTM execution time and cause high
power consumption on mobile GPUs. Unfortunately, previous
proposed technologies on CNNs cannot effectively address
these challenges as LSTMs have completely different computation patterns from CNNs (See detailed comparison between
LSTMs and CNNs in Section II-A).
In this study, we aim to explore the memory friendly LSTMs
on mobile GPUs, thus, achieving the substantial improvements
on both performance and power. We propose the optimizations
at both inter-cell and intra-cell levels that address the above
two challenges, respectively, to hierarchically reduce the offchip memory accesses.
At the inter-cell level, we observe an interesting feature of
the LSTM layer that the context links between some adjacent
cells are weak. We propose to divide the LSTM layer into
multiple independent sub-layers at these weak links. Since the
weak links are lost between sub-layers, a predicted link is
further applied to each sub-layer to recover the accuracy loss.
We then parallelize the sub-layers, which enhances the data
locality across cells from different sub-layers and substantially
reduces the redundant data movements.
At the intra-cell level, unlike CNNs, we observe that some
rows in weight matrices have trivial contributions to the cell
output. We propose dynamic row skip (DRS) technique to
skip the loads and computations of those trivial rows in each
LSTM cell with negligible impact on the output accuracy.
We introduce a light-weight module to the GPUs architecture. It reorganizes the cooperative thread arrays (CTAs) to
dynamically disable the threads which are assigned to process
the trivial rows. DRS reduces the number of rows and the
size of the weight matrices required by a LSTM cell, hence,
effectively relaxing the pressure on the off-chip memory
bandwidth.
To our best knowledge, this is the first work to address the
memory bottlenecks for LSTMs executing on mobile GPUs.
Our proposed techniques gain the substantial performance and
power benefits without sacrificing the user-perceptible output
accuracy. Moreover, our techniques are equipped with thresholds which provide a unique tunning space for performanceaccuracy trade-offs directly guided by the user preferences,
leading to the best user experience. The contributions of this
paper are as follows:
• We observe that memory is the bottleneck for LSTMs on
Cell = Cell 0 Cell 1 Cell 2 Cell t
...
...
Context 
Link
xt
ht h0 h1 h2 ht
x0 x1 x2 xt
Fig. 1: The schematic of one RNN layer (left) and its unrolled
model (right). The cell represents the operations of mapping
the inputs to the outputs. In the unrolled model (right), the
cell 0 represents cell at timestamp 0 and so on.
Conv op Conv op
x0 x1 x2 x3
h0 h1
FC op FC op FC op
x0 x1 x2
h0 h1 h2
(a) Conv layer (b) FC layer
Fig. 2: The schematics of (a) the CNN convolution (Conv)
layer, and (b) the CNN fully connected (FC) layer.
mobile GPUs. It is mainly caused by the frequent data
re-loads across sequentially processed LSTM cells, and
the large size of the weight matrices per cell.
• We observe the weak context links between some adjacent cells, and leverage this feature to explore the intercell level optimizations that intelligently parallelize the
processing of the LSTM cells, hence, reducing the data
re-loads with user-imperceptible accuracy loss.
• We propose intra-cell level optimizations that dynamically skip the loads and computations of the trivial
weight matrix rows with negligible contribution to the
outputs. We introduce a light-weighted module to the
GPUs architecture for the runtime row skipping in weight
matrices.
• The experimental results show that our proposed techniques achieve on average 2.54X (upto 3.24X) performance improvement and 47.23% energy saving on the
entire system with only 2% accuracy loss that is generally
user imperceptible, comparing with the state-of-the-art
LSTM execution on mobile GPUs. And our optimizations
exhibit the strong scalability with the increasing input
data set. Our user study also shows that our designed
system delivers excellent user experiences.
II. BACKGROUND
A. Recurrent Neural Networks
One RNN layer usually contains one cell which integrates
the operations of mapping the inputs to the outputs, as shown
in Fig.1 (left). The cell produces the outputs periodically
using not only the activations from the last layer, but also the
historic self-output, also known as context link (highlighted
by the red line in Fig.1). This feature helps model the context
dependency within the input activations in the sequence modeling tasks (e.g. language modeling tasks). In order to simplify
the analysis, the RNN layer can be unrolled into a sequence
163
ܹ௙ ܹ௜ ܹ௖ ܹ௢
ܷ௙
ܷ௜
ܷ௖
ܷ௢
+
+
+
+
݅௧
௧ܥ
݂௧ X
X
ܱ௧
X
݄௧
݄௧ିଵ
Output Gate
Input Gate
௧ݔ
Forget Gate
Cell State Value
ܿ௧ିଵ
2
1
3
ܾ௙
ܾ௜
ܾ௖
ܾ௢
Fig. 3: The LSTM cell schematic.
of cells to represent the cell states at different timestamps,
as shown in Fig.1 (right). To clarify, we focus on the RNN
unrolled layer in this study, and a cell in the layer means the
unrolled cell at certain timestamp. Correspondingly, we refer
the previous/next cell in the layer as the unrolled cell at the
previous/next timestamp.
Even the RNN unrolled layer looks similar to CNN neural
network layers, e.g. the convolution (Conv) layer and the fully
connected (FC) layer, it has a completely different computation
pattern. Fig.2 also demonstrates the schematics of the Conv
layer and the FC layer from CNNs for comparison. Firstly,
the input formats of these three layers are totally different:
the Conv layer processes multiple matrix sets, and the FC
layer takes a bunch of single activations as the input while the
RNN unrolled layer processes the activation matrix with each
cell processing an activation vector. Furthermore, the Conv
layer and the FC layer produce the output activations by only
using the layer inputs, which are all ready before the layer
begins. Thus, the Conv/FC operations of the same layer can
be parallelized. However, the operations of each cell in the
RNN unrolled layer involve one more dimension besides the
layer input and output, which is the context link between the
adjacent cells (red line in Fig.1). Therefore, instead of concurrently processing all layer inputs, the RNN layer can only
iteratively process partial input activations at each timestamp.
In other words, only one vector in an input activation matrix
can be processed at a time, and the processing of the following
vector should wait until the processing of the previous vector
finishes.
B. Long-Short Term Memory Networks (LSTMs)
There are mainly three types of RNNs: Simple RNNs (or
vanilla RNNs), Long-Short Term Memory networks (LSTMs)
[22] and Gated Recurrent Unit networks (GRUs) [23]. Simple
RNNs can hardly connect the useful information between
two inputs with the large time interval [24]. LSTMs and
GRUs were introduced to address such problem by setting
gates inside the RNN cell to filter the information from both
the input and the historical self-output, thus only the useful
information is well kept through the unrolled cells to enable
the long-term “memory”. The LSTM cell has more gates than
the GRU cell, which increases the computation complexity
but ensures a better accuracy. In this paper, we focus on the
analysis and optimizations of LSTMs execution on mobile
GPUs, the proposed methods can also be applied to GRUs
with simple adjustment.
Algorithm 1 The LSTM Execution on Mobile GPUs
1: for each layer in LSTM do
2: Kernel Sgemm(Wf,i,c,o, x);  2
3: for each cell in layer do
4: Kernel Sgemv(Uf,i,c,o, ht−1);  1
5: Kernel lstm ew(ft, it, ct−1, ct, ot, ht);  3
6: end for
7: end for
Fig.3 shows the zoom-in view of one LSTM cell located at
the tth timestamp. There are three gates in one LSTM cell:
Input Gate it, Forget Gate ft and Output Gate ot. They help
modify the cell state, which “stores” context information over
the arbitrary time interval. The LSTM cell takes three inputs:
the layer input xt, the historic self-output ht−1, and the cell
state value of the previous cell ct−1; they are all in the form
of vector. The cell also has two outputs: the cell state value
of the current cell ct and the output activations ht; both them
are in the form of vector. The following equations represent
all the computations within one cell in LSTMs:
ft = σ(Wfxt + Ufht−1 + bf ) (1)
it = σ(Wixt + Uiht−1 + bi) (2)
ct = ft · ct−1 + it · tanh(Wcxt + Ucht−1 + bc) (3)
ot = σ(Woxt + Uoht−1 + bo) (4)
ht = ot · tanh(ct) (5)
Eq.1 generates the forget gate ft which will be applied to
the cell state value of the previous cell ct−1. Eq.2 produces
the input gate it which will be merged into the cell state value
of the current cell ct. Eq.3 updates the old cell state ct−1 to
a new cell state ct. Eq.4 and Eq.5 output ht based on the cell
state ct with the output gate ot filtering the output information.
C. LSTM Execution on Mobile GPUs
In modern GPUs with strong backend libraries, e.g. cuDNN
[25], the above computations within one LSTM cell are
divided into three parts, as shown in Fig.3 1 2 3 .
In 1 , since all the (U × ht−1) functions from Eq.1,2,3,
and 4 share the same input ht−1, they are integrated into one
matrix-vector multiplication kernel Sgemv(Uf,i,c,o, ht−1)
with weight matrices (Uf , Ui, Uc, Uo) concatenated into an
united weight matrix Uf,i,c,o.
Similarly, in 2 , all the (W × xt) computations are
combined into an united matrix-vector multiplication kernel
Sgemv(Wf,i,c,o, xt). In large-scale GPUs (e.g. Tesla M40
[26]), cells from different layers can be executed in parallel
as long as they have no data dependence, e.g. the cell at the
jth layer and the t + 1th timestamp can be parallelized with
the cell at (j + 1)th layer at the tth timestamp. However,
such layer level parallelism requires a large-size memory
to hold the weight matrices of multiple layers, and can be
hardly implemented on mobile GPUs (e.g. Tegra X1 [27])
with limited on-chip storage. As a result, the LSTM layers are
processed sequentially on mobile GPUs and the whole layer’s
164
0%
30%
60%
IMDB MR BABI SNLI PTB MT Average
Contribution to 
Pipeline Stalls Memory Access Lack of Resource Inst_fetch Synchronization Other
Fig. 4: The contribution of each major factor to the pipeline
stall cycles when executing Sgemv().
Sgemm
Sgemv
Lstm_ew
Cell 0 Cell 1 Cell 2 Cell 3
...
Layer Time
Begin
Off-chip 
Memory
Weight Matrix
Wf,i,c,o
Weight Matrix
Uf,i,c,o
Fig. 5: The sketch map of the kernel execution for the LSTM
layer
inputs x1 − xn are ready at the beginning of each LSTM
layer execution. To gain the matrix multiplication efficiency,
the originally independent matrix-vector multiplications per
cell shown in 2 are then transformed to one matrix-matrix
multiplication kernel Sgemm(Wf,i,c,o, x) per layer.
Finally, the remaining operations of the cell in 3 are
included into one kernel lstm element wise(f, i, c, o)
which consists of adding and activation functions for each
individual elements. Algorithm.1 summarizes the state-of-theart LSTM execution on the mobile GPUs with the strong
backend library (e.g. cuDNN).
III. THE MEMORY BOTTLENECK
Although several optimizations have been made by GPU
backend libraries, the LSTM execution on mobile GPUs is
still inefficient. In this study, we implement the state-of-theart LSTM execution on a typical mobile GPU, the Jetson-TX1
board, and observe that kernel Sgemv dominates the overall
LSTM execution time (over 90%). We further investigate the
GPU pipeline stalls during the Sgemv execution. Several factors can cause the pipeline stall, such as the off-chip memory
access, the barrier synchronization and so on. Fig.4 plots the
contribution of each major factor to the overall pipeline stall
cycles when executing Sgemv kernels (benchmark details are
presented in Section.VI-A). As it shows, the off-chip memory
access is the major contributor. Besides, previous works [28],
[29] find that the off-chip memory accesses are also very
expensive for mobile GPUs from the power perspective. In
this section, we describe two major memory challenges at
both inter-LSTM-cell and intra-LSTM-cell levels that lead to
the performance and power bottleneck for the efficient LSTM
execution on the mobile GPUs.
A. The Inter-Cell Level Memory Bottleneck: Redundant Data
Movements
As illustrated in Algorithm.1 1 , the Sgemv kernel is
launched per cell when executing one LSTM layer. The united
weight matrix Uf,i,c,o is then repeatedly requested by the
Sgemv kernels across cells at different timestamps in the
layer. Unfortunately, the matrix Uf,i,c,o exhibits quite poor
data locality in the GPUs on-chip storage, leading to the
redundant data movements and intensive off-chip memory
0%
50%
100%
IMDB MR BABI SNLI PTB MT
Utilizations
Off-chip Bandwitdh On-chip Bandwidth
Fig. 6: Utilization of on-chip and off-chip memory when
executing Sgemv().
accesses, as described in Fig.5. This is mainly caused by the
unique LSTM execution pattern: as shown in Fig.3 1 , the
Sgemv kernel at current cell takes ht−1 as the input which
is data dependent on the previous cell in the same layer. This
prevents the Sgemv kernels across cells from being integrated
into one matrix-matrix multiplication kernel, which only needs
one-time load for the weight matrix and being processed once
per layer. As a result, each Sgemv kernel accesses the weight
matrix separately. Even worse, the limited on-chip storage fails
to hold such large-size weight matrix, causing the frequent
loads and evictions for the useful data. We also observe that
the size of the actually loaded data is upto 100X larger than the
original data size, which indicates the quite in-efficient data
re-loads. Moreover, the redundant data movements become
severer as the number of cells increases in the layer since
adding one cell requires additional loads for the united weight
matrix.
To efficiently minimize the redundant data loads and improve the data locality across cells, we propose the inter-cell
level optimization scheme called LSTM layer reorganization.
It divides one LSTM layer into multiple parallel sub-layers,
cells from different sub-layers become independent and are
further combined to enable the reuse on the weight matrix
Uf,i,c,o. More details are described in Section.IV.
B. The Intra-Cell Level Memory Bottleneck: Limited Off-Chip
Bandwidth
The Sgemv kernel inside the cell requires to load the united
weight matrix (Uf,i,c,o) with numerous elements. However, the
limited off-chip memory bandwidth of mobile GPUs fails to
fulfill such high demands. Fig.6 plots both off-chip and onchip bandwidth utilization during the Sgemv kernel execution.
As it shows, the off-chip bandwidth is almost fully utilized,
while the on-chip bandwidth is lightly consumed.
To release the off-chip bandwidth limitation, we propose to
effectively shrink the input data size for the Sgemv kernel. We
explore the dynamic row skip scheme by leveraging the unique
computation features of the LSTM cell to dynamically skip the
data loads for rows in the united weight matrix with trivial
contribution to the final outputs. More details are presented in
Section.V.
IV. INTER-CELL LEVEL OPTIMIZATIONS
In this section, we focus on the inter-cell level optimizations
to enhance the data locality across cells in one LSTM layer.
A. The Irrelevance Between Two LSTM Cells
The sigmoid function (σ) and hyperbolic tangent function
(tanh) are used as the activation functions for the LSTM cell
computations [7]. The sigmoid function takes the input within
165
0
0.25
0.5
0.75
1
-5 -4 -3 -2 -1 0 1 2 3 4 5
-1
-0.5
0
0.5
1
-5 -4 -3 -2 -1 0 1 2 3 4 5
hard
sigmoid
hard
moid
Sensitive Area Sensitive Area
Sigmoid Function tanh Function Insensitive Area
Insensitive
Area
Insensitive
Area
Insensitive
Area
Fig. 7: Sigmoid activation function and tanh activation function
the range of [−∞, +∞] and its output is within the range of
[0, 1], as shown in Fig.7(a). Interestingly, when the input is in
the range of [−2, 2], its output is nearly linear to the input, we
refer it as sensitive area, as shown in Fig.7(a); on the other
hand, its output is insensitive to the input within the range of
[−∞, −2] and [2, +∞], we refer it as insensitive area. This
is also the case for the tanh function as shown in Fig.7(b).
In some neural network frameworks, the sigmoid function is
modeled by the hard sigmoid function (shown in Fig.7(a)) to
accelerate the computations [30]. The boundaries to partition
the sensitive and insensitive areas fit both sigmoid and fast
sigmoid functions.
According to Eq.5, the range of the previous cell’s output
ht−1 is [−1, 1] because ot−1 is the output of sigmoid function
(shown in Eq.4) with the range of [0, 1] and the output of
tanh(ct−1) is within the range of [−1, 1]. As shown in Eq.1,
ht−1 is also the input data for the current cell which will be
multiplied to the matrix Uf , and the range of the multiplication
outputs can be derived once Uf is known. Moreover, the range
of the sigmoid function’s input in Eq.1 can further be derived
once (Wf,i,c,o × xt) is finished at the beginning of the layer
processing. And when the range of the input is [2, +∞], one
would easily tell that the output (i.e. ft in Eq.1) is always close
to 1 based on the feature of sigmoid function discussed above.
In other words, the output ft is irrelevant to the ht−1 values in
this case. The similar derivation can be applied to Eq.2,3,4, and
their outputs it, ct, and ot are irrelevant to the ht−1 values as
well. To summarize, when Uf,i,c,o, (Wf,i,c,o ×xt), and bf,i,c,o
are known, given that ht−1 is within the range of [−1, 1], the
range of (Wf,i,c,oxt + Uf,i,c,oht−1 + bf,i,c,o) can be derived,
and if it falls in insensitive area, the previous cell’s output
ht−1 can be considered as irrelevant to ft, it, ct, and ot, thus,
having no impact to the current cell’s computation. In other
words, there is no context link between these two cells.
B. LSTM Layer Division
Based on the above observation that the context links
between every two cells are not uniform throughout the LSTM
layer, we propose the LSTM layer division scheme which
breaks the context link between cells with no or quite weak
link so that a LSTM layer is divided into multiple independent
sub-layers, as shown in Fig.8(a1). This opens the door for sublayer parallelization and reducing the data reloads which will
be explored in Section IV-C. Though it is weak, the context
link is lost between sub-layers which may affect the final
output accuracy, a predicted context link (Fig.8(a2)) is further
applied to the first cell of each sub-layer (except the first sublayer) to recover the accuracy.
Algorithm 2 Relevance Value Acquisition
Input: Hidden Layer Size Dim; Weight Matrices
Uf , Ui, Uc and Uo; Output Vectors X
f , X
i, X
c
and X
o from the matrix multiplications (i.e., Wfxt,
Wixt, Wcxt, Woxt); Offset Vectors bf , bi, bc and bo
Output: Relevant Value S
1: S ← 0;  initial the relevance value
2: Df,i,c,o ← sum(abs(Uf,i,c,o));
3: for j ∈ [0, Dim − 1] do
4: Sj
f ← min(4, max(Xj
f + b
j
f + Dj
f + 2, 0));
5: Sj
i,c,o ← min{

2 + min(2, abs(Xj
i,c,o + b
j
i,c,o))
, 
min(2, 2 + Dj
i,c,o − max(2, abs(Xj
i,c,o + b
j
i,c,o)))
};
6: Sj ← Sj
o × (Sj
f + Sj
i × Sj
c );
7: S ← S + Sj ;
8: end for
9: return S;
Breakpoints Search: Theoretically, breaking the context link
between two irrelevant cells has no impact on the output
accuracy. However, in most cases, it is hard to find two
consecutive cells in the layer that are completely irrelevant.
Breaking the weak links becomes the main target for the
LSTM layer division scheme, and quantitatively justifying the
relevance between two cells is the first step towards finding
the weak context links for the breakpoints. In this study, we
introduce the relevance value S to describes the impact of
precedent cell’s output on current cell. A smaller S implies a
weaker link between the cells and ”0” means totally irrelevant.
Algorithm.2 calculates S for the link between the precedent
and current cells. In line 2, the range [−D, D] is computed
for each element in the output vector of matrix multiplication
(Uf,i,c,o × ht−1) in current cell. In line 4-5, the computed
ranges, values from the offset vector bf,i,c,o, and the values
from the output vector of matrix multiplications (Wf,i,c,o×xt)
in current cell are used to calculate the range of input values
for the activation function. The range is then compared with
the sensitive area to measure the overlapping value between
them. Since there are multiple activation functions in each
LSTM cell, in line 6, the range overlapping values for all
activation functions are combined to calculate S for one input
element. Finally in line 7, Ss for all elements in the input
vector are summed up to derive the overall S for current cell.
At the beginning of each LSTM layer, the relevance value
Ss for each two cells are computed since our algorithm does
not take any timestamp-based value. Then each S value will be
compared with a relevance threshold αinter to determine the
weak context links for current LSTM layer: if S is lower than
the threshold, the two cells are considered as weakly linked
which will be selected as the breakpoint.
Accuracy Recovery: We use a pre-determined vector to predict
all the context links lost at the breakpoints (one context link
is one vector). Although the predicted vector is not quite
accurate when applying to all breakpoints, it can well recover
the application output accuracy since the weak context links
166
Cell 0 Cell 1 Cell 2 Cell 3 Cell 4
Weight Matrix
Uf,i,c,o
Off-chip
Memory
Original LSTM Layer
Cell 0 Cell 1 Cell 2
Cell 3
Cell 8
Independent Sub-layers
Cell 5
Cell 7 Weak
Context 
Link
Weak
Context 
Link
Strong
Context 
Link
Cell 6 Cell 7 Cell 8
Weak
Context 
Link
Cell 4 Cell 5 Cell 6
(a2)
Accuracy
Recovery
(a2)
Accuracy
Recovery
Predicted 
Context Link
Cell 0 Cell 1 Cell 2 Cell 3
Cell 7 Cell 8 Cell 4 Cell 5 Cell 6
Tissue 0 Tissue 1 Tissue 2
Exceed 
Maximum 
Tissue Size
(b1)
Tissue
Formation
(b1)
Tissue
Formation
(b2)
Tissue
Alignment
(b2)
Tissue
Alignment
Cell 0 Cell 1 Cell 2 Cell 3 Cell 7 Cell 8 Cell 4 Cell 5 Cell 6
Tissue 0 Tissue 1 Tissue 2
Weight Matrix
Uf,i,c,o
Off-chip
Memory
Reorganized LSTM Layer
(a1) Breakpoints
Search
Fig. 8: The overview of the inter-cell level optimization.
have relatively small impact on the application output and are
insensitive to a small prediction error.
We predict the weak context links by analyzing the distribution of a large set of context links which are collected
through executing LSTMs offline with large training datasets.
Note that we study the distribution of all context links since
the weak context links share quite similar distribution pattern
with strong context links. It is unnecessary to particularly
focus on the weak context links which vary with the relevance
threshold. Since the context link is in the form of vector, the
value distribution for each element will be collected, and the
expectation of each element in the weak context link can be
achieved by the following equation:
hj = n
i=0
hj (i) × ρij (6)
where hj is the expectation for jth element in the context
link, and ρij represents the possibility for the distribution of
the jth element in the context link. The expectations of all the
elements compose a vector which is the predicted context link
at the breakpoints.
C. LSTM Layer Reorganization
Tissue Formation: Given the independent LSTM sub-layers,
we parallelize them via fusing cells from the sub-layers into
tissues. One cell will be selected per sub-layer, and the selected
cells together form a tissue. For example, in Fig.8, the LSTM
layer is divided into four sub-layers, cells 0, 3, 4, and 7
from them, respectively, are combined into one tissue; and
the next cells from these sub-layers which are only cells
1, 5, and 8 from the first three sub-layers as the second
sub-layer only contains cell 3, are combined into another
tissue. As Fig.8 shows, the LSTM layer is transformed into
a sequence of tissues. The cells inside each tissue will be
executed concurrently. Note that the data dependency across
cells in each sub-layer still maintains which is treated as the
data dependency across tissues.
In this study, we define the number of the cells per tissue
as the tissue size. Ideally, when there are more sub-layers and
more cells are fused into a tissue, there will be fewer tissues
in the layer and thus, fewer re-loads for the weight matrices
and performance are improved as well. However, we observe
that keeping increasing the tissue size would even hurt the
performance. Fig.9 demonstrates the normalized performance
of one LSTM layer as the tissue size increases when executing
the investigated benchmarks (The baseline case introduced in
Section VI-A.). As it shown, the performance first increases
with the increasing tissue size, and then drops when the tissue
0x
1.5x
3x
4.5x
6x
12345678
0%
25%
50%
75%
100%
Layer Speed Up
Tissue Size
Shared Memory 
Bandwidth
IMDB MR BABi SNLI PTB MT
IMDB MR BABi SNLI PTB MT
Best Performance
Fig. 9: The normalized performance of one LSTM layer
and shared memory bandwidth utilization as the tissue size
increases.
size exceeds a certain number (e.g. 6 for BABI benchmark, 5
for the others in Fig.9). We define this number as the maximum
tissue size (MTS).
The performance drop is caused by the limited on-chip
bandwidth (i.e. shared memory bandwidth) of the mobile
GPUs. Fig.9 also plots the utilization of the shared memory
bandwidth. As it shows, the bandwidth utilization increases
with the increasing tissue size, and it approaches to 100%
at the MTS. Further increasing the tissue size would cause
the kernel re-configuration at the compilation time to ensure
that the on-chip bandwidth utilization is below 100%. The
re-configuration reduces the on-chip bandwidth requirements
per thread but increases the thread amount in the kernel. As
a result, the execution time per tissue significantly increases
which could not be well compensated by the saved time on
the reduced matrix re-loads, leading to the overall performance
droop. Note that the MTS is determined by the GPU configurations, a framework is needed to dynamically implement the
LSTM layer reorganization scheme for various LSTM layer
configurations on different mobile GPUs.
Tissue Alignment: Since the tissue formation mechanism
simply combines multiple cells into tissues but ignores the
MTS, it may generate both fat and thin tissues. Fat tissues
have more cells than MTS (e.g. Tissue 0 in Fig8(b1) as MTS
is 3 in this example) leading to the over-utilized share-memory
bandwidth, while thin tissues have quite few cells (e.g. Tissue
2 in Fig8(b1)) and are unable to effectively reuse the weight
matrix. Both will affect the performance boost. To maximize
the performance, we further explore the tissue alignment
mechanism to well balance the tissue size by moving cells
from the fat tissues to thin tissues, e.g. moving cell7 and 8
from Tissue0 and Tissue1 to Tissue1 and Tissue2, respectively,
in Fig8(b1). Note that tissue alignment does not further break
any context link and ensures every tissue size is below or equal
to the MTS.
167
GPU
Config
LSTM
Config
MTS
Kenrel 
Sgemm(Wf,i,c,o, xt)
Breakpoints 
Search
Kenrel 
Sgemm(Uf,i,c,o, ht-1)
Tissue
Alignment
Input
Batching
Kenrel 
lstm_ew
User 
Accuracy 
Preference
Layer 
Begin
Predicted 
Context
link
LSTM 
Begin
Layer 
End
LSTM 
End
1
Set Threshold 
Upper Limit
Statistics
2 P 4
5 9
Output 
Accuracy
Offline 
Preparison
LSTM 
Runtime 
Operation
Thresholds 
Update
3
Accuracy 
Recovery
6
Per-App Processing
Per-Execution Processing
Per-Layer Processing
Tissue
Formation
7 8
Fig. 10: The implementation of the inter-cell level optimizations.
D. The Implementation
Offline Operations: We first execute LSTMs on the target
GPUs platform with various tissue sizes to determine the
MTS (Fig.10 1 ). Ideally, when the tissue size is MTS for
every tissue in the layer, the number of tissues for this layer
is minimized, leading to the maximal performance. Therefore,
the minimal number of tissues can be conducted by:
Nmin =
Norigin
MTS

(7)
where Norigin is the original number of LSTM cells in the
layer. Next, we execute LSTMs equipped with our optimizations to obtain a value for the relevance threshold αinter which
leads to Nmin number of tissues. This value is set as the
upper limit for αinter (Fig.10 2 ). Then, αinter is initialized
to its upper limit aiming to the best performance. Since the
accuracy loss may be considerable when the system gains the
best performance, αinter will be adjusted per each execution of
the application given the accuracy difference between the user
preferred accuracy and the application output accuracy, thus,
leading to the optimal performance-accuracy trade-offs from
the user perspective (Fig.10 3 ). Furthermore, the predicted
context link is produced based on the LSTM configurations
(Fig.10 4 ). Note that the operations 1 2 4 are determined
by the GPU and LSTM configurations and only processed
once per application.
Runtime Operations: At the LSTM runtime, after performing the per-layer multiplication kernel Sgemm(Wf,i,c,o, x),
the breakpoints search (Fig.10 5 ) and the accuracy recovery
(Fig.10 6 ) are triggered to break the layer into a set of
sub-layers, which will be further transformed into a set of
tissues with balanced tissue size (Fig.10 7 8 ). In each tissue,
the cells are concurrently processed by batching their input
vectors ht into a united input matrix Ht, and the input state
vectors ct are batched into one matrix Ct as well (Fig.10 9 ).
Correspondingly, the originally per-cell matrix-vector multiplication Sgemv(Uf,i,c,o, ht) kernels are now combined into one
per-tissue matrix-matrix multiplication Sgemm(Uf,i,c,p, Ht)
kernel. The weight matrix Uf,i,c,o is effectively re-used by all
the cells within the tissue, and its loading frequency reduces
from one per cell to one per tissue.
V. INTRA-CELL LEVEL OPTIMIZATIONS
As illustrated in Section.III, besides the redundant data
movements across cells, the off-chip memory bandwidth is the
other major performance limitation for each LSTM cell. Since
weight matrix Uf,i,c,o is the largest input data for one cell, it is
Uf,i,c
● ● ●
● ● ●
ct
⁞
ot ⁞
0
⁞
0
● ● ●
ht Eq.5
Eq.5
2
1
3
4
Eq.1,2,3
Weight matrix
Cell State
Output
Output 
Gate
Uf
Ui
Uc
Fig. 11: Irrelevant rows in weight matrix Uf,i,c that have trivial
impact on cell output ht.
important to shrink it, hence addressing the bottleneck inside
the LSTM cell. In this section, we focus on the intra-cell level
optimization and effectively reducing the data loads per cell.
Generally, a common mechanism to shrink the weight matrix
size is targeting at each weight element and erasing the nearzero ones [31]. Noticing that weights in LSTM are processed
in the row order, and especially, the elements from different
rows are totally irrelevant. We leverage this unique feature to
propose the row-level weight compression technique, called
dynamic row skip, which compacts weights in the matrix at
the row level without affecting the output accuracy.
A. Dynamic Row Skip
As an interesting observation, we find that some rows in
the weight matrix Uf,i,c have trivial contribution to the cell
output vector ht. This is because ht is strongly affected
by the output gate ot. As shown in Eq.5, if one element
in ot is near zero, the corresponding output element in ht
will become near-zero (Fig.11 1 ) no matter what value the
corresponding element in ct is (Fig.11 2 ). Furthermore, since
ct is calculated based on the weight matrices Uf , Ui, Uc as
shown in Eq.1,2,3 (Fig.11 3 ), the corresponding rows in these
matrices can be treated as irrelevant to the final output element
in ht (Fig.11 4 ).
We propose the Dynamic Row Skip (DRS) scheme to
dynamically skip those irrelevant rows in the weight matrices
Uf , Ui, Uc whose computations have trivial contribution to the
cell output vector ht. By doing this, the skipped rows will not
be loaded and their computations are ignored as well, leading
to both performance and energy optimizations. Note that the
DRS will also affect the state vector ct, some of its elements
corresponding to the skipped rows will be approximated to
zero. However, this impact is observed to be quite limited to
the overall accuracy since the next cell will use the forget gate
168
Algorithm 3 LSTM Computation Flow with DRS
1: for each layer in LSTM do
2: Kernel Sgemm(Wf,i,c,o, x);
3: for each cell in layer do
4: Kernel Sgemv(Uo, ht−1);
5: Kernel lstm ew(ot);
6: Kernel DRS(ot, αintra, R)
7: Kernel Sgemv(Uf,i,c, ht−1, R);
8: Kernel lstm ew(ft, it, ct−1, ct, ht);
9: end for
10: end for
Grid Management Unit (GMU)
Pending Kernel Pool
Software TID 
Queue
TRB
Mask
Decode
Prefix Sum
Hardware TID Queue
Accumulation Shifter
CTA schedule
32
32
32
32
Hardware 
Work
Queue
CTAs 
Reorganization
Module
LD
2
DTIDs
Offsets
Fig. 12: The architecture of CTAs reorganization module.
ft+1 to filter the state vector from previous cell as shown in
Eq.3. Unlike the traditional weight pruning methods performed
offline [31], DRS is conducted at runtime for each LSTM cell
as it requires the latent information, and the rows to be skipped
vary across different LSTM cells.
B. The Implementation
In DRS, the rows to be skipped is determined by the
latent vector ot. In other words, only when the near-zero
elements in the ot are available, the corresponding rows in
weight matrices Uf , Ui and Uc can be identified and skipped
in the cell execution. This requires the modification of the
computation flow for the LSTM cell to generate ot before
processing the weight matrices Uf , Ui, and Uc. We split the
Sgemv(Uf,i,c,o, ht−1) kernel into two kernels: one multiplies
weight matrix Uo with the input vector ht−1, and the other
multiplies the weight matrix Uf,i,c with ht−1.
Algorithm 3 illustrates the reorganized computation flow
by the DRS. In each cell, Sgemv(Uo, ht−1) kernel will be
launched first (line 4) followed by the lstm ew(ot) kernel
to compute the latent vector ot (line 5). Then, each element
in ot is compared with a near-zero threshold (αintra) in our
DRS(ot, αintra, R) kernel to obtain the trivial rows whose ID
are saved as the list R. (line 6). Next, Sgemv(Uf,i,c, ht, R)
kernel will be launched to perform matrix multiplication
Uh,i,c × ht with the trivial rows in Uf,i,c disabled, which are
indicated by R (line 7). Finally, the remaining computations
in the cell are finished by lstm ew(ft, it, ct−1, ct, ht) (line
8). Note that the near-zero threshold is also adjusted based
on the user preferred accuracy requirement to deliver the user
satisfied performance-accuracy trade-offs.
Hardware Design: Even our DRS can be implemented by
the pure software method, i.e. assigning different operations
for trivial and non-trivial rows, it causes branch divergence
during the GPU execution and decreases the warp efficiency
Skipped 
Rows 
Number
Breakpoints
Information
Threshold 
sets Outputs
LSTMs
Pytorch
Baidu
DeepBench
Performance
Power
Jetson TX1
Gate-Level
Simulation
Fig. 13: The evaluation diagram for our optimizations.
TABLE I: Platform Specifications
Hardware Specification
System Tegra X1 SoC
CPU Cortex-A57 +Cortex-A53
Memory 4GB LPDDR4, 25.6GB/s
GPU Maxwell, 256 Core, 998MHz
and the system performance. We propose a hardware design to
support the DRS, which introduces CTAs reorganization module (CRM) to the grid management unit (GMU), as illustrated
in Fig.12. The CRM is able to identify the threads assigned
to process the trivial rows and re-organize the CTAs to skip
them. After a kernel being launched, its information (e.g.
kernel name and argument number) can be acquired through
the initialization. And a kernel with additional argument (i.e.,
R) implies containing trivial rows. It will then be assigned to
CRM for CTAs re-organization.
Once a kernel enters into the CRM, a load module (LD)
first loads and saves the trivial rows IDs to the trivial rows
buffer (TRB), and then the disabled thread IDs (DTIDs) can
be decoded based on the trivial rows IDs and the grid configurations. Note that there are two kinds of thread IDs during the
GPU kernel execution, one is the software thread ID (STID)
within a kernel launch, and the other is the hardware thread ID
(HTID) that indicates the hardware thread slot assigned to the
software thread. Since some threads will be disabled during the
kernel execution, there will be an offset between the STID and
HTID for each thread. Next, each thread’s STID in the kernel
is filtered by the DTID and sent to the prefix sum to determine
the offset, which is then used to sort and shift the STID to
acquire the correct HTID. This HTID acquisition process is
conducted at the unit of 32 threads, which is the warp size that
is usually divisible by the CTA size. It is further partitioned
into two stages shown by the dash boxes in Fig.12 for the
pipelined process in the CRM. Finally, the re-organized CTAs
will be sent to hardware work queue and wait to be issued.
VI. EVALUATION
A. Experimental Setup
In this work, we leverage a software-hardware cooperated
method to evaluate our optimizations for the LSTM execution.
From the software side, we employ PyTorch [32] which
is a popular open-source machine learning framework that
supports the dynamic computation graphs; we also use Baidu
DeepBench [33], a tool to benchmark the operations of deep
learning on the hardware platform. From the hardware side,
considering that the current GPU architecture simulators (e.g.
GPGPU-Sim [34]) cannot support the latest backend libraries
169
TABLE II: The state-of-the-art NLP applications investigated
in our study
Name Abbr. Hidden Size Layers Length
IMDB [37] SC 512 3 80
MR [38] SC 256 1 22
BABI [11] QA 256 3 86
SNLI [39] ET 300 2 100
PTB [40] LM 650 3 200
MT [41] MT 500 4 50
for machine learning (e.g. cuDNN [25] and cuBLAS [35]), we
employ the Jetson Tegra-X1 develop kit [36], a representative
mobile GPU development board with the configurations listed
in Table I.
Fig.13 illustrates the evaluation diagram. Both our interand intra-level optimizations require data approximation that
affects the output accuracy, and computation flow change that
affects the performance and power. The data approximation
can be implemented on PyTorch to obtain the accuracy results.
The computation flow changes can hardly be implemented on
PyTorch as the latest GPU machine learning backend libraries
(i.e. cuDNN) are released as pre-compiled binaries. We thus
use PyTorch, DeepBench, and real GPU cooperated method to
evaluate our techniques on performance and power. We first
use PyTorch to produce the breakpoints information for the
inter-level optimizations and the number of trivial rows for
the intra-level optimizations. These informations will then be
sent to the DeepBench to simulate the LSTM execution with
our optimizations on the Jetson Tegra-X1 board. We obtain
both performance and energy results from the board, note the
obtained energy result describes the energy consumption of
the overall system including CPU, GPU, etc. To consider the
performance and power overheads caused by our hardware
design, we model it via the gate-level simulation and include
the overheads into our results.
Benchmarks: We employ 6 state-of-the-art NLP Apps listed in
Table II as the LSTM benchmarks. Each App has the unique
LSTM configurations, where the Hidden Size indicate the
weight matrix size and the length indicates the number of cells
per LSTM layer. IMDB [37] and MR [38] perform sentiment
classification (SC) that predict the positive or negative attitude
of texts. BABI [11] performs question answering (QA) for
automatic text understanding and reasoning. SNLI [39] is
a collection of 570k human-written English sentence pairs
manually labeled for balanced classification with the labels
entailment (ET). PTB [40] is used for word-level language
modeling (LM). And MT [41] performs the English to French
translation (MT).
In general, 2% accuracy loss is imperceptible to the end
users. We first fix the user preferred accuracy requirement as
98% when evaluating the performance and energy improvements gained by our techniques. We also conduct the user
study by tunning the accuracy requirement per each individual
user.
B. The Effectiveness of the Overall System
Fig.14 plots the performance speed-up and the energy
savings obtained by our inter-cell level optimizations, intra-cell
level optimizations, and the overall system with the combined
0x
1x
2x
3x
4x
Speed Up
Inter-cell Intra-cell Combined
(a)
0%
20%
40%
60%
80%
IMDB MR BABI SNLI PTB MT Average
Energy Saving
(b)
Fig. 14: The (a) speedup and (b) energy saving achieved by our
system when applying inter-cell level optimizations, intra-cell
level optimizations and the overall system with the combined
optimizations.
optimizations. The results are normalized to the baseline case
that executing the state-of-the-art LSTMs on mobile GPUs.
1) Inter-cell Level Optimizations: On average, the intercell level optimizations achieve 2.05X speed up and 35.94%
energy saving compared with the baseline case. We observe
that our techniques show even stronger capability in improving
the performance and energy consumptions when the length
(i.e. the number of LSTM cells) of the LSTM layer increases.
For example, PTB with the longest layer length among all
investigated benchmarks achieves the highest performance and
energy enhancements. This implies that our techniques well
scales with the longer LSTM layer.
We further investigate the effectiveness of our inter-cell
optimizations on each LSTM layer, shown in Fig.15. As it
shows, our techniques perform better for the earlier (e.g., layer
1) than the latter layers (e.g., layer 3). This is because the
context information is closer to the original text inputs, and
the context links are more distinct for the earlier layers. They
can be divided into more sub-layers for higher performance
and energy gains.
2) Intra-cell Level Optimizations: Fig.14 also shows that
on average, our intra-cell level optimizations achieve 1.65X
speed up and 16.93% energy saving compared with the
baseline case. We observe that our techniques gain higher
performance and energy improvements with the larger weight
matrices. For example, PTB with the largest weight matrices
among all investigated benchmarks achieves the highest performance and energy saving. In one sentence, our techniques
exhibit the strong scalability with the increasing input data set,
which is the trend of the NLP-based IPA applications.
We further compare our intra-cell level techniques with
the popular weight matrix compression scheme (zero-pruning
[31]) and pure software-based DRS, shown in Fig.16. As it
shows, the zero-pruning scheme reduces 37% data movements
with only 7% power saving and even degrades the performance
by 35%, comparing with the baseline case. This is because
the zero-pruning scheme prunes the near-zero elements in
the weight matrices without considering the possible branch
divergences when executing the LSTMs on GPUs. Excitingly,
170
0%
12%
24%
36%
48%
60%
1.0x
1.5x
2.0x
2.5x
3.0x
3.5x
Layer Energy Saving
Layer Speed Up
Application-LSTM layer
Speed up Energy Saving
Fig. 15: Per-layer speed up and energy saving when applying
the inter-cell level optimizations.
0x
0.5x
1x
1.5x
2x
2.5x Speed Up
0%
7%
14%
21%
28%
35%
Energy Saving
0%
20%
40%
60%
80% Compression Ratio
Zero-pruning Pure Software-based DRS DRS with hardware design
(a) (b) (c)
Fig. 16: (a) Weight matrix compression ratio, (b) speed up and
(c) energy saving when applying different weight compression
schemes
our DRS scheme achieves better weight compression ratio
(i.e., on average 50.35%) and better energy saving (i.e.,
16.92%) than the zero-pruning scheme. The pure softwarebased DRS still induces the branch divergence and can only
achieve small performance gain 1.07X on acerage. With the
hardware design to enable the CTAs re-organization, our intracell level optimizations maintain the high warp efficiency and
achieve additional 57.78% speed up than the pure software
method.
3) Putting It All Together: As shown in Fig.14, on average,
our system with the combined intra- and inter-level optimizations outperforms the baseline case by 2.54X (upto 3.24X)
in performance and 47.23% (upto 58.82%) in energy saving.
Note that the improvements gained by the overall system are
not the sum of the improvements obtained by each technique
as there are some overlaps on the data movements reduction
between the two level techniques.
C. Performance-Accuracy Trade-offs
To explore the design space for the performance and accuracy trade-offs, we conduct the sensitivity analysis by tunning
the two thresholds applied in our techniques, i.e., αinter and
αintra. Note that the energy saving is proportional to the
performance boost and we mainly analyze the performanceaccuracy trade-offs. For each threshold, we explore 11 values
increasing from ‘0‘ (representing the baseline case without
any accuracy loss) to its maximal value (representing the most
aggressive case with the maximal performance boost). We then
obtain 11 threshold sets with each set containing a pair of
values for αinter and αintra, respectively. Threshold set 0 (10)
has the lowest (highest) threshold values. Fig.19 demonstrates
the normalized speedup and accuracy when different sets of
thresholds are applied for the investigated applications. We
denote AO (accuracy oriented) as the threshold set corresponding to the optimizations with user-imperceptible accuracy loss
(i.e., 2%). As the figure shows, a higher threshold value
leads to a better performance gain, and we denote BPA (best
0x
2x
4x
6x
8x
0% 20% 40% 60% 80%
Speed Up
Normalized Accuracy Loss
256-86 512-86
768-86 1024-86
(a) (b)
0x
2x
4x
6x
0% 20% 40% 60% 80%
Speed Up
Normalized Accuracy Loss
256-26 256-46
5% 5% 256-66 256-86
Fig. 17: The performance-accuracy trade-offs of LSTMs for
BABI with (a) different hidden unit sizes; (b) different input
lengths. Each line represents a configuration of (hidden unit
size - input length) pair.
0
1
2
3
4
5
IMDB MR BABI SNLI PTB MT
User Satisfaction Score
Baseline AO BPA UO
Fig. 18: The user satisfaction score on different schemes
performance-accuracy) as the threshold set leading to the
highest Speedup × Accuracy.
D. Impact of Model Capacity
Model capacity defines the size and input format of the
LSTMs, which affects computation scale. To evaluate the
impact of model capacity on our techniques, we conduct
a sensitivity analysis on performance-accuracy trade-offs of
LSTMs given different model capacity (e.g., hidden unit size
and input length) when applying our techniques. Fig.18 shows
the trade-off results for one representative benchmark BABI
due to the space limit. As it shows, given the same accuracy
requirement, our techniques achieve higher speedup when
the hidden unit size or the input length increases. On the
other hand, when the accuracy loss is small (e.g. <5%),
the speedup achieved by our technique varies slightly across
different hidden unit size and input length. In other words,
model capacity has trivial impact on our technique since NLP
tasks usually have high accuracy requirement.
E. User Study
Since our techniques require both software and hardware
simulations, it is impossible to test on a real product. To
evaluate our system impact on the user experience, we build
a replay program that provides users the pre-produced outputs
(thus, output accuracy) with a response delay (thus, performance) according to the selected thresholds. We compare
four schemes: the baseline case, the scheme applying the AO
threshold set, the scheme applying the BPA threshold set,
and finally, our UO (user oriented) scheme that dynamically
adjusts the thresholds by further taking each individual user’s
preferences as the user input.
We randomly recruit 30 participants on a college campus.
We let them experience multiple replays for several NLP
applications, and rate the satisfaction score (i.e., 1 being
unsatisfied and 5 being most satisfied) based on the output and
the response delay. Each participant will be asked to rate 100
replays for each application with the scheme changed every
171
40%
55%
70%
85%
100%
0x
2x
4x
6x
8x
0 1 2 3 4 5 6 7 8 9 10
Speed Up
Accuracy
Thresholds Set
Speed up Accuracy
AO
BPA
(a) IMDB
40%
55%
70%
85%
100%
0x
1x
2x
3x
4x
0 1 2 3 4 5 6 7 8 9 10
Speed Up
Accuracy
Thresholds Set
Speed up Accuracy
AO
BPA
(b) MR
0%
25%
50%
75%
100%
0x
2x
4x
6x
8x
0 1 2 3 4 5 6 7 8 9 10
Speed Up
Accuracy
Thresholds Set
Speed up Accuracy
AO BPA
(c) BABI
40%
55%
70%
85%
100%
0x
2x
4x
6x
8x
0 1 2 3 4 5 6 7 8 9 10
Speed Up
Accuracy
Thresholds Set
Speed up Accuracy
AO BPA
(d) SNLI
20%
40%
60%
80%
100%
1x
3x
5x
7x
9x
0 1 2 3 4 5 6 7 8 9 10
Speed Up
Accuracy
Thresholds Set
Speed up Accuracy
AO
BPA
(e) PTB
20%
36%
52%
68%
84%
100%
1x
2x
3x
4x
5x
6x
0 1 2 3 4 5 6 7 8 9 10
Speed Up
Accuracy
Thresholds Set
Speed Up Accuracy
BPA
AO
(f) MT
Fig. 19: Performance-Accuracy trade-offs under different sets of thresholds across the different applications.
25 replays. The order of the schemes is random. Fig.18 shows
the averaged user satisfaction scores on different schemes. As
it shows, AO always achieves better user satisfaction score
against the baseline case because the application response time
is reduced and the users can not notice the accuracy loss.
However, BPA does not achieve good user satisfaction score
as most users are not willing to trade much accuracy loss for
aggressive performance improvements. Finally, in general, our
UO scheme achieves the best users satisfaction score among
all the four schemes since it takes the user preferences into
the consideration to dynamically tune the threshold for the
excellent user experience.
F. Overhead Analysis
The inter-cell level optimizations introduce some lightweight computations, causing only 2.23% performance and
1.65% power overheads on average. The intra-cell level optimizations modify the LSTM computation flow and introduce
some extra computations at the software side, which cause
3.39% performance and 3.21% power overheads on average.
At the hardware side, the CTAs reorganization module is
mainly composed of simple logic gates which only causes
1.47% performance and <1% power overheads based on our
gate-level simulations.
VII. RELATED WORKS
There have been multiple studies on CNNs optimizations.
Some of them target at CNNs optimizations on mobile GPUs
[16], [18]. while others design the ASIC accelerators for high
performance neural netwroks [42], [43]. Also several studies
have been well conducted on the weight compression for
CNNs via erasing trivial elements [19], [20], [44]–[46]. And
the execution-efficiency-aware weight matrices compression
for CNNs are well studied by [13], [47]–[49]. For example,
[13] proposes DeftNN to compress the CNNs weight matrix
by eliminating columns, [47] explores the node pruning for
CNNs. Since the execution patterns of LSTMs are far different
from CNNs, these works are not applicable to the LSTMs.
There are also some works on RNNs computation optimizations. [50], [51] propose the scheme to eliminate memory
bandwidth pressure of uploading recurrent weights on-chip.
However, these optimizations can hardly be implemented on
mobile device as the limited on-chip storage of mobile GPUs
can not eliminate the redundant data accesses. Besides, [52],
[53] explore the accelerator design for high performance
RNNs execution. Our work focus on mobile GPUs which are
more flexible to process various applications with different
LSTMs configurations.
Several studies have exploited optimizations on computation
flow [54]–[57]. These works leverage the computation characteristics of their applications to explore the parallelism. However, none of these works can be directly applied to the layer
processing of LSTMs. Our work is the first work to explore the
parallelism inside each LSTM layer via analyzing the unique
mathematical characteristics of LSTM cell computations.
VIII. CONCLUSION
NLP-based IPAs become increasingly popular in mobile
devices. Their core machine learning technique is RNNs, especially LSTMs. With the continuously improved performance
of mobile GPUs, local processing has become a promising
solution to the large data transmission and privacy issues
induced by the cloud-centric computations of IPAs. However, LSTMs exhibit quite inefficient memory access pattern
when executed on mobile GPUs due to the redundant data
movements and limited off-chip bandwidth. In this study, we
propose two level optimizations to hierarchically explore the
memory friendly LSTM on mobile GPUs, thus, achieving the
substantial improvements on both performance and power.
At the inter-cell level, we propose LSTM layer division and
reorganization techniques to greatly improve the data locality
across cells. At the intra-cell level, we propose dynamic row
skip (DRS) techniques to conduct dynamic row-level weight
matrix compression. Based on our experiment results, our
proposed techniques achieve on average 2.54X (upto 3.24X)
performance improvement and 47.23% energy saving on the
entire system with only 2% accuracy loss that is generally
user imperceptible, comparing with the state-of-the-art LSTM
execution on mobile GPUs. And our optimizations have the
strong scalability in dealing with the increasing size of input
data. Our user study also shows that our designed system
delivers excellent user experiences.