Abstract‚ÄîMemory systems are becoming bandwidth constrained and data compression is seen as a simple technique to
increase their effective bandwidth. However, data compression
requires accessing Metadata which incurs additional bandwidth
overheads. Even after using a Metadata-Cache, the bandwidth
overheads of Metadata can reduce the benefits of compression.
This paper proposes Attache¬¥, a framework that reduces the
overheads of Metadata accesses. The Attache framework consists ¬¥
of two components. The first component, called the Blended
Metadata Engine (BLEM), enables data and its Metadata to be
accessed together. BLEM incurs additional Metadata accesses
only 0.003% times and removes almost all Metadata bandwidth
overheads. The second component, called the Compression Predictor (COPR), predicts if the memory block is compressed. The
COPR predictor uses a fine-grained line-level predictor, a coarsegrained page-level predictor, and a global indicator. This enables
Attache to predict the compressibility of the memory block before ¬¥
sending a memory read request. We implement Attache on a ¬¥
memory system that uses Sub-Ranking. On average, Attache¬¥
achieves 15.3% speedup (ideal 17%) and saves 22% energy
consumption (ideal 23%) when compared to a baseline system
that does not employ data compression. Attache is completely ¬¥
hardware-based and uses only 368KB of SRAM.
Index Terms‚ÄîData Compression, Metadata, Bandwidth, SubRanking, Memory Systems
I. INTRODUCTION
Increasing the main memory bandwidth is instrumental for
increasing the performance of processor chips and accelerators [1], [2], [3], [4]. Designers have traditionally improved
bandwidth by increasing the frequency or the pin count of the
memory interface [5], [6], [7], [8], [9]. These techniques tend
to have area and energy costs. Furthermore, interface changes
are slow, as new memory standards are proposed only every 2
to 3 years [1], [2]. One can overcome these challenges by using
data compression. Compression enables the memory systems to
transfer data over fewer pins and fewer memory chips, thereby
unlocking higher bandwidth [10]. However, identifying if the
data is compressed requires additional Metadata [10], [11],
[11], [12]. The additional memory accesses to Metadata can
offset the benefits of compression. This paper tries to mitigate
the memory access overheads of obtaining Metadata.
Main memory systems tend to comprise of Dynamic Random
Access Memories (DRAM) [13], [14]. On a memory request,
commodity memory modules transmit/receive 64 bytes of data
(a cacheline). Each module typically contains 8 DRAM chips
*Both authors contributed equally towards this paper.
‚Ä†Currently affiliated with Kyungpook National University, South Korea.
Email: seokin@knu.ac.kr
and each chip contributes 8 bytes of the 64-byte memory block.
If we could compress a 64-byte block to a 32-byte block, then
we can enable only 4 DRAM chips for each request, thereby
doubling the effective bandwidth. This technique is called SubRanking and together with data compression, Sub-Ranking can
be used to improve memory bandwidth [15], [16], [17].
Unfortunately, the Metadata overheads involved in data
compression can offset its bandwidth benefits. This is because
each cacheline-size memory block from main memory will
require a unique Metadata to identify its compressibility [10],
[11], [12], [18]. For a high capacity memory system, it is
impractical to store the Metadata in the memory controller.
For instance, in a 16GB memory system if each cacheline
requires 1 bit of Metadata, then the memory controller will
need 32MB of storage. It is impractical to implement a 32MB
SRAM in the memory controller due to latency, area, and
energy overheads [19], [20], [21]. Therefore, Metadata tends
to be stored in a separate location within the main memory
and tends to require issuing an additional memory request.
One can reduce the bandwidth overhead of Metadata by
employing a small Metadata-Cache within the memory controller [10]. Unfortunately, additional Metadata-Cache eviction
and install requests can reduce the performance benefits of data
compression. Figure 1 shows the proportion of compressed
memory blocks (30 Bytes) and the additional bandwidth
overheads of Metadata accesses. For this analysis, we use
a reasonably large 1MB Metadata-Cache inside the Memory
Controller. We also plot the values for SPEC [22] and GAP [23]
workloads. Ideally, we can reduce the additional memory
requests for accessing Metadata by up to 85%. The goal of this
paper is to reduce almost all Metadata accesses. To this end,
this paper proposes Attache¬¥, a framework that helps mitigate
Metadata accesses to provide a near-ideal speedup.
Fig. 1: The memory access overheads of Metadata accesses for
SPEC [22] and GAP [23] benchmarks. Metadata can increase
the memory traffic by up to 85%. The goal of this paper is to
mitigate this overhead and obtain near-ideal speedup.
326
2018 51st Annual IEEE/ACM International Symposium on Microarchitecture
978-1-5386-6240-3/18/$31.00 ¬©2018 IEEE
DOI 10.1109/MICRO.2018.00034
Attache tackles Metadata bandwidth overheads by storing the ¬¥
Metadata of a cacheline-size memory block within the block
itself. On a memory read, Attache gets data and Metadata ¬¥
together in one access. Therefore, Attache avoids almost all ¬¥
additional accesses to the memory. To this end, Attache consists ¬¥
of two components. The first component, called the Blended
Metadata Engine (BLEM), tries to place data and Metadata
together. The second component, called Compression Predictor
(COPR), predicts the values of Metadata for memory accesses.
The predictor enables the memory controller to proactively
issue memory requests only to the predicted Sub-Ranks.
The Blended Metadata Engine (BLEM) tries to place the
data and Metadata in the same memory block during writes
to memory. This is easy when the cacheline is compressible,
as compression creates additional space for storing Metadata.
However, if the cacheline is not compressed, then there is
no additional space for Metadata. To overcome this problem,
irrespective of whether the data is compressed or not, BLEM
interprets the first few bits of the cacheline as its Metadataheader. A Metadata-header consists of a Compressed ID (CID)
and an Exclusive ID (XID).
The Compressed ID (CID) identifies the compression status
of the cacheline. The CID value is chosen randomly at boottime and stored in the memory controller. For instance, if we
have a 15-bit CID, the memory controller appends the 15-bit
CID value to the compressed cacheline during a write. As each
Sub-Rank stores 32 bytes of compressed data, for a 15-bit CID,
the target compression size of a cacheline is 30 bytes. For a
64-byte cacheline that cannot be compressed to 30 bytes, the
memory controller does not append the CID value (as there is
no additional space) and simply writes-back the uncompressed
data into both the Sub-Ranks (32 Bytes in each Sub-Rank).
During a read, if the line is compressed, then its top 15-bits
will match the CID value. Therefore, the CID value can be
used to identify compressed memory blocks. Unfortunately,
while reading uncompressed 64-byte memory blocks, the top
15-bits can match the CID value by chance and there can
be false-positives. In fact, for a 15-bit CID value, there is a
1
215 chance (0.003%) that the top 15-bits of the uncompressed
memory blocks will match the CID value 1. This makes it
difficult to identify whether the block is really compressed or
simply a false positive. We refer to such cases as CID collisions
and BLEM must identify all CID collisions.
The second part of Metadata-header, a 1-bit Exclusive ID
(XID), helps identify CID collisions. XID is checked only if
the top 15-bits of the data match the CID value. If the XID
value is set to ‚Äò1‚Äô, it indicates a CID collision. During a write,
if the memory block is compressible to 30 Bytes, then the
XID value is reset to ‚Äò0‚Äô and appended after the CID value.
However, if the memory block is not compressible, then instead
of writing-back the unmodified memory block, the memory
controller checks the top 15-bits of the memory block for a
CID collision. In case of a CID collision, the memory controller
1Modern memory systems randomize data by performing data scrambling.
Due to this, the probability of a 15-bit CID false positive for any uncompressed
data memory block is 0.003%.
proactively writes an XID value of ‚Äò1‚Äô as the 16th-bit of the
line, therefore altering the original data bit in the uncompressed
memory block. The 16th data-bit replaced by XID is written
into a separate memory region called the Replacement Area.
On a CID collision, the memory controller reads the entire
64-byte memory block and fetches the 16th data-bit from the
Replacement Area. BLEM incurs additional memory reads
only during CID collisions. As CID collisions are rare (0.003%
times), BLEM incurs negligible bandwidth overheads.
While BLEM minimizes the memory bandwidth overheads
of Metadata to 0.003%, it only allows accessing Metadata
together with data. However, to identify if both Sub-Ranks
need to be enabled for a given memory block (when it is
not compressed), the memory controller needs to access the
Metadata before accessing data. To enable Metadata lookup
before accessing data, several prior works have proposed
using a Metadata-Cache. The Metadata-Cache stores the most
recently accessed Metadata. Unfortunately, Metadata-Cache has
bandwidth overheads as cache-misses have additional evictions
and replacement memory requests.
This paper proposes replacing the Metadata-Cache with
a predictor. This predictor, called a Compression Predictor
(COPR), predicts the compression status of the memory block.
The COPR consists of three levels of predictors. The first level,
called the line-level predictor (LiPR), provides predictions for
cachelines within a DRAM page. The second level, called
the page-level predictor (PaPR), provides predictions for the
compressibility of different DRAM pages. The third level,
called the global indicator (GI), is a simple two-bit counter
that keeps tracks of the compressibility of the most recent four
memory accesses.
After a 32-byte memory block is read, the memory controller
can check if its Metadata prediction was correct by interpreting
the Metadata from BLEM. If the prediction was incorrect, the
memory controller takes a corrective action by reading the
remaining 32-byte memory block. As BLEM ensures that the
Metadata is always read/written from/into the memory, COPR
does not cause additional Metadata accesses.
Overall this paper has the following contributions:
1) Blending Data and Metadata (BLEM), a technique
to encode Metadata within the data irrespective of the
compressibility of the data. Thereby eliminating almost
all bandwidth overheads (99.997%) of Metadata accesses.
2) A Simple Compression Predictor, a high-confidence
predictor that enables the BLEM engine to guess the
compressibility of memory blocks.
Attache provides 15.3% speedup and 22% energy reduction ¬¥
by efficiently compressing data into a system that uses SubRanking. Attache does not require any software support. ¬¥
Furthermore, Attache requires only 368KB of SRAM for the ¬¥
predictor and a single register to store the CID value.
II. BACKGROUND AND MOTIVATION
We provide a brief background on the main memory organization and data compression. Furthermore, we also highlight
the bandwidth overheads involved in managing Metadata.
327
Fig. 2: Comparison of Baseline System, Sub-Ranking, and Sub-Ranking with compression. Figure (a) shows the baseline system
which does not employ Sub-Ranking or compression. Figure (b) shows that Sub-Ranking can unlock 2x bandwidth but can also
increase latency by 2x. Figure (c) shows that Sub-Ranking with compression can reduce the latency overheads of Sub-Ranking
and still provide 2x bandwidth for compressible lines (more requests/second).
A. Main Memory System Organization
Commodity memory modules consist of several DRAM
chips. Each DRAM chip consists of several bank-groups
and each bank-group is further divided into multiple banks.
Each bank contains millions of DRAM cells and operates
independently within a channel [1], [2], [24], [25], [26].
A memory request from the memory controller activates a
row of DRAM cells in the same bank across all chips. The
row of activated DRAM cells is read into the row-buffer. As
each memory module typically consists of 8 DRAM chips, the
row buffer is split across each chip. For instance, in the case
of an 8KB row buffer, each chip will hold 8Kb of data in its
row buffer. Each memory request will fetch the appropriate 64
bits data from the 8Kb row buffer of a chip. As there are 8
chips, each memory request accesses 64 bytes of data [27].
B. Memory Bandwidth and Latency Tradeoff
As each memory module consists of several DRAM chips,
one can improve bandwidth up to 2x allowing individual
memory requests to be serviced by independently by 4 DRAM
chips. However, each DRAM chip will now have to provide 128
bits of data as compared to 64 bits (2x more) and will increase
memory access latency by 2x [27], [28] 2. To reduce latency,
commodity memory modules tend to operate all DRAM chips
in lockstep as shown in Figure 2(a). This reduces the memory
access latency while providing tolerable bandwidth [15], [30].
Ideally, we would like to get higher bandwidth with no
additional latency overheads.
C. Get Bandwidth without Paying Latency
One can unlock higher bandwidth without incurring additional latency by using Sub-Ranking with Compression.
1) Sub-Ranking to Unlock Higher Bandwidth: One way
to improve bandwidth is by enabling the memory request to
access a smaller group of DRAM chips within a module. Each
group of DRAM chips is called a Sub-Rank [15]. Unfortunately,
simply Sub-Ranking memory modules increase its latency [16],
[17]. For instance, if a memory module has two Sub-Ranks,
then each memory request is catered by 4 DRAM chips instead
of 8 DRAM chips. This means that each chip will still have to
provide 2x more data and this will increase the access latency
by 2x as shown in Figure 2(b).
2One may also use pseudo-channels in HBM2 [29] without additional latency
overheads.
2) Compress Data to Reduce Latency: One way to mitigate
the additional access latency from Sub-Ranking is by compressing data. For instance, if a 64-byte memory block is compressed
to at least 32 Bytes, then each Sub-Rank can provide this
compressed data with 2x lower access latency (same latency as
the baseline system) as shown in Figure 2(c). Therefore, SubRanking with compression is a practical low-latency solution
for enabling high bandwidth memory systems [15], [31].
D. Main Memory Data Compression
To compress data, memory systems can use low-latency
algorithms that are implemented in the memory controller.
1) Efficient Data Compression Algorithms: Data values tend
to be similar within a cacheline. The Base-Delta-Immediate
(BDI) algorithm uses this insight to compress cachelinesize data blocks [12], [18]. Similarly, the Frequent-PatternCompression (FPC) [32] algorithm keeps track of frequently
occurring data patterns. Data patterns like all-zeros tend to
occur frequently and FPC represents these patterns with fewer
bits. To this end, FPC requires only a small lookup table.
2) Compression-Decompression Engine: The data compression is performed by a compression-decompression engine in
the memory controller as shown in Figure 3. The compressiondecompression engine typically runs one or more compression
algorithms. During a write, the compression engine is activated
and the data is compressed as it flows through the engine into
the Sub-Rank. During a read, the decompression engine is
activated and the data is decompressed as it flows from the
Sub-Rank to the cores and caches.
 	














 


 

!
Fig. 3: The compression-decompression engine for a SubRanked memory system (not drawn to scale). Reads flow
through the decompression engine and writes flow through
the compression engine.
328
A key metric for a compression-decompression engine is its
latency. Fortunately, BDI requires only simple arithmetic operations and FPC requires only table lookups. These compression
algorithms can compress-decompress within 1 cycle.
E. Data Compression: Potential
To understand the effectiveness of data compression, we look
at the compressibility of cachelines (64-byte memory blocks)
for memory intensive SPEC and GAP workloads. Figure 4
shows the percentage of cachelines that can be compressed to
less than 30 bytes.
Fig. 4: The percentage of cachelines compressible to 30 Bytes.
On average, 50% of the cachelines (64-byte memory blocks) are
compressible to 30 Bytes and can be stored within a Sub-Rank
without additional latency overheads.
On average, 50% of the cachelines are compressible to
30 bytes. For these compressible lines, we can potentially
obtain 2x higher bandwidth by using Sub-Ranking. For the
remaining lines, we can disable Sub-Ranking and still maintain
1x bandwidth. Therefore, on average, we can ideally obtain 1.5x
higher bandwidth by employing data compression with SubRanking. However, in practice, a memory system employing
compression accesses additional Metadata and this lowers the
bandwidth benefits of compression.
F. Data Compression: Metadata Overheads
A memory system that employs compression also maintains
additional Metadata information for each memory block. For
instance, the decompression engine will require a 1-bit per
memory block to identify its compression status.
1) Capacity Overheads: Even if each 64-byte memory block
has only 1 bit of Metadata, the main memory contains millions
of memory blocks. Therefore, the overall capacity overheads of
Metadata tends to be large. For instance, a 16GB main memory
system will need 32MB of Metadata. Due to its large size, it is
impractical to place Metadata on the processor chip. Therefore,
Metadata is typically stored in a separate region within the
main memory with a capacity overhead of only 0.2%.
2) Bandwidth Overheads: Storing Metadata in main memory
has bandwidth overheads. Each memory request requires an
additional access to the Metadata region. These additional
accesses tend to lower the benefits of data compression.
G. Pitfalls of using a Metadata-Cache
A Metadata-Cache can be used to reduce the memory
bandwidth overheads of Metadata. If the Metadata is present
within the Metadata-Cache (cache hit), then no additional
Fig. 5: The Impact of Metadata-Cache hit-rate on performance.
On average, even after using a 1MB Metadata-Cache with a
77% hit-rate, we can obtain only 8% speedup.
memory accesses are required. However, if Metadata is absent
within the Metadata-Cache (cache miss), then two additional
memory requests (eviction and install) may be necessary.
We can increase the hit-rate of the Metadata-Cache by increasing its size. Unfortunately, it is impractical to create a large
Metadata-Cache within the memory controller. Furthermore, a
large Metadata-Cache will incur a significant lookup latency
and also increases the chip area considerably. Figure 5 shows
the average hit-rate of different sizes of Metadata-Cache. On
average, even with an impractically large 1MB Metadata-Cache,
we can obtain only 8% speedup [33].
III. ATTACHE¬¥: AN OVERVIEW
Figure 6 shows an overview of Attache for a 16GB main ¬¥
memory system. Broadly, Attache consists of two components. ¬¥
The first component of Attache, called the Blended Metadata ¬¥
Engine (BLEM) embeds Metadata with data for both compressed and uncompressed cachelines. To this end, BLEM
interprets the first few bits of data as the Metadata header that
consists of Compression ID (CID) and Exclusive ID (XID).
For cachelines that are uncompressed and their CID value
collides, BLEM will replace a data-bit with the XID value of
1. BLEM also uses a Replacement Area (RA) to store bits
that are replaced by XID. The second component of Attache,¬¥
called the Compression Predictor (COPR), helps predict if a
cacheline is compressible or not. COPR acts as a replacement
to the Metadata-Cache and avoids all bandwidth overheads
that result from managing the Metadata-Cache. Attache uses a ¬¥
compression-decompression engine to compress/decompress
memory blocks and provide the compressibility information
of the memory blocks to BLEM and COPR.
 	

 	




Fig. 6: An Overview of Attache (not drawn to scale). Attach ¬¥ e¬¥
consists of a Blended Metadata Engine (BLEM) and a Compression Predictor (COPR). The compression-decompression
engine will compress and decompress memory blocks. The
Replacement Area (0.2% of main memory) stores the bits that
are replaced by XID.
329
IV. THE ATTACHE¬¥ FRAMEWORK
This section describes the design of Attache framework. The ¬¥
Attache framework broadly consists of the Blended Metadata ¬¥
Engine (BLEM) and the Compression Predictor Unit (COPR).
A. The Blended Metadata Engine (BLEM)
The Blended Metadata Engine (BLEM) aims to store
Metadata and data together for all cachelines, irrespective
of their compressibility. However, traditional memory systems
face challenges in storing and accessing Metadata.
1) Pitfalls of Conventional Metadata Storage: Prior work
on memory compression places data and its Metadata in the
same row-buffer [10]. The memory controller can fetch the
data and Metadata by issuing two consecutive read requests
to the same row-buffer. This reduces the latency of fetching
Metadata. Figure 7 shows such an implementation. Typically
a DRAM row-buffer is 8KB (or 128 cachelines) and each
Metadata memory request accesses 64 Bytes (or 512 bits) of
Metadata. Therefore, even if a data cacheline uses 4-bits of
Metadata, a Metadata memory request can prefetch Metadata
for 127 data cachelines in the DRAM row.
 	

















	


 















	

!
 
"#
 
$#!%&	

Fig. 7: The conventional technique for storing Metadata.
Metadata is stored in the same row-buffer as data. Therefore,
Metadata and data can be accessed consecutively using two
memory requests to the same row-buffer.
Storing Metadata in this manner has three drawbacks. First,
Metadata will need to be accessed before accessing data. This is
because the memory controller will need the Metadata to figure
out the Sub-Rank or Sub-Ranks (if data is uncompressed) that
need to be enabled. This increases the data access latency and
read traffic from main memory. Second, for several workloads,
only a few cachelines tend to be accessed within a row-buffer.
Therefore, fetching Metadata for all cachelines in a row-buffer
is wasteful. Thirdly, the newly fetched Metadata cacheline can
cause the eviction of another cacheline from the Metadatacache. This increases the write traffic to the main memory. If
we can somehow fetch Metadata in the same access as the
data, then we can eliminate the drawbacks of storing Metadata
in the memory.
2) Insight: Intelligently prepend Metadata with data: One
can try to prepend Metadata with data. This is simple for
compressible cachelines as compressing a cacheline creates
additional space within the cacheline for storing Metadata.
However, for cachelines that are not compressed, there is no
additional space to store the Metadata.
Attache stores Metadata by interpreting the first few bits of ¬¥
the cacheline as the Metadata-Header and comparing it against
a Metadata-header that is stored in the memory controller. The
Attache framework performs this comparison irrespective of ¬¥
the compressibility of the cacheline. The Metadata-Header has
two components; the Compression ID and the Exclusive ID.
3) Identify Compression with Compression ID (CID):
Attache uses the Compression ID (CID) to identify if the ¬¥
cacheline is compressed or not. During writes, the CID is
prepended in front of the compressed cachelines. For cachelines
that are not compressed, there is no additional space for storing
CID. Therefore, the memory controller simply writes the lines
as they are. During a read, the memory controller uses the
insight that lines that do not have CID as the first few bits are
uncompressed lines. However, as there are millions of lines
in the memory system, it is possible that the first few bits of
some uncompressed lines will match the CID values. We refer
to this scenario of false positives as a CID collision.
4) Probability of CID Collision and the size of CID: The
probability of CID collision depends on the length of CID. For
instance, if CID is 3-bits long, then CID will collide every 8
memory requests (12.5% probability). In this paper, we use a
15-bit CID to reduce the probability of CID collision to 0.003%
(i.e 1
215 ). Figure 8 shows the probability of CID collision with
the number of accesses to uncompressed lines. Even in the
worst case, if all accesses are only to uncompressed lines, a
15-bit CID collides only every 32K accesses.





 
            	







 !
"#
 	$

%$

$


	&!



Fig. 8: The probability of a CID collision versus the number of
accesses to uncompressed lines. For a 15-bit CID, we expect
a CID collision every 32K accesses.
5) Extending CID to store more information: Currently, by
using only a 15-bit CID, Attache can identify if the cacheline is ¬¥
compressible or not. However, it is possible that the cacheline
is dynamically compressed by choosing either the BDI or the
FPC algorithm. To decompress such a cacheline, we will need
to add more Metadata information. To this end, we simply
reduce the size of CID to be 14-bits and use the 15th bit to
identify the compression algorithm. CID can be easily extended
to store additional information. Table I shows the CID size,
the number of information bits and the probability of collision.
TABLE I: Extending CID to store additional information
CID Size Additional Probability
Information Bits of Collision
15 0 0.003%
14 1 0.006%
13 2 0.01 %
330
 


 
	





 

	



 

 

 


 
	





  	





 


 
	





 

	





 

 



  	




	
	




 


 

     
Fig. 9: Analysis of BLEM for reads and writes. During writes, BLEM proactively inserts XID = 1 only for uncompressed lines
that have CID collisions. During reads, BLEM uses this information to identify collisions.
Using this insight, CID can be extended to cater to multiple
compression algorithms on the fly. Unfortunately, even a large
CID will collide at some point in time. If a CID collision is
not identified, the memory controller will wrongly interpret
the uncompressed line as a compressed line and cause data
corruption. Therefore, CID collisions will need to be detected.
6) Detecting CID collisions with Exclusive ID (XID): A
1-bit Exclusive ID (XID) is used to identify CID collisions.
During writes, if there is a CID collision, the memory controller
proactively replaces the 16th data-bit as XID equal to ‚Äò1‚Äô.
It stores the replaced data-bit in a separate area called the
Replacement Area (RA). If there is no collision, the memory
controller does not modify the uncompressed line. Therefore,
even if all memory blocks are uncompressed, the Replacement
Area will incur accesses only 0.003% of the time. If the memory
blocks are compressed, then the memory controller will simply
append an XID equal to ‚Äò0‚Äô after the CID. During reads, the
Replacement Area is read-only if the CID matches and the
16th data-bit is set to ‚Äò1‚Äô (collision).
7) Handling the data-bit replaced By XID: To cater to
the worst case in which all cachelines can encounter a CID
collision, each cacheline in the memory system has 1-bit in
the Replacement Area and indexes into it in a direct-mapped
manner. The area overhead of the Replacement Area is only
0.2% (i.e. 1
512 ) of the total capacity of the main memory. The
Replacement Area is invisible to the OS and is visible only to
the memory controller.
B. BLEM: Implementation and Flow
For reliability and security, memory controllers scramble
memory blocks using a Scrambling-Descrambling unit [34],
[35]. After scrambling, the data value appears pseudo-random.
Interpreting the Metadata-Header in the BLEM engine after
the data flows through the Scrambling unit will ensure that the
expected 15-bit CID collision probability is 0.003% 3.
Figure 9 shows how BLEM works for reads and
writes. BLEM uses the compression information from the
Compression-Decompression engine and can disable the
Compression-Decompression engine for uncompressed lines 4.
As shown in Figure 9(a), on a write request, if data cannot be
compressed, BLEM writes data as it is. However, as shown
in Figure 9(b), on a CID collision, BLEM proactively inserts
3Scrambling-Descrambling units tend to choose hashes with memory block
address as an input. This ensures that even if the same data is written in every
memory block, data will still appear pseudo-random [36]. 4As Scrambling-Descrambling unit is placed after the CompressionDecompression engine, scrambling the data has no effect on its compressibility.
XID as ‚Äò1‚Äô in the 16th data-bit. Thereafter, it will write the 16th
data-bit into the Replacement Area (not shown in the Figure).
As shown in Figure 9(c), for compressed lines, BLEM simply
prepends the CID and XID of ‚Äò0‚Äô in front of the data.
As shown in Figure 9(d), on a read request, if the top 15-bits
do not match the CID, BLEM simply reads the data and as
the data is deemed not compressed. However, as shown in
Figure 9(e), if CID matches and XID is set to ‚Äò1‚Äô (collision),
then BLEM fetches the 16th data-bit from the Replacement
Area (not shown in the Figure) and does not decompress the
data. As shown in Figure 9(f), for compressed lines, BLEM
simply checks if the CID matches and XID is set to ‚Äò0‚Äô and
sends these lines to be decompressed.
C. Compression Predictor (COPR): Design
While BLEM reduces the Metadata bandwidth overheads, it
delivers Metadata together with data. However, to identify if
Sub-Ranking needs to be enabled during reads, the memory
controller will need to access Metadata before accessing data.
Therefore, prior works have proposed using a Metadata-Cache.
1) Pitfalls of Metadata Caches: Unlike regular caches,
Metadata-Caches tend to reside within the memory controller.
The goal of the Metadata-Cache is to keep the most recently
accessed Metadata. Furthermore, the Metadata-Cache must be
amenable to a small lookup latency by the memory controller.
This is important as the memory controller will probe the
Metadata-Cache for every memory request and if the lookup
latency is large, it can cause slowdown. Therefore, prior
work in both industry and academia have assumed MetadataCaches that are typically small [10], [33]. In this paper, we
optimistically assume a 1MB Metadata-Cache (impractical)
within the memory controller.
Our analysis shows that a 1MB Metadata-Cache achieves
a hit-rate of 77%. Unfortunately, additional memory accesses
are involved for the 23% of the requests that miss on the
Metadata-Cache. In case of a Metadata-Cache miss, the memory
controller needs to issue a separate request to the Metadata
region in the DRAM row to fetch the new Metadata. After the
new Metadata is fetched, the Metadata-Cache should evict a
line of Metadata to install the new Metadata line. If the evicted
line is dirty, then the Metadata-Cache will require the memory
controller to issue another write request.
2) Compression Predictor to mitigate overheads: The key
drawback of the Metadata-Cache is that it is in charge of
managing Metadata. Therefore, Metadata-Cache must ensure
that Metadata is always kept updated in the main memory
and incurs bandwidth overheads. However, in the case of
331
Attache, BLEM is responsible for managing the Metadata. ¬¥
Due to this, the Attache framework does not need a Metadata- ¬¥
Cache for issuing write-back and install requests to or from the
main memory. Due to this, the Attache framework can replace ¬¥
the Metadata-Cache with a predictor called the Compression
Predictor (COPR).
On a read, the COPR simply predicts the compression status
of the cacheline. After reading the cacheline and passing it
through BLEM, the memory controller can determine if the
COPR predictor was correct. Thereafter, the memory controller
updates COPR with the correct prediction. If COPR can obtain
the atleast the same prediction accuracy as the hit-rate of
the Metadata-Cache, then COPR will mitigate the additional
memory accesses that stem from the Metadata-Cache.
3) COPR: Enabling multi-granularity prediction: The memory system can have compressible cachelines that are sparsely
distributed over an entire row or evenly spread throughout the
row. For instance, there may be rows that have a few cachelines
that are compressible to 30B. On the other hand, there may also
be rows in which all cachelines are compressible to 30B. To
predict both these instances, COPR employs a multi-granularity
predictor. The multi-granularity predictor is composed of three
components as shown in Figure 10.
The Global Indicator (GI): GI tries to provide an overall
information of the data-compressibility of the running applications. GI is composed of eight two-bit saturating counters,
each of which keeps track of the compressibility of 1
8
th the
memory space. At boot time, the counter values are initialized
to zero. For every memory request, a counter corresponding
to the requested memory space is incremented by one when
the cacheline is compressible, otherwise it is reinitialized to
zero. GI can be used as an accurate indicator for predicting
the compressibility within a memory space if there is abundant
similarity in compressibility.
‚Ä¶
Line-level Predictor (LiPR)
Page-level Predictor (PaPR)
0
0
0
1
0
1
1
0
3
0
0
0
0
0
0
0
0
0
0
0
0
Global Indicator Memory Compressibility Map
(GI)
Fill Zero 
Counter >
Threshold
Yes
No
Counter 1
Counter 2
Counter 8
Fill Max. Value 
Increase a counter 
when the accessed 
cacheline is 
compressible
Incompressible Cacheline
Compressible Cacheline
Way1 Way2 Way3 Way4 Way5 Way6 Way16
Way1 Way16
Address
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
..
..
..
..
..
..
..
0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0
1 0 1 0 1 1 0
‚Ä¶
0
0
0
0
1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0
1 0 0 0 0 0 1
‚Ä¶
1
1
0
0
..
..
‚Ä¶.
‚Ä¶
..
Fig. 10: The Compression Predictor (COPR). COPR contains
three components; the Global Indicator (GI), the Page-level
Predictor(PaPR), and the Line-Level Predictor (LiPR).
Page-Level Predictor (PaPR): By exploiting the similarity
in the compressibility of cachelines within an OS page [12],
[18], [37], PaPR provides compression predictions at the page
granularity. PaPR is a small set-associative cache structure that
is indexed by the page number. Each entry of PaPR is a two-bit
saturating counter. On memory access, if the accessed cacheline
is compressible, then the PaPR entry indexed by the page
number is incremented. In case the cacheline is incompressible,
the entry is decremented. If the entry value is greater than
or equal to 2, then the cachelines of the corresponding page
are predicted to be compressible. Otherwise, cachelines are
predicted to be incompressible. If there is no entry for the
accessed page, PaPR allocates a new entry for the page with
an initial value determined by the GI. When the corresponding
counter value of GI is greater than a threshold value, the new
entry is set to its maximum value (i.e., 3), otherwise it is reset
to zero. For this paper, we use PaPR of 192KB5.
Line-Level Predictor (LiPR): The goal of LiPR is to
provide compression predictions at 64-byte cacheline (memory
block) granularity. LiPR helps predict the compressibility of
cachelines even for pages with low similarity in compressibility
of cachelines. LiPR is a set-associative cache structure indexed
by the page number. Each LiPR entry is 64 bits long and
represents the compressibility of contiguous cachelines in a
page. After a cacheline indexes into LiPR entry, LiPR updates
the entry value if the prediction is not correct. At this time,
LiPR also proactively updates the neighboring entries to the
current entry. To this end, LiPR uses the two-bit values of
PaPR to determine if the neighboring cachelines have the same
compressibility. If the two-bit values are greater than or equal
to 2, LiPR updates the neighboring entries because the page
is deemed to have similar compressibility across cachelines.
If the two-bit values are less than 2, then the page has a mix
of compressible and incompressible cachelines. For this paper,
we use LiPR of 176KB5.
4) Prediction accuracy of COPR: Figure 11 shows the
prediction accuracy of COPR. On average, COPR provides a
prediction accuracy of 88% which is 8% higher than the hit-rate
of a 1MB Metadata-Cache. COPR achieves this by using the
insight that neighboring lines have similar compressibility and
that certain rows tend to have all lines that are compressible.
Furthermore, even for benchmarks that produce a low prediction
accuracy, BLEM does not require any Metadata accesses.
Fig. 11: The prediction accuracy of Compression Predictor
(COPR). On average, COPR provides an accuracy of 88%.
5The area overhead of COPR (192KB+176KB), which is estimated using
McPAT, is only 0.9% of the chip area in the baseline [38].
332
D. Change in Compressibility and Software Support
Attache does not use the free space made available by ¬¥
compression. Therefore, as far as the capacity goes, the real to
physical (compressed) memory sizes are the same. Thus, even
if the compressibility of data changes, the data overflowing
the memory capacity is not possible in Attache.¬¥
Attache is a completely hardware-based approach. The
main memory provisions 0.2% of the capacity for a reserved
area and does not expose this area to the OS. The memory
modules simply show a reduced capacity (99.8%) to the
OS at boot time. Therefore Attache requires no software
interactions and does not change the Virtual to Physical
memory management. The memory controller alone handles
the compression-decompression and metadata management.
E. Tying it together: Attache with Sub-Ranking ¬¥
To highlight the benefits of Attache, this paper evaluates ¬¥
Attache on a memory system that uses Sub-Ranking. It should ¬¥
be noted that the implementation of the Attache framework is ¬¥
not limited to a system that uses Sub-Ranking and the Attache¬¥
framework can be easily applied to other proposals as well.
In memory system that uses two Sub-Ranks, we can unlock
2x the bandwidth for cachelines that are compressed to 30
bytes (32 bytes with Metadata-header). We can disable SubRanking for uncompressed cachelines and maintain the same
latency as the baseline. For simplicity, our implementation
tries to compress cachelines in odd-numbered rows into the
first Sub-Rank and cachelines in even-numbered rows into the
second Sub-Rank.
If the memory controller receives a read request to an oddnumbered row, Attache checks COPR to predict if the cacheline ¬¥
is compressed or not. If COPR says that the cacheline is
compressed, then it fetches the cacheline entirely from the first
Sub-Rank with the same latency as the baseline. After reading
the line, BLEM can interpret the Metadata and determine if the
prediction was accurate. If the prediction is inaccurate, then
Attache simply fetches the rest of the cacheline by enabling ¬¥
the second Sub-Rank. If COPR predicts that the cacheline
is not compressed, then both the Sub-Ranks are enabled and
the entire 64 Byte cacheline is fetched. The same process is
followed for read requests to even-numbered row. However,
the top 32 Bytes of the uncompressed data are flipped and
stored. Therefore, in the case of uncompressed cachelines, the
memory controller will fetch the top 32 Bytes from the second
Sub-Rank. This is important as the Metadata-Header is stored
in the top 2 Bytes by design. After the read operation, Attache¬¥
updates COPR with the correct values of Metadata.
V. EXPERIMENTAL METHODOLOGY
To evaluate the performance and energy benefits of Attache,¬¥
we use the SST simulation framework [39]. For modeling the
processor core, we use the Ariel core component in SST. Ariel
is a simple core model that dynamically generates memory
instruction streams from a running application by using a
Pintool. We extended the Ariel core to model detailed out-oforder (OoO) execution.
TABLE II: Baseline System Configuration
Number of cores (OoO) 8
Processor clock speed 4 GHz
Issue width 4
Last Level Cache (Shared) 8MB, 8-Way, 64-byte lines
LLC access latency 20 cycles
Memory bus speed 1600 MHz
Memory channels 2
Ranks per channel 1
Banks Groups 4
Banks per Bank Group 4
Rows per bank 64K
Memory blocks (64 bytes) per row 128
DRAM Access Timings: TRCD-TRP-TCAS 22-22-22
DRAM Refresh Timings: TRFC/TREF I 350ns/7.8s
To model the memory system, we use CramSim [40].
CramSim is a cycle-accurate main memory component of SST.
CramSim enforces strict timing and also models JEDEC DDR4
protocol specifications. Each rank in the memory module has
8 DRAM chips and can have two sub-ranks. This is enabled
by provisioning two different chip-select signals for groups
of 4 DRAM chips. We configure CramSim to prioritize read
requests over write requests. The memory controller also has a
write buffer that drains writes to the main memory once a high
watermark is reached. To enable compression, the compression
engine compresses a memory block using both BDI and FPC,
and selects the one with the best compression ratio for the
block. As per prior work, we assume that compression and
decompression of data occurs within 1 cycle [11]. For both
Metadata-Cache and COPR, we assume that the access latency
is 8 cycles, which is the same as that of the L2 cache. CramSim
models the energy and power overheads using a DRAMSim2
style power calculator [41]. The baseline system does not
employ compression and Sub-Ranking. The parameters for the
baseline system are shown in Table II.
For our evaluations, we chose memory intensive benchmarks,
which have greater than ‚Äù1 Miss Per 1000 Instructions‚Äù
from Last Level Cache, from the SPEC2006 [22] and GAP
suites [23]. We warm up the caches and the memory for 40
Billion instructions and execute 4 Billion instructions. We
execute all benchmarks in the rate mode, in which all eight
cores execute the same benchmark. We also evaluate two 8-
threaded mixed workloads, which are created by classifying
workloads and forming four categories from highly compressible to incompressible. We randomly pick two benchmarks
from each category to form mixed workloads.
VI. RESULTS
This section discusses the performance and energy benefits
of Attache, and the sensitivity of Attach ¬¥ e to design parameters. ¬¥
A. Performance
Figure 12 shows the performance gains of using Attache¬¥
when compared to a system that only uses Metadata Caching.
On average, Attache provides 15.3% speedup across all ¬¥
benchmarks using only 368KB of COPR 6. This is almost close
to the ideal case of 17%. On the other hand, a system that uses
6As the data is scrambled, irrespective of the benchmark and its data contents,
only 0.003% of the memory accesses go into the replacement area, on average.
For instance, even if the baseline system had data that were all 0s, after data
scrambling, the data written to the memory tends to be random 64 Byte strings.
The negligible bandwidth overhead into the replacement area tends to be
imperceivable in performance and energy simulations.
333
Fig. 12: The performance of Attache when compared to the baseline system that does not employ compression. On average, ¬¥
Attache provides 15.3% speedup (ideal 17%) which is 7% higher than a system that uses Metadata Caching. The results on ¬¥
synthetic RAND and STREAM benchmarks highlight the robustness of Attache to regular and irregular data patterns. ¬¥
Fig. 13: The energy consumption of the Attache memory system when compared to the baseline system that does not employ ¬¥
compression. On average, across High-MPKI SPEC benchmarks, Attache reduces energy consumption by 22% (ideal 23%) ¬¥
which is 12% higher than a system that uses Metadata Caching. The results on synthetic RAND and STREAM benchmarks
highlight the energy benefits of Attache irrespective of the data patterns. ¬¥
a 1MB Metadata Caching only provides 8% speedup. Attache¬¥
remains robust over synthetic benchmarks and consistently
provides speedup irrespective of the access pattern. On the other
hand, Metadata-Cache is not useful for the RAND benchmark
and therefore it shows a slowdown of 17%.
The performance gains of using Attache come from its ¬¥
bandwidth improvement. Figure 14(a) shows the memory
bandwidth usage and Figure 14(b) shows the average memory
latency. On average, Attache enables 16% higher bandwidth ¬¥
which results in 14% lower average memory latency.
(a) Memory Bandwidth Usage
(b) Average Memory Latency
Fig. 14: Memory bandwidth improvement and latency reduction
due to Attache.¬¥
The performance of Metadata caching is correlated to
the Metadata-Cache hit rate. For instance, bc.kron shows a
slowdown as it has a poor hit-rate in the Metadata Cache.
Benchmarks like libquantum are not compressible, and as it
is a streaming application, there is no performance gain for it.
Additionally, while a system with Metadata caches will require
additional write-back operations, Attache does not require this ¬¥
for any benchmark. Due to this, even for the RAND benchmark,
Attache has no additional bandwidth overheads from Metadata. ¬¥
B. Energy Reduction
Figure 13 shows the energy benefits of using Attache when ¬¥
compared to a system that only uses Metadata Caching. On
average, Attache saves 22% energy across all benchmarks, ¬¥
close to the ideal case of 23%. On the other hand, a system
that uses Metadata caching only provides energy savings of
10%. The energy savings of Attache also sustain over synthetic ¬¥
benchmarks. On the other hand, Metadata-cache is not useful
for the RAND benchmark and therefore shows an increased
energy consumption of 40%.
The energy overhead of Metadata Caching is correlated to
the Metadata-Cache miss-rate. For instance, bc.kron shows
20% higher energy as it has a low hit-rate in the Metadata
Cache. For the same benchmark, Attache shows a lower energy ¬¥
consumption as it does not incur any additional Metadata
accesses. Benchmarks like libquantum are not compressible,
however as it is a streaming application, there are no energy
savings for it. Attache consistently saves energy when compared ¬¥
to the baseline for all benchmarks.
C. Additional traffic due to Metadata-cache
Figure 15 shows the additional memory traffic due to
Metadata caching. On average, even a large Metadata-cache
increases the memory traffic by 25%. This is because of
cache evictions and installs from the Metadata region in the
main memory. Furthermore, most additional requests are read
334
Fig. 15: Normalized number of memory requests in a system that uses Metadata caching. On average, even a large 1MB
Metadata-cache incurs 25% additional memory accesses due to evictions and installs.
requests (installs). This is because the compressibility of the
line does not change much during its lifetime. Therefore, the
Metadata bits associated with the compressibility of cachelines
tends to remain the same. As a result, Metadata cachelines
tend to be predominantly clean.
D. Sensitivity to Replacement Algorithm
Figure 16 shows the hit-rates of a 1MB Metadata cache with
state-of-the-art replacement policies like DRRIP and SHIP [42],
[43]. As compared to last level caches that have hit rates
between 40% to 60%, Metadata caches tend to have a much
higher hit-rate (77%) [42], [43]. Therefore, other state-of-art
policies like DRRIP and SHIP provide only a 2% increase in
the hit-rate of the Metadata cache.
Fig. 16: Hit rate of a 1MB Metadata-Cache with different
replacement policies. The baseline LRU policy provides a very
high hit-rate (77%) and other policies only increase the hit-rate
by 2%.
E. Sensitivity to Predictor Components
Different components of COPR contribute to its performance
benefits. Figure 17 shows that PaPR (page-level predictor) alone
can provide 11.5% speedup. However, after combining with
GI (global indicator) can provide 15.3% speedup. The LiPR
(line-level predictor) is only useful for mixed workloads.
Fig. 17: The performance with different components of COPR.
On average, except for mixed benchmarks, the PaPR (pagelevel predictor) and GI (global indicator) provide most of the
prediction accuracy.
VII. RELATED WORK
In this section, we describe prior work that is closely related
to our work.
A. Data Compression for Main Memory
Memzip [10] compresses data for improving the bandwidth
of the main memory. To this end, Memzip employs static
Sub-Ranking and uses a Metadata-Cache. Attache can be ¬¥
easily applied to the Memzip framework to reduce metadata
overheads. A prior work by Lee and Hong used the XRL
compression algorithm for low latency decompression [44].
While lower compression-decompression latency improves
performance, it does not reduce the overheads of Metadata
caching. Pekhimenko et. al. [12] proposed an efficient technique
for the main memory compression. However, this compression
technique is focused on improving the memory capacity.
Similarly, Abali et. al [45] proposed an alternative technique
of implementing compression in main memories, primarily for
improving memory capacity. These techniques for compression
do not focus on primarily mitigating the bandwidth overheads
of Metadata. Non-Volatile Memories can also use compression
to reduce energy and improve performance [46].
Deb et. al. [21] describes the challenges in maintaining
Metadata and recommend using ECC storage to store Metadata
with a predictor. While this technique is useful for memory
modules that have ECC in them, commodity memory modules
do not have ECC chips [34], [47], [48], [49]. Attache is ¬¥
applicable to non-ECC systems as well. Furthermore, the
predictor by Deb et. al. [21] is different from COPR. COPR
employs a multi-granularity predictor which enables COPR to
track compression status at page-level and line-level granularity.
Compression is also useful for increasing the effective
memory capacity. Prior works from the industry such as IBM
MXT and VMWare ESX use ‚ÄúBalloon Drivers‚Äù to allocate and
hold unused memory when data becomes incompressible or
when Virtual Machines exceed capacity thresholds [50], [51],
[52], [53], [54]. Similarly, Kim et. al. proposed ‚ÄúDual Memory
Compression‚Äù that helps improve memory capacity. Attache¬¥
is a bandwidth optimization technique and does not improve
memory capacity [55]. Nevertheless, Attache can be used for ¬¥
efficient metadata storage in those systems too.
B. Data Compression for Energy Savings
Kim et al. [56] introduce a bit-plane compression technique
that uses a bit-plane transformation to achieve a high compres335
sion ratio. They exploit this high compression ratio to save
bandwidth. Pekhimenko et. al. [57] addresses an inefficiency of
the memory compression techniques and its consequent increase
in bus energy consumption. This is because compression
naturally increases the number of bit toggles. These work
are efficient at improving the memory interface performance
and energy consumption. Unfortunately, these techniques do
not address the Metadata bandwidth overheads and therefore
Attache can be easily applied to these techniques. ¬¥
C. Data Compression for Caches
Prior work has also focussed on using compression to
improve the effective cache-capacity and lower energy [32],
[58]. To this end, prior work focuses on the similarity of data
content present in caches and employ compression. In the
similar spirit, Young et. al. [11] use compression in DRAM
caches to improve both capacity and bandwidth dynamically.
While this work tries to improve the bandwidth of DRAM
caches, it does not incur any Metadata overheads. This is
because the tag storage in caches acts as natural avenues for
storing Metadata and can be fetched together with data. Attache¬¥
works well for systems, such as main memories, that do not
have this design and tend to be bandwidth constrained.
Caches can also use a dictionary-based compression scheme
for higher compressibility [59]. Attache can be easily used with ¬¥
schemes for obtaining Metadata along with data. Other cachecompression techniques like YCC, SCC, Sc2, and Decoupled
Compressed Cache can easily use Attache to fetch data and ¬¥
Metadata together [60], [61], [62], [63].
D. Compression for NOCs
Thuresson et al. [64] try to improve the bandwidth of the
network on chips by compressing cachelines. In this work, the
compression is primarily employed between the LLC and the
memory controller. Providing additional lanes for Metadata at
this interface is relatively simpler. This is because the NOC
is designed by the processor vendor and therefore can be
customized. However, the DDR interface is a standard and
therefore it is challenging to transmit Metadata over the memory
interface without any additional accesses. Nevertheless, Attache¬¥
can easily be extended to other levels of the memory hierarchy.
E. Data Compression for GPUs
Sathish et al. [65] try to save memory bandwidth for GPUs.
In this work, they propose using both lossy and lossless
compression for GPU content. Attache is orthogonal to the ¬¥
compression technology and characteristics. Therefore, Attache¬¥
can easily be applied over this work for GPUs to get additional
bandwidth savings.
F. Other Relevant Work
Sardashti and Wood [37] observe that cachelines in the
same page may not have similar compressibility. If the
compressibility of a page varies, the Metadata cache will still
perform poorer than COPR. Due to varying compressibility,
the traffic from Metadata cache will now dominate write-backs.
Hallnor et. al. [66] proposed using compressed data throughout the memory hierarchy. Such an approach reduces the
overheads of compression and decompression at every level in
the hierarchy. Attache is orthogonal to this approach and can ¬¥
be used with this approach at every level to improve bandwidth
efficiency even further.
Recent work by Han et. al. [67] and Kadetotad et. al. [68]
used compression with deep neural networks to significantly
improve performance and reduce energy. This work primarily
focused on improving capacity. However, even to improve
bandwidth, frameworks like Attache will help such deep ¬¥
networks as it can save Metadata bandwidth for every level of
the deep neural network.
New technologies like stacked memories and non-volatile
memories tend to have ECC, security primitives, and endurance
optimizations for increasing their lifetime [69], [70], [71], [72],
[73]. Attache can be applied independently to any of these ¬¥
schemes and is broadly applicable. Going forward, Attache¬¥
can also be co-optimized with caching policies [42], [43] and
memory-request scheduling policies [74].
VIII. SUMMARY
Main memory systems are bandwidth constrained and data
compression is a practical technique to improve their bandwidth.
While one can use Sub-Ranking to unlock higher bandwidth,
one cannot simply employ data compression to utilize this
bandwidth. This is because data compression relies on Metadata
for identifying the compressibility of the cacheline. Therefore
each access to the line will also require an additional access to
its metadata. While one can resolve this issue by completely
placing the Metadata in an on-chip buffer near the processor,
such a design will not be scalable. For instance, in a 16GB
memory system, if each cacheline requires 1 bit of Metadata,
the size of the buffer will be 32MB. Therefore, prior work
has proposed caching Metadata in an on-chip Metadata-cache.
This paper observes that even at high Metadata-cache hit-rates
Metadata-Cache requests can cause bandwidth overheads.
To this end, this paper proposes Attache, a technique that re- ¬¥
duces the bandwidth overheads of Metadata accesses to 0.003%.
Attache is a scalable technique and can be implemented without ¬¥
any software changes. For a 16GB main memory, the hardware
overhead of Attache is only 368KB. Furthermore, Attach ¬¥ e¬¥
requires 0.2% Replacement Area in the main memory for
storing data from cachelines that incur CID collisions. Overall,
Attache provides 15.3% higher performance (ideal is 17%) and ¬¥
22% lower energy (ideal is 23%) by reducing the Metadata
bandwidth for compression.