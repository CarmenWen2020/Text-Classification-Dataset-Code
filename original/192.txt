Artificial intelligence (AI) technology has greatly expanded human capabilities through perception, understanding, action, and learning. The future of AI depends on cooperation between humans and AI. In addition to a fully automated or manually controlled machine, a machine can work in tandem with a human with different levels of assistance and automation. Machines and humans cooperate in different ways. Three strategies for cooperation are described in this article, as well as the nesting relationships among different control methods and cooperation strategies. Based on human thinking and behavior, a hierarchical human–machine cooperation (HMC) framework is improved and extended to design safe, efficient, and attractive systems. We review the common methods of perception, decision-making, and execution in the HMC framework. Future applications and trends of HMC are also discussed.
SECTION I.Introduction
With the continuous development of artificial intelligence (AI) and automation, robots are gradually entering the daily life and work of human beings and have been increasingly used for various tasks, including search and rescue [1], surgery [2], space exploration [3], and oceanic discovery [4]. Even though the autonomy of robots has progressed rapidly in recent years, human intervention in the form of high-level reasoning and planning is inevitable in unstructured environments. It is well known that human agents (HAs) and robotic agents (RAs) have complementary capabilities. HAs have strong reasoning ability, adaptability, and robustness, independent of data. RAs can store and analyze a large amount of data and have strong numerical abilities, repeatability, and high precision [5]. Therefore, human–machine cooperation (HMC) is beneficial for completing tasks. The advantages of HAs and RAs can be fully combined to improve some metrics of human–machine systems.

A discussion about HMC should begin with the definition of the word, cooperation. In general, “cooperation” means “working together” or “the action or process of working together toward common goals” (Oxford Dictionaries). While some literature distinguishes between the terms “cooperation” and “collaboration” [6]–[7][8] the two are often used interchangeably. Many theories have been written and formulated under the name 
“cooperation” with a very wide scope, which is no longer compatible with a narrower definition. Therefore, the term collaboration is not explicitly used here. Cooperation is defined by Smith [9] as “working together to accomplish shared goals.” Human–machine cooperativeness is defined by Flemisch et al. [10] as a trait concerning the degree to which a machine is generally agreeable in its relations, behavior and interaction with humans, or better. It can be useful to think of cooperation as a cluster concept rather than a clear-cut definition [11].

HMC is discussed in a large number of papers [12]–[13][14][15][16][17][18]. Flemisch et al. [19], [10] discussed a dynamic balance between humans and automation and provided an overview of commonalities and differences in shared control and human–machine cooperation. Gervasi et al. [6] examined the concept of collaboration and provide a conceptual framework for analyzing and evaluating human–robot collaborations. Losey et al. [15] summarized the physical human-robot interaction in intent detection, arbitration, and communication aspects. Music et al. [13] surveyed advances in human-robot team interaction and identified factors affecting control sharing.

This work aims to provide researchers interested in HMC in the robotics domain with a hierarchical HMC framework combining people’s thinking and behaviors and a review of common methods of perception, decision-making, and execution. The article is organized as follows. Section II discusses cooperation strategies within the HMC. A hierarchical framework is proposed in Section III. Next, HMC is reviewed from the three functional levels of perception (Section IV), decision-making (Section V), and execution (Section VI). In Section VII, decomposition of applications along the three function layers, perception, decision-making, and execution (PDE), is illustrated. A discussion is presented in Section VIII. Finally, Section IX concludes this article.

SECTION II.Cooperation Strategies
HMC has different taxonomies based on different standards. Millot and Mandiau defined the vertical and horizontal structures of cooperation. Rieger and Greenstein proposed explicit and implicit modes of cooperation. Therefore modes of cooperation are more dealing with authority management for task allocation. Schmidt et al. [20] proposed three forms of cooperation. According to the number of interactive agents, cooperation can be divided into single human-single robot interactions, single human-multiple robot interactions, multiple human-single robot interactions, and multiple human-multiple robot interactions. Based on whether the control interface and guidance system are mechanically coupled, the form of cooperation is classified as coupled or uncoupled [18] or haptic and state [21]. On the basis of the consistency of the tasks, cooperation paradigms can be divided into direct, unified, overlapping, and orthogonal paradigms [13]. In [15], Losey et al. proposed classifications according to the types of human-robot role arbitration hypothesizing the minimization of human error and control power, divided into four categories: coactivity, master–slave, teacher–student, and collaboration. Six modes are introduced in light of the modes of control in human–machine interaction [5]: traded, indirect, coordinated, collaborative, virtual constraints, and continuous.

Human–robot collaborative control was defined as a mode of human–machine interaction [22], which stressed the fact that humans and machines share the same tasks and control a situation cooperatively [19], [23]. There are three forms of strategies based on the control authority between humans and robots, as shown in Fig. 1: human-dominant and robot-auxiliary control, robot-dominant and human-auxiliary control, and human–robot consensus control.
In cases where there are many uncertain factors, such as unstructured, nonlinear, and time-varying factors, manual control methods are usually adopted. In this control strategy, the system depends mainly on HAs to perceive the external information, to make decisions in response, and to output control commands. However, due to uncertain psychological and physiological factors, HAs may tend to have incomplete perception of environmental information, and sometimes obvious deviations and mistakes occur. To compensate for the shortcomings of HAs, it is necessary for RAs to supplement the perception of HAs and assist HAs with control. The human-dominant and robot-auxiliary control architecture is shown in Fig. 1(a).

Robots can independently solve some problems that are structured, linearized, quantitatively calculated, or difficult for humans to solve through reliance on their mature autonomous intelligence. In this control strategy, the RA can automatically perceive external information, make decisions based on relevant decision-making knowledge and experience, and output control commands through the cooperative controller, while the HA intervenes only in some special circumstances or affects the generation of autonomous actions by high-level commands. The robot-dominant and human-auxiliary control architecture is shown in Fig. 1(b).

Human–robot consensus control is applied when agents have similar skills or different and complementary skills. The partial or global results of the other agents are compared to determine which agent performs the function. The cooperative controller blends the authority or allocates the subtasks between the human and robot based on trust and self-confidence, among other factors. The human–robot consensus control architecture is shown in Fig. 1(c).

The main feature of HMC relates to the two-way dialogue initiated between the human operator and the robot. The results of cooperation can be adjustable autonomy or adaptive automation, which reflects in a shift of autonomy (the adaptivity and adaptability of the system), and allocation of tasks or functions (fully or partially delegating) between different agents [24]–[25][26][27]. We discuss this aspect in detail in Section V. Obviously, these concepts are not exclusive, but are nested. In Fig. 2, we show the nested relationship in HMC adapted and extended from [10].

Fig. 2. - Nested relationship in HMC, which is adapted and extended from [10].
Fig. 2.
Nested relationship in HMC, which is adapted and extended from [10].

Show All

SECTION III.Cooperation Framework
A. Hierarchical Framework
Numerous studies from automation, cognition and different disciplines are exchanged to establish a desired and efficient HMC framework. Cooperation is presented from different perspectives such as levels of task [28] and the type of function shared between humans and machines [29], [30]. The vertical dimension (operational, tactical, and strategic levels) and horizontal extension (information gathering, information analysis, decision-making, and action implementation) of shared control have been extended in recent work [31], [32]. Abbink et al. [17] proposed a hierarchical framework with communication of symbols, signs, and signals at four levels, strategic, tactical, operational, and executional (STOE). Different from task-centered theory, the man–machine integration system theory was proposed by Yongxiang et al. in 1995 [33], and . Canjun [34] offered a practical discussion in his doctoral thesis. Based on the hierarchical intelligent control framework [35], [36] combined with people’s thinking and behaviors, the whole system can be regarded as three main layers: perception, decision-making, and execution. External information is perceived through the sensory organs of humans, and signals are transmitted by the afferent nerves to the brain for processing to obtain intentions. This progress is similar to the function of the perception layer in the proposed framework. According to the intention, the brain decides how to react and generates the corresponding motion plan and motion instruction, which corresponds to the decision layer of the framework. The efferent nerves transmit the signal of the behavioral response to the muscles and motor organs to ensure the accurate execution of motion instructions, which matches the executive layer. The PDE framework has been improved, as shown in Fig. 3, where the number next to a line (or curve) with an arrow indicates the relationship between the elements.

Fig. 3. - PDE hierarchical framework for human–machine cooperation. The framework has three layers: perception, decision-making, and execution.
Fig. 3.
PDE hierarchical framework for human–machine cooperation. The framework has three layers: perception, decision-making, and execution.

Show All

The comprehensive information of the human operator, robot sensors, and environment is fused by the perception layer, including human characteristics, robot dynamic parameters, and environmental information. The perceptual layer outputs the possible intentions of the HA and RA. In the perception layer, human perception includes (1) human perception of the environment; (5) human sensing information sent to the human–robot interface; (23) human perception of self-motion; (25) human perception of the object state and its feedback; and (29) human self-sensing. Robot perception includes (2) robot perception of the environment; (6) robot sensing information sent to the human–robot interface; (24) feedback from actuations; (26) robot perception of the object state and its feedback; and (30) robot self-sensing.

According to the intentions obtained from the perception layer, the decision interface in the decision layer organizes the activities in the system, including human tasks and robot works, or blends the control between the human and the robot. The motion plan and high-level commands are outputted from the decision interface in the decision layer. Serial numbers 13 and 14 indicate the transfer of the result of the decision from the human–robot interface to the decision interface and then to interactive media, respectively. In the decision layer, human decision includes the following: (3) human sensing information is sent to the HA; (7) the results of robot decisions and robot sensing information are received from the human–robot interface; (9) human decision-making is sent to the human–robot interface; and (11) the HA receives decision results from the decision interface. The robot decision includes the following: (4) robot sensing information is sent to the RA; (8) the results of human decisions and human sensing information are received from the human–robot interface; (10) the robot decision-making is sent to the human–robot interface; and (12) the RA receives decision results from the decision interface.

The executive layer converts high-level instructions into low-level instructions and is responsible for the accurate execution of movement. Depending on the case, it is crucial to note that the robot or operator may require full self-control. In the execution layer, human execution includes the following: (15) the motion commands are sent from the HA; (17) the HA acts on interactive media; (19) the results of the robot execution are sent to the HA; (21) the human reflects motion to the outside motivation; and (27) the human copes with the object at hand. Robot execution includes the following: (16) the control signals are sent from the RA; (18) the RA acts on interactive media; (20) the results of the human execution are sent to the RA; (22) the signals from the sensors are directly sent to the actuations; and (28) the robot works on the object. In (31), the human and robot jointly work on the object; in (32), the object and the environment interact.

B. Design Principle
The design purpose of the HMC system is to foster collaboration between HAs and RAs to solve problems efficiently, where the main task of an RA is to assist the operator in accomplishing the mission rather than performing it himself. It should be noted here that for problems that can be solved independently by an RA, the possibility of autonomous completion is not ruled out. The following principles for the design of a HMC system are proposed.

To encourage cooperation and minimize the conflict between human and robot intentions, human-centered automation [37] advocates modeling the robot behavior based on human behavior [17]. In addition, the RA must take into account the personal habits and preferences of the collaborators.

Human safety needs to be prioritized regardless of whether there is a conflict between the intentions of the HA and the RA [38], [39].

With the assistance of an RA, the operator’s input sometimes does not need to be precise. It is not necessary for the RA to provide the operator with the final answer, but rather with feasible suggestions or solutions along with reasonable explanations for the results.

HAs and RAs must be able to read each other’s intentions and predict each other’s goals, which can affect safety, comfort, and cooperative performance.

The integration of hierarchical functions should not be fixed but dynamically adjusted depending on the spatial and temporal information, the difficulty of the task, the level of autonomy in the context, and the HA’s confidence in automation.

SECTION IV.Perception Layer
HMC is based on information sharing, and convenient communication is the link between HAs and RAs. In human–machine systems, perception has always been instrumental in human–robot interactions, determining whether the HA and RA can understand each other. There is an enormous amount of literature on human–robot interface design, often for particular applications. It is outside the scope of this article to perform an exhaustive review of the different technologies. In the following, two aspects of the perception of robots regarding humans and the perception of humans regarding robots are explored.

A. Perception of Robots With Regard to Humans
Compared with humans, the most significant advantage of robot perception is its high accuracy, and relevant physical quantities can be quantitatively detected by various sensors. Thus far, the perception range for robots is far beyond that of human beings. The common ways that robots perceive humans are shown in Fig. 4. According to the perception function, the perception mode of robots to humans can be divided into the following three parts.

Fig. 4. - Common ways that robots perceive humans.
Fig. 4.
Common ways that robots perceive humans.

Show All

1) Human Motion Perception
Currently, the use of exoskeletons [40], [41], data gloves [42], and force feedback devices [4] constitutes the most mature and reliable motion perception methods. It is easy to measure the motion parameters of the human body by a motor encoder or angle sensor and to then directly control the joint of the robot. However, mechanical equipment is redundant and heavy, which affects the comfort of operators. Optical pose tracking methods are the least physically restrained of all kinds of solutions, and they are designed to capture poses by tracking reflective markers, such as OptiTrack. Nevertheless, the calibration of the devices is complicated, and the data processing algorithm is complex. Despite depth cameras such as Kinect [43] being viable for motion capture, they easily experience infrared interference, which makes them vulnerable to failure in outdoor environments. In recent years, increasing interest in deep learning has driven the development of monocular image-based human pose estimation [44]–[45][46], which can be used when the required precision is not high. Multiple inertial sensors can also be employed to measure human motions in real time [47], using inverse kinematics to calculate human joint angles [48]. However, system errors may accumulate over time, which eventually leads to incorrect measurement results.

It is also challenging to directly and effectively translate a human operator’s command into robot actions after obtaining human motion data. There are three main methods for human arm motion mapping: end-to-end mapping [49], [50], joint-to-joint angle mapping [51], and functional pose mapping [52]–[53][54][55], which enable the operator to control the remote manipulator through the action of their own arm.

2) Human Intention Perception
Human intention perception is a natural method of interaction that enables a robot to understand human intention, such that the robot can properly assist or implement the action according to the intention of the HA. The extracted biological signals can be used to study the mechanisms of human physiological systems and to reflect human intentions.

The bandwidth of an electromyography (EMG) signal is approximately 1 kHz, and the useful signal components are concentrated between 0 and 500 Hz. The difference in muscle contraction potential measured on the body surface is between tens and hundreds of microvolts. Although the signal is very weak and accompanied by considerable noise, the EMG signal has the function of directly reflecting human body activity [56]. Therefore, it is considered an advanced human–computer interface for predicting user intention [57]. Electroencephalogram (EEG) signals express two parts of human response information to external stimulation [58]. One part is the sensory information generated by external stimulation, and the other part is the neural signal of an action response to external excitation. The latter can accurately reflect the real intention of a human and is an appropriate human–computer interface for controlling an intelligent robot [59]–[60][61]. The amplitude of an electrooculogram (EOG) is usually 15 to 200 μV, which is almost linear with eye movement [62]. The user’s goal, future behavior, and mental state can be conveyed by an EOG [63]. Many works have combined eye tracking technology with machine learning to identify human intentions for indirect input [64] or task assistance [65]. In addition, intention prediction via motion can be natural and seamless and can be used as a supplement to other interfaces [66].

3) Human Affective Perception
Emotion is a key way for humans to express and understand their intentions. Ekman [67] proposed six main human emotions (anger, disgust, fear, happiness, sadness, and surprise) in human beings. Facial expressions (FEs) are vital signaling systems that convey clues regarding the emotional state of a person [68]. Speech signals also contain information that reflects the emotional state of the speaker (such as special modal particles and changes in tone). Emotion recognition extracts emotional features from collected speech signals or image information and maps out the relationship between these features and human emotions. The basic process includes four steps: image/speech acquisition, preprocessing, feature extraction, and emotion recognition. The perception of human emotions could be used to improve safety and perceived safety [69]. If the user appears anxious due to the robot motion, the control system takes corrective action (by slowing down, stopping the robot, or modifying the robot trajectory) sooner than a controller based only on physical factors [70].

B. Perception of Humans With Regard to Robots
Human perception is the result of the work of a physiological analyzer, a complex neural structure composed of receptors (such as visual and auditory receptors), afferent nerves, and the cerebral cortex. Analyzers and actuators (motor organs) form a perception system that connects people with the outside world. The common ways that humans perceive robots are shown in Fig. 5. The perception of humans regarding robots is also feedback that humans feel from the outside.

Fig. 5. - Common ways that humans perceive robots.
Fig. 5.
Common ways that humans perceive robots.

Show All

1) Visual Perception
In the process of understanding the outside world, 70% of the information is provided by vision [71]. Visual feedback, which is generally regarded as the most important form of perception in human–robot interaction, can provide users with information on the state of a robot and its current motion, and users can respond based on this perceived information. For many simple tasks, the results can be directly expressed in the form of numbers or graphs, such as lines and curves [72]. In addition, a 3-D display is used in the human–robot interaction to complete tasks [73]. To increase the immersive experience of users, virtual reality (VR) technology and augmented reality (AR) technology provide methods of simulating reality. Initially, these technologies were mainly designed for entertainment, but now, with the development of technology and the emergence of a variety of applications and lower costs, they have been extended to various industries [74] to achieve safer human–computer interactions.

2) Haptic Perception
Haptic perception can be subdivided into kinesthetic feedback (forces and torque applied to the human body and sensed at the muscles and joints) and cutaneous or tactile feedback (forces and sensations sensed through the mechanoreceptors in our skin) [15]. Kinesthetic cues can be used to gradually limit the degrees of freedom (DOFs) available to human operators based on the difficulty of the task or the user’s experience. This type of perception exists in multiple applications and tasks, for example, guiding the operator toward a reference position [75] to avoid certain areas of the environment [76] and training for manual tasks [77]. The richness of haptic stimulation can also be utilized to provide multiple pieces of information through the tactile sense channel [78]. A combination of vibration rotation and kinesthetic feedback is used to guide users away from the singularity points and joint constraints of the manipulator [79]. In addition, tactile feedback has been shown to enhance the cooperation between users and devices, which is essential for motor learning. A flexible vibration tactile belt has been developed [80] that can be worn on the human body to achieve rich tactile communication. The layout of the feedback module must also be considered [81], such that the users can obtain the best feedback experience.

3) Acoustic Perception
Auditory feedback may reallocate perceptual workload and reduce distraction [82] and is often used in human–robot interaction. Acoustic perception is used to indicate warnings or confirmation and sometimes to provide higher level information [83]. The additional information provided by hearing is a valuable supplement to relying only on vision [71]. Audiovisual interfaces also improve assembly tasks [84] and teleoperation [71]. Similar forms of auditory feedback can be applied to robot-assisted movement training [85]. Operators can also use auditory information to locate the sources of sounds in unmanned aerial vehicle control [86].

SECTION V.Decision-Making Layer
The most crucial part of HMC is human–machine integrated decision-making. Reasonable decisions affect the performance of a task, which is the key to later human–machine integrated execution. Decision-making in different modes and arbitration are discussed in this chapter.

A. Decision-Making in Different Control Modes
Goertz [87] proposed manipulators for handling radioactive material which can turn cranks according to coarse user inputs in 1963. This is one of the earliest examples of shared control. In later work, Sheridan [88] comprehensively introduced the theory and technology of human–robot cooperation in remote robot systems and developed various forms of cooperative control on this basis. Since then, research on this topic has proposed multifarious approaches for assistance, ranging from a robot dealing with a subtask to guiding the operator control and from shifting between predefined discrete levels of autonomy [89] to blending the policy between HA(s) and RA(s) [90], [91]. A framework is proposed to determine a robot’s autonomy level, which is beneficial for guiding the cooperation between humans and machines [6], [24]. Different control modes of HMC require different methods of decision-making. Fig. 6 lists the different decision-making methods. Table I provides a brief summary of decision-making methods for different control modes.

TABLE I Decision-Making Methods for Different Control Modes


Fig. 6.
Decision-making methods under different control modes. (a) Guided control. (b) Traded control. (c) Supervisory control. (d) Indirect shared control. (e) Direct shared control. (f) Allocation control.

Show All

1) Guided Control
Generally, the purpose of guided control is to improve the operator’s perception of the environment. The RA derives sensory cues according to programmed criteria and displays these cues to the HA, with the aim of influencing the input of the HA by stimulation. Through tactile and kinesthetic feedback, users can identify the sources of force cues, keep robotic arms away from singularities and joint limits [79], or guide movement toward potential targets [99], thus assisting users in completing tasks in unknown environments. Guidance control can reveal the presence and location of virtual or real obstacles rather than the dynamics, behavior, or performance [5]. While increased environmental perception may lead to a reduction in task cost, this method cannot directly satisfy the goal of reducing task cost.

2) Supervisory Control
The concept of supervisory control was coined by Ferrell and Sheridan [92], who emphasize that the human operator supervises a lower level intelligence embodied in the teleoperator itself by intermittent monitoring and reprogramming as necessary for either routine or emergencies. One of the best-known applications of supervisory control is in teleoperation. Supervisory control does not require full robot autonomy but “merely” the ability to independently achieve some goals, while the human supervisor sets high-level intermediary goals [17]. In the supervisory control mode, the authority of robots is greater than that of humans, and the specific operation is reflected in the autonomous decision-making of remote robots dominating.

3) Traded Control
The HA or RA has control at some instant, and one or the other may transfer full authority to either agent in a trigger event. One common paradigm launches a fully autonomous takeover when a trigger such as a user command is activated [100], when a robot arm enters a critical region around an obstacle, or when a goal predictor exceeds some confidence threshold [93], [94]. Traded control is highly applicable for some simple scenarios and tasks, such as teleoperation with delay [42], while trigger conditions are difficult to universally apply to all tasks.

4) Shared Control
Shared control is sometimes also termed “shared autonomy” or “shared authority.” A hierarchy of subtasks was introduced by Rakita et al. [101]: shared control unburdens the user from the demands of low-level commands and enables them to concentrate on higher level task objectives by letting a robot handle some aspects of the control process. Another concept of shared autonomy [102] is one in which the inputs from a human operator are integrated with the computation of an autonomous system to produce the final behavior of a robot. Shared autonomy emphasizes that human(s) and machine(s) share control over a system together [10], and one definition is that both the human operator and robotic system act simultaneously to achieve shared goals [103]. Additionally, the definition emphasizes that some metrics (e.g., performance and comfort) can be improved. In shared control, human(s) and robot(s) interact congruently in a perception-action cycle to perform a dynamic task in which either the human or the robot can act individually under ideal circumstances [17].

Shared control can be divided into direct shared control and indirect shared control [104]. Indirect shared control is also called coordinated control in some areas of literature [5]. In this scheme, an operator can control a robot only indirectly through a controller, and the automation assimilates high-level input and converts it into lower level commands to generate robot actions [105]. The authors in [106] and [107] developed a shared control system that enables users to specify 2-D end effector path, and the RA plans in joint space based on this path. The vast majority of related work relies on known models of a given system’s dynamics and the user’s control influence to define a shared control paradigm [108]. In data-driven shared control [109], the necessary data from observations of the human and robot interactions are collected, and then the interaction information is integrated into an autonomous policy generation method [103] and used to regulate how the control authority is shared between the two partners [110].

Direct shared control treats the HA and the RA as two independent sources and combines them with some arbitration functions that determine the relative contributions of each. Linear functions are usually used to combine the control commands of the HA and RA [66]. The formula for the linear function is as follows:
cS=αcR+(1−α)cH(1)
View SourceRight-click on figure for MathML and additional features.where α is the control weight of the RA, with a value range of 0 to 1. cH and cR are the control commands of the HA and RA, respectively. The value of the weight can be fixed or variable.

The optimal weight value is determined through experiments or simulations in fixed weight cases [111], [112]. Weight-fixed shared control can achieve better operation effects in some specified environments or operation objects, but the optimal weights need to be determined in different scenes, leading to a lack of universality. Dynamic weight can better adapt to the environment and improve task efficiency [113]. Dragan and Srinivasa [66] proposed a predict-then-act method that predicts the user’s intention according to the user’s input and conducts human–robot weight arbitration depending on the robot’s confidence in target reasoning. The blending method can integrate the control of HA and RA intuitively and simply. However, it can lead to catastrophic failure, when two independent decisions are combined without an evaluation of the actions that would be executed. Even if each individual decision is successful [95]. Thus, probability-based approaches, such as Gaussian product [21] and operator biased linear trajectory blending [95], are also used to blend commands of HA and RA.

5) Allocation Control
In this control mode, the whole task is divided into two independent subtasks. The HA is responsible for a specific input subset (direction [114], speed, or several degrees of freedom of a manipulator [115]), while the RA provides the rest of the input or assistance [116]. For instance, in the next-best viewpoint for an external camera in-hand robot method, a human operator has access to a system consisting of two manipulators [97], one of which is equipped with a gripper and the other with a camera to avoid occlusion of the manipulator itself. An autonomous algorithm is in charge of regulating a subset of the gripper DOFs to facilitate the approach toward an object of interest. Additionally, the human operator is able to steer the gripper along the remaining null-space directions by acting on a force feedback device [117]. Additionally, the RA modifies or prohibits the subspace of manual commands (including the speed, direction, or motion trajectory) to meet arbitrary constraints of the state. Diverse virtual constraint methods such as potential fields [118], virtual fixtures [119], and shared dynamic curves [101] have been proposed for assistance. For example, Rahal et al. [120] assisted the operator in cutting tasks by limiting lateral motion, in situ rotation, and sharp turns. The setting of constraints is related to the target task. Daniel et al. [98] constructed a two handed action library by analyzing how people perform two handed operations, inferring the current action in the action vocabulary and taking the corresponding action assistance, with the aim of effectively improving the execution of specific two handed actions.

B. Arbitration
Arbitration refers to the division of control between HAs and RAs during human–robot interaction [121]–[122][123][124]. One key challenge in formulating cooperative strategies is finding a balance between the control authority of humans and machines. An enormous amount of research has focused on fusing human and machine “decision-making” in human–robot interaction communities, machine learning, and control theory [95].

It is common in the human–robot interaction community to design rules that combine the control of the HA and RA. One direct way is to design α in (1) based on the performance metrics of human–robot interaction. Most work predefined a different kind of function according to prior knowledge to calculate α. The use of piecewise functions is universal. Performance metrics such as hinge-loss [125], probability [66], the entropy of the probability distribution [126], and prediction uncertainty [127], [128] are used to adjust the control authority, which is used to determine the grasping target in the process of teleoperation.

The family of the exponential function is the other approach to gain a continuous and smooth α. Muelling et al. [113] compute α using a sigmoid function considering the minimal control contribution of a user to enable smooth, seamless distribution of the control authority. Note that an excessively large value of the derivative of the exponential function may lead to too steep of authority switching [126].

Additionally, Trautman [95] proposes extension of linear blending that properly conditions the autonomy according to user statistics, where shared control is formulated as a random process and the joint distribution over the random operator, autonomy, and crowd functions is described.

Local weighted regression [129], [130], Gaussian mixture regression [21], task-parameterized semihidden Markov model [131], and other machine learning methods are used to encode and learn human actions and trajectory distributions offline to generate the actions of the autonomous system. When certain trigger conditions [130] are met or in other special cases, the RA assumes complete autonomy. Sylvain et al. [21] represented the control input of both the human and automation by a GMM and combined them using a Gaussian product. The product takes the confidence of the actors into account, as expressed in the covariance matrices.

The thriving of deep learning and reinforcement learning has driven the development of shared autonomy. A partially observable Markov decision process (POMDP) [132], [133] is used to model shared autonomy with hindsight optimization to approximate [103], [134], estimating the best robot action at each time step. Reddy et al. [135] used human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observations and user inputs to agent action values.

Game theory, in which human–robot interaction is deemed a two-agent game, provides useful tools for analyzing complex interactive behaviors involving multiple agents [136]. In game theory, interactive behaviors can be described by different combinations of individual objective/cost functions and different optimization criteria [137]. The strategy type of HA and RA can be derived using Nash equilibrium, Stackelberg equilibrium and Pareto equilibrium [104]. In Nash equilibrium, simultaneously, each agent considers its own cost function such that each individual strategy is a best response to the others. Nash equilibrium can be achieved by the optimal control developed in [138] and adaptive optimal control based on policy iteration and the critic neural network developed in [137] to enable the HA and RA to simultaneously exert control on the robot. Subsequently, Li Y. et al. [139] optimized the cost function to model the task objectives of robots and humans and proposed a differential game theory framework for physical human–robot interactions. A Stackelberg equilibrium emerges in situations where one agent (i.e., the HA or RA) is the leader and the other agent serves as a follower. For example, Nikolaidis et al. [140] presented a game-theoretic model of human partial adaptation to a robot, where the human responds to the robot’s actions by maximizing a reward function. Both HAs and RAs try to reach a binding agreement of interest, and their strategies are derived from global optimality in the Pareto equilibrium [141]. A bargaining game based distributed model predictive control scheme is proposed in [142], where several players jointly decide which strategy is best concerning their mutual benefit.

SECTION VI.Execution Layer
In the execution layer, the main interaction between the human and robot is the transfer of mechanical energy, such as force and position. Many research results show that dynamic human behavior in human–robot interaction is similar to that of a mechanical damper [143]. In the process of human motion, if there is an external force field affecting the movement of human limbs, humans can change the viscosity of soft tissue in the human skin by adjusting the contraction/elongation of the muscle through neural signals to counteract the influence of an external force, showing variable stiffness characteristics [144]. The key to human–robot integration in task execution lies in the mutual adaptability of two independent subsystems. If the goal of the system is consistent with that of the human, there is a low-impedance interaction beneficial to the human in terms of control effort and performance [145]. Through impedance control of the neuromuscular system, a human can respond much faster to forces on their control interface than visual cues.

The control interface exchanges force and position with human limbs through physical interaction in the execution layer, which corresponds to haptic shared control [79], [97], [120]. Cooperative properties can be tuned by adapting force feedback gain and stiffness feedback gain to yield desirable responses [146], depending on the dynamics of the system [147]–[148][149].The dynamics of the human–machine system in the Cartesian space are
Mx(xr)x¨r+Cx(xr)x˙r+Gx(xr)=FH+FR(2)
View SourceRight-click on figure for MathML and additional features.

where xr, x˙r, and x¨r denote the robot end effector position, velocity, and acceleration, respectively, Mx(xr) is the inertia matrix, Cx(xr,x˙r) denotes the Coriolis and centrifugal forces, Gx(xr) is the gravitational force, and FH and FR are the force inputs attributed to the human and robot controllers, respectively.

It is an interesting way that dynamically changing the impedance of the control interface to smoothly shift control authority in the execution layer, and more guidance would be provided only when needed [144], [150]. The impedance stiffness coefficient is adaptively adjusted according to predefined adjustable functions [151], [152] or reinforcement learning algorithms [153].

It is also important to mention that when cooperative control is active, both decision and execution levels occur at the same time. The decision level calculates the level of authority of the RA that would support the human with greater or lower force according to the conditions and is applied at the execution level.

SECTION VII.Application
There are various cooperative control approaches catering to the specific needs of each domain, such as patient assistance [154], [155], autonomous driving [156], [157] unmanned aerial vehicle systems [158], [159], assembly [160], and remote operation [116], [149]. How the applications fit into the proposed PDE framework is shown in Table II. Taking underwater teleoperation as an example, an underwater manipulator is mounted on a remotely operated vehicle (ROV) to carry out underwater assembly, spraying, welding, and underwater salvage, as shown in Fig. 7. Depth sensors, underwater cameras, and tactile sensors are used to obtain information about the surrounding environment of the ROV. The data of the underwater environment are transmitted back to shore through an optical fiber, and the actual underwater operating environment can be reproduced by VR technology. Information perceived by both agents is shared in the perception layer. In the decision-making layer, the decision interface performs task/function allocation and autonomy adjustment to generate high-level control instructions. In the execution layer, the operator controls the underwater manipulator with force feedback equipment according to the feedback vision and force information and completes the target task with the assistance of autonomous intelligence. Shared control teleoperation was initially realized in [126], as shown in Fig. 8.

TABLE II Example Applications and the Corresponding Decomposition of Hierarchical Task Execution Along the Three PDE Function Layers
Table II- Example Applications and the Corresponding Decomposition of Hierarchical Task Execution Along the Three PDE Function Layers
Fig. 7. - Application of human–machine cooperative control to underwater robots.
Fig. 7.
Application of human–machine cooperative control to underwater robots.

Show All


Fig. 8.
Human–machine shared control remote operation [126].

Show All

SECTION VIII.Discussion
The occurrence of conflicts should be taken into account in HMC. The definition and types of conflict in HMC are proposed in [163]. A well-designed HMC system should minimize conflicts between the human and the robot [17]. One method based on the human-centered automation principle requires the human operator to bear final authority [37]. Another method is adapting the system control strategy to that of humans or shutting off support in case of conflicts. In addition, one of the potential approaches to minimizing conflict is to establish a mental model consistent and compatible with partners’ capabilities, authority, control, and responsibilities [19]. Beyond human-centered automation, the final authority depends on a metric to evaluate and analyze the conflicts between the HA(s) and RA(s) [104]. Research on automation and human–robot interaction provides insights for identifying variables influenced by robot autonomy (such as safety, workload, trust reliability, and transparency) [6], [24]. The triple binds between ability, authority and responsibility have to be taken into account in the metric [19]. In the proposed framework, the following four aspects are considered to achieve successful combinations of humans and machines.

A. Multimodal Perception Fusion
Because each sensor provides information only on a specific area of the environment, multimodal perception fusion technology is needed to integrate the information from various sensors into a unified perception system and to organically utilize the information obtained by multiple sensors [164]. The high universality of multimodal signals can reduce the influence of sensor noise [165]. As a result of the information transmission mechanism in the HMC system, a multimodal perception fusion model must be developed that is accurate and reliable. Based on a certain optimization criterion, the model integrates the sensory dynamics of human operators along with the complementary and redundant information of various perceptions in both space and time to infer the operator’s intent. In this way, it improves the comfort and safety of human–machine systems and reduces the conflict between the human operator and automation. In light of changes in data and dynamic mapping, the multimodal perception fusion model should be active (feedforward control strategy prior to situational changes) rather than reactive (feedback control strategy after situational changes) [166].

B. Integrating Humans and Robots Decision-Making Dynamics With Feedback
In HMC, the main challenge is to integrate human cognitive abilities with a robot’s ability to autonomously execute tasks, while maximizing the task performance efficiency and intuitiveness of the interaction [13]. Determining an appropriate balance between human intervention and autonomous assistance remains an open research question. Some studies [106] have found that users prefer autonomous help since autonomy makes tasks easier. However, when an automation system fails, the problem is often not detected, and an operator not in the loop cannot intervene in time [100]. Due to the inconsistencies in the logical mechanisms of the HA and RA and their temporal and spatial relationships, cooperative control is difficult. Cooperation involves the interaction and transformation of data, information, wisdom, and knowledge [166]. In fact, the cooperative decision-making model should integrate environmental information, spatial and temporal information, the difficulties of a task, the levels of automation within the context, the current self-confidence of personnel, robotic trust in automation, and commands from the HMC system to balance decision-making between HAs and RAs.

C. Integrating Humans and Robots for Efficient Task Execution
Previous work has proven that a good understanding of neuromuscular-skeletal dynamics constrains the dynamics of perception-action coupling [104]. Generally, human intention can be expressed by the applied force/torque to obtain the reference motion of the system, that is, to transfer it to system dynamics to determine the reference state (speed and position) of the system through the human limb dynamics model. However, it has been proven that the muscle impedance of human limbs is a kind of mechanical sensor unit that can be adjusted during a task and displays the characteristics of variable stiffness [144]. The robot controller must adjust the damping of the system to meet the dynamic requirements of the human to accomplish a task. In contrast, the human neural system adapts to the robot by adjusting the dynamic characteristics of the operator. A dynamic model between impedance regulation and human intention must be established to increase the stability of tasks and to avoid subtle conflict.

D. Feedback From the Virtual and Tangible Environment
In HMC, the information exchange between the human and the robot is bidirectional. Users expect to obtain more feedback when interacting with a robot, and this feedback makes them feel that the robot is a part of their body. Tangibles can be added to support VR and AR content to provide more modalities of sensory experience [167]. The properties and physical constraints of physical objects in tangible interfaces can restrict how they can be manipulated, which helps to bridge the real and virtual worlds [168], [169]. With the combination of visual, haptic, and other feedback approaches and the use of technologies such as VR and AR [170], the perception of users is simulated, presenting a virtual and tangible environment where users can operate at will and feel immersed. This results in enhancement of the interactivity, multisensitivity, and autonomy of the feedback function and improves the user’s sense of immersion and presence.

SECTION IX.Conclusion
This article reviews the related concepts and approaches in HMC research. Human intelligence and autonomous intelligence are combined in HMC systems, exhibiting the following characteristics.

Active: An HMC system perceives external information and shares control strategies to produce near-perceptual operation or control under the interaction of the human–machine-environment rather than reactively responding to external changes.

Clear division: The clear division between HAs and RAs means that neither side needs to complete all tasks; they need to perform only what they are good at.

Adaptive: To ensure optimal performance, the control strategy of the HMC system changes as it interacts with the outside environment.

This leads to the cooperative system being widely used in medical treatments, autonomous vehicles, and remote operation, but it is not appropriate for all situations. An RA can complete simple and repetitive tasks on its own, and its effect can be better than that of HMC. The common cooperation strategies include human-dominant and robot-auxiliary, robot-dominant and human-auxiliary, and human–robot consensus. Strategic evaluation must consider both objective factors, such as time consumption and accuracy, and subjective factors, such as operator willingness and feelings.

A coevolution of humans and robots is required in HMC [166], which is dependent on the level of intelligence of the RA. Both the HA and RA must learn from each other, the external environment, experience, and lessons, and continuously modify their own states to optimize the configuration of the system based on the states.

Efforts in cognitive neuroscience and artificial intelligence are improving the performance of HMC systems. HMC may soon play a significant role across a wide range of fields.