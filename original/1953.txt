Abstract‚ÄîIn this paper, we conduct a systematic analysis to
show that existing CPU optimizations targeting scientific/server
workloads are not always well suited for mobile apps. In particular, we observe that the well-known and very important concept
of identifying and accelerating individual critical instructions
in workloads such as SPEC, are not as effective for mobile
apps. Several differences in mobile app characteristics including
(i) dependencies between critical instructions interspersed with
non-critical instructions in the dependence chain, (ii) temporal
proximity of the critical instructions in the dynamic stream, and
(iii) the bottleneck shifting to the front from the rear of the
datapath pipeline, are key contributors to the ineffectiveness of
traditional criticality based optimizations. Instead, we propose
the concept of Critical Instruction Chains (CritICs) ‚Äì which are
short, critical and self contained sequences of instructions, for
aggregate level optimization. With motivating results, we show
that an offline profiler/analysis framework can easily identify
these CritICs, and we propose a very simple software mechanism
in the compiler that exploits ARM‚Äôs 16-bit ISA format to
nearly double the fetch bandwidth of these instructions. We have
implemented this entire framework - both profiler and compiler
passes, and evaluated its effectiveness for 10 popular apps from
the Play Store. Experimental evaluations show that our approach
is much more effective than two previously studied criticality
optimizations, yielding a speedup of 12.65%, and energy savings
of 15% in the CPU (translating to a system wide energy savings
of 4.6%), requiring very little additional hardware support.
Index Terms‚ÄîCriticality, CPU, Mobile, Energy
I. INTRODUCTION
The proliferation of mobile devices over the past decade
has been fueled by not just hardware advancements, but also
by the numerous and diverse applications (apps) that these
devices can support. The number of such devices far exceeds
the desktop and server markets, with nearly 2.6 billion mobile
devices serving more than 35% of the world population today
[1]‚Äì[3]. To a large extent, the hardware and software evolution
of these devices has drawn from lessons learned over the
years from their desktop/server counterparts and adapted them
for different resource constraints ‚Äì energy/power, form-factor,
etc. On the other hand, many of the mobile apps have very
different characteristics, and are used in very different ways
compared to desktop/server workloads (e.g., high amount of
user-interaction, handling sensors, etc.). And so, it is not clear
whether the same high-end device optimizations are effective
for the mobile platforms.
Picking two well-studied optimizations (instruction prioritization and memory prefetching) that try to exploit a very
important property, namely ‚Äúcriticality‚Äù, of the instructions
in server/desktop domains, this paper points out that these
mechanisms are not well suited to mobile apps. Instead, this
work proposes to track criticality at the granularity of selfcontained instruction chains, and assigns a criticality metric to
each chain. With the bottleneck shifting from the rear to the
front of the CPU datapath pipeline in these mobile apps, this
paper introduces a novel way of prioritizing and aggregating
these Critical Instruction Chains (CritIC) in software to solve
this problem, requiring little to no additional hardware support.
While one could throw extra resources and hardware
mechanisms into a superscalar processor‚Äôs datapath to boost
performance in embedded domain, it is even more important
to better utilize the existing resources amongst the competing
instructions, especially in resource-constrained environments.
One such well studied mechanism for prioritization amongst
competing instructions is based on ‚Äúcriticality‚Äù. When a critical
instruction is brought into the processor datapath, different
prioritizations/optimizations can be employed. Over the years,
numerous criticality based optimizations have been proposed
and studied for high-end workloads - prioritizing CPU resources
[4]‚Äì[7], caches [8]‚Äì[10], memory request queues [11]‚Äì[13],
predicting the result of the instruction [14]‚Äì[17], issuing
prefetch requests [18], etc. However, the impact of these
optimizations has not been studied to date for mobile workloads,
and that is one important void this paper intends to fill.
Unlike many desktop/server workloads which are very
throughput demanding, mobile apps are highly user-interactive.
User actions like screen swipes, and sensor inputs like positional (GPS) and/or movement (accelerometer) frequently
control the execution, with the app reacting to such actions
and coming back for additional inputs. The consequent code,
though rich in the conventionally deemed ‚Äúcritical instructions‚Äù,
are not conducive enough for independent instruction-level
optimizations, i.e., they are often dependent on each other with
possibly one or more non-critical instructions coming into the
dependence chain between them. For instance, we show that one
such recent optimization [18], which prioritizes critical loads,
does very well for SPEC workloads (as in prior works), but
provides a measly 0.7% speedup for a wide spectrum of mobile
apps. Any criticality optimization targeting mobile apps should,
thus, consider these sequences/groups of critical instructions at
a time, rather than optimize for each individually. We identify
two additional differences: (i) the bottleneck for these critical
instructions shifts from the back-end (Execute/Commit stages
of the superscalar pipeline) to the front-end (Fetch stage) of
the pipeline when we move from SPEC to mobile apps; and
(ii) sequences/groups of critical instructions are much smaller,
and occur close together in the dynamic execution stream, for
mobile apps compared to SPEC workloads, making them more
amenable for aggregate-level software based optimizations.
We are not aware of any prior work which has pointed out
the insufficiencies of criticality-based individual instruction
optimizations for mobile apps, and their workload differences

TU"OOVBM*&&&"$.*OUFSOBUJPOBM4ZNQPTJVNPO.JDSPBSDIJUFDUVSF
¬•*&&&
%0*.*$30
requiring a revisit of this topic in the mobile context.
Motivated by this insight for several off-the-shelf Android
apps, we make the following contributions in this work:
‚Ä¢ We use the concept of a self-contained Instruction Chain (IC)
within a Data Flow Graph (DFG), which can be executed
independent of other instructions, when encountered. We
introduce a criticality metric for an IC, that is calculated as
the average fan-out per instruction in that chain. ICs with a
criticality exceeding a threshold are marked as CritICs, and
the entire CritIC sequence of instructions is given priority.
Unlike SPEC, CritICs in mobile apps are relatively short
(order of 5 instructions) and are not that widely spaced out in
the dynamic instruction stream either, making them suitable
for software (e.g., compiler, profiler) identification.
‚Ä¢ We note that CritIC instructions in mobile apps are bottlenecked in the Fetch stage of the pipeline, as opposed to
SPEC which are back-ended (execute/commit stages), since
the number of high latency instruction is a much smaller
fraction in the former. Both the producer side which feeds
instructions into the pipeline, and the consumer side which
drains the instructions from the fetched queue are equally
important contributors to this bottlenecked stage.
‚Ä¢ Our identification of Critical and Self-contained ICs, provides a convenient abstraction for tackling the fetch bottleneck. We could theoretically hoist and aggregate all these
instructions together as a macro instruction. But the number
of possible CritIC sequences makes this option expensive.
Adhering to the philosophy of imposing minimal hardware
enhancements, we instead propose a novel approach to
doubling the fetch bandwidth for these CritIC instructions by
leveraging ARM‚Äôs 16-bit instruction format. We convert all
these instructions into the 16 bit representation in a compiler
pass, as long as there is no loss in their functionality, together
with a preceding command to instruct the ARM hardware to
switch to 16-bit format for these instructions. This proposed
hoisting and 16-bit conversion from the software side can be
employed in all current ARM CPU based devices [19]‚Äì[21].
‚Ä¢ We have implemented a profiler on top of the AOSP
emulator [22] and GEM5 simulator [23] for identifying
CritIC sequences. We have also added a compiler pass in
the Android Runtime Compiler (ART) to hoist, aggregate and
emit the 16-bit representations for the CritIC instructions.
We have evaluated our proposal in GEM5 for a Google
Tablet configuration using a diverse and popular suite of 10
stock Android apps from the Play Store.
‚Ä¢ Results show that our approach provides as much as
15% speedup (12.65% speedup on the average) for these
apps, while two previously well regarded single instruction
criticality based load prefetching and single instruction
prioritization provide only 0.7% and 5% speedup on the
average. The 16-bit representations nearly doubles the fetch
bandwidth for all CritIC sequences, buying back 3.6% of the
time on the producer-side of the fetch. Packing them backto-back reduces the data flow time across these instructions,
buying back 2.5% of the time on the consumer-side of the
fetch. All this is achieved with little to no extra hardware
requirements in the existing processor datapath.
‚Ä¢ While one could increase i-cache capacity, improve branch
prediction accuracies, and/or employ instruction prefetchers
to address the fetch bottleneck, all these may require extra
hardware than what current mobile platforms support because
of resource-constraints. Even if future platforms incorporate
such additional hardware to address the fetch problem, we
show that our simple software technique can do as well
as any of these hardware approaches (even a platform that
has 4√ó the i-cache capacity and a perfect branch predictor).
Further, it can synergistically provide an additional 11%
speedup over what these hardware techniques can provide.
‚Ä¢ It may appear that one could opportunistically use the 16-bit
ARM format for all instructions to optimize the fetch stage
for all instructions. We show that while this can provide
6% improvement, our approach of identifying, hoisting and
grouping CritIC sequences, and selectively using the format
for such instruction sequences, does much better (12.65%).
In fact, using our solution, and then opportunistically using
the 16-bit format for other instructions, complement each
other to provide 16% improvement.
II. CRITIQUING CRITICALITY
Within the confines of the given resources of a superscalar
processor, one of the most important issues is deploying and
assigning these resources to the incoming stream of instructions.
This is essentially determined by the priority order (scheduling)
for fetching and executing these instructions. When there are
adequate resources, we would give all instructions the resources
that they need. However, when resources are constrained,
priority has to be given to ‚Äúcritical instructions‚Äù [4], [9], [12],
[24]‚Äì[26]. In this work, we use a simple definition of criticality,
similar to those in some prior works [4], [26] - an instruction
is critical if its execution time becomes visible (i.e., does not
get hidden) in the overall app execution.
A. Conventional criticality identification
As per the above definition, an instruction can be marked
critical, only after its execution - by which time it is too
late to assign resources for it. Hence, prior works propose
different ways of estimating criticality of an instruction before
it is even fetched. Two common heuristics for marking an
instruction as critical are by using thresholds for (i) execution
latency of an instruction (a long latency instruction implies






      
	


 


 !"
#$!%!& '"
	


 !
	


& '
()
(a)



 


*
)



+
(! +
( &!
      
,!	! ,!	!- ./0/!


! 

  ./ . 0/!
(1
(
 2,! 2 , !
 



 

(

131 131


1
 

(
/	! / !-


/	
 
 	!
3 (
(b)
Fig. 1: (a) Despite having frequent Critical Instructions, mobile apps do not
benefit as much. (b) Reason: Critical instructions in SPEC do not depend
much on other critical instructions. But, Android apps have two successive
high-fanout instructions in a dependence chain, with 0(direct-dependence) to
5 low fanout instructions between them.

instructions depending on it have to be delayed, thus making
it more critical) [8], [9], [26] and (ii) number of dependent
instructions (referred to as fanout in this paper), particularly in
the ROB at the time the instruction is being executed (as many
instructions require its output before they can begin). A table
is maintained for those instructions exceeding the threshold
based on prior execution (similar to branch predictors), and
upon an instruction fetch, this table is looked up with the PC
to find whether that instruction is critical or not.
B. Do these criticality schemes work for mobile apps?
Different optimizations can be employed upon fetching a critical instruction - prioritizing CPU resources [26]‚Äì[28], caches
[8], [9], [18], memory requests [11], predicting instruction
results [14], [29]‚Äì[31], issuing prefetches [18], etc. Until now,
these optimizations have been primarily proposed and evaluated
for server/desktop workloads and not for mobile apps/platforms.
(a) Example DFG
 	

 
	

	
	
		
	


	

   	 
 
  
      	 
 
  
  
     	 	 	 	 	 	 	 	 	 	   	 	 	 	 	
     
    
 
   	 		 	
 	 	 	 

 	 	 	 
 
  
    	 	
 	
 	 
 
	
   	 
 
  
      	 
 
  
 
     	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	
    	 		 
 
   
    	
 	 	 	 

 	  	 
    	 
 
  	 	
 	
 	 
 
	
(b) Timing in superscalar processor with issue width 2
Fig. 2: Illustrating why high-fanout prioritization may not help.
Without loss in generality, we have
taken two representative, wellstudied and well-proven criticality optimizations in prioritizing
two important resources - one for
memory which issues prefetches
for critical loads [18] and another
for ALU resources in instruction
scheduling [4], [32], [33]. These
proposals identify high-fanout loads to mark them as critical
to issue prefetch [18] and prioritize the critical instructions
for ALU resource allocations [32], [33]. These techniques
have shown significant benefits for server workloads. The highfanout based optimization has also been shown to outperform
the latency based ways of identifying and exploiting criticality
[18], [34]. We next evaluate the usefulness of both these
criticality optimizations (depicted as bars) in mobile apps and
compare the mean speedup obtained from employing both
these techniques for SPEC.int, SPEC.float and Android apps
in Fig. 1a (experimental details are in Sec. IV-B).
As can be seen, the performance gains from prefetching
high-fanout loads and prioritizing them at ALU resource
scheduling are both quite significant for SPEC.int (15% from
prefetching, 9% from prioritizing) and SPEC.float (34% from
prefetching, 25% from prioritizing), re-affirming prior results
[18], [33]. Interestingly, the gains from these two optimizations
are a relatively measly 0.7% from prefetching and 5% from
prioritizing in the mobile apps. Based on this, one may think
that perhaps mobile apps do not have a significant number
of high fanout loads/ALU instructions to benefit from these
optimizations. On the contrary, we observe that (in right y-axis
of Fig. 1a) the mobile apps have a much higher percentage of
critical instructions than their SPEC counterparts. This should
have, in turn, resulted in more opportunities for optimizing
the execution. To understand why this is not the case, we next
identify scenarios where these optimizations may not work and
point out that such scenarios are common in mobile apps.
C. Why do they not work?
Fig. 2a shows an example DFG (Directed Flow Graph)
where, executing the first instruction I0, triggers ten following
instructions (I1 to I10) to become ready for execution. Any high
fanout optimization will obviously execute I0 first. After this
step, let us say I10 again has a fanout of 10 (i.e., instructions I11
to I20 become ready), which would cause I10 to be prioritized
in the execution over say I1. If, subsequently, I11 and I12
each have 2 fanouts and each of I13 to I20 has a fanout of
just 1, I11 and I12 will get scheduled before I13 to I20. But
since each of I13 to I20 instructions has a fanout of 1, a highfanout instruction prioritization will not differentiate between
them. Note that, I20 in turn has a dependent high fanout
instruction, I22, that cannot be scheduled till I20 is completed.
So, as seen in Fig. 2b, by not doing this optimization of
prioritizing I20 over its siblings, single a instruction criticality
optimization scheme as described previously, stalls 2 cycles (12,
13) in the execution. This scenario occurs commonly in mobile
executions (explained below), where an instruction despite
having a low fanout, requires high-priority since there is a
subsequently dependent high-fanout instruction. Consequently,
it is insufficient to optimize individual high-fanout instructions
independently. Instead, the whole sequence of dependent
instructions from I0, I10, I20 to I22 should be scheduled as
early as possible, even though I20 is a low-fanout instruction.
We find evidence of this scenario occurring much more in
mobile apps compared to their SPEC counterparts as shown in
Fig. 1b, which breaks down the dependence chains containing
high-fanout instructions in terms of the number of low-fanout
instructions between two successive high fanout instructions
in a dependence chain. We find that the dependence chains
can have between 1(22%) to 5(7%) low-fanout instructions
in the dependence chain between two high fanout critical
instructions, for cumulatively 52% of the time in Android apps.
On the other hand, the SPEC.float and SPEC.int apps have no
dependent high-fanout instructions for around 60% and 35% of
the time. Compare that to Android apps, where this hardly ever
happens, i.e., there is at least 1 low fanout instruction between
2 successive high fanout, and thereby critical ones. It is no
surprise that SPEC apps benefited from optimizing each critical
instruction individually as opposed to Android ones, where
such dependent chains reduce the effectiveness of individual
optimizations. These mobile app results also suggest that:
(a)prioritizing/optimizing each critical instruction individually
as it comes (i.e., for the ‚Äúpresent‚Äù) would not be as effective
in rightfully apportioning the given resources; and (b)we
need to consider these temporally proximate and dependent
critical instructions (chains/sequences) together for possible
optimizations, i.e., look into the future as well. Traditional
criticality based optimizations [9], [11], [12], [18] have targeted
one critical instruction at a time, rather than groups or chains.

D. What do these instructions need?
Before optimizing for these closely occurring and dependent
critical instructions in Android apps, it is important to understand where they spend their time amongst the different superscalar pipeline stages. Towards this, we present a breakdown
of their execution profiles amongst these stages in Fig. 3(a).



*
4


+
(! +
( &!
+5
!0

6 67


1! 
!7


+5
!7

 +5
!
8!# 9
!3



 


*
+
(! +
( &!
-
9
#9-
+5
!
:6!+5'
:6!1
6(6! 

) 
*
+
;2- 3
!
 !/- 3
!
 <

*
&! 
: 6
 
6(6!8=1



 
8=1
& 
8=1
8=1
8
Fig. 3: (a) Fetch to Commit breakdown of high-fanout instructions in SPEC vs Android (b) In Android, Fetch is more
bottlenecked due to both (i) stalling for instructions to be fetched (F.StallForI), and (ii) stalling for resources and
dependencies (F.StallForR+D) to move the instructions down the pipeline. (c) Mobile apps have fewer high latency
instructions compared to SPEC.
In the same graph, we
provide a similar profile for critical instructions identified by the
same ‚Äùhigh fanout‚Äù metric in the SPEC.float
and SPEC.int apps. From
these results, we observe
the following: (i) unlike
SPEC apps, where the
Execute stage, and consequently the back-pressure
in ROB queue residencies, are quite dominant,
the Android apps have a
much lower Execution stage latency (and consequently the
ROB residency). The mix of critical instructions in Android
apps do not take as much execution time (fewer long latency
instructions compared to their SPEC counterparts as shown in
Fig. 3(c)). (ii) However, the fetch stage, and the decode stage
to some extent, are much more dominant in Android apps,
compared to the SPEC ones (due to the drop in contribution
from the Execute stage). As much as 40% of the time goes
in the Fetch stage, while similar critical instructions in SPEC
spend less than 5% of their time in this stage.
This shift in the profile from the rear to the front Fetch stage
(consumes 40%) of the pipeline in Android, warrants us to take
a closer look into this stage. Fig. 3(b) breaks down the Fetch
execution time in these apps into two parts - F.StallForI,
which is responsible for supplying the instruction stream into
this stage, and the F.StallForR+D which pulls out the
instruction from this stage for subsequent decoding. The former
depends on the I-cache latency, miss costs, and branch misprediction costs, while the latter is largely determined by the
back-pressure exerted by the subsequent pipeline stages (i.e.,
wait for decode to commit time for the prior instructions).
The relative contributions of the F.StallForI and
F.StallForR+D (2:3) to the overall Fetch side overheads are
quite comparable across the SPEC and Android apps. However,
the actual values are quite different. While F.StallForI
contributes to 3% of the overall execution in SPEC, Android
apps execute from a much larger code base with a diverse
set of libraries (>7k APIs [35]‚Äì[37]) with more frequent
function calls, which causes i-cache stalls for 15% of the
execution and branch prediction stalls for another 2% from the
F.StallForI. At F.StallForR+D, SPEC apps execute
many high-latency instructions that creates a back-pressure on
the fetch stage by 3.6% (out of the 5.4% in SPEC.float) and
13% (out of 21% in SPEC.int). In Android apps, as shown
in Fig. 3(c), majority of the high-fanout instructions are lowlatency instructions, not imposing much back-pressure from the
execute stage itself (6% out of 40%). Instead, the dependence
resolutions between various instructions (as discussed in Fig.
1b) causes the most stall (11%) for the F.StallForR+D
in these apps. Thus, any optimization for these critical
instructions should try to reduce both F.StallForI and
F.StallForR+D latencies, i.e., a simple i-cache/branchprediction optimization, or a back-end optimization alone may
not suffice as we will show later on.
Key Insights on Android Apps: (a) With high fanout,
and thereby ‚Äùcritical‚Äù instructions occurring in close temporal
proximity with one or more non-critical/low-fanout instructions
in the dependence chain between them, we should consider
optimizing groups/sequences of these instructions concurrently,
rather than one at a time; (b) Fetch stage is much more
important for these instructions, and optimizations for this stage
are likely to yield more rewards than throwing more hardware
for the conventionally bottlenecked execute-to-commit stages;
(c) We need to accelerate not just the rate of bringing in the
instructions to the Fetch stage, but also accelerate the rate of
pushing out instructions into the rest of the pipeline.
III. CRITICS: CRITICAL INSTRUCTION CHAINS
Having identified the requirements, we now explore (i)
how to identify these ‚Äùcritical‚Äù instructions occurring in a
dependence chain/sequence in close temporal proximity, and
(ii) how to optimize for these sequences to provide the minimal
F.StallForI and F.StallForR+D latencies in the fetch
stage with minimal hardware extensions.
A. Identifying CritICs
1) CritIC Sequences: As was shown in the example in
Fig. 2a, identifying and optimizing for individual high fanout, and thereby critical, instructions can only provide limited
options. Instead, we need to look into the future, and find other
possible future critical instructions which are in its forward
dependence chain/graph. Consequently, the entire chain should
be prioritized/optimized even if intermediate instructions in
the forward dependence chain (such as I20 in the forward
dependence chain of I10, I20, I22) may not traditionally
have been marked as critical because of their low fan-outs.
Towards identifying such ‚ÄúCritical Instruction Chains (CritIC)‚Äù,
we first introduce the following metric and definitions.

Instruction Chain (IC): An instruction chain is any
acyclic path of a Data Flow Graph (DFG) that is independently
schedulable at that instant in the execution. In our previous
example DFG of Fig. 2a:
‚Ä¢ The paths I0, I10, I20, I22 and I0, I10, I11 are
independent of the other paths in the DFG. So, they are
independently schedulable, and both qualify as ICs.
‚Ä¢ The path I0, I1, I21 does not qualify as an IC as it
depends on another path, I0, I10, I11, I21 and is thus
not independently schedulable.
‚Ä¢ Still, the sub-path I0, I1 qualifies as an IC as it does not
depend on any other paths of this DFG, i.e., any sub-path
of an IC is also an IC.
An IC is thus a self-contained sequence of instructions, and
is executable as an atomic entity (e.g., a macro instruction [38]‚Äì
[44] consisting of several micro-instructions in the sequence)
without any dependencies into its individual instructions. We
will exploit this property later when optimizing critical ICs.
Crit: At any instant, a DFG has several individual
ICs. The goal is then to find the right order for executing
these ICs - to prioritize based on their relative criticalities.
(a) Example DFG
 	

 
	
		
	

 
	


3       * ) 4 <      
	


              
(>?   * ) 4 <     4 <   
(>?         * )   

3       * ) 4 <     
	


             
(>?   4         * )  
(>?  * ) 4 <     <   
(b) Timing in superscalar processor with issue width 2
Fig. 4: Example: Need to optimize CritICs
For example, in Fig.
4b, the execution at
the top (high-fanout
optimization) shows
that prioritizing an
IC with low-fanout
instructions, I1, I6,
I7, I8, I9, I10, I11,
I12 is inefficient -
as observed in cycles 10, 11, the execution becomes serialized and there is
no ILP for 2 cycles.
However, identifying the relative
criticalities of ICs
is non-trivial, since
each instruction in an IC can have a different fan-out/criticality.
Simply adding up the fan-out of all its constituent instructions
may not paint an accurate measure of an IC‚Äôs criticality since
there could be high variance amongst its instructions - a
cumulatively high-fanout IC may have a very high fanout
instruction at the beginning, with all subsequent instructions
ending up with very low fanout, or vice-versa. While one
could consider higher order representations for capturing such
variances in future work, in this paper, we use a simple average
fanout per instruction of an IC to capture the criticality of
an IC. ICs whose average fanout per instruction exceed a
certain threshold (e.g. 8) are marked as CritIC sequences in
this work. Fig. 4 gives an example DFG, where a conventional
instruction-level fanout based prioritization would give an
execution as in the top part of (b) taking 14 cycles on a
2-way issue superscalar processor, while our CritIC approach
would identify two ICs I1, I6, I7, I8, I9, I10, I11, I12 and
I0, I5, I18 and prioritize the latter over the former because of
its higher average fanout per instruction (4 vs. 2). This results
in a schedule as in the lower part of (b), taking only 13 cycles.
2) How to find them?: There are two broad strategies for
identifying CritICs: (a) using hardware predictor tables as
used in many prior works [4], [8], [12], [24] and/or (b) using
software profile-driven compilation. As explained, we would
like to minimize hardware requirements as much as possible,
especially since mobile devices can become highly resource
constrained. So we opt for the latter approach, which raises
additional issues that we address as discussed below:
‚Ä¢ Ability to do this without User Intervention: Unlike desktop
environments where users may write their own apps, many
of the mobile apps are published a priori (on the Play
store, iTunes, etc.). It is not unreasonable for many of these
popular apps to have undergone a profile-driven compiler
optimization phase, which many of them already do (for
quality, revisions, performance, bugs, etc. [45], [46]) before
they get published. Our solution can be integrated into such
phases for appropriate code generation.
‚Ä¢ Dealing with diverse inputs (user-interactivity): Even if apps
are available a priori, their execution can depend a lot on
the input data - this is especially true for mobile apps which
have high user interactivity. Conveniently, common cases of
user inputs are readily provided for many of these apps in
standard formats [47], [48], that we avail for our approach.
‚Ä¢ Ability to track long ICs and their spread: Software based
approaches are often criticized because of their restricted
scope in analyzing large segments of code concurrently. This
would pose a problem if the ICs were long and spread out
considerably in the dynamic instruction stream. For instance,
if we were to apply our approach for SPEC apps, Fig. 5a
shows that we would need to track ICs of lengths up to
1.3K, which are spread over up to 6.3K instructions in
the dynamic stream. On the other hand, in our favor, ICs
for the mobile apps (as shown in the Fig. 5a), are at the
maximum 20 instructions long, and are at most spread over
540 instructions to make them conducive to our approach.
‚Ä¢ Tractability of tracking all ICs: Even when tracking 5 to 10
instruction long ICs, an app execution can generate a huge
volume of profile data (100s of GB of CritIC sequences),
with numerous sequences at any given instant. So, instead of
tracking and optimizing for every possible CritIC sequence
in an app, we track the top few CritIC sequences based
on their coverage in the dynamic execution stream. This
substantially reduces the profile size to a few kBs.
We have built this profile-driven compilation framework to

4
*

<*
+
( +
(! &!

 2
9 2 95 2
9 95
(a) (b)
Fig. 5: (a)IC length and their corresponding spread in dynamic instruction
execution in SPEC vs Android apps; (b)CDF of coverage by unique CritICs.

@ @ @ @ @ @ @ @ @ @ @ @
 . *. . *. . *. . *. . *. . *. .
#
4.AB .A
!2
'!!#!      
5"
@ @ @ @ @ @ @ @ @ @
 .  . *. . *. . *. . *. . *. .
<*.A B .A 
!2

!


1

 2
"
     
 
@! @
. .
 #6!

@! @
*. .
*#0

.

 .
 &!C
*. . *. . *. .
C !

"	
	
#!
*.
 
* .


. *. . *. .
 "

	
  
! 2
"	
	 
 #! 
	$	$


6
D2E

.
 &!C
. *. .
C %"		&

	$
$
'


4 . A  B   . A  
		&

	$

() 
*+ $


! 
$






 

Fig. 6: Each CritIC instruction is transformed from the original 32-bit format
to the 16-bit Thumb format of ARM ISA [51], [52].
automatically identify and optimize the CritIC sequences in a
large number of Android apps. The app execution is profiled using AOSP emulation [22], [49] and GEM5 hardware simulator
[23] to get the instruction stream from which we identify the
CritIC sequences. The on-device Android Runtime Compiler
(ART compiler) then generates optimized ARM binary using
various compiler optimization passes [50]. After these passes,
we have implemented an additional instrumentation pass in
the compiler which visits every CritIC in the optimized DFG
generated, to generate the optimization that is discussed next.
B. Optimizing CritIC Sequences
Our solution to optimizing these sequences is motivated
by the two important observations: (i) CritIC sequence
instructions spend nearly 40% of their execution in their
fetch stage, with both F.StallForI and F.StallForR+D
contributions becoming equally important; and (ii) Each
CritIC sequence‚Äôs instructions are ‚Äùself-contained‚Äù and can
execute in sequence without being influenced by any other
sequence. Ideally, each CritIC sequence could, thus, be
made a macro-instruction whose functionality is equivalent
to executing each of its constituent instructions one after
another. If our compiler could replace this entire sequence
by the corresponding macro-instruction, we would avoid
individual fetches for each of the constituents, and incur only
1 fetch operation - this would reduce the F.StallForI
contribution. Further, by hoisting up this entire dependent chain
of critical instructions into a single macro-instruction, we have
reduced/eliminated any unnecessary gap between them, thus
shortening the data flow from one to the other - this would
reduce the F.StallForR+D contribution waiting for the later
stages of the pipeline to flush out.
Creating Macro-Instructions: One obvious choice for
implementing such macro-instructions is by extending the ISA
with either (i) multiple mnemonics - one for each CritIC
sequence, or (ii) having a new mnemonic with a passed
argument that indexes a structure to find the CritIC sequence.
In either case, the new macro-instruction has to know the exact
sequence of micro-instructions that it needs to execute. This
may be a reasonable option if the CritIC sequences are
somewhat limited, i.e., there are a few common sequences
which are widely prevalent across several apps as was the
case in solutions such as [42], [43], [53], [54]. However,
Fig. 5b shows that the number of unique CritIC sequences
(opcode+operands of all constituent instructions) is large - even
Fig. 7: Proposed Software Framework for our Methodology
each app can have 106 unique CritIC sequences - making
it impossible to extend the ISA for this purpose, or building
dedicated hardware for each unique CritIC sequence.
Exploiting ARM ISA: Instead, we need a mechanism
for dynamically creating/mimic-ing such macro instructions
based on the CritICs at hand, and we propose a novel
way of achieving this in the ARM ISA. Fig. 6(a) shows the
contemporary ARM ISA format [52] that uses 32 bits to
represent an instruction - containing 12 to 20 bits for opcodes,
12 bits for representing 2 source and 1 destination operand
registers. It also supports a concise format using 16-bits called
‚ÄùThumb extension‚Äù (Fig. 6(b)). In this mode, the opcode is
represented in 6 bits while the operands are represented in
3-4 bits each. The 16 bit format [52] is used in embedded
controllers for optimizing binary size. The existing ARM
decoders can decode any of these formats based on simple
flags and pending queue structures [51].
We propose to represent each instruction of a CritIC
sequence, that we would like to optimize, in the 16-bit format
(Fig. 6(d)). Even though past studies [51], [52], [55] report that
the 16-bit format produces ‚âà 1.6√ó more instructions to execute
(and causes slowdown) because (i) it cannot have predicated
executions, and (ii) it cuts the number of architected registers
as operands from 16 to 11, we point out that the 16-bit format
is very amenable for CritIC instructions 1. We illustrate this
by plotting the CDF of coverage of the dynamic instruction
stream by the instructions in all identified CritIC sequences
of the original code (in 32 bit format) in Fig. 5b. In the
same figure, we also plot the CDF of coverage by the CritIC
instructions that can be represented in the 16-bit format without
any change, i.e., they have neither predications nor use more
than the allowed 11 registers. As we can see, there are very few
CritIC instructions that cannot be represented (4.5% of the
unique CritIC sequences), referred to as CritIC.Ideal
in Sec. IV-E, which demonstrates the promise of our proposal.
Additionally, the ARM Decoder has to be informed of the
instruction format, to switch back-and-forth between 32 and 16
bit representations. There are two possible ways to inform the
decoder of the format switch: (i) in the current ARM hardware,
this is done using explicit Branch instructions [52]. But, as we
will show in Sec. IV-A, this incurs additional overheads especially for relatively short (< 10) CritIC instruction sequences;
and (ii) our proposed alternative to extend an already existing
instruction mnemonic to support CritIC thumb format switch
in the decoder hardware (evaluated in Sec. IV-B).
C. Summarizing our Methodology
Fig. 7 summarizes the software framework for performing
and implementing the CritIC optimizations:
1If any instruction of a CritIC sequence cannot be represented in the
16-bit format as is, then the entire sequence is left as is (in the original format)
and is not optimized, i.e., all or nothing property (quantified in Fig. 5b).

‚Ä¢ Trace Collection: We run the Android apps in QEMU [22]
emulator with Android OS, where all the hardware components (CPU, GPU, touch, GPS, network, accelerometer,
gyro, display, speakers, etc.) are modeled. We instrument
its disassembler (with 1.6k lines of code or LOC) to output
the trace of instructions executed and data accessed by the
isolated process (app in consideration), for offline profiling.
‚Ä¢ Identifying CritICs: This trace is used for detailed microarchitectural simulation in Gem5 [23], with modifications
to identify critical instructions based on their fanouts across
ROB entries (3.3k LOC). To get CritICs from the critical
instructions, we implement additional tracking logic to dump
all the independently schedulable ICs (whose lengths vary
as discussed in Fig. 5) which results in 100s of GBs of ICs.
These are processed offline with a distributed hash-table
using Spark PairRDD [56] to sort and get the top CritICs (ICs
with average fanout threshold > 8) with the most coverage
(3.8k LOC). We fix 8 as the most beneficial average fanout
threshold and also observe that other values result in slight
performance degradations. The resulting CritICs is relatively
concise (‚âà10KB) to account for ‚âà30% of dynamic coverage.
‚Ä¢ Compilation: Next, we modify the open-source ART compiler to add a final pass (CritIC instrumentation pass) that
applies CritIC optimizations on the apk binary (.oat generation). Note that, the ART compiler already comes with different optimization passes such as constant folding,
dead code elimination, etc., which work on DEX
intermediate representation, as well as load store
elimination, register allocation, etc., which
work on the destination ARM assembly code before binary
generation. Our CritIC pass works on ARM assembly code
(similar to instruction simplifier pass) to take
each CritIC (from the profile), checks whether each of its
instructions are convertible into a 16-bit Thumb format, and
if so, it lays down the entire CritIC sequence instructions
one after another in this 16-bit format with appropriate
two approaches explained next for switching the instruction
format (1.8k LOC). Note that, other than hoisting and Thumbconverting the CritICs encountered, this pass does not affect
the existing instruction scheduling.
‚Ä¢ Off the Shelf Apps: Our framework can be readily applied
to any off-the-shelf app (apk file) from the PlayStore [57].
Table II shows the ten mobile apps we use for evaluations.
These apps belong to a diverse set of domains ranging from
texting to gaming and video/audio streaming. These apps are
also top rated and have millions of downloads in PlayStore.
‚Ä¢ Net Benefits: We have roughly doubled the instruction
fetch rate (halving F.StallForI) of the critical instruction sequences by switching formats, and reduced the
F.StallForR+D delays by making this self-contained dependent chain contiguous in time. We will also demonstrate
that our proposed solution has very little hardware overhead
to interpret the format switch. In fact, our first approach
can be readily done on current hardware, albeit with some
inefficiencies as shown next. The second approach uses an
existing mnemonic to switch format, which is a very small
extension to the switch supported in existing ARM decoders.


#

	,

	+


#


# 

	-

	.

	*


&

	)


&


&


& 

	/


#

	0


#


&

	(
  !    
"#


 & 

 #
&!/1
	!&!/ 	!&!/
1


 &
 


 &

	 .

	 *


 &

	 )


 &


 &


 & 

	 (


 &

	 /


 #

	 0


 #

	 +


 #


 # 

	 -


 #

	 ,
 
 


 &


 


*
)
4
<



 


*
)
4
<
$  %!& !"#'
	() *
F 0

.62 /! /! 1!
* ()  
	 . 1

	
) ()  
	* 1

	
   . >
1(G2"?G

( & 2H+F

&!
!2H1!&-&
0

.H862(  *. 

(& 

(&
 *. 

(&  

(&
+ ()  
	) 1

	
- ()  
	( 1

	
, ()  
	/ 1

	
<  . >
1(.G2"?G

(. # 2H+F

#!
!2H1!#-#
0

.H862(  *. 

(. # 

(.#
 *. 

(.#  

(. #
*) ()  
	 0 1

	
*( ()  
	 + 1

	
*/ ()  
	 - 1

	
*0 ()  
	 , 1

	


 &
 


 &
Fig. 9: Code Generation after CritICs have been identified. There are 2
CritICs, A and B, in this original instruction sequence.
IV. EVALUATIONS
A. Switching Approach 1: On Actual Hardware
We use the conventional approach present in ARM decoders
for switching between the two instruction formats, where two
unconditional branch instructions are added at the beginning
and end of each CritIC instruction sequence. The purpose
of these branch instructions is to inform the decoder of the
impending format switch and so both their target branch
addresses are statically encoded to point to the subsequent
instruction. As shown in Fig. 6, (i) the branch before the
CritIC sequence, is in 32 bit format (that sets the Thumb flag
at decode), and jumps to the first instruction of the CritIC;
(ii) the subsequent 5 CritIC instructions are decoded in 16 bit
Thumb format at the decoder; (iii) the branch after the CritIC
sequence is also in 16 bit format, with its target set to the next
instruction after the CritIC, that resets the format flag to 32-bit
at the decoder. Note that the costs of these branches would
mandate long CritIC sequences in order to amortize them.
We have implemented this on a Google Tablet hardware having 4 ARM cores and 2 GB LPDDR3 memory.










&! !!!
 #


 

!


3
#
.!

 

5
!
3
Fig. 8: Optimizing CritICs in existing hardware leaves
11% performance gap with the Ideal scenario.
Fig. 8 shows
the gains on
this hardware
from our CritIC
optimization
for all 10 apps.
Along with the
actual speedup
gains (of a
measly 3% on the average), we also show the lost potential
which we could have got if there were no branches before
and after the CritICs for the format switches. We are getting
only 1
5 th of the possible gains since the CritIC sequences are
not long enough (typically of length 5) to amortize the branch
overheads. Motivated by this, we next propose an alternative,
which does a very slight enhancement to the hardware, to
address this problem to win back those gains.
B. Switching Approach 2: Extending Existing ARM Instruction
To avoid the aforementioned overheads, we propose to use
an already existing instruction mnemonic, CDP (Co-processor

CPU 4 wide Fetch/Decode/Rename/ROB/Issue/Execute/Commit superscalar pipeline;
128 ROB entries, 4k Entry 2 level BPU [20], [59]
Memory2-way 32KB i-cache, 64KB d-cache, 2 cycle hit latency; 8-way 2MB L2 with
System CLPT prefetcher (1024√ó7bits entries) [18]; hit=10 cycles; 1 Ch;2 Ranks/Ch;
8 Banks per rank; open-page; Vdd = 1.2V; tCL,tRP,tRCD = 13, 13, 13 ns
TABLE I: Baseline Simulation configuration.
Domain App Activities Performed Domain
Acrobat View, add comment Document readers
Angrybirds 1 Level of game Physics games
Browser Search and load pages Web interfaces
Facebook RT-texting Instant messengers
Email Send,receive mail Email clients
Mobile Maps Search directions Navigation
Music 2 minutes song Music/audio players
Office Slide edit, present Interactive displays
PhotoGallery Browse Images Image browsing
Youtube HQ video stream Video streaming
SPEC.int bzip2, hmmer, libquantum, mcf, gcc, gobmk, sjeng, h264ref
SPEC.float sperand, namd, gromacs, calculix, lbm, milc, dealII, leslie3d
TABLE II: Popular Mobile and SPEC apps used in evaluation.
Data Processing call), and the 3-bit argument with it to denote
that the next l+1 instructions would be 16-bit format to inform
the decoder accordingly. Fig. 9 illustrates this translation by our
compiler pass for a CritIC sequence. In the first 32-bit word,
the first half contains the CDP command, together with the l
argument (Fig. 6(d)). The second half of this word contains
the first instruction of the CritIC sequence in 16-bit format.
The next l/2  32-bit words contain the next l instructions of
the CritIC sequence in 16-bit format. Upon encountering
the CDP command, the decoder puts the subsequent l +1 (1
coming in the latter half of the CDP word itself, and the other l
coming from the remaining l/2  words) CritIC instructions
for 16-bit decoding. With the CDP argument having 3 bits, this
allows us to translate up to 1+23 = 9 CritIC instructions
into the 16-bit format using a single CDP command. Note that
we can also allow longer sequences by simply issuing more
CDP commands subsequently, though we find that CritIC
sequences up to 5 instructions suffice to provide the bulk of the
savings (detailed in Sec. IV-H). After the last 16-bit instruction
of this sequence passes through, the subsequent words get
switched to the 32-bit decoding format. We also implemented
and laid out the logic for the mode switch on CDP call on
Synopsys Design Compiler(H-2013.03-SP5-2) [58] with 45 nm
technology library and find that the extra logic only consumes
80Œºm2 area, dynamic and leakage power consumptions as
58ŒºW and 414nW respectively. Although the timing for this
logic is only 160ps, we conservatively assume a 1 cycle extra
decoding stage delay when processing the CDP command.
Even though we have not cut the entire CritIC sequence
down to one instruction fetch as in the above ‚Äúmacroinstruction‚Äù approach, our compiler-based ARM 16-bit translation roughly doubles the instruction fetch rate (halving
F.StallForI) compared to the original alternative. Further,
since these instructions are next to each other in the dynamic
stream, the dataflow gap is reduced, thereby helping in the
F.StallForR+D as well.
C. Simulation Results
We next describe the evaluation platform used for conducting
our experiments on different design scenarios and conduct an
in-depth evaluation of the proposed CritIC optimizations on
performance and energy consumption.
Hardware: We evaluate the app executions using the hardware
configuration of a Google Tablet in GEM5 [23]. As shown in
Table I, this hardware consists of 4 CPUs, each with a 4-issue
wide superscalar core, 32KB i-cache and a 64KB d-cache [60].
Further, we also simulate a detailed memory model for a 2GB
LPDDR3 using DRAMSim2 [61], [62]. This setup enables
us to execute apps in a cycle-level hardware simulation and
obtain performance and power consumption for CPU, caches,
and memory of the SoC.
App Execution: During the profiling phase (Sec. III-A2), these
apps are emulated for an average of five minutes and execute,
on average, around 100M instructions. This translates to ‚âà90
seconds of app execution time without the emulator overheads.
For our evaluations, we pick 100 samples at random, each
containing ‚âà500k contiguous instructions of app executions
tallying to a total of ‚âà50 million instructions (same parts for
all the optimizations evaluated).
D. Design Space
To quantify the performance effects of the proposed CritIC
design on mobile apps, we evaluate three design choices, and
compare them to the baseline configuration in Table I.
‚Ä¢ Hoist: Since our solution employs two mechanisms - one
hoisting all instructions of a CritIC sequence and another
replacing them with 16-bit Thumb formats - we would
like to study their effectiveness individually. Towards this,
we implement a scheme which only does the former (i.e.,
identifies CritIC sequences, and hoists each sequences‚Äô
instructions), but leaves them in 32-bit ARM format. We
call this as Hoist in our evaluations.
‚Ä¢ CritIC: This is our proposed CritIC design that aims to
tackle the fetch side bottlenecks for high-fanout instructions
as well as the F.StallForR+D bottlenecks by hoisting/aggregating the constituent instructions together and
also translating these instructions to 16-bit Thumb format.
‚Ä¢ CritIC.Ideal: As was noted earlier in Fig. 5b, we choose
to leverage only a subset of the total number of CritIC
sequences - (i) those that are at most length 5, and (ii) those
whose instructions can be translated directly to the 16-bit
Thumb format. In order to find out the lost opportunity, we
also evaluate a scheme called CritIC.Ideal which hypothetically aggregates and Thumb-translates for all CritIC
instructions (i.e., the black CDF of Fig. 5b).
E. Performance Results
Fig. 10a plots the CPU execution speedup of each app for
the three scenarios discussed above to study the individual
as well as combined effects of the two components of CritIC
optimizations. We discuss app level speedups of each of these
optimizations normalized with respect to the baseline design.
When we consider the individual optimizations evaluated in
Fig. 10a, we see that the CritIC optimizations consistently
perform well in all apps with 9% (Music) to 15% (Acrobat)
speedup. However, Hoist (which only targets StallforRD) by
itself, only gives marginal improvements (average gain of 2.5%)
compared to CritIC which combines both F.StallForI and
F.StallForR+D optimizations, suggesting that just moving

&!. &23
.
#!/ +
 6.!!$ 9 9
 @ !!
23
I!

. &D2




;!




(
;!




(
;!




(
;!




(
;!




(
;!




(
;!




(
;!




(
;!




(
;!




(
;!




(


(a)



*
+5
!0

6(6! 6(6!8=1
# 

(b)



 


*
)
3
:+23
D2
-


'+5
!
9
!3
(c)
Fig. 10: (a)Speedup over baseline; (b) Fetch stage savings of CritIC instructions; (c) Energy gains with CritIC optimization
instructions does not suffice. Since this scheme only reduces
the dataflow gap across critical instructions, without boosting
the fetch efficiency, the impact of just a F.StallForR+D
optimization is not felt across these apps, reiterating the
need for fetch side improvements. Of the apps, Maps and
Youtube are more bottlenecked in the F.StallForR+D
(26.7% in Youtube in baseline of Fig. 10b) and this in turn
translates to the most benefits when it comes to optimizations
for F.StallForR+D (3.1%). All the other apps have even
less improvements from hoisting the CritIC instructions, with
Browser and Photogallery showing the least benefits of 1.7%.
CritIC, which implements both 16-bit conversions to boost
the fetch bandwidth, as well as the Hoist improvements, gives
12.6% speedup improvements on the average. In fact, we see
that the differences between CritIC and CritIC.Ideal,
to be quite small (e.g. only 1% gap in Acrobat, Browser and
Office). Limiting ourselves to CritIC lengths of 5 or to those
that can be directly translated to 16-bit Thumb format, does not
seem to hurt. This is because, a majority of CritIC instructions
are amenable to 16-bit Thumb representation, leaving <1%
room for any further improvement on the average. As discussed
in Sec. III, the volume of CritIC instructions representable
with the 16-bit format is within 5% of the entire CritIC
instruction volume. We note that the average 12.6% speedup
with CritIC significantly outperforms the previously proposed
single instruction criticality optimizations - load prefetching
and ALU prioritization - for which we showed speedups of
0.7% and 4.1% respectively.
F. System-Wide Energy Gains
The effect of our CritIC optimizations in terms of the
energy gains from various components of the mobile SoC is
plotted in Fig. 10c. Recall that CritIC optimizations decrease
the number of accesses to the i-cache by 40% (Fig. 6) for each
IC execution by representing 5 √ó 32-bit instructions as 3 √ó
32-bit instructions. This translates to energy gains from i-cache
by 0.8% for the whole SoC. The CPU speedup discussed
above also results in additional energy gains for both CPU and
memory. On an average, CPU contributes to 2.2% of the energy
savings and the memory side of the execution contributes an
additional 1.5%. Overall, we observe 4.6% energy saving for the
whole system on the average, with the maximum energy savings
of 6.3% (in Photogallery). Specifically, the CPU execution
alone (excluding peripherals, ASIC accelerators, etc.) realizes
an average energy saving of 15%.
G.Comparing with Conventional Hardware Fetch Optimizations
One may note that numerous prior hardware enhancements
proposed to address the Fetch stage problems, including larger






 
 





 


 :

(a)



 





 


+5
!0

6(6! 6(6!8=1
 :

(b)
Fig. 11: Comparison with Hardware Mechanisms (a) Speedup and (b) Impact
on F.StallForIand F.StallForR+D.
and more intelligently managed i-caches [63]‚Äì[65], better
branch predictors [66]‚Äì[69], and/or instruction prefetchers [70]‚Äì
[74]. While adding sophisticated hardware for high end CPUs
may be acceptable, the resource constraints of mobile platforms
may not warrant such sophisticated hardware. Still, we have
implemented a number of hardware solutions for addressing
the Fetch bottleneck (described below), and compared them to
the speedup obtained with our software-only solution ‚Äì CritIC:
‚Ä¢ 2√óFD: Since CritIC uses a 16-bit format to put 2 instructions
into each fetched word (selectively doubling fetch bandwidth
for critical instructions), we consider a hypothetical hardware
where the Fetch and Decode stage bandwidths are doubled
(for all instructions - not just critical ones), with no change to
other stages. In this scheme (2√óFD), we simulate a hardware
with half the i-cache latency and double the resources
(hardware units/queues) in the fetch and decode stages.
‚Ä¢ 4√ói-cache: Though unreasonable, we compare with a
hardware that has 4√ó the i-cache capacity (128KB vs. 32KB)
to reduce instruction misses.
‚Ä¢ EFetch [71]: We implemented a recently proposed instruction prefetcher [71] that is specifically useful for user-event
driven applications, as in our mobile apps. This prefetcher
[71] tracks history of user-event call stack, and uses it to
predict the next functions and prefetch its instructions. It
needs a 39KB lookup table for maintaining the call stacks.
‚Ä¢ PerfectBr: This is a hypothetical system where we assume
there is no branch misprediction in the entire execution.
Since CritIC addresses both (i) F.StallForI which the
above 3 address; and (ii) F.StallForR+D, which is somewhat addressed by prior criticality optimizations such as [5],
[6], [14], [16], [25], [31], [75]‚Äì[77], which prioritize the backend resources for those instructions, we additionally consider
the following configurations:
‚Ä¢ BackendPrio [33]: This platform implements the prioritization hardware for the back-end resources proposed in [33],
using the tracking hardware proposed in [32], which requires
1.5KB SRAM for maintaining the tokens.
‚Ä¢ AllHW: This consists of hardwares for both front and




 






    ) <




D260






 2


  
D260

(a)






  




  
+5
!
!D.3!


D
!
!
(b)
Fig. 12: Sensitivity Analysis: (a) Fetch savings and speedup w.r.t CritIC length;
and (b) Speedup w.r.t CritIC Profile Coverage.
backends, i.e., 4√ói-cache+EFetch+PerfectBr+BackendPrio.
‚Ä¢ With CritIC: In addition to comparing with vanilla CritIC,
which has no additional hardware needs, we also study CritIC
in combinations with every above hardware mechanisms.
Results: We observe in Fig. 11a that previously proposed
hardware mechanisms yield ‚âà4% to 12% speedup. However,
it is important to optimize for both F.StallForI and
F.StallForR+D. These hardware mechanisms only benefit
one of these two stalls (Fig. 11b). For example, 2√óFD, 4√ó
larger i-cache and EFetch lower miss penalties to reduce
the F.StallForI by ‚âà7%, while PerfectBr completely
eliminates branch penalties to reduce fetch stalls by 12%. These
mechanisms have no effect on F.StallForR+D. Similarly,
BackendPrio only addresses the F.StallForR+D problem,
reducing it by 3% and does not tackle the F.StallForI.
While one could throw all this hardware to tackle both
these stalls, as in AllHW, to get the overall speedup benefits
of 23.2%, such extensive hardware may be unacceptable for
a mobile platform. CritIC, by itself, which does need any
additional hardware, does significantly better than each of these
individual hardware mechanisms. If future mobile platforms
are to incorporate one or more of these F.StallForI and
F.StallForR+D hardware mechanisms, our results in Fig.
11a show that CritIC can synergistically boost the benefits
further. In fact, even with a system that incorporates all of the
above hardware (AllHW) which gives a speedup of 23.2%, can
be boosted to give a speedup of 31% with CritIC on top.
H. Sensitivity to CritIC length
The speedup and energy gains reported above are for a small
CritIC size of 5 instructions. We next investigate the impact
of CritIC length on application performance.
Even though CritIC.Ideal showed not much difference
compared to the realistic CritIC (which uses lengths of up
to 5 instructions), it is interesting to see which CritIC length
gives the most rewards individually, i.e., not just all CritICs up
to length n, but for each individual n. Note that as n increases,
we are saving more on the fetch costs - both F.StallForI
and F.StallForR+D latencies. However, the probability of
finding a CritIC of exactly length n, where all its n instructions
can be directly translated to the 16-bit Thumb format, decreases
as n increases. To study these trade-offs, in Fig. 12a we study
the impact of a given n (x-axis) on the fetch cost savings
(right y-axis) and the consequent speedup (left y-axis). As
expected, fetch costs keep dropping with larger n, though
with diminishing returns. The speedup increases up to a point
(n = 5), beyond which it starts dropping since the probability
of finding such sequences diminishes. In fact, we observe a
drop in coverage of CritICs executed from 16% to 15% as we
move for a longer CritIC.
I. Sensitivity to Profiling
Since our technique uses offline profiling to identify and
modify critical chains, we also study the sensitivity of results to
the extent of profiling, i.e. the percentage of the app execution
that is profiled. Fig. 12b shows the speedup (y-axis) as a
function of the percentage of the execution that is profiled
(x-axis), averaged across all apps. The results presented so far
use profiling that covers 72% of the execution. While a lower
coverage does reduce the speedup obtained, we see that even
when only a third of the execution is profiled and transformed
into CritIC thumb sequences, we still get 10% speedup across
these apps. If we further the profiling, and transform the entire
application, we can get up to 15% speedup on the average.
V. WHY EVEN BOTHER WITH CRITICALITY?
While we have proposed the use of Thumb 16-bit format to
nearly double the fetch bandwidth of the CritIC instructions,
one may use this approach opportunistically for all instructions
amenable to such modification in the instruction stream. If so,
one could question why we bothered to identify CritICs in
the first place. To justify the need, in Fig. 13a, we plot the
speedup obtained with the following schemes:
‚Ä¢ OPP16: In this approach, we opportunistically convert any
amenable sequence of consecutive dynamic instructions
(sequence has to be of at least length 3) to the 16-bit
Thumb format, regardless of whether they are critical or not.
Note that if there is an instruction which is not amenable
to such format conversion between two other instructions
which are amenable, OPP16 will NOT move the instructions
around for the conversion. Also, as explained earlier, if
the dynamic sequence exceeds 9 contiguous instructions
that can be converted, we use another CDP instruction to
accommodate longer sequences for such conversion.
‚Ä¢ Compress: This is a state-of-the-art thumb compression
technique, implementing the Fine-Grained Thumb Conversion heuristic from [78], that first converts a whole function
to Thumb, then replaces frequently occurring ‚Äúslower thumb
instructions‚Äù back to 32 bit ARM instructions.
‚Ä¢ CritIC: This implements our CritIC mechanism described
earlier, moving/hoisting identified CritIC sequence instructions and converting them to 16-bit format as long as they
are amenable to such conversion and they are of length ‚â§ 5.
‚Ä¢ OPP16+CritIC: We combine CritIC (for CritIC sequence instructions) and OPP16 (for others) in this approach.
As seen, just opportunistically leveraging the 16-bit Thumb
format (in OPP16) only provides 6% benefit on the average










@* 
!
 

 @*=

(a)



 


13

!

!D!0

.6!

@* 

 
!

(b)
Fig. 13: Opportunistically transforming to 16-bit Thumb format. (a) Speedup
and (b) Percentage of Dynamic Instructions converted to 16-bit format.

over the baseline. Even smartly employing the Thumb format
(Compress), as in [78], only yields a 8% speedup. Since both
OPP16 and Compress are agnostic to critical instruction chains,
they can only save on fetch costs (F.StallForI) whenever
possible without hoisting the dependent instructions in the
chain. Hence, both these techniques provide less than 40%
of the benefits provided by our CritIC optimization, even
though as shown in Fig. 13b, CritIC converts around 37%
and 50% fewer instructions in the dynamic stream to the 16-bit
format compared to OPP16 and Compress respectively. This
clearly points out the need to identify the critical instruction
sequences for such optimization, instead of blindly doing this
for all instructions. In fact, nothing precludes adding on the
optimization for other instructions on top of CritIC, as is
shown for OPP16+CritIC schemes, furthering the speedup by
25% over doing CritIC alone.
VI. RELATED WORK
Criticality: Instruction criticality has been shown to be an
important criterion in selectively optimizing the instruction
stream. Prior work has revolved around both (i) identifying
critical instructions [4], [9], [12], [24]‚Äì[26] using metrics such
as fanout, tautness, execution latencies, slack, and execution
graph representations, as well as (ii) optimizing for those
identified using techniques such as critical load optimizations
[9], [11], [12], [18], [79] or even backend optimizations
for critical instructions such as [5], [6], [14], [16], [24],
[25], [31], [75]‚Äì[77]. While one can potentially employ these
optimizations for mobile apps, as we showed (in Fig. 1b),
mobile apps have close data-dependent, clustered occurrences
of critical instructions, requiring their ensemble optimization
rather than their consideration individually.
Optimizing Instruction Chains/Ensembles: There are prior
works, specifically for high-end processors, in identifying
and extracting dependence chains [80]‚Äì[82]. However, such
techniques require fairly extensive hardware to identify these
chains, and optimizing for them, e.g. techniques such as [18],
[76], [77] require 16KB SRAM, and [79] incurs 22% additional
power, making them less suitable for resource-constrained
mobile SoCs. In contrast, our solution is an entirely software
approach for identifying dependence chains, and a software
approach in optimizing for them by intelligently employing the
ARM 16-bit thumb compression [19]‚Äì[21], [51] mechanism.
Front-end Optimizations for Mobile Platforms: There has
been significant recent interest to optimize mobile CPU
execution [83]‚Äì[89]. Some of these optimizations target specific
domains (e.g. web-browsers [90]‚Äì[93]), while others address
overall efficiency [37], [94]‚Äì[96]. Unlike our approach, many
of these optimizations either provision more CPU hardware
[90], [95], [96], or optimize for only specific app domains [90]‚Äì
[92]. This paper is amongst the first to show that mobile apps
are bottlenecked in the Fetch stage of the pipeline, suggesting
that there can be considerable rewards in targeting this stage.
Fetch stage bottlenecks have been extensively addressed in high
end processors through numerous techniques - smart i-cache
management (e.g. [63]‚Äì[65], [97]‚Äì[99]) prefetching (e.g. [70]‚Äì
[74]), branch prediction (e.g [66]‚Äì[69]), instruction compression
[100] SIMD [38], [39], VLIW [40], vector processing [41],
etc. However, many of these require extensive hardware that
mobile platforms may not be conducive for. As we showed,
our software solution employs a simple trick of hoisting and
Thumb conversion on critical instructions to extract the same
performance that many of these high-end hardware mechanisms
provide. Further, as mobile processors evolve to incorporate
more hardware for optimizing the fetch stage, as shown, our
CritIC software approach can synergistically integrate with
them to significantly boost the improvements. While similar
in spirit to some of the prior work on instruction stream
compression [101]‚Äì[103], we quantitatively showed the need to
identify critical chains and hoisting the instructions selectively
before doing the compression.
Software Profiling for Mobile Platforms: A number of
software profiling frameworks have been proposed [35], [104]‚Äì
[107] - studying library usage [35], [106], app-market level
changes to the source/advertisement models, [104], [105],
dynamic instrumentation mechanisms [107], developer side
debugging/optimizations [108], [109] etc. Some of these tools
can also be extended for the profiling and compilation phases
described in this work. We have built on top of the AOSP
emulation [22], [49] and Gem5 hardware simulator [23] for
profiling, and ART compiler for code transformation.
VII. CONCLUSION
This paper targets to enhance the performance a growing
class of applications - mobile apps ‚Äì that are more prevalent and
user driven than traditional server/scientific workloads. In this
context, we show that mobile apps have unique characteristics
such as high volume of critical instructions occurring as short
sequences of dependent instructions that makes them less attractive for exploiting well-known criticality-based optimization
techniques. We instead introduce the concept of CritICs as a
granularity for tracking and exploiting criticality in these apps.
We present a novel profiler-driven approach to identify these
CritICs, and hoist and aggregate them by exploiting existing
ARM ISA‚Äôs Thumb instruction format in a compiler pass to
boost the front-end fetch bandwidth. The end-to-end design
starting from application profiling, identification of CritICs,
hoisting those instructions and transformation them to the 16-
bit Thumb format has been evaluated for a Google Tablet using
the GEM5 simulator to estimate the performance and energy
benefits. Evaluations with ten popular mobile apps indicate that
the proposed solution results in an average 12.6% speedup and
4.6% reduction in system-wide energy consumption compared
to the baseline design, requiring little to no hardware support.
The proposed technique can also be synergistically integrated
with other optimizations such as hardware prefetching, or even
opportunistically converting as many instructions as possible
to the Thumb format, to further the benefits.