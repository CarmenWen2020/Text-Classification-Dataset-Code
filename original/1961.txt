ABSTRACT
We show that selecting a data width for all values in Deep Neural
Networks, quantized or not and even if that width is different per
layer, amounts to worst-case design. Much shorter data widths can
be used if we target the common case by adjusting the data type
width at a much finer granularity. We propose ShapeShifter, where
we group weights and activations and encode them using a width
specific to each group and where typical group sizes vary from
16 to 256 values. The per group widths are selected statically for
the weights and dynamically by hardware for the activations. We
present two applications of ShapeShifter. In the first, that is applicable to any system, ShapeShifter reduces off- and on-chip storage
and communication. This ShapeShifter-based memory compression
is simple and low cost yet reduces off-chip traffic to 33% and 36%
for 8-bit and 16-bit models respectively. This makes it possible to
sustain higher performance for a given off-chip memory interface
while also boosting energy efficiency. In the second application, we
show how ShapeShifter can be implemented as a surgical extension
over designs that exploit variable precision in time.
1 INTRODUCTION
While training Deep Learning models requires some floating point
arithmetic [1, 2], the models can be trained to use a different data
type during inference. For several tasks, e.g., image classification,
fixed-point arithmetic has proven sufficient, with simple downconversion to 16-bit fixed point (int16) arithmetic being generally
applicable. Such down-conversion is a rudimentary form of quantization. Quantization to the shortest data width possible is especially
appealing for Deep Learning workloads for two reasons: First, most
of their energy expenditure is due to data transfers. Using a short
data type reduces the volume of this data. Second, since the data is
input to numerous multiply-accumulate (MAC) operations which
exhibit great data parallelism, the shorter the data type, the more
functional units we can deploy for a given on-chip area and the
higher performance and energy efficiency.
Given the immediate benefits on commodity hardware and accelerators alike, quantization has naturally attracted a lot of attention. As a result, today, quantization to int8 is usually possible for
state-of-the-art image classification models, and selectively in other
application domains [3–11]. Quantization to shorter data widths
such as 4b or 2b [7, 12, 13], or even to 1b has been demonstrated in
certain cases [14–17].
Regardless, the aforementioned quantization methods target a
fixed data width either per network or per layer. We observe that
data widths can be trimmed further since: 1) by design, the expected
per-layer distribution of values in deep learning models, be it for
weights or activations, is that most will be near zero and few will be
of higher magnitude, and 2) which values will assume a high magnitude will vary with the input. Targeting a specific data width for
all values, even if this is adjusted per layer [18–25], disproportionately favors the exceedingly few high magnitude values, incurring
28
MICRO-52, October 12–16, 2019, Columbus, OH, USA Delmás Lascorz et al.
significant overheads for the vast majority of data transfers and
computations. However, the data width used at any given point
of time needs to accommodate only 1) the activation values for
the specific input at hand, and further 2) the activation and weight
values that are being processed or transferred concurrently. Accordingly, we propose mechanisms to exploit the expected distribution
of values in neural nets to deliver memory footprint, traffic, and
computation throughput improvements in a way that is generally
applicable to any network, regardless of whether any specific form
of quantization was applied or is possible.
The key idea proposed in this work is to use per group data
width adaptation, where a group is a set of values that are either
calculated upon or transferred from/to memory together. For example, these could be 16 values that are read from off-chip memory
or 32 values that are being processed together in a SIMD style execution unit. We propose to select the data width dynamically for
activations and statically for weights. For example, if we are storing
16 values from an int8 network where the maximum magnitude
among them is 0x3 our goal is to use only 2b per value plus some
metadata to store them in memory. For another group where the
maximum magnitude happens to be 0xf we wish to use instead
4b per value. We further wish to develop a compute unit where
if processing a group where the maximum magnitude requires 8b
takes C time, processing the two aforementioned groups would
require instead just C
8
× 2 and C
8
× 4 time respectively. Note that
block-floating point[26], flexpoint [27] and narrow floating point
formats [28] do not take advantage of the expected distribution of
the neural net values to adapt data width. Floating-point representations expand the value range but still represent all values using
the same number of bits. Moreover, most floating-point proposals
target training, and generally fixed-point is a lower cost alternative
for inference.
Our first contribution is to show that adjusting the data width
per group dynamically for activations and statically for weights
yields much shorter effective data widths than even per layer width
selection. It does so without affecting numerical fidelity and for
that accuracy. As our second contribution we bring attention to
a property of popular quantization methods which is that while
they successfully “squeeze” broader value ranges to fit within a
target data width, they also “expand” shorter data ranges into the
target data width. This expansion is often unnecessary and obscures
opportunities for per group data width reduction. We deploy a
range-aware quantization method, preserving the benefits of per
group data length adaptation.
Our first novel hardware technique is a low cost lossless
memory compression method that is plug-in compatible with most
Deep Learning hardware. Our low cost ShapeShifter hardware compresses and decompresses groups of weights and activations offchip reducing energy considerably and boosting the effective offchip bandwidth. For weights, the compression is done once as a
pre-processing step in software. For activations, it is performed
dynamically at the output of the previous layer or at the source.
Compressed data is decompressed on-the-fly when fetched from offchip. To show that our technique is compatible with most hardware,
we demonstrate benefits over a range of accelerators.
Our second novel hardware technique adapts the width of
activations and weights at runtime so that execution time is proportionally reduced per group of concurrently processed values.
This is compatible with designs that exploit data width variability
in time, e.g., [29–31] as opposed to designs that do so spatially [25].
We present SStripes, a surgical extension over the Stripes accelerator [29]. A major advantage of our proposal is that it requires
modest hardware changes yet the resulting performance, communication, storage, and energy efficiency improvements are anything
but. Moreover, the area overhead is exceedingly small.
Our ShapeShifter approach is not a quantization method and does
not affect numerical range, value nor accuracy. It simply adjusts
the data container width to accommodate the values at hand at a
fine, programmer transparent granularity. Quantization methods
transform the values so that they fit a particular data width. For
virtually all quantization methods, the width has to be large enough
to accommodate all values within a large set of data, typically a
layer or the whole network. For this reason there will be values that
in practice could be represented in a narrower width and thus there
is going to be opportunity for ShapeShifter. Naturally, the smaller
the quantization width, the smaller the relative opportunity will be.
But, unless it becomes possible to quantize all networks of interest
to a small data width, ShapeShifter delivers proportional benefits
for all networks. Unique among quantization methods, is outlieraware quantization [32] which uses two data widths. Most values
(97% to 99%) use the shorter width (e.g., 4b or 5b), and only the
remaining few use the full data width (e.g., 8b or 16b). The two data
widths are fixed, design time parameters and the network has to be
retrained explicitly for those. We show that ShapeShifter directly
benefits from this form of quantization providing additional benefits
by further reducing data width for both sets of values. However,
contrary to outlier-aware quantization ShapeShifter works with
any model, quantized or not and imposes no design or runtime
constraints on the data widths used, their relative frequeny or their
spatial distribution.
We highlight the following findings:
• ShapeShifter off-chip memory compression reduces traffic
to 27% and 36% respectively for the 16b and the range-aware
quantized 8b models studied. With Tensorflow quantization,
the benefits are much lower as traffic is reduced to at most
80% for some of the models only. ShapeShifter compression
is robust and never increases traffic.
• ShapeShifter compression boosts the effective memory bandwidth yielding energy savings and boosts performance, e.g.,
performance for BitFusion improves by 87% with DDR4-3200
memory for 16b models.
• ShapeShifter Stripes (SStripes) improves compute performance
over Stripes by 1.61× for 16b models, and 2.17× or 1.49× for
8b models depending on the quantization method (RangeAware or Tensorflow respectively). It is also faster than Bit
Fusion by 3.75× and 2.3× for the 8b models.
• ShapeShifter compression delivers virtually all the memory
traffic reduction possible on outlier-aware quantized models despite not being specialized for them. It further boosts
compression rates by 24.5% on average.
29
ShapeShifter: Enabling Fine-Grain Data Width Adaptation in Deep Learning MICRO-52, October 12–16, 2019, Columbus, OH, USA
Width
Fraction of Activations
(a) GoogLeNet, Conv 1
Width
Fraction of Activations
(b) GoogLeNet, 5a-1x1
Width
Fraction of Activations
(c) ResNet50, res3a_branch1
Width
Fraction of Activations
(d) ResNet50, res5a_branch1
Figure 1: Per Group vs. Per Layer Activations Width Needs: 16b Models
0 1 2 3 4 5 6 7 8 9 10111213141516
Width[bits]
0.0
0.2
0.4
0.6
0.8
1.0
Fraction of Weights
16
32
64
128
256
static
(a) GoogLeNet, Conv 1
0 1 2 3 4 5 6 7 8 9 10111213141516
Width[bits]
0.0
0.2
0.4
0.6
0.8
1.0
Fraction of Weights
16
32
64
128
256
static
(b) GoogLeNet, 5a-1x1
0 1 2 3 4 5 6 7 8 9 10111213141516
Width[bits]
0.0
0.2
0.4
0.6
0.8
1.0
Fraction of Weights
16
32
64
128
256
static
(c) ResNet50, res3a_branch1
0 1 2 3 4 5 6 7 8 9 10111213141516
Width[bits]
0.0
0.2
0.4
0.6
0.8
1.0
Fraction of Weights
16
32
64
128
256
static
(d) ResNet50, res5a_branch1
Figure 2: Per Group vs. Per Layer Weights Width Needs: 16b Models
2 DATA WIDTH NEEDS
This section demonstrates that, for a variety of 16b and 8b models:
1) the widths needed at a finer-than-a-layer granularity are considerably shorter than those needed for the whole layer, 2) only a
small number of value groups require the maximum width needed
for the layer as a whole, 3) the width needed varies with the input,
and 4) some quantization methods may unnecessarily expand value
ranges to the full range afforded by the target width.
Activations: Figures 1a-1d show data width (henceforth referred
to as just width) measurements for four convolutional layers – two
from each of GoogleNet and SkimCaffe-ResNet50 [33] (a weight
pruned network), both originally quantized to 16b. Similar trends
were observed in other 16b models and their layers. The measurements are over 5,000 randomly selected images from the IMAGENET dataset [34]. The graphs show the cumulative distribution
of the width needed per group of activations for various group
sizes ranging from 16 to 256 values, where each group uses the
width needed for its least favorable activation. Two vertical lines
report the width when considering all activation values: 1) the
profile-derived (“static”) over all images, and 2) the one that can be
detected dynamically (“dynamic”) for one randomly selected image
to illustrate that widths also vary per input.
Figure 1a reports measurements for conv1, the first layer of
GoogleNet. The profile-determined width is 10b while for one specific image all values within the layer could be represented with
just 7b, an improvement of 30% in width length. The cumulative distribution of the widths needed per group of 256 activations shows
that further reduction in width is possible. For example, about 80%
of these groups require 6b only, and about 15% just 5b. Smaller
group sizes further reduce the effective width, however, the differences are modest. These results suggest that picking a width for the
whole layer exacerbates the importance of a few high-magnitude
activations.
The first layer usually exhibits different value behavior than the
rest since it processes the direct input image to discover visual
features whereas the inner layers tend to try to find correlations
among features. Layer 5a-1x1 in Figure 1b shows more pronounced
improvements in effective width with the smaller group sizes. Figures 1c and 1d show that the behavior persists even in a sparse
network. We include this measurement to demonstrate that dynamic width variability is a phenomenon that is orthogonal to
weight sparsity and thus of potential value to designs that target
sparsity. Moreover, dynamic width variability is not only limited to
activations.
Weights: Figure 2a reports measurements for the weights in the
first layer of GoogleNet. While the profiled-determined width is
9b, dynamic precision determines that 80% of the groups can be
represented using only 8b for all group sizes. For a group size of 16,
70% of the weight groups require only 6b. This is more pronounced
for Figure 1b where 80% of the bits can be represented using only
4b rather than the 6b profiled. Figures 2c and 2d show that this
behaviour persists in sparse networks.
8b Quantization: Figure 3a reports the distribution of widths
needed for GoogleNetS quantized to 8b under two quantization
methods: 1) Tensorflow (TF) and 2) Range Aware (RA). The figure
shows that Tensorflow may unnecessarily expand the value range
30
MICRO-52, October 12–16, 2019, Columbus, OH, USA Delmás Lascorz et al.
0 1 2 3 4 5 6 7 8
Width[bits]
0.0
0.2
0.4
0.6
0.8
1.0
Fraction of Activations
reduction2 RA
icp5_out1 RA
icp7_out0 RA
reduction2 TF
icp5_out1 TF
icp7_out0 TF
(a) GoogLeNetS Activations
0 1 2 3 4 5 6 7 8
Width[bits]
0.0
0.2
0.4
0.6
0.8
1.0 Fraction of Weights
reduction2 RA
icp5_out1 RA
icp7_out0 RA
reduction2 TF
icp5_out1 TF
icp7_out0 TF
(b) GoogLeNetS Weights
0 1 2 3 4 5 6 7 8
Width[bits]
0.0
0.2
0.4
0.6
0.8
1.0
Fraction of Activations
conv1 RA
convDec4 RA
softmax RA
conv1 TF
convDec4 TF
softmax TF
(c) SegNet Activations
0 1 2 3 4 5 6 7 8
Width[bits]
0.0
0.2
0.4
0.6
0.8
1.0 Fraction of Weights
conv1 RA
convDec4 RA
softmax RA
conv1 TF
convDec4 TF
softmax TF
(d) SegNet Weights
Figure 3: Per Group Data Width Needs for 8b models with Tensorflow (TF) and Range Aware (RA) quantization.
0
2
4
6
8
10
12
14
16
Average Width (bits)
Weights Activations
Figure 4: Average data width needed for Weights and Activations: Per Layer (top) vs. Per Value (bottom).
to 8b even if a layer does not need to, whereas Range-Aware quantization does not. The figure shows three of the longer running layers
which are representative of the range of behaviors seen over all
layers (best, average, and worst). Under range-aware quantization,
most values require considerably less than 8b. In reduction2 and
icp5_out1 more than 80% of the activations need just 3b or less, a
reduction of at least 87.5% over 8b. Even in icp7_out0, the most demanding layer, more than 90% of the activations need only 6b - a 25%
reduction over 8b. Under Tensor flow quantization, the same model
exhibits much higher data width needs. Even for reduction2, 96%
of the values now need 6b. Figure 3b shows unnecessary value
expansion by the Tensorflow quantization also for the weights. In
the case of icp5_out1, Tensorflow needs the whole 8b to represent
all the groups, in contrast to Range Aware quantization requires
only 4b. Figures 3c and 3d show similar behavior for SegNet.
Other Models: Figure 4 demonstrates that width requirements
vary considerably between the layer and individual values for a
broad range of neural network models. It compares the average
effective width with per layer profiling or with per value detection
for weights and activations for a variety of models. The top point of
the surface for weights, and the bar for activations, corresponds to
the per layer and the per value widths respectively. These measurements are for 1,000 randomly selected images from IMAGENET [34]
Off-Chip Memory
On-Chip Memory
Decode Encode
Memory Controller
On-Chip Memory
Decode Encode (a) Data Type Conversion
Zero Value Vector u
16
Prec
4
V0
P
V1
P
VN
P
(b) Off-chip Group Data Container
0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0
0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0
0 0 0 0 0 0 0 0 0 0 0 1 0 1 0
0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0
0 0
MSB b15 LSB b0
Leading 1 detector
nH = 1011
4
A0
A1
A2
A3
10010000 1010010 0
(c) Detecting per group widths for activations.
Figure 5: Using per group widths to reduce off-chip memory
bandwidth and storage.
for the image classification models [33, 35–39] and for 100 images
from CamVid [40] for SegNet [41](segmentation), 100 and 10 inputs
from Pascal VOC [42] respectively for YOLO V2 [43] (detection) and
FCN8 [44] (segmentation), 10 images from CBSD68 [45–47], 10 inputs for IRCNN [48] (denoising) and VDSR [49] (super-resolution),
500 inputs from WMT14 for Seq2Seq [50] (translation), 500 inputs from COCO [51] for LRCN [52] (captioning), and 500 inputs
from Flickr8k [53] for Bi-Directional LSTM [54] (captioning). The
31
ShapeShifter: Enabling Fine-Grain Data Width Adaptation in Deep Learning MICRO-52, October 12–16, 2019, Columbus, OH, USA
measurements are across the whole network. It also reports the
reduction in work (left Y-axis) for weights and activations when
per-value width is used. The results illustrate that selecting a width
per layer grossly overestimates that needed for the common case.
Finally, Table 1 reports the effective data width achieved per layer.
Due to space limitations we report results for the 16b models with a
group size of 16 values along the channel dimension. The effective
width values are fractional as they are averaged over all groups
within the layer and weighted accordingly to their runtime use.
3 REDUCING OFF-CHIP STORAGE AND
COMMUNICATION
The bulk of energy in DNNs is expended by off- and on-chip memory accesses. Further, when on-chip storage is limited, off-chip
bandwidth can easily be the bottleneck. Fortunately, the variable
width needs of activations and weights can be used to reduce the
amount of storage and bandwidth needed on- and off-chip. Here
we limit attention to the off-chip compression scheme.
Off-chip Memory Data Container: We encode weights and activations in groups of N values. We find that N = 16 offers a good
balance between compression rate and metadata overhead. Figure 5b shows the memory container. For each group, we determine
the width in bits, pi
, that the group needs either statically (for
weights) or dynamically (using the hardware unit of Figure 5c) at
the output of the previous layer or at the input source for the first
layer (for the activations). We then store that as a prefix using loд(P)
bits, where P represents the maximum data width, followed by the
16 values each stored using pi bits. In addition, to avoid storing
zero values altogether, we use a 16b “is zero” vector with one bit per
original value and store only the non-zero values off-chip. In total,
this scheme requires 4 + 16 bits of metadata per group of sixteen
16b values. Uncompressed these sixteen values occupy 256b for 16b
values (or 128b for 8b values) so the metadata overhead is far less
than the benefits obtained from compressing the values for typical
cases. Figure 6 shows an example of how two groups each of eight
8b values are represented and decoded with our approach.
Detecting the Per Group widths for Activations: Figure 5c
shows how the hardware adjusts the width at runtime for an example group of four 16b activations A0 through A3. This is needed
when it is necessary to store the values off-chip at the output of
each layer. It trims the unnecessary prefix bits by detecting the most
significant bit position needed to represent all values within the
group. The example activations can all be represented using just 12
bits as the highest bit position a 1 appears, nH , is position 11. The
hardware calculates 16 signals, one per bit position, each being the
OR of the corresponding bit values across all four activations. The
implementation uses OR trees to generate these signals. A “leading
1” detector identifies the most significant bit that has a 1, and reports its position in 4 bits. The detector can be extended to handle
negative values by converting them first to a sign-magnitude representation, and placing the sign at the rightmost (least significant)
place. This is useful for weights and for networks that use activation
functions that attenuate negative values [55, 56].
Memory Layout and Access Strategy: Memory compression
and bandwidth amplification methods for general purpose programs have to support random accesses at cache block aligned
addresses, e.g., [57] . For this reason they need to be able to quickly
determine where each compressed block is stored, how long it is,
and to handle cases where the compressed block needs more space
than when uncompressed. By taking advantage of the unique requirements of neural networks, Shapeshifter compression has to
meet considerably looser requirements. This is easier to understand
in the case where we can size the on-chip memory buffers so that
each weight and activation needs to be accessed from off-chip only
once per layer [58]. This is practical for all networks studied. In
this case, for each layer the accelerator reads from off-chip two
input arrays (weights and activations) and writes a single output
array (output activations). The off-chip accesses to each array can
be sequential. As a result Shapeshifter needs to support: 1) direct
accesses to the beginning of three large containers (whole activation or weight arrays), and 2) sequential accesses within these
containers. Within each container data can be compressed as desired without concern about how much space it occupies and where
it is stored: the incoming stream will be decoded sequentially.
When the on-chip memory is not sufficiently large, we have to
access some activations and/or weights multiple times. To reduce
off-chip traffic, an appropriate dataflow will be used and naturally
such a dataflow will access data in large blocks from off-chip in
order to minimize energy. The starting addresses (access handles)
of these blocks/containers can be easily identified statically and
accesses within each container can remain sequential. Generally,
for any given dataflow it will be possible to identify access handles
either statically for the weights or dynamically for the activations
and pre-record those in a small separate table.
Decompression/Compression: For clarity, let us first describe
decompression as a sequential process, assuming a group size of 8
and and a maximum datawidth of 8b as per the example of Figure 5a
The accelerator has its own memory request engine which is issuing
requests to the memory controller. The access engine maintains
a base register plus length for each input array (normally one for
activations and one for weights) plus a sequential address generator.
The number of arrays, their length, and how the rate of requests
should be interleaved is metadata that comes with the layer and is
not different than what would be used for an uncompressed model.
Starting from the beginning of an activation or weight array,
the decompressor reads the first 8+3=(Z, P) bits containing the
metadata for the first group of 8 values. Walking through the Z
one bit a time, the decompressor either outputs a zero or reads the
next P bits from memory expanding to the data width used on-chip.
Upon finishing with the current group, the decoder has arrived at
the header for the next group and the process repeats.
Rather than using a single, sequential decompressor we use multiple ones organized in a two level hierarchy as shown in Figure 5a
and detailed in Figure 6d. A single, first level decompressor (L1D)
identifies where groups start handing off the per group value decompression to the second level decompressors (L2Ds), one per on-chip
memory bank. Specifically, upon inspecting a group’s (Z, P) header
1 , the L1D immediately determines which lines in its buffer contain the payload for the current group. It hands-off decompression
to an L2D by 1) communicating (addr,Z, P) where addr is the starting address (line, and bit offset) for the group, and 2) copying the
relevant lines from its buffer to the L2D’s private buffer 2 . At
the same time, the L1D can then determine where the next groups
32
MICRO-52, October 12–16, 2019, Columbus, OH, USA Delmás Lascorz et al. 00000100 11110000 11000000 01010000 00000000 00000000 00001000 10000000 01000000
00000000
10100000
00000000
00000000
00000000
10000000
11100000
Group A Group B
(a) Two Groups of 8b Values 00001100 101
000001
111100
110000
010100
000010
100000
Z P
01011100
010
010
101
100
111
Z P
Group A Group B
(b) Encoded
000001 111100 110000 010
000010 100000
00001100 101
010 101
100 111
100 01011100 010
………………………………………………………………………
(c) Memory Layout
100 111
100 000010 100000 01011100 010 010 101
………………………………………………………………………
00001100 101 000001 111100 110000 010
100 111
100 000010 100000 01011100 010 010 101
………………………………………………………………………
From Memory
00001100 101 000001 111100 110000 010
100 000010 100000 01011100 010 010 101
L2 Decoder #1 L2 Decoder #2
………………………………………………………………………
8 8
32
8b 8b
8 x 8b
L1 Decoder
addr Z P
addr Z P addr Z P
2
3
4
AUL 1
(d) Decoding Architecture
Figure 6: An example how ShapeShifter encoding/decoding works. (a): Two groups of eight 8b values. In group A only 6 bits
are needed and in group B 3b are sufficient. (b): The encoded groups. Two metadata fields are introduced. Z is a bit vector that
identifies zero values. These zero values are not stored in memory. P represents the number of bits used to store the remaining
non-zero values. (c): The groups are stored in memory back-to-back in the order we expect them to be read. (d): Decoding using
two decoders. The encoded groups are fetched from off-chip memory in order and are placed first into a per memory interface
buffer. The example assumes a 32b wide external memory interface for clarity. The on-chip controller inspects the header of
the first group and copies the rows containing the first group to the internal buffer of the first decoder. The rows containing
the next group are then copied to the second decoder along with an index indicating where the group starts. Each decoder
then expands each assigned group one value at a time and places the result into its output register. In our example, the output
register contains eight 8b values. ShapeShifter is completely transparent to the on-chip execution engine.
Table 1: Average Per Layer widths with ShapeShifter.
Network Effective Activation Width Per Layer Reduction
AlexNet 6.52-4.7-3.48-3.23-2.68-2.19-2.59-2.35 41.09%
GoogLeNet 7.42-5.14-5.05-4.01-4.01-3.03-4.01-3.34-4.47
-4.26-4.26-3.86-3.34-5.14-3.99-3.96-3.96-4.2
-3.96-2.51-4.78-2.27-2.99-3.4-2.99-2.7-3.39-5.24
-3.36-3.41-3.36-2.66-4.18-4.08-4.08-3.01-3.18
-1.67-3.14-2.96-2.96-3.04-2.96-1.87-3.34-3.99
-2.3-2.11-3.1-2.5-4-3.85-2.31-1.79-1.65-1.33-2.29 44.34%
VGG_M 6.37-3.67-2.51-2.25-2.63-1.94-2.39-2.32 50.23%
VGG_S 5.39-3.71-3.67-2.25-2.44-1.52-2.43-3.06 46.35%
ResNet50 6.44-6.21-5.21-3.81-4.27-3.78-3.34-3.01-4.03
-3.08-3.78-4.09-3.14-3.35-3.45-4.02-2.86-3.15
-4.06-2.95-2.65-3.06-2.18-2.79-3.32-3.32-2.36
-3.27-3.16-1.97-1.98-3.06-2.43-1.96-3.01-2.24
-1.79-2.94-1.54-2.33-3.83-1.65-2.45-4.01-3.05
-1.73-2.27-2.55-1.93-1.83-2.36-1.74-1.65-3.26 53.46%
Yolo 4.99-6.03-5.29-5.19-4.19-6.36-4.3-5.18-2.66
-4.32-4.17-5.29-4.16-3.35-4.3-4.87-4.29-4.87
-3.98-4.85-3.09-4.29 33.52%
MobileNet 6.68-7.01-8.36-5.41-7.25-7.24-8.02-6.05-7.09
-5.94-7.71-4.77-7.84-6.44-7.3-7.12-9.5-6.15-8.54
-5.23-8.55-6.14-9.5-5.06-8.74-4.41-9.05-7.97 32.08%
Network Effective Weight width Per Layer Reduction
AlexNet 4.16-4.69-3.49-4.5-4.6-3.55-3.2-3.73 45.58%
GoogLeNet 5.58-6.86-6.1-4.91-5.68-4.75-3.89-4.18-5.12-5.28
-4.39-4.44-4.61-4.48-4.32-4.01-5.04-4.58-3.03
-3.88-5.01-4.57-3.68-4.95-2.87-4.31-4.82-4.8
-4.95-2.97-4.34-4.66-4.78-4.01-4.96-3.83-4.2
-4.76-3.36-4.27-4.15-3.68-4.67-4.56-3.31-3.33-3.59
-2.69-3.99-3.65-4.05-4.52-2.63-3.61-1.91-3.29-4.11 33.37%
VGG_M 4.57-3.91-4.31-3.99-3.98-3.79-2-3.17 35%
VGG_S 4.63-3.64-5.28-3.94-3.93-3.12-2.94-3.61 30.92%
ResNet50 5.6-4.9-6.53-3.97-4.43-3.62-3.37-5.24-4.55
-4.35-3.27-4.04-3.42-3.85-4.11-3.11-3.83-2.96
-2.07-3.5-3.39-4.39-3.93-3.92-3.68-2.99-3.41
-3.82-3.38-3.26-3.62-3.57-3.33-4.53-3.57-3.33
-3.49-3.75-3.3-3.6-3.83-3.31-3.63-4.11-3.66
-4.03-3.44-4.22-3.93-3.24-4.49-4.8-4.17-4.27 38.45%
Yolo 8-6.97-7-7.8-6.71-5.97-5.98-4.98-6.7-5.83
-5.74-6.81-6.7-3.99-5.98-4.98-4.98-4.98-4.79
-6.7-4.79-4.89 3.8%
MobileNet 3.88-3.3-4.91-2.11-3.96-2.76-3.68-1.95-3.39-2.53
-3.17-1.87-2.92-2.39-3.54-1.64-2.77-2.06-2.78
-2.06-2.84-1.66-2.84-2.77-3.43-2.11-3.05-1.68 68.68%
starts; the address update logic (AUL) block updates the fetch address for the level 1 decoder to point to the beginning of the next
compressed group in memory (+= ones(Z) × P), inspects the new
(Z, P) header 3 and sends the corresponding set of lines to another
L2D 4 .
Upon receiving a payload header from the L1D, an L2D expands
the values to 8b and does so serially, one value at a time. The use
of narrow read ports for the L1D and L2D buffer avoids the use of
wide crossbars or shuffling networks.
Finally, at the output of each layer, the output activations are
assembled in groups in each bank, and are encoded accordingly
using the width detected by the hardware of Figure 5c. The memory controller writes the resulting data containers as they become
available.
33
ShapeShifter: Enabling Fine-Grain Data Width Adaptation in Deep Learning MICRO-52, October 12–16, 2019, Columbus, OH, USA
4 REDUCING EXECUTION TIME
Here we discuss how ShapeShifter can be used to reduce execution
time with designs that exploit data width variability in time. As representative of these designs, this section explains how ShapeShifter
can benefit Stripes [29]. Figure 7a shows an example illustrating
what the goal is: with ShapeShifter, Stripes (SStripes) will adjust the
number of cycles it needs to process a group of activations (8b in
the example) to accommodate the value with the highest magnitude within the group. This is different than the original Stripes
where each group was processed in the same number of cycles; a
profile-derived precision dictated how many cycles to use.
Lets us first review how Stripes exploits per layer precisions
to improve performance. Figure 7b shows a Stripes serial innerproduct unit (SIP). The SIP multiplies and accumulates 16 (activation,
weight) pairs (16b values). The SIP processes the activations one
bit at time and thus its execution time varies according to the data
width chosen. Any data width up to 16b is supported. For example,
if the layer precision was determined to be 12 (light grey area), then
the SIP will take 12 cycles per group of 16 activations.
By introducing a precision detection unit prior to feeding the
activations to the SIP, it becomes possible to adjust the number
of cycles needed on a per group basis. For the specific group of
values, it will be possible to process the values in 8 cycles despite
the precision chosen for the layer being 12.
Figure 7c shows the overall Stripes architecture identifying the
ShapeShifter extensions. Stripes comprises several tiles. Each tile
processes 16 filters and 16 weights per filter (design parameters).
A local to the tile weight memory (WM) provides the weights.
Each tile contains 256 SIPs organized into 16 rows of 16 columns.
The SIPs along the same row process different windows of the
same output channel and reuse the same set of weights. A local
activation memory scratchpad (ASpad) provides the activations
via 256 wires, one per activation. The SIPs along the same column
process the same window across different output channels (filters).
This way weights and activations are reused spatially. Further reuse
is possible by tiling the computation. Partial sums are accumulated
within the SIPs and eventually shipped out to per tile partial sum
memories (PM). An activation memory (AM) broadcasts activations
to the tiles. A dispatcher per activation memory bank takes care of
transposing the values and communicating them bit-serially to the
tiles. The AM can be centralized or distributed along the tiles. The
dark boxes in Figure 7c identify the ShapeShifter extensions. A per
dispatcher width detection unit (Figure 5c) inspects the activation
group and communicates the data width needed via an “end of
group” (EOG) signal (not shown). Processing proceeds as in the
original Stripes starting from the least significant bit (which is
always the same for simplicity) but may terminate early according
to the EOG signal.
The second, optional extension to Stripes is the “Composer”
block. It contains a 2 × 36b adder every two rows. This allows the
results of two SIPs adjacent along the same column to be added prior
to writing them to the PM. It is to be used with SIPs that process
8b weights instead of the 16b weight SIPs originally used in Stripes
— the SIPs still support up 16b activations and 36b accumulators.
SIPs configured to process 8b weights are 1.8× smaller than SIPs
processing 16b weights. For those layers where profiling determines 000001
111100
110000
010100
000000
000000
000010
100000
010
000
101
000
000
000
100
111
Group A
Group B
Time
6 cycles
3 cycles
(a) Timing
+
1
1
16
16
W0
W1
16
16
36
A0
A15
0000 0000 0000 1001
0000 0001 0000 0011
(b) STRIPES Processing Element
SIP
(0,0)
SIP
(15,0)
SIP
(0,15)
SIP
(15,15)
16
16
1 1
PM
Dispatcher
AM A0 A15 A254 A255
W0
W15
W240
W255
WM
Composer
Width Detector
ASpad
1 1 1 1
A0 A15 A254 A255
TILE
(c) SStripes Tile
Figure 7: SStripes: Extending Stripes with ShapeShifter
that more than 8b are needed for the weights, we use two adjust
along the column SIPs to process the upper and the lower 8 bits of
the weights respectively. By adding the partial sums accumulated
in the two SIPs we can then correctly calculate the final output
activation for weights that need more than 8b in total. This is done,
after processing all groups and as the two values are brought out
of the SIP array prior to writing them to the PM.
Stripes exploits precisions in the time domain only. Bit Fusion
instead exploits precisions spatially up 8b, and then in time for
16b values. Bit Fusion is thus a spatial-first design and as presented
cannot adapt to precisions at a fine granularity. The composer block
allows SStripes to exploit precisions up to 8b in time, and then up to
16b spatially. Accordingly, the resulting design is time-first and thus
can adapt to per group precisions. Adapting a spatial-first design
to adjust precisions at a fine-granularity is left for future work.
Simplicity and low cost are major advantages of our proposal.
The area overhead of per group width adaptation is negligible, at
below 2% compared to the tile. SStripes does not affect accuracy,
and produces the same numerical result as Stripes.
34
MICRO-52, October 12–16, 2019, Columbus, OH, USA Delmás Lascorz et al.
Table 2: Neural Networks Studied. ⋆Pruned.
Model Dataset Application
int16
Alexnet [59] ImageNet [34] Classification
Alexnet-S⋆ [37] ImageNet [34] Classification
Alexnet-S2⋆ [33] ImageNet [34] Classification
Googlenet-S⋆ [37] ImageNet [34] Classification
Googlenet-S2⋆ [33] ImageNet [34] Classification
VGG_M [60] ImageNet [34] Classification
VGG_S [60] ImageNet [34] Classification
ResNet50 [61] ImageNet [34] Classification
Resnet50-S⋆ [33] ImageNet [34] Classification
Yolo [43] ImageNet [34] Real-Time Object Detection
MobileNet [39] ImageNet [34] Classification
int8: Tensorflow Quantized
Alexnet-S⋆ [37] ImageNet [34] Classification
Googlenet-S⋆ [37] ImageNet [34] Classification
Resnet50-S⋆ [33] ImageNet [34] Classification
MobileNet [39] ImageNet12 [34] Classification
int8: Range-Aware Quantized
Alexnet-S⋆ [37] ImageNet [34] Classification
Googlenet-S⋆ [37] ImageNet [34] Classification
BiLSTM [54] Flickr8k [53] Captioning
SegNet [41] CamVid [40] Segmentation
int16/int4 or int16/int5: Outlier-Aware Quantized
ResNet50 [61] ImageNet12 [34] Classification
MobileNet-v2 [62] ImageNet12 [34] Classification
5 EVALUATION
All accelerators were modeled using the same methodology for
consistency. A custom cycle-accurate simulator models execution
time. To estimate power and area, all designs were synthesized
with the Synopsys Design Compiler [63] for a TSMC 65nm library
(unfortunately, due to licensing constraints, presently 65nm is the
best technology that we have access to) and laid out with Cadence
Innovus. Circuit activity was captured with ModelSim and fed into
Innovus for power estimation. All designs operate at 1GHz and all
results reported are post-layout measurements. However, for Bit Fusion we use the power and area reported for 45nm technology [25].
The SRAM activation buffers, and the Activation and Weight memories were modeled using CACTI [64]. Three design corner libraries
were considered prior to layout. The typical case library was chosen
for layout since bit-serial designs are affected less by the worst-case
design corner. The on-chip memories were sized so that for most
layers it is possible to read each value from off-chip memory at most
once per layer using the method of Siu et al., [58]. Since off-chip
accesses dominate energy consumption appropriately sizing the
on-chip memory is a primary design consideration [65]. We use
4MB for activations and 4MB for weights for the 8b models and
double that for the 16b models. This configuration allows us to hold
a complete row of input activation windows on-chip at any given
point of time for most layers. Measurements are reported over all
layers of each network. Table 2 details the neural networks studied.
The outlier-aware models are used in Section 5.4 to demonstrate
that ShapeShifter compression naturally supports such models delivering the memory traffic reduction that is expected but without
specializing to this form of quantization.
5.1 ShapeShifter Memory Compression
Compression Rate: Figure 8a reports relative off-chip traffic
for four schemes: a) baseline, no compression (“Base”), b) compressing using per layer profile-derived widths (“Profile”) [22],
c) ShapeShifter compression (“ShapeShifter”), and d) runlength zero
compression (“Zero compression”) as used in Eyeriss and SCNN.
The compression group for Profile and ShapeShifter is 16 values adjacent along the depth dimension. For the 16b models, ShapeShifter
compression reduces traffic to less than 30% on average. The Tensorflow Quantized 8b networks see less benefit from our technique
because they quantize in such a way that dynamic precision reduction cannot be applied effectively. However, the Range-Aware
Quantized 8-bit networks see a 30-50% reduction in memory traffic.
ShapeShifter compression also achieves better traffic reduction than
zero compression.
Figure 8b reports relative off-chip traffic for non-profiled networks. While profiling is feasible it is not always possible, e.g., when
the test data set is not available. ShapeShifter memory compression
achieves a traffic reduction to less than 65% for the non-profiled 16-
bit networks, while for the non-profiled 8-bit networks the traffic
is reduced to 30%-50%. The Tensorflow Quantized 8-bit networks
require more traffic than the dynamic precision compressed RangeAware Quantized 8-bit networks without profiled precisions. For
the sparse Range-Aware Quantized 8-bit network AlexNetS the
off-chip traffic reduction matches that of zero compression and
outpeforms it for all other networks.
We demonstrate how by reducing off-chip memory traffic
ShapeShifter compression benefits DaDianNao*, Bit Fusion, and
SCNN improving overall performance and energy efficiency.
ShapeShifter compression resulted in energy savings for all offchip memories regardless of their speed rating. To demonstrate that
it can also considerably improve performance we report speedups
with a lower-end DDR4-2133 off-chip memory, a higher-end DDR4-
3200 off-chip memory, and a halfway between those, DDR4-2400
off-chip memory.
5.1.1 DaDianNao*. Figure 9a shows the performance of a DaDianNao variant, DaDianNao* using the three different off-chip compression schemes. ShapeShifter compression speedups execution
by 2.4× and 2.01× for the 16b and 8b Range-Aware quantized models respectively compared to no compression with DDR4-2133. As
expected, the average speedup is a modest 10% for the Tensorflow
quantized models. Speedups are more pronounced for the older
networks (AlexNetS and the VGG variants) as these have large
fully-connected layers that are memory-bound. These layers could
benefit much more from methods such as Deep Compression [66].
Even with higher bandwidth memory nodes ShapeShifter delivers benefits: ShapeShifter with DDR4-3200 achieves a 2.15× and
1.85× speedup over no compression for 16b and 8b Range-Aware
quantized models respectively.
As Figure 9b shows, ShapeShifter greatly reduces energy compared to no compression or profiled compression. Not only does it
naturally reduce the energy from memory accesses due to compression, but also reduces memory stalls saving on energy expended
by idle computation units. We observe energy savings up to 78%
compared to DaDianNao* without compression. SegNet is mainly
35
ShapeShifter: Enabling Fine-Grain Data Width Adaptation in Deep Learning MICRO-52, October 12–16, 2019, Columbus, OH, USA
0%
20%
40%
60%
80%
100%
120%
No compression Profiled ShapeShifter Zero compression
(a) Reduction for Profiled Networks
0%
20%
40%
60%
80%
100%
120%
No compression ShapeShifter Zero compression
(b) Reduction for Non-Profiled Networks
Figure 8: Offchip traffic reduction with compression schemes. The leftmost networks are 16b, the middle networks are 8b
Tensorflow Quantized and the rightmost networks are 8b Region-Aware Quantized.
compute-bound; therefore, memory compression offers little benefit.
5.1.2 Bit Fusion. BitFusion [25] targets precision profiled networks. It is composed of a systolic array of bit-level processing
elements that can be grouped to match layer precision. This allows
BitFusion to natively support per layer precisions of 8, 4, and 2
bits for both weights and activations. It also supports 16b models
by decomposing them into 8b multiplications which it performs
sequentially in time. Figures 9c and 9d report respectively performance and energy efficiency for BitFusion with each of the three
off-chip compression schemes and with different off-chip memory
technology nodes. The results demonstrate that BitFusion benefits
from ShapeShifter compression.
5.1.3 SCNN. Figure 10 shows the performance of SCNN with
ShapeShifter compression versus the run length encoding compression scheme used by SCNN for a set of 16b pruned neural networks –
SCNN targets pruned models. On average, SCNN with ShapeShifter
performs 9% faster and specifically for the newer ResNet50 network,
it performs 29% better. The Figure also reports energy efficiency;
SCNN with ShapeShifter is 9% more energy efficient than SCNN
with the baseline compression.
5.1.4 Layer Fusion. We evaluated the benefits of ShapeShifter compression in combination with layer fusion [67]. As shown in Figure
11, the combination of fusion and ShapeShifter compression provides considerable reductions in external memory bandwidth as
opposed to layer fusion alone.
5.2 ShapeShifter Stripes
This section compares SStripes to Stripes and Bit Fusion under an isoarea constraint. To compare with Stripes, both Stripes and SStripes
are configured to use the same total on-chip area. Stripes has 16 tiles
each containing 256 serial processing units whose worst-case peak
compute bandwidth is 4K multiplications per cycle. Stripes uses per
layer profile-based compression [22] as originally proposed [29].
The on-chip activation and weight memories are 4MB each modeled
as SRAMs for the 8b models and double that for the 16b models.
These are practical and appropriate memory sizes for server class
designs. By comparison the TPU1 chip has more than 20MB of
on-chip storage [68]. Both designs use a dual channel DDR4-3200
memory interface which proves sufficient for delivering the necessary bandwidth for keeping the units busy. SStripes is configured
with 8b weight/16b activation SIPs as per Section 4 plus a composer
column per tile. Each SStripes tile contains 16x28 SIPs as they are
smaller compared to the original Stripes SIPs.
Figure 12 reports the speedup of SStripes over Stripes and Figure 13 reports the compute-memory breakdown for SStripes. All
measurements are with a dual channel DDR4-3200 memory. For
the 16b models, SStripes boosts performance by 61% on average. For
the more recent ResNet50, the speedup is 88%. Older models such
as AlexNet and some of the VGG models are still memory-bound,
so most of the benefits come from memory compression on their
relatively large fully-connected layers. MobileNet 16b accelerates
the most at 2.35× since it is compute bound. For the Tensorflow
quantized 8b models, benefits are lower at 49% on average. This
is expected as this method forgoes opportunities to use precisions
less than 8b.
The execution time breakdown shows that networks that are
mostly convolutional such as SegNet, or GoogLeNet are computebound. Other networks such BiLSTM or those that have relatively
large fully-connected layers have higher memory bandwidth requirements. While 16b MobileNet benefits from a larger reduction
in memory traffic with ShapeShifter, the Tensorflow quantized MobileNet 8b waits for off-chip data 40% of the time. The benefits are
higher at 2.17× on average for the range-aware quantized 8b models.
BiLSTM benefits the most. This is due to the ShapeShifter memory
compression working particularly well for the fully-connected and
LSTM layers which are memory-bound. For SegNet which is predominantly compute-bound, SStripes is 93% faster where most of
the benefits are from per group compute adaptation.
Figure 12 also reports the energy efficiency of SStripes over
Stripes. Benefits generally follow the performance trends. Specifically, efficiency for the 16b networks increases from 1.7× up to
3.1× and for the range-aware quantized 8b models is on average
36
MICRO-52, October 12–16, 2019, Columbus, OH, USA Delmás Lascorz et al.
DDR4-2133
DDR4-2400
DDR4-3200
0
1
2
3
4
5
Speedup
ShapeShifter Profiled No compression
(a) DaDianNao*: Performance
DDR4-2133
DDR4-2400
DDR4-3200
0
0.2
0.4
0.6
0.8
1
Relative Energy 
No compression Profiled ShapeShifter
(b) DaDianNao*: Energy
DDR4-2133
DDR4-2400
DDR4-3200
0
1
2
3
4
5
Speedup
ShapeShifter Profiled No compression
(c) Bit Fusion: Performance
DDR4-2133
DDR4-2400
DDR4-3200
0
0.2
0.4
0.6
0.8
1
Relative Energy 
No compression Profiled ShapeShifter
(d) Bit Fusion: Energy
Figure 9: Performance and Energy Efficiency of with ShapeShifter compression with DDR4-2133 (left bar), DDR4-2400 (middle
bar), and DDR4-3200 (right bar) off-chip memory relative to DDR4-2133 no compression.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
0.2
0.4
0.6
0.8
1
1.2
1.4
AlexNetS AlexNetS2 GoogLeNetS GoogLeNetS2 ResNetS Geomean
Relative Energy
Speedup
ShapeShifter speedup ShapeShifter energy
Figure 10: Speedup and Relative Energy for SCNN with
ShapeShifter and DDR4-2133 off-chip memory normalised
to SCNN baseline compression.
2.93× higher. These benefits require a negligible overhead over
Stripes: the detection logic requires just 0.188mm2
, which is an
additional 1.93% (0.3%) in area plus 0.3% (0.06%) power compared
to the compute cores (cores+on-chip memory). Similarly, each of
the L1D and L2D decoders requires less than 0.02% area.
5.2.1 Comparison with Bit Fusion. We compare SStripes to Bit Fusion under iso-area constraints. Bit Fusion uses profiled, per layer
precisions for compute and memory compression. We conservatively scaled Stripes to 45nm following constant voltage scaling [69].
Under these constraints Bit Fusion is configured with 512 units per
tile. Figure 14 reports relative execution time for SStripes over Bit
0
1
2
3
4
5
6
7
AlexNet VGG_M VGG_S Geomean
Compression Rate
BW Savings Fused Layer BW Savings Fused Layer + SS
Figure 11: Layer Fusion: Compression ratios with and without ShapeShifter as opposed to using neither.
Fusion. Since Bit Fusion suffers from significant time overheads
when processing layers using more than 8b we report results for
8b networks only.
In general, SStripes is faster than Bit Fusion on these models, with
the benefits being more pronounced for the range-aware quantized
models, where SStripes is 3.75× faster on average. SStripes benefits
from per group width adaptation and delivers benefits even for
precisions that are not power of 2. However, we note that these
models are not quantized to better fit Bit Fusion. Unfortunately, it is
not currently possible to investigate performance using the models
used in the original Bit Fusion study. Only the precision profiles are
available for them whereas studying SStripes requires access to the
values too. Regardless, as we have seen, Bit Fusion benefits from
37
ShapeShifter: Enabling Fine-Grain Data Width Adaptation in Deep Learning MICRO-52, October 12–16, 2019, Columbus, OH, USA
0
1
2
3
0
0.5
1
1.5
2
2.5
3
Relative Energy Efficiency
Speedup
ShapeShifter Stripes Speedup ShapeShifter Stripes Energy
Figure 12: Speedup and Relative Energy Efficiency of
SStripes over Stripes. Iso-Area Comparison.
0%
20%
40%
60%
80%
100%
Compute Memory
Figure 13: ShapeShifter Compute-Memory breakdown.
ShapeShifter compression, and we expect these benefits to persist
even for models explicitly quantized for Bit Fusion. Moreover, future
work may investigate whether it is possible to modify Bit Fusion
to reconfigure its compute units at a finer in time granularity than
that of a layer and with the value content.
Figure 14 shows that SStripes is also more energy efficient than
Bit Fusion with the benefits closely following the performance
improvements. The benefits are the lowest for the Tensorflow quantized AlexNetS for two reasons: differences between profiled and
dynamic precisions are small, and the model is memory bound due
to the its relatively large fully-connected layers.
5.2.2 Smaller On-Chip Memory Configurations. Figure 15 shows
how SStripes performs with limited on-chip buffers. As on-chip
storage diminishes, performance becomes limited by off-chip bandwidth. SStripes provides benefit in both regimes.
5.3 ShapeShifter Loom
Dynamic width adaptation can also benefit Loom [70], an architecture that exploits width variability both in activations and weights.
Loom is faster and more energy efficient than Stripes for smaller
configurations. Loom uses bit-serial processing similar to Stripes.
Due to limited space we report summary results: for the 8b RA models, ShapeShifter Loom with 16b SIPs (no composition) performs
2.1× faster on average, and up to 2.3× for GoogLeNetS.
0
1
2
3
4
5
6
0
1
2
3
4
5
6
Relative Energy Efficiency
Speedup
ShapeShifter Stripes Speedup ShapeShifter Stripes Energy
Figure 14: Speedup and Relative Energy Efficiency of
SStripes over Bit Fusion. Iso-Area comparison.
0
2
4
6
8
10
12
14
GoogleNet VGG_M VGG_S ResNet50 MobileNet
Combined Speedup 
AlexNet
No compression 80KB
No compression 108KB
No compression 288KB
Profiled compression 80KB
Profiled compression 108KB
Profiled compression 288KB
ShapeShifter compression 80KB
ShapeShifter compression 108KB
ShapeShifter compression 288KB
Figure 15: Performance of Stripes using ShapeShifter for limited on-chip buffers, with DDR4-3200 external memory.
5.4 Outlier-Aware Quantization
Figure 16 shows the resulting compression of ShapeShifter when applied to Outlier-Aware Quantized networks [32]. MobileNetV2 [62]
and pruned ResNet50 [33] are quantized with 1% 16b outliers and 5b
and 4b common values respectively. The parameters were chosen
to maintain comparable TOP-1 accuracy to full precision [32]. The
figure reports memory traffic vs. storing all values in 16b [58].
We compare ShapeShifter with Outlier-Aware and “OutlierAware with zero skipping” (ZS) schemes. Outlier Aware stores
the common values in 4b or 5b, and the outliers in 32b, 16b for the
value and 16 for the position index (using a position index was
more space efficient than using an index per group of 16 values).
Furthermore, Outlier-Aware ZS compresses zero values to 1b by
allocating an additional bit for every non-outlier value. ShapeShifter
compression outperforms the Outlier-Aware scheme. In comparison to the Outlier Aware ZS scheme, our approach performs better
for the dense MobileNetV2 weights, while maintaining similar compression ratios for the sparse weights and activations.
These results demonstrate that ShapeShifter delivers off-chip
memory benefits with models quantized with outlier-aware quantization. Section 5.1 demonstrated that ShapeShifter does the same
with two other quantization schemes. Collectively, these demonstrate that ShapeShifter effectively delivers most of the memory
38
MICRO-52, October 12–16, 2019, Columbus, OH, USA Delmás Lascorz et al.
0.00x
0.05x
0.10x
0.15x
0.20x
0.25x
0.30x
0.35x
Outlier
Aware
Outlier
Aware ZS
ShapeShifter Outlier
Aware
Outlier
Aware ZS
ShapeShifter
ResNet
Memory Footprint
Activations Weights
MobileNet V2
Figure 16: Performance of SStripes compression with
Outlier-Aware quantized weights and activations.
traffic and footprint reduction benefits possible with various quantization methods without mandating that all networks use a specific
method.
Demonstrating compute performance benefits with outlieraware quantized networks and a comparison with accelerator designs specifically tailored for outlier-aware quantized models is left
for future work. If all networks of interest can be quantized with
the outlier-aware technique, a more specialized design such as that
of Park et al., [71] may be preferable.
6 RELATED WORK
The Efficient Inference Engine (EIE) uses weight pruning and sharing,
zero activation elimination, and network retraining to drastically
reduce the weight storage and communication when processing
fully-connected layers [72]. This is an aggressive form of weight
quantization and compression and where possible will greatly outperform ShapeShifter for weights. However, ShapeShifter is complementary to the Deep Compression method used by EIE. ShapeShifter
benefits activations also, works for all layers, and exploits a phenomenon that is mostly orthogonal to those EIE exploits. Moreover,
in recent CNNs the size and number of fully-connected layer has
been drastically decreased compared to early CNN models. While
codebook-based compression methods such as Deep Compression
can be applied to convolutional layers, these require retraining and
larger codebooks [73].
Datatypes such as Intel’s Flexpoint [27] or Tensorflow’s
bfloat16 [28] are lower precision alternatives to 32-bit floating
point. They provide a wider range than fixed-point representations.
While they target primarily training there are models, for example in natural language processing or recommendation systems,
that achieve their best accuracy only with floating-point arithmetic.
These floating-point datatypes are statically sized and require 7, 8
or 16 bits of mantissa, and 8 bits of exponent regardless of value
magnitude. The models will still exhibit a lopsided value distribution and will natural exhibit variable precision requirements per
layer. Accordingly, it may be possible to apply ShapeShifter to the
mantissa and exponent to adjust the least significant bits used after
profile has been used to trim the precision needed per layer.
Quantization to lower precisions reduces memory and computation cost in neural networks. However, quantization is not without
its drawbacks. Although a fixed precision may be suitable for certain networks, using a fixed quantization scheme may not be optimal/sufficient for all problems, e.g., those using RNNs [3] or computational imaging tasks performing per-pixel prediction, e.g., [48, 49]
which process raw sensor data of 12b or more, or even some image
classification tasks and speech processing [74]. Therefore, flexible precision support is presently important to generalize across
different applications. While quantization attenuates some of the
expected benefits with ShapeShifter the underlying phenomenon
that ShapeShifter exploits persists: there will be few high values.
This is the reason why it benefits even 8b quantized networks. Further, SStripes offers benefits even for those networks that can be
quantized further, even for non powers of two precisions, e.g., [75]
without requiring all networks to be quantized to obtain benefits.
This applies also to models quantized to binary weights [76] or
activations.
The Compressing DMA engine also uses vector-based zero compression [77] and has shown that it is very effective for reducing
off-chip traffic and storage during training.
Tartan extends Stripes so it can take advantage of per layer
weight precisions [78] so that it can speed up fully-connected layers
too. ShapeShifter is directly compatible with Tartan and would
increase benefits by adjusting precisions per weight group instead.
Due to limited space an evaluation of this design is left for future
work.
Diffy improves upon ShapeShifter by using it to encode activations as deltas [79]. Diffy exploits the spatial value correlation
found in the activation values of neural networks implementing
computational imaging tasks.
The idea of adapting operand precision for applications exhibiting algorithmic fault tolerance has been proposed by Nam Sung
et al. [80], transmitting a fixed number of shift amounts (1-valued
bits) per value at the cost of larger accuracy loss.
7 CONCLUSION
Per group precision adaptation opens up several directions for future work including how to combine with other accelerator engines,
how it can boost the effectiveness of algorithms for pruning or for
precision reduction or quantization of weights and of activations,
and whether it can accelerate training. The accelerators we proposed do not require any changes to the input network to deliver
benefits. However, they do reward advances in precision reduction
including quantization and thus if deployed will provide incentive
for further innovation.
