ABSTRACT
Convolutional neural networks (CNNs) have recently demonstrated
superior quality for computational imaging applications. Therefore,
they have great potential to revolutionize the image pipelines on
cameras and displays. However, it is difficult for conventional CNN
accelerators to support ultra-high-resolution videos at the edge due
to their considerable DRAM bandwidth and power consumption.
Therefore, finding a further memory- and computation-efficient
microarchitecture is crucial to speed up this coming revolution.
In this paper, we approach this goal by considering the inference
flow, network model, instruction set, and processor design jointly
to optimize hardware performance and image quality. We apply
a block-based inference flow which can eliminate all the DRAM
bandwidth for feature maps and accordingly propose a hardwareoriented network model, ERNet, to optimize image quality based on
hardware constraints. Then we devise a coarse-grained instruction
set architecture, FBISA, to support power-hungry convolution by
massive parallelism. Finally, we implement an embedded processor—
eCNN—which accommodates to ERNet and FBISA with a flexible
processing architecture. Layout results show that it can support
high-quality ERNets for super-resolution and denoising at up to 4K
Ultra-HD 30 fps while using only DDR-400 and consuming 6.94W
on average. By comparison, the state-of-the-art Diffy uses dualchannel DDR3-2133 and consumes 54.3W to support lower-quality
VDSR at Full HD 30 fps. Lastly, we will also present application
examples of high-performance style transfer and object recognition
to demonstrate the flexibility of eCNN.
CCS CONCEPTS
• Computer systems organization → Embedded hardware; •
Computing methodologies → Machine learning.
KEYWORDS
convolutional neural network, computational imaging, edge inference, hardware accelerator, ultra-high-definition
1 INTRODUCTION
Convolutional neural networks (CNNs) recently draw a lot of attention for their great success in the fields of computer vision and
computational imaging. Their hardware accelerators also become
an emerging need to enable edge applications. The performance
of pixel throughput and inference quality is determined jointly by
model structure, processor architecture, and inference flow.
The CNN model structure has evolved mainly for object recognition, e.g. from shallow AlexNet [33] to deep VGGNet with small
filters [54] and ResNet with residual connections [21]. Several
hardware-oriented variants, like depth-wise convolution [23] and
feature squeezing [26, 52], were also proposed to reduce model
complexity for edge inference. On the other hand, CNNs have also
shown dominant performance for computational imaging applications [39], such as image denoising [18, 62, 63], super-resolution
[16, 32, 35, 36], image deblurring [9, 46], and view synthesis [31, 57].
They even can provide novel applications which are hard to achieve
using traditional methods, like style transfer [29, 66], DSLR-quality
conversion [27], and algorithm mimicking [10]. However, there
were seldom discussions on hardware-oriented models for computational imaging despite their potential to enable next-generation
image pipelines on edge devices.
On the other hand, several hardware accelerators have been
proposed for deep neural networks. For example, DaDianNao [11],
Cambricon-ACC [37], TPU [30], and DNPU [53] were designed for
general-purpose inference. In contrast, ShiDianNao [17], Eyeriss
[12] , and Morph [22] were dedicatedly optimized for classification CNNs. The weight sparsity in these CNNs has been used to
reduce computation complexity [3, 48, 64, 65], and the bit sparsity
182
MICRO-52, October 12–16, 2019, Columbus, OH, USA C.-T. Huang et al.
in activations was deployed in [2]. Another approach for saving
complexity is to use low-precision computation, such as dynamic
fixed-point format [45, 49] and even binary networks [5]. However,
these accelerators are not optimized for computational imaging and
also not for high-resolution videos, especially in terms of DRAM
bandwidth and computing capability. Recently, Diffy [41] attacked
this problem by utilizing the bit sparsity in activation differences to
reduce DRAM access and computing power. But many Diffy tiles
and high-end DRAM settings are still required for Full-HD videos.
Finally, the inference flow of a given CNN model determines
the data reuse scheme for an accelerator and thus its memory
access efficiency. A systematic approach to partition CNNs into
computation sub-blocks was introduced in [59], and several energyefficient dataflows were analyzed in [12]. In particular, a line-based
flow was considered for layer fusion in [4] which avoids external
traffics for feature maps by applying pyramid inference on moving
blocks. For the overlapped features between blocks, a reuse scheme
was chosen for fusing up to five CNN layers. However, the line
buffer size will increase linearly with model depth, image width,
and channel number. For example, 9.3MB of SRAM will be required
for supporting VDSR [32] in Full HD resolution. Similar trade-offs
between the on-chip SRAM size and off-chip DRAM bandwidth
have also been widely studied for image processing applications,
such as motion estimation [56] and discrete wavelet transform [24].
In this work, we aim to enable high-quality edge inference at up
to 4K Ultra-HD (UHD) 30 fps for computational imaging tasks. In
particular, we target low-end DRAM settings for cost-effective and
power-efficient integration on embedded devices. We found this
challenging goal is hard to achieve by directly accelerating state-ofthe-art models which mostly have wide features and deep layers.
Instead, we expand our design space to consider the inference flow,
model structure, instruction set, and processor design jointly for
optimizing both hardware performance and image quality.
We first propose a block-based truncated-pyramid inference flow
which can eliminate all the DRAM bandwidth for feature maps
by storing them in on-chip block buffers. To avoid huge on-chip
storage, we choose to recompute the overlapped results between
neighboring blocks. The block buffer size is proportional to model
width, and the recomputation overhead almost increases quadratically with model depth. As a result, these two factors defy the
rule of thumb that simply adds more features and more layers to
enhance model quality. Instead, we propose a novel ERNet model
to optimize CNNs under these hardware constraints. Then we construct a feature-block instruction set architecture (FBISA) to support
highly-parallel convolution. It specifies block-buffer-level operations in the fashion of Single Instruction, Multiple Data (SIMD). In
addition, it provides flexibility for programmers and compilers to
optimize the computing flow based on different constraints. Finally,
we implement an embedded CNN processor—eCNN—which flexibly
accommodates to ERNet and FBISA with highly-parallel filters and
locally-distributed parameters.
In summary, the main contributions and findings of this work
are:
• We propose a block-based flow to enable high-resolution
inference with low DRAM bandwidth and also analyze its
computation and bandwidth overheads. (Section 3)
• We propose a hardware-aware ERNet to optimize image
quality based on hardware constraints and also build training
procedures for model optimization and dynamic fixed-point
precision. (Section 4)
• We devise a coarse-grained FBISA with parallel parameter
bitstreams to provide massive computation parallelism efficiently and flexibly. (Section 5)
• We design an embedded processor, eCNN, to support FBISA
with highly-parallel convolution using 81,920 multipliers.
(Section 6)
• We train ERNets for image super-resolution (SR) and denoising with 8-bit precision. In particular, the quality for
four-times SR can outperform VDSR [32] by 0.57 dB and 0.44
dB in PSNR when eCNN delivers Full HD and 4K UHD 30
fps respectively. (Section 7.1)
• Layout results show that eCNN can achieve 41 TOPS (tera
operations per second) on 40 nm technology. It supports highquality ERNets at up to 4K UHD 30 fps while consuming
6.94W and using DDR-400. By comparison, Diffy consumes
54.3W and uses dual-channel DDR3-2133 for VDSR at Full
HD 30 fps. (Section 7.2)
• Computer-vision tasks can also be well supported by FBISAcompatible models, such as style transfer and object recognition. (Section 7.3)
2 MOTIVATION
Recent research on CNN accelerators mainly focuses on object
recognition/detection networks. Therefore, two specific features
for computational imaging networks are not considered for optimization: 1) the spatial resolution of feature maps is not aggressively
downsampled and 2) the models are not very sparse. The former results in a dramatically-high amount of memory bandwidth, and the
latter introduces an extremely-high demand of computing power.
The aggressive downsampling for object recognition is as shown
in Fig. 1(a). It can extract high-level features and also reduce the data
amount for feature maps (volume of cuboids) in deeper layers. Most
of conventional accelerators can thus apply a frame-based inference
flow to perform convolution layer-by-layer with limited DRAM
bandwidth. However, this flow will induce a huge amount of DRAM
bandwidth for computational imaging networks. It is because highresolution feature maps are required to generate texture details and
only mild downsampling is allowed [39]. Take the plain network
without downsampling in Fig. 1(b) as an example, its corresponding
DRAM bandwidth for feature maps, except input and output images,
can be derived as
H ×W × C × (D − 1) × f R × L × 2, (1)
where H stands for image height, W for image width, C for the
number of feature channels (model width), D for model depth, f R
for frame rate, L for the bit length of each feature, and the factor
2 for writing per-layer feature maps into DRAM and then loading
back for the next layer. Accordingly, the 20-layer 64-channel VDSR
will require 303 GB/s of memory bandwidth for Full HD 30 fps when
using 16-bit features. Even with the state-of-the-art compression,
Diffy still requires dual-channel DDR3-2133 (34 GB/s) to meet this
Full-HD specification. When the resolution is raised to 4K UHD, the
183
eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge Inference MICRO-52, October 12–16, 2019, Columbus, OH, USA
(a) (b)
Figure 1: CNN examples in per-layer feature maps for (a) object recognition and (b) computational imaging.
(a) (b)
Figure 2: Quality degradation of computational imaging networks for using sparsity techniques. (a) Weight pruning for
DnERNet-B16R1N0. (b) Depth-wise convolution in residual
blocks for EDSR-baseline (SR×2 and SR×4).
DRAM bandwidth will be four times larger and thus unaffordable
for small-form-factor and power-limited edge devices.
On the other hand, the sparsity of object recognition models
has been deployed to develop many complexity-saving techniques,
such as weight pruning [20] and depth-wise convolution [23]. However, computational imaging networks rely on the variety of parameters to extract local features and generate fine textures. Their
image quality is highly related to the model size, so the sparsity
techniques could result in significant degradation. Two such examples are shown in Fig. 2. One is pruning weights for a denoising
DnERNet model (Section 7). When pruning 75% of weights away,
its PSNR gain over the benchmark CBM3D [14] drops by 0.2-0.4 dB
for two datasets (CBSD68 [43] and Set5 [7]) and could even become
negative. Another example is using depth-wise convolution for
EDSR-baseline models [36]. Although 52-75% of complexity can be
saved, the quality drop is 0.3-1.2 dB for four datasets (Set5, Set14
[61], BSD100 [43], and Urban100 [25]) and thus makes the saving unjustified. Therefore, we need to confront the computation demand
for computational imaging CNNs. Furthermore, high-resolution
image generation will make this demand more challenging. For
example, VDSR already demands as high as 83 TOPS for Full HD
real-time applications and will require 332 TOPS for 4K UHD.
The issues of huge DRAM bandwidth and computing power
motivate us to find a novel approach for ultra-high-resolution CNN
acceleration. In the following, we will propose the block-based
inference flow and the hardware-oriented ERNet to resolve the
memory issue. And the computation issue will be addressed by the
coarse-grained FBISA and the corresponding highly-parallel eCNN
processor.
Figure 3: Proposed block-based inference flow.
Figure 4: Truncated-pyramid inference for a plain CNN.
3 BLOCK-BASED INFERENCE FLOW
The proposed block-based flow is shown in Fig. 3. An input image
is partitioned into several blocks which can be processed independently, and all the output blocks are then stitched to form a final
output image. In contrast to layer fusion [4], we recompute blockoverlapped features to avoid huge on-chip SRAM. However, this
induces additional bandwidth and computation. To reduce these
overheads, we then propose the truncated-pyramid inference (output block larger than one pixel) as detailed in the following.
To analyze the overheads of this inference flow, we use the plain
network in Fig. 4 as an example. It consists of only CONV3×3
layers, and the receptive field is thus linked directly to the depth
D. As the convolution goes to deeper (upper) layers, the effective
region will become smaller as a truncated pyramid. For example,
an xi × xi
input block will generate an xo × xo output block where
xo = xi − 2D. When the depth (receptive field) is increased, more
input blocks and thus more DRAM bandwidth will be required
because fewer output pixels are generated for the same input block
size. This bandwidth overhead can be evaluated by a normalized
bandwidth ratio (NBR) which is equal to the bandwidth for all input
and output blocks over that for an output image:
N BR =
3x
2
o + 3x
2
i
3x
2
o
= 1 +
1
(1 − 2β)
2
, (2)
where RGB images are considered and β is a depth-input ratio,
D/xi
. Similarly, there are also computation overheads for the recomputed features among neighboring blocks. It can be evaluated
by a normalized computation ratio (NCR) which represents the
computation complexity of this block-based flow over that of the
frame-based one (intrinsic):
NCR =
volume of truncated pyramid
volume of center cuboid =
1
3
+
2
3
1 − β
(1 − 2β)
2
, (3)
where the volume (in Fig. 4) is proportional to the amount of features and therefore that of computing operations.
184
MICRO-52, October 12–16, 2019, Columbus, OH, USA C.-T. Huang et al.
(a) (b)
Figure 5: Bandwidth and computation overheads for
truncated-pyramid inference. (a) NBR and NCR versus
depth-input ratio for the plain network. (b) NCR versus
block buffer size for 20-layer VDSR and 37-layer SRResNet.
(L = 16)
These two ratios both grow rapidly with respect to the depthinput ratio β as shown in Fig. 5(a), and they eventually go to infinity
when β = 0.5 for xo = 0, i.e. no valid output pixels. If β is not too
close to 0.5, the induced bandwidth overhead is generally acceptable
compared to the bandwidth we save. For example, the NBR is 26×
for a large β = 0.4 while the bandwidth overhead of the frame-based
flow can be derived as 2C(D−1)
3
based on (1) and is as high as 811×
for VDSR. However, the computation overhead does become the
main side effect of the block-based flow. We will spend 90% of the
computing power for feature recomputation when β approaches 0.4.
To avoid this situation, we will need to adopt larger block buffers
to reduce β for deeper networks.
The evaluation of the above plain example can be extended to
more complicated models, and the conclusions on bandwidth and
computation overheads are similar. Most of state-of-the-art networks have deep layers of 3×3 (or larger) filters and wide channels
of feature maps, e.g. C ≥ 64. Therefore, they will either require
huge block buffers of size CLX2
i
to reduce NCR or suffer significant
computation overheads for using small buffers to save area. Note
that usually more than one block buffer will be required for switching between input and output layers, which makes the area cost
more severe.
Fig. 5(b) shows this trade-off between the NCR and block buffer
size for VDSR and also a state-of-the-art SRResNet [35] which outperforms VDSR by 0.6 dB [36]. The NCR for the 20-layer VDSR
is well controlled as 2× using 1MB block buffers. But the 37-layer
SRResNet needs around 2MB to have a similar NCR. Using smaller
block buffers to save area for SRResNet will make the NCR skyrocket quickly. Therefore, with the block-based flow it is difficult
to have a low NCR and use small buffers simultaneously for deep
high-quality networks. In the following, we will achieve this goal
by considering these hardware constraints as early as model construction and accordingly introduce the ERNet.
4 ERNet
We will first introduce the basic building modules of ERNet and
then present a procedure for model optimization. The quantization
method we adopt for dynamic fixed-point precision will also be
discussed.
4.1 Model Structure
Consider a thin network which can use small block buffers. To
increase its capacity without enlarging the NCR and buffer area,
we explore another direction of model construction by temporarily
expanding the model width. This is achieved by the ERModule
shown in Fig. 6(a). It uses a CONV3×3 layer to expand the model
width by Rm times and a following CONV1×1 to reduce it back. A
residual connection is added for robust training. All the operations
are performed internally without accessing to block buffers. Therefore, we can pump complexity into ERModule to improve image
quality with the same block buffer size and model depth.
We only consider integer expansion ratios for Rm to guarantee
high hardware utilization. To increase model flexibility, we further
construct a larger building block by connecting B ERModules as
shown in Fig. 6(b). The first N modules can be assigned an incremented Rm = R + 1 to make the overall expansion ratio RE
as a fraction R
N
B
. Accordingly, we now have two model hyperparameters to build networks: B for increasing depth and RE for
pumping complexity.
Fig. 7 shows a model example, SR4ERNet, for performing fourtimes SR. It basically replaces the residual blocks in SRResNet [35]
or EDSR-baseline [36] by ERModules. It starts with small images of
1/4 size in width and height and uses two pixel-shuffle upsamplers
to restore full resolution. In addition, we reduce the channel number
from 64 to 32 for saving area for block buffers.
4.2 Model Optimization
The major hardware constraint considered here is the overall computation complexity, i.e. NCR×(intrinsic complexity), since the
bandwidth overhead is usually small. Each complexity target will
correspond to a real-time throughput, and we aim to optimize image quality under such constraints. Our model selection procedure
can be illustrated using SR4ERNet as shown in Fig. 8. We assume
the size of input blocks is 128×128 and consider three computation
constraints: 164, 328, and 655 KOP/pixel (thousand operations per
output pixel).
First of all, we derive the largest possible expansion ratio RE
for each module number B under each constraint, and we choose
RE ≤ 4 as a system upper bound. As shown at the top of Fig. 8,
RE will decrease quickly as the model depth grows with B because
of the fast-increasing NCR. In the case of 655 KOP/pixel, NCR can
be as high as 2.8-5.9×, and the corresponding intrinsic complexity
is as low as 223-107 KOP/pixel. Note that deeper networks do not
necessarily perform better now because of their lower intrinsic
complexity. Then we scan all of these candidate models with a
lightweight training setting, e.g. using smaller patches and fewer
mini-batches. After that, we test their image quality using validation
datasets and pick the best model for each constraint as shown at
the bottom of Fig. 8. Finally, we will further polish the best models
by retraining them with a full setting. In this example, the highestquality SR4ERNet-B34R4N0 can even outperform SRResNet by 0.04
dB using a thinner and less-complex network.
4.3 Dynamic Fixed-Point Precision
We further quantize the polished models for saving computation
logics and on-chip memory. The multiplications and block buffers
185
eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge Inference MICRO-52, October 12–16, 2019, Columbus, OH, USA
(a) (b)
Figure 6: Basic building modules of ERNet: (a) ERModule and (b) connected ERModules (RE = R
N
B
).
Figure 7: SR4ERNet for four-times SR.
Figure 8: Model scanning of SR4ERNet for three computation constraints with xi = 128.
Figure 9: Examples of 8-bit Q-formats: Qn and UQn.
are both considered in 8-bit precision while the internal partial
sums are accumulated in full precision to preserve quality. We
adopt the fixed-point Q-format for decimals as illustrated in Fig.
9. Qn and UQn stand for signed and unsigned values respectively,
and n is the fractional position of the last effective bit. We apply
dynamic fixed-point precision to optimize image quality, so each
convolution layer has its own Q-formats for weights, biases, and
feature outputs, respectively. We then build a two-stage procedure,
quantization and fine-tuning, based on [6, 19, 49].
The quantization stage is to determine the best fractional precision nˆ of each Q-format. With a collection Ω of the corresponding
floating-point values, we can use either L1-norm [49] or L2-norm
[6] errors for the optimization:
nˆl = arg min
n
Õ
x ∈Ω
|x − Qn(x)|l
, l ∈ {1, 2}, (4)
where the quantization functionQn(·) performs clipping and rounding for precision n. The value distributions of parameters are directly derived from the floating-point model, and those of feature
maps are collected by inferencing on the training dataset. Since this
8-bit quantization induces up to 3.69 dB of PSNR loss for denoising
and SR, we then use the fine-tuning method in [19] to refine these
quantized parameters. For calculating gradients more accurately,
we add clipped ReLU (rectified linear unit) functions to the model
for the clipping behavior ofQn(·). As a result, the fixed-point ERNet
has only 0.08 dB of PSNR degradation on average.
5 FBISA
We design the SIMD instruction set, FBISA, to support the truncatedpyramid inference for fully convolutional networks. To increase
its flexibility, we also include a zero-padded inference type and
a variant of upsamplers and downsamplers. FBISA provides massive parallelism by coarse-grained instructions between feature
blocks and internal accessing of parameter memories which will
be introduced sequentially.
5.1 Instruction Set
Fig. 10 shows the instruction format. An opcode can specify a
convolution task with specific attributes, e.g. inference type and
block size. There are two kinds of operands for features and parameters, respectively, and they also have their own attributes, in
particular for Q-formats. For the feature operands, there are two
mandatory types to inform the source (src) and destination (dst) of
the convolution. In addition, two supplementary ones (srcS/dstS) are
designed to support feature accumulation among instructions, such
as skip/residual connection or partial sums. Finally, the parameter
operand specifies where to access the corresponding weights and
biases in parameter memories for the opcode. For these operands,
we use named expressions, instead of the conventional ordered
ones, to improve readability.
Table 1 provides an overview of the instruction set. The smallest
computing task in FBISA is called a leaf-module, and it performs a
32ch-to-32ch CONV3×3 filter on one feature block. Each opcode
186
MICRO-52, October 12–16, 2019, Columbus, OH, USA C.-T. Huang et al.
Figure 10: FBISA instruction format.
Table 1: FBISA instruction overview.
can contain up to four leaf-modules based on its attribute. The
opcodes mainly differ on their usage purposes and thus on the
post-processing of outputs. For example, the opcode UPX2 shuffles pixels for spatial upsampling while DNX2 performs strided- or
max-pooling for downsampling. In particular, ER is devised specifically for ERModule and its leaf-module has an additional 32ch
CONV1×1 for feature reduction. If wider filters are required for
CONV3×3, they can be constructed by using 32ch-based opcodes
and accumulating partial sums via srcS.
Regarding the feature operands, we apply two strategies to provide efficient data movement for highly-parallel convolution. First,
they are specified on the basis of block buffers (BBs), instead of
conventional small registers or vectors. Therefore, the internal partial sums for one instruction can be accumulated inside hardwareoptimized datapaths without accessing to large SRAM or DRAM
which is mostly bandwidth-limited and power-hungry. Another
advantage is that we can have small-sized programs and avoid complex compilers. For example, the high-quality SR4ERNet-B34R4N0
uses only 45 lines of instructions.
The second strategy is not using conventional load-store instructions for external feature reading and writing. Instead, we devise
operands DI and DO as virtual block buffers for data input and
output respectively. They can be implemented by FIFO interfaces
and stream data in the same way as normal block buffers. Therefore,
the processor pipeline can be fully optimized for a computationonly instruction set. This strategy also decouples FBISA from the
data structure in the main memory for better system integration
portability.
Figure 11: Weight bitstream for one filter position.
5.2 Parameter Format
The data movement of parameters is also of paramount importance
for CNN acceleration. To avoid retransmitting parameters for each
block, we keep them in internal parameter memories for reuse.
This is feasible thanks to the small-sized computational imaging
networks. For example, the numbers of parameters in VDSR and
SRResNet are 651K and 1479K respectively while it is 11M for
ResNet-18 [21].
In FBISA, we split the filter weights into 20 bistreams to enable
parallel loading and distribution of them in the processor: 18 for
CONV3×3 and two for CONV1×1. Each spatial filter position corresponds to two bitstreams for its first and second halves of output
channels in leaf-modules. Fig. 11 shows the format of one such
bitstream. The weights are compressed to increase the supported
model size, and we adopt the DC Huffman coding in JPEG [28].
This simple coding algorithm enables fast and parallel decoding
with small hardware overheads. We also found that the weights
are mostly uncorrelated, so differential encoding is unnecessary.
On the other hand, the filter biases are gathered in another one
bistream and compressed in the same way.
A decoding restart mechanism is further devised to enable parameter reuse between different instructions. In this case, a byte-aligned
address referred to the bias bitstream should be specified as the
restart attribute in the parameter operand. The Huffman table will
be placed first and followed by the encoded bitstream. For the 20
weight bitstreams, their restart addresses will be synchronized to
8× of the restart attribute. It is because each of them contains 512
coefficients for one leaf-module but the bias bistream only has 64.
Finally, the 21 bistreams are synchronized for each restart segment
by padding shorter ones. Regarding compression efficiency, we
found that one Huffman table for each restart segment in each
bitstream is sufficient since the 8-bit quantized parameters have
similar distributions. As a result, the compression ratio is around
1.1-1.5× for denoising and SR ERNets.
6 eCNN
This embedded processor is implemented to support FBISA with
highly-parallel convolution for high-performance and also powerefficient computing. In the following, we will first present its system
architecture for top control and embedded integration. Then we will
187
eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge Inference MICRO-52, October 12–16, 2019, Columbus, OH, USA
Figure 12: Processing flow for system control and eCNN.
introduce its two main functional units for distributing parameters
and performing convolution, respectively.
6.1 System Architecture
6.1.1 Processing Flow. For one target model, its program and parameters are loaded into eCNN only once. Then the inference for
each image is performed based on a model hierarchy: sub-model(s),
instructions, and leaf-modules. On the other head, the image is
also processed with a pixel-grouping hierarchy: blocks and tiles
(4×2 32ch features). These two hierarchies are interleaved to form
a flexible processing flow shown in Fig. 12. In particular, a deep
model can be partitioned into few shallower sub-models to reduce
computation overheads. However, the intermediate features between them may increase DRAM bandwidth sharply, which is a
performance tradeoff.
The eCNN is designed for embedded integration, e.g. controlled
by a main processor and connected to a DMA controller via FIFO
interfaces. These system transactions can be handled on a block
basis without inducing heavy system burdens. The eCNN then
accelerates the computation for each block in an instruction-byinstruction fashion. For each instruction, it calculates one 32ch
leaf-module for a 4×2-tile in one cycle. And, for each tile, the leafmodules of this same instruction are calculated consecutively to
accumulate their partial sums on-the-fly without precision loss and
SRAM access. After all of the specified 4×2-tiles are processed, the
eCNN will repeat similar acceleration for the next instruction.
6.1.2 Block Diagram. Fig. 13 shows the system block diagram to
implement the above-mentioned processing flow. It consists of two
functional units: information decode unit (IDU) and CNN inference
unit (CIU). The IDU is responsible to decode instructions and parameters, and the CIU computes the corresponding convolution.
To enable highly-parallel computing, we deploy a massive amount
of multipliers in two convolution engines in CIU: LCONV3×3 and
LCONV1×1. They perform the 32ch CONV3×3 and CONV1×1, respectively, in each leaf-module, and the latter is used for ERModule.
We keep all of the model parameters in IDU to avoid excessive
external bandwidth for parameter retransmission. However, the
multipliers need to access up to 10,240 weights for each leaf-module
in one single cycle. The throughput is much higher than the affordable bandwidth of the parameter memories. Therefore, we devise an
instruction pipelining scheme to distribute parameters efficiently.
As a result, the IDU has a whole pipeline stage to progressively
decode the parameters for one instruction. Meanwhile, they are
sequentially sent to the locally-distributed registers inside the multipliers of CIU and will be used for the convolution in the next
pipeline stage. In the following, we will introduce the implementation details for the IDU and CIU.
6.2 Information Decode Unit (IDU)
For each instruction, the IDU will first decode its opcode and
operands and then trigger a parameter decompression procedure.
The 21 parameter bitstreams mentioned in Section 5.2 are stored in
21 corresponding memories and decoded by 21 parallel decoders.
For each leaf-module, a weight decoder is responsible to decode
512 weights while the bias one generates at most 64 biases. All the
parameters follow a ping-pong distribution scheme between the
IDU and CIU.
Fig. 14 shows the distribution scheme for the weights in the first
half of output channels in CONV3×3, and the other cases are similar. Nine decoders are deployed for nine filter positions, and each
one decodes two weights in one cycle for two input channels. The
decoded weights are then distributed through a two-stage network:
the first and second stages are for output and input channels respectively. In particular, there are 16×32 local register files (Weight
2D 3×3) accompanied with their corresponding 2D filters (Filter
2D 3×3). Each register file will keep the decoded parameters in a
ping-pong fashion for the CIU convolution in the next instruction
pipeline. Also, it can switch between four leaf-modules for the consecutive computation in one instruction. In most cases, the IDU
decodes one leaf-module in 256 cycles and completes one instruction faster than the CIU of which the run time is proportional to
the number of 4×2-tiles.
6.3 CNN Inference Unit (CIU)
All of the computation in the CIU goes through an inference datapath which is closely coupled to the two convolution engines and
three block buffers (BBs) as shown in Fig. 13. The details of these
designs are discussed as follows.
6.3.1 Tile-Pipelined Inference Datapath Engine. The highly-parallel
convolution is prone to inefficiency of data movement and inflexibility of model supporting. Thus we carefully designed the inference datapath engine to alleviate these issues. It follows a 32ch
4×2-tile pipeline as shown in Fig. 15 and mainly consists of two
functions: input preparation for the LCONV3×3 engine and output
post-processing for different opcodes and operands.
The first function prepares input 6×4-tiles for 3×3 filtering. However, a 6×4-tile is three times as large as a 4×2-tile and thus induces
heavy bandwidth for block buffers. To reduce the bandwidth, we
only read 4×2-tiles from block buffers and store them in a line FIFO
buffer. Then for each leaf-module the corresponding 6×4-tile can
be rearranged in a register file (RF6×4), and up to four leaf-modules
are supported. In addition, a data reordering circuit (Src Reorder) is
used to address a tile misalignment issue (Section 6.3.3).
The second function, output post-processing, provides model
flexibility and there are five sub-functions supported: 1) ERModule
through the LCONV1×1 engine; 2) Accumulation which uses an
adder (ADDE) for calculating cross-instruction partial sums and
another adder (ACCI) for internal ones; 3) Upsampling which writes
188
MICRO-52, October 12–16, 2019, Columbus, OH, USA C.-T. Huang et al.
Figure 13: eCNN system block diagram and instruction pipelining scheme.
Figure 14: Weight distribution scheme for the first half of output channels in the LCONV3×3 engine.
Figure 15: Inference datapath engine.
Figure 16: LCONV3×3 engine.
data in pixel-shuffle order (by Dst Reorder); 4) Downsampling for
strided- or max-pooling (also by Dst Reorder); 5) Quantization
which quantizes features or partial sums to their 8-bit Q-format
before going to block buffers or output FIFO interface. In addition,
one 8-bit quantization circuit is used inside LCONV3×3 to reduce
the input bitwidth of LCONV1×1 for saving area.
6.3.2 Highly-Parallel Convolution Engine. The LCONV3×3 and
LCONV1×1 engines serve a 4×2-tile in one cycle for 3×3 and 1×1
filtering respectively. They employ the weight-stationary strategy
[12] to optimize data reuse for the block-based inference. Also, compared to the conventional accelerators with much fewer multipliers,
their massive parallelism enables power-efficient accumulation of
internal partial sums. It is because their communications are all
locally hardwired without going through additional register files,
SRAM, or DRAM. For example, the LCONV3×3 engine contains
32×32 2D filters (Filter 2D 3×3) as shown in Fig. 16. And each 2D
filter shares the same 3×3 weights for a 4×2-tile and reuses them
for a block.
6.3.3 Eight-Bank Block Buffer Mapping. The highly-parallel data
movement also brings a misalignment issue for the block buffers:
the features are stored in 4×2-tiles but their accesses are not always
189
eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge Inference MICRO-52, October 12–16, 2019, Columbus, OH, USA
(a)
(b) (c)
Figure 17: Eight-bank BB implementation. (a) Access patterns for reading and writing 4×2-tiles in a block. (b) Normal
bank mapping. (c) Interleaved bank mapping.
Table 2: eCNN configurations.
tile-aligned. To address this issue, we implement each block buffer
using eight sub-buffer banks as shown in Fig. 17. A normal mapping
is sufficient for all cases except for pixel-shuffle upsampling which
causes sub-buffer conflicts; therefore, another interleaved mapping
is devised to resolve this issue.
7 EVALUATION
The configurations of our implementation are listed in Table 2. The
three computation constraints used for model optimization correspond to three real-time specifications: 4K UHD 30fps (UHD30),
Full HD 60fps (HD60), and Full HD 30fps (HD30). In the following,
we will first present the results for ERNet models and then the
layout performance for the eCNN processor. We will also introduce
two application examples in computer vision, style transfer and
object recognition, to demonstrate the flexibility of our approach.
7.1 ERNet Models
Model structure. In addition to SR4ERNet for four-times SR, we
also implement SR2ERNet and DnERNet for two-times SR and
denoising respectively. Their models are derived by accordingly
removing one and two upsamplers from the SR4ERNet in Fig. 7. We
do not use batch normalization layers as suggested in [36]. Also, we
pad 29 zero-valued channels for RGB images to form 32ch inputs
for eCNN.
Training. The hyper-parameters are listed in Table 3 for the
three stages of our training procedure: model scanning, polishment,
and fine-tuning for quantization. The scanning uses lightweight
settings for speeding up the process, and then the other two apply heavy settings for improving image quality. We train on two
Table 3: ERNet training settings.
Table 4: PSNR performance of polished ERNet models.
datasets, DIV2K [1] and Waterloo Exploration [40], for the following comparison with the state-of-the-art networks of SR and
denoising, respectively.
Polished models. The PSNR performance of the picked models
and their polished results is shown in Table 4. For comparison, we
include VDSR and SRResNet (implementation in [36]) for SR and
also CBM3D and FFDNet [63] for denoising. For the HD30 specification, the hardware-constrained ERNet models can achieve similar
quality compared to the state-of-the-art SRResNet and FFDNet.
When we increase the specification, the PSNR performance will
drop as the intrinsic complexity goes down. However, for UHD30
the SR4ERNet can still outperform VDSR by 0.49 dB while the
SR2ERNet and DnERNet are comparable to the benchmark VDSR
and CBM3D respectively.
Fixed-point precision and entropy coding. We tested both
the L1-norm and L2-norm quantization on the polished models,
and the results are shown in Table 5. For the case of SR4ERNet
for HD30, the bitstreams will exceed the capacity of the parameter
memory in Table 2 using 8-bit precision. Therefore, we further
perform 7-bit quantization on some selected parameter groups to
match the capacity. In general, the L1-norm causes more quality
degradation at first because more large values are cropped, but
it can be well recovered after fine-tuning. We chose to use the
L1-optimized models for their better PSNR quality despite their
higher entropy for larger dynamic range, and the compression ratio
is around 1.1-1.5×. As a result, the PSNR drops are well limited
between 0.05 to 0.14 dB for using dynamic 8-bit precision and
190
MICRO-52, October 12–16, 2019, Columbus, OH, USA C.-T. Huang et al.
Table 5: Model quantization and entropy coding.
Figure 18: Program of DnERNet-B3R1N0 (UHD30).
1,288KB of parameter memory. In addition, the values of cross
entropy are close to the Shannon limits, which justifies the usage
of the simple encoding method.
Program. The coarse-grained FBISA instructions result in concise programs. Fig. 18 shows a six-line program for the six-layer
DnERNet for UHD30. The attributes of the opcodes specify the
sizes of output blocks in terms of 4×2-tiles, and most of other fields
identify the dynamic Q-formats.
7.2 eCNN Performance
Implementation. We implemented eCNN in Verilog HDL for
TSMC 40nm technology and used ARM memory compilers to generate all SRAM macros. We used Synopsys IC Compiler for placement and routing. We performed layouts for five essential and
well-pipelined macro circuits which constitute the eCNN in a collectively exhaustive way. For fast and accurate power estimation,
we ran RTL simulation to generate signal activity waveforms and
then propagated them to post-layout netlists with extracted parasitics.
Layout performance. The eCNN processor can run at 250MHz
and achieves up to 41 TOPS of inference performance. The total
area is 55.23mm2
and the average power consumption is 6.94W at
0.9V. The details are summarized in Table 6. The LCONV3×3 engine
delivers 90% of inference performance and thus occupies the most
resource, i.e. 65.8% of area and 87.4% of power. And the LCONV1×1
engine is responsible for the rest 10% of performance and uses
another 7.0% of area and 6.6% of power. On the other hand, the
three block buffers (1536KB) and the parameter memory (1288KB)
contributes 11.3% and 7.9% of area for storing feature maps and
parameters respectively. But they consume only 3.9% of power in
total thanks to well-constrained word depths and highly-optimized
SRAM macros. The computation for ERNets is profiled in Fig. 19
where the inference time indicates real-time capability and NCR
Table 6: Area and power consumption of eCNN.
Figure 19: Inference time (left) and NCR (right).
Figure 20: Power consumption breakdown on ERNet models
(left) and circuit types (right).
shows computing overheads. Note that there is a tradeoff between
inference time and image quality as shown in Table 4.
Power breakdown. The power consumption for each ERNet
model is shown in Fig. 20. We found that the variation between
different specifications is related to the quality difference. For example, DnERNets have the largest power variation, 1.58W, while
they have 0.58 dB of PSNR drop from HD30 to UHD30. In contrast,
SR4ERNets have the smallest variations in both power consumption
and PSNR. Fig. 20 also shows the breakdown for three circuit types.
The combinational circuits contribute 82-87% of power consumption for the highly-parallel convolution. The sequential circuits
constantly occupy about 10% for the locally-distributed parameter
registers, 4×2-tile pipeline registers, and clock tree. The rest 3-7%
is then consumed by SRAMs.
DRAM bandwidth and power. The DRAM access via the data
input and output FIFOs is highly regular and can be optimized in a
deterministic way. The DRAM bandwidth and dynamic power consumption for each ERNet model are shown in Fig. 21. The DnERNets
191
eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge Inference MICRO-52, October 12–16, 2019, Columbus, OH, USA
Figure 21: DRAM bandwidth and dynamic power.
require the most bandwidth for each specification: 1.66GB/s for
UHD30, 0.94GB/s for HD60, and 0.5GB/s for HD30. But the NBRs are
still only 2.2×, 2.5×, and 2.7×, respectively. Therefore, the eCNN can
support high-end applications with low-end DRAM configurations.
For example, DDR-400 (3.2GB/s), DDR-266 (2.1GB/s), and DDR-200
(1.6GB/s) are sufficient for UHD30, HD60, and HD30 respectively.
Regarding power consumption, we use Micron DDR4 SDRAM
System-Power Calculator [44] for evaluation on DDR4-3200. The
small bandwidth of eCNN consumes only less than 120mW of
dynamic power (activation/read/write) while the leakage power
consumes 267mW.
Comparison. Table 7 compares the eCNN with two state-of-theart processors for computational imaging applications: IDEAL [42]
for BM3D and Diffy [41] for CNN. Both of them can only support the
HD30 specification and already require high-end DRAM settings,
i.e. dual-channel DDR3-1333 to DDR3-2133. However, eCNN can
deliver up to UHD30 performance using only DDR-400. Another
advantage of eCNN is its constant pixel throughput to facilitate
real-time applications. In contrast, the performance of IDEAL and
Diffy highly varies with input images since statistical properties
are deployed for acceleration.
To compare power consumption, we list the reported numbers
from [42] and [41] for IDEAL and Diffy on the right of Table 7.
Note that these numbers cannot be used to determine superiority directly because they are highly related to technology nodes,
implementation details, and deployed models/algorithms. For denoising at HD30, IDEAL needs 12.05W for BM3D and Diffy demands
27.16W (8 tiles) for FFDNet; however, eCNN consumes only 7.34W
for DnERNet which is 0.39 dB better than CBM3D and comparable
to FFDNet. For four-times SR at HD30, Diffy demands 54.32W (16
tiles) for VDSR while eCNN consumes only 7.08W for SR4ERNet
(0.57 dB better than VDSR).
We also used a CNN accelerator simulator, SCALE-Sim [51],
to simulate the performance of ERNets with the same processor
configuration as the classical TPU [30]. Note that TPU is a highperformance 28nm processor which provides 92 TOPS at 40 W
and has 28 MB of SRAM to store feature maps and parameters for
data reuse. The simulation shows that 4K UHD 21.9 fps and Full
HD 55.3 fps are achieved for SR4ERNet-B17R3N1 and SR4ERNetB34R4N0 respectively. And the required DRAM bandwidths are
12.2 GB/s and 8.3 GB/s. As a result, eCNN provides 3.1× and 1.2× of
throughput efficiency (fps/TOPS) and, in particular, 6.4× and 14.4×
of arithmetic intensity (TOPS/GB/s), respectively, for these two
models. This also demonstrates the advantage of our joint-design
approach for computational imaging tasks.
(a)
(b)
Figure 22: Computer vision models based on FBISA: (a) style
transfer and (b) object recognition.
7.3 Computer Vision Applications
Model structure. To show the model flexibility of our approach,
we built FBISA-compatible models for style transfer and object
recognition as shown in Fig. 22. They differ from the ones for computational imaging mainly in three respects: spatial downsampling,
wider channels, and batch normalization layers. The first two are
supported in FBISA by concatenating 32ch leaf-modules. The last
one is used to stabilize model pre-training and will be merged into
convolutional layers for quantization and inference.
Style transfer. We used two downsamplers to increase the receptive field as suggested in [29]. Since this will increase NCR
significantly, we split the model into two sub-models as shown
in Fig. 22(a) to reduce computing overheads. After training and
quantization, our model can deliver similar style transfer effects
with [29]. The performance on eCNN is Full HD 29.5 fps for this
model while Nvidia Titan X GPU is used in [29] and only generates
512×512 20fps. In addition, the DRAM bandwidth of our approach
is only 1.91GB/s, which can enable this advanced application on
embedded devices.
Object recognition. We devised a 40-layer residual network in
Fig. 22(b) for eCNN to perform object recognition. To reduce the
amount of parameters, we avoided 512ch ResBlocks and instead put
more computation in thinner layers. The final 8-bit model achieves
69.7% top-1 accuracy for ImageNet [15] with 5M parameters. The
performance is comparable to ResNet-18 (69.6%; 11M) and VGG-16
(71.5%; 138M) [54]. To support this model, we need to increase the
size of parameter memory by three times, and the area of eCNN
would become 63.99 mm2
. Then the performance achieves 1344
fps (0.74 ms per image) with 308 MB/s of DRAM bandwidth. For
each image, eCNN only consumes 5.25 mJ of energy and 231 KB of
DRAM access. For comparison, the Eyeriss [13], which has 12.25
mm2 of core area with 65nm technology, delivers 0.7 fps (4.3 s for a
batch of three images) with 236mW of power consumption and 74
MB/s of DRAM bandwidth for VGG-16. Thus it demands as high
as 337 mJ of energy and 106 MB of DRAM access for one image.
In this case study, we demonstrate the model flexibility of eCNN
and also show that our joint hardware-model approach can benefit
object recognition tasks as well.
192
MICRO-52, October 12–16, 2019, Columbus, OH, USA C.-T. Huang et al.
† The 40nm technology outperforms its 65nm counterpart at half of the power consumption under the same operation speed [55].
Table 7: Comparison of computational imaging processors.
8 RELATED WORK
Instruction set. Previous SIMD works usually devised load instructions for parameters and adopted medium-grained operands
for features, such as vector/matrix [37], 2D tile [49], and compute
tile [47], for providing flexibility. In contrast, we apply a parameterinside approach and large-grained feature operands for optimizing
power consumption and computing capability for highly-parallel
convolution.
Model structure. Most of previous hardware-oriented models
aim to reduce complexity. In particular, SqueezeNet [26] temporarily reduces model width and then expands back for residual connections. And MobileNetV2 [52] moves the connections to thinner
layers to reduce storage. This results in an expansion-reduction
structure similar to ERNet. However, our goal is to increase complexity under hardware constraints, and thus the implementation
details are quite different.
Winograd convolution. It is an efficient algorithm to reduce
multipliers for CONV3×3 and recently shows advantages on GPU
[34], FPGA [38], and embedded processor [58]. However, it increases 23.5% of area in our case because the overheads of long
internal bitwidths and additional pre-/post-processing become significant for our 8-bit implementation. Therefore, we used a direct
implementation instead.
Cross-frame optimization. It is a new research direction to
reduce CNN computation by exploiting temporal redundancy [8, 67]
or input similarity [50] across video or audio frames. Moreover,
this concept has also been applied to compensate the unreliability
brought by pruning [60]. This direction is complementary to our
approach and can be used to further enhance the performance of
eCNN.
9 CONCLUSION
In this paper, we investigate a hardware-first framework to support
computational imaging CNNs for up to 4K Ultra-HD applications
on edge devices. Instead of accelerating existing models, we devise
the hardware-oriented ERNets for the adopted block-based inference flow which can eliminate DRAM bandwidth for feature maps.
For providing high computing capability, we construct the coarsegrained FBISA to enable highly-parallel convolution. Finally, we
implement the high-performance eCNN processor to incorporate
ERNet and FBISA. The training and layout results show that this
framework can provide superior hardware performance and image
quality at the same time. In addition, its flexibility is demonstrated
by the usage examples of style transfer and object recognition. Our
future work is to include more CNN variants for different applications and unleash their power on edge devices.