ABSTRACT
Deep neural networks have become the compelling solution for the
applications such as image classification, object detection, speech
recognition, and machine translation. However, the great success
comes at the cost of excessive computation due to the over-provisioned
parameter space. To improve the computation efficiency of neural
networks, many pruning techniques have been proposed to reduce the amount of multiply-accumulate (MAC) operations, which
results in high sparsity in the networks.
Unfortunately, the sparse neural networks often run slower than
their dense counterparts on modern GPUs due to their poor device utilization rate. In particular, as the sophisticated hardware
primitives (e.g., Tensor Core) have been deployed to boost the performance of dense matrix multiplication by an order of magnitude,
the performance of sparse neural networks lags behind significantly.
In this work, we propose an algorithm and hardware co-design
methodology to accelerate the sparse neural networks. A novel
pruning algorithm is devised to improve the workload balance
and reduce the decoding overhead of the sparse neural networks.
Meanwhile, new instructions and micro-architecture optimization
are proposed in Tensor Core to adapt to the structurally sparse
neural networks. Our experimental results show that the pruning
algorithm can achieve 63% performance gain with model accuracy
sustained. Furthermore, the hardware optimization gives an additional 58% performance gain with negligible area overhead.
CCS CONCEPTS
â€¢ Computer systems organization â†’ Single instruction, multiple
data.
KEYWORDS
neural networks, graphics processing units, pruning
1 INTRODUCTION
Deep neural networks (DNNs) have achieved state-of-the-art performance in many different tasks, such as image recognition [31,
39, 67], speech recognition [62], and natural language processing [15, 36, 71]. The underlying representational power of these
neural networks comes from the huge parameter space which results in an extremely large amount of computation and memory
usage. There have been plenty of prior works to improve both
performance and energy efficiency of neural networks on various
hardware platforms, such as GPUs [9, 18, 38, 44, 60, 73, 78], FPGAs [19, 28, 81], and ASICs [2, 3, 11â€“14, 20, 21, 23, 32, 34, 35, 45, 47,
49, 54, 57, 59, 61, 63, 65, 68, 69, 75, 82, 83]. Among these prior arts,
sparsity-centric optimization techniques [4, 28â€“30, 56, 64], which
exploit the sparsity in weights and activations, have achieved outstanding results for both Convolutional Neural Networks (CNNs)
and Recurrent Neural Networks (RNNs).
By leveraging the intrinsic redundancy in the weights of neural
networks, various sparsifying techniques have been discussed for
weight pruning. Those techniques can result in very high sparsity in
the weight matrices. For instance, prior work [28â€“30] has reported
that top-K sparsifying and retraining could result in more than 90%
sparsity with negligible impact on the model accuracy. Nonetheless,
such high sparsity does not necessarily guarantee that the sparse
neural networks can be more performant than their dense counterparts due to the irregularity of data layout. In particular, the sparse
neural networks can hardly obtain performance gain on Graphics Processing Units (GPUs). The state-of-the-art sparse library,
CUSPARSE, encodes a sparse weight matrix to Compressed Sparse
Row (CSR) format [8]. Since a sparse weight matrix pruned by the
top-K sparsifying has a random number of non-zero elements in
a row, the CSR format often leads to poor workload balance. As a
consequence, the GPU is extremely underutilized when running
the sparse library kernels.
On the other hand, general matrix multiplication (GEMM) has
seen contiguous optimization on modern GPUs, as it is one of the
359
MICRO-52, October 12â€“16, 2019, Columbus, OH, USA Maohua Zhu, Tao Zhang, Zhenyu Gu, and Yuan Xie
fundamental primitives of many popular neural networks. For example, NVIDIA has built high performance GEMM kernels with a
hand-tuned machine code entity in the state-of-the-art CUBLAS
library [51]. In addition, Tensor Core has been introduced in Volta
architecture [53] to provide 8Ã— peak TFLOPs than the FP32 CUDA
Core (112TFLOPs v.s. 14TFLOPs). Unfortunately, Tensor Core focuses only on the acceleration of dense matrix multiplication. Since
sparse GEMM cannot take advantage of Tensor Core, we have
seen little speedup when running sparse neural networks by top-K
sparsifying on it.
To address the performance issue, structural sparsifying methods [70, 76] have been proposed by removing entire rows or columns
from a matrix. As a result, the structurally pruned matrices are
able to sustain their denseness and thus use the Tensor Core to
achieve high performance. However, such coarse-grained pruning on the weight matrices has a negative impact on the model
accuracy. Although prior studies [70, 76] have reported comparable accuracy of the structurally pruned networks on small datasets
(e.g., MNIST [40]), we have observed significant accuracy drop from
large-scale neural networks when the structural pruning is applied
(See Section 6 for the detail).
In this paper, we propose VectorSparse, a SIMD (Single Instruction,
Multiple Data)-friendly sparsifying method, to tackle the problem.
VectorSparse divides a weight matrix into multiple vectors and
prunes each vector to the same sparsity. The sparse weight matrices generated by VectorSparse exhibit a better workload balance
and higher parallelism than the top-K pruned weight matrices. To
further improve the performance, we extend the instruction set of
Volta to allow the VectorSparse neural networks to run on Tensor
Core. The extension requires only minor changes to enable the necessary indexing to the register files. The simulation results show
that VectorSparse neural networks are faster than either the dense
or top-K sparse counterparts with negligible accuracy impact.
To the best of our knowledge, this is the first work to exploit the
efficiency of sparse neural networks on Tensor Core. The contributions of this paper include:
â€¢ We go through a comprehensive performance analysis to
demonstrate the inefficiency of GPU when running the sparse
neural networks.
â€¢ We propose VectorSparse as a novel sparsifying algorithm
that can achieve 63% performance improvement with negligible accuracy drop.
â€¢ We further extend the instruction sets of the Volta GPU to
support the operand indexing in the register file.
â€¢ We also show the details of the micro-architecture design
to mitigate the performance bottleneck, which achieves 58%
performance gain with negligible area overhead.
2 BACKGROUND AND MOTIVATION
In this section, we first review some prior work on sparsity-centric
optimization for neural networks, and then describe the existing
sparsifying techniques in detail.
2.1 Sparsity-Centric Optimization for DNNs
Recently, DNNs have demonstrated significant redundancy in the
parameterization [17]. The over-sized parameter space results in
2.1 -0.1 1.2 -0.3
0.2 0 0.6 0.1
1.5 0.3 -0.2 0.4
-0.8 0.9 -0.5 0
2.1 -0.1 1.2 -0.3
0.2 0 0.6 0.1
1.5 0.3 -0.2 0.4
-0.8 0.9 -0.5 0
2.1 0 1.2 0
0 0 0 0
1.5 0 0 0
0 0.9 0 0
2.1 -0.1 1.2 -0.3
0.2 0 0.6 0.1
1.5 0.3 -0.2 0.4
-0.8 0.9 -0.5 0
2.1 -0.1 1.2 -0.3
0.2 0 0.6 0.1
1.5 0.3 -0.2 0.4
-0.8 0.9 -0.5 0
2.1 0 0 0
0.2 0 0 0
1.5 0 0 0
-0.8 0 0 0
L2 Norm: 2.71 0.95 1.45 0.51
(a) Generic sparsifying
(b) Unified sparsifying
Figure 1: Examples of (a) generic sparsifying and (b) unified
sparsifying. Both examples enforce 75% sparsity on a 4x4
matrix.
high sparsity in a neural network. In addition to the weight parameters, the activations of each layer of a network also possess
sparsity, a factor that stems mainly from the activation functions
(e.g. ReLU) [39].
As the sparsity in weight parameters does not depend on the
input data, it is often referred to as static sparsity. On the other hand,
the sparsity in the activations depends on not only the weight, but
also the input data. Therefore, such sparsity in the activations is
denoted as dynamic sparsity. In this work, we focus on exploiting
the static sparsity in neural networks to accelerate the inference
phase of applications.
Early efforts that exploit the static sparsity concentrate on pruning the weights of the neural networks with top-K sparsifying [4,
28â€“30, 82]. The top-K pruning achieves great success in terms
of compression ratio. However, the randomness in the positions
of the non-zero elements in the top-K pruned weight matrices
makes them unable to leverage sophisticated software libraries,
e.g. CUBLAS [51], or hardware resources, e.g. Tensor Core [53] on
modern GPUs. Hence they exhibit far lower data throughput than
the corresponding dense neural networks.
To improve the efficiency of the sparse neural networks on GPUs,
some work has proposed the structural sparsifying methods [70, 76].
The structural sparsifying puts certain spatial constraints on the
non-zero elements to keep the denseness of the output matrices
after sparsifying. The generated dense weight matrices have less
parameters and can take full advantage of dense GEMM libraries.
Even though such structural sparsifying has high performance, it
incurs severe accuracy drop for large commercial models1. This is
because the restricted spatial constraints of the weights make it
hard to train the networks.
2.2 Existing Sparsifying Methods on GPUs
In general, the sparsifying methods [27, 30, 70, 76] can be classified into two categories, generic sparsifying and unified sparsifying.
The generic sparsifying method is illustrated in Figure 1(a), which
1We have also observed that small neural networks could achieve good speedup and
similar accuracy with structural sparsifying. These small models, however, are out of
our interests since they cannot be widely adopted by the industry.
360
Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs MICRO-52, October 12â€“16, 2019, Columbus, OH, USA
0%
1%
2%
3%
4%
5%
6%
7%
0%
10%
20%
30%
40%
50%
60%
70%
80%
50% 75% 90% 96%
Normalized L2 Read Throughput
Normalized Performance 
over Dense Counterparts
Sparsity
AlexNet-Conv2-Perf AlexNet-Conv4-Perf
AlexNet-Conv2-L2-Throughput AlexNet-Conv4-L2-Throughput
Figure 2: The normalized performance and L2 cache
throughput of generic sparse CONV layers over dense CONV
layers on a Tesla V100 GPU.
picks the largest four elements in absolute value as key elements in
the matrix (highlighted in yellow). The key elements are kept unchanged while the rest of the elements are forced to be zero, which
results in 75% sparsity in the matrix. Note that the coordinates of
the key elements have to be kept along with the values as they can
be arbitrary due to the flexibility of sparsifying.
Although the generic sparsifying achieves a very high compression ratio, it exposes several inefficiencies on GPUs [66]. Firstly,
the variation of the row/column length of a generic sparse matrix
makes it difficult to partition the workload evenly into GPUs [43].
Secondly, the number of non-zero elements in each row is unknown
until runtime, leaving it difficult to choose an optimal tiling scheme
for data reuse. Thirdly, the computation amount of a highly sparse
matrix is not enough to hide the long memory access latency, and
therefore the benefit from the high sparsity vanishes.
To reveal the problem, we run two sparse convolutional layers
(Conv2 and Conv4) in AlexNet [39] on an NVIDIA Tesla V100
GPU. The convolution operation in these two layers is converted
to GEMM by im2col transformation [10]. The implementation of
the sparse layers is based on CUSPARSE, the state-of-the-art GPU
library for sparse linear algebra2. Figure 2 shows the performance
in GPU execution time and the read throughput of L2 cache, which
are normalized to the dense implementations on CUBLAS.
Intuitively, layers with high sparsity should have better performance since they need less computations. However, as shown in
Figure 2, the sparse layers are less performant than dense layers.
Even when the sparsity is 96%, the sparse layers can only achieve
73% of the dense layer performance. The low L2 read throughput
indicates that the device memory bandwidth is underutilized, which
means the performance of the sparse layers is bounded by computation. We figure out that the compute units are underutilized in
this case. The low utilization is due to the poor workload balance
because we have observed better performance from Conv2 layer
when it exhibits better workload balance.
2Please refer to Section 5 for the detail of the experiment setup.
V(0,0)
V(1,0)
V(2,0)
...
V(M,0)
V(0,1)
V(1,1)
V(2,1)
...
V(M,1)
...
...
...
...
...
V(0, ceil(N/L)-1)
V(1, ceil(N/L)-1)
V(2, ceil(N/L)-1)
...
V(M, ceil(N/L)-1)
E(0) E(1) ... E(L-1)
E(0) E(1) ... E(L-1)
Figure 3: Dividing a M Ã— N matrix into L-dim vectors for locality characterization.
On the other hand, the unified sparsifying is illustrated in Figure
1(b). Distinct from generic sparsifying, 75% sparsity is achieved by
a column-wise sparsifying. The values of an entire column remain
unchanged while the rest are forced to be zero. Therefore, it is easy
to encode/decode the coordinate information. In this example, the
unified sparsifying evaluates the L2 norm of each column and picks
the column with the largest result.
A consequence of this selection process is the high probability
that some key elements that would be kept in the generic sparsifying are removed in the unified sparsifying. Similar to the unified
sparsifying, coarse-grained sparsifying in large blocks [27] can even
remove an entire block out of a matrix at a time. Even though this approach allows more flexibility by adjusting the block size, it cannot
use the highly optimized dense libraries to get good performance
boost over the dense counterparts.
Therefore, we would like to see if there is an opportunity to
get the best from both worlds. That is, we try to find a sparsifying
method to achieve better performance than generic sparsifying
and meanwhile eliminate the accuracy drop brought by the unified
sparsifying. For this purpose, a highly flexible structural sparsifying
method is desirable to preserve a comparable model accuracy with
the generic sparsifying while the workload is balanced enough to
ensure high performance on modern GPUs.
2.3 The Characterization of Sparsity
To find such a sparsifying algorithm, we first characterize the spatial
locality of the non-zero values in the sparse neural networks. We
prune ResNet-18 [31] and NMT [50] with the generic sparsifying
method used in Deep Compression [30]. The two pruned networks
have more than 90% sparsity and comparable accuracy with that of
the dense references.
After a network is pruned, we split each row of its weight matrices into multiple L-dim vectors. Note that the vectors do not
overlap with each other. Figure 3 shows an example of the split.
Vector V (y, x) contains the elements with row index y, and column
indices from x Ã— L to (x + 1) Ã— L âˆ’ 1 in the M Ã— N weight matrix
W . If N is not divisible by L, the residue vectors are padded with
zero. For each vector within the sparse weight matrix, we count the
total number of zeros in the vector (in the range [0,L]), and then
compute the local sparsity degree of each vector. The local sparsity
is defined as the number of zeros divided by L.
Figure 4 shows the cumulative distribution of local sparsity degree with three vector sizes, 4, 8, and 16. As shown, only less than
30% of 4-dim vectors have â‰¤ 75% sparsity. As there are only 4 elements in a 4-dim vector, this result indicates that more than 70% of
the 4-dim vectors do not have any non-zero elements. Moreover,
only less than 2% of the 4-dim vectors have â‰¤ 25% sparsity. In other
361
MICRO-52, October 12â€“16, 2019, Columbus, OH, USA Maohua Zhu, Tao Zhang, Zhenyu Gu, and Yuan Xie
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
20% 30% 40% 50% 60% 70% 80% 90% 100%
Number of the vectors with sparsity
no more than x%
Sparsity=x%
ResNet-18, L=4 NMT, L=4 ResNet-18, L=8
NMT, L=8 ResNet-18, L=16 NMT, L=16
Less than 30% 4-dim vectors have 75% or less sparsity.
Less than 5% 8-dim vectors have 75% or less sparsity.
Less than 2% 16-dim vectors have 75% or less sparsity.
Figure 4: The cumulative distribution of the vectors vs. the
local sparsity degree in a vector. The vertical axis is the number of the vectors with the sparsity not greater than x%. The
horizontal axis is the sparsity x%.
words, very few of the vectors have more than 2 non-zero elements.
Based on this observation, the spatial distribution of the non-zero
elements in the sparse neural networks are generally retained if we
only keep up to 2 non-zero elements in each vector. Instead of the
location-unaware element selection used in the generic sparsifying,
this approach generates a 50% sparse matrix with a balanced spatial
distribution of non-zero weights.
Similarly, the experimental results for 8-dim and 16-dim vectors
demonstrate that more than 95% of vectors have â‰¥ 75% sparsity.
The cumulative distribution shows that there are less low sparsity
vectors when the vector size is increasing. This is because the local
sparsity in a larger scope more likely resembles the global sparsity.
According to the observation, it inspires us to divide a weight
matrix into L-dim vectors so that each vector can be sparsified
independently to achieve both sparsity balance and comparable
model accuracy.
3 VECTORSPARSE PRUNING
The characterization of spatial locality opens a great opportunity
to avoid accuracy penalty by splitting weight matrices into vectors
and sparsifying each vector to the same sparsity. However, the
encoding formats for generic sparse matrices, e.g. CSR format [8],
do not contain the information associated with the vectors. In this
section, we first propose a balanced vector-wise encoding format
for sparse matrices that simplifies the workload partitioning on
GPUs. Then, we design a novel vector-wise sparsifying algorithm
to prune a trained dense network to a sparse network that can
maximize the vector-wise encoding efficiency.
3.1 Vector-wise Sparse Matrix Encoding
To improve the workload balance of the sparse neural networks
on GPUs, we propose a three-phase vector-wise encoding method
to sparsify a matrix. In the first phase, we divide a matrix into
L-dim vectors, as shown in Figure 3. An M Ã— N matrix thus has
0
0
0
0
NZ0 1
NZ2 2
NZ1
0
0
0
0
0
0
0
0
NZ6
NZ4
0
NZ3
NZ5
3
0
Original W Encoded W and offset
0
0
NZ0
0
0
NZ2
0
0
0
0
NZ3
0
0
NZ5
0
0
NZ1
0
NZ4
NZ6
6
2
5
4
Figure 5: An example of vector-wise sparse matrix encoding
with L=8 and K=2. Two non-zero elements in a row vector are
compressed into one compact vector associated with their
indices. All row vectors are encoded to the same length. If
a row vector has less non-zero elements than the compact
vector length K, the empty entries are padded with zeros.
M Ã— N/L vectors. In case N is not divisible by L, the residue
vectors are padded with zero. In the second phase, we count the
number of non-zero elements Nnz in each vector, and then denote
the maximum Nnz of all the vectors in this matrix as K (K â‰¤ L). In
the third phase, we compress each vector into a K-dim vector along
with their associated indices in the original vector. Note that a vector
might have less than K non-zero elements after compression. For
example, the second row vector in Figure 5 has only one non-zero
element NZ2. For such vectors, the empty entries are filled with
zeros to assure those vectorsâ€™ K-dim. The vector-wise encoding can
either be column-wise or row-wise. Without losing generality, we
use column vectors unless specifically illustrated.
Theoretically, loÐ´2L bits are required to encode each index
in a L-dim vector. Consequently, the overall compression ratio of
this encoding is PÃ—L
(P+loÐ´2L)Ã—K , where P stands for the number
of bits used to store the value of an element. Figure 5 shows an
example of the encoding for a 4Ã—8 matrix, where L=8, K=2. In
this 4Ã—8 FP16 matrix, we observe that each element of the offset
index array can be represented in only 3-bit, as the index is in the
range [0, 7]. Therefore, the compression ratio is 3.37x. As K is the
maximum number of non-zero elements in a vector, this encoding
could achieve an ideal compression ratio when all vectors have
the same number of non-zero elements. However, if the number of
non-zero elements vary too much, the compression ratio could be
far from the ideal case.
Fortunately, neural network pruning allows us to tailor the topology of weights to achieve the spatial distribution for the ideal case of
the vector-wise encoding. Instead of pursuing high overall sparsity,
the pruning method for the vector-wise encoding tries to minimize
the maximum number of non-zero elements in a vector.
3.2 VectorSparse: a Methodology of Iterative
Vector-wise Sparsifying and Retraining
To achieve a spatially even distribution of non-zero elements in a
neural network weight matrix, we propose VectorSparse: a pruning
methodology with iterative vector-wise sparsifying and retraining
for CNNs and RNNs. The vector-wise sparsifying can be considered
as local sparsifying, which takes advantage of the aforementioned
vector-wise sparse encoding. For convolutional layers in CNNs, we
refer to the N Ã— (CHW ) matrix generated by the im2col transformation [10] as the weight matrix, where N is the number of filters,
362
Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs MICRO-52, October 12â€“16, 2019, Columbus, OH, USA
Algorithm 1: VectorSparse algorithm: pruning with vectorwise sparsifying.
input :Weight matrix of a trained NN layer, W0;
The vector size, L;
Maximum accuracy drop, EÎ´ .
output:Pruned vector-wise sparse weight matrix, Ws
1 W =W0;
2 Divide W into vectori,j ;
3 Nzero=0;
4 E0 = ValidationError(W );
5 E = E0;
6 while Eâˆ’E0
E0 < EÎ´ do
7 Nzero=Nzero + 1;
8 for each i, j do
9 Sort absolute values of all elements in vector[i][j] in
ascending order and save sorted elements in
sorted[L];
10 Tij=sorted[Nzero ];
11 for each element in vector[i][j] do
12 Remove element if abs(element) < Tij ;
13 end
14 end
15 Fine tune the pruned W ;
16 E = ValidationError(W );
17 end
18 Ws=W ;
C is the number of channels of each filter, and H and W are the
height and width of a filter, respectively.
The vector-wise sparsifying sorts the elements in each vector
by their absolute values. The largest K elements in absolute value
are kept unchanged while all other elements are pruned. After
this phase, all vectors have at most K non-zero elements so that
they can be encoded to K-dim vectors by our vector-wise sparsity
encoding. Although setting a small K can easily increase the overall
compression ratio, directly pruning a dense weight matrix to a small
K vector-wise encoding could lead to significant accuracy drop of
the neural networks.
To address the accuracy drop issue, we propose a progressive
pruning method by gradually decreasing K in the vector-wise sparsifying. Algorithm 1 shows the flow of our VectorSparse pruning.
Starting from a trained, dense neural network, our algorithm prunes
the network layer by layer. Given a dense weight matrixW0, a userspecified vector size L as the input parameter, and the maximum
accuracy drop EÎ´ as the acceptable error rate, the sparsity of the
weight matrix gradually steps up (Line#7) until the validation error
of the pruned neural network exceeds the error rate (Line#6). Starting from a dense weight matrix, VectorSparse prunes the elements
that do not fall into the Top-N absolute values within each vector.
Then the weights are tuned based on the pruned topology with the
same training dataset. After the fine tuning, the algorithm evaluates
the validation error with that of the dense network, E0. The relative
difference of the validation error between the pruned network and
the dense network is used to determine if the pruning process can
(a) (b)
Figure 6: The spatial distribution of non-zero weights in neural networks pruned by (a) the generic method with 96%
sparsity and (b) the vector-wise approach with 75% sparsity,
respectively. Each yellow pixel represents a non-zero element. It is clear that vector-wise pruning achieves better regularity.
continue. VectorSparse provides the flexibility to specify the acceptable error rate, which usually varies in different applications. If an
application is more sensitive to latency rather than accuracy, the
maximum accuracy drop EÎ´ can be set higher to gain more sparsity.
Otherwise, the maximum accuracy drop should be set small enough
to ensure the accuracy.
Because of the additional spatial constraint, VectorSparse usually
chooses a different set of weights from the generic pruning [30]
before each fine tuning step. The prior studies have figured out that
the difference in the pruning pattern has negligible impact on the
speed of convergence when comparing to the generic pruning [22,
48, 80, 84]. On the other hand, the number of pruned synapses
between two retraining phases does affect the accuracy of a pruned
neural network. This factor is controlled by the vector size L and
the sparsity Nzero in the algorithm. Figure 6 shows the spatial
distribution of non-zero elements of pruned ResNet-50 weights by
our vector-wise pruning, which leads to better workload balance
than the generic pruning.
3.3 GPU Kernel Design
Being aware of the spatial distribution of the non-zero elements, our
vector-wise encoding method allows the sparse matrix multiplication to be efficiently mapped on GPUs with good workload balance.
Correspondingly, we make minor modification to the GPU kernel
for vector-wise sparse matrix multiplication. The vector-wise encoded sparse matrix multiplication kernel defines a user-interface
function similar to the standard GEMM API with two extra parameters, vector size L and the maximum number of non-zero element
in a vector K.
Letâ€™s assume a general matrix multiplication, C=AÃ—B, where the
sizes of matrices A, B, and C are 4Ã—8, 8Ã—6, and 4Ã—6, respectively.
In the traditional dense matrix multiplication kernel, the product
of every row of A and every column of B needs to be computed,
Encoded A and offset
NZ0 1
NZ2 2
NZ3
NZ5
3
0
NZ1
0
NZ4
NZ6
6
2
5
4
Dense B Dense C
b1
b6
c0 = NZ0 âˆ™ b1 + NZ1 âˆ™ b6
c0
h 
Figure 7: An example of vector-wise sparse matrix multiplication.
363
MICRO-52, October 12â€“16, 2019, Columbus, OH, USA Maohua Zhu, Tao Zhang, Zhenyu Gu, and Yuan Xie
L1 Instruction Cache
Memory IO
Unified L1 Data Cache and Shared Memory Texture Memory
Warp Scheduler
SIMD 
Dispatch Unit
FP64/32 
INT
CUDA 
Cores
&
SFU
Tensor
Core
Tensor
Core
Register File LD/ST 
Unit
Warp Scheduler
SIMD 
Dispatch Unit
FP64/32 
INT
CUDA 
Cores
&
SFU
Tensor
Core
Tensor
Core
Register File LD/ST 
Unit
Warp Scheduler
SIMD 
Dispatch Unit
FP64/32 
INT
CUDA 
Cores
&
SFU
Tensor
Core
Tensor
Core
Register File LD/ST 
Unit
Warp Scheduler
SIMD 
Dispatch Unit
FP64/32 
INT
CUDA 
Cores
&
SFU
Tensor
Core
Tensor
Core
Register File LD/ST 
Unit
To/From L2 Cache
Figure 8: The architecture of a streaming multiprocessor
(SM) of the Volta GPUs. Branch unit, L0 instruction cache,
and constant cache are omitted for brevity.
regardless of sparsity. In particular, the first row of matrix C needs
48(=8Ã—6) multiplications. On the other hand, as shown in Figure 7,
after it is pruned by the VectorSparse algorithm, the 4Ã—8 weight matrix A becomes a 4Ã—2 vector-wise sparse matrix with an associated
offset matrix of the same size. As a consequence, the vector-wise
sparse matrix multiplication kernel only needs a subset of the elements of matrix B to compute the product matrix C. For example,
since the first row of sparse matrix A has non-zero elements at
column 1 and 6, the calculation of first row c0 in C is equivalent to
NZ0 Â· 
b1 + NZ1 Â· 
b6, where 
b1 and 
b6 stand for the corresponding
rows in B. As a result, only 12(=2Ã—6) multiplications are executed,
resulting in a 75% multiplication reduction.
In high-performance GPU matrix multiplication kernels, tiling
is widely used for large size matrix multiplications. A warp is responsible for the computation of a tile of the product matrix C. The
proposed vector-wise sparse matrix multiplication is orthogonal to
the tiling techniques so that tiling can be applied to it as well. To
make sure the tiled multiplication performs well, the indexing of
data-dependent row in matrix B should be designed carefully. We
will cover this topic in the next section.
4 SPARSE TENSOR CORE
So far, all benefits given by the VectorSparse pruning algorithm
are in theory. As we know, generic sparse matrix suffers from the
poor workload balance and cumbersome coordinate decoding for
its overall performance. In fact, VectorSparse could present the
same issue if the hardware design is unaware of the new algorithm.
In this section, we go through the design details which make the
hardware adaptive to the algorithm. In particular, we modify the
Tensor Core in order to have full support for VectorSparse, which
we refer to sparse Tensor Core.
4.1 Baseline Tensor Core Architecture
The Tensor Core is a fast hardware functional block for dense
matrix multiplication. It was first introduced in NVIDIAâ€™s Volta











 
 
 
 
  
 
 
 
  

 
  
	
	
 	 	

	 	
	
	
Figure 9: The mapping of a 16Ã—16Ã—16 matrix multiplication
into four worktuples in a warp [58]. The computation task
for the product matrix D is evenly partitioned into four
worktuples.
architecture [53]. Each Tensor Core is able to execute a 4Ã—4Ã—4
matrix multiplication and addition in one cycle. The Tensor Core in
Volta GPUs provides two execution modes, FP16 mode and mixed
precision mode. In the FP16 mode, all matrices are in FP16. In the
mixed precision mode, the Tensor Core uses FP32 accumulators
and writes back the results to an FP32 matrix.
Figure 8 shows the architecture of one of the streaming multiprocessors (SMs) in Volta GPU. As illustrated, an SM consists of
four subcores. In each subcore, there is a warp scheduler, a math
dispatch unit, streaming processor arrays for multiple data types
(a.k.a. CUDA Cores), special function units (SFUs), two Tensor
Cores, LD/ST unit, and register files. The L1 data cache and shared
memory are shared among the four subcores within the SM.
During program execution, two Tensor Cores are used concurrently by a warp [58]. In the CUDA programming model [53], Tensor Cores are exposed to programmers in the CUDA WMMA (Warp
Matrix Multiply and Accumulate) API. The WMMA API includes
dedicated matrix load and store primitives, and matrix multiply
and accumulate operations for Tensor Cores. The WMMA matrix
load and store operations are designed for moving data between
register files and the memory hierarchy.
Given A, B, C, and D are 16Ã—16 matrices, a warp computes a
matrix multiply and accumulate, D=AÃ—B+C. Even though NVIDIA
has not disclosed the design details of Tensor Core, some work
has revealed how the 32 threads within a warp collaborate to conduct the 16Ã—16Ã—16 matrix multiply and accumulate operation efficiently [58]. To execute a WMMA, the 32 threads in a warp are
divided into 8 threadgroups. The threadgroup Id of a given thread is
 threadId 4 . All threads in a threadgroup work together to compute
4Ã—4 tile multiplications. Furthermore, for better data reuse, two
threadgroups work together as a worktuple. Worktuple i consists of
threadgroup i and threadgroup i + 4.
Figure 9 shows the elements processed by each worktuple in
one WMMA operation. Each worktuple is responsible for computing one 8Ã—8 tile of D. For example, Worktuple 0 computes
D[0:7, 0:7]. To achieve this, Worktuple 0 multiplies A[0:7, 0 : 15]
and B[0 : 15, 0:7], adds the product 8Ã—8 tile with C[0:7, 0:7], and
saves the result to D[0:7, 0:7].
During the compilation time, a WMMA operation breaks down
into four sets of machine-level HMMA instructions [58]. In the
mixed precision mode, each set of HMMA instructions computes
the product of a 4Ã—4 tile of A and a 4Ã—8 tile of B. Figure 10 left shows
364
Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs MICRO-52, October 12â€“16, 2019, Columbus, OH, USA
Threadgroup 0 Threadgroup 4 Shared
Set 0
Set 1
Set 2
Set 3
ABC àµˆ àµ… àµŒ D
Set 0
Set 1
Set 2
Set 3
A àµˆ B C àµ… àµŒ D
Each Set of dense HMMA:
HMMA.884.F32.F32.STEP0 RD, RA, RB, RC;
HMMA.884.F32.F32.STEP1 RD, RA, RB, RC;
HMMA.884.F32.F32.STEP2 RD, RA, RB, RC;
HMMA.884.F32.F32.STEP3 RD, RA, RB, RC;
Each Set of Sparse HMMA:
SHMMA.FETCHIDX RO;
SHMMA.EXEC.F32.F32 RD, RA, RB, RC;
Figure 10: Elements processed by the threadgroups in Worktuple 0 in the dense mode (left) [58] and the sparse mode
(right) of a dense/sparse WMMA PTX instruction, respectively.
the tiles processed by each set of HMMA instructions in Worktuple 0. The four sets of HMMA instructions of Threadgroup 0 and 4
compute the product submatrices D[0:3, 0:7] and D[4:7, 0:7],
respectively. At the execution of one set of the HMMA instructions, the two threadgroups in Worktuple 0 share a 4Ã—8 tile of B in
each set. On the other hand, the 4Ã—4 tiles of A are private to the
threadgroups, respectively.
Figure 11 illustrates how a WMMA operation is mapped to the
Tensor Core architecture [58]. There are two octets in a Tensor
Core. Inside an octet, there are eight dot product (DP) units, each
of which can compute a 4-dim vector dot product per cycle. During
the execution, a worktuple is mapped to one octet and thus each
threadgroup takes four DP units, respectively. The octet has operand
buffers to feed the worktuple via the tiled data when executing one
set of HMMA instructions. Each threadgroup has dedicated operand
buffers for Operand A and Operand C. Each operand buffer can
hold a 4Ã—4 tile. On the other hand, the operand buffer dedicated to
Operand B can hold a 4Ã—8 tile and the data inside are shared by the
two threadgroups in the same worktuple.
One threadgroup computes the multiplication of a 4Ã—4 tile by
a 4Ã—8 tile in a set of HMMA instructions, which is 4Ã—8=32 4-dim
dot products. Because four DP units compute four 4-dim vector
dot products per cycle, those set of HMMA instructions require at
least 8 clock cycles to finish the computing of the 4Ã—8Ã—4 matrix
multiplication.
4.2 The Extension of HMMA Instruction Set
With two threadgroups, a worktuple computes an 8Ã—8Ã—4 matrix
multiplication in a set of HMMA instructions. Such a set of the
HMMA instructions for the mixed precision mode are listed below.
Note that the four HMMA instructions must be used together and
in this particular order [53, 58].
â€¢ HMMA.884.F32.F32.STEP0 RD, RA, RB, RC;
â€¢ HMMA.884.F32.F32.STEP1 RD, RA, RB, RC;
â€¢ HMMA.884.F32.F32.STEP2 RD, RA, RB, RC;
â€¢ HMMA.884.F32.F32.STEP3 RD, RA, RB, RC;
Pipeline
Register
Ã— + +
+
+
Ã— Ã— Ã—
DP
(Dot Product)
Unit
Operand Bus 2
Operand Bus 3
Octet 3 Octet 2 Octet 1
Writeback
B 
Buf
A
Buf
Register File
Octet 0
Threadgroup 0
Tensor
Core
Tensor
Core
Ã—
+ FP16 Multiplier
FP32 Adder
Accumulator 
Buffer
1
2
3
Operand Bus 1
Threadgroup 4
A
Buf
Figure 11: Tensor Core architecture [58].
A register name in the HMMA instructions stands for a register pair.
Each register pair contains four FP16 operands. For example, if register RA is mapped to R28, it means the register pair, R27 and R28, is
holding the 4Ã—4 tile of data for A. The shared 4Ã—8 tile of B is loaded
from the RB. The result D is written back to RD. The instructions
for FP16 mode look similar, but have FP16 accumulators instead of
FP32 accumulators. Without loss of generality, we only consider
the mixed precision mode in this work, as prior profiling results
have shown that it has lower latency than the FP16 mode [58].
To run the vector-wise sparse matrix multiplication on the Tensor
Core, we create a vector-wise sparse mode and refer to the original
execution mode as dense mode. Figure 12 shows the sparse HMMA
(SHMMA) flow on the Tensor Core to execute the vector-wise
sparse matrix multiply and accumulate operation. The matrix A is
encoded into a 16Ã—4 matrix with the setting L=16 and K=4 so that
we map four rows of the encoded A to each worktuple. Worktuple i
computes Row 4i to Row 4i +3. Therefore, in the vector-wise sparse
mode, Tensor Core still computes a 16Ã—16 matrix multiply and
accumulate operation at warp level. Similar to Figure 5, the 16Ã—16
sparse matrix A is encoded to a 16Ã—4 data matrix and an associated
16Ã—4 offset matrix. Since all the offsets in this encoding are in the
range [0, 15], each offset only requires 4 bits in memory. Therefore,
each row of the encoded A only requires 16 bits to store the 4 offsets,
which means they can be stored in one register.
Different from the dense mode, the two threadgroups compute
the same row of A in the vector-wise sparse mode, as shown in
Figure 10 right. As illustrated in Figure 7, Row i of D is computed
by multiplying the four non-zero elements in Row i of A with the
corresponding four rows of B, respectively, and then accumulating
the results with Row i of C. The four rows of B to be multiplied are
determined by the four offset indices saved in the offset register.
To implement the vector-wise sparse mode, we extend the Tensor
Core instruction set by adding two SHMMA instructions and one
offset register:
â€¢ SHMMA.FETCHIDX RO;
â€¢ SHMMA.EXEC.F32.F32 RD, RA, RB, RC;
The instruction SHMMA.FETCHIDX fetches the offset indices of the
four elements in a row of A from RO to an implicit, dedicated offset
365
MICRO-52, October 12â€“16, 2019, Columbus, OH, USA Maohua Zhu, Tao Zhang, Zhenyu Gu, and Yuan Xie
Dense B
Set 0
Worktuple 0
Set 1
Set 2
Set 3
Worktuple 1
Worktuple 2
Worktuple 3
h
16

Dense C
16
4 6 8 9
4
6
8
9
A Offset
Instructions of Set 0:
SHMMA.FETCHIDX RO;
SHMMA.EXEC.F32.F32 RD, RA, RB, RC 
Figure 12: Sparse WMMA (SWMMA) execution flow on the
Tensor Core architecture. Operand A is vector-wise encoded
to a 16Ã—4 matrix and a 16-dim offset array. Each entry of the
offset array contains the 4 offset indices of the elements in
the associated row.
register. The instruction SHMMA.EXEC first decodes the offset register
to determine which rows of B to be fetched from RB. After the data
is loaded to operand buffer B, instruction SHMMA.EXEC.F32.F32
computes the 16 4-dim dot products and accumulates the results
with C. Instead of four HMMA instructions in one set in the dense
mode, two SHMMA instructions form one set in the sparse mode,
where the two instructions must work together and in order.
In the vector-wise sparse mode, each threadgroup computes 8
columns of B by a set of SHMMA instructions. Therefore, four sets
of SHMMA instructions are sufficient to complete the vector-wise
sparse WMMA operation (SWMMA), denoted as one swmma.mma
PTX instruction for sparse WMMA API. The SWMMA instructions
are shown below, where K stands for the number of non-zero elements in each row of the sparse matrix A. Note that we do not
show the load instructions for dense matrix B and C as well as the
store instruction for D since they are the same as the APIs of the
dense WMMA.
â€¢ Load A: swmma.load.a.K ra, [pa];
â€¢ Load Offset: swmma.load.offset.K ro, [po];
â€¢ Math: swmma.mma.f32.f32.K rd, ra, rb, rc, ro;
4.3 Micro-architecture Design for Sparse
Tensor Core
The SHMMA instructions require some modifications to the original
Tensor Core. We highlight the changes in Figure 11. 1 We first add
the dedicated offset registers in the register file. The offset registers
can only be implicitly accessed by the SHMMA instructions.
In the baseline Tensor Core architecture, the operand buffer
B only needs to hold 4Ã—8 FP16 numbers as an octet loads a 4Ã—8
tile in each set of the HMMA instructions [58]. To improve the
utilization of the DP units, 2 we not only double the buffer size to
accommodate the four rows of buffer B, but also add another buffer
to hide the load latency. In addition, 3 we enable the broadcasting
of operand buffer A to the four DP units it connects to so that all
DP units in an octet can read the same row of A. By doing this, a
threadgroup can compute the dot products of a row of A and four
columns of B per clock cycle.
We use Figure 13, i.e. the execution timeline of each mode, to
illustrate the performance benefit of SHMMA. In the dense mode
(Figure 13(a)), the operand buffers A and B are filled in 2 cycles,
8
Fetch A
Compute
2
Fetch A
Compute
2
Decode offset 1
Fetch B 2
2
8
2
2
8
2
2
8
2
2
Fetch B
Fetch offset 1
4
1
1
2
2
4
1
1
2
2
4
1
1
2
2
4
1
1
â€¦
2
Fetch A
Compute
2
Decode offset 1
Fetch B
Fetch offset 1
4
1
1
2
2
4
1
1
2
2
4
1
1
2
2
4
1
1
40 cycles
26 cycles
20 cycles
(a)
(b)
(c)
â€¦
â€¦
Explicit Register Fetch
Implicit Offset Decoding
Dot Product Operation
Figure 13: The execution timeline of (a) the dense mode, (b)
the vector-wise sparse mode without ping-pong buffer, and
(c) the vector-wise sparse mode with ping-pong buffer on the
Tensor Core.
followed by the execution of HMMA instructions that takes 8(=4Ã—2)
cycles to complete a 4Ã—8Ã—4 GEMM. As a result, the warp has to
take 40 cycles to complete the 16Ã—16Ã—16 GEMM computation in
the dense mode.
In the vector-wise sparse mode (Figure 13(b)), the instruction
SHMMA.FETCHIDX takes 1 cycle to load the offsets to the offset register. It then takes another cycle for the register file to decode the
offset register and set up the control signals for the operand buffer
Bâ€™s datapath. As the size of operand buffer B is doubled from 4Ã—8
to 4Ã—16 FP16 operands, it takes 4 cycles to load data to operand
buffer B3. Thanks to the vector-wise sparsity, the computation time
is reduced to only 2 cycles. As a result, the vector-wise sparse mode
only takes 26 cycles for the computation, a 1.54Ã— speedup.
One observation from Figure 13(a)(b) is that dense mode is
compute-bound while vector-wise sparse mode is memory-bound
(i.e., feeding data to operand buffer B). This is because the vectorwise sparse matrix multiplication has a much lower operational
intensity (i.e., FLOPs/Byte) [77] than the dense matrix multiplication. It also explains why a even higher sparsity does not help to
reduce the latency of the inference. To further improve the performance, we add another buffer to hide the register fetch latency. In
total, the design requires a 4Ã— larger buffer of B. With the pingpong buffer design, one buffer can be read by DPs while the other
is loading data from the register file. In this way, the total latency
of the vector-wise sparse WMMA is further reduced from 26 cycles
to 20 cycles (Figure 13(c)), an additional 1.3Ã— speedup. In fact, the
dense mode can also benefit from this larger buffer. However, since
it is compute-bound, dense mode sees moderate latency reduction,
from 40 cycles to 34 cycles. Considering the area overhead, it is not
worthy adding a ping-pong buffer for dense mode.
5 EXPERIMENTAL METHODOLOGY
The algorithm and hardware co-design aims to accelerate the inference phase of neural networks with minimal impact on the quality
of the models. To evaluate both the model accuracy and the speedup
over generic sparse neural networks and dense neural networks, we
3We assume the same buffer load bandwidth, which takes 2 cycles to fetch 4Ã—8 FP16
numbers to the buffer.
366
Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs MICRO-52, October 12â€“16, 2019, Columbus, OH, USA
picked five popular neural networks in three domains: image classification, image captioning, and machine translation. We trained the
neural networks with the generic sparsifying method [30] and the
proposed VectorSparse method, respectively. The training was done
on a single DGX-1 station with four NVIDIA Tesla V100 GPUs.
To evaluate the performance of the vector-wise sparse mode
of the Tensor Core, we extended the WMMA PTX code model in
the GPGPU-Sim simulator [5, 37, 42, 58]. We added the SHMMA
instructions to the simulator with the timing parameters given in
Figure 13. We configured the simulator to model a Tesla V100 GPU
with Tensor Core [53]. The simulated V100 GPU has 80 SMs with
640 Tensor Cores and 5120 CUDA Cores inside. Equipped with
16GB HBM2 [41], V100 has 900 GB/s device memory bandwidth.
For the image classification applications, we pruned and retrained CNNs with the ImageNet [16] dataset, which comprises 1.2
million training examples and 50 thousand validation examples. To
verify that our vector-wise sparsifying methods can work for commercial applications, we selected four popular CNNs, AlexNet [39],
VGG-16 [67], ResNet-50 [31], and ResNeXt-50 [79] on the ImageNet
ILSVRC-2012 dataset. AlexNet and VGG-16 are two popular networks that achieved high accuracy on the ILSVRC-2012 dataset.
ResNet-50 has residual layers which make the training process easier for very deep neural networks. Furthermore, we also evaluated
the ResNeXt-50 [79] with 32x4d configuration to show the accuracy
impact on modern networks. We used the networks in TensorFlow
model repository [26] as the reference.
In addition to CNNs, we also examined Long Short-Term Memory (LSTM) [33] as a representative of RNNs. We used the Show and
Tell model [74] for the image captioning experiment. The image
captioning model consists of an Inception V3 model [72] with an
LSTM layer attached to the last layer of the CNN. The LSTM layer
has 512 cells by default. Since our interest is only in the LSTM layer,
we used a pre-trained Inception V3 model and randomly initialized the parameters of the LSTM layer similar to what was done
in the original Show and Tell work [74]. The training dataset is
MSCOCO [46] and the mini-batch size is set to 64. We trained the
model for 500K steps (about 55 epochs under default configuration) in each retraining phase. To quantify the quality of generated
captions, we calculated the BLEU [55] score for each training configuration on the MSCOCO test dataset. The code we used is taken
from the TensorFlow model zoo [25].
For the machine translation application, we trained an encoderdecoder architecture with an attention mechanism to perform Neural Machine Translation (NMT) [7, 50]. We use an architecture with
a 2-layer LSTM encoder, a 4-layer LSTM decoder, and an attention
module. The first layer of the LSTM encoder is bidirectional while
the rest of LSTM layers are unidirectional. Both the unidirectional
and bidirectional layers have 512 LSTM cells. In the experiments,
we also used the BLEU score [55] as the metric for the neural machine translation model. The WMT 16 English-German dataset [1]
is used for training. We followed the instructions to reproduce the
NMT training with an open source framework [24], except that
only 16 epochs are used in the retraining phases. This simplification
stems from our observation that the validation BLEU score becomes
stable after 12 epochs.
In our experiments, the workloads were first trained with their
default training methods to achieve the reference model accuracy.
Then we applied our VectorSparse pruning method to the reference dense models. We use FP32 for the weights, activations, and
gradients in the training process and CUDA Core based inference
kernels. For Tensor Core based kernels, we dynamically downsized
the FP32 weights and input activations to FP16 in each layer to
avoid accuracy loss. Since we use the mixed precision mode, the
output activations are still in FP32.
On the software side, we implemented vector-wise sparse matrix multiplication kernels based on CUTLASS [52], rather than
CUBLAS due to the unavailability of its source code. CUTLASS is
an open-source high-performance GEMM template library. It can
achieve near-CUBLAS performance for most GEMM problems [52].
This library provides C++ GEMM interfaces and allows the data
streams to be customized for GEMM-like computation. The CUDA
Core based vector-wise sparse matrix multiplication kernels are
built on the SGEMM kernels. The sparse Tensor Core based kernels
are similar to the WMMA-GEMM kernels, but we call our sparse
WMMA API instead of the dense WMMA API. The convolution
operations in the CNN workloads are converted to GEMM by the
im2col [10] method.
Besides the dense baseline, we also trained the workloads with
the unified pruning method [76] and compared it against our VectorSparse method. For the sake of a fair comparison, we iteratively
reduced the number of columns or channels of the weights and
retrained the networks with the same setting as our VectorSparse
method used. Then we evaluated the test accuracy of the trained
neural network models.
6 EXPERIMENTAL RESULTS
To validate the proposed vector-wise pruning method, we first
present the accuracy of the models pruned by our VectorSparse
method with various configurations. And then we show the performance gain of the pruned vector-wise sparse networks with sparse
Tensor Core design. Finally, we do the design overhead analysis on
the sparse Tensor Core.
6.1 Impact on Accuracy
To demonstrate the generality of our work, we applied our VectorSparse pruning to various workloads. Figure 14 and 15 show
the validation accuracy of the CNN and RNN models pruned by
VectorSparse with L=16, the generic pruning method, and the unified pruning method, respectively. We also add the result of L=8 to
show how badly the model accuracy drops for each workload when
sparsity increases. In fact, we also run experiments with L >16 and
found a marginal impact on the accuracy, regardless of the sparsity. Since the accuracy is insensitive to the vector size once L is
greater than 16, we choose L=16 as the optimal size, which requires
only 4 bits for storing the offset indices and enables finer-grained
tiling strategies. During the pruning and retraining process of the
VectorSparse pruning and the unified pruning, we recorded the
validation accuracy of each step. The accuracy is represented in the
relative deviation from the reference dense models.
As illustrated in Figure 14, all CNN models pruned with VectorSparse with L=16 can retain their accuracy until the sparsity
reaches 80%. Similarly, the accuracy of the RNNs is comparable
to the reference model when the sparsity does not exceed 75%, as
367
MICRO-52, October 12â€“16, 2019, Columbus, OH, USA Maohua Zhu, Tao Zhang, Zhenyu Gu, and Yuan Xie
-10%
-8%
-6%
-4%
-2%
0%
2%
20% 30% 40% 50% 60% 70% 80% 90% 100%
Relative Accuracy
Vector Sparsity
AlexNet, vector-wise, L=16 VGG-16, vector-wise, L=16 ResNet-50, vector-wise, L=16
ResNeXt-50, 32x4d, vector-wise, L=16 AlexNet, vector-wise, L=8 VGG-16, vector-wise, L=8
ResNet-50, vector-wise, L=8 ResNeXt-50, 32x4d, vector-wise, L=8 AlexNet, generic
VGG-16, generic ResNet-50, generic ResNeXt-50, 32x4d, generic
AlexNet, unified VGG-16, unified ResNet-50, unified
ResNeXt-50, 32x4d, unified
Less than 1% diff 
at 75% sparsity
Figure 14: Accuracy vs. sparsity in the weight matrices of
CNN workloads. Each workload runs with generic and unified pruning as described in Section 2.2. VectorSparse pruning is run with L=8 and 16, respectively. All accuracy results
are normalized to the dense baseline.
shown in Figure 15. Although the generic pruning method outperforms the VectorSparse with L=16 when the sparsity is higher than
80%, the generic sparse matrix is not necessarily able to be encoded
into a vector-wise sparse format, if top-K elements concentrate on
a few rows.
On the other hand, if vector size is set to L=8, the model accuracy
drops more quickly than that of L=16. At the point of 75% sparsity,
the L=8 scheme suffers more than 2% accuracy loss, which is usually
unacceptable in many applications. This is because L=8 puts too
much spatial constraint on the element removal. Figure 15 shows
that RNNs are more resilient to the L=8 pruning than the CNNs.
However, their accuracy of L=8 pruning is still incomparable to
that of L=16.
Based on the results, it is clear that there is a trade-off between
sparsity and accuracy. Given a vector size L, higher sparsity can
achieve better performance by sacrificing the accuracy. In this
work, the accuracy is the first-order metric so we opt out the L=8
scheme. However, we do believe that L=8 can be useful in some
application domains. Even though it is out of the scope of this paper,
we recommend that the trade-off should be done case by case.
Another observation is that unified pruning incurs the most
significant accuracy drop for both CNNs and RNNs. By cutting the
number of weight columns by half, the CNN and RNN models see
more than 2% accuracy loss at the point of 50% sparsity. Due to
the spatial constraint, the unified pruning has too few options for
removing weights in each step of the retraining process. The lack of
flexibility in turn limits the representative power of the network. In
contrast, the vector-wise pruning has more freedom on removing
weights so that it can even result in a similar topology to the generic
pruning when L is not extremely small.
-9%
-8%
-7%
-6%
-5%
-4%
-3%
-2%
-1%
0%
1%
20% 30% 40% 50% 60% 70% 80% 90% 100%
Relative BLEU Score
Vector Sparsity
Show and Tell, vector-wise, L=16 NMT, vector-wise, L=16
Show and Tell, vector-wise, L=8 NMT, vector-wise, L=8
Show and Tell, generic NMT, generic
Show and Tell, unified NMT, unified
Less than 1% diff 
at 75% sparsity
Figure 15: BLEU score vs. sparsity in the weight matrices of
RNN workloads.
In summary, with L=16 vector-wise pruning, 75% sparsity is
good enough to assure the accuracy. In other words, we can keep
only 4 non-zero elements in each 16-dim vector. Since the warp
size 32 is a multiple of 164, letting L=16 is also favorable for CUDA
Cores. Therefore, we choose L=16 and 75% sparsity as the optimal
configuration in the performance evaluation.
6.2 Performance Evaluation
We run the workloads under six different configurations to evaluate
the performance of VectorSparse pruning method and the hardware
design in Tensor Core. The baseline is CUDA Core Dense, where the
dense NN workloads are running on the FP32 CUDA Cores. We also
evaluate CUDA Core Generic and CUDA Core Unified, where NN
workloads are pruned by generic and unified sparsifying method,
respectively. The CUDA Core Generic has 96% sparsity and CUDA
Core Unified has 50% sparsity. Then, we examine Tensor Core Dense,
where the dense NNs are running on the Tensor Core. In this case,
the weights and input activations are dynamically converted to
FP16 in each layer while the accumulators are still in FP32. The
conversion causes negligible accuracy impact.
Furthermore, CUDA Core Vector-wise Sparse is evaluated to justify
the VectorSparse pruning method. Finally, we evaluate Tensor Core
Vector-wise Sparse as our proposal, where vector-wise sparse NNs
are running on sparse Tensor Core. Since Tensor Core does not
support generic or unified sparse GEMMs, we opt out the options of
running generic or unified sparse NNs on Tensor Core. Both CUDA
Core Vector-wise Sparse and Tensor Core Vector-wise Sparse have
75% sparsity. Note that even with the relatively high sparsity, the
accuracy, or BLEU score, of Vector-wise Sparse is still higher than
CUDA Core Unified.
Figure 16 shows the inference performance after the CNNs and
RNNs have been trained and pruned (if necessary). All results are
normalized to CUDA Core Dense. Unsurprisingly, Tensor Core
4AMD uses a different terminology called wavefront. One wavefront consists of 64
threads, which is still a multiple of 16.
368
Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs MICRO-52, October 12â€“16, 2019, Columbus, OH, USA
0.89 0.86 0.86 0.92
0.64
0.77 0.82
1111111
1.63 1.64
1.32 1.34
1.84
1.31
1.50
1.8
1.63 1.56 1.48
1.81
1.53 1.63 1.63 1.75
2.06
1.93
1.68
1.42
1.73
2.33
2.54
3.17
2.84
2.50
2.17
2.57
0
0.5
1
1.5
2
2.5
3
3.5
AlexNet VGG-16 ResNet-50 ResNeXt-50 Show and Tell NMT GeoMean
CUDA Core Generic (96% Sparsity) CUDA Core Dense
CUDA Core Unified (50% Sparsity) CUDA Core Vector-wise Sparse (75% Sparsity)
Tensor Core Dense Tensor Core Vector-wise Sparse (75% Sparsity)
1.49x
Figure 16: Normalized speedup over CUDA Core based dense
NNs on V100 GPU. The vector-wise sparse NNs have 75%
sparsity. The generic pruned sparse NNs have 96% sparsity. The unified pruned sparse NNs have 50% sparsity. All
the vector-wise sparse NNs have better accuracy than the
generic pruned NNs and the unified pruned NNs as shown
in Figures 14 and 15.
Dense is faster than CUDA Core Dense since Tensor Core has
higher TFLOPs than CUDA Core [53]. On the other hand, CUDA
Core Generic is 18% slower than CUDA Core Dense, even if the
former has 96% sparsity (i.e., only 4% computation is needed). The
slowdown testifies the inefficiency of GPU to support generic sparse
NNs, which is the motivation for this work. Alternatively, CUDA
Core Unified on average has 1.50Ã— speedup over CUDA Core Dense.
The gain mainly comes from the half size of the dense WMMA
operations.
On average, Tensor Core Vector-wise Sparse can achieve 2.57Ã—
speedup over the baseline. The root cause of the performance gain
is two fold. First, with a relaxed spatial constraint, our vector-wise
sparse NNs benefit from the high sparsity so that CUDA Core
Vector-wise Sparse has 63% performance gain than the baseline.
Secondly, with the customized SHMMA instructions and microarchitecture design, these sparse vector-wise NNs can take advantage of the powerful Tensor Core, which contributes an additional
58% performance improvement versus the CUDA Core Vector-wise
Sparse. Also note that Tensor Core Vector-wise sparse has 1.49Ã—
speedup over Tensor Core Dense.
6.3 Design Overhead Analysis
In the vector-wise sparse mode of the Tensor Core, the hardware
design requires a 4Ã— large buffer for Operand B to hold 4Ã—16 FP16
numbers and enable the ping-pong buffer. To support the row indexing for Operand B, an offset register is added for each octet.
The original size of Operand B buffer in each octet is 512b
(=4Ã—8Ã—16b), and each Tensor Core has two octets, which makes the
buffer size 1Kb. Our vector-wise sparse mode requires a 4Ã— large B
buffer so that a 4Kb buffer is added to each Tensor Core. As each
SM has 8 Tensor Cores, it needs a 4KB buffer. We use CACTI7 [6]
to evaluate the timing and area overhead shown in Table 1. A 4KB
SRAM takes 0.019mm2 at 22nm process node. The 0.4ns cycle time
is smaller than V100â€™s nominal cycle period (0.65ns at 1530MHz),
which does not incur any timing overhead.
Table 1: Design Overhead Analysis via CACTI7 [6]
Process SRAM Size Area Cycle Time
22nm 4KB 0.019mm2 (0.069Ã—0.275) 0.4ns
As V100 is fabricated in 12nm, we further scale the area down
to 0.007mm2. In addition, a Tensor Core needs two extra registers
serving as the offset register for the two octets, so an SM needs 16
extra offset registers to fetch the operands to buffer B. Given V100â€™s
area is 815mm2, the overall area overhead is negligible.
6.4 Summary and Discussion
The experimental results show that our VectorSparse pruning method
could be used in the same way as the generic pruning. Although
the sparsity is lower than the generic sparse NNs, the vector-wise
sparse NNs enable modern GPUs to efficiently exploit the benefit
from the weight pruning. Compared to the CSR format, our vectorwise encoding eliminates the row indices and allows each row to
be split evenly in order to guarantee thread-level parallelism.
The evaluation of the vector-wise pruned networks also shows
that the vector-wise sparse NNs with L=16 and 75% sparsity have
negligible accuracy drop and promising speedup over their dense
counterparts. To further boost the performance of these NNs, we
added hardware support to the Tensor Core to enable the vectorwise sparse matrix multiplication. Though we suggest L=16 and
K=4 as the current solution, the vector-wise sparse Tensor Core
can be easily extended into many variants. As the quick evolution
of the neural networks, it is possible that the sparse Tensor Core
will become compute-bound again. We would like to leave this as
our future work.
7 CONCLUSION
In this work, we observe that the generic sparse neural networks
can hardly beat the dense neural networks on modern GPUs because of the highly optimized software and hardware support for
dense GEMM. To efficiently exploit the intrinsic redundancy of the
neural networks, we propose VectorSparse, a vector-wise pruning
method that guarantees the pruned networks to have a balanced
workload. The encoding for the vector-wise sparse matrices requires only 1-dim, fixed-length offset indices, instead of the 2-dim,
variable-length indices for the generic sparse matrices. With the
VectorSparse pruning method, we can prune a neural network to
have 75% sparsity with negligible impact on the model accuracy.
The good workload balance in the vector-wise sparse matrix multiplication makes it 63% faster than the dense counterparts on the
GPU CUDA Cores. To further improve the performance of the
vector-wise sparse matrix multiplication, we enabled it to run on
the Tensor Core by adding a sparse mode with extended instruction set and hardware support. With negligible area overhead, our
sparse Tensor Core can achieve 1.49Ã— speedup over the dense mode
of the Tensor Core with comparable model accuracy.