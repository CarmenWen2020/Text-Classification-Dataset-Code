ABSTRACT
Resource control in heterogeneous computers built with subsystems
from different vendors is challenging. There is a tension between
the need to quickly generate local decisions in each subsystem
and the desire to coordinate the different subsystems for global
optimization. In practice, global coordination among subsystems is
considered hard, and current commercial systems use centralized
controllers. The result is high response time and high design cost
due to lack of modularity.
To control emerging heterogeneous computers effectively, we
propose a new control framework called Tangram that is fast, globally coordinated, and modular. Tangram introduces a new formal
controller that combines multiple engines for optimization and
safety, and has a standard interface. Building the controller for a
subsystem requires knowing only about that subsystem. As a heterogeneous computer is assembled, the controllers in the different
subsystems are connected hierarchically, exchanging standard coordination signals. To demonstrate Tangram, we prototype it in
a heterogeneous server that we assemble using components from
multiple vendors. Compared to state-of-the-art control, Tangram reduces, on average, the execution time of heterogeneous applications
by 31% and their energy-delay product by 39%.
CCS CONCEPTS
• Computer systems organization→Heterogeneous (hybrid)
systems; • Hardware → Platform power issues; Chip-level power
issues; Temperature control.
KEYWORDS
Distributed resource management, formal control, heterogeneous
computers, modular control.
1 INTRODUCTION
An emerging trend in today’s computing systems is to integrate
subsystems built by different vendors into heterogeneous computers [10, 45, 47, 55, 77, 79, 86]. Such subsystems can be CPUs, GPUs,
or various accelerators. This approach is attractive because the
individual components are often easier and cheaper to develop
separately, and they can be reused across multiple products. For
example, the same GPU design is used in AMD’s Ryzen™ mobile
processors [8] and in Intel’s multi-chip Core i7-8809G [30].
Such heterogeneous systems, like other computers, need a resource control system — a vital unit that keeps the execution efficient and safe. Resource controllers attain efficiency by customizing
the usage of limited resources like energy and storage to match
application requirements. They also protect the hardware from
hazardous conditions like high temperature or high current variation with time (dI/dt) [40]. Computers today are increasingly being
equipped with microcontrollers that monitor execution conditions,
run control algorithms, and actuate a set of configurable parameters
in the computer [15, 17, 19, 41].
Building resource controllers is challenging. There is a tension
between the need to generate local decisions in each subsystem
quickly for timely response and the desire to coordinate the different
subsystems for global optimization. Global coordination is especially challenging in heterogeneous computers with multi-vendor
subsystems, as one needs to compose logic from different vendors
that was designed oblivious of the full system configuration.
The current approach chosen by industry is to use centralized
decision-making [3, 5, 15, 17, 19, 41, 67], despite the availability of
per-subsystem sensors and actuators. The reason is the difficulty
of composing independently designed controllers for system-wide
efficiency [19, 57, 58]. There are many heuristic policies that are
difficult for designers to develop even within a single subsystem
like a CPU; it is even harder to redesign such logic to make it work
across subsystems [19].
Researchers too have examined the joint optimization of multiple hardware subsystems. For example, they have optimized the
combination of CPU and GPU [57, 58], GPU and memory [56], CPU
and memory [23], multiple cores in a multicore [14, 27, 63, 82, 83],
and servers in a datacenter [62]. In many of these works, however,
the decision-making is centralized [14, 23, 27, 56–58, 82, 83]. In
addition, many of these systems also rely on heuristics.
384
MICRO-52, October 12–16, 2019, Columbus, OH, USA Pothukuchi et al.
GPU CPU
DPTF
Sensors Inputs
OCC 
Master
GPU
GPU
GPU
GPU
OCC 
Slave
SMU 
Master
SMU 
Slave
SMU 
Slave
SMU 
Slave
GPU
Other socket
(a) Intel DPTF [22] GPU CPU
DPTF
Sensors Inputs
OCC 
Master
GPU
GPU
GPU
GPU
OCC 
Slave
SMU 
Master
SMU 
Slave
SMU 
Slave
SMU 
Slave
GPU
BAPM
Other socket
(b) IBM OCC [67] GPU CPU
DPTF
Sensors Inputs
OCC 
Master
GPU
GPU
GPU
GPU
OCC 
Slave
SMU 
Master
SMU 
Slave
SMU 
Slave
SMU 
Slave
GPU
Other socket
(c) AMD SMU [3, 17]
Figure 1: State-of-the-art resource control architectures for heterogeneous computers from leading vendors.
To control emerging heterogeneous computers effectively, we
need a new approach that is fast, globally coordinated, and modular.
This paper presents such an approach, based on a new control
framework called Tangram. Tangram is a decentralized framework
for fast response time. However, decentralization does not come at
the expense of global optimization. Further, Tangram is modular,
so it can be used in different computer configurations.
Tangram introduces a new controller that combines an optimizing engine from Robust Control [73] with fast engines for safety
and reconfiguration. This controller has a standard interface and is
present in each subsystem of the computer. As a heterogeneous computer is built by assembling different subsystems, the controllers
of the subsystems are also connected hierarchically, exchanging
standard coordination signals. The resulting Tangram control framework is fast, globally coordinated, and modular. It is fast because
each controller makes decisions on its own local subsystem immediately. It is globally coordinated because coordination signals
propagate information and constraints between controllers. Finally,
it is modular because each controller is built with knowledge of
only its subsystem and has a standard interface.
We prototype Tangram in a multi-socket heterogeneous server
that we built using components from three different vendors. The
server has two quad-core CPU chips and a GPU chip. The controllers use a robust control theoretic design. They run as privileged software, accessing the System Management Units (SMUs)
of the subsystems. Compared to state-of-the-art control, Tangram
reduces, on average, the execution time of heterogeneous applications by 31% and their Energy-Delay Product (EDP) by 39%. The
contributions of the paper are:
• Tangram, a fast, globally coordinated, and modular control
framework to manage heterogeneous computers.
• A novel controller design that combines multiple engines
and uses formal control principles.
• A prototype of Tangram in a server that we build using
components from three different vendors, and its evaluation.
2 COMPUTER CONTROL TODAY
Controlling the operation of heterogeneous computer systems is a
challenging problem that is currently addressed in different ways.
2.1 Organization
Most controllers from leading vendors are centralized (Figure 1).
As shown in Figure 1a, Intel uses the Dynamic Power and Thermal
Framework (DPTF) to manage the CPU and GPU in their multi-chip
Core i7-8809G [19, 22]. The DPTF is a centralized kernel driver. Each
chip exposes sensor data and allows DPTF to set its controllable
inputs.
Figure 1b shows the On Chip Controller (OCC) in IBM POWER9.
It is a centralized hardware controller that actuates each on-chip
core and the GPUs attached to the chip [66, 67]. In a 2-socket system,
one OCC becomes the master for global control, and the other a
slave with restricted decision-making. The slave OCC is limited to
sending sensor data and applying input values given by the master.
Figure 1c shows the hardware System Management Unit (SMU)
design from AMD [17]. AMD’s EPYC™ and Ryzen™ processors
consist of one or more dies, each of which has one SMU. When
multiple dies are used in a socket, one SMU becomes the master
and the others are slaves, as with IBM. The slave SMUs handle only
events like high temperatures or current, where quick response
is necessary. The master SMU makes centralized decisions for the
modules in all the dies in the system. In a two-socket system, there
is a single master for both sockets.
When an on-die GPU is integrated with the CPU, AMD uses the
Bidirectional Application Power Management (BAPM) algorithm,
which is a centralized algorithm running on firmware to manage
power between the CPUs and GPU [3]. With discrete GPUs, there is
no communication channel between the CPU and GPU controllers.
Therefore, system-level coordination needs to be handled through
software drivers, as with the Intel design above.
Centralized hardware control is also the choice in research [14,
23, 27, 44, 56–60, 82, 83]. Unfortunately, centralized control is slow
because data and decisions must cross chip boundaries, and experience contention at the single controller. It is also non-modular
because integrating a new component or using a different configuration of the subsystems requires a full re-design of the controller.
As an alternative, Raghavendra et al. [62] describe a Cascaded
design for power capping in datacenters. The proposal is shown in
Figure 2. The datacenter is organized as a set of enclosures, each
containing a set of server blades. A Group divider splits the total
power budget among the enclosures. Then, the Enclosure divider in
each enclosure splits its designated power level Penclosur e among
its blades. Then, each blade supervisor (Sup) receives its designated
power level Pbl ade , and enforces it by providing a target for a
PID controller. The PID controller changes the blade frequency to
achieve the target. The enclosure divider and the blade supervisor
always keep the power of the enclosure and blade at the respective
Penclosur e and Pbl ade values they receive.
While this design is scalable, it has the limitation that the dividers
are not controllers that could optimize the system; they just divide
the power budget. In addition, changing a blade’s power budget
385
Tangram: Integrated Control of Heterogeneous Computers MICRO-52, October 12–16, 2019, Columbus, OH, USA
Enclosure 
divider
+– PID Blade
Penclosure Pblade Utilblade
Sup
+– PID Blade
Pblade Utilblade Sup
Group 
divider Enclosure
Group
Enclosure
Enclosure 
divider
+– PID Blade
Penclosure Pblade
Sup
+– PID Blade
Pblade Sup
Group 
divider Enclosure
Group
Enclosure
+– PID Blade
Group 
divider
Enclosure 
divider
Penclosure Pblade Sup
+– PID Blade
Pblade Sup
Enclosure
Group
Enclosure
Figure 2: Cascaded control of a datacenter.
requires a long chain of decisions. When it is necessary to increase
the blade’s power, the group divider first increases Penclosur e , after
which the enclosure divider can raise Pbl ade . Then, the supervisor
changes the target and, finally, the PID controller can increase
frequency. As the group divider’s decision loop is much slower than
the innermost PID controller [62], there is a long delay between
the need for a power increase and the actual increase. This may
cause suboptimal operation and instability [12].
2.2 Controller Objectives
There are Safety controllers that protect the system from dangerous
conditions (e.g., high current or voltage droops), and Enhancement
controllers that optimize the execution for goals like power, performance, or EDP [17]. Safety controllers usually provide continuous
monitoring and an immediate response, while enhancement controllers periodically search through a multidimensional trade-off
space for the best operating point. In current industrial designs,
these two types of controllers typically operate in a decoupled
manner [17].
Designs from research typically focus on enhancement, disregarding interaction with safety mechanisms. Some exceptions are
works that consider temperature as a soft constraint (e.g., [34, 60,
62]), or those that probabilistically characterize safety mechanisms
like circuit breaker tripping (e.g., [27]).
2.3 Formal Control vs Heuristics
Current industrial designs typically use heuristics for resource
control [3, 37, 61, 67, 69, 74]. A few use heuristics plus PID controllers [16, 17, 19, 41]. In most cases, controllers monitor one single
parameter (i.e., output), like power or skin temperature, and actuate a single parameter (i.e., input), like frequency. Hence, they
are Single Input Single Output (SISO). Often, multiple controllers
actuate the same input, such as the CPU frequency. In this case, the
conflicting decisions are combined using heuristics. For example,
IBM’s OCC assigns each controller a vote and a majority algorithm
sets the input [66].
Heuristics can result in unintended inefficiencies [28, 42, 44,
59, 78, 84]. Further, they make it difficult to decentralize resource
control [19], which is necessary for fast response and modularity.
Paul et al. [57, 58] show real examples of how multiple controllers
using heuristics fail to coordinate in a system with a CPU and GPU.
For instance, in a workload whose performance is limited by the
GPU, CPU heuristics see low CPU memory traffic and boost CPU
frequency. This does not improve performance and wastes power.
3 BACKGROUND: ROBUST CONTROL
Robust control focuses on controlling systems with only partial
information [73]. Figure 3 shows a robust control loop. S is the
system (e.g., a multicore) and K is the robust controller. The system
has outputs y (e.g., power consumed) and configurable inputs u (e.g.,
frequency). The outputs must be kept close to the output targets r.
The controller reads the deviations of the outputs from their targets
(∆y = r − y), and sets the inputs. In this paper, we use controllers
with Multiple Inputs Multiple Outputs (MIMO); they can set several
inputs to meet several output targets simultaneously. This removes
the need for several piecemeal optimization algorithms.
Component
Arbiter
Safety 
engines
Coordination 
signals Full Component Controller
Outputs
Inputs
Mode 
Detector Preconfigured 
engines
Robust 
Control Block
System 
S
Robust Controller 
K
Inputs 
u
Outputs 
y
Output targets 
r
Output deviations 
Δy
+ K ─
Δmargin 
Issued 
inputs
Actual 
inputs + S
+
+–
Component Robust 
Controller 
Targets Inputs Outputs
+– Planner 
Coordination 
signals Robust Control Block
Robust 
Controller
Issued 
inputs
Actual 
inputs Component
Uncertainty Δmargin
+– +
–
Figure 3: Robust control loop.
The controller is a state machine characterized by a state vector,
x(T ), which is its accumulated experience that evolves over time
T . The controller advances its state to x(T + 1) and generates the
system inputs u(T ) by reading the output deviations ∆y(T ) and
using its state x(T ):
x(T + 1) = A × x(T ) + B × ∆y(T )
u(T ) = C × x(T ) + D × ∆y(T )
x(0) = 0 (1)
where A, B, C, and D are matrices that encode the controller. Equation 1 is similar to how non-robust controllers like LQG [59] or
MPC [44] operate. However, with robust control, the controller K
has special properties that are relevant for this work.
First, designers can specify how big the output deviations can be
for each output. Hence, important outputs can be set with tighter deviation bounds. Designers can also specify the relative overheads of
changing each individual input. Then, the controller will minimize
the changes to the inputs with higher overheads.
Second, the controller can be built with only a partial model
of the true system. All unmodeled behavior is considered “uncertainty”, and designers specify the worst case impact of such
uncertainty, called Uncertainty Guardband. For example, a 50% uncertainty guardband means that the true system’s outputs can be up
to 50% different from what the model predicts. The controller guarantees to keep the output deviations within bounds even though it
was built with this much inaccurate system information.
Robust controller design is automated [32] and tools aid designers in setting the controller parameters. Recently, robust control
has been used in Yukta [60], to co-design controllers for different
layers in the computing stack (e.g., hardware and OS).
4 TANGRAM: DECENTRALIZED CONTROL
Our goal is to design and prototype a control framework for heterogeneous computers that is decentralized, globally coordinated, and
modular. Decentralization is needed for fast control. However, it
should not come at the expense of global optimization. Further, the
386
MICRO-52, October 12–16, 2019, Columbus, OH, USA Pothukuchi et al.
framework should be modular to be usable in different computer
configurations. These requirements rule out the conventional centralized and cascaded organizations. Moreover, for effectiveness,
the controllers in this framework should combine safety and enhancement functionalities, be formal rather than heuristic-based,
and be MIMO. We call our new framework Tangram.
In this section, we start by introducing the novel controller in
Tangram, and then the Tangram modular control framework. Later,
in Section 5, we build a Tangram prototype.
4.1 Controller Architecture
To the Safety and Enhancement types of controllers, we add a third
one, which we call Preconfigured. Table 1 shows the controller differences and the control strategies they follow. Section 2.2 described
safety and enhancement controllers. A preconfigured controller
looks for a certain well-known operating condition. When the execution is under such a condition, the preconfigured controller uses a
preset decision to bring the system to an optimal configuration. The
priority of preconfigured controllers is lower than safety controllers
but higher than enhancement controllers.
Table 1: Types of controllers.
Safety Enhancement Preconfigured
Goal: Hardware safety Optimality Optimality
Strategy: Simple, preset Complex, search based Simple, preset
Priority: Highest Low Medium
Operation: Nearly always Periodic Nearly always
Response time: Immediate Fast Immediate
In Tangram, we propose to build a controller that combines
enhancement, safety, and preconfigured engines.
4.1.1 Enhancement Engine. We use robust control [73] to build a
MIMO enhancement controller. The controller monitors all local
outputs to be optimized, like performance, power, and temperature,
and sets the local inputs to keep all outputs close to the desired targets. It works with a Planner, which changes these targets to match
changing conditions and to optimize metrics combining multiple
outputs like EDP. The combination of controller and planner is the
enhancement engine (Figure 4).
System
Arbiter
Safety 
Engines
Coordination 
Signals Full Controller
Outputs
Inputs
Mode 
Detector Preconfigured 
Engines
Enhancement 
Engine
System 
S
Robust Controller 
K
Inputs 
u
Outputs 
y
Output targets 
r
Output deviations 
Δy
+ K ─
Δmargin 
Issued 
inputs
Actual 
inputs + S
+
+–
Component Robust 
Controller 
Targets Inputs Outputs
+– Planner 
Coordination 
signals Robust Control Block
Robust 
Controller
Issued 
inputs
Actual 
inputs Component
Uncertainty Δmargin
+– +
–
System Robust 
Controller 
Targets Inputs Outputs
+– Planner 
Coordination 
Signals Enhancement Engine
Figure 4: Enhancement engine.
For example, to minimize EDP, the planner can search along
two directions: increasing performance targets more than power
targets, or decreasing power targets more than performance targets.
For each target point selected, the robust controller will determine
what inputs can make the system outputs match the targets. The
planner then computes the EDP and may select other performance
and power targets that may deliver a better EDP. In Appendix A,
we describe a generic search algorithm for the planner. Algorithms
like Gradient Descent can also be used to search for the best targets.
The planner is also the point of communication with other controllers, if any, and exchanges coordination signals with them. It
uses some of these incoming signals to generate the local targets.
4.1.2 Adding Safety and Preconfigured Engines. We add a safety engine that continuously monitors for hazards like high temperature
or current. If the engine is triggered, it picks the most conservative
values of the inputs. This is done without any search overhead. For
example, if the computer overheats, the safety engine simply runs
the cores at the lowest frequency.
Similarly, we add a preconfigured engine that continuously monitors for execution conditions that are well understood and for
which there is an optimal configuration. If the engine is triggered,
it sets the inputs to a predefined configuration, skipping any search
by the enhancement engine. For example, if there is a single thread
running, the engine boosts the active core’s frequency, and powergates the other cores.
Figure 5 shows the full architecture of our controller, with potentially multiple safety and preconfigured engines in parallel with
the enhancement engine. All engines are connected to an arbiter. A
mode detector chooses one engine by controlling the arbiter. Each
engine can monitor potentially different outputs.
System
Arbiter
Safety 
Engines
Coordination 
Signals Full Controller
Outputs
Inputs
Mode 
Detector Preconfigured 
Engines
Enhancement 
Engine
System 
S
Robust Controller 
K
Inputs 
u
Outputs 
y
Output targets 
r
Output deviations 
Δy
+ K ─
Δmargin 
Issued 
inputs
Actual 
inputs + S
+
+–
Component Robust 
Controller 
Targets Inputs Outputs
+– Planner 
Coordination 
signals Robust Control Block
Robust 
Controller
Issued 
inputs
Actual 
inputs Component
Uncertainty Δmargin
+– +
–
System Robust 
Controller 
Targets Inputs Outputs
+– Planner 
Coordination 
Signals Enhancement Engine
Figure 5: Our proposed controller.
The mode detector uses the following priority order to select the
engine that sets the system’s inputs: (i) any active safety engine,
starting from the most conservative one, (ii) any active preconfigured engine, starting from the most conservative one, and (iii) the
enhancement engine.
At runtime, the enhancement engine optimizes the system. It
may inadvertently trigger a safety engine, which then sets the
inputs to the lowest values. The change induced by the safety
engine is within the uncertainty guardband used in the controller’s
design. Once the hazardous condition is removed, the enhancement
engine resumes operation but it remembers (using its state) to avoid
further safety triggers. Thus, the enhancement engine can optimize
inputs without repeated conflicts with the safety engines [25].
4.2 Subsystem Interface
Computers are organized as a hierarchy of subsystems, possibly
built by different manufacturers. For example, a motherboard that
387
Tangram: Integrated Control of Heterogeneous Computers MICRO-52, October 12–16, 2019, Columbus, OH, USA
contains a GPU and a CPU chip is a subsystem, and a multicore
chip that contains multiple cores is also a subsystem. To build
decentralized, globally-coordinated modular control, we propose
that each subsystem has a controller with a standard interface.
Figure 6 shows the interface. To understand it, we logically break
a subsystem into its controller and the rest of the subsystem, which
we call the Component. The figure shows one subsystem with its
controller and its component. The controller generates or receives
three sets of signals:
Chip1 
Module1
Node
Module2
Chip1 
Component C
Chip2 
Chip2 
Component C
C
Module1 Component
C Module2 Component
Node Component
C
Subsystem
C
C 
C
Component
C
2 1
3 3
1 Local inputs 2 Local outputs 3 Coordination signals 
Parent to Child
Max power allowed
Max temperature allowed
Min performance required
Set on/off, #threads
Child to Parent
Actual power consumed
Actual temperature measured
Actual performance delivered
On/off status, #threads on
C
2
1
1
Subsystem
Component
C C
C
3 3
Local inputs 2 Local outputs 3 Coordination signals: 
Parent to Child:
Max power allowed
Max temperature allowed
Min performance required
Set #subsystems
Child to Parent:
Actual power consumed
Actual performance delivered
Actual temperature measured
#subsystems active
C
2
1
1
Subsystem
Component
C C
C
3 3
Local inputs 2 Local outputs 3 Coordination signals: 
Parent to Child:
Max power allowed
Max temperature allowed
Min performance required
Set #active subsystems
Child to Parent:
Actual power consumed
Actual temperature measured
Actual performance delivered
#active subsystems
Figure 6: Proposed controller interface. In the figure, C
means controller.
• Coordination Signals ○3 . These signals connect a controller
with its parent controller and its potentially multiple child controllers. The figure shows a controller with two child controllers.
The Coordination signals are shown on the right of the figure. From
a controller to its child controllers, the signals set the operating constraints (i.e., the maximum power allowed, maximum temperature
allowed, minimum performance required, and number of active
subsystems). The child controllers use this information as constraints as they optimize their own components. From a controller
to its parent controller, the signals report on the operating conditions (i.e., actual power consumed, actual temperature measured,
actual performance delivered, and number of active subsystems).
The parent controller uses this information to potentially assign
new constraints to all of its children. The coordination signals use
parameters readily available in current systems.
• Local Inputs ○1 and Local Outputs ○2 . These are the conventional signals that a controller uses to change and sense its component, respectively. They require no coordination and, therefore, can
be manufacturer-specific. Examples of local inputs are frequency
and cache size, and of local outputs are performance, power, temperature, and dI/dt. Figure 5 shows how the inputs are generated
from the output measurements.
To build a modular control framework, the manufacturer of a
subsystem has to include a controller that provides and accepts the
standard coordination signals from parent and child controllers.
4.3 Tangram Control Framework
As a computer is built by assembling different subsystems hierarchically, the controllers of different subsystems are also connected
hierarchically, exchanging the standard Coordination signals (Section 4.2). The result is the Tangram control framework. Figure 7
shows the framework – without the proprietary Local Input and Local Output signals – for a computer node that contains two modules,
with one of the modules containing two chips.
Chip1 
Module1
Node
Module2
Chip1 
Component
C
Chip2 
Chip2 
Component C
C
Module1 Component
C Module2 Component
Node Component
C
C C
C
Module1
Chip1
Chip2
Module 2 C
C
Node
Figure 7: The decentralized, modular Tangram framework.
C means controller.
The Tangram control framework is modular, fast, and globally
coordinated. It is modular because each controller is built with
knowledge of only its subsystem. For example, in Figure 7, the designers of Chip 1 and Chip 2 develop their controllers independently.
Similarly, the Module 1 controller is developed without knowing
about the Chip 1 and Chip 2 controllers. Further, changing a subsystem is easy – only the interfacing controllers need to be rewired
and reprogrammed. For example, if we change Module 2, only the
Node controller is affected.
The framework is fast because each controller makes decisions
on its own subsystem immediately. This is unlike in cascaded designs where, to make a change that affects the local system requires
a long chain of decisions (Section 2.1). It is also unlike centralized
systems, where decisions are made in a faraway central controller.
Finally, the framework is globally coordinated because there are
coordination signals that propagate information and constraints
across the system. These signals are used differently than in cascaded systems. In Tangram, the local controller in a subsystem uses
the constraints given by the coordination signals from the parent to
identify the subsystem’s best operating point; the local controller
in a subsystem passes constraints to the child controllers. In the
cascaded design discussed earlier (Section 2.1), instead, the divider
simply provides, at each level, the exact parameters that fully determine the subsystem’s operating point; the local controller at the
leaf node tries to keep the outputs at this operating point.
4.4 Comparison to Contemporary Systems
The modular structure of Tangram may make the design appear
obvious. Therefore, why do current systems not use a similar framework? A major reason is that their controllers do not use formal
control. The use of a MIMO robust controller in each subsystem ensures that its optimizations work in the presence of other controllers
in other subsystems. The controllers connected in a hierarchy can
coordinate their actions. These benefits cannot be guaranteed by
the current heuristic controllers used in individual subsystems,
and simply using them together does not lead to cohesive decisions [19, 57].
4.5 Tangram Implementation
Tangram can be implemented in hardware or in software. For
time-critical and hardware-specific measures such as DVFS, the
controllers should be implemented in hardware or firmware, and
signals should be carried by a special control network. Examples
of such a network are AMD’s SMU in EPYC™ systems [17] and
388
MICRO-52, October 12–16, 2019, Columbus, OH, USA Pothukuchi et al.
IBM’s OCC in POWER9 [67] (Section 2.1). For less critical measures,
controllers can be implemented in software, and communication
between controllers can proceed using standard software channels.
In a hardware implementation, it is not necessary to have dedicated pins and physical connections for every signal. A few ports
and links are sufficient, as controllers can pass information in the
form of <property,value> pairs.
While verifying a decentralized system is a challenging task in
general, Tangram reduces verification cost because it uses formal
control. Further, each Tangram controller is simple, compared to a
single, large centralized controller.
4.6 Example of Tangram’s Operation
Figure 8 is an example of how Tangram works. The figure considers
a module composed of a CPU chip and a GPU chip. It shows the timeline of the actions of the three controllers, as they run in preconfigured, safety, or enhancement modes. The figure shows the coordination signals passed between the three controllers, which can be the
local values measured (solid) or new constraints (dashed). For simplicity, we only consider power-related and activation/deactivation
signals. As shown in the # Tasks timeline, the execution starts with
zero tasks, then one CPU task appears, then one GPU task is added,
and then many CPU tasks are added.
On B
A A
R
R
R D A
RD
R D A
RD
R D A
R D A
D
R D A R D A R D A
D
R D
R D
R D
K
P
K
P
K
P
R D A
Tresp
Tresp
Tresp
T0 T1 Tp
Chip1
Chip2
Module1
App
… … 
… … 
Preconfigured Safety Enhancement
Zero Single Many
Chip1
Chip2
Module1
#Threads
Preconfigured Safety Enhancement
1 2
Node
V
Off
B
3
3
3 V
4
3 B 3 V 3 B
B
3 V 3 B
3 3 V 3 B
R
R
R
R R
3
Time
Activity 
increased
PID raises 
frequency
Utilization
Loss
Power
Sup raises 
Utilblade
Group divider 
raises Penclosure
Enclosure divider 
raises Pblade
Sup lowers 
Utilblade
PID lowers 
frequency
PID raises 
frequency
Time
On B
Zero Single Many, low compute 
Chip1
Chip2
Module1
#Threads
Preconfigured Safety Enhancement
1 2
Node
Off V
B
3
3
3 V 4
3 B 3 V 3 B
B
3 V 3 B
3 3 3 V 3 B
Time
Many, high compute 
On B
Zero Single Many, low compute 
Chip1
Chip2
Module1
#Threads
Preconfigured Safety Enhancement
Node
Off V
B
3
3
3 V 4
3 B 3 V 3 B
B
3 V 3 B
3 3 3 V 3 B
Time
Many, high compute 
Zero 1 CPU
#Tasks
Preconfigured Safety Enhancement
Node
Time
1 CPU + 
1 GPU
CPU Chip
GPU Chip
Module
Many CPU + 1 GPU
2
3
4
5 6 7 8
9
1 10
Figure 8: Example of coordination in Tangram. Solid arrows
send measured local values, while dashed arrows provide
new constraints.
All controllers begin in preconfigured modes. At ○1 , the module controller turns the GPU chip off. When a CPU task arrives,
the CPU controller changes to a new preconfigured mode. As the
module realizes that there is one thread running (○2 ), it changes
the power assignment to the CPU chip, and changes to a new preconfigured state. When a GPU task arrives, the module changes to
enhancement mode, wakes up the GPU chip and assigns it a power
budget (○3 ). The GPU controller enters enhancement mode. The
GPU chip optimizes itself using the power assigned. When many
CPU tasks arrive, there is a current emergency in the CPU chip,
which causes the controller to enter safety mode (○4 ). The CPU
chip controller eventually transitions to enhancement mode. At ○5 ,
the module reads values from both chips and shifts power from
the GPU chip to CPU chip. There is local optimization and some
communication to find the best power across the module in ○6 . At
○7 , there is a thermal emergency in the module, which causes the
controller to lower the power limits of the chips. On recovery, the
module controller continues in enhancement mode, reading values
and providing constraints (○8 ). At ○9 , the GPU chip overheats and
recovers from it, but the other subsystems are unaffected. At ○10,
the Node controller, which is the parent of the module controller,
reads the module’s state and provides new constraints for it.
4.7 Scalability of Tangram
Tangram’s scalability is helped by the fact that Tangram uses MIMO
control and has a hierarchical organization. Using MIMO helps scalability because we can increase the number of inputs and outputs
of the controller, and the controller’s latency increases only proportionally to the sum of the number of inputs and outputs. A
hierarchical organization helps scalability because the depth of
Tangram’s network typically grows only logarithmically with the
number of subsystems. Of course, with more subsystems, the root
controller has longer timescales. However, this is generally not a
problem because the time constants at which control is required
also grow with large systems. For example, a chip’s power supply
cares about fine-grain current changes because it has a modest input capacitance, but a node’s power supply only cares about longer
timescales because it has a large capacitance.
5 TANGRAM PROTOTYPE
We prototype Tangram in a multi-socket heterogeneous server that
we build using components from different vendors. We bought a
computer motherboard from GIGABYTE [29], which we call the
Node. The motherboard comes with an AMD Ryzen™ 7 1800X CPU
cluster that has two quadcore processors with 2-way SMT [7]. To
this, we add a GPU card from MSI [49] that contains an AMD
Radeon™ RX 580 GPU [6]. Figure 9a shows a picture of the computer, and Figure 9b its organization. The system is a node with
two subsystems: a CPU Cluster and a GPU Chip. The CPU Cluster
has two quad-core CPU chips. The computer has subsystems from
GIGABYTE, AMD, and MSI.
(a) Physical system.
CPU 
Chip1
CPU Cluster
GPU 
Chip
CPU 
Chip2
Node
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
CPU Chip1 
CPU Cluster
CPU 
Chip1 C
CPU 
Cluster C
1
2
3
4
CPU Chip2 
CPU 
Chip2 C
5
6
5
6
CPU Cluster 
Node
CPU 
Cluster C
Node 
C
1
2
3
GPU Chip
GPU 
Chip C
5
6
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
CPU Chip1 
CPU Cluster
CPU 
Chip1 C
CPU 
Cluster C
1
2
3
4
CPU Chip2 
CPU 
Chip2 C
5
6
5
6
CPU Cluster 
Node
CPU 
Cluster C
Node 
C
1
2
3
GPU Chip
GPU 
Chip C
5
6
C
4
5
CPU Chip1
C
4
5
CPU Chip2
C
2 1
3
CPU Cluster
C
C
4
5
GPU Chip
C
2 1
3
Node
3
CPU Cluster
GPU 
Chip
CPU 
Chip1
CPU 
Chip2
Node
CPU Cluster C C
C
CPU 
Cluster
CPU 
Chip1
CPU 
Chip2
GPU 
Chip C
C
Node
Stage 1: CPU Chips and CPU Cluster
CPU Stage 2: GPU Chip and Node
Cluster C
CPU 
Chip1 C
GPU 
Chip C
Node 
C
CPU 
CPU Chip2 C
Chip1
CPU Cluster
GPU 
Chip
CPU 
Chip2
Node
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
Stage 1
Stage 2
(b) Subsystems.
CPU 
Chip1
CPU Cluster
GPU 
Chip
CPU 
Chip2
Node
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
CPU Chip1 
CPU Cluster
CPU 
Chip1 C
CPU 
Cluster C
1
2
3
4
CPU Chip2 
CPU 
Chip2 C
5
6
5
6
CPU Cluster 
Node
CPU 
Cluster C
Node 
C
1
2
3
GPU Chip
GPU 
Chip C
5
6
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
CPU Chip1 
CPU Cluster
CPU 
Chip1 C
CPU 
Cluster C
1
2
3
4
CPU Chip2 
CPU 
Chip2 C
5
6
5
6
CPU Cluster 
Node
CPU 
Cluster C
Node 
C
1
2
3
GPU Chip
GPU 
Chip C
5
6
C
4
5
CPU Chip1
C
4
5
CPU Chip2
C
2 1
3
CPU Cluster
C
C
4
5
GPU Chip
C
2 1
3
Node
3
CPU Cluster
GPU 
Chip
CPU 
Chip1
CPU 
Chip2
Node
CPU Cluster C C
C
CPU 
Cluster
CPU 
Chip1
CPU 
Chip2
GPU 
Chip C
C
Node
Stage 1: CPU Chips and CPU Cluster
CPU Stage 2: GPU Chip and Node
Cluster C
CPU 
Chip1 C
GPU 
Chip C
Node 
C
CPU 
CPU Chip2 C
Chip1
CPU Cluster
GPU 
Chip
CPU 
Chip2
Node
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
Stage 1
Stage 2
(c) Controllers.
Figure 9: Tangram organization.
389
Tangram: Integrated Control of Heterogeneous Computers MICRO-52, October 12–16, 2019, Columbus, OH, USA
Figure 9c shows the Tangram framework. To demonstrate Tangram’s modularity, we build it in two stages, similar to how we
assembled the computer. In stage one, we design and interconnect
the controllers in the CPU Cluster subsystem. In stage two, we
design the controllers for the GPU Chip and the Node subsystems,
and interconnect them with the stage one controllers to build the
full Tangram network.
We built the controllers as software processes. They run as privileged software, accessing the System Management Units (SMUs)
of the subsystems with internal calls. An alternative, higher performance implementation in hardware requires major changes to the
testbed, as the SMUs are inside the chips.
The controllers read outputs and change inputs using Model
Specific Registers (MSRs) [3] and internal SMU calls with proprietary libraries. Since AMD GPUs do not provide public access to
dynamic performance counters, we read them using an internally
developed library that intercepts OpenCL™ calls to identify the
running kernel and its performance.
5.1 Stage One: CPU Cluster Subsystem
Figure 10 shows the Tangram network for the CPU cluster subsystem, with the different signals labeled. Table 2 shows the controllers’
output and input signals that we use, based on the available sensors and actuators in our testbed. As we see, in a controller, the
enhancement, safety, and preconfigured engines measure different
outputs. CPU 
Chip1
CPU Cluster
GPU 
Chip
CPU 
Chip2
Node
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
CPU Chip1 
CPU Cluster
CPU 
Chip1 C
CPU 
Cluster C
1
2
3
4
CPU Chip2 
CPU 
Chip2 C
5
6
5
6
CPU Cluster 
Node
CPU 
Cluster C
Node 
C
1
2
3
GPU Chip
GPU 
Chip C
5
6
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
CPU Chip1 
CPU Cluster
CPU 
Chip1 C
CPU 
Cluster C
1
2
3
4
CPU Chip2 
CPU 
Chip2 C
5
6
5
6
CPU Cluster 
Node
CPU 
Cluster C
Node 
C
1
2
3
GPU Chip
GPU 
Chip C
5
6
C
4
5
CPU Chip1
C
4
5
CPU Chip2
C
2 1
3
CPU Cluster
C
C
4
5
GPU Chip
C
2 1
3
Node
3
CPU Cluster
GPU 
Chip
CPU 
Chip1
CPU 
Chip2
Node
CPU Cluster C C
C
CPU 
Cluster
CPU 
Chip1
CPU 
Chip2
GPU 
Chip C
C
Node
Stage 1: CPU Chips and CPU Cluster
CPU Stage 2: GPU Chip and Node
Cluster C
CPU 
Chip1 C
GPU 
Chip C
Node 
C
Stage 1: CPU Chips and CPU Cluster
Stage 2: GPU Chip and Node
CPU 
CPU Chip2 C
Chip1
CPU Cluster
GPU 
Chip
CPU 
Chip2
Node
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
Figure 10: Tangram network in Stage One.
Table 2: Inputs and outputs of Stage One controllers.
Controller Local outputs Local inputs
Enhancement Safety Preconfigured
CPU
Chip
○5 Chip
performance,
power
○5 Current,
temperature
○5 #threads ○4 frequency,
#cores on
CPU
Cluster
○2 Cluster
performance,
power
○2 Current,
temperature
○2 #threads ○1 Cluster
frequency,
#chips on
Consider Table 2. A CPU chip controller monitors many outputs.
The enhancement engine monitors the chip’s performance (measured in billions of instructions committed per second or BIPS),
and power. The safety engine monitors the Thermal Design Current (TDC) used to prevent voltage regulator overheating [21], and
the hotspot temperature. The preconfigured engine monitors the
number of running threads. The controller sets two inputs, namely,
the chip’s frequency (2.2 GHz – 3.6 GHz) and the number of active
cores (0 – 4).
The CPU cluster controller monitors the same outputs at its level,
which combine the contributions of both chips, caches, and other
circuitry in the cluster. The controller sets the cluster frequency
of peripheral components (1.6 GHz – 3.6 GHz) and the number of
active chips (0 – 2).
The coordination signals (○3 ) measure the values and set the
constraints discussed in Section 4.2.
5.2 Stage Two: Node Subsystem
Figure 11 shows the Tangram network for the whole node. It shows
the details for the new controllers at this stage, namely, the controllers for the GPU Chip and Node. Table 3 lists the controllers’
output and input signals, organized as in Table 2.
CPU 
Chip1
CPU Cluster
GPU 
Chip
CPU 
Chip2
Node
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
CPU Chip1 
CPU Cluster
CPU 
Chip1 C
CPU 
Cluster C
1
2
3
4
CPU Chip2 
CPU 
Chip2 C
5
6
5
6
CPU Cluster 
Node
CPU 
Cluster C
Node 
C
1
2
3
GPU Chip
GPU 
Chip C
5
6
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
CPU Chip1 
CPU Cluster
CPU 
Chip1 C
CPU 
Cluster C
1
2
3
4
CPU Chip2 
CPU 
Chip2 C
5
6
5
6
CPU Cluster 
Node
CPU 
Cluster C
Node 
C
1
2
3
GPU Chip
GPU 
Chip C
5
6
C
4
5
CPU Chip1
C
4
5
CPU Chip2
C
2 1
3
CPU Cluster
C
C
4
5
GPU Chip
C
2 1
3
Node
3
CPU Cluster
GPU 
Chip
CPU 
Chip1
CPU 
Chip2
Node
CPU Cluster C C
C
CPU 
Cluster
CPU 
Chip1
CPU 
Chip2
GPU 
Chip C
C
Node
Stage 1: CPU Chips and CPU Cluster
CPU Stage 2: GPU Chip and Node
Cluster C
CPU 
Chip1 C
GPU 
Chip C
Node 
C
Stage 1: CPU Chips and CPU Cluster
Stage 2: GPU Chip and Node
CPU 
CPU Chip2 C
Chip1
CPU Cluster
GPU 
Chip
CPU 
Chip2
Node
CPU 
Chip1 C
CPU 
Cluster C
Node 
C
CPU 
GPU Chip2 C
Chip C
Stage 2
Stage 1
Figure 11: Tangram network in Stage Two.
Table 3: Inputs and outputs of Stage Two controllers.
Controller Local outputs Local inputs
Enhancement Safety Preconfigured
GPU
Chip
○5 GPU
performance,
power
○5 Current,
temperature
○5 #kernels ○4 Compute
frequency,
memory frequency
Node ○2 Node
performance,
power
○2 Current,
temperature
○2 #tasks,
task type
○1 Node
frequency
As shown in Table 3, the GPU chip controller monitors the following outputs: the GPU performance and power (enhancement),
the current and temperature (safety), and the number of kernels
(preconfigured). The controller sets the frequency of the GPU compute units (300 – 1380 MHz) and of the graphics memory (300 –
2000 MHz). The node controller monitors similar outputs at its
level (including whether the threads are CPU-type, GPU-type, or
both), and sets the node frequency for the board’s circuitry (300 –
2000 MHz).
The coordination signals (○3 ) measure the values and set the
constraints discussed in Section 4.2. The Node controller has no
parent controller.
390
MICRO-52, October 12–16, 2019, Columbus, OH, USA Pothukuchi et al.
5.3 Structures in the Controllers
We now build the structures in each controller: enhancement engine
(robust controller plus planner), safety engines, and preconfigured
engines.
Enhancement Engine - Robust Controller: To design a robust
controller, we need to (i) model its system (e.g., a CPU chip), and
(ii) set the controller’s input weights, uncertainty guardband, and
output deviation bounds [73]. For the former, we use the System
Identification modeling methodology [43]. In this approach, we run
training applications on the system and, during execution, change
the system inputs. We log the observed outputs and the inputs.
From the data, we construct a dynamic polynomial model of the
system:
y(T ) = a1 × y(T − 1) + . . . + am × y(T − m)+
b1 × u(T ) + . . . + bn × u(T − n + 1)
(2)
In this equation, y(T) and u(T) denote the outputs and inputs at
time T. This model describes outputs at any time T as a function
of the m past outputs, and the current and n-1 past inputs. The
constants ai and bi are obtained by least squares minimization
from the experimental data [43].
Appendix A describes how we set the controller parameters:
input weights, uncertainty guardband, and output deviation bounds.
With the model and these parameters, standard tools [32] generate
the set of matrices that encode the robust controller (Section 3).
Enhancement Engine - Planner: The planner monitors local outputs and receives coordination signals from the parent and child
controllers. With this information, it issues the best targets for all
local outputs to the robust controller, and coordination signals to
parent and child controllers. For example, the planner in a CPU
chip’s controller receives power, performance, temperature, and
activation settings from the CPU cluster controller, and generates
targets for its controller to optimize EDP. Our planners use the
Nelder-Mead algorithm [48] described in Appendix A to search
for the best targets under constraints received from the parent
controller. We choose this search algorithm for its simplicity, effectiveness, and low resource requirements to run on firmware
controllers [48].
Safety Engines: We consider two safety conditions: current and
temperature. A hazard occurs if any exceeds the limits. Appendix A
lists the hazardous values for current and temperature. If a hazard
occurs in the CPU chip, the controller turns off all the cores except
one, and sets the latter to the lowest frequency. If it occurs in the
CPU cluster, the controller turns off one CPU chip and runs the
other at the lowest frequency. If it occurs in the GPU chip or in the
Node, the controllers set the frequencies to the lowest values.
Preconfigured Engines: We build the preconfigured engines of
the different controllers to have the modes in Table 4.
5.4 Controller Overhead and Response Time
To show the nimbleness of our prototype, we list the overhead and
response time of the controllers. Table 5 lists the overhead of the
four structures that comprise the CPU Chip controller. For each
structure, the table lists the dimension, storage required, number of
instruction-like operations in the computation needed to produce an
output, latency of computation, and power consumption. A robust
Table 4: Modes of the different preconfigured engines.
Controller Preconfigured Regime Action
CPU chip No active thread Only one core on, which runs at the
lowest frequency
Single active thread Only one core on, which runs at the
highest frequency
CPU cluster
No active thread One CPU chip can use up to 1/8 of its
TDP; the other CPU chip is turned off
Single active thread One CPU chip can use up to 1/2 of its
TDP; the other CPU chip is turned off
#threads ≤ 8 (i.e., # of
SMT contexts in a chip)
One CPU chip can use its full
TDP; the other CPU chip is turned off
GPU chip No active task GPU chip goes to a low power mode
Node CPU-only tasks CPU cluster can use its full TDP;
GPU chip can use up to 1/8 of its TDP
GPU-only tasks CPU cluster can use up to 1/8 of its TDP;
GPU chip can use its full TDP
controller’s dimension is the number of elements in its state (i.e., the
length of vector x in Equation 1 of Section 3). A planner’s dimension
is the number of possible modes in the Nelder-Mead search in
Appendix A. From the table, we see that the storage, operation
count, latency, and power values are very small – especially for the
safety and preconfigured engines. The overheads of the controllers
in the CPU cluster, GPU chip, and Node are similar.
Table 5: Overheads of Tangram’s CPU chip controller.
Structure Dimension Storage # Ops Latency Power
Robust controller 9 1 KB ≈245 ≈15 µs 10-15mW
Planner 5 125 B ≈350 ≈25 µs 10-15mW
Safety engines – 8 B 2 < 1 µs < 1mW
Preconfigured engines – 8 B 2 < 1 µs < 1mW
We now consider the response time of the enhancement engines.
Each enhancement engine has a robust controller and a planner (Figure 4). The response time of the robust controller includes reading
outputs and targets, deciding on new inputs, and applying the new
inputs. The response time of the planner includes reading outputs
and coordination signals, deciding on targets, and communicating targets to the controller. Table 6 shows the measured response
time of the robust controllers and planners in the enhancement
engines of the different controllers in Tangram. For a parent controller/planner, the response time includes the time for its decisions
to propagate through all the children and grandchildren until they
affect the leaf robust controller’s decision to change inputs. This
may take multiple invocations of the leaf robust controller, which
is activated every 50 ms. For comparison, we also show data for a
centralized and a cascaded control framework that we implemented.
The Tangram robust controller and planner in the CPU chip
and GPU chip have a response time of 15 ms. Hence, performance,
power, and temperature can be controlled in a fine-grain manner.
As we move up in the hierarchy of controllers, the response time
increases. At the node level, the response times are close to 500 ms.
In Centralized, we place the single enhancement engine in the
Node subsystem. Since the engine has to read many sensors, buffer
391
Tangram: Integrated Control of Heterogeneous Computers MICRO-52, October 12–16, 2019, Columbus, OH, USA
Table 6: Response time of the enhancement engines.
Subsystem Tangram Centralized Cascaded
Robust Planner Robust Planner Robust Divider
Controller Controller Controller
CPU Chip 15 ms 15 ms – – 15 ms –
CPU Cluster 115 ms 115 ms – – – 165 ms
GPU Chip 15 ms 15 ms – – 15 ms –
Node 515 ms 515 ms 200 ms 200 ms – 665 ms
the data, and change many inputs across the system, it has a sizable
response time (200 ms). Hence, it is not suited for fast response.
In Cascaded, we build the design in Figure 2. Only the leaf subsystems (CPU and GPU chips) have robust controllers, and their
response time is similar to those in Tangram. Higher levels in the
hierarchy only have dividers, which set the power levels. A leaf controller cannot steer the system to new outputs until all the dividers
in the hierarchy, starting from the topmost one, have observed a
change in regime and, sequentially, agreed to a change in the power
assignment. Because each divider is activated at longer and longer
intervals as we move up the hierarchy, a round trip from the leaf
subsystem to the outermost divider in the Node and back to the
leaf takes 665 ms. Therefore, Cascaded has a long response time.
Table 6 shows that our software implementation of Tangram is
fast. In mainstream processors, control algorithms are typically implemented as firmware running on embedded micro-controllers [17,
19, 52, 67, 68], and operate at ms-level granularity. Therefore, we
envision the controllers in Tangram to be deployed as vendorsupplied firmware running on micro-controllers in their respective
subsystems. This requires little change to existing hardware. Further, the storage overhead and number of operations from Table 5
indeed show that Tangram can be easily run as firmware on a
micro-controller. With a firmware implementation, we estimate
that Tangram’s response times in Table 6 reduce by about one order
of magnitude, providing much better real-time control. A firmware
implementation would also lower the response times of the other
frameworks in Table 6, but is unlikely to change the relative difference between the frameworks.
6 EVALUATING THE PROTOTYPE
6.1 Applications
We use the Chai applications [31], which exercise both the CPUs and
the GPU simultaneously, unlike most benchmarks. They cover many
collaboration patterns and utilize new features in heterogeneous
processors like system-wide atomics, inter-worker synchronization,
and load balancing of data parallel tasks. We use two applications
for training (pad and sc) and five for evaluation (bfs, hsti, rscd,
rsct, and sssp). For Stage One controllers, we run NAS 3.3 [24] and
PARSEC 2.1 [13]. From NAS, we use two applications to train (bt
with dataset D and mg with dataset C) and nine for evaluation (dc
with dataset B, cg, ft, lu, sp and ua with dataset C, and ep, is and mg
with dataset D). From PARSEC, we use two applications to train
(raytrace with dataset native and swaptions with dataset simlarge)
and eight for evaluation (blackscholes, bodytrack, facesim, ferret,
swaptions, fluidanimate, vips and x264, all with dataset native).
6.2 Designs for Comparison
Our evaluation is comprised of three sets of comparisons, each
evaluated using the appropriate subsystem of the prototype that
can give us the most insights. The systems compared are: different
enhancement engine designs on a CPU chip, different control architectures on a CPU cluster, and different complete frameworks
on our full prototype. In all cases, our goal is to minimize the EDP
of the system under constraints of maximum power, temperature,
and current in each subsystem.
Comparing Enhancement Engine Designs. We compare our
enhancement engine (which we call Robust) to alternative designs,
such as LQG and Heuristic (Table 7) on a CPU chip. LQG is the
Linear Quadratic Gaussian approach proposed by Pothukuchi et
al. [59]. Heuristic is a collection of heuristics that use a gradientfree search to find the inputs that optimize the EDP metric. Instead
of using a controller or a planner, it approximates the gradient
using the past 2 output measurements and navigates the search
space. The search uses random-restart after convergence, to avoid
being trapped in local optima. This design is based on industrial
implementations [2, 3, 17].
Table 7: Comparing enhancement engine designs.
Strategy Description
LQG LQG controller from [59].
Heuristic Industrial-grade gradient-free optimization heuristics [3].
Robust Our enhancement engine of Figure 4
Comparing Control Architectures. We take our proposed controller from Figure 5 and use it in Tangram, Centralized, and Cascaded architectures on a CPU cluster. Centralized uses a single
instance of our Figure 5 controller in the CPU cluster. Cascaded
follows the design by Raghavendra et al. [62]. It uses a controller
in each leaf subsystem, and a divider in the CPU cluster. For Centralized and Cascaded, each subsystem has its own safety engine
for fast response time, as in existing systems [17].
Comparing Complete Frameworks. Finally, we compare complete framework designs on our full computer. The framework
designs are built with combinations of the above control architectures and enhancement engine designs, as listed in Table 8. Specifically, Tangram Robust is our proposed framework with our robust
controller. Cascaded LQG is a state-of-the-art design combining
prior work [59, 62]. Tangram LQG uses our control framework with
LQG-based controllers. Tangram Heuristic uses our control framework with controllers based on industry-class heuristics. For this
complete framework evaluation, we use the Chai programs.
Table 8: Comparing complete control frameworks.
Control System Description
Cascaded LQG Architecture based on [62] with LQG controllers from [59].
Tangram LQG Tangram architecture using LQG-based controllers.
Tangram Heuristic Tangram architecture using industry-standard heuristics.
Tangram Robust Tangram architecture with our proposed controllers.
392
MICRO-52, October 12–16, 2019, Columbus, OH, USA Pothukuchi et al.
7 RESULTS
7.1 Comparing Enhancement Engine Designs
We compare the enhancement engine designs in Table 7 on a single CPU chip running NAS and PARSEC applications. Figures 12a
and 12b show the execution time and EDP, respectively, with LQG,
Heuristic, and Robust enhancement engines, normalized to LQG.
cg dc ep ft is lu mg sp ua bl bo fa fe sw fl vi x2 Av
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Normalized Execution Time
LQG Heuristic Robust
(a) Normalized execution time (lower is better).
cg dc ep ft is lu mg sp ua bl bo fa fe sw fl vi x2 Av
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Normalized Energy Delay Product
LQG Heuristic Robust
(b) Normalized EDP (lower is better).
Figure 12: Comparing enhancement engine designs.
We see that LQG has the highest average execution time. To
understand why, note that LQG controllers converge more slowly
than robust controllers, as they are less capable of handling the
relatively unpredictable execution of the applications. For example,
LQG controllers in our design converge on the targets given by a
planner after ≈6 intervals. For a robust controller, this value is 2.
This effect worsens when there is interference with safety engines. For example, when the LQG enhancement engine inadvertently increases current or temperature too much, the safety engines
lower the frequency. As a result, performance falls much below its
target. Then, the LQG engine responds aggressively to reduce its
output deviations, but lacks the robustness to avoid future safety
hazards. The planner does revise the output targets, but it is invoked
only every 6 intervals of the LQG, because of the LQG’s longer
convergence time.
Heuristic also operates inefficiently, with oscillations between
safety and enhancement engines as with LQG. It cannot effectively
identify a configuration that is optimal and safe with heuristics
alone. It has the highest average EDP. Finally, Robust has the fastest
execution because the robust controller learns to optimally track
output targets without safety hazards. Since it converges faster than
LQG and keeps the output deviations within guaranteed bounds, the
planner’s search is effective and completes fast. Overall, Figure 12
shows the superiority of the Robust engine. On average, it reduces
the execution time by 33%, and the EDP by 27% over LQG.
For more insight, Figure 13 shows a partial timeline of the power
consumed by a CPU chip when running ep, an embarrassingly parallel NAS application that has a uniform behavior. The power is
shown normalized to the maximum power that the CPU chip can
consume in steady state. For this application, LQG and Heuristic
have many oscillations due to switching between the safety and
enhancement engines. However, the power in Robust converges
rapidly and stays constant, thanks to the better control of its enhancement engine.
0 5 10 15 20 25 30
Time (s)
0.0
0.5
1.0
1.5
2.0
Normalized Power
(a) LQG
0 5 10 15 20 25 30
Time (s)
0.0
0.5
1.0
1.5
2.0
Normalized Power
(b) Heuristic
0 5 10 15 20 25 30
Time (s)
0.0
0.5
1.0
1.5
2.0
Normalized Power
(c) Robust
Figure 13: Partial timeline of the power consumed by ep.
7.2 Comparing Control Architectures
We compare Tangram to the Centralized and Cascaded architectures
running applications in the CPU cluster. They all use our proposed
controller from Figure 5 with a robust enhancement engine. Tangram uses a controller in each subsystem, Centralized uses a single
controller in the CPU cluster, and Cascaded uses a controller in each
leaf subsystem and a divider in the CPU cluster. Figures 14a and 14b
show the execution time and EDP of the architectures, respectively,
normalized to Centralized.
cg dc ep ft is lu mg sp ua bl bo fa fe sw fl vi x2 Av
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Normalized Execution Time
Centralized Cascaded Tangram
(a) Normalized execution time (lower is better).
cg dc ep ft is lu mg sp ua bl bo fa fe sw fl vi x2 Av
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Normalized Energy Delay Product
Centralized Cascaded Tangram
(b) Normalized EDP (lower is better).
Figure 14: Comparing control architectures.
393
Tangram: Integrated Control of Heterogeneous Computers MICRO-52, October 12–16, 2019, Columbus, OH, USA
These architectures differ mainly in the response times of their
robust controllers and planners/dividers. In Centralized, there is a
single planner and robust controller that have long response times.
As a result, they sometimes miss opportunities to adjust power and
performance, even though they have a global view. This results in
inefficient execution. In Cascaded, the CPU chip controllers respond
fast, but their interaction with the next-level divider is slow. As
a result, the targets used by the controller lag behind the system
state, limiting efficiency. Moreover, a divider is not as effective as
a controller in setting the targets. Finally, Tangram has the lowest
response times at all levels. Therefore, on average, it reduces the
execution time by 11% and the EDP by 20% over Centralized.
To provide more insight, Figure 15 shows the power consumed
by the entire CPU cluster as a function of time in dc, another NAS
application. The power is shown normalized to the maximum power
that the CPU cluster can consume in steady state. We see that
Tangram uses higher power than Centralized and Cascaded. This
is because it is responsive to application demands with its fast
response time. While controllers communicate, they independently
optimize their components for changing conditions by generating
output targets and inputs fully locally. Therefore, the application
finishes the earliest and even consumes the least energy.
0 25 50 75 100 125
Time (s)
0.00
0.25
0.50
0.75
1.00
1.25
Normalized Power
(a) Centralized
0 25 50 75 100 125
Time (s)
0.00
0.25
0.50
0.75
1.00
1.25
Normalized Power
(b) Cascaded
0 25 50 75 100 125
Time (s)
0.00
0.25
0.50
0.75
1.00
1.25
Normalized Power
(c) Tangram
Figure 15: CPU cluster power in dc as a function of time.
Due to their longer response time, Centralized and Cascaded
consume lower power even when the application can use higher
power. Cascaded attempts to follow application demands, thanks
to the low response times of the CPU chip controllers. However,
the longer response time of the next level of control often results
in stale targets, which triggers oscillations. This results in worse
behavior than Centralized.
7.3 Comparing Complete Frameworks
Finally, we compare our proposed Tangram Robust framework to
state-of-the-art designs (Table 8) on our full heterogeneous prototype. Figures 16a and 16b show the execution time and EDP,
respectively, running the heterogeneous Chai applications. The
bars are normalized to those of Cascaded LQG, which we consider
the state-of-the-art.
If we compare Cascaded LQG to Tangram LQG, we see that the
latter has a lower execution time and EDP. This is because Cascaded
is a slow response-time architecture in large systems (Table 6).
Moreover, it lacks the hierarchy of MIMO controllers that optimize
each level of the control hierarchy. In particular, the bfs application
suffers from this limitation.
bfs hsti rscd rsct sssp Avg
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Normalized Execution Time
Cascaded LQG
Tangram Robust
Tangram LQG Tangram Heuristic
(a) Normalized execution time (lower is better).
bfs hsti rscd rsct sssp Avg
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Normalized Energy Delay Product
Cascaded LQG
Tangram Robust
Tangram LQG Tangram Heuristic
(b) Normalized EDP (lower is better).
Figure 16: Comparing complete control frameworks.
Comparing Tangram LQG to Tangram Heuristic, we see that
the latter is a worse design. With heterogeneity and complex application patterns (e.g., hsti), the heuristics are unable to identify
system-wide efficient settings.
Finally, we see that our proposed framework (Tangram Robust)
has the lowest execution time and EDP. This efficiency is due to
two factors: its fast response time (Table 6), and the safety and
optimality guarantees from using robust controllers at each level
of the hierarchy. We see large gains even for programs like rsct
that finely divide compute between the CPUs and the GPU. Overall,
Tangram Robust reduces, on average, the execution time by 31%
and the EDP by 39% over the state of the art Cascaded LQG. This
makes Tangram Robust a significant advance.
For more insight, consider rsct, which has rapidly-changing GPU
kernels and CPU threads. Figure 17 shows a partial timeline of the
number of active CPU threads and GPU kernels. Figure 18 shows
a partial timeline of the power consumed by the CPU Cluster and
the GPU. The power is shown normalized to the maximum power
that the node can consume in steady state.
0 4 8 12 16
Time (s)
0
5
10
15
#Tasks
CPU Cluster GPU
(a) Cascaded LQG
0 4 8 12 16
Time (s)
0
5
10
15
#Tasks
CPU Cluster GPU
(b) Tangram LQG
0 4 8 12 16
Time (s)
0
5
10
15
#Tasks
CPU Cluster GPU
(c) Tangram Robust
Figure 17: Partial timeline of the number of active threads
in the CPU cluster and number of kernels in the GPU in rsct.
The frequent peaks and valleys in the three charts of Figure 17
show that this application is very dynamic. The number of active
threads in the CPU cluster and the number of kernels in the GPU
394
MICRO-52, October 12–16, 2019, Columbus, OH, USA Pothukuchi et al.
0 4 8 12 16
Time (s)
0.0
0.2
0.4
0.6
0.8
Normalized Power
CPU Cluster GPU
(a) Cascaded LQG
0 4 8 12 16
Time (s)
0.0
0.2
0.4
0.6
0.8
Normalized Power
CPU Cluster GPU
(b) Tangram LQG
0 4 8 12 16
Time (s)
0.0
0.2
0.4
0.6
0.8
Normalized Power
CPU Cluster GPU
(c) Tangram Robust
Figure 18: Partial timeline of the power consumed by the
CPU cluster and the GPU in rsct.
changes continuously. Hence, all the frameworks try to continuously change the power assigned to the CPU cluster and the GPU,
based on their activity. In many cases, they trigger the preconfigured engines, such as when only a few CPU threads are active or
no GPU kernel is running.
However, as shown in Figure 18, the different frameworks manage power differently. In Figure 18a, we see that Cascaded LQG is
slow to shift power between the CPU cluster and the GPU, and
vice-versa. This framework reacts slowly for two reasons. First,
the cascaded architecture intrinsically has a long response time
(Table 6). Second, the LQG controller takes long to converge. As a
result, many tasks in the CPU or GPU start and complete before
the power to that subsystem is changed.
In Figure 18b, we see that Tangram LQG is more responsive. However, the slow LQG engine in Tangram LQG can hardly match the
fast-changing execution. In contrast, Tangram Robust in Figure 18c
quickly reassigns power to the subsystem which best improves the
overall EDP. This capability explains the programs’ lower execution
time and EDP in this framework.
8 RELATED WORK
General Control
There is a large body of work on controlling homogeneous or singleISA heterogeneous processors [1, 2, 23, 27, 38, 44, 46, 51, 56, 59, 60,
62, 65, 70, 75, 78, 85]. Only a few consider heterogeneous processors
with CPUs and GPUs [11, 57, 58, 69, 80]. Still, they use non-modular
controllers that do not match the modular heterogeneous environments we target.
Resource control in production computers is predominantly
heuristic [2–4, 16, 17, 37, 53, 57, 58, 61, 68, 69, 72, 74, 76, 78, 81].
As we indicated, heuristic control has limitations.
Research works propose many optimizing controllers [27, 34,
36, 38, 44, 59, 60, 62, 82–84]. Most do not consider interference
from dedicated safety controllers. Therefore, they do not simultaneously guarantee optimality and safety. Some works do consider
temperature as a soft constraint [34, 60, 62] while some probabilistically characterize mechanisms like circuit breaker tripping for
their search [27]. In real designs, there are many safety engines
that interrupt and override optimizing engines unpredictably. We
guarantee optimality and safety simultaneously.
Enhancement Approaches
Heuristic Control. Many designs rely heavily on heuristics for resource control [23, 36, 37, 56–58, 68, 69, 72, 78]. While easy to
implement for simple systems, designing, tuning and verifying
heuristics becomes dramatically expensive as systems and resource
management goals become complex. This can result in unintended
inefficiencies [28, 42, 44, 59, 78, 84].
Formal Control. PID controllers are popularly used in many works
due to their simplicity [9, 17, 18, 28, 39, 50, 51, 62, 64, 68, 70, 71].
However, PID controllers are Single Input Single Output (SISO)
designs, inadequate to meet the multiple objectives in computers [44, 59, 60]. LQG [59, 63] and MPC [44] controllers can handle Multiple Input Multiple Output (MIMO) systems, but are relatively less effective in uncertain and multi-controller environments [60, 73]. Yukta [60] proposes the use of robust controllers for
computer systems. These controllers operate well in environments
that are not fully modeled. Yukta introduces the use of a robust
controller for each system layer (e.g., the hardware and OS layers).
In this paper, we focus only on a single layer, and propose a framework with a controller in each subsystem, forming a hierarchy of
controllers connected with coordination signals.
Other Systematic Methods. Some works formulate Energy×Delayn
minimization as a convex optimization problem solved with linear
programming solvers [34, 35, 65]. Solver-based approaches require
more time to generate a decision than robust controllers. Some use
market-theory [82, 83] or game-theory [27] to manage resources in
specific contexts. Finally, some researchers use machine learning
(ML) techniques for resource management [14, 20, 26, 33]. Mishra
et al. [51] use ML to tune a PID controller and a solver that manage
a big.LITTLE processor.
Control System Architectures
Centralized. Most works use centralized frameworks (e.g., [34, 35,
44, 57–59, 84]). Some use two-step proxy designs where a proxy
module in each component requests resources and a centralized
manager performs the allocation [14, 27, 36, 38, 82, 83]. Centralized
designs are not modular, do not scale to multi-chip computers, and
do not fit IP-based system designs. As systems grow large, the
controller’s response time degrades quickly because it runs a bulky
algorithm and becomes a point of contention.
Cascaded. Raghavendra et al. [62] propose a multilevel cascaded
system to manage power in a datacenter. Rahmani et al.[63] propose
a similar 2-level design for a big.LITTLE processor. Here, each
component has two LQG controllers. A supervisor chooses one of
them to control the system and provides targets for all local outputs.
We showed that cascaded is non-modular and has a poor response
time.
Other Architectures. Muthukaruppan et al. [54] use a combination of
cascaded and decoupled PID controllers for a big.LITTLE processor.
Some designs order decoupled controllers by priority for limited
coordination [28, 71].
9 CONCLUSION
To control heterogeneous computers effectively, this paper introduced Tangram, a new control framework that is fast, globally coordinated, and modular. Tangram introduces a new formal controller
that combines multiple engines for optimization and safety, and has
a standard interface. Building the controller for a subsystem requires
knowing only about that subsystem. As a heterogeneous computer
395
Tangram: Integrated Control of Heterogeneous Computers MICRO-52, October 12–16, 2019, Columbus, OH, USA
is assembled, the controllers in the different subsystems are connected, exchanging standard coordination signals. To demonstrate
Tangram, we prototyped it in a multi-socket heterogeneous server
that we assembled using subsystems from multiple vendors. Compared to state-of-the-art control, Tangram reduced, on average,
the execution time of heterogeneous applications by 31% and their
energy-delay product by 39%.