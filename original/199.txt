This article reviews the human-machine interaction (HMI) technologies used for telemanipulation by small unmanned systems (SUS) with remote manipulators. SUS, including land, air, and sea vehicles, can perform a wide range of reconnaissance and manipulation tasks with varying levels of autonomy. SUS operations involving physical interactions with the environment require some level of operator involvement, ranging from direct control to goal-oriented supervision. Telemanipulation remains a challenging task for all levels of human interaction because the operator and the vehicle are not colocated, and operators require HMI technologies that facilitate manipulation from a remote location. This article surveys the human operator interfacing for over 70 teleoperated systems, summarizes the effects of physical and visual interface factors on user performance, and discusses these findings in the context of telemanipulating SUS. This article is of importance to SUS researchers and practitioners who will directly benefit from HMI implementations that improve telemanipulation performance.
SECTION I.Introduction
This article surveys the human–machine interaction (HMI) technologies for small unmanned systems (SUS) equipped with robotic manipulators capable of performing telemanipulation tasks. SUS have historically been used for visual reconnaissance, but needs beyond observation expanded research efforts toward physical interaction with remote environments [1]. Telemanipulation capabilities of SUS enable a greater understanding of the remote environment through physical contact, and are particularly advantageous when tasks are too dangerous, complex, costly, or difficult for humans to perform [2]. Domain-specific examples of SUS telemanipulation include construction material transport with unmanned aerial vehicles (UAVs) [3]–[4][5], space exploration and sampling with unmanned ground vehicles (UGVs) [6]–[7][8], underwater sampling with remotely operated vehicles (ROVs) [9], [10], and aquatic sampling with unmanned surface vehicles (USVs) [11]–[12][13]. SUS can also perform specialized tasks such as bomb disposal [14] and maintenance and service tasks [15]. These examples highlight the applications of SUS capable of remote manipulation, which range from conducting unique scientific explorations to performing repetitive practical tasks.

Research trends in perception [16], motion planning [17], and real-time control [18] indicate that the autonomous capabilities of SUS are improving; however, performing fully autonomous manipulation tasks in real-world environments still remains a challenge [19], [20]. A telemanipulation task in the context of SUS operations is any action that requires physical interaction with the remote environment with a robotic manipulator, based on control input that arrives through teleoperation. Teleoperation is the act of using a machine that extends input from a human operator to an SUS in a remote location, and the operator control input can range from direct human control to supervisory control. Performing telemanipulation tasks from a teleoperated SUS will require some level of human input; therefore, it is important to understand how HMI technology affects telemanipulation performance.

An SUS includes the vehicle (or robot), which may be a ground, aerial, surface, or tethered underwater vehicle, a ground control station, an operator, and a communication link. “Small” unmanned systems are systems with vehicles that are self-contained and portable by humans. The vehicle in the context of telemanipulation must include a robotic manipulator capable of performing a manipulation task. This article considers the HMI for SUS that perform general telemanipulation tasks in relatively small workspaces and in shorter time periods (i.e., minutes, not hours). This article focuses only on the telemanipulation task and does not include navigation tasks. The types of remote tasks considered in this article include pick-and-place, grasping, probing, pulling, and pushing tasks. This article excludes teleoperated systems with unusually large workspaces (e.g., excavation vehicles), prohibitively long time delays (e.g., space operation), or specialized training requirements (e.g., telesurgery).

Teleoperated SUS with robotic manipulators operate on a spectrum of autonomous capabilities. This article includes system autonomy in the analysis of human–machine interfacing because the robot's level of autonomy affects the overall human–robot interaction [21], [22]. There are multiple autonomy taxonomies that appear in the literature [22]–[23][24]; however, this article considers autonomy to be the extent to which a robot can sense the environment, plan based on that environment, and act upon that environment, with the intent of reaching some goal without external control [22]. This article considers the following four levels of autonomy [25], [26]: remote control, human-assisted, human-delegated, and human-supervised. These categories are described in greater detail below (note that fully autonomous systems are not considered in this analysis as human operators are not involved with the telemanipulation task and can be ignored by the system [23]).

A. Remote Control Systems
Remote control occurs when the operator interface represents all manipulator degrees of freedom, and the system operator directly controls the manipulator actuators [25], [26]. Remote control is either noncomputer-aided, where operator inputs map directly to manipulator outputs, or computer-aided, where computer transformation occurs prior to sensing or actuating the manipulator [26]. Both cases are remote control operation because there is no system autonomy, and the system only performs manipulation tasks when there are control inputs. An example of remote control telemanipulation is using a hardware interface to directly control a manipulator attached to a tethered ROV to perform geological sampling in deep sea environments.

B. Human-Assisted Systems
Human-assisted teleoperation is a form of shared control [24], [26]. Under human-assisted telemanipulation, the operator manually controls a larger proportion of manipulator functions than the system can autonomously perform; therefore, the human is the ultimate decision-maker [25]. An example of human-assisted telemanipulation is when a UGV manipulator operator selects a desired object within an interface, confirms a sequence of actions, and the robot automatically grasps the object.

C. Human-Delegated Systems
Human-delegated teleoperation is also a form of shared control [24], [26]. In human-delegated systems, the SUS maintains more control than the operator, and the operator provides guiding commands that use autonomous functionality within the system [25]. An example of human-delegated telemanipulation is the confirmation of objects to retrieve after autonomous detection by the SUS, or the confirmation of manipulator trajectories autonomously computed by the SUS and provided to the operator.

D. Human-Supervised Systems
In a human-supervised system, the operator provides high-level goal-oriented commands, and the decision-making capabilities and manipulator control lie with the SUS [25]. Human-supervised systems receive goal-oriented commands, autonomously gather and process sensory data, make decisions, and carry out actions, all while informing the operator of its status. For example, a human-supervised UGV may continuously perform a fully autonomous grasping task and will only alert the human operator when it encounters a problem. This article applied the following inclusion criteria to collect appropriate references.

The system was an unmanned ground, aerial, surface, or tethered underwater vehicle equipped with a robotic manipulator, or a teleoperated manipulator similar in spatial and temporal scales and task compared to SUS telemanipulation.

This article focused on the telemanipulation task.

This article provided empirical or experimental evidence related to operator performance when using a specified HMI technology for telemanipulation.

The task type included some form of physical interaction between the manipulator and the remote environment.

Applying these inclusion criteria resulted in references from the UAV, UGV, ROV, and USV literature, as well as literature from the broader human–robot interaction community when the scope of the telemanipulation task was similar to SUS telemanipulation. Data and results presented in this article include the observations and task performance metrics reported in the included references.
The rest of this article is organized as follows. Section II describes common telemanipulation operator interface technologies. Section III describes physical factors that affect telemanipulation, and Section IV describes types of visualization that affect telemanipulation performance. Finally, Section V concludes this article.

SECTION II.Operator Interfaces Used for Telemanipulation Inputs
This section describes interface types used for operator telemanipulation inputs, including semiautomatic controls, master-slave devices, computer workstations, and mobile tablets.

A. Master–Slave Controllers
A typical telemanipulation input scheme is a master–slave configuration where the master controller is geometrically similar to the manipulator at certain scales [27]. In this setup, operator inputs from the master arm map directly to the slave arm under remote control operation, and the position and force responses of both master and slave devices may be identical or scaled proportionally, depending on the task and system. The master controller can control each degree offreedom individually, or a subset of the degrees of freedom. Additionally, the master is often a kinematic replica of the slave, providing an intuitive interface [24].

For SUS operation, tethered ROVs normally use master–slave controllers to perform subsurface manipulation. Commercially available manipulators, such as the seven-degrees-of-freedom Orion (Schilling Robotics, USA), are commonly deployed with ROVs due to their robustness in deep-sea environments and readily available master controllers [9], [28]–[29][30]. The master controller for these systems is a scaled-down kinematic replica attached to a console panel with push keys, a small screen, and status indicators. Master joint movements produce an equivalent or scaled movement in the slave arm, and the keys determine manipulator control mode (e.g., rate or position) and functions (e.g., freezing or enabling hydraulics).

B. Semiautomatic Input Devices
Semiautomatic input devices enable remote control and human-assisted control during telemanipulation tasks. These devices have geometries dissimilar to the manipulator [27], such as joysticks or other physical controllers. Joysticks can operate under either position or rate control; under position control the joystick commands the desired position of the joint or end effector, and under rate control the joystick commands the desired velocity. In both cases, the input commands are proportional to the joystick displacement [24]. Dual joysticks (e.g., gamepad controllers) provide input if the slave manipulator requires control for both orientation and translation [24]. Joysticks and gamepad controllers have operated manipulators on ROVs [31], [32], UGVs [33], and simulated UAVs [34].

Nonjoystick, three-dimensional (3-D) semiautomatic devices can also be used for telemanipulation input under direct and human-assisted control. Examples include the Novint Falcon haptic device (Novint Technologies LLC, USA), a three-degrees-of-freedom version of the original delta-robot configuration [35], and the Geomagic TouchTM (3D Systems, USA). These smaller, inexpensive master controllers are widely studied for applications in robotics and gaming [36], [37] and can provide force feedback based on user input. These controllers normally have smaller working volumes compared to traditional master–slave configurations.

C. Computer Workstations
Operators can control teleoperated systems via communication networks through computer workstations, including laptops and desktops. Networked SUS can be controlled through user interface software implemented on a workstation, which requires a keyboard and/or mouse for user input [38]. For remote control or human-assisted operation, operators can control the manipulator by deciding where to click within the interface or through keystroke inputs, which control manipulators by position and may require mapping 2-D user input to locations in 3-D space. Full 3-D representation in remote control requires interface elements that control each degree of freedom separately (e.g., control in the x-, y-, and z-axes [39]) or control the position of the end effector through inverse kinematics [40]. In general, workstations can process and display large amounts of data and complex mixed visualizations [41]–[42][43]; however, one constraint of workstations is providing an adequate 2-D representation of the manipulator and robot that exist in a 3-D remote environment. Adequate 2-D representations are normally attempted through visualizations such as video streams [44], [45], stereo vision [46], [47], or mixed reality (MR) displays [6], [48]. Additionally, high-level directives can be issued from a computer workstation to facilitate human-delegated or human-supervised operation.

D. Mobile Tablets
Mobile tablets are an appealing interface technology to use for telemanipulation control because they are widely available, are low cost, and use standard gestures [49]–[50][51]. Operators can give either high level directives or control the manipulator position directly through tablet interfaces, enabling support for a wide range of autonomy levels. Touch-based interfaces are generally intuitive for nonexpert users [51], but the interface elements that control the manipulation affect operator performance under remote control operation. The development of telemanipulation interfaces for these handheld devices is becoming increasingly important; however, challenges remain, including intuitively mapping 2-D inputs to 3-D space [44], decreased computational power and the need for efficient algorithms [42], [52], and designing effective gesture-based touch controls [51]. Additionally, mobile device visualizations are less complex compared to large workstations, as bandwidth constraints limit how much sensor data and information can stream to and be processed on the device.

SECTION III.Effects of Input Device on Telemanipulation
This section discusses the types of physical factors that affect SUS operator performance during telemanipulation, including controller form factors and feedback variations of manual control. Table I presents an overview of the physical interface implementations surveyed in this section. The results from telemanipulation HMI that were evident across multiple studies included in this section were synthesized into common themes, or findings, that are presented in Table II.

TABLE I Summary of Physical Interface Implementations for Telemanipulation
Table I- Summary of Physical Interface Implementations for Telemanipulation
TABLE II Summary of the Effects of Multiple Physical Factors on Telemanipulation Performance
Table II- Summary of the Effects of Multiple Physical Factors on Telemanipulation Performance
A. Input Form Factor
Control input devices are a critical link between operators and teleoperated systems in remote environments [53], [54]. The form factor and design of input devices influence intuitiveness and learning requirements, which subsequently affect operator performance [33], [34]. The sections below discuss how telemanipulation interfaces affect operator performance, and includes the following interface devices—physical hand controllers, workstations, and touch-based mobile devices.

1) Physical Hand Controllers
Common telemanipulation hand controllers include master–slave devices, joysticks, and other 3-D devices. Depending on the level and location of control within the manipulator, each design affords different types of interaction between the controller device and robot. For example, hand controllers that are geometrically similar and retain all robotic manipulator degrees of freedom are normally easier for users to learn because they intuitively map user input to output [9], [33]. A remote operator identifies their body and immediate environment with the remote vehicle and its environment; therefore, a geometrically similar device better enables the match between their own movements and the remote manipulator attached to the vehicle [55].

Joysticks are also commonly used input devices, but because they are geometrically dissimilar to manipulators, visual motor mapping between the end effector and joystick affects task completion [56], [57]. Situation awareness of the vehicle's orientation in the surrounding environment is necessary to achieve proper mental mapping between the SUS manipulator and joystick [58]. To improve performance, joystick input should be analogous (i.e., moving the joystick to the left moves the vehicle to the left); however, this becomes difficult as manipulators increase in complexity and planer movements do not correspond directly to responses in the system. Joysticks may not be as intuitive as master–slave devices for direct joint control, but they can be effective for end-effector control [9].

Some studies have specifically compared the effects of using joysticks versus master controllers as input devices for telemanipulation tasks. Nixon et al. [34] found that joystick input increased task completion time compared to a 3-D input device when performing a simulated UAV manipulation task. Vozar et al. [33], [44] studied the effects of a master–slave device compared to dual joysticks for teleoperating a UGV manipulator and found that a master–slave device was more effective than a traditional gamepad controller [33]. These results are consistent with previous findings that position master–slave configurations are more suitable for dexterous manipulation because they have a natural correspondence in time and space as the operator performs movements [53], [55].

2) Workstation Input Devices
Workstation input devices, such as keyboard and mice, facilitate control of systems that have higher levels of automation and complexity. Keyboards and mice are also widely available physical interfaces that nonexpert operators are likely accustomed to using. One limitation, however, is the immobility of networked workstations that require operation in close proximity to their operator.

Common input modes for workstation control include “click-and-drag,” where the user clicks a point on the robot on a screen and drags the cursor to indicate desired translation, or “point-and-click,” where the user defines a goal end-effector position by clicking within the remote environment [41]. You and Hauser [59] compared multiple click-and-drag mouse input schemes for remote manipulator control, including direct joint control, inverse kinematics, reactive potential field, and a real-time motion planner. The real-time motion planning strategy reduced task completion time and was rated favorably by users because it allowed users to focus more on manipulator positioning, instead of on constructing paths.

A potential issue when using a workstation and 2-D mouse for manipulator control input is precise positing in a 3-D remote environment. Technologies, such as 3-D mice, can be used to assist with precise cursor positioning. Materna et al.  [43] developed an interface that used a 2-D mouse to interact with and set the 3-D scene, and a 3-D mouse set the end-effector goal pose within that space. Users found the 3-D mouse to be more comfortable and intuitive, and it enabled more precise manipulation of the end effector.

Buttons, including keyboard strokes or clicking buttons with a mouse on a workstation interface, should primarily generate sequences of actions (e.g., behavior chaining), rather than directly control manipulators [45], [60], [61]. The operator control unit for the SURROGATE ground vehicle ran on a workstation desktop and was operated with a standard keyboard and mouse. Using this interface, operators sent entire sequences of actions by chaining behaviors together to reduce operator interaction time [61]. Using keyboards and buttons for direct control can degrade task performance, although some operators prefer the kinesthetic feedback from physically pressing buttons [60], which can result in more accurate performance when precise movements are required [62].

3) Touch Input Devices
A variety of 2-D touch elements enable the operation of manipulators in 3-D environments. Lopez et al. [51] created a touch-based version of direct control hardware. Their interface contained four control element options, including virtual buttons, virtual joysticks, touch-screen gestures, and tilt gestures. Touch-based buttons and joysticks yielded the best task completion scores because they enabled more precise control of manipulator movements (compared to the tilt and touch gesture interfaces).

Other tablet interface implementations have taken a direct manipulation design approach, where users can touch the screen to control the robot “directly” in the remote environment (similar to a click-and-drag input scheme). Hashimoto et al. [50] developed a touch-screen application in which users directly controlled a UGV manipulator by touching it on a view of the world, as seen from a third person view obtained by pointing a camera at the robot. While the touch controls were intuitive, some users requested a stylus pen to enable more precise interaction, and others reported it was difficult to understand the depth of the space from a single view. Singh et al. [60] also developed a direct touch-screen control method for a remote manipulator where operators could touch and drag the manipulator end effector in a desired trajectory, which reduced task completion time and mental workload.

While touch-based gestures may be effective, limited bandwidth and computational power limit the use of advanced data visualizations and control algorithms on tablets [42]. Mobile tablets only afford simplified data visualizations and interaction elements as it is impractical to send and display all robot and sensor data [42]. Additionally, when designing kinematic algorithms for touch-and-drag control (which is often preferred by users [50], [60]), they must be computationally efficient in order to run on tablet devices. For example, Parga et al. [52] and Singh et al. [60] developed touch-based interfaces for a remote manipulator with inverse kinematic solvers capable of running on small tablet devices.

Direct control can be difficult on a touch-based 2-D device due to a lack of input precision [50], [51], but operators can more readily program behavior and action sequences when using a mobile device. Bengel et al. [64] developed a pictorial tablet interface that enabled human-delegated inspection tasks by “teaching” the UGV desired navigation and inspection tasks, after which the robot autonomously completed the same mission. Herbert et al. [61] developed a tablet interface for controlling a UGV equipped with two manipulators. Their interface only needed high-level directives and relied on autonomy to carry out individual actions by first selecting the object in the image, followed by selecting the appropriate behavior sequence. Saito and Suehiro [70] developed the TeachIng Tablet Interface with a 2-D input screen to enable pick-and-place operations by a portable manipulator. Operators drew pick-up and place frames on the 2-D screen and assigned them to predefined trajectories for the system to complete, effectively teaching the robot to perform manipulation.

B. Feedback Variations of Manual Control
Methods for mapping control inputs to outputs affect performance when the operator has some level of manual control of the manipulator [33], [53], [63] (e.g., remote control or human-assisted). This section includes the following variations of feedback for manual control—direct control with no feedback (including both position and rate control), bilateral control (haptic feedback), and virtual fixtures (a type of force feedback). Additionally, this section includes information on the location of control in the manipulator, for example, if the user controls only the end effector or has command over individual degrees of freedom.

1) Direct Control With No Feedback
Under direct position control, operators control each joint or degree-of-freedom of the manipulator individually, and a reference position maps from the input device to the output device [53]. Master-slave and joystick interfaces are commonly used for direct control and can operate by controlling either the position or rate of each joint. Early studies analyzed the effects of position and rate control on target acquisition and found that using position controlled joysticks resulted in improved performance compared to velocity controlled joysticks [74]–[75][76]. Experiments by Kim et al. [77], [85] examined pick-and-place tasks using two isotonic joysticks to control a remote manipulator and also found better operator performance under position control. If the control device is slow, however, superior performance of position control diminishes, and it is recommended to use position control for small workspace tasks and rate control for slow-workspace tasks [77].

Whether the operator has control over the end effector or each joint also affects operator performance, and the optimal location of control depends on the importance of speed versus accuracy on the task. Atherton and Goodrich [63] found that participants worked faster to control a remote manipulator with a joystick under joint control compared to end-effector control, but joint control resulted in more collisions with the environment. Additionally, operators may be imprecise in positioning the end effector at the desired location and tended to overshoot the target when operating a master–slave controller under position control [78]. Draper [53] also notes that when the operator is in constant manual control of a remote manipulator, the user should be in control of the end effector as direct joint control can be inefficient. For example, inverse kinematics can compute manipulator joint angles to give the operator direct end-effector control [9], [67].

Achieving transparency under position control has traditionally been the goal of master–slave manipulator systems [86]. Master–slave kinematic laws have generalized this type of system for mixed position and rate control [87]; however, when controlling both manipulator position and rate, the process for switching between these modes affects user performance [54], [65]. Herdocia et al. [65] found that a manual switch between position and rate control when operating a remote manipulator resulted in slower performance and higher error rates than a differential-end-zone coordination scheme. In their differential-end-zone coordination scheme, a fixed inner space controlled position, and velocity control occurred at the boundary of the inner space.

Operator fatigue is another issue that can arise when operating master–slave devices [53], [78]. Operator fatigue under remote control can degrade operator performance due to controller complexity [88]. For example, Gupta et al. [78] found that operators became tired after repeating thirty pick-and-place tasks on average when using a master controller, despite that a majority of users became proficient within a few minutes. Position control enhancements can reduce fatigue in master–slave operators. Love and Book [89] found that an adaptive impedance manipulator controller reduced the total energy output of the operator when executing remote manipulation tasks compared to nonadaptation. Force scaling can also reduce the required magnitude of operator input by scaling input forces up when operating large devices, or scaling down when performing micromanipulations [86]. Implementing smaller working volumes for the master controller also reduces operator fatigue [90], and fatigue likely decreases after being adequately trained with the master controller [91].

2) Bilateral Force Feedback
Force feedback (also known as haptic feedback) is a broad term that includes both tactile and kinesthetic information. In general, forces felt by the robot using tactile and other sensors are then fed to a haptics device to provide force feedback to a user [92]. Feedback and additional cues from force and tactile sensors in teleoperation systems can complement visual information, increase spatial awareness of the remote environment, and reduce error rate and magnitudes of applied forces [80], [93].

The presence of haptic feedback can decrease completion time. Implementing haptic feedback with proximity sensors for remote grasping tasks reduced completion time compared to visual-only feedback, despite that users were not requested to be time-efficient [68]. Haptic feedback can also reduce average and maximum applied forces [73], [81]. Haptic feedback in a master controller reduced contact forces and the occurrence of large robot-environment interaction forces during telemanipulation [73]. Acceleration haptic feedback can significantly reduce peak and average contact forces when grasping flexible objects [81]. A reduction in manipulator force is especially important for operations when the manipulation target is highly sensitive to force. In general, haptic feedback with small workspace controllers can often offer performance benefits for direct or human-assisted control operations for short-range SUS deployments when communication delays between the operator and vehicle are not significant.

3) Virtual Fixtures
Virtual fixtures are task-dependent computer-generated guides overlaid on a remote workspace reflection [94]. Virtual fixtures provide force feedback similar to general haptics discussed in the previous section; however, virtual fixtures assist the operator with force feedback for a specific, structured teleoperation task. Examples of virtual fixture implementations include a guide from the robot gripper to an object in the remote workspace [69] or force clues influencing the trajectory of the gripper [66].

Virtual fixtures are either “guidance” or “forbidden region”: guidance virtual fixtures assist in keeping the manipulator on desired paths, while forbidden region virtual fixtures physically prevent motion in the remote workspace in specific forbidden zones [84] (note that “guiding” fixtures are likely better at micro scales due to slight inaccuracies in system control [95]). Virtual fixtures have been successful in guiding remote control operations, providing localized references, reducing mental workload, and increasing precision [94], [96].

Studies show that virtual fixtures can reduce task completion time for manipulation and positioning tasks [69], [72]. Payandeh and Stanisic [69] reduced task completion time for a remote manipulator through a human-assisted control scheme where the operator positioned the gripper close to a position fixture, and the system autonomously completed the task using the virtual force cues. Kuang et al. [66] similarly found improved positioning of a remote manipulator with the assistance of haptic and graphic virtual fixtures.

One limitation of implementing virtual fixtures, however, is their dependence on a priori knowledge of the nature and geometry of the task. The implementation of virtual fixtures works well for controlled tasks, such as path following or manufacturing; however, SUS deployment environments are often dynamic and uncertain, and obtaining a priori knowledge of the remote environment is difficult. One potential solution is to use remote sensing methods to develop graphical models of the remote environment at the site of manipulation in real time [72], [97], which is only possible if the system is accurately modeled. In changing environments, adaptive virtual fixtures can help overcome new obstacles [98]; however, these systems require training data sets to learn desired paths prior to dynamic environmental changes. More research is needed regarding the modeling and development of virtual fixtures in real time in unstructured environments before the benefits of virtual fixtures can be fully realized for SUS operations.

SECTION IV.Types of Visual Displays That Affect Telemanipulation
This section discusses multiple types of visual displays that affect SUS operator performance during telemanipulation. Common remote manipulation displays include MR [including augmented reality (AR) and augmented virtuality (AV)], virtual reality, and stereo vision/depth imagery. Table III presents an overview of the visual interface implementations surveyed in this section. The results from this section were synthesized into common themes and findings and presented in Table IV.

TABLE III Summary of Visual Interface Implementations for Telemanipulation
Table III- Summary of Visual Interface Implementations for Telemanipulation
TABLE IV Summary of the Effects of Visualization-Type on Telemanipulation Performance
Table IV- Summary of the Effects of Visualization-Type on Telemanipulation Performance
A. MR Displays
MR is a display subclass that spans between the extrema on the reality-virtuality continuum that juxtaposes real and virtual objects within a scene [99]. Contrary to virtual environments where the user is totally immersed in and interacts with a purely synthetic world, MR displays integrate elements of both virtual and real worlds. MR displays provide a user with additional or enhanced information which can reduce operator mistakes [100], communicate processed sensor data [101], and improve human–robot collaboration [102]. MR displays are either AR, where a display of the real environment is augmented by virtual objects, or AV, where a virtual environment is augmented with real-world objects [99].

1) Augmented Reality
AR interfaces display real-world imagery with synthetic object and indicator overlay cues and aids. This section focuses on monitor-based AR displays with computer-generated objects overlaid onto the imagery, as opposed to immersive environments. These object overlays, such as coordinate systems [48], [103], depth information [33], [101], or virtual handles [50], aid in manipulator positioning under remote control and human-assisted teleoperation.

Augmented coordinate system displays can improve the operator's mental model of the position and orientation of the manipulator and reduce operation errors. Chintamani et al. [103] found that augmenting video with the manipulator's end-effector coordinate system reduced reversal errors and total distance traveled by the end effector. Nawab et al. [48] generated virtual color-coded coordinate systems on the end-effector of a manipulator that mapped to similarly color-coded joysticks for controlling position and orientation, which also reduced total distance traveled and reversal errors.

Virtual handles, or 3-D widgets [104], in an AR interface alert the user to the types of allowable interactions within a remote environment to improve interaction. For example, a 3-D ring with arrows indicates potential rotation movement. Hashimoto et al. [50] implemented touch-based interface for a UGV manipulator with two virtual handles, including a lever to indicate manipulator movement and a ring to indicate rotation movement, which participants found intuitive to use. Chen et al. [105] also implemented virtual rings on an assistive ground robot interface to indicate rotation direction around identified objects.

Augmented depth information informs an operator of how far away objects are and when an object of interest is within reach of the manipulator. An early implementation of AR depth sensing was the AR through graphical overlays on stereo video remote manipulation system by Milgram et al. [106], [107]. Their system used virtual pointers and tape measures to calculate and display distances between user-selected points, which improved user performance [108]. Vozar and Tilbury [33] implemented a virtual crosshair on a UGV manipulator interface that changed colors to either green (if the object was within reach), or red (if the object was out of reach) to provide depth cuing. Users felt enhanced presence in the remote workspace when using the AR interface, but performance dropped compared to using a video-only interface.

Depth distortions due to calibration errors can negatively affect performance when using virtual elements [109]; therefore, AR systems should draw digitally generated graphics with the same calibration parameters as the video, so users can accurately align graphic objects to real objects [110], [111]. Additionally, digitally overlaid objects will always occlude objects in the video, and interface designers should create object displays that minimize occlusion [109]. For example, instead of using solid reconstructions of remote objects (which might appear the most realistic), wireframe reconstructions reduce occlusion yet accurately represent object shape, size, and location [50], [107], [112].

2) Augmented Virtuality
AV displays use virtual environments augmented with real objects [99]. Generating AV is more computationally intensive than AR, and the quality of the virtual environment reconstruction is primarily dependent on the quality of sensor used in reconstruction [117]. Generation of virtual 3-D reconstructions of remote environments requires laser scanners [46], [63] or imagery from multiple views [118].

Moore et al. [46] developed a UGV interface to display a virtual ground vehicle reconstruction using stereo imagery and LiDAR data, augmented by real-world object imagery and virtual handles. Atherton and Goodrich [63] developed an AV interface for a remote manipulator to display both the virtual environment and the manipulator generated by a range imaging scanner, and augmented the virtual environment with video from a camera mounted on the end effector. Results from user experimentation included decreased mental workload and increased situation awareness; however, their AV interface increased task time, likely caused by nonoptimal calibration of the virtual elements. When designed carefully, however, virtual elements can potentially result in teleoperation performance comparable to line of sight performance [45].

B. Virtual Reality (VR)
VR is an extrema on the reality-virtuality continuum where the environment contains only virtual objects [99]. A VR display can improve situational awareness and partially compensate for communication delays between the vehicle and operator, especially in deep sea explorations [10], [119]. Virtual environments enable the operator to test, preview, and verify planned sequences of motion. Sensors, such as stereo cameras, range finders, or multiple camera views, are necessary to capture both depth and position information when creating a virtual environment from the robot's surroundings.

To facilitate human-assisted or human-delegated operation, virtual reconstruction environments can display a preview of predefined manipulator actions before they are physically carried out by the system. An ROV VR interface provided users with a planning tool to assist in creating and reviewing manipulator sequences on a ground vehicle [6]. Another VR interface-enabled testing and optimization of an ROV manipulator, and users noted that small manipulation behaviors during system tuning were easier to see in the VR reconstruction as opposed to on the real manipulator [120].

Virtual reconstructions are particularly helpful for visualization of ROV manipulators in underwater environments where limited viewing angle, turbidity, and poor lighting can obstruct images. A VR interface allowed operators to input the desired ROV end-effector position with a single click to generate and preview a manipulator trajectory [9]. Zhang et al. [116] also simulated motion command inputs in a VR environment before the ROV semiautonomously completed the movements.

VR environments are beneficial when the remote environment is unknown but can be modeled. A VR interface for the ROV developed by Marani et al. [114] graphically reconstructed the vehicle in a virtual environment and displayed robot and sensor information in side panels. In addition to reconstructing the ocean bottom profile, this VR environment also simulated autonomous mission planning. Hine et al. [10] used stereo imagery and navigational information to generate a virtual environment terrain model for deep sea sampling. In addition to virtually reconstructing unknown remote environments, VR interfaces also enable the user to interact with the system from any point of view [10], [114], [118].

Without accurate representation of the vehicle's surroundings in the virtual reconstruction, operators may find the model misleading or incomplete due to a lack of information. The interface for Nomad, a ground vehicle for desert operations [7], included a VR representation of the robot's state. Their system did not transmit detailed local terrain models, and operators choose not to view the VR reconstruction because of a lack of contextual information.

On the contrary, 3-D displays that are too information-rich can impair performance. Olmos et al. [121] found that when one view in a split-screen display is more information-rich (e.g., 3-D immersive environment), operators inappropriately allocate attention to that display, even when the task required attention on a different display. Similarly, Thomas and Wickens [122] observed a “cognitive tunneling” effect when participants used an immersive display compared to an egocentric display, likely caused by a failure to integrate information accurately across two different frames of reference. It has been suggested that interfaces should combine visual and auditory alerts with information-rich VR views to improve attention allocation and lessen visual loading [121], [123], [124].

C. Stereo Vision and Depth Imaging
Stereo vision is widely used in teleoperation [128] and requires two cameras at a fixed displacement to obtain two different scene views. Many studies found that stereo 3-D displays are beneficial and indicate clear, positive performance benefits for spatial manipulations compared to monoscopic displays [125]. In particular, stereovision is beneficial for aiding in end-effector positioning in a remote workspace. Stereo video also improved deep sea exploration in unknown environments using the Virtual Environment Vehicle Interface control software developed by NASA, which enabled successful collection and sampling using a robotic ROV manipulator [10].

Depth sensors, such as time-of-flight and structured-light sensors [129], are also used to acquire and display 3-D image data in two dimensions. Active range imaging systems use light pulses and resolve distances based on the known speed of light. Day et al. [14] used active range sensors in a UGV bomb disposal interface to track and display distance from the end effector to objects of interest. Their interface was successfully tested by subject matter experts who found the display generated by the range sensor desirable. One limitation of active depth sensors, however, is low resolution [14], [129], [130], although time-of-flight and stereo vision data create more precise depth maps when combined [131].

Although often beneficial, stereoscopic depth displays do not always yield better performance over monoscopic displays. Drascic et al. [126] found that stereovision decreased task completion time only after the user gained enough operational experience when operating a mobile explosive disposal ground robot. Additionally, Draper et al. [127] found that stereovision was better only under difficult task conditions when assessing task performance. Additionally, limited research exists comparing the usage of imagery from stereo vision to active time-of-flight range sensors for performing manipulation tasks.

SECTION V.Discussion and Conclusion
The purpose of this article was to collate, summarize, and discuss literature focused on HMI for performing telemanipulation tasks using SUS and identify areas where further research is needed. This article focused only on the telemanipulation task for systems with small work environments, and did not include navigation tasks or tasks with time delays. This article included interfacing technologies and implementations from over 70 different teleoperated SUS manipulator systems. Tables I and III included summaries of physical and perceptual technologies commonly used for telemanipulation, while Tables II and IV presented a summary of findings from the surveyed literature. In addition to presenting the interface technologies and input devices for each system, robot level of autonomy was also included in this analysis as it affects human–robot interaction [22]. In all cases, an operator assumes the role of being primarily responsible for performing or monitoring telemanipulation tasks in a remote environment, which provided a basis for comparison.

As illustrated in Tables I and III, a majority of telemanipulating SUS in the literature operate under remote control. The use of remote control manipulators was likely due to the unstructured nature of operational environments for SUS, for example, in search and rescue missions. When systems cannot know the remote environment a priori, manual or computer-aided teleoperation becomes a default operation mode. A rich history of literature exists on stationary manual teleoperation HMI, and much of that work has been applied to SUS telemanipulation, for example, the use of scaled down master–slave controllers and force feedback. Different types of advanced visualization aid in direct telemanipulation, with a majority of systems relying on some form of real-world imagery through a live video feed. Additionally, studies have applied previous telemanipulation findings to recent technologies, including end-effector control implementations on workstations and tablets, and force feedback through portable, small hand controllers, although more work is needed to conduct focused, field-based HMI research using these technologies.

Physical control input devices are an important link between the remote system and human operator, and form factor of the device facilitates the type and autonomy of interaction with the system. Nearly all master–slave and joystick controllers (and other variations of physical controllers) only allow for direct remote control or human-assisted operation, which require near constant input from the operator. Control should generally be at the end effector when operating under manual teleoperation, instead of each individual joint. While continuous teleoperation using these systems can be fatiguing, control enhancements such as haptic feedback or assistive virtual fixtures improve overall performance. Physical devices with computing power, such as laptops or tablets, afford goal-oriented input and directives from the operator when systems have advanced autonomous capabilities. Virtual fixtures allow humans to safely cope with more complex and unstructured tasks and remain within the control loop, but adaptive fixtures require additional research to develop constraints in near real time, as this article was in its early stages [96]. Additionally, the past 5–10 years have seen a trend of increased mobile tablet use for controlling teleoperated systems, which will likely continue to increase as computing power and connectivity evolve; however, further evaluation of touch-based gesturing and interaction is needed to develop effective manipulator direct control through mobile devices.

Visual enhancements using processed sensor data offer great potential for improving telemanipulation performance of SUS. Depth sensing through stereoscopic or time-of-flight sensors can enhance overall task performance and end-effector placement. Visual cues using AR (such as coordinate system displays or virtual handles) offer task performance benefits during direct control operations with minimal computational effort; however, the virtual objects should be calibrated appropriately. Complete VR environments, typically used for motion planning and trajectory confirmation, are effective for systems operating at levels of autonomy higher than remote control, especially when poor visibility of the remote environment is prohibitive. It is recommended that additional research compare AV to both VR and AR, as researchers suggest AV offers the benefits of both virtual reconstructions and real-world imagery and makes relationships in the environment perceptually evident to the user [63]. Overall, the quality and type of visual display greatly affect operator performance, and the appropriate type of visual display depends on SUS capabilities, manipulation task, and level of autonomy.

Operators can take on a more supervisory role while in the loop, but a majority of tasks still require some form of direct teleoperation control. Sheridan [132] notes that “all robots for the foreseeable future will be controlled by humans, either as teleoperators steered by continuous manual movement or as telerobots intermittently monitored and reprogrammed by human supervisors.” A large proportion of focused telemanipulation SUS studies used remote control or human-assisted systems, but it is evident that the autonomous capabilities of manipulating SUS are advancing from remote control toward human-supervised systems. Experimentation is needed to verify that manipulating SUS have the necessary technical maturity and fit the requirements of desired missions with higher levels of autonomy [133]. Additionally, instead of replacing a human with a fully autonomous robot, focus should shift toward designing HMI to facilitate human-supervised interaction. High-fidelity experiments are recommended to understand the appropriate interfacing, controls, and level of operator interaction as telemanipulation missions shift toward higher levels of autonomy.