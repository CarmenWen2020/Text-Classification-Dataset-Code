ABSTRACT
Sprinting is a class of mechanisms that provides a short but significant performance boost while temporarily exceeding the thermal
design point. We propose DynaSprint, a software runtime that manages sprints by dynamically predicting utility and modeling thermal headroom. Moreover, we propose a new sprint mechanism for
caches, increasing capacity briefly for enhanced performance. For a
system that extends last-level cache capacity from 2MB to 4MB per
core and can absorb 10J of heat, DynaSprint-guided cache sprints
improve performance by 17% on average and by up to 40% over
a non-sprinting system. These performance outcomes, within 95%
of an oracular policy, are possible because DynaSprint accurately
predicts phase behavior and sprint utility.
CCS CONCEPTS
• Computer systems organization → Architectures; • Hardware
→ Power and energy.
KEYWORDS
performance optimization, power/thermal management, caches
1 INTRODUCTION
Computational sprinting [34, 35] is a mechanism for dark silicon,
which describes systems that can only power a fraction of their peak
resources [16, 47]. Sprints provide a short but significant performance boost by activating reserve cores and/or increasing frequency
and voltage. Sprints draw extra power and temporarily increase chip
temperature beyond the thermal design point (TDP). Sprints often
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MICRO-52, October 12–16, 2019, Columbus, OH, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6938-1/19/10. . . $15.00
https://doi.org/10.1145/3352460.3358301
require thermal packages that deploy phase change material to increase the system’s thermal capacitance, buffer heat during a sprint,
and dissipate that heat during normal operation.
Because sprints cannot be sustained, the system needs a mechanism to decide when to start and stop a sprint. Such a mechanism
needs to assess both the benefit (i.e., performance gains) and the
cost (i.e., thermal headroom consumed). Prior work has either relied
on an obvious trigger (e.g., parallel code region activates additional
cores [35]) or offline profiles to estimate utility [17]. However, such
approaches are limited to certain types of sprint and cannot adapt to
workload dynamics.
We propose DynaSprint, a utility and thermal aware run-time that
determines when to initiate and terminate a sprint. Based on hardware support available in current processors, we present a software
framework that evaluates utility and makes sprinting decisions. This
framework integrates new approaches to online phase classification
with prior approaches in phase prediction. In every management
epoch, the framework predicts the workload’s utility from sprinting and assesses the system’s thermal profile. We show that these
predictions are accurate and permit judicious sprints over time.
We demonstrate DynaSprint for a new class of sprints that increase microarchitectural capacity. Specifically, we propose cache
sprints that expand last-level cache capacity and exceed TDP when
that capacity is most useful. We use the cache as an example, but microarchitectural sprints could apply to resources such as the reorder
buffer, issue queue, etc. Prior work in LLC control minimizes interference [25, 28, 55], maximizes throughput [9, 15, 33], or improves
fairness [27, 31, 49, 52]. However, sprints present new challenges.
Because sprint decisions made in the present affect those in the
future, systems must pace sprints to maximize long-run performance
under thermal constraints.
Our study of cache sprinting demonstrates its viability. We find
that utility from extra cache capacity varies across program execution. Such phase behavior permits judicious sprints that increase
capacity when utility is high and cool the system when utility is low.
Furthermore, our power models indicate that cache sprint durations
can be long enough to capture lengthy, high-utility phases. Longer
durations are possible because increasing cache capacity is less
power-intensive than activating cores or scaling voltage/frequency.
We implement DynaSprint in software and evaluate it using cachesensitive applications. We prototype the run-time system on an Intel
Xeon Broadwell that supports last-level cache allocation via Intel
Cache Allocation Technology [20]. We show that DynaSprint-guided
cache sprints improve performance by 17% on average and by up to
1
426
MICRO-52, October 12–16, 2019, Columbus, OH, USA Ziqiang Huang, José A. Joao, Alejandro Rico, Andrew D. Hilton, and Benjamin C. Lee
40%. Moreover, DynaSprint outperforms a greedy policy that sprints
at every opportunity and performs within 95% of oracular policies
that use perfect knowledge of sprint utility and thermal conditions.
2 DYNASPRINT DESIGN
Computational sprinting temporarily exceeds a chip’s sustainable
thermal budget to boost performance. For example, it activates additional cores and/or boosts frequency, exceeding the processor’s
thermal design point (TDP) by an order of magnitude or more. Starting and stopping these sprints can be straight-forward. Additional
cores are activated at the beginning of a parallel region. Benefits
from frequency boosts are relatively easy to estimate. However, managing sprints for microarchitectural resources to maximize long-run
performance is challenging. Thus, we design a general framework
called DynaSprint that can (1) manage microarchitectural sprints, (2)
identify sprint opportunities accurately, and (3) minimize hardware
and management overhead.
Broad Application. To find broad application to varied microarchitectural sprint mechanisms, DynaSprint looks beyond simple
triggers like those proposed for processor core allocation. Instead, it
integrates new approaches to online phase classification with prior
work in phase prediction to sprint based on application phase characteristics.
High Accuracy. DynaSprint must “sprint when it really matters”.
Sprinting without justified benefit in the present not only wastes
energy but can potentially hinder future sprints. DynaSprint uses
multiple history-based predictors to ensure each sprint decision is
made with high confidence.
Low Overhead. Since sprints are usually applied to power and
thermal constrained systems, the management framework must incur
low overheads. DynaSprint is a software runtime system that utilizes
hardware support already available on modern processors. Moreover,
DynaSprint works well with coarse-grained epochs that span 100M
instructions, making management overheads negligible.
Figure 1 motivates these design goals with a snapshot of program
execution split into phases. At “current time,” DynaSprint’s four
tasks are to (1) confirm that the previous epoch was actually “phase
B”, (2) predict that the next epoch is “phase A”, (3) predict the
utility from sprinting in “phase A”, and (4) determine whether the
thermal headroom in the next epoch permits a sprint during phase A.
DynaSprint must complete these tasks accurately and efficiently.
Figure 1: Example program execution in phases
2.1 Management Architecture
Figure 2 presents an overview of DynaSprint. DynaSprint manages
sprinting by collecting data about the previous epoch. When an
epoch ends, the phase classifier assesses hardware performance
counters, which supply the phase signature, to decide whether the
epoch corresponds to a previously observed phase or a new one
(Section 2.2).
DynaSprint makes a series of predictions about the next epoch
to decide whether to sprint. First, the phase predictor uses the most
recently completed epoch’s phases to query the phase history pattern
table and predict the next epoch’s phase (Section 2.3). Second, the
utility and energy predictor uses the next epoch’s phase as well as
its historical performance and energy measurements to predict the
effects of sprinting (Section 2.4). Third, the thermal tracker uses
the previous epoch’s energy consumption to determine how much
thermal headroom remains in the system (Section 2.5). Finally, the
sprinting coordinator aggregates predictions to decide whether to
sprint in the next epoch (Section 2.6).
2.2 Phase Classifier
DynaSprint classifies epochs into phases to facilitate the prediction
of utility from sprints. Epochs associated with the same phase exhibit
similar characteristics such as instruction level parallelism, memory
intensity, branch prediction accuracy, etc. DynaSprint captures these
characteristics using hardware performance counters available on
most modern processors. DynaSprint uses a phase signature, defined by data from a few counters, to classify each epoch into a
corresponding phase.
Phase Signature. DynaSprint constructs a phase signature from
three performance counters: the number of L1 and L2 cache misses
per thousand instructions and the number of branch mispredictions
per thousand instructions (abbreviated L1, L2, BR MPKI).
These hardware counters present a number of advantages. First,
these counters are good indicators of program phase and cache utility.
They measure activity in both datapath and caches, allowing them to
distinguish between program phases. Moreover, these counters are
independent of last-level cache capacity, producing the same phase
signatures for the same instructions in both normal and cache sprinting modes. Finally, these counters are available on most processors,
making DynaSprint compatible and portable.
Phase Signature Boundaries. DynaSprint tracks phases and
their corresponding signatures during program execution. For each
epoch, DynaSprint determines whether its signature matches a previously observed phase or differs enough from prior signatures to
describe a new phase. DynaSprint makes this determination by specifying boundaries around phases and their corresponding signatures.
When a measured signature lies within an existing phase’s boundaries, the epoch is associated with that phase. When it lies beyond
any existing boundary, the epoch is associated with a new phase.
An epoch with a phase signature of N counters can be viewed as a
point in a N-dimensional space with each counter being its respective
coordinate. Figure 3 shows a 2D example in which a phase signature
only consists of two counters: L1 MPKI and BR MPKI. In this
example, the phase classifier has already identified three phases: A,
B and C, represented by the rectangles with the solid lines. The dots
within each rectangle represent the program epochs that have been
classified into the corresponding phase. Suppose the coordinates
of the next epoch “x” are within the rectangle of phase B, then x
is classified into phase B, and phase B’s boundary is updated after
427
DynaSprint: Microarchitectural Sprints with Dynamic Utility and Thermal Management MICRO-52, October 12–16, 2019, Columbus, OH, USA
Figure 2: DynaSprint overview: (1) phase classifier reads the phase signature, IPC and energy counters for the current epoch from
system monitor (2) thermal tracker reads the energy counters (3) phase classifier identifies the phase for the current epoch (4) phase
predictor predicts the next phase (5) utility/energy predictor predicts the sprint utility/energy for next epoch (6) thermal tracker
calculates the current thermal headroom (7) sprinting coordinator sets the sprint utility threshold (8) sprinting coordinator decides
whether to sprint or not for the next epoch
including x. However, suppose the coordinates of the next epoch
“y” are not within any of the existing rectangles, then y forms a new
phase D.
Figure 3: Phase Classifier Example
Algorithm 1 details the analysis of phase and signature boundaries.
When an epoch ends, DynaSprint collects a signature for the current
epoch PScur. The algorithm determines whether that signature lies
in the neighborhood of signatures for previous observed phases
PSpre[i].
Phase signature PS1 is within the boundary of PS2 if, for every
counter Cj
in the signature, the relative difference between PS1.Cj
and PS2.Cj
is smaller than boundary parameter Brd or the absolute
difference between PS1.Cj and PS2.Cj
is smaller than parameter
Bad. We assess both absolute and relative differences because comparisons between small values (e.g., those less than one) can produce
large relative differences. DynaSprint associates the current epoch
with the nearest signatures and phases based on Euclidean distance.
Boundary Parameters. The values of Brd and Bad determine
phase granularity, which ultimately determines the total number of
phases that describe the program. The bigger the boundaries, the
smaller the number of phases. On one hand, fewer phases mitigate
classifier overheads, which are linear in the number of phases. On
Algorithm 1 Boundary-based Phase Classification
1: procedure CLASSIFY(PSpre[ ], PScur)
2: PhaseID = -1
3: closest = INT_MAX
4: for PSi ∈ PSpre do
5: if for every Cj ∈ PS:
|PSi
.Cj−PScur.Cj
|
PSi
.Cj
< Brd or
6: |PSi
.C j −PScur.C j| < Bad then
7: dist = EuclideanDistance(PScur, PSi)
8: if dist < closest then
9: closest = dist
10: PhaseID = PSi
.PhaseID
11: if PhaseID == -1 then
12: PhaseID = NewPhaseID
13: PSpre[PhaseID] = PScur
the other hand, more phases increase analysis granularity and permit
more accurate predictions of utility and energy. Finer granularities
lower variance and improve prediction for utility and energy for each
phase. We set boundary parameters to Brd = 0.30 and Bad = 2 and
show, in Section 5.4, that these parameters require a small number
of phases yet predict utility and energy accurately.
2.3 Phase Predictor
DynaSprint implements phase prediction using a Global Phase History Table (GPHT) predictor [24]. First, the classifier assigns the
just-completed epoch to a phase. Then, it uses the classified phase to
index and update the history table. Finally, it uses the table to predict
the next epoch’s phase.
The green box in Figure 2 details the Global Phase History Table
(GPHT). Its main structure consists of a global shift register, called
the Global Phase History Register (GPHR), that tracks the last few
observed phases. GPHR contents are used to index into a Pattern
History Table (PHT), which caches several previously observed
phase patterns (PHT Tags), their corresponding predictions for next
428
MICRO-52, October 12–16, 2019, Columbus, OH, USA Ziqiang Huang, José A. Joao, Alejandro Rico, Andrew D. Hilton, and Benjamin C. Lee
phase (PHT Pred-n), as well as recency information (Age/Invalid).
When GPHR and PHT tags do not match, the predictor falls back to
Last Value Prediction, predicting the last observed phase, stored in
GPHR[0], as the next phase. Such table-based history predictors are
much more effective than simple statistical predictors, particularly
for highly variable programs [13, 24].
This phase prediction strategy exploits the fact that many programs consist of phases with similar characteristics and behaviors.
Accurately predicting phases as the program runs benefits various
online optimizations such as hardware reconfiguration, voltage and
frequency scaling (DVFS), thermal management, and hotcode optimization [5, 7, 11, 21, 22, 24, 29, 45, 51]. Phase behaviors are
repetitive and prior work has proposed table-based history predictors
to capture past phase patterns for future prediction [13, 24].
2.4 Utility and Energy Predictor
DynaSprint uses historical data, recorded in the phase classifier, to
predict the utility from sprinting in the next epoch. The classifier
tracks, for each phase, four measures—instruction throughput and
processor energy when running in normal and sprinting modes (IPCn,
IPCs, En, Es).
Updating the Predictor. When an epoch is classified and assigned to a phase, its performance and energy profile updates the
classifier’s data corresponding to its phase and mode. Initially, when
the classifier encounters the first epoch observed for a phase, it will
record the performance and energy profile for either the normal or
sprinting mode. Later, when the classifier encounters the second
epoch for the same phase, the sprinting coordinator collects data for
the other mode by initiating or halting a sprint.
This mechanism ensures the classifier collects data for normal
and sprinting modes at least once for each phase, thereby exploring
the utility from sprinting. After the classifier has initialized a phase’s
entry with its first two epochs, performance and energy histories are
updated automatically as the sprinting coordinator makes decisions
at run-time.
Invoking the Predictor. The phase predictor forecasts the next
epoch’s phase, producing a phase ID. This ID indexes into the phase
classifier’s table to produce corresponding historical data for instruction throughput and processor energy, which serve as the predicted
utility and cost from sprinting in the next epoch.
2.5 Thermal Tracker
The thermal tracker monitors power dissipation and thermal headroom to constrain sprint decisions. The tracker can measure power
with live measurements or power models [2]. DynaSprint measurements drive a thermal model that calculates headroom, which quantifies the heat (measured in Joules) that can be expended before
exceeding the processor package’s thermal capacitance. When the
package uses a phase change material [35], the thermal model uses
the following parameters:
• Cpcm: The phase change material’s thermal capacitance, which
determines the heat that can be buffered.
• Rpcm: The phase change material’s thermal resistance, which
determines maximum power dissipated during a sprint.
• Rpackage: The processor package’s thermal resistance, which
determines how quickly heat transfers from the phase change
material into the ambient after a sprint.
• Rtotal: The system’s total thermal resistance, which determines the processor’s maximum sustained power.
Let us denote the system’s nominal power as Pn, its sprinting
power as Ps, and its thermal design point as Pt
. Denote the system’s
thermal headroom as Ht at time t. Suppose the system sprints from
t1 to t2 without exhausting the thermal headroom and then operates
in normal mode from t2 to t3. We can estimate the thermal headroom
at these points in time.
H2 = H1 −(Ps −Pt)×(t2 −t1)
H3 = H2 + (Pt −Pn)×(t3 −t2)
2.6 Sprinting Coordinator
The sprinting coordinator initiates and terminates sprints. The decision to sprint must balance utility in the present and the future,
a trade-off encapsulated by two questions. First, how significant is
utility in the next epoch Unext compared to potential utilities further
into the future? Second, if the system sprints and reduces thermal
headroom in the present, will it be able to sprint and exploit highutility epochs in the near future? Answers to these questions would
permit the system to use thermal headroom judiciously and enhance
performance over the long run.
The coordinator pursues these objectives given utility predictions
and thermal models for the next epoch. If the coordinator is aggressive and sprints when utility is low, it may deplete the system’s
thermal headroom that might have been helpful in the future. If
the coordinator is conservative and does not sprint when utility is
low, it may lose an opportunity to translate thermal headroom into
performance.
Thermal Proportional Threshold (TP-T). We propose a policy,
TP-T, that determines whether the coordinator initiates a sprint in
the next epoch. Shown in Equation 1, TP-T sets a threshold for
initiating a sprint Uthreshold by multiplying the fraction of the thermal
headroom already consumed by the maximum utility Umax derived
from recent sprints (e.g., over the last ten epochs). The coordinator
initiates a sprint if predicted utility exceeds the threshold and the
system possesses sufficient thermal headroom.
Uthreshold = Umax×
Hconsumed
Htotal
(1)
In effect, the coordinator treats thermal headroom as a resource
whose value varies depending on the amount available and it treats
utility as the return from spending that resource. As the amount
of thermal headroom decreases, it becomes more valuable and the
coordinator requires a higher return when consuming it.
3 CACHE SPRINTING ARCHITECTURE
We highlight the advantages of DynaSprint for cache sprints, an
instance of a new class of techniques we call “microarchitectural
capacity sprinting.” Cache sprinting briefly expands the processor’s
last-level cache (LLC) capacity beyond what is dictated by the thermal design point. During normal operation, half of the LLC ways
are powered off and unused. During a sprint, the remaining ways
are powered on. When sprinting, the LLC dissipates more dynamic
429
DynaSprint: Microarchitectural Sprints with Dynamic Utility and Thermal Management MICRO-52, October 12–16, 2019, Columbus, OH, USA
Figure 4: Performance comparison: scaling frequency vs increasing LLC capacity, baseline 2.1GHz with 2MB LLC
Figure 5: Performance gain: gcc-s04 running with 4MB LLC over baseline 2MB
power because each access must check twice the number of tags
and the cache supplies more data in response to hits. The LLC also
dissipates more static power.
3.1 Case for Cache Sprinting
Cache sprinting complements prior mechanisms in computational
sprinting that activate additional cores and boost their frequencies.
Sprinting with the last-level cache serves memory-bound applications that do not benefit from boosted processor cores.
High Performance Impact. Figure 4 shows the performance
gains for two alternative sprint mechanisms that consume 2W of
extra power—scaling frequency and increasing LLC capacity—on
an Intel Xeon Broadwell processor. Scaling frequency from 2.1GHz
to 2.3GHz improves performance for compute-intensive applications
by 10% on average but has little effect for memory-intensive applications. In contrast, increasing cache capacity from 2MB to 4MB
improves performance for memory-intensive applications significantly. Although the potential benefits of LLC sprints are significant,
the management mechanism must exploit that potential.
Time-Varying Utility. Intelligent sprints make sense only if utility from cache capacity varies across phases, permitting sprints when
utility is high and cooling when utility is low. This variance exists
in most workloads and we present gcc as an example in Figure 5.
The x-axis shows epochs of 100M instructions and the y-axis shows
speedup with a 4MB LLC over 2MB LLC in each epoch. Utility
varies from 0% ( excellent times to cool) to 80%, with sustained
periods of 70% (excellent times to sprint).
Low Power Density. LLC power density is much lower than that
of processor cores used in prior sprint studies [47]. Cache sprints
exhibit smaller power deltas due to the cache’s lower power density
and smaller difference between max and average power. Smaller
deltas permit longer, more aggressive sprints before hitting thermal
constraints. Although cache sprints generate extra heat, primarily due
to static power, the LLC is unlikely to become a hotspot. Datapath
structures, such as the register file and ALUs, exhibit much higher
temperatures [26]. Thus, cache sprints will be constrained by the
processor’s global heat profile rather than any local hot spots.
Note that our framework generalizes beyond LLC sprints to
other memory microarchitectures or technologies. For example, 3DDRAM could offer hundreds of MB of LLC but would benefit from
a sprint framework that exploits its huge capacity while managing
its limited cooling capabilities.
3.2 System Architecture
DynaSprint is a software runtime system that controls cache sprints.
Software decides when to sprint and communicates that decision to
hardware via a control register. Although full hardware control is
possible and permits decisions at finer granularities, software control permits more sophisticated management policies. The operating
system can track information about each process’s behavior and perform complex computation at coarse granularities to make informed
sprinting decisions that reflect program behavior.
When a sprint starts, the additional cache ways are empty and
fill over time. If the sprint is timely, the ways fill with useful data,
430
MICRO-52, October 12–16, 2019, Columbus, OH, USA Ziqiang Huang, José A. Joao, Alejandro Rico, Andrew D. Hilton, and Benjamin C. Lee
the hit rate increases, and performance improves. As the sprint
progresses, it consumes the chip’s thermal headroom by raising the
processor’s temperature toward safety tolerances or transforming
the phase change material into an amorphous state. The sprint ends
when the software finds little utility from extra cache capacity or the
hardware finds that the thermal headroom has been exhausted.
Sprints must end with sufficient time and thermal headroom to
write dirty data to memory before powering down. In an extreme
case, every block in the additional ways is dirty and block addresses
are distributed poorly across memory pages. For example, writing
back 16MB of dirty data would require less than 500 microseconds,
a negligible transition delay given sprints that span several seconds.
Although software can make decisions about utility, it cannot
be trusted with the chip’s physical safety. Hardware must monitor
thermal headroom and halt sprints, even if it means overriding the
operating system, when headroom is exhausted. Specifically, hardware must set the machine state register’s (MSR’s) sprint bit to zero
and ignore writes to that register until thermal headroom has been
restored so that the cache can sprint without jeopardizing thermal
budgets. In the event of a thermal emergency, the hardware halts
processor cores until cache ways are powered off.
4 EXPERIMENTAL METHODOLOGY
System Setup. We emulate LLC sprints on an off-the-shelf chip
multiprocessor, restricting its LLC capacity in nominal mode and
restoring its capacity in sprint mode. We focus our sprint evaluation
on serial workloads running on a single core and size LLC capacity
accordingly.
Table 1 summarizes the system configuration. We conduct experiments on physical hardware using an 8-core Intel Broadwell Xeon
E5-2620 v4 processor. The Xeon processor has a 20MB, 20-way
last-level cache, which corresponds to 2.5MB per core. In normal
mode, we configure Intel’s Cache Allocation Technology(CAT) [20]
to restrict a core and its single-threaded program to use 2MB (not
2.5MB because CAT allocates cache capacity at one-way, 1MB
granularity). In sprinting mode, we permit a core to use 4MB of
cache.
We configure the system to minimize interference and ensure
deterministic results. First, we disable C-states and DVFS, fixing
all cores’ frequencies to 2.1GHz. Second, we disable the Watchdog
hang timer as well as the Address Space Layout Randomization.
Third, we stop all non-essential system daemons/services. Fourth,
we use cset command for CPU shielding so that other processes will
not run on the core targeted for experiments. Finally, we run each
application three times and use the average number for all reported
performance values. We collect the relevant hardware counters’ values for phase signatures, from Section 2.2, using Performance Application Programming Interface (PAPI) [48]. These numbers are
collected every 100 million instructions using the PAPI_overflow()
function.
Applications. We evaluate SPECCPU2006 [19], SPLASH2 [50]
and PBBS [43] benchmark suites. We focus on the 17 LLC-sensitive
applications from Figure 4, which exhibit higher performance gains
from larger cache capacity than with processor frequency scaling.
Applications in SPECCPU2006 and SPLASH2 are run to completion
with the largest input size, which usually takes minutes. Applications
Table 1: System Specification
Field Value
Core 8-core Broadwell, 2.1GHz
L1 Caches 32KB, private, split D/I, 8-way
L2 Caches 256KB, private, 8-way
L3 Cache 20MB, shared, 20-way
Way-partitioning with Intel CAT [20]
Memory 8GB, RDIMM, DDR4 2400MT/s
OS Red Hat Enterprise Linux 7.5
Linux kernel version 3.10.0
in PBBS are much smaller and are run multiple times, with different
inputs, until 100B instructions have been executed.
Power Model. We estimate our system’s power consumption
using Intel’s Running Average Power Limit (RAPL) driver [2]. Static
power is estimated by measuring a microbenchmark that only calls
the sleep() function while dynamic power is measured during the
execution (minus estimated static power).
On our system, the available RAPL domains only provide power
for the package (cores + LLC + memory controller) as a whole.
We use power breakdowns from prior work [10] to estimate the
individual power of cores and LLC. In summary, we estimate the
nominal and sprinting power of a scaled system (single core + 2MB
LLC) to be 6W and 8W, respectively, in which LLC consumes about
1W per MB.
Thermal Model. We use a thermal model like those in prior sprint
studies, which use PCM to increase thermal capacitance [34, 35].
We consider heat spreaders with low thermal resistance that take heat
from the processor die and present a uniform temperature distribution
to the heat sink [26]. We consider an amount of PCM that absorbs
10J of heat before melting completely, enabling a sprint spanning
tens of seconds. Note that PCM parameters affect the run-time’s
thermal model because we emulate systems with lower TDPs that
require intelligent sprints. The baseline system has a TDP of 7W for
1 core and 2MB LLC. We evaluate sensitivity to these parameters in
Section 5.5.
5 EVALUATION
We evaluate DynaSprint and its thermal-proportional threshold policy against several alternatives. Since there is no prior work in managing LLC sprinting, we compare against three other policies that
vary in their knowledge of sprint utility and thermal headroom.
Greedy (G) initiates a sprint whenever thermal headroom is available, ignoring utility. Once thermal headroom is exhausted (i.e.,
PCM transition to amorphous state completes), it waits for the system to cool and restore thermal headroom before starting the next
sprint.
Local-Oracle (LO) has perfect knowledge of sprint utility from
all prior epochs (Uhistory) as well as the next epoch (Unext). It initiates
a sprint if Unext is greater than the average of Uhistory and stops
otherwise. LO is an oracular policy, which represents heuristics that
only incorporate local information obtained through online profiling
or prediction. The sprint threshold is set based on utility alone.
431
DynaSprint: Microarchitectural Sprints with Dynamic Utility and Thermal Management MICRO-52, October 12–16, 2019, Columbus, OH, USA
Figure 6: DynaSprint Performance: compared against various policies
Global-Oracle (GO) has perfect knowledge of sprint utility from
all past and future epochs in the program’s execution. It sorts epochs
based on their sprint utilities and iteratively selects the epoch with
the highest utility for sprinting execution. If the selected epoch
cannot sprint due to lack of thermal headroom, the epoch is marked
for normal execution and GO proceeds to the next epoch. GO uses
global information obtained through offline profiling and cannot be
implemented online. However, this policy provides an upper bound
on performance from cache sprinting.
Thermal-Proportional Threshold (TP-T) predicts the sprinting
utility of the next epoch (Unext) and tracks the maximum utility
observed from a sprint in the recent past (Umax). Then it sets the
sprint utility threshold (Uthreshold) by multiplying the percentage
of the already consumed thermal headroom and Umax as shown in
Equation 1. Finally it initiates a sprint if Unext exceeds Uthreshold.
5.1 Performance
Figure 6 shows performance gains from LLC sprints over a baseline
that operates under nominal power. (The blue dots indicate upper
bounds on performance given unlimited thermal headroom and a
continuous 4MB sprint.) DynaSprint-guided LLC sprints improve
performance by 17%, on average, and by up to 40%. When compared
with alternatives, DynaSprint’s TP-T policy outperforms greedy
heuristics and is competitive with oracular policies.
Comparison against Greedy (G). TP-T significantly outperforms G for 8 of 17 benchmarks because its sprints are more judicious and timely. Figure 7(a) shows the probability density of
sprint utility for three of these applications: gcc, ocean and bfs (one
from each benchmark suite). Their sprint utility distribution often
appears bimodal, revealing many epochs that benefit greatly from
sprinting and many that do not. The large variance in sprint utility offers TP-T an opportunity to spend its limited thermal budget
wisely to maximize performance gains. For instance, ocean frequently achieves large performance gains between 50 to 60% and
also achieves small performance gain between 0 to 20%. TP-T is
more likely to reserve its sprints for the large gains. Greedy, on the
other hand, could waste its thermal budget in low-utility epochs and
have insufficient headroom for high-utility ones.
Greedy performs comparably to TP-T for several benchmarks.
Figure 7(b) shows the probability density of sprint utility for three of
these applications. They often exhibit very small variance in sprint
Figure 7: PDF for utility: (a) bimodal, (b) unimodal
utility and all epochs benefit similarly from sprinting. For example,
most of soplex’s epochs report utility between 10 to 20%. There is
not much opportunity to prioritize sprints for high-utility epochs. As
a result, TP-T’s threshold will be lower than the average sprint utility
for most epochs when thermal headroom is available. In effect, TP-T
behaves very similarly to greedy.
Comparison against Local Oracle (LO). TP-T’s performance
is close to LO’s. Indeed, TP-T performs better than LO for 7 of 17
benchmarks. Although LO has perfect knowledge of local utilities,
its sprint thresholds neglect the system’s current thermal conditions.
As a result, LO can sprint too conservatively when thermal headroom
is abundant and too aggressively when thermal headroom is scarce.
TP-T is aware of thermal constraints and paces its sprints according
to available headroom.
There are cases where LO outperforms TP-T and is close to GO
(e.g., bfs, st) due to two reasons. First, because of their bimodal
utility distribution, when LO sets sprint thresholds based on average
432
MICRO-52, October 12–16, 2019, Columbus, OH, USA Ziqiang Huang, José A. Joao, Alejandro Rico, Andrew D. Hilton, and Benjamin C. Lee
Figure 8: Energy: DynaSprint and 2.1GHz-4MB configurations relative to 2MB baseline
Figure 9: DynaSprint sprinting behavior: (a) ocean_ncp, (b) gcc-s04
of local utilities, the high-utility epochs will also exhibit utilities
greater than the thresholds. Second, these applications tend to have
a small number of recurring phases such that local information
captures global behavior.
Comparison against Global Oracle (GO). On average, TP-T
performs within 95% of the performance upper bound set by GO.
GO has perfect knowledge of sprint utility for every epoch and perfect knowledge of the system’s thermal condition. This oracular
knowledge persists even as epochs are selected for normal or sprinting computation. The small performance gap between TP-T and GO
indicates that DynaSprint accurately predicts sprint utility and that
the TP-T thresholds are effective for pacing sprints to maximize
performance gains over the long run.
5.2 Energy
Figure 8 shows DynaSprint’s energy consumption compared with the
baseline configuration that uses only 2MB of LLC. The energy overhead of the configuration that constantly uses 4MB of LLC is also
plotted for reference. On average, DynaSprint increases energy by
less than 5%. For several applications, such as bfs and st, DynaSprint
even reduces energy by about 10% due to significant reductions in
execution time. Although the goal of sprints is not improving energy
efficiency but maximizing performance under thermal constraints,
these results show that sprints can improve efficiency.
5.3 Sprinting and System Dynamics
Figure 9(a) shows system dynamics during the complete execution
of ocean_ncp. The orange line plots offline profiles of sprint utility.
The green line plots online measurements of sprint utility achieved
433
DynaSprint: Microarchitectural Sprints with Dynamic Utility and Thermal Management MICRO-52, October 12–16, 2019, Columbus, OH, USA
Figure 10: Impact of phase signature definition on phase classification accuracy
Figure 11: Impact of signature boundary on: (a) phase classification accuracy, (b) total number of phases
from DynaSprint’s TP-T policy. Note that the orange and green
lines align in most cases. The red line shows the available thermal
headroom. The blue line shows DynaSprint’s threshold for deciding
to sprint.
The figure illustrates two major system behaviors. First, DynaSprint accurately identifies high-utility epochs and executes sprints
during these times. For epochs that benefit most from sprints, the
orange line overlaps with the green line. Second, DynaSprint dynamically adjusts its sprint threshold according to recently observed
performance data and the available thermal headroom according to
the TP-T policy.
Ocean_ncp is representative of applications that have a few highlyrepetitive phases and have a bimodal utility distribution. DynaSprint
can manage sprinting effectively for these applications because its
phase classifier and predict can capture phase behaviors effectively.
Moreover, its TP-T policy can easily identify high-utility epochs and
trigger timely sprints.
Figure 9(b) shows the same system dynamics for a different type
of application, gcc. Compared to ocean_ncp, gcc has many more
phases and those phases are much less repetitive. Gcc is representative of applications that go through many different stages during their
execution. This type of application is harder to manage as its less
predictable. However, we see that DynaSprint still performs well,
sprinting at most of the high-utility epochs and dynamically adapting
the threshold to phases and thermal budget. The gap between TP-T
and GO is mainly caused by phase and utility mispredictions.
5.4 Prediction Accuracy
Phase Classification. DynaSprint assigns epochs to phases as the
first step in predicting utility, which is critical to the effectiveness
of the TP-T policy. We measure accuracy by assigning an epoch to
a phase tracked by the classifier and determining how closely that
phase’s performance predicts the epoch’s performance. Error is the
percentage difference between the phase’s instruction throughput and
the epoch’s actual throughput. Two design elements affect accuracy:
434
MICRO-52, October 12–16, 2019, Columbus, OH, USA Ziqiang Huang, José A. Joao, Alejandro Rico, Andrew D. Hilton, and Benjamin C. Lee
Figure 12: Utility prediction accuracy
the definition of the phase signature and the boundaries that define
neighborhoods around phase signatures.
Figure 10 shows classification error when including varied hardware counters in the phase signature. Including more counters in the
signature improves accuracy. A single counter can perform poorly
(e.g., bzip2, sa) and a second counter significantly improves accuracy.
Moreover, using diverse measures of activity improves accuracy.
Among signatures defined by two counters, combining either L1 or
L2 MPKI with BR MPKI performs better than combining L1 and L2
MPKI. L1 and L2 MPKI both characterize memory intensity while
BR MPKI characterizes datapath activity. DynaSprint uses all three
counters to define phase signatures that achieve high accuracy across
all applications.
Figure 11(a) shows classification error when using varied boundaries (Brd) around phase signatures. Smaller boundaries capture finergrained phases and improve accuracy. However, Figure 11(b) shows
that finer-grained phases increases the number of phases the classifier tracks, which increases overhead. DynaSprint sets Brd = 0.3
to limit the number of phases yet accurately classify performance
to within 97% of the actual value. Although narrowing boundaries
(Brd = 0.20) improves accuracy slightly, it significantly increases
the number of phases for a few applications. On the other hand,
broadening boundaries (Brd = 0.40) noticeably reduces accuracy
without much reducing the number of phases.
Utility Prediction. Figure 12 assesses how the phase classifier’s
accuracy translates into utility predictor’s accuracy. Error is the
percentage difference between predicted and actual speedups from a
cache sprint where speedup is predicted from instruction throughputs
reported by the classifier. Overall, DynaSprint accurately predicts
utility and sprint speedups to within 95% of actual values.
Utility prediction accuracy exhibits larger variance across applications than phase classification accuracy. Some applications achieve
higher accuracy because of their relatively stable phases and sprint
utility (e.g., omnetpp and fft). But some applications are harder to
predict because of high variance in their phases (e.g., bzip2) and
sprint utility (e.g., xalancbmk). The delaunnay and sa benchmarks
report the largest errors, for both phase classification and utility
prediction, because they have many epochs with similar phase signatures but different performance profiles.
5.5 Sensitivity Analysis
Thermal Design Power (TDP). We have been evaluating a system
that operates nominally at 6W, sprints at 8W, and is constrained by a
7W thermal design point. For this system, the cooling duration that
restores the thermal headroom must match the sprint duration. Time
spent generating heat at 1W above TDP must be matched by time
spent dissipating heat at 1W below TDP; the sprint-to-cool ratio is
1:1.
Figure 13(a) presents performance under tighter thermal constraints where TDP is set to 6.5W, closer to nominal power. As the
gap between nominal and thermal design power narrows, the system requires more time to cool after sprint. For our parameters, the
cooling duration must be 3× longer than the sprinting duration; the
sprint-to-cool ratio is 1:3. Sprints become more expensive, degrading
sprint performance across all policies.
Costly sprints reduce the system’s tolerance to poor decisions.
TP-T’s advantage over G grows under tighter thermal constraints
(e.g., fft, bfs, mst). TP-T identifies high-utility epochs and sprints judiciously whereas G luckily sprints during high-utility epochs when
thermal headroom was generous but suffers from poor decisions
when headroom becomes scarce. As sprints become more expensive,
prediction accuracy becomes more important and TP-T cannot compete wth oracular policies. TP-T underperforms LO, which benefits
from perfect knowledge of sprint utility (e.g., bzip2, gcc).
Total Thermal Headroom (TTH). We have been evaluating
a system with PCM that provides 10J of thermal headroom. Figure 13(b) presents performance when headroom is halved to 5J.
Performance is unaffected for most applications. Headroom determines the duration of a full sprint. For example, 10J permits our
system to sprint for tens of seconds. But when applications prefer
sprints that exceed the full duration, headroom impacts performance
less than TDP because it does not change the maximum sustainable
sprint-to-cool ratio. As long as the application derives varied utilities
from sprints across time, TP-T will exploit low-utility epochs for
cooling and judiciously exploit thermal headroom by dynamically
setting the thresholds for sprints.
Sprinting Intensity (SI). We have been evaluating a sprint that
doubles LLC capacity from 2MB to 4MB. Figure 13(c) presents
performance when sprints triple capacity to 6MB. More intense
sprints do not necessarily translate into long-run performance and
only three applications benefit. As sprint intensity increases to 6MB,
sprint power increases to 10W and the maximum sustainable sprintto-cool ratio falls to 1:3.
Figure 14 shows the time spent sprinting when a sprint expands
cache capacity to 4MB or 6MB. Unless performance from 6MB
is much greater than that from 4MB, the more intense sprint does
not justify the extra power. Cache capacity suffers from diminishing
marginal returns and most applications in our study do not benefit
significantly more at 6MB compared to 4MB.
When designing a system for bimodal operation, sprint or normal,
architects must understand applications’ performance sensitivities to
resource allocations. Ideally, the sprinting and normal modes are configured to maximize marginal performance gains given a marginal
resource allocation. If applications’ sensitivities vary, the system
could support multiple sprint intensities. DynaSprint could easily
435
DynaSprint: Microarchitectural Sprints with Dynamic Utility and Thermal Management MICRO-52, October 12–16, 2019, Columbus, OH, USA
Figure 13: Performance sensitivity to (a) TDP (6.5W), (b) total thermal headroom (5J), (c) sprinting intensity (6MB)
Figure 14: Impact of sprinting intensity on sprinting time
track performance histories for each sprint mode and application
phase, deciding which mode improves performance most efficiently.
Comparison to Sustained Operation at TDP. We set our baseline, nominal power at 6W, which is 1W below the 7W thermal
design point that dictates sustainable power draw. However, we also
compare DynaSprint against an operating point that dissipates 7W,
assuming that the processor uses power to operate exactly at the
thermal design point. Because our models indicate that the system
requires 1W of power per 1MB of cache capacity, sustained operation at 7W permits the sustained use of a 3MB LLC.
Figure 15 compares performance from sprinting (dynamic 2MB
and 4MB modes) and sustained operation at TDP (static 3MB mode)
Figure 15: DynaSprint performance against perfect TDP
relative to a baseline 2MB mode. For applications that exhibit high
variance in sprint utility, DynaSprint significantly outperforms sustained operation at TDP. For applications that exhibit low variance,
performance depends on application sensitivity to cache size. Some
applications (e.g., omnetpp) benefit more from a cache size much
larger than permissible under TDP and prefer alternating between
sprint and nominal modes. Others (e.g., xalancbmk and soplex) are
insensitive to cache sizes between 2MB and 4MB such that sustained
operation with 3MB matches the performance of sprint-and-cool.
436
MICRO-52, October 12–16, 2019, Columbus, OH, USA Ziqiang Huang, José A. Joao, Alejandro Rico, Andrew D. Hilton, and Benjamin C. Lee
5.6 Discussion
Multi-core Implications. Although we only evaluate single-core
applications in this paper, DynaSprint can naturally be extended to
manage multi-program sprints. The phase predictor should remain
effective because the framework can collect per-process performance
counters. However, a program’s sprinting strategy should account
for system dynamics (i.e., other programs’ decisions). Each program
can request sprints based on its own utility, but the system grants
sprints based on system performance and fairness metrics.
Multi-resource Implications. While we focus on managing sprints
with a single resource (i.e., last-level cache)in this paper, DynaSprint
can also be extended to manage sprints with multiple resource types.
For example, DynaSprint could assess the compute and memory
intensities within and across applications, deciding whether to boost
core frequency or expand last-level cache capacity. DynaSprint could
also apply various techniques to reduce the management overhead.
For instance, if DynaSprint already knows that an epoch has high utility with cache sprinting, it should not boost core frequency because
these two mechnisms complement each other.
6 RELATED WORK
Computational sprinting has been applied to datacenters [17, 44, 53].
At such scale, power constrains not only the processor chip but also
the servers and clusters that share a power supply. Uncoordinated
sprints risk tripping the circuit breakers. Although batteries can
supply power to complete computation during power emergencies,
future sprints are forbidden until batteries recharge.
Management policies have been proposed to partition the lastlevel cache to minimize interference [25, 28, 55], maximize system throughput [9, 15, 33], or improve fairness [27, 31, 49, 52].
Performance-centric partitioning mechanisms often construct miss
rate curves (MRC), which characterize the miss rate as a function of
the cache allocation. Utility-based Cache Partitioning (UCP) [33]
uses simple hardware monitors to estimate the MRC, relying on the
stack property of the LRU replacement policy [30]. Ubik [25] uses
an analytical model that relies on transient behavior being analyzable and makes assumptions about future cache microarchitecture.
Prior work has also estimated the MRC in software [8, 36, 37, 46],
tracing memory addresses and using analytical models [8, 14]. Unfortunately, tracing memory addresses is expensive and introduces
significant slowdowns over native execution.
Many hardware design techniques have been proposed to dynamically resize caches [4, 6, 18, 32]. These techniques focus on reducing
leakage power to save energy subject to constraints on performance
degradation. In contrast, sprinting seeks to maximize long-term performance gains under thermal constraints and must assess trade-offs
between sprinting now and sprinting in the future.
Most dynamic phase analysis techniques observe that performance is strongly correlated with executed code [11, 12, 38, 40, 41].
These techniques often collect some form of execution frequency
vectors (EFV) that identify the code executed at some point in time.
Prior works have constructed EFVs with instructions [11, 38, 40] or
basic blocks [12, 41]. Unfortunately, EFVs are expensive because
they record multiple samples of instruction addresses for accuracy.
Because of performance counter skid [3], recording the precise instruction address requires tools like Intel PEBS [1] which introduce
additional overhead [38].
Prior approaches have used dynamic phase analysis to predict
future application behavior [13, 23, 24, 39, 42, 54]. Observing that
phases recur, prior studies use a table-based history predictor to
capture past phase patterns [13, 24]. These techniques often tailor
phase definitions and boundaries for a specific optimization. For
example, prior work define phases based on the ratio of memory
bus transactions to micro-ops and statically determines phase boundaries to guide DVFS [24]. More sophisticated dynamic resource
management requires more careful phase definition.
7 CONCLUSION
We propose DynaSprint, a software runtime system that manages
sprints intelligently based on the application’s sprint utility and
the system’s thermal headroom. We also propose cache sprinting,
which dynamically allocates last-level cache capacity. With a modest amount of thermal headroom provided by phase change materials, DynaSprint-guided cache sprinting can improve performance
by 17% on average and up to 40% over a non-sprinting system.
Moreover, the system performs within 95% of a globally optimized
oracular policy.