ABSTRACT
Compression is seen as a simple technique to increase the e!ective
cache capacity. Unfortunately, compression techniques either incur tag area overheads or restrict cache block placement to only
include neighboring addresses. Ideally, we should be able to place
compressed cache blocks without any restrictions or overheads.
This paper proposes Touché, a framework for storing multiple
compressed blocks from arbitrary addresses within a cacheline without tag area overheads. The Touché framework consists of three
components. The "rst component, called the “Signature” (SIGN)
engine, creates shortened signatures from the tag addresses of compressed blocks. Due to this, the SIGN engine can store multiple
signatures in each tag entry. On a cache access, the physical cacheline is accessed only if there is a signature match (which has a
negligible probability of false positive). The second component,
called the “Tag Appended Data” (TADA) mechanism, stores the full
tag addresses with data. TADA enables Touché to detect false positive signature matches by providing the full tag address. The third
component, called the “Superblock Marker” (SMARK) mechanism,
uses a unique marker in the tag entry to indicate compressed cache
blocks from neighboring physical addresses in the same cacheline.
Touché is hardware-based and achieves an average speedup of 12%
(ideal 13%) when compared to an uncompressed baseline.
CCS CONCEPTS
• Hardware → On-chip resource management; Static memory.
KEYWORDS
Compression, Caches, Tag Array, Data Array, Hashing
1 INTRODUCTION
As Moore’s Law slows down, the number of transistors-per-core
for Last-Level caches (LLC) tends to be stagnating [6, 17, 18, 26,
28, 65, 66]. One can employ data compression at the hardwarelevel and increase the e!ective LLC capacity per core [4, 16, 35, 47].
Unfortunately, naively adopting data compression may incur significant tag area overheads [2, 5, 34]. This is because, in conventional
caches each block needs a separate tag. As compression increases
the number of blocks within the LLC, the LLC tag area overheads
will also increase. We can reduce the tag area overheads by storing
compressed blocks only from neighboring addresses [57–60]. This
enables us to use a single overlapping tag for all compressed blocks.
However, such an approach restricts the adoption of data compression to only regions that contain contiguous compressed blocks.
Ideally, we would like to employ data compression within the LLC
without any data placement restrictions and tag area overheads.
Tag overheads are a key roadblock for cache compression. For
instance, if we store 4x more blocks, the e!ective LLC capacity
can be increased by 4x. But we will also incur the area overheads
for maintaining 4x unique tags. Furthermore, it is likely that these
unique tags have no locality, cannot be combined together, and
therefore incur signi"cant area overheads [59, 60]. One can reduce
the tag area overhead with placement restrictions. For instance,
if we set a rule that only compressed blocks from neighboring
memory addresses can reside in a physical cacheline, then we can
overlap their tags. These contiguous compressed blocks are called
“superblock” and their tag is called a “superblock-tag” [57, 58]. For
a 4MB 8-way cache, superblock-tags can track 4 compressed blocks
per cacheline with 1.35x tag area.
Restricting block placement by using superblocks reduces the
bene"ts of compression. Figure 1 shows the e!ective LLC capacity
for four designs executing 29 memory-intensive SPEC workloads in
mixed and rate modes on a 4MB shared LLC [71]. The "rst design
is a baseline LLC without data compression. The second design
employs data compression in LLC while using superblocks. While
such a design has a tag area of 1.35x as compared to the baseline, it
also increases the e!ective LLC capacity only by 20%. This is because
only blocks from neighboring addresses can be compressed and
stored in the cacheline. The third design uses data compression to
place compressed blocks from arbitrary addresses within the same
cacheline. While this design increases the e!ective LLC capacity by
38%, it also requires 3.7x the tag area. The fourth design is an ideal
design which places compressed blocks from arbitrary addresses in
the same cacheline without any area overheads. This paper presents
453
MICRO-52, October 12–16, 2019, Columbus, OH, USA Hong, Abali, Buyuktosunoglu, Healy, and Nair
Figure 1: The e!ective capacity and tag area overheads for
a 4MB last-level cache employing compression. Superblocktags uses 1.35x tag area while providing 20% higher e!ective
capacity. Arbitrary-tags uses 3.7x tag area while providing
38% higher e!ective capacity. The goal of this paper is to obtain 38% higher e!ective capacity with no tag overhead.
Touché, a framework that helps achieve the fourth design to enable
near-ideal LLC compression.
Touché mitigates the tag area overheads by using a shortened
signature of the full tag address for each compressed block. This
has two key bene"ts. First, short signatures require fewer bits as
compared to full tag addresses. Due to this, multiple signatures
from di!erent tags addresses can be placed in the space that was
originally reserved for only a single tag address. Second, by enabling
arbitrary signatures to reside next to each other, we can overcome
restrictions of prior work that require compressed blocks to be
from neighboring addresses. Furthermore, as compression creates
unused space in the data array, tag addresses can be appended to
compressed blocks and stored in this unused space.
The Touché framework consists of three components. The "rst
component, called the “Signature” (SIGN) engine, creates shortened
signatures from the tag addresses and places them in the tag array.
The second component, called the “Tag Appended Data” (TADA)
mechanism, appends full tag addresses to the compressed blocks
and stores them in the data array. The third component, called the “
Superblock Marker” (SMARK) mechanism, uses a unique marker in
the tag-bits to enable Touché to identify superblocks that contain 4
contiguous compressed blocks from neighboring physical addresses.
We describe each mechanism below:
(1) Signature (SIGN) Engine: The Touché framework is implemented within the LLC controller. The core provides the LLC controller with a 48-bit physical address for each request. The LLC
controller uses this physical address to index into the appropriate
set. At the same time, Touché invokes the SIGN engine to create
a shortened 9-bit signature of the tag and looks up all the ways
for a matching signature. On a signature match, the corresponding
compressed block is accessed from the data array. As these signatures are only 9-bits long, several signatures, each belonging to a
di!erent tag address, can co-reside in a tag entry. For instance, a
4MB 8-way LLC with 64 Byte cachelines has tag entries that store
29-bit tag address. In this case, the SIGN engine can store up to three
9-bit signatures in the space that was designed for a single 29-bit
tag address. This enables Touché to store up to three compressed
blocks from arbitrary addresses within a cacheline without any tag
area overheads.
Unfortunately, simply using shortened 9-bit signatures can lead
to false positive tag matches. These cases are called as signature
collisions. Signature collisions cause the LLC controller to incorrectly access blocks that do not have matching tag addresses for
each access. For instance, in a workload that has a 0% cache hit-rate
(worst case scenario), a 9-bit signature has an average signature collision rate of 0.19% (i.e., 1
29 ). Furthermore, as each way in a set can
have up to three 9-bit signatures, Touché potentially needs to check
twenty-four 9-bit signatures in an 8-way LLC (worst case scenario)
which results in a signature-collision rate of 4.58%. Therefore, it is
essential to also check the full tag address on a signature collision.
(2) Tag Appended Data (TADA) Mechanism: The full tag addresses of the compressed blocks can be stored in the data array.
Touché re-provisions a portion of the additional space that is obtained by compression to store the full tag addresses. To this end,
Touché uses the “Tag Appended Data” (TADA) mechanism to append full tags beside the compressed blocks. The TADA mechanism
appends metadata information on compressibility (3-bits), dirty
and valid state (2-bits), and the full tag address (29-bits) to each
compressed block. Overall, TADA increases the block size by only
34 bits (4.25 Bytes) and our experiments show that it only minimally
reduces the e!ective LLC capacity. On an access, TADA interprets
the last few bits in a compressed cacheline as metadata and tag
addresses. As TADA checks the full tags on all signature matches
and collisions, it guarantees the correctness of each LLC access.
(3) Superblock Marker (SMARK) Mechanism: Shortened 9-bit
signatures enable storing up to three compressed blocks. However,
there can be instances of cachelines that contain four compressed
blocks from neighboring addresses (superblock). To address this
scenario, Touché uses a “Superblock Marker” (SMARK) mechanism
to generate a unique 16-bit marker. Touché stores this 16-bit marker
in the tag bits, and uses this marker to indicate that there is a
superblock present within a cacheline.
With a negligible probability (0.012%), the unique 16-bit marker
can $ag a match with the signatures that are stored by the SIGN
engine. We call these scenarios as SMARK collisions. Fortunately,
SMARK collisions cause no correctness problems. This is because
even after a marker collision, the TADA mechanism will read the
data array and check for full tag matches. During a collision, the
tag addresses will not match and the compressed blocks are not
processed by the LLC. The SMARK mechanism enables Touché
to derive all the bene"ts of superblocks while also enabling the
storage of up to three compressed blocks from arbitrary addresses.
Touché provides a speedup of 12% (ideal 13%) without any area
overheads to the LLC. Touché requires comparators and lookup
tables within the LLC controller. Therefore, Touché is a completely
hardware-based framework that enables near-ideal compression.
2 BACKGROUND AND MOTIVATION
We provide a brief background on last-level cache organization and
highlight the potential of data compression.
2.1 Last-Level Caches: Why Size Matters
Processors tend to have several levels of on-chip caches. Caches are
designed to exploit spatial and temporal locality of accesses. Due
to this, caches help improve the performance of processors as they
reduce the number of o!-chip accesses and reduce the latency of
454
Touché: Towards Ideal and E!icient Cache Compression By Mitigating Tag Area Overheads MICRO-52, October 12–16, 2019, Columbus, OH, USA
memory requests. Caches are usually designed such that each level
is progressively larger than its previous level. Consequently, the
Last-Level Cache (LLC) tends to have the largest size, is typically
shared, and occupies signi"cant on-chip real-estate. Due to this, it
is bene"cial to increase the LLC capacity per core as this would
enable the designers to "t a larger number of blocks on-chip and
further reduce the number of o!-chip accesses [70].
2.2 Last-Level Caches: Capacity Stagnation
Figure 2 shows the LLC capacity per core for commercial Intel and
AMD processors from 2009 until 2018. On average, as the number
of cores has increased, the LLC capacity per core has reduced. In
current multi-core systems, the LLC capacity per core tends to be
less than 1MB. Therefore, going into the future, it is bene"cial to
look at techniques to improve the e!ective capacity of the LLC [40]. LLC Size (MB)/Core
0
1
2
3
4
5
5 10 15 20 25 30
Number of (logical) Cores
MEAN
Intel
AMD
Figure 2: The Last-Level Cache (LLC) capacity per (logical)
core for Intel and AMD processors from 2009 to 2018. On average, as the number of cores has increased, the LLC capacity
per core has reduced.
2.3 Last-Level Caches: Organization
A Last-Level Cache (LLC) is organized into data arrays and tag
arrays. Each cacheline in the data array has a corresponding tag
entry in the tag array. Furthermore, groups of cachelines form “sets”
and each cacheline in a set corresponds to a separate “way”. As
the size of the LLC is signi"cantly smaller than the total physical
address space, several blocks can map into the same set. Because
of this, the LLC controller stores a tag address in the tag entry to
uniquely identify the block in the cacheline. For instance, as shown
in Figure 3, a 4MB 8-way LLC with 64-byte lines, uses 29 bits of tag
address. On a cache access, all the tag entries for each of the ways
in a set is searched in parallel by the LLC controller.
LLC Controller
(4MB Cache)
Tag Arrays Data Arrays 8 Ways
Set
To/From 
L2 Cache
To/From 
Main Memory
Tag Entry Cacheline
29-bit Tag 64 Byte
8192 Sets
Figure 3: The organization of a 4MB Last-Level Cache (LLC).
The LLC consists of data arrays, tag arrays, and an LLC controller. The tags are 29-bits long and all tag entries across the
ways in a set are searched in parallel.
2.4 Compression: Higher E!ective Capacity
Several prior works have proposed using e#cient and low-latency
algorithms to compress blocks, thereby storing more blocks and
improving the e!ective LLC capacity. Typically, LLC compression
techniques are implemented within the LLC controller.
2.4.1 E!icient Data Compression Algorithms. The Base Delta Immediate (BDI) and Frequent Pattern Compression (FPC) are two
state-of-the-art low-latency compression algorithms [3, 47]. BDI
uses the insight that data values tend to be similar within a block
and therefore can be compressed by representing them using small
o!sets. FPC uses the insight that blocks contain frequent patterns
like all-zeros, all-ones, etc. FPC represents frequent patterns with
fewer bits. Prior work has shown that both BDI and FPC can be
implemented to execute with a a single-cycle delay and can be
implemented within the LLC controller [3, 47].
LLC Controller
Data Arrays
To/From 
L2 Cache
To/From 
Main Memory
Decompression 
Engine
Tag
Manager
Compression 
Engine
Compressed?
Compressed?
Tag Arrays
Figure 4: The LLC compression-decompression engine. The
compression-decompression engine taps the data bus and
stores compressibility information in the tag entries.
2.4.2 Compression-Decompression Engine. As shown in Figure 4,
the LLC controller implements a compression-decompression engine that taps the bus going into the cache data array. The LLC
controller contains a separate “tag manager” to manage tag entries.
The compression-decompression engine implements both BDI and
FPC and chooses the best algorithm. The tag manager maintains
the compressibility information in the tag entries.
2.4.3 Distribution of Compressed Data Size. Figure 5 shows the
distribution of the size of blocks after compression for 29 SPEC
workloads containing rate and mixed workloads. On average, 55%
of the blocks can be compressed to less than 48 bytes in size. Furthermore, 17% of the lines can be compressed to be less than 16
bytes in size. Therefore, several workloads tend to have blocks with
low entropy and can bene"t from compression.
0
20
40
60
80
100
mcf
lbm
soplexmilc libquantum omnetpp bwavesgcc
sphinx GemsFDTD leslie3dwrf cactusADM zeusmp bzip2
dealII xalancbmk mix1 mix2
mix3 mix4
mix5 mix6
mix7
mix8 mix9
mix10 mix11
mix12
AMEAN
Percentage of Memory Blocks
16 Bytes 32 Bytes 48 Bytes 64 Bytes
Figure 5: The distribution of the size of blocks for 29 SPEC
workloads (rate and mix modes). On average, up to 55% of
the blocks can be compressed to 48 Bytes.
455
MICRO-52, October 12–16, 2019, Columbus, OH, USA Hong, Abali, Buyuktosunoglu, Healy, and Nair
2.5 LLC Compression: Tag Area Overheads
Modern computing systems tend to operate on 64-byte blocks. Figure 6 (a) shows the design of the tag entry and the cacheline in the
data array for a 4MB 8-way LLC that does not employ compression.
The tag entry for each block requires a valid bit and a dirty bit.
Furthermore, we assume that replacement policy is maintained at
the cacheline-level and the largest block in the selected cacheline
is evicted. To reduce the number of encoding bits in the tag array,
blocks are compressed into 16 byte, 32 byte, or 64 byte boundaries.
To reduce the number of bits in the tag entry further, one can restrict cachelines to store blocks only from contiguous addresses.
Such a contiguous set of blocks is called a superblock. Prior work
has shown that superblocks with 4 compressed blocks can reduce
the tag area overheads to 8 bits [58]. As shown in Figure 6 (b), a
4MB 8-way LLC that stores up to four blocks per cacheline will
require 46 bits of tag entry. While superblocks help reduce tag area
overheads, they limit the potential bene"ts of LLC compression as
they restrict block placement to include only neighboring addresses.
If one can store blocks from arbitrary addresses, we can unlock all
the bene"ts of LLC compression. However, the disadvantage of this
approach is that, as shown in Figure 6 (c), a 4MB 8-way LLC that
stores up to four blocks per cacheline will require 127 bits of tag
entry (3.7x higher than the baseline).
A Tag Entry in Tag Array A Cacheline in Data Array
Valid-Bit
Dirty-Bit
Tag Address Bits
Replacement Policy 
Bits (3-bits)
29-bits 64 Bytes
34-bits
(a) Baseline (No Compression)
Memory 
Block 1
35-bits
46-bits
(b) Compression with Superblock Tag Addresses 
Memory 
Block 2
Memory 
Block 4
Memory 
Block 3
116-bits
127-bits
(c) Compression with Arbitrary Tag Addresses 
Block
1
1 23 4
Compressed Blocks
Compressed Blocks
1 43 233 129
Figure 6: The tag area overheads for di!erent techniques. (a)
The baseline technique that does not employ any compression has no tag area overheads. (b) The superblock technique
increases the tag area to 1.35x. (c) Storing arbitrary tags increases the tag area to 3.7x.
2.6 LLC Compression: Potential
Figure 7 shows the overheads and bene"ts of LLC compression for
three techniques. The baseline technique does not employ compression, has no tag overheads and has an average hit-rate of 31.5%. The
second technique employs superblocks for compression, has a tag
area of 1.35x and increases the average hit-rate to 36%. The third
technique highlights the potential hit-rate with compression when
each cacheline can store up to 4 compressed blocks. Unfortunately,
the third technique uses a tag area of 3.7x while also increasing the
average LLC hit-rate to 38.5%.
 20
 25
 30
 35
 40
 45
 50
 55
 60
RATE MIX AVERAGE
Hit Rate (%)
Baseline
Superblock (1.35x Tag Area)
Ideal (3.7x Tag Area in Practice)
Figure 7: The potential of LLC compression. Enabling blocks
from arbitrary addresses increases the average hit-rate of
the LLC from 31.5% to 38.5%.
3 THE TOUCHÉ FRAMEWORK
3.1 An Overview
Figure 8 shows an overview of the Touché framework. Touché consists of three components. The "rst component, called the Signature
(SIGN) Engine, generates shortened signatures of the tag addresses.
The SIGN engine is designed within the tag manager. The second
component, called the Tag Appended Data (TADA) mechanism, attaches full tag addresses to compressed memory blocks. The TADA
mechanism taps the data bus after the compression-decompression
engine and obtains the full tag address from the tag manager. The
third component, called Superblock Marker (SMARK) mechanism,
keeps track of superblocks by using a unique 16-bit marker in
the tag entry. The SMARK mechanism is implemented in the tag
manager. Touché requires changes only in the LLC controller.
LLC Controller
Data Arrays
To/From 
L2 Cache
To/From 
Main Memory
Decompression 
Engine
Tag
Manager Compression 
Engine
Compressed?
Compressed?
Tag Arrays
SIGN
Engine
SMARK
TADA Full Tag Address
Figure 8: An overview of Touché. Touché consists of
three components. The Signature (SIGN) Engine, the Tag
Appended Data (TADA) mechanism, and the Superblock
Marker (SMARK) mechanism. All components are implemented in the LLC controller with no changes to the LLC.
3.2 Signature (SIGN) Engine
The Signature (SIGN) Engine is implemented in the tag manager.
The SIGN Engine generates shortened signatures from the full tag
addresses supplied during the read and write accesses to the LLC.
3.2.1 Identifying Compressed blocks. On a LLC write, the compressiondecompression engine informs the tag manager if the block is compressible; a block can be compressed to 16B, 32B or 48B. The tag
456
Touché: Towards Ideal and E!icient Cache Compression By Mitigating Tag Area Overheads MICRO-52, October 12–16, 2019, Columbus, OH, USA
manager uses the original valid bit and the dirty bit in its tag entry to encode this information. We use the insight that, for valid
blocks, the valid bit and the dirty bit can only exist in two states.
For instance, a cacheline cannot be marked as both invalid and
dirty at the same time. The tag manager uses this unused state to
$ag cachelines that contain compressed blocks. Thereafter, for a
cacheline that stores compressed blocks, the 1st and 2nd bits of the
tag address are used to indicate if any of these blocks are dirty.
As shown in Table 1, on a read, the tag manager checks the original valid and dirty bits in the tag entry to identify if the cacheline
contains valid compressed blocks. If the cacheline is deemed to
contain valid compressed blocks, then the tag manager reads the
1st and 2nd bits from the tag address to determine if any of the
blocks are dirty.
Table 1: Identifying Compressed blocks
Cacheline Status Valid Dirty Tag Address Tag Address
Bit Bit 1st Bit 2nd Bit
Invalid 0 0 N/A N/A
Uncompressed: Valid and Clean 1 0 N/A N/A
Uncompressed: Valid and Dirty 1 1 N/A N/A
Compressed: Valid and Clean 0 1 1 0
Compressed: Valid and Dirty 0 1 1 1
3.2.2 Using Shortened Signatures. To store multiple signatures
within a single tag entry, the SIGN engine shortens the full tag
address into a 9-bit signature. For a 4MB 8-way LLC, the full tag
address is 29-bit long. For a cacheline containing compressed blocks,
as the top 2 bits of the tag address space in its tag entry are already
used to indicate if any of blocks are dirty, we have 27 unused bits
remaining in the tag address space. Therefore, we can store up to
three 9-bit signatures corresponding to three compressed blocks.
Figure 9 shows the design of the signature generator in the SIGN
engine. The signature generator uses the least 27-bits of the full
tag address and divides it into three 9-bit segments. Each bit of
these 9-bit segments is then XORed together to generate a 9-bit
output. The 9-bit output then indexes into a 512 entry hash table.
Each entry in the hash table is populated at boot-time with unique
hashes. The hash table then generates a unique 9-bit signature for
each 9-bit input. The overall latency of generating signatures is the
delay of one 3-bit XOR gate and one parallel hash table lookup. For
a high-performance processor executing at 3.2GHz, we estimate
the signature generation to incur only 1 cycle. Furthermore, the
latency of signature generation is masked by the latency of reading
the tag entries for each LLC access (up to 5 cycles). Least 27 Bits of the Full Tag Address 9-Bits 9-Bits 9-Bits
+
XOR
9-Bits
9-Bit
Signature
512 Entry
Hash Table
Figure 9: The signature generator within the SIGN Engine.
The signature generator only requires one XOR operation
and a hash table lookup for each LLC access.
3.2.3 Checking for Matching Signatures. On an LLC read, the tag
manager reads the tag entries from all the ways of the indexed set.
At the same time, the SIGN engine forwards its 9-bit signature to the
tag manager. The tag manager identi"es if the cacheline contains
compressed blocks using the original valid and dirty bits. For an
uncompressed cacheline, the tag manager ignores the signature
and uses the full tag address to check for a match.
If the cacheline contains valid compressed blocks, the tag manager ignores the "rst two bits of the tag address. Thereafter, the
remaining 27 bits in the tag address space of the tag entry are partitioned into three 9-bit entries. The tag manager then compares
each of these three 9-bit entries with the 9-bit signature from the
SIGN Engine. If the 9-bit entry does not match the 9-bit signature,
then the block is guaranteed to be absent. On the other hand, if
the 9-bit signature matches in any one of the ways, then the block
is likely to be present. As a 9-bit signature is smaller than its full
29-bit tag address, there is a small chance of 0.19% ( 1
512 ) that each
9-bit entry comparison with the 9-bit signature can result in a false
positive match. We call these false positive matches of signatures
as “signature collisions”.
3.2.4 Collision Rate of Signatures. As each tag entry can store up
to three 9-bit signatures, an 8-way LLC would require up to twentyfour 9-bit signature comparisons for each access. As signatures are
shorter than full tags, several tags may map into the same signature.
As we perform twenty-four signature checks (in the worst-case), it
is likely that some of LLC accesses will result in signature collisions.
Figure 10 shows the probability of collisions as the number of
signatures present in the 8-ways varies from zero (all ways are
uncompressed) to twenty-four (all ways have three compressed
blocks) for di!erent LLC hit-rates.
0%
1%
2%
3%
4%
5%
0 4 8 12 16 20 24
Probability of Signature Collision 
Per Access
Number of 9-Bit Signature Entries in a Set
0% Hit Rate
25% Hit Rate
50% Hit Rate
75% Hit Rate
100% Hit Rate
Worst-Case = 4.58% 
Figure 10: The probability of collision for a 9-bit signature as
the number of signature entries vary in a set. In the worstcase, for an 8-way LLC, we expect a signature collision to
occur 4.58% of the time for each access.
In the worst case, for a workload that has 0% hit-rate, we can
expect a collision to occur 4.58% of the time. As signature collisions
can cause the LLC to forward blocks with incorrect tag addresses
to the cores, it is essential to check full tags.
3.3 Tag Appended Data (TADA) Mechanism
The Tag Appended Data (TADA) mechanism is implemented in
the LLC controller and taps the data-bus between the compressiondecompression engine and the data array.
457
MICRO-52, October 12–16, 2019, Columbus, OH, USA Hong, Abali, Buyuktosunoglu, Healy, and Nair
3.3.1 Appending Full Tag Addresses to Data. During an LLC write,
the TADA mechanism uses the full tags that are supplied by the
tag manager. The TADA mechanism then appends the full tag addresses (29 bits), the valid-bit, the dirty-bit, and the compressibility
information (3 bits) to the end of the cacheline (total 34 bits or
4.25 Bytes). Figure 11 shows a cacheline storing three compressed
blocks and the TADA mechanism appending the information for
each of these blocks at the end of the cacheline.
A Tag Entry in Tag Array A Cacheline in Data Array
34-bits
Valid-Bit
Dirty-Bit 
9-Bit Signatures
Replacement Policy 
Bits (3-bits)
Block 1 Block 200 Block 423
Full Tag and
Valid Bit and
Dirty Bit and
Compressibility
Full Tag and
Valid Bit and
Dirty Bit and
Compressibility
Full Tag and
Valid Bit and
Dirty Bit and
Compressibility
4.25 Bytes
Figure 11: A cacheline storing compressed blocks with
TADA mechanism. The TADA mechanism appends full tag
addresses, valid bit, dirty bit, and compressibility information for each block at the end of the cacheline.
3.3.2 Appending Full Tag Addresses to Data. The TADA mechanism
appends 34 bits (4.25 Bytes) of information to the end of the cacheline containing compressed blocks. As a result, TADA reduces the
space available to store the compressed block. We can store three
16B compressed blocks or a pair of a 32B and a 16B compressed
blocks in the data array; the block size is stored in the compressibility information "eld. Fortunately, this additional loss of space only
causes a few lines to reduce their e!ective capacity. Figure 12 shows
the reduction in e!ective LLC capacity due to TADA for an LLC
that can store up to 3 arbitrary compressed blocks. TADA decreases
the e!ective cache capacity by only 4.15% points as compared to an
ideal scheme that can store three compressed blocks from arbitrary
addresses without any storage overheads.
0.9
1
1.1
1.2
1.3
1.4
RATE MIX AVERAGE
Baseline
TADA based Block Storage (3 Compressed Blocks)
Ideal (3 Compressed Blocks)
4.15 % 
Points
Effective Capacity of LLC
Figure 12: The reduction in the e!ective LLC capacity by
the TADA storage overhead. While TADA uses 4.25 Bytes
per compressed block, it decreases the e!ective LLC capacity only by 4.15% points as compared to an ideal scheme that
does not require the metadata storage in the data array.
3.3.3 Detecting Collisions of Signatures. TADA helps detect signature collisions. This is because, on a signature collision, the cachelines from the selected way(s) in the data array are read by the tag
manager. The TADA mechanism extracts the full tag address from
the cachelines and checks if they match the full tag address of the
LLC access. If there is no match, TADA $ags a signature collision.
Therefore, TADA guarantees the detection of signature collisions
and thereby ensures correctness. Furthermore, TADA extracts the
the compressibility information and supplies to the decompression
engine. The valid and dirty bits of compressed blocks are also stored
using TADA. Therefore, TADA helps to avoid using any additional
bits in the tag entry to store additional information.
3.4 Latency Overheads
As the data array needs to be accessed during a signature collision,
it can increase the LLC access latency.
3.4.1 Additional Accesses to Data Arrays. In the baseline system,
an LLC access probes all the ways in the indexed set from the tag
array. The cacheline from the data array is read-only in case of
an LLC hit. As a tag access occurs for every access, irrespective
of whether the access is a hit or a miss, the tag array is designed
with lower access latency as compared to the data array. Typically,
accessing the LLC tag array incurs a latency overhead of only 5
cycles. On the other hand, reading the LLC data array incurs an
overhead of 30 cycles in modern processors [25].
In the Touché framework, the data arrays are likely to be accessed
even in case of an LLC miss. This is because the SIGN engine may
incur signature collisions and may invoke the TADA mechanism
to access the data array to detect signature collisions. In the worstcase, for a workload with 0% hit-rate, this scenario may occur only
4.58% of the times. Therefore, signature collisions will increase the
overall latency of LLC access. Table 2 shows the average latency of
an LLC access during a collision.
Table 2: Additional Data Arrays Accessed on a Collision
Number of Data Arrays Accessed Probability Latency (cycles)
1 0.9768 35
2 0.0229 70
3+ 0.0003 105+
Average: 1.0235 1 35.82
In the worst-case, all accesses can be a cache miss and a collision
can occur 4.58% (probability of 0.0458) of the times. As shown in
Table 2, collisions increase the access latency to 35.82 cycles. For a
worst-case workload with a 0% hit-rate, the increase in the LLC tag
access latency is denoted by Equation 1.
New Tag Access Latency = (1 
3.4.2 Mitigating Latency Overheads with Dynamic Touché: One can
mitigate signature-collision latency overheads by compressing only
when it is useful. To this end, Touché continuously monitors the
average memory latency at the LLC controller. The average memory
latency is de"ned as the total latency that is experienced by each
request and this can emanate from the LLC and main memory.
458
Touché: Towards Ideal and E!icient Cache Compression By Mitigating Tag Area Overheads MICRO-52, October 12–16, 2019, Columbus, OH, USA
Touché enables compression only when the average memory
access latency is 100x greater than the latency overheads of signature collisions. As signature collisions increase the LLC tag access
latency by 1.4 cycles, Touché enables compression only when the
average memory latency is greater than 140 cycles. This has two
key advantages. First, Touché is only enabled for workloads that
showcase a large memory latency and bene"t from LLC compression. Second, the latency overheads from Touché will be capped
at 1%. As shown in Figure 13, for memory intensive benchmarks,
the average memory latency for reads is 541 cycles (signi"cantly
higher than 140 cycles). Therefore, the observed latency overhead
of Touché is only 0.26%.
200
400
600
800
1000
1200
1400
mcf
lbm
soplexmilc libquantum omnetpp bwavesgcc
sphinx GemsFDTD leslie3dwrf cactusADM zeusmp bzip2
dealII xalancbmk mix1 mix2 mix3 mix4 mix5
mix6 mix7 mix8
mix9
mix10 mix11
mix12
Amean
Average Memory Read Latency
140 cycles
Figure 13: The average memory latency for reads. On average, the average memory access latency is 541 cycles. Therefore, Touché has a latency overhead of only 0.26%.
3.5 Superblock Marker (SMARK) Mechanism
The SIGN Engine enables storage of up to three blocks. However,
some cachelines may contain superblocks.
3.5.1 Benefits of Including Superblocks. Figure 14 shows the hitrate of Touché while maintaining up to three compressed blocks and
compares it against a scheme that also stores superblocks (up to four
neighboring blocks). For a superblock, Touché tries to compress
each block to 15 Bytes. This enables Touché to get the bene"ts of
storing both the superblock-tags and the arbitrary tags. If we can
store superblocks and arbitrary blocks at the same time, we can
increase the average hit-rate of Touché from 31.5% to 37.5%.
20
25
30
35
40
45
50
RATE MIX AVERAGE
Hit Rate (%)
Touché: 3 Arbitrary Blocks
Touché: 3 Arbitrary Blocks + Superblocks
Figure 14: The average hit-rate of Touché with 3 blocks versus 3 blocks with superblocks. On average, the hit-rate increases to 37.5% by combining superblocks.
3.5.2 Identifying Potential Cachelines. During an LLC install, if
the block is compressible and if the candidate cacheline already
contains compressed blocks from its neighboring addresses, then
this cacheline is also a superblock candidate. The TADA mechanism
is used to identify superblock candidates by extracting the full tag
addresses for all the blocks in a cacheline during an LLC install.
3.5.3 Generating Markers. Touché implements a “Superblock Marker”
(SMARK) mechanism in the tag manager. SMARK mechanism generates a random 16-bit marker at boot-time and uses this marker
throughout the operational time of the system.
Once the TADA mechanism identi"es a superblock cacheline, it
informs the tag manager. The tag manager then retrieves the 16-bit
marker from the SMARK mechanism. It then informs the SIGN
engine to ignore the last 2-bits (corresponding to four neighboring
addresses) of the full tag address to generate a unique 9-bit signature.
This ensures that neighboring addresses in the superblock generate
the same 9-bit signature. Thereafter, the tag manager appends the
9-bit signature to the 16-bit marker and writes the 27-bit tag of the
"rst block within the superblock at the end of the cacheline. The
TADA mechanism also appends a dirty-bit per each compressed
block to the data array. For a valid superblock, all compressed blocks
within a superblock are always valid. Therefore, TADA does not
need to append additional per-block valid bits. Figure 15 shows the
superblock marker mechanism.
A Tag Entry in Tag Array A Cacheline in Data Array
34-bits Superblock
12 13 14 15
Superblock
Tag + Metadata
SMARK
SIGN
Tag Manager
16-bits 9-bits
Figure 15: The Superblock Marker (SMARK) mechanism.
3.5.4 Checking for Matching Markers. On a read, the tag manager
will check for matching 16-bit marker values in all the ways that
store compressed blocks within a set. If there is a marker match,
then the tag manager uses the 9-bit signature (generated from by
ignoring the least two signi"cant bits) and checks for a match.
If the signature matches, then the cacheline is read from the
data array. The TADA mechanism extracts the 27-bit tag address
and checks if the LLC access is for one of the blocks within the
superblock. If there is a match, the block is processed by the LLC
controller. It is likely, the cacheline may not contain the requested
block and it may simply be a false positive match. Similar to signature collisions, we call this scenario as a marker collision.
3.5.5 E!ect of Marker Collisions. Marker collisions are extremely
rare. This is because, we use markers which are 16 bits long. For
instance, in an 8-way cache, the probability of a marker collision
for each access is only 0.012% and their impact on LLC latency
is negligible. Furthermore, even in the case of marker collisions,
the TADA mechanism ensures that the full tag address is checked
before forwarding the compressed block. Therefore, SMARK works
with TADA to guarantee correctness while storing superblocks.
3.6 Touché Operation: Reads and Writes
Figure 16 (a) and Figure 16 (b) show the $owchart for Touché
for LLC access and install requests respectively. Touché invokes
the SIGN, TADA, and SMARK mechanisms only for compressed
data. For incompressible data, Touché works just like the baseline.
Furthermore, TADA mechanism is always invoked for LLC hits of
compressed blocks. This enables Touché to guarantee correctness.
459
MICRO-52, October 12–16, 2019, Columbus, OH, USA Hong, Abali, Buyuktosunoglu, Healy, and Nair
Process Request
Invoke SMARK (16 bit Marker)
Invoke SIGN (9 bit Signature)
Invoke SIGN (9 bit Signature)
 and
Write Memory Block with 
the TADA mechanism
Check for Signature/Marker Collisions
No
Invoke SIGN Read/Write
like Baseline
Compressible?
(b)
LLC Miss
Yes
Compressible?
Candidate?
Superblock
No
Yes
No
Full Tag in Tag Entry
Write (like Baseline) 
and
Write Memory Block with 
the TADA mechanism
Write 9 bit Signature
Install
End
(a)
No
Access
Read the Tag Entries
Yes
Invoke SIGN (Superblock)
Signature
Matches?
SMARK Matches?
Superblock?
No
Yes
Yes
Invoke TADA and Read/Write Data
Write Superblock Tag
Figure 16: The "owchart detailing the high-level operations of Touché for install and access requests. (a) Shows the "owchart
for install requests. (b) Shows the "owchart for access requests.
3.7 Discussions
In the baseline LLC, the tag entry contains metadata such as the
replacement policy bits and coherence states (for private LLCs). We
discuss how these a!ect the design of Touché.
3.7.1 Handling Cache Coherence: Touché assumes a shared LLC
and therefore does not encounter coherence concerns. However, in
case the LLC is private and a snooping-based coherence protocol
is employed, Touché will need to maintain coherence states with
minimal overheads. Touché will store the coherence state as well
as the full tag for each compressed block in the data array. Thus,
the LLC controller needs to access the data array for tag matching
and checking the coherence states. This operation will increase the
latency of tag matching for the coherence requests.
However, such an operation can be designed to incur low performance overheads. This is because, the additional access to the data
array is required only if there is a signature match. Also, the coherence state can be updated after the critical requests are serviced.
Additionally, if the coherence request is a “BusRd” (read request
made by another core), the servicing core needs to send the entire
block to the requesting core anyway. In this case, the additional data
array access does not cause any performance overheads. Furthermore, as implemented in commercial processors, we can eliminate
the performance impact of snooping-based coherence protocols by
simply using a directory-based coherence protocol [27].
3.7.2 Handling Cache Replacement Policy: Each tag entry in the
baseline system is already equipped with replacement information
bits. As Touché stores multiple compressed blocks per cacheline,
ideally, it would be preferable to equip each of these blocks with
additional replacement bits in the tag entry. However, this would
require us to add 3 ⇠ 4 bits per compressed block in the tag entry.
To minimize the overheads for storing the replacement information, whenever a cacheline is accessed, Touché only updates the
original replacement bits. Touché does not keep track of individual replacement bits for each block. During replacement, Touché
selects the victim cacheline based on the original replacement bits
and randomly evicts one block from within the victim cacheline.
4 EXPERIMENTAL METHODOLOGY
To evaluate the performance bene"ts of Touché, we develop a tracebased cache simulator based on the USIMM [9] (which is a detailed
main memory system simulator). We extended the USIMM to model
processor cores and a detailed cache hierarchy. Our processor model
implements an out-of-order (OoO) execution of the benchmark
trace. Our detailed cache model supports various replacement policies such as LRU, DRRIP [29], and DIP [52]. The baseline system
con"guration is described in Table 3. To enable e#cient compression, the compression engine modeled in the cache model employs
the BDI [47, 48] and FPC [3] compression algorithms and uses
the algorithm that provides the best compression ratio for each
cacheline. Based on prior work in BDI and FPC, we assume that
compression and decompression of data incurs only a single-cycle
latency. We compare Touché to a previous state-of-the-art scheme
called YACC that uses only “superblocks” [58]. We also compare
our scheme against an “Ideal” scheme that can store either three
arbitrary blocks or a superblock (four neighboring blocks) without
any area overheads. The Ideal scheme uses the same replacement
policy as Touché (described in Section 3.7.2).
Table 3: Baseline System Con#guration
Number of cores (OoO) 4
Processor clock speed 3.2 GHz
Issue width 8
L1 Cache (Private) 32KB, 8-Way, 64B lines, 4 cycles
L2 Cache (Private) 256KB, 8-Way, 64B lines, 12 cycles
Last Level Cache (Shared) 4MB, 8-Way, 64B lines
LLC Tag Access Latency 5 cycles
LLC Data Access latency 30 cycles
Memory bus frequency 1600MHz (DDR 3200MHz) [30]
Memory channels 2
Ranks per channel 1
Banks Groups 4
Banks per Bank Group 4
Rows per bank 64K
Columns (cache lines) per row 128
DRAM Access Timings: TRCD -TRP -TCAS 22-22-22 [8]
DRAM Refresh Timings: TRFC 420ns [39, 53]
460
Touché: Towards Ideal and E!icient Cache Compression By Mitigating Tag Area Overheads MICRO-52, October 12–16, 2019, Columbus, OH, USA
0.9
1
1.1
1.2
1.3
1.4
1.5
mcf
lbm
soplexmilc libquantum omnetpp bwavesgcc
sphinx GemsFDTD leslie3dwrf cactusADM zeusmp bzip2
dealII xalancbmkmix1
mix2
mix3
mix4
mix5mix6
mix7
mix8
mix9
mix10
mix11
mix12
Gmean
Speedup
Baseline
YACC
Touché
Ideal
1.88 1.95
1.91
Figure 17: Speedup of Touché as compared to a baseline system that does not employ compression. On average, Touché achieves
a speedup of 12% (Ideal – 13%, YACC – 10.3%) by enabling compressed blocks from arbitrary addresses to be placed next to each
other while also allowing superblocks to be stored.
0
10
20
30
40
50
60
70
80
90
100
mcf
lbm
soplexmilc libquantum omnetpp bwavesgcc
sphinx GemsFDTD leslie3dwrf cactusADM zeusmp bzip2
dealII xalancbmkmix1mix2
mix3
mix4
mix5mix6mix7mix8
mix9
mix10
mix11 mix12
Amean
Hit Rate(%)
Baseline
YACC
Touché
Ideal
Figure 18: The Hit-Rate of Touché as compared to a baseline system that does not employ compression. On average, Touché
increases the hit-rate by 6% points (Ideal – 7% points, YACC – 4% points) by storing a larger number of blocks within the LLC.
We chose memory intensive benchmarks, which have greater
than 1 MPKI (LLC Misses Per 1000 Instructions), from the SPEC
CPU2006 benchmarks. We warm up the caches for 2 Billion instructions and execute 4 Billion instructions. To ensure adequate representation of regions of compressibility [12] and performance [51],
the 4 Billion instructions are collected by sampling 400 Million
instructions per 1 Billion instructions over a 40 Billion instruction
window. We execute all benchmarks in rate mode. We also create
twelve 4-threaded mixed workloads by forming two categories of
SPEC2006 Benchmarks; a low MPKI category and a high MPKI
category. As described in Table 4, we randomly pick one benchmark from each category to form high MPKI mixed workloads and
medium MPKI mixed workloads. We perform timing simulation
until all the benchmarks in the workload "nish execution.
Table 4: Workload Mixes
mix1 mcf, libquantum, GemsFDTD, wrf
mix2 lbm, gcc, bzip2, bwaves
mix3 milc, sphinx, leslie3d, zeusmp
mix4 soplex, omnetpp, cactusADM, dealII
mix5 xalancbmk, mcf, gcc, sphinx
mix6 omnetpp, lbm, milc, xalancbmk
mix7 astar, mcf, milc, calculix
mix8 omnetpp, gobmk, sjeng, libquantum
mix9 namd, gcc, lbm, dealII
mix10 soplex, tonto, hmmer, perlbench
mix11 GemsFDTD, bwaves, povray, zeusmp
mix12 wrf, xalancbmk, h264, gamess
5 RESULTS
This section discusses the performance, hit-rate, and sensitivity
results of Touché.
5.1 Performance Impact
Figure 17 shows the speedup of Touché when compared to a baseline
system that does not employ compression. On average, Touché has
a speedup of 12%. Ideally, if we can place compressed memory
blocks without any area overheads in tag and data arrays, we get a
speedup of 13%. On the other hand, YACC achieves 10.3% speedup
by capturing the superblocks. Our analysis shows that gcc bene"ts
the most from LLC compression. gcc is extremely sensitive to the
LLC capacity and as the miss rate of gcc drops from 45% to 5% (by
9x) due to Touché, gcc experiences a low memory access latency.
This is because, at 5% miss-rate, almost all of its working set now "ts
in the LLC. Therefore, gcc shows a speedup of 91% due to Touché.
For all other workloads, the drop in miss-rate is at most 2.4x (see
Figure 18) and they show up to speedup of up to 50%.
5.2 E!ect on Last-Level Cache Hit-Rate
Figure 18 shows the speedup of Touché when compared to a baseline
system that does not employ compression. On average, Touché
increases the hit rate by 6% points. In the ideal case, if we can place
compressed memory blocks from arbitrary addresses without any
area overheads in tag and data arrays, the hit-rate increases by 7%
points. On the other hand, YACC increases the hit-rate by 4% points.
Furthermore, some workloads like gcc, mix2, mix5, and mix9 obtain
signi"cant increase in hit rate.
461
MICRO-52, October 12–16, 2019, Columbus, OH, USA Hong, Abali, Buyuktosunoglu, Healy, and Nair
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
mcf
lbm
soplexmilc libquantum omnetpp bwavesgcc
sphinx GemsFDTD leslie3dwrf cactusADM zeusmp bzip2
dealII xalancbmkmix1mix2mix3mix4mix5
mix6mix7mix8mix9
mix10 mix11 mix12
Gmean
Speedup
Baseline
Touché with LRU
Touché with DIP
Touché with DRRIP
1.91 1.98 2.04
Figure 19: The sensitivity of Touché to di!erent replacement policies. As Touché is only a LLC compression framework, it
is orthogonal to the replacement policy. Touché shows an increasing speedup of 12%, 14.5%, and 16.7% for the LRU, DIP, and
DRRIP replacement policies respectively.
We also observe that the hit-rates of Touché either increase or
remain the same for benchmarks. Furthermore, Touché closely follows the hit-rate of an ideal LLC compression technique. The slight
loss in hit-rate from the ideal LLC compression is because of the
loss in capacity within the data array from the TADA mechanism.
5.3 Sensitivity to Replacement Policy
As Touché is an LLC compression technique, it is orthogonal to
the replacement policy. Typically, the LLC controller will choose
a victim cacheline based on a replacement policy. Touché then
evicts a random block from within the victim cacheline. Therefore,
di!erent replacement policies can be combined alongside Touché.
Figure 19 shows the speedup of Touché for three di!erent cache
replacement policies. On average, Touché increases the speedup
from 12% while using LRU, to 14.5% while using DIP. The speedup
is further increased to 16.7% while using the DRRIP replacement
policy. Therefore, irrespective of the replacement policy, Touché
continues to provide high performance with e#cient compression.
5.4 Impact on Memory Latency
Figure 20 shows the impact of Touché on the average memory
latency for reads. As Touché provides a higher LLC hit rate, it also
reduces the average memory read latency. On average, Touché
reduces the memory read latency from 541 cycles to 489 cycles. In
the ideal case, we can reduce the average memory read latency to
478 cycles as this scheme provides slightly higher hit-rate .
400
450
500
550
600
RATE MIX AVERAGE
Baseline
Touché
Ideal
Average Memory Latency
(Reads)
Figure 20: The average memory read latency for Touché. On
average, Touché reduces the memory read latency from 541
cycles to 489 cycles.
5.5 Sensitivity to Last-Level Cache Size
Figure 21 shows the impact of LLC size on the e!ectiveness of
Touché. Touché continues to be e!ective at di!erent LLC sizes.
For instance, even while using a 2MB cache, Touché provides an
average speedup of 10%. Even after doubling the LLC size from
4MB to 8MB, Touché still provides an average speedup of 9%.
 0.95
 1
 1.05
 1.1
 1.15
 1.2
2MB 4MB 8MB
Speedup
Baseline
Touché
Figure 21: The sensitivity of Touché to the size of the LLC.
Even after varying the LLC size, Touché continues to provide
an average speedup of atleast 9%.
5.6 Impact on Low-MPKI Benchmarks
Until now, we have presented the results for high MPKI benchmarks.
However, for implementation purposes, it is vital that Touché does
not hurt the performance of low MPKI benchmarks. Figure 22 shows
the impact of Touché on the performance of Low MPKI workloads
from the SPEC2006 suite. Overall, Touché does not cause slowdown
for any Low MPKI benchmark. On the contrary, Touché provides
an average speedup of 1.9% for these workloads.
0.9
1.0
1.1
1.2
hmmer
perlbench
h264
astar
gromacs
gobmk
sjeng
namd
tonto
calculix
gamess
povray
Gmean
Speedup
Baseline
Touché
Ideal
Figure 22: Impact of Touché on low MPKI workloads.
Touché does not hurt the performance of any low MPKI
benchmark. Touché provides an average speedup of 1.9% for
these workloads.
462
Touché: Towards Ideal and E!icient Cache Compression By Mitigating Tag Area Overheads MICRO-52, October 12–16, 2019, Columbus, OH, USA
Table 5: Comparison: E$cient Tag Management Techniques
Techniques Last-Level Cache DRAM Cache Variable Touché
[57–60] Tags in Cache [38] High-Bandwidth [74] Granularity Cache [37]
Compression 3 7 3 7 3
Stores Superblocks 3 7 3 7 3
Stores Arbitary Blocks 7 3 7 3 3
Maintains Entire Memory Block 3 3 3 7 3
Tag Overhead 35% 0% 0% 0% 0%
6 RELATED WORK
In this section, we describe prior work that is closely related to the
ideas discussed in this paper.
6.1 E$cient Compression Algorithms
Cache compression algorithms like Frequent Pattern Compression
(FPC) [3], Base-Delta-Immediate (BDI) [47], and Cache Packer (CPACK) [10] have low decompression latency and require low implementation cost (i.e., area overhead). The C-PACK algorithm can
be improved further by detecting zero cache lines [57]. Recently,
Kim et al. [33] introduce a bit-plane compression algorithm that
uses a bit-plane transformation to achieve a high compression ratio. Touché is orthogonal to all of these compression algorithms.
Touché can select any of these algorithms to meet the hardware
budget, latency constraints, and application’s requirements.
6.2 Cache Compression with Tag Management
Prior works have proposed compressed cache architectures to
improve the e!ective cache capacity [2, 57–60]. For instance, a
variable-size compressed cache architecture using FPC was proposed [2]. This architecture doubles the cache size when all cachelines are compressed while requiring twice as many tag entries.
Table 5 lists techniques that propose e#cient tag management.
To reduce tag overhead of the compressed cache, DCC [60] and
SCC [57] use superblocks to track multiple neighbor blocks with a
single tag entry. Recently, YACC [58] was proposed to reduce the
complexity of SCC by exploiting the compression and spatial locality. YACC still restricts the mapping of compressed cachelines as it
requires superblocks that contain cachelines only from neighboring
addresses. Furthermore, YACC requires that those cachelines be
of the same compressed size. Touché eliminates this fundamental
limitation of the super block-based compressed cache. On average, YACC provides 10.3% speedup while requiring additional bits
in the tag area resulting in 1.35x tag area. Touché provides 12%
speedup without any area overheads. To increase LLC e#ciency,
Amoeba-Cache [37], proposes storing tag and data together while
eliminating the tag area. However, to create space for tags, AmoebaCache stores only parts of the memory block within the cache. As
DRAM caches do not encounter tag storage problems and tend to be
bandwidth sensitive, Young et. al. [74] use compression in DRAM
caches to improve both capacity and bandwidth dynamically.
6.3 Compression using Deduplication
Data deduplication exploits the observation that several memory
blocks in the LLC contain the same identical value [15, 23, 67]. To
improve e#ciency, these techniques store only a single value of
these memory blocks within the LLC and design techniques to
maintain tags that point to such memory blocks.
Exploit the presence of identical memory blocks in the LLC,
Dedup [67] changes the LLC to enable several tags to point to the
same data. To this end, the tag array is decoupled from the data
array. Each tag entry is then equipped with pointers to enable them
to point to arbitrary memory blocks in the data array. Touché is
orthogonal to Dedup, as Touché is compression technique that
compresses arbitrary memory blocks independently and enables
Dedup to be applied over it.
6.4 Main Memory Compression Techniques
Compression can also be used for main memory. Memzip compresses data for improving the bandwidth of the main memory [63].
Pekhimenko et. al. [48] and Abali et. al [1] have proposed e#cient
techniques to improve the e!ective capacity of main memory using
compression. Compression can also be used in Non-Volatile Memories (NVM) to reduce energy and improve performance [46]. As
compression increases the number of bit-toggles on the bus, Pekhimenko et. al. [50] minimizes bit-toggles and reduces the bus energy
consumption. Recently, Compresso [13] memory system was proposed to reduce the additional data movement caused by metadata
accesses for additional translation, changes in compressibility of the
cacheline, and compression across cacheline boundaries. Similarly,
DMC [36] was proposed to improve memory capacity.
Compression can use software support and increase the main
memory capacity. Products like IBM MXT and VMWare ESX use
“Balloon Drivers" to allocate and hold unused memory when data
becomes incompressible or when Virtual Machines exceed capacity
thresholds [7, 19, 64, 68, 69]. One can also use compression in the
context of memory security. Morphable Counters [55] compress
integrity tree and encryption counters to reduce the size and height
of the integrity tree within the main memory.
6.5 Metadata Management for Main Memory
To reduce metadata bandwidth overheads from compression, Attaché [24] and PTMC [73] enables data and metadata to be accessed
together. Deb et. al. [14] describes the challenges in maintaining
metadata in main memory and recommend using ECC to store
Metadata. While this technique is useful for memory modules that
have ECC in them, LLC uses tag entries and does not have to rely on
ECC to store metadata [11, 32, 41–45, 54, 56, 72]. However, Touché
can be expanded to include the ECC within LLC to store metadata.
6.6 Other Relevant Work
Sardashti and Wood [61] observed that cachelines in the same page
may not have similar compressibility. Hallnor et. al. [21] proposed
using compressed data throughout the memory hierarchy. This
approach reduces the overheads of compression and decompression
at every level of memory hierarchy. Sathish et al. [62] try to save
463
MICRO-52, October 12–16, 2019, Columbus, OH, USA Hong, Abali, Buyuktosunoglu, Healy, and Nair
memory bandwidth by using both lossy and lossless compression
for GPUs. Recent work from Han et. al. [22] and Kadetotad et.
al. [31] used compression with deep neural networks to signi"cantly
improve performance and reduce energy. These prior work are
orthogonal to Touché.
Cache compression has also been used to reduce cache power
consumption. Residue cache architecture [35] reduces the last-level
cache area by half, resulting in power saving. Other prior works
have been proposed to lower the negative impacts of compression on the replacement. ECM [5] reduces the cache misses using
Size-Aware Insertion and Size-Aware Replacement. CAMP [49]
exploits the compressed cache block size as a reuse indicator. BaseVictim [20] was also proposed to avoid performance degradation
due to compression on the replacement. The power-performance
e#ciency of Touché can be improved using these prior work.
7 SUMMARY
The Last-Level Cache (LLC) capacity per core has stagnated over
the past decade. One way to increase the e!ective capacity of LLC is
by employing data compression. Data compression enables the LLC
controller to pack more memory blocks within the LLC. Unfortunately, the additional compressed memory blocks require additional
tag entries. The LLC designer needs to provision additional tag area
to store the tag entries of compressed blocks. We can also restrict
data placement within each cacheline to neighboring addresses
(superblocks) and reduce the tag area overheads. Ideally, we would
like to get the bene"ts of LLC compression without incurring any
tag area overheads.
To this end, this paper proposes Touché, a framework that enables LLC compression without any area overheads in the tag or
data arrays. Touché uses shortened signatures to represent full tag
address and appends the full tags to the compressed memory blocks
in the data array. This enables Touché to store arbitrary memory
blocks as neighbors. Furthermore, Touché can be enhanced further to include superblocks. Touché is completely hardware based
and achieves a near-ideal speedup of 12% (ideal 13%) without any
changes or area overheads to the tag and data array.