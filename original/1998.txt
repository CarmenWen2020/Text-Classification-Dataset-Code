ABSTRACT
High main memory latency continues to limit performance of modern high-performance out-of-order cores. While DRAM latency has
remained nearly the same over many generations, DRAM bandwidth has grown significantly due to higher frequencies, newer
architectures (DDR4, LPDDR4, GDDR5) and 3D-stacked memory
packaging (HBM). Current state-of-the-art prefetchers do not do
well in extracting higher performance when higher DRAM bandwidth is available. Prefetchers need the ability to dynamically adapt
to available bandwidth, boosting prefetch count and prefetch coverage when headroom exists and throttling down to achieve high
accuracy when the bandwidth utilization is close to peak.
To this end, we present the Dual Spatial Pattern Prefetcher
(DSPatch) that can be used as a standalone prefetcher or as a lightweight adjunct spatial prefetcher to the state-of-the-art delta-based
Signature Pattern Prefetcher (SPP). DSPatch builds on a novel and
intuitive use of modulated spatial bit-patterns. The key idea is to:
(1) represent program accesses on a physical page as a bit-pattern
anchored to the first “trigger" access, (2) learn two spatial access
bit-patterns: one biased towards coverage and another biased towards accuracy, and (3) select one bit-pattern at run-time based
on the DRAM bandwidth utilization to generate prefetches. Across
a diverse set of workloads, using only 3.6KB of storage, DSPatch
improves performance over an aggressive baseline with a PC-based
stride prefetcher at the L1 cache and the SPP prefetcher at the L2
cache by 6% (9% in memory-intensive workloads and up to 26%).
Moreover, the performance of DSPatch+SPP scales with increasing
DRAM bandwidth, growing from 6% over SPP to 10% when DRAM
bandwidth is doubled.
CCS CONCEPTS
• Computer systems organization → Processors and memory architectures.
KEYWORDS
Data prefetching, microarchitecture, memory latency
1 INTRODUCTION
High main memory latency continues to limit the performance of
modern high-performance out-of-order (OOO) cores. Prefetching is
a well-studied approach to mitigate the performance impact of the
high memory latency [25, 29, 38, 44, 46, 50, 52, 55, 65, 68, 72, 76–
78, 81]. A primary metric to improve performance using prefetchers
is coverage, which is the fraction of program loads to memory that
are removed by the prefetcher. At odds with coverage, but still
very important, is prefetcher accuracy, which is the fraction of
issued prefetches that are actually needed by the program loads.
Inaccurate prefetches can pollute the small on-die caches and can
cause excessive pressure on memory bandwidth, which in turn can
increase the latency of responses from memory.
While DRAM latency has remained nearly the same over decades
[27, 61], DRAM bandwidth has grown significantly [40, 60]. Higher
DRAM core frequencies and new DRAM architectures (e.g., DDR4
[10], LPDDR4 [12], GDDR5 [11]) boost memory bandwidth at the
same memory interface width. Newer 3D-stacked memory packages [7, 8, 24, 60] enable higher bandwidth by increasing the memory interface width. When higher DRAM bandwidth headroom
is available, the negative impact due to inaccurate prefetches is
lower. While the DRAM bandwidth is shared among multiple cores
in a system, several prior studies across mobile, client and server
systems [40, 48, 53, 64, 71, 79] have observed that the available
memory bandwidth is not heavily utilized. Latency is often a bigger
bottleneck than bandwidth, since either (1) there are very few active threads running in the system, (2) not all threads are memory
sensitive, or (3) there is not enough memory parallelism present in
the program to fully utilize the memory bandwidth [40]. Yet, as we
demonstrate in Figure 1, current state-of-the-art prefetchers (Signature Pattern Prefetcher (SPP) [55], Best Offset Prefetcher (BOP) [65]
and Spatial Memory Streaming (SMS) [77]) do not scale well in
performance with increasing peak DRAM bandwidth.1 Prefetching techniques need to evolve to make the best use of this critical
DRAM bandwidth resource when power budget is available. Specifically, prefetchers need to possess the ability to dynamically adapt
to available DRAM bandwidth, boosting predictions and coverage
when headroom exists and throttling down to achieve high accuracy
when the bandwidth utilization is close to peak.
Prefetching is a speculation mechanism to predict future addresses to be accessed by the program. Address access patterns can
be represented in various forms, including full addresses, offsets
in a spatial region (typically a 4 KB page), or address deltas between accesses. Choosing an address access representation that has
the best chance of exposing repeating patterns can help to boost
prefetch coverage and performance.
Patterns in address accesses that are readily apparent when taking a global or accumulative view of accesses may not be visible
1The BW line in Figure 1 reflects the increase in memory bandwidth. One should not
confuse it with the performance trend. Our evaluation methodology and workloads
are described in Section 5.
531
MICRO-52, October 12–16, 2019, Columbus, OH, USA Bera and Nori, et al.
0
5
10
15
20
25
30
35
40
45
10%
15%
20%
25%
30%
35%
40%
10 15 20 25 30 35 40
Peak DRAM Bandwidth (GBps)
Performance Delta over Baseline (%)
Peak DRAM Bandwidth (GBps)
BOP SMS SPP BW
Figure 1: Prefetcher performance scaling with DRAM bandwidth (points correspond to single and dual channels of
DDR4-1600, 2133 and 2400)
when taking a restricted view of deltas between recent consecutive
accesses. Figure 2 illustrates an example of multiple streams of
accesses within a single spatial region and their representation in
various formats. The first access to the region is called the “trigger” access. Access streams B through E have the same trigger
offset in the spatial region and touch all the same offsets but in
different temporal order. Such variations are typically an artifact
of reordering due to out-of-order scheduling in the core and the
cache/memory sub-systems. The longer the access sequence, the
higher the probability of variations [44]. These access streams all
have different representations when successive address deltas are
used to represent the access patterns. Yet, we realize that they can
actually be represented by a single spatial bit-pattern. For example,
access streams B and C with trigger offset 1 have two different
delta representations (+4,-1,+7,+1 and +4,+6,-7,+8) but the same
(i.e., single) bit-pattern representation BP2 (0100 1100 0001 1000).
Crucially, we observe that when bit-patterns are anchored to the
“trigger” offset (rotated left in this case), all access streams in the example can be represented by a single anchored bit-pattern. Such an
anchored bit-pattern essentially represents two views of the delta
stream: the deltas between consecutive accesses (we call this the
local view of deltas) and the deltas relative to the trigger access (we
call this the global view of deltas).
Figure 2: Multiple different address access streams in a single memory region can be represented by a single spatial bitpattern, anchored to their respective trigger accesses.
While the use of anchored spatial bit-patterns can boost coverage, it does not provide the ability to adapt and scale prefetch
coverage based on the available resources and DRAM bandwidth
in the system. Multiple access streams in a spatial region can have
anchored bit-patterns that are very similar (i.e., have some common
set bits) but not exactly the same. We propose a novel and intuitive
approach to simultaneously optimize coverage and accuracy by
learning two bit-patterns: one biased towards coverage and another
towards accuracy. A bitwise OR operation on the recently-observed
anchored bit-patterns in a given memory region adds bits into a
resultant bit-pattern (called the coverage-biased bit-pattern) and
thus modulates the resultant bit-pattern for higher coverage. Similarly, a bitwise AND operation on the recently-observed anchored
bit-patterns in a given memory region subtracts bits away from
a resultant bit-pattern (called the accuracy-biased bit-pattern) and
thus modulates the second resultant bit-pattern for higher accuracy. Figure 3 shows an example of how multiple different address
patterns to the same memory region that map to three different
anchored bit-patterns can be modulated into a coverage-biased bitpattern (shown in green) and an accuracy-biased bit-pattern (shown
in red). As we will show, dynamic modulation of these bit-patterns
enables simultaneous optimization for both coverage and accuracy,
even though these metrics are at odds with each other. The available
memory bandwidth headroom, coupled with a quantified measure
of accuracy and coverage can be used to select between the two
two modulated bit-patterns dynamically at run-time.
Figure 3: Illustration of two modulated bit-patterns that
DSPatch takes advantage of
We present the Dual Spatial Pattern Prefetcher (DSPatch), a lightweight spatial prefetcher that can be used as a standalone prefetcher
or as a light-weight adjunct spatial prefetcher to the state-of-theart delta-based Signature Pattern Prefetcher (SPP). DSPatch builds
on an intuitive use of simple logical OR and AND operations to
learn two modulated spatial bit-pattern representations of accesses
to a given memory region (i.e., a physical page). One bit-pattern
is biased towards coverage and the other bit-pattern is biased towards accuracy. DSPatch employs a simple but effective method to
track coverage and accuracy characteristics of each modulated bitpattern as well as the overall DRAM bandwidth utilization. Based
on this tracking information, DSPatch dynamically selects one bitpattern to generate prefetches. If the DRAM bandwidth utilization is
high, DSPatch selects the accuracy-biased bit-pattern for prefetching. If the DRAM bandwidth utilization is low, DSPatch selects
the coverage-biased bit-pattern if the bit-pattern has good enough
accuracy, or the accuracy-biased bit-pattern otherwise.
We make the following key contributions in this work:
532
DSPatch: Dual Spatial Pattern Prefetcher MICRO-52, October 12–16, 2019, Columbus, OH, USA
• We observe that even though peak DRAM bandwidth is growing with newer DRAM architectures and packages, state-ofthe-art prefetcher performance does not scale well with the
growing DRAM bandwidth.
• We show that a spatial bit-pattern representation anchored
around a trigger access to a region can effectively capture all
deltas (local and global) from the trigger. This transformation exposes similar patterns that are otherwise obfuscated
to look different due to memory access reordering in the
processor and the memory subsystem.
• We introduce a new prefetching algorithm that learns two
modulated bit-patterns to prefetch in a given memory region
by using simple logical OR and AND operations. One bitpattern is biased towards coverage and the other is biased
towards accuracy.
• We propose a simple method to track DRAM bandwidth
utilization and to measure the coverage and accuracy of
modulated bit-patterns. We show that this method enables
effective dynamic selection of a single bit-pattern to generate
prefetch candidates at run-time.
Across a diverse set of 75 workloads, with only 3.6KB of storage,
DSPatch improves performance over an aggressive baseline that
employs a PC-based stride prefetcher at the L1 cache and the SPP
at the L2 cache by 6% (9% in memory-intensive workloads and
up to 26%). As a standalone prefetcher, DSPatch has slightly (1%)
higher performance than the state-of-the-art SPP with only 2/3
r d
of the storage requirements of SPP. We find that, the use of SPP and
DSPatch together combines the benefits of both the state-of-theart fine-grained delta-based prefetching and the state-of-the-art
bit-pattern-based prefetching. We show that, by simultaneously
optimizing for both coverage and accuracy, every 2% increase in
coverage with DSPatch comes at only a 1% increase in mispredictions. Finally, the performance of DSPatch+SPP scales well with
increasing memory bandwidth, growing from 6% over SPP to 10%
when DRAM bandwidth is doubled.
2 BACKGROUND AND MOTIVATION
Prefetching is a speculation technique that predicts the addresses
of high-latency accesses in the program and brings the associated
data into low-latency on-die caches for use by the program. Highlatency accesses, typically to DRAM main memory, often stall the
retirement of instructions in a core [68, 69]. They also reduce the
look-ahead for instruction-level-parallelism (ILP) extraction, since
filling up of the re-order buffer (ROB) due to memory access related
stalls prevents allocation of younger independent instructions into
processor structures [68, 69].
Multiple mechanisms to represent address access patterns in a
program have been studied over the years. Address access patterns
can be represented via various means, including (1) full addresses,
(2) offsets in a spatial region (typically a 4 KB page) or (3) address
deltas between consecutive accesses. In order to identify such patterns, prefetchers typically examine a subset of memory accesses
filtered by certain program context. For example, a PC-based stride
prefetcher tries to learn a constant stride between consecutive
cacheline addresses referenced by a program counter (PC) value.
Here, the PC acts as a program context that filters out accesses, in
order to easily discover the access pattern. We call this program
context that is used for filtering accesses as signature. A signature
can be constructed using memory access information of a program
like physical addresses, offsets or deltas between consecutive accesses, and can potentially be augmented with program control flow
information like the PC. Prefetchers typically learn the repeating
program address access pattern by correlating it with a signature
and predict that the learnt pattern will be needed again when the
signature is seen again. The design choice a prefetcher makes on
what to use as a signature and as the address access pattern representation determines its effectiveness at optimizing the four main
metrics of prefetching:
• Coverage: The fraction of high-latency memory accesses of
the program that are saved by the prefetcher (the higher the
better)
• Timeliness: The fraction of the latency of the high-latency
accesses hidden by the prefetcher (the higher the better)
• Accuracy: The fraction of prefetched addresses that are later
needed by the program (the higher the better)
• Storage: The hardware storage requirements of the prefetcher
(the smaller the better)
In the rest of this section, we comprehensively examine three
state-of-the-art prefetchers (SPP [55], BOP [65] and SMS [77]) with
respect to their choices of signature and address access pattern
representations. By analyzing a wide range of workloads, we compare the performance2
and identify the merits of the three types of
prefetching. Finally, for each of the prefetchers, wherever possible,
we evaluate potential dynamic bandwidth-aware tuning opportunities (similar to [35, 78]) for higher coverage to arrive at the
prefetcher’s best possible scalability in the presence of memory
bandwidth headroom.
2.1 Signature Pattern Prefetcher (SPP)
SPP [55] is the state-of-the-art delta-based prefetcher. It uses a signature comprised solely of up to four recent consecutive address
deltas (which we call “local” deltas) observed in a 4KB page. Each
signature tracks at most four possible next deltas as prefetch candidates, along with a confidence value associated with each candidate.
This allows SPP to track complex but repeating address delta patterns at low cost. SPP uses a recursive look-ahead mechanism to
boost prefetch distance and timeliness. SPP appends each prefetch
candidate delta recursively to the candidate delta’s signature to
generate further prefetch candidates. The confidence of each new
prefetch candidate is a cascaded product of confidences leading to
the candidate’s level and the candidate’s stored confidence value. A
prefetch delta candidate whose cascaded confidence value is above
a threshold value triggers a prefetch.
A big advantage of a delta-based prefetcher is that every access
can participate in generating prefetches. This effectively allows
multiple “bites” at the “apple” (coverage) to boost performance.
With low storage requirements and through the use of cascaded
confidence values, SPP has fine-grained control over coverage, timeliness and accuracy. As seen in Figure 4, SPP outperforms both BOP
and SMS in six out of our nine workload categories, as well as on
average.
2
Section 5 describes our evaluation methodology and workloads.
533
MICRO-52, October 12–16, 2019, Columbus, OH, USA Bera and Nori, et al.
0%
10%
20%
30%
40%
Client
Server
HPC
FSPEC06
ISPEC06
FSPEC17
ISPEC17
Cloud
SYSmark
GEOMEAN
Performance Delta
over Baseline (%)
BOP SMS SPP
Figure 4: Performance of BOP, SMS and SPP L2 prefetchers
over a baseline with an L1 PC-stride prefetcher and a single
channel of DDR4-2133
However, there are scenarios where SPP loses out on coverage or
timeliness. In pages with sparse and highly irregular access patterns,
SPP cannot track all possible deltas, losing out on coverage. The
deltas it tracks have low confidence values, limiting the recursive
prefetch distance and hence timeliness. Due to these shortcomings,
SPP performs worse than either or both of the other two prefetchers
in ISPEC17, Cloud and SYSMark workload categories.
Bandwidth-aware tuning opportunity. SPP uses a static confidence threshold value of 25% to allow the prefetching of a candidate delta. With a simple dynamic scheme that monitors the
available DRAM bandwidth headroom,3 we could modulate this
threshold to lower values when DRAM bandwidth utilization is
low. In Section 2.5 we evaluate an enhanced version of SPP (eSPP)
that has the ability to lower its confidence threshold value to 12.5%
if more than half of the DRAM bandwidth is not utilized. We observe that even eSPP shows poor performance scaling with higher
memory bandwidth.
2.2 Best Offset Prefetcher (BOP)
BOP [65] is a delta-based prefetcher that aims to determine the
set of optimal “global” deltas between accesses within a memory
region (e.g., 4KB). For example, if a program experiences a repeating
series of successive local deltas (1,2,1,2,1,2...), BOP identifies a single
global delta of 3 (or its multiples) as an effective representation of
address access patterns in the program. Further, BOP tracks and
utilizes the most appropriate global delta to achieve timeliness (a
multiple of 3 in this example).
Unlike SPP, which constructs a local view of accesses, BOP constructs a global view of accesses that helps BOP in two ways. First,
the global view exposes more patterns in a memory region than a
restricted local view of consecutive accesses. This especially helps
BOP to predict future accesses in workloads with irregular access
patterns with only few accesses per page. Second, the global view is
robust against program access reordering, which can further disrupt
the pattern learning based on a restricted local view of accesses. As
seen in Figure 4, BOP, at a prefetch degree of two, has the highest
performance among all prefetchers in the ISPEC17 workload category. However, BOP learns only a limited set of global deltas for all
access streams in the program in a statically-defined epoch (defined
by the number of accesses). This severely limits BOP’s coverage
and timeliness in HPC and Server workload categories.
Bandwidth-aware tuning opportunity. The original BOP proposal tracks only a limited set of possible global deltas (in a 4KB
page, 126 possible deltas from -63 to +63 exist) and statically picks
3We describe our scheme for monitoring available bandwidth headroom in Section 3.2.
a single best global delta per epoch (for a prefetch degree of one). In
Section 2.5, we evaluate an enhanced bandwidth-aware version of
BOP (called eBOP) that can adapt to DRAM bandwidth headroom.
eBOP has a default prefetch degree of one, but can dynamically
increase its degree to two and four if the bandwidth headroom is
more than 25% and 50%, respectively. We find that neither BOP’s
nor eBOP’s performance improvement scales well with additional
memory bandwidth. This is because of two reasons. First, BOP’s
predictions suffer from poor accuracy since BOP does not use any
program context information as a signature to generate prefetches.
Second, any limit on prefetch degree hurts BOP’s coverage.
2.3 Spatial Memory Streaming (SMS)
SMS [77] tracks address accesses within a spatial region (e.g., 2KB
or 4KB) as a spatial bit-pattern. It maps each region’s address access pattern to a signature comprising the trigger access PC and
trigger offset in the region. The trigger access is defined as the first
access to the region that adds the region to the structure that tracks
recently-accessed regions. Doing so, SMS effectively exploits spatial
correlations between an access from a PC and other accesses in the
region. A bit-pattern representation inherently captures a global
view of accesses in the region. Used in conjunction with a trigger
PC based signature, SMS performs better than SPP in ISPEC17,
Cloud and SYSmark workload categories (Figure 4).
However, SMS makes a number of static decisions that negatively affect its overall performance. Based on a study of available
workloads [77], it statically decides to track 2KB regions (rather
than 4KB) for accuracy reasons, thereby limiting prefetch distance
and timeliness opportunities compared to SPP and BOP. With no
explicit mechanism to track the accuracy of the stored bit-patterns,
SMS relies on a high degree of access filtering through the use of
sophisticated signature (PC+Offset). Therefore, to increase overall
coverage, SMS relies on tracking a large number of signatures, increasing its storage requirements to tens of KB. Figure 5 shows that
reducing the number of entries of pattern history table (i.e., the
table that stores the correlation between signature and bit-pattern)
in SMS from a baseline 16K entries (16-way associative) at 88KB
storage down to 256 entries at 3.5KB storage approximately halves
SMS’s average performance improvement across all of our evaluated workloads. Furthermore, the current SMS design provides
no clear opportunities to dynamically tune performance based on
increase in DRAM bandwidth.
0%
5%
10%
15%
20%
16K 4K 1K 256
Performance Delta 
over Baseline (%)
SMS Pattern History Table entries
16.5%
8.8%
Figure 5: Performance impact of reducing SMS storage size
from 16K entries at 88KB to 256 entries at 3.5KB, averaged
across all workloads
534
DSPatch: Dual Spatial Pattern Prefetcher MICRO-52, October 12–16, 2019, Columbus, OH, USA
2.4 Cache Pollution
Inaccurate prefetches can cause pollution in on-die caches by evicting useful cache blocks. The impact of pollution can be mitigated via
the use of dead-block prediction [39, 49, 54] and prefetch-aware replacement and insertion policies [33–36, 47, 78, 83]. In fact, multiple
generations of last-level cache replacement policies have specifically targeted identifying and replacing dead blocks [39, 54, 82]. We
do not observe significant pollution impact of inaccurate prefetches.4
We find that the pressure on the memory bandwidth resource is
the primary negative impact of inaccurate prefetches in the stateof-the-art prefetchers we examine.
2.5 Performance Scaling of Prefetchers with
Memory Bandwidth Scaling
Figure 6 shows how the performance improvement of each prefetcher scales with increased memory bandwidth. We draw three major
conclusions from the figure. First, the performance improvement
of none of the three prefetchers scales well with increased memory
bandwidth. In other words, the benefit of each prefetcher saturates
as memory bandwidth increases. Second, the rate of the increase
in performance improvement with increase in memory bandwidth
is higher in SMS. In fact, the performance improvement of SMS
matches that of SPP at higher memory bandwidth points. This
shows the benefit of spatial bit-pattern prefetching in the presence of higher memory bandwidth. Third, eBOP enjoys the best
performance scaling with memory bandwidth due to its dynamic
modulation of the prefetch degree. However, the lack of program
context information and the limited prefetching degree constrains
eBOP coverage and leaves significant performance on the table.
0
5
10
15
20
25
30
35
40
45
10%
15%
20%
25%
30%
35%
40%
10 15 20 25 30 35 40
Peak DRAM Bandwidth (GBps)
Performance Delta over Baseline (%)
Peak DRAM Bandwidth (GBps)
BOP SMS SPP
eSPP eBOP BW
Figure 6: None of the five state-of-the-art prefetchers we examine, including eSPP and eBOP, scale well in performance
with higher DRAM bandwidth.
2.6 Takeaways and Our Goal
In summary, our analysis of state-of-the-art prefetchers lead to
three major takeaways:
• None of the state-of-the-art prefetchers we examine scale
well in performance when higher DRAM bandwidth is available. SMS inherently lacks the ability to use available memory bandwidth in its algorithm to fine tune prefetch aggressiveness. SPP and BOP can be made bandwidth aware, yet
4
See appendix for more detail.
they scale poorly in performance (as we see for eSPP and
eBOP Figure 6).
• A spatial bit-pattern representation anchored around a trigger access to a memory region effectively captures all deltas
in the region: local (deltas between consecutive accesses) and
global (deltas with respect to the trigger access). This representation exposes patterns that are otherwise obfuscated by
reordering in the processor and the memory subsystem.
• Using simple bit operations like OR and AND on the recently
seen access bit-patterns in a memory region, we can learn
two modulated bit-patterns, one biased towards coverage
and the other biased towards accuracy. We can dynamically
select the appropriate bit-pattern to generate prefetches to
increase prefetch coverage when memory bandwidth utilization is low, or to increase prefetch accuracy when memory
bandwidth utilization is high.
Our goal is to design a spatial bit-pattern prefetcher that integrates the memory bandwidth inherently into its learning algorithm
to dynamically adjust its notion of aggressiveness so that it can scale
its performance improvement with higher memory bandwidth. To
this end, we present the Dual Spatial Pattern Prefetcher (DSPatch),
which makes use of the dual modulated spatial bit-patterns we
introduced earlier to simultaneously optimize prefetch coverage
and accuracy based on the memory bandwidth utilization.
3 DUAL SPATIAL PATTERN PREFETCHER
In this section, we describe the Dual Spatial Pattern Prefetcher
(DSPatch), a spatial bit-pattern prefetcher that learns two bit-patterns per memory region (i.e., a physical page) and associates them
with a program counter (PC) based signature:
• One bit-pattern (calledCovP) is biased towards higher coverage. It is calculated as a simple OR of the recently observed
spatial program access bit-patterns to the physical page. The
OR operation adds bits to the learnt bit-pattern and grows
the bit-pattern for higher coverage, up to a certain threshold.
• The other bit-pattern (called AccP) is biased towards higher
accuracy. It is calculated as a simple AND of the coveragebiased bit-pattern (CovP) and the currently observed program access bit-pattern to the physical page. The AND operation reduces the set bits to maximize accuracy but since
AccP is derived from CovP, coverage is kept in check.
DSPatch mainly comprises of two hardware structures: Page
Buffer (PB) and Signature Pattern Table (SPT ). The purpose of PB is
to record the observed spatial bit-patterns as the program accesses a
physical page. The purpose of SPT is to store the two modulated spatial bit-patterns (CovP andAccP), derived from previously-observed
bit-patterns, by associating them with the trigger PC into the page.
Thus, DSPatch observes program accesses per physical page, but
learns overall program access patterns in a page-agnostic way by
associating the spatial bit-patterns with the triggering PC signature.
SPT is looked up with the triggering PC when a new physical page
is accessed, to retrieve the two modulated bit-patterns: CovP and
AccP. The key goal of DSPatch is to dynamically adapt prefetching for either higher coverage or higher accuracy depending on
the DRAM bandwidth utilization. Using a simple 2-bit bandwidth
utilization signal broadcast from the memory controller to all the
535
MICRO-52, October 12–16, 2019, Columbus, OH, USA Bera and Nori, et al.
cores, DSPatch selects either the CovP (when memory bandwidth
utilization is low) or the AccP (when memory bandwidth utilization
is high) bit-pattern to drive the prefetching.
Section 3.1 shows a high-level view of DSPatch. Section 3.2 discusses how DSPatch tracks the overall bandwidth utilization across
all the cores. The subsequent sections describe the algorithm to
modulate, learn and predict the spatial bit-patterns.
3.1 Overview
Figure 7 depicts the overall architecture of DSPatch.
Page Buffer (PB)
Signature 
Prediction 
Table
(SPT)
Program
Access
BW
utilization
1
2
3
4
5
CovP
AccP
Figure 7: DSPatch block diagram and overall organization
Page Buffer (PB) and Signature Prediction Table (SPT ) are the two
prime structures of DSPatch. Each PB entry tracks accesses in a
4KB physical page and accumulates L1 misses in the page’s stored
bit-pattern (step 1 ). The first access (step 2 ) to each 2KB segment
in the 4KB physical page is eligible to trigger prefetches. The PC of
this trigger access is stored in the PB entry and used to index into
the SPT , which retrieves the two CovP and AccP bit-patterns and
the measure of their goodness (step 3 ). Selection logic, detailed
in Section 3.6, uses the memory bandwidth utilization measure to
select a bit-pattern to generate prefetch candidates (step 4 ). The
selected bit-pattern is anchored (i.e., rotated) to align to the trigger
access offset before issuing prefetches. On eviction from the PB
(step 5 ), for each trigger (per 2KB segment), the stored bit-pattern
is first anchored (i.e., rotated) to trigger offset. Then, SPT is looked
up using the stored trigger PC and the stored bit-patterns and the
counters are updated as described in Section 3.6.
3.2 Tracking Bandwidth Utilization
DSPatch tracks memory bandwidth utilization with a simple counter
at the memory controller that counts the number of issued DRAM
column access (CAS)5
commands in a time window of (4 × tRC) cycles (where tRC is the minimum allowed time between two DRAM
row activations). To include hysteresis in tracking, the counter is
halved after every window. The number of channels and the width
of each channel determines the peak DRAM bandwidth, as well as
the peak possible number of CAS commands in each tRC window.
We further bucket this counter into quartiles (25%, 50% and 75%)
5A DRAM column access command, i.e., a read or write to the contents of an open row
in a DRAM bank, indicates a data transfer out of or into the DRAM chip, and hence
directly captures DRAM data bus activity. For more detail on DRAM operation, we
refer the reader to prior works [42, 57, 60, 62, 73].
of peak bandwidth. Every tRC cycle, the value of the counter is
compared to each of the three quartile thresholds, resulting in a
two bit (2b) quantized value representing which quartile the current bandwidth utilization falls into (e.g., 3 indicates more than
75% bandwidth utilization, whereas 0 indicates less than 25% bandwidth utilization). This 2-bit quantized bandwidth utilization value
is broadcast to all cores and is used as a representative of current
memory bandwidth utilization in the DSPatch algorithm.
3.3 Anchored Spatial Bit-patterns
To maximize the possibility of exposing data access patterns, DSPatch uses a program access representation that is robust against
reordering of accesses in the processor and the memory hierarchy.
Figure 2 motivates the use of spatial bit-patterns anchored to the
trigger (i.e., first) access to a memory region to capture all local
and global deltas from the trigger. DSPatch uses such anchored bitpatterns to represent and predict program address access patterns
to a given memory region. Our implementation of DSPatch employs
a Page Buffer (PB) that tracks the 64 most-recently-accessed 4KB
physical pages at the L2 cache level. Each PB entry stores a 64b bitpattern that accumulates the L2 cache block addresses referenced
by the program loads and stores in the page.
3.4 The Choice of Signature and
Signature-Pattern Mapping
The choice of signature has a significant impact on the design of
a prefetcher. The more information a signature encodes, the more
prefetch filtering is achieved and hence the higher the expected
accuracy. Prior bit-pattern prefetchers use the PC of the trigger
access along with the offset in the page [77] or the actual page
address [26]. They implicitly expect high prediction accuracy and
hence just store and use the last occurring bit-pattern per signature
without explicitly tracking accuracy. However, this comes at the cost
of extra storage since the prefetcher needs to track a large enough
set of frequently occurring signatures to achieve high coverage.
DSPatch uses just the PC of the trigger access to a physical page
as the signature. It learns two modulated bit-patterns from the
recently seen accesses to a physical page and stores the bit-patterns
by associating them with a PC signature in the Signature-Pattern
Table (SPT ). Upon encountering the same signature, DSPatch looks
up the SPT and selects a bit-pattern to generate prefetch candidates
associated with that signature. DSPatch organizes the SPT as a 256-
entry tagless direct-mapped structure. A simple folded-XOR hash
of the PC is used to index into this structure. While this indexing
can reduce storage requirements, there are associated trade-offs in
accuracy and coverage. The use of only PC as a program signature
can result in lower accuracy while aliasing of multiple PCs into a
single entry can have an unpredictable impact if we only store and
use the last occurring bit-pattern. Therefore, DSPatch uses simple
mechanisms (with bitwise AND and PopCount operations) to track
coverage and accuracy of stored bit-patterns (as we describe in
Section 3.5). Crucially, DSPatch uses two modulated bit-patterns,
one biased towards coverage (through OR operations) and the other
biased towards accuracy (through AND operations). This allows
DSPatch to simultaneously optimize for both coverage and accuracy
(see Section 3.6). We describe these components of DSPatch in the
subsequent sections.
536
DSPatch: Dual Spatial Pattern Prefetcher MICRO-52, October 12–16, 2019, Columbus, OH, USA
3.5 Quantifying Accuracy and Coverage
Figure 8 depicts a simple scheme for quantifying the accuracy and
coverage of bit-pattern predictions for a given physical page. PopCount of the predicted bit-pattern gives the prefetch count (Cpr ed ),
whereas the PopCount of the access bit-pattern generated by the
program (called the program bit-pattern) gives the total number of
accesses (Cr eal). Similarly, PopCount of the bitwise AND operation between the program bit-pattern and the predicted bit-pattern
gives the accurate prefetch count (Cacc ). Prediction accuracy is
computed as the ratio Cacc /Cpr ed whereas prediction coverage
is computed as Cacc /Cr eal . Instead of computing the exact fractional value, we quantize our measure of accuracy and coverage
into four quartiles via simple shift and compare operations.6
Bit-pattern PopCount
Program 1011 0100 0011 1100 8
Predicted 1010 0110 0000 0001 5
Bitwise-AND 1010 0100 0000 0000 3
<25% 25-50% 50-75% >=75%
Prediction Accuracy 3/5 ü
Prediction Coverage 3/8 ü
Figure 8: Prediction accuracy and coverage can be measured
by simple bitwise AND and PopCount operations
3.6 Modulated Dual Bit-patterns:
Coverage-biased and Accuracy-biased
A crucial goal of DSPatch is to have the ability to simultaneously
optimize for prefetch coverage and accuracy based on memory
bandwidth utilization. To this end, DSPatch stores two modulated
bit-patterns per SPT entry, one is biased towards coverage (CovP)
and the other is biased towards accuracy (AccP), as shown in Figure 9.
Figure 9: Two modulated spatial bit-patterns that can simultaneously optimize for both coverage and accuracy
Coverage-biased Bit-pattern (CovP). Since an anchored bitpattern effectively captures all deltas from a trigger access, adding
more deltas to increase predictions and coverage is a simple matter of setting the appropriate bits in the bit-pattern. This can be
6
25% (50%) quartile can be computed by right shifting the denominator by 2 (1) and then
comparing with the numerator. 75% quartile can be computed by first taking difference
between the denominator and the numerator and then comparing the difference value
with the denominator right shifted by 2.
achieved via simple bitwise OR operations on the predicted bitpattern with the program bit-pattern. However, since too many repeated ORs could eventually set all bits in a bit-pattern, we limit updates to at most three OR operations. DSPatch uses a 2b saturating
counter named OrCount for each CovP to track the number of OR
operations. OrCount is incremented every time the OR operation
adds any bits to the predicted bit-pattern. DSPatch also employs a
2b saturating counter called MeasureCovP to quantify the goodness
of CovP. MeasureCovP is incremented in two cases: (1) if the CovP
prediction accuracy is less than a threshold value (called AccThr)
or (2) if the prefetch coverage from CovP is less than a threshold
value (called CovThr). Thus, a saturated MeasureCovP value essentially indicates that prefetching with the CovP bit-pattern would
either lack in prefetch accuracy or prefetch coverage, and hence
CovP needs to be relearnt from scratch. DSPatch resets CovP to the
current program bit-pattern when MeasureCovP is saturated and
either of the two following conditions are satisfied: (1) current memory bandwidth utilization is in the highest quartile or (2) prefetch
coverage is less than 50%. We use the 50% quartile threshold value
for both AccThr and CovThr.
Accuracy-biased Bit-pattern (AccP). The accuracy-biased bitpattern requires retaining recurring bits in the bit-pattern, which
can be achieved by an AND operation. Rather than recursive AND
operations on AccP, on every update, AccP is replaced by a bitwise
AND operation of the program bit-pattern and the CovP. Similar to
the MeasureCovP , DSPatch also uses a 2b saturating counter called
MeasureAccP to quantify the goodness of AccP. MeasureAccP is incremented if AccP prediction accuracy is less than 50%, and is decremented otherwise. Thus, a saturated MeasureAccP counter value
essentially indicates that prefetching with the AccP bit-pattern
would lack in prefetch accuracy. DSPatch uses MeasureAccP to
completely throttle down predictions when memory bandwidth
utilization is high.
Bit-pattern Selection for Prefetch Generation. Figure 10
shows the algorithm DSPatch uses to choose between CovP and
AccP for prefetch generation. When DRAM bandwidth utilization
is in the highest quartile (75%), we select AccP for prefetching if
MeasureAccP is not saturated. When bandwidth utilization is in
the second highest quartile (between 50% and 75%), we select AccP
for prefetching if MeasureCovP is saturated (indicating that CovP
is inaccurate) and CovP otherwise. When bandwidth utilization
is less than 50%, we simply select CovP for prefetching. To minimize any pollution effect when bandwidth utilization is less than
50%, we fill the prefetched blocks at low priority in the on-die L2
cache and LLC, if MeasureCovP is saturated (indicating that CovP
is inaccurate).
3.7 2KB (32b) vs 4KB (64b) Predictions and
Multiple Triggers
Prior bit-pattern based prefetching proposals [37, 77] do not explicitly track accuracy and hence statically limit themselves to 2KB
(32b) bit-patterns. Since DSPatch incorporates measures to track
accuracy and to throttle its predictions, it can dynamically make
predictions at both 2KB and 4KB memory region. Instead of using 64b bit-patterns for CovP and AccP, we split them into two
32b bit-patterns. The 2b MeasureCovP and MeasureAccP counters
537
MICRO-52, October 12–16, 2019, Columbus, OH, USA Bera and Nori, et al.
BW 
utilization 
>=75%?
Prefetch 
using AccP
No prefetches
Y Y
N
BW 
utilization 
>=50%?
N
MeasureCovP
saturated? Prefetch 
using CovP
N
Y Y
N
MeasureAccP
saturated?
Figure 10: Selection of CovP versus AccP bit-patterns for
prefetching based on DRAM bandwidth utilization and measures of the goodness of predictions
track 2KB (32b) bit-patterns and the prefetch generation is done
per 2KB (32b) segment of a 4KB page. Splitting a 64b bit-pattern
into two 32b bit-patterns also enables a further benefit for DSPatch:
two prefetch triggers per 4KB page (one per 2KB segment). The first
(trigger) access to each 2KB segment in the 4KB page can attempt to
trigger prefetches. The trigger to the first 2KB segment is allowed
to predict both 32b bit-patterns (i.e., for the full 4KB page) while
the trigger to the second 2KB segment is only allowed to predict a
single 32b bit-pattern (for the 2KB region relative to the trigger).
3.8 Compressing Bit-patterns to Further
Reduce Storage Requirements
DSPatch uses one final optimization to further reduce the storage
overhead of the bit-patterns. We see that deltas +1 and -1 are the
two most frequently occurring deltas in programs. As shown in
Figure 11(a), these two deltas together appear more than 50% of
the time on average. Therefore, instead of storing bit-patterns with
each bit representing a 64B cacheline, we store a compressed bitpattern where each bit represents two adjacent 64B cachelines. We
call this compression technique 128B-granularity compression and
the resultant compressed bit-pattern 128B-granularity bit-pattern.
128B-granularity compression halves DSPatch’s pattern storage
requirements. While this compression technique could theoretically have up-to 50% inaccuracy in predictions, we observe less
than one misprediction for every five cacheline predictions (20% inaccuracy). Figure 11(b) shows the distribution of misprediction rate
caused by 128B-granularity compression across all of our evaluated
single-threaded workloads. As we can see from the figure, 128Bgranularity compression incurs no mispredictions 42% of the time,
across all workloads. This also means, 128B-granularity compression is able to represent the exact bit-pattern by consuming only
half of the storage in 42% of the time. In fact, for 70% of the time,
the misprediction rate caused by 128B-granularity compression is
lower than 25%.
3.9 Storage Requirements
Table 1 shows that DSPatch requires only 3.6KB of storage for the
configuration we evaluate in Section 5.
4 METHODOLOGY
We evaluate DSPatch using an in-house cycle accurate simulator
that models dynamically-scheduled x86 cores clocked at 4 GHz. 0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Deltas
Delta Occurrence Distribution
+1
-1
+2,+3
Distribution of misprediction rate due to 
128B-granularity compression
(a) (b)
42%
12%
16%
9%
7%
14%
Exactly 0% 0%-12.5% 12.5%-25%
25%-37% 37%-50% Exactly 50%
Figure 11: (a) +1 and -1 are the two most frequently occurring deltas, occurring more than 60% of time across all workloads. (b) 128B-granularity compression induces no mispredictions 42% of the time.
Structure Field (#bits in each entry) Entries #Bits
PB Page number (36) + Bit-pattern (64) + 2x[PC (8) +
Offset (6)] = 158 bits 64 10112
SPT CovP (32) + 2*Measur eCovP (2) + 2*ORCount (2)
+ AccP (32) + 2*Measur eAcc P (2) = 76 bits 256 19456
Total 3.6 KB
Table 1: DSPatch storage overhead
The core micro-architectural parameters are taken from the latest
Intel Skylake processor [1] and are listed in Table 2. Single-thread
(ST) simulations use a 2MB LLC and a single DDR4-2133 channel,
while multi-programmed (MP) simulations use an 8MB LLC shared
across four cores and two DDR4-2133 channels. Therefore, both
ST and MP configurations provide the same LLC capacity per core
but MP configurations have half the memory bandwidth per core
(8.5GBps) of ST (17GBps).
Core 4 cores, 4-wide OoO, 224-entry ROB, 80-entry load buffer
L1 cache Private, 32KB, 64B line, 8 way, LRU, 16 MSHRs, 5-cycle round-trip latency
L2 cache Private, 256KB, 64B line, 8 way, LRU, 32 MSHRs, 8-cycle round-trip latency
LLC
ST: 2MB; MP: shared 8MB, 64B line, 16 way, Prefetch aware dead-block
predictor similar to [39], 32 MSHRs per LLC Bank, 30-cycle round-trip latency
Main Memory
ST: Single channel; MP: Dual channel DDR4-2133MHz, 2 ranks/channel,
8 banks/rank, 64b data bus width per channel, 2KB row buffer/bank,
tCL=15ns, tRCD=15ns, tRP=15ns, tRAS=39ns
L1D prefetch PC-based stride prefetcher [38], tracks 64 PCs
Table 2: Simulation parameters
4.1 Prefetchers
Our baseline configuration has a PC-based stride prefetcher in
the L1 cache. We evaluate prior prefetching proposals SMS [77],
BOP [65] and SPP [55], as well as DSPatch, as the L2 prefetcher. Each
L2 prefetcher is trained on L1 misses (both demand and prefetch
misses from L1) and fills prefetched lines into the L2 cache and
the LLC. We fine tune each prefetcher individually in our simulation environment to produce the best possible result. Table 3
shows the prefetcher configurations. We also evaluate the AMPM
prefetcher [44] but do not show its results as it under-performs all
other prefetchers in single-thread simulations.
538
DSPatch: Dual Spatial Pattern Prefetcher MICRO-52, October 12–16, 2019, Columbus, OH, USA
BOP [65] 256-entry RR, MaxRound=100, MaxScore=31, BadScore=1,
Degree=2 (for ST), 1 (for MT) 1.3 KB
SMS [77] 2KB page region, 64-entry AT, 32-entry FT, 16K-entry PHT 88 KB
SPP [55] 256-entry ST, 512-entry PT, 8-entry GHR, 12b compressed
delta path, 10b feedback 6.2KB
Table 3: Parameters of each evaluated prefetcher
4.2 Workloads
We evaluate a diverse set of 75 workloads, including all benchmarks
from the SPEC CPU2017 [17] and the SPEC CPU2006 [16] suites. These
workloads span various types of real-world applications, and we
categorize them into 9 classes. Table 4 shows the workload classes
along with example workloads from each class. We present the average performance of each of these classes as well as the geometric
mean performance across all 75 workloads.
Class Example Workloads
Client 7-zip compression and decompression [2], vp9-encoding/decoding [23]
Server TPC-C [22], SPECjbb 2015 [19], SPECjEnterprise2010 [20], Spark pagerank [5]
HPC linpack [9], NAS Parallel Benchmarks [13], PARSEC [14], SPEC-ACCEL [15],
SPEC MPI [18]
FSPEC06 All benchmarks. e.g., sphinx3, soplex, GemsFDTD
ISPEC06 All benchmarks. e.g., gcc, mcf, omnetpp
FSPEC17 All benchmarks. e.g., namd, povray, lbm
ISPEC17 All benchmarks. e.g., omnetpp, xalancbmk, leela
Cloud Bigbench [6], Cassandra [3], Hadoop-hbase, kmeans, streaming [4]
SYSMark SYSmark-excel, photoshop, word, sketchup [21]
Table 4: Evaluated workload categories
We use both homogeneous and heterogeneous workload mixes
to simulate a multi-programmed system. To construct the homogeneous workload mixes, we select each of the 42 high-MPKI7
workloads from our full workload set and run four copies of it,
one in each core of the simulator. To construct the heterogeneous
workload mixes, we randomly select four workloads from the 42
high-MPKI workloads and generate 75 heterogeneous workload
mixes.
5 EVALUATION
5.1 Single-thread Performance
Figure 12 shows the performance comparison of prior prefetchers
and DSPatch, both as a standalone prefetcher and as an adjunct
prefetcher to SPP. We make three major observations from Figure 12. First, as a standalone prefetcher, DSPatch outperforms SMS
0%
10%
20%
30%
40%
Client
Server
HPC
FSPEC06
ISPEC06
FSPEC17
ISPEC17
Cloud
SYSmark
GEOMEAN
Performance Delta 
over Baseline (%)
BOP SMS SPP DSPatch DSPatch+SPP
Figure 12: Single-thread performance results
by 3% on average across all workloads while requiring only less than
1/20th of the storage requirements of SMS. The use of dual modulated bit-patterns and multiple triggers helps DSPatch outperform
7We consider a workload to be high-MPKI if its LLC MPKI is greater than 5.
the traditional bit-pattern prefetching employed by SMS. Second,
DSPatch performs 1% better than SPP on average while requiring
only less than 2/3
r d of the storage of SPP. DSPatch’s gains over
SPP mainly come from the SYSmark, Cloud and ISPEC workload
categories. All other workload categories showcase the benefits
of the fine-grained delta-based prefetching paradigm employed by
SPP. Third, the combination of fine-grained delta-based prefetching
of SPP and dual modulated spatial bit-pattern-based prefetching of
DSPatch achieves 6% performance improvement over standalone
SPP on average, outperforming every standalone prefetcher in
all workload categories. This clearly makes the case for using
DSPatch as a lightweight adjunct prefetcher to SPP so that we can
extract the benefits of both prefetching paradigms.
Figure 13 shows the performance line graph for the set of 42
memory-intensive single-thread workloads. On average, the combined DSPatch+SPP prefetcher outperforms the standalone SPP
by 9%. DSPatch+SPP performs worse than SMS in only one TPC-C
workload, which has very large code footprint. With more than
4000 trigger PCs per kilo instructions, SMS benefits from its large
signature storage capability of 16K entries. DSPatch+SPP outperforms standalone SPP by 26% in NPB, by 20% in BigBench and by
16% in SYSMark-excel and mcf (ISPEC06) workloads.
-20%
0%
20%
40%
60%
80%
100%
Performance Delta over Baseline(%)
Workloads
SMS SPP DSPatch+SPP
602.gcc
429.mcf
NPB
BigBench
TPC-C
SYSmark-excel
Figure 13: Performance line graph for 42 memory-intensive
single-thread workloads.
Figure 14 shows the performance of BOP and a 256-entry SMS
(iso-storage with DSPatch) as adjunct prefetchers to SPP. We make
two observations from Figure 14. First, as an adjunct prefetcher
0%
10%
20%
30%
40%
Client
Server
HPC
FSPEC06
ISPEC06
FSPEC17
ISPEC17
Cloud
SYSmark
GEOMEAN
Performance Delta 
over Baseline (%)
SPP BOP+SPP SMS(iso_storage)+SPP DSPatch+SPP
Figure 14: Performance of BOP, 256 entry SMS and DSPatch
as adjunct prefetchers to SPP
to SPP, DSPatch provides much higher performance than BOP
and SMS with similar storage requirements. Second, DSPatch+SPP
outperforms BOP+SPP by 2.1%, mainly because DSPatch+SPP has
higher prefetch coverage than BOP+SPP (60% vs 55%).
539
MICRO-52, October 12–16, 2019, Columbus, OH, USA Bera and Nori, et al.
For comprehensiveness, we also evaluate DSPatch in conjunction with SPP and BOP together. DSPatch improves average performance by 2.6% on top of the SPP+BOP combination prefetcher on
average (not shown here). This clearly indicates non-overlapping
coverage opportunities between BOP and DSPatch, encouraging
further optimization and research in this direction.
5.2 Performance Scaling with Memory
Bandwidth
Figure 15 shows how the performance improvements of different
prefetchers scale as we scale the DRAM bandwidth from a single
channel DDR4-1600 (with 12.5GBps bandwidth) to dual channel
DDR4-2400 (with 38 GBps bandwidth).
10%
15%
20%
25%
30%
35%
40%
10 15 20 25 30 35 40
Performance Delta over Baseline(%) 
Peak DRAM Bandwidth (GBps)
BOP SMS SPP eBOP+SPP DSPatch+SPP
1ch-1600
1ch-2133
1ch-2400
2ch-1600
2ch-2133 2ch-2400
Figure 15: Performance scaling with DRAM bandwidth
Two key takeaways emerge from the data. First, the performance
of DSPatch+SPP scales well with increasing memory bandwidth,
growing from 6% over SPP to 10% when the memory bandwidth
is doubled, when going from the single channel DDR4-2133 system to the dual channel DDR4-2133 system. Second, as an adjunct
prefetcher to SPP, the performance gap between eBOP+SPP and
DSPatch+SPP increases with increase in memory bandwidth headroom, growing from 2.1% in the single channel DDR4-2133 system
to 5% in the dual channel DDR4-2400 system. We conclude that,
DSPatch, via its fundamental design choices, is best suited to extract
higher performance from higher memory bandwidth.
5.3 Impact On Coverage And Accuracy
Figure 16 quantifies the coverage and misprediction rates of the
evaluated prefetchers. On average, DSPatch+SPP has 15% higher
coverage than the standalone SPP prefetcher. This comes at a 6.5%
increase in the rate of mispredictions. Since DSPatch uses dual
modulated patterns simultaneously optimized for both coverage
and accuracy, we achieve a 2:1 ratio in the impact on coverage and
accuracy: a 2% increase in coverage comes at only a 1% increase in
mispredictions.
5.4 Multi-programmed Performance
Multiple cores competing for the DRAM bandwidth resource reduces the headroom for prefetchers to boost coverage and performance. The use of the accuracy-biased bit-pattern in DSPatch
plays a crucial role in such scenarios to generate highly accurate
prefetches to make the best use of the scarce DRAM bandwidth.
Figure 17 shows the performance improvement of all prefetchers
on 42 homogeneous workload mixes. We make two observations.
First, as an adjunct prefetcher to SPP, DSPatch improves performance by 5.9% over the standalone SPP. Second, even though SMS
outperforms the standalone SPP by 3.2% and 2.5% in the Cloud and
SYSmark workload categories, DSPatch+SPP outperforms SMS by
4% and 7.3% in these workload categories.
-10%
0%
10%
20%
30%
40%
50%
Client
Server
HPC
FSPEC06
FSPEC17
ISPEC06
ISPEC17
Cloud
SYSmark
GEOMEAN
Performance Delta
over Baseline (%)
BOP SMS SPP DSPatch+SPP
Figure 17: Multi-programmed performance results across 42
homogeneous workload mixes
Figure 18 compares the performance improvement of all prefetchers on homogeneous and heterogeneous workload mixes for two
different DRAM bandwidth configurations: dual channel DDR4 at
2133 MHz and 2400 MHz. We make two observations. First, in the
baseline system with two DDR4-2133 channels, DSPatch improves
average performance by 7.4% over the standalone SPP across all
0%
50%
100%
150%
BOP
SMS
SPP
DSPatch+SPP
BOP
SMS
SPP
DSPatch+SPP
BOP
SMS
SPP
DSPatch+SPP
BOP
SMS
SPP
DSPatch+SPP
BOP
SMS
SPP
DSPatch+SPP
BOP
SMS
SPP
DSPatch+SPP
BOP
SMS
SPP
DSPatch+SPP
BOP
SMS
SPP
DSPatch+SPP
BOP
SMS
SPP
DSPatch+SPP
BOP
SMS
SPP
DSPatch+SPP
Client Server HPC FSPEC06 ISPEC06 FSPEC17 ISPEC17 Cloud SYSmark AVG
Fraction of 
L2 accesses (%)
Covered Uncovered Mispredicted
Figure 16: Coverage and mispredictions of different prefetchers
540
DSPatch: Dual Spatial Pattern Prefetcher MICRO-52, October 12–16, 2019, Columbus, OH, USA
heterogeneous mixes. Second, increasing the DRAM channel frequency by 12.5% (i.e., from 2133 MHz to 2400MHz) also increases
performance by 7.7% and DSPatch+SPP outperforms the standalone
SPP by 15.1%. We conclude that DSPatch maintains the ability to
scale up performance based on available memory bandwidth even
when memory bandwidth is a scarce resource.
0%
10%
20%
30%
40%
Homogeneous Heterogeneous Homogeneous Heterogeneous
DDR4-2133 DDR4-2400
Performance Delta
over Baseline (%) 
BOP SMS SPP DSPatch+SPP
Figure 18: Performance improvement of prefetchers on homogeneous and heterogeneous multi-programmed workload mixes for two different DRAM bandwidths
5.5 Contribution Of Accuracy-biased Patterns
We study the performance impact and contribution of the accuracybiased predictions in the DSPatch design. In DSPatch, accuracybiased bit-patterns provide highly accurate predictions when memory bandwidth utilization is close to peak (greater than 75%). Figure 19 shows the full-blown DSPatch’s performance improvement
along with two other configurations of DSPatch that never useAccP
for prediction: (1) one DSPatch configuration (named AlwaysCovP)
always uses CovP for prediction, even when DRAM bandwidth
utilization is high and (2) another DSPatch configuration (named
ModCovP) dynamically throttles down CovP to restrict aggressiveness when DRAM bandwidth utilization is high. We make two
observations from the figure. First, always using CovP for prediction irrespective of the memory bandwidth utilization significantly
hurts performance. AlwaysCovP loses 4.5% performance over the
full-blown DSPatch. Second, throttling down CovP prediction at
high DRAM bandwidth utilization scenarios alone does not solve
the problem: ModCovP loses 1.4% performance over the full-blown
DSPatch. We conclude that statically selecting any single type of
bit-pattern is sub-optimal in performance and we need two modulated bit-patterns to enable a dynamic selection of the appropriate
bit-pattern at run-time.
10%
13%
15%
18%
20%
DSPatch AlwaysCovP ModCovP
Performance Delta 
over Baseline (%)
Figure 19: Performance improvement of the full-blown
DSPatch versus two other DSPatch variants that do not use
the accuracy-biased bit-pattern. The y-axis starts at 10%.
6 RELATED WORK
Prefetching is an extensively studied approach to hide high memory latency with a large body of work over decades covering a
wide range of algorithms and implementations. To our knowledge,
this is the first work to use two spatial bit-patterns and a memorybandwidth-driven dynamic selection of bit-patterns to achieve better scalability in performance with scaling in memory bandwidth.
We divide past prefetching solutions into three major categories:
pre-computation, temporal, and non-temporal prefetchers. We compare DSPatch with each of these categories, as well as prior prefetchthrottling mechanisms and highlight how DSPatch fundamentally
differs from them.
Pre-computation Prefetchers. One flavor of prefetching relies on pre-computation to hide latency. Examples include runahead execution [32, 41, 52, 66–68] and helper thread prefetching [28, 30, 63, 80, 84] proposals. The use of pre-computation makes
these proposals highly accurate with the ability to provide coverage even when no patterns exist in address accesses. However,
pre-computation prefetchers are higher in complexity compared
to light-weight hardware prefetchers that capture access patterns.
DSPatch, being a traditional prefetching proposal, differs completely
from pre-computation prefetchers and predicts future accesses only
by learning patterns in past accesses.
Temporal Prefetchers. Temporal prefetchers including STeMS
[81], ISB [46] and the Domino prefetcher [25] are built on the
Markov prefetching [50] model by tracking the temporal order of
full cache-line address accesses rather than address deltas or cacheline offsets in spatial regions. While tracking repeating patterns of
full cacheline addresses can be quite accurate, it has multi-megabyte
storage requirements, which necessitates storing meta-data in memory. DSPatch requires only 3.6 KB, which can easily fit inside a core.
Non-temporal Prefetchers. Prefetchers that predict deltas or
bit-patterns in a spatial region (like a 2KB or 4KB page) have significantly lower storage requirements and generally lower complexity
than pre-computation or temporal prefetchers. Stream [29, 51] and
stride [38] prefetchers capture simple repeating deltas. More recently, prefetching proposals that can capture more complex delta
patterns like a repeating series of deltas have emerged. We further
categorize these prefetchers into two broad groups.
(1) Delta-based Prefetchers. Delta-based prefetchers like VLDP
[76] and SPP [55] use a history of address deltas (inspired by the
TAGE [75] branch predictor) to predict future deltas . As we discussed and evaluated, SPP uses its prefetch confidence values to
recursively prefetch further ahead to improve timeliness. BOP [65]
uses a set of global deltas to capture a repeating series of smaller
deltas. We have already comprehensively compared DSPatch to
both SPP and BOP proposals in this work. Another prior proposal,
Kill-the-PC [56] co-designs both the prefetching and the cache replacement policy to be aware of each other. However, the prefetching component of KPC (called KPC-P) is identical to SPP. Our evaluation of the prefetching component of KPC showed no significant
improvement over SPP.
(2) Bit-pattern-based Prefetchers. Bit-pattern-based prefetchers, exemplified by SMS [77], use the PC as part of their signature
to predict bit-patterns. These prefetchers have higher storage requirements (of the order of many tens of KB) than delta-based
541
MICRO-52, October 12–16, 2019, Columbus, OH, USA Bera and Nori, et al.
prefetchers. Rotated bit-patterns were first used by Ferdman et
al. [37] to eliminate the cacheline offset in the spatial region (2KB)
from the signature and reduce storage requirements to around 40KB.
DSPatch further reduces storage by compressing the bit-pattern
where each bit represents one 128B block, instead of a single 64B
cacheline. A recent work, Bingo [26], extends bit-pattern-based
prefetching to use both long and short history events (again inspired by the TAGE [75] branch predictor). In addition to the offset
in the region along with the PC as part of the signature (a short
event in their terminology), Bingo also supports a long event signature utilizing the full cacheline address. Bingo fuses these signatures
into the same prediction table, enable multiple predictions from a
single entry for higher coverage than SMS. However, Bingo still
consumes over 100KB of area. DSPatch significantly simplifies bitpattern prefetching using a mere 3.6KB of storage with anchored
bit-patterns along with mechanisms to track and boost coverage
and accuracy for higher performance. DSPatch’s design choices fundamentally enable good performance scaling with higher memory
bandwidth as well.
Prefetch-throttling Mechanisms. The prefetcher throttling
mechanism plays a crucial role in any aggressive prefetcher design.
Multiple prior proposals [31, 34–36, 43, 58, 59, 70, 72, 74, 78, 83] take
prefetching metrics like coverage, accuracy and bandwidth consumption into consideration to selectively throttle or drop prefetch
requests in an attempt to reduce prefetcher-induced pollution in
cache capacity as well as in available memory bandwidth. DSPatch
also has inherently simple mechanisms to track prefetch accuracy
and coverage, along with the ability to scale performance with
increase in memory bandwidth. Even so, prior prefetch-throttling
proposals can be orthogonally applied to DSPatch as well to further
adjust its prefetch aggressiveness.
7 SUMMARY
We introduce DSPatch, a new spatial bit-pattern prefetcher that
uses memory bandwidth utilization inherently in its algorithm to
adjust prefetch aggressiveness and provide better scaling in performance improvement with increase in memory bandwidth. DSPatch
exploits two key ideas. First, it learns two spatial bit-patterns to
generate prefetches in a given memory region (i.e., a physical page)
by using simple logical OR and AND operations. One bit-pattern is
biased towards coverage and the other bit-pattern is biased towards
accuracy. Second, DSPatch dynamically selects any one bit-pattern
to generate prefetches at run-time based on the memory bandwidth
utilization and the coverage and accuracy of each bit-pattern. These
two ideas in unison help DSPatch to achieve better scaling in performance with increase in memory bandwidth than state-of-the-art
prefetchers. Our evaluations show that, using only 3.6 KB of hardware storage, DSPatch improves performance by 6%, on average
across 75 single-thread workloads, over an aggressive baseline with
a PC-based stride prefetcher at the L1 cache and the SPP prefetcher
at the L2 cache. DSPatch’s performance improvement grows from
6% to 10% when DRAM bandwidth is doubled. As memory bandwidth continues to increase with improvements in DRAM architecture and packaging, we believe that the next-generation processors
will significantly benefit from DSPatch’s ability to extract higher
performance in the presence of higher memory bandwidth.