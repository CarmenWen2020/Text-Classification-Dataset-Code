ABSTRACT
Important workloads, such as machine learning and graph analytics
applications, heavily involve sparse linear algebra operations. These
operations use sparse matrix compression as an effective means to
avoid storing zeros and performing unnecessary computation on
zero elements. However, compression techniques like Compressed
Sparse Row (CSR) that are widely used today introduce significant
instruction overhead and expensive pointer-chasing operations to
discover the positions of the non-zero elements. In this paper, we
identify the discovery of the positions (i.e., indexing) of non-zero
elements as a key bottleneck in sparse matrix-based workloads,
which greatly reduces the benefits of compression.
We propose SMASH, a hardware-software cooperative mechanism that enables highly-efficient indexing and storage of sparse
matrices. The key idea of SMASH is to explicitly enable the hardware to recognize and exploit sparsity in data. To this end, we devise
a novel software encoding based on a hierarchy of bitmaps. This
encoding can be used to efficiently compress any sparse matrix,
regardless of the extent and structure of sparsity. At the same time,
the bitmap encoding can be directly interpreted by the hardware.
We design a lightweight hardware unit, the Bitmap Management
Unit (BMU), that buffers and scans the bitmap hierarchy to perform highly-efficient indexing of sparse matrices. SMASH exposes
an expressive and rich ISA to communicate with the BMU, which
enables its use in accelerating any sparse matrix computation.
We demonstrate the benefits of SMASH on four use cases that
include sparse matrix kernels and graph analytics applications. Our
evaluations show that SMASH provides average performance improvements of 38% for Sparse Matrix Vector Multiplication and 44%
for Sparse Matrix Matrix Multiplication, over a state-of-the-art CSR
implementation, on a wide variety of matrices with different characteristics. SMASH incurs a very modest hardware area overhead
of up to 0.076% of an out-of-order CPU core.
KEYWORDS
sparse matrices, compression, hardware-software cooperation, accelerators, memory, efficiency, specialized architectures, linear algebra, graph processing

1 INTRODUCTION
Sparse linear algebra operations are widely used in modern applications like recommender systems [33, 49, 61], neural networks [29,
50], graph analytics [8, 12], and high-performance computing [10,
20, 24, 25, 34]. The matrices involved in these operations are very
large in size and highly sparse, i.e., the vast majority of the elements
are zeros. For example, the matrices that represent Facebookâ€™s and
YouTubeâ€™s social network connectivity contain 0.0003% [75] and
2.31% [45] non-zero elements, respectively. These highly sparse
matrices lead to significant inefficiencies in both storage and computation. First, they require an unnecessarily large amount of storage space, which is largely occupied by zeros. Second, computation
on highly sparse matrices involves a large number of unnecessary
operations (such as additions and multiplications) on zero elements.
The traditional solution to these inefficiencies is to compress the
matrix and store only the non-zero elements, and then operate only
on the non-zero values.
Prior works take two major approaches to designing such compression schemes. The first approach is to devise general compression formats or encodings [38, 44, 53, 68, 72, 86, 89, 93], such as
CSR [53], COO [72], BCSR [38], and CSR5 [53]. Such formats essentially store the non-zero elements and their positions within
the matrix using additional data structures and different encodings.
Such encodings are general in applicability and are highly-efficient
in storage, with high compression ratios. However, this approach involves repositioning and packing the non-zero values in the matrix,
which leads to significant computation overheads that diminish the
overall benefit. Determining the positions of the non-zero elements
in the compressed encoding (i.e., indexing) requires a series of
pointer-chasing operations in memory that, as we demonstrate, are
highly inefficient in modern processors and memory hierarchies.
The second approach taken by prior work is to leverage a certain known structure in a given type of sparse matrix to avoid
the cost of discovering the non-zero regions of the sparse matrix [7, 16, 30, 41, 42, 46, 77]. For example, the DIA format [7]
is highly effective in matrices where the non-zero values are concentrated along the diagonals of the matrix. Specializing the compression scheme to patterns in the sparsity can be efficient in both
computation and storage but it is highly specific to certain types of
600
MICRO-52, October 12â€“16, 2019, Columbus, OH, USA Kanellopoulos et al.
matrices and inapplicable when the structure and extent of sparsity
are not known a priori.
Our goal in this work is to enable efficient and general sparse
matrix computation with a technique that satisfies three major requirements: 1) high computation and storage efficiency by storing
and operating on only non-zero elements; 2) minimal overheads
from the compression scheme (e.g., efficient discovery of non-zero
elements) and 3) generality and applicability to any sparse matrix,
regardless of its structure or the extent of its sparsity.
Our key idea is a new hardware-software co-design where we
enable the hardware to recognize and exploit the compression encoding used in software for any sparse matrix. This allows us to
add hardware support for highly-efficient storage and retrieval of
non-zero values in sparse matrices, avoiding the overheads of software indexing (requirement 2). Our software encoding is designed
to maintain low storage requirements (requirement 1) and be generally applicable to any sparse matrix without any assumption of
structure or extent of sparsity (requirement 3).
We propose SMASH (Sparse MAtrix Software/Hardware), a general hardware-software cooperative mechanism that efficiently compresses and operates on sparse matrices. The key construct behind
SMASH is the use of efficiently encoded hierarchical bitmaps to
express sparsity, where each bit represents a region of non-zero values. These bitmaps are recognized by both hardware and software.
On the software side, sparse matrices of any form are flexibly encoded using our hierarchical bitmap representation. SMASH adapts
to each matrixâ€™s sparsity characteristics by supporting multiple
compression granularities throughout the bitmap hierarchy. On
the hardware side, the bitmap representation allows us to use a
lightweight hardware unit, the Bitmap Management Unit (BMU),
to perform highly-efficient scans of the bitmap hierarchy. The BMU
hardware enables low-cost indexing (that avoids expensive pointerchasing lookups) and efficient sparse matrix computation.
To enable wide applicability, SMASH exposes five new instructions that enable the software to communicate with the Bitmap
Management Unit. The new instructions enable efficient lookup of
non-zero matrix regions, and are sufficiently rich to express a wide
variety of operations on any type of (sparse) matrix.
We evaluate SMASH on four use cases: two sparse matrix kernels,
Sparse Matrix Vector Multiplication (SpMV) and Sparse MatrixMatrix Multiplication (SpMM), as well as two graph analytics applications, PageRank and Betweenness Centrality (BC). For our
experiments, we use a collection of sparse matrices with varying
structure and sparsity characteristics [19]. We compare SMASH
to two state-of-the-art compression formats, CSR [53] and BCSR
[38]. We find that SMASH improves average performance by 41.5%
for SpMV and SpMM, across 15 matrices and by 20% for PageRank
and BC, compared to a state-of-the-art CSR implementation [40].
We also compare the software-only version of SMASH against two
state-of-the-art sparse matrix frameworks [1, 40] on a real system.
We find that even with no hardware support, SMASHâ€™s bitmap
encoding outperforms a state-of-the-art CSR implementation [40].
In this paper, we make the following key contributions:
â€¢ We show that discovering the positions of non-zero elements
(indexing) is a key bottleneck in sparse matrix computation.
We demonstrate that efficient indexing can boost the performance of sparse matrix operations significantly.
â€¢ We introduce SMASH, a hardware-software cooperative
mechanism that enables the hardware to recognize and exploit the compression encoding used in software. SMASH
consists of 1) a novel software encoding that uses a hierarchy of bitmaps to efficiently compress sparse matrices and
2) hardware support to enable highly-efficient indexing of
sparse matrices that are compressed using SMASHâ€™s software encoding.
â€¢ We show how SMASH can efficiently compress sparse matrices with diverse structure and sparsity characteristics using
the hierarchical bitmap encoding. We design and demonstrate the effectiveness of the Bitmap Management Unit
(BMU) that efficiently buffers and scans the bitmap hierarchy in hardware to identify non-zero regions in the matrix.
We introduce an expressive ISA that enables the flexible use
of SMASH in a wide variety of sparse matrix operations.
â€¢ We evaluate SMASH on important sparse matrix kernels and
graph analytics applications using a collection of matrices
with diverse structure and sparsity. We find that SMASH
provides significant performance improvements compared
to state-of-the-art CSR implementations while incurring a
very modest area overhead in a modern out-of-order CPU.
2 MOTIVATION
Sparse matrix operations are widely used in a variety of applications
including sparse linear algebra [39, 69], graph processing [8, 12],
convolutional neural networks (CNNs) [50], and machine learning (ML) [33, 49, 61, 97]. These applications involve matrices with
very high sparsity, i.e., a large fraction of zero elements. Using a
compression scheme is a straightforward approach to avoid unnecessarily 1) storing zero elements and 2) performing computations
on them. To this end, a variety of sparse matrix representation
formats (e.g., [38, 44, 53, 68, 72, 86, 89, 93]) have been proposed to
compress the sparse matrix. The most widely used state-of-the-art
format is Compressed Sparse Row (CSR) [53]. In this section, we
describe the CSR format and demonstrate its inefficiency.
2.1 Compressed Storage Formats
The Compressed Sparse Row (CSR) format [53] is widely used in
many libraries that involve sparse matrix operations [1, 23, 40, 87,
92]. It consists of three one-dimensional arrays: row_ptr, col_ind,
and values. Given an M Ã— N matrix, the row_ptr array is used
to store (and determine) the number of non-zero elements per row;
the col_ind array indicates the column indices of the non-zero elements; and the values array stores the values of only the non-zero
elements. Discovering the position of a non-zero element in row i
requires streaming through col_ind from col_ind[row_ptr[i]]
up to col_ind[row_ptr[i+1]] to discover its column index in the
row. Figure 1 illustrates an example of a compressed matrix using
the CSR format. In this example, in order to discover the non-zero
elements of row 1 (i.e., second row from the top) of A, we search in
col_ind starting from index col_ind[row_ptr[1] == 1] up to
col_ind[row_ptr[2] == 3].
A variant of CSR is Compressed Sparse Column (CSC). CSC
stores the elements in column-major order instead of row-major.
The col_ptr array is used to store (and determine) the number of
non-zero elements per column, the row_ind array holds the row
indices of the non-zero elements, and the values array stores the
values of the non-zero elements themselves.
601
SMASH: Co-designing Software Compression and Hardware-Accelerated Indexing
for Efficient Sparse Matrix Operations MICRO-52, October 12â€“16, 2019, Columbus, OH, USA
A =
row_ptr:
col_ind:
values:
0 1 3 4 6
0 0 2 3 0 1
3.2 0.0 0.0 0.0
1.2 0.0 4.2 0.0
0.0 0.0 0.0 5.1
5.3 3.3 0.0 0.0 3.2 1.2 4.2 5.1 5.3 3.3
Figure 1: Compressed Sparse Row format for a 4 Ã— 4 matrix
with 6 non-zero elements. We count the number of non-zero
elements of row i by computing (row_ptr[i +1]-row_ptr[i]).
col_ind holds the column index of each non-zero element.
CSR significantly reduces the amount of memory needed to
store a sparse matrix, especially when the matrix is large and its
sparsity is high. However, CSR and schemes with CSR-like structures [53, 72] have one major requirement: in order to determine
where the non-zero elements are located in the original matrix,
the corresponding indices need to be retrieved from the row_ptr
and col_ind data structures. Accessing these data structures adds
many additional instructions and requires a series of indirect datadependent memory accesses. These overheads reduce the benefits
of avoiding the computation on zero elements. Hence, even though
CSR-like formats reduce storage requirements and avoid needless
computation, discovering the positions of the non-zero elements
still is an unsolved challenge that causes performance and efficiency
overheads.
2.1.1 Sparse Matrix Vector Multiplication (SpMV). We consider
the SpMV kernel y := y + Ax, where A is a sparse matrix, x is a
dense vector, and y is the output vector. The naive 2D implementation of the SpMV kernel involves performing computations on
every element of the two-dimensional matrix A and incurs high
computational and storage overheads. Code Listing 1 presents the
CSR-based implementation. In this case, the algorithm iterates over
only the non-zero elements and avoids unnecessary zero-element
computations. However, it introduces a pointer-chasing operation
when col_ind is loaded and then used as an index to load the
appropriate element of the vector (x[col_ind[j]]). Only after
this complex indexing operation can we perform the multiplication
with the corresponding non-zero element in matrix A (values[j]),
as shown in Line 3 of Code Listing 1.
1. for (i = 0; i < N; i++)
2. for (j = row_ptr[i]; j < row_ptr[i+1]; j++)
3. y[i] += values[j] * x[col_ind[j]]
Code Listing 1: CSR-based SpMV implementation. The column index of each element is needed to perform the multiplication with the x vector.
2.1.2 Sparse Matrix Matrix Multiplication (SpMM). SpMM is traditionally performed using inner product multiplication [66]. This
results in a series of dot product operations between each row
of the first matrix (A) and each column of the second matrix (B)
to produce the final elements of the result matrix (C). The naive
O(n
3
) SpMM implementation is prohibitively expensive due to the
high number of unnecessary computation operations on zero elements. A CSR-based implementation of SpMM, shown in Code
Listing 2, avoids such unnecessary computations. In SpMM, matrix A is compressed using CSR and matrix B using CSC. SpMM
iterates over the rows of A and columns of B (Lines 1-2 in Code
Listing 2). For each non-zero element in each row of A, we need
to search through col_ind of matrix A and row_ind of matrix B
to discover which elements should be multiplied during each dot
product. This process is called index matching (Lines 4-6 in Code
Listing 2). Figure 2 presents an example of index matching. We need
to match the positions of the non-zero elements of matrix A at row
0 and the non-zero elements of matrix B at column 0 to perform the
dot product correctly. Given that index matching is performed for
every dot product operation, a CSR-based SpMM implementation
requires a large number of position-finding operations for non-zero
elements and thus the indexing mechanism plays a critical role in
performance and efficiency.
1. for each row of A
2. for each column of B
3. for each non-zero element in row of A
4. k1 = search_in_col_ind_of_A()
5. k2 = search_for_row_ind_of_B()
6. if (k1 == k2)
7. y[i][j] += a_val[k1] * b_val[k2]
Code Listing 2: CSR-based inner-product SpMM implementation. Index matching (Lines 4-6) is needed to perform the
multiplication of Aâ€™s rows and Bâ€™s columns (line 7).
A C E B
D
0
0
Z
Row 0
A B
C D
A C E
0 1 2
B D Z
col_ind 0 1 3
values
row_ind
values
Col 0
values A B C D
Matrix A Matrix B Matrix C
Indices match
Figure 2: Index matching in SpMM. We search col_ind of matrix A and row_ind of matrix B to find indices that match.
Only the indices of the first two elements of Aâ€™s Row 0 and
Bâ€™s Column 0 match. The remaining two elements in Aâ€™s
Row 0 or Bâ€™s Column 0 have at least one zero element in
them and their indices do not match.
2.2 Limitations of Existing Compressed
Storage Formats
Compressed storage formats, such as CSR [53], BCSR [38], and
CSR5 [53], are effective at reducing the storage area and avoiding redundant computations on the zero elements of the matrix.
However, as described above, they require additional computation
and indirect memory accesses to find the indices, i.e., the row and
column positions, of non-zero elements. This increases the computation burden and memory traffic, and hence lowers the potential
gains from the compression scheme.
To understand the impact of this indexing overhead on sparse
matrix processing, we conduct an experiment where we compare
a state-of-the-art CSR implementation to an idealized version in
which accessing the positions of non-zero elements does not incur
any additional computation or memory access. Figure 3 shows the
602
MICRO-52, October 12â€“16, 2019, Columbus, OH, USA Kanellopoulos et al.




  








	
 
	

Figure 3: Speedup and normalized number of executed instructions of an ideal indexing scheme over the baseline
CSR, averaged across 15 sparse matrices for Sparse Matrix
Addition, SpMV, and SpMM (see Section 6 for methodology).
speedup and executed instruction count of this idealized CSR over
the regular state-of-the-art CSR. As shown in the figure, eliminating the indexing part of the CSR format significantly improves
performance: 2.21Ã— for Sparse Matrix Addition, 2.13Ã— for SpMV,
and 2.81Ã— for SpMM. These performance benefits come from the
reduced number of executed instructions (by 49%, 42%, 65%, respectively) and from eliminating the expensive pointer-chasing lookups
in memory.
2.3 Other Approaches to Sparse Matrix
Compression
Other approaches [7, 16, 30, 41, 42, 46, 77] aim to maximize the
efficiency of sparse matrix computations by specializing to particular matrix types and thus trading off generality. These approaches
assume and leverage a specific matrix structure or known pattern.
Saad et al. [70] assume a diagonal matrix and base the compression scheme around the assumption that all non-zero elements are
only along the matrix diagonal. Kourtis et al. [41] assume matrices
with few unique non-zero elements in designing the compression
scheme. As a result, these approaches are not applicable to a wide
range of important classes of applications like CNNs and graph
processing algorithms, where such assumptions do not hold.
3 SMASH : DESIGN OVERVIEW
We introduce SMASH, a hardware-software cooperative mechanism
that 1) enables highly-compressed storage and avoids unnecessary
computation; 2) significantly reduces the overheads imposed by the
compression scheme itself (i.e., enables efficient discovery of the
positions of non-zero elements); and 3) can be used generally across
a diverse set of sparse matrices and sparse matrix operations.
The key idea of SMASH is to enable the hardware to recognize
and exploit the compression encoding used in software. We devise
a new construct, recognized by both the hardware and software, to
compress sparse matrices efficiently: a hierarchy of bitmaps. Each
bitmap in the hierarchy efficiently encodes sparsity by denoting
the presence/absence of non-zero values in a matrix region using
a single bit. The size of the region varies with the level of the
bitmap in the hierarchy and can be adjusted by software. This
representation does not assume any structure in the matrix and can
be used to efficiently compress a diverse set of sparse matrices. At
the same time, as we demonstrate, it enables designing hardware
that can exploit the known sparsity in data and hence perform
highly-efficient indexing of sparse matrices.
3.1 Design Challenges
Designing SMASH involves addressing two major challenges:
Challenge 1: Efficiently Encoding Bitmaps. Representing
each element in a matrix with a single bit to denote sparsity is
highly-inefficient in terms of storage and computation. Hence, we
need a more efficient bitmap representation that is effective regardless of the matrix sparsity and the location/distribution of the
non-zero values. At the same time, hardware should be able to
flexibly interpret and leverage this bitmap encoding.
Challenge 2: Flexibility and Expressiveness. To express a
diverse set of sparse matrix operations in any application, we need
a rich cross-layer interface between the application and the underlying hardware that 1) allows the software to flexibly manipulate
and index sparse matrices encoded using our hierarchical bitmap
encoding and 2) enables hardware to easily interpret the sparse
matrix operations in the application and effectively accelerate those
operations.
3.2 SMASH: Key Components
Hierarchy of Bitmaps. To address Challenge 1, we represent
the positions of the non-zero values in any sparse matrix with a
hierarchy of bitmaps. Our system is designed to support a certain
maximum number of levels of the hierarchy. Each level of the hierarchy encodes the presence of non-zero values with a configurable
compression ratio. This compression ratio is determined by the
software based on the sparsity and distribution of non-zero values
in a given matrix. With this representation, a zero matrix would
require only one bit and one level of the bitmap encoding.
In Figure 4, we show an example with a 3-level bitmap hierarchy.
Each bit in Bitmap-2 encodes the presence of non-zero values in
two consecutive regions in Bitmap-1 (hence, the compression ratio
at this level is two). Bitmap-1, on the other hand, encodes the presence of non-zero values in four consecutive regions in Bitmap-0
(hence, the compression ratio at this level is four). The selection of
compression ratio at any level is a tradeoff between computation
efficiency and storage efficiency. With a higher compression ratio,
we store fewer bits to encode the presence/absence of non-zero
elements but may perform unnecessary computations on zero elements. With a lower compression ratio, on the other hand, we can
compute only on non-zero elements, but this would incur a higher
storage overhead.
The non-zero elements of the matrix are kept in a data structure called the Non-Zero Values Array (NZA). The granularity
at which they are stored in memory depends on the compression
ratio of the lowest level of the bitmap hierarchy (Bitmap-0). As
we show in the example bitmap hierarchy of Figure 4, Bitmap-0
encodes the 4-element non-zero blocks of the NZA using a single
bit (i.e., the compression ratio is 4:1).
Our hierarchical bitmap compression mechanism enables efficient representation of matrices with varying degrees of sparsity
and varying distributions of non-zero elements by adjusting the
bitmap representation granularity (i.e., compression ratios at all
levels of the bitmap hierarchy). It can also be flexibly and efficiently
interpreted by hardware, as we describe next.
Bitmap Management Unit (BMU). We design a new hardware unit that buffers and efficiently scans the bitmap hierarchy to
quickly find the non-zero elements. The BMU is responsible for efficiently and quickly calculating the indices of the non-zero region