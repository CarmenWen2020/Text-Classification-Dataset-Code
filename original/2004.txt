ABSTRACT
Graph analytics is an emerging application which extracts insights
by processing large volumes of highly connected data, namely
graphs. The parallel processing of graphs has been exploited at the
algorithm level, which in turn incurs three irregularities onto computing and memory patterns that significantly hinder an efficient
architecture design. Certain irregularities can be partially tackled
by the prior domain-specific accelerator designs with well-designed
scheduling of data access, while others remain unsolved.
Unlike prior efforts, we fully alleviate these irregularities at their
origin—the data-dependent program behavior. To achieve this goal,
we propose GraphDynS, a hardware/software co-design with decoupled datapath and data-aware dynamic scheduling. Aware of data
dependencies extracted from the decoupled datapath, GraphDynS
can elaborately schedule the program on-the-fly to maximize parallelism. To extract data dependencies at runtime, we propose a
new programming model in synergy with a microarchitecture design that supports datapath decoupling. Through data dependency
information, we present several data-aware strategies to dynamically schedule workloads, data accesses, and computations. Overall,
GraphDynS achieves 4.4× speedup and 11.6× less energy on average with half the memory bandwidth compared to a state-of-the-art
GPGPU-based solution. Compared to a state-of-the-art graph analytics accelerator, GraphDynS also achieves 1.9× speedup and 1.8×
less energy on average using the same memory bandwidth.
CCS CONCEPTS
• Hardware → Application specific integrated circuits.
KEYWORDS
graph analytics, accelerator, software and hardware co-design

1 INTRODUCTION
In the big data era, graphs are used as effective representations
of data in many scenarios. Graph analytics can mine valuable insights from big data for a variety of applications, such as social
network analysis [30, 35, 38], cybersecurity analysis [36], knowledge graphs [23], autonomous vehicles [47], brain science [8], web
search [48], and robot navigation [19].
Although graph analytics inherently possesses a high-degree
parallelism, it is difficult to achieve practical acceleration due to the
irregularity challenges [16, 17, 32]. High-degree parallelism exists
since millions of vertices and hundreds of millions of edges can be
processed simultaneously via the popular Vertex-Centric Programming Model (VCPM) which is widely used by recent work [1, 21,
24, 34, 42]. However, due to the data-dependent program behavior [16, 29, 32] of graph algorithms, existing architectures face several challenges. Specifically, imbalanced workloads, large amounts
of random memory accesses across diverse memory regions, and
redundant computations result from the following three types of
irregularities. (1) Workload irregularity: The workloads among
threads are significantly imbalanced [21, 42]. VCPM partitions the
program into threads, where different active vertices are attributed to distinct threads. Consequently, various number of edges
are processed by each thread, as each vertex is usually associated
with a different number of edges in the graph [10, 40, 42, 46]. (2)
Traversal irregularity: Edges are traversed irregularly in each
iteration [24, 25, 33, 39, 56]. This is caused by the different active
vertices among iterations and the irregular connections between
active vertices and their neighbors. The unpredictable traversal of
the edges during each iteration leads to two critical difficulties. First,
it introduces a large number of random memory accesses due to
the poor locality. Second, it incurs long-latency atomic operations
when multiple edges update the same vertex [33, 39, 41, 55]. (3) Update irregularity: Both the update of the vertex property and the
activation of vertex vary iteration-by-iteration [46]. Although few
vertices are actually updated and activated, the program needs to
check all of them. This update irregularity leads to a large amount
of unnecessary computations and memory accesses.
Table 1: Our work solves all three types of irregularities.
Irregularity GPU-based solutions Graphicionado Our work
Workload Expensive
preprocessing
✘ ✔
Traversal inefficient ✔
Update ✘ ✔
615
MICRO-52, October 12–16, 2019, Columbus, OH, USA Mingyu Yan et al.
Due to the aforementioned irregularities, GPU-based solutions
suffer from workload imbalance, memory divergence, high synchronization overhead, and branch divergence[10, 26, 33, 40, 46, 54].
Most GPU-based solutions rely on preprocessing to tackle these
irregularities [20, 26, 27, 33, 42, 52]. However, the preprocessing is
costly. Unless multiple applications run on the same static graph
repeatedly, the preprocessing overhead usually offsets its benefits.
Recently, domain-specific hardware has been proposed to partially
address the irregularity challenges of graph analytics. For example,
Graphicionado [24], a state-of-the-art graph analytics accelerator,
stores almost all the data, as long as it has a random access pattern,
in a large on-chip scratchpad memory to mitigate the traversal irregularity. Therefore, it achieves 1.76-6.54× speedup while consuming
50-100× less energy compared to a state-of-the-art software graph
analytics framework. However, as shown in Table 1, workload and
update irregularities remain unsolved.
In fact, above irregularities are caused by the data-dependent
program behavior. It raises a critical question in graph analytics:
could we schedule the program by considering the data dependency
in order to tackle these irregularities? Inspired by the question,
this work introduces GraphDynS—a hardware/software co-design
with decoupled datapath and data-aware dynamic scheduling.
GraphDynS can alleviate all three types of irregularities in graph analytics at its origin. Decoupled datapath is used to extract the data
dependency in microarchitecture level. A Dispatching/Processing
programming model is proposed to extract the workload size as
well as the exact prefetching indication of the coming accessed data.
Moreover, the execution flow is decoupled into two pipelined stages.
Along with the proposed programming model, a microarchitecture
design is also proposed to facilitate the decoupling of microarchitecture datapath. Data-aware dynamic scheduling is used to
schedule the program on-the-fly by considering the data dependency. To address the workload irregularity, we dynamically dispatch
workloads to processing elements in a balanced manner with knowledge of the precalculated workload sizes. To mitigate the traversal
irregularity, we perform an exact prefetching to prefetch graph data
with knowledge of the exact prefetching indication. Furthermore,
we propose a store-reduce mechanism with a microarchitectural
pipeline, which dynamically schedules the read-after-write data access for eliminating any stalls caused by atomicity. As for the update
irregularity, we maintain a bitmap to record the ready-to-update
vertices with the indication from the proposed pipeline. During the
update, only the marked vertices are scheduled to compute.
Our contributions are summarized as follows:
• We propose Dispatching/Processing programming model and the
accelerator architecture to decouple the microarchitecture datapath for data dependency extraction at runtime.
• We propose data-aware dynamic scheduling considering data
dependencies, which elaborately schedules program on the fly,
effectively tackling all three types of irregularities.
• We implement GraphDynS in RTL and evaluate it using a detailed
microarchitectural simulation. Our comprehensive evaluations
involve five well-known graph analytics algorithms with six
large real-world graphs and five synthetic graphs. Compared to
a state-of-the-art GPGPU-based solution, GraphDynS achieves
4.4× speedup and 11.6× less energy on average with half the
memory bandwidth. Compared to a state-of-the-art graph analytics accelerator, GraphDynS achieves 1.9× speedup and 1.8×
less energy on average with the same memory bandwidth.
2 BACKGROUND
In this section, we introduce common graph representation and
programming models for graph analytics algorithms.
2.1 Graph Representation
Compressed sparse row (CSR) is an efficient and popular format
for storing graphs, widely used by various software frameworks
[22, 43, 49, 51] and graph accelerators [1, 5, 24, 39] due to its efficient
usage of memory storage. As shown in Fig. 1, CSR format represents
graphs by three one-dimensional arrays: offset, edge, and vertex
property. The offset array stores the offset pointing to the start
of the edge list for each vertex in the edge array. The edge array
successively stores outgoing edges (i.e., neighbour IDs and weights
in the case of weighted graphs) of all vertices. The vertex property
array stores the property value of every vertex. The offset and
vertex property arrays are indexed by the vertex ID, while the edge
array is indexed by the offset.
Vertex ID: 6 245 4228
3 99
Offset array
Edge array
(Neighbour IDs)
Vertex prop
array
Random access to offset
Random access to first edge
Random access to vertex prop
(a)
(b)
Read after write
conflict
6838
6 245 4228
3
99 126
3232
1 2 3 99
6838
Vertex ID:
6838 3232 6838
6838
Figure 1: CSR representation. (a) Graph example. (b) CSR format and random data accesses.
2.2 Graph Programming Model
Deriving from the scattered structure of graphs, millions of vertices
and ten millions edges are processed simultaneously and iteratively.
In order to exploit the parallelism in graph analytics, Vertex-Centric
Programming Model (VCPM) was first proposed by the Google
Pregel system [34]. With advantages of simplicity, high scalability,
and good expressiveness, VCPM has been widely adopted by various
software frameworks [20, 21, 27, 31] and graph accelerators [1, 5,
24, 39, 56]. The push-based implementation of VCPM (PB-VCPM)
is shown in Algorithm 1, which consists of two alternately running
phases Scatter and Apply. First, in the Scatter phase, outgoing edges
of each active vertex are traversed to update the temporary vertex
property v.tProp of its destination vertex with application-defined
Process_Edge and Reduce functions. Second, in the Apply phase,
Apply function is executed with the constant vertex property v.cProp
and v.tProp. Then, if the result of the Apply function applyRes is not
equal to the vertex property v.prop, v.prop is updated and the vertex
is activated. Finally, the above process executes iteratively until
no more vertex is activated or a maximum number of iterations
is reached. The application-defined functions in PB-VCPM of five
well-known graph analytics algorithms, i.e., Breadth-First Search
(BFS), Single Source Shortest Path (SSSP), Connected Components
616
Alleviating Irregularity in Graph Analytics Acceleration: a Hardware/Software Co-Design Approach MICRO-52, October 12–16, 2019, Columbus, OH, USA
(CC), Single Source Widest Path (SSWP), and PageRank (PR) are
shown in Table 2.
Algorithm 1: Vertex-Centric Programming Model
◁ Scatter Phase
1 for each act ive ver tex u do
2 offset ← OffsetArray[u.vid];
3 while EdдeAr r ay[of f set].sr c_vid == u.vid do
4 e(u,v) ← EdgeArray[offset++];
5 edgeProResult ← Process_Edge(u.prop, e.weight);
6 v.tProp ← Reduce(v.tProp, edgeProResult);
7 end
8 end
◁ Apply Phase
9 for each ver tex v do
10 applyRes ← Apply(v.prop, v.tProp, v.cProp);
11 if v.prop!=applyRes then
12 v.prop ← applyRes;
13 activate vertex with v.vid and v.prop;
14 end
15 end
Table 2: Application-defined functions. Edge e=(u, v), source
vertex u and destination vertex v.
Algorithm Process_Edge Reduce Apply
BFS u.prop + 1 min(v.t P rop, r es) min(v.prop, v.t P rop)
SSSP u.prop + e .we iдht min(v.t P rop, r es) min(v.prop, v.t P rop)
CC u.prop min(v.t P rop, r es) min(v.prop, v.t P rop)
SSWP min(u.prop, e .we iдht) max(v.t P rop, r es) max(v.prop, v.t P rop)
PR u.prop v.t P rop + r es (α + β · v.t P rop)/v.deд
Note: The r es and v.d eд represent edдe P r oResu l t and v.c P r op respetively. The α and β are constant.
3 MOTIVATION
In this section, we motivate our approach by identifying irregularities in graph analytics and limitations of prior work.
3.1 Challenges of Graph Analytics
Although VCPM helps exploit the parallelism of graph analytics, it
incurs significant irregular computing and memory patterns, both
of which raise the difficulty for designing efficient architecture. As
mentioned above, the irregularities can be categorized into three
classes: 1) Workload irregularity, 2) Traversal irregularity, and 3)
Update irregularity.
Workload irregularity. The active vertices in each iteration
usually have different degrees, leading to a varying number of
edges to process. As shown in Fig. 2, the degree distribution of active
vertices within an iteration can vary significantly. For example, in
each iteration shown in Fig. 2, the degree of active vertices can
vary from 1 to over 64. Since VCPM partitions and distributes
workloads to distinct threads based on active vertices, the size of
each workload varies significantly among different threads. Such
workload irregularity can degrade GPU utilization by up to 25.3% -
39.4% for commonly used graph analytics algorithms [27, 42].
Traversal irregularity. Edges are traversed irregularly in each
iteration due to the diverse connections in the graph. Pointer chasing during traversal introduces abundant random memory accesses,
which not only raises challenges for efficient prefetching, but also
possibly incurs read-after-write (RAW) conflicts. As shown in Fig. 1(b),
random accesses include accesses to offset, first edge, and vertex
property. Such random memory access results in a limited cache
efficiency and higher bandwidth demands [1, 3, 4, 18, 46]. For example, the hit rate of L2 cache is only 10% for graph traversal
workloads in CPU [4]. Moreover, since threads may share vertices,
long-latency atomic operations are adopted to avoid thread contention when multiple threads modify the same vertex property.
Previous study [33] shows that atomic operations cause a slow
down of up to 32.15×.
Update irregularity. During runtime, vertices are irregularly
updated and activated across iterations [46]. Although the number of vertices which are updated and activated may actually be
few, all vertices need to be checked during the program execution.
Therefore, unnecessary computation and memory accesses are introduced by such irregularity. As shown in Fig. 2 with the update
line, 76% of iterations only update 10% of vertices. Up to 50% of iterations update less than 100 vertices while the graph has up to one
million vertices. According to our evaluation, unnecessary memory
access takes up to 50% of total memory accesses and results in 20%
performance overhead in Graphicionado [24].
0
10
20
30
40
50
0
10
20
30
40
50
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Iteration Number
[0,0] [1,2] [3,4] [5,8] [9,16]
[17,32] [33,64] 64< #Update
Number of Active Vertex
in Different Degree Interval (x10000)
Number of Vertex Updat
e (x10000)
Figure 2: The number of active vertices within different degree intervals and the number of vertex updates on Flickr
dataset as iteration goes (SSSP workload).
3.2 Limitation of State-of-the-Art
Graphicionado [24] partially addresses the traversal irregularity via
an on-chip buffer to improve performance and energy efficiency.
However, the overhead of atomics within the traversal irregularity,
workload irregularity, and update irregularity have not yet been
mentioned in their work. First, the workload irregularity leads
to workload imbalance within pipeline. Because active vertices
possess more than ten neighbors on average, the back-end of the
pipeline has 10× more workload than the front-end. Moreover,
hash-based (i.e., random) workloads allocated to different pipelines
lead to only half of the pipelines experiencing workloads most of
the time. Second, Graphicionado enforces atomicity by stalling the
pipeline when contention is detected. These stalls cause up to 20%
additional execution time. Third, update irregularity results in 20%
additional execution time and 40% additional energy consumption.
In fact, these irregularities result from the data dependent program
behavior, which relies on intermediate results within and across
iterations. Therefore, we propose a hardware/software co-design
617
MICRO-52, October 12–16, 2019, Columbus, OH, USA Mingyu Yan et al.
to completely tackle these irregularities by data dependency aware
scheduling on the fly.
4 GRAPHDYNS ARCHITECTURE
In light of the overhead introduced by the workload, traversal, and
update irregularities, we propose a hardware/software co-design
including optimized programming model, hardware accelerator, and
data-aware dynamic scheduling strategies to address such issues.
As shown in Fig. 3(a), the optimized programming model allows
GraphDynS to enable the hardware design datapath decoupling and
to endow more visibility for the scheduling. Our hardware design
helps to extract runtime data dependencies. The dynamic scheduling fully utilizes the knowledge of data dependency to schedule
workloads, data accesses and updates.
4.1 Optimized Programming Model
We optimize the programming model based on VCPM for better
practicality, since VCPM has been adopted by many recent graph
processing frameworks and accelerator designs. Note that our programming model shares the same programmability with VCPM
and our optimizations are transparent to users. The optimizations
include: 1) determining the workload size during execution for
scheduling a balanced workload, 2) obtaining the prefetching indication during execution for exact prefetching, and 3) decoupling
the phases into two pipeline stages to overlap workload scheduling
and execution.
4.1.1 Dynamically Determining Workload Size. To obviate the workload imbalance overhead introduced by workload irregularity, we
optimize the programming model to export the workload size statistics. The key idea is to determine the number of edges for each
active vertex by the offset array in the Apply phase, and use it in
the Scatter phase of the next iteration.
The workload size of a processing element is dependent on the
number of active vertices and the number of edges for each active vertex in Scatter phase, as well as the number of vertices in
Apply phase. 1) Active vertex count and vertex count can be directly obtained in the original PB-VCPM, because active vertices
are determined before each iteration and the vertex count is fixed
after a dataset specified. 2) However, the number of edges for each
active vertex (i.e., edge counter edgeCnt) is dependent on the vertex
ID of the active vertex in each iteration and cannot be obtained
in the original PB-VCPM. Therefore, we modify the programming
model to acquire the offset array OffsetArray in Apply phase for
edgeCnt calculation. As shown in Algorithm 2, we access the offset
array sequentially in the processing stage of Apply phase. The offset
of current vertex and succeeding vertex are used to calculate the
edgeCnt of every active vertex. v.prop, o f f set and edдeCnt are
used to activate a vertex, which constitutes active vertex data. This
way, we dynamically acquire the workload size that can be used as
hints for scheduling.
4.1.2 Acquiring Exact Edge-Prefetch Indication. After quantitative
analysis towards the graph applications, we observe that many of
active vertices only possess 4-8 edges, as shown in Fig. 2, which
is smaller than one cacheline size (64bytes). Therefore, accesses to
edge lists have limited data locality and become the new bottleneck
Algorithm 2: Optimized Programming Model
◁ Dispatching Stage of Scatter Phase
1 for each active vertex u do
2 dispatch(u.prop, u.offset, u.edgeCnt) to PE;
3 end
◁ Processing Stage of Scatter Phase
4 for e(u,v) ←EdgeArray[u.o f f set : u.o f f set + u.edдeCnt]
do
5 edgeProResult ← Process_Edge(u.prop, e.weight);
6 v.tProp ← Reduce(v.tProp, edgeProResult);
7 end
◁ Dispathching Stage of Apply Phase
8 for each vertex list do
9 dispatch(start id vListStartID and size vListSize of vertex
list) to PE;
10 end
◁ Processing Stage of Apply Phase
11 for vid ← vListStartID : vListStartID +vListSize do
12 edgeCnt ← OffsetArray[vid+1] - OffsetArray[vid];
13 applyRes ← Apply(vvid .prop, vvid .tProp, vvid .cProp);
14 if vvid .prop != applyRes then
15 vvid .prop ← applyRes;
16 activate vvid with vvid .prop, OffsetArray[vid],
edgeCnt;
17 end
18 end
after the random access to vertex property issue has been solved. To
address this issue, we propose an exact prefetching technique. Exact
prefetching means that only the necessary data will be prefetched
given the deterministic prefetching address and amount to prefetch.
Exact prefetching helps to maximize the number of in-flight memory requests in order to utilize the memory bandwidth more efficiently and hide memory latency. All the sequentially accessed data
such as active vertex data and vertex data can be prefetched exactly,
since the data address and the amount to prefetch are available
before these data are required.
To prefetch edge data exactly, we need the exact prefetching
indication including offset and edgeCnt for all active vertices. As
mentioned in Sec. 4.1.1, we have acquired offset and edgeCnt for all
active vertices from the programming model. More details about
exact edge prefetching hardware implementation are shown in
Sec. 5.2.1.
4.1.3 Decoupling Execution Flow. To overlap the latency of work
scheduling and execution, we decouple Scatter and Apply phases
into Dispatching and Processing stages as shown in Algorithm 2.
Dispatching Stage: Workloads are dispatched to processing elements (PEs). In the Scatter phase, edge workload of each active
vertex is dispatched to PEs. In the Apply phase, vertex workload is
dispatched to every PE.
Processing Stage: PEs process the workload. In the Scatter phase,
PEs process edge workload by executing Process_Edge and Reduce
618
Alleviating Irregularity in Graph Analytics Acceleration: a Hardware/Software Co-Design Approach MICRO-52, October 12–16, 2019, Columbus, OH, USA
Hardware Design
(a)
Data-aware
Dynamic Scheduling
Hardware Design
Cross layer design
(b)
Schedule workloads, data
accesses and updates
Enable datapath decoupling
Extract run-time data-dependency
1
3 4
1
3
4 High Bandwidth Memory
PE
DE DE Dispatcher
Processor
UE
Updater
Epref
Vpref
Crossbar
SIMT
Core
RU AU
VB
Dispatcher Processor Updater
S2: Dispatch
Edge
Workloads
S1: Read
Active Vertex
Data
S5: Reduce
Temp Vertex
Property
S3: Read
Edge
S4: Process
Edge
S2: Dispatch
Vertex
Workloads
S1: Generate
Vertex
Workloads
S5: Update
and Activate
Vertex
S3: Read
Vertex
S4: Apply
Vertex
(c)
(d)
RB
Prefetcher
Dispatcher Processor Updater
S2V
Optimized Programming Model
2
2 Endow more visibility
Figure 3: GraphDynS architecture overview. (a) Cross-layer design, (b) hardware design, hardware-platform stages in (c) Scatter
phase and (d) Apply phase.
functions. In the Apply phase, PEs process vertex workload by
executing Apply function and activating vertex for next iteration.
4.2 Hardware Design
In conjunction with the proposed programming model, we propose
GraphDynS hardware with several components to decouple the
datapath in microarchitecture.
4.2.1 Hardware Components. As shown in Fig. 3(b), GraphDynS
consists of following 4 components:
Dispatcher: Dispatcher dispatches workload to Processor and
consists of 16 Dispatching Elements (DEs), which are simple cores.
Processor: Processor receives the workload from Dispatcher and
processes the workloads. Processor consists of 16 PEs. Each PE consists of a Scalar to Vector Unit (S2V) and a SIMT (8 lanes) core. S2V
unit is used to transform workload from a scalar to a SIMT vector.
We construct the PE using single-precision floating point adders,
multipliers, and comparators.
Prefetcher: Prefetcher prefetches graph data from High Bandwidth Memory (HBM), and consists of Vertex Prefetcher (Vpref) and
Edge Prefetcher (Epref). Vpref prefetches active vertex and vertex
data to vertex prefetching buffer (VPB). Epref prefetches edge data
to edge prefetching buffer (EPB).
Updater: Updater receives the stored result from Processor and
updates the vertex property or activates vertex, which is composed
of a 128-radix crossbar switch and 128 Updating Elements (UE). UE
consists of a Vertex Buffer (VB, 256 KB dual-ported on-chip eDRAM),
a Ready-to-Update Bitmap (RB, 256 entries), a Reducing Unit (RU),
and an Activating Unit (AU). VB is used to cache all temporary
vertex property data. To process larger graphs (i.e., VB cannot hold
all temporary vertex property), the graph is sliced into several slices
and a single slice is processed at a time with the slicing technique
proposed in Graphicionado [24]. RB is used to indicate the readyto-update vertex. RU is used to execute Reduce function, including
a microarchitecture pipeline called Reduce Pipeline. AU is used to
activate vertices, and consists of four 16-entry buffer queues to
store active vertices.
4.2.2 Hardware-Platform Stage. According to the functionality of
aforementioned components, the hardware-platform stage of Scatter phase and Apply phase are described as follows:
Scatter phase (Fig. 3(c)): First, DE reads active vertex data from
VPB in step S1 and then dispatches edge workloads (i.e., edge list)
to PEs in step S2. Next, PE reads edges from EPB in step S3 and then
executes Process_Edge function to process edges in step S4. Finally,
UE reads temporary vertex property from VB, executes Reduce and
writes result to VB in step S5.
Apply phase (Fig. 3(d)): First, DE generates a vertex index list as
vertex workloads in step S1, and then dispatches vertex workloads
to PE in step S2. Next, PE reads vertex data from VPB in step S3 and
then executes Apply function to process vertex in step S4. Finally,
UE updates vertex property and activates vertex for next iteration
in step S5.
4.2.3 Decoupling Datapath. To acquire the information at runtime, we decouple the datapath into the following three segments
based on the optimized programming model and aforementioned
components.
Workload Management Sub-Datapath: dispatching and processing workloads. As shown in Fig. 3(b), DE dispatches workloads
to PE and the S2V unit transforms workloads to vector workloads
with a loop unrolling fashion. To alleviate workload irregularity,
we dispatch workloads to every PE in a balanced way and extend
workload processing with SIMT execution model to improve workload processing throughput using information of workload size,
which is discussed in Sec. 5.1 in detail.
Data Access Sub-Datapath: off-chip memory access and onchip memory access. As shown in Fig. 3(b), Prefetcher exactly
prefetches graph data from off-chip memory, Processor and Dispatcher coalesce access data from on-chip memory (i.e., VPB, EPB,
and VB), and RU accesses VB atomically. To alleviate traversal irregularity, we schedule data access using data type information, access
size and access address to reduce random accesses, improve data
access throughput and remove atomic stalls, which is discussed in
Sec. 5.2 in detail.
Data Update Sub-Datapath: data update includes vertex property update and vertex activation. As shown in Fig. 3(b), in the
Scatter phase, RU writes RB to mark vertex as ready-to-update
if its temporary vertex property is modified. In the Apply phase,
Prefetcher prefetches vertex data and informs the Dispatcher to
dispatch vertex workloads to PE for update computation. Next, AU
updates vertex property and activates vertex. To alleviate update
irregularity, we dynamically select ready-to-update vertices using
their modification status, and then prefetch their data and process
them. Moreover, we coalesce stores to off-chip memory of active
vertices given updated computation results, which is discussed in
Sec. 5.3 in detail.
619
MICRO-52, October 12–16, 2019, Columbus, OH, USA Mingyu Yan et al.
Vectorize workload processing
with loop unrolling(SIMT) Workload queue
Thread 1
Thread 2
Thread t
Vectorized Workloads
SIMT
Core
S2V
V18,
V128
V86,
V256
Thread 3
edge 1
edge 2
edge 17
edge 18
edge 19
edge 32
RAM 1
V128
 Number of edge < threshold: V18, V86. Number of edge threshold: V128, V256




PE 1
EPB
Crossbar Switch
RU
UE x

VB


RU
VB


UE y
Data vector Access
order Data scalar

Process

Edge
V18
V128
V86
V256
RAM 2
RAM 1 RAM 2
edge 1
edge 2
edge 3
edge 1
edge 2
edge 3 edge 5
edge 1
edge 2
edge 17
edge 18
edge 19
edge 32
edge 1
edge 16
edge 17
edge 32 V256
Epref
Arrange
edge data
VPB
EPB
V18
V86 V128 V256
edge 1
edge 5
edge 1
RAM 1
edge 6
edge 13


PE 1
VPB
UE 1
UE 8
Apply
vertex 8
RAM 1
vertex 1
vertex 8
vertex 1 vertex 16
vertex 23
VB
AU
VB
AU
SIMT SIMT
PE 1
PE 2
edge 16
DE 1
DE 2
(a) (b) (c) (d) (e)
V128,
V18 V86,
V256,
Figure 4: Data-aware Dynamic Scheduling. (a) DEs dispatch workloads to PEs with a threshold. (b) S2V unit vectorizes workload
processing with loop unrolling. (c) Data organization of VPB and EPB. On-chip vectorized access procedure of PE in (d) Scatter
phase and (e) Apply phase.
5 DATA-AWARE DYNAMIC SCHEDULING
Leveraging the run-time information from the optimized programming model and decoupled datapath, we present a data-aware dynamic scheduling scheme to schedule workloads, data accesses, and
update computations based on the proposed architecture. In this
section, we show how the proposed techniques tackle the workload,
traversal, and update irregularities.
5.1 Workload Scheduling
We first balance the workload given the workload size, then explain
how to improve workload processing throughput based on the SIMT
execution model.
5.1.1 Workload-Balanced Dispatch. To achieve workload balance,
we dynamically dispatch workloads to each PE according to the
workload size. Different from Graphicionado, our design processes
the edge list of low-degree vertices in the PE in batches, and distributes the edge list of active vertices with high degrees to each
PE evenly. In this way, the number of scheduling operations are
reduced significantly, and the workloads are balanced in each PE.
Next, we describe the scheduling flow:
Scatter phase: DE reads active vertex data from VPB and then
dispatches the edge list of this active vertex to the PE according to
edgeCnt. DE dispatches the entire processing workload of the edge
list to the PE with the same number (i.e.,DEi sends to PEi
) if edgeCnt
is smaller than a predefined threshold eThreshold. Otherwise, DE
partitions the edge list into several even sub edge lists (the size
of each sub edge list is eThreshold) and then dispatches them to
every PE. For example, as shown in Fig. 4(a), the edgeCnt of active
vertices V18(3 edges) and V86(2 edges) are smaller than eThreshold
(i.e., 16) while V128(32 edges) and V256(32 edges) are larger than
eThreshold. Thus, DE1 dispatches the entire workload of V18 to
PE1 while splitting the edge list of V128 into two even sub edge
lists and then dispatches them to every PE. PE saves these lists in a
workload queue before processing them.
Apply phase: DE generates a vertex list (size of each vertex
list is vListSize) and dispatches processing workload of the vertex list to PE with the same number (i.e., DEi sends to PEi
). The
vertex list generated by DEi starts from i × vListSize with stride
number o f DE × vListSize.
5.1.2 Workload-Processing Vectorization. To improve workload
processing throughput, we extend the execution unit of PE with the
SIMT execution model. We use S2V unit to execute loop unrolling
on the workload list (i.e., edge or vertex list) so that the workload is
transformed from a scalar to vector pattern. As shown in Fig. 4(b),
S2V unit in PE2 gets edge lists of V86 and V128 from the workload
queue and assigns an edge to each SIMT thread simultaneously.
Next, SIMT core executes these workloads together. In addition,
we also combine the small workload list which is smaller than the
number of threads of SIMT core together to improve SIMT efficiency.
Not only does this optimization improve processing throughput,
but it also improves the computation efficiency.
5.1.3 Configuration for Dispatch and Vectorization. To improve
the workload dispatching and processing efficiency, we need to
appropriately set the value of the number of SIMT threads (nSIMT),
the number of sub edge list size(eListSize), eThreshold, and vListSize.
As shown in Fig. 2, more than 60% of active vertices have more
than 5 neighbors over iterations and up to 99% of active vertices
have more than 3 neighbors over iterations. Moreover, active vertices have more than 10 neighbors on average. So, considering the
efficiency of SIMT and degree distribution, we set nSIMT as 8. To
reduce the complexity of Dispatcher and workload imbalance due to
high-degree active vertices, we set eThreshold as 128. Considering
the data access granularity of PE and latency of EPB, we set eListSize
as 16. To simplify the access to VB, we set vListSize as 8, which is
discussed in Sec. 5.2.2.
5.2 Data Access Scheduling
In this section, we first propose exact edge prefetching hardware
implementation, and then vectorize the data access to on-chip memory. Finally, we propose a zero-stall atomic maintenance mechanism
with a customized microarchitecture pipeline.
5.2.1 Exact Edge Prefetching Implementation. After random accesses to vertex property data have been solved by the on-chip
memory in Graphicionado, accesses to poor-locality edge list become the new bottleneck as mentioned in Sec. 4.1.2. With the exact edge prefetching indication from our programming model, we
design an exact Prefetcher, which prefetches other sequentially
accessed data too.
Exact prefetching the edge list of all active vertices is executed
as following. Step 1, Vpref uses the base address of active vertex
array and the number of active vertices to send active vertex access
requests to HBM. Step 2, Vpref receives offset and edgeCnt. Step 3,
Vpref sends offset and edgeCnt to Epref. Step 4, Epref uses offset and
edgeCnt to send edge access request to HBM. Step 5, Epref receives
edge data into EPB.
620
Alleviating Irregularity in Graph Analytics Acceleration: a Hardware/Software Co-Design Approach MICRO-52, October 12–16, 2019, Columbus, OH, USA
Previous work introduced long latency to start edge prefetching or a large on-chip memory caching offset array to reduce the
latency, since it needs a random access to start location offset of
edge list with active vertex ID [1, 24]. In addition, it costs additional
memory storage as well as abundant unnecessary memory accesses
for source vertex ID, src_vid. In previous design, every edge data
includes src_vid, which is compared against the active vertex ID
u.vid in order to determine the end of the edge list traversal for
an active vertex. As a result, each edge traversal requires access to
extra elements, thus wasting up to half of the bandwidth.
On the contrary, our design begins prefetching as soon as possible, since the accesses to offset in our work do not rely on active
vertex ID and the offset can be prefetched. The Prefetcher gets the
exact number of edges (i.e, edgeCnt) to prefetch, so that it can efficiently prefetch edges without causing unnecessary data accesses
and additional memory storage. Moreover, since an off-chip memory request acquires a significant amount of active vertex data, our
work can use these offsets and edge counters to coalesce memory
accesses to edge data and maximize the number of in-flight memory
requests. Consequently, our optimization enables better utilization
of the bandwidth.
5.2.2 Vectorizing On-Chip Data Access. To improve on-chip memory access efficiency and throughput, we vectorize the on-chip
memory access. We first organize the data in Prefetching Buffer
with SIMT granularity and then arrange the data access to Vertex
Buffer.
Prefetching buffer organization: Both VPB and EPB have 16
RAMs. The vertex data placement are based on a hash algorithm
(i.e., RAM id = accessing order % RAM number). Each RAM of VPB
and EPB only can be accessed by specific DE or PE (i.e., DEi and
PEi only access RAMi
).
Since the size of edge list for each active vertex varies, it is neccessary to maintain the processing edge vector in PE corresponding to
edge data vector in EPB. Therefore, Epref adopts the same workloadbalance strategy of DE to arrange the edge data in EPB. We illustrate
an example of on-chip data organization in VPB and EPB, as shown
in Fig. 4(c). The RAM1 of VPB and RAM1 of EPB are connected to
PE1, while the RAM2 of VPB and RAM2 of EPB are connected to PE2.
Four active vertices (V18, V86, V128, V256), are placed into RAM
of VPB dependent on their access order. Then, the edge workload of
these four active vertices will be dispatched to the corresponding
PE according to the dispatch strategies in Section 5.1.1. As shown
in Fig. 4(a), the edge workload of V18 and partial workload of V128
are assigned to PE1. In the meantime, Epref sequentially reads active vertex data of V18, V86, V128, V256 in VPB for edge data
prefetching. After receiving the prefetched edge data, Epref places
the edge data of active vertices to the corresponding RAM according to edgeCnt and eThreshold. For example, as same as workload
dispatching flow, V18’s edge data are stored in RAM1 of EPB while
V128’s edge data are stored in RAM1 and RAM2 evenly. In this
way, PE1 read edge data of V18 and partial of V128 in order with
their corresponding workload. Note that PEi can access nSIMT
data from RAMi each time.
VB data access arrangement (Scatter phase): To improve VB
access throughput in Scatter phase, we split the vertex temporary
property data evenly into 128 partitions according to a hash algorithm and place them in 128 VBs respectively (i.e., VB ID = Vertex
ID % number of UE). Moreover, we use a crossbar switch to route
the access of each thread to different VBs by access address. Fig. 4(d)
illustrates the on-chip memory access procedure of PE1 in Scatter
phase. Step 1, PE1 reads an edge vector from RAM1 of EPB each
time. Step 2, PE1 processes the edge vector by Processing_Edge function. Step 3, PE1 sends the edge processing result vector to Updater.
Step 4, Updater uses a crossbar switch to route the edge processing
result from each thread to different UEs by the destination vertex
temporary property store address. Step 5, RU reads the destination
vertex temporary property from VB and executes Reduce function.
Step 6, RU writes the reduced result to VB. The details of data accesses inside the Updater are explained in Sec. 5.2.3. By arranging
accesses to VB as explained, complex logic design is not needed to
move data from VB to PE and to transform the scalar data to the
vector data.
VB data access arrangement (Apply phase) To simplify the
accesses to the VB and improve VB access throughput in Apply
phase, each PE only accesses nSIMT with consecutive VBs. Since
the vertex IDs within the vertex list are consecutive and generated
by DE, each vector access from PE can directly accesses nSIMT
consecutive VBs. Moreover, the vertex lists are generated by the
same DE with a stride (i.e., number o f DE ×vListSize as mentioned
before). Thus, the access of PEs can be distributed to nSIMT with
specific VBs. Hence, conflict arising from accessing VBs can be
avoided. PE just receives the data from nSIMT registers, since both
access and data return of the same UE occur at the same time and the
access latency is constant. Fig. 4(e) illustrates the on-chip memory
access procedure of PE1 in Apply phase. Step 1, PE1 reads nSIMT
vertex vector from RAM1 of VPB each time. Step 2, PE1 reads the
temporary vertex property vector from VB of U E1 – U E8, and PE1
uses it and the vertex vector to execute Apply function. Step 3, PE1
sends the result vector to Updater and then Updater sends result of
each thread to AU of U E1 – U E8, respectively. The details of data
access inside the Updater are explained in Sec. 5.3.2. By arranging
the access of VB as explained, PE can use simple logic to transform
scalar data to vector data.
5.2.3 Maintaining Atomicity with Zero Stall. Previous work[55]
shows that atomic operation is another major bottleneck due to
the parallel data conflict on scattered feature of graph processing,
especially for graphs with power-law degree distribution. In addition, the atomic bottleneck in complete edge access algorithms
(e.g. PageRank) becomes more significant when other issues are
addressed. It introduces many stalls during access to the temporary
vertex property, causing heavy performance degradation.
Opportunity: Fortunately, we find an opportunity to eliminate
stalls from atomic operations based on the following observations:
1) Read-after-write (RAW) only occurs in Reduce function, where
v.tProp is modified, 2) Each algorithm only has one specified Reduce
function, and 3) The Reduce function of most graph workloads is a
simple compare or accumulation operation (i.e, only one instruction
and few instruction operation types)[21, 39].
Key idea: Based on these observations, we propose a storereduce mechanism with a microarchitecture pipeline. The key idea
621
MICRO-52, October 12–16, 2019, Columbus, OH, USA Mingyu Yan et al.
is to shorten the distance between RAW conflict data and the arithmetic unit. Next, up-to-date data is selected for executing in the
arithmetic unit.
Specially, we first shorten the distance between the compute
pipeline and data to five cycles by the store-reduce mechanism.
Next, we customize the compute pipeline to further shorten the distance to one cycle. Finally, we select up-to-date data by comparing
the access address between a source operand and the destination
operand, and send the data to the arithmetic unit inside the microarchitecture pipeline.
Implementation: We first transform read, reduce and write
operations of v.tProp to a store operation to VB and then execute
Reduce function in RU (namely store-reduce). Each PE executes the
store operation to send the store address of v.tProp and the edge
processing result edgeProResult to Updater. The crossbar switch
inside the Updater routes the data in each thread to corresponding
UE based on v.tProp store address. Next, RU reads the v.tProp from
VB, executes Reduce function, and writes the v.tProp to VB.
Cycle 1: VB Read
(RD)
Cycle 2: Execution
(EXE)
Returned Result
Cycle 3:VB Write
(WB)
Operation
Register
OP0
OP1 Operand0
Register
 Read
VB
Write VB
Operation
Register
Operand1
Register
Address
Compare
Pipeline
Register
Result
Register
OP1
Addr. Pipeline
Register
Address
Compare
Address
Register
FALU
Addr. Pipeline
Register
Register Floating point
arithmetic unit
Vertex buffer
access
Comparato
r
Operation
register
Control signal Data signal
Returned Result
Mux
Mux
Mux
Figure 5: Reduce Pipeline.
To avoid pipeline stalls inside RU, we present a three-stage
pipeline Reduce Pipeline to execute the Reduce function. As shown
in Fig. 5, we remove the instruction fetching and instruction decode
stages of the classic five-stage pipeline since operation is decided
after graph algorithm is specified. 1) RD Stage, the old v.tProp is
read from VB with the address of v.tProp. Whether the read v.tProp
or the result returned from WB Stage will be set as Operand 1 Register is decided by an address comparison of addresses from the RD
Stage and WB Stage. The returned result is set as Operand 1 Register
if these two addresses are equal. Otherwise, the read v.tProp is set
as Operand 1 Register. The address is pipelined next to EXE Stage.
2) EXE Stage, the floating point arithmetic logical unit (FALU) calculates the result. Similar to RD Stage, the Operand 1 of FALU is
also determined by address comparison. The returned result from
WB Stage is sent to FALU if these two address are equal. Otherwise,
the value of Operand 1 Register is sent to FALU. The address is forwarded next to WB Stage. The execution stage only consumes one
cycle due to the simplicity of Reduce Operation. 3) WB Stage, the
new v.tProp is written into VB. The result and address are returned
to RD Stage and EXE Stage. We integrate Reduce Pipeline into every
RU so that the stalls are removed while atomicity is maintained.
5.3 Data Update Scheduling
During Apply phase, it is not necessary to update all the vertices.
To avoid unnecessary updates, we select the vertices that require an
update with the indication from RU. And then, to reduce irregular
(intermittent) memory accesses due to random updates, we coalesce
these intermittent accesses to effectively use the bandwidth.
5.3.1 Selecting Ready-to-Update Data. We next show the opportunity and implementation that selects ready-to-update data.
Opportunity: The result of Reduce function can indicate whether
a vertex is ready to update for most algorithms. Therefore, it poses
an opportunity to select out the vertices that require an update via
the result of RU. As shown in Algorithm 2, Apply function uses
the result of Reduce function whose modification makes applyRes
change in most algorithms. It indicates that the vertex is ready to
update. With the indication from RU, we maintain a bitmap (namely,
Ready-to-Update Bitmap, i.e, RB) to record vertices that are ready to
update in Scatter phase. Next, we only update these ready-to-update
vertices in Apply phase. This way, the computation and memory
access for unnecessary updates are eliminated.
Implementation: We integrate RB into each UE to record v.tProp
whose vertex has been modified in Scatter phase of current iteration. In the Apply phase, Vpref only prefetches the ready-to-update
vertex data and DE only generates ready-to-update vertex workload based on the bit value in RB. To determine the bitmap size,
the trade-off between unnecessary computation and the bitmap
implementation overhead needs to be considered. To simplify the
hardware implementation and to leverage the SIMT advantages,
we use 1 bit to represent the ready status of 256 consecutive vertices. Experimental results show that it is effective to reduce the
unnecessary computations.
5.3.2 Coalescing Intermittent Accesses. To alleviate the memory
bandwidth pressure resulting from random updates, we coalesce the
intermittent off-chip memory accesses to the active vertex array.
Opportunity: Whether or not the active vertex array needs to
be updated is determined by a data-dependent condition shown in
Algorithm 2 lines 14-17, which introduces intermittent accesses to
off-chip memory leading to inefficient bandwidth utilization. Since
there is only a single path in this branch, we can transform this
path into a conditional store operation. Next, each UE uses two
sets of buffer queues working in a double buffer fashion to buffer
active vertex data and write them to off-chip memory when the
buffer queue is full or at the end of Apply phase. Thus the stall
possibility incurred by the out of buffer queue is reduced and the
off-chip memory access efficiency is improved.
Implementation: The store operation executed in PE sends the
result of conditional branch (i.e., condition flag), applyRes, offset
and edgeCnt to UE. The AU in the UE activates vertex if condition
flag is true, and then writes the active vertex data to the buffer
queue. Vertex properties are stored in off-chip memory together
whether condition flag is true or not to reduce random accesses.
5.4 Summary
To summarize, in this work we first extract the data dependencies at
runtime with decoupling the datapath with our optimized programming model and hardware design. Next, we dynamically schedule
balanced workloads and data accesses elaborately, and eliminate
unnecessary updates through our data dependency aware scheme.
In particular, to mitigate Workload Irregularity, Dispatcher leverages the workload size information to balance workload across
622
Alleviating Irregularity in Graph Analytics Acceleration: a Hardware/Software Co-Design Approach MICRO-52, October 12–16, 2019, Columbus, OH, USA
multiple PEs and Processor utilizes SIMT execution model to process
workloads in vector. To address Traversal Irregularity, Prefetcher
leverages the offset and length of various data structure to exactly
prefetch graph data and to vectorize on-chip memory access. Moreover, Updater leverages the access address of vertex property to
maintain atomicity with zero stall based on the observation of
the simplicity of reduce operations. To address Update Irregularity,
Updater utilizes the modification status of temporary vertex property to implement selective updates and coalesces the intermittent
off-chip access of active vertex data.
6 EXPERIMENTAL METHODOLOGY
Methodology. The performance and energy of GraphDynS and
Graphicionado are measured by the following tools.
Accelerator simulator. For the performance evaluation, a customized cycle-accurate simulator was designed and implemented
to measure execution time in number of cycles. This simulator
models the microarchitectural behavior of each hardware module
of our design. In addition, the performance model implements a
detailed cycle-accurate scratchpad memory model and a crossbar
switch model. This is also integrated with Ramulator [28], a memory
simulator, to simulate the cycle-accurate behavior of the memory
accesses to HBM.
CAD tools. For area, power and critical path delay (in cycles)
measurements, we implemented a Verilog version of each hardware
module, then synthesized it. We used the Synopsys Design Compiler
with the TSMC 16nm standard VT library for the synthesis, and
estimated the power consumption using Synopsys PrimeTime PX.
eDRAM, crossbar switch and HBM measurements. The area, power
and access latency for the on-chip scratchpad memory were estimated using Cacti 6.5 [53]. Since Cacti only supports down to 32nm
technology, we apply four different scaling factors to convert them
to 16nm technology as in [46]. The area, power and throughput for
the crossbar switch were estimated using the wire length, routing
resources and physical parameters of the metal layers with the
model in [9]. Similarly, we also apply the four different scaling
factors mentioned above to convert them to 16nm technology. The
energy for HBM 1.0 was estimated using 7 pJ/bit as in [44].
Baselines. To compare the performance and energy efficiency of
GraphDynS with state-of-the-art work, we evaluate Graphicionado
and a GPU-based solution Gunrock [52] in a Linux workstation
equipped with an Intel Xeon E5-2698 v4 CPU, 256GB memory, and
an NVIDIA V100 GPU. Table 3 shows the system configurations
for above implementations.
Table 3: System used for GraphDynS and baselines.
GraphDynS Graphicionado Gunrock (V100)
Compute Unit 1Ghz 16×SIMT8 1Ghz 128×Streams 1.25Ghz 5120×cores
On-chip memory 32MB eDRAM 64MB eDRAM 34MB
Off-chip memory 512GB/s HBM 1.0 512GB/s HBM 1.0 900GB/s HBM 2.0
Note: GPU’s on-chip memory consists of register file, shared memory and L2 cache.
Graph datasets and algorithms. Table 4 describes the graph
datasets used for our evaluation. Both real-world graphs and synthetic graphs are used for the evaluation. We use SSSP, CC, BFS,
SSWP, and PR algorithms to evaluate GraphDynS. For the evaluation on unweighted real-world graphs, random integer weights
between 0 and 255 were assigned.
Table 4: Graph datasets used in evaluation.
Graph #Vertices #Edges Brief Explanation
Flickr (FR)[15] 0.82M 9.84M Flickr Crawl
Pokec (PK)[15] 1.63M 30.62M Pokec Social Network
LiveJournal (LJ)[15] 4.84M 68.99M LiveJournal Follower
Hollywood (HO)[15] 1.14M 113.90M Movie Actors Social
Indochina-04 (IN)[15] 7.41M 194.11M Crawl of Indochina
Orkut(OR)[15] 3.07M 234.37M Orkut Social Network
RMAT scale 22 (RM22)[37] 4.19M 67.11M Synthetic Graph
RMAT scale 23 (RM23)[37] 8.38M 134.22M Synthetic Graph
RMAT scale 24 (RM24)[37] 16.76M 268.44 M Synthetic Graph
RMAT scale 25 (RM25)[37] 33.52M 536.88M Synthetic Graph
RMAT scale 26 (RM26)[37] 67.04M 1073.76M Synthetic Graph
7 EXPERIMENTAL RESULTS
In this section, we compare GraphDynS to baselines and provide
analysis.
• Speedup: We first compare the performance speedup normalized
to Gunrock, as shown in Fig. 6. The last set of bars, labeled as GM,
indicate the geometric mean across all algorithms. Overall, GraphDynS achieves 4.4× speedup over Gunrock and only possesses half
the off-chip memory bandwidth. Meanwhile, compared to Graphicionado, GraphDynS achieves 1.9× speedup and only consumes 50%
of on-chip memory. The better performance of GraphDynS over
others comes from highly effective memory bandwidth utilization,
elimination of atomic stalls, and the reduction of update operations.
In the further step, we analyze the performance improvement
on different applications. Compared to Gunrock, the speedup of
CC algorithm is smaller than other algorithms since the online
preprocessing of Gunrock efficiently reduces unnecessary workloads by filtering unnecessary active vertices [52]. On the other
hand, PR algorithm achieves higher speedup on GraphDynS and
Graphicionado due to fewer random accesses to off-chip memory.
Compared to Graphicionado, GraphDynS achieves higher speedup
on PR algorithm than other algorithms. The reasons are 1) exact
prefetching that eliminates the redundant data access that harvests
more bandwidth to improve throughput, and 2) zero-stall atomic
optimization that eliminates the stall caused by frequent RAW conflicts introduced by high-throughput graph computing. For the
other algorithms, the performance speedup is lower than PR. This
is due to the long latency introduced by non-sequential accesses
to the first edge of active vertices. Furthermore, most of the active vertices have an edge list smaller than off-chip memory access
granularity, leading to underutilized bandwidth. We also analyze
the influence of dataset and observe that speedup on HO dataset
is higher than other datasets. This is because HO dataset has high
edge-to-vertex ratio. Edge data possesses high spatial locality and
thus access latency can be hidden.
0
2
4
6
8
10
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
BFS SSSP CC SSWP PR GM
Speedup to Gunrock
Graphicionado GraphDynS
12 32
Figure 6: Speedup over Gunrock.
• Throughput: Additionally, we compare the throughput of these
three techniques, as shown in Fig. 7. Throughput is defined as the
number of edges processed per second (GTEPS, i.e., giga-traversed
623
MICRO-52, October 12–16, 2019, Columbus, OH, USA Mingyu Yan et al.
edges per second). The ideal peak throughput is 128 GTEPS. GraphDynS achieves 43 GTEPS on average, while Graphicionado and
Gunrock achieve 21 GTEPS and 8 GTEPS respectively. Although
GraphDynS achieves 5× throughput over Gunrock on average, its
average speedup over Gunrock is 4.35×, since online preprocessing
dominates execution time of Gunrock and fewer edges are processed
compared to GraphDynS.
Due to atomic operation stalls and low data access throughput of
prefetch buffer, PR algorithm in Graphicionado only achieves 37.5
GTEPS throughput on average. With the optimization of zero-stall
atomics and prefetch buffer organization, PR algorithm in GraphDynS achieves throughput of 87.5 GTEPS on average. Although
all the edges on PR algorithm are processed iteratively, PR algorithm does not achieve peak throughput since DRAM refresh and
memory accesses to vertex data consumes bandwidth. For the other
algorithms, CC achieves higher throughput than SSSP, SSWP and
BFS on average since it starts from all vertices (i.e., all vertices in
first iteration are active vertices) and converges quickly.
0
25
50
75
100
FR PK LJ HO IN OR FR PK LJ HO IN OR FR PK LJ HO IN OR FR PK LJ HO IN OR FR PK LJ HO IN OR
BFS SSSP CC SSWP PR GM
Throughput (GTEPS)
Gunrock Graphicionado GraphDynS
102 101
Figure 7: Throughput.
• Power/Area: Fig. 8 shows the power and area breakdown of
GraphDynS except for HBM. The total power and area of GraphDynS are 3.38 W and 12.08mm2
respectively. Dispatcher and Prefetcher
only cost 5% power as well as 2% area. Meanwhile Processor costs
59% power as well as 8% area due to its 128 processing elements.
Updater costs 36% power as well as 90% area due to its 32MB
eDRAM and crossbar switch. However, Graphicionado requires
64MB eDRAM to cache temporary vertex property array and offset
array, which is 50% larger than our design. Hence, although GraphDynS is composed of 128 reducing pipelines and 128 active units,
its power and area are still less than Graphicionado, only 68% and
57% of Graphicionado.
0%
8%
90%
2%
Area
Dispatcher
Processor
Updater
Prefetcher
Area
1%
59%
36%
4%
Power
Dispatcher Processor Updater Prefetcher
Power Figure 8: Power and area breakdown.
• Energy: Fig. 9 shows the normalized energy consumption of
GraphDynS and the baselines (including energy consumption of
HBM), normalized to Gunrock. GraphDynS reduces energy consumption by 91.4% compared to Gunrock on average. It also reduces energy consumption by 45% on average. The main reason is
the reduction of a large number of memory accesses due to exact
prefetching and unnecessary updates. Compared to Gunrock, CC
algorithm running on FR dataset and PR algorithm on IN dataset
are less energy efficient than other cases on GraphDynS since these
two cases introduce many off-chip memory accesses.
As shown in Fig. 10, 7.8% of total energy is consumed on average by the components of GraphDynS while 92.2% of energy is
consumed by off-chip memory access (i.e., HBM). The reason is
that these graph algorithms have extremely low computation-tocommunication ratio. The Processor consumes 4.0% of energy while
the Updater consumes 3.0%. The rest of the components consume
less than 0.8%.
0
10
20
30
40
50
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
BFS SSSP CC SSWP PR GM
Normalized Energy (%)
Graphicionado GraphDynS
Figure 9: Energy consumption normalized to Gunrock (including HBM).
0
2
4
6
8
10
12
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
BFS SSSP CC SSWP PR GM
Energy Breakdown (%)
Prefetcher Dispatcher Processor Updater
Figure 10: Energy breakdown.
• Memory Storage Usage: Fig. 11 shows the maximum off-chip
memory storage usage of GraphDynS and the baselines at runtime,
normalized to Gunrock. GraphDynS only uses 35% and 63% memory
storage with respect to Gunrock and Graphicionado on average. The
main reason for the reduction of memory storage is that GraphDynS
does not require storing extra information such as preprocessing
metadata and additional data in edges. Gunrock uses more than 2×
storage than original graph data for storing preprocessing metadata.
In the Graphicionado design, the edge data includes the source
vertex ID src_vid and the active vertex data includes the active
vertex ID u.vid. Contrarily, src_vid and u.vid are not required by
GraphDynS since it has an edge counter and offset for each active
vertex.
0
20
40
60
80
100
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
BFS SSSP CC SSWP PR GM
Normalized Storage (%)
Graphicionado GraphDynS
Figure 11: Off-chip memory storage usage normalized to
Gunrock.
• Memory Access: Fig. 12 shows total data accessed from off-chip
memory of GraphDynS and the baselines during runtime, normalized to Gunrock. Although GraphDynS accesses offset array additionally in each iteration, GraphDynS still reduces data accesses by
64% and 47% on average compared to Gunrock and Graphicionado.
Gunrock produces many memory requests resulting from the random access for edge traversal. However, online preprocessing helps
Gunrock significantly reduce data access on CC algorithm. Note
that the data access of Gunrock in the cases of CC algorithm running on FR dataset and PR algorithm on IN dataset are smaller than
624
Alleviating Irregularity in Graph Analytics Acceleration: a Hardware/Software Co-Design Approach MICRO-52, October 12–16, 2019, Columbus, OH, USA
GraphDynS since there is good data locality of vertex property in
these two cases. GraphDynS and Graphicionado use the on-chip
memory to reduce a large amounts of random accesses. Nevertheless, as mentioned above, each access to an edge in Graphicionado
includes src_vid, leading to extra accesses which are proportional
to the number of edges.
0
45
90
135
180
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
FR
PK
LJ
HO
IN
OR
BFS SSSP CC SSWP PR GM
Normalized Access (%)
Graphicionado GraphDynS
388
Figure 12: Memory accesses normalized to Gunrock.
• Utilization of Memory Bandwidth: Fig. 13 shows the average
memory bandwidth utilization of GraphDynS and others. Benefiting from our prefetcher design, GraphDynS achieves 56% memory
bandwidth utilization on average. A large amount of random accesses cause low bandwidth utilization of only 31% for Gunrock.
Graphicionado has similar bandwidth utilization to GraphDynS
since Graphicionado has more sequential accesses to edge data (i.e.,
extra access to src_vid, 1.65 × over GraphDynS on average) and
thus the row buffer miss rate is lower than GraphDynS. However,
GraphDynS performs better than Graphicionado due to more efficient utilization of memory bandwidth.
0
25
50
75
100
FR PK LJ HO IN OR FR PK LJ HO IN OR FR PK LJ HO IN OR FR PK LJ HO IN OR FR PK LJ HO IN OR
BFS SSSP CC SSWP PR GM
Bandwidth Utilization (%)
Gunrock Graphicionado GraphDynS
Figure 13: Memory bandwidth utilization.
7.1 Effects of Scheduling Optimization
A detailed evaluation on LJ dataset is presented to provide more
insight into the scheduling optimization. The optimizations include Workload Balancing (WB), Exact Prefetching(EP), Atomic
Optimization (AO) and Update Scheduling (US). We compared the
following combinations to understand the effectiveness of the proposed techniques: 1) combining the workload balancing and exact
prefeching techniques (WE); 2) combining WE with atomic optimization techniques (WEA); 3) combining WEA with update
scheduling techniques (WEAU).
• Effects of Workload Scheduling. We first evaluate the load-balance
scheduling method. Fig. 14(a) shows the number of scheduling operation reduction of GraphDynS with respect to GraphDynS without
workload scheduling optimization. With WB optimization, GraphDynS reduces scheduling operations by up to 94% on average. Meanwhile, reducing 128 DEs to 16 DEs reduces 80% of Dispatcher’s area.
Although 87.5% DEs are reduced, performance does not degrade.
The key reason for scheduling complexity reduction is because of
the proposed coarse granularity (a batch of edges or vertices) scheduling mechanism, enabling 16 DEs to achieve peak throughput of
Processor.
As shown in Fig. 14(b), normalized workload among PEs in several heaviest workload iterations on SSWP algorithm are near to
ideal value 1.0 (normalized to average workload in corresponding iteration) in GraphDynS. WB optimization improves performance by
7% performance on average, as shown in Fig. 14(c). Hence, WB optimization not only reduces scheduling overhead, but also balances
workload efficiently.
• Effects of Data Access Scheduling. Next, we analyze the effectiveness of data access scheduling. As shown in Fig. 14(c), WE
achieves 1.39× speedup on average, compared to Graphicionado.
The data access to HBM has been reduced by 30% with exact
prefetching optimization (EP) on average, As shown in Fig. 14(d).
With EP optimization, Prefetcher prefetches edge list as quickly as
it can after access to the active vertex data. Therefore, in Scatter
phase, memory level parallelism is improved and performance stall
due to data misses is reduced. Because most of the edge property
have less than 2 elements, our exact edge prefetching will significantly improve memory access efficiency and traversal irregularity.
Compared to other algorithms, BFS algorithm exhibits less speedup
by WE because it spends most of time on Apply phase instead of
Scatter phase.
In the next step, we analyze performance with WEA that shows
1.57× speedup compared to Graphicionado. Specifically, PR and CC
algorithms benefit from AO optimization, achieving 20% and 5%
of performance improvement respectively. This is because they
have more RAW conflicts per cycle than other algorithms due to
their higher throughput. For PR and CC running HO dataset, the
performance improvement achieves 30% and 15% respectively. The
reason is that HO dataset has larger edge-to-vertex ratio, which
deteriorates the RAW conflicts with even higher throughput.
In summary, not only does data access scheduling reduce a large
amount of unnecessary data access and execution time, but it also
efficiently overcomes the traversal irregularity.
• Effects of Update Scheduling. Lastly, we evaluate the effectiveness of update scheduling. As shown in Fig. 14(c), WEAU achieves
1.8× speedup compared to Graphicionado. US technique alone reduces the execution time by 10%.
In the next step, we show the breakdown of the improvement
brought by update operation and data access reduction. As shown
in Fig. 14(d), the data access to off-chip memory has been reduced
by 18% compared to Graphicionado. 60% of the update operations
are eliminated with US technique. Specifically, for BFS algorithm,
US reduces up to 88% update operations, 55% of data accesses and
28% of execution time, more than other algorithms. The reason is
that Apply phase occupies up to 50% execution time while in other
algorithms it occupies only 30% or even less. For PR algorithm,
US cannot provide improvement because PR requires updating all
vertices in each iteration. For SSSP algorithm, US reduces 63% of
update operations and 8% of data accesses. However, it only reduces
4% execution time as Apply phase only occupies 20% of execution
time for SSSP.
In summary, update scheduling greatly reduces unnecessary
update operations and data accesses, which efficiently addresses
the update irregularity and reduces execution time.
7.2 Scalability Analysis
• Performance over number of UEs: Fig. 14(e) shows the performance over the number of UEs on LJ dataset, normalized to the
625
MICRO-52, October 12–16, 2019, Columbus, OH, USA Mingyu Yan et al.
1
1.2
1.4
1.6
1.8
2
2.2
2.4
BFS
SSSP
CC
SSWP
PR
GM
Speedup
WB WE WEA WEAU
0
10
20
30
40
50
60
BFS
SSSP
CC
SSWP
PR
GM
Access Reduction Ratio(%)
EP US
0 90
91
92
93
94
95
96
97
98
BFS
SSSP
CC
SSWP
PR
GM
Scheduling Reduction (%)
(f)
1 3 5 0.98 7
0.99
1
1.01
1.02
1 3 5 7 9 11 13 15
Normlazied Workload
#PE
1 2 3 4 5 6 7 8
(a) (b) (d)
0
20
40
60
80
100
BFS
SSSP
CC
SSWP
PR
256 128 64 32
Normalized Performance (%)
(c) (e)
25
45
65
85
105
125
R22 R23 R24 R25 R26
Throughput (GTEPS)
GraphDynS Graphicionado
Figure 14: Efficiency of dynamic scheduling and scalability results. (a) Reduction ratio of scheduling of DEs, (b) Normalized
workload among PEs in several iterations, (c) Breakdown of Speedup, (d) Normalized off-chip data access to Graphicionado, (e)
Performance over number of UEs, (f) Throughput over the five synthetic graphs.
performance of 128 UEs. Results of PR and CC show that these two
algorithms slow down 53% and 20% respectively from 128 UEs to 32
UEs. They are affected heavily by the number of UEs. This is because
the reduce operations from different SIMT lanes of different PEs
may access the same UEs, a situation which causes contention on
UEs. These two algorithms can achieve up to 43 GTEPS throughput
with 128 UEs while others achieve less than 17 GTEPS. Therefore,
they have more contention compared to other algorithms, leading to performance degradation. In summary, the algorithm that
achieves high throughput is more sensitive to the number of UEs.
• Throughput over large-scale synthetic graphs: Fig. 14(f) shows
the throughput over the five large-scale synthetic graphs with the
same edge-to-vertex ratio on PR algorithm. We use throughput
of PR algorithm to evaluate the scalability of system performance
since the throughput of PR algorithm heavily depends on the edgeto-vertex ratio of graph. The throughput of GraphDynS slows down
slightly in R24, R25 and R26. This is because graphs are sliced and a
single slice is processed at a time, a process of which causes repetitive accesses to active vertices. However, since Graphicionado can
cache 2× temporary vertex property than GraphDynS, the throughput of Graphicionado slows down more gradually compared to
GraphDynS. These results show that both GraphDynS and Graphicionado scale well to larger graphs.
8 RELATED WORK
• Graph analytics accelerators: Recent work [2, 6, 7, 11, 12, 45]
use FPGA to accelerate graph workloads. These works need specific
data partitioning to reduce the random memory accesses and RAW
conflicts. Furthermore, limited by hardware resources, FPGA-based
solutions cannot efficiently accelerate large scale graph processing
at high performance. To overcome these limitations, Ozdal et al. [46]
presents an architecture template based on asynchronous execution
model to exploit memory-level parallelism. Although it achieves
high throughput, it needs a complex design to ensure sequential
consistency, and also the traversal irregularity causes inefficient
bandwidth utilization since the random memory access still exits.
Graphicionado [24] uses an on-chip memory to reduce large amount
random memory access, and thus it achieves high performance and
high efficiency. Whereas, it cannot tackle workload and update
irregularities. Our work addresses these irregularities efficiently
and achieves up to 1.9× speedup while consuming 1.8× less energy
compared to Graphicionado.
Besides, recent work [1, 13, 56] modify the logic layer of 3D
memory technology Hybrid Memory Cube (HMC) to offload graph
workloads into HMC so that they can utilize the internal high bandwidth of memory-centric architecture. Nevertheless, logic layer
has limited area and thermal budget. Other work [14, 50, 57] implement ReRAM-based graph processing accelerators to leverage
the efficient compute capacity of ReRAM. Above work cannot be
adopted as a near term solution since this memory technology is
still immature. Different from above methods, our work presents a
practical solution by leveraging the off-the-shelf HBM.
• GPU-based solutions: Integrated with high bandwidth memory, GPU-based solutions [20, 26, 27, 33, 42, 52] achieve orders of
magnitude performance improvement over CPU solutions. However, most of these need expensive offline/online preprocessing to
transform irregularity to be regular, which dominates the whole
execution period, up to 2× of the processing time in Gunrock [52].
However, our work alleviates irregularity without preprocessing
and achieves 4.4× speedup while reducing energy consumption by
91.7% compared to Gunrock.
9 CONCLUSION
In this paper, we propose a hardware/software co-design for graph
analytics, called GraphDynS. Based on the decoupled datapath,
GraphDynS can schedule the workload, data access, and update
computation by considering the data dependency at runtime to
alleviate the irregularity in graph analytics. GraphDynS achieves
1.9× speedup while consuming 1.8× less energy compared to the
state-of-the-art graph analytics accelerator. Compared to the stateof-the-art GPU-based solution running on a NVIDIA V100 GPU,
GraphDynS achieves 4.4× speedup with half the memory bandwidth
while consuming 11.6× less energy