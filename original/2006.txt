Abstract
Estimating depth from stereo vision cameras, i.e., "depth from
stereo", is critical to emerging intelligent applications deployed in
energy- and performance-constrained devices, such as augmented
reality headsets and mobile autonomous robots. While existing
stereo vision systems make trade-offs between accuracy, performance and energy-efficiency, we describe ASV, an accelerated
stereo vision system that simultaneously improves both performance and energy-efficiency while achieving high accuracy.
The key to ASV is to exploit unique characteristics inherent to
stereo vision, and apply stereo-specific optimizations, both algorithmically and computationally. We make two contributions. Firstly,
we propose a new stereo algorithm, invariant-based stereo matching (ISM), that achieves significant speedup while retaining high
accuracy. The algorithm combines classic “hand-crafted” stereo
algorithms with recent developments in Deep Neural Networks
(DNNs), by leveraging the correspondence invariant unique to stereo
vision systems. Secondly, we observe that the bottleneck of the ISM
algorithm is the DNN inference, and in particular the deconvolution operations that introduce massive compute-inefficiencies. We
propose a set of software optimizations that mitigate these inefficiencies. We show that with less than 0.5% hardware area overhead,
these algorithmic and computational optimizations can be effectively integrated within a conventional DNN accelerator. Overall,
ASV achieves 5× speedup and 85% energy saving with 0.02% accuracy loss compared to today’s DNN-based stereo vision systems.
CCS Concepts
• Human-centered computing → Mobile computing; Mobile
devices; • Hardware → Hardware accelerators; • Computing
methodologies → Computer vision tasks.
Keywords
Stereo vision, Depth from stereo, Mobile computing, DNN accelerator, data-flow, tiling, constrained-optimization

Artifacts
ISM Algorithm: https://github.com/horizon-research/ism-algorithm
Systolic-array Data-flow Optimizer: https://github.com/horizonresearch/systolic-array-dataflow-optimizer

1 Introduction
The demand for intelligent applications running on a diverse range
of mobile and embedded platforms, such as micro-robots, augmented reality headsets, and smart-city sensor nodes, shows no
sign of slowing down. A key primitive in these applications is estimating depth information from the environment, which in turn
serves as the building block for extracting higher-level semantics.
For instance, depth information enables a mobile robot to detect
and manipulate objects that are in close proximity.
Among numerous depth sensing techniques, we focus on stereo
camera systems, which estimate depth from a pair of horizontally displaced cameras that capture two different views of the
scene, mimicking the human binocular vision. Compared to other
depth sensing techniques such as LiDAR and structured-light sensors [65, 67], stereo vision systems are much cheaper, consume less
power, and are physically more compact [3]. In response to the
rising significance of stereo vision, recent mobile vision platforms
integrate specialized stereo vision accelerators, such as the Stereo
Depth Block in the Movidius Enhanced Vision Accelerator Suite [5]
and the Stereo & Optical Flow Engine (SOFE) in the Nvidia Xavier
mobile Systems-on-a-chip (SoC) [9].
Stereo vision algorithms presented to date broadly define a frontier in the accuracy-efficiency design space. Fig. 1 compares the
frame rate and accuracy for four well-known classic stereo algorithms that use “hand-crafted” features, including GCSF [15],
SGBN [33], HH [33], and ELAS [26], as well as four state-of-the-art
DNNs solutions [16, 37, 48, 59]. The DNN data is characterized on
both a Pascal mobile GPU [8] (“-GPU” suffix), as well as on a DNN
accelerator [57] (“-Acc” suffix). In using low-dimensional “handcrafted” features, classic algorithms lead to high error rates (x-axis),
but are compute efficient, mostly operating at close to real-time
(e.g., 30 FPS, y-axis). In contrast, DNNs models achieve very low
error rates, but require 2–5 orders of magnitude more arithmetic
operations, resulting in much lower frame rates.
This paper presents ASV, an accelerated stereo vision system that
operates in real-time while achieving DNN comparable accuracy.
643
0.01
0.1
1
10
100FPS
0 4 8 12 16
Error Rate (%)
30 FPS
Better
 GCSF
 SGBN
 HH
 ELAS
 DispNet-Acc
 FlowNetC-Acc
 PSMNet-Acc
 GCNet-Acc
 DispNet-GPU
 FlowNetC-GPU
 PSMNet-GPU
 GCNet-GPU
ASV
Classic
Algo.
 DNNs
Fig. 1: ASV demonstrates both real-time (30 FPS) frame rates
and DNN-like accuracy for stereo vision.
While today’s vision accelerators are primarily built for monocular
vision tasks, ASV exploits unique characteristics of stereo vision,
and applies stereo-specific optimizations, both algorithmic and
computational. Critically, we show that with careful algorithmic
choices, these stereo-specific optimizations can largely be implemented on the same computer architecture as conventional DNN
accelerators with a set of basic, yet principled, hardware extensions,
which widens the applicability of this work.
At the core of ASV is a new low-latency, high-accuracy stereo
vision algorithm. We exploit the temporal invariance introduced
by stereo cameras: a single physical point projects to a unique pair
of pixels on the left and right image planes; although the pixel
locations move over time, their corresponding geometric relationship is fixed. Our algorithm, ISM, uses compute-intensive DNNs
to extract pixel correspondences from a small set of key frames.
The correspondences are then propagated as initial estimates for
subsequent non-key frames, where we make use of cheaper, classic
algorithms. By combining learnt features from DNNs, and classic
algorithms that explicitly model the physical world, ISM achieves
high accuracy while reducing the compute cost.
While our ISM algorithm reduces the compute overhead, DNNs
remain critical, as they generate the initial estimate of the correspondence information. We observe that stereo DNNs make heavy
use of the deconvolution operation1
that exposes specific kernel
sparsity, making conventional DNN accelerators inefficient. While
prior work proposed specialized hardware to exploit deconvolution
sparsity [60, 76], we demonstrate that static software optimizations
achieve better results without unnecessary hardware modifications.
Our approach is to transform an inherently sparse deconvolution
layer into a sequence of dense convolutions, which can then be
executed by canonical DNN accelerators. More importantly, this
transformation uniquely exposes a new data reuse opportunity:
inter-layer activation reuse (ILAR), which does not exist in conventional DNNs. While exhaustive search has been previously used
to optimize data reuse patterns, it does not scale to optimizing deconvolution, because the transformation increases the layer count
by up to 8×, and ILAR adds another search dimension. Instead, we
propose a constrained-optimization formulation, and demonstrate
an efficient solver using dynamic programming.
We implement a software/hardware co-designed prototype of
ASV. The hardware builds on top of a conventional systolic DNN
1Deconvolution in deep learning is an incredibly unfortunate misnomer that should
really be called “transposed convolution.”
accelerator [36] implemented in 16nm technology. The ASV hardware minimally extends the baseline accelerator with less than 0.5%
area overhead. The software integrates the ISM algorithm and the
deconvolution optimizations.
We evaluate ASV on a set of standard stereo vision benchmarks.
Compared to the DNN baseline, ASV achieves 5× speedup and
85% energy saving with 0.02% accuracy loss. We also demonstrate
the general applicability of software deconvolution, by applying
it to Generative Adversarial Networks (GANs), which also make
heavy use of deconvolutions. Under the same compute and memory
resource constraints, we achieve 1.4 × speedup over a purpose-built
deconvolution accelerator, due to the unique ILAR that we exploit.
To our best knowledge, this is the first paper that demonstrates
a cost-effective stereo vision system. Using a software-hardware
co-design approach, we show that carefully designed software optimizations achieve significant performance and energy improvements with simple, principled changes to existing DNN accelerators,
which widens the applicability of our work. More specifically:
• We propose the first stereo vision algorithm, ISM, that exploits temporal invariance in stereo imaging to improve the
performance with minimal accuracy loss;
• We propose the first static optimization framework for deconvolution, a key operation in stereo DNNs, which eliminates
the sparsity-induced compute inefficiencies in deconvolution
layers without hardware changes;
• We are the first to identify inter-layer activation reuse in
deconvolution, a unique data reuse opportunity exposed by
our transformation framework, and which we exploit using
an efficient constrained optimizer.
• We co-design the hardware with the proposed software optimizations to achieve fast, low-power stereo vision with
minimal changes to existing DNN accelerators.
The remainder of the paper is organized as follows. Sec. 2 gives an
overview of necessary background. Sec. 3 introduces our invariantbased stereo matching algorithm. Sec. 4 describes the software
optimizations for efficient implementation of the deconvolution operation. Sec. 5 presents the design of ASV, including both software
and hardware considerations. Sec. 6 and Sec. 7 are experimental
methodology and results, respectively. Sec. 8 positions ASV in the
context of related work, and Sec. 9 concludes the paper.
2 Background
We first describe the scope of our work: vision-based systems that
extract 3D information from 2D stereo images (Sec. 2.1). We then
introduce the necessary background of stereo vision algorithms,
including both classic hand-crafted algorithms and contemporary
stereo DNNs (Sec. 2.2).
2.1 Depth Sensing
There are two essential methods to extract depth information: passive sensing and active sensing. Passive sensing techniques observe
the environment, primarily through cameras, and infer depth using
computer vision algorithms. In contrast, active sensing techniques
transmit signals and analyze the response to calculate depth; examples include structured light [65] and LiDAR [67].
644
Stereo Cameras
D
f
Scene Point
B
B+Z
Image
Planes Xr Xl
O O
(a) Triangulation.
Left (Reference)
Image
Right (Matching)
Image
Disparity Map
<xl
, yl
> <xr
, yr
>
<xl
, yl
>
xr
 = xl
 + D<xl
, yl
>
y
r
 = y { l
D<x, y> denotes the
pixel value of <x, y>
in the disparity map.
(b) Disparity Map.
Fig. 2: “Depth from stereo” illustration: given an image
pair, stereo matching algorithms first generate the disparity
map (b), from which depth is then calculated through triangulation (a). Triangulation is computationally trivial; this
paper focuses on optimizing stereo matching algorithms.
This paper focuses on camera-based passive sensing. Compared
to alternatives such as LiDAR, cameras are much cheaper and less
bulky [3]. As a result, camera-based depth sensing is widely adopted
in systems such as autonomous vehicles and AR headsets. According to Allied Market Research, the adoption of stereo cameras is
expected to grow 60.4% by 2020 [1]. The recent industry trend of integrating dedicated stereo vision accelerators into mobile SoCs (e.g.,
Movidius [5] and Nvidia [9]) further underlines the significance of
stereo vision for depth sensing.
2.2 Depth From Stereo
Triangulation The key idea behind stereo depth estimation is that
a single physical scene point projects to a unique pair of pixels, via
two observing cameras; the horizontal displacement between the
two pixels captured on the left and right image planes is inversely
proportional to the distance of the point from the observer (i.e.,
the depth). Fig. 2a illustrates this process, where the scene point is
captured at position x
l
and x
r on the left and right image planes,
respectively. Using similar triangles, the depth D is calculated by:
D = B f /Z, (1)
where f is the focal length of the cameras, B is the distance between the two camera lenses, and Z is the disparity x
r − x
l
, i.e.,
the horizontal displacement between the two corresponding pixels in the left and right images. This process is widely known as
triangulation [30, 61].
Stereo Matching and Disparity Map Since both B and f are
camera intrinsic parameters, the key to triangulation is to calculate the disparity Z. Given the left (reference) image and the right
(matching) image, we must find the pixels in each image that are
the projections of the same physical point, a process also known as
stereo matching. In the end, stereo matching generates a “disparity
map”, whose <x,y> coordinates are taken to be coincident with the
pixel coordinates of the reference image. Fig. 2b shows one such
example, in which the correspondence between a pixel <x
l
,y
l > in
the left image and a pixel <x
r
,y
r > in the right image is given by:
x
r = x
l + D
<x
l
,y
l >
, y
r = y
l
, (2)
FlowNetC
DispNet
GC-Net
PSMNet
0
20
40
60
80
100
Cost Distribution (%)
FE (Conv.)
Others
MO (Conv.)
DR (Deconv.)
Fig. 3: The arithmetic operation distribution of stereo
matching DNNs.
0.0 0.1 0.2
Disparity Error (pixel)
0
1
2
3
4
5
Error in Depth (m)
10m 15m 30m
Fig. 4: Depth estimation accuracy is sensitive to stereo
matching accuracy.
where D<x
l
,y
l > denotes the pixel value at <x
l
,y
l > in the disparity
map. Note that the compute cost of triangulation is trivial (Equ. 1),
and thus we focus on stereo matching.
Stereo matching algorithms consist of three stages [13, 58]: Feature Extraction (FE), Matching Optimization (MO), and Disparity
Refinement (DR). Both conventional algorithms [13, 33, 58, 63] and
DNN-based algorithms [16, 37, 48, 59, 63] follow this processing
pipeline, but differ in their implementations. Conventional methods
extract hand-crafted features (e.g., SIFT [43], HOG [20], or plain
pixel values) in the FE stage, search the feature space of both images
to find the matching pixels in the MO stage, and improve the disparity resolution in the DR stage using techniques such as iterative
gradient descent [62]. DNNs, in contrast, implement each stage
using a set of learnt parameters.
Deconvolution in Stereo DNNs Stereo matching DNNs implement FE and MO stages as convolution layers. The DR stage
fundamentally requires deconvolution (a.k.a. transposed convolution) layers [53]. Deconvolution layers generate large activation
maps from small input feature maps, essentially up-sampling the
input. The up-sampling in DR is critical to compensate the downsampling in FE and MO that scale down the input images to extract
high-level features.
To illustrate the importance of deconvolution in stereo vision, Fig. 3
shows the time distribution of four state-of-the-art stereo matching
DNNs across the three stages. The convolution and deconvolution
layers combined account for over 99% of the execution time, from
which 38.2% is attributed to the deconvolution layers.
High Accuracy Stereo Matching Stereo matching is critical
because it generates the disparity, from which depth is estimated
(Equ. 1). Using the industry-standard Bumblebee2 stereo camera [2]
as an example (B is 120 mm, f is 2.5 mm, and pixel size is 7.4 µm),
Fig. 4 shows how the depth estimation error (y-axis) varies with
the disparity error in pixels (x-axis). Different curves correspond
to objects at different distances. We find that even two tenths of a
pixel error in stereo matching can result in a depth estimation error
of 0.5m–5m, which could be catastrophic at the application level.
While existing stereo matching systems achieve high accuracy
at the expense of high compute cost, ASV achieves DNN-level
accuracy with significantly less compute.
645
3 Invariant-based Stereo Matching
This section introduces our new invariant-based stereo matching
algorithm (ISM). The key idea of ISM is to exploit the correspondence
invariant between the stereo images over time. After introducing
the high-level concept (Sec. 3.1), we then describe the detailed
algorithm (Sec. 3.2), and discuss important algorithmic design decisions (Sec. 3.3). We make the implementation of ISM available at:
https://github.com/horizon-research/ism-algorithm.
3.1 Overview
Stereo matching produces a disparity map (Fig. 2b), from which
depth information is easily obtained through triangulation (Fig. 2a).
Classic stereo matching algorithms generate the disparity map by
matching pixels/features in the left (reference) frame with pixels/features in the right (matching) frame, typically by searching in a
finite window. However, the accuracy of search-based algorithms is
sensitive to the heuristics used in the search, such as feature selection, search window size, matching criterion, etc. In contrast, DNN
approaches largely avoid heuristics and instead directly learn the
matching pairs. Unfortunately, DNNs come at the cost of a massive
increase in compute requirement.
Instead of the binary choice between DNNs and conventional
search-based algorithms, we use DNNs to guide the search process
of classic methods. The key observation is that two matched pixels,
one from the left image and the other from the right image, correspond to the same point in the physical world. While the locations
of the two pixels move from frame to frame, they are always projections of the same scene point, and therefore are always a matched
pair in any frame. In other words, the geometric correspondence
relationship between two matched pixels is invariant.
Our new stereo matching algorithm, ISM, exploits this correspondence invariant by operating in two modes. It obtains stereo
correspondences on “key frames” through accurate but computeintensive DNNs. The correspondences are then propagated to subsequent non-key frames as good initial guesses to guide the cheaper
search-based methods. By combining learnt correspondences with
search-based methods that explicitly model the physical world, ISM
reduces the total compute cost while retaining DNN-like accuracy.
3.2 Algorithm
We illustrate ISM in Fig. 5. ISM consists of four main components.
ISM runs DNN inferences ( 1 ) on key frames to obtain pixel correspondences, which are used to guide feature matching on non-key
frames ( 2 , 3 , and 4 ).
1 DNN InferenceAssuming the left and right frames at timestep
t are regarded as key frames, ISM performs DNN inference to generate a disparity map for the left image, in which each pixel value
represents the disparity (i.e., Z in Fig. 2a) of each pixel in the left
frame. In conventional DNN approaches, this disparity map is used
only for triangulation (not shown in the figure) to estimate depth,
and is discarded after the depth map is generated.
2 Reconstruct Correspondences Instead of discarding the
disparity map, ISM uses it to identify the correspondences in the
left and right frames. As per the definition of disparity (Equ. 2),
every <xt
,yt > pixel in the disparity map with the value D
<x,y>
t
indicates that the <xt
,yt > pixel in the left frame (P
L
t
) and the <xt +
L R
Disparity
Map (t)
L R
Disparity
Map (t+1)
Time
<xl
t
, yl
t
> <xr
t
, yr
t
> 1 CNN Inference
Reconstruct
Correspondence 2
<xl
t+1, yl
t+1>
<xr
t+1, yr
t+1>
3
key frame (t)
<xl
t+1, y
l
t+1> = <xl
t, yl
t
> + ΔL(t+1, t)
Pl
t Pr
t
Pl
t+1 Pr
t+1 Refine
Correspondence 4
Propagate
Correspondence
non-key frame (t+1)
<xr
t+1, y
r
t+1> = <xr
t, yr
t
> + ΔR(t+1, t)
Fig. 5: The ISM algorithm obtains correspondences in key
frames using DNNs, and propagates the correspondences to
non-key frames to guide the cheap correspondence search.
Time progresses from top to bottom in the figure.
D
<x,y>
t
,yt > pixel in the right frame (P
R
t
) form a correspondence
pair. By iterating through all the pixels in the disparity map, ISM
identifies all the correspondence pairs in the left and right frames
at timestep t.
3 Propagate Correspondences A new pair of frames arrives
at the next timestep (t + 1). ISM exploits a well-known observation
that pixels in consecutive video frames are highly-correlated in
time. For instance, P
L
t
has moved to P
L
t+1
, and P
R
t
has moved to
P
R
t+1
. Critically, since P
L
t
and P
R
t
are a correspondence pair projected
from a scene point, P
L
t+1
and P
R
t+1 must correspond to the same
point, and hence highly likely to also be a correspondence pair at
timestep (t + 1).
The exact coordinates of P
L
t+1
and P
R
t+1
can be obtained through
a motion estimation (ME) algorithm. For each pixel in the left
(right) frame, the ME algorithm generates a motion vector ∆P
L
(t+1,t)
(∆P
R
(t+1,t)
), representing the displacement between the pixel in
frame t and frame (t + 1). Thus:
P
L
t+1 = P
L
t + ∆P
L
(t+1,t)
P
R
t+1
= P
R
t+1
+ ∆P
R
(t+1,t)
4 Refine Correspondences Given the correspondence pairs
(e.g., P
L
t+1
and P
R
t+1
) at timestep (t + 1), ISM then calculates the
disparity map at (t + 1). If the motion estimation from t to (t + 1)
is precise, the propagated correspondence pairs at (t + 1) are also
precise. Accordingly, the disparity map could be simply obtained by
calculating the horizontal offsets between all the correspondence
pairs. For instance, given the correspondence pair P
L
t+1
and P
R
t+1
, the
disparity at <x
l
t+1
,y
l
t+1
> in the disparity map would be x
r
t+1
−x
l
t+1
.
In reality, motion estimation is imperfect due to various visual
artifacts such as occlusion and fast motion [42]. Thus, the correspondences propagated from t are a noisy estimate of the true
correspondences at (t + 1). To further refine the estimate of (t + 1)
in ISM, we use classic correspondence search, and initializes the
search window with the propagated correspondences. This allows
ISM to avoid compute-intensive DNNs on non-key frames without
sacrificing accuracy.
646
3.3 Algorithmic Design Decisions
Computing non-key frames requires reconstructing, propagating,
and refining correspondences. Reconstructing correspondences has
little overhead. The cost of propagating correspondences is dominated by motion estimation, and the cost of refining correspondences is dominated by the correspondence search. Thus, we must
carefully choose the motion estimation and correspondence search
algorithms such that the compute cost is much lower than DNNs
with little accuracy loss. We discuss algorithmic choices below.
Motion Estimation The literature is rich with motion estimation algorithms, which differ in the coverage and densities of estimated motion. The disparity map in stereo matching should ideally be calculated on a per-pixel basis across the frame, so as to
enable fine-grained depth estimation. This requirement rules out
many classic motion estimation algorithms such as block matching
(BM) [35], and sparse optical flow [34, 45]. BM estimates motion at
the granularity of a block of pixels, and thus does not provide the
pixel-level motion that stereo vision requires. Sparse optical flow
algorithms such as Lucas-Kanade [45] and Horn-Schunck [34] only
provide pixel-level motion for feature points such as corners, and
do not cover all the frame pixels.
Instead, we use a dense optical flow algorithm, specifically the
Farneback algorithm [21, 22], for motion estimation. Farneback
generates per-pixel motion for all the pixels, and is computationally
efficient. 99% of the compute in Farneback is due to three operations:
Gaussian blur, “Compute Flow”, and “Matrix Update”. Gaussian blur
is inherently a convolution operation that convolves a Guassian
kernel (2D matrix) with the image. The latter two are point-wise
operations that resemble the activation function in DNNs. Thus,
motion estimation in the ISM algorithm can be computed using a
DNN accelerator to simplify the hardware design.
Correspondence Search ISM performs correspondence search
to refine the initial correspondence estimation propagated through
motion. Correspondence search algorithms have been well-studied
in the classic computer vision literature [13, 58], and generally
fall into two categories: local methods and global methods. At the
cost of higher compute demand, global methods provide higher
accuracy by minimizing the pixel motion inconsistencies across the
entire image. However, with the initial correspondences propagated
through key-frames, we find that local methods suffice.
In particular, we leverage the block matching algorithm [35]
for local correspondence search. For each pixel in the left image
(e.g., P
L
t+1
in Fig. 5), ISM uses the block of pixels surrounding it
to search in a 1D window in the right image in order to find the
closest match. The search window is centered around the initial
correspondence estimation (e.g., P
R
t+1
in Fig. 5). We use the sum
of absolute differences (SAD) cost function. The horizontal offset
between the two matched blocks is the disparity for P
L
t+1
.
Similar to optical flow, the block matching algorithm has a
“convolution-like” structure [55]; the block in the left image is equivalent to a kernel, and the search window in the right image is equivalent to the input image. The only difference is that block matching
computes the SAD between the input feature map and the kernel
(
ÍN
i=1
|ai − bi
|) as opposed to the dot product in canonical convolution (ÍN
i=1
aibi
). Thus, the correspondence search can share the
same architecture as DNNs and optical flow.
Compute Cost Due to our algorithmic choices, computation on
non-key frames is much cheaper than key-frames. For instance, for
a typical qHD frame (960 × 540), computating a non-key frame requires about 87 million operations while stereo DNN inference (key
frame) requires about 102×–104× more arithmetic operations. Thus,
ISM leads to significant performance and energy improvements by
avoiding DNN inference altogether in non-key frames.
4 Deconvolution Optimizations
While the ISM algorithm removes DNN inference in non-key frames,
DNNs remain critical for generating initial key frame correspondences. This section describes optimizations for stereo DNNs, in
particular the dominant deconvolution layers. We propose novel
software-only optimizations that mitigate the compute overheads
in deconvolution (Sec. 4.1), while capturing unique data reuse opportunities (Sec. 4.2).
We make our optimization framework publicly available at: https:
//github.com/horizon-research/systolic-array-dataflow-optimizer.
It targets the systolic-array accelerator architecture, supports the
deconvolution optimization described here, and applies tiling optimizations to minimize the inference latency and/or DRAM traffic.
4.1 Deconvolution Transformation
Deconvolution layers on average contribute to 38.2% (50% max)
of the total MACs in stereo DNNs (Fig. 3). Due to the inherent
sparsity of deconvolution, a naive mapping to hardware results
in over 75% of redundant computations due to one or more zero
operands. Deconvolution is also used in Generative Adversarial
Networks (GANs), and recent studies have proposed specialized
hardware specifically for deconvolution [60, 76]. In contrast to previous studies, we propose a purely algorithmic transformation that
eliminates inefficiencies due to sparsity. We show that an inherently sparse deconvolution layer can be translated to a series of
dense convolutions, which then effectively map on to existing DNN
accelerators. We next explain the inefficiencies in deconvolution,
and then describe our algorithmic transformations.
Standard Deconvolution The standard process (Fig. 6) deconvolves a 3x3 input feature map (ifmap) with a 3x3 kernel. The ifmap
is first upsampled with zero padding, before being convolved with
the 3x3 kernel to generate an output feature map (ofmap). Note
that the upsampling step essentially performs disparity refinement,
which is fundamental to general stereo DNNs, rather than being specific to a particular network (Sec. 2.2). The zeros in the upsampled
ifmap leads to redundant computation and memory traffic.
A key characteristic of deconvolution is that different elements
in the ofmap are calculated in different “patterns.” Consider the first
2 × 2 outputs in the ofmap: (1, 1), (1, 2), (2, 1), and (2, 2). Each of
the four outputs is generated using a different set of elements from
the kernel. For instance, (1, 1) requires only e while (1, 2) requires
d and f . Critically, there are only four different patterns, which
are repeated across the ofmap. Pixels (4, 4) and (2, 2) are calculated
using the same elements from the kernel, as are (1, 1) and (5, 5), (1,
2) and (5, 4), as well as (2, 1) and (4, 5). Due to the various patterns
needed to generate different output elements, deconvolution is
clearly an “irregular” operation. Prior work [76] exploits the four
647
ofmap = ifmap kernel
(1,1) = A*e
(1,2) = A*d + B*f
(2,1) = A*b + D*h
A B C (2,2) = A*a + B*c + D*g + E*i
D E F
G H I
Upsample
(zeropadding)
ifmap
a b c
d e f
g h i
kernel
Convolve
(no padding)
1,1 1,2 1,3 1,4
2,1 2,2 2,3 2,4
3,1 3,2 3,3 3,4
4,1 4,2 4,3 4,4
1,5
2,5
3,5
4,5
5,1 5,2 5,3 5,4 5,5
ofmap
e
d f
b
h
a c
g i
Four sub-kernels
1
2
3
4
A B C
D E F
G H I
a b c
d e f
g h i
kernel
Decompose A B C
D E F
G H I
ifmap
Convolve
2,2 2,4
4,2 4,4
1,1 1,3
3,1 3,3
1,5
3,5
5,1 5,3 5,5
1,2 1,4
3,2 3,4
5,2 5,4
2,1 2,3
4,1 4,3
2,5
4,5
Gather
Four sub-ofmaps
Standard
Deconvolution
Our Algorithm
(4, 4) = E*a + F*c + H*g + I*i
(4, 5) = F*b + I*h
(5, 4) = H*d + I*f
(5, 5) = I*e
1 ⊛ifmap 2 ⊛ifmap 3 ⊛ifmap 4 ⊛ifmap
⊛
⟨
Fig. 6: Translating deconvolution into multiple convolutions. Standard deconvolution first upsamples the ifmap before convolving with the kernel. Note that this example assumes the upsampled ifmap is not further padded before the convolution,
i.e., a 7 × 7 ifmap results in a 5 × 5 ofmap. Our translation algorithm holds regardless of padding.
unique computation patterns by augmenting a conventional DNN
accelerator with custom hardware units.
Deconvolution Transformation In contrast, we find that existing DNN accelerators already provide the necessary architectural
substrate to efficiently execute the four different patterns. The key
is to recognize that the four computation patterns are essentially
four different convolutions, each convolving the original ifmap with
a distinct kernel that is part of the original kernel. For instance, (2,
2), (2, 4), (4, 2), and (4, 4) are generated by convolving  a c
д i

with
ifmap. More generally, the deconvolution in Fig. 6 is calculated as:






a b c
d e f
д h i






⊛b I = G(
e

⊛ I,

d f
⊛ I,

b
h

⊛ I,

a c
д i

⊛ I)
where ⊛b denotes the deconvolution operation, ⊛ denotes the standard convolution operation, I is the ifmap, and G is a gather operation to assemble the ofmap from the results of the four convolutions.
G is simply implemented as a set of load operations to the on-chip
buffer. Essentially, our algorithm decomposes the original 3 × 3
kernel into four sub-kernels, each requiring a smaller dense convolution with the original ifmap, which can be executed efficiently
on a conventional DNN accelerator.
This transformation generalizes to kernel shapes other than
3 × 3. Formally, a 2D kernel K with a dimension KH × KW will be
decomposed into four sub-kernels (S0, S1, S2, S3):
S
(i,j)
0
= K
(2i,2j)
i ∈ [0, ⌈KH /2⌉), j ∈ [0, ⌈KW /2⌉)
S
(i,j)
1
= K
(2i+1,2j)
i ∈ [0, ⌊KH /2⌋), j ∈ [0, ⌈KW /2⌉)
S
(i,j)
2
= K
(2i,2j+1)
i ∈ [0, ⌈KH /2⌉), j ∈ [0, ⌊KW /2⌋)
S
(i,j)
3
= K
(2i+1,2j+1)
i ∈ [0, ⌊KH /2⌋), j ∈ [0, ⌊KW /2⌋)
where S
(i,j)
∗
is the element (i, j) in a particular sub-kernel, and K
(∗,∗)
is an element in the original kernel K. For instance, S
(i,j)
0
= K
(2i,2j)
means that element (i, j) in the first sub-kernel comes from element
(2i, 2j) in the original kernel. The boundary condition of each case
denotes the dimension of the corresponding sub-kernel (notice the
different floor and ceiling functions in each). Hence, decomposing
a 3 × 3 kernel results in four sub-kernels of shapes 2 × 2, 1 × 2,
2 × 1, and 1 × 1, confirming the specific example above. The general
formulation of the deconvolution transformation with an arbitrary
N-dimensional kernel is described in Appendix A.
4.2 Exploiting Inter-Layer Activation Reuse
A beneficial trait of our transformation is that each sub-convolution
reads the same ifmap, which in modern DNNs does not fit in on-chip
buffers and must spill to main memory. In contrast, our transformation can uniquely exploit inter-layer activation reuse because
each sub-convolution layer shares the same ifmap. The challenge
is to systematically maximize the reuse exploited across the entire
network while minimizing the inference latency.
We primarily consider loop tiling, which is known to be critical
to exploiting data reuse in DNNs [52, 74]. Prior work in DNN tiling
predominately searches for the tiling strategy in a brute-force manner [32, 46]. However, brute-force search does not scale to stereo
DNNs for two reasons. First, our translation scheme significantly
increases the number of layers, each of which must be individually searched. For instance in the example of Fig. 6, the number
of layers quadruples; a 3D kernel could increase layers by 8×. Second, exploiting the inter-layer ifmap reuse adds another scheduling
dimension, further increasing the search space.
Instead of a search, we formulate the reuse optimization as a
constrained optimization problem, minimizing layer latency while
648