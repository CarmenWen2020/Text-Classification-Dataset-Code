ABSTRACT
Resource management strategies for many-core systems dictate the
sharing of resources among applications such as power, processing cores, and memory bandwidth in order to achieve system goals.
System goals require consideration of both system constraints (e.g.,
power envelope) and user demands (e.g., response time, energyefficiency). Existing approaches use heuristics, control theory, and
machine learning for resource management. They all depend on
static system models, requiring a priori knowledge of system dynamics, and are therefore too rigid to adapt to emerging workloads or
changing system dynamics.
We present SOSA, a cross-layer hardware/software hierarchical
resource manager. Low-level controllers optimize knob configurations to meet potentially conflicting objectives (e.g., maximize
throughput and minimize energy). SOSA accomplishes this for
many-core systems and unpredictable dynamic workloads by using rule-based reinforcement learning to build subsystem models
from scratch at runtime. SOSA employs a high-level supervisor to
respond to changing system goals due to operating condition, e.g.,
switch from maximizing performance to minimizing power due to a
thermal event. SOSA’s supervisor translates the system goal into lowlevel objectives (e.g., core instructions-per-second (IPS)) in order
to control subsystems by coordinating numerous knobs (e.g., core
operating frequency, task distribution) towards achieving the goal.
The software supervisor allows for flexibility, while the hardware
learners allow quick and efficient optimization.
We evaluate a simulation-based implementation of SOSA and
demonstrate SOSA’s ability to manage multiple interacting resources
in the presence of conflicting objectives, its efficiency in configuring
knobs, and adaptability in the face of unpredictable workloads. Executing a combination of machine-learning kernels and microbenchmarks on a multicore system-on-a-chip, SOSA achieves target performance with less than 1% error starting with an untrained model,
maintains the performance in the face of workload disturbance, and
automatically adapts to changing constraints at runtime. We also
demonstrate the resource manager with a hardware implementation
on an FPGA

1 INTRODUCTION
As system size and capability scale, designers face a large space
of configuration parameters controlled by actuation knobs, which
in turn generate a large number of cross-layer actuation combinations [53]. Making runtime decisions to configure knobs in order
to achieve a simple goal (e.g., maximize performance) can be challenging. That challenge is exacerbated when considering a goal
that may change throughout runtime, and consist of conflicting objectives (e.g., maximize throughput within a power budget) [46].
Additionally, modern embedded devices, like in cars, are expected to
support combinations of a wide range of applications, e.g., assistant
and entertainment systems, without any prior knowledge of their
workload.
Consider a typical entertainment system scenario in which the
resource management goal is to maximize performance within a
power budget. One could write an optimization heuristic to find
the optimal operating point for a small number of knobs, but such a
solution does not scale well to coordinate a large number of knobs. A
heuristic that does a thorough optimal estimation is not be efficient,
and one with a rougher estimation is not be flexible enough to
changes in workload.
Alternatively, we could use feedback control to adaptively configure our knobs to achieve high performance within a power budget.
Control theoretic solutions have been proposed to achieve exactly
685
MICRO ’52, October 12–16, 2019, Columbus, OH, USA Donyanavard, et al.
this goal in a variety of ways, most recently using LQG [35] and
SSV [36]. Control theoretic resource managers can coordinate knobs
with the added benefit of formalism, and can scale in cases that the
system and its knobs can be decomposed into subsystems.
Consider now that the system we are controlling experiences a
thermal event. The goal is no longer to maximize performance: the
controller should change its priority to minimize power. Classical
control theoretic solutions lack the ability to adapt to changing goals.
Rahmani et al. [39] use Supervisory Control Theory to address
the issue of dynamic goals by changing the priorities of low-level
controllers adaptively. However, designing a controller requires a
stable (sub)system model to be identified. This model is fixed and
must be known at design time. If our user installs a new application
that exercises the system in ways we do not anticipate (e.g., using
Bluetooth connectivity), it may break our resource manager. Due to
model dependency, the controller may not converge when introduced
to an unknown combination of applications.
Machine learning is well-suited to navigate large configuration
spaces. Machine learning would be useful for finding the optimal
operating point for a set of knobs in order to achieve the provided
goal. We could also build and update a model at runtime using
reinforcement learning. Reinforcement learning is not commonly
applied to scenarios such as ours due to the computational cost.
Mishra et al. [29] use reinforcement learning to tune feedback control parameters at runtime, but the learning is done periodically on
a remote server, and requires an initial model. We need the ability
to capture the dynamics of a system during execution without any
previous observation, and continuously update that model. If we
can learn system dynamics at runtime, beginning with a blank slate
(i.e., empty model), we can define (1) objectives without static relationships to subsystem states, and (2) optimization goals without
access to the targeted hardware platform. For example, we can define
objective targets in term of application-specific quality of service
(QoS) metrics (e.g. heartbeats [21], frames-per-second (FPS)), and
learn the dynamics for different applications and combinations of
applications.
In this paper, we leverage supervisory control to adapt to changing goals at runtime, in combination with reinforcement learning
hardware that allows us to efficiently optimize for any unpredictable
workload or system dynamics. To our knowledge, SOSA is the first
learning-control-hybrid resource manager to provide self-adaptivity
via software supervisor and self-optimization via hardware-based
reinforcement learning on-device. Key contributions of this paper
are:
• We provide self-adaptivity to resource managers
through hierarchical supervision, allowing the resource manager
to respond to changing system goals. We achieve self-adaptivity
through dynamic goal management by updating the policy (i.e., parameters) of low-level controllers according to high-level goal(s).
• We enable self-optimization of low-level controllers through reinforcement learning in order to adapt to unpredictable workloads.
Reinforcement learning is accomplished by implementing learning classifier tables (LCTs) as low-level controllers in hardware.
The LCTs capture system dynamics by building a model from
scratch at runtime, and continuously updating the model. LCTs
enable us to perform model-independent rule-based learning ondevice.
• Experimental Case Study: We deploy SOSA on a FPGA board
including a hardware implementation of LCTs and supervisor
to validate the function and design feasability. We compare the
effectiveness of SOSA to state-of-the-art alternatives for resource
management of multicores in a simulated environment, showing SOSA’s accuracy in meeting performance demands and responsiveness to dynamic power constraints for workloads with
unpredictable background task interference.
2 BACKGROUND AND RELATED WORK
Resource management approaches for processors can be broadly
categorized in three primary ways: (1) heuristic-based approaches
[8, 10–12, 17, 19, 23, 24, 34, 38, 47, 49], (2) control-theory-based
approaches [22, 26, 27, 30, 33, 35–37, 39–41, 44, 45], and (3) stochastic / machine-learning-based approaches [1, 6, 7, 9, 25, 29].
There exist some proposed solutions that incorporate aspects of
multiple categories, e.g., there are a number of works that use learning to build predictive models, and use heuristics to make runtime
decisions based on the predictive models [4, 13, 15, 16].
We can use a number of properties to describe the capabilities
of existing adaptive resource managers. To start, as a requirement
all approaches must be efficient enough to deploy at runtime and
be responsive, and coordinate multiple knobs to achieve one or
more objectives. Machine learning and feedback-control based approaches have the added benefit of providing formalism. Classical
control-theoretic approaches can provide robustness by guaranteeing bounded behavior. Reinforcement learning approaches can selfoptimize by continuously updating models based on observation.
Table 1 shows the coverage of existing on-chip resource management approaches in handling key issues. Some machine-learningbased and heuristic approaches (e.g., [4, 16–18]) focus on efficiency (3) and coordination (4), but fail to address other attributes
such as providing robustness (1) against unexpected corner cases.
Classical control-theoretic approaches (e.g., [35, 37]) provide means
to address robustness (1), formalism (2), and efficiency (3), with
the ability to concurrently coordinate (4) and control multiple objectives in a non-conflicting manner. However, classical control lacks
scalability (5) for heterogeneous multi-processing (HMP) architectures due to 1) the exponential growth in computational complexity
with increasing numbers of inputs and outputs, and 2) the difficulty
of performing Dynamic System Model identification for large systems. Although multiple simple controllers have been used in nested
loops to achieve scalability in simple control problems [22, 26],
they suffer from scalability issues in complex resource management
problems for many-core systems where coordination of multiple
actuators is necessary.
Recently, in Yukta [36], Pothukuchi et al. solve the scalability (5)
issue of classical controllers by using Robust Control and hierarchically linking controllers to perform resource management at various
layers in the system stack. However, Yukta, like classical controllers,
lacks self-adaptivity (6), which enables rapid responses to abrupt
runtime changes. In SPECTR [39], Rahmani et al. also solve the
scalability issue via hierarchy. SPECTR uses Supervisory Control
Theory at the top of its hierarchy which additionally provides selfadaptivity (6) in conjunction with classical controllers by coordinating their reference values and updating priorities dynamically.
686
SOSA MICRO ’52, October 12–16, 2019, Columbus, OH, USA
Methods Estimation-/Modelbased Heuristics
Classical Control
Theory
Machine
Learning
Hierarchical
Control
Hybrid Control
+ Machine Learning
[10, 11, 13, 17, 24] [22, 26, 35, 40, 41] [4, 16, 18] [36, 39] [SOSA]
1. Robustness ✓ ✓
2. Formalism ✓ ✓ ✓ ✓
3. Efficiency ✓ ✓ ✓ ✓ ✓
4. Coordination ✓ ✓ ✓ ✓ ✓
5. Scalability ✓ ∗
6. Self-Adaptivity ✓ ∗
7. Self-Optimization ✓ ✓
8. Model-Independence ✓ ∗
Table 1: Major on-chip resource management approaches and the key challenges they address (∗ = uniquely addressed by SOSA).
In this paper, we deploy a hierarchical supervisory controller in order to provide scalability and self-adaptivity. We use rule-based reinforcement learning by deploying Learning Classifier Tables, or LCTs,
as low-level controllers. The LCTs provide self-optimization (7)
and model-independence (8) by continuously updating the optimal
configuration(s) based on runtime observation. Theoretical investigation into managing DVFS using reinforcement learning [6] for
a single objective has been promising. Prior to our work, Mishra
et al. proposed a learning and control hybrid resource manager in
CALOREE [29]. CALOREE uses a predictive model to optimize the
control parameters for the controller making decisions. CALOREE
requires an initial model trained ahead of execution. The model
is updated using reinforcement learning at runtime, however, the
learning is done off-device, and requires communication with a remote server. Continuously updating a statistical model on device
was applied by Kasture et al. in [25] to control DVFS in datacenters
for latency-critical workloads. Compared to SOSA their approach is
neither self-adaptive to runtime changes, nor provides coordination
of several conflicting objectives.
3 MOTIVATION
3.1 Challenges of Model-dependence
Consider the DVFS feedback controller shown in Figure 1. The
controller sets the operating frequency (and voltage) of a single-core
system to achieve a desired heartbeat rate. The heartbeat rate is a
quality-of-service (QoS) metric the application designer specifies
through source code annotation [21].
In the case of control theory, the controller is designed based on
a static model that identifies the achievable heartbeat based on the
operating frequency. This assumes that a physical system is available
for observation of system dynamics, which is required to generate
the model used to design the controller ahead of deployment. Furthermore, the frequency→HB relationship is application-specific. In
other words, (a) workloads must be known ahead of design time, (b)
systems must be available for observation of known workloads, and
(c) each resulting controller only applies to the specific workload it
was designed for. These are impractical assumptions when designing
controllers for general-purpose systems with dynamic and unpredictable workloads. More challenges arise due to changes in system
dynamics over time or between devices. Consider the effects of process variability on the behavior of different devices with respect to
DVFS
Controller + System
-
Frequency
(Voltage)
Actuators
Control Input
Reference
Heartbeat Reference Heartbeat
Heartbeat
(IPS)
Sensors
Measured Output
Figure 1: Feedback controller with frequency as control input
and heartbeat [21] rate as measured output.
operating frequency and voltage. It is impractical to expect an optimized model to be derived at design time for each device that utilizes
the controller. As a result, a system may display non-ideal behavior
according to the model used to design the controller. The controller
would make potentially poor decisions due to an inaccurate model
[14].
The challenges outlined so far assume that the system dynamics
being modeled can be estimated with a simple linear equation, which
is the case for the frequency→HB controller. However, some knobs
have more complex system dynamics and are not practical to model
with discrete difference equations, e.g., task migration. Complex
models with large configuration spaces such as task migration are
proper candidates to apply learning. It is important to manage the
scale of a complex model if it is to be learned at runtime. A modelbased learner that uses a static model would face the same challenges
as described thus far. To solve these issues we can employ online
reinforcement learning. Online reinforcement learning addresses the
static-model challenges by continuously updating the system model
based on runtime observations. If we can implement such a learner
on-device, it can capture complex dynamics such as task migration.
3.2 Benefits of Reinforcement Learning
Consider again the DVFS feedback controller shown in Figure 1. We
implement the feedback controller in two different ways: (1) using
single-input-single-output (SISO) control theory, and (2) using rulebased reinforcement learning (LCT). Figure 2 shows the accuracy
achieved by SISO (blue) and LCT (orange) controllers tracking a
specified HB for the k-means clustering algorithm executing on a
simulated ARM core (detailed in Section 6.1). The SISO begins
with an error of 20 %, and is able to eventually reduce the error to
less than 5 % after two seconds of execution. The LCT begins with
near 100 % error, and is able to to reduce the error to 15% after two
687
MICRO ’52, October 12–16, 2019, Columbus, OH, USA Donyanavard, et al.
0 1 2 3 4 5
0
20
40
60
80
100
Time (seconds)
Error (%)
Measured Error - SISO Measured Error - LCT Controller
Figure 2: Accuracy of classical (SISO) and learning (LCT) controllers tracking application heartbeat rate using core operating frequency.
seconds of execution, and eventually down to 7 % after five seconds.
Although the SISO is robust, its design requires a model at design
time. The LCT is a blank-slate that learns the model during execution,
which is why it begins with nearly 100% error. In this instance we
did not tune LCT parameters or optimize rules – with some design
iterations, we could reduce the error further. The LCT’s ability to
learn to manage HB on the fly indicates that there is opportunity
to exploit this approach to coordinate knobs for subsystems in the
context of a resource management hierarchy.
In the example, we define both the LCT and SISO objectives in
terms of instructions-per-second (IPS), and use a HB→IPS converter
to set HB references. Although the converter is a requirement for the
classical controller due to its inability to adapt to each application’s
unique frequency→HB model, it is not a restriction on the LCT. Our
design decision is made for fairness in the comparison, but the online
learning done by the LCT allows it to define its objective in terms
of HB directly, with the ability to adapt to different applications.
A controller using a fixed model (e.g., classical controller) simply
cannot model the relationship between application-level metrics
to hardware-level knobs at runtime for a dynamic workload. The
ability to specify an objective for a low-level controller in terms of
an application-specific user-defined metric is a significant advantage
when providing self-optimization.
The final advantage of self-optimization through reinforcement
learning over classical control theory is the ability to minimize or
maximize objectives, as opposed to achieve a fixed setpoint. Our
example is a simple one with a single objective (HB), but still requires an achievable setpoint for the classical controller to behave
desirably, meaning an optimizer is required to calculate the desired
setpoint. The learner can minimize or maximize a given objective,
integrating the functionality of an optimizer.
3.3 Hardware Efficiency
The LCT hardware implementation allows for efficient runtime reinforcement learning and has distinct advantages over software controllers. For one, the LCT has direct access to hardware sensors
and actuators, enabling much shorter periods between invokations
(epochs). Shorter epochs means that finer-grained actuations can
be supported by the LCT. The LCT can still support more coarsegrained actuations over longer epochs that involve software sensors
or actuators. For some actuations, short epochs only enabled by hardware are required to provide a sufficient sampling rate to support
MON/SEN CORE 3 ACTUATORS3
LLCs
MON/SEN CORE 2 ACTUATORS2
LLCs
Control
Feedback
Updates
Control Params
Variable Goals / Policies
Supervisor
System Model
LCT1
Condition Action Fitness
X01X 1001 0.5
100X 0010 0.33
MPSoC
SENSORS1 CORE1 ACTUATORS1
User Input Operating Conditions
Info
Figure 3: SOSA hierarchical overview on a multicore processor.
SOSA components in the shaded region.
the learning. The optimal rules must be learned through numerous
observations, which may take too long to be effective in a software
implemented controller.
4 SOSA
SOSA is a hierarchical resource manager (Section 4.1) consisting
of a high-level supervisor (Section 4.2) that guides distributed lowlevel controllers (Section 4.3) to achieve a global goal. We design the
entire hierarchy in the context of our case study: managing QoS of a
focus application within a power budget on an MPSoC (Section 4.4).
4.1 Hierarchical System Architecture
Figure 3 depicts a high-level view of SOSA for many-core system
resource management. Either the user or the system software may
specify Variable Goals and Policies. The Supervisor aims to achieve
goals by managing the low-level controllers. High-level decisions
are made based on the feedback given by the high-level model, or
System Model, which provides an abstraction of the entire system.
Low-level controllers are implemented as LCTs, which control the
system via actuators. The supervisor provides objective-function parameters such as output references (i.e., target values) or constraints
to each LCT during runtime according to the system policy. Actions taken by the LCTs indirectly update the system model through
senors to maintain the global system state, and potentially trigger
the supervisor to take action. The high-level model can be designed
in various fashions (e.g., rule-based or estimator-based [20, 31, 43])
to track the system state and provide the supervisor with guidelines.
4.2 Supervisor
We use Supervisory Control Theory (SCT) [42] to design our supervisor similarly to [39]. SCT solves complex synthesis problems
by breaking them into small-scale sub-problems, known as modular
synthesis. The results of modular synthesis characterize the conditions under which decomposition is effective. In particular, results
identify whether a valid decomposition exists. A decomposition is
valid if the solutions to sub-problems combine to solve the original
688
SOSA MICRO ’52, October 12–16, 2019, Columbus, OH, USA
problem, and the resulting composite supervisors are non-blocking
and minimally restrictive. This horizontal decomposition in subproblems allows us to divide the overall system into several smaller
subsystems which are controlled by individual controllers.
Figure 4 illustrates how a supervisory control structure can hierarchically manage feedback control loops. As shown in the figure,
supervision is vertically decomposed into tasks performed at different levels of abstraction [48]. The supervisory controller is designed
to control the high-level system model, which represents an abstraction of the system. The subsystems compose the pre-existing system
that does not meet the given specifications without the aid of a controller or a supervisor. The information channel provides information
about the updates in the high-level model to the supervisory controller. Due to the fact that the system model is an abstract model,
the controlling channel is an indirect virtual control channel. In
other words, the control decisions of the supervisory controller will
be implemented by controlling the low-level controller(s) through
control parameters. Consequently, the low-level controller(s) can
control one or multiple subsystems using the control channel and
gather information via feedback. The changes in the subsystems can
trigger model updates in the state of the high-level system model.
These updates reflect the results of low-level controllers’ controlling
actions.
The scheme of Figure 4 describes the division of supervision
into high-level management and low-level operational supervision.
Virtual control exercised via the high-level control channel can be implemented by modifying control parameters to adaptively coordinate
the low-level controllers, e.g., by adjusting their objective functions
according to the system goal. The combination of horizontal and
vertical decomposition enables us to not only physical divide the system into subsystems, but also to logically divide the sub-problems in
any appropriate way, e.g., due to varying epochs (control invocation
period) or scope. The important requirement of this hierarchical
control scheme is control consistency and hierarchical consistency
between the high-level model and the low-level system, as defined
in the standard Ramadge-Wonham control mechanism [48]. For a
detailed description of SCT, we refer the reader to [2, 42, 43, 48].
Low-level Subsystem
Controller
Low-level
Controller
Low-level
Controller
Subsystem
Supervisory
Controller
System
Model
Control
Parameters
Model
Updates
Information
Virtual
Control
Feedback
Control
Leaf Controllers System
Figure 4: Supervisory Control structure. Low level control
loops are guided by the Supervisory Controller that achieves
system-wide goals based on the high-level system model.
4.3 Learning Classifier Table Low-Level
Controllers
In general, a controller’s task is to find the optimal actuation knob
configuration for a given system state. The optimum is defined by
some (measurable) metric, the objective function. The objective
function (δ) can take one or more objectives (δ1,δ2,...,δn) into
account
δ

δ1,δ2,...,δn,...
(1)
to compare the desirability of different system states. In our hierarchical setup, the low level controllers’ task is to execute actions that
result in a more desirable subsystem state over time according to the
given objective function.
Zeppenfeld and Herkersdorf use learning classifier tables (LCTs)
as low-level controllers in ASoC [52], which is an approach exploiting autonomic principles for runtime task distribution and frequency
scaling. In ASoC, low-level controllers are unsupervised. The overall system goal emerges from the controllers’ objective function
definition and coordination.
LCTs are a subset of Wilson’s XCS [51], which is a type of
learning classifier system (LCS). LCSs describe the system they
control by ’if condition then action’ rules, i.e., classifiers. The
condition corresponds to a specific set(s) of possible sensor values
(i.e., system state). The action modifies the actual knob configuration to change system settings. Each rule contains a fitness value
which describes its capability to improve the system state according to the objective function. A rule’s fitness is updated each time
the rule’s condition matches the system state and its action is applied. The fitness is updated based on the rule’s effectiveness toward
achieving the objective. This is how the LCT builds and updates the
system model, without any initial training required.
A set of rules, population [P], is needed to model system dynamics.
Different rules may have overlapping conditions or actions. Figure 5
represents the general operating mode of LCS implemented as an
Population [P]
idx cond action fitness ...
#1 101X 101 0.23
#2 X001 001 0.58
#3 1X0X 011 0.02
··· ··· ··· ···
#n XXX1 111 0.89
Match Set [M]
idx cond action fitness ...
#8 1X1X 011 0.12
#53 X111 001 0.56
#116 1111 011 0.52
Action Set [A]
idx cond action fitness ...
#8 1X1X 011 0.12
#116 1111 011 0.52
Credit
Assignement
[A]t−1
Action
Selection
Algorithm
Sensors
Actuators
System
Feedback 1
Roulette Wheel 2
Reward Update 5
3
Action performed
Control
Objectives
4
Genetic
Alg.
Figure 5: Overview of the LCT logic (dashed lines correspond
to the fitness update path; dotted entities are part of LCSs, but
not LCTs).
68