ABSTRACT
Processing-In-Memory (PIM) architectures based on recent technology advances (e.g., Hybrid Memory Cube) demonstrate great
potential for graph processing. However, existing solutions did
not address the key challenge of graph processing—irregular data
movements.
This paper proposes GraphQ, an improved PIM-based graph
processing architecture over recent architecture Tesseract, that fundamentally eliminates irregular data movements. GraphQ is inspired
by ideas from distributed graph processing and irregular applications to enable static and structured communication with runtime
and architecture co-design. Specifically, GraphQ realizes: 1) batched
and overlapped inter-cube communication by reordering vertex processing order; 2) streamlined inter-cube communication by using
heterogeneous cores for different access types. Moreover, to tackle
the discrepancy between inter-cube and inter-node bandwidth, we
propose a hybrid execution model that performs additional local
computation during the inter-node communication. This model
is general enough and applicable to asynchronous iterative algorithms that can tolerate bounded stale values. Putting all together,
GraphQ simultaneously maximizes intra-cube, inter-cube, and internode communication throughput. In a zSim-based simulator with
five real-world graphs and four algorithms, GraphQ achieves on
average 3.3× and maximum 13.9× speedup, 81% energy saving compared with Tesseract. We show that increasing memory size in PIM
also proportionally increases compute capability: a 4-node GraphQ
achieves 98.34× speedup compared with a single node with the
same memory size and conventional memory hierarchy.
CCS CONCEPTS
• Computer systems organization → Parallel architectures; •
Hardware → 3D integrated circuits; Emerging architectures.
KEYWORDS
graph analytics, data movement, memory systems, 3D-stacked
memory, processing-in-memory, near-data processing
1 INTRODUCTION
Graphs capture relationships between data items, such as interactions or dependencies. Graph analytics have emerged as an important way to understand the relationships between heterogeneous
types of data, allowing data analysts to draw valuable insights from
patterns in the data for a wide range of applications, including machine learning tasks [69], natural language processing [4, 20, 70],
anomaly detection [52, 62, 68], clustering [55, 58], recommendation [17, 21, 38, 44], social influence analysis [9, 63, 67], bioinformatics [3, 16, 29].
To improve programmability, graph-oriented programming model,
e.g., vertex program [26, 40, 43] can easily express graph algorithms
by allowing programmers to “think as the vertex”. Programmers
can express algorithms in vertex and edge function based on neighbor vertices. A graph can be represented as an adjacency matrix,
where each element represents an edge between a source and a
destination. Typically, the adjacency matrix is sparse and stored
in compressed representation. One example is Compressed Sparse
Row (CSR), which has three arrays: 1) vertex array stores vertices
sequentially with each entry pointing to the start of the vertex’s
outgoing edge list; 2) edge array stores the edges of each vertex
sequentially; and 3) compute array keeps the updates of the destination vertex when each edge is processed. The accesses to vertex
and edge array are mostly sequential, but accesses to compute array are random. Moreover, graph algorithms require high memory
bandwidth since they perform a small amount of computation on
randomly accessed data.
In essence, the two problems are both irregular data movements
in conventional memory hierarchy. Processing-In-Memory (PIM)
can reduce data movement between memory and computation by
placing computing logic inside memory dies. Though once believed
to be impractical, PIM recently becomes an attractive architecture due to the emerging 3D stacked memory technology, such as
712
MICRO ’52, October 12–16, 2019, Columbus, OH, USA Zhuo, et al.
Hybrid Memory Cube (HMC) [12] and High Bandwidth Memory
(HBM) [31]. In general, the architecture is composed of multiple
memory cubes connected by external links (e.g., SerDes links in
HMC with 120GB/s per link). Within each cube, multiple DRAM
dies are stacked with Through Silicon Via (TSV) and provide higher
internal memory bandwidth up to 320GB/s. At the bottom of the
dies, computation logic (e.g., simple cores) can be embedded. Performing computation at in-memory compute logic can reduce data
movements in memory hierarchy. More importantly, PIM provides
“memory-capacity-proportional” bandwidth and scalability.
Tesseract [1] is a PIM-based graph processing architecture that
supports vertex programming model with architectural primitives
to enable inter-cube communication. GraphP [72] is another architecture that co-designs the programming model and architecture to
reduce inter-cube communication. The two schemes only reduce
data movement but did not change its irregular nature, which has
two implications.
The first problem is the high overhead in handling small messages in inter-cube communication with unpredictable arrival time.
This communication is determined by graph partition, i.e., the graph
data is partitioned into memory cubes. For an edge, if source and
destination vertices are assigned to different cubes, inter-cube communication is required to update the corresponding remote compute
array entry. Such communication is unpredictable since it depends
on the processing order of destination vertices. In Tesseract, remote
cubes handles inter-cube message with interrupt and executes a
function to perform the update. Due to the unpredictable message
arrival time, it incurs performance overhead since the receiver
cube’s local execution is interrupted. In addition, it will lead to load
imbalance since some cube can receive more messages. It is ideal if
each cube knows when the messages will arrive from which source.
Second, processing cores in a cube perform both sequential accesses to vertex and edge array and random accesses to compute
array. This access pattern destructs cache locality with interference. Moreover, locality is further affected by performing remote
compute array update requests during inter-cube communication.
Third, all prior architectures are based on a single PIM node.
To support multi-node, while vertex programming model requires
no change, the runtime system can be transparently extended to
map remote destination vertex updates to either inter-cube or internode communication, the key challenge is the substantial lower
bandwidth of inter-node communication (6GB/s) compared to intercube (120GB/s) and intra-cube (360GB/s) communication.
To address the challenges, this paper proposes GraphQ, an improved PIM-based graph processing architecture over the recent
architecture Tesseract that eliminates irregular data movements.
GraphQ is inspired by ideas from distributed graph processing and
irregular applications to enable static and structured communication with runtime and architecture co-design. Specifically, GraphQ
realizes: 1) batched and overlapped inter-cube communication by
reordering vertex processing order; 2) streamlined inter-cube communication by using heterogeneous cores for different access types
to eliminate the interference. Moreover, GraphQ is the first PIMbased graph processing architecture that supports multi-node. To
tackle the discrepancy between inter-cube and inter-node communication bandwidth, we propose a hybrid execution model that
is a midpoint between synchronous and asynchronous execution,
performing additional local iterations during the long inter-node
communication. This model is general enough and can be applied
to asynchronous iterative algorithms tolerate bounded stale values [66]. Putting all together, GraphQ simultaneously maximizes
intra-cube, inter-cube, and inter-node communication throughput.
No code modification is required since the runtime system transparently realizes the ideas with minor architecture supports.
GraphH [13] is another PIM graph processing architecture that
uses interconnection reconfiguration and relies on the host processor to control the switch status of all connections. Perhaps it is most
closely related since it also reduces the irregularity of inter-cube
data movement. Unlike GraphQ, GraphH only improves inter-cube
communication and is purely implemented in hardware and cannot
enjoy the flexibility of our co-designed approach.
We evaluate GraphQ with a zSim-based simulator using five realworld graphs and four algorithms, the results show that GraphQ
achieves on average 3.3× and maximum 13.9× speedup, 81% energy
saving compared with Tesseract. Comparing with GraphP, GraphQ
achieves more speedup to Tesseract. With the hybrid model, a
4-node GraphQ achieves an average speedup of 2.98× compared
with single-node GraphQ, and 98.34× speedup compared with a
single node with the same memory size using conventional memory
hierarchy.
2 BACKGROUND AND MOTIVATIONS
2.1 Basics of Graph Processing
Table 1: Vertex Programming APIs
Function Input Output
processEdge source vertex value partial update
reduce reduced/partial update reduced update
apply reduced update/old value new value
1 for (v ← Graph.vertices) {
2 for (e ← outEdges(v)) {
3 res = processEdge(e, v.value, ...)
4 u ← comp[e.dest]
5 u.temp = reduce(u.temp, res)
6 }
7 }
8 for (v ← Graph.vertices) {
9 v.value, v.active = apply(comp[v].temp, v.value)
10 }
Figure 1: Vertex Programming Model
A graph G is defined as an ordered pair (V, E), where V is a set
of vertices connected by E, a set of edges. To ease the development
of graph algorithms, several domain-specific programming models
based on “think like a vertex” principle are proposed, such as vertex
program [40], gather-apply-scatter program [19], amorphous dataparallel program [51] and some other frameworks [57]. Among
them, vertex program is supported by many software and hardware
accelerated graph processing frameworks, including Tesseract [1],
GraphLab [39], and Graphicionado [22]. Figure 1 lists the semantics
of three APIs of vertex program. Figure 1 shows a general graph
application expressed with these primitives.
713
GraphQ: Scalable PIM-Based Graph Processing MICRO ’52, October 12–16, 2019, Columbus, OH, USA
Vault
Cube
Inter-Cube bandwidth:
120GB/s per link
……
DRAM Layers
Logic Layer
Through Silicon
Via (TSV)
Intra-Cube bandwidth:
360GB/s
10GB/s
Processing-In-Memory (PIM)
Node 0 Node 1 Node 2 Node 3
Inter-Node bandwidth: 6GB/s
Figure 2: Processing-In-Memory Architecture
During processing, each vertex in vertex array is visited and all its
outgoing edges in edge array are processed, involving three steps. 1)
process: for each outgoing edge e of vertex v, function processEdge
computes the contribution of source vertex v through edge e to
the destination vertex u (accessed in line 4). 2) reduce: from the
perspective of u, a new update returned by processEdge (res) is
combined using a reduce function with the existing value of u in
compute array, i.e., u.temp, incurring a random access. 3) apply:
after the whole graph is processed in an iteration, the new value
of each vertex in compute array is applied to vertex array with
the apply function. In an iterative graph algorithm, the procedure
repeats multiple iterations until certain convergence condition has
been reached.
We can summarize two characteristics of graph processing: random accesses to compute array (line 4); the high ratio of memory
accesses to computation (processEdge is typically simple). As a
result, graph algorithms incur random accesses and require high
memory bandwidth.
2.2 Processing-In-Memory
Processing-In-Memory (PIM) architecture reduces data movements
by performing computations close to where the data are stored.
3D memory technologies (e.g., Hybrid Memory Cubes (HMC) [12]
and High Bandwidth Memory (HBM) [31]) make PIM feasible by
integrating memory dies and compute logic in the same package,
achieving high memory bandwidth and low latency.
Similar to Tesseract and GraphP, this paper considers a general
PIM architecture (shown in Figure 2) that captures key features
of specific PIM implementations. The architecture is composed of
multiple cubes connected by external links (e.g., SerDes links in
HMC with 120GB/s per link). Within each cube, multiple DRAM
dies are stacked with Through Silicon Via (TSV) and provide higher
internal memory bandwidth up to 320GB/s. At the bottom of the
dies, computational logics (e.g., simple cores) could be embedded.
In Tesseract [1], a small single-issue in-order core is placed at the
logic die of each vault. It is feasible because the area of 32 ARM
Cortex-A5 processors including an FPU (0.68 mm2
for each core [5])
corresponds to only 9.6% of the area of an 8 Gb DRAM die area (e.g.,
226 mm2
[56]). GraphQ assumes the same setting. With 16 cubes,
the whole system delivers 5 TB/s memory bandwidth, considerably
larger than conventional memory systems. Moreover, the memory
bandwidth grows proportionally with capacity in a scalable manner.
2.3 PIM-Based Graph Processing
Tesseract [1] is a PIM-based graph processing accelerator with 16
cubes. Tesseract provides low-level primitives to support vertex
program model. For each vertex, the program iterates over all its
edges/neighbors and executes a put function for each of them. The
signature of this put function is put(id, void* func, void*
destination vertices
source vertices
cube 0
cube 1
cube 2
cube 3
i
j
j
irregular message from
cube 0 ➛ cube 2
1 ➔ 2
1 ➔ 5
1 ➔ 8
2 ➔ 1
2 ➔ 5
2 ➔ 3
2 ➔ 7
3 ➔ 2
3 ➔ 5
3 ➔ 8
5 ➔ 6
8 ➔ 9
E,QWUDFXEHDFFHVVHV sequential source vertex accesses
1 2
1
sequential edge accesses
2
3
random destination vertex accesses
3
D,QWHUFXEHFRPPXQLFDWLRQ
remote func (v)
process
reduce
apply
batching
Figure 3: Tesseract Communication and Access Pattern
arg, size_t arg_size, void* prefethch_addr). It executes a
function call func with argument arg on the id-th cube, therefore
it could be either 1) a remote call if the destination vertex resides
on a different cube from source vertex; or otherwise 2) a local
function call. In the end, a barrier ensures that all operations in
one iteration are performed before the next iteration.
Figure 3 (a) shows the inter-cube communication in an adjacency matrix view, where the rows and columns correspond to the
source and destination vertex of edges, and each dot is an edge. All
vertices are partitioned among four cubes—each cube is assigned
to a set of rows. The circled dot represents an edge from a vertex
in cube 0 to a vertex in cube 2: (vi → vj
), which corresponds to an
inter-cube message from cube 0 to cube 2. Thus, each edge across
cubes incurs such a message and the destination cube is determined
by the graph structure (e.g., the destination of the edge). These small
and irregular inter-cube messages are generated during execution
to unpredictable destination cube at any time.
On the receiver side, the core is interrupted to execute the remote
function, incurring overhead due to context switch. Tesseract uses
batching to mitigate interrupt overhead by buffering the received
remote function calls in a queue and executing multiple functions
together at certain point later. It can be seen as the square in Figure 3
(a): the functions corresponding to the edges inside the square are
executed in batch by a core in remote cube. Due to the large number
of inter-cube messages, batches generated are too small to offset
the performance impact of interrupt overhead.
Moreover, irregular communication between cubes may incur
imbalanced load and hardware utilization. Due to graph-dependent
communication pattern, when messages are sent to the same cube
from different senders, its message queues may become full and put
backpressure on the network to prevent senders from generating
more messages. In this case, cores in the receiver cube will be
overwhelmed by handling remote function call requests without
making progress in processing its local data.
Finally, the dynamic communication pattern leads to excessive
energy consumption of inter-cube links. To save energy, each intercube link can be set to a low-power state (e.g., the Power-Down
mode in HMC [23]). However, this optimization is not applicable
to the scenario when the message can be sent at any time.
Figure 3 shows the problem with intra-cube data movement.
If the destination of an edge is in the local cube, a local apply
is performed, which incurs random accesses and causes locality
interference. Specifically, accesses to vertex array (❶) and edge
array (❷) are sequential reads. However, the accesses to compute
array for the destination vertices are random (❸). Besides, remote
function call also incur random accesses.
714