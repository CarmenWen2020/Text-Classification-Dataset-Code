ABSTRACT
Recent studies from several hyperscalars pinpoint to embedding
layers as the most memory-intensive deep learning (DL) algorithm
being deployed in today’s datacenters. This paper addresses the
memory capacity and bandwidth challenges of embedding layers
and the associated tensor operations. We present our vertically
integrated hardware/software co-design, which includes a custom
DIMM module enhanced with near-memory processing cores tailored for DL tensor operations. These custom DIMMs are populated
inside a GPU-centric system interconnect as a remote memory pool,
allowing GPUs to utilize for scalable memory bandwidth and capacity expansion. A prototype implementation of our proposal on real
DL systems shows an average 6.2−17.6× performance improvement
on state-of-the-art DNN-based recommender systems.
CCS CONCEPTS
• Computer systems organization → Parallel architectures.
KEYWORDS
System architecture, memory architecture, near-memory processing, DIMM, machine learning, neural network, graphics processing
unit (GPU), neural processing unit (NPU)

1 INTRODUCTION
Machine learning (ML) algorithms based on deep neural networks
(DNNs), also known as deep learning (DL), is scaling up rapidly. To
satisfy the computation needs of DL practitioners, GPUs or customdesigned accelerators for DNNs, also known as neural processing
units (NPUs) are widely being deployed for accelerating DL workloads. Despite prior studies on enhancing the compute throughput
of GPUs/NPUs [3, 5, 8, 10, 11, 33, 34, 63], a research theme that has
Figure 1: Topological structure of emerging DL applications. The
figure is reproduced from Facebook’s keynote speech at the Open
Compute Project summit 2019 [67], which calls out for architectural
solutions providing “High memory bandwidth and capacity for embeddings”. This paper specifically addresses this important memory
wall problem in emerging DL applications: i.e., the non-MLP portions (yellow) in this figure.
received relatively less attention is how computer system architects should go about tackling the “memory wall” problem in DL:
emerging DL algorithms demand both high memory capacity and
bandwidth, limiting the types of applications that can be deployed
under practical constraints. In particular, recent studies from several hyperscalars [15, 24, 27, 52, 64] pinpoint to embedding lookups
and tensor manipulations (aka embedding layers, Section 2.3) as the
most memory-intensive algorithm deployed in today’s datacenters,
already reaching several hundreds of GBs of memory footprint,
even for inference. Common wisdom in conventional DL workloads (e.g., convolutional and recurrent neural networks, CNNs and
RNNs) was that convolutions or matrix-multiplications account for
the majority of inference time [25, 63]. However, emerging DL applications employing embedding layers exhibit drastically different
characteristics as the embedding lookups and tensor manipulations
(i.e., feature interaction in Figure 1) account for a significant fraction
of execution time. Facebook for instance states that embedding layers take up 34% of the execution time of all DL workloads deployed
in their datacenters [64].
Given this landscape, this paper focuses on addressing the memory capacity and bandwidth challenges of embedding layers (Figure 1). Specifically, we focus our attention on recommender systems [1] using embeddings which is one of the most common DL
workloads deployed in today’s datacenters for numerous application domains like advertisements, movie/music recommendations,
news feed, and etc. As detailed in Section 2.3, the model size of
embedding layers (typically several hundreds of GBs [27, 64]) far
740
MICRO ’52, October 12–16, 2019, Columbus, OH, USA Youngeun Kwon, Yunjae Lee, Minsoo Rhu
exceeds the memory capacity of GPUs. As a result, the solution
vendors take is to store the entire embedding lookup table inside
the capacity-optimized, low-bandwidth CPU memory and deploy
the application 1) by only using CPUs for the entire computation
of DNN inference, or 2) by employing a hybrid CPU-GPU approach
where the embedding lookups are conducted on the CPU but the
rest are handled on the GPU. We observe that both these approaches
leave significant performance left on the table, experiencing an average 7.3−20.9× slowdown compared to a hypothetical, GPU-only
version which assumes the entire embeddings can be stored in the
GPU memory (Section 3.2). Through a detailed application characterization study, we root-cause the reason behind such performance
loss to the following factors. First, the embedding vectors are read
out using the low-bandwidth CPU memory, incurring significant
latency overheads compared to when the embeddings are read
out using the bandwidth-optimized GPU memory. Second, the low
computation throughput of CPUs can significantly slowdown the
computation-heavy DNN execution step when solely relying on
CPUs, whereas the hybrid CPU-GPU version can suffer from the
latencies in copying the embeddings from CPU to GPU memory
over the thin PCIe channel.
To tackle these challenges, we present a vertically integrated,
hardware/software co-design that fundamentally addresses the
memory (capacity and bandwidth) wall problem of embedding
layers. Our proposal encompasses multiple levels in the hardware
and software stack as detailed below.
(Micro)architecture. We present TensorDIMM, which is based
on commodity buffered DIMMs but further enhanced with nearmemory processing (NMP) units customized for key DL tensor operations, such as embedding gathers and reductions. The NMP units
in TensorDIMM are designed to conduct embedding gathers and
reduction operations “near-memory” which drastically reduces the
latency in fetching the embedding vectors and reducing them, providing significant improvements in effective communication bandwidth and performance. Additionally, our TensorDIMM leverages
commodity DRAM devices as-is, so another key advantage of our
proposal as opposed to prior in-memory architectures [72, 76, 77]
is its practicality and ease of implementation.
ISA extension and runtime system. Building on top of our
TensorDIMM architecture, we propose a custom tensor ISA and runtime system that provides scalable memory bandwidth and capacity
expansion for embedding layers. Our proposal entails 1) a carefully
designed ISA tailored for DL tensor operations (TensorISA), 2) an
efficient address mapping scheme for embeddings, and 3) a runtime
system that effectively utilizes TensorDIMMs for tensor operations.
The TensorISA has been designed from the ground-up to conduct
key DL tensor operations in a memory bandwidth efficient manner.
An important challenge with conventional memory systems is that,
regardless of how many DIMMs/ranks are physically available per
each memory channel, the maximum memory bandwidth provided
to the memory controller is fixed. TensorISA has been carefully codesigned with both TensorDIMM and the address mapping scheme
so that the aggregate memory bandwidth provided to our NMP
units increases proportional to the number of TensorDIMMs. In effect, our proposal offers a platform for scalable memory bandwidth
expansion for embedding layers. Compared to the baseline system,
our default TensorDIMM configuration offers an average 4× increase
in memory bandwidth for key DL tensor operations.
System architecture. Our final proposition is to aggregate
a pool of TensorDIMM modules as a disaggregated memory node
(henceforth referred to as TensorNode) in order to provide scalable memory capacity expansion. A key to our proposal is that
TensorNode is interfaced inside the NVLINK-compatible, GPU-side
high-bandwidth system interconnect1
. In state-of-the-art DL systems, the GPUs are connected to a high-bandwidth switch such
as NVSwitch [62] which allows high-bandwidth, low-latency data
transfers between any pair of GPUs. Our proposal utilizes a pooled
memory architecture “inside” the high-bandwidth GPU-side interconnect, which is fully populated with capacity-optimized memory
DIMMs – but in our case, the TensorDIMMs. The benefits of interfacing TensorNode within the GPU interconnect is clear: by storing the
embedding lookup tables inside the TensorNode, GPUs can copy
in/out the embeddings much faster than the conventional CPU-GPU
based approaches (i.e., approximately 9× faster than PCIe, assuming
NVLINK(v2) [61]). Furthermore, coupled with the memory bandwidth amplification effects of TensorISA, our TensorNode offers
scalable expansion of “both” memory capacity and bandwidth. Overall, our vertically integrated solution provides an average 6.2−15.0×
and 8.9−17.6× speedup in inference time compared to the CPU-only
and hybrid CPU-GPU implementation of recommender systems,
respectively. To summarize our key contributions:
• The computer architecture community has so far primarily focused on accelerating the computationally intensive,
“dense” DNN layers (e.g., CNNs/RNNs/MLPs). To the best
of our knowledge, this work is the first to identify, analyze,
and explore architectural solutions for “sparse” embedding
layers, a highly important building block with significant
industrial importance for emerging DL applications.
• We propose TensorDIMM, a practical near-memory processing architecture built on top of commodity DRAMs which
offers scalable increase in both memory capacity and bandwidth for embedding gathers and tensor operations.
• We present TensorNode, a TensorDIMM-based disaggregated
memory system for DL. The efficiency of our solution is
demonstrated as a proof-of-concept software prototype on
a high-end GPU system, achieving significant performance
improvements than conventional approaches.
2 BACKGROUND
2.1 Buffered DRAM Modules
In order to balance memory capacity and bandwidth, commodity
DRAM devices that are utilized in unison compose a rank. One or
more ranks are packaged into a memory module, the most popular
form factor being the dual-inline memory module (DIMM) which
has 64 data I/O (DQ) pins. Because a memory channel is typically
connected to multiple DIMMs, high-end CPU memory controllers
often need to drive hundreds of DRAM devices in order to deliver
the command/address (C/A) signals through the memory channel.
1While we demonstrate TensorDIMM’s merits under the context of a high-bandwidth
GPU-side system interconnect, the effectiveness of our proposal remains intact for
NPUs (e.g., Facebook’s Zion interconnect [19]).
741
TensorDIMM: A Near-Memory Processing Architecture for Sparse Embedding Layers in Deep Learning MICRO ’52, October 12–16, 2019, Columbus, OH, USA
Because modern DRAMs operate in the GHz range, having hundreds of DRAM devices be driven by handful of memory controllers
leads to signal integrity issues. Consequently, server-class DIMMs
typically employ a buffer device per each DIMM (e.g., registered
DIMM [71] or load-reduced DIMM [28]) which is used to repeat the
C/A signals to reduce the high capacitive load and resolve signal
integrity issues. Several prior work from both industry [46] and in
academia [4, 6, 21] have explored the possibility of utilizing this
buffer device space to add custom logic designs to address specific
application needs. IBM’s Centaur DIMM [46] for instance utilizes a
buffer device to add a 16 MB eDRAM L4 cache and a custom interface between DDR PHY and IBM’s proprietary memory interface.
2.2 System Architectures for DL
As the complexity of DL applications skyrocket, there has been a
growing trend towards dense, scaled-up system node design with
multiple PCIe-attached co-processor devices (i.e., DL accelerators
such as GPUs/NPUs [23, 60]) to address the problem size growth. A
multi-accelerator device solution typically works on the same problem in parallel with occasional inter-device communication to share
intermediate data [37]. Because such inter-device communication
often lies on the critical path of parallelized DL applications, system vendors are employing high-bandwidth interconnection fabrics
that utilize custom high-bandwidth signaling links (e.g., NVIDIA’s
DGX-2 [58] or Facebook’s Zion system interconnect fabric [19]).
NVIDIA’s DGX-2 [58] for instance contains 16 GPUs, all of which
are interconnected using a NVLINK-compatible high-radix (crossbar) switch called NVSwitch [62]. NVLINK provides 25 GB/sec of
full-duplex uni-directional bandwidth per link [61], so any given
GPU within DGX-2 can communicate with any other GPU at the
full uni-directional bandwidth up to 150 GB/sec via NVSwitch.
Compared to the thin uni-directional bandwidth of 16 GB/sec (x16)
under the CPU-GPU PCIe(v3) bus, such high-bandwidth GPU-side
interconnect enables an order of magnitude faster data transfers.
2.3 DL Applications with Embeddings
DNN-based recommender systems. Conventional DL applications for inference (e.g., CNNs/RNNs) generally share a common
property where its overall memory footprint fits within the (tens of
GBs of) GPU/NPU physical memory. However, recent studies from
several hyperscalars [15, 24, 27, 52, 64] call out for imminent systemlevel challenges in emerging DL workloads that are extremely
memory (capacity and bandwidth) limited. Specifically, these hyperscalars pinpoint to embedding layers as the most memory-intensive
algorithm deployed in their datacenters. One of the most widely
deployed DL application using embeddings is the recommender
system [1], which is used in numerous application domains such
as advertisements (Amazon, Google, Ebay), social networking service (Facebook, Instagram), movie/music/image recommendations
(YouTube, Spotify, Fox, Pinterest), news feed (LinkedIn), and many
others. Recommendation is typically formulated as a problem of
predicting the probability of a certain event (e.g., the probability of
a Facebook user clicking “like” for a particular post), where a ML
model estimates the likelihood of one or more events happening
at the same time. Events or items with the highest probability are
ranked higher and recommended to the user.
User0
User1
User2
User3
…
Usern
Item0
Embedding lookup tables
(Each for user & item)
Item1
Item2
Item3
…
Itemn
Batch0
Batch1
Batch2
Batch3
Batch0
Batch1
Batch2
Batch3
Embeddings
(Batched)
[Step 1]
Embedding lookup
(GATHER instruction)
Tensor OP
(Reduce)
Batch0
Batch1
Batch2
Batch3
Batch0
Batch1
Batch2
Batch3
[Step 2]
Tensor manipulation
(REDUCE/AVERAGE instructions)
Embeddings
(Reduced)
Embeddings
(Concatenated)
Age
…
[Step 3]
DNN computation
Age
…
DNNs
…
Gender
…
Gender …
…
Figure 2: A DNN based recommender system. The embedding layer
typically consists of the following two steps. (1) The embedding
lookup stage where embeddings are “gathered” from the (potentially multiple, two tables in this example) look-up tables up to its
batch size and form (batched) embedding tensors. These tensors go
through several tensor manipulation operations to form the final
embedding tensor to be fed into the DNNs. Our proposal utilizes
custom tensor ISA extensions (GATHER/REDUCE/AVERAGE) to accelerate
this process, which we detail in Section 4.4.
Without going into a comprehensive review of numerous prior
literature on recommendation models, we emphasize that current
state-of-the-art recommender systems have evolved into utilizing
(not surprisingly) DNNs. While there exists variations regarding
how the DNNs are constructed, a commonly employed topological
structure for DNN-based recommender systems is the neural network based collaborative filtering algorithm [26]. Further advances
led to the development of more complex models with wider and
deeper vector dimensions, successfully being applied and deployed
in commercial user-facing products [64].
Embedding lookups and tensor manipulation. Figure 2 illustrates the usage of embeddings in recommender systems that
incorporate DNNs [26]. The inputs to the DNNs (which are typically
constructed using fully-connected layers or multi-layer perceptrons,
FCs or MLPs) are constructed as a combination of dense and sparse
features. Dense features are commonly represented as a vector of
real numbers whereas sparse features are initially represented as
indices of one-hot encoded vectors. These one-hot indices are used
to query the embedding lookup table to project the sparse indices
into a dense vector dimension space. The contents stored in these
lookup tables are called embeddings, which are trained to extract
deep learning features (e.g., Facebook trains embeddings to extract
information regarding what pages a particular user liked, which are
utilized to recommend relevant contents or posts to the user [64]).
The embeddings read out of the lookup table are combined with
other dense embeddings (feature interaction in Figure 1) from other
lookup tables using tensor concatenation or tensor “reductions” such
as element-wise additions/multiplications/averages/etc to generate
an output tensor, which is then forwarded to the DNNs to derive
the final event probability (Figure 2).
Memory capacity limits of embedding layers. A key reason
why embedding layers consume significant memory capacity is
because each user (or each item) requires a unique embedding vector
742
MICRO ’52, October 12–16, 2019, Columbus, OH, USA Youngeun Kwon, Yunjae Lee, Minsoo Rhu
inside the lookup table. The total number of embeddings therefore
scales proportional to the number of users/items, rendering the
overall memory footprint to exceed several hundreds of GBs just
to keep the model weights themselves, even for inference. Despite
its high memory requirements, embedding layers are favored in
DL applications because it helps improve the model quality: given
enough memory capacity, users seek to further increase the model
size of these embedding layers using larger embedding dimensions
or by using multiple embedding tables to combine multiple dense
features using tensor reduction operations.
3 MOTIVATION
3.1 Memory (Capacity) Scaling Challenges
High-end GPUs or NPUs commonly employ a bandwidth-optimized,
on-package 3D stacked memory (e.g., HBM [32] or HMC [48]) in
order to deliver the highest possible memory bandwidth to the
on-chip compute units. Compared to capacity-optimized DDRx
most commonly adopted in CPU servers, the bandwidth-optimized
stacked memory is capacity-limited, only available with several
tens of GBs of storage. While one might expect future solutions in
this line of product to benefit from higher memory density, there are
technological constraints and challenges in increasing the capacity
of these 3D stacked memory in a scalable manner. First, stacking
more DRAM layers vertically is constrained by chip pinout required
to drive the added DRAM stacks, its wireability on top of silicon
interposers, and thermal constraints. Second, current generation of
GPUs/NPUs are already close to the reticle limits of processor die
size2
so adding more 3D stack modules within a package inevitably
leads to sacrificing area budget for compute units.
3.2 Memory Limits in Recommender System
As discussed in Section 2.3, the model size of embedding lookup
tables is in the order of several hundreds of GBs, far exceeding the
memory capacity limits of GPUs/NPUs. Due to the memory scaling
limits of on-package stacked DRAMs, a solution vendors take today
is to first store the embedding lookup tables in the CPU and read
out the embeddings using the (capacity-optimized but bandwidthlimited) CPU memory. Two possible implementations beyond this
step are as follows. The CPU-only version (CPU-only) goes through
the rest of the inference process using the CPU without relying
upon the GPU. A hybrid CPU-GPU approach (CPU-GPU) [59] on
the other hand copies the embeddings to the GPU memory over
PCIe using cudaMemcpy, and once the CPU→GPU data transfer is
complete, the GPU initiates various tensor manipulation operations
to form the input tensors to the DNN, followed by the actual DNN
computation step (Figure 2).
Given this landscape, DL practitioners as well as system designers are faced with a conundrum when trying to deploy recommender systems for inference. From a DL algorithm developer’s perspective, you seek to add more embeddings (i.e., more embedding
lookup tables to combine various embedding vectors using tensor
2NVIDIA V100 for instance has already reached the reticle limits of 815 mm2 die
area, forcing researchers to explore alternative options such as multi-chip-module
solutions [5] to continue computational scaling.
64
128
256
512
1024
2048
4096
8192
64
128
256
512
1024
2048
4096
8192
16384
32768
256
512
1024
2048
4096
8192
Embedding dimension
Model size (GB)
MLP dimension
Figure 3: Model size growth of neural collaborative filtering
(NCF [26]) based recommender system when the MLP layer dimension size (x-axis) and the embedding vector dimension size (y-axis)
is scaled up. Experiment assumes the embedding lookup table contains 5 million users and 5 million items per each lookup table. As
shown, larger embeddings rather than larger MLP dimension size
cause a much more dramatic increase in model size.
manipulations, such as tensor reduction) and increase embedding dimensions (i.e., larger embeddings) for complex feature interactions
as it improves model quality. Unfortunately, fulfilling the needs of
these algorithm developers bloats up overall memory usage (Figure 3) and inevitably results in resorting to the capacity-optimized
CPU memory to store the embedding lookup tables. Through a
detailed characterization study, we root-cause the following three
factors as key limiters of prior approaches (Figure 4) relying on
CPU memory for storing embeddings:
(1) As embedding lookup tables are stored in the low-bandwidth
CPU memory, reading out embeddings (i.e., the embedding
gather operation) adds significant latency than an unbuildable, oracular GPU-only version (GPU-only) which assumes
infinite GPU memory capacity. Under GPU-only, the entire
embeddings can be stored locally in the high-bandwidth GPU
memory so gathering embeddings can be done much faster
than when using CPU memory. This is because gathering
embeddings is a memory bandwidth-limited operation.
(2) CPU-only versions can sidestep the added latency that hybrid CPU-GPU versions experience during the PCIe communication process of transferring embeddings, but the (relatively) low computation throughput of CPUs can significantly lengthen the DNN computation step.
(3) Hybrid CPU-GPU versions on the other hand can reduce the
DNN computation latency, but comes at the cost of additional CPU→GPU communication latency when copying
the embeddings over PCIe using cudaMemcpy.
3.3 Our Goal: A Scalable Memory System
Overall, the key challenge rises from the fact that the source operands
of key tensor manipulations (i.e., the embeddings subject for tensor
concatenation or tensor reduction) are initially located inside the
embedding lookup tables, all of which are stored inside the capacityoptimized but bandwidth-limited CPU memory. Consequently, prior
743
TensorDIMM: A Near-Memory Processing Architecture for Sparse Embedding Layers in Deep Learning MICRO ’52, October 12–16, 2019, Columbus, OH, USA
0.0
0.5
1.0
1.5
2.0
B(001)
B(008)
B(064)
B(128)
B(001)
B(008)
B(064)
B(128)
B(001)
B(008)
B(064)
B(128)
B(001)
B(008)
B(064)
B(128)
B(001)
B(008)
B(064)
B(128)
NCF YouTube Fox Facebook Average
Normalized
Performance
CPU-only
CPU-GPU
GPU-only
Figure 4: Performance of baseline CPU-only and hybrid CPU-GPU versions of recommender system, normalized to an oracular GPU-only
version. B(N) represents an inference with batch size N. CPU-only
exhibits some performance advantage than the CPU-GPU version
for certain low batch inference scenarios, but both CPU-only and
CPU-GPU suffers from significant performance loss compared to oracular GPU-only. Section 5 details our evaluation methodology.
solutions suffer from low-bandwidth embedding gather operations
over CPU memory which adds significant latency. Furthermore,
CPU-only and CPU-GPU versions need to trade-off the computational bottleneck of low-throughput CPUs over the communication
bottleneck of PCIe, which adds additional latency overheads (Figure 1). What is more troubling is the fact future projections of DL
applications utilizing embedding layers assume even larger number
of embedding lookups and larger embeddings themselves [27, 64],
with complex tensor manipulations to combine embedding features
to improve algorithmic performance. Overall, both current and future memory requirements of embedding layers point to an urgent
need for a system-level solution that provides scalable memory
capacity and bandwidth expansion. In the next section, we detail
our proposed solution that addresses both the computational bottleneck of low-throughput CPUs and the communication bottleneck
of low-bandwidth CPU-GPU data transfers.
4 TENSORDIMM: AN NMP DIMM DESIGN
FOR EMBEDDINGS & TENSOR OPS
4.1 Proposed Approach
Our proposal is based on the following key observations that
open up opportunities to tackle the system-level bottlenecks in
deploying memory-limited recommender systems.
(1) An important tensor operation used in combining embedding features is the element-wise operation, which is equivalent to a tensor-wide reduction among N embeddings. Rather
than having the N embeddings individually be gathered and
copied over to the GPU memory for the reduction operation
to be initiated using the GPU, we can conduct the N embedding gathers and reductions all “near-memory” and copy a
single, reduced tensor to GPU memory. Such near-memory
processing (NMP) approach reduces not only the latency to
gather the embeddings but also the data transfer size by a
factor of N and alleviates the communication bottleneck of
CPU→GPU cudaMemcpy (Figure 5).
(2) DL system vendors are deploying GPU/NPU-centric interconnection fabrics [19, 58] that are decoupled from the legacy
host-device PCIe. Such technology allows vendors to employ
custom high-bandwidth links (e.g., NVLINK, providing 9×
Embedding0
(Copied from CPU to GPU over PCIe)
+
Embedding1
(Copied from CPU to GPU over PCIe)
+
Embedding2
(Copied from CPU to GPU over PCIe)
=
Reduced embedding
(Reduction conducted using GPU)
Embedding0
(Read locally within TensorDIMM and reduced)
+
Embedding1
(Read locally within TensorDIMM and reduced)
+
Embedding2
(Read locally within TensorDIMM and reduced)
=
Embedding
(Copied from TensorNode to GPU over NVLINK)
(a) Hybrid CPU-GPU approach (b) TensorDIMM + TensorNode
1) Embedding lookup time (Tlookup)
= N*sizeof(embedding)/ BandwidthCPUmemory
2) Embedding transfer time (Ttransfer)
= N*sizeof(embedding)/BandwidthPCIe
1) Embedding lookup time (Tlookup)
= N*sizeof(embedding)/ BandwidthTensorNode
2) Embedding transfer time (Ttransfer)
= 1*sizeof(embedding)/BandwidthNVLINK
Figure 5: Our proposed approach: compared to the (a) hybrid
CPU-GPU version, (b) our solution conducts embedding gathers and
tensor-wide reductions locally “near-memory” first, and then transfers the reduced tensor over the high-bandwidth NVLINK. Consequently, our proposal significantly reduces both the latency
of gathering embeddings from the lookup table (Tl ookup ) and
the time taken to transfer the embeddings from the capacityoptimized CPU/TensorNode memory to the bandwidth-optimized
GPU memory (Tt r ans f er ). The CPU-only version does not experience Tt r ans f er but suffers from significantly longer DNN execution
time than CPU-GPU and our proposed design.
higher bandwidth than PCIe) for fast inter-GPU/NPU communication. But more importantly, it is possible to tightly integrate “non”-accelerator components (e.g., a disaggregated
memory pool [39, 41, 42]) as separate interconnect endpoints
(or nodes), allowing accelerators to read/write from/to these
non-compute nodes using high-bandwidth links.
Based on these observations, we first propose TensorDIMM which
is a custom DIMM design including an NMP core tailored for tensor
“gather” and tensor “reduction” operations. We then propose a disaggregated memory system called TensorNode which is fully populated with our TensorDIMMs. Our last proposition is a software architecture that effectively parallelizes the tensor gather/reduction operation across the TensorDIMMs, providing scalable memory bandwidth and capacity expansion. The three most significant performance limiters in conventional CPU-only or hybrid CPU-GPU recommender system are 1) the embedding gather operation over the
low-bandwidth CPU memory, 2) the compute-limited DNN execution using CPUs (for CPU-only approaches) and 3) the CPU→GPU
embedding copy operation over the PCIe bus (for hybrid CPU-GPU),
all of which adds severe latency overheads (Section 3.2). Our vertically integrated solution fundamentally addresses these problems
thanks to the following three key innovations provided with our
approach (Figure 5). First, by storing the entire embedding lookup table inside the TensorDIMMs, the embedding gather operation can be
conducted using the ample memory bandwidth available across the
TensorDIMMs (4× higher than CPU) rather than the low-bandwidth
CPU memory. Second, the TensorDIMM NMP cores conduct the N
tensor reduction operation before sending them to the GPU, reducing the TensorNode→GPU communication latency by a factor of
N, effectively overcoming the communication bottleneck. Furthermore, the high-bandwidth TensorNode↔GPU links (i.e., NVLINK)
744
MICRO ’52, October 12–16, 2019, Columbus, OH, USA Youngeun Kwon, Yunjae Lee, Minsoo Rhu
Vector
ALU
DDR
PHY
NMP-local Memory Controller
Input Q
(A)
Input Q
(B)
Output Q
(C)
Protocol
Engine
Local
DRAM
DDRx
Memory
Channel
DRAM
Buffer device
DRAM DRAM DRAM
DRAM
Buffer device
DRAM DRAM DRAM
DRAM
Buffer device
DRAM DRAM DRAM
DRAM
Buffer device
DRAM DRAM DRAM
(a) NMP core (b) TensorDIMM
NVSwitch
TensorNode
TensorDIMM
TensorDIMM
TensorDIMM
TensorDIMM
GPU0
GPUn
…
(c) System architecture with TensorNode
NVLINK
(150 GB/sec)
Figure 6: High-level overview of our proposed system. (a) An NMP core is implemented inside the buffer device of each (b) TensorDIMM, multiples of which are employed as a (c) disaggregated memory pool, called TensorNode. The TensorNode is integrated inside the high-bandwidth
GPU-side interconnect. The combination of NVLINK and NVSwitch enables GPUs to read/write data to/from TensorNode at a communication
bandwidth up to 9× higher than PCIe.
enable further latency reduction, proportional to the bandwidth
difference between PCIe and NVLINK (approximately 9×). Lastly,
all DNN computations are conducted using the GPU, overcoming
the computation bottleneck of CPU-only implementations. As detailed in Section 6, our proposal achieves an average 6.2−15.0×
and 8.9−17.6× performance improvement over the CPU-only and
hybrid CPU-GPU, respectively, reaching 84% of an unbuildable, oracular GPU-only implementation with infinite memory capacity. We
detail each components of our proposal below.
4.2 TensorDIMM for Near-Memory Tensor Ops
Our TensorDIMM is architected under three key design objectives
in mind. First, TensorDIMM should leverage commodity DRAM
chips as-is while be capable of being utilized as a normal buffered
DIMM device in the event that it is not used for DL acceleration.
Second, tensor reduction operations should be conducted “nearmemory” using a lightweight NMP core, incurring minimum power
and area overheads to the buffered DIMM device. Third, in addition
to memory capacity, the amount of memory bandwidth available
to the NMP cores should also scale up proportional to the number
of TensorDIMM modules employed in the system.
Architecture. Figure 6(a,b) shows the TensorDIMM architecture,
which consists of an NMP core and the associated DRAM chips. As
depicted, TensorDIMM does not require any changes to commodity
DRAMs since all modifications are limited to a buffer device within
a DIMM (Section 2.1). The NMP core includes a DDR interface, a
vector ALU, and an NMP-local memory controller which includes
input/output SRAM queues to stage-in/out the source/destination
operands of tensor operations. The DDR interface is implemented
with a conventional DDR PHY and a protocol engine.
TensorDIMM usages. For non-DL use-cases, the processor’s
memory controller sends/receives DRAM C/A and DQ signals
to/from this DDR interface, which directly interacts with the DRAM
chips. This allows TensorDIMM to be applicable as a normal buffered
DIMM device and be utilized by conventional processor architectures for servicing load/store transactions. Upon receiving a
TensorISA instruction for tensor gather/reduction operations however, the instruction is forwarded to the NMP-local memory controller, which translates the TensorISA instruction into low-level
DRAM C/A commands to be sent to the DRAM chips. Specifically,
the TensorISA instruction is decoded (detailed in Section 4.4) in
order to calculate the physical memory address of the target tensor’s location, and the NMP-local memory controller generates the
necessary RAS/CAS/activate/precharge/etc DRAM commands
to read/write data from/to the TensorDIMM DRAM chips. The data
read out of DRAM chips are temporarily stored inside the input
(A and B) SRAM queues until the ALU reads them out for tensor
operations. In terms of NMP compute, the minimum data access
granularity over the eight x8 DRAM chips is 64 bytes for a burst
length of 8, which amounts to sixteen 4-byte scalar elements. As
detailed in Section 4.4, the tensor operations accelerated with our
NMP cores are element-wise arithmetic operations (e.g., add, subtract, average, . . .) which exhibit data-level parallelism across the
sixteen scalar elements. We therefore employ a 16-wide vector ALU
which conducts the element-wise operation over the data read out
of the (A and B) SRAM queues. The vector ALU checks for any
newly submitted pair of data (64 bytes each) and if so, pops out
a pair for tensor operations, the result of which is stored into the
output (C) SRAM queue. The NMP memory controller checks the
output queue for newly inserted results and drain them back into
DRAM, finalizing the tensor reduction process. For tensor gathers,
the NMP core forwards the data read out of the input queues to the
output queue to be committed back into DRAM.
Implementation and overhead. TensorDIMM leverages existing DRAM chips and the associated DDR PHY interface as-is, so the
additional components introduced with our TensorDIMM design are
the NMP-local memory controller and the 16-wide vector ALU. In
terms of the memory controller, decoding the TensorISA instruction into a series of DRAM commands is implemented as a FSM
control logic so the major area/power overheads comes from the
input/output SRAM queues. These buffers must be large enough
to hold the bandwidth-delay product of the memory sourcing the
data to remove idle periods in the output tensor generation stream.
745
TensorDIMM: A Near-Memory Processing Architecture for Sparse Embedding Layers in Deep Learning MICRO ’52, October 12–16, 2019, Columbus, OH, USA
A conservative estimate of 20 ns latency from the time the NMPlocal memory controller requests data to the time it arrives at the
SRAM queue is used to size the SRAM queue capacity. Assuming
the baseline PC4-25600 DIMM that provide 25.6 GB/sec of memory
bandwidth, the NMP core requires (25.6 GB/sec×20 ns) = 0.5 KB
of SRAM queue size (1.5 KB overall, for both input/output queues).
The 16-wide vector ALU is clocked at 150 MHz to provide enough
computation throughput to seamlessly conduct the element-wise
tensor operations over the data read out of the input (A and B)
queues. Note that the size of an IBM Centaur buffer device [46] is
approximately (10mm×10mm) with a TDP of 20 W. Compared to
such design point, our NMP core adds negligible power and area
overheads, which we quantitatively evaluate in Section 6.5.
Memory bandwidth scaling. An important design objective
of TensorDIMM is to provide scalable memory bandwidth expansion for NMP tensor operations. A key challenge with conventional memory systems is that the maximum bandwidth per each
memory channel is fixed (i.e., signaling bandwidth per each pin
× number of data pins per channel), regardless of the number of
DIMMs (or ranks/DIMM) per channel. For instance, the maximum
CPU memory bandwidth available under the baseline CPU system
(i.e., NVIDIA DGX [57]) can never exceed (25.6 GB/sec×8)=204.8
GB/sec across the eight memory channels (4 channels per each
socket), irrespective of the number of DIMMs actually utilized (i.e.,
8 DIMMs vs. 32 DIMMs). This is because the physical memory channel bandwidth is time-multiplexed across multiple DIMMs/ranks.
As detailed in the next subsection, our TensorDIMM is utilized as
a basic building block in constructing a disaggregated memory
system (i.e., TensorNode). The key innovation of our proposal is
that, combined with our TensorISA address mapping function (Section 4.4), the amount of aggregate memory bandwidth provided to
all the NMP cores within TensorNode increases proportional to the
number of TensorDIMM employed: an aggregate of 819.2 GB/sec
memory bandwidth assuming 32 TensorDIMMs, a 4× increase over
the baseline CPU memory system. Such memory bandwidth scaling
is possible because the NMP cores access its TensorDIMM-internal
DRAM chips “locally” within its DIMM, not having to share its
local memory bandwidth with other TensorDIMMs. Naturally, the
more TensorDIMMs employed inside the memory pool, the larger
the aggregate memory bandwidth becomes available to the NMP
cores conducting embedding gathers and reductions.
4.3 System Architecture
Figure 6 provides a high-level overview of our proposed system
architecture. We propose to construct a disaggregated memory
pool within the high-bandwidth GPU system interconnect. Our
design is referred to as TensorNode because it functions as an interconnect endpoint, or node. GPUs can read/write data from/to
TensorNode over the NVLINK compliant PHY interface, either
using fine-grained CC-NUMA or coarse-grained data transfers
using P2P cudaMemcpy3
. Figure 6(c) depicts our TensorNode design populated with multiple TensorDIMM devices. The key advantage of TensorNode is threefold. First, TensorNode provides a
3CC-NUMA access or P2P cudaMemcpy among NVLINK-compatible devices is already
available in commercial systems (e.g., Power9 [29], GPUs within DGX-2 [58]). Our
TensorNode leverages such technology as-is to minimize design complexity.
platform for increasing memory capacity in a scalable manner as
the disaggregated memory pool can independently be expanded
using density-optimized DDRx, irrespective of the GPU’s local,
bandwidth-optimized (but capacity-limited) 3D stacked memory.
As such, it is possible to store multiples embedding lookup tables
entirely inside TensorNode because the multitude of TensorDIMM
devices (each equipped with density-optimized LR-DIMM [28])
enable scalable memory capacity increase. Second, the aggregate
memory bandwidth available to the TensorDIMM NMP cores has
been designed to scale up proportional to the number of DIMMs
provisioned within the TensorNode (Section 4.4). This allows our
TensorNode and TensorDIMM design to fulfill not only the current
but also future memory (capacity and bandwidth) needs of recommender systems which combines multiple embeddings, the size
of which is expected to become even larger moving forward (Section 3.2). Recent projections from several hyperscalars [27, 64] state
that the memory capacity requirements of embedding layers will
increase by hundreds of times larger than the already hundreds of
GBs of memory footprint. TensorNode is a scalable, future-proof
system-level solution that addresses the memory bottlenecks of
embedding layers. Third, the communication channels to/from the
TensorNode is implemented using high-bandwidth NVLINK PHYs
so transferring embeddings between a GPU and a TensorNode becomes much faster than when using PCIe (approximately 9×).
4.4 Software Architecture
We now discuss the software architecture of our proposal: the address mapping scheme for embeddings, TensorISA for conducting
near-memory operations, and the runtime system.
Address mapping architecture. One of the key objectives of
our address mapping function is to provide scalable performance
improvement whenever additional TensorDIMMs (i.e., number of
DIMMs=NMP cores) are added to our TensorNode. To achieve this
goal, it is important that all NMP cores within the TensorNode concurrently work on a distinct subset of the embedding vectors for
(gather/reduce) tensor operations. Figure 7(a) illustrates our address
mapping scheme which utilizes rank-level parallelism to maximally
utilize both the TensorDIMM’s NMP computation throughput and
memory bandwidth. Because each TensorDIMM has its own NMP
core, maximally utilizing aggregate NMP compute throughput requires all TensorDIMMs to work in parallel. Our address mapping
function accomplishes this by having consecutive 64 bytes4 within
each embedding vector be interleaved across different ranks, allowing each TensorDIMM to independently work on its own slice of
tensor operation concurrently (Figure 7(b)). As our target algorithm
contains abundant data-level parallelism, our address mapping technique effectively partitions and load-balances the tensor operation
across all TensorDIMMs within the TensorNode. In Section 6.1, we
quantitatively evaluate the efficacy of our address mapping function
in terms of maximum DRAM bandwidth utilization.
It is worth pointing out that our address mapping function is
designed to address not just the current but also future projections
on how DL practitioners seek to use embeddings. As discussed in
Section 3.2, given the capability, DL practitioners are willing to
4We assume TensorDIMM is built using a x64 DIMM. With a burst length of 8, the
minimum data access granularity becomes 64 bytes.
746
MICRO ’52, October 12–16, 2019, Columbus, OH, USA Youngeun Kwon, Yunjae Lee, Minsoo Rhu
Bit index … 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0
Embedding index Embedding vector
Row Column Bank Rank Column
Embedding table
DRAM address bitmask
Virtual address Virtual page number Page offset
(a)
Input
Embedding0
Input
Embedding1
Output
Embedding
Input
Embedding2
Input
Embeddingn
Subject for
element-wise operations
Rank 0 Rank 1 … Rank 15
64B 64B … 64B
…
…
…
…
NMP
Core
(DIMM0)
NMP
Core
(DIMM1)
…
NMP
Core
(DIMM15)
64B 64B … 64B
64B 64B … 64B
64B 64B … 64B
64B 64B … 64B
(b)
Figure 7: (a) Proposed DRAM address mapping scheme for embeddings, assuming each embedding is 1 KB (256-dimension) and is
split evenly across 16 ranks (or DIMMs) (b) Example showing how
rank-level parallelism is utilized to interleave and map each embedding across the 16 TensorDIMMs. Such rank-level parallelism centric address mapping scheme enable the memory bandwidth available to the NMP cores to increase proportional to the number of
TensorDIMMs employed.
adopt larger embedding vector dimensions to improve the algorithmic performance. This translates into a larger number of bits to
reference any given embedding vector (e.g., enlarging embedding
size from 1 KB to 4 KB increases the embedding’s bit-width from
10 bits to 12 bits in Figure 7(a)) leading to both larger memory
footprint to store the embeddings and higher computation and
memory bandwidth demands to conduct tensor reductions. Our
system-level solution has been designed from the ground-up to
effectively handle such user needs: that is, system architects can
provision more TensorDIMM ranks within TensorNode and increase
its memory capacity proportional to the increase in embedding size,
which is naturally accompanied by an increase in NMP compute
throughput and memory bandwidth simultaneously.
TensorISA. The near-memory tensor operations are initiated
using our custom ISA extension called TensorISA. There are three
key TensorISA primitives supported in TensorDIMM: the GATHER
instruction for embedding lookups and the REDUCE and AVERAGE
instructions for element-wise operations. Figure 8 and Figure 9
summarize the instruction formats of these three instructions and
pseudo codes describing each of its functional behavior. Consider
the example in Figure 2 which assumes an embedding layer that
uses two embedding lookup tables with a batch size 4 to compose
two tensors, followed by an element-wise operation among these
two for reduction. As discussed in Section 2.3, an embedding layer
starts with an embedding lookup phase that gathers multiple embeddings up to the batch size from the embedding lookup table,
followed by various tensor manipulations. Under our proposed
system, the GPU executes this embedding layer as follows. First,
OpCode InputBase AUX OutputBase Count
GATHER tableBase idxBase outputBase count
REDUCE inputBase1 inputBase2 outputBase count
AVERAGE inputBase averageNum outputBase count
Instruction format
Element-wise reduction
Element-wise average
Embedding lookup
Figure 8: Instruction formats for GATHER/REDUCE/AVERAGE.
# READ64B(void* src): read 64 bytes from address src
# WRITE64B(void* dst, REG64B v): store 64 bytes value to address dst
# nodeDim: number of TensorDIMMs within a TensorNode
# tid: unique ID assigned to each TensorDIMM
# A, B and C: 64 bytes vector register
# X is a 64 bytes array which can be accessed in 4 bytes granularity (i.e., X[0] to X[15])
00 #pseudo code of embedding lookup (GATHER)
01 for i in range(count/16):
02 X ß READ64B(idxBase + i)
03 for j in range(16):
04 C ß READ64B(tableBase + (X[j] * nodeDim) + tid)
05 WRITE64B(outputBase + (i * 16 + j) * nodeDim + tid, C)
(a)
00 #pseudo code of element-wise operations (REDUCE)
01 for i in range(Count):
02 A ß READ64B(inputBase1 + i * nodeDim + tid)
03 B ß READ64B(inputBase2 + i * nodeDim + tid)
04 C ß A <OP> B
05 WRITE64B(outputBase + i * nodeDim + tid, C)
(b)
00 #pseudo code of element-wise average (AVERAGE)
01 for i in range(count):
02 C ß 256'b0
03 for j in range(averageNum):
04 A ß READ64B(inputBase + (i * averageNum + j) * nodeDim + tid)
05 C ß C + A
06 C ß C <divide> averageNum
07 WRITE64B(outputBase + i * nodeDim + tid, C)
(c)
Figure 9: Pseudo code explaining the functional behavior of (a)
GATHER, (b) REDUCE, and (c) AVERAGE.
the GPU sends three instructions, two GATHER instructions and one
REDUCE, to the TensorNode. The TensorISA instruction is broadcasted to all the TensorDIMMs because each NMP core is responsible
for locally conducting its share of the embedding lookups as well as
its slice of the tensor operation (Figure 9(a,b)). For instance, assuming the address mapping function in Figure 7(a) and a TensorNode
configuration with 16 TensorDIMMs, a single GATHER instruction
will have each TensorDIMM gather (4×64 bytes) = 256 bytes of data
under a contiguous physical address space, the process of which
is orchestrated by the TensorDIMM NMP-local memory controller
using DRAM read/write transactions (Section 4.2). The GATHER
process is undertaken twice to prepare for the two tensor slices
per each TensorDIMM rank. Once the two tensors are gathered, a
REDUCE instruction is executed by the NMP cores (Section 4.2).
Runtime system. DL applications are typically encapsulated
as a direct acyclic graph (DAG) data structure in major DL frameworks [9, 66, 75]. Each node within the DAG represents a DNN
layer and the DL framework compiles down the DAG into sequence
of host-side CUDA kernel launches that the GPU executes one layer
at a time. The focus of this paper is on recommender systems which
utilizes embedding lookups and various tensor manipulations. Under our proposed system, embedding layers are still executed using
normal CUDA kernel launches but the kernel itself is wrapped
around specific information for our TensorDIMM runtime system
747
TensorDIMM: A Near-Memory Processing Architecture for Sparse Embedding Layers in Deep Learning MICRO ’52, October 12–16, 2019, Columbus, OH, USA
Table 1: Baseline TensorNode configuration.
DRAM specification DDR4 (PC4-25600)
Number of TensorDIMMs 32
Memory bandwidth per TensorDIMM 25.6 GB/sec
Memory bandwidth across TensorNode 819.2 GB/sec
to utilize for near-memory tensor operations. Specifically, as part
of the embedding layer’s CUDA kernel context, information such
as the number of table lookups, the embedding dimension size,
tensor reduction type, the input batch size, and etc are encoded
per TensorISA instruction’s format (Figure 8) and is sent to the
GPU as part of the CUDA kernel launch. When the GPU runtime
receives these instructions, they are forwarded to the TensorNode
for near-memory processing as discussed in Section 4.2.
As TensorNode is a remote, disaggregated memory pool from the
GPU’s perspective, the runtime system should be able to (de)allocate
memory inside this remote memory pool. Our proposal builds upon
our prior work [39] which proposes several CUDA runtime API
extensions for remote memory (de)allocation under a GPU-side
disaggregated memory system. We refer the interested readers to
[13, 39] for further details on the runtime CUDA APIs required for
memory (de)allocations within a disaggregated memory system.
5 EVALUATION METHODOLOGY
Architectural exploration of TensorDIMM and its system-level implication within TensorNode using a cycle-level simulator is challenging for several reasons. First, running a single batch inference
for DL applications can take up to several milliseconds even on
high-end GPUs, so running cycle-level simulation on several tens
to hundreds of batches of inference leads to intractable amount
of simulation time. Second, our proposal covers multiple levels
in the hardware/software stack, so a cycle-level hardware performance model of TensorDIMM and TensorNode alone will not properly reflect the complex interaction of (micro)architecture, runtime
system, and the system software, potentially resulting in misleading conclusions. Interestingly, we note that the key DL operations
utilized in embedding layers that are of interest to this study are
completely memory bandwidth limited. This allows us to utilize the
existing DL hardware/software systems to “emulate” the behavior of TensorDIMM and TensorNode on top of state-of-the-art real
DL systems. Recall that the embedding lookups (GATHER) and the
tensor reduction operations (REDUCE/AVERAGE) have extremely low
compute-to-memory ratio (i.e., all three operations are effectively
streaming applications), rendering the execution of these three operations to be bottlenecked by the available memory bandwidth.
Consequently, the effectiveness of our proposal primarily lies in
1) the effectiveness of our address mapping scheme on maximally
utilizing the aggregate memory bandwidth for the tensor operations across all the TensorDIMMs, and 2) the impact of “PCIe vs.
NVLINK” on the communication latency of copying embeddings
across the capacity-optimized “CPU vs. TensorNode” memory and
bandwidth-optimized GPU memory (Figure 5). We introduce our
novel, hybrid evaluation methodology that utilizes both cycle-level
simulation and a proof-of-concept prototype developed on real DL
systems to quantitatively demonstrate the benefits of our proposal.
SM SM SM SM
Crossbar
MC
L2
MC
L2
MC
L2
MC
L2
DRAM DRAM DRAM DRAM
Modern GPU architecture
NMP
cores
TensorNode’s NMP local memory channels
and the associated memory bandwidth
GPU-side
Interconnect
GPU0
Used as a
normal GPU
GPU1
“Emulated” as a
TensorNode
Figure 10: Emulation of TensorDIMM and TensorNode using a real
GPU: the GPU cores (aka SMs [56]) and the GPU-local memory
channels (and bandwidth) corresponds to the NMP cores within
TensorDIMMs and the aggregate memory bandwidth available across
the TensorNode, respectively.
Cycle-level simulation. As the performance of TensorDIMM
and TensorNode is bounded by how well they utilize the DRAM
bandwidth, an evaluation of our proposal on memory bandwidth
utilization is in need. We develop a memory tracing function that
hooks into the DL frameworks [66, 75] to generate the necessary
read/write memory transactions in executing GATHER, REDUCE, and
AVERAGE operations for embedding lookups and tensor operations.
The traces are fed into Ramulator [36], a cycle-accurate DRAM simulator, which is configured to model 1) the baseline CPU-GPU system
configuration with eight CPU-side memory channels, and 2) our
proposed address mapping function (Section 4.4) and TensorNode
configuration (Table 1), which we utilize to measure the effective
memory bandwidth utilization when executing the three tensor
operations under baseline and TensorNode.
“Proof-of-concept” prototype. We emulate the behavior of
our proposed system using the state-of-the-art NVIDIA DGX [57]
machine, which includes eight V100 GPUs [60]. Each V100 GPU contains 900 GB/sec of local memory bandwidth with six NVLINKs for
communicating with other GPUs up to 150 GB/sec [61]. To emulate
the system-level effects of the high-bandwidth NVLINK communication channels between TensorNode↔GPU, we use a pair of
GPUs in DGX and treat one of them as our proposed TensorNode
while the other acts as a normal GPU (Figure 10). Because the
tensor operations accelerated using our TensorDIMMs are memory
bandwidth-limited, streaming workloads, the performance difference between a (hypothetical) real TensorNode populated with N
TensorDIMMs and a single V100 that we emulate as our TensorNode
will be small, provided the V100 local memory bandwidth matches
that of our assumed TensorNode configuration. As such, when evaluating the effective memory bandwidth utilization of TensorNode,
we configured the number of ranks (i.e., TensorDIMMs) to be N=32
such that the aggregate memory bandwidth available within a single TensorNode approximately matches that of a single V100 (900
GB/sec vs. 819.2 GB/sec, Table 1). After validating the effectiveness of TensorNode in utilizing memory bandwidth, commensurate to that of V100, we implement a software prototype of an
end-to-end recommender system (configurable to the four DL applications discussed below) using Intel’s Math Kernel Library (MKL
version 2019.0.3 [30]), cuDNN (version 7 [55]), cuBLAS [54], and
our in-house CUDA implementation of embedding layers (including
748
MICRO ’52, October 12–16, 2019, Columbus, OH, USA Youngeun Kwon, Yunjae Lee, Minsoo Rhu
Table 2: Evaluated benchmarks and default configuration.
Network Lookup tables Max reduction FC/MLP layers
NCF 4 2 4
YouTube 2 50 4
Fox 2 50 1
Facebook 8 25 6
GATHER/REDUCE/AVERAGE) as well as other layers that do not come
with MKL, cuDNN or cuBLAS. We cross-validated our in-house
implementation of these memory bandwidth-limited layers by comparing the measured performance against the upper bound, ideal
performance, which exhibited little variation. Under our CUDA implementation of tensor reduction operations, the GPU cores (called
SMs in CUDA [56]) effectively function as the NMP cores within
TensorNode because the CUDA kernels of REDUCE/AVERAGE stage
in/out the tensors between (SM↔GPU local memory) in a streaming fashion as done in TensorDIMM. The TensorNode↔GPU communication is orchestrated using P2P cudaMemcpy over NVLINK
when evaluating our proposal. When running the sensitivity of
TensorDIMM on (TensorNode↔GPU) communication bandwidth,
we artificially increase (decrease) the data transfer size to emulate
the behavior of smaller (larger) communication bandwidth and
study its implication on system performance (Section 6.4).
Benchmarks. We choose four neural network based recommender system applications using embeddings to evaluate our
proposal: (1) the neural collaborative filtering (NCF) [26] based
recommender system available in MLPerf [49], (2) the YouTube
recommendation system [14] (YouTube), and (3) the Fox movie recommendation system [7] (Fox), and the Facebook recommendation
system [52] (Facebook5
). To choose a realistic inference batch size
representative of real-world inference scenarios, we refer to the
recent study from Facebook [64] which states that datacenter recommender systems are commonly deployed with a batch size of
1−100. Based on this prior work, we use a batch size of 64 as our
default configuration but sweep from batch 1 to 128 when running
sensitivity studies. All four workloads are configured with a default
embedding vector dimension size of 512 with other key application configurations summarized in Table 2. We specifically note
when deviating from these default configurations when running
sensitivity studies in Section 6.3.
Area/power. The implementation overheads of TensorDIMM are
measured with synthesized implementations using Verilog HDL,
targeting a Xilinx Virtex UltraScale+ VCU1525 acceleration dev
board. The system-level power overheads of TensorNode are evaluated using Micron’s DDR4 power calculator [47]. We detail these
results in Section 6.5.
6 EVALUATION
We explore five design points of recommender systems: the 1) CPUonly version (CPU-only), 2) hybrid CPU-GPU version (CPU-GPU),
3) TensorNode style pooled memory interfaced inside the highbandwidth GPU interconnect but utilizes regular capacity-optimized
DIMMs, rather than the NMP-enabled TensorDIMMs (PMEM), 4) our
5
Facebook’s Deep Learning Recommendation Model [52] was released after this work
was submitted for peer-review at MICRO ’52. As such, we implement and evaluate this
model using our proof-of-concept emulation framework, rather than using Facebook’s
open-sourced PyTorch [66] version, for consistency with the other three models.
0
100
200
300
400
500
600
700
800
900
2
8
14
20
26
32
38
44
50
56
62
68
74
80
86
92
98
104
110
116
122
128
Bandwidth utilization
(GB/sec)
Batch size
 AVERAGE(TDIMM) REDUCE(TDIMM) GATHER(TDIMM)
 AVERAGE(CPU) REDUCE(CPU) GATHER(CPU)
Figure 11: Memory bandwidth utilization for the three tensor operations. TensorNode assumes the default configuration in Table 1 with
32 TensorDIMMs. For CPU-only and CPU-GPU, the tensor operations are
conducted over a conventional CPU memory system, so a total of 8
memory channels with 32 DIMMs (4 ranks per each memory channel) are assumed.
0
600
1200
1800
2400
3000
3600
32 DIMMs
64 DIMMs
128 DIMMs
32 DIMMs
64 DIMMs
128 DIMMs
32 DIMMs
64 DIMMs
128 DIMMs
32 DIMMs
64 DIMMs
128 DIMMs
32 DIMMs
64 DIMMs
128 DIMMs
32 DIMMs
64 DIMMs
128 DIMMs
GATHER REDUCE AVERAGE GATHER REDUCE AVERAGE
Embeddings inside CPU Embedding inside TensorNode
Bandwidth utilization
(GB/sec)
Figure 12: Memory throughput as a function of the number of
DIMMs employed within CPU and TensorNode. Evaluation assumes
the embedding size is increased from the default value by 2−4×,
which proportionally increases the embedding lookup table size,
requiring a proportional increase in memory capacity (i.e., more
DIMMs) to store these lookup tables.
proposed TensorNode with TensorDIMMs (TDIMM), and 5) an unbuildable, oracular GPU-only version (GPU-only) which assumes
that the entire embeddings can be stored inside GPU local memory,
obviating the need for cudaMemcpy.
6.1 Memory Bandwidth Utilization
To validate the effectiveness of TensorNode in amplifying effective
memory bandwidth, we measure the aggregate memory throughput
achieved with TensorNode and the baseline CPU-based systems,
both of which utilizes a total of 32 DIMMs (Figure 11). TensorNode
significantly outperforms the baseline CPU system with an average 4× increase in memory bandwidth utilization (max 808 vs. 192
GB/sec). As discussed in Section 4.2, conventional memory systems
time-multiplex the memory channel across multiple DIMMs, so
larger number of DIMMs only provide enlarged memory capacity not bandwidth. Our TensorDIMM is designed to ensure that the
aggregate memory bandwidth scales proportional to the number
of TensorDIMMs, achieving significant memory bandwidth scaling. The benefits of our proposal is more pronounced for more
future-looking scenarios with enlarged embedding dimension sizes.
Figure 12 shows the effective memory bandwidth for tensor operations when the embedding dimension size is increased by up to
4×, necessitating larger number of DIMMs to house the proportionally increased embedding lookup table. As depicted, the baseline
749
TensorDIMM: A Near-Memory Processing Architecture for Sparse Embedding Layers in Deep Learning MICRO ’52, October 12–16, 2019, Columbus, OH, USA
0
0.2
0.4
0.6
0.8
1
1.2
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
NCF YouTube Fox Facebook
Latency
(normalized)
Embedding lookup cudaMemcpy Computation Else
Figure 13: Breakdown of latencies during an inference with batch 64,
normalized to the slowest design point (i.e., CPU-only or CPU-GPU).
CPU memory system’s memory bandwidth saturates at around 200
GB/sec because of the fundamental limits of conventional memory
systems. TensorNode on the other hand reaches up to 3.1 TB/sec
of bandwidth, more than 15× increase than baseline.
6.2 System-level Performance
TensorDIMM significantly reduces the latency to execute memorylimited embedding layers, thanks to its high memory throughput
and the communication bandwidth amplification effects of both
near-memory tensor operations and the high-bandwidth NVLINK.
Figure 13 shows a latency breakdown of our studied workloads
assuming a batch size of 64. With our proposed solution, all four
applications enjoy significant reduction in both embedding lookup
latency and the embedding copy latency. This is because of the
high-bandwidth TensorDIMM tensor operations and the fast communication links utilized for moving the embeddings to GPU memory. Figure 14 summarizes the normalized performance of the five
design points across different batch sizes. While CPU-only occasionally achieves better performance than CPU-GPU for low batch
inference scenarios, the oracular GPU-only consistently performs
best on average, highlighting the advantages of DNN acceleration
using GPUs (and NPUs). TensorDIMM achieves an average 84% (no
less than 75%) of the performance of such unbuildable oracular GPU,
demonstrating its performance merits and robustness compared to
CPU-only and CPU-GPU (an average 6.2× and 8.9× speedup).
6.3 TensorDIMM with Large Embeddings
A key motivation of our work is to provide a scalable memory
system for embeddings and its tensor operations. So far, we have assumed the default embedding size of each workloads as summarized
in Table 2. With the availablity of our pooled memory architecture,
DL practitioners can provision much larger embeddings to develop
recommender systems with superior model quality. Because our
evaluation is conducted over an emulated version of TensorDIMMs,
we are not able to demonstrate the model quality improvements
larger embedding will bring about as we cannot train these scaledup algorithms (i.e., the memory capacity is still constrained by the
GPU memory size). Nonetheless, we conduct a sensitivity study of
the TensorDIMMs for scaled-up embedding sizes which is shown
in Figure 15. With larger embeddings, the embedding layers cause
a much more serious performance bottleneck. TensorNode shows
even higher performance benefits under these settings, achieving
an average 6.2−15.0× and 8.9−17.6× (maximum 35×) performance
improvement than CPU-only and CPU-GPU, respectively.
6.4 TensorDIMM with Low-bandwidth System
Interconnects
To highlight the maximum potential of TensorDIMM, we discussed
its merits under the context of a high-bandwidth GPU-side interconnect. Nonetheless, it is still possible for TensorDIMM to be utilized
under conventional, CPU-centric disaggregated memory systems.
Concretely, one can envision a system that contains a pooled memory like TensorNode interfaced over a low-bandwidth system interconnect (e.g., PCIe). Such design point looks similar to the hybrid
CPU-GPU except for one key distinction: the tensor operations are
done using the NMP cores but the reduced tensor must be copied
to the GPU over a slower communication channel. Figure 16 summarizes the sensitivity of our proposal on the TensorNode↔GPU
communication bandwidth. We study both PMEM (i.e., disaggregated
memory “without” TensorDIMMs) and TDIMM under low-bandwidth
system interconnects to highlight the robustness our TensorDIMM
brings about. Overall, PMEM is much more sensitive to the communication bandwidth than TDIMM (maximum 68% performance
loss) because the benefits of NMP reduction is lost with PMEM. Our
TensorDIMM design (TDIMM) on the other hand only experiences
up to 15% performance loss (average 10%) even with a 6× lower
communication bandwidth. These results highlight the robustness
and the wide applicability of TensorDIMM.
6.5 Design Overheads
TensorDIMM requires no changes to the DRAM chip itself but adds
a lightweight NMP core inside the buffer device (Section 4.2). We
implement and synthesized the major components of our NMP
core on a Xilinx Virtex UltraScale+ FPGA board using Verilog
HDL. We confirm that the added area/power overheads of our
NMP core is mostly negligible as it is dominated by the small (1.5
KB) SRAM queues and the 16-wide vector ALU (Table 3). From a
system-level perspective, our work utilizes GPUs as-is, so the major
overhead comes from the disaggregated TensorNode design. Assuming a single TensorDIMM uses 128 GB load-reduced DIMM [28],
its power consumption becomes 13W when estimated using Micron’s DDR4 system power calculator [47]. For TensorNode with 32
TensorDIMMs, this amounts to a power overhead of (13×32)=416W.
Recent specifications for accelerator interconnection fabric endpoints (e.g., Open Compute Project [20]’s open accelerator module [19]) employ a TDP of 350−700W so the power overheads of
TensorNode is expected to be acceptable.
7 RELATED WORK
Disaggregated memory [41, 42] is typically deployed as a remote
memory pool connected over PCIe, which helps increase CPU accessible memory capacity. Prior work [13, 38, 39] proposed systemlevel solutions that embrace the idea of memory disaggregation
within the high-bandwidth GPU interconnect, which bears similarity to our proposition on TensorNode. However, the focus of
these two prior work is on DL training whereas our study primarily focuses on DL inference. Similar to our TensorDIMM, several
prior work [4, 6, 21, 46] explored the possibility of utilizing the
DIMM buffer device space to add custom acceleration logics. Alian
et al. [4] for instance adds a lightweight CPU core inside the buffer
device to construct a memory channel network (MCN), seeking to
750
MICRO ’52, October 12–16, 2019, Columbus, OH, USA Youngeun Kwon, Yunjae Lee, Minsoo Rhu
0
0.2
0.4
0.6
0.8
1
1.2
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
CPU-only
CPU-GPU
PMEM
TDIMM
GPU-only
Batch 8 Batch 64 Batch 128 Batch 8 Batch 64 Batch 128 Batch 8 Batch 64 Batch 128 Batch 8 Batch 64 Batch 128 Geometric
NCF YouTube Fox Facebook mean
Performance
(normalized)
Figure 14: Performance of the five design points of recommender systems, normalized to the oracular GPU (GPU-only).
0
2
4
6
8
10
12
14
16
18
20
22
24
Batch 8
Batch 64
Batch 128
Batch 8
Batch 64
Batch 128
Batch 8
Batch 64
Batch 128
Batch 8
Batch 64
Batch 128
Batch 8
Batch 64
Batch 128
Batch 8
Batch 64
Batch 128
Batch 8
Batch 64
Batch 128
Batch 8
Batch 64
Batch 128
Embedding
(1x)
Embedding
(2x)
Embedding
(4x)
Embedding
(8x)
Embedding
(1x)
Embedding
(2x)
Embedding
(4x)
Embedding
(8x)
Compared to CPU-only Compared to Hybrid CPU-GPU
Performance
(normalized)
Figure 15: TensorDIMM performance with larger embedding size (up
to 8×). Results are averaged across the four studied benchmarks.
0
0.2
0.4
0.6
0.8
1
1.2
25 GB/sec
50 GB/sec
150 GB/sec
25 GB/sec
50 GB/sec
150 GB/sec
25 GB/sec
50 GB/sec
150 GB/sec
25 GB/sec
50 GB/sec
150 GB/sec
25 GB/sec
50 GB/sec
150 GB/sec
25 GB/sec
50 GB/sec
150 GB/sec
25 GB/sec
50 GB/sec
150 GB/sec
25 GB/sec
50 GB/sec
150 GB/sec
Embedding
(1x)
Embedding
(2x)
Embedding
(4x)
Embedding
(8x)
Embedding
(1x)
Embedding
(2x)
Embedding
(4x)
Embedding
(8x)
PMEM TDIMM
Performance
(normalized)
Figure 16: Performance sensitivity of PMEM (i.e., pooled memory
without NMP acceleration) and TensorDIMM to the communication
bandwidth. Results are averaged across the four studied benchmarks, and are normalized to the default configuration of 150
GB/sec data point.
amplify effective memory bandwidth exposed to the overall system.
While TensorDIMM’s near-memory processing approach bears
some similarity with MCN, direct application of this design for
embedding layer’s tensor operations will likely lead to sub-optimal
performance because of MCN’s inability to maximally utilize DRAM
bandwidth for embeddings. Concretely, Gupta et al. [24] showed the
performance of embedding lookup operations with CPUs, reporting that the irregular, sparse memory access nature of embedding
operations render the CPU cache hit-rate to become extremely low.
As such, the latency to traverse the cache hierarchy leads to less
than 5% of the maximum DRAM bandwidth being utilized, as opposed to the TensorDIMM NMP cores being able to reach close to
maximum DRAM bandwidth on average. In general, the scope of
all these prior studies is significantly different from what our work
focuses on. To the best of our knowledge, our work is the first in
Table 3: FPGA utilization of a single NMP core (i.e., SRAM queues
and vector ALU with single-precision floating point (FPU) and fixed
point ALU) on Xilinx Virtex UltraScale+ VCU1525 acceleration development board.
LUT [%] FF [%] DSP [%] BRAM [%]
SRAM queues 0.00 0.00 0.00 0.01
FPU 0.19 0.01 0.20 0.00
ALU 0.09 0.01 0.01 0.00
both industry and academia to identify and address the memory
capacity and bandwidth challenges of embedding layers, which
several hyperscalars [15, 27, 52, 64, 67] deem as one of the most
crucial challenge in emerging DL workloads.
Other than these closely related studies, a large body of prior
work has explored the design of a single GPU/NPU architecture
for DL [8, 10–12, 17, 18, 22, 31, 35, 43–45, 50, 51, 68, 69, 73, 74] with
recent interest on leveraging sparsity for further energy-efficiency
improvements [2, 3, 16, 25, 34, 53, 63, 70, 78–81]. A scale-out acceleration platform for training DL algorithms was proposed by Park
et al. [65] and a network-centric DL training platform has been
proposed by Li et al. [40]. These prior studies are orthogonal to our
proposal and can be adopted further for additional enhancements.
8 CONCLUSION
In this paper, we propose a vertically integrated, hardware/software
co-design that addresses the memory (capacity and bandwidth)
wall problem of embedding layers, an important building block
for emerging DL applications. Our TensorDIMM architecture synergistically combines NMP cores with commodity DRAM devices
to accelerate DL tensor operations. Built on top of a disaggregated
memory pool, TensorDIMM provides memory capacity and bandwidth scaling for embeddings, achieving an average 6.2−15.0× and
8.9−17.6× performance improvement than conventional CPU-only
and hybrid CPU-GPU implementations of recommender systems. To
the best of our knowledge, TensorDIMM is the first that quantitatively explores architectural solutions tailored for embeddings and
tensor operations