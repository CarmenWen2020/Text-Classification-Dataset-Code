ABSTRACT
Recurrent Neural Networks (RNNs) are a key technology for applications such as automatic speech recognition or machine translation. Unlike conventional feed-forward DNNs, RNNs remember
past information to improve the accuracy of future predictions and,
therefore, they are very effective for sequence processing problems.
For each application run, each recurrent layer is executed many
times for processing a potentially large sequence of inputs (words,
images, audio frames, etc.). In this paper, we make the observation
that the output of a neuron exhibits small changes in consecutive
invocations. We exploit this property to build a neuron-level fuzzy
memoization scheme, which dynamically caches the output of each
neuron and reuses it whenever it is predicted that the current output
will be similar to a previously computed result, avoiding in this
way the output computations.
The main challenge in this scheme is determining whether the
new neuronâ€™s output for the current input in the sequence will be
similar to a recently computed result. To this end, we extend the
recurrent layer with a much simpler Bitwise Neural Network (BNN),
and show that the BNN and RNN outputs are highly correlated:
if two BNN outputs are very similar, the corresponding outputs
in the original RNN layer are likely to exhibit negligible changes.
The BNN provides a low-cost and effective mechanism for deciding
when fuzzy memoization can be applied with a small impact on
accuracy.
We evaluate our memoization scheme on top of a state-of-the-art
accelerator for RNNs, for a variety of different neural networks
from multiple application domains. We show that our technique
avoids more than 24.2% of computations, resulting in 18.5% energy
savings and 1.35x speedup on average.
CCS CONCEPTS
â€¢ Computer systems organizationâ†’Neural Networks;â€¢ Computing Methodologies â†’ Machine Learning;
KEYWORDS
Recurrent Neural Networks, Long Short Term Memory, Binary
Networks, Memoization

1 INTRODUCTION
Recurrent Neuronal Networks (RNNs) represent the state-of-the-art
solution for many sequence processing problems such as speech
recognition [15], machine translation [34] or automatic caption generation [32]. Not surprisingly, data recently published in [20] show
that around 30% of machine learning workloads in Googleâ€™s datacenters are RNNs, whereas Convolutional Neuronal Networks (CNNs)
only represent 5% of the applications. Unlike CNNs, RNNs use information of previously processed inputs to improve the accuracy
of the output, and they can process variable length input/output
sequences.
Although RNN training can be performed efficiently on GPUs [7],
RNN inference is more challenging. The small batch size (just one
input sequence per batch) and the data dependencies in recurrent
layers severely constrain the amount of parallelism. Hardware acceleration is key for achieving high-performance and energy-efficient
RNN inference and, to this end, several RNN accelerators have been
recently proposed [17, 18, 22, 23].
Neurons in an RNN are recurrently executed for processing the
elements in an input sequence. An analysis of the output results
reveals that many neurons produce very similar outputs for consecutive elements in the input sequence. On average, the relative
difference between the current and previous output of a neuron
is smaller than 23% in our set of RNNs, whereas previous work
in [28] has reported similar results. Since RNNs are inherently error
tolerant [36], we propose to exploit the aforementioned property
to save computations by using a neuron-level fuzzy memoization
scheme. With this approach, the outputs of a neuron are dynamically cached in a local memoization buffer. When the next output is
predicted to be extremely similar to the previously computed result,
the neuronâ€™s output is read from the memoization buffer rather
than recalculating it, avoiding all the corresponding computations
and memory accesses.
782
MICRO-52, October 12â€“16, 2019, Columbus, OH, USA Silfa, et al.
0 0.1 0.2 0.3 0.4 0.5 0.6
Threshold
0
2
5
10
15
20
WER Loss (%)
0
10
20
30
40
50
60
70
80
Computation Reuse (%)
DeepSpeech
WER Loss
Computation Reuse
0 0.1 0.2 0.3 0.4 0.5 0.6
Threshold
0
2
5
10
15
20
WER Loss (%)
0
10
20
30
40
50
60
70
80
Computation Reuse (%)
EESEN
WER Loss
Computation Reuse
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Threshold
0
5
10
15
20
25
Accuracy Loss (%)
0
10
20
30
40
50
60
70
Computation Reuse (%)
IMDB Sentiment
Accuracy Loss
Computation Reuse
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Threshold
0
2
5
10
15
Bleu Loss (%)
0
10
20
30
40
50
60
Computation Reuse (%)
Machine Translation
Bleu Loss
Computation Reuse
Figure 1: Accuracy loss of different RNNs versus the relative output error threshold using an oracle predictor. If the difference
between the previous and current output predicted is smaller than the threshold, the memoized output is employed instead
of calculating the new one.
Figure 1 shows the potential benefits of this memoization scheme
by using an oracle that accurately predicts the relative difference
between the next output of the neuron and the previous output
stored in the memoization buffer. The memoized value is used
when this difference is smaller than a given threshold, shown in
the x-axis of Figure 1. As it can be seen, the RNNs can tolerate
relative errors in the outputs of a neuron in the range of 30-50%
with a negligible impact on accuracy. With these thresholds, a
memoization scheme with an oracle predictor can save more than
30% of the computations.
A key challenge for our memoization scheme is how to predict
the difference between the current output and the previous output
stored in the memoization buffer, without performing all the corresponding neuron computations. To this end, we propose to extend
each recurrent layer with a Bitwise Neural Network (BNN) [21]. We
do this by reducing each input and weight to one bit that represents
the sign as described in [11]. We found that BNN outputs are highly
correlated with the outputs of the original recurrent layer, i.e. a similar BNN outputs indicates a high likelihood of having similar RNN
output (although BNN outputs are very different to RNN outputs).
The BNN is extremely small, hardware-friendly and very effective
at predicting when memoization can be safely applied.
Note that by simply looking at the inputs, i.e. predicting that
similar inputs will produce similar outputs, might not be accurate.
Small changes in an input that is multiplied by a large weight will
introduce a significant change in the output of the neuron. Our
BNN approach takes into account both the inputs and the weights.
In short, we propose a neuron-level hardware-based fuzzy memoization scheme that works as follows. The output of a neuron in
the last execution is dynamically cached in a memoization table,
together with the output of the corresponding BNN. For every new
input in the sequence, the BNN is first computed and the result is
compared with the BNN output stored in the memoization table. If
the difference between the new BNN output and the cached output
is smaller than a threshold, the neuronâ€™s cached output is used as
the current output, avoiding all the associated computations and
memory accesses in the RNN. Otherwise, the neuron is evaluated
and the memoization table is updated.
Note that only using the BNN would result in a large accuracy
loss as reported elsewhere [27]. In this paper, we take a completely
different approach and use the BNN to predict when memoization
can be safely applied with negligible impact on accuracy. The inexpensive BNN is computed for every element of the sequence and
every neuron, whereas the large RNN is evaluated on demand as
indicated by the BNN. By doing so, we maintain high accuracy
while saving more than 24.2% of RNN computations.
In this paper we make the following contributions:
â€¢ We provide an evaluation of the outputs of neurons in recurrent layers, and show that they exhibit small changes in
consecutive executions.
â€¢ We propose a fuzzy memoization scheme that avoids more
than 24.2% of neuron evaluations by reusing previosly computed results stored in a memoization buffer.
â€¢ We propose the use of a BNN to determine when memoization can be applied with small impact on accuracy. We show
that BNN and RNN outputs are highly correlated.
783
Neuron-Level Fuzzy Memoization in RNNs MICRO-52, October 12â€“16, 2019, Columbus, OH, USA





 

	

 







 

 

Figure 2: Structure of a LSTM cell.  denotes an elementwise multiplication of two vectors. Ï• denotes the hyperbolic
tangent.



 	 






Figure 3: Structure of a GRU cell.
â€¢ We implement our neuron-level memoization scheme on top
of a state-of-the-art RNN accelerator. The required hardware
introduces a negligible area overhead, while it provides 1.35x
speedup and 18.5% energy savings on average for several
RNNs.
2 BACKGROUND
2.1 Recurrent Neural Networks
A Recurrent Neural Network (RNN) is a state-of-the-art machine
learning approach that has achieved tremendous success in applications such as machine translation or video description. The
key characteristic of RNNs is that they include loops, a.k.a. recurrent connections, that allow the information to persist from one
time-step of execution to the next ones and, hence, they have the
potential to use unbounded context information (i.e. past or future) to make predictions. Another important feature is that RNNs
are recurrently executed for every element of the input sequence
and, thus, they are able to handle input and output with variable
length. Because of these characteristics, RNNs provide an effective
framework for sequence-to-sequence applications (e.g. machine
translation), where they outperform feed forward Deep Neural
Networks (DNNs) [16, 29].
Basic RNN architectures can capture and exploit short term dependencies in the input sequence. However, capturing long term
dependencies is challenging since useful information tend to dilute
over time. In order to exploit long term dependencies, Long Short
Term Memory (LSTM) [19] and Gated Recurrent Units (GRU) [10]
networks were proposed. These types of RNNs represent the most
successful and widely used RNN architectures. They have achieved
tremendous succcess for a variety of applications such as speech
recognition [5, 24], machine translation [9] and video description [32]. The next subsections provide further details on the structure and behavior of these networks.
2.1.1 Deep RNNs. RNNs are composed of multiple layers that
are stacked together to create deep RNNs. Each of these layers
consists of an LSTM or a GRU cell. In addition, these layers can be
unidirectional or bidirectional. Unidirectional layers only use past
information to make predictions, whereas bidirectional LSTM or
GRU networks use both past and future context.
The input sequence (X) is composed of N elements, i.e. X =
[x1, x2, ..., xN ], which are processed by an LSTM or GRU cell in
the forward direction, i.e. from x1 to xN . For backward layers in
bidirectional RNNs, the input sequence is evaluated in the backward
direction, i.e from xN to x1.
2.1.2 LSTM Cell. Figure 2 shows the structure of an LSTM cell.
The key component is the cell state (ct ), which is stored in the cell
memory. The cell state is updated by using three fully connected
single-layer neural networks, a.k.a. gates. The input gate, (it , whose
computations are shown in Equation 1) decides how much of the
input information, xt , will be added to the cell state. The forget gate
(ft , shown in Equation 2) determines how much information will be
erased from the cell state (ctâˆ’1). The updater gate (Ð´t , Equation 3)
controls the amount of input information that is being considered
a candidate to update the cell state (ct ). Once these three gates are
executed, the cell state is updated according to Equation 4. Finally,
the output gate (ot , Equation 5) decides the amount of information
that will be emitted from the cell to create the output (ht ).
Figure 4 shows the computations carried out by an LSTM cell. As
it can be seen, a neuron in each gate has two types of connections:
forward connections that operate on xt and recurrent connections
that take as input htâˆ’1. The evaluation of a neuron in one of these
gates requires a dot product between weights in forward connections and xt , and another dot product between weights in recurrent
connections and htâˆ’1. Next, a peephole connection [13] and a bias
are also applied, followed by the computation of an activation function, typically a sigmoid or hyperbolic tangent.
2.1.3 GRU Cell. Analogous to an LSTM cell, a GRU cell includes
gates to control the flow of information inside the cell. However,
GRU cells do not have an independent memory cell (i.e. cell state).
As it can be seen in Figure 3, in a GRU cell the update gate (zt )
controls how much of the candidate information (Ð´t ) is used to
update the cell activation. On the other hand, the reset gate (rt )
modulates the amount of information that is removed from the
previous computed state. Note that GRUs do not include an output
gate and, hence, the whole state of the cell is exposed at each
timestep. The computations carried out by each gate in a GRU cell
are very similar to those in Equations 1, 2 and 3. We omit them for
784