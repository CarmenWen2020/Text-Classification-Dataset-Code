Fig. 1. Point cloud segmentation using the proposed neural network. Bottom: schematic neural network architecture. Top: Structure of the feature spaces
produced at different layers of the network, visualized as the distance from the red point to all the rest of the points (shown left-to-right are the input
and layers 1–3; rightmost figure shows the resulting segmentation). Observe how the feature space structure in deeper layers captures semantically similar
structures such as wings, fuselage, or turbines, despite a large distance between them in the original input space.
Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output
of most 3D data acquisition devices. While hand-designed features
on point clouds have long been proposed in graphics and vision, however,
the recent overwhelming success of convolutional neural networks
(CNNs) for image analysis suggests the value of adapting insight from
CNN to the point cloud world. Point clouds inherently lack topological
information, so designing a model to recover topology can enrich the
representation power of point clouds. To this end, we propose a new
neural network module dubbed EdgeConv suitable for CNN-based highlevel tasks on point clouds, including classification and segmentation.
EdgeConv acts on graphs dynamically computed in each layer of the
network. It is differentiable and can be plugged into existing architectures.
Compared to existing modules operating in extrinsic space or treating
each point independently, EdgeConv has several appealing properties: It
incorporates local neighborhood information; it can be stacked applied to
learn global shape properties; and in multi-layer systems affinity in feature
space captures semantic characteristics over potentially long distances
in the original embedding. We show the performance of our model on
standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.
CCS Concepts: • Computing methodologies → Neural networks;
Point-based models; Shape analysis;
Additional Key Words and Phrases: Point cloud, classification, segmentation
1 INTRODUCTION
Point clouds, or scattered collections of points in 2D or 3D, are arguably the simplest shape representation; they also comprise the
output of 3D sensing technology, including LiDAR scanners and
stereo reconstruction. With the advent of fast 3D point cloud acquisition, recent pipelines for graphics and vision often process
point clouds directly, bypassing expensive mesh reconstruction or
denoising due to efficiency considerations or instability of these
techniques in the presence of noise. A few of the many recent applications of point cloud processing and analysis include indoor
navigation (Zhu et al. 2017), self-driving vehicles (Liang et al. 2018;
Qi et al. 2017a; Wang et al. 2018b), robotics (Rusu et al. 2008b), and
shape synthesis and modeling (Golovinskiy et al. 2009; Guerrero
et al. 2018).
These modern applications demand high-level processing of
point clouds. Rather than identifying salient geometric features
like corners and edges, recent algorithms search for semantic cues
and affordances. These features do not fit cleanly into the frameworks of computational or differential geometry and typically require learning-based approaches that derive relevant information
through statistical analysis of labeled or unlabeled datasets.
In this article, we primarily consider point cloud classification
and segmentation, two model tasks in point cloud processing. Traditional methods for solving these problems employ handcrafted
features to capture geometric properties of point clouds (Lu et al.
2014; Rusu et al. 2009, 2008a). More recently, the success of deep
neural networks for image processing has motivated a data-driven
approach to learning features on point clouds. Deep point cloud
processing and analysis methods are developing rapidly and outperform traditional approaches in various tasks (Chang et al. 2015).
Adaptation of deep learning to point cloud data, however, is far
from straightforward. Most critically, standard deep neural network models require input data with regular structure, while point
clouds are fundamentally irregular: Point positions are continuously distributed in the space, and any permutation of their ordering does not change the spatial distribution. One common approach to process point cloud data using deep learning models is
to first convert raw point cloud data into a volumetric representation, namely a 3D grid (Maturana and Scherer 2015; Wu et al. 2015).
This approach, however, usually introduces quantization artifacts
and excessive memory usage, making it difficult to go to capture
high-resolution or fine-grained features.
State-of-the-art deep neural networks are designed specifically
to handle the irregularity of point clouds, directly manipulating
raw point cloud data rather than passing to an intermediate regular representation. This approach was pioneered by PointNet (Qi
et al. 2017b), which achieves permutation invariance of points by
operating on each point independently and subsequently applying
a symmetric function to accumulate features. Various extensions
of PointNet consider neighborhoods of points rather than acting
on each independently (Qi et al. 2017c; Shen et al. 2017); these allow
the network to exploit local features, improving upon performance
of the basic model. These techniques largely treat points independently at local scale to maintain permutation invariance. This independence, however, neglects the geometric relationships among
points, presenting a fundamental limitation that cannot capture
local features.
To address these drawbacks, we propose a novel simple operation, called EdgeConv, which captures local geometric structure
while maintaining permutation invariance. Instead of generating
point features directly from their embeddings, EdgeConv generates edge features that describe the relationships between a point
and its neighbors. EdgeConv is designed to be invariant to the ordering of neighbors, and thus is permutation invariant. Because
EdgeConv explicitly constructs a local graph and learns the embeddings for the edges, the model is capable of grouping points
both in Euclidean space and in semantic space.
EdgeConv is easy to implement and integrate into existing deep
learning models to improve their performance. In our experiments,
we integrate EdgeConv into the basic version of PointNet without
using any feature transformation. We show the resulting network
achieves state-of-the-art performance on several datasets, most
notably ModelNet40 and S3DIS for classification and segmentation.
Key Contributions. We summarize the key contributions of our
work as follows:
• We present a novel operation for learning from point clouds,
EdgeConv, to better capture local geometric features of point
clouds while still maintaining permutation invariance.
• We show the model can learn to semantically group points
by dynamically updating a graph of relationships from layer
to layer.
• We demonstrate that EdgeConv can be integrated into multiple existing pipelines for point cloud processing.
• We present extensive analysis and testing of EdgeConv and
show that it achieves state-of-the-art performance on benchmark datasets.
2 RELATED WORK
Hand-Crafted Features. Various tasks in geometric data processing and analysis—including segmentation, classification, and
matching—require some notion of local similarity between shapes.
Traditionally, this similarity is established by constructing feature
descriptors that capture local geometric structure. Countless
papers in computer vision and graphics propose local feature
descriptors for point clouds suitable for different problems and
data structures. A comprehensive overview of hand-designed
point features is out of the scope of this article, but we refer the
reader to Biasotti et al. (2016), Guo et al. (2014), and Van Kaick
et al. (2011) for discussion.
Broadly speaking, one can distinguish between extrinsic and
intrinsic descriptors. Extrinsic descriptors usually are derived from
the coordinates of the shape in 3D space and includes classical
methods like shape context (Belongie et al. 2001), spin images
(Johnson and Hebert 1999), integral features (Manay et al. 2006),
distance-based descriptors (Ling and Jacobs 2007), point feature
histograms (Rusu et al. 2009, 2008a), and normal histograms
(Tombari et al. 2011), to name a few. Intrinsic descriptors treat
ACM Transactions on Graphics, Vol. 38, No. 5, Article 146. Publication date: October 2019.
Dynamic Graph CNN for Learning on Point Clouds • 146:3
Fig. 2. Left: Computing an edge feature, ei j (top), from a point pair, xi and xj (bottom). In this example, hΘ() is instantiated using a fully connected layer,
and the learnable parameters are its associated weights. Right: The EdgeConv operation. The output of EdgeConv is calculated by aggregating the edge
features associated with all the edges emanating from each connected vertex.
the 3D shape as a manifold whose metric structure is discretized
as a mesh or graph; quantities expressed in terms of the metric
are invariant to isometric deformation. Representatives of this
class include spectral descriptors such as global point signatures
(Rustamov 2007), the heat and wave kernel signatures (Aubry et al.
2011; Sun et al. 2009), and variants (Bronstein and Kokkinos 2010).
Most recently, several approaches wrap machine learning schemes
around standard descriptors (Guo et al. 2014; Shah et al. 2013).
Deep Learning on Geometry. Following the breakthrough results
of convolutional neural networks (CNNs) in vision (Krizhevsky
et al. 2012; LeCun et al. 1989), there has been strong interest to
adapt such methods to geometric data. Unlike images, geometry
usually does not have an underlying grid, requiring new building
blocks replacing convolution and pooling or adaptation to a grid
structure.
As a simple way to overcome this issue, view-based (Su et al.
2015; Wei et al. 2016) and volumetric representations (Klokov and
Lempitsky 2017; Maturana and Scherer 2015; Tatarchenko et al.
2017; Wu et al. 2015)—or their combination (Qi et al. 2016)—“place”
geometric data onto a grid. More recently, PointNet (Qi et al. 2017b,
2017c) exemplifies a broad class of deep learning architectures on
non-Euclidean data (graphs and manifolds) termed geometric deep
learning (Bronstein et al. 2017). These date back to early methods
to construct neural networks on graphs (Scarselli et al. 2009), recently improved with gated recurrent units (Li et al. 2016) and
neural message passing (Gilmer et al. 2017). Bruna et al. (2013)
and Henaff et al. (2015) generalized convolution to graphs via the
Laplacian eigenvectors (Shuman et al. 2013). Computational drawbacks of this foundational approach were alleviated in follow-up
works using polynomial (Defferrard et al. 2016; Kipf and Welling
2017; Monti et al. 2017b, 2018), or rational (Levie et al. 2017) spectral filters that avoid Laplacian eigendecomposition and guarantee localization. An alternative definition of non-Euclidean convolution employs spatial rather than spectral filters. The Geodesic
CNN (GCNN) is a deep CNN on meshes generalizing the notion of
patches using local intrinsic parameterization (Masci et al. 2015).
Its key advantage over spectral approaches is better generalization as well as a simple way of constructing directional filters.
Follow-up work proposed different local charting techniques using anisotropic diffusion (Boscaini et al. 2016) or Gaussian mixture
models (Monti et al. 2017a; Veličković et al. 2017). In Halimi et al.
(2018) and Litany et al. (2017b), a differentiable functional map
(Ovsjanikov et al. 2012) layer was incorporated into a geometric
deep neural network, allowing to do intrinsic structured prediction of correspondence between nonrigid shapes.
The last class of geometric deep learning approaches attempts
to pull back a convolution operation by embedding the shape into
a domain with shift-invariant structure such as the sphere (Sinha
et al. 2016), torus (Maron et al. 2017), plane (Ezuz et al. 2017), sparse
network lattice (Su et al. 2018), or spline (Fey et al. 2018).
Finally, we should mention geometric generative models, which
attempt to generalize models such as autoencoders, variational
autoencoders (VAE) (Kingma and Welling 2013), and generative
adversarial networks (GAN) (Goodfellow et al. 2014) to the nonEuclidean setting. One of the fundamental differences between
these two settings is the lack of canonical order between the input
and the output vertices, thus requiring an input-output correspondence problem to be solved. In 3D mesh generation, it is commonly
assumed that the mesh is given and its vertices are canonically ordered; the generation problem thus amounts only to determining
the embedding of the mesh vertices. Kostrikov et al. (2017) proposed SurfaceNets based on the extrinsic Dirac operator for this
task. Litany et al. (2017a) introduced the intrinsic VAE for meshes
and applied it to shape completion; a similar architecture was used
by Ranjan et al. (2018) for 3D face synthesis. For point clouds, multiple generative architectures have been proposed (Fan et al. 2017;
Li et al. 2018b; Yang et al. 2018).
3 OUR APPROACH
We propose an approach inspired by PointNet and convolution operations. Instead of working on individual points like PointNet,
however, we exploit local geometric structures by constructing a
local neighborhood graph and applying convolution-like operations on the edges connecting neighboring pairs of points, in the
spirit of graph neural networks. We show in the following that
such an operation, dubbed edge convolution (EdgeConv), has properties lying between translation-invariance and non-locality.
Unlike graph CNNs, our graph is not fixed but rather is dynamically updated after each layer of the network. That is, the set of
k-nearest neighbors of a point changes from layer to layer of the
network and is computed from the sequence of embeddings. Proximity in feature space differs from proximity in the input, leading
to nonlocal diffusion of information throughout the point cloud.
As a connection to existing work, Non-local Neural Networks
(Wang et al. 2018a) explored similar ideas in the video recognition
field, and follow-up work by Xie et al. (2018) proposed using
ACM Transactions on Graphics, Vol. 38, No. 5, Article 146. Publication date: October 2019.
146:4 • Y. Wang et al.
Fig. 3. Model architectures: The model architectures used for classification (top branch) and segmentation (bottom branch). The classification model takes
as input n points, calculates an edge feature set of size k for each point at an EdgeConv layer, and aggregates features within each set to compute EdgeConv
responses for corresponding points. The output features of the last EdgeConv layer are aggregated globally to form an 1D global descriptor, which is used
to generate classification scores for c classes. The segmentation model extends the classification model by concatenating the 1D global descriptor and all
the EdgeConv outputs (serving as local descriptors) for each point. It outputs per-point classification scores for p semantic labels. ⊕: concatenation. Point
cloud transform block: The point cloud transform block is designed to align an input point set to a canonical space by applying an estimated 3 × 3 matrix.
To estimate the 3 × 3 matrix, a tensor concatenating the coordinates of each point and the coordinate differences between its k neighboring points is used.
EdgeConv block: The EdgeConv block takes as input a tensor of shape n × f , computes edge features for each point by applying a multi-layer perceptron
(mlp) with the number of layer neurons defined as {a1, a2, ..., an }, and generates a tensor of shape n × an after pooling among neighboring edge features.
non-local blocks to denoise feature maps to defend against
adversarial attacks.
3.1 Edge Convolution
Consider an F -dimensional point cloud with n points, denoted by
X = {x1,..., xn } ⊆ RF . In the simplest setting of F = 3, each point
contains 3D coordinates xi = (xi,yi, zi ); it is also possible to include additional coordinates representing color, surface normal,
and so on. In a deep neural network architecture, each subsequent
layer operates on the output of the previous layer, so more generally the dimension F represents the feature dimensionality of a
given layer.
We compute a directed graph G = (V, E) representing local
point cloud structure, where V = {1,...,n} and E ⊆V×V are
the vertices and edges, respectively. In the simplest case, we construct G as the k-nearest neighbor (k-NN) graph of X in RF . The
graph includes self-loop, meaning each node also points to itself. We define edge features as eij = hΘ(xi, xj ), where hΘ : RF ×
RF → RF 
is a nonlinear function with a set of learnable parameters Θ.
Finally, we define the EdgeConv operation by applying a
channel-wise symmetric aggregation operation  (e.g.,  or max)
on the edge features associated with all the edges emanating from
each vertex. The output of EdgeConv at the i-th vertex is thus given
by
x
i =
j:(i,j)∈E
hΘ(xi, xj ). (1)
Making analogy to convolution along images, we regard xi as the
central pixel and {xj : (i, j) ∈ E} as a patch around it (see Figure 2).
Overall, given an F -dimensional point cloud with n points, EdgeConv produces an F 
-dimensional point cloud with the same number of points.
Choice of h and . The choice of the edge function and the aggregation operation has a crucial influence on the properties of
EdgeConv. For example, when x1,..., xn represent image pixels
on a regular grid and the graph G has connectivity representing
patches of fixed size around each pixel, the choice θm · xj as the
edge function and sum as the aggregation operation yields standard convolution:
x 
im =

j:(i,j)∈E
θm · xj . (2)
Here, Θ = (θ1,..., θ M ) encodes the weights of M different filters.
Each θm has the same dimensionality as x, and · denotes the Euclidean inner product.
A second choice of h is
hΘ(xi, xj ) = hΘ(xi ), (3)
encoding only global shape information oblivious of the local
neighborhood structure. This type of operation is used in PointNet, which can thus be regarded as a special case of EdgeConv.
A third choice of h adopted by Atzmon et al. (2018) is
hΘ(xi, xj ) = hΘ(xj ) (4)
ACM Transactions on Graphics, Vol. 38, No. 5, Article 146. Publication date: October 2019.