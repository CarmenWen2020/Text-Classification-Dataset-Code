In order to provide an immersive visual experience, modern displays require
head mounting, high image resolution, low latency, as well as high refresh
rate. This poses a challenging computational problem. On the other hand, the
human visual system can consume only a tiny fraction of this video stream
due to the drastic acuity loss in the peripheral vision. Foveated rendering and
compression can save computations by reducing the image quality in the peripheral vision. However, this can cause noticeable artifacts in the periphery,
or, if done conservatively, would provide only modest savings. In this work,
we explore a novel foveated reconstruction method that employs the recent
advances in generative adversarial neural networks. We reconstruct a plausible peripheral video from a small fraction of pixels provided every frame.
The reconstruction is done by finding the closest matching video to this
sparse input stream of pixels on the learned manifold of natural videos. Our
method is more efficient than the state-of-the-art foveated rendering, while
providing the visual experience with no noticeable quality degradation. We
conducted a user study to validate our reconstruction method and compare
it against existing foveated rendering and video compression techniques.
Our method is fast enough to drive gaze-contingent head-mounted displays
in real time on modern hardware. We plan to publish the trained network to
establish a new quality bar for foveated rendering and compression as well
as encourage follow-up research.
CCS Concepts: • Computing methodologies → Neural networks; Perception; Virtual reality; Image compression.
Additional Key Words and Phrases: generative networks, perceptual rendering, foveated rendering, deep learning, virtual reality, gaze-contingent
rendering, video compression, video generation
1 INTRODUCTION
Despite tremendous advances in consumer hardware for real-time
rendering and video compression, the demand for high-fidelity visuals continues to grow. Recent advances in head-mounted displays
allow us to achieve a new level of immersion by delivering the
imagery straight to the eyes. However, such displays also require
a significantly higher resolution and refresh rate to provide high
quality immersion and good visual experience across the entire field
of view. Rendering this high-quality content is challenging even on
current high-end desktop systems.
On the other hand, the human eye has a very heterogeneous resolution density. It is able to resolve objects as small as 1 arcminute
in the fovea, the center 5.2◦
region of the retina, and experiences a
rapid acuity falloff outside the fovea toward the periphery [Curcio
et al. 1990]. Fovea covers roughly 0.8% of pixels on a regular display under common viewing conditions [Guenter et al. 2012] and
around 4% of pixels on consumer virtual reality (VR) headsets [Patney et al. 2016], such as HTC Vive and Oculus Rift. With the recent
developments in gaze-contingent VR displays, such as the recently
announced HTC Vive Pro Eye, it is also possible to estimate the
user gaze in real time and perform gaze-contingent rendering and
compression. This provides an important opportunity to optimize
the amount of computation required to drive such displays, enabling
ACM Trans. Graph., Vol. 38, No. 4, Article 212. Publication date: July 2019.
212:2 • Kaplanyan, A. S. et al.
higher quality visuals, larger resolution displays, and facilitating
the miniaturization into mobile and wearable headsets, ultimately
improving immersion and visual experience.
The foveation effect of human vision has been studied in various
fields, including foveated video compression [Lee et al. 2001], and
more recently foveated rendering [Guenter et al. 2012], achieving
50–70% in savings [Weier et al. 2017]. While peripheral compression
has significant potential, one has to be careful about the artifacts
that can be detected in the periphery, such as the loss of contrast
with details (tunnel vision effect) and flicker [Patney et al. 2016], to
which peripheral vision is especially sensitive [Rovamo et al. 1984].
Rendering can also employ a sparse foveated pattern [Stengel et al.
2016; Weier et al. 2016], which becomes a practical option with the
recent advances in real-time ray tracing hardware, such as NVIDIA
RTX, and fine resolution control for rasterization.
This motivated us to use a stochastically foveated video as an
input, which is general enough to cover a wide variety of use cases.
For example, this input is suitable both for foveated rendering and
for foveated video compression. We use a sparse video stream with
only a small fraction of pixels stochastically distributed per frame.
For regular videos, we use a simple compressor that stochastically
drops pixels in the periphery according to human visual acuity. This
approach also makes the compressor compatible with many foveated
rendering methods. Moreover, a video-producing module can be
treated as a black box, compressing video streams produced from a
variety of existing applications, e.g., for untethered or cloud-based
VR gaming.
Our main contribution is the peripheral reconstruction method.
DeepFovea starts with a sparse stream of color pixel values as an
input. Given this sparse stream, we formulate the peripheral reconstruction problem as a projection-to-manifold problem, where
the goal is to find the closest natural video that corresponds to the
sparse foveated input on the manifold of all natural videos. This
approach is similar to the internal model of the human visual system that infers content in the periphery using the available sparse
and aliased peripheral information [Geisler 2008]. We employ this
relation by using the recent advances in the adversarial training of
generative video networks to train a reconstruction network to infer
peripheral details based on the learned manifold of natural videos.
This manifold also allows to infer the spatio-temporal semantic context based on an input video stream of sparse pixels. This allows us
to achieve a significant reduction in the amount of required content
without degrading the perceived quality in the peripheral vision.
At a glance, the contributions of our work are:
• A novel neural reconstruction that can inpaint details in the
fovea and in-hallucinate temporally stable peripheral video
content.
• A universal method that supports a video content produced
by a black-box method.
• Over 14x reduction for foveated rendering without noticeable
quality degradation.
• Real-time and low-latency reconstruction performance to
drive gaze-contingent displays.
• Gaze-contingent user studies to analyze the quality and the
detectability.
• Publishing the method as a baseline in foveated compression
for the follow-up work.
In the remainder of the paper, Section 2 provides background on
human perception and the potential of foveated compression, discusses existing foveated rendering and video compression methods
and suitable quality metrics, and provides an overview of similar methods and recent generative methods in machine learning.
Section 3 discusses our initial setting, design goals, and input assumptions. Section 4 describes the main reconstruction algorithm,
network design, and training methodology we use. To evaluate the
quality, we conduct a user study described in Section 5. The results,
implementation details, and discussion are provided in Section 6.
2 PREVIOUS WORK
2.1 Background on Visual Perception
The ultimate receiver of the visual signal is the human eye. Its
physiological structure determines how that visual signal is encoded
for processing at subsequent stages of the visual system. The number
of photoreceptors in the eye rapidly decreases from the fovea to the
periphery [Curcio et al. 1990]. This fundamentally couples spatial
sampling rate to eccentricity, the angular distance from the fovea. As
sampling rate decreases with increasing eccentricity, our ability to
perceive fine and mid-level details also decreases. Despite this loss
in spatial resolution, temporal sensitivity remains roughly constant
across all eccentricities [Rovamo et al. 1984].
When a video is presented, the light is captured by the 4.6 million
cone photoreceptors [Curcio et al. 1990]. These photoreceptors
feed subsequent retinal layers that encode this data for the midget
ganglion cells which provide the pathway out of the eye. Thus,
visual acuity, the ability to perceive spatial detail at or below a
spatial frequency, is limited by the density of these midget cells.
The Contrast Sensitivity Function (CSF) [Kelly 1984; Robson 1966]
models this loss in perceptual contrast sensitivity of a stimulus as a
function of its spatiotemporal frequency. Geisler and Perry [1998]
provide a formulation of spatial frequency sensitivity in terms of
eccentricity. Figure 2 relates cone and midget cell densities to visual
acuity as a function of an angular distance from the fovea. The
reduction in cell density from fovea to periphery (0◦– 40◦
) is on
the order of 30x [Dacey and Petersen 1992], providing a guide for
reducing spatial details according to retinal physiology.
When designing a model based on this reduction, the spatiotemporal sensitivity of the eye must be carefully considered. Undersampling spatial details every frame without applying an appropriate
pre-filter leads to aliasing-induced flicker as objects traverse points
in the visual field. Neglecting spatiotemporal frequencies introduces
another source of flicker as well as “tunnel vision” phenomena. Designing a model that respects these sensitivities and avoids flicker
across the entire visual field is challenging.
2.2 Foveated and Perceptual Rendering
Delivering high quality content to each location in a head-mounted
display (HMD) is computationally expensive. To save computation,
peripheral compression becomes increasingly important for both
rendered and captured video content. However, foveated rendering
ACM Trans. Graph., Vol. 38, No. 4, Article 212. Publication date: July 2019.
DeepFovea: Neural Reconstruction for Foveated Rendering and Video Compression using Learned Statistics of Natural Videos • 212:3
Fig. 2. Dropoff in acuity (green, cycles per degree) vs degrees eccentricity [Geisler and Perry 1998]; cone cell density [Curcio et al. 1990] and
midget cell density [Bradley et al. 2014] distributions (blue).
can produce the aforementioned visual artifacts. Simply downsampling with eccentricity introduces aliasing and jitter. These phenomena encumber the design of an efficient and visually lossless
foveated rendering.
The seminal initial work on foveated rendering [Guenter et al.
2012] addresses these problems by computing three gaze-centered
concentric rings at progressively lower resolution. The resolution
falloff and the size and the placement of the rings are chosen according to the perceptual detectability threshold [Geisler and Perry 1998].
To suppress the flicker in the periphery, jittering and temporal filtering are applied. Recent notable follow-up work performs sparser
rendering in the periphery with either stochastic sampling and inpainting [Stengel et al. 2016] or using reduction of shading rate [He
et al. 2014] followed by advanced image filtering [Patney et al. 2016].
Sun et. al [Sun et al. 2017] proposed a method for both foveation
as well as accommodation for light field displays using a sparsely
and adaptively sampled light field. In contrast, our work does not
require rendered content because it takes color-only samples as input and is focused on foveation-only reconstruction. Vlachos [2015]
proposed to use a checkerboard pattern to skip 2 × 2 pixel blocks to
sparsify rendering in the periphery followed by a simple inpainting
algorithm for hole filling. Foveated ray tracing [Weier et al. 2016]
combines reprojection with temporal filtering to avoid artifacts.
We refer an interested reader to the recent survey on perceptual
rendering [Weier et al. 2017].
In contrast to most foveated rendering methods, we design a
foveated reconstruction method that does not require any knowledge about how the image was generated, such as rendering-specific
attributes, or a decomposition into visibility and shading. Instead,
our method is inspired by the compression and inference in human
visual system that is crafted to rely on natural video statistics. This
allows us to design a single method for both synthetic content as
well as regular videos and images. To avoid perceptual artifacts in
the periphery, we rely on in-hallucinating the video content based
on the learned statistics of natural videos to achieve high quality
foveated compression.
2.3 Foveated Video Compression
For decades, video compression standards have emerged from incremental changes to the hybrid video encoder model. H.264, H.265
(HEVC), VP9, and AV-1 are the most popular standards used by media platforms. Unfortunately, these standards do not apply directly
to foveated video compression. Attempts at applying various modifications to the input signal to exploit these encoder approaches
include [Lee et al. 2001; Wang et al. 2001, 2003]. The recent work in
applying convolutional neural networks (CNNs) to compression has
yielded Wave One [Rippel et al. 2018], a highly efficient compression method that exploits non-constrained latent space for better
compression.
360◦ video formats are becoming increasingly popular with the
advent of fisheye cameras and HMDs. By exploiting both eye and
head tracking, only the viewed portion of the entire scene needs to
be decoded at full resolution. A fully functioning real-time foveated
compression system has the potential to impact how 360◦ video
formats are evolving, which is an active area of discussion within
the Joint Video Exploration Team (JVET) [Ye et al. 2017].
2.4 Image and Video Error Metrics
Image and video quality metrics, such as Peak Signal-to-Noise Ratio
(PSNR), Structural Similarity Index (SSIM) [Wang et al. 2004], Video
Quality Metric (VQM) [Pinson and Wolf 2004], and Spatio-Temporal
Reduced Reference Entropic Differencing (ST-RRED) [Soundararajan and Bovik 2013] have been successfully predicting human subjective performance in the fovea [Liu et al. 2013]. Recent work on
Learned Perceptual Image Patch Similarity (LPIPS) [Zhang et al.
2018] uses the calibrated perceptual metric, which passes the reconstructed image and the target image through a VGG network [Simonyan and Zisserman 2014] pretrained on ImageNet dataset. This
recent metric was demonstrated to have an excellent perceptual
performance on various distortions. We employ this metric both as
a training loss and as one of the image metrics for ablation studies.
Only a few quality metrics were designed for foveated image quality assessment. Foveated Wavelet Image Quality Index (FWQI) [Wang
et al. 2001] computes a multiscale Discrete Wavelet Transform
(DWT) on images, weighs coefficients using a frequency and eccentricity dependent CSF, then pools the result using a ℓ2 norm.
Foveation-based content Adaptive Structural Similarity Index (FASSIM) [Rimac-Drlje et al. 2011] first weighs SSIM by a CSF that
depends on frequency, eccentricity, and retinal velocity then averages these weighted coefficients. Swafford et. al [2016] extends
HDR-VDP2 [Mantiuk et al. 2011] with the eccentricity-dependent
CSF and a cortical magnification term.
2.5 Neural Denoising, Inpainting and Reconstruction
Machine learning models have been successfully used in a wide
range of image processing tasks. The most common model design
is a convolutional neural network (CNN), which is a feedforward
network with a cascade of convolution layers [Lecun et al. 1998].
Such networks are able to efficiently analyze the image and build
a hierarchy of multiresolutional semantic features that are trainable for specific tasks. Following recent efforts towards stabilizing
training for deep networks [Krizhevsky et al. 2012], CNNs have
been able to achieve impressive results in many areas of image
processing, such as object localization [Girshick et al. 2014], image
denoising [Schmidhuber 2015], inpainting [Pathak et al. 2016] and
superresolution [Ledig et al. 2017]. We refer an interested reader to
the survey [Schmidhuber 2015]. Residual networks [He et al. 2016]
ACM Trans. Graph., Vol. 38, No. 4, Article 212. Publication date: July 2019.
212:4 • Kaplanyan, A. S. et al.
reformulate the problem of learning a function to learning a delta between the input and the output. This change allows better gradient
flow, often leading to better convergence and output quality.
Recurrent networks are often used for video processing tasks.
They maintain temporal context by conditioning the current frame
on the previous frames. This allows the model to exploit correlation
across frames. Multiple types of recurrent networks are used from
simple architectures [Chaitanya et al. 2017] to Long Short-term
Memory networks (LSTM) [Hochreiter and Schmidhuber 1997].
2.6 Learning the Manifold of Natural Images and Videos
High quality images and video follow regular natural scene statistics [Ruderman 1994]. The human visual system has adapted to
expect these statistics [Geisler 2008] and heavily relies on it when
inferring the peripheral details. As a result, learning these statistics
can enable more powerful perceptual compression methods.
Generative adversarial networks (GAN) [Goodfellow et al. 2014]
can learn complex distributions, such as a manifold of natural images
or videos, by combining a generator with a trainable adversarial
loss, implemented using another network called a discriminator.
This trainable loss has enough capacity to learn extremely highdimensional distributions of data, such as the distribution of natural
images or videos. The discriminator plays a minimax game with the
generator network by learning to distinguish between the samples
from the generator’s distribution and real data samples.
Due to the inherent unstable equilibrium of the minimax game,
the training process for adversarial networks is unstable and sensitive to hyperparameters. For example, if there is a significant
capacity imbalance between the generator and the discriminator
networks, the training can collapse with a trivial win of one network. Regularization and training improvements have improved
the training robustness and stability. The Wasserstein GAN [Arjovsky et al. 2017] redefines the adversarial training problem as a
simultaneous optimization of the generator and the Wasserstein-1
measure (also known as an earthmover’s distance) represented by a
discriminator network (also called critic). This new measure stabilizes the training by providing a smoother distance function between
target and learned probability densities. It allows the generator to
progress in training even when the discriminator has advanced
further in training, avoiding training collapse. One recent improvement to Wasserstein GAN is called a Spectral Normalization GAN
(SN-GAN) [Miyato et al. 2018] and it imposes the required Lipschitz
continuity on the Wasserstein measure, while relaxing the restrictions on the underlying discriminator network and thus allowing
for efficient training.
GANs have recently been used for large-scale single-image inpainting [Liu et al. 2018], high-resolution image generation [Karras
et al. 2018], and generation using patch classification (PatchGAN) [Li
and Wand 2016].
Recent advances in learning video manifolds with GANs demonstrate the potential of generating temporally coherent video results.
Similar to human perception, generative networks can inpaint large
portions of the video by learning high-level semantics and motion
dynamics around the missing video fragment. For example, recent
work [Wang et al. 2018] shows the feasibility of generating realistic,
stable video from segmentation masks. A nested network design
is used with background-foreground separation and an adversarial loss on optical flow. Another work [Pérez-Pellitero et al. 2018]
introduces a recurrent network design for temporally stable video
superresolution, which is trained using a variant of optical flow loss
that promotes temporally consistent movements. This is achieved
by comparing the current generated frame to a previous generated
frame after warping according to an estimated optical flow. By contrast, a video-to-video retargeting work [Bansal et al. 2018] achieves
temporally consistent results without optical flow. The method is
able to generate stable frame sequences by requiring the result video
to match the original after being mapped back and forth between
different domains.
We employ recent advances in GANs and introduce two adversarial losses to train DeepFovea reconstruction network to reconstruct
missing details in the periphery according to the learned statistics
from the manifold of natural videos.
3 PROBLEM SETTING
In rendering systems, each pixel requires a high amount of computation. To reduce this workload, we draw a tiny subset of the
total number of required pixels each frame and infer the rest with
our model. Video captured from both the real world and realistic
renders follow strong statistical regularities known as natural scene
statistics [Kundu and Evans 2015; Ruderman 1994]. The human visual system is also adapted to comprehend real-world imagery that
naturally possesses these statistics [Geisler 2008]. This provides a
great opportunity for compression by relying on the statistics that
form the manifold of all natural videos.
3.0.1 Sparse Input. To reduce the number of bits required to encode
a signal, we subsample each frame using a sparse randomized mask.
By reducing the number of samples in the mask, the compression
rate directly increases. By shaping this mask according to the cell
density layout of the retina, we can perceptually allocate bits.
For each pixel position x of a source video frame, we first compute
the sampling rate R(x) ∈ [0; 1] based on the maximum perceptible
frequency, the geometric setup of the display, and the desired compression rate. Please see supplementary material for more details.
For each video frame, our foveated sampling procedure fills an
NxM binary mask, M, according to M(x) = 1R(x)>U, where U is
a random variable bounded [0, 1], which can follow some uniform
random distribution. In the spirit of Mitchell [1991] and to better
follow the distribution of retinal cones [Cook 1986], we use a lowdiscrepancy blue noise sequence (see Figure 1), using the void and
cluster algorithm [Ulichney 1993]. Valid pixels for a frame are then
selected based on this mask, and the mask itself is provided as
an input to reconstruction. We have also tested the network with
other sampling patterns, including uniform random sampling. The
network is largely agnostic to the sampling pattern, however, the
reconstruction quality degrades.
Importantly, the mask is sampled independently at every frame,
so the network can accumulate more context over time.
ACM Trans. Graph., Vol. 38, No. 4, Article 212. Publication date: July 2019.
DeepFovea: Neural Reconstruction for Foveated Rendering and Video Compression using Learned Statistics of Natural Videos • 212:5
~ ~ ~ ~
Residual block Temporal block 3x3 conv 3x3 conv Avg pool
+
Res block 1
Res block 2
Res block 3
Res block 4
Res block 5
Temp block 1
Temp block 2
Temp block 3
Temp block 4
3x3 conv
3x3 conv
Bilinear upsample
+
Layer norm
Reconstruction network
G
Time (frames)
Fig. 3. The network design used for video reconstruction is a recurrent video encoder-decoder network architecture with skip connections (based on U-Net).
The decoder part is modified to be stateful and hierarchically retains temporal context by concatenating (denoted with ∼) recurrent connections (orange).
3.1 Reconstruction Methodology
Let X = {x1, x2, ..., xK } be a sequence of K video frames, where
X ∈ R
N ×M×K . Let M = {m1, m2, ..., mK } be a sequence of binary
masks described in the previous section. We produce a sampled
video Xˆ = {xˆ1, xˆ2, ...xˆK } by applying each mask to a corresponding
source video frame as Xˆ = X ⊙ M. The goal of the network G we
train is to learn to approximate the mapping Xˆ 7→ X by leveraging
the large prior of the natural video manifold.
Our approach to the problem of sparse reconstruction is based on
a framework of generative adversarial networks, which was recently
shown to be able to learn large high-dimensional manifolds [Karras
et al. 2018]. Note that in contrast to generative networks, the input to
our network is not a random variable. The reconstruction network
design is based on a popular U-Net encoder-decoder architecture. To
allow the network to make use of inter-frame correlations, we add
recurrent layers to the decoder part of the DeepFovea network. We
use various techniques to stabilize network output in the temporal
domain, such as optical flow and temporal regularizations. Since
the ultimate goal of this network is to learn the projection from
sampled sparse video to a manifold of natural videos, we train it on
a large set of real-life videos. We discuss details of the DeepFovea
algorithm in the subsequent section.
3.2 Design Goals
There are several goals that we would like to achieve with our
method. First, the DeepFovea network should be able to operate
in an online mode, i.e., it should be able to reconstruct the current
frame based only on the past frames. Second, since we are targeting
gaze-contingent display systems, the network should be able to
operate in real time. This prohibits using complicated models or any
significant number of past or future frames.
There are also strict requirements for output quality. The human
visual system is not sensitive to high-frequency details in the periphery, however, motion and flicker are easily detectable. Therefore,
while the peripheral reconstruction can omit fine details, it should
not introduce significant noise to achieve plausible results with high
compression. Given the uncertainty of the sparse video input, the
network needs to balance between introducing the new content
timely and suppressing flicker due to the inbound noise.
3.2.1 Causal Temporal Network with Recurrence. In order to leverage the temporal redundancy of the video and at the same time
achieve higher temporal stability of the reconstruction, we employ
a recurrent convolutional network architecture. This retained state
is then used at the next frame, allowing the network to super-resolve
the details through time (Figure 3). A common alternative approach,
early fusion, feeds a network a sliding window of L last frames,
however, it does not meet our performance requirements.
3.2.2 Performance Considerations. If the method is used for gaze
contingent reconstruction, it has to exhibit under 50ms of latency for
each frame in order to be unnoticeable for human vision [Guenter
et al. 2012]. Moreover, for head-mounted displays, the method has
to run at HMD’s native refresh rate and high resolution to avoid
motion sickness and provide a comfortable experience. For many
existing VR HMDs the minimum refresh rate is 90Hz.
4 NEURAL RECONSTRUCTION
4.1 DeepFovea Network Design: Recurrent U-Net
For the reconstruction network G of our system (Figure 3), we
chose the U-Net encoder-decoder design with skip connections [Ronneberger et al. 2015]. It transforms an image into a hierarchy and
skip connections allow to bypass high frequencies and improve the
gradient flow during training.
Each decoder block does the reverse of an encoder block, performs
a spatial bilinear upsampling, while decreasing the feature count
correspondingly to the symmetric encoder block. The input to a
decoder block is the upscaled output of the previous decoder block
concatenated with the output of the corresponding encoder block
(skip connection, dashed arrows in Figure 3).
We use ELU activation function [Clevert et al. 2016] in all networks and layers (including recurrent and discriminator layers) to
accelerate the training.
4.1.1 Recurrence. In order to generate temporally stable video content, the network needs to accumulate state through time. Moreover,
a temporal network is able to super-resolve features through time
and can work with sparser input while achieving the same quality.
However, our network has to be causal (i.e., cannot see the future
video stream) and should have a compact state to retain over time
due to high video resolution and performance constraints. Complex
recurrent layers like LSTM [Hochreiter and Schmidhuber 1997] have
a large state and are computationally demanding. Therefore, in the
spirit of Chaitanya et. al [2017], we employ a recurrent modification
of the U-Net design with a a simple convolutional recurrent layer.
A hidden state h in this layer is an output from the previous time
step, i.e., for ith decoder block oi = hi = f (x, hi−1) (orange arrows
in Figure 3). ELU activation gives more freedom to the recurrent
ACM Trans. Graph., Vol. 38, No. 4, Article 212. Publication date: July 2019.
212:6 • Kaplanyan, A. S. et al.
Spatiotemporal 3D discriminator Fourier-domain
3D discriminator
LPIPS loss (VGG-19) Optical flow loss
D1
D1
3D FFT
3D res. block 1
3D res. block 2
3D res. block 3
3D res.
block 4
3D res.
 block 5
VGG block 1
VGG block 2
VGG block 3
VGG block 4
VGG
block 5
Whole 3D video 3D Fourier spectrum One 2D video frame
Generated frames
Reference
optical flow
Warped frames
Warp
Warp
x0 x1 x2
W(x1) W(x2)
L1
L1
Fig. 4. Different losses we use to train the DeepFovea network to better learn video statistics and reconstruct plausible and temporally consistent videos.
layer compared to bounded activations (such as sigmoid), however,
it potentially allows to have a recurrent filter with an unbounded
positive feedback. Therefore, extra caution needs to be taken when
training these recurrent layers to be stable on very long videos. We
apply additional regularizations to recurrent connections to make
the network trainable, as described in Section 4.3.
Recurrent blocks are able to handle sudden temporal changes
of pixel density from gaze movements. It is sufficient to train the
network with constant-density videos, while stochastically sampling
the density for each video in the minibatch. In high-density regions,
the temporal hidden state stores high details. When the gaze leaves
the region, the recurrent blocks smoothly degrade the high-detailed
information over time in this region and simultaneously update
the temporal representation to be consistent with the sparse inflow
of pixels. The fovea reconstruction is always high quality and not
affected by the change of the density, because all valid pixels from
the input frame bypass the reconstruction and are sent directly to
the output frame. This bypass also forces the network to learn to
eliminate any temporal lag in the reconstruction during dynamic
gaze conditions, e.g., at the end of a saccade.
Convolutional recurrent blocks cannot efficiently move the content laterally to large image-space distances, because they are limited by the receptive field of their kernels. We reproject the hidden
state to compensate for the large-scale lateral motion to assist the
network with the head rotation in the HMD setup. We analytically
calculate an optical flow for each recurrent connection using two
view matrices for the last and the current frames to perform the
hidden states re-projection. We treat each hidden state as a texture
with multiple channels. We take each texel’s 2D coordinates in hidden state and project them back to the camera rotated view space.
To do so we calculate the product of inverted view (rotation only)
and projection matrices used to render the current frame assuming
the content is at infinity. Then we project coordinates back to the
screen space using view and projection matrices used to render the
last frame. Now having two 2D texel coordinates, last and current,
we copy texel’s data from the last to the current location. If the last
location is outside of the hidden state 2D bounds then the current
texel value is preserved.
Our choice of recurrent design, while being dictated by performance considerations, leads to a lightweight network that is able
to efficiently accumulate and retain temporal context from a sparse
video at multiple scales.
4.2 Losses
We optimize the generator network G with respect to a weighted
sum of three losses (see Figure 4), namely, adversarial loss, perceptual spatial loss (LPIPS), and optical flow loss for temporal dynamics:
LG = wadv · Ladv + wLPIPS · LLPIPS + wflow · Lflow.
4.2.1 Adversarial loss. Adversarial loss is modeled by a discriminator network. The discriminator allows to learn the spatiotemporal
manifold of natural videos by providing a boundary between a distribution of interest and the rest of possible videos. The discriminator
- in contrast to the generator - processes the entire video sequence
at once and can therefore reason about space-time relations and
analyze the spatiotemporal dynamics. The goal of the discriminator
is to classify videos into fake (constructed by the generator) and
real (sampled from the dataset).
We use a Wasserstein GAN (WGAN) design [Arjovsky et al. 2017],
which stabilizes the training due to its robust loss function. We use
a 3D convolutional network D1 as a Wasserstein measure (see Figure 4) with recent Spectral Normalization GAN (SN-GAN) [Miyato
et al. 2018] to ensure 1-Lipschitz continuity. SN-GAN enables fast
training on videos, while providing more stable adversarial training.
The network D1 has a 3D funnel structure and consists of residual
blocks [He et al. 2016] with decreasing spatial size. The network
operates on the whole video as an input. In order to enable full
analysis of spatiotemporal features, we employ 3D convolutional
layers with 3 × 3 × 3 spatiotemporal kernels. Each block contains
two 3D convolutions, followed by a 3D average pooling operation
that averages both spatial dimensions and the temporal one. We use
ELU as activation functions to allow the discriminator to recover
from sparsity, which reduces chances of training collapse. To focus
the network on fine details, instead of reducing the video to a single
scalar value, we follow a PatchGAN loss [Isola et al. 2017] and
require the network to classify local patches of generated videos.
4.2.2 Spectral normalization. An inherent assumption of WGAN
design is that the discriminator should be 1-Lipschitz continuous,
i.e., ∀x1, x2 : |f (x1)−f (x2)| ≤ |x1−x2 |. Standard networks generally
violate this constraint. There are several approaches to ensure 1-
Lipschitz continuity. We use recent Spectral Normalization in the
discriminator [Miyato et al. 2018] that bounds the matrix spectrum
of each layer’s weights. This approach allows for fast training, which
is crucial for training video networks, while leading to comparable
results with other state-of-the-art methods.
ACM Trans. Graph., Vol. 38, No. 4, Article 212. Publication date: July 2019.
DeepFovea: Neural Reconstruction for Foveated Rendering and Video Compression using Learned Statistics of Natural Videos • 212:7
4.2.3 Fourier-domain Discriminator. It is well known that the natural images have a characteristic statistics of a vanishing Fourier
spectrum. Natural videos also obey a similar natural spectral statistics [Kundu and Evans 2015]. Choi and Bovik [2018] introduce
flicker detection in 3D Fourier domain. In the same spirit, to help
the discriminator to learn the intricate relations between spatial
features and their natural motions, we introduce the second network
in the adversarial loss that learns the manifold of the spatiotemporal
spectra of natural videos. For that, we first Fourier-transform the
whole input video into its 3D spectrum. Then we use another discriminator network with the same design as D1 to learn the spectral
manifold of natural videos. Since there are no image patches anymore, we append two fully connected layers with 256 and 1 unit
correspondingly, with one ELU activation in between. This helps
to learn the structure of spatiotemporal frequencies that occur in
natural videos. Particularly, this loss helps detecting unnatural noise
and flicker.
4.2.4 Perceptual Spatial Loss. To promote similarity of each reconstructed frame to its source frame, some measure of similarity is
needed. Per-pixel L1 loss is too low-level and prescriptive.
Instead, we use the calibrated perceptual loss (LPIPS) [Zhang et al.
2018]. By minimizing LPIPS, our network learns to endow each
reconstructed frame of the video with natural image statistics. This
also bootstraps the adversarial training, while providing enough
freedom to the reconstruction. A pretrained VGG-19 consists of five
blocks, each of which corresponds to a different level of abstraction
of the initial image. We take outputs of the conv2 layer from each
block to use as feature extractors:
LLPIPS(x1, x2) =
Õ
5
i=1
∥convi,2(x1) − convi,2(x2)∥1
Unfortunately, this loss improves only spatial (intra-frame) features,
while providing no temporal relation between frames. For peripheral
video quality, it is more important to enforce temporal coherency.
To make it cooperate with spatiotemporal losses and encourage
the gradient flow through recurrent connections, we exponentially
downweigh this loss for the first eight frames of the video. This loss
corresponds well with human perception [Zhang et al. 2018] and
gives enough freedom to the network.
4.2.5 Optical flow loss. We use optical flow loss to stimulate temporal consistency across frames and disentangle the spatio-temporal
correlation of video frames. There are multiple ways to employ the
optical flow in video generation. One is to estimate the optical flow
directly in the generator and require the generator to match the
target optical flow, as well as match the ground truth picture with
the warped image [Wang et al. 2018]. However, this adds complexity to the network and does not meet our inference performance
constraints. Our methodology here is inspired by the recent work
on video super-resolution [Pérez-Pellitero et al. 2018], where the
optical flow is applied only during training by requiring the network
to match reconstructed frame with previous reconstructed frame,
warped by the known optical flow W as Lflow(xˆi
, xˆi−1,W(i−1)→i
) =
∥xˆi −W(i−1)→i
(xˆi−1)∥1. Here, W(i−1)→i
(·) is the warping operator
that applies optical flow to reproject pixels of the frame i − 1 to the
frame i.
This indirect approach encourages the network to retain consistent content and smooth movements over time, while not prescribing any particular spatial content.
4.3 Training Details
4.3.1 Network Parameters. There are five encoder blocks in our network. Each consecutive encoder block downscales the input spatial
dimensions twice and increases the feature count (Figure 3). An encoder block consists of two 3x3 convolutions with ELU activations.
The second convolution layer is followed by an average pooling
layer. Both convolution layers in a block have the same number
of filters (32-64-128-128-128 for each block, correspondingly). The
bottleneck block processes the output of the last encoder layer with
a low spatial resolution and operates on high-level image semantics.
It is identical to the last encoding block, except that it upsamples
the input and has no skip connection.
Each decoder block consists of a 3x3 convolutional layer with a
recurrence (see next paragraph), followed by the second spatial 3x3
convolution layer, and a bilinear upsampling layer. Each layer is
followed by an ELU activation. The output of the recurrent layer
undergoes a layer normalization before activation. Decoder blocks
have the same number of convolution filters as the corresponding
encoder blocks (128-128-128-64-32). Symmetric padding is used
everywhere to prevent boundary artifacts on the image border.
4.3.2 Video Dataset. We train on videos sampled from a video
dataset [Abu-El-Haija et al. 2016] that contains a variety of natural
content such as people, animals, nature, text, etc. Each video has
resolution up to 640x480 and up to 150 frames. For each video, we
precompute the optical flow using the FlowNet2 network [Ilg et al.
2017]. Next, the video is downsized to 128x128 to meet GPU memory
restrictions. Lastly, the videos are sliced into 32-frame-long chunks
with an overlap of 8 frames. The total number of video sequences
in the training set is about 350,000. During training, each 32-frames
video segment is corrupted with a stochastic binary mask, which
is generated with the same method as for the final reconstruction,
and the location on the retina is randomly sampled.
4.3.3 Training Hyperparameters. Training follows a standard adversarial approach of interleaving updates to generator and discriminator, with one update for each network. The network G starts to train
with 10% valid pixels in each frame, and this percentage is gradually
decreased to 1% during the first two epochs. It allows the network
to learn quicker in the beginning of the training process. We weigh
the losses as wadv = 1,wLPIPS = 100,wflow = 20 to roughly equalize
their magnitudes. We use ADAM optimizer [Kingma and Ba 2014]
with β1 = 0, β2 = 0.95 for 30 epochs and learning rate 3e-4. For
training a video network, we implemented a parallel distributed
training. Training for 30 epochs takes 48 hours on 7 NVIDIA DGX-1
nodes with 8 GPUs each. Batch size is chosen to be 56, corresponding
to one video per GPU.
4.3.4 Stabilizing a Recurrent Video Network. We found that the
network is unstable on long videos during the testing phase by
collapsing into a constant color video. This collapse occurs from the
unbounded positive feedback loop within the recurrent connections.
We use several techniques to stabilize the recurrent connections.
ACM Trans. Graph., Vol. 38, No. 4, Article 212. Publication date: July 2019.
212:8 • Kaplanyan, A. S. et al.
       







	





Fig. 5. Ablation results on the recurrent blocks.
First, we apply layer normalization [Ba et al. 2016] to recurrent
layers, which helps keep the activations bounded. Second, we train
RNNs in a stateful manner, i.e., the hidden state is retained from
one minibatch to another. This allows each recurrent layer to start
with a reasonable content in hidden activations that lies within the
current working range during training, which helps the network to
remain stable during very long videos at inference time. At the very
beginning of the training, we initialize the hidden state using zeromean Gaussian noise with σ = 0.02. These improvements help the
stability of the recurrent design by preventing activation explosion
during inference.
4.4 Ablation study
To validate the design choices made for the network, we conducted
an ablation study. We analyze the network capacity, depth, as well as
the contribution of that loss to the final result. We use FWQI metric
to determine the spatial quality of reconstruction. Unfortunately,
since FWQI detects only artifacts of spatial reconstruction in a single
frame, it is not helpful to measure the temporal artifacts in peripheral
vision, such as flicker, which is of utmost importance for peripheral
reconstruction quality. To the best of our knowledge, there is no
peripheral spatiotemporal video quality metric, therefore, in order to
assess temporal reconstruction quality we provided sample videos
in the supplemental material.
4.4.1 Network depth. Our experiments show that the network benefits from increasing the number of UNet blocks. The FWQI value
first increases sharply from 1 to 3, and then plateaus from 3 to 5
blocks. All networks have similar number of parameters (around
3M). One explanation is due to the sparse nature of the input, the
network benefits from the increase in receptive field. We use 5 levels
in the final design.
4.4.2 Network capacity. The number of filters follows a pattern of
doubling every layer with a cap of 128 filters, so we provide only
the number of filters in the first layer. Figure 6 shows that FWQI
increases when increasing the filters from 8 to 16. The metric keeps
a steady increase for values of 24 and 32, while plateauing at 48
features. In order to constrain the network’s inference performance,
we choose 32 as the final setting.
       


	
	
	
	
		
	




	



	
Fig. 6. Ablation results on network capacity. FWQI metric shows the difference in spatial quality after 30 epochs. Different capacity is defined by a
number of filters in the first layer and a doubling every block.
4.4.3 Recurrent blocks. We show that recurrent blocks are essential
for the reconstruction with the sparse input. As shown in Figure 5,
recurrent design significantly outperforms the non-recurrent one as
measured by FWQI. We also demonstrate this result in the supplementary video for subjective quality assessment. The non-recurrent
network works on a single sparse frame and is not able to accumulate additional details from previous frames, therefore, introducing
a significant amount of temporal noise even on high levels of input
density.
4.4.4 Losses. To validate that each of our losses improves the reconstruction, we compared multiple variants of the network with
losses being enabled one after another as LLPIPS , LLPIPS + Ladv,
LLPIPS +Ladv +Lflow. Unfortunately, FWQI does not provide a meaningful comparison and can even decrease during this process. However, when observed, the video quality improves with each added
loss. The reason is because FWQI does not account for temporal
stability, which is the target of Ladv and Lflow losses. We provide
videos in the supplementary to demonstrate the improvements.
Unsurprisingly, LLPIPS allows the network to learn a only a singleframe reconstruction, leaving a substantial amount of flicker. The
adversarial loss Ladv significantly improves the temporal stability
and suppresses a large portion of flicker. Optical flow loss Lflow
provides an additional improvement and reduces temporal noise,
such as pixel crawling, especially in case of long lateral camera
movements. Please refer to the accompanying video for comparison.
5 USER STUDY
Our design of the reconstruction network and its training is motivated by the reconstruction process in human visual system that
is based on the natural video statistics. However, in order to validate our method, we conduct an extensive user study. We compare
DeepFovea to the Multiresolution [Guenter et al. 2012] foveated
rendering method, and to the baseline with Concentric H.265 compression. We use these two methods, because, unlike many foveated
rendering methods, they do not require any additional attributes
(such as surface normals, or semantics of the geometry). While
ACM Trans. Graph., Vol. 38, No. 4, Article 212. Publication date: J