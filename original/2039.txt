Augmented reality (AR) for smartphones has matured from a technology
for earlier adopters, available only on select high-end phones, to one that
is truly available to the general public. One of the key breakthroughs has
been in low-compute methods for six degree of freedom (6DoF) tracking on
phones using only the existing hardware (camera and inertial sensors). 6DoF
tracking is the cornerstone of smartphone AR allowing virtual content to
be precisely locked on top of the real world. However, to really give users
the impression of believable AR, one requires mobile depth. Without depth,
even simple effects such as a virtual object being correctly occluded by the
real-world is impossible. However, requiring a mobile depth sensor would
severely restrict the access to such features. In this article, we provide a novel
pipeline for mobile depth that supports a wide array of mobile phones, and
uses only the existing monocular color sensor. Through several technical
contributions, we provide the ability to compute low latency dense depth
maps using only a single CPU core of a wide range of (medium-high) mobile
phones. We demonstrate the capabilities of our approach on high-level AR
applications including real-time navigation and shopping.
CCS Concepts:• Computing methodologies→Computer vision; Epipolar geometry; 3D imaging; Mixed / augmented reality;
Additional Key Words and Phrases: depth from motion, structure from
motion, motion stereo.
1 INTRODUCTION
In recent years, the mobile industry has significantly invested in
augmented reality (AR) for smartphones. Your mobile phone can
now be a viewfinder on an augmented world, where virtual content is rendered on top of the color camera feed in real-time. Almost all emerging high-end or mid-range phones now have some
form of six degree of freedom (6DoF) tracking capability using just
the typical sensors found inside these devices – the color camera
and inertial measurement unit (IMU). This shift has required many
breakthroughs in visual inertial odometry (VIO) and simultaneous
localization and mapping (SLAM).
These advancements have led to the release of sparse 6DoF tracking platforms such as ARKit [Apple 2018] and ARCore [Google
2018] with AR applications that create the illusion that users can
place and lock virtual objects in their environments. However, the
illusion breaks as soon as visual inconsistencies appear, such as
wrong occlusions between real and virtual objects.
To achieve another level of immersion, one needs dense depth
maps on device, and in real-time. Depth is a prerequisite for more
realistic AR, including correct handling of occlusions of virtual content by real objects, better placement of virtual content, and enabling
interactions (such as physics collisions) between real and virtual
content. However, despite many breakthroughs on smartphone AR,
none of these devices can currently provide real-time dense depth
maps without adding a dedicated new sensor. While we are clearly
heading towards a future where dedicated depth sensors will become more ubiquitous, for now, adding such sensors means that a lot
of the ubiquity and appeal of smartphone AR is lost, with additional
negative impact on cost, power, and industrial design. Therefore,
leveraging monocular color cameras is the best path to scale dense
depth estimation to millions of existing devices. Although the literature on monocular depth estimation is extensive (see related
work), no method exists that is capable of providing dense and
edge-preserving depth maps at low computation on mobile phones
today.
This paper introduces a novel pipeline capable of supplying dense
and low latency QVGA depth maps at 30Hz, leveraging only a single
RGB camera and a single CPU core of a smartphone. Figure 1 illustrates how this depth map can be used to realistically render virtual
objects in real-time, enabling new effects that can enhance a wide
array of mobile applications including shopping, street navigation
and self expression.
The task at hand has several challenges. First, our approach needs
to work across a variety of different phones with different camera
sensors for which we do not have control over parameters like
exposure or focus. Second, we must deliver dense depth at low
latency and low computation to the user, even under conditions
of poor tracking or untextured environments. Third, we want to
support mid-tier mobile CPUs—e.g. single 1.9GHz Qualcomm A53
CPU core.
We solve this task through a novel depth from motion pipeline,
comprising of the following technical contributions:
• A procedure allowing the use of polar rectified images for
efficient stereo matching.
• A new keyframe selection strategy.
• A highly optimized stereo matching algorithm leveraging the
best aspects of PatchMatch Stereo [Bleyer et al. 2011] and
HashMatch [Fanello et al. 2017a].
• New extensions of the bilateral solver [Barron and Poole
2016] for depth post-processing that: (a) lead to higher quality point clouds by encouraging planar solutions, (b) a new
initialization scheme leading to faster convergence and lower
computation requirements, and (c) a novel formulation for
producing temporally stable results.
• Finally, a new late stage rendering step that provides a fluid
low-latency experience to users.
2 RELATED WORK
Mobile Depth. We focus on related work for estimating depth on
mobile phones using existing monocular sensors, as opposed to
dedicated hardware. For a more general discussion about the state
of the art on passive depth estimation, we refer the interested reader
to [Fanello et al. 2017a; Hamzah and Ibrahim 2016; Kendall et al.
2017; Scharstein and Szeliski 2002].
In the literature, passive depth estimation on mobile platforms has
almost exclusively been studied in the context of 3D reconstruction.
These types of algorithms fuse sparse depth maps over time into
an underlying volumetric representation. Notable methods that use
Truncated Signed Distance Functions (TSDF) as a representation
include [Kähler et al. 2015; Ondrúška et al. 2015; Schöps et al. 2017b].
The approach described in [Ondrúška et al. 2015] generates sparse
depth maps of size 320 × 240 at 50Hz on an iPhone GPU using an
approximation of PatchMatch Stereo [Bleyer et al. 2011] and a dense
tracking pipeline akin to DTAM [Newcombe et al. 2011] but with
IMU initialization. Their overall pipeline is distributed across the
CPU and the GPU and runs at 25Hz, but is unfortunately unable to
handle general camera motion. The pipeline of [Schöps et al. 2017b]
computes sparse 320×240 depth maps at 12Hz on the GPU of Tango
Tablets. These depth maps are then integrated in a TSDF at 8Hz.
[Kähler et al. 2015] describe a pipeline that fuses depth maps at
24Hz on an iPad Air 2, but assumes that the depth is provided by a
third party.
Unfortunately, the use of mobile GPUs for continuous mobile
usage is prohibitive, due to power consumption, thermal considerations (GPUs cause thermal issues if continually maxed), and the
need to have such GPU resources available for rendering. TSDF representations also generally require significant amounts of memory
and also need large camera motions to carve away edge-fattening
caused by stereo-matching and to fill holes. Surfels are less memory
demanding than a TSDF and are used by [Kolev et al. 2014] to fuse
depth maps generated by the technique described in [Tanskanen
et al. 2013], which takes seconds per frame.
Another approach is described in [Schöps et al. 2014] where
320 ×240 semi-dense depth maps are computed at 15Hz on a mobile
CPU, from which a collision mesh is extracted. The mesh does not
cover the whole image however, and object boundaries are not
explicitly handled. Therefore, triangles can span objects belonging
to the foreground and background of the scene.
Fusion-based methods aside, [Suwajanakorn et al. 2015] estimate
depth from defocus. Given a focal sweep captured by a moving
ACM Trans. Graph., Vol. 37, No. 6, Article 193. Publication date: November 2018.
Depth from Motion for Smartphone AR • 193:3
camera, this technique compensates for small viewpoint changes
that occur during the acquisition of the focal stack and performs
auto-calibration and depth estimation. Although a mobile phone is
used for the data capture, the inference process takes 20 minutes
per depth map.
Various deep learning-based formulations to depth estimation
have been recently investigated. [Liu et al. 2016] trained unary and
pairwise MRF potentials using CNNs to predict depth from a single
image in 120ms. A two-scale deep network was proposed in [Eigen
et al. 2014] and trained on images and their corresponding depth
maps. [Garg et al. 2016] minimize a reconstruction loss using a
warp image for single view depth estimation, but needs to linearize
their objective using Taylor expansion to backpropagate through
it. [Godard et al. 2017] take a similar approach, but instead use
bilinear sampling for a fully differentiable objective function. Although [Eigen et al. 2014] and [Godard et al. 2017] report inference
at < 35ms per frame, they both still need a high-end desktop GPU
to achieve this real-time performance.
Late stage rendering. To reduce perceived latency, late stage rendering or time-warping [Evangelakos and Mara 2016; Van Waveren
2016] can be used. It is mostly studied in the context of AR/VR
rendering where head-motion occurs after rendering and needs to
be compensated for prior to display to reduce perceived latency. For
best user experiences, this end-to-end latency [Mine and Bishop
1993] should not exceed 20ms [Zhang and Luo 2012].
Common approaches apply a rotational-only correction for asynchronous reprojection by applying a homography which transforms
the rendered viewpoint to the final display viewpoint [Hartley and
Zisserman 2003]. Fully positional-aware warping solutions require
access to perfect depth of the scene, which is feasible for rendered
content, but more difficult in the general case. In this paper, we
present a novel screen-space technique to reduce latency that does
not rely on accurate depth estimation or simplified scene assumptions.
Depth map densification. Most stereo or depth-recovery algorithms
contain a densification step in which noisy, sparse, or otherwise
incomplete depth observations are turned into a dense and smooth
depth map, often using a “reference” RGB image to encourage depth
edges to co-occur with color edges. For an extensive overview of
the literature, we refer the interested reader to [Pan et al. 2018; Park
et al. 2014; Weerasekera et al. 2018].
Variational inpainting [Oliveira et al. 2001] approaches based on
Total Variation (TV) [Shen and Chan 2002] have been shown to run
in real-time on the GPU of a tablet [Schöps et al. 2017a]. However,
these systems have significantly more computational performance
than a CPU core of a phone, while carrying the challenges associated
with mobile GPUs outlined earlier.
The fast bilateral solver [Barron and Poole 2016] is another approach to denoise and complete depth maps. It produces high-quality
results by solving a global optimization problem in “bilateral space”
as opposed to over all pixels in an image, resulting in runtimes that
are largely independent of image resolution. The bilateral solver
has been used successfully on mobile devices as shown in [Wadhwa
et al. 2018] and recently received a “hardware-friendly” extension
described in [Mazumdar et al. 2017] that allows it to more efficiently
leverage parallelization and vectorization on mobile hardware. We
build upon this work in this paper.
Although there is a significant body of work on using deep learning to inpaint color images (e.g, [Oord et al. 2016; Pathak et al. 2016],
the literature on deep depth inpainting is more recent. Given aligned
color and depth input, [Zhang and Funkhouser 2018] predict surface
normals and occlusion boundaries from the color and then solve a
linear system to inpaint the depth. Their approach runs at 1.8 seconds per frame on a Titan X GPU, making it prohibitive for mobile
use cases.
Depth map temporal filtering. To achieve a temporally coherent
depth map many works incorporate temporal consistency directly
into the stereo matching process. The works most closely related
to our approach are [Hosni et al. 2011; Richardt et al. 2010] where
a spatio-temporal filter based on the guided image filter and bilateral grid are used to smooth a cost volume. Other approaches use
spatio-temporal filters to post-process the depth map to provide a
more coherent solution [Richardt et al. 2012]. However, these approaches perform the filtering on a GPU. We opt for incorporating
temporal consistency into the bilateral solver which allows for a
computationally efficient and effective approach.
3 SYSTEM OVERVIEW
We now outline all the components of the proposed depth from
motion pipeline.
As the user navigates through their environment with a smartphone in hand, our pipeline starts by tracking 6DoF poses using the
off-the-shelf VIO platform of ARCore [Google 2018]. Note, that our
system could use any other VIO or SLAM platform at this stage.
Once the the 6DoF tracking pipeline is initialized and given the
latest available camera image (we use grayscale images for computational reasons), the first step towards computing a depth map
consists of identifying a keyframe from the past image frames that
is suitable to perform stereo matching.
Next, the relative 6DoF pose between the keyframe and the current frame is used to perform polar rectification. Stereo-rectification
is not a mandatory step, but it significantly speeds up stereo matching by reducing the correspondence search to the same horizontal
lines in both images.
A very fast CRF solver is used to compute correspondences; wrong
estimates are discarded via an efficient machine learning based
solution, leading to disparity maps that are almost free of outliers.
From disparities, one can estimate a sparse depth map through
triangulation.
The sparse depth map is then fed into a novel variant of the fastbilateral solver [Barron and Poole 2016] that generates a bilateral
grid of depth (as opposed to a depth map). The bilateral grid can be
converted on-demand into a dense, spatio-temporal smooth depth
map. We do this by slicing the grid with the most recent available
image (as opposed to the frame used to populate the bilateral grid)
which ensures that the edges of the produced depth map are aligned
with the RGB image currently displayed on the smartphone.
ACM Trans. Graph., Vol. 37, No. 6, Article 193. Publication date: November 2018.
193:4 • Valentin et al.
Live image Selected keyframe Rectified images
Keyframe
selection
Occlusion rendering Depth Latest viewfinder
image Late-stage slicing
Keyframe pool
…
Planar Bilateral Solver
Matching +
Invalidation
AR
Assets
1
4
1
1 1
Dense Depth Map
On-demand late-stage rendering
Fig. 2. Depth from Motion pipeline. Dense depth map estimation (generation of the bilateral grid) and late stage rendering using the latest viewfinder image
(conversion of the bilateral grid into a dense, spatio-temporal smooth depth map) are running on different threads, allowing to provide depth with very low
latency.
The generation of the bilateral grid and the slicing are decoupled
and run on separate threads. Depth maps can therefore be generated at high frame-rate with very low latency, efficiently making it
independent of the run-time of the CRF inference, allowing deployment of our system on mid and high-end devices without sacrificing
quality or effective framerate.
Finally, high level applications can leverage this real-time depthestimation to enable effects such as AR occlusions, as shown in
Figure 1. Figure 2 illustrates the main steps of the above-described
pipeline, which we detail in the following sections.
4 KEYFRAME SELECTION
Our approach for depth estimation is based on stereo matching
between the most recent image and a past keyframe. The choice
of the keyframe is dependent on several disparate factors, each of
which contribute to the potential matching quality of a candidate
keyframe. For instance, greater depth accuracy is gained by increasing the stereo baseline between the chosen keyframe and the present
position, but such frames are also further back in time, which can
introduce temporal inconsistencies.
Previous approaches for motion stereo often rely on keyframes
from a fixed time delay [Kim et al. 2016; Zhou et al. 2017]. This
method is robust when under constant movement, such as in a
vehicle, but usage for handheld devices results in sporadic uneven
motion. Other methods rely on feature or geometry tracking, rather
than matching correspondences to a specific frame [Karsch et al.
2016; Li et al. 2006], but these algorithms often result in sparse depth
or are unable to run at adequate framerates. [Pradeep et al. 2013]
select the optimal keyframe for stereo matching in terms of baseline
and image overlap. Finally, [Schönberger et al. 2016; Zheng et al.
2014] introduce notable techniques for pixel-wise view selection, but
these approaches are unfortunately computationally too demanding
for mobile scenarios.
In this paper, we define a soft-cost function to select the optimal
keyframe for the latest target frame using several metrics. We keep
a fixed-capacity pool of potential keyframes, with each newly captured frame being added to this pool if the 6-DoF visual-inertial
tracking is successful, replacing outdated frames. The cost metrics
we use to pick the best keyframe from the pool are:
• bi,j
: The baseline distance in 3D between two frames i and j.
We want this value to be large.
• ai,j
: The fractional overlap, in the range [0, 1], of the image
areas for the framesi and j, computed based on their viewing
frustums. We want to maximize this value.
• ei,j
: The measured error of pose-tracking statistics for the
two frames. We want to keep this value small.
We use a multidimensional cost function to select a keyframe k
to pair with the latest reference image r, producing the minimum
total cost from the keyframe pool K:
argmin
k ∈K
ωb
br,k
+ ωa(1 − ar,k
) + ωe er,k
(1)
ACM Trans. Graph., Vol. 37, No. 6, Article 193. Publication date: November 2018.
Depth from Motion for Smartphone AR • 193:5
The choice of the weighting ωb
for the baseline is relative to the
nominal desired baseline for the target scene depth. Choosing candidate frames based on a known baseline is a classic technique for
motion stereo [Jain et al. 1987; Nevatia 1976], but for modern mobile
systems this metric must also be weighted against other considerations. In practice we set ωb = 0.4, with a strict limit of minimum
allowed baseline of 4 cm.
The cost term for area overlap ar,k
is weighted heavily with ωa,
since although successful matching can occur between frames with
only a partial overlap, greater overlap reduces the need of depth
values to be extrapolated to other parts of the target image. We set
ωa = 0.8, with a strict threshold of minimum allowed overlap at
40%.
If 6-DoF motion tracking–e.g. ARKit or ARCore–produces a frame
that is measured to have poor confidence, then the frame is never
added to the keyframe pool at all. However, even candidate keyframes
with high confidence may have some relative error to the latest reference frame. This relative error cost is weighted against the other
costs with ωe . This weight is set at ωe = 0.5, with a strict thresholds ensuring the measured velocity variance between the chosen
keyframe and the latest reference frame does not exceed 5×10−4 m/s,
as well as ensuring the measured acceleration bias does not exceed
0.2m/s
2
.
The above parameters were chosen by maximizing pixel-wise
depth error and minimizing the number of reported invalid pixels
across a set of test datasets. The choice of these weights improved
depth accuracy by about 10%. Now that a keyframe has been identified, we next perform stereo rectification.
5 STEREO RECTIFICATION
Given two cameras and their respective poses, one can estimate
the fundamental matrix [Hartley and Zisserman 2003] that governs
how pixels corresponding to the same 3D point are related. For two
corresponding pixels x and x
′
that come from the projection of a 3D
point X, and the fundamental matrix F , one can observe that x’ lies
on the line l
′ = Fx. The correspondence search is then constrained
to a one-dimensional problem. Once correspondences are estimated,
for instance using the matching algorithm described in Section 6,
the depth of any given pixel can be estimated through triangulation.
Due to the linear cache pre-fetch behavior of modern CPUs, it is
significantly more efficient to perform this 1-d search along horizontal lines in images that are stereo-rectified. Such images have
the property that all epipolar lines are parallel to the horizontal
axis of the image. Standard dual-camera setups with a fixed baseline have the advantage that at each frame, the captured images
are already close to be stereo-rectified. Mostly due to mechanical
imprecision, these images still need to be stereo-rectified in software. For these setups, practitioners usually resort to using planar
rectification [Faugeras et al. 2001; Loop and Zhang 1999] due to
the availability of production-level public implementations [Bradski
and Kaehler 2000].
When dealing with a single camera that is freely moving, epipoles
can be anywhere on the image plane. In particular, when the user is
moving forward with their camera, the epipole is inside the image,
which causes traditional techniques such as planar rectification to
fail. In this paper, we aim at allowing users to freely move with their
mobile phones and opt for using the polar rectification technique
described in [Pollefeys et al. 1999].
5.1 Using polar rectified images for stereo matching.
In [Pollefeys et al. 1999], the authors describe an algorithm that,
given a pair of images and their relative pose, transforms these
images such that their epipolar lines are parallel and corresponding
epipolar lines have the same vertical coordinate. Also, for computational reasons described earlier, it is generally desired that for a point
(x,y) in the left-rectified image, the correspondence lies at (x
′
,y)
in the right-rectified image with x
′ < x and τmin < x − x
′ < τmax ,
with τmin a small constant, usually 0 or 1, and τmax the maximum
disparity, 40 in our case. [Pollefeys et al. 1999] does not describe
how to obtain rectified images for which corresponding pixels lie
in a fixed disparity range. Conversely, we propose a solution that
constrains solutions to a known disparity interval which can be
efficiently exploited while estimating correspondences.
Estimating image-flip. As illustrated in Figure 3, some camera configurations can lead to rectified images that, modulo disparity, are
flipped version of each other. These configurations break the requirement of x
′ < x. These cases are detected when the dot products
between the ray through the optical centers and their respective
image center in world coordinates, and the vector that links both
optical centers, have opposite signs. In that case, one of the images
needs to be flipped.
Fig. 3. Image flip. Notice how the configuration of the input images (top)
lead to rectified images that are flipped versions of each other. Indeed, in
the bottom left image, the red circle is on the left side of the yellow circle,
where the situation is flipped in the bottom right image.
Estimating image-swap and x-shift. As mentioned earlier, efficient
stereo matching assumes that all correspondences reside in a predefined range of disparity values [τmin, τmax ], with τmin > 0. This
assumption can be violated with negative disparities, illustrated in
Figure 4, or disparities that exceed the maximum disparity assumed
by the stereo-matcher. We therefore apply a shift to bring disparities
into a valid range.
ACM Trans. Graph., Vol. 37, No. 6, Article 193. Publication date: November 2018.
193:6 • Valentin et al.
First, given the relative transform between the two frames and a
pre-defined range of depth values [Dmin,Dmax ] the system is expected to handle, we estimate [τ
L
min; τ
L
max ] and [τ
R
min; τ
R
max ]. These
intervals correspond to the disparity ranges required to make predictions if the left or right image is used as reference, respectively.
We then select the reference image and the amount of horizontal
shift required to fit in the expected disparity range [τmin, τmax ]
based on which configuration requires the smallest shift and on
the sign of the estimated disparities. In the event that the expected
range of disparities is too small to accommodate the current pair,
one can re-size the rectified images accordingly.
Fig. 4. Horizontal shift. In this configuration, we have ∆fg = 4, ∆bg = -3.
Efficient stereo-matchers assume that candidate disparities range between
0 and a pre-defined maximum disparity. By default, this assumption can be
violated when using polar-rectified images.
Improving horizontal resolution for higher quality stereo-matching
results. When operating on textured scenes, standard techniques
like PatchMatch Stereo [Bleyer et al. 2011] or HashMatch [Fanello
et al. 2017a] have a very similar sub-pixel accuracy around 0.2 pixels.
By default, the rectified images can be significantly bigger that the
original image, which is not a desirable property for computational
reasons. A naive strategy would be to re-size the rectified images
to the original resolution, which can lead to rectified images with a
width smaller than it could be, directly impacting depth-accuracy
due to the fixed sub-pixel accuracy described above. Hence, to increase depth-accuracy, we propose to re-size the rectified images
such that the size of [τmin; τmax ] matches the maximum expected
number of disparities (40 in our case), while keeping the total number of pixels constant.
Accounting for some pose uncertainty. Most imprecision in the relative transform between the two images lead to gross rectification
problems. In practice, systems like ARCore only suffer from minor
pose imprecision. When the epipoles are ‘far’ from the images, some
small pose imprecision lead to imprecision in the position of the
epipole, which in turn can lead in solving for both negative and
positive disparities. We found that setting τmin between [5, 10] as
opposed to 0 or 1 effectively combats small pose inaccuracies. When
the epipole(s) are located within the image, we preemptively invalidate a disk of pixels located around the epipole. In our system the
radius of this disk is 20 pixels.
At this stage, regardless of the trajectory that the smartphone
has followed, we have a rectified pair of images. The next section
addresses performing stereo-matching on these rectified images.
6 STEREO-MATCHING
Recent works [Fanello et al. 2017b] and [Fanello et al. 2017a] have
demonstrated high-resolution stereo-depth estimation respectively
at 500Hz and 1000Hz on a high end GPU. In the following, we briefly
describe the paradigm on which these techniques are based on.
Assuming the likelihood of solutions are well captured by a conditional probability from the exponential family
P(Y |D) =
1
Z(D)
e
−E(Y |D)
, (2)
practitioners often favor a factorization of the form
E(Y |D) =
Õ
i
ψu (yi = li) +
Õ
i
Õ
j ∈Ni
ψp (yi = li
,yj = lj) (3)
which is usually referred to as a pairwise-CRF. Here Y := {y1 . . .yn }
is the set of latent variables associated to pixels {x1 . . . xn }. Each
yi ∈ Y can take values in L, which is a subset of R and corresponding to disparities. Finally, Ni
is the set of pixels adjacent to pixel
i. Although NP-hard to solve in general [Boykov et al. 2001], this
decomposition has been widely used for numerous computer vision
tasks. The term ψu is usually referred to as the unary potential, and
in the context of depth estimation via stereo matching, measures
the likelihood that two pixels are in correspondence. The function
ψp is commonly referred to as the pairwise potential and acts as a
regularizer that encourages piece-wise smooth solutions. The components that make the minimization costly are evaluating ψu and
the number of steps that the chosen optimizer takes to converge to
good solutions. In this work, we use the unary potential described in
[Fanello et al. 2017a] for its low-computational requirements, a truncated linear pairwise potential as pairwise potential. We optimize
this cost function using a hybrid of PatchMatch [Barnes et al. 2009]
and HashMatch [Fanello et al. 2017a] that is particularly efficient
on CPU architectures, which we introduce below.
6.1 Vectorized Inference
The basic idea of CRF inference using PatchMatch [Barnes et al.
2009] is to propagate good labels to neighboring pixels, exploiting
local smoothness of solutions. Different propagation/update strategies have been explored in the literature, some of them designed for
GPU architectures [Bailer et al. 2012; Fanello et al. 2017a; Pradeep
et al. 2013].
In this section, we introduce a propagation strategy that is tailored for modern CPU instruction sets that rely on Single Instruction
Multiple Data (SIMD) instructions to achieve intra-register parallelism and improved throughput. Achieving high performance on
such architectures, mainly ARM NEON for mobile devices, requires
efficient utilization of these vector registers. In particular, loading
data from memory benefits from use of coherent loads - that is, for
a vector register with n lanes of width b bits, SIMD instruction set
ACM Trans. Graph., Vol. 37, No. 6, Article 193. Publication date: November 2018.
Depth from Motion for Smartphone AR • 193:7
architectures (ISAs) typically offer an instruction that loads n ∗ b
sequential bits, filling all lanes with one instruction. The counterpart is a diverged load, where each lane of the vector register is
inserted into the vector register one at a time. Such diverged loads
are required when the memory to be loaded is not sequential. This
behavior poses a challenge to stereo algorithms, which typically
samples diverged offsets per pixel when exploring the solution space.
To make matters worse, this data parallelism is inherently directional. For typical image layouts in memory, a vector register of
pixels represents a subset of a particular image row, which prevents
typical inference schemes (e.g. PatchMatch) from mapping well to
SIMD architectures. The propagation of information horizontally in
the image prevents efficient utilization of vector registers.
Recently, [Fanello et al. 2017a] has demonstrated an inference
technique that allows updates to each pixel in parallel, allowing
depth map prediction at 1000Hz on GPU. Compared to PatchMatch,
HashMatch requires more iterations for information to propagate
further in the image, but each iteration is substantially cheaper and
can be performed independently. PatchMatch, by contrast, is completely sequential in nature: it iteratively goes from one pixel to the
next, evaluates some particle/solutions for that pixel, and continues
until reaching the end of the image. Motivated by the strengths of
each approach, we propose a hybrid variant that is well-suited for
SIMD architectures. Instead of performing multiple independent
propagation passes for each of the eight directions, we perform k
passes in sequence, each designed to utilize the data-parallelism of
the underlying vector architecture. For typical scenes, k ranging
from 2 to 4 is sufficient. During even-numbered passes, each pixel
considers hypotheses from the three neighbors above it (that is,
the pixel at (x,y) considers hypotheses from (x − 1,y − 1), (x,y −
1), (x + 1,y − 1)) in addition to the currently stored hypothesis. During odd-numbered passes, each pixel considers hypotheses from
the three neighbors below it in addition to the currently stored
hypothesis. Rows are processed sequentially, starting at the top of
the image in even-numbered passes and the bottom of the image in
odd-numbered passes. Consequently, all pixels for a given row are
independent of all other pixels in the same row, allowing parallel
processing. Is is important to note that NEON acceleration in the
inference step is possible due to breaking of dependency chains in
the X direction (rows). The reason for this is that vector units of
SIMD processors load data in chunks along the X direction, and
hence they are more efficient at vectorizing operations on pixels at
discrete Y values. As is standard for such inference strategies, we
evaluate each hypothesis by summing the stereo matching unary
cost and a weighted smoothness term, the aforementioned truncated linear pairwise potential. The unary cost evaluation cannot
be fully parallelized due to the distinct disparity value in each lane
of the vector register. However, we can fully parallelize the rest of
the data movement: the load of initial disparity values, the load of
neighboring disparity values, and the smoothness cost computation.
Overall, the proposed approach has been consistently measured as
4x faster than HashMatch and 10x faster than PatchMatch.
Fig. 5. The proposed factorization makes any pixel belonging to a given
horizontal line (e.g. green pixels) to be independent to each other. If we now
focus on pixel p, one can update it’s prediction by evaluating the particle
currently at p, and the particles in the neighbors of p (black link); the
updated solution is the particle that minimizes this local CRF. Best viewed
in color.
6.2 Invalidation
The approximate MAP inference performed over the pairwise conditional random field yields one disparity value estimated for each
pixel in the image. Unfortunately, when the scene lacks texture (e.g.
white wall in Figure 7, second line) or contains repetitive patterns,
the MAP solution of the corresponding pixels can be wrong. It is usually preferred to invalidate such pixels since the distribution of their
errors depends on image content, and hence estimating the MLE of
these distributions is non-trivial (e.g. to be used in KinectFusion-like
filtering). Invalidation is usually performed using thresholding on
the unary cost of the solution [Fanello et al. 2017b; Mühlmann et al.
2002], using left-right consistency check [Mühlmann et al. 2002;
Scharstein and Szeliski 2002], using connected component analysis
[Fanello et al. 2017b], or a combination of the above. Performing a
left-right consistency check leads to good invalidation results, but
involves computing a disparity map for each image, which adds a
significant computational cost to the pipeline.
(a) (b) (c)
Fig. 6. CRF-cost invalidation in disparity space. (a): Rectified image. (b):
Raw output from the matcher. (c): CRF-cost invalidation.
By breaking the whole CRF in cliques containing each pixel and
their immediate neighbours, one can compute the negative loglikelihood Li using:
Li = ψu (li) +
Õ
j ∈Ni
ψp (li
,lj) (4)
ACM Trans. Graph., Vol. 37, No. 6, Article 193. Publication date: November 2018.
193:8 • Valentin et al.
(a) (b) (c)
Fig. 7. Connected component invalidation in disparity space. (a): Rectified
image. (b): CRF-cost invalidation. (c): Connected component invalidation.
This formulation leads to slightly better invalidation results than
only considering the unary potential. As can be observed in Figure 6,
pruning unlikely solutions removes a large portion of undesirable
pixels. Unfortunately, in the case of untextured regions, the likelihood of solutions can be high yet incorrect. Following prior art
[Fanello et al. 2017b], we invalidate small connected components
in disparity space. The resulting depth map is free from the vast
majority of unstable predictions, as can be observed in Figure 7.
Depending on the compute architecture, running this last invalidation step can become as expensive as solving the CRF described in
Equation 3. We approximate the connected component invalidation
step using a single decision tree to minimize the computational
resources required [Criminisi et al. 2012]. In particular, we model
this invalidation step as a classification problem, where we assign to
valid pixels a positive label y = 1 and for invalid pixel a label y = −1.
In the context of predicting the confidence of time of flight, the
authors of [Reynolds et al. 2011] also propose to use Random Forest.
Unfortunately, their approach runs in 5s on 200 × 200 frames. For
the sake of completeness, we briefly describe the training procedure
for decision trees. A decision tree consists of split nodes and leaves.
Each split node n stores a ‘weak learner’ that is parameterized by
function parameters θn and a scalar threshold τn. To perform inference over the tree for pixel p, one starts at the root of the tree and
evaluates:
s(p,n) = 1[f (p, θn) > τn], (5)
Ifs(p,n) evaluates to 0, the inference continues over the left children
of node n, and over the right children otherwise. This process repeats
until reaching a leaf, which contains a binary probability distribution
over the prediction space, invalidation in this case.
As it is common practice, we chose f to be a dot product between
the values of two pixel indices located around p. The values of θn
and τn are greedily optimized to maximize the information gain:
IG(θn, τn) = E(S) − Õ
c ∈L,R
|Sc |
|S| E(Sd
), (6)
with E the Shannon entropy, L, R the left and right children of node
n. Finally, each leaf stores the probability for a pixel to be valid or
invalid.
At test time, inference over that tree is performed for each pixel,
allowing to decide which pixels should be invalidated.
6.3 Disparity to depth
Now that we have an outlier-free disparity map, we can finally
infer depth. When one uses planar rectification, given a disparity
d, a baseline b, and a focal length f , the depth Z can be trivially
computed as Z =
b f
d
. However, this closed form solution cannot be
used in the case of polar rectification.
A well known solution in literature to deal with these cases is the
optimal triangulation method proposed in paragraph 12.2 of [Hartley and Sturm 1997]. Optimal triangulation requires minimizing a
polynomial of degree 6, which could be inefficient on mobile architectures. Therefore we resort to solving the simpler linear problem
described in [Hartley and Zisserman 2003], which is not optimal
but fast to solve.
7 BILATERAL SOLVER EXTENSIONS
The previous sections describe how to obtain depth maps with few
false positives. However, these depth maps are sparse (containing
information only in textured regions), temporally inconsistent, and
are not aligned with the edges in the image. In this section we use a
novel extension of the bilateral solver to efficiently generate dense,
temporally stable, and edge aligned depth maps with low latency.
Before modifying the bilateral solver, we first review it as it has
been previously described in the literature. We build upon the “hardware friendly” variant of the solver as presented in [Mazumdar et al.
2017], which is built upon the original bilateral solver as described in
[Barron and Poole 2016], which itself builds upon the optimization
approach of [Barron et al. 2015].
The bilateral solver is defined as an optimization problem with
respect to a “reference” image r (in our case, a grayscale image from
the camera), a “target” image t of noisy observed values (in our case,
a noisy depth map as computed in Section 6), and a “confidence”
image c (in our case, the inverse of the invalidation mask as defined
in Section 6.2). The solver recovers an “output” image x that is close
to the target where the confidence is large while being maximally
smooth with respect to the edges in the reference image, by solving
the following optimization problem.
minimize
x
λ
2
Õ
i,j
Wˆ
i,j

xi − xj
2
+
Õ
i
ci(xi − ti)
2
(7)
The first term encourages that all pairs of pixels (i, j) to be smooth
according to the bilateral affinity matrix Wˆ
i,j and the smoothness
parameter λ (set to 4 in our experiments), while the second term
encourages each xi to be close to each ti when ci
is large. The
Wˆ matrix is a bistochastic version of a bilateral affinity matrix W,
where each Wi,j
is defined as:
Wi,j = exp
©
­
­
«
−

p
x
i
− p
x
j
2
+

p
y
i
− p
y
j
2
2σ
2
xy
−

ri − rj
2
2σ
2
r
ª
®
®
¬
(8)
ACM Trans. Graph., Vol. 37, No. 6, Article 193. Publication date: November 201