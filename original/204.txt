We report two studies that used facial features to automatically detect mind wandering, a ubiquitous phenomenon whereby attention drifts from the current task to unrelated thoughts. In a laboratory study, university students (N=152) read a scientific text, whereas in a classroom study high school students (N=135) learned biology from an intelligent tutoring system. Mind wandering was measured using validated self-report methods. In the lab, we recorded face videos and analyzed these at six levels of granularity: (1) upper-body movement; (2) head pose; (3) facial textures; (4) facial action units (AUs); (5) co-occurring AUs; and (6) temporal dynamics of AUs. Due to privacy constraints, videos were not recorded in the classroom. Instead, we extracted head pose, AUs, and AU co-occurrences in real-time. Machine learning models, consisting of support vector machines (SVM) and deep neural networks, achieved F1 scores of .478 and .414 (25.4 and 20.9 percent above-chance improvements, both with SVMs) for detecting mind wandering in the lab and classroom, respectively. The lab-based detectors achieved 8.4 percent improvement over the previous state-of-the-art; no comparison is available for classroom detectors. We discuss how the detectors can integrate into intelligent interfaces to increase engagement and learning by responding to wandering minds.

SECTION 1Introduction
Most of us can recall a time when we realized our attention had drifted away from thinking about what we were trying to do towards something completely unrelated. For example, we might be reading a book or news article and suddenly realize that we have no idea what we were reading. Or we might find ourselves attending a lecture but have no recollection of what the speaker just said. Such lapses in attention, known as mind wandering [1], are ubiquitous experiences. For example, one large-scale study that used experience sampling to track mind wandering of 5,000 people in 86 countries found that it occurred 46.9 percent of the time during day-to-day life [2]. Mind wandering is not merely incidental; recent meta-analyses have confirmed that it is negatively related to performance across a variety of tasks [3], [4]. Here, our goal is to develop automated methods to detect mind wandering to support a variety of applications aimed at improving task performance.

1.1 What Is Mind Wandering?
At its core, mind wandering is an attentional shift away from the processing of task-related information to the processing of task-irrelevant thoughts or ideas [1], [5], [6], [7], [8], [9], [10], [11], [12]. By task-related we mean thoughts that support the primary task objective. For example, during reading, inferences or memory retrievals that go beyond the textual content would not be considered mind wandering as long as they are related to the content, whereas reflecting on how boring the text is would. These shifts in the locus of attention usually occur without intention or even awareness [1], [9] but people can also intentionally go off task [13]. Mind wandering is related to, but not the same as, boredom [14] and aligns with the attentional subcomponent of the cognitive component of tripartite (affective, cognitive, and behavioral [15]) models of engagement [16].

There are multiple hypotheses regarding the cognitive mechanisms underlying mind wandering (reviewed in [17]). According to the executive-resource hypothesis [10], when a task does not sufficiently consume all of one's attentional resources, unused resources are directed to task-unrelated thoughts, leading the mind to wander. In contrast, the control-failure hypothesis posits that mind wandering occurs when executive control fails to suppress task-unrelated thoughts [11], [18]. Despite these differences, the basic idea is that both task-related and task-unrelated thoughts compete for consciousness, a limited resource, and mind wandering occurs when task-unrelated thoughts win the competition of consciousness [19].

There are many antecedents of mind wandering (e.g., current concerns and prospective thoughts, aspects of the task stimulus, environmental distractions, introspection, semantic and autobiographical memory retrievals; see [20], [21]). It is also more likely to occur when a person is in a negative mood [22], [23] and among those diagnosed with dysphoria (depression) [24] or attention-deficit/hyperactivity disorder [25]. Importantly, in semantically-rich tasks contexts, like reading, the stimulus itself is often a source of mind wandering due to the automaticity of memory associations (see [21]).

1.2 Current Study
We explore video-based detection of mind wandering as a step towards intelligent technologies that sense and respond to users’ mental states. We focus on mind wandering detection during learning with technology, due its high incidence and negative consequences in this context. In particular, mind wandering is frequent during routine learning activities like computerized reading and video lecture viewing [10], [26], occurring between 20 to 40 percent of the time [4]. And although mind wandering does have some benefits [27], such as the association between trait day dreaming and creative problem solving [28], mind wandering during learning is consistently negatively related to learning outcomes (e.g., [5], [26] and recent meta-analysis in [4]).

There is considerable potential for intelligent learning environments (e.g., intelligent tutoring systems, e-textbooks, massive open online courses or MOOCs) to improve engagement and learning by automatically detecting and adapting the learning environment when minds wander [29], [30], [31], [32]. For example, the technology might ask the student to take a short quiz when mind wandering is detected [33], encourage re-reading [34], change topics, or even suggest taking a break. Alternatively, instructors and instructional designers might be given feedback about incidence of students’ mind wandering to identify course materials that could be made more engaging. Such strategies necessitate methods for automatic mind wandering detection, which is the focus of this paper.

1.3 Related Work
There has been considerable work on automated engagement detection in general [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45] including research in the context of learning environments (see recent review [16]). However, these previous studies are different from mind wandering detection in that they either conceptualize engagement as a holistic construct (e.g., [35], [43], [45]), or focus on different aspects of engagement, such as behavioral engagement (e.g., going off-task) [38], [40] or affective states such as interest [46], curiosity [47], or boredom [48], [49]. In contrast, mind wandering is most closely related to attentional disengagement, which is related to boredom [14]. Further, whereas most previous work focuses on overt appearances of disengagement [35], [37], mind wandering reflects a covert state of inattention [31], making it particularly challenging to detect.

To illustrate, the left column in Fig. 1 depicts examples of facial expressions preceding mind wandering reports (i.e., when people catch themselves mind wandering – see Section 2.1) whereas the right columns depicts cases where people did not report mind wandering. The person on the bottom-left has her eyes closed and subsequently reported mind wandering, whereas the person on the bottom-right appears to be bored or uninterested due to a prolonged yawn but did not report mind wandering. Consider the top row – here the people may appear to be engaged in both cases (subjectively speaking), but reported mind wandering for only the example on the left. Similarly, the middle row depicts two people who appear to be intently focused with eyes directed toward the screen. Here, the person on the left reported mind wandering while the one on the right did not. As these examples illustrate, facial indicators of mind wandering are nonobvious (to the extent that we can rely on self-reports as discussed in Section 2.1). Thus, research has mainly focused on alternate modalities for mind wandering detection, as reviewed below.


Fig. 1.
Examples of facial expressions for positive (left column) and negative mind wandering (right column) cases.

Show All

Before proceeding, it is useful to point out that in all the studies reviewed below, and indeed in almost all of research in this area, mind wandering is measured via self-reports, either using thought probes (e.g., “were you zoned out [or attending] at the time of the probe?”) or relying on self-caught instances of mind wandering (e.g., “press the Z key every time you catch yourself zoning out”) – see Section 2.1 for a methodological discussion.

Mind Wandering Detection from Eye Gaze. In a pioneering reading study, Smilek et al. [50] found that participants blinked more frequently and fixated (focused their eyes on one spot) less frequently while mind wandering compared to normal reading. Thus, tracking the location of eye gaze, ostensibly in tandem with the words being read, should be diagnostic of mind wandering [51]. Consequently, machine learning methods applied to eye gaze data have proven effective for automatic mind wandering detection, achieving above-chance accuracies ranging from 28 to 45 percent [51], [52], [53], [54], [55] when validated in a person-independent manner. Unfortunately, these studies were conducted in laboratory contexts and utilized high-quality gaze trackers that cost tens of thousands of dollars, raising substantial scalability concerns.

Hutt et al. [56] addressed this problem by using consumer-off-the-shelf (COTS) gaze tracking in real-world classrooms, achieving 46 percent above-chance improvements. However, they used a $99 USD1 eye tracker called the EyeTribe which is no longer available after the company was acquired. A similarly priced gaze tracker, the Tobii 4C, requires an additional license ($2000 USD when we last enquired) for research usage. Furthermore, most schools have very limited budgets, and even purchasing such relatively inexpensive hardware is untenable at a large scale.

Mind Wandering Detection from Physiology. Physiological features (e.g., heart rate, skin conductance) have also been the basis of some mind wandering detection research [57], [58]. Blanchard et al. [57] utilized the Affectiva Q wrist-mounted sensor to measure physiology during reading, achieving a 22 percent above-chance mind wandering detection accuracy. This and similar sensors are still prohibitively expensive (about $1,700 USD2) for classroom use. Current COTS alternatives (e.g., Fitbit HR, $100 USD3) typically do not include the physiological channels utilized in costly research-grade physiological devices (e.g., Empatica, Shimmer), do not provide the same fine-grained sampling rates, and might still be prohibitively expensive for classroom use at scale.

An alternative is to obtain physiological signals indirectly from video. In particular, in a recent study [58], participants watched online lectures while placing their fingers over the camera lens of a smart phone with the flash on. Heart rate was measured from changes in color due to blood pumping through the finger (photoplethysmography). They achieved a 22 percent above-chance accuracy for mind wandering detection via heart rate extracted from smart phone cameras. Though innovative, it is not clear how well their method works beyond mobile applications, or whether it would be effective outside a laboratory setting when finger placement is harder to control, and battery life is of central concern.

Mind Wandering Detection from Reading and Textual Features. Some researchers have adopted approaches to mind wandering detection based on reading activities (keypresses) alone. In one of the earliest studies, participants read a text one word at a time [59] and were classified as having mind wandered when they spent too little or too much time on difficult sections of the text, as determined by word length, number of syllables, and word familiarity. Despite achieving a 45 percent above-chance accuracy, this method is limited by the threshold-based approach and the unnaturalness of word-by-word reading.

Mills and D'Mello [60] addressed these limitations by using machine learning to detect mind wandering from reading times and textual features (e.g., number of words, text difficulty) in more naturalistic reading paradigms. They achieved a 20.7 percent above-chance accuracy with person-independent validation. Though promising in terms of scalability, an obvious limitation is that the detector cannot be applied to non-reading contexts.

Mind Wandering Detection from Facial Features. Most similar to the present research are two of our own studies on mind wandering detection from video in the lab. In the first study [61], we recorded videos of participants’ faces as they watched a narrative film for approximately 35 minutes. We extracted facial action units (AUs) with FACET, a commercial version of the Computer Expression Recognition Toolbox (CERT) [62], and body movement using a motion tracking algorithm [63]. We trained a variety of classifiers including support vector machines, logistic regression, naïve Bayes, and others to detect mind wandering from the video features. The best-performing model achieved a person-independent mind wandering F1 score of .390 – a 13 percent above-chance improvement.

In a subsequent study [64], we analyzed the generalizability of this method across task contexts. One set of participants watched a narrative film, while a separate set of participants read a scientific text. The model trained on narrative film data achieved a 25 percent above-chance accuracy and generalized to the scientific text reading task (21 percent above-chance accuracy). The model trained on scientific text data also achieved a 25 percent above-chance accuracy, and after tuning the mind wandering prediction threshold, also generalized to the narrative film watching task (22 percent above-chance accuracy).

These studies study demonstrated the potential for video-based mind wandering detection and their generalizability, but used basic facial features and achieved only low to moderate accuracy, which we improve on here.

1.4 Novelty of Current Study
Researchers have demonstrated the feasibility of automatic mind wandering detection, but with some drawbacks (see Section 1.3). To address these limitations, we propose mind wandering detection based on facial and movement features derived from video. This offers several advantages over previous work. First, cameras are almost universally present on laptops and mobile computing devices used in schools, or can be purchased quite cheaply (for under $10 USD4). Second, cameras require little to no expertise to set up and require no calibration compared to gaze trackers and some physiological sensors. Third, facial features are not strictly dependent on the task at hand and should generalize across domains. In contrast, gaze features are more dependent on the stimulus (e.g., fixation durations, scan paths, etc. are different for reading compared to scene viewing [65]), and are less likely to generalize.

We also extend previous work [61], [64] on detecting mind wandering from video in the following five ways:

Feature Engineering. Whereas previous work exclusively relied on basic descriptives (mean, standard deviation, max) of AUs and body movement, we propose a novel combination of features extracted at six levels of complexity: (1) gross body movement; (2) head pose; (3) facial texture patches; (4) individual AUs; (5) co-occurring AU pairs; and (6) temporal dynamics of AUs. We hypothesize that such a multifaceted analysis is needed since mind wandering is a visually subtle phenomenon (as illustrated in Fig. 1) and its overt behavioral cues are unknown.

Comparison and Fusion of Feature Types. We compare individual models trained using different feature to identify which feature sets capture facial cues that communicate mind wandering. Furthermore, we show that a combination of these different feature sets improves detection accuracy over previous approaches that relied on feature sets (1-2) and (4) from above.

Classifiers. We also improve on previous work by considering more complex classification algorithms, including SVMs with a range of hyperparameters, and deep neural networks with varying structures.

Feature Selection. We introduce a novel feature selection method for datasets with a large number of dimensions and non-linear feature–label relationships. Such datasets are commonly encountered in affective computing applications, where there may be many input features but few instances – thus necessitating feature selection.

Classroom Context. Finally, we also explore video-based mind wandering detection in an authentic classroom environment, where participants interacted with a computerized biology tutor. The classroom environment is especially challenging due to privacy concerns (no videos could be recorded), thereby incurring the added constraint of real-time feature extraction. Additionally, all processing had to be performed on budget hardware already available in the school classroom, which required a simplification of the feature set. Our results indicate that our approach was successful despite these challenging constraints.

SECTION 2Study 1: Self-Caught Mind Wandering Detection During Reading in the Lab
We reanalyzed video data previously reported in [64] to enable comparisons of the proposed approach with previous work. The data itself was collected as part of a larger study – see [66] for full details.

2.1 Data Collection
Participants (152 university students) read the introductory chapter of Soap Bubbles: Their Colors and the Forces that Mould Them by C.V. Boys [67]. The text is about the physical behaviors of soap bubbles, how surface tension enables bubble formation, and how chemical composition affects bubble formation. We used this text because it is likely to be unfamiliar to most participants but is written to be understandable without prior knowledge of the topic.

The text was presented on 57 screens (called pages) with about 114 words per page. Participants used the right arrow key to advance to the next page. Videos of participants’ faces were recorded with a Logitech C270 webcam ($20 USD5) at 12.5 frames per second. Of the 152 participants, 10 were removed due to video recording errors and three were removed because they did not sign a data release agreement, leaving 139 participants in the dataset.

Participants used pre-designated keys to report whenever they caught themselves zoning out – a colloquial term for mind wandering. These served as “ground-truth” labels for supervised machine learning. Zoning out was defined as: At some points during reading, you may realize that you have no idea what you just read. Not only were you not thinking about what you are actually reading, you were thinking about something else altogether. This is called “zoning out”. Participants were further instructed to distinguish between two types of zone outs – task-related interferences vs. task-unrelated thoughts – as part of a larger study. However, both these types of zone outs were grouped because they are related, and multiclass detection was infeasible given the dataset size.

We used the self-caught method here vs. the probe-caught method (Study 2) because we were interested in tracking mind wandering without task disruptions and were focused on mind wandering with meta-awareness [1] (i.e., people are consciously aware that they are mind wandering).

It is important to emphasize a few points about this method to track mind wandering. First, the method relies on self-reports because mind wandering is an inherently internal phenomenon, which requires conscious awareness for reporting [20]. At this time, there are no reliable neurophysiological or behavioral markers that can accurately substitute for the self-report methodology [20]. Second, self-reports of mind wandering have been objectively linked to a host of theoretically-grounded behavioral and physiological signals [6], [8], [57], [58], [59], [60], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], providing convergent validity for this approach. Self-reports also consistently correlate with objective outcome measures, which provides evidence for their predictive validity [3]. Finally, our reliance on self-reports to measure mind wandering is consistent with the state of the art in the psychological and neuroscience literatures [20].

2.2 Extracting Video Clips
There were a total of 2,577 mind wandering reports across 7,923 pages of text (about one report every 3 pages). On average, each participant provided 18.5 reports (SD=13.5) As shown in Fig. 2, the number of reports was quite variable across participants, which makes person-independent mind wandering detection quite challenging.


Fig. 2.
Histogram of the number of self-caught mind wandering reports made by participants.

Show All

Participants reported mind wandering an average of 16 seconds into the page. Accordingly, we extracted video clips in 10s windows leading up to each mind wandering report; these corresponded to positive instances of mind wandering. We used 10s as a compromise between having longer, potentially more informative clips, while maximizing the number of clips that could be extracted. Of the 2,577 clips, 1,339 clips overlapped across pages and were discarded because of the concern that the action of leaning forward and looking at the keyboard to find the page-turn key might have influenced facial feature tracking.

We also added a 4s buffer before the mind wandering report to ensure that clips did not capture the movements associated with the self-report key press. We chose a 4s buffer length based on a pilot study where four raters made judgments on whether the keypress was visible in 540 randomly-selected video clips with buffer lengths ranging from 0-6s. Raters were instructed to report “if there is apparent hand or eye movement at the end of the clip as participants look and reach for the MW key.” Two raters initially coded 250 clips with 0s-4s buffers. They reported apparent hand movements in 73 percent of clips with a 0s buffer (eye movements in 93 percent), down to 4 percent hand movements and 5 percent eye movements for 4s clips. We increased the buffer lengths to 5s and 6s, and obtained ratings from the same raters and two new raters, finding no further decrease in apparent hand or eye movements with longer buffers. Thus, we proceeded with a 4s buffer length.

A further 207 clips were removed because the face could not be automatically detected for at least 1 second of the clip, which was our minimum threshold for usable data. A real-time application of our methods could also discard such clips, so removing them does not harm validity. In total, there were thus 1,031 usable mind wandering clips of which 64 percent were task-unrelated mind wandering reports. These served as positive instances for the classifiers,

Negative instances were extracted from periods of time between mind wandering reports (see Fig. 3). We divided each video into 14s instances (10s window of data + 4s offset to avoid including page turn movements) and removed any instances that coincided with page turn events. We also removed any negative mind wandering instances that fell within a 30s period before each mind wandering report, because the participant might have been mind wandering but had not yet realized or reported it. The duration of mind wandering is an open question [20], but is hypothesized to not exceed 20s (see [79]); the 30s buffer was taken out of an abundance of caution.


Fig. 3.
Instance extraction scheme illustrating how we selected positive and negative instances of mind wandering. We eliminated instances that overlapped with page turn events, because body and head movements due to page turn actions are tangential to mind wandering in general. We selected negative instances of mind wandering that were at least 30 seconds before mind wandering self-reports.

Show All

We randomly selected 2,406 negative mind wandering instances from the remaining instances to obtain a 30 percent mind wandering rate, which is consistent with previous research on the incidence of mind wandering during learning, especially during reading (see meta-analysis in [4]). The dataset comprised a total of 3,437 instances (1,031 positive mind wandering).

2.3 Feature Extraction
We extracted features at five levels of granularity, ranging from a simple measure of upper-body motion to complex patterns in AU temporal dynamics.

Upper-body Movement Features. We used a validated motion silhouetting method [63], where each video frame is compared to a continuously-updated background image formed by the weighted average of the previous four frames. Gross body movement was estimated as the proportion of pixels that changed compared to the background motion silhouette (Fig. 4A). This movement estimation method also serves as an accurate proxy for pressure-sensitive posture sensors [63]. We extracted the following statistical features from the body movement time series in each 10s clip: mean, median, standard deviation, minimum, maximum, and range.


Fig. 4.
Feature extraction examples illustrating: A) upper-body motion, B) estimates of action units (AUs) and head pose provided by EmotientSDK, C) local binary patterns extracted from key areas of the face, D) Jensen-Shannon divergence measuring similarity between pairs of AU estimates, and E) counting positive and negative responses of a Gabor filter convolved across an AU estimate.

Show All

Head Pose Features. We utilized head pose features as a proxy for gaze direction, motivated by the link between eye gaze and mind wandering [79], [80]. Specifically, we extracted head yaw (looking to the side), pitch (looking up or down), and roll (tilting to the side; Fig. 4B), summarizing each with mean, median, standard deviation, minimum, maximum, and range across the 10s clips – yielding 18 head pose features in total.

Local Binary Pattern (LBP) Texture Features. We extracted texture patch features with local binary patterns [81], which have been shown to be effective for engagement classification [35], [37]. LBP features capture texture patterns, which are indicative of changes in facial expressions changes. For example, texture patches near the mouth change during smiles as wrinkles appear on the skin, lips widen, and teeth become visible.

LBP features were computed following the uniform, rotation invariant method [81]. Features were computed for individual pixels in a patch (see below) by measuring brightness in a ring around that pixel. Pixels brighter than the central pixel were coded as 1 while dimmer pixels were coded as 0, producing an eight-digit binary pattern for the pixel (e.g., 00001111 – see Fig. 4C). The method then counts the frequency of the various patterns in the patch. Uniform LBP features are those with one consecutive area of brightness (e.g., 01110000 but not 01010100). All non-uniform patterns were grouped together before counting pattern frequencies. Rotation-invariant LBP features are those that were equivalent after bit-shifting so that orientation of the pattern did not matter (e.g., 1110000 and 00011100 were counted together, since both have three consecutive bright pixels). All rotations of the same pattern were grouped to yield 10 patterns in all: a non-uniform pattern, a sequence of all 0's, and 8 possible sequences of consecutive 1's.

We extracted LBP features from fifteen 16×16 pixel patches from both eyes and the center of the mouth, automatically located with OpenFace [82]. We selected eye regions to capture events such as blinking and general eye movement patterns (e.g., horizontal saccades should be indicative of normal reading), which have both been linked to mind wandering [50], [80]. The mouth regions were chosen to capture movements such as yawning that would result in texture changes (e.g., as the teeth became visible).

We extracted ten LBP features from each patch in each frame, and aggregated over frames in each clip with minimum, maximum, mean, median, range, and standard deviation functionals to obtain a total of 900 LBP features (15 patches × 10 features/patch × 6 functionals/feature).

Basic Action Unit (AU) Features. Facial action units (AUs) represent specific muscle activations; the AUs we considered were AU1 (inner brow raiser), AU2 (outer brow raiser), AU4 (brow lowerer), AU5 (upper lid raiser), AU6 (cheek raiser), AU7 (lid tightener), AU9 (nose wrinkler), AU10 (upper lip raiser), AU12 (lip corner puller), AU14 (dimpler), AU15 (lip corner depressor), AU17 (chin raiser), AU18 (lip puckerer), AU20 (lip stretcher), AU23 (lip tightener), AU24 (lip pressor), AU25 (lips part), AU26 (jaw drop), and AU28 (lip suck). We detected AUs with EmotientSDK, an updated commercial version of the Computer Expression Recognition Toolbox (CERT) [62]. CERT has been previously validated against human annotations of facial expressions on thousands of video frames [36], [83]. It recognizes AUs by extracting the responses of 72 two-dimensional Gabor filters and uses support vector machines (SVMs) for AU classification. AU intensity is estimated by measuring the distance from the decision boundary of the SVM [84]. Fig. 4B illustrates time series of example AUs. We extracted the mean, median, standard deviation, minimum, maximum, and range across the 10s clips, resulting in 114 AU features (6 functionals × 19 AUs).

Co-occurring AU Features. We captured co-occurrence relationships between AUs to model more complex expressions. For example, co-occurring muscle movements near both the mouth and eyes when smiling can indicate genuine smiles compared to smiles involving the mouth only [85]. We estimated AU co-occurrences based on the similarity between their distributions within each clip using Jensen-Shannon divergence (JSD) [86], which is an extension of Kullback-Leibler divergence (KLD) [87]. KLD Equation (1) measures the information lost by using a prior distribution Q to approximate a posterior distribution P, given probability density functions p and q for P and Q respectively. JSD Equation (2) is a modification of KLD that is symmetric, which allows measurement of symmetric relationships between AUs (e.g., co-occurrence of eyebrow + mouth movements is equivalent to mouth + eyebrow movements). JSD was chosen over other measures (e.g., correlation-based measures) because it captures non-linear relationships [86]. Furthermore, JSD measures expressions that consist of multiple AUs activating in the same clip, even if they do not activate at exactly the same moment. For example, JSD features can measure a mouth movement that is accompanied by an eyebrow movement within the same clip. We computed a total of (19×[19−1]/2)=171 JSD features for each AU pair.
KLD(P||Q)=∫∞−∞p(x)logp(x)q(x)dx(1)
View Source
JSD(P||Q) =12 KLD(P||12[P+Q])+12KLD(Q||12[P+Q]).(2)
View Source

AU Temporal Dynamics Features. We captured changes in AUs over time to model facial expression dynamics that might be obscured by mean aggregation as with the basic AU features. We applied one-dimensional Gabor filters to AU time series using an existing method [88]. Gabor filters capture responses in specific frequencies and can thus distinguish between static facial expressions (such as an open mouth) and dynamic facial expressions (such as a yawn) if tuned to the right frequency [88].

Gabor filters consist of a cosine wave multiplied by a Gaussian envelope. From a filter wavelength λ and sample frequency f (i.e., frames per second), a frequency multiplier k scales sample indices to the appropriate domain Equation (3). Then a filter G is defined with the same width w as the number of frames in the video Equation (4).
k =2πλf(3)
View SourceRight-click on figure for MathML and additional features.
G [t]=cos((t−w2)k) e−18(t−w2)2k2.(4)
View SourceRight-click on figure for MathML and additional features.

Filters were convolved across the frame-by-frame AU time series with missing values (failed face tracking) linearly interpolated. Filter responses indicate changes in AUs that occurred over a period of time similar to the period of the cosine wave. Following the method of [88], we squared the filter response to emphasize larger values and counted regions of both positive and negative responses. We applied a bank of 8 filters with periods ranging from 1 to 12 seconds (Fig. 5), because the specific duration of any facial expressions associated with mind wandering is unknown but similar periods have been effective in previous research [88]. Periods much shorter than 1 second would be unlikely to work for our videos since they were recorded at 12.5 frames per second; filters would have few samples from which to recognize short periods (e.g., just 5 frames for a period of 0.4 seconds).


Fig. 5.
Gabor filters that were convolved over AU intensity time series.

Show All

The temporal filter features were counts of positive and negative responses for each filter (i.e., the number of times the filter produced a positive or negative response within a clip). We then grouped counts into bins (number ranges) according to size of response (area under the squared filter response curve), ranging from -6 to 6 in accordance with previous research [88]. With 19 AUs, 8 filter wavelengths, and 12 bins, there were 1,824 temporal filter features.

2.4 Supervised Classification
We build models with two commonly-used classifiers: SVMs and feed-forward deep neural networks (DNNs). We used SVMs because of their flexibility and general efficacy in related computer vision research [89], [90]. Furthermore, they are well-suited to the high-level features we extracted and the relatively small size of the dataset. We also explored DNNs given their efficacy [91], [92] – though their full potential is typically only realized with large amounts of data. For the same reason, we did not consider convolutional neural networks (CNNs). Pre-trained CNNs could be applied to extract feature maps from individual video frames, but would yield tens of thousands of dimensions when applied across clips with up to 125 frames per clip. We thus restricted analyses to the relatively low-dimensional feature types described above.

We trained individual SVM and DNN models for each feature set (motion, head pose, LBP textures, AUs, co-occurring AUs, and temporal AU dynamics) and also considered fusion of feature sets so that the predictive power of different types of facial features could be compared (e.g., static versus dynamic facial expressions).

We used person-independent four-fold cross validation [93]. By person-independent, we mean that all data from a participant was either in the training or testing data, but never both, thereby increasing the likelihood of generalization to new participants (at least within similar populations and interaction contexts). We used data from three of the four folds (data from 75 percent of participants) for training, while models were tested on the remaining fold (25 percent of participants). We further split training data to select features, weigh instances, and select hyperparameters with two-fold nested cross-validation (see Fig. 6). These procedures were only applied to the training data.


Fig. 6.
Illustration of SVM model training procedure showing which portions of data we trained and tested on at each step

Show All

Feature Selection. For DNN models, we added L1 regularization to the first layer, thereby minimizing the influence of ineffectual features. For SVM models, we applied feature selection to reduce the dimensionality of the feature space. We initially experimented with forward feature selection (FFS) [94], given that model-free alternatives such as RELIEF-F and correlation-based feature selection (CFS) do not capture the same nonlinear patterns in data that our classifiers do [95], [96]. However, FFS was computationally impractical due to the large number of LBP features and hyperparameter combinations (described below).

Thus, we developed a new two-step variation of FFS (test-correlate feature selection; TCFS) as a compromise between the model-specific advantages of forward feature selection and the computational simplicity of model-free methods. In TCFS, we trained an SVM on each feature and then ranked features based on the accuracy of these individual feature models, as measured by the area under the receiver operating characteristic curve estimated from a single point (minimum proper curve [97]). We eliminated any feature that was correlated (Spearman′srho>.6) with a better ranked feature. The final models were trained on up to 50 (if there were that many) of the highest-ranked remaining features. The 50-feature maximum was informed by recommendations that the square root of the number of instances per training fold (3437× 3/4= 2578  in our case) is an appropriate conservative limit on the number of features [98].

Instance Weighting and Hyperparameter Tuning. Initial experiments with unweighted instances yielded models that exclusively predicted the majority class. Thus, we weighted training instances such that the sum of weights for positive and negative mind wandering instances were equal. SVM models fit the decision boundary according to these weights, setting the decision boundary further from higher weight (minority class) instances. Similarly, DNN models made larger parameter adjustments for minority class instances.

We trained SVM models with radial basis function (RBF) kernels, which require a regularization hyperparameter C and a support vector radius of influence hyperparameter γ. We varied C and γ via grid search; values of C varied from 10−2 to 102 and values of γ varied from 10−5 to 102 by powers of 10. We selected hyperparameters for each feature set individually because each captures different behavioral expressions and has different distributions.

We also varied DNN hyperparameters via grid search, including the number of hidden layers (1,2,4,or8), number of neurons in each hidden layer (4,8,16,or32), dropout [99] applied before each hidden layer (0%,25%,50%,or75%), L1 regularization applied to the input layer (0, .001, .01, or .1), and learning rate for the Adam [100] optimizer (.01, .001, or .0001). The DNN decision threshold was initially set to a typical default of .5, but this resulted in very few positive mind wandering predictions (e.g., < 5 percent for four of the six individual feature set models) Thus, we instead chose DNN decision thresholds to match the SVM predicted rate of mind wandering as closely as possible (since DNNs produced continuous predictions while SVMs did not) to enable unbiased model comparisons.

Fusion Methods. We considered three methods to fuse the individual feature sets. For feature-level fusion, we concatenated the selected features from each set, performed another round of feature selection to reduce the feature set size, and trained a new model. For majority voting, we classified an instance as mind wandering if at least three of the six individual SVM models classified it as such, or if the sum of DNN prediction probabilities exceeded a threshold tuned to match SVM prediction rates. We also trained a Classification and Regression Tree (CART) model on the predictions of the individual feature set models [101]. We cross-validated fusion models with the same training and testing folds as the individual models, thus preserving person-independence.

2.5 Study 1: Mind Wandering in the Lab Results
2.5.1 Classification Results
We measured accuracy primarily with the F1 score of mind wandering. F1 is the harmonic mean of precision (proportion of instances classified as mind wandering that were truly mind wandering) and recall (proportion of true mind wandering instances that were classified as mind wandering). As a baseline, chance-level mind wandering F1 (.300) was defined as the mind wandering F1 obtained by randomly assigning positive mind wandering labels to 30 percent of the instances (i.e., the base rate) and negative to the rest. We also computed area under the receiver operating characteristic curve (AUC), where .500 represents chance level and 1 represents perfect classification. For SVMs we utilized the distance of each instance from the separating hyperplane as a measure of confidence (which is required to calculate AUC) while DNNs naturally yield continuous confidence predictions. The results in Table 1 indicate that detection accuracy was modest, but better than chance in terms of both F1 and AUC (for SVM models more so than DNNs).

TABLE 1 Mind Wandering (MW) Detection Results in the Lab

We compared models using mixed-effects logistic regression to predict agreement between the model outputs and the self-reports (1 for agree; 0 for disagree). We included participant as an intercept-only random effect for all comparisons, due to the repeated and nested structure of the data – one or more instances nested within a participant. We also included participant-level predicted mind wandering rates as a fixed effect covariate, because predicted mind wandering rates were correlated with model accuracy (Pearson rs between -.158 and -.409). For this reason, comparisons might not align with F1 scores in the table since those do not adjust for prediction rates. We used the lme4 [102] package in R [103] for model fitting, the car package [104] for significance testing, and the emmeans package [105] for pairwise comparisons.

We first compared SVMs versus DNNs across all six individual feature sets by regressing accuracy on the classifier type (two-level categorical variable for SVM or DNN) × self-reported mind wandering label (1 or 0) interaction term, with feature set included as a fixed effect. The classifier type × label interaction term allows us to examine model accuracy for positive vs. negative instances of (self-reported) mind wandering, the former being of interest here. The results indicated a significant interaction term, χ2 (1)=5.11,p<.05, which suggests that relative model accuracies varied for positive vs. negative instances of mind wandering. Focusing on the positive instances, estimated marginal means comparison showed that SVMs yielded statistically better results than DNNs, on average.

Focusing on SVM models only, we compared individual feature sets by regressing correctness on the feature set (six-level categorical variable for the individual feature sets) × label interaction term. The interaction was significant (χ2 (5) = 183,p< .001), so we conducted pairwise comparisons between feature sets with false discovery rate adjusted for 15 comparisons. The results yielded the following overall pattern for positive mind wandering instances: LBPTextures>[BasicAUs=BodyMotion=TemporalAUs]>[Co−occurringAUs=HeadPose]. Notably, Basic AUs and Temporal AUs features were more effective than Co-occurring AUs – perhaps because Basic and Temporal AUs capture simpler, first-order expressions of a single facial muscle, while Co-occurring AUs capture subsets of these facial expressions, which might have been too sparse given available data.

Finally, comparing the best individual feature set (LBP Textures) to SVM fusion models yielded a significant model × interaction term (χ2 (3)=281,p< .001). Comparisons for positive mind wandering instances indicated the models were statistically ranked as follows: CARTFusion>[MajorityVote=LBPTextures]>Feature−levelFusion. Thus, results indicate that the CART model was most accurate, though its tendency to predict high levels of mind wandering might be undesirable in some applications, in which case the Majority Vote model may be a better choice.

2.5.2 Comparison Across Mind Wandering Types
Students could report mind wandering incidents that were either task-related or task-unrelated thoughts. We replicated individual SVM models described above (Section 2.4) as a three-class classification task (task-unrelated thought, task-related interference; not mind wandering). We computed accuracy by combining predictions of either mind wandering type and re-computing F1, so that we could directly compare accuracy to the binary model. However, F1 scores were ≈ .300 (chance level) with the exception of Body Motion (F1 =.336), which was not higher than the binary classification F1 (.429). This is likely due to the fact that the class imbalance for the binary classification gets much more severe for the three-way classification.

To further examine differences across mind wandering types, we inspected the proportions of each mind wandering type that were correctly classified (recall) by the binary classifiers. We first compared recall across each mind wandering type for each feature channel (Table 2). Overall, recall was similar across the two types of mind wandering. Next, focusing on the positive mind wandering instances only, we regressed model correctness (correct or incorrect) on the mind wandering type and feature set interaction. Neither the mind wandering type main effect nor the feature set × mind wandering type interaction were significant, indicating that models were similarly accurate for both types of mind wandering.

TABLE 2 Comparison of Recall for Task-Related and Task-Unrelated Types of Mind Wandering
Table 2- 
Comparison of Recall for Task-Related and Task-Unrelated Types of Mind Wandering
2.5.3 Comparison Across Genders
Students (39 percent male) reported their gender following the text reading portion of the study. We compared mind wandering reports and classification accuracies across genders, regressing agreement on the gender × mind wandering label interaction term, again including feature set as a fixed effect. The interaction term was significant (χ2 (1)=73,p<.001), and pairwise comparisons revealed that the individual feature set models were significantly more accurate for male than female students – despite the fact that we controlled for individual mind wandering rates in this comparison. This difference was primarily due to higher recall for male students (.797 versus .665), especially since precision was higher for the female students (.326 for females; .386 for males).

2.5.4 Including Undetectable Face Instances
We removed 207 positive mind wandering instances because no face could be detected in the video clips (Section 2.2). These could also be removed in a real-time application of our detectors, but gaps in predictions might need to be filled in – for example, to create uniform time series predictions. We thus examined the influence of making random predictions for these instances, with additional negative mind wandering instances sampled to maintain the 30 percent mind wandering base rate. We set the randomly-predicted mind wandering rate equal to that of the best model (CART Fusion; .593 predicted rate), appended the random predictions to the list of CART Fusion predictions, and recomputed accuracy to simulate a scenario where the CART Fusion predictions are supplemented with random predictions when needed. Given the small number of unusable positive instances, this approach has minimal detriment to accuracy – the newly calculated F1 was .466, precision was .351, and recall was .694 (versus .478, .360, and .711, respectively; Table 1).

2.6 Comparison to Human Observers
Some visual perception tasks are relatively easy for humans but difficult for computers (e.g., recognizing faces, following gaze directions). To assess the difficulty of recognizing mind wandering from facial expressions, we recruited human observers from Amazon's Mechanical Turk's [106] crowdsourcing platform to each code a random subset of 100 video clips (30 of which corresponded to self-reported mind wandering) for mind wandering. We recruited nine different human observers per clip and used majority voting to determine the final observer mind wandering label for each clip.

Observer achieved mind wandering precision, recall, and F1 scores of .333, .467, and .389, respectively. On the same subset of 100 clips, the CART decision-level fusion model yielded precision, recall, and F1 scores of .421, .800, and .552 with a predicted mind wandering rate of 57 percent compared to 42 percent for human observers. Accuracy varied considerably across observation rounds (Fig. 7), though none exceeded the accuracy of the CART model (F1 of .552 on this sample).

Fig. 7. - 
Observer accuracy (F1) for all nine rounds of human mind wandering ratings
Fig. 7.
Observer accuracy (F1) for all nine rounds of human mind wandering ratings

Show All

Despite the small sample size, this result illustrates the difficulty of the task for humans. It also highlights the potential for automatic mind wandering detectors to outperform humans, though more formal validation with a larger sample is needed.

SECTION 3Study 2: Probe-Caught Mind Wandering Detection with an Intelligent Tutoring System in the Classroom
We followed up on the lab study with a classroom study, using a different participant sample (high-school students), a different method to obtain mind wandering reports (probe-caught) and with a more interactive task: learning from an intelligent tutoring system called Guru [107]. We also used a different method to extract facial features as elaborated below. These methodological differences were due to practical constraints, but provide an opportunity to test core components of our approach in a vastly different context. Finally, participant-level demographics were not available for these data, so we could not compare model accuracy by gender as in Study 1.

3.1 Guru Tutor
Guru is an intelligent tutoring system designed to teach biology topics (e.g., osmosis; protein function) aligned with state curriculum standards. It engages students in one-on-one collaborative conversations in natural language [107]. It was modeled after interactions with expert human tutors and has been shown to be effective at promoting learning at levels compared to small group human tutoring [107].

Guru utilizes an animated pedagogical agent that references a multimedia workspace (see Fig. 8). The tutor communicates via synthesized speech and gestures, while students communicate by typing their responses, which are analyzed using natural language processing. Guru maintains a dynamic model of student progress (called a student model [108]), which it uses to adapt instruction to individual students.

Fig. 8. - 
Screenshot of Guru in the CGB phase.
Fig. 8.
Screenshot of Guru in the CGB phase.

Show All

A topic in Guru involves interrelated concepts and facts, which are covered in 15- to 40-minute tutoring sessions. Guru begins with an introduction to motivate the topic, which is then followed by a five-phase tutorial session (see [107] for details of each phase).

3.2 Data Collection
Data were collected from 135 (41 percent male) high-school freshmen and sophomores enrolled in an introductory biology course. Students provided written assent to participate, while their parents provided written consent. The study was approved by the university institutional review board, and by the high school's principal. Students were given a $10 gift card for participating.

The study occurred over the course of two days in students’ regular biology classroom with students sharing a desk (Fig. 9). There were seven class periods per day, with enrollment ranging from 14 to 30 students per class. Students used a school-provided laptop to interact with Guru, which we equipped with an inexpensive (Logitech C270) external webcam. The cameras for the two students at each desk were connected to a third laptop, which was used solely for facial feature processing and was synchronized to the Guru laptops via an internet time server.

Fig. 9. - 
Example classroom layout.
Fig. 9.
Example classroom layout.

Show All

Upon providing assent, students were introduced to the study, followed by a 30-minute Guru learning session on one topic, a short break, and another 30-minute Guru session on a different topic.

We used the probe-caught method [9] to monitor mind wandering during the two Guru sessions. Specifically, we defined mind wandering to students before their first interaction with Guru, provided instructions on how to report mind wandering to the probes, and also administered a brief quiz to verify their understanding.

Thought probes occurred pseudorandomly every 90-120 seconds. The 90-120s time range was selected based on previous research which tracked mind wandering during interactions with Guru [109]. The probes automatically paused the tutoring session. If the tutor was speaking at the time the probe was to be triggered, the probe was delayed until the tutor finished speaking. The probe consisted of an auditory beep along with an opaque overlay on screen, instructing the participant to press the “N” key if they were not mind wandering, “I” if they were intentionally (deliberately) mind wandering, or “U” if they were unintentionally (spontaneously) mind wandering. Here, we do not differentiate between intentional and unintentional mind wandering in order to maximize the number of instances for machine learning.

Participants encountered an average of 12 probes over the course of each session; they reported mind wandering for an average of 27.6%(SD=23.5%). There was considerable variability in the mind wandering distribution across participants as noted in Fig. 10.


Fig. 10.
Histogram of mind wandering rates.

Show All

As expected, the classroom environment was much less controlled than the lab environment. Students interrupted and distracted each other, left to go to the bathroom, and occasionally even used their cellphones. Due to computer failures (e.g., power supply failure, unexpected software updates), data from 10 students were unusable, resulting in data from 125 students.

3.3 Automatic Mind Wandering Detection
Real-Time Feature Extraction. Due to privacy considerations, videos of students could not be recorded for later feature extraction and analysis. Therefore, features were extracted in real-time. We could not extract features with EmotientSDK, as we did in the lab study, due to licensing constraints. Instead, we extracted AUs and head pose with OpenFace [82]. The feature extraction frame rate was variable because of external computational resource demands (e.g., system processes) and varying demands of the feature extraction process itself (e.g., when face tracking is lost the entire image must be searched to rediscover the face – a computationally expensive process). For this reason, frame rate was also relatively low (mean = 4.6 frames per second) compared to the lab study (exactly 12.5 frames per second). Additionally, temporal filter features could not be extracted from AU estimates because of the variable timing and sparsity of frames. Body motion and LBP features were also not extracted since they add additional computational complexity. Thus, we extracted head pose and AU features real-time, and calculated AU co-occurrence features (JSD features) offline.

Instance Extraction. We extracted 2,888 instances, each 10s long, from the 125 students. We discarded 502 instances because they contained fewer than 5 frames of data (approximately 1s), leaving 2,386 instances (25.9 percent positive mind wandering instances, 62.5 percent of which were unintentional).

Supervised Classification. As in the lab study, we trained SVM and DNN classifiers for the individual channels (Basic AUs, Co-occurring AUs, and Head Pose only) using the exact same cross-validation, feature selection, instance weighting, and hyperparameter tuning procedures from Study 1. We also trained similar feature-level fusion, decision-level fusion (CART), and majority vote models as in Study 1.

3.4 Results
Mind wandering detection was accurate above chance level (up to F1 =.414, 20.9 percent above chance of .259 for the feature-level fusion model; Table 3). AUC results indicated a possible advantage for SVMs over DNNs, which we followed up via statistical comparisons of accuracy in the same manner as Study 1 (see Section 2.5.1) to rigorously investigate this possibility. We found a similar trend toward SVM models outperforming DNNs overall for the individual feature sets, but it was not significant (p=.154). Follow-up analysis of pairwise comparisons for the individual feature sets for SVMs (as in Study 1) revealed the statistical ordering: BasicAUs>Co−occurringAUs>HeadPose. Interestingly, the results show that the feature-level fusion model had the highest overall F1, exceeding the individual feature sets and even the decision-level fusion methods (statistical ordering: Feature−levelFusion>CARTFusion>BasicAUs>MajorityVote. Once again, however, it is worth noting that the most accurate model predicts mind wandering at a high rate, so other models may be preferable if this is of concern.

TABLE 3 Overview of Mind Wandering (MW) Detection Results in the Classroom

3.4.1 Comparison Across Mind Wandering Types
We further analyzed each feature set, comparing recall for intentional and unintentional mind wandering report types (Table 4). We focused on the SVM models, given their higher accuracies. Similar to Study 1, there were no significant differences in detector accuracies between the two types of mind wandering included in Study 2. Recall was similar as well: .369 for intentional versus .345 for unintentional (Table 4) mind wandering.

TABLE 4 Comparison of Detector Recall for Intentional Versus Unintentional Mind Wandering Cases

3.4.2 Including Undetectable Face Instances
There were 502 instances in which the face could not be detected for at least 1 second (see Section 3.3). As in Study 1, we generated random mind wandering predictions with the same predicted rate as the feature-level fusion SVM (highest F1 model). We found that F1 was only slightly diminished at .396 (recall was .545, and precision was .311) with these instances included compared to them being excluded (F1 =.414, recall = .573, precision = .324).

SECTION 4Discussion
We review the main findings, discuss limitations, and point to opportunities for future research.

4.1 Main Findings
Automatic mind wandering detection is a challenging problem, especially given the lack of prototypical mind wandering facial expressions (Fig. 1), variance in mind wandering reports across participants (in fact, 25 percent of participants reported no mind wandering at all in Study 1 [Fig. 2] and 29 percent in Study 2 [Fig. 10]), and the difficulty of the task for human observers (Section 2.6). Despite these challenges, we found that automatic computer vision methods detected mind wandering at better than chance-levels in both a laboratory reading context (decision-level fusion F1=.478 versus .300 chance) and in a noisy biology classroom with real-time feature extraction (feature-level fusion F1=.414 versus .259 chance).

Although these results reflect a modest improvement over chance level predictions (25.4 percent for the lab study and 20.9 percent for the classroom study), the models surpassed previous state-of-the-art face-based mind wandering during reading in a lab context. Specifically, mind wandering detection accuracy using all six feature sets was F1 =.478, compared to F1=.441 previously reported using basic AU and head pose features [64] – an improvement of 8.4 percent. This finding demonstrates a slight advantage for considering multiple feature sets when detecting subtle facial expressions associated with mind wandering. In fact, LBP features (a feature set not previously considered for mind wandering detection) were the most accurate at the task. Additionally, our analysis of decision-level and feature-level fusion models in both studies showed a statistically significant increase in accuracy compared to the best individual feature set, though with high mind wandering prediction rates.

These results showed a notable advantage for SVM models versus DNNs in both studies. In Study 1, the best SVM model yielded a 9.8 percent improvement over the best DNN model; the improvement was 11.2 percent for Study 2. The SVM advantage in these studies comes despite a thorough hyperparameter search for DNNs. DNNs may have suffered from the relatively small size (3,437 instances in Study 1 and 2,386 in Study 2) and high-level feature sets, compared to machine learning problems where DNNs typically excel: millions of instances and low-level features.

We also investigated model accuracies for two different types of mind wandering in each study. In Study 1, we found no statistical difference in accuracy for task-related and task-unrelated mind wandering instances. Similarly, Study 2 showed no difference in accuracy for intentional versus unintentional mind wandering, indicating that mind wandering is not easier to detect across these types, and may be associated with similar facial expressions.

There were some instances where facial expressions could not be detected (207 in Study 1 and 502 in Study 2). However, for some applications it may be necessary to make predictions for all instances. We therefore computed accuracy with random predictions set to match the classifier prediction rate for these instances, and found that model accuracy was not drastically diminished in Study 1 (F1 decreased from .478 to .466) nor in Study 2 (F1 changed from .414 to .396), thus indicating models could be deployed in such applications without notable decreases in accuracy.

4.2 Limitations and Future Work
The most notable limitation of the current paper is the modest accuracy achieved for mind wandering detection. However, this is expected since the problem of mind wandering detection has been shown to be exceedingly difficult with other modalities as well, such as eye gaze and physiology. That said, gaze-based mind wandering detectors do appear to outperform video-based detectors (see [110]) on the same data. Thus, future work should strive to improve these results through additional feature engineering methods and additional deep learning methods which have been successful in other domains.

Additionally, the features that could be extracted in the classroom environment were limited by the processing power of the computers available. While this is a realistic constraint that must be dealt with, future work with increased processing power for real-time feature extraction will be necessary for determining performance upper limits in this context.

It is also possible that facial expressions of mind wandering differ in contexts with more or less social pressure to appear engaged. Participants read alone in Study 1, but were still aware (at least initially) that they were being recorded, which might have increased self-regulatory behaviors. Thus, one possible avenue for future work is to compare facial expressions in contexts where individuals do not know they are being observed. Similarly, participants’ interest in reading or learning about a topic might influence their rate of mind wandering [109], [111], [112], [113]. Our results also indicated that models in Study 1 were significantly more accurate for male than female students, an effect driven by differences in recall, which implies that demographic differences are worthy of further exploration.

Another limit pertains to our instance extraction scheme in Study 1, which required discarding a large number of clips (1,339) because they contained a page turn event. This process limits the mind wandering detectors to function only in situations with no such events. However, this limitation is necessary to avoid the possibility of models simply detecting movement associated with page-turn keypresses (a trivial task), which in turn might be related to mind wandering, but only in a highly task-specific way. On the other hand, data discarded due to undetectable facial features might be improved upon in future work, by imputing missing values (e.g., with a Kalman filter).

4.3 Applications
The mind wandering detection approach reported here represent the first automatic face-based mind wandering detection in a laboratory and in a classroom. The results we presented indicate that mind wandering can be detected at levels above chance – though far from perfectly. Although more research is needed to ascertain a plausible upper-bound for mind wandering detection accuracy, the current level of accuracy is likely sufficient to support fail-soft, probabilistic interventions that utilize these detectors in computerized learning environments. For example, a computerized reading environment with an automated mind wandering detector could recommend a break if repeated mind wandering is detected. Similarly, brief test questions could be inserted into a learning session for occasional instances of detected mind wandering. The learning environment can also sense mind wandering passively and provide class-level aggregates (by leveraging the principle of aggregation to improve reliability of noisy signals [114]) of students' attentional levels to teachers of instruction designers to guide pedagogy. Thus, the next critical step is to use the detectors in these and other ways in order to provide a more enjoyable, efficient, and effective learning experience for all students.