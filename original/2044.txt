Compared to DSLR cameras, smartphone cameras have smaller sensors,
which limits their spatial resolution; smaller apertures, which limits their
light gathering ability; and smaller pixels, which reduces their signal-tonoise ratio. The use of color filter arrays (CFAs) requires demosaicing, which
further degrades resolution. In this paper, we supplant the use of traditional
demosaicing in single-frame and burst photography pipelines with a multiframe super-resolution algorithm that creates a complete RGB image directly
from a burst of CFA raw images. We harness natural hand tremor, typical in
handheld photography, to acquire a burst of raw frames with small offsets.
These frames are then aligned and merged to form a single image with red,
green, and blue values at every pixel site. This approach, which includes no
explicit demosaicing step, serves to both increase image resolution and boost
signal to noise ratio. Our algorithm is robust to challenging scene conditions:
local motion, occlusion, or scene changes. It runs at 100 milliseconds per
12-megapixel RAW input burst frame on mass-produced mobile phones.
Specifically, the algorithm is the basis of the Super-Res Zoom feature, as well
as the default merge method in Night Sight mode (whether zooming or not)
on Google’s flagship phone.
CCS Concepts: · Computing methodologies → Computational photography; Image processing.
Additional Key Words and Phrases: computational photography, superresolution, image processing, photography
1 INTRODUCTION
Smartphone camera technology has advanced to the point that taking pictures with a smartphone has become the most popular form
of photography [CIPA 2018; Flickr 2017]. Smartphone photography
offers high portability and convenience, but many challenges still
exist in the hardware and software design of a smartphone camera that must be overcome to enable it to compete with dedicated
cameras.
Foremost among these challenges is limited spatial resolution.
The resolution produced by digital image sensors is limited not only
by the physical pixel count (e.g., 12-megapixel camera), but also by
the presence of color filter arrays (CFA)1
like the Bayer CFA [Bayer
1976]. Given that human vision is more sensitive to green, a quad
of pixels in the sensor usually follows the Bayer pattern RGGB;
i.e., 50% green, 25% red, and 25% blue. The final full-color image is
generated from the spatially undersampled color channels through
an interpolation process called demosaicing [Li et al. 2008].
1Also known as a color filter mosaic (CFM).
ACM Trans. Graph., Vol. 38, No. 4, Article 28. Publication date: July 2019.
28:2 • Wronski et al.
Demosaicing algorithms operate on the assumption that the color
of an area in a given image is relatively constant. Under this assumption, the color channels are highly correlated, and the aim
of demosaicing is to reconstruct the undersampled color information while avoiding the introduction of any visual artifacts. Typical
artifacts of demosaicing include false color artifacts such as chromatic aliases, zippering (abrupt or unnatural changes of intensity
over consecutive pixels that look like a zipper), maze, false gradient,
and Moiré patterns (Figure 1 top). Often, the challenge in effective
demosaicing is trading off resolution and detail recovery against
introducing visual artifacts. In some cases, the underlying assumption of cross-channel correlation is violated, resulting in reduced
resolution and loss of details.
A significant advancement in smartphone camera technology in
recent years has been the application of software-based computational photography techniques to overcome limitations in camera
hardware design. Examples include techniques for increasing dynamic range [Hasinoff et al. 2016], improving signal-to-noise ratio
through denoising [Godard et al. 2018; Mildenhall et al. 2018] and
wide aperture effects to synthesize shallow depth-of-field [Wadhwa
et al. 2018]. Many of these recent advancements have been achieved
through the introduction of burst processing2 where on a shutter
press multiple acquired images are combined to produce a photo
that is of greater quality than that of a single acquired image.
In this paper, we introduce an algorithm that uses signals captured across multiple shifted frames to produce higher resolution
images (Figure 1 bottom). Although the underlying techniques can
be generalized to any shifted signals, in this work we focus on applying the algorithm to the task of resolution enhancement and
denoising in a smartphone image acquisition pipeline using burst
processing. By using a multi-frame pipeline and combining different
undersampled and shifted information present in different frames,
we remove the need for an explicit demosaicing step.
To work on a smartphone camera, any such algorithm must:
• Work handheld from a single shutter press ś without a
tripod or deliberate motion of the camera by the user.
• Run at an interactive rate ś the algorithm should produce
the final enhanced resolution with low latency (within at
most a few seconds).
• Be robust to local motion and scene changes ś users
might capture scenes with fast moving objects or scene changes.
While the algorithm might not increase resolution in all such
scenarios, it should not produce appreciable artifacts.
• Be robust to noisy input data ś in low light the algorithm
should not amplify noise, and should strive to reduce it.
With these criteria in mind, we have developed an algorithm that
processes multiple successively captured raw frames in an online
fashion. The algorithm tackles the tasks of demosaicing and superresolution jointly and formulates the problem as the reconstruction
and interpolation of a continuous signal from a set of sparse samples.
Red, green and blue pixels are treated as separate signals on different
planes and reconstructed simultaneously. This approach enables the
2We use the terms multi-frame and burst processing interchangeably to refer to the
process of generating a single image from multiple images captured in rapid succession.
production of highly detailed images even when there is no crosschannel correlation ś as in the case of saturated single-channel
colors. The algorithm requires no special capturing conditions; natural hand-motion produces offsets that are sufficiently random in
the subpixel domain to apply multi-frame super-resolution. Additionally, since our super-resolution approach creates a continuous
representation of the input, it allows us to directly create an image
with a desired target magnification / zoom factor without the need
for additional resampling. The algorithm works on a mobile device
and incurs a computational cost of only 100 ms per 12-megapixel
processed frame.
The main contributions of this work are:
(1) Replacing raw image demosaicing with a multi-frame superresolution algorithm.
(2) The introduction of an adaptive kernel interpolation / merge
method from sparse samples (Section 5) that takes into account the local structure of the image, and adapts accordingly.
(3) A motion robustness model (Section 5.2) that allows the algorithm to work with bursts containing local motion, disocclusions, and alignment/registration failures (Figure 12).
(4) The analysis of natural hand tremor as the source of subpixel
coverage sufficient for super-resolution (Section 4).
2 BACKGROUND
2.1 Demosaicing
Demosaicing has been studied extensively [Li et al. 2008], and the
literature presents a wide range of algorithms. Most methods interpolate the missing green pixels first (since they have double sampling density) and reconstruct the red and blue pixel values using
color ratio [Lukac and Plataniotis 2004] or color difference [Hirakawa and Parks 2006]. Other approaches work in the frequency
domain [Leung et al. 2011], residual space [Monno et al. 2015], use
LAB homogeneity metrics [Hirakawa and Parks 2005] or non local
approaches [Duran and Buades 2014]. More recent works use CNNs
to solve the demosaicing problem such as the joint demosaicing and
denoising technique by Gharbi et al. [2016]. Their key insight is to
create a better training set by defining metrics and techniques for
mining difficult patches from community photographs.
2.2 Multi-frame Super-resolution (SR)
Single-image approaches exploit strong priors or training data. They
can suppress aliasing3 well, but are often limited in how much they
can reconstruct from aliasing. In contrast to single frame techniques,
the goal of multi-frame super-resolution is to increase the true
(optical) resolution.
In the sampling theory literature, multi-frame super-resolution
techniques date as far back as the ’50s [Yen 1956] and the ’70s [Papoulis 1977]. The work of Tsai [1984] started the modern concept
of super-resolution by showing that it was possible to improve
resolution by registering and fusing multiple aliased images. Irani
and Peleg [1991], and then Elad and Feuer [1997] formulated the
3
In this work we refer to aliasing in signal processing terms ś a signal with frequency
content above half of the sampling rate that manifests as a lower frequency after
sampling [Nyquist 1928].
ACM Trans. Graph., Vol. 38, No. 4, Article 28. Publication date: July 2019.
Handheld Multi-Frame Super-Resolution • 28:3
a) RAW Input Burst
b) Local Gradients
d) Alignment Vectors
e) Local Statistics
c) Kernels
f) Motion Robustness
g) Accumulation
h) Merged Result
Fig. 2. Overview of our method: A captured burst of raw (Bayer CFA) images (a) is the input to our algorithm. Every frame is aligned locally (d) to a single
frame, called the base frame. We estimate each frame’s contribution at every pixel through kernel regression (Section 5.1). These contributions are accumulated
separately per color channel (g). The kernel shapes (c) are adjusted based on the estimated local gradients (b) and the sample contributions are weighted
based on a robustness model (f) (Section 5.2). This robustness model computes a per-pixel weight for every frame using the alignment field (d) and local
statistics (e) gathered from the neighborhood around each pixel. The final merged RGB image (h) is obtained by normalizing the accumulated results per
channel. We call the steps depicted in (b)-(g) the merge.
algorithmic side of super-resolution. The need for accurate subpixel registration, existence of aliasing, and good signal-to-noise
levels were identified as the main requirements of practical superresolution [Baker and Kanade 2002; Robinson and Milanfar 2004,
2006].
In the early 2000s, Farsiu et al. [2006] and Gotoh and Okutomi [2004]
formulated super-resolution from arbitrary motion as an optimization problem that would be infeasible for interactive rates. Ben-Ezra
et al. [2005] created a jitter camera prototype to do super-resolution
using controlled subpixel detector shifts. This and other works inspired some commercial cameras (e.g., Sony A6000, Pentax FF K1,
Olympus OM-D E-M1 or Panasonic Lumix DC-G9) to adopt multiframe techniques, using controlled pixel shifting of the physical
sensor. However, these approaches require the use of a tripod or a
static scene. Video super-resolution approaches [Belekos et al. 2010;
Liu and Sun 2011; Sajjadi et al. 2018] counter those limitations and
extend the idea of multi-frame super-resolution to video sequences.
2.3 Kernel Based Super-resolution and Interpolation
Takeda et al. [2006; 2007] formulated super-resolution as a kernel
regression and reconstruction problem, which allows for faster processing. Around the same time, Müller et al. [2005] introduced a
technique to model fluid-fluid interactions that can be rendered using kernel methods introduced by Blinn [1982]. Yu and Turk [2013]
proposed an adaptive solution to the reconstructing of surfaces of
particle-based fluids using anisotropic kernels. These kernels, like
Takeda et al. ’s, are based on local gradient Principal Component
Analysis (PCA), where the anisotropy of the kernels allows for simultaneous preservation of sharp features and smooth rendering of
flat surfaces. Similar adaptive kernel based method were proposed
for single image super-resolution by Hunt [2004] and for general
upscaling and interpolation by Lee and Yoon [2010]. We adopt some
of these ideas and generalize them to fit our use case.
2.4 Burst Photography and Raw Fusion
Burst fusion methods based on raw imagery are relatively uncommon in the literature, as they require knowledge of the photographic
pipeline [Farsiu et al. 2006; Gotoh and Okutomi 2004; Heide et al.
2014; Wu and Zhang 2006]. Vandewalle et al. [2007] described an algorithm where information from multiple Bayer frames is separated
into luminance and chrominance components and fused together to
improve the CFA demosaicing. Most relevant to our work is Hasinoff
et al. [2016] which introduced an end-to-end burst photography
pipeline fusing multiple frames for increased dynamic range and
signal-to-noise ratio. Our paper is a more general fusion approach
that (a) dispenses with demosaicing, (b) produces increased resolution, and (c) enables merging onto an arbitrary grid, allowing
for high quality digital zoom at modest factors (Section 7). Most
recently, Li et al. [2018] proposed an optimization based algorithm
for forming an RGB image directly from fused, unregistered raw
frames.
2.5 Multi-frame Rendering
This work also draws on multi-frame and temporal super-resolution
techniques widely used in real-time rendering (for example, in video
games). Herzog et al. combined information from multiple rendered
frames to increase resolution [2010]. Sousa et al. [2011] mentioned
the first commercial use of robustly combining information from two
frames in real-time in a video game, while Malan [2012] expanded
its use to produce a 1920 × 1080 image from four 1280 × 720 frames.
ACM Trans. Graph., Vol. 38, No. 4, Article 28. Publication date: July 2019.
28:4 • Wronski et al.
Subsequent work [Drobot 2014; Karis 2014; Sousa 2013] established
temporal super-resolution techniques as state-of-the-art and standard in real-time rendering for various effects, including dynamic
resolution rendering and temporal denoising. Salvi [2016] provided a
theoretical explanation of commonly used local color neighborhood
clipping techniques and proposed an alternative based on statistical
analysis. While most of those ideas are used in a different context,
we generalize their insights about detecting aliasing, misalignment
and occlusion in our robustness model.
2.6 Natural Hand Tremor
While holding a camera (or any object), a natural, involuntary hand
tremor is always present. The tremor is comprised of low amplitude
and high frequency motion components that occur while holding
steady limb postures [Schäfer 1886]. The movement is highly periodic, with a frequency in the range of 8ś12 Hz, and its movement is
small in magnitude but random [Marshall and Walsh 1956]. The motion also consists of a mechanical-reflex component that depends on
the limb, and a second component that causes micro-contractions in
the limb muscles [Riviere et al. 1998]. This behavior has been shown
to not change with age [Sturman et al. 2005] but it can change due
to disease [NIH 2018]. In this paper, we show that the hand tremor
of a user holding a mobile camera is sufficient to provide subpixel
coverage for super-resolution.
3 OVERVIEW OF OUR METHOD
Our approach is visualized in Figure 2. First, a burst of raw (CFA
Bayer) images is captured. For every captured frame, we align it
locally with a single frame from the burst (called the base frame).
Next, we estimate each frame’s local contributions through kernel
regression (Section 5.1) and accumulate those contributions across
the entire burst. The contributions are accumulated separately per
color plane. We adjust kernel shapes based on the estimated signal
features and weight the sample contributions based on a robustness
model (Section 5.2). We perform per-channel normalization to obtain
the final merged RGB image.
3.1 Frame Acquisition
Since our algorithm is designed to work within a typical burst processing pipeline, it is important that the processing does not increase
the overall photo capture latency. Typically, a smartphone operates
in a mode called Zero-Shutter Lag, where raw frames are being
captured continuously to a ring buffer when the user opens and
operates the camera application. On a shutter press, the most recent
captured frames are sent to the camera processing pipeline. Our algorithm operates on an input burst (Figure 2 (a)) formed from those
images. Relying on previously captured frames creates challenges
for the super-resolution algorithm ś the user can be freely moving
the camera prior to the capture. The merge process must be able
to deal with natural hand motion (Section 4) and cannot require
additional movement or user actions.
3.2 Frame Registration and Alignment
Prior to combining frames, we place them into a common coordinate
system by registering frames against the base frame to create a set
of alignment vectors (Figure 2 (d)). Our alignment solution is a
refined version of the algorithm used by Hasinoff et al. [2016]. The
core alignment algorithm is coarse-to-fine, pyramid-based block
matching that creates a pyramid representation of every input frame
and performs a limited window search to find the most similar tile.
Through the alignment process we obtain per patch/tile (with tile
sizes of Ts ) alignment vectors relative to the base frame.
Unlike Hasinoff et al. [2016], we require subpixel accurate alignment to achieve super-resolution. To address this issue we could use
a different, dedicated registration algorithm designed for accurate
subpixel estimation (e.g., Fleet and Jepson [1990]), or refine the block
matching results. We opted for the latter due to its simplicity and
computational efficency. We have explored estimating the subpixel
offsets by fitting a quadratic curve to the block matching alignment error and finding its minimum [Kanade and Okutomi 1991];
however, we found that super-resolution requires a more accurate
method. Therefore, we refine the block matching alignment vectors
by three iterations of Lucas-Kanade [1981] optical flow image warping. This approach reached the necessary accuracy while keeping
the computational cost low.
3.3 Merge Process
After frames are aligned, the remainder of the merge process (Figure 2 (b-g)) is responsible for fusing the raw frames into a full RGB
image. These steps constitute the core of our algorithm and will be
described in greater detail in Section 5.
The merge algorithm works in an online fashion, sequentially
computing the contributions of each processed frame to every output pixel by accumulating colors from a 3 × 3 neighborhood. Those
contributions are weighted by kernel weights (Section 5.1, Figure 2
(c)), modulated by the robustness mask (Section 5.2, Figure 2 (f)),
and summed together separately for the red, green and blue color
planes. At the end of the process, we divide the accumulated color
contributions by the accumulated weights, obtaining three color
planes.
The result of the merge process is a full RGB image, which can
be defined at any desired resolution. This can be processed further
by the typical camera pipeline (spatial denoising, color correction,
tone-mapping, sharpening) or alternatively saved for further offline
processing in a non-CFA raw format like Linear DNG [Adobe 2012].
Before we explain the algorithm details, we analyze the key characteristics that enable the hand-held super-resolution.
4 HAND-HELD SUPER-RESOLUTION
Multi-frame super-resolution requires two conditions to be fulfilled [Tsai and Huang 1984]:
(1) Input frames need to be aliased, i.e., contain high frequencies that manifest themselves as false low frequencies after
sampling.
(2) The input must contain multiple aliased images, sampled
at different subpixel offsets. This will manifest as different
phases of false low frequencies in the input frames.
Having multiple lower resolution shifted and aliased images allows
us to both remove the effects of aliasing in low frequencies as well
as reconstruct the high frequencies. In a (mobile) camera pipeline,
ACM Trans. Graph., Vol. 38, No. 4, Article 28. Publication date: July 2019.
Handheld Multi-Frame Super-Resolution • 28:5
-1.5 -1 -0.5 0 0.5 1 1.5
Horiztonal Angular Displacement (radians 10-3)
-1.5
-1
-0.5
0
0.5
1
1.5
Vertical Angular Displacement (radians 10-3
)
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
Angular Velocity (radians/sec)
0
0.05
0.1
0.15
0.2
Bin Count / Total Count
a) b)
Fig. 3. (a) Horizontal and vertical angular displacement (i.e., not including
translational displacement) arising from handheld motion evaluated over a
test set of 86 captured bursts. The red circle corresponds to one standard
deviation (which maps to a pixel displacement magnitude of 0.89 pixels)
showing that the distribution is roughly symmetrical. (b) Histogram of
angular velocity magnitude measured over the test set showing that during
captures the rotational velocity remains relatively low.
(1) means an image sensor having distances between pixels larger
than the spot size of the lens. Our algorithm assumes that the input
raw frames are aliased (see discussion in Section 7).
Most existing multi-frame super-resolution approaches impose
restrictions on the types of motion noted in (2). This includes commercially available products like DSLR cameras using sensor shifts
of a camera placed on a tripod. Those requirements are impractical
for casual photography and violate our algorithm goals; therefore
we make use of natural hand motion. Some publications like Li
et al. [2018] use unregistered, randomly offset images for the purpose of super-resolution or multi-frame demosaicing, however to
our knowledge no prior work analyzes if the subpixel coverage
produced by hand tremor is sufficient to obtain consistent results.
We show that using hand tremor alone is enough to move the device
adequately during the burst acquisition.
To analyze the actual behavior when capturing bursts of photographs while holding a mobile device, we have examined the hand
movement in a set of 86 bursts. The analyzed bursts were captured
by 10 different users during casual photography and not for the
purpose of this experiment. Mobile devices provide precise information about rotational movement measured by a gyroscope, which
we use in our analysis. As we lack measurements about translation
of the device, we ignore translations in this analysis, although we
recognize that they also occur. In Section 5.2, we show how our algorithm is robust to parallax, and occlusions or disocclusions, caused
by translational camera displacement.
First, we used the phone gyroscope rotational velocity measurements and integrated them to find the relative rotation of the phone
compared to the burst capture start. We plotted them along with
a histogram of angular velocities in Figure 3. Our analysis confirms that the hand movement introduces uniformly random (no
directions are preferred) angular displacements and relatively slow
rotation of the capture device during the burst acquisition. The following section analyzes movement in the subpixel space and how
it facilitates random sampling.
4.1 Handheld Motion in subpixel Space
Although handshake averaged over the course of a long time interval
is random and isotropic, handshake over the course of a short burst
might be nearly a straight line or gentle curve in X-Y [Hee Park
and Levoy 2014]. Will this provide a uniform enough distribution
of subpixel samples? It does, but for non-obvious reasons.
Consider each pixel as a point sample, and assume a pessimistic,
least random scenario ś that the hand motion is regular and linear. After alignment to the base frame, the point samples from all
frames combined will be approximately uniformly distributed in
the subpixel space (Figure 4). This follows from the equidistribution
theorem [Weyl 1910], which states that the sequence {a, 2a, 3a, . . .
mod 1} is uniformly distributed (if a is an irrational number). Note
that while the equidistribution theorem assumes infinite sequences,
the closely related concept of rank-1 lattices is used in practice to
generate finite point sets with low discrepancy for image synthesis [Dammertz and Keller 2008] in computer graphics.
Obviously, not all the assumptions above hold in practice. Therefore, we verified empirically that the resulting sample locations are
indeed distributed as expected. We measured the subpixel offsets
by registration (Section 3.2) for 16 × 16 tiles aggregated across 20
handheld burst sequences. The biggest deviation from a uniform
distribution is caused by a phenomenon known as pixel locking and
is visible in the histogram as bias towards whole pixel values. As
can be seen in Figure 5, pixel locking causes non-uniformity in the
distribution of subpixel displacements. Pixel locking is an artifact
of any subpixel registration process [Robinson and Milanfar 2004;
Shimiziu and Okutomi 2005] that depends on the image content
(high spatial frequencies and more aliasing cause a stronger bias).
Despite this effect, the subpixel coverage of displacements remains
sufficiently large in the range (0, 1) to motivate the application of
super-resolution.
5 PROPOSED MULTI-FRAME SUPER-RESOLUTION
APPROACH
Super-resolution techniques reconstruct a high resolution signal
from multiple lower resolution representations. Given the stochastic
nature of pixel shifts resulting from natural hand motion, a good
reconstruction technique to use in our case is kernel regression
(Section 5.1) that reconstructs a continuous signal. Such continuous representation can be resampled at any resolution equal to or
higher than the original input frame resolution (see Section 7 for
discussion of the effective resolution). We use anisotropic Gaussian
Radial Basis Function (RBF) kernels (Section 5.1.1) that allow for
locally adaptive detail enhancement or spatio-temporal denoising.
Finally, we present a robustness model (Section 5.2) that allows our
algorithm to work in scenes with complex motion and to degrade
gracefully to single frame upsampling in cases where alignment
fails.
ACM Trans. Graph., Vol. 38, No. 4, Article 28. Publication date: July 2019.
28:6 • Wronski et al.
1
st frame (base frame) 2
nd
 frame 3
rd
 frame 4
th
 frame All frames aligned to
base frame
Fig. 4. Subpixel displacements from handheld motion: Illustration of a burst of four frames with linear hand motion. Each frame is offset from the
previous frame by half a pixel along the x-axis and a quarter pixel along the y-axis due to the hand motion. After alignment to the base frame, the pixel
centers (black dots) uniformly cover the resampling grid (grey lines) at an increased density. In practice, the distribution is more random than in this simplified
example.
0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Sub-pixel Displacement
0
0.05
0.1
0.15
Bin Count / Total Count
x displacement
y displacement
Fig. 5. Distribution of estimated subpixel displacements: Histogram
of x and y subpixel displacements as computed by the alignment algorithm
(Section 3.2). While the alignment process is biased towards whole-pixel
values, we observe sufficient coverage of subpixel values to motivate superresolution. Note that displacements in x and y are not correlated.
5.1 Kernel Reconstruction
The core of our algorithm is built on the idea of treating pixels of
multiple raw Bayer frames as irregularly offset, aliased and noisy
measurements of three different underlying continuous signals,
one for each color channel of the Bayer mosaic. Though the color
channels are often correlated, in the case of saturated colors (for
example red, green or blue only) they are not. Given sufficient spatial
coverage, separate per-channel reconstruction allows us to recover
the original high resolution signal even in those cases.
To produce the final output image we processes all frames sequentially ś for every output image pixel, we evaluate local contributions
to the red, green and blue color channels from different input frames.
Every input raw image pixel has a different color channel, and it contributes only to a specific output color channel. Local contributions
are weighted; therefore, we accumulate weighted contributions and
weights. At the end of the pipeline, those contributions are normalized. For each color channel, this can be formulated as:
C(x,y) =
Í
n
Í
i
cn,i
· wn,i
· Rˆ
n
Í
n
Í
i wn,i
· Rˆ
n
, (1)
Edge
Flat
Detailed Area
Fig. 6. Sparse data reconstruction with anisotropic kernels: Exaggerated example of very sharp (i.e., narrow, kd e t ai l = 0.05px) kernels on a
real captured burst. For demonstration purposes, we represent samples corresponding to whole RGB input pictures instead of separate color channels.
Kernel adaptation allows us to apply differently shaped kernels on edges
(orange), flat (blue) or detailed areas (green). The orange kernel is aligned
with the edge, the blue one covers a large area as the region is flat, and the
green one is small to enhance the resolution in the presence of details.
where (x,y) are the pixel coordinates, the sum Í
n is over all contributing frames, Í
i
is a sum over samples within a local neighborhood (in our case 3×3), cn,i denotes the value of the Bayer pixel at
given frame n and sample i, wn,i
is the local sample weight and Rˆ
n
is the local robustness (Section 5.2). In the case of the base frame, Rˆ
is equal to 1 as it does not get aligned, and we have full confidence
in its local sample values.
To compute the local pixel weights, we use local radial basis
function kernels, similarly to the non-parametric kernel regression
framework of Takeda et al. [2006; 2007]. Unlike Takeda et al., we
don’t determine kernel basis function parameters at sparse sample
positions. Instead, we evaluate them at the final resampling grid
positions. Furthermore, we always look at the nine closest samples in a 3 × 3 neighborhood and use the same kernel function for
all those samples. This allows for efficient parallel evaluation on a
GPU. Using this "gather" approach every output pixel is independently processed only once per frame. This is similar to work of
ACM Trans. Graph., Vol. 38, No. 4, Article 28. Publication date: July 2019.
Handheld Multi-Frame Super-Resolution • 28:7
Fig. 7. Anisotropic Kernels: Left: When isotropic kernels (ks t r e tch =
1, kshr ink = 1, see supplemental material) are used, small misalignments cause heavy zipper artifacts along edges. Right: Anisotropic kernels
(ks t r e tch = 4, kshr ink = 2) fix the artifacts.
Yu and Turk [2013], developed for fluid rendering. Two steps described in the following sections are: estimation of the kernel shape
(Section 5.1.1) and robustness based sample contribution weighting
(Section 5.2).
5.1.1 Local Anisotropic Merge Kernels. Given our problem formulation, kernel weights and kernel functions define the image quality
of the final merged image: kernels with wide spatial support produce noise-free and artifact-free, but blurry images, while kernels
with very narrow support can produce sharp and detailed images. A
natural choice for kernels used for signal reconstruction are Radial
Basis Function kernels - in our case anisotropic Gaussian kernels.
We can adjust the kernel shape to different local properties of the
input frames: amounts of detail and the presence of edges (Figure 6).
This is similar to kernel selection techniques used in other sparse
data reconstruction applications [Takeda et al. 2006, 2007; Yu and
Turk 2013].
Specifically, we use a 2D unnormalized anisotropic Gaussian RBF
for wn,i
:
wn,i = exp 
−
1
2
d
T
i Ω
−1
di

, (2)
where Ω is the kernel covariance matrix and di
is the offset vector
of sample i to the output pixel (di = [xi − x0,yi − y0]
T
).
One of the main motivations for using anisotropic kernels is that
they increase the algorithm’s tolerance for small misalignments
and uneven coverage around edges. Edges are ambiguous in the
alignment procedure (due to the aperture problem) and result in
alignment errors [Robinson and Milanfar 2004] more frequently
compared to non-edge regions of the image. Subpixel misalignment
as well as a lack of sufficient sample coverage can manifest as zipper
artifacts (Figure 7). By stretching the kernels along the edges, we can
enforce the assignment of smaller weights to pixels not belonging
to edges in the image.
5.1.2 Kernel Covariance Computation. We compute the kernel covariance matrix by analyzing every frame’s local gradient structure
tensor. To improve runtime performance and resistance to image
noise, we analyze gradients of half-resolution images formed by
decimating the original raw frames by a factor of two. To decimate a
Bayer image containing different color channels, we create a single
Presence of an edge
Presence of a sharp feature
1.0
0.8
0.6
0.4
0.2
0.0
0.1 0.5 0.9
0.01 0.015 0.03
Fig. 8. Merge kernels: Plots of relative weights in different 3 × 3 sampling
kernels as a function of local tensor features.
pixel from a 2 × 2 Bayer quad by combining four different color
channels together. This way, we can operate on single channel luminance images and perform the computation at a quarter of the full
resolution cost and with improved signal-to-noise ratio. To estimate
local information about strength and direction of gradients, we use
gradient structure tensor analysis [Bigün et al. 1991; Harris and
Stephens 1988]:
Ωb =

I
2
x
Ix Iy
Ix Iy I
2
y

, (3)
where Ix and Iy are the local image gradients in horizontal and
vertical directions, respectively. The image gradients are computed
by finite forward differencing the luminance in a small, 3 × 3 color
window (giving us four different horizontal and vertical gradient
values). Eigenanalysis of the local structure tensor Ωb gives two
orthogonal direction vectors e1, e2 and two associated eigenvalues
λ1, λ2. From this, we can construct the kernel covariance as:
Ω =

e1 e2


k1 0
0 k2
 e
T
1
e
T
2

, (4)
where k1 and k2 control the desired kernel variance in either edge
or orthogonal direction. We control those values to achieve adaptive super-resolution and denoising. We use the magnitude of the
structure tensor’s dominant eigenvalue λ1 to drive the spatial support of the kernel and the trade-off between the super-resolution
and denoising, where λ1
λ2
is used to drive the desired anisotropy of
the kernels (Figure 8). The specific process we use to compute the
final kernel covariance can be found in the supplemental material
along with the tuning values. Since Ω is computed at half of the
Bayer image resolution, we upsample the kernel covariance values
through bilinear sampling before computing the kernel weights.
5.2 Motion Robustness
Reliable alignment of an arbitrary sequence of images is extremely
challenging ś because of both theoretical [Robinson and Milanfar
2004] and practical (available computational power) limitations.
Even assuming the existence of a perfect registration algorithm,
changes in scene and occlusion can result in some areas of the
photographed scene being unrepresented in many frames of the
ACM Trans. Graph., Vol. 38, No. 4, Article 28. Publication date: July 2019.