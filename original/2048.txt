Learning physics-based locomotion skills is a dicult problem, leading
to solutions that typically exploit prior knowledge of various forms. In
this paper we aim to learn a variety of environment-aware locomotion
skills with a limited amount of prior knowledge. We adopt a two-level
hierarchical control framework. First, low-level controllers are learned that
operate at a ne timescale and which achieve robust walking gaits that
satisfy stepping-target and style objectives. Second, high-level controllers
are then learned which plan at the timescale of steps by invoking desired
step targets for the low-level controller. The high-level controller makes
decisions directly based on high-dimensional inputs, including terrain maps
or other suitable representations of the surroundings. Both levels of the
control policy are trained using deep reinforcement learning. Results are
demonstrated on a simulated 3D biped. Low-level controllers are learned for
a variety of motion styles and demonstrate robustness with respect to forcebased disturbances, terrain variations, and style interpolation. High-level
controllers are demonstrated that are capable of following trails through
terrains, dribbling a soccer ball towards a target location, and navigating
through static or dynamic obstacles.
CCS Concepts: â€¢ Computing methodologies â†’ Animation; Physical
simulation; Control methods; Reinforcement learning;
Additional Key Words and Phrases: physics-based character animation, motion control, locomotion skills
1 INTRODUCTION
Physics-based simulations of human skills and human movement
have long been a promising avenue for character animation, but it
has been dicult to develop the needed control strategies. While
the learning of robust balanced locomotion is a challenge by itself,
further complexities are added when the locomotion needs to be
used in support of tasks such as dribbling a soccer ball or navigating
among moving obstacles. Hierarchical control is a natural approach
towards solving such problems. A low-level controller (LLC) is desired at a ne timescale, where the goal is predominately about
balance and limb control. At a larger timescale, a high-level controller (HLC) is more suitable for guiding the movement to achieve
longer-term goals, such as anticipating the best path through obstacles. In this paper, we leverage the capabilities of deep reinforcement
learning (RL) to learn control policies at both timescales. The use
of deep RL allows skills to be dened via objective functions, while
enabling for control policies based on high-dimensional inputs, such
as local terrain maps or other abundant sensory information. The
use of a hierarchy enables a given low-level controller to be reused
in support of multiple high-level tasks. It also enables high-level
controllers to be reused with dierent low-level controllers.
Our principal contribution is to demonstrate that environmentaware 3D bipedal locomotion skills can be learned with a limited
amount of prior structure being imposed on the control policy. In
support of this, we introduce the use of a two-level hierarchy for
deep reinforcement learning of locomotion skills, with both levels
of the hierarchy using an identical style of actor-critic algorithm. To
the best of our knowledge, we demonstrate some of the most capable
dynamic 3D walking skills for model-free learning-based methods,
i.e., methods that have no direct knowledge of the equations of
motion, character kinematics, or even basic abstract features such
as the center of mass, and no a priori control-specic feedback
structure. Our method comes with its own limitations, which we
also discuss.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 41. Publication date: July 2017.
41:2 â€¢ Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel van de Panne
2 RELATED WORK
Modeling movement skills, locomotion in particular, has a long history in computer animation, robotics, and biomechanics. It has also
recently seen signicant interest from the machine learning community as an interesting and challenging domain for reinforcement
learning. Here we focus only on the most closely related physicsbased work in computer animation and reinforcement learning.
Physics-based Character Control: A recent survey on physicsbased character animation and control techniques provides a comprehensive overview of work in this area [Geijtenbeek and Pronost
2012]. An early and enduring approach to controller design has been
to structure control policies around nite state machines (FSMs)
and feedback rules that use a simplied abstract model or feedback law. These general ideas have been applied to human athletics
and running [Hodgins et al. 1995] and a rich variety of walking
styles [Coros et al. 2010; Lee et al. 2010; Yin et al. 2007]. Many
controllers developed for physics-based animation further use optimization methods to improve controllers developed around an
FSM-structure, or use an FSM to dene phase-dependent objectives
for an inverse dynamics optimization to be solved at each time step.
Policy search methods, e.g., stochastic local search or CMA [Hansen
2006], can be used to optimize the parameters of the given control
structures to achieve a richer variety of motions, e.g., [Coros et al.
2011; Yin et al. 2008], and ecient muscle-driven locomotion [Wang
et al. 2009]. Policy search has been successfully applied directly
to time-indexed splines and neural networks in order to learn a
variety of bicycle stunts [Tan et al. 2014]. An alternative class of
approach is given by trajectory optimization methods, which can
compute solutions oine, e.g., [Al Borno et al. 2013], that can be
adapted for online model-predictive control [HÃ¤mÃ¤lÃ¤inen et al. 2015;
Tassa et al. 2012], or that can compute optimized actions for the
current time-step using quadratic programming, e.g., [de Lasa et al.
2010; Macchietto et al. 2009]. Wu and PopoviÄ‡ [2010] proposed a
hierarchical framework that incorporates a footstep planner and
model-predictive control for bipedal locomotion across irregular
terrain. To further improve motion quality and enrich the motion
repertoire, data-driven models incorporate motion capture examples
in constructing controllers, most often using a learned or modelbased trajectory tracking method [da Silva et al. 2008; Liu et al. 2016,
2012; Muico et al. 2009; Sok et al. 2007].
Reinforcement Learning for Simulated Locomotion: The neuroanimator work [Grzeszczuk et al. 1998] stands out as an early demonstration of a form of direct policy gradient method using a recurrent
neural network, as applied to 3D swimming movements. Recent
years have seen a resurgence of eort towards tackling reinforcement learning problems for simulated agents with continuous state
and continuous action spaces. Many of the example problems consist of agents in 2D and 3D physics-based simulations that learn
to swim, walk, and hop. Numerous methods have been applied to
tasks that include planar biped locomotion or planar swimming:
trajectory optimization [Levine and Abbeel 2014; Levine and Koltun
2014; Mordatch and Todorov 2014]; trust region policy optimization
(TRPO) [Schulman et al. 2015]; and actor-critic approaches [Lillicrap
et al. 2015; Mnih et al. 2016; Peng and van de Panne 2016].
Progress has recently been made on the harder problem of achieving 3D biped locomotion using learned model-free policies without
the use of a priori control structures. Tackling this problem de novo,
i.e., without any hints as to what walking looks like, is particularly
challenging. Recent work using generalized advantage estimation
together with TRPO [Schulman et al. 2016] demonstrates sustained
3D locomotion for a biped with ball feet. Another promising approach uses trajectory optimization methods to provide supervision
for a recurrent neural network to generate stepping movements
for 3D bipeds and quadrupeds [Mordatch et al. 2015], and with
the nal motion coming from an optimization step rather than directly from a forward dynamics simulation. Hierarchical control
of 3D locomotion control has been recently proposed [Heess et al.
2016], and is perhaps the closest work to our own. Low-level, highfrequency controllers are rst learned during a pretraining phase, in
conjunction with a provisional high-level controller with access to
task-relevant information and that can communicate with the lowlevel controller via a modulatory signal. Once pretraining has been
completed, the low-level controller structure is xed and other highlevel controllers can then be trained. The 3D humanoid is trained
to navigate a slalom course, an impressive feat given the de novo
nature of the locomotion learning. In our work we demonstrate:(a)
signicantly more natural locomotion and the ability to walk with
multiple styles that can be interpolated; (b) locomotion that has
signicant (quantied) robustness; (c) the learning of controllers
for four high-level tasks that use high-dimensional input, i.e., the
ability to see the surroundings using ego-centric terrain maps; and
(d) learning of a soccer-dribbling biped controller. In support of this,
we use: a bilinear phase transform for the low-level controllers; the
use of one or several reference motions; optional style reward terms;
and the use of a two-step foot plan that the high-level controller
can communicate to the low-level controller. We note that reference
motions have been previously utilized to guide the training of neural network policies for 3D bipedal locomotion [Levine and Koltun
2013].
Learning of high-level controllers for physics-based characters
has been successfully demonstrated for several locomotion and
obstacle-avoidance tasks [Coros et al. 2009; Peng et al. 2015, 2016].
Alternatively, planning using a learned high-level dynamics model
has also been proposed for locomotion tasks [Coros et al. 2008].
However, the low-level controllers for these learned policies are still
designed with signicant human insight, and the recent works of
Peng et al. are demonstrated only for planar motions.
Motion Planning: Motion planning is a well-studied problem,
which typically investigates how characters or robots should move
in constrained environments. For wheeled robots, such problem can
usually be reduced to nding a path for a point robot [Kavraki et al.
1996]. Motion planning for legged robots is signicantly more challenging due to the increased degrees of freedom and tight coupling
with the underlying locomotion dynamics. When quadrupeds are
equipped with robust mobility control, a classic A
âˆ— path planner
can be used to compute steering and forward speed commands to
the locomotion controller to navigate in real-world environment
with high success [Wooden et al. 2010]. However, skilled balanced
motions are more dicult to achieve for bipeds and thus they are
ACM Transactions on Graphics, Vol. 36, No. 4, Article 41. Publication date: July 2017.
DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning â€¢ 41:3
Fig. 2. System Overview
harder to plan and control [Kuner et al. 2005]. Much of the work in
robotics emphasizes footstep planning, e.g., [Chestnutt et al. 2005],
with some work on full-body motion generation, e.g., [Grey et al.
2016]. Possibility graphs are proposed [Grey et al. 2016] to use highlevel approximations of constraint manifolds to rapidly explore the
possibility of actions, thereby allowing lower-level motion planners
to be utilized more eciently. Our hierarchical planning framework
and the step targets produced by the HLC are partly inspired by this
previous work from humanoid robotics.
Motion planning in support of character animation has been studied for manipulation tasks [Bai et al. 2012; Yamane et al. 2004] as well
as full-body behaviours. The full-body behaviour planners often
work with kinematic motion examples [Lau and Kuner 2005; Lee
and Lee 2004; PettrÃƒÄ¾ et al. 2003]. Planning for physics-based characters is often achieved with the help of abstract dynamic models
in low-dimensional spaces [Mordatch et al. 2010; Ye and Liu 2010].
A hybrid approach is adopted in [Liu et al. 2012] where a high-level
kinematic planner directs the low-level dynamic control of specic
motion skills.
3 OVERVIEW
An overview of the DeepLoco system is shown in Figure 2. The
system is partitioned into two components that operate at dierent
timescales. The high-level controller (HLC) operates at a coarse
timescale of 2 Hz, the timescale of walking steps, while the low-level
controller (LLC) operates at 30 Hz, the timescale of low-level control
actions such as PD target angles. Finally, the physics simulation
is performed at 3 kHz. Together, the HLC and LLC form a twolevel control hierarchy where the HLC processes the high-level
task goals Ğ´H and provides the LLC with low-level intermediate
goals Ğ´L that direct the character towards fullling the overall task
objectives. When provided with an intermediate goal from the HLC,
the LLC coordinates the motion of the characterâ€™s various joints in
order to fulll the intermediate goals. This hierarchical partitioning
of control allows the controllers to explore behaviours spanning
dierent spatial and temporal abstractions, thereby enabling more
ecient exploration of task-relevant strategies.
The inputs to the HLC consist of the state, sH , and the highlevel goal, Ğ´H , as specied by the task. It outputs an action, aH ,
which then serves as the current goal Ğ´L for the LLC. sH provides
both proprioceptive information of the characterâ€™s conguration
as well as exteroceptive information about its environment. In our
framework, the high level action, aH , consists of a footstep plan for
the LLC.
The LLC receives the state, sL, and an intermediate goal, Ğ´L, as
specied by the HLC, and outputs an action aL. Unlike the high-level
state sH ,sL consists mainly of proprioceptive information describing
the state of the character. The low-level action aL species target
angles for PD controllers positioned at each joint, which in turn
compute torques that drive the motion of the character.
The actions from the LLC are applied to the simulation, which in
turn produces updated states sH and sL by extracting the relevant
features for the HLC and LLC respectively. The environment then
also provides separate reward signals rH and rL to the HLC and
LLC, reecting progress towards their respective goals Ğ´H and Ğ´L.
Both controllers are trained with a common actor-critic learning
algorithm. The policy (actor) is trained using a positive-temporal
dierence update scheme modeled after CACLA [Van Hasselt 2012],
and the value function (critic) is trained using Bellman backups.
4 POLICY REPRESENTATION AND LEARNING
Let Ï€(s,Ğ´) : S Ã— G â†’ A represent a deterministic policy, which
maps a state s âˆˆ S and goal Ğ´ âˆˆ G to an action a âˆˆ A, while a
stochastic policy Ï€(s,Ğ´, a) : SÃ—GÃ—A â†’ R represents the conditional
probability distribution of a given s and Ğ´, Ï€(s,Ğ´, a) = p(a|s,Ğ´). For
a particulars and Ğ´, the action distribution is modeled by a Gaussian
Ï€(s,Ğ´, a) = G(Âµ(s,Ğ´), Î£), with a parameterized mean Âµ(s,Ğ´) and xed
covariance matrix Î£. Each policy query in turn samples an action
from the distribution according to
a = Âµ(s,Ğ´) + N, N âˆ¼ G(0, Î£) (1)
generated by applying Gaussian noise to the mean action Âµ(s,Ğ´).
While the covariance Î£ = diaĞ´({Ïƒi }) is represented by manuallyspecied values {Ïƒi } for each action parameter, the mean is represented by a neural network Âµ(s,Ğ´|Î¸) with parameters Î¸. Large values
of {Ïƒi } can cause excessively noisy motions, which are prone to
falling, while small values can lead to slow learning. We found that
setting âˆš
Ïƒi to approximately 10% of the allowed range of values for
each joint to be eective in practice.
During training, a stochastic policy enables the character to explore new actions that may prove promising, but the addition of
exploration noise can impact performance at runtime. Therefore,
at runtime, a deterministic policy, which always selects the mean
action Ï€(s,Ğ´) = Âµ(s,Ğ´), is used instead. The choice between a stochastic and deterministic policy can be denoted by the addition of a
binary indicator variable Î» âˆˆ {0, 1}
a = Âµ(s,Ğ´) + Î»N (2)
where 1 indicates a stochastic policy with added exploration noise,
and 0 a deterministic policy that always selects the mean action. During training, Ïµ-greedy exploration can be incorporated by randomly
enabling and disabling exploration noise according to a Bernoulli
distribution Î» âˆ¼ Ber(Ïµ), where Ïµ represents the probability of action
exploration by applying noise to the mean action.
In reinforcement learning, the objective is often to learn an optimal policy Ï€
âˆ—
that maximizes the expected long term cumulative
reward J(Ï€), expressed as the discounted sum of immediate rewards
rt âˆˆ R with discount factor Î³ âˆˆ [0, 1].
J(Ï€) = Er0,r1,...rT
[r0 +Î³r1 + ... +Î³
T
rT |Ï€] (3)
ACM Transactions on Graphics, Vol. 36, No. 4, Article 41. Publication date: July 2017.
41:4 â€¢ Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel van de Panne
where T is a horizon that may be innite. The reward function
rt = r(st
,Ğ´t
, at ) provides the agent with feedback regarding the
desirability of performing action at at state st given goal Ğ´t
. The
reward function is therefore an interface through which users can
shape the behaviour of the agent by assigning higher rewards to
desirable behaviours, and lower rewards to less desirable ones. If
Ï€ is modeled as a parametric function with parameters Î¸, then the
expected cumulative reward can be re-expressed as J(Î¸), and the
goal of learning Ï€
âˆ—
can be formulated as nding the optimal set of
parameters Î¸
âˆ—
Î¸
âˆ— = arg max
Î¸
J(Î¸) (4)
Policy gradient methods are a popular family of algorithms for
solving this class of problems [Sutton et al. 2000]. These methods
perform gradient ascent on the objective using empirical estimates
of the policy gradient OÎ¸
J(Î¸), i.e. the gradient of J(Î¸) with respect
to the policy parameters Î¸. This class of methods lies at the heart of
our framework.
Algorithm 1 illustrates the common learning algorithm for both
the LLC and HLC. For the purpose of learning, the characterâ€™s experiences are summarized by tuples Ï„i = (si
,Ğ´i
, ai
,ri
,s
0
i
, Î»i), recording
the start state, goal, action, reward, next state, and application of
exploration noise for each action performed by the character. The
tuples are stored in an experience replay memory D and used to
update the policy. Each policy is trained using an actor-critic framework, where a policy Ï€(s,Ğ´, a|Î¸Âµ ) and value functionV (s,Ğ´|Î¸v ), with
parameters Î¸Âµ and Î¸v , are learned in tandem. The value function
is trained to predict the expected cumulative reward of following
the policy starting at a given state s and goal Ğ´. To update the value
function, a minibatch of n tuples {Ï„i } are sampled from D and used
to perform a Bellman backup
yi â† ri +Î³V (s
0
i
,Ğ´i
|Î¸v ) (5)
Î¸v â† Î¸v + Î±v

1
n
Ã•
i
OÎ¸v
V (si
,Ğ´i
|Î¸v )(yi âˆ’V (si
,Ğ´i
|Î¸v ))!
(6)
with Î±v being the critic stepsize. The learned value function is then
used to update the policy. Policy improvement is performed using
a CACLA-style positive temporal dierence update [Van Hasselt
2012]. Since the policy gradient as dened above is for stochastic
policies, policy updates are performed using only tuples with added
exploration noise (i.e. Î»i = 1).
Î´i â† ri +Î³V (s
0
i
,Ğ´i
|Î¸v ) âˆ’V (si
,Ğ´i
|Î¸v ) (7)
if Î´i > 0 :
Î¸Âµ â† Î¸Âµ + Î±Âµ

1
n
OÎ¸Âµ
Âµ(si
,Ğ´i
|Î¸Âµ )Î£
âˆ’1
(ai âˆ’ Âµ(si
,Ğ´i
|Î¸Âµ ))
(8)
where Î±Âµ is the actor stepsize. Equation 8 can be interpreted as
a stochastic gradient ascent step along an estimate of the policy
gradient for a Gaussian policy.
5 LOW-LEVEL CONTROLLER
The low-level controller LLC is responsible for coordinating joint
torques to mimic the overall style of a reference motion while satisfying footstep goals and maintaining balance. The reference motion is
ALGORITHM 1: Actor-Critic Algorithm Using Positive Temporal Dierence Updates
1: Î¸Âµ â† random weights
2: Î¸v â† random weights
3: while not done do
4: for step = 1, ..., m do
5: s â† start state
6: Ğ´ â† goal
7: Î» â† Ber(Ïµt )
8: a â† Âµ(s, Ğ´ |Î¸Âµ ) + Î»N, N âˆ¼ G(0, Î£)
9: Apply a and simulate forward one step
10: s
0 â† end state
11: r â† reward
12: Ï„ â† (s, Ğ´, a, r, s
0
, Î»)
13: store Ï„ in D
14: end for
15: Update value function:
16: Sample minibatch of n tuples {Ï„i = (si
, Ğ´i
, ai
, ri
, s
0
i
, Î»i )} from D
17: for each Ï„i do
18: yi â† ri + Î³V (s
0
i
, Ğ´i
|Î¸v ) âˆ’ V (si
, Ğ´i
|Î¸v )
19: end for
20: Î¸v â† Î¸v + Î±v

1
n
Ã
i OÎ¸v V (si
, Ğ´i
|Î¸v )(yi âˆ’ V (si
, Ğ´i
|Î¸v ))
21: Update policy:
22: Sample minibatch of n tuples {Ï„j = (sj
, Ğ´j
, aj
, rj
, s
0
j
, Î»j)} from D
where Î»j = 1
23: for each Ï„j do
24: Î´j â† rj + Î³V (s
0
j
, Ğ´j
|Î¸v ) âˆ’ V (sj
, Ğ´j
|Î¸v )
25: if Î´j > 0 then
26: 4aj â† aj âˆ’ Âµ(sj
, Ğ´j
|Î¸Âµ )
27: Î¸Âµ â† Î¸Âµ + Î±Âµ

1
n OÎ¸Âµ
Âµ(sj
, Ğ´j
|Î¸Âµ )Î£
âˆ’14aj

28: end if
29: end for
30: end while
represented by keyframes that specify target poses at each timestep
t. The LLC is queried at 30Hz , where each query provides as input the
Fig. 3. le: The character state features consist of the positions of each link
relative to the root (red arrows), their rotations, linear velocities (green
arrows), and angular velocities. right: The terrain features consist of a
2D heightmap of the terrain sampled on a regular grid. All heights are
expressed relative to height of the ground immediately under the root of
the character. The heightmap has a resolution of 32x32 and occupies an
area of approximately 11x11m.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 41. Publication date: July 2017