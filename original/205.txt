Electrodermal activity (EDA) is indicative of psychological processes related to human cognition and emotions. Previous research has studied many methods for extracting EDA features; however, their appropriateness for emotion recognition has been tested using a small number of distinct feature sets and on different, usually small, data sets. In the current research, we reviewed 25 studies and implemented 40 different EDA features across time, frequency and time-frequency domains on the publicly available AMIGOS dataset. We performed a systematic comparison of these EDA features using three feature selection methods, Joint Mutual Information (JMI), Conditional Mutual Information Maximization (CMIM) and Double Input Symmetrical Relevance (DISR) and machine learning techniques. We found that approximately the same numbers of features are required to obtain the optimal accuracy for the arousal recognition and the valence recognition. Also, the subject-dependent classification results were significantly higher than the subject-independent classification for both arousal and valence recognition. Statistical features related to the Mel-Frequency Cepstral Coefficients (MFCC) were explored for the first time for the emotion recognition from EDA signals and they outperformed all other feature groups, including the most commonly used Skin Conductance Response (SCR) related features.
SECTION 1Introduction
Robust information about the emotional state of a user is key to provide an empathetic experience during Human-Machine Interaction [1] or Human-Robot Interaction [2], [3]. In this regard, one important psychophysiological source of information is user's Electrodermal Activity (EDA), which refers to the conductivity of the skin, usually measured in palmar sites, and which can reflect changes in cognitive and emotional processes, such as cognitive effort or emotional arousal [4]. EDA signal, a non-stationary signal, is an aggregate of two different components: a tonic component, which accounts for the general levels of the conductivity of the skin, and whose levels vary slowly over time, and a phasic component, which manifests as sharper peaks over the tidal drift of the tonic EDA, which in general result from momentary sympathetic activation when arousing stimuli are present, although they also occur spontaneously in some individuals [4], [5], [6]. The tonic component of EDA is commonly called skin conductance level (SCL), while the short-term phasic responses are called skin conductance responses (SCRs). Thus, the EDA signal typically shows relatively rapid increments in its levels, followed by much slower decrements back to the baseline level (see Fig. 1). EDA mean levels usually range between 2 and 20μS, and vary within a range between 1 and 3μS for different individuals; the typical rise time from a valley to a peak is about 1 to 3 seconds and the typical recovery time of half of the SCR amplitude is between 2 and 10 seconds [4].


Fig. 1.
Example EDA signal: Adapted from [4].

Show All

Due to its low cost and easy-to collect nature, EDA measurement has been commonly applied in the context of psychological research. Recently, researchers have begun to explore the adequacy of machine learning algorithms for predicting user's mental states from EDA when interacting with diverse technologies. In this sense, EDA measurement may be especially useful when users have some kind of limitation at providing self-reported information, as is the case in a wide range of health conditions (e.g., [2], [3]). However, to date, there has not been a systematic exploration of the predictive power of using different combinations of EDA features. The goal of the current research is to contribute in filling this gap by examining the performance of a broad set of EDA features for emotion recognition.

The remainder of this article is structured as follows: Section 2 reviews feature extraction and feature selection methods used for emotion classification from EDA. In Section 3, we describe the methods used for extraction, selection, and classification of the EDA features along with the employed public dataset, and the processing of EDA signal for features comparison. We provide the results in Section 4, and they are discussed in Section 5. Finally, Section 6 concludes the article by summarizing this work while highlighting its limitation and proposing future steps.

SECTION 2Related Work
Emotion recognition from EDA has been commonly used for the assessment of user's experience in a variety of contexts such as recreational and serious games [7], driving [8], or patient-robot interaction [9]. Previous research has explored the predictive power of a diverse set of EDA features of different types, including time domain, frequency domain, and time-frequency domain features.

2.1 EDA Features
2.1.1 Time Domain Features
Regarding time domain features, some of the most usually considered features are statistical parameters of the signal during a relatively long recordings, including features such as signal mean value, standard deviation, kurtosis, or skewness (e.g., [10], [11], [12]). In other cases, researchers have focused on event-related features of EDA. Event related features refer to the attributes of the short-term responses, a few seconds after the presentation of a certain stimulus (e.g., images or sounds), such as the presence or absence of a SCR. In this sense, SCRs can be automatically detected in longer time windows and features can be extracted from them. This involves the definition of thresholds, in order to restrict the analysis only to non-negligible responses, and discarding those small changes in the signal that does not reach the thresholds and therefore cannot be considered as SCRs. In this sense, a traditional threshold for SCR amplitude has been 0.05μS [5]. Examples of event-related aspects of EDA considered in previous studies are SCR amplitude, SCR peaks count, mean SCR rise time, or the sum of SCR areas [8], [11], [13], [14], [15].

Other types of time domain features that have been successfully applied for emotion recognition in other types of physiological signals, such as Higher Order Crossing (HOC) features [16], [17], or Hjorth features from EEG signals [18], [19], might also have predictive potential in the case of EDA. However, to our knowledge, these features have not been previously applied to EDA signals.

2.1.2 Frequency Domain Features
Compared to time-domain features, fewer research has focused on the predictive power of EDA features related to the frequency domain, although the transient characteristics of the EDA signal can be better understood by analyzing the frequency domain representation of the signal. Indeed, the frequency domain analysis has shown superior capability for the gradient component's detection of individual SCR over traditional amplitude analysis [20]. Fast Fourier Transform (FFT), Short-time Fourier Transform (STFT) and power spectral density (PSD) estimation using Welch's method have been the algorithms most commonly used to obtain the frequency domain representation of the signal. Due to the different rate of physiological processes, EDA signals vary significantly with the frequency [21]. Hence, frequency oscillations of EDA signals can be divided into different frequency subbands to analyze it in more detailed manner [10], [21]. Indeed, previous research has considered statistical aspects (variance, range, signal magnitude area, skewness, kurtosis, harmonics summation) and spectrum power of five frequency bands, as well as their minimum, maximum, and variance [8], [10], [15], [21], [22], [23].

2.1.3 Time-Frequency Domain Features: Wavelets
Since EDA exhibits non-stationary behavior, wavelets have been found suitable for modeling EDA activity [24].

Discrete Wavelet Transform. When the wavelets are discretely sampled, the wavelet transform is known as Discrete Wavelet Transform (DWT). Denoised DWT wavelet coefficient features have been used for emotional state classification in patient-robot interaction [9].

Stationary Wavelet Transform. On the other hand, Stationary Wavelet Transform (SWT) also present some advantages for the analysis of EDA. SWT is redundant, linear and hence shift invariant in comparison to the DWT [25]. SWT also provides better sampling rates in the low-frequency bands compared with a standard DWT [25]. The SWT technique has been successfully applied to denoising of EDA signals with high efficacy and less computational complexity [26].

2.1.4 Mel-Frequency Cepstrum Features
The EDA signal can be characterized by a sequence of overlapping rapidly varying phasic SCRs overlying a slowly varying tonic activity (i.e., SCL). This superposition complicates the proper decomposition of the Skin Conductance (SC) data and hence limits the ability of classical methods for the assessment of SCRs [5]. Sudomotor nerve activity can be considered as a driver of EDA and it consists of a sequence of mostly distinct impulses (i.e., sudomotor nerve bursts). These bursts trigger the specific impulse response (i.e., SCRs) and thereby the SC can be modeled by driver-impulse response (IR) convolution. As a result of this process, SC can be represented by [27]
SC=SCtonic+SCphasic=(Drivertonic+Driverphasic)∗IR.(1)
View Source

In this model, SC is considered the output of the skin system driven by an excitation sequence of sudomotor nerve bursts, but such convolution of the response and the drivers cannot be easily separated in the time domain. Cepstrum analysis (CA) is an important technique for analyzing similar models of the speech signal [28].

The cepstrum of a discrete-time signal is the inverse discrete-time fourier transform of the logarithm of the magnitude of the discrete-time Fourier transform (DTFT) of the signal and is given by:
c[n]=12π∫+π−πlog|X(eiω)|eiωndω,(2)
View Source

where X(eiω) is the DTFT of the signal.

CA has been used successfully to isolate the basic waveform and the excitation function of the physiological signals such as electrocardiogram [29], electroencephalogram [30] and even EDA [21]. In [21], while analyzing EDA signals using CA, it was concluded that CA could be useful for analysis of superimposed EDA signals given its ability to magnify small amplitude variations. It suggests possible advantages of using Mel-frequency cepstral coefficients (MFCC), a new type of cepstrum representation based on the weighted cepstrum distance measures [31], that is widely established for many pattern recognition problems related to speech signals [28], as a feature vector of EDA signals. However, to our knowledge, this has not been addressed in previous studies.

In summary, several studies have explored the predictive potential for emotion classification of a broad set of EDA features; however, no previous study has tackled a systematic comparison of them, and the reliability of the recognition results remains challenging. Moreover, processing high dimensional data demands significant computational and space complexity. Therefore, extracting emotion information from high dimensional EDA data can be challenging, especially if the processing is to be done online. Furthermore, many of the EDA features can be irrelevant for the emotion classification or they might be redundant. Hence, it is important to automatically identify meaningful smaller subsets of these EDA features to achieve efficient emotion recognition from EDA signals, which stresses the need for using effective feature selection methods.

2.2 Feature Selection Methods
Most of the studies on EDA features have either not employed any algorithm for feature selection (FS) or have just applied data reduction techniques such as principal component analysis (PCA) prior to the classification [9], [21]. PCA is not a beneficial transformation for pattern recognition problems as the objective of PCA is not related to the automated target recognition [32]. Also, the recognition reported with PCA cannot be easily generalized to a general set-up [32]. FS methods can be generally divided into classifier-dependent (‘wrapper’ and ‘embedded’ methods), and classifier-independent (‘filter’ methods) categories [33]. Wrapper and embedded methods are computationally expensive and both use quite a strict model structure assumptions and hence may produce classifier specific feature subsets. In contrast, filter methods produce generic feature subsets, as they are model independent. Filter methods also take into account how potentially useful a feature or feature subset may be when used in a classifier [33]. In [33], the authors provide a comprehensive review of information based filter FS algorithms. Furthermore, they propose three desirable characteristics of an information-based selection criterion, which are [33]:

Whether it includes reference to a conditional redundancy term?

Whether it balances the relevance and the redundancy terms?

Whether it uses a low dimensional approximation?

The research conducted in [33] found that only three FS algorithms, Joint Mutual Information (JMI), Conditional Mutual Information Maximization (CMIM), and Double Input Symmetrical Relevance (DISR), show these desirable characteristics of an information-based selection criterion, and, therefore, we focus on these three methods.

2.2.1 Joint Mutual Information
Joint Mutual Information criterion [34] provides the best trade-off in terms of accuracy, stability, and flexibility [33]. JMI focuses on increasing complementary information between features. The JMI score for feature Xk is
JJMI(Xk)=∑Xj∈SI(XkXj;Y).(3)
View Source

This is the information between the target Y and a joint random variable XkXj, defined by pairing the candidate Xk with each feature Xj previously selected. The candidate feature Xk that maximizes this mutual information is chosen and added to the feature subset S. JMI offers two significant advantages [34]:

JMI can distinguish among features even when all of them have same mutual information (MI) and

JMI can eliminate the redundancy in the features when one feature is a function of other features.

2.2.2 Conditional Mutual Information Maximization
Conditional Mutual Information Maximization [35] is a versatile filter measure and is a good measure for general FS problems [36]. The CMIM criterion for each feature Xk is measured as
JCMIM(Xk)=minXj∈S[I(Xk:Y|Xj)].(4)
View Source

I(Xk:Y|Xj) is the conditional mutual information between candidate Xk and the target Y given Xj. The candidate feature Xk that minimizes this conditional mutual information is chosen and added to the feature subset S which means that it carries information about the class that is not already captured by the features in the selected set. CMIM can properly identify truly redundant features and noisy features, and gives preference to informative, uncorrelated features [36].

2.2.3 Double Input Symmetrical Relevance
Double Input Symmetrical Relevance [37] is a normalized variant of JMI. DISR takes into consideration the variable complementary and a lower bound on the mutual information. It uses the following modification of the JMI criterion
JDISR(Xk)=∑Xj∈SI(XkXj;Y)H(XkXjY).(5)
View SourceRight-click on figure for MathML and additional features.

DISR criterion motivates the selection of a complementary variable of an already selected one with a higher probability.

2.3 Problem Statement
It is not known which features are most appropriate for emotion recognition from EDA and previous works have made limited contributions on a systematic comparison of EDA features. Hence, the research goals of our work are as follows:

To provide an inclusive review of EDA features for emotion recognition.

To provide a first-ever systematic comparison of features on one database using multiple FS methods.

To identify the most significant EDA features for emotion recognition.

SECTION 3Methods
In order to tackle the above research goals, we described a wide set of features that can be extracted from EDA signals in different domains. We then extracted these features from EDA signals of a publicly available, annotated dataset, the AMIGOS dataset [38], and implemented a systematic FS and comparison process to determine the most significant features. The whole process is described below.

3.1 Features Set
A literature review was conducted in order to determine a complete set of features available for emotion classification from EDA, as well as other types of features that have proven effective for this purpose when applied to other types of psychophysiological signals, even if they have not been applied to EDA. We retrieved the relevant publications by performing an online search of the PubMed, IEEE Xplore, ScienceDirect research databases for studies published in English until date. The keywords used for the search were combinations of relevant words such as EDA, Electro-dermal Activity, Emotion etc. Based on the their titles and the abstracts, we manually identified the studies discussing EDA features and selected only original studies published in journals or conferences. We created a data extraction spreadsheet for the collection of different EDA features from these studies. Furthermore, we discarded the duplicate references by manually reviewing the feature list and the references. Finally, 25 papers were used for exploring and implementing the 40 different EDA related features. The complete list of selected features is presented in Table 1.

TABLE 1 EDA Features Used in the Research
Table 1- 
EDA Features Used in the Research
3.2 AMIGOS Database
The AMIGOS dataset [38] is a publicly available dataset containing, among other multimodal data, measures of EDA from two experiments: one experiment in which participants watched short (<250s) emotional videos (40 participants), and another experiment in which participants (alone or in groups of four) watched longer (>14min) videos able to elicit diverse emotional states (37 participants, 17 of them in individual setting and 20 in groups). Among others, the dataset includes annotations for emotional arousal and valence of participants in both experiments, provided by three external observers that visually inspected frontal videos of participants’ faces during the viewing, and provided an annotation for every 20 second segment of the viewing. A total of 12,580 video segments were annotated this way (340 segments by 37 participants, from both the experiments with short and long videos). The arousal and valence scales used for these annotations were continuous and ranged from −1 (low arousal or valence) to +1 (high arousal or valence), and the agreement between annotators was very good for both the variables (Cronbach's α = 0.96 for arousal, and Cronbach's α = 0.98 for valence).

3.3 Feature Extraction
Table 2 describes the dimensionality of each feature type for each participant. A feature matrix was generated from the EDA data of 340 annotated segments for each of the two annotated variables (arousal and valence) for each participant. Features were extracted from all the three domains leading to a total of 621 features. We z-normalized the features to have mean 0 and to have a standard deviation of 1. The problem of singularities may occur for FS methods [17]. Hence, we removed all almost identical features which produced a correlation coefficient higher than .98. In what follows, we explain the process that we used to extract each type of features.

TABLE 2 Feature Vector Dimension

3.3.1 Extraction of Time Domain Features
Event Related Features. We used the process described in [8], [11], [13] to extract the following event-related features: SCR Amplitude, SCR peak count, mean SCR amplitude, mean SCR rise time, sum of SCR peaks amplitudes, sum of SCR rise times, area under the curve of SCRs and sum of SCR areas.

Statistical Features. Based on the methods described in [10], [44], we extracted the following statistical features:power, mean, standard deviation, kurtosis, skewness, mean of the 1st difference, and mean of the 2nd difference.

Hjorth Features. In addition to the commonly used event-related and statistical features from EDA, we also extracted three Hjorth features, following the methods described in [47]. In this regard, for a given time series signal X(n) of the EDA signal, we extracted:

Activity (A):
AX=∑n=1N(X(n)−μ)2(6)
View Source

Mobility (M):
MX=var(X(n)˙)var(X(n))−−−−−−−−−−−−−−−−√(7)
View Source

Complexity (C):
CX=M(X(n)˙)M(X(n)),(8)
View SourceRight-click on figure for MathML and additional features.

where μ is the mean, var(X(n)) is the variance, var(X(n)˙) is the derivative of the variance of the EDA signal X(n).
HOC. We also extracted the HOC features of given EDA time-series signals using the method described in [48]. When a specific sequence of filters is applied iteratively to a time series Zt, the corresponding sequence of zero-crossings is known as HOC sequence and is represented by
Ik{Zt}=∇k−1Zt.(9)
View SourceRight-click on figure for MathML and additional features.

∇ is iteratively applied backward difference operator ∇Zt≡Zt−Zt−1 and the order k is selected in range of 1−50 that would result in the maximum classification rate of the given signals [16]. The desired simple HOC are then obtained by counting the symbol changes Dk in Ik{Zt}. HOCs are then used to construct the feature vector FVHOC as follows:
FVHOC=[D1,D2,…,DL],1<L≤J.(10)
View Source

In order to determine the most suitable order for the HOC features, we performed an iterative classification step. We computed the classification rate for several orders of HOC features using stratified 10-fold cross-validation. The classification of the HOC features data was performed by means of quadratic discriminant analysis with diagonal covariance estimates (i.e., Naive Bayes). Fig. 2 shows the plot of the HOC order versus corresponding classification rate for arousal recognition and for valence recognition. As it is clear from the plot, HOC gets its highest classification rate at order value 5 for both the arousal and the valence recognition, hence we chose the HOC order as 5 for the AMIGOS dataset.

Fig. 2. - 
HOC order versus classification rate.
Fig. 2.
HOC order versus classification rate.

Show All

3.3.2 Extraction of Frequency Domain Features
In order to extract the frequency domain features, the recommended frequency range of EDA signal (0.05−0.50Hz) was split into five bands following suggestions from [10], [21]. The features extracted from the resulting frequency domain representation of the EDA signal are a set of statistical features (variance, range, signal magnitude area, skewness, kurtosis, harmonics summation) and the spectrum power of five frequency bands, their minimum, maximum, and variance [8], [10], [15], [21], [22], [23].

3.3.3 Extraction of Time-Frequency Domain Features
Discrete Wavelet Transform. Based on a previous work [9], we used the detail coefficients (Equation (13)) of the DWT signal as EDA features. Wavelet analysis of a signal consists of translations (k∈Z) of the father wavelet ϕ(t) (scaling function) and dilations and translations (k∈Z, j∈Z) of the mother wavelet ψ(t). The wavelet series representation of a signal x(t) is then
x(t)=∑jcj,kϕk(t)+∑j∑kdj,kψj,k(t),(11)
View Sourcewhere (cj,k) are the approximation coefficients and (dj,k) are the detail coefficients of the wavelet coefficient set and are calculated as follows:
cj,k=⟨x(t),ϕj,k(t)⟩(12)
View Source
dj,k=⟨x(t),ψj,k(t)⟩.(13)
View SourceRight-click on figure for MathML and additional features.

In DWT, wavelet acts as a band-pass filter from signal processing point of view, where the scaling and wavelet functions serve as low pass (h[n], Equation (14)) and high pass filters (g[n], Equation (15)), respectively
ϕ(t)=∑nh[n]2–√ϕ(2t−n)(14)
View SourceRight-click on figure for MathML and additional features.
ψ(t)=∑ng[n]2–√ψ(2t−n),(15)
View Source

where
g[n]=h[2N−1−n].(16)
View Source

When applied, the above decomposition halves the temporal resolution and doubles the frequency resolution. The above procedure can be applied iteratively for multilevel decomposition of the signal. Wavelet decomposition levels correspond to different frequency bands and this correspondence is based on the sampling frequency of the signal.

Stationary Wavelet Transform. To obtain the SWT of the given EDA signal, we used the process described in [25]. The basic DWT algorithm described above can be modified to obtain SWT of given EDA signal. We applied the low-pass and the high-pass filters as described for DWT, but without any decimation. We extracted the following SWT based features:

Wavelet Energy: E(j)=∑i=1jDi(n)2

Wavelet Decomposition Energy: The percentage of energy corresponding to the approximation and to the details coefficients.

Wavelet Entropy: E(j)=−∑i=1jDi(n)2log(Di(n)2)

Wavelet Root Mean Square (RMS): RMS(j)=1N∑i=1j|Di(n)|2−−−−−−−−−−−−⎷

where Di(n) are the detail coefficients of the EDA signal.

3.3.4 Extraction of MFCC Features
The process applied for extracting MFCC features is illustrated in Fig. 3 and is explained below:

Fig. 3. - 
MFCC feature extraction.
Fig. 3.
MFCC feature extraction.

Show All

The EDA signal was filtered to remove the motion artifacts using the sophisticated SWT based filtering method [26].

A Hamming window was applied on the filtered EDA signal to enable the analysis over short window durations. Given a sampling frequency of f, the recommended value of frame size (N) is, N=2×f and of overlapping window duration (M) is, M=0.5×f. The sampling frequency for EDA signals from the AMIGOS dataset is 128 Hz. While the frequency of the EDA signals is very high, the latency of the gradual changes in principle EDA components against an elicited stimulus is between 1.0 and 3.0 s [4]. Keeping this in mind, we took the overlapping window duration M as 0.5 seconds, so as to cover the starting segment of responses for specific stimuli occurring at 1.0 seconds. Moreover, since we are analyzing a 20 seconds EDA segment of the dataset, we could not select a value of 2×f=256 seconds for the value of N. Hence, we decided to make 10 equal windows for the EDA signal and thereby chose the value of N as 2 seconds.

For each window, the frequency spectrum was obtained by applying a FFT.

The frequency spectrum was then mapped onto the Mel-scale through Mel-filters to obtain the Mel-spectrum. The Mel-scale mapping from the actual frequency f can be given as follows:
fmel=2595log10(1+f700).(17)
View Source

Next we took the log of the Mel-spectrum values.

Finally, CA was required as per Equation (2) on the Mel-spectrum to obtain the MFCC features. While applying Equation (2), the logarithm computation was committed, since it was computed in the previous processing step 5. Also, instead DTFT, the discrete cosine transform was applied because the absolute value of the Mel-spectrum is real and symmetric. Hence, for a windowed frame of EDA signal the final MFCC coefficients C[n] are computed by
C[n]=1R∑r=1Rlog(MFm[r]).cos[2πR(r+12)n],(18)
View SourceRight-click on figure for MathML and additional features.

where R is the number of Mel-filters, MFm[r] is the Mel-spectrum of the frame r.

For the purposes of MFCC extraction, we selected only the last 13 components, since the rest of them carry little information. For a given EDA signal, the number of MFCC coefficients obtained via the above process resulted in 13×Numframes, where Numframes=ceil(((length(EDAsignal)−N)/M)).

3.4 Feature Selection
As explained in Section 2.2, among the several information theoretic FS methods available only JMI, CMIM and DISR satisfy the three desirable characteristics of an information based selection criterion [33]. Hence, we applied JMI, CMIM and DISR for selecting meaningful features from EDA. We used more than one FS method because the results obtained from multiple methods are more robust [49]. We applied each of these three FS methods for each participant individually and collectively for ALL participants after performing the scaling and discretization on the extracted EDA features. The formulas for applying the JMI, CMIM and DISR methods are given by the Equations (3), (4) and (5) respectively. We evaluated the classifier performance by selecting the top n features as given by the FS algorithm. We varied the value of n between 5 to 200. We chose the upper value of 200 to check approximately one-third values of the feature vector (total number of features are 621 as given in Table 2).

3.5 Classification
Since we intended to analyze temporal evolution of arousal and valence from EDA signals, we employed the external affect annotations of the AMIGOS dataset. We aggregated the ratings from all the three annotators to form a single rating value of more significant meaning for each annotated video segment. Since the valence and arousal scales were continuous and ranged from −1 (low arousal) to 1 (high arousal), we categorized rating values to LOW or HIGH valence/arousal depending on whether the ratings were less or greater than the mean value of the scale 0.00.

Based upon the above categorization, 9,886 samples were assigned to class LOW and 2,694 to class HIGH for arousal labeling, and 9,566 were assigned to class LOW and 3,014 to class HIGH for valence labeling. This provided an imbalanced two-class dataset. Hence, we used adaptive synthetic (ADASYN) sampling method to improve class balance towards equally-sized classes. ADASYN uses a weighted distribution for different minority class samples according to their level of difficulty in learning and it generates more synthetic data for minority class samples that are harder to learn in comparison to those minority samples that are easier to learn [50]. Adaptively generating synthetic data samples in this manner reduces the bias introduced by the imbalanced data distribution.

The overall methodology of our recognition system is illustrated in Fig. 4. The sample set was partitioned for each of the 37 participants individually (containing 340 samples for each participant) and collectively for ALL participants using the whole dataset (12,580 samples). The ADASYN method was applied for each data partition to remove the class imbalance and the data was then divided into the ratio of 70:15:15 for training, validation and testing respectively. We evaluated each of the listed FS methods for each subject individually based on the classification accuracy with a support vector machine (SVM) classifier and radial basis function (RBF) kernel. We employed grid search and 3-fold cross validation method to determine the optimal regularization parameter C and also to determine the free parameter γ of the Gaussian RBF. We applied SVM since it has been reported to provide the best classification accuracy during the recognition of affective states from physiological cues [51], [52], [53].

Fig. 4. - 
Block diagram of supervised classification system.
Fig. 4.
Block diagram of supervised classification system.

Show All

SECTION 4Results
4.1 Optimal Number of Features
Table 3 presents the optimal accuracies for arousal recognition, average F1 score (F1, average of score for both classes) and the optimal number of features in tabular form across all the 37 subjects and the three FS methods. The optimal accuracies are the highest accuracies obtained at the optimal number of features. The table also presents the average results for all 37 subjects across all three FS methods, as well as the subject-independent (ALL) classification results. Table 4 shows the same information for valence recognition.

TABLE 3 Accuracy, F1 Score, Optimal Number of Features for Different FS Methods Listed for Each Subject for Arousal Recognition

TABLE 4 Accuracy, F1 Score, Optimal Number of Features for Different FS Methods Listed for Each Subject for Valence Recognition

We tested if the values of F1-scores provided by the different selection algorithms are significantly higher than 0.5 (at the p<.05 level). In the case of arousal detection, the JMI (M=0.58;SD=0.17;t(36)=2.78;p=.009), the CMIM (M=0.63;SD=0.1;t(36)=7.92;p<.001), and the DISR (M=0.64;SD=0.11;t(36)=7.49;p=<.001) algorithms provided mean values of F1-scores that were significantly higher than 0.5. Also in the case of valence, the JMI (M=0.61;SD=0.1;t(36)=6.49;p<.001), CMIM (M=0.59;SD=0.13;t(36)=4.32;p<.001), and DISR (M=0.63;SD=0.09;t(36)=8.85;p<.001) algorithms provided values significantly higher than 0.5 for F1-scores.

In order to provide insight on how the performance of the subject-dependent classification differs from the subject-independent classification, we also compared the results of subject-independent (ALL) classification with subject-dependent (37 individual subjects) across all three FS methods for arousal and valence recognition. In the case of arousal, compared to the subject-independent (ALL) recognition, subject dependent recognition provided significantly higher accuracy (JMI: t(36)=17.1888,p<0.001, CMIM: t(36)=18.4018,p<0.001, DISR: t(36)=20.5805,p<0.001), while having a significantly lower optimal number of features (JMI: t(36)=−8.184,p<.001, CMIM: t(36)=−5.0724,p<.001, DISR: t(36)=−11.0016,p<.001).

A similar pattern was observed for valence recognition, including a higher accuracy (JMI: t(36)=17.389,p<0.001, CMIM: t(36)=18.3873,p<0.001, DISR: t(36)=21.8212,p<0.001), and lower optimal number of features (JMI: t(36)=−10.3046,p<.001, CMIM: t(36)=−4.7272,p<.001, DISR: t(36)=−6.31,p<.001) for the subject-dependent recognition compared to the subject-independent (ALL) recognition.

A series of paired t-tests were conducted in order to determine if there was a significant difference between the average values (across the three FS algorithms) of accuracy for the prediction of arousal and valence, and the results showed that there was not, t(36)=1.79;p=.08.

Finally, the outputs of the three FS algorithms in terms of accuracy and optimal number of features were compared using a series of repeated measures (within-subjects), analysis of variance (ANOVAs), in order to examine if there is a significant difference between them. Regarding arousal, no significant difference (at p<.05 level) was found between the three algorithms regarding their accuracy (F(2,105)=0.1;p=.91), or optimal number of features (F(2,105)=1.07;p=.35). Neither in the case of valence there were significant differences in terms of the accuracy (F(2,105)=0.01;p=.99), or optimal number of features (F(2,105)=0.39;p=.68). In other words, there is no evidence that one of the algorithms outperforms the others.

4.2 Significant Features
In order to identify the most frequently selected features by the different FS methods and the features with the most significant performance for arousal and valence recognition, we followed an approach similar to the one described in [17]. It involves the computation of the relative frequency of each of the feature types, which is obtained via the following process:

We first created a histogram of the feature occurrence, given the features selected for the optimal number of features across all the participants and the FS methods.

Then, in order to take into account the random assignment of the features, each bin of the histogram was normalized by dividing the occurrence of each feature type by the feature's cardinality (e.g., 5 in the case of HOC).

Finally, while averaging, these relative frequencies were weighted across the FS methods by multiplying with the achieved classification accuracy.

Employing the statistics above, the relative frequency of the feature types varies between 0 and 1 and the most significant features score higher than the non-significant features for emotion recognition. Figs. 5 and 6 show the weighted relative frequency of each feature type for arousal and valence recognition, respectively. The most frequently selected features group for arousal recognition is the MFCC statistical features. Statistical features related to SCR of the EDA signal in the time domain and the band power related features in the frequency domain had the second best performance among all the feature groups. The single best performing feature among all the employed features is the AUC feature followed by SMA and the signal energy feature. The standard deviation or variance of the SCR signal along with the derivative features of the SCR signal also yield a better performance compared to other time domain statistical features. In general, the statistical features of the SCR signal perform better than the SCR related features in the time domain. It is worth noting that the time domain statistical features, band power and the frequency domain statistical features show a higher variance than the other feature groups. It indicates that particular features in these groups are more informative than others. These feature types are AUC, signal energy and SMA in the time domain statistical features, band power and the frequency domain statistical features, respectively. The variance among the MFCC statistical features is lower in comparison to the three other feature types. It means that all the features in this type are significant. The least frequently selected features are the coefficients of the wavelet and the coefficients of the MFCC, as they have low weighted relative occurrence scores.

Fig. 5. - 
Weighted relative frequency of each EDA feature type for arousal recognition. Black horizontal bar represents the mean for the group, and the vertical dashed black line represents the standard deviation for each group.
Fig. 5.
Weighted relative frequency of each EDA feature type for arousal recognition. Black horizontal bar represents the mean for the group, and the vertical dashed black line represents the standard deviation for each group.

Show All

Fig. 6. - 
Weighted relative frequency of each EDA feature type for valence recognition. Black horizontal bar represents the mean for the group, and the vertical dashed black line represents the standard deviation for each group.
Fig. 6.
Weighted relative frequency of each EDA feature type for valence recognition. Black horizontal bar represents the mean for the group, and the vertical dashed black line represents the standard deviation for each group.

Show All

We found that the feature usage for valence recognition follows the same trend as that of arousal recognition. The most frequently selected feature group for valence recognition is the MFCC statistical features, also followed by the time domain and the band power related features in the frequency domain. The single best performing feature among all the employed features is also the AUC feature followed by SMA and the signal energy feature.

SECTION 5Discussion
We did not find any significant difference in the performance of the three FS methods employed in this study, but all of them yielded a high average classification accuracy and F1-scores for both arousal and valence recognition. The results obtained in Tables 3 and 4 indicate that a high number of EDA features (∼95 for arousal recognition and ∼96 for valence recognition) are required to obtain the optimal accuracy. This is a significant finding and have not been reported by any earlier study. The similarity between the optimal number of features for arousal and valence recognition suggests that the computational complexity of the recognition for these two dimensions of emotions is quite similar.

While EDA is generally more related to emotional arousal[4], we found similar recognition performances for both arousal and valence. In this regard, the results reported by [38] (e.g., F-1 scores of 0.528 for valence and 0.541 for arousal for all videos, using a different set of signals and features), although showing a slightly better performance for arousal, also show that the classification performance for the two variables does not diverge considerably, which is aligned with our results. We found that there was a significant and quite high correlation between the annotated arousal and valence scores of AMIGOS dataset, r=0.56,p<.001, which may help to explain why the recognition performance for both arousal and valence is similar.

Regarding the significance of specific features, it is worth noting that SCR features that have been widely used in the literature yielded low weighted relative occurrence scores in our study. Our results revealed that, among all SCR features, the amplitude of the SCR peaks is the most significant one for recognition of both arousal and valence from EDA. We also demonstrated that the commonly used rise time feature does not play an important role in this regard. Whereas AUC of the EDA signal has not been usually exploited in the literature, as a single feature it yields a high performance for emotion recognition from EDA signals. This is also true for SMA and signal energy features of the EDA signal. Furthermore, the most significant finding of our study is the performance of the statistical features related to MFCC, which outperformed all other feature types across all the three domains. Taken together, these results suggest that EDA features with the highest potential for emotion classification have been either never used in previous studies (MFCC feature) or have been used with limited exploitation (AUC and SMA features).

Superior subject-dependent classification accuracies, using a lower number of features, were obtained in comparison to the subject-independent classification. The big difference in such classification results is not surprising. It is known that different individuals usually have different physiological responses to the same stimuli [54]. In addition, non-emotional individual contexts vary in a complex manner among different subjects [11]. If the subject is known in advance to the system or if the system can undergo a learning phase for each subject prior to the classification, then the emotion classification can be done in a user-dependent way. We believe that this is one of the biggest challenges of real-time emotion recognition and it goes beyond the scope of this article.

SECTION 6Conclusion
In this paper, we reviewed and implemented 40 different EDA features for emotion recognition suggested by 25 studies. We analyzed the significance and the suitability of different EDA feature across time, frequency and time-frequency domains using three FS methods, JMI, CMIM and DISR, and employing machine learning techniques on the publicly available AMIGOS dataset. All the three FS methods indicated use of ∼95 features on an average for arousal recognition and of ∼96 features on an average for valence recognition. The results reported an average accuracy of 85.75 percent (F1-score:0.63) for arousal recognition and an average accuracy of 83.9 percent (F1-score:0.61) for valence recognition. Also, the subject-dependent classification results were significantly higher than the subject-independent classification for both arousal and valence recognition. Statistical MFCC features along with the AUC and SMA features outperformed the commonly used SCR features of the EDA signal.

Our research not only describes the significance of a more exhaustive set of features than previous studies, but also points out the utility and informative properties for emotion classification of a new set of features from EDA signal that have been largely neglected in previous research, such as MFCC features. By doing so, it opens venues for the future development of new emotion recognition systems based on EDA with higher accuracy and minimizing its computational cost, which is key for the development of emotion detection applications that may work in real time.

One limitation of the current study is that we only relied in a dimensional model of emotions (i.e., the conceptualization of emotions in terms of valence and arousal), since the annotations of the AMIGOS dataset were provided in terms of arousal and valence. It may also be interesting to examine the predictive power of EDA features for classifying discrete emotions like joy, sadness, fear, or surprise, and thus this is an aspect that should be addressed in future studies. Another limitation of the present work is that we tested only a specific context for emotion elicitation (i.e., the context used in the AMIGOS study, that is, watching emotional videos). It is possible that different emotional contexts (e.g., a stressing job interview) may produce different patterns in EDA signal that might be better captured by focusing on different features of the signal.

Our results also stress the need to account for individual differences in subjects with different psychophysiological profiles, which tend to have different physiological responses for the same stimuli [54]. Failure to address this individual variability can negatively affect the classification performance of emotional state, which is shown by the results of subject-independent classification in our results. To tackle this issue, a general model for emotion classification could be trained with a sufficiently higher number of subjects, and then such a model could be fine tuned with the baseline values of the new users.