Performance is a critical challenge in mobile image processing. Given a reference imaging pipeline, or even human-adjusted pairs of images, we seek
to reproduce the enhancements and enable real-time evaluation. For this,
we introduce a new neural network architecture inspired by bilateral grid
processing and local affine color transforms. Using pairs of input/output images, we train a convolutional neural network to predict the coefficients of a
locally-affine model in bilateral space. Our architecture learns to make local,
global, and content-dependent decisions to approximate the desired image
transformation. At runtime, the neural network consumes a low-resolution
version of the input image, produces a set of affine transformations in bilateral space, upsamples those transformations in an edge-preserving fashion
using a new slicing node, and then applies those upsampled transformations
to the full-resolution image. Our algorithm processes high-resolution images on a smartphone in milliseconds, provides a real-time viewfinder at
1080p resolution, and matches the quality of state-of-the-art approximation
techniques on a large class of image operators. Unlike previous work, our
model is trained off-line from data and therefore does not require access to
the original operator at runtime. This allows our model to learn complex,
scene-dependent transformations for which no reference implementation is
available, such as the photographic edits of a human retoucher.
CCS Concepts: • Computing methodologies → Computational photography; Image processing;
Additional Key Words and Phrases: real-time image processing, deep learning, data-driven methods, convolutional neural networks
1 INTRODUCTION
The high resolution of images and videos produced by contemporary
cameras and mobile devices puts significant performance pressure
on image processing algorithms, requiring sophisticated code optimization by skilled programmers. While systems contributions
have sought to facilitate the implementation of high-performance
executables, e.g. [Hegarty et al. 2014; Mullapudi et al. 2016; RaganKelley et al. 2012], they require programmer expertise, their runtime
cost still grows with the complexity of the pipeline, and they are only
applicable when source code is available for the filters. Additionally, because image enhancement is subjective, it is often desirable
to learn an enhancement model directly from human adjustments,
e.g. [Bychkovsky et al. 2011]. To this end, we present a machine
learning approach where the effect of a reference filter, pipeline, or
even subjective manual photo adjustment is learned by a deep network that can be evaluated quickly and with cost independent of the
reference’s complexity. We focus on photographic enhancements
that do not spatially warp the image or add new edges, e.g. [Aubry
et al. 2014; Hasinoff et al. 2016].
ACM Transactions on Graphics, Vol. 36, No. 4, Article 118. Publication date: July 2017.
118:2 • Michaël Gharbi, Jiawen Chen, Jonathan T. Barron, Samuel W. Hasinoff, and Frédo Durand
We share the motivation of prior work that seeks to accelerate
“black box” image processing operations, either by using a remote
server, e.g. [Gharbi et al. 2015] or by processing a low-resolution
image and then using the low-resolution output to approximate a
high-resolution equivalent [Chen et al. 2016]. For some operations,
these approaches can achieve large speedups but they suffer from
significant limitations: the underlying image processing operation
must be somewhat scale-invariant (Figure 9), and must be fast to
evaluate at low resolution. In addition, these techniques rely on
the availability of an explicit reference implementation, and therefore cannot be used to learn an implicitly-defined operation from a
database of human annotated input/output pairs.
Many deep learning architectures have been used for image-toimage transformations, e.g. [Isola et al. 2016; Liu et al. 2016; Long
et al. 2015; Xu et al. 2015; Yan et al. 2016]. However, most prior
work incur a heavy computational cost that scales linearly with
the size of the input image, usually because of the large number of
stacked convolutions and non-linearities that must be evaluated at
full resolution. This general form allows for flexible models to be
learned, but this expressivity comes at a price: such architectures are
orders of magnitude too slow for real-time viewfinder applications,
requiring seconds to process a 1 megapixel image on the best desktop
GPUs—more than 1000× slower than our proposed model (2 ms on
GPU). Our speedup is enabled by specifically targeting photographic
transformations, which are often well-approximated with linear
operations in bilateral space [Chen et al. 2016], and accordingly
learning our model in this space.
We present a new network architecture that is capable of learning a rich variety of photographic image enhancements and can
be rapidly evaluated on high-resolution inputs. We achieve this
through three key strategies: 1) We perform most predictions in a
low-resolution bilateral grid [Chen et al. 2007], where each pixel’s
x,y coordinates are augmented with a third dimension which is a
function of the pixel’s color. To do this, we introduce a new node for
deep learning that performs a data-dependent lookup. This enables
the so-called slicing operation, which reconstructs an output image
at full image resolution from the 3D bilateral grid by considering
each pixel’s input color in addition to its x,y location. 2) We follow
previous work which has observed that it is often simpler to predict the transformation from input to output rather than predicting
the output directly e.g., [Chen et al. 2016; Gharbi et al. 2015; Shih
et al. 2013]. This is why our architecture is designed to learn, as
an intermediate representation, a local affine color transformation
that will be applied to the input through a new multiplicative node.
3) While most of our learning and inference is performed at low
resolution, the loss function used during training is evaluated at
full resolution, which causes the low-resolution transformations we
learn to be directly optimized for their impact on high-resolution
images.
Taken together, these three strategies (slicing, affine color transform, and full-resolution loss) allow us to perform the bulk of our
processing at a low resolution (thereby saving substantial compute
cost) yet reproduce the high-frequency behavior of the reference
operator.
We demonstrate the expressiveness of our model on a benchmark of 7 applications including: approximating published image
filters [Aubry et al. 2014; Hasinoff et al. 2016], reverse-engineering
black-box Photoshop actions, and learning the retouching style of
photographers [Bychkovsky et al. 2011] from a set of manually corrected photographs. Our technique produces output whose quality
is comparable to or better than previous work, while being more
widely applicable by not requiring some reference implementation
of the image operation being approximated, being end-to-end learnable from input/output image pairs, and running in real-time on
mobile hardware. The forward pass of our network takes 14 ms to
process a full screen resolution 1920 × 1080 image on a Google Pixel
phone, thereby enabling real-time viewfinder effects at 50 Hz.
2 RELATED WORK
Though image enhancement algorithms have been the focus of a
great deal of research, most sophisticated algorithms are too expensive to be evaluated quickly on mobile devices, which is where the
vast majority of digital images are captured and processed. Because
of this, previous work has identified specific critical operations and
developed novel algorithms to accelerate them. For instance, Farbman et al. [2011] introduced convolution pyramids to accelerate
linear translation-invariant filters. Similarly, many approaches have
been proposed to accelerate bilateral filtering, due to the ubiquity of
edge-aware image processing [Adams et al. 2010; Chen et al. 2007;
Paris and Durand 2006; Tomasi and Manduchi 1998].
One way to accelerate an operator is to simply apply it at low resolution and upsample the result. A naïve upsampling will generally
lead to an unacceptably blurry output, but this issue can often be
ameliorated by using a more sophisticated upsampling technique
that respects the edges of the original image. Joint bilateral upsampling [Kopf et al. 2007] does this by using a bilateral filter on a
high-resolution guidance map to produce a piecewise-smooth edgeaware upsampling. Bilateral space optimization [Barron et al. 2015;
Barron and Poole 2016] builds upon this idea by solving a compact
optimization problem inside a bilateral grid, producing upsampled
results which are maximally smooth.
Gharbi et al. [2015] focus on learning the transformation from
input to output instead of the output itself. They approximate a
large class of complex, spatially-varying operators with a collection
of simple local models—a transform recipe—that is tailored to a given
input/output pair. The task of computing the operator and fitting
the recipe is offloaded to the cloud while the mobile device need
only apply the recipe, thereby saving time and energy. Similarly,
Chen et al. [2016] approximate an image operator with a grid of local
affine models in bilateral space, the parameters of which are fit to an
input/output pair in a manner resembling the guided filter [He et al.
2013]. By performing this model-fitting on a low-resolution image
pair, this technique enables real-time on-device computation. We
build upon this bilateral space representation, but rather than fitting
a model to approximate a single instance of an operator from a pair of
images, we construct a rich CNN-like model that is trained to apply
the operator to any unseen input. This bypasses the need for the
original operator at runtime and opens up the opportunity to learn
non-algorithmic transformations (i.e., hand-adjusted input/output
image pairs). This also allows us to optimize the affine coefficients
to model the operator running at full resolution, which is important
for filters that vary with scale (Figure 9).
ACM Transactions on Graphics, Vol. 36, No. 4, Article 118. Publication date: July 2017.
Deep Bilateral Learning for Real-Time Image Enhancement • 118:3
Neural networks for image processing. Recently, deep convolutional networks have achieved significant progress on low-level
vision and image processing tasks such as depth estimation [Eigen
et al. 2014], optical flow [Ilg et al. 2016], super-resolution [Dong
et al. 2014], demosaicking and denoising [Gharbi et al. 2016; Zhang
et al. 2016], image matting [Shen et al. 2016], colorization [Iizuka
et al. 2016], and general image-to-image “translation” tasks [Isola
et al. 2016]. Recent work has even explored learning deep networks
within a bilateral grid [Jampani et al. 2016] though this work does
not address our task of learning image transformations in that space,
and instead focuses on classification and semantic segmentation.
Some architectures have been trained to approximate a general
class of operators. Xu et al. [2015] develop a three-layer network
in the gradient domain to accelerate edge-aware smoothing filters.
Liu et al. [2016] propose an architecture to learn recursive filters
for denoising, image-smoothing, inpainting and color interpolation.
They jointly train a collection of recursive networks and a convolutional network to predict image-dependent propagation weights.
While some of this work can process low-resolution images on a
desktop GPU at interactive rates, they remain too slow for our application: real-time processing of high-resolution images on a mobile
device.
Automatic photo editing. Our model can be trained to automatically correct photographs from input/output image pairs provided
by a human retoucher. This is the task introduced by Bychkovsky et al.
[2011], who estimate global brightness/contrast adjustments that
characterize the personal style of 5 trained photographers. They
train a regression model with handcrafted features that capture both
low-level information and semantic content (e.g., faces) on a dataset
of 5000 raw images. Hwang et al. [2012] approach the problem with
a coarse-to-fine search for the best-matching scenes that takes more
than a minute for a 500×333 image. Kaufman et al. [2012] learn local
color and contrast manipulations from hard-coded features (faces,
blue skies, clouds, underexposed areas), running over 2 minutes for
a VGA image. More recently, Yan et al. [2016] use a compact pixelwise neural network and handcrafted features. Their network takes
1.5 s to process a 1 megapixel image (on top of the time needed for
object detection, dense image segmentation, and scene recognition
used in their features). Our model can learn similar global tonal adjustments and generalizes to more complex effects, including color
corrections and local edits, in addition to being much faster.
3 OUR ARCHITECTURE
We propose a new convolutional network architecture that can be
trained to perform fast image enhancement (Figure 2). Our model
is designed to be expressive, preserve edges, and require limited
computation at full resolution. It is fully end-to-end trainable and
runs in real-time at 1080p on a modern smartphone.
We perform most of the inference on a low-resolution copy ˜
I of the
input I in the low-res stream (Fig. 2, top), which ultimately predicts
local affine transforms in a representation similar to the bilateral
grid [Chen et al. 2016]. In our experience, image enhancements often
depend not only on local image features but also on global image
characteristics such as histograms, average intensity, or even scene
category. Therefore, our low-res stream is further split into a local
path and a global path. Our architecture then fuses these two paths
to yield the final coefficients representing the affine transforms.
The high-res stream (Fig. 2, bottom) works at full resolution and
performs minimal computation but has the critical role of capturing high-frequency effects and preserving edges when needed. For
this purpose, we introduce a slicing node inspired by bilateral grid
processing [Chen et al. 2007; Paris and Durand 2006]. This node performs data-dependent lookups in the low-resolution grid of affine
coefficients based on a learned guidance map. Given high-resolution
affine coefficients obtained by slicing into the grid with the fullresolution guidance map, we apply local color transforms to each
pixel to produce the final output O. At training time, we minimize
our loss function at full resolution. This means that the low-res
stream, which only processes heavily downsampled data, still learns
intermediate features and affine coefficients that can reproduce
high-frequency effects.
As a first approximation, one can think of our work as alleviating the need for the reference filter at runtime in Chen et al.’s
Bilateral Guided Upsampling [2016]. In a sense, we seek to predict
the affine color transform coefficients in the bilateral grid given
a low-resolution version of the image. However, there are several
key elements that go beyond this. First, the downsampling into the
bilateral grid is learned. Second, the guidance image is also learned
and not restricted to luminance. Finally, we apply the loss function
not on the affine coefficients, but on the final image at full resolution, which allows us to capture high-frequency effects and handle
operators that are not scale-invariant (Figure 9). We illustrate the
role of each component of our architecture with an ablation study
in Figures 3, 4, 5 and 7.
3.1 Low-resolution prediction of bilateral coefficients
The input ˜
I to the low-res stream has a fixed resolution 256 ×
256. It is first processed by a stack of strided convolutional layers
(S
i
)i=1,...,nS
to extract low-level features and reduce the spatial resolution. Then, in a design inspired by Iizuka et al. [2016], the last lowlevel features are processed by two asymmetric paths: the first path
(L
i
)i=1,...,nL
is fully convolutional [Long et al. 2015] and specializes
in learning local features that propagate image data while retaining
spatial information. The second path (G
i
)i=1,...,nG
uses both convolutional and fully-connected layers to learn a fixed-size vector
of global features (e.g. high-level scene category, indoor/outdoor,
etc.) with a receptive field covering the entire low-resolution image ˜
I. The outputs of the two paths, G
nG and L
nL , are then fused
into a common set of features F . A pointwise linear layer outputs a
final array A from the fused streams. We interpret this array as a
bilateral grid of affine coefficients (Section 3.2). Since we produce a
3D bilateral grid from a 2D image in a content-dependent fashion,
we can view the low-res stream as implementing a form of learned
splatting.
3.1.1 Low-level features. We first process the low-resolution
image S
0
:= ˜
I with a stack of standard strided convolutional layers
with stride s = 2 (Figure 2):
S
i
c
[x,y] = σ
*
.
,
b
i
c +
X
x
′
,y
′
,c
′
w
i
cc′

x
′
,y
′

S
i−1
c
′

sx + x
′
,sy + y
′
+
/
-
(1)
ACM Transactions on Graphics, Vol. 36, No. 4, Article 118. Publication date: July 2017.