Redirected walking techniques can enhance the immersion and visualvestibular comfort of virtual reality (VR) navigation, but are often limited
by the size, shape, and content of the physical environments.
We propose a redirected walking technique that can apply to small physical environments with static or dynamic obstacles. Via a head- and eyetracking VR headset, our method detects saccadic suppression and redirects
the users during the resulting temporary blindness. Our dynamic path planning runs in real-time on a GPU, and thus can avoid static and dynamic
obstacles, including walls, furniture, and other VR users sharing the same
physical space. To further enhance saccadic redirection, we propose subtle
gaze direction methods tailored for VR perception.
We demonstrate that saccades can significantly increase the rotation
gains during redirection without introducing visual distortions or simulator
sickness. This allows our method to apply to large open virtual spaces and
small physical environments for room-scale VR. We evaluate our system via
numerical simulations and real user studies.
CCS Concepts: • Human-centered computing → Virtual reality; • Computing methodologies → Perception;
Additional Key Words and Phrases: virtual reality, redirected walking, human
perception, saccade
1 INTRODUCTION
Room-scale virtual reality (VR) increases presence and decreases
discomfort caused by visual-vestibular inconsistency by allowing
the user to walk freely in a physical space [Usoh et al. 1999]. However, a direct one-to-one mapping from virtual to physical space is
impractical for most applications. Today’s room-scale experiences
either constrain the virtual space through scenario design or frequently interrupt the users and break their sense of presence by
requiring them to walk back to the center of the physical room or
consciously teleport in the virtual world. A major challenge for VR is
embedding a large virtual space within a small, irregular, multi-user
physical space while minimizing interruptions. The ideal solution
would create the perception of infinite walking in the virtual space
within a small, finite physical space.
Treadmills or other physical devices can address the infinite walking problem, but are undesirable for general applications because
they are expensive, bulky, and can compromise the user’s balance,
while also preventing free user movements such as kneeling and
jumping. The current state-of-the-art techniques for solving the
mapping problem using only a head-mounted display (HMD) are
redirected walking [Razzaque et al. 2001, 2002; Steinicke et al. 2010]
and warping [Dong et al. 2017; Sun et al. 2016]. These methods
create a distorted mapping of the virtual environment by applying to the world subtle rigid-body and nonlinear transformations,
respectively. These magnify the effective physical space, but stateof-the-art methods still require an unoccluded space of 36 m2
to
be simultaneously imperceptible and effective [Azmandian et al.
2015]. This is a significant step towards practical room-scale VR
for unconstrained scenarios, but it is still too large to accommodate many home and office rooms. We believe the main cause is
the perceptually-imposed limitation of traditional redirection systems that cannot respond to the real-time user and environmental
changes.
We present a novel, dynamic solution to the infinite walking problem. It is the first to be demonstrated as effective for physical areas as
small as 12.25 m2
. This significant advance beyond previous results
meets for the first time the standard for practicality: these bounds
match the recommended consumer HMD room-scale installation
bounds, e.g., for HTC Vive and Oculus Rift. Our key innovation is
redirecting the user much more aggressively, yet still imperceptibly,
by tracking rapid eye movements called saccades using a HMD
equipped with internal gaze-tracking cameras, and incorporating
guided navigation and planning based on the scenario.
Saccades are rapid eye movements, during which viewers are
momentarily blind in a phenomenon called saccadic suppression.
Saccades occur frequently, but our high-level visual system prevents conscious awareness of the blindness. The visual system also
essentially recalibrates its orientation after a saccade on the assumption that the world itself has not changed [Hopp and Fuchs 2004].
We exploit that assumption to change the virtual world imperceptibly and avoid predicted future collisions with physical objects. Our
method retains faithful visual and vestibular experiences across a
broader range of virtual and physical spaces than previous methods.
To further enhance the effectiveness of the technique, we also employ subtle gaze directions to opportunistically trigger additional
saccades, and a content-aware path planner to adapt to dynamic
environmental changes. Our main contributions are:
• An end-to-end redirected walking system based on saccadic
suppression, effective for consumer room-scale VR;
• A real-time path planning algorithm that automatically avoids
static and dynamic obstacles by responding to individuals’ eye
movements − our optimization links user behavior and physical
changes, considers possibilities of near future through real-time
sampling, and finds the best numerical solution for online camera manipulation;
• The use of subtle gaze direction (SGD) methods in VR to induce
more saccades for the system to exploit;
• Validation through simulations and real redirected walking scenarios with game-like tasks, such as search and retrieval.
2 RELATED WORK
2.1 Redirected Interaction in VR
Redirected interaction, such as walking [Dong et al. 2017; Hodgson
and Bachmann 2013; Razzaque 2005; Razzaque et al. 2001; Sun et al.
2016] and touching [Azmandian et al. 2016c; Cheng et al. 2017], has
received recent attention in the graphics and HCI community as
a technique that uses mapping and rendering methods to enhance
presence. It works by modifying what the user sees while they are
physically interacting with their surroundings [Azmandian et al.
2017]. Due to the dominance of vision over other senses, the user
perceives the physical interaction as being consistent to the visual
stimulus. This way, physical interactions can be redirected. In particular, redirected walking can influence the user’s walking path in
an imperceptible fashion, simulating larger virtual environments
within smaller physical ones and avoiding walls and obstacles.
Researchers have proposed two primary methods of redirected
walking: those that work by dynamically scaling user motion and
head rotation for the virtual camera [Azmandian et al. 2017; Razzaque et al. 2001, 2002; Steinicke et al. 2010] due to sensory conflicts
in virtual environments [Steinicke et al. 2008], and those that work
by warping the virtual scene [Dong et al. 2017; Sun et al. 2016].
Notwithstanding the specific technique, contemporary redirected
techniques assume that users are aware of the environment at all
times. The techniques do not consider perceptual masking effects
like saccades, blinks, and other perceptual suppressions. In this paper, we enhance redirected interaction by detecting masking effects
and amplifying redirection during the events without introducing
virtual scene warping. Concurrent work by Langbehn et al. [2018]
conducts perceptual experiments to measure translation and rotation thresholds during eye blinks to facilitate redirected walking.
In comparison, our method considers rotations but not translations
during eye saccades, in conjunction with subtle gaze direction and
GPU path planning for real-time performance.
2.2 Gaze-contingent Rendering in VR
Gaze-contingent graphics is a widely studied area with several applications in medicine, optometry, vision science, and computer
graphics [Duchowski et al. 2004; Reder 1973]. However, due to the
increasing availability of high-quality eye trackers [Vincent and
ACM Trans. Graph., Vol. 37, No. 4, Article 67. Publication date: August 2018.
Towards Virtual Reality Infinite Walking: Dynamic Saccadic Redirection • 67:3
Brannan 2017] as well as growing research into potential applications, gaze-contingent rendering has also gained popularity in
virtual and augmented reality. When used for foveated rendering,
gaze-contingent rendering helps improve visual quality without
performance compromises [Albert et al. 2017; Duchowski and Çöltekin 2007; Guenter et al. 2012; Levoy and Whitaker 1990; Luebke
and Hallen 2001; Patney 2017; Patney et al. 2016]. When used to
simulate high-dynamic range [Jacobs et al. 2015] or to provide focus cues [Duchowski et al. 2014], gaze-contingent graphics enable
new experiences on contemporary displays. Finally, eye-tracking
is a useful interaction tool for virtual environments [Pfeuffer et al.
2017]. Thus, eye-tracking support is foreseen in the next generation
commodity VR/AR devices. Our system employs eye-tracking to
determine occurrences of perceptual suppression for VR redirected
walking.
2.3 Saccadic and Blink Suppression
A saccade is the rapid eye movement that occurs when we change
fixation points. During normal viewing, saccades occur several times
a second, contain extremely fast motion (up to 900 ◦
/sec), and are
long (20–200 ms) compared to VR frame durations [Bahill et al. 1975],
although the speed at which they can be detected varies depending
upon the chosen algorithms [Andersson et al. 2017]. Saccades are
among many behaviors that trigger temporary perceptual suppression. Others include masking by patterns, tactile saccades [Ziat et al.
2010], and blinks [Ridder III and Tomlinson 1997]. While our system
for redirected walking could potentially extend to any of these, we
explicitly evaluate it under saccades in this paper.
Saccadic suppression (a.k.a. saccadic omission) of perception occurs before, during, and after each saccadic eye motion [Burr et al.
1994]. While the exact mechanism behind it is an area of active
research [Burr et al. 1994; Diamond et al. 2000; Ibbotson and Cloherty 2009], the characteristics are well-known [Matin 1974; McConkie and Loschky 2002; Ross et al. 2001]. Our system exploits
the particular documented phenomenon − suppression of image
displacement [Bridgeman et al. 1975; Li and Matin 1990].
A key property of visual saccades is that they are ballistic in
nature [Bahill et al. 1975] and their velocity profile and landing position can often be predicted mid-flight [Arabadzhiyska et al. 2017;
Han et al. 2013]. This, in addition to saccadic suppression lasting
for a short period after the saccade itself completes, suggests that
detecting saccades and altering rendering based on the detection
should be fairly tolerant of current VR eye-tracking-to-photon latency of around 35 ms [Albert et al. 2017]. Recent work established
reorientation and repositioning thresholds for VR during saccades
[Bolte and Lappe 2015] and blinks [Langbehn et al. 2016]. We leverage those established perceptual thresholds to build and evaluate a
redirected walking system.
2.4 Subtle Gaze Direction
Subtle gaze direction (SGD) uses image-space modulation to direct a
viewer’s gaze to a specific target [Bailey et al. 2009]. When applied
in peripheral regions these can direct attention without affecting
net perception of the scene. Previous work used SGD to trigger
controlled saccades to enhance visual search performance [McNamara et al. 2008, 2009] and as a narrative tool [McNamara et al.
2012]. Recent work suggests that SGD can drive user gaze in VR
experiences as well [Grogorick et al. 2017; Sridharan et al. 2015]. We
integrate SGD into our system to dynamically and subtly increase
the frequency of saccades, which we then exploit as opportunities
for imperceptible transformation of the world.
3 PILOT STUDY OF VISUAL SACCADES
The efficacy of redirection during saccadic suppression depends
on several factors, including frequency and duration of saccades,
perceptual tolerance of image displacement during saccadic suppression, and the eye-tracking-to-display latency of the system.
To quantify these, we have conducted a short pilot study with
six participants using an HTC Vive HMD with integrated SMI eyetracking. They were instructed to walk a pre-defined path in the
small “Van Gogh room” scene and search for six fixed task objects.
We recorded their gaze orientations (Figures 2e and 2f) and used the
method of adjustment to identify the angular rotation redirections.
Specifically, we tuned the rotation angles up/down until the participants could/could not recognize the difference between saccadic
redirection, head-only redirection, and walking without redirection
by answering “Yes, I noticed something in the camera orientation”
or “No, I do not. They are all normal and the same”.
We determined no participant could detect camera rotation less
than 12.6
◦
/sec (0.14◦
at 90 frames per second) when their gaze
velocity was above 180◦
/sec. We increase redirection for longer
saccades linearly, which is consistent with previous perceptual experiments [Bolte and Lappe 2015; Li and Matin 1990]. Bolte and
Lappe [2015] have shown that “participants are more sensitive to
scene rotations orthogonal to the saccade”. However, since our overall system computes across multiple frames (Section 5.2), saccade
directions may change within this period. To guarantee imperceptibility, we choose a conservative gain threshold assuming orthogonal
saccades.
We then augmented the data from our experiment with captured
head and gaze orientation recorded from a participant playing commercial VR arcade games NVIDIA VR Funhouse (Funhouse) , and
horror defense game The Brookhaven Experiment (Brookhaven), for
10 minutes each (Figure 2). While less controlled as experimental
settings, these represent the state of the art for VR presence, rendering quality, and entertainment tasks. They are more realistic
and less biased for evaluating the potential for redirected walking
than our specially-constructed lab scenario. For each frame in the
collected data, we used our previously measured gaze thresholds to
predict the maximum imperceptible redirection.
Over one-minute intervals, the proportion of redirected frames
varied between 2.43% and 22.58% in Funhouse, and between 10.25%
and 22.02% in Brookhaven. The average proportion of frames with
redirection was approximately 11.40% for Funhouse, and approximately 15.16% for Brookhaven, which can sufficiently provide 1.4
◦
/sec and 1.9 ◦
/sec angular gains. We conclude that the frequency
and distribution of redirection depend on the content, yet contain
significant extra gains due to saccadic suppression.
ACM Trans. Graph., Vol. 37, No. 4, Article 67. Publication date: August 2018.
67:4 • Sun, Patney, Wei, Shapira, Lu, Asente, Zhu, McGuire, Luebke, and Kaufman
(a) NVIDIA VR Funhouse
0 1 2 3 4 5
Time (sec)
0
200
400
600
800
1000
Gaze speed (deg/sec)
VR Funhouse
No saccade
Saccade
(b) Gaze plot for (a)
(c) The Brookhaven
0 1 2 3 4 5
Time (sec)
0
200
400
600
800
1000
Gaze speed (deg/sec)
Brookhaven
No saccade
Saccade
(d) Gaze plot for (c)
(e) The Van Gogh Room
0 1 2 3 4 5
Time (sec)
0
200
400
600
800
1000
Gaze speed (deg/sec)
Van Gogh Room (walking)
No saccade
Saccade
(f ) Gaze plot for (e)
Fig. 2. Saccade analysis for VR applications. We recorded head and gaze
data for a user playing two VR games and a simple scene with VR walking
to estimate the potential benefits from saccadic redirection. We plot five
seconds of angular gaze velocity for these applications, showing frames
that we detected as saccades (above the 180◦
/sec threshold visualized in
green lines). Section 2.4 describes our pilot study setup and analysis using
(e). Scene (e) courtesy of ruslans3d.
4 METHOD
Reorientation is a technique that modifies the user’s virtual camera to
decrease the likelihood of exiting the physical play area. Since minor
changes in the virtual camera during head rotation are generally
imperceptible, this helps provide richer experiences without the
user noticing the redirection. Our system also reorients the virtual
camera, but it does so not only during head rotations, but also
during, and slightly after, eye saccades. Similar to the case with head
rotation, small changes to the camera orientation during saccades
are imperceptible, and hence offer opportunities for introducing
more frequent and greater amounts of redirection.
Our redirected walking method consists of the following three
parts:
Saccade detection Use gaze tracking to detect saccades and identify opportunities to reorient the virtual camera for redirection (Section 4.1).
Dynamic path planning Use the saccade detection thresholds
and the physical space around the user to dynamically determine the best virtual camera orientation for redirection
(Sections 4.2 and 4.3).
Subtle gaze direction (SGD) Render temporally-modulated stimuli in a user’s visual periphery to induce visual saccades
(Section 4.4).
Algorithm 1. Overview of our approach. We perform saccade-aware redirection and dynamic path planning before each frame. We begin by detecting
saccade to determine the acceptable perceptual thresholds for virtual camera redirection. We then run our path planning optimization, amortized
over several frames. After redirection, we apply image/object-space SGD to
scene rendering.
1: PathPlanningState = Ready
2: ∆θ = 0
3: function RenderRedirected(t, M(t))
4: Ecurr = GetLatestEyePos
5: Hcurr = GetLatestHeadPose
6: Gcurr = CombineHeadGaze(Hcurr, Ecurr)
7: ∆д = MeasureAngle(Gcurr, Gprev)
8: ∆h = MeasureAngle(Hcurr, Hprev)
9: Γд = 0
10: Γh = 0
◃ Detect saccades
11: ∆t = GetFrameDeltaTime
12: if ∆д > 180 · ∆t then
◃ Gaze gain threshold
13: Γд = 12.6 · ∆t
14: end if
◃ Path planning (amortized over 2-5 frames)
15: if PathPlanningState is Ready then
16: Initialize optimization by sampling S using Equation (3)
17: PathPlanningState = Running
18: else if PathPlanningState is Running then
19: Perform iterations of planning optimizer (Equation (8))
20: if Optimization is done then
21: Update redirection angle ∆θ (Equation (8))
22: PathPlanningState = Ready
23: end if
24: end if
◃ Perform redirection
25: if ∆θ > 0 then
◃ Head rotation gain [Steinicke et al. 2010]
26: if (sдn(∆θ) = sдn(∆h)) then λ = 0.49 else λ = −0.2
27: Γh = λ · ∆h
28: ∆θt = sдn(∆θ) · min(∥Γh ∥ +



Γд



 , ∥∆θ ∥)
◃ From Equations (1) and (2)
29: M(t + 1) ← R(∆θt )M(t)
30: ∆θ = ∆θ − ∆θt
31: end if
◃ Subtle gaze direction (SGD) and rendering
32: if SGDMode is ObjectSpace then
33: Modulate material luminance of selected objects
34: end if
35: Draw current frame
36: if SGDMode is ImageSpace then
37: Modulate luminance of selected peripheral pixels
38: end if
39: Display rendered frame
40: Gprev = Gcurr
41: Hprev = Hcurr
42: end function
ACM Trans. Graph., Vol. 37, No. 4, Article 67. Publication date: August 2018.
Towards Virtual Reality Infinite Walking: Dynamic Saccadic Redirection • 67:5
Algorithm 1 summarizes the steps that constitute each frame of
our approach. During each frame, we first use current and previous
gaze orientation to detect visual saccades, identifying the opportunity for redirected walking. We then update our dynamic path
planning algorithm, which we amortize over 2–5 frames to maintain
real-time performance. After its final iteration, our path planning
algorithm returns a direction and magnitude of desired redirection.
If the current frame is a candidate for redirection, either due to an
ongoing saccade or head rotation, we modify the camera viewpoint
in the direction of desired redirection by a magnitude subject to
our perceptual limits. Finally, while rendering the frame, we add
subtle gaze direction − temporally-modulated stimuli in a user’s
peripheral vision to imperceptibly encourage visual saccades. We
can apply SGD stimuli in either object space or image space.
4.1 Saccade Detection for Camera Reorientation
Saccade detection. Once calibrated, our high-speed eye-tracker
is relatively noise-free. Thus we use a simple heuristic to determine whether users are currently making visual saccades. At the
beginning of each frame, we use the previous two gaze samples to
estimate the current angular velocity of the user’s gaze. If the angular velocity is greater than 180◦
/sec, we conclude that a saccade is
either currently ongoing or has recently finished.
In our implementation we use the average position of the user’s
left and right gaze locations. This helps reduce noise in detecting
location and in estimating velocity. More robust detection (e.g., Hidded Markov Model or Hidded Markov Model [Andersson et al. 2017])
are potential future research for lower-quality tracking devices.
Due to the latency of contemporary eye-trackers as well as VR
rendering and display pipelines, saccade detection generally lags
actual saccades by tens of milliseconds. However, since the duration
of visual saccades ranges from 20–200 ms and saccadic suppression
lasts for 100 ms after a saccade begins [McConkie and Loschky 2002;
Ross et al. 2001], we find that our detection is relatively tolerant of
tracking and rendering latency, especially for saccades with large
angular amplitude. Our pilot studies as described in Section 2.4
indicated that the empirically-determined threshold of 180◦
/sec
accounts for this tolerance.
Camera reorientation thresholds. When saccades are detected within
a frame, we slightly re-orient the virtual camera by up to 0.14◦
/
frame as described in Section 2.4. If we respect this threshold, our
path planning algorithm can successfully perform redirections with
meaningful direction and magnitude without alerting the user. Saccadic redirection can be combined with conventional head-only
reorientation. For the latter, we use the previously studied angular gain threshold within [−20%, 49%] [Steinicke et al. 2010] precalibrated within this range for individual users as some may have
lower detection thresholds than others [Grechkin et al. 2016]. Although rotation during head movement allows more redirection,
large head rotations are less frequent than large saccades, so we
expect an overall improvement by using both for redirected walking.
The saccadic detection threshold 180◦
/sec and gain speed 12.6
◦
/sec
were set through our pilot study (Section 2.4).
4.2 Dynamic Path Planning
The saccade-guided camera manipulation and subtle gaze direction
(SGD) facilitate VR redirected walking. However, to guide users
away from both stationary and moving obstacles, the system must
dynamically compute the virtual camera orientation in each frame.
Existing off-line mapping approaches [Dong et al. 2017; Sun et al.
2016] require slow pre-processing, incompatible with saccadic actions that happen dynamically and unpredictably in real time. We
would also like to avoid any visual distortion caused by virtual scene
warping and rely only on larger, rigid transformation gains enabled
by saccadic suppression. Thus, we present a real-time dynamic path
planning approach driven by perceptual factors (such as SGD), scene
properties (e.g. floor layouts and scene object placements), and GPU
parallelization.
Formulation. For a given frame t and a 2D virtual position x =
(x,y), we model the corresponding physical position u = (u,v)
using an affine transformation M between the virtual and physical
spaces:
u(x,t) = M(t) (x − xc (t)) + xc (t)
M = [R|T]
(1)
where xc (t) is the user’s current virtual space position. This formulation interprets x and u as the next virtual and real user positions
to allow optimization for the near future, such as avoiding obstacles.
The goal of the real-time path planner is to find the next frame’s
optimal translation T(t + 1) and rotation R(t + 1) components so
that the redirected walking path during saccades can guide users
away from boundaries and obstacles. In our initial investigations
we have found R to be much more effective than T with saccades
and head rotations, so we set T(t) = 0 to reduce the real-time,
multidimensional computation workload:
M(t + 1) ← 
cos(∆θ(t)) − sin (∆θ(t))
sin (∆θ(t)) cos(∆θ(t))

M(t) (2)
where ∆θ is the redirection angle to optimize for (Section 4.3).
Dynamic sampling. Inspired by [Dong et al. 2017; Sun et al. 2016],
we perform optimization via virtual scene samples. However, instead
of global uniform sampling, we dynamically allocate the sample
set S locally, adapting to the user’s position and orientation to
enhance optimization quality and speed. Specifically, we design an
importance-based real-time sampling mechanism emphasizing areas
that are (1) close to the user’s current position and (2) visible and
within the user’s current camera frustum, to predict possibilities in
the nearer future, as exemplified in Figure 3. To achieve fast results,
we created a closed-form formulation for the intuition above. The
importance is computed in the polar coordinates (r(x), θ(x)) of the
virtual space with x as the origin:
I(x) =

− erf
α
r
0
r(x) + α
r
1

+ α
r
2

×

exp 
−
(cos(θ(x) − θc ) − 1)
2
α
a
0

+ α
a
1

+ α
o
(3)
where erf(x) =
1√
π
∫ x
−x
e
−t
2
dt is the error function, θc is the user’s
current virtual camera direction, α
r
i ∈ {0,1,2}
and α
a
i ∈ {0,1}
are parameters fitting to a given space size, and α
o
is added to avoid zero
ACM Trans. Graph., Vol. 37, No. 4, Article 67. Publication date: August 201