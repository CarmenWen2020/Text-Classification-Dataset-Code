A key promise of Virtual Reality (VR) is the possibility of remote social
interaction that is more immersive than any prior telecommunication media.
However, existing social VR experiences are mediated by inauthentic digital
representations of the user (i.e., stylized avatars). These stylized representations have limited the adoption of social VR applications in precisely those
cases where immersion is most necessary (e.g., professional interactions and
intimate conversations). In this work, we present a bidirectional system that
can animate avatar heads of both users’ full likeness using consumer-friendly
headset mounted cameras (HMC). There are two main challenges in doing
this: unaccommodating camera views and the image-to-avatar domain gap.
We address both challenges by leveraging constraints imposed by multiview
geometry to establish precise image-to-avatar correspondence, which are
then used to learn an end-to-end model for real-time tracking. We present
designs for a training HMC, aimed at data-collection and model building, and
a tracking HMC for use during interactions in VR. Correspondence between
the avatar and the HMC-acquired images are automatically found through
self-supervised multiview image translation, which does not require manual
annotation or one-to-one correspondence between domains. We evaluate
the system on a variety of users and demonstrate significant improvements
over prior work.
CCS Concepts: • Human-centered computing → Virtual reality; •
Computing methodologies→Computer vision; Unsupervised learning; Animation.
Additional Key Words and Phrases: Face Tracking, Unsupervised Image Style
Transfer, Differentiable Rendering
1 INTRODUCTION
Virtual Reality (VR) has seen increased ubiquity in recent years.
This has opened up the possibility for remote collaboration and
interaction that is more engaging and immersive than achievable
through other media. Concurrently, there has been great progress
in generating accurate digital doubles and avatars. Driven by the
gaming and movie industries, a number of compelling demonstrations of state of the art systems have recently attracted interest in
the community [Epic Games 2017; Hellblade 2018; Magic Leap 2018;
Seymour et al. 2017; Unreal Engine 4 2018]. These systems show
ACM Trans. Graph., Vol. 38, No. 4, Article 67. Publication date: July 2019.
67:2 • Wei et al.
highly photo-realistic avatars driven and rendered in real-time. Although impressive results are achieved, they are all designed for
one-way interactions, where the actor is equipped with sensors
optimally placed to capture facial expression. Unfortunately, these
sensor placements are not compatible with existing VR-headset designs, which largely occlude the face. Thus, these systems are better
suited to live performances than interaction.
If we consider instead works that are aimed at bidirectional communication in VR, we discover that existing systems mostly use
non-photorealistic/stylized avatars [BinaryVR 2019; Li et al. 2015;
Olszewski et al. 2016]. These representations tend to have a more
limited range of expressivity, which renders errors from facial expression tracking less perceptible than with photo-real avatars. In
this work, we argue that this is no coincidence, and that precise face
tracking from consumer-friendly headset-mounted camera configurations is significantly harder than in conventional settings. There
are two main reasons for this. First, instead of capturing a complete
and unobscured view of the face, headset-mounted camera placements tend to provide only partial and non-overlapping views of
the face at extreme and oblique views. Minimizing reconstruction
errors in these viewpoints often does not translate to correct results
when viewed frontally. Secondly, these cameras often operate in
the infrared (IR) spectrum, which is not directly comparable to the
avatar’s RGB appearance and makes analysis-by-synthesis techniques less effective. To partially alleviate these difficulties, existing
systems are designed to work using structurally and mechanically
sub-optimal sensor designs that are more accommodating to the
computer-vision tracking problem. Despite this, their performance
is still only suitable for stylized avatar representations.
Although the difficult sensor configurations of headset-mounted
cameras (HMC) can prove challenging for classical face alignment
approaches like [Saragih et al. 2009; Xiong and la Torre 2013], in this
work we show that end-to-end deep neural networks are capable of
learning the complex mapping from sensor measurements to avatar
parameters, given the availability of sufficient high-precision training examples relating the two domains. We demonstrate compelling
results where fully expressive and accurate performances can be
tracked in real time, at a precision matching the representation
capacity of modern photo-realistic avatars. The challenge, then, is
how to acquire the correspondences required to pose the problem
as supervised learning.
Our solution for acquiring correspondence is to leverage multiview geometry in addressing both the problem of oblique viewpoints as well as the sensor-avatar domain gap. Specifically, we
propose the use of a training HMC design that shares a sensor configuration with a consumer-friendly design (the tracking HMC),
but has additional cameras to support better coverage and viewing angles while minimally disturbing the quality of data acquired
from the shared cameras. With these additional viewpoints, classical analysis-by-synthesis constraints become more meaningful,
and results generalize better to common vantage points (i.e., frontal
view of the face). These additional views also provide more signal
to improve the fidelity at which we can perform domain translation
to address the sensor-avatar domain gap. With data collected from
these cameras along with a pre-trained personalized avatar, our
system learns to discover correspondences through self-supervision,
without requiring any manual input or semantically defined labels.
The results are highly precise estimates of the avatar’s parameters
that match the user’s performance, which are suitable for learning
an end-to-end mapping relating them to the tracking HMC.
In the following, we discuss prior work in §2, and present our
method for finding image-to-avatar correspondence in §3, including
hardware design, image style transfer and the core optimization
problem. Real-time facial animation is then covered in §4. We present
results of our approach in §5, and conclude in §6 with a discussion
and directions of future work.
2 RELATED WORK
2.1 Face Tracking in VR
Tracking faces in VR is a unique and challenging problem because
the object we want to track is largely occluded by the VR headset.
In the literature, solutions vary in how hardware designs are used
to circumvent sensing challenges, as well as in methods to bridge
the domain gap between sensor data and the face representation in
order to find their correspondence. Specifically, sensors used to build
the face model and later drive it are typically comprised of different
sets of camera configurations. Modeling sensors (i.e., cameras for
building a shape and appearance model of the face) are typically
multiview, high-fidelity RGB cameras with an unobstructed view
of the face [Beeler et al. 2011; Dimensional Imaging 2016; Fyffe
et al. 2014; Lombardi et al. 2018]. Tracking sensors (i.e., cameras for
driving the face model to match a user’s expressions) are typically
mounted on the HMD, resulting in a patch-work of oblique and partially overlapping views of different facial parts with narrow depth
of field, and typically operate in the infra-red (IR) spectrum [Li et al.
2015; Olszewski et al. 2016]. Moreover, since the HMDs obscure the
face, and modeling sensors require an unobscured view, data from
modeling and tracking sensors can not be captured concurrently.
As the eyebox in a VR headset is enclosed, the face is typically
divided into an occluded upper face and a visible lower face. As such,
some works use specialized strategies to obtain the facial state for
each part separately, followed by a compositing step to get the full
face state in realtime. In [Li et al. 2015] and [Thies et al. 2018], an
RGBD sensor is used, which allows direct registration of the lower
face with the model’s geometry. To have a better viewpoint, Li et al.
[2015] attach the sensor with a protruding mount, placing it slightly
below the mouth. Similar to [BinaryVR 2019], this frontal viewpoint
is optimal for modeling lower mouth expressions, but is not ideal
from a hardware design standpoint. In [Thies et al. 2018], the sensor
is placed in the environment which limits the range of a user’s head
pose. For the upper face, Li et al. [2015] use strain gauges to sense
voltage changes that accompany facial expressions. By building a
skeleton headset without a display unit that otherwise occludes the
upper face, they can acquire face model parameters corresponding
to strain gauge measurements through depth registration. They use
this to train a real-time upper face blendshape regressor. However,
this input signal has low SNR, exhibits drift, and contains only
limited information about facial expressions. In comparison, Thies
et al. [2018] use infrared (IR) cameras pointing at the eye region
to avoid interfering with VR usage. They use a calibration process
where subjects are instructed to gaze at known positions to obtain
ACM Trans. Graph., Vol. 38, No. 4, Article 67. Publication date: July 2019.
VR Facial Animation via Multiview Image Translation • 67:3
correspondence for training. Although effective in estimating gaze
direction and eyelid blinks, it does not capture other upper-face
expressions such as eyebrow motion and the temporal pattern of
wrinkles in the forehead, nose and areas surrounding the eyes.
If the parametric facial model exhibits appearance statistics matching those from the sensors used to drive the model, then “analysisby-synthesis” or “vision as inverse graphics” approaches [Kulkarni
et al. 2015; Nair et al. 2008; Yildirim et al. 2015] can be used to find
correspondences. Here, the challenge is to find the parameters of the
models that, when rendered from the corresponding camera view,
match the images obtained from the driving sensors. This is typically
achieved though variants of the gradient-descent algorithm [Blanz
and Vetter 1999; Cootes et al. 1998; Thies et al. 2016]. Unfortunately,
for VR applications, the domain gap between imaging sensors and
modeling sensors makes it unlikely that minimizing the difference
between the rendered model and the sensor’s image will result in an
expression matching that of the user. Although explicit landmark
detectors can be used to define sparse geometric correspondence
between the model and tracking images [Cao et al. 2014], landmarks
alone lack expressiveness, and the oblique viewpoints from HMD
mounted cameras make reprojection errors less effective.
Other approaches correspond sensor data and face parameters
using non-visual information such as manual semantic annotations
of facial expressions and audio signals. For example, in [Olszewski
et al. 2016], to model the entire face using a single RGB sensor for the
lower face and IR cameras for the eyes, the subjects were instructed
to performed a predefined set of expressions and sentences that were
used as correspondence with a blendshape model face animation
of the same content. To generate more correspondence for training
a neural network with a small temporal window, dynamic time
warping of the audio signal was used to align the sentences with the
animation. Although the approach demonstrated realistic results,
the animation tends to exhibit tokenized expressions which look
plausible but are not faithful reconstructions of the user’s facial
motion. This approach is also limited by the granularity at which
facial expressions can be expressed, as well as their repeatability.
Unpaired learning-based methods have also been investigated.
In [Lombardi et al. 2018], synthetic renderings of the avatar are
used together with real tracking sensor images to build a domainconditioned variational autoencoder (VAE) while simultaneously
learning a mapping from the latent space to face model parameters,
using correspondences from the rendered domain exclusively. In
that work, correspondences are found by leveraging parsimony in
the VAE network, and the model is never trained directly for the
target setting (i.e., input is headset images, output is avatar parameters). Although compelling results were demonstrated for speech
sequences, its performance deteriorates with expressive content.
While our method also leverages unpaired learning methods, differently, we explicitly transfer multiview IR images to avatar-like
rendered images with Generative Adversarial Networks (GANs),
which can generate high quality modality-transferred images while
better preserving facial expression. We also leverage the additional
views provided by the training HMC, and infer more precise correspondences through differentiable rendering.
2.2 Image Style Transfer
Image style transfer is the task of transforming one image to match
the appearance of another while retaining its semantic and structural content [Gatys et al. 2016]. Recent works [Isola et al. 2017]
have tackled this task using GANs [Goodfellow et al. 2014], which
can generate images with high realism. CycleGAN [Zhu et al. 2017]
introduced the concept of cycle-consistency which improves on the
mode-collapse problem with GANs. Although architectural choices,
such as the U-net architecture, in CycleGAN encourages structural
consistency during transfer, it can still suffer from semantic drift,
especially when the distributions between the domains are not balanced. In [Fu et al. 2018], a geometric loss was added to encourage
the preservation of spatial structure, and in [Mueller et al. 2018], a
silhouette matching term was used instead. To further encourage
the preservation of semantic information during transfer, [Bansal
et al. 2018] extend the idea of cycle-consistency by utilizing temporal structures of data. Instead of adding additional terms, in [Harley
et al. 2019], an “uncooperative” optimization strategy is employed
instead to prevent semantic drift caused by the forward and backwards transformations colluding to complement errors produced by
each other. In our method, the preservation of facial expression is
particularly important, because we rely on generated (fake) images
as supervision for optimizing face parameters. Besides matching
distribution carefully and the use of uncooperative optimization,
we present cross-view cycle consistency to further enforce semantic preservation, utilizing synchronized multiview data from our
designed HMC.
2.3 Differentiable rendering
In analysis-by-synthesis approaches [Tewari et al. 2017; Thies et al.
2018], differentiable rendering a textured mesh is necessary to allow the error signal to flow from pixel errors to parameters of
the graphics engine in a system trained end-to-end. However, it
involves a discrete rasterization step to assign triangles to every
pixel, which has a non-differentiable property causing the gradients
from pixel errors hard to be propagated to mesh geometry. Tewari
et al. [2017] circumvent this issue by formulating the loss over the
vertices instead of the image pixels. Kato et al. [2018] address the
problem by approximating gradients according to the geometric
relationship between a triangle and a pixel. In our application, this
problem manifests as failures in matching face silhouettes, and we
address it through a formulation whereby gradients of background
pixels that are not covered by any rasterized triangle can still be
backpropagated to affect mesh geometry.
3 ESTABLISHING CORRESPONDENCE
In recent years, end-to-end regression from images has proven effective for high-quality and real-time facial motion estimation [Laine
et al. 2017; Tewari et al. 2017]. These works leverage the representation capacity of deep neural networks to model the complex
mapping between raw image pixels and the parameters of a parametric face model. With input-output pairs, the problem can be
posed within a supervised learning framework, where a precise
mapping that generalizes well can be found (see §4). The main challenge, therefore, is to acquire a high quality training set: a sufficient
ACM Trans. Graph., Vol. 38, No. 4, Article 67. Publication date: July 2019.
67:4 • Wei et al.
(a)
(b) (c)
✓1
✓2
✓1 ⌧ ✓2
(d) (e)
Fig. 2. Headset mounted cameras (HMC); (a) Training HMC, with standard cameras (circled in red) and additional cameras (circled in blue). It is
used for collecting data to help establish better correspondence between
HMC images and avatar parameters. (b) Tracking HMC, used for realtime
face animation, with minimal camera configuration. (c) Example of captured
images, with colored frames indicating standard views (red) and additional
views (blue). (d) Multi-plane calibration pattern used to geometrically calibrate all cameras in the training and tracking HMCs. (e) An illustration of
the challenges of ergonomic camera placement (blue), where large motions
such as mouth opening project to small changes in the captured image, in
comparison to more accommodating camera placements (orange).
number of precise correspondence between input HMC images and
avatar parameters spanning diverse facial expressions. This is particularly challenging for VR applications, where existing methods
have significant limitations as described in §2.1.
In this work, we establish high-quality correspondence using
domain-transferred multi-view analysis-by-synthesis. We describe
our training- and tracking-HMC designs in §3.1. Our approach for
multiview consistent correspondence estimation is then described
in §3.2, with a detailed treatment of domain transfer in §3.3, and
background-aware differentiable rendering in §3.4.
3.1 Data Capture
Consumer-grade VR headset designs need to be structurally and
mechanically robust, ergonomic, and aesthetically pleasing. HMC designs that fit this criteria typically have partial and oblique views of
the face which are challenging to use with image-space reconstruction losses typically employed in registration algorithms. Fig. 2(e)
illustrates this problem. For this reason, most published [Li et al.
2015; Olszewski et al. 2016] and commercial [BinaryVR 2019] VR face
tracking systems are designed to operate with more accommodating
designs, at the expense of size, weight, and cost considerations. The
core idea of our work is that the challenging images acquired from
optimal camera mounting configurations do, in fact, contain sufficient information for precise expression estimation, as long as there
are sufficient number of high-quality samples relating those images
to the face model’s expression space. In support of this idea, we built
two versions of the same headset; one with a consumer-friendly
design with a minimally intrusive camera configuration (i.e., the
tracking HMC), and another with an augmented camera set with
more accommodating viewpoints to support correspondence finding
(i.e., the training HMC). Shown in Fig. 2, the training HMC is used
to collect data and build a mapping between the minimal headset
camera configuration and the user’s facial expressions. Specifically,
the minimal set consists of 3 IR VGA cameras for the mouth, lefteye and right-eye, respectively. The training HMC has 6 additional
cameras: an additional view of each eye, and 4 additional views of
the mouth, strategically placed lower to capture lip-touching and
vertical mouth motion, and on either side to capture lip protrusion.
All cameras are synchronized and capture at 90Hz. They were geometrically calibrated using a custom 3D printed calibration pattern
that ensures that parts of the pattern are within the depth of field
of each camera, as shown in Fig. 2(d).
To build the dataset, we captured each subject twice using the
same stimuli; once using the training HMC, and again using the
tracking HMC. The content included 73 expressions, 50 sentences,
a range of motion, a range of gaze directions and 10 minutes of free
conversation. This set was designed to cover the range of natural
expressions. Collecting the same content in both devices ensures
a roughly balanced distribution of facial expressions between the
two domains, which is important for unpaired domain transfer
algorithms to work well (see §3.3).
3.2 Overall Algorithm
We illustrate our overall algorithm to establish correspondences
in Fig. 3. It assumes the availability of a pre-trained personalized
parametric face model. For the experiments in this paper, we use the
deep appearance model [Lombardi et al. 2018] which generates geometry and view-conditioned texture from an l-dimensional latent
code z ∈ R
l
and a 6-DOF rigid transform v ∈ R
6
from the avatar’s
reference frame to the headset (represented by a reference camera)
using a deep deconvolutional neural network D:
M,T ← D(z,v). (1)
Here, M ∈ R
n×3
is the facial shape comprising n-vertices, and
T ∈ R
w×h
is the generated texture. A rendered image R can be
generated from this shape and texture through rasterization:
R ← R(M,T,A(v)), (2)
where A denotes the camera’s projection function.
Given multiview images H = {Hi }i ∈C acquired from a set of
headset cameras C, our goal is to estimate the user’s facial expression as seen in these views. We solve this by estimating the latent
parameters z and headset’s pose v that best aligns the rendered
face model to the acquired images. However, instead of performing
this task separately for each frame in a recording, similar to Tewari
et al. [2017], we simultaneously estimate these attributes over the
entire dataset comprising thousands of multiview frames. Specifically, we estimate the parameters θ of a predictor Eθ
that extracts
ACM Trans. Graph., Vol. 38, No. 4, Article 67. Publication date: July 2019.
VR Facial Animation via Multiview Image Translation • 67:5 & %
' (



HMC Images
Style Transferred
Mesh View-dependent Texture
Rendered Avatar
Differentiable
Rendering
§3.4
{} {}
{} {}
,
,
Established Correspondence
{}

{}
,
{} {}
,
Image Style
Translation
§3.3
Avatar
Parameter
Headset's
6DOF Pose
HMC Camera
Calibration


ˆ




3
1-Loss




Fig. 3. Overall optimization for establishing correspondence. We train a network Eθ to jointly estimate avatar parameters z and headset position v
from 9-view images Hi from a training HMC. These estimated values are later used by D and R to render multiview images of the avatar Ri
. In order to apply
an L1 reconstruction loss, we bridge the domain gap between HMC images and rendered images by also training an image style transformer Fϕ (along with
Gψ ) to convert Hi to the rendered domain Rˆ
i
, with preserved semantics. Colored modules (Eθ and Fϕ) have trainable parameters, while parameters of D are
frozen and R is parameter free. On the right, the found correspondences between H and z can be used as pairs of 3-view images for a tracking HMC (shown in
pink-boxes) and avatar parameters for later regression (see §4).
{z
t
,v
t
}, the latent code and headset’s pose for frame t ∈ T, by
jointly considering data from all cameras at that time instant:
z
t
,v
t ← Eθ
(Ht
), ∀t ∈ T. (3)
Note that the same predictor is used for all frames in the dataset.
Analogous to non-rigid structure from motion [Xiao et al. 2006], this
has the benefit that regularity in facial expression across time can
further constrain the optimization process, making the estimation
proces more resistant to terminating in poor local minima.
Due to the domain-gap between the rendered image R and the
camera images H, they are not directly comparable. To address this,
we also learn the parameters ϕ of a view-dependent domain transfer
network:
Rˆ
i = Fϕ(Hi
;i), ∀i ∈ C. (4)
In its simplest form, this function is comprised of independent networks for each camera i. With this, we can formulate the analysisby-synthesis reconstruction loss:
L(θ,ϕ) =
Õ
t ∈T Õ
i ∈C



Rˆ
t
i − R
t
i




1
+ λδ(z
t
)
!
, (5)
where R
t
i
is the rendered face model from Eq. (2), rasterized using
the known projection function Ai whose parameters are obtained
from the calibrated camera i, as mentioned in § 3.1. Here, δ is a regularization term over the latent codes z, and λ weights its contribution
against the L1-norm reconstruction of the domain-transferred image.
Although at first glance this formulation appears to be reasonable,
with high capacity encoder Eθ and domain-transfer Fϕ functions,
there is a space of solutions where one network can compensate
for the semantic error incurred by the other, leading to low reconstruction errors but incorrect estimation of expression z and
headset pose v. Without additional constraints, we observe that
this phenomenon often occurs in practice, which we refer to as
collaborative self-supervision. When the domain gap comprises primarily of appearance differences, we conjecture that collaborative
self-supervision tends to be more prominent in architectures that
do not retain spatial structure. This is the case in our system, where
the latent code z is a vectorized encoding of the image. This problem has been observed in the style-transfer community, where a
common solution is to use fully convolutional architectures with
skip connections [Isola et al. 2017; Zhu et al. 2017]. As spatial structure is propagated through the entire network, it is easier to retain
structure, and thus, it tends to remain unchanged when differences
in style can be well explained by appearance alone.
Motivated by the compelling results achieved by methods such
as [Zhu et al. 2017], we decouple Eq. (5) into two stages. First, we
learn the domain transfer Fϕ that converts headset images Hi
into
fake rendered images Ri without changing the apparent expression
(i.e., semantics). In the second stage, we fix Fϕ and optimize Eq. (5)
with respect to Eθ
.
3.3 Expression-Preserving Domain Transfer
Our expression-preserving transfer is based on unpaired image
translation networks [Zhu et al. 2017]. This architecture learns a
bidirectional domain mapping (Fϕ and Gψ ) by enforcing cyclic consistency between the domains and an adversarial loss for each of the
ACM Trans. Graph., Vol. 38, No. 4, Article 67. Publication date: July 2019.
67:6 • Wei et al.
Synthetically
Rendered Avatar
Captured
HMC Images
Real-world
Imaging
Unknown
Controlled
Rendering
푃(푧, 푣) 푃
ˆ
(푧, 푣) Shared
Stimuli
Landmark
Detection
푃
ˆ
(푣)
푃
ˆ
(푧)
Approximated
'
( %퐻 %푅
Fig. 4. Matching the distribution of spatial structures in images across
domains is important for training expression-preserving image style transfer
networks Fϕ and Gψ . We use landmark detection to estimate a distribution
over headset pose, and use the same expression stimuli during data capture
(for both avatar building and HMC capture) to ensure the distribution of
expressions is comparable.
two. To achieve preservation of expression, we need to eliminate the
tendency of generators to modify spatial structure on the images.
With a fully convolutional architecture where random initialization
already leads to retained image structures, this tendency mainly
comes from the pressure to prevent opposing discriminators from
spotting fake images from their spatial structure. In other words,
if the distribution of spatial structure, which in our case is jointly
determined by headset positions v and facial expressions z, is balanced, the generators then have no pressure to begin modifying
them. In the following, we begin by describing how we balance the
distribution when preparing real datasets before training, followed
by the learning problem itself.
3.3.1 Matching Distribution of Image Spatial Structure. While we
don’t have control over the underlying expression z
t
and headset
position v
t
in captured headset data {H
t
i
}t ∈T, we can generate a
set of rendered images {R
s
i
}s ∈S with the desired statistics if we
have an estimate Pˆ(z,v) of the joint distribution P(z,v). However,
since estimating individual z
t
and v
t
for headset data is our original
problem, we need to find a proxy to approximate Pˆ ≈ P.
Fig. 4 summarizes our strategy. Here, we assume independent distribution between z and v, or Pˆ(z,v) = Pˆ(z)Pˆ(v), and estimate Pˆ(z)
and Pˆ(v) individually. For Pˆ(z), we rely on the data capture process,
where the subject is captured twice with the same stimuli as mentioned in § 3.1. Even though this does not lead to a frame-to-frame
mapping between the captures, we can assume the distribution of
facial expression is comparable. Therefore, we can use the set of expression codes from the modeling capture as approximate samples
from P(z).
For the distribution over headset pose Pˆ(v), we resort to fitting
the face model’s 3D geometry to detected landmarks on headset
images by collecting 2D landmark annotations and training landmark detectors [Wei et al. 2016]. Although landmark fitting alone
Fig. 5. Landmarks on the texture map of the avatar (left) and HMC
images (right). Colors of landmarks indicate the point-to-point correspondence across domains. We detect landmarks on all 9 views (only 3 views are
shown) from detectors trained from manual annotations. The uv-coordinates
of these landmarks are then jointly solved with z
t
and v
t
across multiple
frames. Note that we do not minimize projected distance of landmarks in
Eq. (5) to find image-to-avatar correspondence.
does not produce precise enough estimates of expression (because
landmarks are not able to describe complete and subtle facial expressions), it can often give reasonable estimates for v owing to its low
dimensionality and limited range of variation. One of the challenges
in fitting a 3D mesh to 2D detections is defining correspondence
between mesh vertices and detected landmarks. Typically, the landmark set for which annotations are available does not match exactly
to any vertex in a particular mesh topology. Manually assigning a
single vertex to each landmark can lead to suboptimal fitting results
for coarse mesh topologies. To address this problem, while fitting
individual meshes, we simultaneously solve for each landmark’s
mesh correspondence (used across all frames) in the texture’s uvspace {uj ∈ R
2
}
m
j=1
, where m is the number of available landmarks.
To project each landmark j on rendered images of every view, we
calculate a row vector of the barycentric-coordinates bj ∈ R
1×3 of
the current uj
in its enclosing triangle, with vertices indexed by
aj ∈ N
3
, and then linearly interpolate projections of the enclosing
triangle’s 3D vertices, Maj
∈ R
3×3
, where M is the mesh from Eq. (1).
Specifically, we solve the following optimization problem:
min
uj
,vt
,z
t
Õ
t ∈T
Õ
i ∈C
Õm
j=1
w
t
ij





pij
H
t
i

− bjP(M
t
aj
,Aiv
t
)






2
, (6)
where pij ∈ R
2
is the 2D detection of landmark j in HMC camera i,
P is a camera projection generating 2D points in R
3×2
, andw
t
ij is the
landmark’s detection confidence in [0, 1]. Note that for a landmark
j not observable by view i, w
t
ij is zero. In practice, we initialize the
uj
’s to a predefined set of vertices in the template mesh to prevent
divergence. We also avoid using landmarks in regions where the
avatar doesn’t have mesh vertices, such as the pupils and the mouth
interior. Fig. 5 shows an example of the uj
’s at convergence.
By solving Eq. (6), we obtain a set of HMC pose {v
t
}t ∈T from
each frame Ht
. We can now render the required dataset {R
s
i
}s ∈S
by randomly sampling a HMC pose |S| times from {v
t
}t ∈T, and
an expression code also |S| times from the set of encoded values
ACM Trans. Graph., Vol. 38, No. 4, Article 67. Publication date: July 2019