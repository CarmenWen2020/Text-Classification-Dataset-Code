Holograms display a 3D image in high resolution and allow viewers to
focus freely as if looking through a virtual window, yet computer generated
holography (CGH) hasn’t delivered the same visual quality under plane
wave illumination and due to heavy computational cost. Light field displays
have been popular due to their capability to provide continuous focus cues.
However, light field displays must trade off between spatial and angular
resolution, and do not model diffraction.
We present a light field-based CGH rendering pipeline allowing for reproduction of high-definition 3D scenes with continuous depth and support
of intra-pupil view-dependent occlusion. Our rendering accurately accounts
for diffraction and supports various types of reference illuminations for hologram. We avoid under- and over-sampling and geometric clipping effects
seen in previous work. We also demonstrate an implementation of light
field rendering plus the Fresnel diffraction integral based CGH calculation
which is orders of magnitude faster than the state of the art [Zhang et al.
2015], achieving interactive volumetric 3D graphics.
To verify our computational results, we build a see-through, near-eye,
color CGH display prototype which enables co-modulation of both amplitude
and phase. We show that our rendering accurately models the spherical illumination introduced by the eye piece and produces the desired 3D imagery
at the designated depth. We also analyze aliasing, theoretical resolution
limits, depth of field, and other design trade-offs for near-eye CGH.
CCS Concepts: • Computing methodologies → Mixed / augmented
reality;
Additional Key Words and Phrases: computer generated holography, light
field, computational photography, vergence-accommodation conflict
1 INTRODUCTION
Making a comfortable visual experience is key to making practical
daily use Virtual Reality (VR) and Augmented Reality (AR) successful. Providing a wide field of view, high resolution, interactivity,
view-dependent occlusion, and being able to support continuous
focus cues to avoid vergence-accommodation conflict are some of
the most important issues for near eye displays.
Recently, near-eye light field displays have been explored as potential methods to fulfill the aforementioned requirements, in particular the continuous focus cues. Other near-eye display architectures
have also been proposed to achieve a comparable viewing experiences, e.g., multi-focal displays and varifocal displays. However,
combining continuous focus cues, resolution, space-time trade-offs,
mechanical movement, and diffraction limits creates a challenging
design space.
Holographic displays, especially using Computer Generated Holography (CGH), have been the hope of many to create the ultimate
display since the late 60s and 70s. Holographic displays, though,
are rarely used as mainstream displays because under plane wave
illumination, the small diffraction angle directly leads to a narrow
ACM Transactions on Graphics, Vol. 36, No. 6, Article 236. Publication date: November 2017.
236:2 • L.Shi et al.
field of view. For near-eye applications, using spherical wave illumination enables a trade-off between field of view, viewing distance
to the display (eye relief), and size of exit pupil (eye box).
Holographic calculations are also notoriously computationally
expensive. Various approximations, e.g. sacrificing depth fidelity or
limiting the parallax to horizontal-only, are often required to speed
up calculations enough to make computation time practical.
In this work, we show how to drive a CGH based near-eye display
using light field rendering, We discuss the previously mentioned
problems, the trade-offs between different illumination schemes,
an implementation of holographic light field rendering optimized
for high performance, and the benefits and the limitations of our
choices made both in illumination and in the implementation.
Specifically, we make the following contributions:
• We propose a new rendering algorithm that avoids aliasing in both over-sampling, under-sampling and geometry
clipping found in previous work. Our near-eye holographic
paradigm supports continuous focus cues, intra-pupil occlusion, various reference wave illuminations and significantly
improves the computational speed without sacrificing accuracy.
• We analyze theoretical limits on sampling, spatial resolution, and the connections between holographic displays
and light field displays. We discuss the theoretical trade-off
space for designing near-eye displays using CGH.
• We show a GPU implementation based on the Fresnel
diffraction integral that is five orders of magnitude faster
than the prior work [Zhang et al. 2015] and achieves interactive performance at high resolution and per-pixel depth
for the first time.
• We build a complex-modulation see-through color display
prototype supporting continuous focus cues and an expanded field of view to verify our rendering and calculation
results.
2 PREVIOUS WORK
2.1 Fourier Holography
A Fourier hologram produces a flat image in the far field and is often
viewed through a lens [Benton and Bove 2008]. In such holograms,
an inverse Fourier-transformed target image is typically approximated by a binary amplitude mask [Brown and Lohmann 1966] or
by an iteratively optimized phase modulation pattern [Fienup 1982;
Gerchberg 1972; Lesem et al. 1969]. Multi-plane reconstructions are
achieved by iterating between Fourier planes and applying phase
retrieval techniques [Dorsch et al. 1994; Makowski et al. 2005] (without guarantee of convergence). Unfortunately, Fourier holograms
provide little 3D information. A Fresnel hologram, as discussed below,
can produce near-field 3D imagery.
2.2 Physically-based CGH
A physically-based CGH simulates optical propagation and interference to generate diffractive patterns that produce a desired light
distribution [Benton and Bove 2008]. In such holograms, a digital
3D object is represented as the interference of waves from pointemitters (point-based method) [Waters 1966] or from polygonal tiles
(polygon-based method) [Leseberg and Frère 1988].
The point-based method based on the Fresnel diffraction integral
has been accelerated by elementary fringe mapping [Lucente 1993],
texture-based GPU parallelization [Maimone et al. 2017; Masuda
et al. 2006; Petz and Magnor 2003], and visibility-based non-uniform
sampling [Chen and Wilkinson 2009]. In practice, such methods
require a dense point cloud to reproduce a continuous surface and
occlusion is often not well-handled, limiting virtual objects to simple
geometries.
The polygon-based method leverages FFT-based angular spectrum (AS) decomposition [Goodman 2005] to efficiently calculate the
optical field of polygons at the location of the hologram plane [Tommasi and Bianco 1993]. Remapping the surface property function
to the angular spectrum allows texturing, shading, and reflection
[Matsushima 2005]. Fully analytical frequency representations eliminate per-polygon AS transforms and reduce computational cost
[Ahrenberg et al. 2008; Jia et al. 2014; Kim et al. 2008]. Silhouette
masking during AS propagation accounts for self and mutual occlusion [Matsushima and Nakahara 2009; Matsushima et al. 2014].
Although polygon-based methods produce realistic 3D scenes with
smooth surfaces, occlusion handling requires storing AS decompositions at intermediate planes and iterative optimization, which is
difficult to parallelize.
2.3 Image-based CGH
The advance of computer graphics techniques has led to two types of
image-based CGHs: the holographic stereogram and the layer-based
method.
Conventional holographic stereogram (HS), either interferometrybased or computer-generated, partitions a hologram spatially into
multiple horizontal bars with each recreating a distinct perspective
captured from an imaging system or through rendering [DeBitetto
1969; Yatagai 1976]. Diffraction-specific calculations of HSs partition the hologram into elementary hologram patches, with each
producing a local ray distribution that together reconstruct multiple views [Lucente and Galyean 1995]; a close analogy to integral
imaging. The lack of focus cues and limited depth of field due to
flat wavefronts is mitigated by introducing depth-dependent wavefront curvatures [Kang et al. 2016; Smithwick et al. 2010; Yamaguchi
et al. 1993] or employing an intermediate ray sampling plane with
angular spectrum propagation [Wakunami et al. 2013]. Resolution
loss due to spatial partitioning and independent treatment of elementary hologram may be eliminated by combining the HS model
with the point-based method [Zhang et al. 2015]. However, previous
methods only generate discrete views from the center of each elementary hologram, causing aliasing, geometry clipping, and poor
extensibility to spherical wave illumination, as shown in Section 5.1.
The layer-based method slices objects at multiple depths and superimposes the wavefronts from each slice on the hologram through
inverse Fresnel transform or FFT-based AS decomposition [Bayraktar and Özcan 2010; Zhao et al. 2015]. View-dependent occlusion is
achieved by angularly tiling groups of sliced layers [Chen and Chu
2015] or combined with the HS model to generate a layered-based
elemental hologram for each spatial partition [Zhang et al. 2016].
ACM Transactions on Graphics, Vol. 36, No. 6, Article 236. Publication date: November 2017.
Near-eye Light Field Holographic Rendering with Spherical Waves for Wide Field of View Interactive 3D Computer Graphics • 236:3
Maimone et al. [2017] show a real-time single layer implementation
on the GPU. Despite considerable computational saving for complex
3D objects, layer-based holograms cannot support continuous focus
cues and accurate occlusion due to discrete plane sampling.
2.4 Holographic Display with Extended Field of View
The solid angle spanned by the maximum visible imagery to the
eye defines the field of view (FoV). Chen and Chu [2015] demagnify
the display pixel pitch via a 4f system to increase the Field of View
at the cost of a smaller display panel. The magnification mismatch
is compensated by pre-distorting the graphics model [Zhang et al.
2012]. Others have tiled multiple displays in a circular shape with
each producing a specific part of scene towards the viewer [Hahn
et al. 2008; Kozacki et al. 2012; Yaraş et al. 2011]. The Fourier optical
system [Reichelt et al. 2012; Sato and Sakamoto 2012; Senoh et al.
2011] places a lens right against the display to produce a converging
illumination and steers the viewing cones to the focal plane; the
viewing zone is limited between finite depths in return for a shorter
viewing distance and a wider field of view. A similar idea has also
been demonstrated using holographic optical elements or compound
microscopic optical systems to further expand the field of view [Li
et al. 2016; Maimone et al. 2017].
2.5 Angular Displays and Light Field Displays
Surface materials with angular variations are achieved with programmable BRDFs using light field [Fuchs et al. 2008] or wave optics
by directly optimizing diffraction [Glasner et al. 2014] or factorizing
the Wigner Distribution Function [Ye et al. 2014]. Light field models
the space-angle distribution in geometric optics, and have also been
shown to support focus cues with dense angular sampling [Maimone et al. 2013] or additive multilayers [Lee et al. 2016; Narain et al.
2015]. Near eye displays have been shown using pinlights [Maimone
et al. 2014], microlens arrays [Lanman and Luebke 2013], or attenuation based multilayer compression [Huang et al. 2015; Maimone and
Fuchs 2013] to support intra-ocular occlusions which are important
to depth perception [Zannoli et al. 2016]. However, diffraction limits
further enhancement of spatial and angular resolution of light field
displays. On the other hand, Ziegler et al. [2007] demonstrate a
hologram transform based on off-line light field rendering, and we
show a more efficient paradigm that generates high-resolution CGH
interactively.
3 PRELIMINARY
In this section, we first review the mathematical foundations of
computer generated holography and briefly discuss the limitations
of using plane wave illumination. We then connect light fields to
holograms via phase space modeling using the Wigner Distribution
Function. We also discuss the benefits of using spherical wave illumination in Section 4 and introduce our proposed rendering and
calculation in Section 5.
3.1 Computer Generated Holography (CGH)
A hologram converts an input “reference” light wave ER (x) to the
desired output “object” light wave EO (x). This requires knowledge
of both the reference wave and the object wave. Usually, the form of
the reference wave is a given, and the purpose of CGH is to compute
the diffraction pattern which will do the conversion. This leaves the
question of calculating the desired output waveform at the location
of the hologram.
The desired output waveform at the location of the hologram
may be calculated by starting at the position of the desired target
3D point and propagating light towards the hologram using the
Fresnel diffraction integral. For object made up of points, this may
be thought of as a summation of spherical waves originating from
the points themselves:
EO (x) =
X
j
Aj
rj
(x)
e
i(
2π
λ
rj
(x )+ϕj )
, (1)
where λ is the wavelength of the monochromatic light source, Aj
is
the amplitude of the point j at location (xj
, zj
), as shown in Figure 2,
rj
(x) =
q
(xj − x)
2 + z
2
j
is the Euclidean distance from the point to
the pixel (x, 0) on the hologram, and ϕj
is the random initial phase
associated with each diffuse point. Note that the resulting electric
field is complex-valued.
A computer generated hologram modulates an input wavefront
multiplicatively. For plane wave (collimated) illumination, of which
the value of the wavefront is a constant across the hologram plane,
the hologram to display is exactly the object electric field. For spherical wave illumination, the object electric field should be multiplied
by a complex exponential with a quadratic phase to cancel out the
quadratic phase of the spherical reference wave illumination.
Displaying the correct diffraction pattern requires devices which
can modulate both the amplitude and the phase of the waveform
with spatially varying patterns, e.g. liquid crystal (LC) displays or
spatial light modulators (SLMs).
One major limitation of holographic displays using LC-SLMs is
the narrow maximum deflection angle between the incident light
angle θin and the outgoing angle θout , which is characterized by
the grating equation:
λ
2∆p
= |sinθin − sinθout | , (2)
where ∆p is the SLM pixel pitch. A typical 8µm pitch SLM can
maximally deflect 532nm green light by 3.81◦
, whereas a traditional
analog hologram with nanometer-scale diffracting pattern can easily
achieve a maximum angle greater than 100◦
.
The small diffraction angle of SLMs severely impacts the practical
application of SLMs to near eye displays under plane wave illumination, as shown in Figure 2. To observe the full extent of a SLM
of width ws , the minimal distance d
min
e
from the eye to the SLM,
called eye relief, is determined by the two converging rays deflected
from the edges of the SLM, and is given by:
d
min
e =
ws
2 tan 
sin−1

λ
2∆p
  . (3)
The maximum angle αmax subtended by the SLM determines the
maximum field of view (FoV):
αmax = 2 tan−1

ws
2d
min
e
!
= 2 sin−1

λ
2∆p
!
. (4)
ACM Transactions on Graphics, Vol. 36, No. 6, Article 236. Publication date: November 2017.
236:4 • L.Shi et al.
Fig. 2. Plane wave illuminated holographic display. Each display pixel
diffracts wavefront into a spatially-uniform light cone. Rays are back-traced
for each point emitter within the diffraction cone. The viewer can only observe the full extent of the displayed 3D image behind the intersection of
two marginal cones diffracted from the edge of the display.
The red region allowing eye to move freely and still be able to see
the entire hologram is called the eye box. When eye relief de > d
min
e
,
the eye box is obtained as
we =
de − d
min
e
d
min
e
ws . (5)
As such, a 1080p SLM with 8µm pitch sets a minimum 265mm eye
relief and a maximum field of view of only 3.81◦
. Further, Equation (4) shows the FoV does not scale with SLM width, making CGH
under plane wave illumination a less ideal candidate for near-eye
displays. In section 4 we detail how spherical wave illumination
surmounts these difficulties.
3.2 Holography, Wigner Distributions, and Light Fields
A light field L(x, θ ) models the discrete ray space-angle distribution
in geometric optics and is widely used as a content format for multiview displays. However, a high resolution light field display often
suffers from diffractive blur [Huang et al. 2015]; since geometric
optics models do not consider diffraction and interference, they cannot be used to display holograms. We will show how to transform
light field content into a holographic wavefront in Section 5, and
here we first discuss their similarity.
In light fields, the angle θ corresponds to the normal direction of
the wavefront and can be related to the spatial-frequency u = θ/λ of
the Fourier transformed object wave F (EO (x)), also known as the
angular spectrum. Similar to light fields, the Wigner Distribution
Function (WDF) [Alonso 2011; Bartelt et al. 1980; Bastiaans 1997]
models both space and spatial-frequency of a scalar function, and is
calculated by Fourier transforming the mutual intensity function of
a scalar wave field E(x):
W (x,u) =
Z *
E

x +
fu
2
!
E
∗

x −
fu
2
!+ e
−i2π fuu
d fu, (6)
where ⟨·⟩ is the time-averaging operator for incoherent illumination. Note that the WDF is real but not necessarily positive due
Side View
Fig. 3. Spherical wave illuminated holographic display. Each display pixel
diffracts a light cone toward the focus. The entire display is observable by
placing pupil in the diamond-shape viewing zone.
to destructive interference; however, projecting the WDF along either axis results in a non-negative function. In particular, we obtain
the intensity distribution by projecting along the spatial-frequency
(u-axis):
Z +∞
−∞
W (x,u)du = |E(x)|
2
. (7)
A non-negative WDF that resembles a light field can also be obtained
by convolving the WDF of E(x) with the inverted WDF of a finite
aperture function [Zhang and Levoy 2009].
The definition of the WDF, unlike the light field, reveals an important property of holograms: the angular light distribution is
inherently encoded in the spatial pattern of the scalar function E(x),
and no spatial resolution is sacrificed.
Despite the similarity and relationship between light fields and
WDFs, reversing a light field to obtain the scalar function E(x)
is challenging: even though the inverse Fourier transform of the
spatially-compressed WDF F−1
(W (x/2,u)) gives the scalar wave
field up to a constant scalar E(x)E
∗
(0) [Claasen and Mecklenbräuker
1980], obtaining the WDF by deconvolving an arbitrary light field
with the inverted WDF of a local aperture is impractical and does not
always yield a valid WDF [Jagannathan et al. 1987]. Even with valid
WDFs, inverting a high resolution (i.e. 1080p) WDF requires storing
and processing trillions of samples which is intractable. Ye et al.
[2014] approximate the nonnegative WDF via iterative global optimization. However, they sacrifice the spatial and angular resolution
by partitioning the scalar field into independent segments.
With the aforementioned computational and physical limitations,
we will not rely on the WDF to generate holograms. Nevertheless,
the WDF’s spatial-angular representation of phase space makes it
ideal for analyzing holographic display characteristics both mathematically and graphically. We will revisit the WDF in Section 7.
ACM Transactions on Graphics, Vol. 36, No. 6, Article 236. Publication date: November 2017.
Near-eye Light Field Holographic Rendering with Spherical Waves for Wide Field of View Interactive 3D Computer Graphics • 236:5
4 SPHERICAL WAVE ILLUMINATION
CGHs under plane wave illumination give a narrow field of view
and set a long eye relief, as discussed in Section 3.1. In holographic
projection, using a diverging illumination expands the projected
image field [Qu et al. 2015]. Similarly, we employ a converging beam
to create a near eye CGH, achieving wider field of view and shorter
eye relief, as shown in Figure 3.
We denote the focus of converging wavefront F , and the distance
between the focus and the SLM dF . Unlike under plane wave illumination, the eye box is confined between two depths determined by
Equation (2):
d{f ar,near}
=
ws
2 tan 
sin−1

sin 
tan−1

ws
2dF
  ±
λ
2∆p
  . (8)
When the eye is located at the focus (eye relief de = dF ), the field of
view αdF
and the eye box we (dF )
follow
αdF
= 2 tan−1

ws
2de
!
, we (dF ) = 2 tan
sin−1

λ
2∆p
! ! dF . (9)
Equation 9 shows the significance of using converging spherical
wave illumination, that (1) the eye relief only depends on the focal
length of wavefront, (2) the field of view depends on both the SLM
width ws and the eye relief de , which is constrained by the demand
for a larger eye box, and (3) for any given eye relief, a smaller pixel
pitch results in a larger eye box.
These relations offer more flexibility in designing near-eye displays, for example, using a shorter focal length wavefront allows
for a wider field of view but a smaller eye box [Maimone et al. 2017].
In this paper, we require a minimal 10mm eye box coverage.
Using spherical wave illumination impacts the entire pipeline
from the optical setup, the CGH calculation in Equation (1), and the
light field rendering. In Section 5, we redesign light field rendering to
adapt the diffraction geometry under both plane wave and spherical
wave illumination, and show how to avoid the aliasing and clipping
problem in the methods proposed by the previous work.
5 RENDERING AND CALCULATION FOR CGH
In the proposed CGH rendering pipeline, we start with polygonbased light field rendering such that occlusion is handled through
the graphics pipeline using z-buffer. The output is a multiview holographic light field which contains color and position information
intended for CGH calculation, as shown in Figure 4. We then transition to the point based method with local partitioning, which allows
for per-pixel depth reconstruction and parallel computation on GPU
at interactive speed in 1080p image quality for the first time.
In the following, we first describe the improved rendering setup
under plane wave illumination, which avoids aliasing and geometry clipping inherited in the previously proposed rendering setup
[Zhang et al. 2015]. We then extend the proposed algorithm to support spherical wave illumination, which enables near-eye CGHs.
The Fresnel diffraction integral based calculation which converts a
holographic light field into CGH is presented at the end.
Phase Map Amplitude Map
Color Map Position Map
CGH Holographic Light Field
Hogel
Elemental
Image
Fig. 4. We render the holographic light field with both color (top-left) and
position (top-right) information . Each elemental image maps one-to-one to
the associated hogel and this allows for parallelized acceleration. However,
within each elemental-to-hogel pair, there is a dense mapping between
every pair of pixels from both sides: every pixel in the hogel accumulates
both phase and amplitude from every pixel in its elemental image, thus
forming a complete bipartite compute graph per elemental-to-hogel pair.
5.1 Holographic Light Field Rendering
Calculating a full parallax hologram demands a densely sampled 4D
light field [Huang et al. 2015; Maimone et al. 2013], where a local
view (elemental image), which records depth-sorted visible scene
points, is rendered for each SLM pixel from the its spatial location
and based on the diffraction geometry. The same 4D light field can
also be rendered through a dense angular view sampling at the SLM
resolution, an approach that is often more preferred due to fewer
rendering passes. Nevertheless, even under the small diffraction
angle of existing SLMs, rendering a 1080p spatial resolution light
field at an angular resolution equivalent to human visual acuity is
computationally impractical for real-time graphics.
Assuming Lambertian surfaces, rendering a dense 4D light field
generates highly similar views for adjacent SLM pixels and results in
large redundancies: a single recording of each object point, regardless of the visibility to each SLM pixel, is sufficient to determine the
point’s wavefront over the entire SLM. Leveraging this observation,
a hologram can be spatially partitioned into abutting grids, called
holographic elements (hogels), as shown in Figure 4, such that only
a single representative view, an elemental image, is rendered and
used in calculation for each hogel, assuming all captured points are
visible to all pixels in the hogel. The reduction in parallax introduces
an error in occlusion bounded by the hogel size wh (about 1mm in
our prototype) within the eye box; this approximation substantially
reduces the number of rendering pass (Section 7.6), and is widely
adopted in holographic stereogram calculations (Section 2.3).
Under plane wave illumination, the previous work renders a
hogel’s view by setting a virtual camera directly on the SLM surface
and at the hogel’s center, as shown in Figure 5 (bottom). The view
frustum, as shown by the pink cone, has a field of view equaling the
maximum diffraction angle defined by the SLM pixel. However, this
scheme cannot provide an accurate per-pixel wavefront accumulation as it only renders the object points visible to the hogel’s center
ACM Transactions on Graphics, Vol. 36, No. 6, Article 236. Publication date: November 2017.
236:6 • L.Shi et al.
Clipping Plane
Our
View Frustum
Hogel
Side View
Conventional
View Frustum
Fig. 5. Holographic light field rendering for plane wave illumination. The
proposed view frustum (top, pink cone) covers the entire space in front of
the SLM, as opposed to the convention setup (bottom, pink cone), where
geometry clipping happens when objects are located between two adjacent
hogels and within a distance dmin to the SLM. In our setup, the rendering
contains a larger area w1 than what each SLM pixel sees, thus we use a
sliding window w2 based on the maximum diffraction angle to disambiguate
the blue segment and select the valid fraction from the rendered elemental
image for each SLM pixel under consideration.
pixel and also extends poorly to spherical wave illumination. Consider the hogel’s bottom SLM pixel, the rendering of objects from the
top red parallelogram is mistakenly incorporated, and that from the
bottom yellow parallelogram is incorrectly excluded. Meanwhile,
to prevent geometry clipping, the separation wh between adjacent
view frustums sets an implicit depth limit
z ≤ −dmin ≡ −
wh
2 tan 
sin−1

λ
2∆p
  , (10)
to the objects and the near clipping plane; artifacts are shown in
Figure 11. We propose a unified framework, holographic light field
rendering, in the following, to support accurate view estimation
under various illuminations.
To gather the part of 3D scene visible to the entire hogel, we
laterally shift the virtual camera in the z direction, as shown in
Figure 5 (top), to the intersection of the two marginal rays, defined
by the diffraction cone associated with the hogel’s top and bottom
pixel. The lateral offset to the SLM is given by dcz = dmin. The offset
camera projects all the scene points, visible to the pixels in the hogel,
to the area w1 = 2 sin−1
(λ/2∆p)(d1 + dcz
) for further processing.
Again, for the hogel’s bottom pixel, we incorrectly incorporate the
top portion of the scene points when using the entire view w1.
To address this problem, a conservative sliding window considers
only the valid fraction in the rendered hogel view during the final
wave calculation. This sliding window, w3 =

1 −
dcz
d1+dcz

w1, on
the front clipping plane is defined by projecting a cone of maximum diffraction angle from the SLM pixel under consideration
(the bottom hogel pixel in fig. 5 (top)) to the scene object. Since we
approximate the projection geometry by replacing the per-pixel rendering with the per-hogel rendering, the 3D projection to the offset
virtual camera also introduces ambiguity segments, as shown in the
inset of Figure 5. For the bottom hogel pixel, an example is shown
in the blue segment, where both the valid yellow triangle (should
be considered) and the invalid red triangle (should be excluded) are
projected to the same location. During the final wave calculation,
we disambiguate this region by testing whether or not the angle to
the SLM pixel under consideration is smaller than the maximum
diffraction angle. To test the visibility, we include the ambiguity
segment by extending the sliding window to the following:
w2 =

1 −
dcz
d2 + dcz
!
w1. (11)
Arranging a camera array under the proposed configuration allows for unrestricted disposition of scene objects. The lateral separation between camera and SLM ensures that adjacent camera views
overlap immediately in front of the SLM, and the tiled frustum array
fully covers the field of view of the entire hologram, allowing near
clipping plane to be set at arbitrary depth in front of SLM. The configuration works well for rendering under plane wave diffraction
geometry. For rendering under spherical wave illumination, the
prior discussion still applies, but the camera geometry has to consider the limited eye box and the expanded field of view as discussed
in Section 4.
5.2 Spherical-Holographic Light Field Rendering
Using spherical wave illumination enables a wider field of view.
However, the view-frustum must be updated to accommodate the
new diffraction geometry. Specifically, the frustums undergo spatiallyvarying transforms: the wavefront curvature introduces an off-axis
rotation to the diffraction cones and centers each frustum on the
local incident ray direction, as shown in Figure 6 (top). Meanwhile,
at a larger incident angle, the diffraction cone and the associated
frustum shrinks according to the grating equation (eq. (2)), but stills
covers the eye box defined by the diffraction cone of the SLM’s
center pixel. Overall, the updated viewing frustums of all SLM pixels collectively widen the field of view. In the conventional setup
(bottom), the cameras’ viewing frustums are largely disjoint to the
pixels’ frustums and fail to provide a properly generalized view.
Extending the proposed camera arrangement to spherical illumination sets the virtual camera at the intersection of the marginal
rays restricted by the eye box (eq. (9)) and skews the field of view.
The separation between camera and SLM is given by
dcz =
dFwh
we (dF ) + wh
. (12)
The offset between camera and hogel center along x- and y-axis
depends on each hogel’s position relative to the eye box. Assuming
2M + 1 by 2N + 1 partitioning of the SLM along x and y, the displacement from the (m,n)-th hogel center to its virtual camera is
ACM Transactions on Graphics, Vol. 36, No. 6, Article 236. Publication date: November 2017.
Near-eye Light Field Holographic Rendering with Spherical Waves for Wide Field of View Interactive 3D Computer Graphics • 236:7
Clipping Plane
Our
View Frustum
Hogel
Side View
Conventional
View Frustum
Fig. 6. Holographic light field rendering for spherical wave illumination.
The camera view frustums are spatially non-uniform and steered off-axis to
capture hogel’s entire field of view. Calculating the per-SLM-pixel view is
similar to that in the plane wave illumination setup. Points in the red triangles (projected to the blue segment) are disambiguated by examining the
angles with respect to the central ray (dashed green line) of the diffraction
cone for the pixel under consideration.
given by
dcx =
mwhdcz
dF
, dcy =
nwhdcz
dF
. (13)
In camera space, we can define the off-axis projection matrix as
P{m,n} =












2dcz
wh
0
2dcx
wh
0
0
2dcz
wh
2dcy
wh
0
0 0 −
d2+d1
d2−d1
−
2d2d1
d2−d1
0 0 −1 0












. (14)
The sliding window w2 inside each elemental image to disambiguate
the projected pixels is
w2 =

1 −
dcz

d2 + dF − dcz

d2dF
!
w1. (15)
We also analyze the error and performance gain due to choices of
large hogel size by examining the error-free fraction in the sliding
window
w3
w2
= 1 −
(d2 − d1)dcz
d1 (d2 − dcz
)
; (16)
more details are discussed in Section 7.6. We emphasize two extreme
choices of hogel size. When each pixel is treated as an individual
hogel, the proposed holographic light field rendering renders the
exact view perceived by each SLM pixel and produces a dense 4D
light field as that in Huang et al. [2015]. On the other hand, when the
entire SLM is treated as a single hogel, depth cue from intra-occular
occlusion [Zannoli et al. 2016] is ignored and the approach is the
same as that adopted by Maimone et al. [2017].
With the rendered holographic light field, as shown in Figure 4
(top row), we can compute the interference patterns (bottom row)
to be shown on the SLM pixel by accumulating the amplitudes and
phases in the cropped elemental image using the sliding window.
5.3 Calculating Object Wave for Displaying on SLM
In holographic light fields, the one-to-one mapping between an
SLM hogel and its visible elemental image, as shown in Figure 4,
facilitates parallel computation of the Fresnel diffraction integral.
Let p denote a SLM pixel in the (m,n)-th hogel at a displacement
(∆x, ∆y) to the hogel center. In its associated camera space under
spherical wave illumination, p’s spatial coordinate is given by (∆x +
dcx
, ∆y + dcy
, −dcz
). Let p’s estimated view be a sliding window of
Pw2 × Pw2
pixels and define qj be the j-th elemental pixel whose
rendered point is located at (xqj
,yqj
, zqj
) with amplitude Aqj
and
initial phase ϕqj
. The Euclidean distance between p and qj
is given
by
r(p,qj
) =
q
(xqj −∆x−dcy
)
2 + (yqj −∆y−dcy
)
2 + (zqj +dcz
)
2
. (17)
Since the CGH converts a reference wave ER (x) to the object wave
EO (x), we need to pre-multiply the object wave with the conjugate
reference wave—a reference wave that propagates away from the
SLM. The conjugate spherical wave also has to be evaluated in a
per-pixel basis that requires the distance from p to the focus of
spherical wavefront F :
r(p, F ) =
q
d
2
F
+ (mwh + ∆x)
2 + (nwh + ∆y)
2
. (18)
Substituting Equation (1) with variables defined for an on-axis spherical wave illumination establishes the complex amplitude on the
SLM at location p:
E(p) =
*
.
.
,
Pw2
2 X
j=1
Aqj
r(p,qj
)
e
i2π
r (p,qj
)
λ
+ϕqj
+
/
/
- | {z }
Object Wave EO (x )

AF
r(p, F )
e
i2π
r (p, F )
λ
!
| {z }
Conjugate Reference
Wave E
∗
R
(x )
, (19)
where AF denotes the amplitude of conjugate spherical wave. Note
that the CGH under plane wave illumination is simply the object
wave EO (x) in the equation, where r(p,qj
) is reformulated according to plane wave rendering geometry. For paraxial region, Equation (19) can be simplified by the Fresnel approximation to remove
the square root operation. In this paper we aim for an accurate
quadruple sum between ps and qs on the SLM and the rendered
scene plane respectively.
6 IMPLEMENTATION AND RESULT
Our implementation depends on the desired trade-off between field
of view and eye box described in Equation (9) and the available offthe-shelf hardware. In this section, our design requires a minimum
eye box of 10mm size, and we will discuss more design trade-off
spaces for software in Section 7.6 and hardware in Section 7.7.
6.1 Computing Hardware and Software
We render holographic light fields on a PC with an Intel core i7
3.3GHz CPU and 16GB of RAM, and a NVIDIA TitanX GPU. The
multiview holographic light field is rendered with Simultaneous
ACM Transactions on Graphics, Vol. 36, No. 6, Article 236. Publication date: November 2017