Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed
reality and robotic applications. However, scalability brings challenges of
drift in pose estimation, introducing significant errors in the accumulated
model. Approaches often require hours of offline processing to globally
correct model errors. Recent online methods demonstrate compelling results but suffer from (1) needing minutes to perform online correction, preventing true real-time use; (2) brittle frame-to-frame (or frame-to-model)
pose estimation, resulting in many tracking failures; or (3) supporting only
unstructured point-based representations, which limit scan quality and applicability. We systematically address these issues with a novel, real-time,
end-to-end reconstruction framework. At its core is a robust pose estimation
strategy, optimizing per frame for a global set of camera poses by considering the complete history of RGB-D input with an efficient hierarchical
approach. We remove the heavy reliance on temporal tracking and continually localize to the globally optimized frames instead. We contribute
a parallelizable optimization framework, which employs correspondences
based on sparse features and dense geometric and photometric matching.
Our approach estimates globally optimized (i.e., bundle adjusted) poses in
real time, supports robust tracking with recovery from gross tracking failures
(i.e., relocalization), and re-estimates the 3D model in real time to ensure
global consistency, all within a single framework. Our approach outperforms
state-of-the-art online systems with quality on par to offline methods, but
with unprecedented speed and scan completeness. Our framework leads to
a comprehensive online scanning solution for large indoor environments,
enabling ease of use and high-quality results.1
CCS Concepts:  Computing methodologies→Computer graphics;
Shape modeling; Mesh geometry models;
Additional Key Words and Phrases: RGB-D, scan, real-time, global consistency, scalable
1. INTRODUCTION
We are seeing a renaissance in 3D scanning, fueled both by applications such as fabrication, augmented and virtual reality, gaming, and
robotics and by the ubiquity of RGB-D cameras, now even available
in consumer-grade mobile devices. This has opened up the need for
real-time scanning at scale. Here, the user or robot must scan an
entire room (or several spaces) in real time, with instantaneous and
continual integration of the accumulated 3D model into the desired
application, whether that is robot navigation, mapping the physical
world into the virtual, or providing immediate user feedback during
scanning.
However, despite the plethora of reconstruction systems, we have
yet to see a single holistic solution for the problem of real-time 3D
reconstruction at scale that makes scanning easily accessible to
untrained users. This is due to the many requirements that such a
solution needs to fulfill:
High-quality surface modeling. We need a single textured and
noise-free 3D model of the scene, consumable by standard graphics
applications. This requires a high-quality representation that can
model continuous surfaces rather than discrete points.
Scalability. For mixed reality and robot navigation scenarios,
we need to acquire models of entire rooms or several large spaces.
Our underlying representation therefore must handle both smalland large-scale scanning while both preserving global structure and
maintaining high local accuracy.
1Our source code and all reconstruction results are publicly available:
http://graphics.stanford.edu/projects/bundlefusion/.
ACM Transactions on Graphics, Vol. 36, No. 3, Article 24, Publication date: May 2017.
24:2 • A. Dai et al.
Fig. 1. Our novel real-time 3D reconstruction approach solves for global pose alignment and obtains dense volumetric reconstructions at a level of quality
and completeness that was previously only attainable with offline approaches.
Global model consistency. With scale comes the need to correct
pose drift and estimation errors, and the subsequent distortions in
the acquired 3D model. This correction is particularly challenging at
real-time rates but is key for allowing online revisiting of previously
scanned areas or loop closure during actual use.
Robust camera tracking. Apart from incremental errors, camera
tracking can also fail in featureless regions. In order to recover, we
require the ability to relocalize. Many existing approaches rely heavily on proximity to the previous frame, limiting fast camera motion
and recovery from tracking failure. Instead, we need to (re)localize
in a robust manner without relying on temporal coherence.
On-the-fly model updates. In addition to robust tracking, input
data needs to be integrated to a 3D representation and interactively
visualized. The challenge is to update the model after data has been
integrated, in accordance with the newest pose estimates.
Real-time rates. The ability to react to instantaneous feedback
is crucial to 3D scanning and key to obtaining high-quality results.
The real-time capability of a 3D scanning method is fundamental
to AR/VR and robotics applications.
Researchers have studied specific parts of this problem, but to
date there is no single approach to tackle all of these requirements
in real time. This is the very aim of this article, to systematically
address all these requirements in a single, end-to-end real-time reconstruction framework. At the core of our method is a robust pose
estimation strategy, which globally optimizes for the camera trajectory per frame, considering the complete history of RGB-D input
in an efficient local-to-global hierarchical optimization framework.
Since we globally correlate each RGB-D frame, loop closure is
handled implicitly and continuously, removing the need for any
explicit loop closure detection. This enables our method to be extremely robust to tracking failures, with tracking far less brittle than
existing frame-to-frame or frame-to-model RGB-D approaches. If
tracking failures occur, our framework instantaneously relocalizes
in a globally consistent manner, even when scanning is interrupted
and restarted from a completely different viewpoint. Areas can also
be revisited multiple times without problem, and reconstruction
quality continuously improves. This allows for a robust scanning
experience, where even novice users can perform large-scale scans
without failure.
Key to our work is a new fully parallelizable sparse-then-dense
global pose optimization framework: sparse RGB features are used
for coarse global pose estimation, ensuring proposals fall within
the basin of convergence of the following dense step, which considers both photometric and geometric consistency for fine-scale
alignment. Thus, we maintain global structure with implicit loop
closures while achieving high local reconstruction accuracy. To
achieve the corresponding model correction, we extend a scalable
variant of real-time volumetric fusion [Nießner et al. 2013], but
importantly support model updates based on refined poses from our
global optimization. Thus, we can correct errors in the 3D model in
real time and revisit existing scanned areas. We demonstrate how
our approach outperforms current state-of-the-art online systems at
unprecedented speed and scan completeness, and even surpasses
the accuracy and robustness of offline methods in many scenarios.
This leads to a comprehensive real-time scanning solution for large
indoor environments that requires little expertise to operate, making
3D scanning easily accessible to the masses.
In summary, the main contributions of our work are as follows:
(1) A novel, real-time global pose alignment framework that considers the complete history of input frames, removing the brittle and imprecise nature of temporal tracking approaches while
achieving scalability by a rapid hierarchical decomposition of
the problem by using a local-to-global optimization strategy
(2) A sparse-to-dense alignment strategy enabling both consistent
global structure with implicit loop closures and highly accurate
fine-scale pose alignment to facilitate local surface detail
(3) A new RGB-D reintegration strategy to enable on-the-fly and
continuous 3D model updates when refined global pose estimates are available
(4) Large-scale reconstruction of geometry and texture, demonstrating model refinement in revisited areas, recovery from
tracking failures, and robustness to drift and continuous loop
closures.
2. RELATED WORK
There has been extensive work on 3D reconstruction over the past
decades. Key to high-quality 3D reconstruction is the choice of underlying representation for fusing multiple sensor measurements.
Approaches range from unstructured point-based representations
[Rusinkiewicz et al. 2002; Weise et al. 2009; Henry et al. 2010;
Keller et al. 2013; Whelan et al. 2015], 2.5D depth map [Merrell
et al. 2007; Meilland and Comport 2013] or height-field [Gallup
et al. 2010] methods to volumetric approaches based on occupancy
grids [Elfes and Matthies 1987; Wurm et al. 2010] or implicit surfaces [Hilton et al. 1996; Curless and Levoy 1996]. While each has
tradeoffs, volumetric methods based on implicit truncated signed
distance fields (TSDFs) have become the de facto method for the
highest-quality reconstructions (e.g., Levoy et al. [2000], Fuhrmann
ACM Transactions on Graphics, Vol. 36, No. 3, Article 24, Publication date: May 2017.
BundleFusion: Real-Time Globally Consistent 3D Reconstruction Using On-the-Fly Surface Reintegration • 24:3
Fig. 2. Our global pose optimization takes as input the RGB-D stream of a commodity sensor, detects pairwise correspondences between the input frames,
and performs a combination of local and global alignment steps using sparse and dense correspondences to compute per-frame pose estimates.
and Goesele [2014], and Fioraio et al. [2015]). They model continuous surfaces, systematically regularize noise, remove the need for
explicit topology bookkeeping, and efficiently perform incremental
updates. The most prominent recent example is KinectFusion [Newcombe et al. 2011a; Izadi et al. 2011], where real-time volumetric
fusion of smaller scenes was demonstrated.
One inherent issue with these implicit volumetric methods is their
lack of scalability due to reliance on a uniform grid. This has become
a focus of much recent research [Roth and Vona 2012; Whelan
et al. 2012; Zeng et al. 2012; Chen et al. 2013; Keller et al. 2013;
Nießner et al. 2013; Steinbruecker et al. 2014; Zhang et al. 2015;
Reichl et al. 2015], where real-time efficient data structures for
volumetric fusion have been proposed. These exploit the sparsity in
the TSDF representation to create more efficient spatial subdivision
strategies. While this allows for volumetric fusion at scale, pose
estimates suffer from drift, causing distortions in the 3D model.
Even small pose errors, seemingly negligible on a small local scale,
can accumulate to dramatic error in the final 3D model [Nießner
et al. 2013]. Zhang et al. [2015] use planar structural priors and
repeated object detection to reduce the effect of drift; however, they
do not detect loop closures or use color data, which makes tracking
difficult in open or planar areas or very cluttered scenes.
Most of the research on achieving globally consistent 3D models
at scale from RGB-D input requires offline processing and access
to all input frames. Li et al. [2013], Zhou and Koltun [2013], Zhou
et al. [2013], Zhou and Koltun [2014], and Choi et al. [2015] provide
for globally consistent models by optimizing across the entire pose
trajectory but require minutes or even hours of processing time,
meaning real-time revisiting or refinement of reconstructed areas is
infeasible.
Real-time, drift-free pose estimation is a key focus in the simultaneous localization and mapping (SLAM) literature. Many real-time
monocular RGB methods have been proposed, including sparse
[Klein and Murray 2007], semidense [Engel et al. 2013; Forster
et al. 2014], or direct methods [Meilland et al. 2011; Engel et al.
2014]. Typically these approaches rely on either pose-graph optimization [Kummerle et al. 2011] or bundle adjustment [Triggs ¨
et al. 2000], minimizing reprojection error across frames and/or
distributing the error across the graph. While impressive tracking
results have been shown using only monocular RGB sensors, these
approaches do not generate detailed dense 3D models, which is the
aim of our work.
MonoFusion [Pradeep et al. 2013] augments sparse SLAM bundle adjustment with dense volumetric fusion, showing compelling
monocular results but on small-scale scenes. Real-time SLAM
approaches typically first estimate posesframe to frame and perform
correction in a background thread (running slower than real-time
rates; e.g., 1Hz). In contrast, DTAM [Newcombe et al. 2011b]
uses the concept of frame-to-model tracking (from KinectFusion
[Newcombe et al. 2011a; Izadi et al. 2011]) to estimate the pose
directly from the reconstructed dense 3D model. This omits the
need for a correction step but clearly does not scale to larger scenes.
Pose estimation from range data typically is based on variants of
the iterative closest point (ICP) algorithm [Besl and McKay 1992;
Rusinkiewicz and Levoy 2001]. In practice, this makes tracking
extremely brittle and has led researchers to explore either the use
of RGB data to improve frame-to-frame tracking [Whelan et al.
2013a] or the use of global pose estimation correction, including
pose graph optimization [Steinbruecker et al. 2013], loop closure
detection [Whelan et al. 2013b], incremental bundle adjustment
[Whelan et al. 2015; Fioraio et al. 2015], or recovery from tracking
failures by image or keypoint-based relocalization [Glocker et al.
2015; Valentin et al. 2015].
These systems are state of the art in terms of online correction of
both pose and underlying 3D model. However, they require many
seconds or even minutes to perform online optimization [Whelan
et al. 2013b; Fioraio et al. 2015]; assume very specific camera trajectories to detect explicit loop closures, limiting free-form camera
motions and scanning [Whelan et al. 2013b]; rely on computing optimized camera poses prior to fusion, limiting the ability to refine the
model afterward [Steinbruecker et al. 2013]; or use point-based representations that limit quality and lack general applicability where
continuous surfaces are needed [Whelan et al. 2015].
3. METHOD OVERVIEW
The core of our approach is an efficient global pose optimization
algorithm that operates in unison with a large-scale, real-time 3D
reconstruction framework; see Figure 2. At every frame, we continuously run pose optimization and update the reconstruction according to the newly computed pose estimates. We do not strictly rely on
temporal coherence, allowing for free-form camera paths, instantaneous relocalization, and frequent revisiting of the same scene
region. This makes our approach robust toward sensor occlusion,
fast frame-to-frame motions, and featureless regions.
We take as input the RGB-D stream captured by a commodity
depth sensor. To obtain global alignment, we perform a sparse-thendense global pose optimization: we use a set of sparse feature correspondences to obtain a coarse global alignment, as sparse features
inherently provide for loop closure detection and relocalization.
ACM Transactions on Graphics, Vol. 36, No. 3, Article 24, Publication date: May 2017.
24:4 • A. Dai et al.
This alignment is then refined by optimizing for dense photometric
and geometric consistency. Sparse correspondences are established
through pairwise Scale-Invariant Feature Transform (SIFT) [Lowe
2004] feature correspondences between all input frames (see Section 4.1). That is, detected SIFT keypoints are matched against all
previous frames and carefully filtered to remove mismatches, thus
avoiding false loop closures (see Section 4.1.1).
To make real-time global pose alignment tractable, we perform
a hierarchical local-to-global pose optimization (see Section 4.2)
using the filtered frame correspondences. On the first hierarchy
level, every consecutive n frames compose a chunk, which is locally
pose optimized under the consideration of its contained frames. On
the second hierarchy level, all chunks are correlated with respect
to each other and globally optimized. This is akin to hierarchical submapping [Maier et al. 2014]; however, instead of analyzing
global connectivity once all frames are available, our new method
forms chunks based on the current temporal window. Note that
this is our only temporal assumption; between chunks there is no
temporal reliance.
This hierarchical two-stage optimization strategy reduces the
number of unknowns per optimization step and ensures our method
scales to large scenes. Pose alignment on both levels is formulated as an energy minimization problem in which both the filtered
sparse correspondences and the dense photometric and geometric
constraints are considered (see Section 4.3). To solve this highly
nonlinear optimization problem on both hierarchy levels, we employ a fast data-parallel GPU-solver tailored to the problem (see
Section 4.4).
A dense scene reconstruction is obtained using a sparse volumetric representation and fusion [Nießner et al. 2013], which scales to
large scenes in real-time. The continuous change in the optimized
global poses necessitates continuous updates to the global 3D scene
representation (see Section 5). A key novelty is to allow for symmetric on-the-fly reintegration of RGB-D frames. In order to update the
pose of a frame with an improved estimate, we remove the RGB-D
image at the old pose with a new real-time de-integration step and
reintegrate it at the new pose. Thus, the volumetric model continuously improves as more RGB-D frames and refined pose estimates
become available; for example, if a loop is closed (cf. Figure 13).
4. GLOBAL POSE ALIGNMENT
We first describe the details of our real-time global pose optimization strategy, which is the foundation for online, globally consistent
3D reconstruction. Input to our approach is the live RGB-D stream
S = {fi = (Ci, Di)}i captured by a commodity sensor. We assume
spatially and temporally aligned color Ci and depth data Di at each
frame, captured at 30Hz and 640×480 pixel resolution. The goal is
to find a set of 3D correspondences between the frames in the input
sequence, and then find an optimal set of rigid camera transforms
{Ti} such that all frames align as best as possible. The transformation Ti(p) = Rip + ti (rotation Ri, translation ti) maps from the
local camera coordinates of the ith frame to the world space coordinate system; we assume the first frame defines the world coordinate
system.
4.1 Feature Correspondence Search
In our framework, we first search for sparse correspondences between frames using efficient feature detection, feature matching,
and correspondence filtering steps. These sparse correspondences
are later used in tandem with dense photometric correspondences,
but since accurate sparse correspondences are crucial to attaining
the basin of convergence of the dense optimization, we elaborate
on their search and filtering later. For each new frame, SIFT features are detected and matched to the features of all previously seen
frames. We use SIFT as it accounts for the major variation encountered during hand-held RGB-D scanning, namely, image translation,
scaling, and rotation. Potential matches between each pair of frames
are then filtered to remove false positives and produce a list of valid
pairwise correspondences as input to global pose optimization. Our
correspondence search is performed entirely on the GPU, avoiding
the overhead of copying data (e.g., feature locations, descriptors,
matches) to the host. We compute SIFT keypoints and descriptors
at 4−5 ms per frame and match a pair of frames in ≈0.05ms (in
parallel). We can thus find full correspondences in real time against
up to over 20K frames, matched in a hierarchical fashion, for every
new input RGB-D image.
4.1.1 Correspondence Filtering. To minimize outliers, we filter the sets of detected pairwise correspondences based on geometric and photometric consistency. Note that further robustness
checks are built into the optimization (not described in this section;
see Section 4.4.1 for details).
Key Point Correspondence Filter. For a pair of frames fi and fj
with detected corresponding 3D points P from fi, and Q from fj ,
the key point correspondence filter finds a set of correspondences
that exhibit a stable distribution and a consistent rigid transform.
Correspondences are greedily aggregated (in order of match distance); for each newly added correspondence, we compute the rigid
transform Tij (p) = (T −1
j ◦ Ti)(p), which minimizes the RMSD between the current set of correspondences Pcur and Qcur, using the
Kabsch algorithm [Kabsch 1976; Gower 1975]. We further check
whether this is an ambiguously determined transform (e.g., the
correspondences lie on a line or exhibit rotational symmetry) by
performing a condition analysis of the covariance of points of Pcur
and Qcur as well as the cross-covariance between Pcur and Qcur; if
any of these condition numbers are high (>100), then the system
is considered unstable. Thus, if the reprojection error under Tij is
high (max residual > 0.02m) or the condition analysis determines
instability, then correspondences are removed (in order of reprojection error) until this is not the case anymore or there are too few
correspondences to determine a rigid transform. If the resulting set
of correspondences for fi and fj do not produce a valid transform,
all correspondences between fi and fj are discarded.
Surface Area Filter. In addition, we check that the surface
spanned by the features is large enough, as correspondences spanning a small physical size are prone to ambiguity. For frames fi and
fj , we estimate the surface areas spanned by the 3D keypoints P
of fi and by the 3D keypoints Q of fj . For each set of 3D points,
we project them into the plane given by their two principal axes,
with surface area given by the 2D oriented bounding box of the
resulting projected points. If the areas spanned by P and Q are
insufficient (<0.032m2), the set of matches is deemed ambiguous
and discarded.
Dense Verification. Finally, we perform a dense two-sided geometric and photometric verification step. For frames fi and fj ,
we use the computed relative transform Tij from the key point correspondence filter to align the coordinate systems of fi and fj .
We measure the average depth discrepancy, normal deviation, and
photoconsistency of the reprojection in both directions in order
to find valid pixel correspondences and compute the reprojection
error of these correspondences. For efficiency reasons, this step
is performed on filtered and downsampled input frames of size
w × h = 80 × 60. Note that when a new RGB-D image fi arrives,
its filtered and downsampled color intensity Clow
i and depth Dlow
i are
ACM Transactions on Graphics, Vol. 36, No. 3, Article 24, Publication date: May 2017.
BundleFusion: Real-Time Globally Consistent 3D Reconstruction Using On-the-Fly Surface Reintegration • 24:5
cached for efficiency. The camera space positions Plow
i and normals
Nlow
i of each Dlow
i are also computed and cached per frame. With
π denoting the camera intrinsics for the downsampled images, the
total reprojection error from fi to fj is
Er(fi, fj ) =
x,y

Tij (pi,x,y ) − qj,x,y


2.
Here, pi,x,y = Plow
i (x, y) and qj,x,y = Plow
j (π−1(Tijpi,x,y )). However, this is sensitive to occlusion error, so we discard correspondences with high depth discrepancy, normal deviation, or lack of
photoconsistency. That is, the potential correspondence at pixel location (x, y) is considered valid if the following conditions hold:

Tij (pi,x,y ) − qj,x,y


2 < τd
(Tij (ni,x,y )) · nj,x,y > τn
Clow
i (x, y) − Clow
j (x, y)


1 < τc.
Matches between fi and fj are invalidated in the case of excessive
reprojection error (>0.075m) or insufficient valid correspondences
(<0.02w
h
), and we use τd = 0.15m, τn = 0.9, τc = 0.1. This
check is efficiently implemented with a single kernel call, such that
each thread block handles one pair of images, with reprojection
computed through local reductions.
If all checks are passed, the correspondences are added to the
valid set, which is used later on for pose optimization. We only
consider a frame-to-frame match if the valid set comprises at least
Nmin correspondences. Note that Nmin = 3 is sufficient to define a
valid frame-to-frame transform; however, we found Nmin = 5 to be
a good compromise between precision and recall.
4.2 Hierarchical Optimization
In order to run at real-time rates on up to tens of thousands of RGBD input frames, we apply a hierarchical optimization strategy. The
input sequence is split into short chunks of consecutive frames. On
the lowest hierarchy level, we optimize for local alignments within
a chunk. On the second hierarchy level, chunks are globally aligned
against each other, using representative keyframes with associated
features per chunk.
Local Intrachunk Pose Optimization. Intrachunk alignment is
based on chunks of Nchunk = 11 consecutive frames in the input
RGB-D stream; adjacent chunks overlap by one frame. The goal
of local pose optimization is to compute the best intrachunk alignments {Ti}, relative to the first frame of the chunk, which locally
defines the reference frame. To this end, valid feature correspondences are searched between all pairs of frames of the chunk, and
then the energy minimization approach described in Section 4.3 is
applied, jointly considering both these feature correspondences and
dense photometric and geometric matching. Since each chunk only
contains a small number of consecutive frames, the pose variation
within the chunk is small, and we can initialize each of the Ti to
the identity matrix. To ensure that the local pose optimization result
after convergence is sufficiently accurate, we apply the Dense Verification test (see Section 4.1.1) to each pair of images within the
chunk using the optimized local trajectory. If the reprojection error
is too large for any pair of images (>0.05m), the chunk is discarded
and not used in the global optimization.
Per-Chunk Keyframes. Once a chunk has been completely
processed, we define the RGB-D data from the first frame in the
chunk to be the chunk’s keyframe. We also compute a representative aggregate keyframe feature set. Based on the optimized pose
trajectory of the chunk, we compute a coherent set of 3D positions
of the intrachunk feature points in world space. These 3D positions
may contain multiple instances of the same real-world point, found
in separate pairwise frame matches. Thus, to obtain the keyframe
feature set, we aggregate the feature point instances that have previously found (intrachunk) matches. Those that coincide in 3D world
space (<0.03m) are merged to one best 3D representative in the
least squares sense. This keyframe feature set is projected into the
space of the keyframe using the transformations from the frames of
origin, resulting in a consistent set of feature locations and depths.
Note that once this global keyframe and keyframe feature set is
created, the chunk data (i.e., intrachunk features, descriptors, correspondences) can be discarded as it is not needed in the second layer
pose alignment.
Global Interchunk Pose Optimization. Sparse correspondence
search and filtering between global keyframes is analogous to that
within a chunk, but on the level of all keyframes and their feature
sets. If a global keyframe does not find any matches to previously
seen keyframes, it is marked as invalid but kept as a candidate, allowing for revalidation when it finds a match to a keyframe observed
in the future. The global pose optimization computes the best global
alignments {Ti} for the set of all global keyframes, thus aligning all
chunks globally. Again, the same energy minimization approach
from Section 4.3 is applied using both sparse and dense constraints.
Intrachunk alignment runs after each new global keyframe has found
correspondences. The pose for a global keyframe is initialized with
the delta transform computed by the corresponding intrachunk optimization, composed with the previous global keyframe pose. After
the intrachunk transforms have been computed, we obtain globally
consistent transforms among all input frames by applying the corresponding delta transformations (from the local optimization) to
all frames in a chunk.
4.3 Pose Alignment as Energy Optimization
Given a set of 3D correspondences between a set of frames S (frames
in a chunk or keyframes, depending on hierarchy level), the goal of
pose alignment is to find an optimal set of rigid camera transforms
{Ti} per frame i (for simpler notation, we henceforth write i for fi)
such that all frames align as best as possible. We parameterize the
4 × 4 rigid transform Ti using matrix exponentials based on skewsymmetric matrix generators [Murray et al. 1994], which yields fast
convergence. This leaves three unknown parameters for rotation and
three for translation. For ease of notation, we stack the degrees of
freedom for all |S| frames in a parameter vector:
X = (R0, t0,..., R|S|, t|S|)
T = (x0,...,xN )
T .
Here, N is the total number of variables xi. Given this notation, we
phrase the alignment problem as a variational nonlinear least squares
minimization problem in the unknown parameters X . To this end,
we define the following alignment objective, which is based on
sparse features and dense photometric and geometric constraints:
Ealign(X ) = wsparseEsparse(X ) + wdenseEdense(X ).
Here, wsparse and wdense are weights for the sparse and dense matching terms, respectively. wdense is linearly increased; this allows the
sparse term to first find a good global structure, which is then refined
with the dense term (as the poses fall into the basin of convergence of
the dense term, it becomes more reliable), thus achieving coarse-tofine alignment. Note that depending on the optimization hierarchy
level, the reference frame is the first frame in the chunk (for intrachunk alignment) or the first frame in the entire input sequence (for
global interchunk alignment). Hence, the reference transform T0 is
not a free variable and is left out from the optimization.
ACM Transactions on Graphics, Vol. 36, No. 3, Article 24, Publication date: May 2017.