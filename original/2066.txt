A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a
physical simulation, thus enabling realistic responses to perturbations and
environmental variation. We show that well-known reinforcement learning
(RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex
recoveries, adapting to changes in morphology, and accomplishing userspecified goals. Our method handles keyframed motions, highly-dynamic
actions such as motion-captured flips and spins, and retargeted motions. By
combining a motion-imitation objective with a task objective, we can train
characters that react intelligently in interactive settings, e.g., by walking in a
desired direction or throwing a ball at a user-specified target. This approach
thus combines the convenience and motion quality of using motion clips to
define the desired style and appearance, with the flexibility and generality
afforded by RL methods and physics-based animation. We further explore a
number of methods for integrating multiple clips into the learning process
to develop multi-skilled agents capable of performing a rich repertoire of
diverse skills. We demonstrate results using multiple characters (human,
Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including
locomotion, acrobatics, and martial arts.
CCS Concepts: • Computing methodologies → Animation; Physical
simulation; Control methods; Reinforcement learning;
Additional Key Words and Phrases: physics-based character animation, motion control, reinforcement learning
1 INTRODUCTION
Physics-based simulation of passive phenomena, such as cloth and
fluids, has become nearly ubiquitous in industry. However, the adoption of physically simulated characters has been more modest. Modeling the motion of humans and animals remains a challenging problem, and currently, few methods exist that can simulate the diversity
of behaviors exhibited in the real world. Among the enduring challenges in this domain are generalization and directability. Methods
that rely on manually designed controllers have produced compelling results, but their ability to generalize to new skills and new
situations is limited by the availability of human insight. Though
humans are adept at performing a wide range of skills themselves,
it can be difficult to articulate the internal strategies that underly
such proficiency, and more challenging still to encode them into a
controller. Directability is another obstacle that has impeded the
adoption of simulated characters. Authoring motions for simulated
characters remains notoriously difficult, and current interfaces still
cannot provide users with an effective means of eliciting the desired
behaviours from simulated characters.
Reinforcement learning (RL) provides a promising approach for
motion synthesis, whereby an agent learns to perform various skills
through trial-and-error, thus reducing the need for human insight.
While deep reinforcement learning has been demonstrated to produce a range of complex behaviors in prior work [Duan et al. 2016;
Heess et al. 2016; Schulman et al. 2015b], the quality of the generated
motions has thus far lagged well behind state-of-the-art kinematic
methods or manually designed controllers. In particular, controllers
trained with deep RL exhibit severe (and sometimes humorous) artifacts, such as extraneous upper body motion, peculiar gaits, and unrealistic posture [Heess et al. 2017].1 A natural direction to improve
the quality of learned controllers is to incorporate motion capture
or hand-authored animation data. In prior work, such systems have
typically been designed by layering a physics-based tracking controller on top of a kinematic animation system [Da Silva et al. 2008;
Lee et al. 2010a]. This type of approach is challenging because the
kinematic animation system must produce reference motions that
1
See, for example, https://youtu.be/hx_bgoTF7bs
ACM Trans. Graph., Vol. 37, No. 4, Article 143. Publication date: August 2018.
143:2 • Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne
are feasible to track, and the resulting physics-based controller is
limited in its ability to modify the motion to achieve plausible recoveries or accomplish task goals in ways that deviate substantially
from the kinematic motion. Furthermore, such methods tend to be
quite complex to implement.
An ideal learning-based animation system should allow an artist
or motion capture actor to supply a set of reference motions for style,
and then generate goal-directed and physically realistic behavior
from those reference motions. In this work, we take a simple approach to this problem by directly rewarding the learned controller
for producing motions that resemble reference animation data, while
also achieving additional task objectives. We also demonstrate three
methods for constructing controllers from multiple clips: training
with a multi-clip reward based on a max operator; training a policy
to perform multiple diverse skills that can be triggered by the user;
and sequencing multiple single-clip policies by using their value
functions to estimate the feasibility of transitions.
The central contribution of our paper is a framework for physicsbased character animation that combines goal-directed reinforcement learning with data, which may be provided in the form of
motion capture clips or keyframed animations. Although our framework consists of individual components that have been known for
some time, the particular combination of these components in the
context of data-driven and physics-based character animation is
novel and, as we demonstrate in our experiments, produces a wide
range of skills with motion quality and robustness that substantially exceed prior work. By incorporating motion capture data into
a phase-aware policy, our system can produce physics-based behaviors that are nearly indistinguishable in appearance from the
reference motion in the absence of perturbations, avoiding many of
the artifacts exhibited by previous deep reinforcement learning algorithms, e.g., [Duan et al. 2016]. In the presence of perturbations or
modifications, the motions remain natural, and the recovery strategies exhibit a high degree of robustness without the need for human
engineering. To the best of our knowledge, we demonstrate some
of the most capable physically simulated characters produced by
learning-based methods. In our ablation studies, we identify two
specific components of our method, reference state initialization
and early termination, that are critical for achieving highly dynamic
skills. We also demonstrate several methods for integrating multiple
clips into a single policy.
2 RELATED WORK
Modeling the skilled movement of articulated figures has a long
history in fields ranging from biomechanics to robotics and animation. In recent years, as machine learning algorithms for control
have matured, there has also been an increase in interest in these
problems from the machine learning community. Here we focus on
the most closely related work in animation and RL.
Kinematic Models: Kinematic methods have been an enduring avenue of work in character animation that can be effective when large
amounts of data are available. Given a dataset of motion clips, controllers can be built to select the appropriate clip to play in a given
situation, e.g., [Agrawal and van de Panne 2016; Lee et al. 2010b;
Safonova and Hodgins 2007]. Gaussian processes have been used to
learn latent representations which can then synthesize motions at
runtime [Levine et al. 2012; Ye and Liu 2010b]. Extending this line
of work, deep learning models, such as autoencoders and phasefunctioned networks, have also been applied to develop generative
models of human motion in a kinematic setting [Holden et al. 2017,
2016]. Given high quality data, data-driven kinematic methods will
often produce higher quality motions than most simulation-based
approaches. However, their ability to synthesize behaviors for novel
situations can be limited. As tasks and environments become complex, collecting enough motion data to provide sufficient coverage
of the possible behaviors quickly becomes untenable. Incorporating physics as a source of prior knowledge about how motions
should change in the presence of perturbations and environmental
variation provides one solution to this problem, as discussed below.
Physics-based Models: Design of controllers for simulated characters remains a challenging problem, and has often relied on human
insight to implement task-specific strategies. Locomotion in particular has been the subject of considerable work, with robust controllers
being developed for both human and nonhuman characters, e.g.,
[Coros et al. 2010; Ye and Liu 2010a; Yin et al. 2007]. Many such
controllers are the products of an underlying simplified model and
an optimization process, where a compact set of parameters are
tuned in order to achieve the desired behaviors [Agrawal et al. 2013;
Ha and Liu 2014; Wang et al. 2012]. Dynamics-aware optimization
methods based on quadratic programming have also been applied to
develop locomotion controllers [Da Silva et al. 2008; Lee et al. 2010a,
2014]. While model-based methods have been shown to be effective
for a variety of skills, they tend to struggle with more dynamics
motions that require long-term planning, as well as contact-rich
motions. Trajectory optimization has been explored for synthesizing physically-plausible motions for a variety of tasks and characters [Mordatch et al. 2012; Wampler et al. 2014]. These methods
synthesize motions over an extended time horizon using an offline
optimization process, where the equations of motion are enforced as
constraints. Recent work has extended such techniques into online
model-predictive control methods [Hämäläinen et al. 2015; Tassa
et al. 2012], although they remain limited in both motion quality
and capacity for long-term planning. The principal advantage of our
method over the above approaches is that of generality. We demonstrate that a single model-free framework is capable of a wider range
of motion skills (from walks to highly dynamic kicks and flips) and
an ability to sequence these; the ability to combine motion imitation
and task-related demands; compact and fast-to-compute control
policies; and the ability to leverage rich high-dimensional state and
environment descriptions.
Reinforcement Learning: Many of the optimization techniques
used to develop controllers for simulated characters are based on
reinforcement learning. Value iteration methods have been used
to develop kinematic controllers to sequence motion clips in the
context of a given task [Lee et al. 2010b; Levine et al. 2012]. Similar
approaches have been explored for simulated characters [Coros et al.
2009; Peng et al. 2015]. More recently, the introduction of deep neural network models for RL has given rise to simulated agents that can
perform a diverse array of challenging tasks [Brockman et al. 2016a;
Duan et al. 2016; Liu and Hodgins 2017; Peng et al. 2016; Rajeswaran
ACM Trans. Graph., Vol. 37, No. 4, Article 143. Publication date: August 2018.
DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills • 143:3
et al. 2017; Teh et al. 2017]. Policy gradient methods have emerged
as the algorithms of choice for many continuous control problems
[Schulman et al. 2015a, 2017; Sutton and Barto 1998]. Although
RL algorithms have been capable of synthesizing controllers using
minimal task-specific control structures, the resulting behaviors
generally appear less natural than their more manually engineered
counterparts [Merel et al. 2017; Schulman et al. 2015b]. Part of the
challenge stems from the difficulty in specifying reward functions
for natural movement, particularly in the absence of biomechanical
models and objectives that can be used to achieve natural simulated
locomotion [Lee et al. 2014; Wang et al. 2012]. Naïve objectives for
torque-actuated locomotion, such as forward progress or maintaining a desired velocity, often produce gaits that exhibit extraneous
motion of the limbs, asymmetric gaits, and other objectionable artifacts. To mitigate these artifacts, additional objectives such as effort
or impact penalties have been used to discourage these undesirable
behaviors. Crafting such objective functions requires a substantial
degree of human insight, and often yields only modest improvements. Alternatively, recent RL methods based on the imitation of
motion capture, such as GAIL [Ho and Ermon 2016], address the
challenge of designing a reward function by using data to induce an
objective. While this has been shown to improve the quality of the
generated motions, current results still do not compare favorably to
standard methods in computer animation [Merel et al. 2017]. The
DeepLoco system [Peng et al. 2017a] takes an approach similar to
the one we use here, namely adding an imitation term to the reward function, although with significant limitations. It uses fixed
initial states and is thus not capable of highly dynamic motions; it is
demonstrated only on locomotion tasks defined by foot-placement
goals computed by a high-level controller; and it is applied to a
single armless biped model. Lastly, the multi-clip demonstration
involves a hand-crafted procedure for selecting suitable target clips
for turning motions.
Motion Imitation: Imitation of reference motions has a long history in computer animation. An early instantiation of this idea was
in bipedal locomotion with planar characters [Sharon and van de
Panne 2005; Sok et al. 2007], using controllers tuned through policy
search. Model-based methods for tracking reference motions have
also been demonstrated for locomotion with 3D humanoid characters [Lee et al. 2010a; Muico et al. 2009; Yin et al. 2007]. Reference
motions have also been used to shape the reward function for deep
RL to produce more natural locomotion gaits [Peng et al. 2017a,b]
and for flapping flight [Won et al. 2017]. In our work, we demonstrate
the capability to perform a significantly broader range of difficult
motions: highly dynamic spins, kicks, and flips with intermittent
ground contact, and we show that reference-state initialization and
early termination are critical to their success. We also explore several
options for multi-clip integration and skill sequencing.
The work most reminiscent of ours in terms of capabilities is the
Sampling-Based Controller (SAMCON) [Liu et al. 2016, 2010]. An
impressive array of skills has been reproduced by SAMCON, and to
the best of our knowledge, SAMCON has been the only system to
demonstrate such a diverse corpus of highly dynamic and acrobatic
motions with simulated characters. However, the system is complex,
having many components and iterative steps, and requires defining
a low dimensional state representation for the synthesized linear
feedback structures. The resulting controllers excel at mimicking
the original reference motions, but it is not clear how to extend the
method for task objectives, particularly if they involve significant
sensory input. A more recent variation introduces deep Q-learning
to train a high-level policy that selects from a precomputed collection of SAMCON control fragments [Liu and Hodgins 2017]. This
provides flexibility in the order of execution of the control fragments,
and is demonstrated to be capable of challenging non-terminating
tasks, such as balancing on a bongo-board and walking on a ball. In
this work, we propose an alternative framework using deep RL, that
is conceptually much simpler than SAMCON, but is nonetheless
able to learn highly dynamic and acrobatic skills, including those
having task objectives and multiple clips.
3 OVERVIEW
Our system receives as input a character model, a corresponding
set of kinematic reference motions, and a task defined by a reward
function. It then synthesizes a controller that enables the character to
imitate the reference motions, while also satisfying task objectives,
such as striking a target or running in a desired direction over
irregular terrain. Each reference motion is represented as a sequence
of target poses {qˆt }. A control policy π(at
|st
,дt ) maps the state of
the characterst
, a task-specific goal дt to an action at
, which is then
used to compute torques to be applied to each of the character’s
joints. Each action specifies target angles for proportional-derivative
(PD) controllers that then produce the final torques applied at the
joints. The reference motions are used to define an imitation reward
r
I
(st
, at ), and the goal defines a task-specific reward r
G (st
, at
,дt ).
The final result of our system is a policy that enables a simulated
character to imitate the behaviours from the reference motions
while also fulfilling the specified task objectives. The policies are
modeled using neural networks and trained using the proximal
policy optimization algorithm [Schulman et al. 2017].
4 BACKGROUND
Our tasks will be structured as standard reinforcement learning
problems, where an agent interacts with an environment according
to a policy in order to maximize a reward. In the interest of brevity,
we will exclude the goal д, from the notation, but the following
discussion readily generalizes to include this. A policy π(a|s) models
the conditional distribution over action a ∈ A given a state s ∈ S.
At each control timestep, the agent observes the current state st
and samples an action at from π. The environment then responds
with a new state s
′ = st+1, sampled from the dynamics p(s
′
|s, a),
and a scalar reward rt that reflects the desirability of the transition.
For a parametric policy πθ
(a|s), the goal of the agent is to learn the
optimal parameters θ
∗
that maximizes its expected return
J(θ) = Eτ ∼pθ (τ )
"Õ
T
t=0
γ
t
rt
#
,
where pθ
(τ ) = p(s0)
ÎT −1
t=0
p(st+1 |st
, at )πθ
(at
|st ) is the distribution over all possible trajectories τ = (s0, a0,s1, ..., aT −1,sT ) induced by the policy πθ
, with p(s0) being the initial state distribution.
ÍT
t=0
γ
t
rt represents the total return of a trajectory, with a horizon
ACM Trans. Graph., Vol. 37, No. 4, Article 143. Publication date: August 2018.
143:4 • Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne
of T steps. T may or may not be infinite, and γ ∈ [0, 1] is a discount
factor that can be used to ensure the return is finite. A popular class
of algorithms for optimizing a parametric policy is policy gradient
methods [Sutton et al. 2001], where the gradient of the expected
return ▽θ
J(θ) is estimated with trajectories sampled by following
the policy. The policy gradient can be estimated according to
▽θ
J(θ) = Est ∼dθ (st ),at ∼πθ (at |st )
[▽θ
log(πθ
(at
|st ))At ],
where dθ
(st ) is the state distribution under the policy πθ
. At represents the advantage of taking an action at at a given state st
At = Rt −V (st ).
Rt =
ÍT −t
l=0
γ
l
rt+l denotes the return received by a particular trajectory starting from state st at time t. V (st ) is a value function
that estimates the average return of starting in st and following the
policy for all subsequent steps
V (st ) = E [Rt
|πθ
,st ] .
The policy gradient can therefore be interpreted as increasing the
likelihood of actions that lead to higher than expected returns, while
decreasing the likelihood of actions that lead to lower than expected
returns. A classic policy gradient algorithm for learning a policy
using this empirical gradient estimator to perform gradient ascent
on J(θ) is REINFORCE [Williams 1992].
Our policies will be trained using the proximal policy optimization
algorithm [Schulman et al. 2017], which has demonstrated stateof-the-art results on a number of challenging control problems.
The value function will be trained using multi-step returns with
TD(λ). The advantages for the policy gradient will be computed
using the generalized advantage estimator GAE(λ) [Schulman et al.
2015b]. A more in-depth review of these methods can be found in
the supplementary material.
5 POLICY REPRESENTATION
Given a reference motion clip, represented by a sequence of target
poses {qˆt }, the goal of the policy is to reproduce the desired motion in a physically simulated environment, while also satisfying
additional task objectives. Since a reference motion only provides
kinematic information in the form of target poses, the policy is
responsible for determining which actions should be applied at each
timestep in order to realize the desired trajectory.
5.1 States and Actions
The state s describes the configuration of the character’s body, with
features consisting of the relative positions of each link with respect
to the root (designated to be the pelvis), their rotations expressed in
quaternions, and their linear and angular velocities. All features are
computed in the character’s local coordinate frame, with the root
at the origin and the x-axis along the root link’s facing direction.
Since the target poses from the reference motions vary with time, a
phase variable ϕ ∈ [0, 1] is also included among the state features.
ϕ = 0 denotes the start of a motion, and ϕ = 1 denotes the end. For
cyclic motions, ϕ is reset to 0 after the end of each cycle. Policies
trained to achieve additional task objectives, such as walking in a
particular direction or hitting a target, are also provided with a goal
д, which can be treated in a similarly fashion as the state. Specific
goals used in the experiments are discussed in section 9. The action
a from the policy specifies target orientations for PD controllers at
each joint. The policy is queried at 30Hz, and target orientations
for spherical joints are represented in axis-angle form, while targets
for revolute joints are represented by scalar rotation angles. Unlike
the standard benchmarks, which often operate directly on torques,
our use of PD controllers abstracts away low-level control details
such as local damping and local feedback. Compared to torques, PD
controllers have been shown to improve performance and learning
speed for certain motion control tasks [Peng et al. 2017b].
5.2 Network
Each policy π is represented by a neural network that maps a given
state s and goal д to a distribution over action π(a|s,д). The action
distribution is modeled as a Gaussian, with a state dependent mean
µ(s)specified by the network, and a fixed diagonal covariance matrix
Σ that is treated as a hyperparameter of the algorithm:
π(a|s) = N(µ(s), Σ).
The inputs are processed by two fully-connected layers with 1024,
and 512 units each, followed by a linear output layer. ReLU activations are used for all hidden units. The value function is modeled
by a similar network, with exception of the output layer, which
consists of a single linear unit.
For vision-based tasks, discussed in section 9, the inputs are augmented with a heightmap H of the surrounding terrain, sampled on
a uniform grid around the character. The policy and value networks
are augmented accordingly with convolutional layers to process
the heightmap. A schematic illustration of this visuomotor policy
network is shown in Figure 2. The heightmap is first processed by a
series of convolutional layers, followed by a fully-connected layer.
The resulting features are then concatenated with the input state s
and goal д, and processed by a similar fully-connected network as
the one used for tasks that do not require vision.
5.3 Reward
The reward rt at each step t consists of two terms that encourage
the character to match the reference motion while also satisfying
Fig. 2. Schematic illustration of the visuomotor policy network. The
heightmap H is processed by 3 convolutional layers with 16 8x8 filters,
32 4x4 filters, and 32 4x4 filters. The feature maps are then processed by
64 fully-connected units. The resulting features are concatenated with the
input state s and goal д and processed by by two fully-connected layer with
1024 and 512 units. The output µ(s) is produced by a layer of linear units.
ReLU activations are used for all hidden layers. For tasks that do not require
a heightmap, the networks consist only of layers 5-7.
ACM Trans. Graph., Vol. 37, No. 4, Article 143. Publication date: August 2018.
DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills • 143:5
additional task objectives:
rt = ω
I
r
I
t + ω
Gr
G
t
.
Here, r
I
t
and r
G
t
represent the imitation and task objectives, with
ω
I
and ω
G being their respective weights. The task objective r
G
t
incentivizes the character to fulfill task-specific objectives, the details
of which will be discussed in the following section. The imitation
objective r
I
t
encourages the character to follow a given reference
motion {qˆt }. It is further decomposed into terms that reward the
character for matching certain characteristics of the reference motion, such as joint orientations and velocities, as follows:
r
I
t = w
p
r
p
t
+ w
v
r
v
t + w
e
r
e
t + w
c
r
c
t
w
p = 0.65,w
v = 0.1,w
e = 0.15,w
c = 0.1
The pose reward r
p
t
encourages the character to match the joint
orientations of the reference motion at each step, and is computed
as the difference between the joint orientation quaternions of the
simulated character and those of the reference motion. In the equation below, q
j
t
and qˆ
j
t
represent the orientations of the jth joint from
the simulated character and reference motion respectively, q1 ⊖ q2
denotes the quaternion difference, and ||q|| computes the scalar
rotation of a quaternion about its axis in radians:
r
p
t
= exp






−2
©
­
«
Õ
j
||qˆ
j
t
⊖ q
j
t
||2ª
®
¬






.
The velocity reward r
v
t
is computed from the difference of local joint
velocities, with qÛ
j
t
being the angular velocity of the jth joint. The
target velocity ˆqÛ
j
t
is computed from the data via finite difference.
r
v
t = exp






−0.1
©
­
«
Õ
j
||ˆqÛ
j
t
− Ûq
j
t
||2ª
®
¬






.
The end-effector reward r
e
t
encourages the character’s hands and
feet to match the positions from the reference motion. Here, p
e
t
denotes the 3D world position in meters of end-effector e ∈ [left
foot, right foot, left hand, right hand]:
r
e
t = exp "
−40 Õ
e
||pˆ
e
t − p
e
t
||2
! # .
Finally, r
c
t
penalizes deviations in the character’s center-of-mass p
c
t
from that of the reference motion pˆ
c
t
:
r
c
t = exp h
−10 
||pˆ
c
t − p
c
t
||2
 i .
6 TRAINING
Our policies are trained with PPO using the clipped surrogate objective [Schulman et al. 2017]. We maintain two networks, one for the
policy πθ
(a|s,д) and another for the value function Vψ (s,д), with
parameters θ and ψ respectively. Training proceeds episodically,
where at the start of each episode, an initial state s0 is sampled
uniformly from the reference motion (section 6.1), and rollouts are
generated by sampling actions from the policy at every step. Each
episode is simulated to a fixed time horizon or until a termination
condition has been triggered (section 6.2). Once a batch of data
has been collected, minibatches are sampled from the dataset and
used to update the policy and value function. The value function is
updated using target values computed with TD(λ) [Sutton and Barto
1998]. The policy is updated using gradients computed from the
surrogate objective, with advantages At computed using GAE(λ)
[Schulman et al. 2015b]. Please refer to the supplementary material
for a more detailed summary of the learning algorithm.
One of the persistent challenges in RL is the problem of exploration. Since most formulations assume an unknown MDP, the agent
is required to use its interactions with the environment to infer the
structure of the MDP and discover high value states that it should
endeavor to reach. A number of algorithmic improvements have
been proposed to improve exploration, such as using metrics for
novelty or information gain [Bellemare et al. 2016; Fu et al. 2017;
Houthooft et al. 2016]. However, less attention has been placed on
the structure of the episodes during training and their potential as
a mechanism to guide exploration. In the following sections, we
consider two design decisions, the initial state distribution and the
termination condition, which have often been treated as fixed properties of a given RL problem. We will show that appropriate choices
are crucial for allowing our method to learn challenging skills such
as highly-dynamic kicks, spins, and flips. With common default
choices, such as a fixed initial state and fixed-length episodes, we
find that imitation of these difficult motions is often unsuccessful.
6.1 Initial State Distribution
The initial state distribution p(s0) determines the states in which an
agent begins each episode. A common choice for p(s0) is to always
place the agent in a fixed state. However, consider the task of imitating a desired motion. A simple strategy is to initialize the character
to the starting state of the motion, and allow it to proceed towards
the end of the motion over the course of an episode. With this design, the policy must learn the motion in a sequential manner, by
first learning the early phases of the motion, and then incrementally
progressing towards the later phases. Before mastering the earlier
phases, little progress can be made on the later phases. This can
be problematic for motions such as backflips, where learning the
landing is a prerequisite for the character to receive a high return
from the jump itself. If the policy cannot land successfully, jumping
will actually result in worse returns. Another disadvantage of a fixed
initial state is the resulting exploration challenge. The policy only
receives reward retrospectively, once it has visited a state. Therefore,
until a high-reward state has been visited, the policy has no way
of learning that this state is favorable. Both disadvantages can be
mitigated by modifying the initial state distribution.
For many RL tasks, a fixed initial state can be more convenient,
since it can be challenging to initialize the agent in other states
(e.g., physical robots) or obtain a richer initial state distribution.
For motion imitation tasks, however, the reference motion provides
a rich and informative state distribution, that can be leveraged to
guide the agent during training. At the start of each episode, a state
can be sampled from the reference motion, and used to initialize the
state of the agent. We will refer to this strategy as reference state
initialization (RSI). Similar strategies have been previously used for
planar bipedal walking [Sharon and van de Panne 2005] and manipulation [Nair et al. 2017; Rajeswaran et al. 2017]. By sampling initial
ACM Trans. Graph., Vol. 37, No. 4, Article 143. Publication date: August 2018.
143:6 • Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne
(a) Humanoid (b) Atlas (c) T-Rex (d) Dragon
Fig. 3. 3D simulated characters. Our framework is able to train policies for a wide range of character morphologies.
states from the reference motion, the agent encounters desirable
states along the motion, even before the policy has acquired the
proficiency needed to reach those states. For example, consider the
challenge of learning to perform a backflip. With a fixed initial state,
in order for the character to discover that performing a full rotation
mid-air will result in high returns, it must first learn to perform a
carefully coordinated jump. However, for the character to be motivated to perform such a jump, it must be aware that the jump will
lead to states that yield higher rewards. Since the motion is highly
sensitive to the initial conditions at takeoff, many strategies will
result in failure. Thus the agent is unlikely to encounter states from
a successful flip, and never discover such high reward states. With
RSI, the agent immediately encounters such promising states during
the early stages of training. Instead of accessing information from
the reference motion only through the reward function, RSI can be
interpreted as an additional channel through which the agent can
access information from the reference motion in the form of a more
informative initial state distribution.
6.2 Early Termination
For cyclic skills, the task can be modeled as an infinite horizon MDP.
But during training, each episode is simulated for a finite horizon.
An episode terminates either after a fixed period of time, or when
certain termination conditions have been triggered. For locomotion,
a common condition for early termination (ET) is the detection
of a fall, characterized by the character’s torso making contact
with the ground [Peng et al. 2016] or certain links falling below
a height threshold [Heess et al. 2016]. While these strategies are
prevalent, they are often mentioned in passing and their impact on
performance has not been well evaluated. In this work, we will use a
similar termination condition as [Peng et al. 2016], where an episode
is terminated whenever certain links, such as the torso or head,
makes contact with the ground. Once early termination has been
triggered, the character is left with zero reward for the remainder of
the episode. This instantiation of early termination provides another
means of shaping the reward function to discourage undesirable
behaviors. Another advantages of early termination is that it can
function as a curating mechanism that biases the data distribution in
favor of samples that may be more relevant for a task. In the case of
skills such as walking and flips, once the character has fallen, it can
be challenging for it to recover and return to its nominal trajectory.
Table 1. Properties of the characters.
Property Humanoid Atlas T-Rex Dragon
Links 13 12 20 32
Total Mass (kg) 45 169.8 54.5 72.5
Height (m) 1.62 1.82 1.66 1.83
Degrees of Freedom 34 31 55 79
State Features 197 184 262 418
Action Parameters 36 32 64 94
Without early termination, data collected during the early stages of
training will be dominated by samples of the character struggling on
the ground in vain, and much of the capacity of the network will be
devoted to modeling such futile states. This phenomena is analogous
to the class imbalance problem encountered by other methodologies
such as supervised learning. By terminating the episode whenever
such failure states are encountered, this imbalance can be mitigated.
7 MULTI-SKILL INTEGRATION
The discussion thus far has been focused on imitating individual motion clips. But the ability to compose and sequence multiple clips is
vital for performing more complex tasks. In this section, we propose
several methods for doing this, which are each suited for different
applications. First, we need not be restricted to a single reference
clip to define a desired motion style, and can instead choose to use
a richer and more flexible multi-clip reward. Second, we can further
provide the user with control over which behavior to trigger, by
training a skill selector policy that takes in a user-specified one-hot
clip-selection input. Third, we can avoid training new policies for
every clip combination by instead constructing a composite policy
out of existing single-clip policies. In this setup, multiple policies
are learned independently and, at runtime, their value functions are
used to determine which policy should be activated.
Multi-Clip Reward: To utilize multiple reference motion clips during training, we define a composite imitation objective calculated
simply as the max over the previously introduced imitation objective
applied to each of the k motion clips:
r
I
t = max
j=1,...,k
r
j
t
,
ACM Trans. Graph., Vol. 37, No. 4, Article 143. Publication date: August 2018.
DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills • 143:7
where r
j
t
is the imitation objective with respect to the jth clip. We
will show that this simple composite objective is sufficient to integrate multiple clips into the learning process. Unlike [Peng et al.
2017a], which required a manually crafted kinematic planner to
select a clip for each walking step, our objective provides the policy with the flexibility to select the most appropriately clip for a
given situation, and the ability to switch between clips whenever
appropriate, without the need to design a kinematic planner.
Skill Selector: Besides simply providing the policy with multiple
clips to use as needed to accomplish a goal, we can also provide the
user with control over which clip to use at any given time. In this
approach, we can train a policy that simultaneously learns to imitate
a set of diverse skills and, once trained, is able to execute arbitrary
sequences of skills on demand. The policy is provided with a goal дt
represented by a one-hot vector, where each entry дt,i corresponds
to the motion that should be executed. The character’s goal then
is to perform the motion corresponding to the nonzero entry of дt
.
There is no additional task objective r
G
t
, and the character is trained
only to optimize the imitation objective r
I
t
, which is computed based
on the currently selected motion r
I
t = r
i
t
, where дt,i = 1 and дt,j = 0
for j , i. During training, a random дt
is sampled at the start of
each cycle. The policy is therefore required to learn to transition
between all skills within the set of clips.
Composite Policy: The previously described methods both learn
a single policy for a collection of clips. But requiring a network to
learn multiple skills jointly can be challenging as the number of skills
increases, and can result in the policy failing to learn any of the skills
adequately. An alternative is to adopt a divide-and-conquer strategy,
where separate policies are trained to perform different skills, and
then integrated together into a composite policy. Since the value
function provides an estimate of a policy’s expected performance in
a particular state, the value functions can be leveraged to determine
the most appropriate skill to execute in a given state. Given a set of
policies and their value functions {V
i
(s), π
i
(a|s)}k
i=1
, a composite
policy Π(a|s) can be constructed using a Boltzmann distribution
Π(a|s) =
Õ
k
i=1
p
i
(s)π
i
(a|s), p
i
(s) =
exp
V
i
(s)/T
Ík
j=1
exp
V j
(s)/T ,
where T is a temperature parameter. Policies with larger expected
values at a given state will therefore be more likely to be selected.
By repeatedly sampling from the composite policy, the character is
then able to perform sequences of skills from a library of diverse
motions without requiring any additional training. The composite
policy resembles the mixture of actor-critics experts model (MACE)
proposed by [Peng et al. 2016], although it is even simpler as each
sub-policy is trained independently for a specific skill.
8 CHARACTERS
Our characters include a 3D humanoid, an Atlas robot model, a
T-Rex, and a dragon. Illustrations of the characters are available
in Figure 3, and Table 1 details the properties of each character.
All characters are modeled as articulated rigid bodies, with each
link attached to its parent link via a 3 degree-of-freedom spherical
joint, except for the knees and elbows, which are attached via 1
Fig. 4. Characters traversing randomly generated terrains. Top-to-bottom:
mixed obstacles, dense gaps, winding balance beam, stairs. The blue line
traces the trajectory of the character’s center-of-mass.
degree-of-freedom revolute joints. PD controllers are positioned
at each joint, with manually specified gains that are kept constant
across all tasks. Both the humanoid and Atlas share similar body
structures, but their morphology (e.g., mass distribution) and actuators (e.g., PD gains and torque limits) differ significantly, with the
Atlas being almost four times the mass of the humanoid. The T-Rex
and dragon provide examples of learning behaviors for characters
from keyframed animation when no mocap data is available, and
illustrate that our method can be readily applied to non-bipedal
characters. The humanoid character has a 197D state space and
a 36D action space. Our most complex character, the dragon, has
a 418D state space and 94D action space. Compared to standard
continuous control benchmarks for RL [Brockman et al. 2016b],
which typically have action spaces varying between 3D to 17D, our
characters have significantly higher-dimensional action spaces.
9 TASKS
In addition to imitating a set of motion clips, the policies can be
trained to perform a variety of tasks while preserving the style
prescribed by the reference motions. The task-specific behaviors are
encoded into the task objective r
G
t
. We describe the tasks evaluated
in our experiments below.
Target Heading: Steerable controllers can be trained by introducing an objective that encourages the character to travel in a target
direction d
∗
t
, represented as a 2D unit vector in the horizontal plane.
The reward for this task is given by
r
G
t = exp h
−2.5 max(0,v
∗ −v
T
t
d
∗
t
)
2
i
,
where v
∗
specifies the desired speed along the target direction d
∗
t
,
and vt represents the center-of-mass velocity of the simulated character. The objective therefore penalizes the character for traveling
ACM Trans. Graph., Vol. 37, No. 4, Article 143. Publication date: August 2018.