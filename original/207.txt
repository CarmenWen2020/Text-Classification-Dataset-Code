Video-based dimensional emotion recognition aims to map human affect into the dimensional emotion space based on visual signals, which is a fundamental challenge in affective computing and human-computer interaction. In this paper, we present a novel encoder-decoder framework to tackle this problem. It adopts a fully convolutional design with the cascaded 2D convolution based spatial encoder and 1D convolution based temporal encoder-decoder for joint spatio-temporal modeling. In particular, to address the key issue of capturing discriminative long-term dynamic dependency, our temporal model, referred to as Temporal Hourglass Convolutional Neural Network (TH-CNN), extracts contextual relationship through integrating both low-level encoded and high-level decoded clues. Temporal Intermediate Supervision (TIS) is then introduced to enhance affective representations generated by TH-CNN under a multi-resolution strategy, which guides TH-CNN to learn macroscopic long-term trend and refined short-term fluctuations progressively. Furthermore, thanks to TH-CNN and TIS, knowledge learnt from the intermediate layers also makes it possible to offer customized solutions to different applications by adjusting the decoder depth. Extensive experiments are conducted on three benchmark databases (RECOLA, SEWA and OMG) and superior results are shown compared to state-of-the-art methods, which indicates the effectiveness of the proposed approach.

SECTION 1Introduction
Perceiving human beings is an important field in Artificial Intelligence (AI), while understanding emotions is a major branch with consistently increasing attention from both the academia and industry. In recent years, emotion recognition techniques have been applied to Human Computer Interaction (HCI) for the deployment of AI systems collaborating with humans more deeply and swimmingly [1], [2]. They are also in ongoing demand by a wide variety of applications including humanoid robot [3], healthcare [4], [5], etc.

Human emotions are typically mapped to a number of discrete universal categories [6], and according to the dimensional theories [7], [8], [9], they can also be modeled in a space-continuous way, which contributes to another way to understand the structure and functions of emotions. Arousal and valence are the two most used dimensions, where arousal describes how active or calm a subject remains and valence measures how positive or negative a subject appears. Emotion recognition can be conducted on multiple modalities of data, i.e., audios, videos, texts, and physiological signals, and video-based solutions have become more popular because videos are conveniently acquired and reputed to contain rich information, such as facial expressions and body gestures. Due to different annotation types, video-based dimensional emotion recognition methods can be categorized into (1) frame-level and (2) clip-level. The former aims at predicting emotion variances of single frames using up-to-time clues, while the latter is to render a label to an entire clip/utterance with information at all the frames. In spite of some differences in processing, they basically share the same key issue, i.e., effectively modeling spatio-temporal visual clues. Meanwhile, compared with clip-level recognition, that of frame-level dynamically models human affect in a more fine-grained way, and thus requires a more carefully designed procedure.

In the literature, a wide range of approaches have been proposed to address video-based emotion recognition problem [10], [11], [12], [13], [14]. However, the overwhelming majority of the spatio-temporal features employed are hand-crafted, which suffer limitations in the following aspects. First, the success of these features is strongly dependent on prior expertise, and cannot be guaranteed for incomplete human cognition. Second, in general, such features are built within a relatively small and fixed temporal window and are incapable of capturing sufficient temporal contextual information.

Recently, deep learning has been investigated to directly learn discriminative features either from static or sequential data. Convolutional Neural Networks (CNNs) prove effective in tackling many visual tasks including emotion recognition [15], [16], [17], where a set of filters are learned to encode local spatial patterns from still images. 3D CNNs are the extended CNN versions, which capture temporal dependency conveyed in consecutive frames, and are adopted in action recognition [18] and emotion recognition [19], [20]. Although some promising results have been achieved, 3D CNN based methods are constrained by their expensive resource consumption for long-range temporal modeling.

On the other side, in video-based dimensional emotion recognition, “intra-class” (samples with similar affective states) differences, e.g., facial changes due to head pose movements, are sometimes larger than “inter-class” similarities for long-interval spans (see an illustration in Fig. 3), thus making long-term contextual dependency modeling necessary. Recurrent Neural Networks (RNNs) show their competency [16], [21], [22], and they are theoretically able to handle the input of an arbitrary length. But it is commonly difficult to train RNN models due to the well-known gradient vanishing and exploding problems. As an advanced version of RNN, Long Short-Term Memory (LSTM) networks are developed to compensate for those shortcomings [23], [24]. Unfortunately, since the use of hyperbolic tangent and sigmoid activation functions incurs gradient decay over layers, their performance degrades on long sequences in practice [25], [26]. Therefore, it remains an unsolved issues to extract discriminative long-term affective representation for recognizing emotions in videos.

In this paper, we propose a novel approach to video-based dimensional emotion recognition, which models long-term contextual affects through an encoder-decoder fully convolutional network. Specifically, the spatio-temporal convolutional encoder consists of a Spatial Convolutional Encoder (SCE) and a Temporal Convolutional Encoder (TCE), which are cascaded to capture spatial and temporal cues in a serial manner. The temporal decoder has a symmetrical structure with TCE, which integrates features of different levels by skip connections between the encoder and decoder. This temporal model is referred to as Temporal Hourglass CNN (TH-CNN) on account of its shape. Temporal Intermediate Supervision (TIS) is then introduced to enhance affective representations generated by TH-CNN under a multi-resolution strategy. See Fig. 1 for the overall pipeline. Compared with the RNN based contextual regressors widely used for this topic, the proposed TH-CNN and TIS pipeline allows to acquire long-term dependency with stacked temporal convolutional layers step-by-step in a coarse-to-fine way. Meanwhile, the explicit intermediate layers in the encoder-decoder structure make the proposed temporal modeling method more flexible to be adapted to different applications by adjusting the decoder depth for higher efficiency.


Fig. 1.
Overall architecture of the proposed method (best viewed in color). We tackle the dimensional emotion recognition problem with a fully convolutional encoder-decoder framework (details for frame-level recognition are shown). Given the video sequence, facial images extracted and aligned from raw frames are fed into Spatial Encoder (Es) to generate static features. Temporal Encoder (Et) takes these spatial features as input, and by hierarchical temporal convolution and pooling blocks, high-level long-term representation is then obtained. The frame-level decoder (Df) has a symmetrical structure with the Temporal Encoder, where linear interpolation is adopted for upsampling and high-level features are integrated with low-level features by skip connections between encoder and decoder. In order to explicitly guide the proposed temporal model to learn the affective fluctuation from coarse to fine, Temporal Intermediate Supervision (TIS) is applied to all decoding layers with different scales.

Show All

We demonstrate the effectiveness of the proposed approach in frame-level emotion recognition by the state-of-the-art results reported on two benchmark datasets, i.e., Recola and SEWA. Moreover, our approach also shows a good generalization ability in the clip-level emotion recognition task by simply replacing the temporal decoder with a temporal global pooling. It is evidenced by the evaluation results on the OMG database, where it reaches significant accuracy improvement over a number of representative baselines.

In summary, the contributions of this paper are three folds:

We propose a fully convolutional spatio-temporal model for video-based dimensional emotion recognition, including frame-level and clip-level tasks.

We propose a Temporal Hourglass Convolutional Neural Network, which builds long-term contextual representation using both the low-level and high-level cues respectively from the layers of the encoder and decoder with identical temporal resolution.

We propose a supervision strategy, Temporal Intermediate Supervision, which guides TH-CNN to learn semantic emotion representation in a coarse-to-fine way, further enhancing its discriminative power.

The remainder of this paper is organized as follows. Section 2 briefly reviews the related work. Section 3 provides problem formulation of video-based emotion recognition. Section 4 describes the proposed spatio-temporal encoder-decoder method in detail. Experimental evaluations and result analysis are presented in Section 5. We conclude the paper in Section 6 with future perspectives.

SECTION 2Related Work
Over the last few decades, a wide range of methods [27], [28], [29], [30] have explored categorical models to classify emotions into discrete classes [6]. Although such categories cover the most common facial expressions according to [6], the natural emotional responses in real-life scenarios are often more complex, compound and even ambiguous [17], [31]. Alternatively, another way to model emotions is in the dimensional space, which embeds human affect into a low-dimensional vector, e.g., arousal, valence, liking, dominance. In this case, dimensional affective states can be modeled as time-continuous signals, facilitating more realistic applications.

Typical video-based dimensional emotion recognition approaches, including both the frame-level and clip-level tasks, contain two major stages, i.e., local spatio-temporal visual feature extraction and contextual/non-contextual regression. In this section, we briefly review the techniques at the two stages respectively.

2.1 Local Spatio-Temporal Features
Handcrafted Features. Appearance features and geometric features are extensively used for emotional state modeling. In [11], Nicolle et al. propose to use three visual features in terms of facial shape deformations as well as global and local facial appearances for continuous prediction of human dimensional emotions. In [32], Meng et al. first generate Motion History Histograms (MHH) from facial image sequences, and Edge Orientation Histograms (EOH) and Local Binary Patterns (LBP) are then calculated as dynamic representations. In [13], Ringeval et al. compute Local Gabor Binary Patterns from Three Orthogonal Planes (LGBP-TOP) [33] on appearances to combine static and dynamic texture cues. They also employ facial landmarks as geometric features in the AVEC 2015 baseline system. He et al. [22] extract Local Phase Quantization from Three Orthogonal Planes (LPQ-TOP) to capture both spatial and temporal dependencies within a fixed small window (3 seconds) to balance time consumption. They further adopt feature selection techniques to reduce the risk of overfitting. Ramirez et al. [34] exploit high-level event-based features such as eye gaze direction, smile intensity, and head tilt, which show advantage over the low-level signal-based ones, e.g., facial feature points. Those features basically depend on prior knowledge and are incapable to encode complex temporal information variations, especially in a long term.

Deep Learning Features. The data-driven CNN models show their ability to learn more discriminative features. Recently, they have demonstrated state-of-the-art performance in many computer vision tasks, and they have thus been attempted in dimensional emotion recognition. In [35], Sun et al. exploit a model of residual convolutional neural network with four blocks. Pre-training is conducted on the FER dataset [36] and the last pooling layer is regarded as the frame-level spatial feature. In [15], Khorrami et al. apply a 3-layer CNN to extract single-frame features, which is very lightweight yet generates comparable features with the well designed handcrafted ones. In [16], Chen et al. investigate the features of two kinds of deep CNN models, i.e., VGG [37] and DenseNet [38], which are both pre-trained on the FERplus dataset [39]. The results of different convolutional layers are analyzed by experiments, and the middle layers are shown to convey more affective information. In [40], Chang et al. propose an integrated CNN based deep learning framework to jointly perform facial attribute, action unit and dimensional emotion recognition in video clips. Peng et al. [41] utilize SphereFace [42] to acquire static information based on a number of selected frames.

CNNs illustrate tremendous improvements in this topic, in particular for static emotion feature extraction. However, they leave much room in efficient and discriminative long-term temporal representation.

2.2 Emotion Regression Models
Noncontextual Regressors. The most straightforward way to perform emotion prediction on all video frames is to apply a shared regression model without making use of any contextual information. In [43], Laurens et al. introduce a linear regressor for frame-level recognition. In their experiment, they find that although there are only weak linear relations between features and labels, using the bias can still lead to performance improvement on a more complex baseline system presented in [44]. Support Vector Regressor (SVR) is one of the most popular noncontextual models widely adopted at this phase, e.g., in the baseline systems of AVEC challenges [13], [14], [33], [45]. The accuracies of the noncontextual approaches are restricted due to the inaccessibility of dependencies of the spatial feature sequence, while the variation of human affective state is successive and context-sensitive.

Contextual Regressors. RNN is a major structure to conduct contextual emotional state regression in the previous state-of-the-art studies. In [21], Chao et al. utilize LSTM to model the temporal dependency. He et al. [22] adopt Bidirectional LSTM (BiLSTM) to encode both preceding and subsequent information. Peng et al. introduce BiLSTM in their solution to clip-level recognition, which also achieves promising results. In [46], Amirian et al. apply Echo State Networks (ESN) to and further extend the plain ESN to a bidirectional version to improve the regression accuracy. In [47], Lee et al. propose a framework that leverages the spatio-temporal attention of video frames. Both spatial appearances and temporal motions of facial video sequences are simultaneously considered using 3D-CNNs. More recently, jointly learning spatial and temporal dependency within a one-stage framework has been studied. In [20], Huang et al. employ 3D convolutional networks with ConvLSTM to extract spatio-temporal information in an end-to-end way.

Although RNNs and LSTM are theoretically able to handle the videos of various lengths, they have some difficulties in training due to the gradient vanishing and exploding problems and are incompetent at learning long-term patterns. 3D CNN based methods suffer from high computational cost for long-range representation. Such shortcomings greatly limit extensive applications of the aforementioned models.

SECTION 3Problem Formulation
The goal of video-based dimensional emotion recognition is to map the input video into the dimensional emotion space, which can be naturally formulated as a multi-label regression problem. To be specific, continuous (frame-level) emotion recognition task aims to predict emotion variances of single frames using up-to-date information and is a many-to-many mapping problem. Let X={x1,x2,…,xn}⊂V be the raw video sequence with n frames, where V is the input space. Function f(⋅;θf) maps X to a vector Z={z1,z1,…,zn}, where zi is a d-dimensional vector in the emotional description space and θf is a set of parameters to learn.

As for clip-level recognition, the only difference lies in that function f(⋅;θc) maps X to a single vector z in the d-dimensional emotional description space, where θc is a set of parameters to learn.

In this study, we model the mapping functions for both the frame-level and clip-level tasks, i.e., f(⋅;θf) and f(⋅;θc), with a unified encoder-decoder framework, which consists of a general spatio-temporal encoder (E(⋅)) and task-specified decoders (Df and Dc) detailed in Section 4.

SECTION 4Method
4.1 Framework Overview
Instead of depending on RNNs to model the temporal dependency in the literature, our dimensional emotion recognition solution builds a fully convolutional encoder-decoder structure as illustrated in Fig. 1. Taking the frame-level task as an example, given the video sequence, facial images are extracted and aligned from raw frames, which are subsequently fed into the Spatial Encoder (Es) with Squeeze-Excitation Blocks to generate static features. By applying temporal convolution and pooling blocks hierarchically to these spatial features, high-level long-term representation is obtained through the Temporal Encoder (Et). The Temporal Decoder (Df) has the symmetric structure with the Temporal Encoder, where linear interpolation is adopted for upsampling. High-level features are integrated with low-level ones by skip connections between the encoder and decoder. The entire temporal model is referred to as Temporal Hourglass CNN on account of its structure shape. Temporal Intermediate Supervision is then applied to all the decoding layers with different resolutions. Thanks to TIS, TH-CNN can learn the affective fluctuations from coarse to fine explicitly. Furthermore, we show the generalization ability of the proposed framework by replacing the decoder with a temporal global pooling decoder (Dc) to address the clip-level emotion recognition problem.

4.2 Encoder
4.2.1 Spatial Convolutional Encoder
We employ 2D CNN for spatial encoding of the facial sequence. The Spatial Convolutional Encoder takes single frames as inputs, and shares weights across all the frames of a video. The Squeeze-and-Excitation (SE) block, which is proposed in [48] and achieves the best performance in ILSVRC 2017 [49], adaptively recalibrates channel-wise features by spatial average pooling and channel re-scaling of the feature maps. Due to its simplicity in design, SE block can be used in modern networks for possible accuracy gain. In this study, we employ 18-layer Residual Network with SE blocks as our spatial CNN to perform dynamic channel-wise feature recalibration, which generates strong discriminative static features as input of the proposed temporal model.

It is worth noting that other modern CNN architectures such as VGG [37], GoogLeNet [50] and ResNet [51] can also be used for spatial affective information encoding, but a comparison of different network architectures is not the focus of this work. On the other side, considering that emotion has a global and fine-grained expression on the human face, SE operation empowers the 2D convolutional network through modeling the channel relationship.

4.2.2 Temporal Convolutional Encoder
Temporal Convolutional Network (TCN) is a time-series model [52], which has proven capable of capturing long-range patterns, which has been increasingly employed for human action segmentation [53], action localization [54] and other sequence modeling tasks [55], [56], [57]. Temporal convolution is a kind of 1-dim convolution conducting conv operations along the time axis. As shown in Fig. 1, we adopt stacked TCNs as our temporal contextual encoder. For TCN, all the computations are performed layer-wise, meaning that every time step is updated simultaneously rather than updated sequentially per-frame. Moreover, convolutions are computed across time, and predictions at each frame are a function of a fixed-length period of time w.r.t. the receptive field.

As illustrated in Fig. 2a, the presented encoder is composed of encoding convolutional blocks, which is formed by a sequence of temporal convolution, weight normalization [58], spatial dropout [59], temporal max pooling and rectified linear unit (ReLU). Inspired by the residual learning scheme in [51], we also use residual connections in every block as it reduces the training difficulty. For each encoding block, we simply add up the input and output of the residual function where the output is reintegrated by a 1×1 convolution to alter the depth of identity mapping. After the element-wise addition, we use another ReLU layer to calculate the activation value. Our temporal convolutional encoder is designed under the paradigm of encoding networks, which decreases the output size after each convolutional layer. Fig. 2b demonstrates the receptive field variation during the encoding process. By controlling the kernel size of both the temporal convolutional and max pooling layer, the patterns at different time intervals are learnt automatically.


Fig. 2.
Temporal encoder-decoder. (a) illustrates the basic module consisting of a temporal convolution layer and a temporal max pooling layer. Temporal Convolution Encoder (TCE) in TH-CNN stacks up those modules to build hierarchical temporal contextual features. (b) displays how the temporal Receptive Field (RF) increases in TCE. The top is the input neuron. After temporal convolution, the RF is enlarged by (TL−1)×S+K, and after temporal max pooling, the RF is augmented by (TL−1)×S, where S and K are the stride and kernel size of the convolution, respectively. For convenience, we set S=1,2; K=3,2 for the convolution and pooling layer. (c) shows the basic module with a temporal up-sampling layer and a temporal convolution layer. Temporal Convolution Decoder (TCD) in TH-CNN is composed of stacked modules to increase the temporal resolution of the features generated by TCE. (d) depicts how the temporal resolution increases in TCD, The top is the output neuron by TCE and after temporal convolution and temporal max pooling, the temporal resolution is enlarged by R, where R is the up-sampling rate of the temporal up-sampling layer. For convenience, we set R=2 for up-sampling and K=3 for convolution.

Show All

4.3 Frame-Level Decoder
4.3.1 Temporal Hourglass Network
For frame-level recognition, the decoder has a corresponding structure with the encoder so that the whole temporal model has a symmetrical design. As illustrated in Figs. 2c and 2d, the decoder takes integrated features, obtained by incorporating high-level decoding features and low-level encoding features, as input. In practice, we perform temporal upsampling operation via linear interpolation due to its simplicity and efficiency. Note that the interpolation rate is set to be equal to the step size of temporal max pooling which leads to an output at the same temporal resolution as the one from the corresponding encoding layer.

Considering the fact that visualization of our proposed temporal model resembles an hourglass and that our method is partly inspired by the human pose estimation model proposed in [60], we refer to our temporal encoder-decoder model as Temporal Hourglass CNN.

For TH-CNN, the number of time steps is determined by the filter size and moving stride of temporal convolution and temporal pooling as
TL−1=(TL−1)×S+F,(1)
View Sourcewhere TL denotes the temporal receptive field of the Lth layer, S is the stride of temporal convolution or pooling, and F is the filter size of convolution. This equation shows that the filter size grows exponentially over the layer number. In practice, we tune the hyper-parameters of TH-CNN so that it can see 300 to 400 time steps.

4.3.2 Temporal Intermediate Supervision
As shown in Fig. 3a, for spontaneous emotion variations in long intervals, large intra-class differences often occur in dimensional emotion recognition even for the same identity, which confuse the temporal regressors and make them easy to overfit. Furthermore, we find that the dimensional emotion distance is in general inversely proportional to the spatial feature similarity within a local temporal region as depicted in Fig. 3b. The two facts suggest modeling the global emotion variance trend and local affective detail in a hierarchical way.

Fig. 3. - 
Dimensional emotion variance of a random sample (train_8.mp4) in the Recola database. (a) The dimensional emotion variance and sampled expression frames. ③, ④ and ⑤ are almost with the same valence value; however, their faces appear very different. In contrast, ① and ③, ② and ⑤ have very similar appearances respectively, but their valence values differ sharply. (b) Emotion difference and corresponding appearance feature similarity (taking a random frame No.1545 as the anchor).
Fig. 3.
Dimensional emotion variance of a random sample (train_8.mp4) in the Recola database. (a) The dimensional emotion variance and sampled expression frames. ③, ④ and ⑤ are almost with the same valence value; however, their faces appear very different. In contrast, ① and ③, ② and ⑤ have very similar appearances respectively, but their valence values differ sharply. (b) Emotion difference and corresponding appearance feature similarity (taking a random frame No.1545 as the anchor).

Show All

To this end, intermediate supervision can be further launched in the proposed model. Actually, intermediate supervision has proven effective for practical training in some other computer vision tasks [60], [61], [62]. Inspired by these studies, we propose the spatial intermediate supervision strategy into the temporal domain, namely Temporal Intermediate Supervision, which is conducted on different temporal decoding layers. As illustrated in Fig. 1, we use the term scale to describe what the temporal resolution is for each intermediate layer. The temporal resolution is reduced by temporal max-pooling during the encoding stage and increased by temporal upsampling during the decoding stage, and the outputs of the intermediate layers thus have different shapes from the target labels. We rescale the target by average convolution to adapt its temporal resolution to that of the corresponding intermediate layer.

More specifically, for the kth layer of the decoder with the depth of L, the corresponding ground truth of the emotion label scaled by γL−k is used for supervision.

Then, we define the loss as follows:
L=∑i=0L∑t=0T−1λi∥y^it−yit∥22,(2)
View Sourcewhere y^it is the predicted result of the ith layer at time step t and yit is the corresponding ground truth label; λi is a parameter used to balance the learning process of different layers.

With more detailed supervision delievered by TIS, TH-CNN is strengthened in a coarse-to-fine manner that emotional states with varying temporal resolutions are taken into account in different decoding phases.

4.4 Clip-Level Decoder
To demonstrate the generability and flexibility of the proposed framework, we extend our fully convolutional network to the clip-level emotion recognition task by replacing the decoder with a clip-level decoder. Noting that the clip-level task needs global information of the whole video clip, instead of naively averaging the frame-level scores, we apply a global pooling decoder to generate video-level representation. Temporal max pooling is adopted in this step for temporal fusion, which operates in a channel-wise manner on input contextual features formulated as
rj=maxi∈Nfji,(3)
View Sourcewhere rj denotes the representation of the whole clip; fi is from the encoder output F={f1,f2,…,fN} with the entire length of N; and j marks the channel of temporal contextual features.

SECTION 5Experiments
To evaluate the proposed video-based dimensional emotion recognition approach, we carry out extensive experiments and make fair comparison to the state-of-the-art counterparts. The databases, metrics, implementation details, and results are presented in the subsequent subsections.

5.1 Datasets
Three popular benchmark databases, i.e., Recola [63], SEWA [64], and OMG [17], are used for evaluation. Recola and SEWA are for frame-level dimensional emotion recognition, and OMG is for clip-level dimensional emotion recognition.

Recola. The Remote Collaborative and Affective Interactions dataset is a multimodal corpus designed to monitor subjects as they work in pairs remotely to complete a collaborative task. The modalities include audio, video, electro-cardiogram (ECG), and electro-dermal activity (EDA). These signals are recorded for 27 French-speaking subjects and separated into three parts, 9 recordings for training, 9 for validation, and 9 for testing. The duration of each recording is 5 minutes with 7,501 frames. The dataset contains two types of dimensional labels (arousal and valence) which are annotated by 6 people.

SEWA. The Automatic Sentiment Analysis in the Wild collects spontaneous and naturalistic interactions consisting of audio, video and text modalities. All signals are recorded “in the wild” through human-human interactions, but in every recording, there are only the behaviors of one person. The duration of the recording ranges from 40 seconds to 3 minutes. This dataset is annotated in three dimensions, namely arousal, valence, and liking.

OMG. The One-Minute-Gradual Emotion Dataset includes 420 videos with an average length of 1 minute, collected from a variety of Youtube channels. The videos are selected automatically based on specific search terms related to “monologue”. Videos are then separated into clips based on utterances, and each utterance is annotated by at least five independent subjects using the Amazon Mechanical Turk tool. The dataset is released with the gold standard for arousal and valence as well as the individual annotations for each reviewer.

Since the labels of the test set are not readily available, we treat the development clips as test samples which are NOT used during the training and validation process. The Models with the best performance in four-fold cross validation on the training set are used for testing as in [15], [47]. The statistical information of the partition of the three datasets is shown in Table 1.

TABLE 1 Statistics of Dimensional Emotion Databases Used for Evaluation
Table 1- 
Statistics of Dimensional Emotion Databases Used for Evaluation
5.2 Metrics
Both arousal and valence are in the range of [−1,1], making the MSE and RMSE values very small and hard to compare. Therefore in the series of the AVEC and OMG challenges, two metrics are widely used to evaluate the performance of continuous emotion recognition systems, i.e., Pearsons Correlation Coefficient (PCC) [65] and Concordance Correlation Coefficient (CCC) [66]. In this paper, we follow their protocol and evaluate the performance of different methods in terms of both PCC and CCC.

PCC is defined as
ρ=cov(y^,y)σy^σy,(4)
View Sourcewhere cov(y^,y) is the covariance between two time series (prediction y^ and ground truth y) and σy^ and σy are the variance of each time series.

CCC is defined as
ρc=2ρσy^σyσ2y^+σ2y−(μy^−μy)2,(5)
View SourceRight-click on figure for MathML and additional features.where ρ is the PCC between y^ and y. σy^; σy are the variance of each time series; and μy^ and μy are their mean values.

It can be seen from the two equations above that PCC is a measure of linear correlation between two variables, while CCC is penalized by the value offset as well as their correlation relationship. In this paper, we follow the protocol in [13], [14], [17], and evaluate the performance of different methods in terms of PCC and CCC.

5.3 Implementation Details
Our implementation is done in PyTorch [67]. The details of network parameters, network training, and feature similarity are given below.

Network Parameters. We apply a 18-layer ResNet with Squeeze-Excitation blocks for static emotion recognition and the activated outputs from the last average pooling layer are used as spatial features. The detailed SCE structure is shown in Table 2. Spatial dropout is used for every encoding block with a rate of 0.2, which is performed on fully convolutional filters instead of individual weights [53], [59]. All the weights are initialized from a Gaussian distribution N(0,0.01). The parameters of TH-CNN is adjusted slightly for different tasks. To be specific, for Recola, the output channels of the temporal encoder are {64,128,256,512} and kernel size is set to 9. For SEWA, the output channels of the temporal encoder are {64,128,256,512} and the kernel size is set to 7. The detailed parameters of the TH-CNN structure including kernel shape and output shape can be found in Table 3. For OMG, the output channels of the temporal encoder are {64,128,256} and the kernel size is set to 5. In all of these TH-CNNs, the output channels of the temporal decoder remains the same with their corresponding encoders. The max pooling step and upsampling rate are set to be equal to half of the kernel size. In this work, taking the dataset size, video length/frequency, and computation efficiency into consideration, we use 768 time steps on Recola, 400 time steps on SEWA, and 400 time steps on OMG, respectively.

TABLE 2 The Output Shapes and Kernel Parameters of the SCE Structure

TABLE 3 The Output Shapes and Kernel Parameters of the TH-CNN and TIS Structure

Network Training. OpenFace [68] is used for both face detection and alignment by landmarks. Our Spatial Encoder is first pre-trained on FERplus [39] for the task of categorical emotion classification. The resulting model is then fine-tuned on the corresponding dimensional emotion datasets. To avoid overfitting, we randomly perform rotations between −10∘ to 10∘, zoom with a factor from 1.1 to 1.5, horizontal flip on the input images for data augmentation at the train phase. Standardization is also applied before feeding the pictures into deep networks. During the training phase, the mini-batch size is set to 32 and we add zero paddings at the end of each video to keep the same sequence length for each batch. We attempt Adam [69] for optimization by minimizing the loss function. We test the initial learning rate of 0.001, 0.003 and 0.01 for different models to achieve the best performance. We also employ exponential decay with a rate of 0.9 every 50 training steps.

Feature Similarity. The spatial feature similarity in Fig. 3b is measured by the cosine similarity sat as
sat=fa⋅ft∥fa∥2∥ft∥2,(6)
View SourceRight-click on figure for MathML and additional features.where fa is the spatial feature vector obtained from the anchor frame at time a, while ft is the one obtained from the time frame t.

5.4 Results
5.4.1 Comparison with the State-of-the-Art Methods
Frame-Level Task. We compare our proposed model with several state-of-the-art methods including both handcrafted feature based and CNN feature based ones on Recola in Table 4. We also compare our proposed model with several recent deep learning based methods on SEWA in Table 5. We can see that the CNN based methods [15], [21], [47] achieve comparable or superior results to the ones of the handcrafted feature based methods including LGBP-TOP, LPQ-TOP and Geometric features [22], [46]. Note that we do not reproduce the experiments of the previous methods and directly cite the scores given in the original papers as the standard protocols are adopted. These methods make use of RNNs (e.g., LSTM, BiLSTM, BiESN) for temporal information modeling, but for the proposed approach, we use fully convolutional network to model both spatial and temporal dependencies. Our method achieves state-of-the-art scores benefiting from long-range context capturing without using any attention mechanism [47]. Moreover, compared with the RNN based temporal model, our TH-CNN is more convenient for parallel computing thanks to the characteristic of convolution operation.

TABLE 4 Comparison Results in Terms of PCC and CCC with the State-of-the-Art Methods on the Recola (AVEC2015) Dataset

TABLE 5 Comparison Results in Terms of PCC and CCC with Other Deep Learning Based Methods on the SEWA (AVEC2017) Dataset

To better analyze the advantage of Temporal Convolution Networks over RNN, we apply it to replace LSTM that is popular to encode temporal clues in the previous solutions for continuous emotion recognition. In Table 7, we simply use TH-CNN instead of LSTM with SCE remaining unchanged, the results are improve by 46 percent for arousal and 18 percent for valence, demonstrating that it is more effective in continuous emotion recognition. In SEWA, such a superiority is relatively not as significant as that on Recola, which is mainly due to the fact that the number of time steps in SEWA is smaller than that in Recola: the shortest video has only 974 time steps in SEWA while the corresponding number is 7,501 in Recola. In those short videos, the capability of TH-CNN on modeling longer-term temporal contextual information is not fully highlighted. Additionally, LSTM (RNN) suffers much from either gradient explosion or vanishing and it is thus hard to train. In contrast, TH-CNN has no recurrent structure in time axis, which enables easily training and taking more time steps compared with RNN based counterparts.

Clip-Level Task. By replacing the decoder of TH-CNN with a temporal global pooling decoder described in Section 4.4, we tackle the clip-level emotion recognition task on OMG and the performance is reported in Table 6. Promising results are obtained comparing to the baselines of CNN and cascaded CNN and LSTM methods, which show a good generability of the proposed approach.

TABLE 6 Comparison Results in Terms of PCC and CCC with the State-of-the-Art Methods on the OMG Dataset
Table 6- 
Comparison Results in Terms of PCC and CCC with the State-of-the-Art Methods on the OMG Dataset
5.4.2 Validation of Intermediate Layers
To validate the coarse-to-fine decoding process, we explore the issue by analyzing the knowledge from intermediate layers. Figs. 6 and 7 demonstrate the predicted results from different layers, all of which are rescaled to the same length of the provided labels for visualization. The features are first learned by the lower layers of the temporal decoder and as the decoding process goes on, more fine-grained affective details are hierarchically acquired

Moreover, we compute the intermediate performance in terms of PCC and CCC as shown in Fig. 4. We can find that both PCC and CCC become higher during decoding. At the 2th layer, PCC reaches comparable results with those of the final output while CCC keeps increasing by learning more fine-grained details via high-level decoder layers. Hence in specific application scenarios, we can selectively stop the forwarding process the balance between accuracy and computation complexity.


Fig. 4.
Intermediate layers’ valence performance on Recola (a) and SEWA (b).

Show All

5.4.3 Ablation Study
To demonstrate the effectiveness of the proposed TH-CNN and TIS pipeline, we perform ablation studies on Recola, SEWA and OMG. LSTM is adopted as the baseline with the same spatial features input. From Table 7, we can see that TH-CNN achieves large improvements over LSTM on all the three datasets, which clearly indicates the advantage of TH-CNN for temporal contextual modeling in both the frame-level and clip-level emotion recognition tasks. By further applying the intermediate loss to supervise the learning process of TH-CNN (for frame-level recognition), TH-CNN shows the ability to learn better contextual dependency and the final recognition results are further ameliorated, as illustrated in Table 7.

TABLE 7 Ablation Study of the Proposed Model in Term of PCC and CCC on Recola, SEWA and OMG
Table 7- 
Ablation Study of the Proposed Model in Term of PCC and CCC on Recola, SEWA and OMG
5.4.4 Confusion Heatmap Visualization
We visualize the predictions of the proposed methods and their corresponding labels via a confusion matrix (heatmap) as illustrated in Fig. 5. Most prediction results distribute in the same or very small interval with the labels, which proves the effectiveness of our approach.

Fig. 5. - 
Confusion heatmaps and corresponding label distributions on Recola (a, c) and SEWA (b, d). Heatmap visualization is based on the proportion histogram in Label and Prediction space.
Fig. 5.
Confusion heatmaps and corresponding label distributions on Recola (a, c) and SEWA (b, d). Heatmap visualization is based on the proportion histogram in Label and Prediction space.

Show All


Fig. 6.
Intermediate layers’ valence predictions versus ground truth of random samples on Recola: dev_2.mp4 (a) and dev_8.mp4 (b). The 0th layer predictions are from the input features to the decoder, and the 4th are the final predictions from the last decoding layer.

Show All


Fig. 7.
Intermediate layers’ valence predictions versus ground truth of random samples on Recola: Devel_01.mp4 (a) and Devel_05.mp4 (b). The 0th layer predictions are from the input features to the decoder, and the 4th are the final predictions from the last decoding layer.

Show All

Indeed, it can be seen from Figs. 5a and 5b that the proposed approach performs better for smaller values and this phenomenon is more obvious on SEWA than Recola. Such results are caused by the imbalance distributions of the two datasets. As the frequency distribution histograms in Figs. 5c and 5d show, the valence distributions on SEWA and Recola are both long-tailed where the number of samples with high values is very limited. Learning based models easily ignore them because their errors account for a much smaller portion in the loss function at the training stage. This can also be regarded as a difficulty of the proposed approach.

SECTION 6Conclusion and Future Work
In this study, we propose a novel and effective framework for video-based dimensional emotion recognition. Specifically, we present an encoder-decoder fully convolutional network integrating a spatio-temporal encoder and a temporal decoder to leverage spatial and long-range contextual dependency. Temporal Intermediate Supervision is then introduced to enhance affective representations, leading to better performance on frame-level recognition. Meanwhile, thanks to the flexible structure, the proposed method also displays a good generalization ability to address the clip-level recognition task by replacing the temporal decoder with a proper one, e.g., a temporal global pooling decoder. Extensive experiments are conducted on three popular benchmark databases, namely RECOLA, SEWA and OMG, and our method shows the superiority compared to the state-of-the-art ones.

In addition, our temporal model, i.e., TH-CNN, can also accept dynamic input of other modalities including audio, text, physical signals, etc. Actually, the network can be conveniently extended to the other modalities, in particular 1-dim signals through replacing the SCE by a modal-specific en-coder, e.g., Vggish [71], [72] as audio encoder and BERT [73] as textual encoder. Moreover, by taking the features of different modalities as inputs such as a concatenated feature, it can capture multi-model clues. In future work, we will extend the proposed approach to multi-modal analysis for further improvement of the overall performance.