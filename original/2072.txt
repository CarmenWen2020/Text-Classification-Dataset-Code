We present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional
structure-from-motion reconstruction to establish geometric constraints on
pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we
use a learning-based prior, i.e., a convolutional neural network trained for
single-image depth estimation. At test time, we fine-tune this network to
satisfy the geometric constraints of a particular input video, while retaining
its ability to synthesize plausible depth details in parts of the video that are
less constrained. We show through quantitative validation that our method
achieves higher accuracy and a higher degree of geometric consistency than
previous monocular reconstruction methods. Visually, our results appear
more stable. Our algorithm is able to handle challenging hand-held captured
input videos with a moderate degree of dynamic motion. The improved
quality of the reconstruction enables several applications, such as scene
reconstruction and advanced video-based visual effects.
CCS Concepts: â€¢ Computing methodologies â†’ Reconstruction; Computational photography.
Additional Key Words and Phrases: video, depth estimation
1 INTRODUCTION
3D scene reconstruction from image sequences has been studied
in our community for decades. Until a few years ago, the structure
from motion systems for solving this problem were not very robust,
and practically only worked â€œin the labâ€, with highly calibrated and
predictable setups. They also, often, produced only sparse reconstructions, i.e., resolving depth at only a few isolated tracked point
features. But in the last decade or so, we have seen good progress
towards enabling more casual capture and producing denser reconstructions, driven by high-quality open-source reconstruction
systems and recent advances in learning-based techniques, as discussed in the next section.
Arguably the easiest way to capture for 3D reconstruction is using
hand-held cell phone video, since these cameras are so readily and
widely available, and enable truly spontaneous, impromptu capture,
as well as quickly covering large spaces. If we could achieve fully
dense and accurate reconstruction from such input it would be
immensely usefulâ€”however, this turns out to be quite difficult.
Besides the typical problems that any reconstruction system has
to deal with, such as poorly textured areas, repetitive patterns, and
occlusions, there are several additional challenges with video: higher
noise level, shake and motion blur, rolling shutter deformations,
small baseline between adjacent frames, and, often, the presence of
dynamic objects, such as people. For these reasons, existing methods
ACM Trans. Graph., Vol. 39, No. 4, Article 71. Publication date: July 2020.
71:2 â€¢ Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf
often suffer from a variety of problems, such as missing regions in
the depth maps (Figure 1b) and inconsistent geometry and flickering
depth (Figure 1c).
Traditional reconstruction methods [Szeliski 2010] combine sparse
structure-from-motion with dense multi-view stereoâ€”essentially
matching patches along epipolar lines. When the matches are correct, this results in geometrically accurate reconstructions. However,
due to the before-mentioned complications, the matches are often
noisy, and typically need to be regularized with heuristic smoothness priors. This often induces incorrect geometry in the affected
regions, so that many methods drop pixels with low confidence
altogether, leaving â€œholesâ€ in the reconstruction (Figure 1b).
There has recently been immense progress on learning-based
methods that operate on single images. These methods do not require heuristic regularization, but instead learn scene priors from
data, which results in better ability to synthesize plausible depth
in parts of the scene that would be weakly or even incorrectly constrained in traditional reconstruction approaches. They excel, in
particular, at the reconstruction of dynamic scenes, since static and
dynamic objects appear the same when we consider a single frame
at a time. However, the estimated depth often flickers erratically
due to the independent per-frame processing (Figure 1c), and it is
not metric (i.e., not related to true depth by a single scale factor).
This causes a video reconstruction to be geometrically inconsistent:
objects appear to be attached to the camera and â€œswimmingâ€ in
world-space.
Several video-based depth estimation methods have also been
developed. These methods address the geometrical consistency of
the reconstruction over time either implicitly via recurrent neural
networks [Patil et al. 2020; Wang et al. 2019b] or explicitly using
multi-view reconstruction [Liu et al. 2019; Teed and Deng 2020].
State-of-the-art video-based depth estimation methods [Liu et al.
2019; Teed and Deng 2020], however, handle only static scenes.
In this work, we present a new video-based reconstruction system
that combines the strengths of traditional and learning-based techniques. It uses traditionally-obtained geometric constraints where
they are available to achieve accurate and consistent depth, and
leverages learning-based priors to fill in the weakly constrained
parts of the scene more plausibly than prior heuristics. Technically,
this is implemented by fine-tuning the weights of a single-image
depth estimation network at test time, so that it learns to satisfy
the geometry of a particular scene while retaining its ability to synthesize plausible new depth details where necessary. Our test-time
training strategy allows us to use both short-term and long-term constraints and prevent drifting over time. The resulting depth videos
are fully dense and detailed, with sharp object boundaries. The reconstruction is flicker-free and geometrically consistent throughout
the video. For example, static objects appear rock-steady when projected into world space. The method even supports a gentle amount
of dynamic scene motion, such as hand-waving (Figure 9), although
it still breaks down for extreme object motion.
The improved quality and consistency of our depth videos enable
interesting new applications, such as fully-automatic video special
effects that interact with the dense scene content (Figure 9). We
extensively evaluate our method quantitatively and show numerous qualitative results. The source code of our method is publicly
available.1
2 RELATED WORK
Supervised monocular depth estimation. Early learning-based approaches regress local image features to depth [Saxena et al. 2008] or
discrete geometric structures [Hoiem et al. 2005], followed by some
post-processing steps (e.g., a MRF). Deep learning based models have
been successfully applied to single image depth estimation [Eigen
and Fergus 2015; Eigen et al. 2014; Fu et al. 2018; Laina et al. 2016; Liu
et al. 2015]. However, training these models requires ground truth
depth maps that are difficult to acquire. Several efforts have been
made to address this issue, e.g., training on synthetic dataset [Mayer
et al. 2016a] followed by domain adaptation [Atapour-Abarghouei
and Breckon 2018], collecting relative depth annotations [Chen et al.
2016], using conventional structure-from-motion and multi-view
stereo algorithms to obtain pseudo ground truth depth maps from
Internet images [Chen et al. 2019a; Li et al. 2019; Li and Snavely
2018], or 3D movies [Ranftl et al. 2019; Wang et al. 2019a]. Our
method builds upon recent advances in single image depth estimation and further improves the geometric consistency of the depth
estimation on videos.
Self-supervised monocular depth estimation. Due to challenges of
scaling up training data collection, self-supervised learning methods have received considerable attention for their ability to learn a
monocular depth estimation model directly from raw stereo pairs
[Godard et al. 2017] or monocular video [Zhou et al. 2017]. The core
idea is to apply differentiable warp and minimize photometric reprojection error. Recent methods improve the performance through
incorporating coupled training with optical flow [Ranjan et al. 2019;
Yin and Shi 2018; Zou et al. 2018], object motion [Dai et al. 2019;
Vijayanarasimhan et al. 2017], surface normal [Qi et al. 2018], edge
[Yang et al. 2018], and visual odometry [Andraghetti et al. 2019;
Shi et al. 2019; Wang et al. 2018b]. Other notable efforts include
using stereo information [Guo et al. 2018; Watson et al. 2019], better
network architecture and training loss design [Gordon et al. 2019;
Guizilini et al. 2019], scale-consistent ego-motion network [Bian
et al. 2019], incorporating 3D geometric constraints [Mahjourian
et al. 2018], and learning from unknown camera intrinsics [Chen
et al. 2019b; Gordon et al. 2019].
Many of these self-supervised methods use a photometric loss.
However, these losses can be satisfied even if the geometry is not
consistent (in particular, in poorly textured areas). In addition, they
do not work well for temporally distant frames because of larger
appearance changes. In our ablation study, however, we show that
long-range temporal constraints are important for achieving good
results.
Multi-view reconstruction. Multi-view stereo algorithms estimate
scene depth using multiple images captured from arbitrary viewpoints [Furukawa et al. 2015; Schonberger and Frahm 2016; Seitz
et al. 2006]. Recent learning-based methods [Huang et al. 2018; Im
et al. 2019; Kusupati et al. 2019; Ummenhofer et al. 2017; Yao et al.
1
https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/
ACM Trans. Graph., Vol. 39, No. 4, Article 71. Publication date: July 2020.
Consistent Video Depth Estimation â€¢ 71:3
2018] leverage well-established principles in traditional geometrybased approaches (e.g., cost aggregation and plane-sweep volume)
and show state-of-the-art performance in multi-view reconstruction.
However, these multi-view stereo techniques assume a static scene.
For dynamic objects, these methods either produce erroneous estimates or drop pixels with low confidence. In contrast, our method
produces dense depth even in the presence of moderate dynamic
scene motion.
Depth from video. Recovering dense depth from monocular video
is a challenging problem. To handle moving objects, existing techniques rely on motion segmentation and explicit motion modeling
for the moving objects in the scene [Casser et al. 2019; Karsch et al.
2014; Ranftl et al. 2016]. Several methods estimate depth by integrating motion estimation and multi-view reconstruction using two
frames [Ummenhofer et al. 2017; Wang et al. 2019a] or a varying
number of frames [Bloesch et al. 2018; Valentin et al. 2018; Zhou et al.
2018]. The state-of-the-art video-to-depth methods [Liu et al. 2019;
Teed and Deng 2020] regress depth (or predict a distribution over
depth) based on the cost volume constructed by warping nearby
frames to a reference viewpoint. Such model designs thus do not
account for dynamically moving objects. In contrast, while we also
leverage constraints derived from multi-view geometry, our depth is
estimated from (fine-tuned) single-image depth estimation models,
and thereby handle dynamic object naturally and without the need
for explicit motion segmentation.
Temporal consistency. Applying single-image based methods independently to each frame in a video often produce flickering results.
In light of this, various approaches for enforcing temporal consistency have been developed in the context of style transfer [Chen
et al. 2017; Huang et al. 2017; Ruder et al. 2016], image-based graphics applications [Lang et al. 2012], video-to-video synthesis [Wang
et al. 2018a], or application-agnostic post-processing algorithms
[Bonneel et al. 2015; Lai et al. 2018]. The core idea behind these
methods is to introduce a â€œtemporal consistency loss" (either at
training or testing time) that encourages similar values along the
temporal correspondences estimated from the input video. In the
context of depth estimation from video, several efforts have been
made to make the estimated depth more temporally consistent by
explicitly applying optical flow-based consistency loss [Karsch et al.
2014] or implicitly encouraging temporal consistency using recurrent neural networks [Patil et al. 2020; Wang et al. 2019b; Zhang et al.
2019b]. Our work differs in that we aim to produce depth estimates
from a video that are geometrically consistent. This is particularly
important for casually captured videos because the actual depth
may not be temporally consistent due to camera motion over time.
Depth-aware visual effects. Dense depth estimation facilitates a
wide variety of visual effects such as synthetic depth-of-field [Wadhwa et al. 2018], novel view synthesis [Hedman et al. 2017; Hedman
and Kopf 2018; Hedman et al. 2018; Shih et al. 2020], and occlusionaware augmented reality [Holynski and Kopf 2018]. Our work on
consistent depth estimation from causally captured videos enables
several new video special effects.
Test-time training. Learning on testing data has been used in
several different problem contexts: online update in visual tracking
[Kalal et al. 2011; Ross et al. 2008], adapting object detectors from
images to videos [Jain and Learned-Miller 2011; Tang et al. 2012], and
learning video-specific features for person re-identification [Cinbis
et al. 2011; Zhang et al. 2019a]. The work most closely related to
ours is that of [Casser et al. 2019; Chen et al. 2019b] where they
improve monocular depth estimation results by fine-tuning a pretrained model using the testing video sequence. Note that any selfsupervised method can be trained at test time (as in [Casser et al.
2019; Chen et al. 2019b]). However, the focus of previous methods
is largely on achieving per-frame accuracy, while our focus is on
achieving an accurate prediction with global geometric consistency.
Our method achieves accurate and detailed reconstructions with a
higher level of temporal smoothness than previous methods, which
is important for many video-based applications.
Aside from these goals, there are important technical differences
between our method and prior ones. The method in [Casser et al.
2019] performs a binary object-level segmentation and estimates
rigid per-object transformations. This is appropriate for rigid objects such as cars in a street scene, but less so for highly deformable
subjects such as people. The method in [Chen et al. 2019b] uses a
geometric loss, similar to ours. However, they only train on consecutive frame pairs and relative poses. We use absolute poses and
long-term temporal connections, which our ablation shows is critical for achieving good results (Figure 6).
3 OVERVIEW
Our method takes a monocular video as input and estimates a camera pose as well as a dense, geometrically consistent depth map (up
to scale ambiguity) for each video frame. The term geometric consistency not only implies that the depth maps do not flicker over
time but also, that all the depth maps are in mutual agreement.
That is, we may project pixels via their depth and camera pose accurately amongst frames. For example, all observations of a static
point should be mapped to a single common 3D point in the world
coordinate system without drifting.
Casually captured input videos exhibit many characteristics that
are challenging for depth reconstruction. Because they are often
captured with a handheld, uncalibrated camera, the videos suffer
from motion blur and rolling shutter deformations. The poor lighting conditions may cause increased noise level and additional blur.
Finally, these videos usually contain dynamically moving objects,
such as people and animals, thereby breaking the core assumption
of many reconstruction systems designed for static scenes.
As we explained in the previous sections, in problematic parts
of a scene, traditional reconstruction methods typically produce
â€œholesâ€ (or, if forced to return a result, estimate very noisy depth.) In
areas where these methods are confident enough to return a result,
however, it is typically fairly accurate and consistent, because they
rely strongly on geometric constraints.
Recent learning-based methods [Liu et al. 2019; Ranftl et al. 2019]
have complementary characteristics. These methods handle the
challenges described above just fine because they leverage a strong
data-driven prior to predict plausible depth maps from any input
image. However, applying these methods independently for each
ACM Trans. Graph., Vol. 39, No. 4, Article 71. Publication date: July 2020.
71:4 â€¢ Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf
Fig. 2. Method overview. With a monocular video as input, we sample a pair of (potentially distant) frames and estimate the depth using a pre-trained,
single-image depth estimation model to obtain initial depth maps. From the pair of images, we establish correspondences using optical flow with forwardbackward consistency check. We then use these correspondences and the camera poses to extract geometric constraints in 3D. We decompose the 3D
geometric constraints into two losses: 1) spatial loss and 2) disparity loss and use them to fine-tune the weight of the depth estimation network via standard
backpropagation. This test-time training enforces the network to minimize the geometric inconsistency error across multiple frames for this particular video.
After the fine-tuning stage, our final depth estimation results from the video is computed from the fine-tuned model.
frame results in geometrically inconsistent and temporally flickering
results over time.
Our idea is to combine the strengths of both types of methods. We
leverage existing single-image depth estimation networks [Godard
et al. 2019; Li et al. 2019; Ranftl et al. 2019] that have been trained
to synthesize plausible (but not consistent) depth for general color
images, and we fine-tune the network using the extracted geometric
constraints from a video using traditional reconstruction methods.
The network thus learns to produce geometrically consistent depth
on a particular video.
Our method proceeds in two stages:
Pre-processing (Section 4): As a foundation for extracting geometric constraints among video frames, we first perform a traditional Structure-from-Motion (SfM) reconstruction pipeline using
an off-the-shelf open-source software COLMAP [Schonberger and
Frahm 2016]. To improve pose estimation for videos with dynamic
motion, we apply Mask R-CNN [He et al. 2017] to obtain people
segmentation and remove these regions for more reliable keypoint
extraction and matching, since people account for the majority of
dynamic motion in our videos. This step provides us with accurate
intrinsic and extrinsic camera parameters as well as a sparse point
cloud reconstruction. We also estimate dense correspondence between pairs of frames using optical flow. The camera calibration
and dense correspondence, together, enable us to formulate our
geometric losses, as described below.
The second role of the SfM reconstruction is to provide us with
the scale of the scene. Because our method works with monocular
input, the reconstruction is ambiguous up to scale. The output of
the learning-based depth estimation network is scale-invariant as
well. Consequently, to limit the amount the network has to change,
we adjust the scale of the SfM reconstruction so that it matches the
learning-based method in a robust average sense.
Test-time Training (Section 5): In this stage, which comprises
our primary contribution, we fine-tune a pre-trained depth estimation network so that it produces more geometrically consistent
depth for a particular input video. In each iteration, we sample a
pair of frames and estimate depth maps using the current network
parameters (Figure 2). By comparing the dense correspondence with
reprojections obtained using the current depth estimates, we can
validate whether the depth maps are geometrically consistent. To
this end, we evaluate two geometric losses, 1) spatial loss and 2)
disparity loss and back-propagate the errors to update the network
weights (which are shared across for all frames). Over time, iteratively sampling many frame pairs, the losses are driven down, and
the network learns to estimate depth that is geometrically consistent for this video while retaining its ability to provide plausible
regularization in less constrained parts.
The improvement is often dramatic, our final depth maps are
geometrically consistent, temporally coherent across the entire
video while accurately delineate clear occluding boundaries even
for dynamically moving objects. With depth computed, we can have
proper depth edge for occlusion effect and make the geometry of the
real scene interact with the virtual objects. We show various compelling visual effects made possible by our video depth estimation
in Section 6.5.
4 PRE-PROCESSING
Camera registration. We use the structure-from-motion and multiview stereo reconstruction software COLMAP [Schonberger and
Frahm 2016] to estimate for each frame ğ‘– of the ğ‘ video frames the
ACM Trans. Graph., Vol. 39, No. 4, Article 71. Publication date: July 2020.
Consistent Video Depth Estimation â€¢ 71:5
intrinsic camera parameters ğ¾ğ‘–
, the extrinsic camera parameters
(ğ‘…ğ‘–
, ğ‘¡ğ‘–), as well as a semi-dense depth map ğ·
MVS
ğ‘–
. We set the values
to zeros for pixels where the depth is not defined.
Because dynamic objects often cause errors in the reconstruction,
we apply Mask R-CNN [He et al. 2017] to segment out people (the
most common â€œdynamic objectsâ€ in our videos) in every frame independently, and suppress feature extraction in these areas (COLMAP
provides this option). Since smartphone cameras are typically not
distorted2
, we use the SIMPLE_PINHOLE camera model and solve
for the shared camera intrinsics for all the frames, as this provides
a faster and more robust reconstruction. We use the exhaustive
matcher and enable guided matching.
Scale calibration. The scale of the SfM and the learning-based
reconstructions typically do not match, because both methods are
scale-invariant. This manifests in different value ranges of depth
maps produced by both methods. To make the scales compatible
with the geometric losses, we adjust the SfM scale, because we can
simply do so by multiplying all camera translations by a factor.
Specifically, let ğ·
NN
ğ‘–
be the initial depth map produced by the
learning-based depth estimation method. We first compute the relative scale for image ğ‘– as:
ğ‘ ğ‘– = median
ğ‘¥
n
ğ·
NN
ğ‘–
(ğ‘¥) / ğ·
MVS
ğ‘–
(ğ‘¥)



ğ·
MVS
ğ‘–
(ğ‘¥) â‰  0
o
, (1)
where ğ·(ğ‘¥) is the depth at pixel ğ‘¥.
We can then compute the global scale adjustment factor ğ‘  as
ğ‘  = mean
ğ‘–
{ğ‘ ğ‘– } , (2)
and update all the camera translations
ğ‘¡Ëœ
ğ‘– = ğ‘  Â· ğ‘¡ğ‘–
. (3)
Frame sampling. In the next step, we compute a dense optical
flow for certain pairs of frames. This step would be prohibitively
computationally expensive to perform for all ğ‘‚(ğ‘
2
) pairs of frames
in the video. We, therefore, use a simple hierarchical scheme to
prune the set of frame pairs down to ğ‘‚(ğ‘).
The first level of the hierarchy contains all consecutive frame
pairs,
ğ‘†0 =

(ğ‘–, ğ‘—)


|ğ‘– âˆ’ ğ‘— | = 1
	
. (4)
Higher levels contain a progressively sparser sampling of frames,
ğ‘†ğ‘™ =

(ğ‘–, ğ‘—)


|ğ‘– âˆ’ ğ‘— | = 2
ğ‘™
, ğ‘– mod 2ğ‘™âˆ’1 = 0
	
. (5)
The final set of sampled frames is the union of the pairs from all
levels,
ğ‘† =
Ã˜
0â‰¤ğ‘™ â‰¤ âŒŠlog2
(ğ‘ âˆ’1) âŒ‹
ğ‘†ğ‘™
. (6)
Optical flow estimation. For all frame pairs (ğ‘–, ğ‘—) in ğ‘† we need to
compute a dense optical flow field ğ¹ğ‘–â†’ğ‘—
. Because flow estimation
works best when the frame pairs align as much as possible, we
first align the (potentially distant) frames using a homographywarp (computed with a RANSAC-based fitting method [Szeliski
2010]) to eliminate dominant motion between the two frames (e.g.,
due to camera rotation). We then use FlowNet2 [Ilg et al. 2017] to
2Our test sequences (Section 6.1) are captured with a fisheye camera, and we remove
the distortion through rectification.
compute the optical flow between the aligned frames. To account of
moving objects and occlusion/dis-occlusion (as they do not satisfy
the geometric contraints or are unreliable), we apply a forwardbackward consistency check and remove pixels that have forwardbackward errors larger than 1 pixel, producing a binary map ğ‘€ğ‘–â†’ğ‘—
.
Furthermore, we observe that the flow estimation results are not
reliable for frame pairs with little overlap. We thus exclude any
frame pairs where |ğ‘€ğ‘–â†’ğ‘—
| is less than 20% of the image area from
consideration.
5 TEST-TIME TRAINING ON INPUT VIDEO
Now we are ready to describe our test-time training procedure, i.e.,
how we coerce the depth network through fine-tuning it with a
geometric consistency loss to producing more consistent depth for
a particular input video. We first describe our geometric loss, and
then the overall optimization procedure.
Geometric loss. For a given frame pair (ğ‘–, ğ‘—) âˆˆ ğ‘†, the optical flow
field ğ¹ğ‘–â†’ğ‘— describes which pixel pairs show the same scene point.
We can use the flow to test the geometric consistency of our current
depth estimates: if the flow is correct and a flow-displaced point
ğ‘“ğ‘–â†’ğ‘— (ğ‘¥) is identical to the depth-reprojected point ğ‘ğ‘–â†’ğ‘— (ğ‘¥) (both
terms defined below), then the depth must be consistent.
The idea of our method is that we can turn this into a geometric
loss Lğ‘–â†’ğ‘— and back-propagate any consistency errors through the
network, so as to coerce it into to producing depth that is more
consistent than before. Lğ‘–â†’ğ‘— comprises two terms, an image-space
loss L
spatial
ğ‘–â†’ğ‘—
, and a disparity loss L
disparity
ğ‘–â†’ğ‘—
. To define them, we first
discuss some notation.
Let ğ‘¥ be a 2D pixel coordinate in frame ğ‘–. The flow-displaced point
is simply
ğ‘“ğ‘–â†’ğ‘— (ğ‘¥) = ğ‘¥ + ğ¹ğ‘–â†’ğ‘— (ğ‘¥). (7)
To compute the depth-reprojected point ğ‘ğ‘–â†’ğ‘— (ğ‘¥), we first lift the
2D coordinate to a 3D point ğ‘ğ‘–(ğ‘¥) in frame ğ‘–â€™s camera coordinate
system, using the camera intrinsics ğ¾ğ‘– as well as the current depth
estimate ğ·ğ‘–
,
ğ‘ğ‘–(ğ‘¥) = ğ·ğ‘–(ğ‘¥) ğ¾
âˆ’1
ğ‘–
ğ‘¥,Ëœ (8)
where ğ‘¥Ëœ is the homogeneous augmentation of ğ‘¥. We then further
project the point to the other frame ğ‘—â€™s camera coordinate system,
ğ‘ğ‘–â†’ğ‘— (ğ‘¥) = ğ‘…
T
ğ‘—

ğ‘…ğ‘– ğ‘ğ‘–(ğ‘¥) + ğ‘¡Ëœ
ğ‘– âˆ’ ğ‘¡Ëœ
ğ‘—

, (9)
and finally convert it back to a pixel position in frame ğ‘—,
ğ‘ğ‘–â†’ğ‘— (ğ‘¥) = ğœ‹

ğ¾ğ‘— ğ‘ğ‘–â†’ğ‘— (ğ‘¥)

, (10)
where ğœ‹

[ğ‘¥, ğ‘¦, ğ‘§]
T

= [
ğ‘¥
ğ‘§
,
ğ‘¦
ğ‘§
]
T
.
With this notation, the image-space loss for a pixel can be easily
defined:
L
spatial
ğ‘–â†’ğ‘—
(ğ‘¥) =



ğ‘ğ‘–â†’ğ‘— (ğ‘¥) âˆ’ ğ‘“ğ‘–â†’ğ‘— (ğ‘¥)




2
, (11)
which penalizes the image-space distance between the flow-displaced
and the depth-reprojected point.
The disparity loss, similarly, penalizes the disparity distance in
camera coordinate system:
L
disparity
ğ‘–â†’ğ‘—
(ğ‘¥) = ğ‘¢ğ‘–


 ğ‘§
âˆ’1
ğ‘–â†’ğ‘—
(ğ‘¥) âˆ’ ğ‘§
âˆ’1
ğ‘—

ğ‘“ğ‘–â†’ğ‘— (ğ‘¥)



 , (12)
ACM Trans. Graph., Vol. 39, No. 4, Article 71. Publication date: July 20