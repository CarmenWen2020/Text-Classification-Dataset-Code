In typical cameras the optical system is designed first; once it is fixed, the
parameters in the image processing algorithm are tuned to get good image
reproduction. In contrast to this sequential design approach, we consider
joint optimization of an optical system (for example, the physical shape of the
lens) together with the parameters of the reconstruction algorithm. We build
a fully-differentiable simulation model that maps the true source image to the
reconstructed one. The model includes diffractive light propagation, depth
and wavelength-dependent effects, noise and nonlinearities, and the image
post-processing. We jointly optimize the optical parameters and the image
processing algorithm parameters so as to minimize the deviation between the
true and reconstructed image, over a large set of images. We implement our
joint optimization method using autodifferentiation to efficiently compute
parameter gradients in a stochastic optimization algorithm. We demonstrate
the efficacy of this approach by applying it to achromatic extended depth of
field and snapshot super-resolution imaging.
CCS Concepts: • Computing methodologies → Computational photography; Reconstruction;
Additional Key Words and Phrases: computational optics
1 INTRODUCTION
The visual systems of animals are often highly adapted to their
environments [Land and Nielsson 2002]. In spite of being used for
a diverse range of applications, digital imaging systems, on the
other hand, have been engineered to mimic only one of these systems: the human eye. While such a general-purpose approach to
imaging is sometimes successful, it leaves an important question
unanswered: What is the optimal camera design for a given task? To
address this question, domain-specific computational cameras have
emerged over the last two decades [Nayar 2006]. By co-designing
camera optics and image processing algorithms, computational cameras have the potential to optimize task-specific performance over
conventional, general-purpose imaging systems in a wide range of
applications.
To date, computational cameras have demonstrated new imaging capabilities, such as extended depth of field [Cossairt and Nayar 2010; Cossairt et al. 2010; Dowski and Cathey 1995], superresolution [Ben-Ezra et al. 2004], and high dynamic range [Debevec and Malik 1997; Mann et al. 1995] imaging. Optical elements
have also been optimized, for example to localize microscopic point
emitters in a 3D volume via point spread function (PSF) engineering [Pavani et al. 2009; Shechtman et al. 2014] or to optimize the
focusing performance of a diffractive optical element across the
color spectrum [Peng et al. 2016]. Yet, all of these approaches are
either heuristic or use some proxy metric on the PSF rather than
considering the image quality after post-processing. Without a true
end-to-end approach for jointly optimizing parameters of the imageforming optics and the algorithm processing the data, being able
to find an optimal computational camera for a given task remains
elusive.
What does optimizing a domain-specific computational camera
entail? First, the task has to be defined and appropriate quality
metrics devised to assess a camera’s performance. Generative image processing tasks include denoising, deconvolution, or other
forms of image reconstruction; their quality is often measured as
peak signal-to-noise-ratio (PSNR). Discriminative tasks, on the other
hand, would use a very different quality metric, such as classification accuracy for image classification. Second, it may be helpful to
characterize the input data for a specific task. Natural images, for
example, follow certain statistics that can be exploited as priors for
generative tasks. But it may not always be obvious what good priors
for domain-specific datasets actually are. Third, post-processing
algorithms may vary drastically between different tasks or even for
the same task, but in different settings. Conventional approaches to
∗These authors contributed equally.
computational camera design, such as PSF engineering, do not offer
the flexibility of addressing all of these challenges simultaneously.
In this paper, we introduce a new paradigm for computational
camera design: end-to-end optimization of a refractive or diffractive optical element with respect to the output of a reconstruction algorithm, using stochastic gradient methods. We build a fullydifferentiable wave optics image formation model that is used to
jointly optimize the optical parameters and the image processing
algorithm parameters for domain-specific computational cameras.
Specifically, our contributions are
• We introduce a framework for end-to-end optimization of an
optical element with respect to the output of a reconstruction
algorithm, using stochastic gradient methods. The framework
includes a wave optics image formation model, object depth
and wavelength-dependent effects, sensor noise and nonlinearities, and the image processing. The source code is publicly
available 1
.
• We validate this framework in simulation for the applications
of achromatic extended depth of field and snapshot superresolution imaging.
• We fabricate the optimized optical elements, using photolithography for diffractive elements and diamond turning for refractive lenses, and verify that experimental results from a
prototype camera setup match the simulations.
Scope. In principle, the proposed framework for end-to-end optimization of optics and image processing generalizes to various
low-level and high-level algorithms. Deep convolutional neural networks, for example, could be used to optimize the lens of a camera
tailored to image classification or other high-level tasks. Exploring
this large space of application- and domain-specific computational
cameras is an exciting vision towards which we take first steps in
this paper. We believe that the insights provided by our work on
developing fully-differentiable wave optics image formation models,
inverting them robustly with tools like TensorFlow, and actually
fabricating the optimized optical elements are invaluable for the
emerging field of computational optics.
2 RELATED WORK
Computational Cameras. Much work on computational photography has focused on improving basic capabilities of a camera, such as
depth of field [Cossairt and Nayar 2010; Cossairt et al. 2010; Dowski
and Cathey 1995], dynamic range [Debevec and Malik 1997; Mann
et al. 1995; Reinhard et al. 2005; Rouf et al. 2011], and image resolution [Ben-Ezra et al. 2004; Brady et al. 2012; Cossairt et al. 2011].
Computational photography has also been used for tasks as diverse
as motion deblurring [Raskar et al. 2006], defocus deblurring [Zhou
et al. 2009; Zhou and Nayar 2009], depth estimation [Levin et al.
2007, 2009], multispectral imaging [Wagadarikar et al. 2008], light
field imaging [Marwah et al. 2013; Ng et al. 2005; Veeraraghavan
et al. 2007], and lensless imaging [Antipa et al. 2016; Asif et al. 2017].
Many of these approaches use either optical coding, multiplexing,
burst photography [Hasinoff et al. 2016], or multi-shot approaches
to capture high-dimensional visual data [Wetzstein et al. 2011].
1
https://vsitzmann.github.io/deepoptics
ACM Trans. Graph., Vol. 37, No. 4, Article 114. Publication date: August 2018.
End-to-end Optimization of Optics and Image Processing for Achromatic Extended Depth of Field and Super-resolution Imaging • 114:3
The proposed end-to-end optimization framework could be applied to many of these applications, as it introduces a general design
paradigm for computational cameras that optimizes directly for the
post-processed output with respect to a chosen quality metric and
domain-specific dataset.
Deep Computational Photography. In computer vision, natural
language processing, and many other fields, the emergence of deep
learning has lead to rapid progress in a number of challenging tasks
and state-of-the-art results for well-established problems. The computational photography community too is at the cusp of adopting
tools from the deep learning community, such as convolutional neural networks. For example, high dynamic range image estimation
from a single low dynamic range photograph was recently demonstrated to achieve unprecedented image quality [Eilertsen et al. 2017;
Kalantari and Ramamoorthi 2017; Zhang and Lalonde 2017]. The
task of super-resolving a single image has also been approached
via deep learning [Dong et al. 2016b; Shi et al. 2016]. Finally, it was
recently shown that it is possible to learn the mapping from a single
image to a light field [Srinivasan et al. 2017] and to produce light
field video clips from a hybrid camera [Wang et al. 2017].
All of these approaches have demonstrated state-of-the-art results
for various computational photography applications. Yet, most of
them only consider the algorithm processing the data. We go one
step further and ask whether it is possible to optimize the co-design
of optics and image processing for domain-specific computational
cameras in an end-to-end fashion. Although we demonstrate the
efficacy of our approach with applications that rely on relatively
simple reconstruction algorithms, in principle, our approach could
also be used to optimize optical elements leveraging more advanced
deep computational photography algorithms.
Optimizing Optical Elements. Optimizing the parameters of optical elements and point spread function engineering are well-known
techniques in the computational optics and visual computing communities. Optimized optical system parameters have proven useful
for extended depth of field [Dowski and Cathey 1995; Flores et al.
2004; Liu 2007], motion [Raskar et al. 2006] and defocus [Zhou and
Nayar 2009] deblurring, 4D light field imaging [Marwah et al. 2013],
super-resolved localization microscopy [Pavani et al. 2009; Shechtman et al. 2014], and full-color imaging with diffractive optics [Heide
et al. 2016; Peng et al. 2016]. Optimization of optical models has also
been proposed for multi-element systems to either arrive at novel
arrangements of off-the-shelf lenses [Sun et al. 2015] or to allow
precise calibration of models [Shih et al. 2012] of these systems.
Two observations remain. First, previously-proposed optimization
approaches of optical elements are mainly based on heuristic cost
functions applied to the PSFs, which may be a feasible approach
for image deconvolution but it remains unclear how the PSF of a
camera affects higher-level computer vision tasks such as image
classification; second, although image processing is applied to the
recorded images to remove residual aberrations or perform some
inference tasks, the post-processing algorithm is usually independent of the optics design and fails to provide significant insights to
guide it.
We also optimize the point spread function of an optical system, but do so in an end-to-end manner using a data-driven approach that applies a cost function on the reconstructed image,
not on the PSF. Our approach is motivated by recent advances in
hardware, autodifferentiation tools, and optimization algorithms
for deep learning. While some recent work investigates joint optimization of either binary masks or color filter arrays with neural
network post-processing for video compressed sensing or demosaicking [Chakrabarti 2016; Iliadis et al. 2016], they do not consider the optimization of phase-modulating optical elements such as
lenses and do not consider diffraction in their forward model. With
this work, we thus take first steps towards utilizing the full potential
of end-to-end optimization for computational camera design.
Achromatic Extended Depth of Field. Extended depth of field (EDOF)
is a classic application of computational imaging. An extension is
the design of a single optical element that combines EDOF with
achromaticity, yielding all-in-focus images with minimal chromatic
aberrations. Depth and wavelength are closely coupled in the image formation, such that the seminal wavefront-coding approach
to EDOF, the cubic phase plate [Dowski and Cathey 1995], displays increased achromaticity, and achromatic elements, such as the
diffractive achromat [Peng et al. 2016], display some extended depth
of field. However, this duality breaks down at the extremes of either
wavelength or depth. One possible solution are metalenses, which
have recently enabled the fabrication of single ultrathin optical elements that encode wavelength-dependent phase patterns onto the
incoming light, thereby achieving physical achromaticity without
the need for post-processing [Chen et al. 2018; Yang et al. 2017]
and, in combination with a digital filter, achromatic EDOF [Colburn
et al. 2018]. However, metalenses are currently difficult and costly
to fabricate, and usually only support small numerical apertures
with low light efficiency. While compound systems are well-known
for their ability to correct chromatic aberrations, they increase the
device footprint. Hybrid elements for achromatic EDOF have been
proposed [Flores et al. 2004; Liu 2007] as a compromise that reduces
chromatic aberrations by combining a single diffractive and a single
refractive optical element, but add complexity to the manufacturing
process.
With this work, we propose a novel perspective of joint optimization of a single diffractive or refractive element with a deconvolution post-processing step, affording achromatic EDOF with standard
diamond-turning or lithography manufacturing techniques, without increasing device footprint, and with no hand-crafted losses
applied to the PSF. We note that this approach does not preclude
more sophisticated optical elements which may offer more degrees
of freedom in the optics design. Future work may thus extend the
proposed end-to-end optimization framework to other optical elements.
3 END-TO-END OPTIMIZATION OF OPTICS AND
RECONSTRUCTION
In the following, we derive a wave-based image formation model
that accounts for diffraction and wavelength-dependent effects
when imaging natural scenes. We assume spatially incoherent light,
meaning that light reflected from an object point interferes with
ACM Trans. Graph., Vol. 37, No. 4, Article 114. Publication date: August 2018.
114:4 • Sitzmann, V. et al.
Incident Wave
Optical
Element
Fresnel Propagation
Sensor PSF
Intensity
Fig. 2. Differentiable wave-based PSF simulation module. A light wave ϕλ,d
with a given wavelength λ and a curvature appropriate for a point source at
a distance d is incident on the aperture plane containing the optical element
(refractive index n) to be optimized. The optical element shifts the phase
of the incident wavefront. The resulting wavefront is propagated by the
aperture-sensor distance z via Fresnel propagation. The intensities of the
sensor-incident wavefront define the PSF pλ,d . Optical elements can be
parameterized both as height maps or using a basis representation, such as
Zernike polynomials.
other light reflected from that same point along a different path, but
that it does not interfere with light from other points in the scene.
The image formation model is based on Fourier optics [Goodman
2017], and we show how to efficiently integrate it into the workflow
of modern deep learning tools.
3.1 Image Formation Model
3.1.1 Wave-based Point Spread Function Model. Consider a single refractive or diffractive optical element, such as a thin lens. This
element delays the phase of a complex-valued wave field proportionally to its thickness h
ϕ

x
′
,y
′

=
2π∆n
λ
h

x
′
,y
′

. (1)
Here, λ is the wavelength and ∆n is the refractive index difference
between air and the material of the optical element.
A wave field Uλ with amplitude A and phase ϕd
incident on the
optical element will be affected as
Uλ

x
′
,y
′
, z = 0

= A

x
′
,y
′

e
i(ϕd (x
′
,y
′
)+ϕ(x
′
,y
′
))
, (2)
such that Uλ
(x
′
,y
′
, 0) is the wave field right after it passed through
the optical element. As illustrated in Figure 2, the field propagates
in free space by a distance z as
Uλ
(x,y, z) =
e
ikz
iλz
ZZ U

x
′
,y
′
, 0

e
ik
2z

(x−x
′
)
2+(y−y
′
)
2

dx ′
dy
′
.
(3)
This formulation uses the Fresnel propagation operator, which is
an accurate model for near and far distances when λ ≪ z. The
wavenumber is k = 2π/λ.
To derive a point spread function (PSF) p of the optical element,
we model a plane wave, representing a single point at optical infinity, propagating along the optical axis through the element before
reaching a sensor at a distance z from the element as
pλ
(x,y) ∝




F

A

x
′
,y
′

e
iϕ(x
′
,y
′
)
e
i
π
λz

x
′2+y
′2
 




2
. (4)
Note that the PSF is wavelength dependent and that its intensity
is given by the squared magnitude of the complex-valued wave
field 
Uλ
(x,y, z)


2
. Detailed derivations of these formulations can
be found in the textbook by Goodman [2017].
3.1.2 From PSF to Image. Although Equation 4 only considers
the image formation of a single point source with a specific wavelength, we can extend this model to account for a natural scene.
In this context, the point spread functions from different scene
points add incoherently, i.e. in intensity, on the sensor. Assuming
that the paraxial approximation is valid, this image formation is a
shift-invariant convolution of the image and the PSF; consequently,
off-axis aberrations like coma or chromatic off-axis aberration are
neglected. We further account for the wavelength sensitivity κc of
the sensor for each of the three color channels R,G, B
Ic (x,y) =
Z
(Iλ ∗ pλ
) (x,y) κc (λ) dλ, (5)
where the index c denotes the color channel. Note that this image
formation is physically accurate for spatially incoherent scenes that
are observed at a distance far from the optical element, because each
scene point is effectively modeled as a plane wave. The benefit of this
simplification is that the image formation becomes computationally
very efficient, but the limitation is that only scenes at optical infinity
are modeled correctly.
To account for this limitation, we can lift the restriction imposed
by plane waves and model point sources at different distances by
modeling spherical waves with the appropriate curvature using the
term ϕd
in Equation 2. In this case, the image formation ceases to be
a shift-invariant convolution. Nevertheless, for sufficiently large distances between the scene and the optical element, a shift-invariant
image formation may still be a good approximation [Antipa et al.
2016].
3.1.3 Sensor. The image Ic formed on the sensor is integrated
over the sensor pixels and corrupted by noise, yielding a measurement yc given by
yc = S (Ic ) + η, (6)
where S is the pixel integration and sampling operator and η ∼
N (0, σ
2
) is Gaussian read noise.
3.1.4 Reconstruction. The final stage of the proposed model is
image reconstruction. We reconstruct the estimate HIc of the source
image Iλ
for RGB wavelengths by solving the Tikhonov regularized
least-squares problem
min
{HI }
∥yc − S (pc ∗HIc )∥
2
2
+γ ∥HIc ∥
2
2
,
(7)
where HI is the unknown variable, pc is the PSF pλ
integrated over
the wavelength sensitivity κc in a narrow band around RGB wavelengths, and γ > 0 is an (optimized) regularization parameter.
When the PSF discretization size matches the sensor pixel size, the
pixel integration operator S is the identity. We can then solve problem (7) in closed form with Wiener filtering under the simplifying
assumption of circular boundary conditions, which we approximate
by symmetric padding of yc . The Wiener filtering operation is given
ACM Trans. Graph., Vol. 37, No. 4, Article 114. Publication date: August