This paper addresses the problem of offline path and movement planning
for wall climbing humanoid agents. We focus on simulating bouldering, i.e.
climbing short routes with diverse moves, although we also demonstrate
our system on a longer wall. Our approach combines a graph-based highlevel path planner with low-level sampling-based optimization of climbing
moves. Although the planning problem is complex, our system produces
plausible solutions to bouldering problems (short climbing routes) in less
than a minute. We further utilize a k-shortest paths approach, which enables
the system to discover alternative paths – in climbing, alternative strategies
often exist, and what might be optimal for one climber could be impossible
for others due to individual differences in strength, flexibility, and reach. We
envision our system could be used, e.g. in learning a climbing strategy, or as
a test and evaluation tool for climbing route designers. To the best of our
knowledge, this is the first paper to solve and simulate rich humanoid wall
climbing, where more than one limb can move at the same time, and limbs
can also hang free for balance or use wall friction in addition to predefined
holds.
CCS Concepts: • Computing methodologies → Physical simulation;
Additional Key Words and Phrases: Climbing Motion Synthesis, Rapidly
Exploring Random Trees, K Shortest-Paths on Graph, Dynamic Graph, A*
Prune
1 INTRODUCTION
Path/motion planning is crucial in many applications [Hoy et al.
2015; Rastgoo et al. 2014]. Motion planning for climbing humanoid
agents is an intricate problem, and its applications vary from searchand-rescue in robotics [Bretl 2006] to simulation of virtual humanoid
agent in computer games [Libeau et al. 2009]. Its difficulty arises
from the high number of degrees of freedom (DoF) of the human
body as well as the long planning horizon required. Even the lowlevel short horizon problem of fully physically based humanoid
balancing while reaching for an object in any direction is a complex one, and efficient controllers for such tasks have only recently
emerged [Hämäläinen et al. 2015; Jain et al. 2009; Tassa et al. 2014].
Planning a sequence of such complex movements is still to a large extent an unsolved problem. In climbing, one must consider collisions
of the agent with the wall and itself as well as stability of balance;
when switching holds, the agent may swing around or even fall.
Finally, we are striving for physically plausible movement, i.e., using
limited forces and torques that further constrain the movements.
In previous work, a variety of simplifications have been utilized.
In his work on multi-limbed climbing robots, Bretl [2006] considered only 2 DoF per limb, and the robot’s motions are planned in
its configuration space and dynamically instable configurations are
not allowed, which admits for a closed-form solution for the robot’s
configuration given its center and end effector positions. To implement human-like motions, we also need to consider full humanoid
body dynamics, and since a closed form solution does not exist,
ACM Transactions on Graphics, Vol. 36, No. 4, Article 43. Publication date: July 2017.
43:2 • Kourosh Naderi, Joose Rajamäki, and Perttu Hämäläinen
an appropriate movement optimization method is needed. Such an
approach is presented in [Jain et al. 2009], however focusing on
single climbing moves without path planning. The agent also implements a "three-hold-rule", i.e., only moving one limb at a time and
keeping others on climbing holds. Similar to other previous work in
humanoid wall climbing, limbs are also restricted to only utilize predefined holds. However, research on human climbing suggests that
climbers may prefer to use less than three holds at a time [Sibella
et al. 2007]. In addition to standing on a hold, a foot can also be used
on the wall for friction climbing, or a leg can hang in the air for
balance, somewhat similar to how animals use their tails.
In this paper we are looking for an approach that can effectively
utilize three or two holds at a time while using the free limbs for
balancing or frictional contacts with the wall. Also, in order to
simulate real climbing, we wish to find a feasible path from a starting
stance that defines two allowed holds for the climber’s hands to the
so called "top hold" that can be reached with either hand, i.e., we
also consider the agent’s transition from standing to the wall. The
only simplification we utilize is that we assume at least two holds to
be used at the same time, in order to limit the size and connectivity
of our search graph. We also focus on bouldering, a rapidly growing
form of climbing on low walls without other safety equipment than
a soft floor. Bouldering reduces the number of holds to consider, but
is also perhaps more interesting from an animation perspective, as
it tends to highlight complex, technical moves and climbing as a
puzzle solving activity, as opposed to climbing long routes to build
endurance or conquer one’s fear of heights.
To summarize our technical approach, we grow a tree of paths
in the climber’s state space, inspired by Rapidly-Exploring Random Trees (RRT:s, [Lavalle 1998]). Planning in high-dimensional
space using randomized actions can, however, take a long time and
sometimes makes it impossible to find a feasible path [Garcia and
How 2005]. Instead, we grow the tree informed by graph search in
the lower-dimensional stance graph, which represents all possible
4-hold configurations or climber stances. Stances with free legs or
hands are also allowed. At any time, an A* Prune search [Liu and
Ramakrishnan 2001] of the stance graph gives us a path to try, and
a sampling-based low-level controller is then utilized to optimize
and simulate the transitions between stances. Even for a small bouldering route, the stance graph can have thousands of vertices and
edges. However, we demonstrate that a set of edge and node preference rules based on observations of human climbers can guide the
search so that the agent most of the time tries feasible paths first. If
the low-level controller fails in a transition, the preference for the
corresponding edge of the graph is adjusted, and A* Prune is re-run
to find an alternative path.
To the best of our knowledge, we are the first to demonstrate rich
humanoid wall climbing with free limbs with a possibility to utilize
wall friction, and to allow the user to browse alternative paths. Our
system also works with balance-intensive routes, e.g., where all
holds lie on a vertical line.
2 RELATED WORK
In this section, we first review some of the central research on path
planning in high-dimensional spaces. We then continue on the more
specific domain of humanoid climbing movements.
2.1 Path Planning in High-Dimensional Space
One of the most popular methods on navigation in high-dimensional
state space is the artificial potential function (APF) [Guldner et al.
1997], which uses the information from both the environment and
the robot for real-time path planning. However, it suffers from
trapping into local minima, and thus there are many variations of
this method trying to solve this problem. [Tanner and Kumar 2005]
use the combination of a centralized method with APF to define a
potential function for a multi-agent problem that navigates them
in the environment without collisions. [Brock and Khatib 2002]
introduce an elastic strip framework, which uses the combination
of global path planning and local path planning. To satisfy global
path planning, the initial and goal configurations are connected
through a couple of traceable points by a local path planner, and
APF is used as the local path planner to control the robot with high
dimensional configuration space. Although there are some methods
that deal with APF’s problem of getting stuck in local minima, these
methods are somewhat limited and need further information from
the environment.
The emergence of sampling based methods like Rapidly-Exploring
Random Tree (RRT) [Lavalle 1998] and Probabilistic Roadmaps
(PRM) [Kavraki et al. 1996] has enabled global search without needing to encode environment information as a potential function. Both
methods sample points in the agent’s configuration space, and then
connect the points to the previously sampled ones. PRM creates a
graph that can be searched, e.g. using Dijkstra’s method, whereas
RRT produces a tree with the current or goal configuration as the
root, and the tree provides a path to any sampled configuration.
As the dimensionality of the search space increases, RRT needs
more samples to find a feasible path, and performance decreases in
cases where narrow passages exist in the space [Garcia and How
2005]. This is due to the fact that numerous samples fall in the highdimensional free space far from the narrow passages. To mitigate
the dimensionality problems, RRT-Connect [Kuffner and LaValle
2000] attempts to greedily connect two trees, one growing from
the start configuration, and the other from the goal configuration.
RRT-Connect is faster than the original RRT [Lavalle 1998]; however,
[Garcia and How 2005] improve the efficiency of RRT even more
by using APF as the steering function that connects sampled points.
[Garcia and How 2005] demonstrate the combination of RRTs with
APF in controlling space crafts. The combination allows the path
planner to grow a tree in the full state of multiple space crafts which
has e.g. 48 DOF for 8 space crafts.
Our approach resembles RRT, as we grow a tree with a standing
pose near the wall as the root. However, although some RRT variants
can handle limited dynamics [LaValle and Kuffner 2001; Webb and
van den Berg 2013], RRTs alone are not enough to solve climbing
humanoid movement with simulated dynamics in addition to wall
and self-collisions.
2.2 Motion Planning for Climbing Humanoid Agents
An investigation about motion planning of a multi-limbed robot
has been carried out by [Bretl 2006]. The method assumes that
each limb of the robot has 2 DOF. The robot stances are kept in
a priority queue and the algorithm explores them by sampling a
ACM Transactions on Graphics, Vol. 36, No. 4, Article 43. Publication date: July 2017.
Discovering and Synthesizing Humanoid Climbing Movements • 43:3
robot stance from the queue and adding adjacent stances to the
sampled one. Adjacent robot stances are explored between holds
by solving for the closed form solution of the robot’s configuration
given the robot’s center and end limbs positions. When final desired
robot stance is found, the method uses PRM to explore the robot’s
configuration space for a detailed path to the stance. Sampled robot
configurations are accepted when they satisfy joint limitations and
the robot’s static stability. However, no closed form solution exists
for limbs with more than 3 DOF, and each limb of our simulated
humanoid agent has 4 controlled DOFs (shoulder 3 and elbow 1).
[Libeau et al. 2009] use inverse kinematics inspired by gradient
descent to implement the transition between different climber’s
stances where each climber stance includes four hold positions.
They use a reinforcement learning method to learn which sequence
of climber’s stances eventually lead the agent to the top of the wall.
They enable the humanoid agent to climb a wall by reusing what
has already been learnt from previous experience. However, using
inverse kinematics to find the transition between climber’s stances
can confine the diversity of behaviors that emerge from the climber,
and lead to unnatural looking motions on the planned path.
[Tonneau et al. 2014] use climbing as a test case for their method
for generating contacts and poses for exerting force in a userspecified direction. Motion is, however, synthesized using an IKbased approach without dynamics. [Olsen et al. 2014] build on this
in their impressive simulation of a humanoid with full finger articulation grasping and hanging from climbing holds. Our work
complements [Olsen et al. 2014] in that we assume a simpler hold
model and instead focus on the path planning and full-body motion
synthesis of complete climbing routes. In the future, we naturally
hope to integrate both approaches.
In particular, our work is inspired by the optimization-based
humanoid climbing movements of [Hämäläinen et al. 2015; Jain
et al. 2009; Mordatch et al. 2012], and the graph-based climbing
stance path search of [Dung and Shimada 2014]. The former have
shown that optimization of full-body humanoid movement control
with respect to a cost function can lead to the emergence of climbing
movements, although longer sequences of such movements were
not demonstrated. [Dung and Shimada 2014] on the other hand
build a graph of climbing robot stances, and find the best path on
the static graph using Dijkstra’s algorithm. Such a stance graph can
also be considered a generalization of footprint-based roadmaps
of [Choi et al. 2011, 2003]. Each node in the graph of [Dung and
Shimada 2014] contains four hold positions for robot’s arms and feet,
and graph building is done by considering all possible combinations
of four holds reachable by the robot’s arms and feet given different
reference points on the wall. The nodes that represent adjacent
robot stances, i.e. only one hold position differs, are connected by
an edge. The algorithm only focuses on the path planning on the
stance graph where the costs on the graph edges are fixed, and it is
assumed that the best path returned by Dijkstra’s algorithm can be
followed by the humanoid robot; here, moving only a single limb at
a time helps due to the three fixed limbs providing stable balance.
In this paper, we combine climbing movement optimization in
the style of [Hämäläinen et al. 2015; Jain et al. 2009; Mordatch et al.
2012] with the stance graph search of [Dung and Shimada 2014].
This allows our agent to plan and execute long sequences of rich,
dynamic climbing movements. Extending [Dung and Shimada 2014],
we allow the agent to move more than one limb at a time leading to
more dynamic movements, and explore multiple shortest paths in
order to allow comparing them, e.g. finding the one that required
least effort in the optimized climbing movements. We also allow
failed moves through the use of a dynamic graph edge costs; after
a failure the cost of an edge is increased to make the graph search
favor alternative paths.
Outside animation and motion synthesis, technological augmentations of climbing have been investigated, e.g., by instrumenting a
bouldering wall with sensors [Aladdin and Kry 2012] and enabling
interactive challenges and user-designed climbing problems with
computer vision and projected graphics [Kajastila et al. 2016].
3 PRELIMINARIES: K-SHORTEST PATHS IN A DYNAMIC
GRAPH
As explained above, we are finding multiple paths in the stance
graph, and the graph edge costs may change dynamically. Thus,
the problem falls in the category of "dynamic shortest paths" or
"shortest paths in a dynamic graph". However, instead of finding
shortest paths from one graph node to all other nodes, we want
k-shortest paths between two terminal nodes. A good review on
methods for the shortest paths problem can be found in [Eppstein
1998; Eppstein et al. 2016]. In this paper we utilize A* prune [Liu and
Ramakrishnan 2001], a general algorithm for returning k shortest
paths between two terminal graph nodes. The method is based on
the original A*; however, instead of keeping two lists, it only keeps
one list of all possible paths to a graph node which is sorted based
on heuristics costs (see Section 5.2).
[Nannicini and Liberti 2008; Roditty and Zwick 2004] provide
reviews for shortest paths methods on dynamic as opposed to static
graphs. There exists methods that replan a path after some changes
happen to the graph [Likhachev et al. 2005], however there is no
general way to return k-shortest paths between two terminal graph
nodes after a graph alteration. Our stance graphs are large, but their
connectivity is fortunately sparse enough to allow simply running
A* prune again after changing graph edge costs.
4 NOTATION AND PROBLEM DEFINITION
Environment: Fig 2 shows climbing humanoid agent in 3D environment where the global z® and x® axes are upwards and to the right,
respectively. The climbing wall and holds are usually in xz surface,
however in some cases the wall is tilted (see Section 6). We denote
positions with x ∈ R
3
.
We have a set of holds, denoted by H, containing N different hold
positions represented by xh for h = 1...N. The holds are simulated
as ball-and-socket joints. The limb-hold joints do not have motors;
the character is thus underactuated and requires balancing when
less than 3 holds are used. The hold denoted by xд is the goal hold to
be grasped by either of the climber’s hands, and it is usually located
on the top of the wall. The climber starts from the ground where
its hands and feet are detached from holds, and has to first get its
hands on user-defined starting holds and then reach xд. During
climbing, the agent is also allowed to use the wall in addition to the
holds (as usual in indoor climbing); however, we only plan the paths
ACM Transactions on Graphics, Vol. 36, No. 4, Article 43. Publication date: July 2017.
43:4 • Kourosh Naderi, Joose Rajamäki, and Perttu Hämäläinen
Fig. 2. Left: Humanoid Body Skeleton. Right: Angles of climber’s legs with
z® axis.
using the holds and possible wall usage emerges from movement
optimization if no target hold is specified for a limb.
Humanoid Climbing Agent: Let us denote the full climbing humanoid state with S. s ∈ S contains [xi
, qi
, vi
,ωi] for i = 1..m
wherem is the number of bones in the humanoid agent, and xi
, qi
, vi
and ωi are position, rotation, linear velocity and angular velocity of
each bone, respectively. Each bone of the humanoid agent is named,
and its end position is denoted by x{bone name}, e.g. xtrunk denotes
the trunk’s position. The positions of climber’s hands and feet are
noted by {lf,rf,lh,rh}, which stand for left and right feet, and left
and right hands, respectively. In addition, S contains information
on whether the climber’s hands and feet are connected to the holds
or not.
Climbing Stance and Connectivity: In addition to the full climber
state s, our method utilizes the concept of a stance, which is defined
as an assignment of holds to the climber’s hands and feet, without
considering body pose. To mitigate the problems of high state space
dimensionality, we first plan the path in the stance space, and utilize
the results to set goals for the state space planning.
We denote a climber’s stance with σ, and the high-level path
planner plans a sequence of stances from starting posture towards
the goal hold. σi = [xlf, xrf, xlh, xrh] denotes left foot, right foot, left
hand and right hand hold positions reached or to be reached, respectively. Each element of σi
, denoted by σi,j where j = {lf,rf,lh,rh},
can be either x−1 or xh ∈ H. We define x−1 as free position or not
attached notation since we want to allow the climber to have a free
hand or foot while climbing. The agent starts at s0 from free stance
σ0 = [x−1, x−1, x−1, x−1]. σi stance is a goal stance if σi,rh or σi,lh
equals xд, and thus if we have n different goals, we denote them
by σд1...σдn. We use the notation of σд in the path planned by the
high-level path planner, denoted by p(σ0, σд), as a goal stance that
is the first to be reached by the high-level path planner from σ0. The
path should start from σ0 and go to σд through a user-defined σstart.
The planned path of p(σ0, σд) is used to grow a tree denoted by
T in the high-dimensional climber’s state space with the help of
the low-level controller. Let us assume s is a state in T. We denote
the stance that the climber has reached at state s by σs. Then, if
σs equals σi
in the planned path, we want to simulate forward the
climber’s state from s towards σi+1 ∈ p(σ0, σд) with the help of the
low-level controller in order to follow the planned path. To reach
σi+1 from σs, the agent first has to let go of the holds that differ
from σi+1. Then, the low-level controller can try to get the climber’s
hands and feet to σi+1. The climber has reached σi+1 if the following
conditions are satisfied.
• For j = {lh,rh}: σs,j = σi+1,j
if ∥xj − σi+1,j ∥ < rh
• For j = {lf,rf}: σs,j = σi+1,j
if ∥xj − σi+1,j ∥ < rh and
θ(leg(j)) < θ0
where rh and θ0 are distance and angle thresholds for reaching conditions of hands and feet, respectively. The angle threshold is only
used in reaching conditions of the feet, and θ(leg(j)) is calculated
as ∠(®z, dir(leg(j))) and denotes the angle between the shin of leg j
with global z® (see Fig 2). If the conditions are not satisfied, the lowlevel controller has failed to get the climber’s hands and feet to σi+1
on the planned path, and the hands and feet that got disconnected
in σs remain disconnected, and thus we reach another stance on the
stance graph instead of the planned σi+1 (see Fig 3c).
5 MOTION PLANNING FOR A CLIMBING AGENT
On a high level, our approach entails building the stance graph,
followed by an iteration of 1) finding a yet non-simulated shortest
path in the graph based on edge cost heuristics, 2) optimizing and
simulating the physically based movements corresponding to path
edges, i.e., stance-to-stance climbing moves, and 3) updating graph
edge costs if step 2 fails. The last step causes the graph search to
return alternative paths, although if there are several failures, failed
moves will eventually be tried again.
The method is presented as pseudocode in Algorithm 1, and
illustrated in Fig 3. As shown in Fig 3d, the end result is a tree of
realized movements in the climber’s state space, where one or more
tree vertices can map to the same stance graph vertex, but tree edges
map uniquely to stance graph edges. In effect, the stances can be
considered points in a low-dimensional subspace of the full state.
We now first go through the algorithm, and then explain the
phases of graph building, path finding, and the low-level optimization of climbing moves in respective subsections.
We initialize the tree at s0 in line 3 and build the stance graph
of Gσ in line 4 (Fig 3a). Section 5.1 explains the details of this
process. The costs for stance vertices and transitions among them
are defined such that the most a-priori preferred path on the stance
graph has the lowest cost. To grow the tree, we use A* prune [Liu
and Ramakrishnan 2001] in line 7 to find yet not simulated paths
from the stance graph. We then use either C-PBP and CMA-ES,
two different physically based sampling/optimization methods to
optimize and simulate the moves corresponding to each not yet
simulated stance path edge. Details are explained in Section 5.3.
Fig 3b illustrates a case where the low-level controller successfully
grows the tree from its starting tree node on the planned path. Thus,
all end states send are added to the tree in line 19, and climber’s
stance at the end of the forward simulation or σend is at target
stance or σi+1 (Line 22). In contrast, Fig 3c shows a case of the lowlevel controller failing to grow the tree toward σi+1. In this case, the
node will be added to the tree in a different stance than σi+1, which
ACM Transactions on Graphics, Vol. 36, No. 4, Article 43. Publication date: July 2017.
Discovering and Synthesizing Humanoid Climbing Movements • 43:5
(a) Initialization of tree root of s0 and stance graph of Gσ . (b) First case: the low-level controller successfully grows the tree on the
sampled path of p(σ0, σд) from the stance graph.
(c) Second case: the low-level controller failed to grow the tree on the
second sampled path from s4 toward σ5 and instead it goes to σ3 at s6.
The transition cost for σ6-to-σ5 is increased, and now has a higher cost
than σ5-to-σ6.
(d) Third case: the low-level controller successfully grows the tree on
the third sampled path starting from s2. Although there are tree nodes
representing σ6 and σд1, new tree nodes are added in growing process.
Fig. 3. Various stages of Algorithm 1. The xz-plane denotes the stance dimensions, and vertical axis denotes the additional dimensions of climber body state
s. Our method grows a tree in the state space (in this abstract figure the 3D space) guided by the lower-dimensional stance graph (here the xz-plane). As
illustrated in subfigure (d), a single stance may map to multiple states, but tree edges uniquely map to stance graph edges. All paths start from σ0 and pass
through user-specified stance σstart. There are two goal stances in the graph denoted by σд1 and σд2. The starting tree node for growing the tree for each
planned path is denoted by green color. The planned path of p(σ0, σд) is denoted by green solid lines above the stance graph. Red dashed edges in the graph
denote increased edge cost after failure.
may have a limb free instead of assigned to the planned target hold
(Line 18). In this case, we increase the transition cost from σi to
σi+1 on the graph (Line 25), and stop growing tree (Line 26) since
we have deviated from the planned path. It should be noted that
the transition cost from σi+1 to σi remains same since a transition
from σi+1 to σi
is not simulated yet. See Section 5.2 on definition of
the costs for the graph. After failure or reaching the goal of current
path, another path is found using A* prune. A* prune will consider
the increased cost and already simulated paths, and return next best
not-simulated path. Fig 3d illustrates how simulating a new path
may result in multiple tree vertices mapping to the same stance
graph vertex.
If the low-level controller fails a move, further attempts may
benefit from trying the move from slightly different starting poses
(remember that many poses and character states may map to the
same stance). This is why we check for failures one node ahead
towards σi+2 on line 11 of Algorithm 1 and re-simulate not only the
failed edge, but also the preceding edge when all already existing
states have failed in reaching σi+2. Fig 4 shows an example of this,
where both edges σ0σstart and σ1σstart have initially failed, and one
of them has to be attempted again to reach the goal nodes. Before
making a new attempt at σ1σstart, we also resimulate σ0σ1 to get a
new starting state s2 instead of s1.
5.1 Building the Stance Graph
We build the stance graph in two steps, and we want to create all
possible stance nodes and edges that a set of holds H can admit,
while keeping the graph simple enough to parse. In step 1, we know
that the climber is at the free stance σ0 where its hands and feet
are detached from the holds, and we are allowed to go only to σstart
which is defined by the user. However, in step 2, we are in σstart and
we are allowed to create any possible stance from H. To do so, given
a stance node σn connected to graph, we find a set of candidate
neighbors σi around σn such that:
• In step 1, σi,j = {x−1, σstart,j
, σn,j } for j = {lf,rf,lh,rh}.
• In step 2, σi,j = {x−1, Hσ , σn,j } for j = {lf,rf,lh,rh} where
Hσ ⊂ H and contains all holds satisfying the half-circle
distance constraints illustrated in Fig 5.
We recursively add to the stance graph all the candidate neighbors
that satisfy the following additional pruning criteria:
(1) If σi
is created in step 1 and σn = σ0 then σi can connect
to graph if at least one of the hands is attached to the holds
(i.e., the climber must start by first grasping a hand on a
start hold).
(2) If σi
is created in step 1 and only one hand is connected
in σn, then σi can connect to the graph when at least one
more hand or foot is attached to the holds (i.e, after the first
hand, the climber must place the second hand or a foot).
ACM Transactions on Graphics, Vol. 36, No. 4, Article 43. Publication date: July 2017.
43:6 • Kourosh Naderi, Joose Rajamäki, and Perttu Hämäläinen
(3) At most two elements in σn and σi can be different (i.e.,
move at most two limbs at the same time).
(4) If first or second conditions are not met, then σi can connect
to σn if in order to reach σi from σn, we always have one of
the hands connected and at least two parts of the climber are
connected to holds (i.e., we do not consider rare movement
that have both hands free or only use a single point of
support).
(5) Foot holds cannot be higher than 0.1cm above the highest
hand hold σi
. While "heel hooks" and "toe hooks" at close to
or equal to hand height are fairly common, it’s rare for the
feet to be much higher than hands. This heuristic avoids
generating unnecessary inverted poses.
(6) If all hands and feet are connected in σi
, the number of
unique holds in σi should be at least three (i.e., only 1 or 2
support points result in unstable balance and is very rare).
(7) Distances of hand to hand, foot to foot and hand to foot in
σi should not exceed the maximum distances allowed in
climber’s body.
Fig. 4. A rare case where the low-level controller initially fails in simulating
both edges σ0σstart and σ1σstart. In this case, edge σ0σ1 gets resimulated,
and simulation of σ1σstart starts from s2 instead of s1.
Fig. 5. Determining candidate neighbor stances based on half-circle distance
constraints at each hold of the current stance (green color), assuming that
at least one hand hold and one foot hold will not change between neighbor
stances. The union of the shaded half-circles denotes the holds among which
all possible stances can be neighbors of the current stance if they satisfy
further pruning criteria. The radius of the half-circles corresponds to the
maximum distance between two end-effectors of the climber model.
5.2 A* Prune
A* prune finds paths as an ordered list of least to highest cost,
with the cost of the path computed as the sum of graph edge and
node costs. As we do the path finding prior to optimizing the actual
movements, we cannot base the edge costs on the torques and forces
exerted by the climber. Instead, we formulate the costs as
cost(σi
, σj) = cdyn(σi
, σj) + ce(σi
, σj) + cn(σj), (1)
where cdyn(σi
, σj) denotes a dynamic cost for stance graph edge
σiσj
. The cost is initially set to zero and increased by parametercfail
when the low-level controller fails to reach σj
(line 25 of Algorithm
1). As the climber can try a move in both directions, cdyn(σi
, σj)
may differ from cdyn(σj
, σi).
The ce(σi
, σj) and cn(σj) in Equation 1 are static edge and node
costs that represent the difficulty and effort of moving from σi to σj
.
The costs are computed based on various heuristics when the stance
graph is built. The following describes the heuristics and related
tuning parameters; parameter values used in our simulations are
given in Section 6. The heuristics are denoted with h(), and tuning
parameters are denoted with c for cost values, and d for distances.
The static edge cost ce(σi
, σj) is composed of three heuristics as
ce(σi
, σj) = hdist(σi
, σj) + h2limbs(σi
, σj) + hunstable(σi
, σj). (2)
The heuristic hdist(σi
, σj) makes the planning avoid redundant
movement; it equals the sum of distances of holds between the
stances. If a limb is free (not attached to a hold) in the source stance,
we cannot know the corresponding distance, and instead use a
tuning parameter dattach. If a limb gets or remains unattached in the
target stance, we use a parameter dfree.
The heuristic h2limbs(σi
, σj) denotes the difficulty of moving more
than one limb at a time. We use a tuning parameter h2limbs(σi
, σj) =
c2limbs if the climber is moving more than one limb between the
stances, and the movement is not considered simple "ladder climbing", which we define as 1) only the opposite hand and leg are
moving, 2) moving limbs get attached in the target stance, and 3)
the distance between the moving limbs is greater than parameter
dtoo-close. Otherwise, h2limbs(σi
, σj) = 0.
The heuristic hunstable(σi
, σj) penalizes cases where the number
of unique holds connected to hands and feet is less than or equal
to two during the move, and the distance between the holds is less
than parameter dunstable. In such moves, balance is often unstable,
and we use cost parameter hunstable(σi
, σj) = cunstable. Otherwise,
hunstable(σi
, σj) = 0.
The static node costcn(σj) of Equation 1 comprises the following
heuristics:
cn(σj) = hfree(σj) + hmatch(σj) + hcrossing(σj) + htoo-close(σj). (3)
The heuristic hfree(σj) is nonzero if the target stance σj has a
free hand or foot. A free foot is more preferable, as climbers often
use free legs for balancing (e.g., so called "flagging"). We thus use
hfree(σj) = cfree-foot for feet and hfree(σj) = cfree-hand for hands.
The heuristic hmatch(σj) = cmatch if two feet are attached to the
same hold, which can be cumbersome in real climbing. Otherwise,
hmatch(σj) = 0. In climbing lingo, "match" means using a single hold
for two limbs.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 43. Publication date: July 2017.
Discovering and Synthesizing Humanoid Climbing Movements • 43:7
Fig. 6. Shortest path trees created by Dijkstra Algorithm for calculating
h(σn).
The heuristic hcrossing(σj) = ccrossing if hands or feet are crossed
along the x axis, or either of the hands is lower than any of the feet.
Otherwise, hcrossing(σj) = 0.
Heuristic htoo-close(σj) = ctoo−close
Í
k max(0, (1 −
dk
dt oo−c l os e
)),
where the summation is over all hand-foot pairs for the limbs that
are attached to holds in stance σj
, and dk
is the distance between
the hand and feet hold in pair k. In other words, we prefer to keep
hands and feet further apart than dtoo−close for stable balance.
Running A* prune: As suggested by the original A* prune paper
[Liu and Ramakrishnan 2001], we run Dijkstra’s algorithm on the
stance graph to compute the sum of the preference heuristics from
any graph node to the closest goal stance. Since there are multiple
goal nodes, we start Dijkstra backwards starting from all goal nodes
on the graph toward the initial σ0. This produces a number of shortest path trees, as illustrated in Fig 6. Then, as long as there is no
failure in following the planned path by the low-level controller,
i.e., no increase in edge costs, we can return multiple paths sorted
by cost using A* prune without restarting it. When a change in
the stance graph happens, we restart A* prune algorithm which
means we re-run Dijkstra algorithm to re-calculate the heuristics,
and clear the list of all possible paths, but the list of already found
paths remains intact.
5.3 Low-Level Climber Controller
To grow the tree, i.e., synthesize movements from a starting state
to a target stance, we use a sampling-based movement optimization/control approach. The supplemental video shows results generated with both Covariance Matrix Adaptation Evolution Strategy
(CMA-ES) [Hansen 2006], a standard technique in the graphics literature, as well as the more recent Control Particle Belief Propagation
(C-PBP) method [Hämäläinen et al. 2015]. We use Open Dynamics
Engine as our simulator with a time step of 1/30 seconds (30 FPS
animation). Both control methods are suitable for model-predictive
control using a black-box dynamics simulator. Both methods can be
used for offline optimization, although C-PBP was originally demonstrated in receding-horizon online control except for an offline toy
example.
In both methods, the optimization process comprises an iteration
of 1) sampling control parameters, 2) simulating the corresponding
climber state trajectories, 3) evaluating a cost function based on the
controls and states, and 4) refining the control sampling distribution
based on the costs. As a probabilistic method, C-PBP internally treats
the costs as negative logarithms of probability densities which leads
to a Gaussian mixture formulation for the sampling distribution. In
CMA-ES, the sampling distribution is a single Gaussian.
For each move, we optimize joint motor target velocities over
a planning horizon of 1.5 seconds (C-PBP offline) or 0.5-2.0 seconds (CMA-ES). CMA-ES samples two "keyframes", each of which
specifies a duration in the range 0.25...0.75 and a target angular
velocity for each joint. The velocities are linearly interpolated (i.e.,
constant acceleration) to the targets over the durations starting
from the angular velocities of the initial simulation state. C-PBP by
default samples velocities for each simulation step, but we use the
same sampled values for 4 successive simulation steps, i.e., there’s
a controller step only for every 4 simulation steps. This reduces
high-frequency noise while still allowing for enough temporal detail.
In all our experiments, we use 64 samples per iteration, except for
256 for the first CMA-ES iteration.
For all methods, we use the following cost function, summed over
the simulated movement trajectory:
costobj =
1
k
2
σ
Õ
j
ϕ(s, j)∥xj − σi+1,j ∥
2
+
1
k
2
com
∥bxcom − xcom∥
2
+
1
k
2
posture
Õ
i
∥θi − θbi ∥
2
+
1
k
2
dir
∥dtorso − dwall ∥
2
+
1
k
2
velocity
Õ
b
∥vb
∥
2
+
1
k
2
force
(
Õ
i
∥τi ∥
2 +
Õ
j
∥Fj ∥
2
)
(4)
where the k:s are tuning parameters corresponding to the following
cost components:
(1) Reaching the target stance σi
(parameter kσ ): This objective
helps the low-level controller to get climber’s hands and
feet from sinit to desired stance σi+1. The summation in
this component is over j = {lf,rf,lh,rh}. ϕ(s, j) is 1 for limbs
with defined targets, and zero otherwise.
(2) Keeping center of mass close to the wall (parameter kcom):
In general, good climbers tend to maintain postures where
they stay close to the wall. bxcom is the closest point of the
wall at the same height as xcom, i.e., the cost function component corresponds to squared horizontal distance from
the wall.
(3) Keeping a preferred posture (parameter kposture): This cost
makes the climber prefer postures close to the default posture in Fig 7 (right) with value of kposture = kposture-climb,
except for the times where the character is standing on the
ground, where we use the T-pose as the default with the
value of kposture = kposture-free. This helps producing more
natural movement and preventing the climber ending up in
a state from which it has difficulties to continue. We denote
joint angles and their default pose values with θi and θbi
,
respectively. The summation is over all joints. Note that
ACM Transactions on Graphics, Vol. 36, No. 4, Article 43. Publication date: July 2017.
43:8 • Kourosh Naderi, Joose Rajamäki, and Perttu Hämäläinen
when using C-PBP, this cost component is implemented implicitly as a Gaussian component in the sampling proposal,
similar to the original paper [Hämäläinen et al. 2015].
(4) Facing towards the wall (parameter kdir): This component
makes the climber prefer facing the wall, although temporary deviations are allowed. We denote the climber’s
direction with dtorso and our desired direction with dwall.
(5) Prefer calm movements (parameter kvelocity). This component is not used with CMA-ES, but it helps reducing sampling noise (jittery movement) with C-PBP. We denote each
body velocity with vb where b is the body name. The summation in this component is over all bodies of the humanoid
agent.
(6) Minimize applied torques and finger strain (kforce param.):
This component, i.e., the control cost, is the sum of the
squares of applied joint motor torques and the forces exerted by hands on hand holds. We denote the torque around
joint angle i by τi
, and force on hand j by Fj
. Note that although we optimize motor velocities instead of raw torques
for better simulation stability, Open Dynamics Engine lets
one query the amount of torque the motor needed for reaching the velocity, and the motors also have a maximum possible torque parameter for realism.
Termination Conditions: To simulate forward climber’s state from
sinit toward σi+1 in Algorithm 1, we first detach climber’s hands and
feet from the holds that differ between the start and end stances, and
then run C-PBP or CMA-ES until one of the following termination
conditions is satisfied:
• Maximum iterations reached.
• When costobj stops reducing.
6 SIMULATIONS
Fig. 7. Left: Climber in T-Pose with measures in meters. Right: Climber’s
preferred posture.
We have simulated our method on five different scenarios, including three different bouldering routes, a 45 degree tilted wall, and
a 10 meter high wall with 48 holds (see supplemental video). Our
climber weights 70kg and the measures of its body are shown in Fig
7. We let the method find k = 10 different routes from s0 where the
climber is in T-pose on the ground, toward the goal hold xд. When
k paths are found, we choose two paths for demonstration on the
supplemental video, 1) the one with least amount of limbs moving
between holds, and 2) the one with the lowest sum of control costs.
On the long wall we only run the method for the first path.
We use the following parameter values: rh and θ0 in Section 4 are
set to 0.125 meters and 120◦
, respectively. cfail in Algorithm 1 and
other edge and node cost parameters of Section 5.2 are shown in
Table 1. The low-level movement optimization cost parameters of
Section 5.3 are given in Table 2.
Table 1. Parameter values for stance graph edge and node costs
Parameter Value Parameter Value
cfail 105
ccrossing 100
cfree-foot 20 ctoo-close 103
cfree-hand 250 c2limbs 120
cmatch 100 cunstable 500
dtoo-close 0.5m dattach 0.5m
dfree 2m dunstable 0.95m
Table 2. Parameter values for low-level controller cost function
Parameter C-PBP CMA-ES
kdir 0.5 0.5
kσ 0.01(m) 0.25 ∗ 10−2
(m)
kvelocity 0.5(m/s
2
) 100(m/s
2
)
kposture-free 0.087rad 0.087rad
kposture-climb 0.785rad 0.785rad
kcom 20(m) 20(m)
kforce 250(N) 250(N)
6.1 Evaluation
As illustrated by Fig 1 and the supplemental video, our system is
able to find plausible solutions to all the simulated problems. On the
bouldering routes, finding the first path takes around one minute
of CPU time on Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40 GHz. The
sampling-based movement optimization may sometimes fail, but the
method is then able to find alternative paths. The cost of building
the stance graph is low compared to the planning; graph building
only takes a few seconds on the bouldering routes. The high wall
naturally takes longer to process, but our system was nevertheless
able to find a path in 13 and 3.2 minutes CPU time with CMA-ES
and C-PBP as the low-level controller, respectively. Building the
stance graph for the high wall takes around one minute.
CMA-ES and C-PBP have both their advantages and disadvantages. When we have shown the results to experienced climbers,
they comment that CMA-ES produces more determined and skilled
movement, while C-PBP looks more like a hobbyist who goes to the
bouldering gym once a week. We agree, and attribute this difference
ACM Transactions on Graphics, Vol. 36, No. 4, Article 43. Publication date: July 2017.
Discovering and Synthesizing Humanoid Climbing Movements • 43:9
mostly to the number of controlled degrees of freedom. CMA-ES
only optimizes a piecewise linear velocity curve with two control
points, which completely removes the movement noise generated by
C-PBP. On the other hand, some less advanced climbers have commented that C-PBP results are more natural, as human movement
usually does have imperfections. C-PBP noise can also be reduced
with increased sampling budget. When we tried to achieve more
natural movement by increasing the number of CMA-ES control
points (thus increasing the number of optimized parameters), we
could not get the optimization to converge anymore. C-PBP due to
its dynamic programming nature is at least in theory less prone to
the curse of dimensionality with regards to the number of planning
steps.
Regarding computing cost, CMA-ES needs more iterations with
similar sampling budget due to oscillation around the final cost
minimum found. This is illustrated in the supplemental video; while
optimizing each move, we visualize both the starting pose and the
end pose of the best sample of the current optimization iteration.
Due to the slow convergence, CMA-ES typically needs about twice
the CPU time to find a path. On the other hand, CMA-ES with
our settings is more reliable in terms of avoiding greedy behaviors
leading to failure, illustrated in Fig 8. We ran 20 simulations on
following the first route planned by A* prune in the bouldering
route of Fig 8 for both optimization methods with 4 failure cases
for CMA-ES and 11 failure cases for C-PBP (offline). The greediness
of C-PBP can be adjusted with the resampling threshold parameter,
but it appears difficult to tune. We have run all our tests with the
default 0.5; convergence degraded rapidly when we tried adjusting
the parameter.
Fig. 8. Low-level controller success (left) and failure (right) cases. In the
failure cases, the left foot reaches for a hold too greedily, trying to go behind
the climber’s back. We observed such failures more often with C-PBP than
CMA-ES.
The maximum torque allowed for the joint motors can be easily
adjusted to simulate stronger or weaker climbers. Weakening the
agent increases the average time needed to find a route, as many of
the tried paths fail; qualitatively, the successful climbs exhibit more
calm and conservative movements. We demonstrate a comparison
between weak and strong climber’s movements on the supplemental
video at 1:35.
Fig. 9. Two examples of scalability test cases using grids of holds with some
random deviation added to the hold positions.
6.2 Scalability
In order to evaluate the scalability of our method, we created different sizes of hold grids, as illustrated in Fig 9. The start and goal
holds were placed in opposite corners, requiring the character to
traverse the wall both horizontally and vertically. The holds were
also randomly displaced from the grid to make the test more realistic.
We tested configurations with 1...4 columns and 2...8 rows of holds
(climbing just one row does not make sense).
Fig. 10. Average time needed to plan and simulate 10 paths as a function of
climbing wall size, using the hold grids of Fig 9.
Fig 10 shows the CPU time of planning and simulating 10 paths
with different wall sizes, averaged over 6 simulations, each with
different random hold displacements. Time was measured on an
Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40 GHz. Fig 11 shows the
corresponding stance graph sizes. The data indicates that with such
hold grids, both graph size and the planning and simulation time
grow roughly linearly as a function of wall height. The linear slope
of graph size is steeper with more hold columns.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 43. Publication date: July 2017.
43:10 • Kourosh Naderi, Joose Rajamäki, and Perttu Hämäläinen
Fig. 11. Average number of nodes in the stance graph as a function of
climbing wall size, using the hold grids of Fig 9.
7 APPLICATIONS
We consider sports coaching and computer-aided climbing route
design/verification (in both virtual and real worlds) as possible applications for our system. Climbing is a rapidly growing sport that
has recently been approved for the 2020 Olympics. Indoor climbing
environments are also highly suitable for technological augmentation, e.g. [Aladdin and Kry 2012; Kajastila et al. 2016]; we are
presently preparing a user study that measures the effectiveness of
our system in helping novice climbers in Augmented Reality climbing. We also believe our approach can be generalized to climbing
more freeform shapes without holds, provided that key features of
the environment such as graspable edges are discretized as sets of
’virtual holds’ when building the stance graph.
8 LIMITATIONS AND FUTURE WORK
Although this paper does advance the state of the art in climbing
movement planning and synthesis, we must acknowledge some
limitations that we intend to address in future work. First, our
heuristics for both graph connectivity (Section 5.1) and edge costs
(Section 5.2) could probably be simplified. On the other hand, there’s
only so much one can do with hand-tuned heuristics; having shown
that the stance graph approach is feasible, we intend to learn the
connectivity and costs from data in the future. Our present system
can act as a powerful exploration and data generation tool, training
a machine learning predictor of stance-to-stance climbing move
difficulty, required torques, and flexibility. Such a predictor could
then be used both in the graph building and path finding, increasing
the probability of finding and simulating relevant and interesting
paths. In a similar manner, we should be able to train faster low-level
controllers based on the outputs of CMA-ES or C-PBP, or at least
learn good initial guesses for transitioning from a given state and
stance to a target stance.
A second limitation of our system is the simulation of the holds
as ball-and-socket joints. This amounts to having all the simulated
holds as what climbers call "jugs", i.e., large holds with a cavity to
place one’s fingers or toes in. In reality, a hold’s shape determines
how much one can pull the hold in all directions without slipping,
and learning to predict this and plan climbing strategy accordingly
is a big part of the sport. In the future, we intend to simulate this
by defining a directionally varying maximum forces that the hold
joints tolerate.
A further limitation is that our low-level controller only considers one stance graph edge at a time. This means that even if our
character can transition between stances, the resulting pose and
movement state may not be optimal for continuing towards the next
stance of the path. We in effect assume that if a different intermediate pose is required, the low-level controller can generate it as
part of the next synthesized movement trajectory. Although our
current handling of low-level controller failures and multiple poses
per stance (Figs 3 and 4) appears sufficient in our test cases, we will
attempt to optimize movements more than one edge at a time in
future work.
9 CONCLUSION
We have proposed and demonstrated a novel approach to planning
and synthesizing physically-based humanoid wall climbing movements. We combine a high-level graph-based path planner with
a low-level controller/optimizer to synthesize plausible climbing
motions. Our high-level path planner utilizes the concept of stances
or assignments of holds for the climber’s hands and feet, and builds
a graph in the low-dimensional stance space. Paths are then found
and simulated using the low-level controller, eventually growing
a tree in the climber’s high-dimensional state space, which gives
multiple plausible solutions for navigating from a start stance to
a goal stance with physically based simulated movements. As a
climbing-specific adaptation of this general approach, we have formulated heuristics for both restricting the graph’s connectivity, and
guiding the pathfinding. We don’t claim that the heuristics are optimal; what is noteworthy is that they allowed us to demonstrate the
feasibility of the graph search approach. In future work we intend
to learn predictors of graph connectivity and edge costs from data
generated by our system, running it on a large enough training set
of randomly generated routes.
We have demonstrated results using two different methods, CPBP and CMA-ES, for the low-level movement synthesis. Both methods were able to generate plausible movements, although performance and movement quality varies. A natural extension that we
currently pursue is using the present optimization for training faster
and better controllers.
