Quadruped motion includes a wide variation of gaits such as walk, pace,
trot and canter, and actions such as jumping, sitting, turning and idling.
Applying existing data-driven character control frameworks to such data
requires a significant amount of data preprocessing such as motion labeling
and alignment. In this paper, we propose a novel neural network architecture
called Mode-Adaptive Neural Networks for controlling quadruped characters. The system is composed of the motion prediction network and the
gating network. At each frame, the motion prediction network computes the
character state in the current frame given the state in the previous frame and
the user-provided control signals. The gating network dynamically updates
the weights of the motion prediction network by selecting and blending
what we call the expert weights, each of which specializes in a particular
movement. Due to the increased flexibility, the system can learn consistent
expert weights across a wide range of non-periodic/periodic actions, from
unstructured motion capture data, in an end-to-end fashion. In addition, the
users are released from performing complex labeling of phases in different
gaits. We show that this architecture is suitable for encoding the multimodality of quadruped locomotion and synthesizing responsive motion in
real-time.
CCS Concepts: • Computing methodologies → Motion capture; Neural
networks;
Additional Key Words and Phrases: neural networks, locomotion, human
motion, character animation, character control, deep learning
1 INTRODUCTION
Quadruped animation is one of the unsolved key problems in computer animation. It has particular relevance for applications in computer games and films, and also presents a challenging topic in
robotics. When animating quadrupeds, animators must go through
special training to design a wide range of complex movements. This
complexity is inherently from the multi-modality of quadruped
motions. For example, there are a number of locomotion modes 1
including walk, pace, trot, canter, and gallop, where the movements
and the phases of the torso and limbs vary in a complex manner
(see Fig. 2).
To the best of our knowledge, there is no prior work on systematically constructing quadruped motion controllers in a data-driven
fashion. This difficulty stems from the complexity mentioned above,
along with the issue that quadruped animals cannot be directed like
humans for a controlled data acquisition. As a result, the captured
data is often less structured with a wide range of random actions
performed one after another. When designing character controllers
using such data, engineers need to manually/semi-automatically extract gait cycles and transitions from the data, stitch them together,
and tune parameters of motion trees and finite state machines.
1Locomotion modes is the term to represent the variation in movements the animals
use to propel, here which is defined based on the binary footfall patterns along the
timeline that the end effectors contact the ground. “Locomotion modes” and “gait types”
are interchangably used in this article.
ACM Trans. Graph., Vol. 37, No. 4, Article 145. Publication date: August 2018.
145:2 • Zhang, Starke et al.
Motion control using neural networks has recently demonstrated
success in producing high-quality animation for biped locomotion
with clear cycles [Holden et al. 2017]. However, merely applying
the same framework to quadrupeds fails because defining single
phase for all four legs is not possible in the transition between gaits
with very distinct footfall patterns. This also makes manual phase
labeling of unstructured quadruped motion data with complex gait
transitions impractical.
In this paper, we propose a novel network architecture called
Mode-Adaptive Neural Networks (MANN) that can learn a locomotion
controller from a large amount of unstructured quadruped motion
capture data. The system is composed of the motion prediction network and the gating network. At each frame, the motion prediction
network computes the character state in the current frame given the
state in the previous frame and the user-provided control signals.
The gating network dynamically updates the weights of the motion
prediction network by selecting and blending what we call the expert weights, each of which specializes in a particular movement.
This architecture provides flexibility such that the system can learn
Fig. 2. Footfall patterns for different quadruped locomotion modes generated by referring to [Huang et al. 2013]. The horizontal bars indicate the
stance phase in each leg. The image in the right indicates the sequence of
the foot contact during the gait cycle.
consistent features across a wide range of non-periodic actions and
periodic unlabeled gait types. This framework can release the developers from the tedious and difficult process of phase labeling,
where the unstructured quadruped motion capture data of different
gait types must be aligned along the timeline. In particular, our
model does not require individual labels for different gaits which
are often difficult to distinguish even for humans, and thus avoids
gait mislabeling during data preprocessing.
The contributions of the paper can be summarized as follows:
• The first systematic approach for constructing data-driven
quadruped character controllers that can synthesize animations in production-quality with a wide variety of locomotion
modes and transitions between them.
• A novel end-to-end neural network architecture that can learn
from unstructured quadruped motion capture data without
providing labels of the phase and locomotion gaits.
• A comprehensive evaluation of the proposed architecture
through comparison with existing approaches.
2 RELATED WORK
Quadruped motion synthesis has mostly been done through procedural modeling and physics-based control. Procedural animation is
useful for animating virtual creatures with many legs [Hecker et al.
2008] or hexapoda [Karim et al. 2013]. Kry et al. [2009] animate
quadrupeds using modal analysis. Although these methods can produce interesting stable gait cycles, they have difficulty in producing
subtle, realistic movements when responding to quick user inputs,
where physical rules such as the conservation of momentum and
ground reaction forces strongly influence the motions. Simulating
such movements requires either using physically-based animation
or data-driven techniques.
In the rest of this section, we first review physics-based quadruped
control, and then data-driven techniques which can potentially be
applied to quadruped motion synthesis. Finally, we briefly review
mixture of experts, which is a concept that inspires our approach.
2.1 Physics-Based Quadruped Controllers
Physically-based controllers are effective to generate dynamic movements, where the animals make use of elasticity, energy minimization and conservation of momentum. Such methods can be divided into trajectory-based approaches where the motion is optimized based on physical properties such as torques, momentum
and feasibility [Levine and Popović 2012; Wampler and Popović
2009; Wampler et al. 2014], and torque-based approaches where
the body is directly driven by torques [Coros et al. 2011; Liu and
Hodgins 2017; Peng et al. 2015, 2016; Raibert and Hodgins 1991;
van de Panne 1996].
Trajectory-Based Approaches: Many trajectory optimization techniques are developed for human motion synthesis [Al Borno et al.
2013; Liu et al. 2005; Liu and Popović’ 2002; Ye and Liu 2012] and
some of these ideas are applied for quadruped motion synthesis.
Wampler and Popovic [2009] compute the motion of various types
of animals including quadrupeds by optimizing an objective function composed of the torques and constraint terms over cyclic gaits.
Wampler et al. [2014] extend this technique to predict the motion
ACM Trans. Graph., Vol. 37, No. 4, Article 145. Publication date: August 2018.
Mode-Adaptive Neural Networks for Quadruped Motion Control • 145:3
of animals whose appearance is known but the motion is unknown.
These are spacetime approaches which cannot be applied for online
applications. Levine and Popovic [2012] adapt the dog locomotion to
dynamically deforming terrains in a quasi-physical manner. Such an
approach is useful for adapting a small number of exemplar motions
to different environments, but still need a set of motion to start from,
which leads to data-driven approaches that are discussed later.
Torque-Based Approaches: Torque-based controllers have long
been developed for human characters [Liu and Hodgins 2017; Peng
et al. 2016; Yin et al. 2007] and quadruped characters[Coros et al.
2011; Peng et al. 2015; Raibert and Hodgins 1991; van de Panne
1996]. Designing such controllers is not trivial as the characters
should perform the desired motion while keeping balance. Raibert
and Hodgins [1991] develop control strategies for trotting, bouncing
and galloping gaits. Van de Panne [1996] optimizes the parameters
of a control graph to control a cat model using speed as the primary metric. Coros et al. [2011] design a detailed controller for
quadrupeds that can simulate a wide range of locomotion including
walk, trot, canter and traverse gallop by optimizing PD controllers
to follow reference movements from videos while satisfying constraints. Peng et al. [2015] apply reinforcement learning to control
quadruped models to adapt to different terrains in 2D. Designing a
stable controller in a high dimensional state space is a very difficult
problem and thus low-dimensional features need to be carefully
hand-tuned to keep the controller stable. Peng et al. [2016] overcome this problem by applying deep reinforcement learning, where
the feature space is automatically computed from the data. This
approach was further enhanced for 3D environments and applied
to biped characters in [Peng et al. 2016].
Physically-based controllers are very powerful tools for designing
dynamic plausible movements though some subtle minor voluntary
movements that make the motion realistic tend to be skipped due to
the difficulty in describing them from simple rewards such as moving forward, energy minimization and balance control. Adversarial
training as done in [Merel et al. 2017a] could be a future direction
to overcome this issue.
2.2 Data-Driven Character Control
A counterpart of physically-based animation is data-driven character animation techniques that make use of motion capture data
for interactive character control. Data structures such as motion
graphs [Arikan and Forsyth 2002; Kovar et al. 2002; Lee et al. 2002]
are introduced to synthesize continuous character movements from
unstructured motion capture data. As the connectivity of the graph
can significantly affect the responsiveness of the controlled character, computer games and other interactive applications often use a
more straightforward structure such as finite state machines where
the connectivity is explicit and the subsequent motion is predictable,
as proposed by Lau and Kuffner [2005].
In the rest of this section, we first review methods that make use
of early machine learning approaches for motion synthesis, time
series models which model the motion from the dynamic perspective
and then the recent methods that make use of deep learning-based
approaches.
Motion Synthesis by Classic Machine Learning Techniques: To improve the transitions between the actions and increase the generality
of the output motion, the system should preferably be synthesizing
novel motions rather than simply replaying the given data. Various
techniques based on machine learning such as K-Nearest Neighbours (KNN), principal component analysis [Chai and Hodgins 2005;
Min and Chai 2012; Safonova et al. 2004; Tautges et al. 2011], radial
basis functions (RBF) [Kovar and Gleicher 2004; Rose et al. 1998],
reinforcement learning [Safonova and Hodgins 2007] and Gaussian
processes (GP) [Grochow et al. 2004; Ikemoto et al. 2009; Mukai and
Kuriyama 2005] are studies to achieve this goal.
Most methods based on classic machine learning techniques suffer from scalability issues: they first require a huge amount of data
preprocessing including motion classification and alignment. KNN
requires keeping all the motion data, and kernel-based approaches
like RBF and GP require square order memory and cube order
computation time. PCA-based approaches perform well in highdimensional problems such as when motion trajectories are used
as the representation, but requires local structures for time-series
models [Chai and Hodgins 2005; Tautges et al. 2011], where again a
lot of preprocessing is needed.
Time Series Models: Time series models predict the motion for the
current frame given the motion in the previous frames. Such models
are useful for real-time applications such as computer games. Similar
to motion interpolation, methods based on linear regression [Hsu
et al. 2005; Xia et al. 2015], KNN [Lee et al. 2010] and kernel-based
approaches [Wang et al. 2008] are proposed. Linear models [Hsu
et al. 2005] are simple but have issues with modeling nonlinearity.
Xia et al. [2015] cope with this problem by using expert gates, though
their method requires predefining the classes of each experts, which
we overcome in our research. Kernel-based approaches [Levine et al.
2012; Wang et al. 2008] can model nonlinearity in motion, but the
trained model is limited to express a specific type of motion. Motion
fields [Lee et al. 2010] require preserving the original data and
conducting KNN search during runtime, which limits the scalability.
Learning Character Motion by Neural Networks: Neural networks
are getting attention due to their scalability and high run-time
performance. Neural networks can learn from a massive amount
of data, while keeping their sizes much smaller than the original
data sizes. Taylor et al. [2009; 2011] present human motion capture
data can be learned by conditional Restricted Boltzmann Machines
(cRBM), although it suffers from the noise due to per-frame sampling.
Recurrent Neural Networks (RNN) that predict the following posture
from the previous motion are attractive frameworks for learning
time series data such as human motion, but are known to suffer
from converging to an average pose [Fragkiadaki et al. 2015]. Li
et al. [2017] avoid the problem by linking the network predictions
into future inputs, which drags the generated motion of the RNN
back to the original motion data. The focus of these works are in
the reconstruction of the time series data, and they do not receive
extra control signals for the control. How they interpolate between
different movements that follow the user instructions is yet to be
examined.
For the character animation purpose, Holden et al. [2015] use convolutional neural network (CNN) and learn temporal filters as the
ACM Trans. Graph., Vol. 37, No. 4, Article 145. Publication date: August 2018.
145:4 • Zhang, Starke et al.
Fig. 3. The architecture of our neural network composed of the gating network and the motion prediction network. The gating network takes as input the
current end effector velocities of the feet, the desired velocity and the action vector. The motion prediction network takes as input the posture and trajectory
control variables from the previous frame, and predicts the updated posture and trajectory for the current frame.
representation of motions. They also demonstrate CNN’s capabilities
to map high-level user instructions to character movements [Holden
et al. 2016]. Holden et al. [2017] propose a real-time framework, the
phase-functioned neural network (PFNN), that maps the inputs from
a gamepad controller to the motion of the character. Naive application of the same technique to quadrupeds exposes issues caused by
the complexity of the quadruped motion and the difficulty in gait
type and phase labeling. Most importantly, they introduce the phase
as a parameter to align the locomotion along the timeline, such that
they do not blend motion at the wrong timing. This is only possible
if the way the feet contact the ground is consistent. For quadrupeds,
this pattern can drastically change between modes, and thus can
result in artifacts. We present such examples in Section 8.
2.3 Mixture of Experts
Mixture of Experts (MoE) [Jacobs et al. 1991; Jordan and Jacobs
1994] is a classic machine learning approach where a number of
experts are used to cope with the inputs in different regions. A
gating network decides which set of experts to use for the given
input. Once trained, the experts become locally specialized for some
subdomain assigned by the gating network. We refer the readers to
Yukel et al. [2012] for a broad survey. Recently its combination with
deep learning architectures [Chang et al. 2018; Eigen et al. 2013;
Shazeer et al. 2017] is showing promising results. Our architecture
has similarity with MoE, as we also use a gating function, though our
blending is occurring at the feature level, while for MoE it happens
at the output level.
3 SYSTEM OVERVIEW
Our system is a time series model that predicts y, the state of the
character in the current frame, given x, the state in the previous
frame and the user control signals. For achieving this task for characters such as quadrupeds who produce a wide range of periodic
and non-periodic movements, we propose a novel neural network
structure called Mode-Adaptive Neural Networks (MANN) (see Section 6, Fig. 3). The motion in the current frame is computed by a
network that we call the motion prediction network (see Section 6.1,
Fig. 3, right), whose network weights are dynamically computed by
another neural network structure that we call the gating network
(see Section 6.2, Fig. 3, left). The gating network receives a motion
feature xˆ, which is a subset of x, and computes the blending coefficients of the expert weights (see Fig. 3, center), each of which is
trained to be specialized in particular movements.
To prepare the training dataset, we first preprocess the dog motion
capture dataset and add the action labels to the data (see Section 4),
and prepare the input and output vectors (see Section 5). During
training, the entire network is trained end-to-end using the prepared
training dataset (see Section 7). During runtime, the system animates
the character in real-time using the previous state of the character
and the control signals provided by the user (see Section 8).
4 DATA PREPARATION
Here we describe the motion capture and motion classification
stages, and present the breakdown of our data.
Dog Motion Capture. Our motion capture data consists of 30 minutes of unstructured dog motion capture data that is composed of
various locomotion modes including walk, pace, trot, and canter, as
well as other types of motions such as sitting, standing, idling, lying
and jumping. The size of the data is doubled by mirroring. All the
motion data was only captured on a flat terrain due to the limitation
of the capture facilities. A skeleton model that is composed of 27
Fig. 4. The skeleton structure of the dog model used in our experiments. It
is composed of 27 bones and has 81 degrees of freedom in total.
ACM Trans. Graph., Vol. 37, No. 4, Article 145. Publication date: August 2018.
Mode-Adaptive Neural Networks for Quadruped Motion Control • 145:5
Table 1. The breakdown of our dog motion dataset for training. This dataset
includes the original and mirrored unstructured dog motion capture.
Motion Type time (sec) frames ratio (%)
idle 1614.37 96862 36.42
locomotion 1828.70 109722 41.25
jump 30.27 1816 0.68
sit 497.93 29876 11.23
lie 397.13 23828 8.96
stand 64.83 3890 1.46
bones (see Fig. 4) is fitted to the captured data. The body has 81
degrees of freedom in total.
Motion Classification. The motions are first classified into locomotion, sitting, standing, idling, lying and jumping such that the
user can control the character during runtime by specifying such
labels. This process is done manually, though, it should not be difficult to automate this process, as the motions are rather distinct
between these classes. The breakdown of the ratio of the motion
data is shown in Table 1.
Locomotion Modes. In our paper, we specifically work on four
types of locomotion modes: walk, pace, trot, and canter. Although
our system does not require the labels of the locomotion modes for
controlling the character during runtime, we analyzed the distribution of the modes in the dataset. Classification of the data into
locomotion modes is a rather difficult process due to the complex
transitions and the ambiguous cases; we first roughly classify the
motions based on the velocity profile and then manually divide
them into each mode. The final breakdown/correlation of the speed
and the mode types are visualized in Fig. 5. This correlation matches
well to the model by Coros et al. [2011].
5 INPUT AND OUTPUT FORMATS
The input and output data formats of our system follow those of
Motion Matching [Clavet 2016] and PFNN [Holden et al. 2017].
The data is composed of the ground trajectory transformations and
velocities in the past and future states, the labels of the action type
at those frames, and the body joint transformations and velocities of
Fig. 5. Distribution of velocities for different quadruped locomotion modes.
the current state. More specifically, for each frame i, we uniformly
sample another 11 frames which cover one second in the future and
past. From these 12 frames of motion, we extract features including
the character positions, rotations and velocities relative to the body’s
local transformation, which is projected onto the ground with its
rotation facing in the character’s forward direction. We call this
the root transformation of our system, which also represents the
trajectory transformation for the current frame. We then include the
action labels at those 12 frames to be part of the state vector. Finally,
all trajectory and body joint transformations are computed relative
to the root trajectory transformation. The desired velocity of each
frame is simply calculated by the length of the future trajectory at
this particular state.
The input vector at frame i is xi = { t
p
i
t
d
i
t
v
i
t
vˆ
i
t
a
i
j
p
i−1
j
r
i−1
j
v
i−1
} ∈
R
n
, where t
p
i
∈ R
2t
are user-predicted trajectory positions of t
samples (in our case 12) in a 2D-horizontal plane relative to the
current state i, t
d
i
∈ R
2t
are trajectory forward facing directions
relative to state i, t
v
i
∈ R
2t
are the trajectory velocities relative
to state i, t
vˆ
i
∈ R
1t
are the desired trajectory velocity values in
state i, and t
a
i
∈ R
6t
are one-hot vectors for the character action
type of the samples along the trajectory. j
p
i−1
∈ R
3j
, j
r
i−1
∈ R
6j
and
j
v
i−1
∈ R
3j
are the relative joint positions, rotations and velocities
of the previous state where j denotes the number of joints (in our
case 27). In addition to the PFNN [Holden et al. 2017], also using
the joint rotations for the input of the network instead of only for
the output layer could be observed to immediately produce a much
sharper motion for our quadruped motion dataset.
Similarly, the output is yi = { t
p
i+1
t
d
i+1
t
v
i+1
j
p
i
j
r
i
j
v
i
˙r
x
i
˙r
z
i
˙r
a
i
} ∈
R
m where t
p
i+1
∈ R
2t
, t
d
i+1
∈ R
2t
and t
v
i+1
∈ R
2t
are the predicted
relative trajectory positions, directions and velocities for the next
state, and j
p
i
∈ R
3j
, j
r
i
∈ R
6j
and j
v
i
∈ R
3j
are the relative joint
positions, rotations and velocities for the current state. ˙r
x
i
∈ R
and ˙r
z
i
∈ R are the root translational x and z velocities relative to
the previous frame which define the local transformation into the
current frame, and ˙r
a
i
∈ R is the corresponding root angular velocity
in the 2D-horizontal plane. This is similar to the data preparation of
PFNN, though our system does not output any information about
the foot contact or the phase increment, but additionally includes the
trajectory velocities to get a better controllability. Also, we represent
the joint rotations by the relative forward and upward vectors in
order to avoid quaternion interpolation issues by the neural network
during training. Those are obtained from the original quaternions
in the motion capture, and transformed back for generating the
motion during runtime.
6 MODE-ADAPTIVE NEURAL NETWORKS FOR
QUADRUPED CONTROL
In this section, we describe the architecture of the Mode-Adaptive
Neural Networks (MANN) in Fig. 3, which is composed of the motion prediction network and the gating network. We first describe
the motion prediction network, which predicts the motion in the current frame given the state in the previous frame, and then describe
the gating network, which computes the weights of the motion
prediction network dynamically.
ACM Trans. Graph., Vol. 37, No. 4, Article 145. Publication date: August 2018.
145:6 • Zhang, Starke et al.
6.1 Motion Prediction Network
The motion prediction network, whose operation is denoted by Θ(·),
is a simple three layer neural network that receives the previous
state of the character x, and outputs the data in the format of y as
follows:
Θ(x;α) = W2 ELU( W1 ELU( W0 x + b0) + b1) + b2, (1)
where the parameters of the network α are defined by α = {W0 ∈
R
h×n
, W1 ∈ R
h×h
, W2 ∈ R
m×h
, b0 ∈ R
h
, b1 ∈ R
h
, b2 ∈ R
m }. Here
h is the number of units used in the hidden layers, which in our work
is set to 512, and the activation function used is the exponential
rectified linear function [Clevert et al. 2015] defined by
ELU(x) = max(x, 0) + exp(min(x, 0)) − 1. (2)
The neural network weights α is computed by blending K expert weights β = {α1, ...,αK }, each of which in the form of neural
network weight configurations: α =
PK
i=1
ωiαi
. K is a meta parameter that can be adjusted according to the complexity and size of
the training data, and ω = {ω1, ...,ωK } are the blending coefficients
computed by gating network described in Section 6.2. We found that
K = 4 is enough to generate high-quality motions, though K = 8
is able to produce even better results with sharper movements. We
will discuss about this in Section 8.
6.2 Gating Network:
The gating network, whose operation is denoted by Ω(·), is a three
layer neural network that computes the blending coefficients ω
given the input data x:
Ω(xˆ; µ) = σ (W′
2
ELU( W′
1
ELU( W′
0
xˆ + b
′
0
) + b
′
1
) + b
′
2
), (3)
where xˆ ∈ R
19 is a subset of x that are the foot end effector velocities,
the current action variables and the desired velocity of the character. The parameters of the network µ are defined by µ = {W′
0
∈
R
h
′×19
, W′
1
∈ R
h
′×h
′
, W′
2
∈ R
K×h
′
, b
′
0
∈ R
h
′
, b
′
1
∈ R
h
′
, b
′
2
∈ R
K }
where h
′
is the number of hidden layer units which is set to 32. σ (·)
is a softmax operator that normalizes the inputs such that they sum
up to 1, which is required for the further linear blending.
We have tested various other inputs including the full x, the end
effector positions, and the combination of their position and velocity,
as well as the rotations, and found those features alone to clearly
produce the best results. This could be due to the strong correlation of the feet velocity with the locomotion phase, resulting in an
effect similar to the phase function in [Holden et al. 2017], which
helps to avoid blending between motions of different phases. Ideally,
the network could learn the informative features from the input,
though this could be difficult due to our relatively small amount of
training data as listed in our experiments in Section 8. Meanwhile,
we observed that the using the action variables and the desired
velocity helps to improve the controllability and responsiveness of
the character.
7 TRAINING
The entire network is trained end-to-end using the processed motion
capture data. The input x and y for each frames are stacked into
matrix form: X = [x1x2...], Y = [y1y2...]. These values are shifted
and scaled using their mean and standard deviation such that the
Fig. 6. Learning curves of vanilla neural network, PFNN and MANN with
4 or 8 expert weights respectively. The warm restarts of the AdamWR
algorithm at epoch 11, 31 and 71 result in hikes of the loss.
data is normalized. As the cycles of trot and canter are very limited,
we copied the data of those motions 11 times each, which helps
them to appear during runtime more robustly.
The goal of training our network is that for a given set of inputs
X, we can produce the corresponding output variables Y, which is
a typical regression task with the following cost function, which
is the mean squared error between the predicted output and the
ground truth:
Cost(X, Y; β, µ) = ∥Y − Θ(X, Ω(Xˆ ; µ); β)∥
2
2
, (4)
We use the stochastic gradient descent algorithm with the warm
restart technique of AdamWR [Loshchilov and Hutter 2017], which
automatically calculates the derivatives of the cost function with
respect to β and µ. The model is implemented in Tensorflow [Abadi
et al. 2016]. As AdamWR does the regularization within the optimization procedure, no regularization term is contained in the cost
function. Instead, it uses two parameters Ti and Tmul t that control
the decrease and restart of the learning rate η and weight decay rate
λ, which we chose to be initialized as η = 1.0·10−4
and λ = 2.5·10−3
.
Ti
is the total number of epochs within the i-th run/restart with an
initial value of 10. At every restart, we choose Ti to be multiplied
by the factor of Tmul t = 2, i.e. Ti+1 = 2Ti
. We set the total epochs
as 150, hence a total of 3 restarts will occur at epoch 11, 31 and
71. During the training, we iterate over the data in mini-batches
with a size of 32, where the training samples in each mini-batch are
selected randomly. Dropout is applied with a retention probability
of 0.7. Full training with 4 or 8 experts networks takes around 20
or 30 hours on a NVIDIA GeForce GTX 970 GPU, respectively. In
Fig. 6, we show the learning curves of different methods. Note that
a lower training loss or test loss does not necessarily correspond to
a higher quality of motion.
8 EXPERIMENTS AND RESULTS
In this section, we first describe the character control scheme during
runtime, and then show the results when controlling the character
interactively during runtime.
ACM Trans. Graph., Vol. 37, No. 4, Article 145. Publication date: August 2018.
Mode-Adaptive Neural Networks for Quadruped Motion Control • 145:7
Runtime Control: The system is implemented in the Unity 3D
engine, and requires ∼2 ms per frame for running the neural network
single-threaded on an Intel Core i-7 CPU using a port of the Eigen
library. The memory usage for the trained model is ∼22 MB using
8 expert weights. The character can be controlled by discrete and
continuous control signals. By selecting the key that corresponds
to the element of the one hot vector, the desired action, which is
either of sit, stand, idle, lie, jump and move, is launched. The speed
of the character, which is also included in the control parameters, is
discretely specified by the arrow keys, increasing and interpolating
the velocity in the forward, backward, left and right directions. The
facing direction of the locomotion is controlled by two keys in
order to perform a turning motion. Both the target velocity and
direction are smoothly interpolated and finally used to predict the
future trajectory which the user wants to navigate. The curve of
the trajectory is extrapolated using an exponentially-weighted bias
value which defines the maximum length of the future trajectory —
and thus providing the desired velocity of the character in m/s. This
always results in a smooth trajectory, which is crucial for the input
of the neural network to achieve good looking motion by trying to
resemble realistic trajectories as in the original motion capture data.
Runtime Results: Various gaits including walk, pace and canter are
successfully produced by setting the speed to 0.5m/s (walk), 1.1m/s
(pace), 1.9m/s (trot) and 3.3m/s (canter), respectively. Smooth transitions can be produced by gradually changing the speed. All types
of locomotion quickly respond to the direction and velocity input,
although some gaits do not include the turning motion. This means
the network is well generalizing the turning motion of the standard
gait like walk to other gaits. Motions such as sit, lie, stand and jump
are successfully launched with little delay when the corresponding hotkey is pressed (see Fig. 1). This indicates that the network
Fig. 7. The quadruped character walking over a smooth uneven terrain
synthesized by full-body inverse kinematics.
can learn at which state particular actions may be performed. The
readers are referred to the supplementary video for further details.
Finally, we prepare an environment where the character can
move along an uneven terrain (see Fig. 7). Instead of training the
character to walk over different terrains as done in [Holden et al.
2017], we simply apply a CCD full-body inverse kinematics to the
character movements. The positions of the paw end effectors are
post-processed such that they incorporate the terrain height offset in addition to the generated motion, and the spine joints are
simultaneously updated with respect to the surrounding heights.
This is required to avoid outstretched limbs when walking up- and
downhills, and thus to create more natural movements.
9 EVALUATION
We evaluate our system through comparison to existing methods
in terms of the motion quality, foot sliding artifacts, leg stiffness,
and responsiveness. We also examine the activation of the expert
weights, and analyze their functionality by deactivating them during
the motion.
Comparison with Other Frameworks: We compare the performance
of the proposed system with baselines such as vanilla feedforward
neural networks and phase-functioned neural networks (PFNN)
with semi-automatic phase labeling. Note that all the rest of the
inputs and outputs are kept the same as our method, and no foot
contact information are included in the state vector of each method.
For a fair comparison, we make the number of layers and parameters
in the vanilla neural network and the PFNN the same as for the
MANN using 4 expert weights. Thus, the vanilla network has 2048
units in the hidden layers, and the PFNN has 4 control points, each
of which has 512 units in the hidden layers (4 × 512). Meanwhile,
we also show the results of MANN with 8 expert weights, and list
the ground truth (GT) values from the original motion capture data.
Vanilla feedforward consistently suffers from undesired blending
which results in blurry movements and loss of high-frequency components. This artifact is apparent even for a constant walk and pace
cycles, for which the data is relatively rich. The artifacts such as foot
skating and stiff legs become significantly worse when conducting
turning behaviors and when landing after jumps.
In comparison to PFNN, our system performs significantly better,
despite the fact that ours does not require any phase labels. For
testing the PFNN, the phase labels are added to the dog motion data.
The phases are defined based on the velocity profile of the front two
legs, computed and aligned by an optimization technique 2
. The gait
type is specified through one-hot vectors. Such a PFNN performs
well in general, though the motion of the back legs often appear
stiff and unnatural, and also resulting in foot skating. This can be
due to the difficulty of defining a consistent phase function that can
well align different gait types; as we define the phase based on the
velocity of the front legs, the artifacts become more observable at
the back legs.
2The cost function is defined by fitting the parameters of a trigonometric function
such that it matches the velocity update of the feet. This takes into account symmetry
information in the motion, and adaptively detects different types of locomotion. The
beginning and end of each cycle is then analytically extracted by computing the either
negative or positive turning points. This enables us to automatically label the phase of
the whole motion dataset with high accuracy.
ACM Trans. Graph., Vol. 37, No. 4, Article 145. Publication date: August 2018.
145:8 • Zhang, Starke et al.
Fig. 8. The character following predefined trajectories on the ground. The parameter τ for correction with the predicted trajectories of the network is set to 0.5
in order to perform realistic motions along artificial paths. It can be observed that the character is following the curves well even when there are sudden turns.
Table 2. The average foot skating for all legs and the back legs in the ground
truth data, and when using the vanilla NN, PFNN and MANN models with
4 or 8 expert weights respectively.
Motion GT vNN PFNN MANN4 MANN8 cm/frame
Walk 0.21 0.32 0.25 0.23 0.22 all legs
0.08 0.15 0.12 0.10 0.09 back legs
Canter 0.14 0.24 0.19 0.17 0.16 all legs
0.05 0.19 0.11 0.08 0.06 back legs
Turn 0.17 0.28 0.34 0.24 0.20 all legs
0.06 0.17 0.21 0.13 0.08 back legs
In biped locomotion, it is relatively easy to align the motion based
on the foot contact pattern for different modes like walking and
running. However, such a simple rule is difficult to be adopted for
quadrupeds, where the relative phases of the swing and support legs
change significantly across the locomotion modes. On the contrary,
the gating network has more degrees of freedom for blending the
features to minimize the cost function, which offers more flexibility
to align different modes of locomotion.
Foot Skating Artifacts: We also compare our system with the
vanilla neural networks and PFNN in terms of foot skating artifacts, listed in Table 2. The foot velocities are added if a position
height h is within a maximum threshold of H = 2.5cm. Each velocity magnitude v in the horizontal plane is further weighted by an
exponential interpolation s = v(2 − 2
h
H ) to estimate the amount of
skating s during motion, where the exponent is clamped between 0
and 1.
Leg Stiffness: In PFNN, the foot skating is caused by the back leg
stiffness, which happens due to the blending of back leg movements
Table 3. The average angular update per joint along all legs and the back
legs in the ground truth data, and when using the vanilla NN, PFNN and
MANN models with 4 or 8 expert weights respectively.
Motion GT vNN PFNN MANN4 MANN8 ◦
/frame
Walk 3.87 3.02 3.36 3.43 3.69 all legs
3.16 2.47 2.71 2.82 3.05 back legs
Canter 6.53 4.61 4.63 5.23 5.72 all legs
5.56 3.73 3.52 4.56 5.14 back legs
Turn 2.74 1.37 1.84 2.24 2.31 all legs
2.15 1.02 1.06 1.54 1.82 back legs
Table 4. Average values for position and angle deviation while aiming to
smoothly follow predefined trajectories of different curves, when using
vanilla NN, PFNN and MANN models.
Path vNN PFNN MANN
Circle 2.28 1.96 2.98 Position Deviation (cm)
3.07 1.70 3.21 Angle Deviation (°)
Square 5.80 3.72 4.65 Position Deviation (cm)
5.20 3.78 7.21 Angle Deviation (°)
Star 7.45 8.37 6.61 Position Deviation (cm)
8.80 11.94 8.76 Angle Deviation (°)
Custom 8.14 9.50 5.31 Position Deviation (cm)
7.69 9.28 6.75 Angle Deviation (°)
under a phase computed by the front leg movements. To quantitatively evaluate such stiffness, we compute the average update per
joint angle along all legs and the back legs (see Table 3). It can be
observed that the leg motion is much smaller in vanilla NN and
PFNN compared to MANN, especially for the back legs.
Responsiveness: We finally evaluate the responsiveness and the
path-following accuracy of our method. We create several predefined paths and instruct the character to follow them in different
modes. In order to make the character follow the trajectory, we use a
blending parameter τ which interpolates between the future points
of the desired trajectory T
∗
as well as the corrected trajectory T
+ in
the output of our network to obtain the input trajectory T for the
motion update. This blending can be defined as T = τT
∗ + (1−τ )T
+.
The trajectories made by the character are shown in Fig. 8. We then
measure the difference between the desired trajectory and the actual
trajectory of the character considering their root transformations
(see Table 4). It can be observed that the character is well following
the paths and the average distance is low.
What are the Networks Learning?: To examine the situation, we
plot the profile of the blending coefficients ω with K = 4 and 8
expert weights when the character is performing different types of
actions and locomotion modes in Fig. 9. It can be observed that the
weight values are equally cycling around for low speed movements
(see Fig. 9, second bottom, bottom, upper rows), making the gating
network function in the same fashion as the phase function in PFNN.
The period that the expert weights corresponding to the purple line
is active, becomes longer and constant as the speed of the locomotion
increases, while the others tend to activate less in such situations.
This means that these expert weights are trained to be responsible
for fast movements, jumpy movements while the others could be
ACM Trans. Graph., Vol. 37, No. 4, Article 145. Publication date: August 2018.
Mode-Adaptive Neural Networks for Quadruped Motion Control • 145:9
responsible for synthesizing rather slower movements. In particular,
by selectively disabling single dimensions of blending coefficients,
we observed the character no longer being able to perform certain
movements, such as single types of locomotion modes, turning or
jumping. Our observations for a learned network with eight trained
Table 5. Resulting motion artifacts and disablings when selectively deactivating a weight αi by ignoring the corresponding blending coefficient of the
gating network. Some weights have learned features which are specifically
responsible for certain motions.
αi Observation
1 jumping motion fails (drops down after initiate)
2 left turning not possible (right turning works normally)
3 jumping motion fails; less accurate movements in general
4 right turning not possible (left turning works normally)
5 canter and trot motion not possible
6 only tiny movements along the limbs
7 right turning fails; everything else works
8 hard to activate modes; rather stiff limbs
Fig. 9. The activation profile of the experts during different actions. The
plots for two cycles of motions are shown.
blending coefficients are listed in Table 5. While most of them are
clearly contributing to the overall quality of motion, a few became
entirely responsible for specific actions or modes. Specifically for
turning either left or right, it could be observed that turning into
the opposite direction remained largely or even entirely unaffected.
Expert Imbalance: One concern that is reported in previous similar architectures [Bengio et al. 2015; Eigen et al. 2013; Shazeer et al.
2017] is that a small set of experts tend to receive higher weights
by the gating network. This imbalance will increase as the favored
experts will be trained even more rapidly. In fact, we observe this
phenomenon when applying the original mixture of experts architecture [Jacobs et al. 1991] in our setup, where we prepare four
experts and blend the outputs using a gating network: only one
expert is trained well without exploiting others. Previous work in
this area copes with this issue by imposing hard constraints [Eigen
et al. 2013], or by imposing additional penalty in the loss that infavors such imbalance [Bengio et al. 2015; Eigen et al. 2013; Shazeer
et al. 2017]. It seems such a penalty is not needed for our system
with a small number of control points, as their usage becomes relatively balanced, and the system is performing well for animating
the character.
10 DISCUSSIONS
Below we discuss the advantages of our proposed model for learning
quadruped motion, followed by the limitations of our system.
Learning Time Series Models: Time series data are rather difficult
to learn, often converging to average poses by blending poses at
different phases in cyclic motions, or timing in acyclic motions.
Also, the output motions tend to be smoothed out due to such
averaging. Although a simple feedforward network can interpolate
well where the samples are rich, such as during the gait cycle, the
results appear smoothed out and unnatural with a lot of foot sliding
when the transitions happen at configurations with less training
data. PFNN is introduced to produce good interpolation even at
such configurations by aligning the motion data along the phase
dimension and only interpolating motion at the same phase. One
way to view MANN is that it is a generalization of PFNN. The
control points can be trained to be specialized for a motion at a
certain timing and its activation can be further adjusted by the gating
network in an unsupervised fashion. Once trained, bad interpolation
can be reduced as there is little blending between different expert
weights which are specialized for different phases.
Sparse Dataset: As mentioned above, deep neural networks can
tend to fail at domains where there is less training data. The domain
with sparse samples can be strongly affected by the surrounding
domain with dense samples. The dog motion capture dataset could
stay in such a distribution until the capture facilities become more
lightweighted. One hour of motion capture data can be considered
small for human data, but is relatively large for quadruped data.
Given such nature of the training dataset, the proposed architecture
is a good match to manage such locally sparse training data.
Limitations: Our dataset is limited to those from flat terrain and
thus movements such as jumping on to/off from high positions
ACM Trans. Graph., Vol. 37, No. 4, Article 145. Publication date: August 2018.
145:10 • Zhang, Starke et al.
cannot be synthesized. The inverse kinematics approach that we
presented is useful for simple motion synthesis, but cannot produce more dynamic movements such as jumping on/off from a box.
Thus we do not conduct any terrain fitting and adaption as done
in [Holden et al. 2017] although it can be easily done once the data is
obtained. One possibility to augment the data to synthesize results
is to apply physically-based animation to synthesize training data
where the characters make use of the preservation of momentum
and bouncing effects and augment the data.
11 FUTURE WORK
In addition to the combination of our method with physically-based
animation as mentioned above, there are several interesting directions for further research.
One direction can be motion retargeting to quadrupeds of different sizes and morphology. The motion capture of quadrupeds is not
straightforward and thus the amount of data is usually small and
the motion for the desired body size may not be always available.
Wampler et al. [2014] propose an optimization approach for synthesizing locomotion of quadrupeds with different sizes and skeletal
structure but the movements are limited to planer frontal movements, and retargeting each motion will be an offline process. It will
be interesting to see if neural domain transfer type of approaches
based on factorization [Bertinetto et al. 2016] or Resnets [Rebuffi
et al. 2017] can be applied such that a wide variation of gaits and
movements can be created from a rich set of locomotion of one
animal and a small set of exemplar motion of another animal.
Another interesting direction to look into is to compute the adversarial loss of the generated motion and use such loss for the
optimization to avoid the ambiguity problem. This has been explored in the physically-based environment [Merel et al. 2017b] and
preliminary results for motion capture data [Barsoum et al. 2017],
but a framework for interactive character control is yet to be done.
Automatically controlling non-player characters to interact with
the user controlled characters or with dynamic obstacles in a complex environment can be an interesting direction of research. One
possibility is to apply reinforcement learning for deciding the control signals of our framework.
MANN proved to be powerful for capturing the multi-modal
nature of quadruped motion. We would like to explore if the MANN
architecture is generally effective on other machine learning tasks
with highly multi-modal data.