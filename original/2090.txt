Camera sensors can only capture a limited range of luminance simultaneously, and in order to create high dynamic range (HDR) images a set of
different exposures are typically combined. In this paper we address the
problem of predicting information that have been lost in saturated image areas, in order to enable HDR reconstruction from a single exposure. We show
that this problem is well-suited for deep learning algorithms, and propose a
deep convolutional neural network (CNN) that is specifically designed taking
into account the challenges in predicting HDR values. To train the CNN
we gather a large dataset of HDR images, which we augment by simulating
sensor saturation for a range of cameras. To further boost robustness, we
pre-train the CNN on a simulated HDR dataset created from a subset of the
MIT Places database. We demonstrate that our approach can reconstruct
high-resolution visually convincing HDR results in a wide range of situations, and that it generalizes well to reconstruction of images captured with
arbitrary and low-end cameras that use unknown camera response functions and post-processing. Furthermore, we compare to existing methods for
HDR expansion, and show high quality results also for image based lighting.
Finally, we evaluate the results in a subjective experiment performed on an
HDR display. This shows that the reconstructed HDR images are visually
convincing, with large improvements as compared to existing methods.
CCS Concepts: • Computing methodologies → Image processing; Neural
networks;
Additional Key Words and Phrases: HDR reconstruction, inverse tone-mapping,
deep learning, convolutional network
1 INTRODUCTION
High dynamic range (HDR) images can significantly improve the
viewing experience – viewed on an HDR capable display or by means
of tone-mapping. With the graphics community as an early adopter,
HDR images are now routinely used in many applications including photo realistic image synthesis and a range of post-processing
operations; for an overview see [Banterle et al. 2011; Dufaux et al.
2016; Reinhard et al. 2010]. The ongoing rapid development of HDR
technologies and cameras has now made it possible to collect the
data required to explore recent advances in deep learning for HDR
imaging problems.
In this paper, we propose a novel method for reconstructing HDR
images from low dynamic range (LDR) input images, by estimating
missing information in bright image parts, such as highlights, lost
due to saturation of the camera sensor. We base our approach on a
ACM Transactions on Graphics, Vol. 36, No. 6, Article 178. Publication date: November 2017.
178:2 • G. Eilertsen, J. Kronander, G. Denes, R. K. Mantiuk and J. Unger
fully convolutional neural network (CNN) design in the form of a
hybrid dynamic range autoencoder. Similarly to deep autoencoder
architectures [Hinton and Salakhutdinov 2006; Vincent et al. 2008],
the LDR input image is transformed by an encoder network to
produce a compact feature representation of the spatial context
of the image. The encoded image is then fed to an HDR decoder
network, operating in the log domain, to reconstruct an HDR image.
Furthermore, the network is equipped with skip-connections that
transfer data between the LDR encoder and HDR decoder domains
in order to make optimal use of high resolution image details in
the reconstruction. For training, we first gather data from a large
set of existing HDR image sources in order to create a training
dataset. For each HDR image we then simulate a set of corresponding
LDR exposures using a virtual camera model. The network weights
are optimized over the dataset by minimizing a custom HDR loss
function. As the amount of available HDR content is still limited
we utilize transfer-learning, where the weights are pre-trained on
a large set of simulated HDR images, created from a subset of the
MIT Places database [Zhou et al. 2014].
Expansion of LDR images for HDR applications is commonly
referred to as inverse tone-mapping (iTM). Most existing inverse
tone-mapping operators (iTMOs) are not very successful in reconstruction of saturated pixels. This has been shown in a number of
studies [Akyüz et al. 2007; Masia et al. 2009], in which naïve methods or non-processed images were more preferred than the results
of those operators. The existing operators focus on boosting the
dynamic range to look plausible on an HDR display, or to produce
rough estimates needed for image based lighting (IBL). The proposed method demonstrates a step improvement in the quality of
reconstruction, in which the structures and shapes in the saturated
regions are recovered. It offers a range of new applications, such as
exposure correction, tone-mapping, or glare simulation.
The main contributions of the paper can be summarized as:
(1) A deep learning system that can reconstruct a high quality
HDR image from an arbitrary single exposed LDR image,
provided that saturated areas are reasonably small.
(2) A hybrid dynamic range autoencoder that is tailored to operate
on LDR input data and output HDR images. It utilizes HDR
specific transfer-learning, skip-connections, color space and
loss function.
(3) The quality of the HDR reconstructions is confirmed in a
subjective evaluation on an HDR display, where predicted
images are compared to HDR and LDR images as well as a
representative iTMO using a random selection of test images
in order to avoid bias in image selection.
(4) The HDR reconstruction CNN together with trained parameters are made available online, enabling prediction from any
LDR images: https://github.com/gabrieleilertsen/ hdrcnn.
2 RELATED WORK
2.1 HDR reconstruction
In order to capture the entire range of luminance in a scene it is
necessary to use some form of exposure multiplexing. While static
scenes commonly are captured using multiplexing exposures in
the time domain [Debevec and Malik 1997; Mann and Picard 1994;
Unger and Gustavson 2007], dynamic scenes can be challenging as
robust exposure alignment is needed. This can be solved by techniques such as multi-sensor imaging [Kronander et al. 2014; Tocci
et al. 2011] or by varying the per-pixel exposure [Nayar and Mitsunaga 2000] or gain [Hajisharif et al. 2015]. Furthermore, saturated
regions can be encoded in glare patterns [Rouf et al. 2011] or with
convolutional sparse coding [Serrano et al. 2016]. However, all these
approaches introduce other limitations such as bulky and custom
built systems, calibration problems, or decreased image resolution.
Here, we instead tackle the problem by reconstructing visually convincing HDR images from single images that have been captured
using standard cameras without any assumptions on the imaging
system or camera calibration.
2.2 Inverse tone-mapping
Inverse tone-mapping is a general term used to describe methods
that utilize LDR images for HDR image applications [Banterle et al.
2006]. The intent of different iTMOs may vary. If it is to display
standard images on HDR capable devices, maximizing the subjective
quality, there is some evidence that global pixel transformations
may be preferred [Masia et al. 2009]. Given widely different input
materials, such methods are less likely to introduce artifacts compared to more advanced strategies. The transformation could be
a linear scaling [Akyüz et al. 2007] or some non-linear function
[Masia et al. 2009, 2017]. These methods modify all pixels without
reconstructing any of the lost information.
A second category of iTMOs attempt to reconstruct saturated
regions to mimic a true HDR image. These are expected to generate
results that look more like a reference HDR, which was also indicated by the pair-wise comparison experiment on an HDR display
performed by Banterle et al. [2009]. Meylan et al. [2006] used a linear
transformation, but applied different scalings in highlight regions.
Banterle et al. [2006] first linearized the input image, followed by
boosting highlights using an expand map derived from the median
cut algorithm. The method was extended for video processing, and
with additional features such as automatic calibration and crossbilateral filtering of the expand map [Banterle et al. 2008]. Rempel
et al. [2007] also utilized an expand map, but computed this from
Gaussian filtering in order to achieve real-time performance. Wang
et al. [2007] applied inpainting techniques on the reflectance component of highlights. The method is limited to textured highlights, and
requires some manual interaction. Another semi-manual method
was proposed by Didyk et al. [2008], separating the image into diffuse, reflections and light sources. The reflections and light sources
were enhanced, while the diffuse component was left unmodified.
More recent methods includes the iTMO by Kovaleski and Oliviera
[2014], that focus on achieving good results over a wide range of
exposures, making use of a cross-bilateral expand map [Kovaleski
and Oliveira 2009].
For an in-depth overview of inverse tone-mapping we refer to the
survey by Banterle et al. [2009]. Compared to the existing iTMOs,
our approach achieves significantly better results by learning from
exploring a wide range of different HDR scenes. Furthermore, the
reconstruction is completely automatic with no user parameters
and runs within a second on modern hardware.
ACM Transactions on Graphics, Vol. 36, No. 6, Article 178. Publication date: November 2017.
HDR image reconstruction from a single exposure using deep CNNs • 178:3
2.3 Bit-depth extension
A standard 8-bit LDR image is affected not only by clipping but also
by quantization. If the contrast or exposure is significantly increased,
quantization can be revealed as banding artifacts. Existing methods
for decontouring, or bit-depth extension, include dithering methods
that use noise in order to hide the banding artifacts [Daly and Feng
2003]. Decontouring can also be performed using low-pass filtering
followed by quantization, in order to detect false contours [Daly and
Feng 2004]. There are also a number of edge-preserving filters used
for the same purpose. In this work we do not focus on decontouring,
which is mostly a problem in under-exposed images. Also, since
we treat the problem of predicting saturated image regions, the bit
depth will be increased with the reconstructed information.
2.4 Convolutional neural networks
CNNs have recently been applied to a large range of computer vision
tasks, significantly improving on the performance of classical supervised tasks such as image classification [Simonyan and Zisserman
2014], object detection [Ren et al. 2015] and semantic segmentation [Long et al. 2015], among others. Recently CNNs have also
shown great promise for image reconstruction problems related to
the challenges faced in inverse tone-mapping, such as compression
artifact reduction [Svoboda et al. 2016], super-resolution [Ledig
et al. 2016], and colorization [Iizuka et al. 2016]. Recent work on
inpainting [Pathak et al. 2016; Yang et al. 2016] have also utilized
variants of Generative Adversarial Networks (GANs) [Goodfellow
et al. 2014] to produce visually convincing results. However, as these
methods are based on adversarial training, results tend to be unpredictable and can vary widely from one training iteration to the next.
To stabilize training, several tricks are used in practice, including
restricting the output intensity, which is problematic for HDR generation. Furthermore, these methods are limited to a single image
resolution, with results only shown so far for very low resolutions.
Recently, deep learning has also been successfully applied for
improving classical HDR video reconstruction from multiple exposures captured over time [Kalantari and Ramamoorthi 2017]. In
terms of reconstructing HDR from one single exposed LDR image,
the recent work by Zhang and Lalonde [2017] is most similar to
ours. They use an autoencoder [Hinton and Salakhutdinov 2006]
in order to reconstruct HDR panoramas from single exposed LDR
counterparts. However, the objective of this work is specifically to
recoverer high intensities near the sun in order to use the prediction
for IBL. Also, the method is only trained using rendered panoramas
of outdoor environments where the sun is assumed to be in the
same azimuthal position in all images. Given these restrictions, and
that predictions are limited to 128 × 64 pixels, the results are only
applicable for IBL of outdoor scenes. Compared to this work, we
propose a solution to a very general problem specification without
any such assumptions, and where any types of saturated regions
are considered. We also introduce several key modifications to the
standard autoencoder design [Hinton and Salakhutdinov 2006], and
show that this significantly improves the performance.
Finally, it should be mentioned that the concurrent work by Endo
et al. [2017] also treats inverse tone-mapping using deep learning
algorithms, by using a different pipeline design. Given a single
(a) f
−1
⎭
⎥
⎥
(D
⎥
)
⎬
⎥
⎥
(b) exp
⎥
(y
⎥ˆ)
⎥
⎥
⎥
(c)
⎥
α ⎥
⎫
(d) Hˆ (e) H
Fig. 2. Zoom-in of an example of the components of the blending operation
in Equation 1, compared to the ground truth HDR image. (a) is the input
image, (b) is prediction, (c) is the blending mask, (d) is the blending of (a-b)
using (c), and (e) is ground truth. Gamma correction has been applied to
the images, for display purpose.
exposure input image, the method uses autoencoders in order to
predict a set of LDR images with both shorter and longer exposures.
These are subsequently combined using standard methods, in order
to reconstruct the final HDR image.
3 HDR RECONSTRUCTION MODEL
3.1 Problem formulation and constraints
Our objective is to predict values of saturated pixels given an LDR
image produced by any type of camera. In order to produce the final
HDR image, the predicted pixels are combined with the linearized
input image. The final HDR reconstructed pixel Hˆ
i,c with spatial
index i and color channelc is computed using a pixel-wise blending
with the blend value αi
,
Hˆ
i,c = (1 − αi
)f
−1
(Di,c ) + αi exp(yˆi,c ), (1)
where Di,c is the input LDR image pixel and yˆi,c is the CNN output (in the log domain). The inverse camera curve f
−1
is used to
transform the input to the linear domain. The blending is a linear
ramp starting from pixel values at a threshold τ , and ending at the
maximum pixel value,
αi =
max(0, maxc (Di,c ) − τ )
1 − τ
. (2)
In all examples we use τ = 0.95, where the input is defined to be in
the range [0, 1]. The linear blending prevents banding artifacts between predicted highlights and their surroundings, as compared to a
binary mask. It is also used to define the loss function in the training,
as described in Section 3.4. For an illustration of the components
of the blending, see Figure 2. Due to the blending predictions are
focused on reconstructing around the saturated areas, and artifacts
may appear in other image regions (Figure 2(b)).
The blending means that the input image is kept unmodified in
the non-saturated regions, and linearization has to be made from
either knowledge of the specific camera used or by assuming a
ACM Transactions on Graphics, Vol. 36, No. 6, Article 178. Publication date: November 2017.
178:4 • G. Eilertsen, J. Kronander, G. Denes, R. K. Mantiuk and J. Unger 320x320x64 320x320x3 160x160x128
80x80x256
40x40x512
10x10x512
20x20x512
320x320x3
320x320x6
320x320x64
320x320x128
160x160x128
160x160x256
80x80x256
80x80x512
40x40x512
40x40x1024
20x20x512 20x20x1024
3x3 conv.
3x3 conv.
3x3 conv.
3x3 conv. + pool
3x3 conv. + pool
3x3 conv.
3x3 conv.
3x3 conv.
3x3 conv. + pool
3x3 conv.
3x3 conv.
3x3 conv.
3x3 conv. + pool
1x1 conv.
1x1 conv.
1x1 conv.
1x1 conv.
1x1 conv.
1x1 conv.
1x1 conv.
10x10x512
3x3 conv. + pool
4x4 deconv.
4x4 deconv.
4x4 deconv.
4x4 deconv.
4x4 deconv.
HDR decoder LDR encoder
(VGG16 conv. layers)
Latent representation
3x3 conv.
Domain transformation
skip-connection
Skip-layer
Fig. 3. Fully convolutional deep hybrid dynamic range autoencoder network, used for HDR reconstruction. The encoder converts an LDR input to a latent
feature representation, and the decoder reconstructs this into an HDR image in the log domain. The skip-connections include a domain transformation from
LDR display values to logarithmic HDR, and the fusion of the skip-layers is initialized to perform an addition. The network is pre-trained on a subset of the
Places database, and deconvolutions are initialized to perform bilinear upsampling. While the specified spatial resolutions are given for a 320 × 320 pixels
input image, which is used in the training, the network is not restricted to a fixed image size.
certain camera curve f . We do not attempt to perform linearization
or color correction with the CNN. Furthermore, information lost
due to quantization is not recovered. We consider these problems
separate for the following reasons:
(1) Linearization: The most general approach would be to linearize either within the network or by learning the weights of
a parametric camera curve. We experimented with both these
approaches, but found them to be too problematic given any
input image. Many images contain too little information in
order to evaluate an accurate camera curve, resulting in high
variance in the estimation. On average a carefully chosen
assumed transformation performs better.
(2) Color correction: The same reasoning applies to color correction. Also, this would require all training data to be properly color graded, which is not the case. This means that given
a certain white balancing transformation of the input, the
saturated regions are predicted within this transformed color
space.
(3) Quantization recovery: Information lost due to quantization can potentially be reconstructed from a CNN. However,
this problem is more closely related to super-resolution and
compression artifact reduction, for which deep learning techniques have been successfully applied [Dong et al. 2015; Ledig
et al. 2016; Svoboda et al. 2016]. Furthermore, a number of
filtering techniques can reduce banding artifacts due to quantization [Bhagavathy et al. 2007; Daly and Feng 2004].
Although we only consider the problem of reconstructing saturated
image regions, we argue that this is the far most important part
when transforming LDR images to HDR, and that it can be used to
cover a wide range of situations. Typical camera sensors can capture
between 8 and 12 stops of dynamic range, which is often sufficient
to register all textured areas. However, many scenes contain a small
number of pixels that are very bright and thus saturated. These can
be reconstructed with the proposed method, instead of capturing
multiple exposures or using dedicated HDR cameras. Our method is
not intended to recover the lower end of the dynamic range, which
is below the noise floor of a sensor. Instead, the problem of underexposed areas is best addressed by increasing exposure time or gain
(ISO). This will result in more saturated pixels, which then can be
recovered using our approach.
3.2 Hybrid dynamic range autoencoder
Autoencoder architectures transform the input to a low-dimensional
latent representation, and a decoder is trained to reconstruct the
full-dimensional data [Hinton and Salakhutdinov 2006]. A denoising
autoencoder is trained with a corrupted input, with the objective of
reconstructing the original uncorrupted data [Vincent et al. 2008].
This is achieved by mapping to a higher level representation that is
invariant to the specific corruption. We use the same concept for
reconstruction of HDR images. In this case the corruption is clipped
highlights, and the encoder maps the LDR to a representation that
can be used by the decoder for HDR reconstruction. This means that
ACM Transactions on Graphics, Vol. 36, No. 6, Article 178. Publication date: November 2017.
HDR image reconstruction from a single exposure using deep CNNs • 178:5
the encoder and decoder work in different domains of pixel values,
and we design them to optimally account for this. Since our objective
is to reconstruct larger images than is practical to use in training,
the latent representation is not a fully connected layer, but a lowresolution multi-channel image. Such a fully convolutional network
(FCN) enables predictions at any resolution that is a multiple of the
autoencoder downscaling factor.
The complete autoencoder design is depicted in Figure 3. Convolutional layers followed by max-pooling encodes the input LDR in a
W
32 ×
H
32 × 512 latent image representation, where W and H are the
image width and height, respectively. The encoder layers correspond
to the well-known VGG16 network [Simonyan and Zisserman 2014],
but without the fully connected layers.
While the encoder operates directly on the LDR input image, the
decoder is responsible for producing HDR data. For this reason the
decoder operates in the log domain. This is accomplished using
a loss function that compares the output of the network to the
log of the ground truth HDR image, as explained in Section 3.4.
For the image upsampling, we use deconvolutional layers with a
spatial resolution of 4 × 4 initialized to perform bilinear upsampling
[Long et al. 2015]. While nearest neighbor up-sampling followed by
convolution has been shown to alleviate artifacts that are common in
decoder deconvolutions [Odena et al. 2016], we have not experienced
such problems, and instead use the more general deconvolutional
layers. All layers of the network use ReLU activation functions, and
after each layer of the decoder a batch normalization layer [Ioffe
and Szegedy 2015] is used.
3.3 Domain transformation and skip-connections
The encoding of the input image means that much of the high resolution information in earlier layers of the encoder are lost. The
information could potentially be used by the decoder to aid reconstruction of high frequency details in saturated regions. Thus, we
introduce skip-connections that transfer data between both high
and low level features in the encoder and decoder.
Skip-connections have been shown to be useful for constructing
deeper network architectures which improve performance in a variety of tasks [He et al. 2016]. For autoencoders, where layers have
different spatial resolution, a separate residual stream can be maintained in full resolution, with connections to each layer within the
autoencoder [Pohlen et al. 2017]. Alternatively, skip-connections
between layers of equal resolution in encoder and decoder have
also been shown to boost performance in a variety of imaging tasks
using autoencoders [Ronneberger et al. 2015; Zhang and Lalonde
2017].
Our autoencoder uses skip-connections to transfer each level of
the encoder to the corresponding level on the decoder side. Since the
encoder and decoder process different types of data (see Section 3.2),
the connections include a domain transformation described by an
inverse camera curve and a log transformation, mapping LDR display values to a logarithmic HDR representation. Since the camera
curve is unknown, we have to assume its shape. Although a sigmoid
function fits well with camera curves in general [Grossberg and
Nayar 2003], its inverse is not continuous overIR+. The linearization
of the skip-connections is therefore done using a gamma function
f
−1
(x) = x
γ
, where γ = 2.
(a) Input (b) Without skip (c) With skip (d) Ground truth
Fig. 4. Zoom-ins of reconstruction without (b) and with (c) the domain
transformation skip-connections. The plain autoencoder architecture can
reconstruct high luminance, but without skip-connections the detail information around saturated regions cannot be fully exploited.
A skip-connected layer is typically added to the output of the layer
at the connection point. However, to allow for additional freedom,
we concatenate the two layers along the feature dimension. That is,
given two W × H × K dimensional layers, the concatenated layer is
W ×H × 2K. The decoder then makes a linear combination of these,
that reduces the number of features back to K. This is equivalent
to using a convolutional layer with a filter size of 1 × 1, where the
number of input and output channels are 2K and K, respectively, as
depicted in Figure 3. More specifically, the complete LDR to HDR
skip connection is defined as
˜h
D
i = σ *
,
W






h
D
i
log 
f
−1

h
E
i

+ ϵ







+ b+
-
. (3)
The vectors h
E
i
and h
D
i
denote the slices across all the feature
channels k ∈ {1, ...,K} of the encoder and decoder layer tensors
y
E
,y
D ∈ IRW ×H×K , for one specific pixel i. Furthermore, ˜h
D
i
is
the decoder feature vector with information fused from the skipconnected vector h
E
i
. b is the bias of the feature fusion, and σ is the
activation function, in our case the rectified linear unit (ReLU). A
small constant ϵ is used in the domain transformation in order to
avoid zero values in the log transform. Given K features, h
E
and h
D
are 1 × K vectors, and W is a 2K × K weight matrix, which maps
the 2K concatenated features to K dimensions. This is initialized to
perform an addition of encoder and decoder features, setting the
weights as
W 0 =











1 0 . . . 0 1 0 . . . 0
0 1 . . . 0 0 1 . . . 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 1 0 0 . . . 1











, b0 = 0. (4)
During training, these weights can be optimized to improve the
performance of the skip-connection. Since the linear combination of
features is performed in the log domain, it corresponds to multiplications of linear HDR data. This is an important characteristic of the
domain transformation skip-connections as compared to existing
skip architectures.
ACM Transactions on Graphics, Vol. 36, No. 6, Article 178. Publication date: November 2017.
178:6 • G. Eilertsen, J. Kronander, G. Denes, R. K. Mantiuk and J. Unger
-6 stops -6 stops -6 stops -6 stops
(a) Input (b) λ = 0.9 (c) λ = 0.05 (d) Ground truth
Fig. 5. Zoom-ins of reconstructions with different relative weight of illuminance and reflectance, λ in Equation 7. A higher weight of illuminance will
in general better predict high intensity regions (b), while a higher weight of
reflectance is better at deducing local colors and details (c).
An example of the impact of the described skip-connection architecture is shown in Figure 4. The autoencoder design is able to
reconstruct HDR information from an encoded LDR image. However, all information needed by the decoder has to travel trough the
intermediate encoded representation. Adding the skip-connections
enables a more optimal use of existing details.
3.4 HDR loss function
A cost function formulated directly on linear HDR values will be
heavily influenced by high luminance values, leading to underestimation of important differences in the lower range of luminaces. The
few existing deep learning systems that predict HDR have treated
this problem by defining the objective function in terms of tonemapped luminance [Kalantari and Ramamoorthi 2017; Zhang and
Lalonde 2017]. In our system the HDR decoder is instead designed
to operate in the log domain. Thus, the loss is formulated directly
on logarithmic HDR values, given the predicted log HDR image yˆ
and the linear ground truth H,
L(yˆ,H) =
1
3N
X
i,c

αi

yˆi,c − log
Hi,c + ϵ



2
, (5)
where N is the number of pixels. SinceHi,c ∈ IR+, the small constant
ϵ removes the singularity at zero pixel values. The cost formulation
is perceptually motivated by the the close to logarithmic response
of the human visual system (HVS) in large areas of the luminance
range, according to the Weber-Fechner law [Fechner 1965]. The law
implies a logarithmic relationship between physical luminance and
the perceived brightness. Thus, a loss formulated in the log domain
makes perceived errors spread approximately uniformly across the
luminance range.
As described in Section 3.1, we use only the information from the
predicted HDR image yˆ around saturated areas. This is also reflected
by the loss function in Equation 5 where the blend map α from
Equation 2 is used to spatially weight the error.
Treating the illuminance and reflectance components separately
makes sense from a perceptual standpoint, as the visual system may
indirectly perform such separation when inferring reflectance or
(a) Input (b) Direct loss (eq. 5) (c) I/R loss (eq. 7) (d) Ground truth
Fig. 6. Zoom-in of a reconstruction with different loss functions. The input
(a) is exposure corrected and clipped to have a large amount of information lost. The direct pixel loss (b) is more prone to generating artifacts as
compared to the illuminance + reflectance loss (c).
discounting illumination [Gilchrist and Jacobsen 1984]. We therefore also propose another, more flexible loss function that treats
illuminance and reflectance separately. The illumination component
I describes the global variations, and is responsible for the high
dynamic range. The reflectance R stores information about details
and colors. This is of lower dynamic range and modulates the illuminance to create the final HDR image, Hi,c = IiRi,c . We approximate
the log illuminance by means of a Gaussian low-pass filter Gσ on
the log luminance L
yˆ
,
log 
I
yˆ
i

=

Gσ ∗ L
yˆ

i
,
log 
R
yˆ
i,c

= yˆi,c − log 
I
yˆ
i

.
(6)
Since the estimation is performed in the log domain, the log reflectance is the difference between yˆ and log illuminance. L
yˆ
is a linear combination of the color channels, L
yˆ
i
= log(
P
c wc exp(yˆi,c )),
where w = {0.213, 0.715, 0.072}. The standard deviation of the
Gaussian filter is set to σ = 2. The resulting loss function using I
and R is defined as
LI R (yˆ,H) =
λ
N
X
i




αi

log 
I
yˆ
i

− log 
I
y
i






2
+
1 − λ
3N
X
i,c




αi

log 
R
yˆ
i,c

− log 
R
y
i,c






2
,
(7)
where y = log(H + ϵ ) to simplify notation. The user-specified
parameter λ can be tuned for assigning different importance to the
illuminance and reflectance components. If not stated otherwise,
we use the illuminance + reflectance (I/R) loss with λ = 0.5 for all
results in this paper. This puts more importance on the illuminance
since the error in this component generally is larger. Figure 5 shows
examples of predictions where the optimization has been performed
with different values of λ. With more relative weight on illuminance,
high intensity areas are in general better predicted, which e.g. could
benefit IBL applications. If the reflectance is given more importance,
local colors and details can be recovered with higher robustness, for
better quality e.g. in post-processing applications.
The visual improvements from using the I/R loss compared to
the direct loss in Equation 5 are subtle. However, in general it tends
to produce less artifacts in large saturated areas, as exemplified in
Figure 6. One possible explanation is that the Gaussian low-pass
filter in the loss function could have a regularizing effect, since
it makes the loss in a pixel influenced by its neighborhood. This
ACM Transactions on Graphics, Vol. 36, No. 6, Article 178. Publication date: November 201