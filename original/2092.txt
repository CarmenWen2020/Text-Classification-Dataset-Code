It has been shown that rendering in the gradient domain, i.e., estimating finite
difference gradients of image intensity using correlated samples, and combining them with direct estimates of pixel intensities by solving a screened
Poisson problem, often offers fundamental benefits over merely sampling
pixel intensities. The reasons can be traced to the frequency content of the
light transport integrand and its interplay with the gradient operator. However, while they often yield state of the art performance among algorithms
that are based on Monte Carlo sampling alone, gradient-domain rendering
algorithms have, until now, not generally been competitive with techniques
that combine Monte Carlo sampling with post-hoc noise removal using
sophisticated non-linear filtering.
Drawing on the power of modern convolutional neural networks, we
propose a novel reconstruction method for gradient-domain rendering. Our
technique replaces the screened Poisson solver of previous gradient-domain
techniques with a novel dense variant of the U-Net autoencoder, additionally taking auxiliary feature buffers as inputs. We optimize our network to
minimize a perceptual image distance metric calibrated to the human visual
system. Our results significantly improve the quality obtained from gradientdomain path tracing, allowing it to overtake state-of-the-art comparison
techniques that denoise traditional Monte Carlo samplings. In particular,
we observe that the correlated gradient samples — that offer information
about the smoothness of the integrand unavailable in standard Monte Carlo
sampling — notably improve image quality compared to an equally powerful
neural model that does not make use of gradient samples.
CCS Concepts: • Computing methodologies → Neural networks; Ray
tracing.
Additional Key Words and Phrases: gradient-domain rendering, gradientdomain reconstruction, screened poisson, ray tracing
1 INTRODUCTION
Realistic image synthesis seeks to produce realistic virtual photographs by computationally solving the Rendering Equation [Kajiya 1986], often by randomly sampling paths that carry light from
the light sources to the sensor. Rendering with too few samples
leaves the image with visually distracting noise. Unsurprisingly,
practical applications constantly struggle with striking a balance between the complexity of content (slower, more noise) and available
computational resources.
Since many Monte Carlo samples are required for a high quality image, this leaves four main approaches for making rendering
faster: (1) making samples faster to evaluate (e.g. GPU rendering,
ray tracing hardware, optimized low-level algorithms), (2) sharing
contributions between nearby paths (e.g. photon mapping), (3) being
clever in choosing the light paths to sample (e.g. Bidirectional Path
Tracing, adaptive importance samplers), and, finally, (4) denoising
or reconstruction, attempting to produce a better picture out of the
samples by relying on various smoothness assumptions or analytic
models of the transport phenomena being modeled.
Despite a long history, and continuous research progress in all of
these areas, significant problems remain. Only naturally, the quality
obtained by “more pure” techniques that rely on few assumptions or
heuristics tends to lag behind those that assume more. For instance,
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
126:2 • Kettunen, M. et al
in cases where the diffuse albedo or surface normal — cheap, easyto-estimate helper variables [McCool 1999] — are good predictors
of the smoothness of the final image, making use of this heuristic
will, naturally, yield better pictures; however, when this is not the
case, but an algorithm assumes it anyway, the reconstruction will be
overly smooth and blurry. It is generally difficult to reason about the
validity of their assumptions in novel scenes. As a concrete example,
the presence of (particularly indirect) shadows cannot be reliably
detected from easily-available auxiliary variables, which leads many
denoising algorithms to struggle with shadow fidelity.
The recently-introduced gradient-domain family of rendering
algorithms is a combination of the final two approaches [Kettunen
et al. 2015; Lehtinen et al. 2013; Manzi et al. 2016a]. In addition to estimating pixel intensities, they also compute Monte Carlo estimates
of image gradients using correlated samples, and combine these two
kinds of samples into the final image by solving a screened Poisson problem. It can be shown that the combined gradient sampling
followed by integration exploits the frequency distribution of the
transport integrand in a way that often results in higher-quality
results within an equal sampling budget [Kettunen et al. 2015; Lehtinen et al. 2013]. Depending on the choice of norm in which the
screened Poisson equation is solved, the techniques remain unbiased (L2) or consistent (L1), while still exploiting smoothness for
better reconstruction, but without relying on heuristics based on
auxiliary helper variables.
In this work, we seek to combine the power of the natural smoothness information provided by gradient samples with the high performance of modern denoising techniques that draw on auxiliary
features, and aim to do this in a way that adapts to the myriad
of different light transport configurations. Determining which input signal to trust to correlate with coherence is a highly contextdependent task. We solve it with a convolutional neural network
that is trained to minimize a perceptually-motivated loss function.
Our network is a novel hybrid of Densely Connected Convolutional
Networks (DenseNet) [Huang et al. 2016] and U-Net [Ronneberger
et al. 2015], an autoencoder with skip connections. Our model is
relatively simple to implement given an existing gradient-domain
path tracer. In particular, we do not require splitting path contributions to separate diffuse and specular channels, as is the case with
the Kernel-Predicting Convolutional Network (KPCN) of Bako et al.
[2017].
Our results surpass the performance of KPCN and the Nonlinearly
Weighted First-order Regression NFOR of Bitterli et al. [2016], two
state-of-the-art denoisers, on held-out test scenes. By directed tests,
we show that gradient samples clearly improve the results compared
to an otherwise equal deep model fed with equal-time primal-only
images. Our reconstruction also significantly improves the quality of
reconstructed shadows despite working with a much lower sample
count in equal time comparisons (Figure 1).
2 RELATED WORK
We will briefly summarize the most important related work in the
following. We refer the reader to Zwicker et al. [2015] for a discussion on denoising methods, and to O’Shea and Nash [2015] for an
introduction to convolutional neural networks.
2.1 Denoising
The non-local means filter (NL-means) [Buades et al. 2005] denoises
pixel colors by averaging over all pixels in an image whose local
neighborhoods look similar. Rousselle et al. [2012] extend NL-means
to filter Monte Carlo renderings, and use repeated reconstructions
to guide sample placement. Rousselle et al. [2013] combine NLmeans filtering with feature buffers using Stein’s Unbiased Risk
Estimate (SURE). Bitterli et al. [2016] evaluate regression weights
from NL-means prefiltered feature buffers, but run a first-order
weighted regression with only the color data. Moon et al. [2016]
run a local polynomial regression with heuristics for selecting the
optimal polynomial degree for each pixel.
Kalantari et al. [2015] use a multilayer perceptron to compute
optimal filter parameters for cross-bilateral and NL-means filters
from primal color and auxiliary feature buffers. The work closest
to ours is the Kernel-Predicting Convolutional Network (KPCN) of
Bako et al. [2017], a convolutional neural network that predicts a
separate smoothing filter kernel for each pixel. They split inputs into
diffuse and specular light transport, and process them in separate
pipelines. Vogels et al. [2018] extend this work to animations, and
build a modular system for supporting multiple samplers with the
same denoising network. Chaitanya et al. [2017] design a featurebased convolutional neural network which uses the multi-resolution
U-Net network structure as its base, enhanced with recurrent connections for use in animation. Their method is tailored to extremely
low sample counts.
2.2 Gradient-Domain Rendering
Lehtinen et al. [2013] introduced gradient-domain rendering by
showing that it is beneficial, in the Metropolis Light Transport
context, to estimate finite differences between adjacent pixels by
sampling and subtracting light paths in highly correlated pairs. They
reconstruct the final image from the typically more noisy colors
and less noisy gradients by solving a screened Poisson equation.
They show that the L2 solution of the screened Poisson equation is
unbiased, but recommend using the slightly biased but consistent L1
version as it typically produces more visually pleasing results. Intuitively, the high performance of gradient-domain renderers, where
applicable, can be explained by examining the screened Poisson
problem: its solution shares information spatially over non-trivially
large image patches.
Kettunen et al. [2015] adapt Path Tracing to Gradient-Domain
Rendering. Subsequently, many other rendering methods have been
adapted for the gradient domain: Gradient-Domain [Manzi et al.
2015] Bidirectional Path Tracing [Veach and Guibas 1995], GradientDomain [Bauszat et al. 2017] Path Reusing [Bekaert et al. 2002],
Gradient-Domain [Hua et al. 2017] Stochastic Progressive Photon
Mapping [Hachisuka and Jensen 2009], Gradient-Domain [Sun et al.
2017] Vertex Connection and Merging [Georgiev et al. 2012], and
rendering of homogenous volumes with Gradient-Domain adaptations [Gruson et al. 2018] of Beam Radiance Estimates [Jarosz et al.
2008], Progressive Photon Beams [Jarosz et al. 2011] and Photon
Planes [Bitterli and Jarosz 2017]. To enforce and exploit temporal
coherence, Manzi et al. [2016a] extend gradient computation to the
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
Deep Convolutional Reconstruction For Gradient-Domain Rendering • 126:3
time domain by mapping random seeds of light paths between successive animation frames, and solve a temporal extension of the
screened Poisson equation.
Somewhat related to our work, Manzi et al. [2016b] regularize
the screened Poisson reconstruction with the soft constraint that
the solution should be expressible locally as a linear combination
of the truncated SVD of auxiliary feature buffers. Rousselle et al.
[2016] design a new reconstruction method for Gradient-Domain
Rendering by formulating the reconstruction with control variates.
Back et al. [2018] formulate an ideal feature for local regression
based denoising, and show how to approximate it with gradientdomain rendering. This effectively joins gradient-domain rendering
with Adaptive Polynomial Rendering [Moon et al. 2016].
Our work is the first to combine gradient-domain sampling with
powerful deep models.
2.3 Neural Networks
Convolutional neural networks are powerful parametric non-linear
models that can be applied to many image restoration tasks, e.g.,
denoising, by tuning their performance over a training dataset of
noisy images and corresponding noise-free target images in an
attempt to match the model’s predictions to the clean targets, given
noisy inputs.
In particular, the U-Net network architecture [Ronneberger et al.
2015] is a simple and powerful modification of the classical autoencoder, i.e., an encoder-decoder model, that has found widespread
use in image restoration [Mao et al. 2016]. Execution first proceeds
towards smaller resolutions, processing with a number of convolutional layers at each resolution, before downsampling; after a
bottleneck layer, the process is mirrored. U-Net’s main innovation
is the addition of “skip connections” that link faraway parts of the
encoder with the decoder, significantly easing the task of producing
pixel-accurate results and facilitating training.
Huang et al. [2016] introduced the DenseNet architecture where
all layers get the results of all previous layers as inputs. This facilitates reuse of computation and improves gradient flow, leading to
improved accuracy and easier training of deep networks. Previous
work in medical imaging has partially combined the DenseNet and
U-Net architectures [Guan et al. 2018; Li et al. 2017; Yan et al. 2018].
We present a different, more connected hybrid architecture in the
following section, and defer discussion until then.
2.4 Perceptual Losses
Measuring image similarity in a way that corresponds to human
judgment is a hard, long-standing problem. Recently, Zhang et al.
[2018] formalized the accumulating anecdotal knowledge that the
hidden variables of deep convolutional image classifier networks
form good perceptual spaces. They introduced the Learned Perceptual Image Patch Similarity (LPIPS), which, given two images,
runs them through an image classification network such as VGG
[Simonyan and Zisserman 2014] or SqueezeNet [Iandola et al. 2016],
and returns a weighted L2 distance of the network activations. In
concurrent work, Kettunen et al. [2019, see supplemental] observe
that LPIPS is not robust against optimization, and suggest an ensembled modification (E-LPIPS) that retains the predictive power
of LPIPS, but is also a robust optimization target. Their key insight
is that applying simple randomized geometry and color transformations to the images makes the feature space much more robust.
We make use of E-LPIPS in training our models. Using perceptual
losses in image restoration tasks is in itself not a novel approach
(e.g. [Johnson et al. 2016]).
3 NEURAL RECONSTRUCTION
Gradient-domain rendering algorithms produce, in addition to a
Monte Carlo estimate Ip of the pixel intensities (p for “primal”),
estimates of the finite difference pixel gradient (Idx , Idy ) using pairs
of correlated path samples. After sampling, the final image is reconstructed by solving
argmin
I



I − Ip



 + α



∇I − (Idx , Idy )



 , (1)
where ∇ denotes a finite difference gradient operator over the pixels
and α is a parameter that can be derived from the relative variances
of primal and gradient samples. Solving the problem in the L2 norm
results in an unbiased image, but the outlier-suppressing L1 norm
is preferred in practice. Both norms have efficient solvers.
To incorporate auxiliary feature information, e.g., depth, normal,
albedo, without designing (potentially complex) smoothness priors
to weight the terms in the equations by hand, and also to support
optimizing the result in terms of complex, non-linear perceptual
image distance metrics, we take a different route and cast the reconstruction into a direct regression problem solved by a convolutional
neural network (CNN)
I = CNN(Ip, Idx , Idy, F1, F2, . . . ; θ) (2)
where the additional inputs F1 . . . are auxiliary feature buffers. (Section 3.3 details the precise inputs given to our model.) Following
earlier deep image restoration models, the parameters θ are obtained
by minimizing the “empirical risk”
argmin
θ
Ei
n
L

CNN(I
i
p
, . . . ; θ) − I
i
o (3)
over a training set comprising of noisy primal and gradient inputs,
as well as the corresponding noise-free targets I
i
, using a variant
of stochastic gradient descent. The loss function L(·) measures the
difference between predictions and targets. Note that while we lose
the analytic tractability of Equation 1, key to prior fast solvers, and
are forced to training by ahead-of-time numerical optimization, this
also leaves the door open for more complex loss functions that do
not admit analytic solutions.
This overall strategy leaves two questions:
(1) the choice of network architecture; and
(2) the choice of loss function.
Both directly affect the characteristics of the results: a good loss
will result in pleasing pictures, and the model needs to be powerful
enough to find good solutions while remaining trainable. We now
treat each in turn.
3.1 Network Architecture
We base our model on the popular U-Net architecture [Ronneberger
et al. 2015], an hourglass-shaped deep autoencoder with non-local
“skip connections” that link corresponding resolutions in the encoder
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
126:4 • Kettunen, M. et al
and decoder. We further present a novel combination of U-Net with
DenseNets [Huang et al. 2016].
The diagram in Figure 2 describes our network structure. Overall,
it is similar to U-Net: computation proceeds from high resolutions to
low resolutions and back, with skip connections linking corresponding layers. Each resolution consists of chained convolutional layers
followed by non-linearities, and the latter half of each resolution
receives data from the first half and from the lower resolution. We
use a total of four different resolutions, up to 1/8 the size of the
original image along both axes. The figure describes the sizes of
convolution kernels as well as numbers of feature maps.
Each resolution of the neural network is also a single densely
connected block from DenseNet, and each of its layers receives as
input not only the previous layer’s activations, but a concatenation
of, loosely speaking, all previous layers of the same resolution, and
also the final results of the lower resolution. While dense blocks
have been used in U-Net architectures before, we take this idea to its
logical conclusion: each resolution is, in its entirety, a single dense
block, with the decoder end getting additional input from the lower
resolutions. As detailed in Section 4, this improves results somewhat
over the standard U-Net.
3.2 The Loss Function and Dynamic Range
3.2.1 Dynamic range. A key problem in neural network based reconstruction in Monte Carlo rendering is that magnitudes of input
values can vary widely; indeed, the dynamic range of the inputs is
unbounded. Particularly tail effects such as caustic paths regularly
result in extremely bright outliers. This is in stark contrast with the
well known behavior that neural networks tend to work best when
the range of the inputs is small and uniform. A common strategy
(see for example Bako et al. [2017] and Vogels et al. [2018]) is thus
to map the network inputs to logarithmic domain by y = log(1 + x),
apply the network in this domain, and invert the mapping at the
end of the network by x = exp(y) − 1. We follow this approach.
The immediate challenge in applying this to gradient-domain is
that the gradients contain negative numbers, and consequently the
logarithmic mapping does not work directly. We observe, however,
that log(1 + x) is very well behaved also near the origin (where
log(1 + x) ≈ x), and the natural extension for gradients is the odd
reflection:
y = sgn(x) log(1 + |x |), (4)
which is a monotonous function with inverse
x = sgn(y) (exp(|y|) − 1) . (5)
This extension not only adds support for negative inputs, but it can
also be used to learn negative targets, for example if using pixel
filters with negative lobes.
3.2.2 Perceptual loss. While the goal for an efficient rendering
method is normally to produce images with as small residual error
as possible as judged by a human observer, it is widely known that
pixel-wise norms, such as L1 and L2, do not correspond to image
similarity as perceived by humans. We make use of the humancalibrated LPIPS perceptual image distance metric that compares
images in the L2 sense in the non-linear feature space of pre-trained
image classifier CNNs [Zhang et al. 2018]. More precisely, we employ
a self-ensembled variant (E-LPIPS) of it [Kettunen et al. 2019, see
supplemental] that has been shown to be a more robust optimization
target. In the results section, we show comparisons to the L1 loss.
While the logarithmic decompression after the network yields
high-dynamic range outputs as desired, evaluating the perceptual
E-LPIPS loss yields one more complication: it expects inputs to be in
the range [0, 1]. We solve this by applying Reinhard tone mapping
[Reinhard et al. 2002] before evaluating the loss:
R(x) =
x
1 + mean(x)
. (6)
E-LPIPS also expects the images to be in non-linear sRGB, so the
images need to be gamma corrected with the sRGB conversion
formula before evaluation:
sRGB(c) =

12.92 c c ≤ 0.0031308
1.055 c
1
2.4 − 0.055 c > 0.0031308.
(7)
Approximation with e.g. sRGB(c) ≈ c
1/2.2
is not possible due to the
singularity in the derivative (at zero) which would make training
unstable.
3.2.3 Regularization by L1 loss. Human judgment is much less
sensitive to error in tone than in shape, and the LPIPS family of
losses is somewhat insensitive to brightness and tone. We still want
our reconstruction to accurately predict tones, so we complement
our training loss with a small L1 component applied to a Reinhardtonemapped result image, as well as its gradients. Since gradients
may be negative, we use the odd reflection: R¯(x) = x/(1+mean(|x |)).
The tone mapping avoids instability that is observed using HDR
inputs. Combining all the components, the final loss function is
L(x,y) = dE-LPIPS(sRGB(R(x)),sRGB(R(y)))
+ β · ∥R(x) − R(y)∥1
(8)
+γ ·



(R¯(∇x) − R¯(∇y)




1
.
We get good results with β = γ = 0.01.
3.3 Network Details
3.3.1 Type of convolution. We use strided 2×2 convolution pooling
and 2×2 convolution transpose unpooling. The input transformation
log(1 + x) is undone at the very end of the network by applying
exp(x) − 1 to the final RGB layer.
3.3.2 Nonlinearity. We use leaky ReLUs [Maas 2013] with factor
0.01 after all convolutional layers, except for the final one whose
purpose is to produce a three-channel image.
3.3.3 Parameterization and optimization schedule. We use Weight
Normalization [Salimans and Kingma 2016] with He initialization
[He et al. 2015] for weights, and optimize with Adam [Kingma and
Ba 2014] with parameters β1 = 0.9, β2 = 0.99 and ϵ = 10−8
. The
learning rate is first ramped geometrically up to 0.0005 during the
first 10 000 minibatches of 20 images, and it slowly starts to decrease
after the first 50 000 minibatches, halving approximately every 11
hours following curve L(t) = L02
−t
.
Since the E-LPIPS loss is stochastic, we evaluate it thrice for each
predicted image during training. While this makes each training
sample somewhat more expensive, we find that this significantly
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
Deep Convolutional Reconstruction For Gradient-Domain Rendering • 126:5
1×1 Conv, LReLU
3×3 Conv, LReLU
2×2 Conv Pool, LReLU
2×2 Conv Tr. Unpool, LReLU
Processing Unit
3×3
1×1
In Out
2N N
Input (16 feat.) Output (RGB)Linear
80
N=40
N=40
N=40
N=40
80
80
80
80
80
80
80
80
80
80
80
80
80
80
40
40
40
40
160
160 160
160
Full Resolution Half 1/4 1/8 1/4 Half Full
160 features
Fig. 2. Network architecture. We process the colors, gradients and other features in multiple resolutions. We repeatedly give the concatenation of all previous
results of the current resolution to a processing unit (left) which contracts the large number of features into a smaller number with a 1 × 1 convolution and
applies a 3 × 3 convolution further contracting the number of features. This result will again be given as input to all future layers of this resolution. At the
half-way of each resolution, all previous results of that resolution are concatenated and pooled with 2 × 2 strided convolution, and the process repeats. At the
end of each resolution, we unpool the concatenated results with a 2 × 2 convolution transpose. Leaky ReLUs are applied after each convolution except the
1 × 1 convolution at the very end of the network. The final result is passed through exp(x) − 1 to undo the initial log(1 + x) mapping of the inputs.
reduces wallclock time-to-convergence. This is in line with earlier
findings [Kettunen et al. 2019].
The optimization schedule and the other hyperparameters (e.g.
sizes and numbers of resolutions) were chosen to approximately
minimize the E-LPIPS loss after training for 72 hours. This is orthogonal to using the E-LPIPS loss for training.
3.3.4 Data augmentation. We augment our dataset by randomly
flipping, mirroring, and transposing each of the 128 × 128 inputtarget crops that we use for training. We also mirror-pad the images
randomly such that the total amount of padding in both x and y
directions is 16 pixels. This essentially makes the network see differently offset versions of the images. (Note that while it is similar to the
internal workings of the E-LPIPS loss, training data augmentation
serves a different purpose.)
We also randomly permute the color channels, and sample nonnegative random color multipliers from the planemean(R,G, B) = 1.
We apply these to the color, gradient and albedo buffers. We also
sample a log-normal brightness multiplier which we apply only to
the color and gradient buffers.
3.3.5 Input data format. The inputs to our method are the standard
color and gradient buffers produced by a gradient-domain renderer,
along with albedos, normals and depths. These are the features we
find the most useful. We did not find feature variances to produce a
significant improvement and dropped them for simplicity. We transform normals to the camera’s perspective, and normalize depths to
the range [0, 1]. Radiance quantities are compressed as detailed in
Section 3.2.
During testing, we normalize all depth buffers to range [0, 1].
Since our training images consist of small crops, normalizing each
of them to [0, 1] would make the network expect a very unrealistic
depth distribution. We instead normalize them to range [0,s] where
s is sampled uniformly from [0.1, 1.1]. Going slightly over 1 during
training is a non-issue, but it makes the network encounter values
close to one slightly more often.
4 RESULTS AND DISCUSSION
In our evaluation, we seek to study the usefulness of gradient samples in the context of modern denoising techniques, neural or otherwise. For this we choose two state-of-the-art comparison techniques
from different algorithm families. The first is the Kernel-Predicting
Convolutional Network (KPCN) of Bako et al. [2017], a generalpurpose supervised learning technique shown to yield good denoising performance for moderately low sample counts. The second is
the NFOR [Bitterli et al. 2016] algorithm that does not make use
of machine learning techniques and instead relies on an a priori
model. Furthermore, we include the L1 screened Poisson solver used
in previous gradient-domain renderers [Lehtinen et al. 2013] as a
baseline, although it is working with strictly less information as it
does not utilize auxiliary features. We compensate the overhead of
gradient sampling for the non-gradient based methods by giving
them input buffers with 2.5 times the samples.
4.1 Test Scenes
Our test scenes include the almost diffuse-only Sponza, chosen to
establish an easy baseline where all algorithms perform well; the
harder Bookshelf, Kitchen, and Cutting Board that all feature
glossy and specular transport, with Cutting Board additionally
featuring depth-of-field; and the most difficult Bedroom that yields
high gradient variance due to transport through transparent curtains.
Bookshelf, Kitchen and Dining Room all test generalization
within the convex hull of the training set. The training scenes are all
more complicated than the test set’s Sponza. In contrast, Cutting
Board and Running Man test generalization outside of the convex
hull of the training data: depth of field and motion blur effects this
strong are very rarely, if ever, present in the training data. The
huge gradient variance seen in the curtains of Bedroom is also not
present in the training data.
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
126:6 • Kettunen, M. et al
Sponza
(Easy) Bookshelf Kitchen Dining
Room
Bedroom
(Failure)
Cutting
Board
Running
Man
4 (10) spp 16 (40) 16 (40) 4 (10) 16 (40) 4 (10) 4 (10)
Input a. L1 Sc. Pois. b. NFOR c. KPCN d. Ours Ground Truth
4 (10) spp 16 (40) 64 (160) 256 (640) 1024 (2560)
E-LPIPS
10−4
10−3
10−2
10−1
10−4
10−3
10−2
10−1
10−4
10−3
10−2
10−1
10−4
10−3
10−2
10−1
10−4
10−3
10−2
10−1
10−4
10−3
10−2
10−1
10−4 10−4
10−3 10−3
10−2 10−2
10−1 10−1 RelMSE
* a b c d
10−4
10−3
10−2
10−1
10
0
* a b c d
10−4
10−3
10−2
10−1
10
0
* a b c d
10−4
10−3
10−2
10−1
10
0
* a b c d
10−4
10−3
10−2
10−1
10
0
* a b c d
10−4
10−3
10−2
10−1
10
0
* a b c d
10−4
10−3
10−2
10−1
10
0
* a b c d
10−4 10−4
10−3 10−3
10−2 10−2
10−1 10−1
10
0 100
Fig. 3. Equal-time comparisons between Path Tracing (*), the baseline L1 screened Poisson reconstruction [Lehtinen et al. 2013] (a), a state-of-the-art
feature-based denoiser NFOR [Bitterli et al. 2016] (b), kernel-predicting deep convolutional denoising [Bako et al. 2017, KPCN] (c), and the proposed method
(d). Path Tracing, NFOR and KPCN are given the sample counts in parentheses.
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
Deep Convolutional Reconstruction For Gradient-Domain Rendering • 126:7
4.2 Metrics
We measure result quality by two metrics, relative mean square
error (RelMSE) [Rousselle et al. 2011] and E-LPIPS [Kettunen et al.
2019]. RelMSE measures squared error of pixels relative to their
brightnesses and is often considered a better match to human judgment than L1 and L2. To avoid measurement bias for E-LPIPS, we
use the VGG version of E-LPIPS for measuring, and train with the
SqueezeNet version. We Reinhard tone map our images (Equation 6)
for E-LPIPS measurement.
4.3 Overall Performance and Analysis
We first compare our method to L1 screened Poisson, NFOR and
KPCN across a variety of test scenes that were not used in training
the deep models. We focus particularly on the low sample regime
studied by Bako et al. [2017], where previous gradient-domain rendering algorithms that do not employ feature buffers struggle. KPCN
and NFOR are given 2.5 times the samples to equalize the time required for generating the inputs. Figure 3 shows insets of low sample
count results and numeric evaluation up to 1024 samples per pixel
for the gradient-based methods (ours and L1 Poisson).
We generally notice our results to be overall of higher subjective
quality compared to previous methods, which is also supported by
the numerical error measurements.
We also observe steady improvement as the sample count increases (Figure 4). We attribute this to training the model using
inputs with many different sample counts and scenes of varying
difficulty – the model has to both estimate the noise level and remove it. While no strict consistency guarantees can be made for
deep nonlinear models like ours, it would be easy to strictly ensure
consistency by simply blending between the reconstruction result
and a screened Poisson solution with increasing sample counts.
4 16 64 256 1024
Samples / Pixel
0.001
0.01
0.1
0.0003
0.003
0.03
RelMSE
Ours
Bedroom
Bookshelf
Kitchen
Dining Room
Running Man
Cutting Board
Sponza
Fig. 4. We study the convergence of our method up to 2048 gradient samples
per pixel. Despite our training set mostly targeting the low sample count
regime, we observe steady convergence up until very high sample counts,
with a slight slowdown in the Dining Room scene after 512 samples per
pixel. The network has never seen input images with over 1024 gradient
samples.
The supplemental material contains all images in an HTML viewer
that facilitates detailed study and comparison.
Sponza features relatively easy light transport, visible as significantly less noisy inputs, and all methods perform relatively well.
Our method yields the best results in the low-sample regime, but
visible differences fade relatively quickly with increasing samples.
The Bookshelf, Kitchen and Dining Room scenes are more
typical scenes with both specular and glossy transport and more
geometric detail. Bookshelf and Kitchen also feature many small
light sources, and Dining Room is lit through a window with blinds.
The shadows and glossy reflections in Dining Room feature interesting complexity. Our method clearly improves over the other
methods in these scenes, even with large sample counts.
The Cutting Board scene is a view of Kitchen with a strong
depth of field effect. Running Man features an animated character
with strong motion blur. This scene looks relatively simple at a first
glance, but metallic light stands, lampshades, and much of the light
entering the scene through a small hole in the ceiling make it quite
challenging. Again, our method comes out on top in these scenes.
In Bedroom, all light enters the scene through large windows
covered by translucent curtains which are extremely difficult for the
gradient sampler. Indeed, in parts of the image the relative variance
of the gradient and primal samples increases to such a level that
numerical performance drops below KPCN at higher sample counts,
and our dense U-Net trained without gradients works overall the
best. Interestingly, the error in our model trained with the E-LPIPS
loss is visible as hallucinated detail where none should exist. This is
also visible in the numerical results, and Figure 5 studies this case
in detail. As with previous gradient-domain rendering algorithms,
high gradient variance relative to primal samples can be detrimental
to result quality. Including more such scenes in the training set may
be useful.
Apart from the pathological case in Bedroom, we observe that
our method generally improves over KPCN, particularly in the lowsample regime. In some scenes our advantage decreases slightly
at higher sample counts. The authors of KPCN originally trained
specialized networks for each sample count. We train KPCN and
our networks for variable amount of noise, and our training set is
concentrated toward smaller sample counts.
Besides our results generally showing fewer artifacts, we also
notice a general trend: our method tends to capture shadows more
accurately, as seen for example in the scenes Bookshelf, Kitchen
and Dining Room. We attribute this to the smoothness information
implicitly carried by gradient samples. Figure 6 presents close-up
demonstrations. We invite the reader to make use of the HTML
viewer for more detailed inspection.
4.4 Ablations and Directed Tests
4.4.1 Are gradients useful? While we have demonstrated best performance overall across the comparison methods, architectural differences between our network and KPCN warrant a question: What
is the role of gradient-domain sampling in the results? For this, we
train a CNN precisely of the same architecture as ours with the
only difference that it does not take gradient inputs; the loss, training procedure, etc., all remain unchanged. The supplemental image
viewer contains their images as well, labeled “Ours (w/o grads)”.
Like KPCN and NFOR, the method is given inputs with 2.5 times
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
126:8 • Kettunen, M. et al
RelMSE (Ours) RelMSE (Ours w/o grads) Var(Grad) / Var(Primal)
Input (32 SPP) Ours Ours w/o grads Ground Truth Var(G) / Var(P) RelMSE (Ours) RelMSE (w/o)
Fig. 5. The top left image shows the RelMSE distribution of our reconstruction in the Bedroom scene which is challenging for our method. Comparing this to
our network trained without gradients (top middle), we observe that the areas where gradients are not beneficial correlate highly with areas of high gradient
variance relative to primal variance (top right), as originally shown for screened Poisson by Kettunen et al. [2015]. The inset below shows this in practice: The
gradient reconstruction “Ours” shows higher error than “Ours w/o grads” where the relative gradient variance is high, and vice versa, as can also be seen in
the RelMSE views.
NFOR KPCN
Ours, variation
(No gradients) Ours Ground Truth
Dining Room Kitchen
Fig. 6. Equal-time visual comparison of shadows between NFOR, KPCN, our network trained without gradients and our method. Since shadows cannot be
well predicted from the traditional feature buffers, gradient-free denoisers (three first columns) tend to have hard time reconstructing them. Shadow edges
and penumbra are better captured in gradient samples, and our gradient-domain reconstruction typically reconstructs shadows more accurately.
as many samples to compensate for the reduced sampling cost and
maintain equal time.
As demonstrated in Figure 7, the gradient-domain version performs consistently better than the non-gradient version with two
exceptions.
The curtains in Bedroom are hard for gradient-domain path
tracing so that scene is best rendered without gradients. Cutting
Board, on the other hand, is a scene with very strong depth of field,
and while our method without gradients improves over our method
with gradients, the difference is rather small.
The gradients in the Bedroom scene are noisy due to the gradient
shift implementation only partially supporting how the rest of the
renderer handles translucency in the curtains, which results in
increased noise but not bias (Figure 5). While simple to fix, we use
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
Deep Convolutional Reconstruction For Gradient-Domain Rendering • 126:9
Spz. Bks. Kit. Din. Bed.† Cut.† Run.†
E-LPIPS
10
−4
10
−3
10
−2
10
−4
10
−3
10
−2
10
−4
10
−3
10
−2
10
−4
10
−3
10
−2
10
−4
10
−3
10
−2
10
−4
10
−3
10
−2
10
−4
10−4
10
−3
10−3
10
−2
10−2
RelMSE
a b
10
−4
10
−3
10
−2
10
−1
a b
10
−4
10
−3
10
−2
10
−1
a b
10
−4
10
−3
10
−2
10
−1
a b
10
−4
10
−3
10
−2
10
−1
a b
10
−4
10
−3
10
−2
10
−1
a b
10
−4
10
−3
10
−2
10
−1
a b
10
−4
10−4
10
−3
10−3
10
−2
10−2
10
−1
10−1
Samples / Pixel: 4 16 64 256 1024
Fig. 7. Our network trained with gradients (b, green) typically improves
over our network trained without gradients (a, orange) for equal-time inputs
(Sponza — Dining Room, Running Man). †
: The Bedroom scene is hard for
the gradient sampler while Cutting Board and Running Man contain very
strong blur effects. See text in Section 4.4.1.
this to study the effect of high relative variance of gradients which
could also result e.g. from an abundance of sub-pixel scale geometric
features, or a complex material not obeying the assumptions behind
the shift mapping.
Overall, the benefits obtained from sampling gradients in addition to pixel radiance appear to carry over to non-linear neural
reconstruction: the smoothness information carried by the finite
difference samples most often helps the algorithm in our test cases.
Unfortunately, the complex non-linear nature of the network makes
analysis in the style of earlier gradient-domain rendering work
[Kettunen et al. 2015] difficult.
4.4.2 Is the perceptual loss useful? In light of earlier work on perceptual error metrics [Kettunen et al. 2019], we expect training our
network with the E-LPIPS loss instead of the traditional L1 loss to
yield an improvement in perceptual visual quality, most likely at
a slight cost to per-pixel color accuracy. We also expect the reconstructions to generally be slightly less blurry and more detailed,
as training with L1 makes the network try to predict pixel-wise
medians of potential ground truth images. We train a network with
the L1 loss applied to the Reinhard-tonemapped image (see Section 3.2.3), and find this to generally be the case. Figure 8 shows two
representative close-ups: the E-LPIPS loss yields more structurally
sound images.
From our test set we observe two exceptions. If the result is
actually supposed to be very blurry, for example due to a strong
defocus effect, an E-LPIPS trained network might still hallucinate
spurious sharp detail, whereas when an L1 trained network sees
an ambiguous situation, it will instead hallucinate blur. Figure 9
shows an example. Careful control of the distribution effects in the
training set may alleviate the issues, but this remains future work.
Ours, variation
(L1 loss)
Ours, actual
(E-LPIPS loss) Ground Truth
Kitchen (16 spp) Bookshelf (16 spp)
Fig. 8. A network trained with the L1 loss produces a slightly over-blurred
image (left column). As E-LPIPS captures the structure of natural images
better, its prediction (middle column) is more natural.
Ours, variation
(L1 loss)
Ours, actual
(E-LPIPS) Ground Truth
Cutting B. (16 spp)
Fig. 9. A scene with very strong depth of field or motion blur effects will
have very noisy inputs, yet the output is supposed to be smooth. A network
trained with the L1 loss will naturally resolve the input noise into a strong
blur (left) while a network trained with E-LPIPS is more prone to seeing
spurious detail in the noise (middle). In this case the L1 network’s overblurring may be more satisfactory.
We study the above effects numerically in Figure 10 – training
with the E-LPIPS loss often produces slightly improved numerical
results, but not so when very strong blur effects are present. The
improved clarity seems to generalize also to very high sample counts
– by visual inspection – even though the numeric results seem to
slightly favor the L1 network in that range. We recommend the
reader to judge the high sample count regime with the viewer in
the supplemental material.
4.4.3 Dense U-Net. Our network structure is a novel hybrid of
DenseNet and U-Net. We find our network to perform consistently
better than a traditional U-Net of the same capacity (Figure 11).
4.4.4 Temporal stability. While our algorithm is designed for still
images, and orthogonal techniques exist for exploiting temporal
coherence in neural denoising and gradient-domain reconstruction
[Chaitanya et al. 2017; Manzi et al. 2016a], the temporal behavior of
our algorithm is still a relevant question.
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
126:10 • Kettunen, M. et al
Spz. Bks. Kit. Din. Bed. Cut. Run.
E-LPIPS
10
−3
10
−2
10
−3
10
−2
10
−3
10
−2
10
−3
10
−2
10
−3
10
−2
10
−3
10
−2
10
−3
10−3
10
−2
10−2
RelMSE
a b
10
−3
10
−2
a b
10
−3
10
−2
a b
10
−3
10
−2
a b
10
−3
10
−2
a b
10
−3
10
−2
a b
10
−3
10
−2
a b
10
−3
10−3
10
−2
10−2
Samples / Pixel: 4 8 16 32 64
Fig. 10. The improved image structure and clarity gained from training with
the E-LPIPS loss (b, green), as opposed to the L1 loss (a, yellow), is often
visible in the numbers in the low–medium sample counts in the typical
scenes (Sponza — Dining Room), but training with the L1 loss often handles
blur effects better (Cutting Board, Running Man). Multiply the sample
counts by 2.5 to get their Path Tracing equivalents. See the supplemental
material for high sample count results.
4 16 64 256 1024
Samples / Pixel
0.001
0.01
0.1
0.0003
0.003
0.03
E-LPIPS RelMSE
Bookshelf
RelMSE E-LPIPS
U-Net
Ours
Fig. 11. Our Dense U-Net architecture (blue) consistently improves over the
traditional U-Net structure with an equal number of model parameters (red),
as indicated here by the perceptual E-LPIPS loss (solid lines) and RelMSE
(dashed lines). The results are similar for all of our test scenes.
We conducted a simple study of temporal stability by rendering
videos for two sample counts with a static camera, with different
random seeds for each frame (Figure 13). The videos are available
in the supplemental material. As expected, we observe some highfrequency temporal flicker, but not more than KPCN, and we believe
that earlier reprojection techniques can be applied to alleviate these
issues.
4 16 64 256 1024
Samples / Pixel
0.001
0.01
0.1
0.0003
0.003
0.03
E-LPIPS RelMSE
Bookshelf
RelMSE E-LPIPS
Average
Max
Bilinear
Lanczos
Ours
RelMSE E-LPIPS
Average
Max
Bilinear
Lanczos
Ours
Fig. 12. Our network uses strided 2 × 2 convolutions for downsampling
and 2 × 2 transposed convolution for upsampling. Other options include for
example average or max pooling followed by nearest neighbor upsampling,
or using bilinear or Lanczos resampling. We find that convolution pooling
followed by convolution transpose unpooling (blue) usually performs best
in our case, although only with a small margin. We show the E-LPIPS (solid
lines) and RelMSE (dashed lines) results for the Bookshelf scene as an
example of the common case.
4.4.5 Choice of pooling. We also empirically validate our decision
of using strided 2 × 2 convolution pooling and corresponding convolution transpose unpooling. See Figure 12.
4.5 Training Set and Generalization
The total training set consists of 2500 pieces of 128 × 128 crops,
each in five different sample counts, totaling 12 500. We supplement
this training set with basic augmentation. The relationship between
training and test scenes is discussed in the beginning of Section 4.
Our training set consists of ten scenes. For each scene we predefine a box guaranteed to be free of occlusions, and sample 250
random camera positions and normally distributed velocity vectors
to simulate motion blur. We set the camera to look towards one of
several predefined targets, with a randomly chosen aperture size to
simulate depth of field. From each of these points of view we sample
a crop of size 128×128 pixels, and render it with six different sample
counts: one target image with 8K samples, and five input images
with sample counts sampled log-uniformly from range [2, 1024],
emphasizing the smaller sample counts where the image quality
varies faster. The training set is rendered with the Gradient-Domain
Path Tracing implementation from Kettunen et al. [2015] built on
top of Mitsuba renderer [Jakob 2010].
When still designing our method, we used leave-one-out cross
validation for our test results. As we did not find the reconstruction
quality to be overly sensitive to the scenes used in training, we now
use a fixed training/test split instead.
See the supplemental material for examples from our training set.
4.6 Implementation Details
4.6.1 Software and hardware, training. We implement our algorithm
in Tensorflow. We train both our network and KPCN [Bako et al.
ACM Trans. Graph., Vol. 38, No. 4, Article 126. Publication date: July 2019.
Deep Convolutional Reconstruction For Gradient-Domain Rendering • 126:11
Ground Truth Ours (16 SPP) KPCN (40 SPP) Ours (128 SPP) KPCN (320 SPP)
y, frame number
x Variance: 3.225 × 10−4
5.502 × 10−4
0.639 × 10−4
1.150 × 10−4
Fig. 13. Equal-time rolling shutter comparison between our method and KPCN at two different rendering times in the Bookshelf scene. The rolling shutter
images, constructed by taking every image row from a different animation frame, show that both methods will show some flickering in animation for both
rendering times. The sample variances of the pixels over 24 independent renders suggest that our method might not be more temporally unstable than KPCN.
See the supplemental material for comparison videos.
2017] on an NVIDIA Tesla V100 GPU using the same training set.
We train all networks for three full days which reaches around 12
million shown training examples for our network. After this, we do
not observe significant qualitative improvements in results for either
model. (See Section 3.3 for the optimization schedule.) Following the
authors’ original procedure, we pre-train KPCN for one day with
separate diffuse and specular targets, and then tune for two more
days with the usual non-separated targets.
4.6.2 Running time. All result images are 1280 × 720 pixels in size.
Our network runs in 0.3 seconds per image, while KPCN takes 1.7
seconds per image. The publicly available NFOR implementation
processes our images in approx. two minutes on an Intel i7 3770K
CPU.
4.6.3 Model size. Our network has 4.4M trainable parameters, whereas
KPCN has 5.8M (31% more). Our model size was chosen so that no
further improvement was observed. For KPCN we use the default
parameters suggested by the authors.
4.6.4 Input data format. Our method, KPCN and NFOR all use
the same feature buffers (albedo, depth and normals). KPCN and
NFOR additionally require the sample variances for all buffers. NFOR
renders the image in two equal-sized sample buckets, while KPCN
requires the separation of light transport into diffuse and specular
components. Ours method requires neither, but to compensate for
the cost of the gradients we give NFOR and KPCN 2.5× as many
samples. This equalizes input rendering time between all methods.
5 CONCLUSIONS
We have shown that Gradient-Domain Path Tracing, combined with
a powerful deep model replacing the screened Poisson solver of
prior work, consistently and visibly improves over the current state
of the art in Monte Carlo rendering in equal time. Comparing our
reconstruction model with and without gradients, we generally
find gradient sampling beneficial, despite their increased cost persample. This is supported by both visual inspection and numerical
error measurements. This indicates that the smoothness information
carried by gradient samples offers benefits also in the non-linear,
feature-driven reconstruction domain.
An additional power of deep neural models is the ability to optimize metrics that match human judgment of visual similarity better
than traditional per-pixel norms or their simple extensions such
as MS-SSIM. We optimize our network to minimize a variant of ELPIPS by light hyper-parameter search and training, and generally
observe improved image quality and less blurring. Unfortunately,
the non-linear nature of the E-LPIPS metric means that we cannot
employ the recently-proposed “noise2noise” training technique that
does not require converged target images [Lehtinen et al. 2018]
but can instead be trained on noisy data alone. Seeking ways to
overcome this limitation remains interesting future work, as does
combining our technique with orthogonal methods for ensuring
temporal consistency for rendering animations. Furthemore, we are
keen to investigate the potential usefulness of kernel prediction
[Bako et al. 2017; Vogels et al. 2018] in gradient-domain reconstruction instead of our “direct prediction” approach.
