The advent of consumer depth cameras has incited the development of a
new cohort of algorithms tackling challenging computer vision problems.
The primary reason is that depth provides direct geometric information that
is largely invariant to texture and illumination. As such, substantial progress
has been made in human and object pose estimation, 3D reconstruction and
simultaneous localization and mapping. Most of these algorithms naturally
benefit from the ability to accurately track the pose of an object or scene of
interest from one frame to the next. However, commercially available depth
sensors (typically running at 30fps) can allow for large inter-frame motions
to occur that make such tracking problematic. A high frame rate depth
camera would thus greatly ameliorate these issues, and further increase
the tractability of these computer vision problems. Nonetheless, the depth
accuracy of recent systems for high-speed depth estimation [Fanello et al.
2017b] can degrade at high frame rates. This is because the active illumination
employed produces a low SNR and thus a high exposure time is required to
obtain a dense accurate depth image. Furthermore in the presence of rapid
motion, longer exposure times produce artifacts due to motion blur, and
necessitates a lower frame rate that introduces large inter-frame motion
that often yield tracking failures. In contrast, this paper proposes a novel
combination of hardware and software components that avoids the need
to compromise between a dense accurate depth map and a high frame rate.
We document the creation of a full 3D capture system for high speed and
quality depth estimation, and demonstrate its advantages in a variety of
tracking and reconstruction tasks. We extend the state of the art active stereo
algorithm presented in Fanello et al. [2017b] by adding a space-time feature
in the matching phase. We also propose a machine learning based depth
refinement step that is an order of magnitude faster than traditional postprocessing methods. We quantitatively and qualitatively demonstrate the
benefits of the proposed algorithms in the acquisition of geometry in motion.
Our pipeline executes in 1.1ms leveraging modern GPUs and off-the-shelf
cameras and illumination components. We show how the sensor can be
employed in many different applications, from [non-]rigid reconstructions
to hand/face tracking. Further, we show many advantages over existing state
of the art depth camera technologies beyond framerate, including latency,
motion artifacts, multi-path errors, and multi-sensor interference.
CCS Concepts: • Computing methodologies → Computer vision; Image and video acquisition;
Additional Key Words and Phrases: depth sensor, high framerate tracking.
1 INTRODUCTION
Computer vision tracking problems span a wide range of applications, from tracking simple parametric models [Nakabo et al. 2000;
Stuhmer et al. 2015] to more complex non-rigid shapes such as human bodies [Dou et al. 2017]. However, when the tracking problem
involves many degrees of freedom (e.g. non-rigid reconstruction of
body parts [Dou et al. 2017, 2016; Zollhöfer et al. 2014]) depth sensors
offer crucial advantages over RGB cameras or IMUs. In particular,
depth images provide richer geometric data while simultaneously
achieving invariance to surface texture and lighting conditions. The
importance of high framerate (fps) for tracking was demonstrated
by the research community (see e.g. [Handa et al. 2012; Kim et al.
2016; Nakabo et al. 2000; Stuhmer et al. 2015]), as well as by the
widespread adoption of higher speed RGB cameras and/or inertial
measurement units (IMUs) in commercial 3D rigid pose estimation
and tracking systems for VR/AR (e.g. Oculus, HTC Vive, Microsoft
HoloLens, and Google Tango). High framerate cameras are also used
in commercial MOCAP systems (e.g. OptiTrack or VICON) for the
tracking of a sparse set of optical markers at framerates higher than
1kHz. In our work, we desire to bridge the gap between MOCAP
systems and depth sensors towards enabling the tracking of dense
geometry in fast motion.
Depth revolution. The recent availability of low-cost commodity
3D capture systems such as the Microsoft Kinect has revolutionized
our ability to tackle challenging computer vision and human computer interaction problems such as body part classification [Shotton et al. 2011; Sridhar et al. 2015], hand pose estimation [Keskin
et al. 2012; Taylor et al. 2016; Tkach et al. 2016], action recognition
[Fanello et al. 2013a,b], 3D scanning [Innmann et al. 2016; Izadi et al.
2011] and 3D scene understanding [Bleyer et al. 2012; Li et al. 2015;
Valentin et al. 2015]. Depth sensors are also widely used in robotics
[Ciliberto et al. 2012; Fanello et al. 2014; Gori et al. 2013] as well as
in virtual and augmented reality [Orts-Escolano et al. 2016]. Despite
their widespread use, depth sensors typically operate at 30fps – a
framerate optimized for color cameras in the motion picture industry. However, this low framerate poses considerable challenges to
computer vision applications because significant motion can occur
between consecutive temporal frames.
Coping with insufficient framerate. Many computer vision algorithms explicitly assume small frame-to-frame motion (e.g. optical
flow [Horn and Schunck 1981]) or implicitly by assuming that the
initial solution is close to the expected global optimum and can
be found by gradient based local optimization (e.g. ICP [Chen and
Medioni 1992]). When the small motion assumption is violated suboptimal solutions are produced. As a result, recent methods design
complex and compute-intensive pipelines in order to cope with high
frame-to-frame variations. For instance, recent parametric models
for non-rigid tracking [Taylor et al. 2016], use sophisticated reinitialization strategies to avoid getting stuck in local minima. Other
dynamic reconstruction pipelines [Dou et al. 2016; Orts-Escolano
et al. 2016] rely on fast frame-to-frame semantic correspondences
[Wang et al. 2016], increasing the overall compute. Despite these
efforts, these tracking systems still struggle to cope with the huge
search space as well as motion artifacts and appearance changes
that occur at lower framerate capture.
High-speed capture. Since the displacement of objects in an image
is linearly dependent on their speed and the sampling frequency of
the camera, large frame-to-frame motion problems can be attacked
by reducing the time interval between consecutive frames. Hence,
we note that employing high frame-rate depth cameras make numerous computer vision problems easier to solve. The advent of
high speed RGB sensors and IMUs in mobile phones and consumer
cameras have already paved the way towards robust 3D pose estimation; e.g. recently Kiani Galoogahi et al. [2017] acquired high
framerate datasets for 2D object tracking. Very recent work [Fanello
et al. 2017b] has shown a fast 3D capture camera that processes
depth maps in 2.5ms, however the method employs a single-mode
laser coupled with a Diffractive Optical Element (DOE) as active
illumination, similar to the one used in Kinect V1. These active illuminators have a very low SNR and therefore they struggle with low
reflectivity areas and large distances. As a consequence, without a
fairly high exposure time of the camera (i.e. 30 ms), the quality of
depth significantly decreases (see Figure 14). However, with such
a high exposure time motion artifacts are more easily introduced.
Thus, in practice, this system cannot produce a high frame rate
depth stream without sacrificing depth quality.
In this paper we address these issues: our 3D capture technology
is based on depth-from-stereo with active structured light infrared
illumination that leverages Vertical Cavity Surface Emitting Laser
(VCSEL) technology [Moench et al. 2016]. This allows for a strong
SNR even in low exposure settings, i.e. 1 − 2ms, enabling very high
framerates without sacrificing depth quality (see Figure 14). As result, we drastically reduce the difficulty of the non-rigid tracking
problems by leveraging high frame rate depth estimation. The overall computational budget required for high-framerate tracking itself
is comparable to tracking at low speeds (due to significantly lower
per frame computation costs), but with the benefit of significantly
more accurate tracking results.
Contributions. We propose a high speed, fast exposure, and low
latency dense capture pipeline for geometry in fast motion that runs
in 1.1ms end-to-end on an Nvidia TitanX. Our contributions are:
• We detail all off-the-shelf hardware components needed to
build a high speed and high quality 3D capture sensor.
• We contribute a hardware solution to effectively eliminate
depth bias inherent in active illumination stereo systems.
• We provide a practical solution to avoid lens flare induced by
the active illumination in multi-sensor setups.
• We extend Fanello et al. [2017a] to leverage temporal coherency with a complexity that is independent of patch size.
• We introduce a novel temporal prior to compute disparity.
• We devise a machine learning approach to refine the disparities by learning to efficiently invalidate outliers.
• We demonstrate the effectiveness of the system in many highlevel tracking applications.
Numerous quantitative and qualitative experiments prove the
effectiveness of the proposed applications for many challenging
capture and tracking scenarios.
ACM Transactions on Graphics, Vol. 37, No. 6, Article 220. Publication date: November 2018.
The Need 4 Speed in Real-Time Dense Visual Tracking • 220:3
distance from camera (meters)
frame to frame displacement (pixels)
30fps @ 33ms 30fps @ 2ms 210fps @ 2ms
frame (t+1) frame (t)
Fig. 2. (left) The screen space displacement (in pixels) between consecutive frames induced by a target object moving 12 m/s with respect to framerate and
distance from the camera. (right) We display consecutive RGB frames captured at different frame-rates and exposures for a subject moving a tennis ball at a
1.5m distance. Our high frame-rate and low exposure camera results in (1) small motions and (2) limited motion blur.
2 RELATED WORK
Solving general tracking problems is one of the most active areas in
computer vision. Generally speaking, tracking applications can be
categorized by the degrees of freedom (DOFs) of the problem. For
instance, in RGB images we may be interested in tracking the 2D
position of a particular object using a known model or a template
[Nakabo et al. 2000], or in a depth image we may want to infer
the 3D position of the object with respect to the camera [Stuhmer
et al. 2015]. Increasing the degrees of freedom also increases the
complexity of the problem, for example in camera localization tasks
the goal is to infer both position and orientation leading to a 6DOF
estimation problem [Forster et al. 2014; Izadi et al. 2011; Newcombe
et al. 2011]. Research has shown that, given a sufficiently high SNR,
high frame rates greatly simplifies tracking problems [Handa 2013;
Newcombe 2012]. For example, commercially available systems such
as VR headsets, successfully solve the camera pose problem with
a combination of higher speed RGB (IR) cameras and extremely
fast IMUs [RoadToVR 2016]. For tracking high-speed rigid motion
without motion blur, high-framerate pan/tilt cameras such as the
one proposed by Okumura et al. [2011] can be employed, but these
do not generalize to the generic tracking scenarios we consider.
Event cameras. Recently, event cameras have also shown very
promising results for localization and tracking problems [Kim et al.
2016; Rebecq et al. 2017; Reinbacher et al. 2017], showing again
the importance of high frame-rates for tracking purposes. However, when the degrees of freedom of the problem increase, RGB
sensors are not sufficient. For these applications, 3D sensors have
Table 1. Depth sensors commercially available compared with the proposed
solution. Notice how we can achieve the fastest frame rate without sacrificing resolution.
Sensor Max Res @ FPS Min Res @ FPS
Kinect v1 640 × 480@30FPS 640 × 480@30FPS
Kinect v2 512 × 424@30FPS 512 × 424@30FPS
DUO MLX 752 × 480@45FPS 320 × 120@320FPS
ZED 4416 × 1242@15FPS 1344 × 376@100FPS
Intel SR300 640 × 480@60FPS 640 × 480@60FPS
Intel D435 1280 × 720@30FPS 640 × 480@90FPS
Proposed 1280 × 1024@210FPS 640 × 512@800FPS
become the standard approach since they provide the additional
depth information that helps to resolve ambiguous cases.
High framerate systems. Some systems for high framerate depth
capture have been proposed in the literature [Gong and Zhang 2010;
Höfling et al. 2015; Hyun et al. 2017; Zhang et al. 2010; Zuo et al.
2013], but the required hardware is bulky and prohibitively expensive as they rely on dynamic patterns from high-speed projection
technology. Conversely, our system could be realized with off-theshelf hardware such as a $5 VSCEL and a $3 Sony IMX camera
typically used in mobile phones for slow-motion videos. The hybrid system recently proposed by Lu et al. [2017] combines a low
framerate depth sensors such as Kinect with a high speed RGB camera, producing 500fps depth with 20ms latency. The authors show
promising results for simple tracking applications, but the generated
depth maps exhibit high levels of noise, which is not acceptable for
precise tracking. Although higher speed consumer depth sensors
are available on the market, such as the DUO MLX and the ZED,
these use passive illumination, thus they produce high speed depth
at a much lower quality than our active sensor. Moreover passive
sensors do not work in low light conditions. Further, our system
captures images with 34× more pixels than the DUO MLX, and
2.5× more pixels than the ZED; see Table 1 for a comparison with
commercially available sensors. Recent works in depth estimation
[Fanello et al. 2016, 2017b; Keselman et al. 2017] show that triangulation systems can achieve high quality results with very low compute.
In particular, Fanello et al. [2017b] presented a 210Hz depth camera,
but this system suffers significant limitations when capturing fast
motion; see Section 6. To cope with these shortcomings, we propose
a novel hardware solution that uses VCSEL technology as opposed
to traditional single-mode lasers and DOE systems (such as in StructureIO). In contrast to single mode lasers, we use VCSELs which
are more efficient in terms of power consumption. We demonstrate
better SNR performance using this hardware allowing for a lower
exposure time and a reduction in generated motion artifacts. On
the algorithmic side, we extend [Fanello et al. 2017b] with a novel
space-time stereo matching scheme.
Space-time stereo matching. Previous work [Davis et al. 2005;
Zhang et al. 2003] cast the task of depth estimation as a space-time
ACM Transactions on Graphics, Vol. 37, No. 6, Article 220. Publication date: November 2018.
220:4 • Kowdle, Rhemann, Fanello et al.
stereo matching task. The general idea consists of aggregating the
matching cost across both space and time. These methods focus
on high quality reconstruction, therefore they pay little consideration to the overall running time. As a consequence, they use an
exhaustive disparity search across both the spatial and temporal
domain, making them intractable for real-time performances. In
addition, these methods fail for fast motion, which can result in
inconsistent observations in the left and right cameras. In contrast
to previous work [Davis et al. 2005; Zhang et al. 2003], our computational requirements do not increase when employing a temporal
window. Indeed, our matching cost is independent of the window
size. Moreover, given the high speed of the cameras and the capability of processing all the frames in real-time, we are more robust to
fast motion. Finally, disparity optimization is carried out in parallel,
as opposed to the sequential PatchMatch-like search of [Fanello
et al. 2017b].
3 OVERVIEW
In the following sections, we show how to build the first complete
system, from hardware design (Sec. 4) to software implementation
(Sec. 5), capable of capturing depth images at high framerate, resolution and quality without artifacts typically caused by interference
between multiple sensors being operated simultaneously. We qualitatively and quantitatively evaluate these improvements (Sec. 6),
highlight how they significantly impact a number of real-time dense
tracking applications (Sec. 7), and present the applicability of our
cameras to low-latency passthrough in mixed reality scenarios
(Sec. 8).
4 SENSOR BLUEPRINT – HARDWARE
Commercially successful commodity depth sensors based on active
illumination mostly fall into these categories: Time-of-Flight (TOF),
Temporal Structured Light (TSL), Spatial Structured Light (SSL), and
Active Stereo (AS); see Appendix A for a detailed discussion. Due to
temporal integration, TOF and TSL cameras either require extremely
high (2000fps) framerate projector/cameras, or they result in systematic artifacts which researchers have recently been attempting
to resolve [Gupta et al. 2015; O’Toole et al. 2014]. SSL sensors can
theoretically provide high-frame rates, but they severely interfere
with each other, hence limiting their applicability. AS sensors do not
suffer any of these shortcomings, and recent work has substantially
reduced their computational burden [Fanello et al. 2017b]. For these
reasons, we selected Active Stereo as our sensor architecture.
Camera choice → OnSemi Python 1300. The ability to successfully
track a fast moving object is heavily influenced by the resulting
motion displacement between image frames as well as induced motion blur. Large displacements and motion blur make tracking more
unstable, and in some cases intractable. The amount of displacement and motion blur can be controlled by the camera framerate
and exposure time, respectively. Ideally, one would desire having
both high frame-rates and low exposure times, but these values are
constrained by capabilities of commodity cameras and illuminators.
Thus, our camera module should be a (1) commercially available
off-the-shelf camera, (2) be easily interfaced with, (3) be capable
of achieving high frame rates, and (4) be a high resolution global
Fig. 3. (left) In the additional materials we provide CAD fabrication models for our active stereo camera systems at different baselines. (right) The
55mm baseline for exo-centric short range interaction (depth only), 80mm
baseline for ego-centric VR headsets (depth+RGB) and hybrid 200mm/55mm
for long range (full body, depth+RGB) capture.
shutter sensor with good quantum efficiency in the infrared (IR)
spectrum.
In order to determine a sufficiently high frame rate and exposure
time, we work backwards by considering the extremely high speed
motions that occur in full body sports scenarios and from rapid
hand movement. In particular, we consider a punch from a boxer as
well as the snap of a finger. These motion trajectories correspond
roughly to a velocity of 12 m/s. We consider a camera with a field of
view of 65 degrees, SXGA resolution that captures at 30fps with an
exposure time of 33ms – typical values in commercial sensors like
the Kinect v1. When the subject is 1m away from the camera, the
induced motion blur and screen-space displacement is about 500
pixels, with 500 pixels of motion blur between the two consecutive
frames, ultimately making frame-to-frame tracking very difficult;
see Figure 2. Taking these aspects into consideration, we considered
the spectrum of widely available off-the-shelf high speed cameras
and selected the OnSemi Python1300 packaged as a USB3 module by
Ximea. This infrared sensor is capable of achieving a framerate of
210fps at a spatial resolution of 1280 × 1024 with a 1ms exposure
time1. In the previously detailed scenario, this results in an 80 pixels
displacement and only 16 pixels of motion blur; see Figure 2 and
Section 7. While the Ximea scientific camera module is relatively
expensive at around ≈ $800, we could also leverage mobile sensors such as the Sony IMX. This sensor has a better SNR/Quantum
efficiency and framerate than the Python1300, and a cost of only
≈ $3 (excluding lenses). However, there are still no devkits available
for these cameras, therefore the final users are required to write
their own custom drivers and build appropriate data transfer interfaces. We note that the choice of the sensor in this work is
purely a function of the availability of off-the-shelf hardware. When
designing for an actual product these would have to be considered
1Note that our current software stack and algorithms support future releases of the
sensors up to 1000 fps at SXGA resolution.
ACM Transactions on Graphics, Vol. 37, No. 6, Article 220. Publication date: November 2018.
The Need 4 Speed in Real-Time Dense Visual Tracking • 220:5
Error (mm)
Depth (mm)
Fig. 4. We target an average expected error of 4mm to select our baselines
(constrained by cameras form factors). Our hybrid 50mm/200mm capture
pod delivers an excellent tradeoff of range vs. precision.
ground up. For instance, we were bound by the USB bandwidth for
the data transfer, but there are sensors that are available such as the
Sony IMX range of sensors that are capable of upwards of 250fps
via a MIPI interface that would be more amenable when designing such a depth sensor from ground up. Note that such sensors
are already mass market, and are available on most recent mobile
devices to allow for slow-motion video capture. One of the most
recent sensors on the Samsung Galaxy S9 range of mobile devices
boasts of a ultra high speed 12 Megapixel sensor capable of 720p at
960fps exemplifying the availability of high speed sensors on mobile
platforms.
Stereo module → baseline and resolution. We mount two Python
1300 sensors in a rigid machined aluminum housing; see Figure 3-
(left). The depth precision of a stereo system is governed by the
baseline, camera resolution and the focal length. Due to our sensor
choice we fixed the maximum resolution at 1280 × 1024 pixels and
focus on optimizing the baseline; see Figure 4. In Section 4.3 we
detail the choices of lenses that in turn affect the system resolution.
In stereo systems, a larger baseline delivers more precise measurements but results in larger occlusions and foreshortening, which
makes the matching process harder for objects close to the camera. Therefore, depending on the tracking scenario we use different
camera baselines. Our specific choices of the baselines were driven
by the sizes of the modules. In the case of a desktop scenario, for
applications such as hand tracking, we chose a baseline similar to
products such as Leap Motion while ensuring that we can pack the
cameras in a tight configuration, which gave us a baseline of 55mm.
In the case of a head-mounted scenario where we would want to
capture close interactions such as the hands of the person wearing
the headset while being able to reconstruct the room the user is
in, we designed the baseline to closely match those of room scale
stereo systems, such as Kinect and StructureIO, while keeping the
cameras as close to each other as possible, resulting in a baseline
of 80mm. Lastly, we built a 200mm baseline pod that allows us to
capture the full-body of a person in the center of a multi-sensor rig;
see Fig. 3. This baseline allows us to reduce the depth error further
away from the camera.
Illuminator → Infrared VCSEL. Effective stereo matching requires
the scene to be textured with a locally unique pattern. One choice
for the illuminator is a low power single mode laser coupled with
a Diffractive Optical Element (DOE) and replicator stack that can
generate a pseudo random pattern. This approach is used in commercial depth sensors such as Microsoft Kinect V1, StructureIO,
Microsoft Kinect v1 Single VCSEL Twin VCSEL (ours)
Fig. 5. Illumination patterns used by active stereo and structured light
system. The Kinect uses a DOE pseudo-random dot pattern, while we use a
VCSEL. This can result in spatial repetitions in the pattern, which we avoid
by employing two illuminators slightly rotated with respect to each other.
and in [Fanello et al. 2017b; Orts-Escolano et al. 2016]. However,
for short exposure times (1 - 5 ms) such an illuminator does not
offer enough SNR for low reflective surfaces and hence results in
fairly sparse depth maps. Unfortunately, higher power lasers cannot be used together with a DOE due to eye safety reasons [OSHA
2017]. Therefore, we leverage the recent success in VCSEL-based IR
illumination used in structured light projectors such as the Google
Tango tablet and Mantis Vision scanners. A VCSEL emits the light
from a much bigger surface than a DOE and therefore can project
much more light while still being eye-safe. VCSELs have also been
shown to be more efficient than lasers [D’Asaro et al. 2016] and can
emit structured light when coupled with a suitable mask; e.g. see
Google Tango and Mantis Vision. Conversely, we propose a simpler
solution: we use a pair of VCSELs – each of them generates a regular
grid of dots. One of the VCSELs is slightly rotated with respect to
the other so that the combination of the two patterns results in the
locally unique pattern shown in Figure 5. In order to maximize the
SNR captured within the short exposure time we additionally pulse
the illuminator in sync with the camera in such a way to reduce
contamination from ambient light. By pulsing the illuminator we
can reduce the exposure time of the camera to 2ms, as well as the
energy consumption due to a reduced duty cycle.
4.1 Multi-sensor and multi-illuminator setups
Multiview tracking scenarios require multiple depth cameras to
work together without interference. Our active stereo system is
naturally robust as the only assumption that we make about the
projected pattern is that it is locally unique. As the combination
of two unique patterns results in another unique pattern, multiple
depth cameras can actively illuminate the same surface without
causing interference. However, two depth cameras that directly face
each other can result in saturation in parts of the image due to lens
flare – scattering of light from one illuminator in the lens of the
other camera. In order to mitigate this effect, we pulse the VCSEL
projectors. Recall that due to pulsing each illuminator and camera
is only active for 2ms. If the cameras run at 210fps we will have a
gap of 4.75ms between two consecutive frames. This means that we
can temporally offset the exposures of cameras such that they do
not interfere, while still maintaining the target frame rate of 210fps.
ACM Transactions on Graphics, Vol. 37, No. 6, Article 220. Publication date: November 2018.
220:6 • Kowdle, Rhemann, Fanello et al.
static pattern dynamic pattern
Fig. 6. (left) A constant structured light pattern results in visible depth
bias for static scenes. (right) A temporally dynamic pattern combined with
spatio-temporal matching windows corrects these artifacts.
4.2 Dynamic pattern – bias correction
Triangulation methods that make use of spatial active illumination
suffer from bias due to the particular structure of the projected
dot pattern; see Figure 6-(left). Even when we aggregate multiple
frames over time, the structured nature of the noise generates errors
that cannot be mitigated via simple averaging schemes. Butler et al.
[2012] showed that bias and interference in standard structured light
can be reduced through small motion of the illuminator. We implement a temporally varying pattern by using a four VCSEL setup,
where each is mounted at a slightly different angle, and randomly
flashing
4
2

+
4
3

combinations in succession; We implemented this
solution in our 200mm baseline pod, which has enough space to
accommodate four VCSEL illuminators; see Figure 3-(bottom right).
The results of this process are showed in Figure 6 where one should
notice the absence of artifacts on the body, and the high frequency
details of the face. Also note that the reconstruction is more complete, especially in low SNR regions such as hair.
4.3 Hardware components
Lenses. The camera lens affects the field of view and hence the
stereo system resolution. Sourcing lenses with exact characteristics
for F number, focal length, field of view etc. is a cumbersome task
due to limited availability of options. While the right approach is
to build custom lenses designed for the specific scenario, building
such custom lenses turns into a very expensive endeavor. In our
depth camera, all the options we built have an M12 or an S-mount
lens holder. We therefore resort to off-the-shelf M12 lenses from
companies such as Megapixel Lenses, Uxcell and Lensation. We
provide three different solutions, that in our experience, cover most
of the high-level computer vision and HCI applications we developed. For short-range scenarios we found that a 60 deg field of view
lens is satisfactory. This includes applications like gesture recognition in front of a laptop and object scanning. We used a 6mm lens
from Megapixel Lenses for this scenario (model number 130620MP).
In ego-centric scenarios, such as in VR applications, wide field of
view lenses are preferred, therefore we selected a 3.6mm Uxcell lens
(model number US-SA-AJD-69585) that can cover up to a 90 deg
field of view. Finally for full body capture applications [Dou et al.
2017] we use a wide field of view (80 deg) lens with low distortion
which provides high quality capture results. For this case we used a
Lensation lens (model number BSM6016S12).
Fig. 7. (left) Underlying PCB. (right) VCSEL illuminator pair where two
illuminator modules can be soldered in series and driven using one constant
current LED driver.
Illuminators. The choice of the type of illuminator used for the
active stereo sensor is also an important one that defines the quality
of depth obtained. There are a number of options when deciding the
illuminator to use. Our initial experiments were using a commonly
used illuminator that involves a DOE and an edge emitting laser
(as in Kinect v1, StructureIO). However, our observation was that
at very low exposure times, as required by a high speed depth
camera the SNR of such illuminators was very poor. In addition,
these illuminators cannot be driven at high currents as this takes it
away from an eye-safe IR margin. In our case we use a VCSEL based
illuminator by Heptagon called Lima. This consists of an array of
VCSELs operating at a wavelength of 850nm and a microlens array
above it that produces a regular grid of dots as shown in Figure 5.
In contrast to the DOE based solutions (Kinect v1 and StructureIO)
the dots produced by the microlens array has a much larger pixel
footprint, specifically the spot size from the Lima is about 2 to 3
times wider than that of a Kinect v1; see Figure 5. The larger spot
size however provides more signal for matching in the active stereo
setup. The field of illumination of the Lima illuminator is around 85
degree diagonal that closely matches the field of view of the lenses.
As discussed, we reduce the ambiguity in stereo matches by using
two Lima modules rotated with respect to each other. These are
soldered onto PCB shown in Figure 7 such that the illuminators are
connected in series. The illuminator pair is then driven by a 1000mA
LuxDrive BuckBlock, which is a constant current LED driver with a
dimmer that allows us to pulse the illuminator synchronized with
the camera exposure. We note that more recently illuminators that
use a VCSEL along with a DOE have been proposed, such as the
Apple iPhoneX, however these are not available off-the-shelf. These
could, however, be viable alternatives. At high frame rates, thermal
and power considerations are important. In all our experiments,
we power the illuminator in sync with the short exposure time of
the camera, thus ensuring that the duty cycle is low and the power
consumption is kept in check. In practice, the scenario the sensor is
being applied to, defines the form-factor and power limitations of
the system. For instance, a room scale sensor could have a larger
form factor and incorporate large heat sinks if the system needs
to be driven at a high power, whereas for a mobile form-factor we
have seen a viable solution evident in the iPhoneX.
IR Filters. Given the lens and the illuminators above, the last
part for the IR optical path is the filter. We need a filter that has
ACM Transactions on Graphics, Vol. 37, No. 6, Article 220. Publication date: November 2018.