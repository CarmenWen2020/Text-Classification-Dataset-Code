Editing talking-head video to change the speech content or to remove filler
words is challenging. We propose a novel method to edit talking-head video
based on its transcript to produce a realistic output video in which the
dialogue of the speaker has been modified, while maintaining a seamless
audio-visual flow (i.e. no jump cuts). Our method automatically annotates
an input talking-head video with phonemes, visemes, 3D face pose and
geometry, reflectance, expression and scene illumination per frame. To edit
a video, the user has to only edit the transcript, and an optimization strategy
then chooses segments of the input corpus as base material. The annotated
parameters corresponding to the selected segments are seamlessly stitched
together and used to produce an intermediate video representation in which
the lower half of the face is rendered with a parametric face model. Finally,
a recurrent video generation network transforms this representation to a
photorealistic video that matches the edited transcript. We demonstrate a
large variety of edits, such as the addition, removal, and alteration of words,
as well as convincing language translation and full sentence synthesis.
CCS Concepts: • Information systems → Video search; Speech / audio
search; • Computing methodologies → Computational photography; Reconstruction; Motion processing; Graphics systems and interfaces.
Additional Key Words and Phrases: Text-based video editing, talking heads,
visemes, dubbing, face tracking, face parameterization, neural rendering.
1 INTRODUCTION
Talking-head video – framed to focus on the face and upper body
of a speaker – is ubiquitous in movies, TV shows, commercials,
YouTube video logs, and online lectures. Editing such pre-recorded
video is challenging, but can be needed to emphasize particular
content, remove filler words, correct mistakes, or more generally
match the editor’s intent. Using current video editing tools, like
Adobe Premiere, skilled editors typically scrub through raw video
footage to find relevant segments and assemble them into the desired
story. They must carefully consider where to place cuts so as to
minimize disruptions to the overall audio-visual flow.
ACM Trans. Graph., Vol. 38, No. 4, Article 68. Publication date: July 2019.
68:2 • Fried, O. et al
Berthouzoz et al. [2012] introduce a text-based approach for editing such videos. Given an input video, they obtain a time-aligned
transcript and allow editors to cut and paste the text to assemble it
into the desired story. Their approach can move or delete segments,
while generating visually seamless transitions at cut boundaries.
However, this method only produces artifact-free results when these
boundaries are constrained to certain well-behaved segments of the
video (e.g. where the person sits still between phrases or sentences).
Neither conventional editing tools nor the text-based approach
allow synthesis of new audio-visual speech content. Thus, some
modifications require either re-shooting the footage or overdubbing
existing footage with new wording. Both methods are expensive
as they require new performances, and overdubbing generally produces mismatches between the visible lip motion and the audio.
This paper presents a method that completes the suite of operations necessary for transcript-based editing of talking-head video.
Specifically, based only on text edits, it can synthesize convincing
new video of a person speaking, and produce seamless transitions
even at challenging cut points such as the middle of an utterance.
Our approach builds on a thread of research for synthesizing
realistic talking-head video. The seminal Video Rewrite system of
Bregler et al. [1997] and the recent Synthesizing Obama project of
Suwajanakorn et al. [2017] take new speech recordings as input,
and superimpose the corresponding lip motion over talking-head
video. While the latter state-of-the art approach can synthesize fairly
accurate lip sync, it has been shown to work for exactly one talking
head because it requires huge training data (14 hours). This method
also relies on input audio from the same voice on which it was
trained – from either Obama or a voice impersonator. In contrast
our approach works from text and therefore supports applications
that require a different voice, such as translation.
Performance-driven puppeteering and dubbing methods, such as
VDub [Garrido et al. 2015], Face2Face [Thies et al. 2016] and Deep
Video Portraits [Kim et al. 2018b], take a new talking-head performance (usually from a different performer) as input and transfer
the lip and head motion to the original talking-head video. Because
these methods have access to video as input they can often produce
higher-quality synthesis results than the audio-only methods. Nevertheless, capturing new video for this purpose is obviously more
onerous than typing new text.
Our method accepts text only as input for synthesis, yet builds on
the Deep Video Portraits approach of Kim et al. [2018b] to craft synthetic video. Our approach drives a 3D model by seamlessly stitching
different snippets of motion tracked from the original footage. The
snippets are selected based on a dynamic programming optimization
that searches for sequences of sounds in the transcript that should
look like the words we want to synthesize, using a novel visemebased similarity measure. These snippets can be re-timed to match
the target viseme sequence, and are blended to create a seamless
mouth motion. To synthesize output video, we first create a synthetic composite video in which the lower face region is masked out.
In cases of inserting new text, we retime the rest of the face and background from the boundaries. The masked out region is composited
with a synthetic 3D face model rendering using the mouth motion
found earlier by optimization (Figure 5). The composite exhibits the
desired motion, but lacks realism due to the incompleteness and
imperfections of the 3D face model. For example, facial appearance
does not perfectly match, dynamic high-frequency detail is missing,
and the mouth interior is absent. Nonetheless, these data are sufficient cues for a new learned recurrent video generation network
to be able to convert them to realistic imagery. The new composite
representation and the recurrent network formulation significantly
extend the neural face translation approach of Kim et al. [2018b] to
text-based editing of existing videos.
We show a variety of text-based editing results and favorable
comparisons to previous techniques. In a crowd-sourced user study,
our edits were rated to be real in 59.6% of cases. The main technical
contributions of our approach are:
• A text-based editing tool for talking-head video that lets editors insert new text, in addition to cutting and copy-pasting
in an existing transcript.
• A dynamic programming based strategy tailored to video synthesis that assembles new words based on snippets containing
sequences of observed visemes in the input video.
• A parameter blending scheme that, when combined with our
synthesis pipeline, produces seamless talking heads, even
when combining snippets with different pose and expression.
• A recurrent video generation network that converts a composite of real background video and synthetically rendered
lower face into a photorealistic video.
1.1 Ethical Considerations
Our text-based editing approach lays the foundation for better editing tools for movie post production. Filmed dialogue scenes often
require re-timing or editing based on small script changes, which
currently requires tedious manual work. Our editing technique also
enables easy adaptation of audio-visual video content to specific
target audiences: e.g., instruction videos can be fine-tuned to audiences of different backgrounds, or a storyteller video can be adapted
to children of different age groups purely based on textual script
edits. In short, our work was developed for storytelling purposes.
However, the availability of such technology — at a quality that
some might find indistinguishable from source material — also raises
important and valid concerns about the potential for misuse. Although methods for image and video manipulation are as old as the
media themselves, the risks of abuse are heightened when applied
to a mode of communication that is sometimes considered to be
authoritative evidence of thoughts and intents. We acknowledge
that bad actors might use such technologies to falsify personal statements and slander prominent individuals. We are concerned about
such deception and misuse.
Therefore, we believe it is critical that video synthesized using
our tool clearly presents itself as synthetic. The fact that the video
is synthesized may be obvious by context (e.g. if the audience understands they are watching a fictional movie), directly stated in
the video or signaled via watermarking. We also believe that it is essential to obtain permission from the performers for any alteration
before sharing a resulting video with a broad audience. Finally, it is
important that we as a community continue to develop forensics,
fingerprinting and verification techniques (digital and non-digital)
to identify manipulated video. Such safeguarding measures would
ACM Trans. Graph., Vol. 38, No. 4, Article 68. Publication date: July 2019.
Text-based Editing of Talking-head Video • 68:3
reduce the potential for misuse while allowing creative uses of video
editing technologies like ours.
We hope that publication of the technical details of such systems
can spread awareness and knowledge regarding their inner workings, sparking and enabling associated research into the aforementioned forgery detection, watermarking and verification systems.
Finally, we believe that a robust public conversation is necessary to
create a set of appropriate regulations and laws that would balance
the risks of misuse of these tools against the importance of creative,
consensual use cases.
2 RELATED WORK
Facial Reenactment. Facial video reenactment has been an active area of research [Averbuch-Elor et al. 2017; Garrido et al. 2014;
Kemelmacher-Shlizerman et al. 2010; Li et al. 2014; Liu et al. 2001;
Suwajanakorn et al. 2017; Vlasic et al. 2005]. Thies et al. [2016]
recently demonstrated real-time video reenactment. Deep video
portraits [Kim et al. 2018b] enables full control of the head pose, expression, and eye gaze of a target actor based on recent advances in
learning-based image-to-image translation [Isola et al. 2017]. Some
recent approaches enable the synthesis of controllable facial animations from single images [Averbuch-Elor et al. 2017; Geng et al.
2018; Wiles et al. 2018]. Nagano et al. [2018] recently showed how
to estimate a controllable avatar of a person from a single image. We
employ a facial reenactment approach for visualizing our text-based
editing results and show how facial reenactment can be tackled by
neural face rendering.
Visual Dubbing. Facial reenactment is the basis for visual dubbing, since it allows to alter the expression of a target actor to
match the motion of a dubbing actor that speaks in a different language. Some dubbing approaches are speech-driven [Bregler et al.
1997; Chang and Ezzat 2005; Ezzat et al. 2002; Liu and Ostermann
2011] others are performance-driven [Garrido et al. 2015]. Speechdriven approaches have been shown to produce accurate lip-synced
video [Suwajanakorn et al. 2017]. While this approach can synthesize fairly accurate lip-synced video, it requires the new audio to
sound similar to the original speaker, while we enable synthesis
of new video using text-based edits. Mattheyses et al. [2010] show
results with no head motion, in a controlled setup with uniform
background. In contrast, our 3D based approach and neural renderer
can produce subtle phenomena such as lip rolling, and works in a
more general setting.
Speech animation for rigged models. Several related methods produce animation curves for speech [Edwards et al. 2016; Taylor et al.
2017; Zhou et al. 2018]. They are specifically designed for animated
3D models and not for photorealistic video, requiring a character rig
and artist supplied rig correspondence. In contrast, our approach
“animates” a real person speaking, based just on text and a monocular
recording of the subject.
Text-Based Video and Audio Editing. Researchers have developed
a variety of audio and video editing tools based on time-aligned transcripts. These tools allow editors to shorten and rearrange speech for
audio podcasts [Rubin et al. 2013; Shin et al. 2016], annotate video
with review feedback [Pavel et al. 2016], provide audio descriptions
of the video content for segmentation of B-roll footage [Truong et al.
2016] and generate structured summaries of lecture videos [Pavel
et al. 2014]. Leake et al. [2017] use the structure imposed by timealigned transcripts to automatically edit together multiple takes of
a scripted scene based on higher-level cinematic idioms specified
by the editor. Berthouzoz et al.’s [2012] tool for editing interviewstyle talking-head video by cutting, copying and pasting transcript
text is closest to our work. While we similarly enable rearranging
video by cutting, copying and pasting text, unlike all of the previous
text-based editing tools, we allow synthesis of new video by simply
typing the new text into the transcript.
Audio Synthesis. In transcript-based video editing, synthesizing
new video clips would often naturally be accompanied by audio
synthesis. Our approach to video is independent of the audio, and
therefore a variety of text to speech (TTS) methods can be used.
Traditional TTS has explored two general approaches: parametric
methods (e.g. [Zen et al. 2009]) generate acoustic features based
on text, and then synthesize a waveform from these features. Due
to oversimplified acoustic models, they tend to sound robotic. In
contrast, unit selection is a data driven approach that constructs
new waveforms by stitching together small pieces of audio (or units)
found elsewhere in the transcript [Hunt and Black 1996]. Inspired by
the latter, the VoCo project of Jin et al. [2017] performs a search in the
existing recording to find short ranges of audio that can be stitched
together such that they blend seamlessly in the context around an
insertion point. Section 4 and the accompanying video present a few
examples of using our method to synthesize new words in video,
coupled with the use of VoCo to synthesize corresponding audio.
Current state-of-the-art TTS approaches rely on deep learning [Shen
et al. 2018; Van Den Oord et al. 2016]. However, these methods
require a huge (tens of hours) training corpus for the target speaker.
Deep Generative Models. Very recently, researchers have proposed
Deep Generative Adversarial Networks (GANs) for the synthesis of
images and videos. Approaches create new images from scratch
[Chen and Koltun 2017; Goodfellow et al. 2014; Karras et al. 2018;
Radford et al. 2016; Wang et al. 2018b] or condition the synthesis
on an input image [Isola et al. 2017; Mirza and Osindero 2014].
High-resolution conditional video synthesis [Wang et al. 2018a]
has recently been demonstrated. Besides approaches that require
a paired training corpus, unpaired video-to-video translation techniques [Bansal et al. 2018] only require two training videos. Video-tovideo translation has been used in many applications. For example,
impressive results have been shown for the reenactment of the human head [Olszewski et al. 2017], head and upper body [Kim et al.
2018b], and the whole human body [Chan et al. 2018; Liu et al. 2018].
Monocular 3D Face Reconstruction. There is a large body of work
on reconstructing facial geometry and appearance from a single image using optimization methods [Fyffe et al. 2014; Garrido et al. 2016;
Ichim et al. 2015; Kemelmacher-Shlizerman 2013; Roth et al. 2017;
Shi et al. 2014; Suwajanakorn et al. 2017; Thies et al. 2016]. Many
of these techniques employ a parametric face model [Blanz et al.
2004; Blanz and Vetter 1999; Booth et al. 2018] as a prior to better
constrain the reconstruction problem. Recently, deep learning-based
approaches have been proposed that train a convolutional network
ACM Trans. Graph., Vol. 38, No. 4, Article 68. Publication date: July 2019.
68:4 • Fried, O. et al
The quick brown ...
Input video
DH IY0 K W IH1 K sp B R AW1 N
Phoneme Alignment Tracking & Reconstruction
The quick brown
spider jumped
Edit Operation
fox
The quick brown spider ...
 ... viper ... ox ...
f ox Viseme Search
brown v ox jumped
Parameter Blending
Head
Parameters Model
Rendering
Preprocessing Background
Retiming
Fig. 2. Method overview. Given an input talking-head video and a transcript, we perform text-based editing. We first align phonemes to the input audio and
track each input frame to construct a parametric head model. Then, for a given edit operation (changing spider to fox), we find segments of the input video
that have similar visemes to the new word. In the above case we use viper and ox to construct fox. We use blended head parameters from the corresponding
video frames, together with a retimed background sequence, to generate a composite image, which is used to generate a photorealistic frame using our neural
face rendering method. In the resulting video, the actress appears to be saying fox, even though that word was never spoken by her in the original recording.
to directly regress the model parameters [Dou et al. 2017; Genova
et al. 2018; Richardson et al. 2016; Tewari et al. 2018a, 2017; Tran
et al. 2017]. Besides model parameters, other approaches regress
detailed depth maps [Richardson et al. 2017; Sela et al. 2017], or 3D
displacements [Cao et al. 2015; Guo et al. 2018; Tewari et al. 2018b].
Face reconstruction is the basis for a large variety of applications,
such as facial reenactment and visual dubbing. For more details on
monocular 3D face reconstruction, we refer to Zollhöfer et al. [2018].
3 METHOD
Our system takes as input a video recording of a talking head with a
transcript of the speech and any number of edit operations specified
on the transcript. Our tool supports three types of edit operations;
• Add new words: the edit adds one or more consecutive words
at a point in the video (e.g. because the actor skipped a word
or the producer wants to insert a phrase).
• Rearrange existing words: the edit moves one or more consecutive words that exist in the video (e.g. for better word
ordering without introducing jump cuts).
• Delete existing words: the edit removes one or more consecutive words from the video (e.g. for simplification of wording
and removing filler such as “um” or “uh”).
We represent editing operations by the sequence of words W in
the edited region as well as the correspondence between those
words and the original transcript. For example, deleting the word
“wonderful” in the sequence “hello wonderful world” is specified as
(‘hello’, ‘world’) and adding the word “big” is specified as (‘hello’,
‘big’, ‘world’).
Our system processes these inputs in five main stages (Figure 2).
In the phoneme alignment stage (Section 3.1) we align the transcript
to the video at the level of phonemes and then in the tracking and
reconstruction stage (Section 3.2) we register a 3D parametric head
model with the video. These are pre-processing steps performed
once per input video. Then for each edit operation W we first
perform a viseme search (Section 3.3) to find the best visual match
between the subsequences of phonemes in the edit and subsequences
of phonemes in the input video. We also extract a region around the
edit location to act as a background sequence, from which we will
extract background pixels and pose data. For each subsequence we
blend the parameters of the tracked 3D head model (Section 3.4) and
then use the resulting parameter blended animation of the 3D head,
together with the background pixels, to render a realistic full-frame
video (Section 3.5) in which the subject appears to say the edited
sequence of words. Our viseme search and approach for combining
shorter subsequences with parameter blending is motivated by the
phoneme/viseme distribution of the English language (Appendix A).
3.1 Phoneme Alignment
Phonemes are perceptually distinct units that distinguish one word
from another in a specific language. Our method relies on phonemes
to find snippets in the video that we later combine to produce new
content. Thus, our first step is to compute the identity and timing
of phonemes in the input video. To segment the video’s speech
audio into phones (audible realizations of phonemes), we assume
we have an accurate text transcript and align it to the audio using
P2FA [Rubin et al. 2013; Yuan and Liberman 2008], a phoneme-based
alignment tool. This gives us an ordered sequence V = (v1, . . . , vn)
of phonemes, each with a label denoting the phoneme name, start
time, and end time vi = (v
lbl
i
, v
in
i
, v
out
i
). Note that if a transcript is not
given as part of the input, we can use automatic speech transcription
tools [IBM 2016; Ochshorn and Hawkins 2016] or crowdsourcing
transcription services like rev.com to obtain it.
3.2 3D Face Tracking and Reconstruction
We register a 3D parametric face model with each frame of the input
talking-head video. The parameters of the model (e.g. expression,
head pose, etc.) will later allow us to selectively blend different aspects of the face (e.g. take the expression from one frame and pose
ACM Trans. Graph., Vol. 38, No. 4, Article 68. Publication date: July 2019.
Text-based Editing of Talking-head Video • 68:5
from another). Specifically, we apply recent work on monocular
model-based face reconstruction [Garrido et al. 2016; Thies et al.
2016]. These techniques parameterize the rigid head pose T ∈ SE(3),
the facial geometry α ∈ R
80, facial reflectance β ∈ R
80, facial expression δ ∈ R
64, and scene illumination γ ∈ R
27. Model fitting is
based on the minimization of a non-linear reconstruction energy.
For more details on the minimization, please see the papers of Garrido et al. [2016] and Thies et al. [2016]. In total, we obtain a 257
parameter vector p ∈ R
257 for each frame of the input video.
3.3 Viseme Search
Given an edit operation specified as a sequence of words W, our
goal is to find matching sequences of phonemes in the video that can
be combined to produce W. In the matching procedure we use the
fact that identical phonemes are expected to be, on average, more
visually similar to each other than non-identical phonemes (despite
co-articulation effects). We similarly consider visemes, groups of
aurally distinct phonemes that appear visually similar to one another
(Section 3.3), as good potential matches. Importantly, the matching
procedure cannot expect to find a good coherent viseme sequence in
the video for long words or sequences in the edit operation. Instead,
we must find several matching subsequences and a way to best
combine them.
We first convert the edit operation W to a phoneme sequence
W = (w1, . . . , wm) where each wi
is defined as (wlbl
i
, win
i
, wout
i
)
similar to our definition of phonemes in the video vi
. We can convert
the text W to phoneme labels wlbl
i
using a word to phoneme map,
but text does not contain timing information win
i
, wout
i
. To obtain
timings we use a text-to-speech synthesizer to convert the edit into
speech. For all results in this paper we use either the built-in speech
synthesizer in Mac OS X, or Voco [Jin et al. 2017]. Note however
that our video synthesis pipeline does not use the audio signal, but
only its timing. So, e.g., manually specified phone lengths could be
used as an alternative. The video generated in the rendering stage
of our pipeline (Section 3.5) is mute and we discuss how we can add
audio at the end of that section. Given the audio of W, we produce
phoneme labels and timing using P2FA, in a manner similar to the
one we used in Section 3.1.
Given an edit W and the video phonemes V, we are looking for the
optimal partition of W into sequential subsequences W1, . . . , Wk
,
such that each subsequence has a good match in V, while encouraging subsequences to be long (Figure 4). We are looking for long
subsequences because each transition between subsequences may
cause artifacts in later stages. We first describe matching one subsequence Wi = (wj
, . . . , wj+k
) to the recording V, and then explain
how we match the full query W.
Matching one subsequence. We define Cmatch(Wi
, V⋆) between a
subsequence of the query Wi and some subsequence of the video V⋆
as a modified Levenshtein edit distance [Levenshtein 1966] between
phoneme sequences that takes phoneme length into account. The
edit distance requires pre-defined costs for insertion, deletion and
swap. We define our insertion cost Cinsert = 1 and deletion cost
Cdelete = 1 and consider viseme and phoneme labels as well as
Table 1. Grouping phonemes (listed as ARPABET codes) into visemes. We
use the viseme grouping of Annosoft’s lipsync tool [Annosoft 2008]. More
viseme groups may lead to better visual matches (each group is more specific
in its appearance), but require more data because the chance to find a viseme
match decreases. We did not perform an extensive evaluation of different
viseme groupings, of which there are many.
v01 AA0, AA1, AA2 v09 Y, IY0, IY1, IY2
v02 AH0, AH1, AH2, HH v10 R, ER0, ER1, ER2
v03 AO0, AO1, AO2 v11 L
v04 AW0, AW1, AW2, OW0, v12 W
OW1, OW2 v13 M, P, B
v05 OY0, OY1, OY2, UH0, UH1, v14 N, NG, DH, D, G,
UH2, UW0, UW1, UW2 T, Z, ZH, TH, K, S
v06 EH0, EH1, EH2, AE0, AE1, AE2 v15 CH, JH, SH
v07 IH0, IH1, IH2, AY0, AY1, AY2 v16 F, V
v08 EY0, EY1, EY2 v17 sp
phoneme lengths in our swap cost
Cswap(vi
, wj) = Cvis(vi
, wj)(| vi
| + | wj
|) + χ


| vi
| − | wj
|


(1)
where |a| denotes the length of phoneme a, Cvis(vi
, wj) is 0 if vi
and wj are the same phoneme, 0.5 if they are different phonemes
but the same viseme (Section 3.3), and 1 if they are different visemes.
The parameter χ controls the influence of length difference on the
cost, and we set it to 10−4
in all our examples. Equation (1) penalized
for different phonemes and visemes, weighted by the sum of the
phoneme length. Thus longer non-matching phonemes will incur a
larger penalty, as they are more likely to be noticed.
We minimize Cmatch(Wi
, V) over all possible V⋆ using dynamic
programming [Levenshtein 1966] to find the best suffix of any prefix
of V and its matching cost to Wi
. We brute-force all possible prefixes
of V to find the best match Vi to the query Wi
.
Matching the full query. We define our full matching cost C between the query W and the video V as
C(W, V) = min
(W1,...,Wk )∈split(W)
(V1,...,Vk )
Õ
k
i=1
Cmatch(Wi
, Vi) + Clen(Wi) (2)
where split(W) denotes the set of all possible ways of splitting
W into subsequences, and Vi
is the best match for Wi according
to Cmatch. The cost Clen(Wi) penalizes short subsequences and is
defined as
Clen(Wi) =
ϕ
|Wi
|
(3)
where |Wi
| denotes the number of phonemes in subsequence Wi
and ϕ is a weight parameter empirically set to 0.001 for all our examples. To minimize Equation (2) we generate all splits(W1, . . . , Wk
) ∈
split(W) of the query (which is typically short), and for each Wi
we find the best subsequence Vi of V with respect to Cmatch. Since
the same subsequence Wi can appear in multiple partitions, we
memoize computations to make sure each match cost is computed
only once. The viseme search procedure produces subsequences
(V1, . . . , Vk
) of the input video that, when combined, should produce W.
ACM Trans. Graph., Vol. 38, No. 4, Article 68. Publication date: July 2019.
68:6 • Fried, O. et al
OF FRE NCH T OAST IN
Original Render Synthesized
Fig. 3. Our parameter blending strategy produces a seamless synthesized result from choppy original sequences. Above, we insert the expression “french toast”
instead of “napalm” in the sentence “I like the smell of napalm in the morning.” The new sequence was taken from different parts of the original video: F
R EH1 taken from “fresh”, N CH T taken from “drenched”, and OW1 S T taken from “roast”. Notice how original frames from different sub-sequences are
different in head size and posture, while our synthesized result is a smooth sequence. On the right we show the pixel difference between blue and red frames;
notice how blue frames are very different. Videos in supplemental material.
Query
Split
Match
W
W1 W2
w1 w5 w6 w12
v1 v5 v6 v12
V
Retime w1 w12
v1 v12
Fig. 4. Viseme search and retiming. Given a query sequence W, We split
it into all possible subsequences, of which one (W1, W2) ∈ split(W) is
shown. Each subsequence is matched to the input video V, producing a
correspondance between query phonemes wi and input video phonemes
vi
. We retime in parameter space to match the lengths of each vi to wi
.
3.4 Parameter Retiming & Blending
The sequence (V1, . . . , Vk
) of video subsequences describes sections
of the video for us to combine in order to create W. However, we
cannot directly use the video frames that correspond to (V1, . . . , Vk
)
for two reasons: (1) A sequence Vi corresponds to part of W in
viseme identity, but not in viseme length, which will produce unnatural videos when combined with the speech audio, and (2) Consecutive sequences Vi and Vi+1 can be from sections that are far
apart in the original video. The subject might look different in these
parts due to pose and posture changes, movement of hair, or camera
motion. Taken as-is, the transition between consecutive sequences
will look unnatural (Figure 3 top).
To solve these issues, we use our parametric face model in order
to mix different properties (pose, expression, etc.) from different
input frames, and blend them in parameter space. We also select a
background sequence B and use it for pose data and background pixels. The background sequence allows us to edit challenging videos
with hair movement and slight camera motion.
Background retiming and pose extraction. An edit operation W
will often change the length of the original video. We take a video
sequence (from the input video) B
′
around the location of the edit
operation, and retime it to account for the change in length the
operation will produce, resulting in a retimed background sequence
B. We use nearest-neighbor sampling of frames, and select a large
enough region around the edit operation so that retiming artifacts
are negligible. All edits in this paper use the length of one sentence
as background. The retimed sequence B does not match the original
nor the new audio, but can provide realistic background pixels and
pose parameters that seamlessly blend into the rest of the video. In
a later step we synthesize frames based on the retimed background
and expression parameters that do match the audio.
Subsequence retiming. The phonemes in each sequence vj ∈ Vi
approximately match the length of corresponding query phonemes,
but an exact match is required so that the audio and video will
be properly synchronized. We set a desired frame rate F for our
synthesized video, which often matches the input frame-rate, but
does not have to (e.g. to produce slow-mo video from standard video).
Given the frame rate F, we sample model parameters p ∈ R
257
by linearly interpolating adjacent frame parameters described in
Section 3.2. For each vj ∈ Vi we sample F | wj
| frame parameters
in [v
in
j
, v
out
j
] so that the length of the generated video matches the
length of the query | wj
|. This produces a sequence that matchesW
in timing, but with visible jump cuts between sequences if rendered
as-is (Figure 4 bottom).
Parameter blending. To avoid jump cuts, we use different strategies for different parameters, as follows. Identity geometry α ∈ R
80
and reflectance β ∈ R
80 are kept constant throughout the sequence
ACM Trans. Graph., Vol. 38, No. 4, Article 68. Publication date: July 2019.
Text-based Editing of Talking-head Video • 68:7
(a) Ground Truth (fi ) (b) Face & Mouth (c) Synth. Comp. (ri )
Fig. 5. Training Corpus: For each ground truth frame fi
(a), we obtain a 3D
face reconstruction. The reconstructed geometry proxy is used to mask out
the lower face region (b, left) and render a mouth mask mi (b, right), which
is used in our training reconstruction loss. We superimpose the lower face
region from the parametric face model to obtain a synthetic composite ri
(c). The goal of our expression-guided neural renderer is to learn a mapping
from the synthetic composite ri back to the ground truth frame fi
.
(it’s always the same person), so they do not require blending. Scene
illumination γ ∈ R
27 typically changes slowly or is kept constant,
thus we linearly interpolate illumination parameters between the
last frame prior to the inserted sequence and the first frame after
the sequence, disregarding the original illumination parameters of
Vi
. This produces a realistic result while avoiding light flickering
for input videos with changing lights. Rigid head pose T ∈ SE(3)
is taken directly from the retimed background sequence B. This
ensures that the pose of the parameterized head model matches the
background pixels in each frame.
Facial expressions δ ∈ R
64 are the most important parameters for
our task, as they hold information about mouth and face movement
— the visemes we aim to reproduce. Our goal is to preserve the retrieved expression parameters as much as possible, while smoothing
out the transition between them. Our approach is to smooth out
each transition from Vi to Vi+1 by linearly interpolating a region
of 67 milliseconds around the transition. We found this length to
be short enough so that individual visemes are not lost, and long
enough to produce convincing transitions between visemes.
3.5 Neural Face Rendering
We employ a novel neural face rendering approach for synthesizing
photo-realistic talking-head video that matches the modified parameter sequence (Section 3.4). The output of the previous processing
step is an edited parameter sequence that describes the new desired
facial motion and a corresponding retimed background video clip.
The goal of this synthesis step is to change the facial motion of the
retimed background video to match the parameter sequence. To
this end, we first mask out the lower face region, including parts of
the neck (for the mask see Figure 5b), in the retimed background
video and render a new synthetic lower face with the desired facial
expression on top. This results in a video of composites ri
(Figure 5d).
Finally, we bridge the domain gap between ri and real video footage
of the person using our neural face rendering approach, which is
based on recent advances in learning-based image-to-image translation [Isola et al. 2017; Sun et al. 2018].
3.5.1 Training the Neural Face Renderer. To train our neural face
rendering approach to bridge the domain gap we start from a paired
Fig. 6. We assume the video has been generated by a sequential process,
which we model by a recurrent network with shared generator G. In practice,
we unroll the loop three times.
Fig. 7. We employ a spatial discriminator Ds , a temporal discriminator Dt ,
and an adversarial patch-based discriminator loss to train our neural face
rendering network.
training corpus T =

(fi
, ri)
	N
i=1
that consists of the N original
video frames fi and corresponding synthetic composites ri
. The
ri are generated as described in the last paragraph, but using the
ground truth tracking information of the corresponding frame (Figure 5), instead of the edited sequence, to render the lower face
region. The goal is to learn a temporally stable video-to-video mapping (from ri to fi
) using a recurrent neural network (RNN) that is
trained in an adversarial manner. We train one person-specific network per input video. Inspired by the video-to-video synthesis work
of Wang et al. [2018a], our approach assumes that the video frames
have been generated by a sequential process, i.e., the generation
of a video frame depends only on the history of previous frames
(Figure 6). In practice, we use a temporal history of size L = 2 in all
experiments, so the face rendering RNN looks at L + 1 = 3 frames
at the same time. The best face renderer G
∗
is found by solving the
following optimization problem:
G
∗ = arg min
G
max
Ds , Dt
L(G, Ds , Dt ) . (4)
Here, Ds is a per-frame spatial patch-based discriminator [Isola et al.
2017], and Dt
is a temporal patch-based discriminator. We train the
recurrent generator and the spatial and temporal discriminator of
our GAN in an adversarial manner, see Figure 7. In the following,
we describe our training objective L and the network components
in more detail.
Training Objective. For training our recurrent neural face rendering network, we employ stochastic gradient decent to optimize the
following training objective:
L(G, Ds , Dt ) = E(fi
,ri)

Lr (G) + λs Ls (G, Ds ) + λt Lt (G, Dt )

.
(5)
ACM Trans. Graph., Vol. 38, No. 4, Article 68. Publication date: July 2019.