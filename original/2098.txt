The goal of light transport acquisition is to take images from a sparse set
of lighting and viewing directions, and combine them to enable arbitrary
relighting with changing view. While relighting from sparse images has
received significant attention, there has been relatively less progress on view
synthesis from a sparse set of "photometric" images—images captured under
controlled conditions, lit by a single directional source; we use a spherical
gantry to position the camera on a sphere surrounding the object. In this
paper, we synthesize novel viewpoints across a wide range of viewing directions (covering a 60◦
cone) from a sparse set of just six viewing directions.
While our approach relates to previous view synthesis and image-based
rendering techniques, those methods are usually restricted to much smaller
baselines, and are captured under environment illumination. At our baselines, input images have few correspondences and large occlusions; however
we benefit from structured photometric images. Our method is based on a
deep convolutional network trained to directly synthesize new views from
the six input views. This network combines 3D convolutions on a plane
sweep volume with a novel per-view per-depth plane attention map prediction network to effectively aggregate multi-view appearance. We train our
network with a large-scale synthetic dataset of 1000 scenes with complex
geometry and material properties. In practice, it is able to synthesize novel
viewpoints for captured real data and reproduces complex appearance effects
like occlusions, view-dependent specularities and hard shadows. Moreover,
the method can also be combined with previous relighting techniques to
enable changing both lighting and view, and applied to computer vision
problems like multiview stereo from sparse image sets.
CCS Concepts: • Computing methodologies → Image-based rendering.
Additional Key Words and Phrases: appearance acquisition, novel view
synthesis
1 INTRODUCTION
A central problem in computer graphics and vision is to acquire
images of a scene and reproduce its appearance under arbitrary
lighting and viewpoint. This has traditionally been accomplished by
densely sampling the scene’s "reflectance field" [Debevec et al. 2000]
and interpolating these images using a combination of image-based
rendering and relighting methods. Recent work has demonstrated
image-based relighting from sparse "photometric" images captured
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
76:2 • Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, and Ravi Ramamoorthi
under controlled directional lighting [Xu et al. 2018], thus significantly reducing the acquisition cost. However, novel view synthesis
methods still require a dense sampling of a scene’s "light field"
[Gortler et al. 1996; Levoy and Hanrahan 1996]. Consequently, they
capture hundreds of images, especially when the scene has complex surface reflectance [Wood et al. 2000]. While recent work has
addressed novel view synthesis from sparse images [Flynn et al.
2016; Kalantari et al. 2016; Srinivasan et al. 2017], these methods are
highly restricted in the range of viewpoints they can synthesize.
Our goal is to make appearance acquisition and rendering more
practical by synthesizing a wide range of novel viewpoints from a
sparse set of images. To do so, we image the scene, consisting of several objects, with six cameras placed on a vertex and the centers of
the adjoining faces of a regular icosahedron (see Fig. 2). This results
in a central camera, and five distributed symmetrically around it at
an angle of about 37◦
. At this large baseline, the captured images
have significant occlusions (see Fig. 1(b)). State-of-the-art multiview stereo methods fail to reconstruct complete geometry from
such sparse views. Yet, we show that our method can interpolate
the entire convex hull of these six viewpoints — a cone of more than
60◦ — while accurately reproducing effects like complex occlusions
and high-frequency, view-dependent specularities (see Fig. 1(c)).
This is made possible by a combination of our structured acquisition procedure and a novel learning-based interpolation scheme.
Unlike previous view synthesis approaches that capture images
under environment illumination, we acquire images under a single directional light. These "photometric" images capture appearance information like shading, shadows, and specularities, and have
been used for scene reconstruction via Photometric Stereo methods [Woodham 1980] (after which they are named) and image-based
relighting [Debevec et al. 2000]. We show that using such inputs
leads to view synthesis results that capture detailed scene appearance; this also enables other applications like novel view relighting
(see Fig. 1, 8).
We introduce a novel deep convolutional neural network that
learns to interpolate the wide-baseline photometric images and render arbitrary output viewpoints between them. Similar to previous
learning-based view synthesis methods [Flynn et al. 2016], our network projects the input images onto multiple depth planes of the
output view to construct a view-dependent plane sweep volume.
This volume is processed to predict the final output view (and depth)
using 3D convolutional layers with downsampling and upsampling
operations; this allows the network to reason about both geometry
and appearance at multiple spatial scales. A key, novel component
of our method is that we explicitly predict per-plane per-input-view
attention maps. These attention maps are used to modulate the 3D
plane sweep volume and allow the network to aggregate multi-view
appearance while accounting for occlusions, viewpoint variations,
etc. (similar to blending weights in IBR methods), and lead to sharper,
more accurate results. We supervise the output image and depth
map, and our network learns to predict the attention maps in an
unsupervised fashion.
We train our CNN with a large-scale, synthetic dataset consisting
of 1000 scenes with procedurally generated shapes and complex
spatially-varying BRDFs. We render these scenes with our six-view
camera configuration under random directional lighting using path
tracing. The rendered images approximate real-world appearance
and light transport well, allowing the network to generalize well to
real captured scenes. This can be seen in Fig. 1, where our method
generates photorealistic interpolated results for a real object with
complex geometry and appearance effects, including challenging
occlusions, high-frequency specularities and cast shadows.
We also demonstrate extensions of our approach that go beyond
view synthesis. We can add multiple lights to our acquisition setup
to capture a sparse set of multi-view, multi-light images. Our view
synthesis network can be extended to synthesize appearance under
novel view and lighting from this sparse data (see Fig. 1c3). We also
show that our view synthesis network can be used to "densify" the
captured views of a scene; these dense views can then be input
to a multi-view stereo algorithm to produce reconstructions that
are significantly better than what is possible with just the sparse
inputs (see Fig. 9). By making these methods work with sparse image
sets, our work takes a step towards eliminating the need for dense
capture systems and making scene acquisition more practical.
2 RELATED WORKS
Light transport acquisition. Traditionally, light transport acquisition methods use complex systems to capture images of a scene
under a dense set of lighting and view directions. These samples
can then be used to explicitly reconstruct scene geometry and reflectance [Debevec et al. 2000; Furukawa et al. 2002; Holroyd et al.
2010; Schwartz et al. 2011; Zhou et al. 2013]. Recent methods have
demonstrated geometry and reflectance reconstruction under more
relaxed settings such as handheld capture [Nam et al. 2018] and
unknown environment lighting [Xia et al. 2016]. For a more detailed
discussion of the previous work on appearance acquisition, we refer
readers to [Weinmann and Klein 2015; Weyrich et al. 2009]. While
these methods can produce high-quality rendering results, they still
require capturing hundreds of images.
This requirement can be relaxed for specific applications. For
example, image-based relighting methods focus on directly combining images captured under varying lighting conditions to relight
the scene under novel lighting. While earlier image-based relighting methods required brute-force sampling [Debevec et al. 2000;
Malzbender et al. 2001], recent methods have leveraged light transport coherence to reduce the number of images required [Peers et al.
2009]. In particular, Xu et al. [2018] demonstrate image-based relighting from only five images. Sparse capture has also been shown
to be sufficient for reflectance estimation for planar samples [Hui
et al. 2017; Li et al. 2018a; Xu et al. 2016], scenes with known geometry [Zhou et al. 2016a] or single-view reflectance [Barron and Malik
2015; Li et al. 2018b]. Our work focuses on capturing multi-view
scene appearance and can reproduce complex effects like viewdependent specularities and occlusions from only six images. Moreover, our method can be extended to capture scene appearance
under both varying view and lighting from only 36 images. This
is a significant change from traditional light transport acquisition
methods that require hundreds to thousands of images.
Novel view synthesis. Novel view synthesis methods focus on
interpolating the appearance of the scene between captured images
[Chen and Williams 1993], i.e., the view interpolation aspect of the
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
Deep View Synthesis from Sparse Photometric Images • 76:3
light transport acquisition problem. This can be done by re-sampling
rays from a densely captured light field [Gortler et al. 1996; Levoy
and Hanrahan 1996]; such light fields can be also reconstructed
from sparse samples by leveraging various forms of correspondence
between multiple views [Dąbała et al. 2016; Vagharshakyan et al.
2018; Yao et al. 2016]. Alternatively, image-based rendering methods
project captured images onto proxy geometry (that can be given or
reconstructed from the captured images), blend and resample them
from the novel viewpoint [Buehler et al. 2001; Chaurasia et al. 2013,
2011; Debevec et al. 1996; Sinha et al. 2009].
Penner and Zhang [2017] propose soft 3D, which reconstructs
depths and visibilities for each input view to achieve better blending.
Other works present different blending techniques to achieve ghosting free synthesis results for inaccurate 3D reconstruction [Bi et al.
2017; Eisemann et al. 2008; Zhou and Koltun 2014]. Hedman et al.
[2018] present a method that learns to predict the blending weights.
These blending techniques require scanned or MVS-reconstructed
geometry as an input, for which densely sampled viewpoints are
necessary. All these methods rely on capturing tens to hundreds of
images with large overlap and reasonable baseline; without this, the
geometric reconstruction and view synthesis would fail. Moreover,
these methods are usually designed for free-viewpoint navigation
and do not focus on reproducing detailed appearance like sharp specularities. In contrast, our method works with only six images that
are captured with a fairly large baseline, and reproduces complex
scene appearance accurately. It does so in an end-to-end fashion and
learns to predict appearance, depths and attention/blending maps.
Surface light fields [Wood et al. 2000] are designed to capture
complex scene appearance under high-frequency point lighting.
These methods reconstruct the surface geometry and represent
appearance on the surface using lumispheres. Because of the highdimensionality of this representation, methods have focused on compressing this data using PCA and vector quantization [Wood et al.
2000] or deep networks [Chen et al. 2018]. Our work is able to capture similar appearance effects with vastly fewer images. Moreover,
our network can be thought of as a scene-agnostic representation
that can interpolate any scene’s images to a new viewpoint.
Learning-based novel view synthesis. Recently, deep learning techniques have been applied to novel view synthesis to achieve unstructured multi-view interpolation [Flynn et al. 2016], narrow-baseline
stereo extrapolation [Zhou et al. 2018], narrow-baseline interpolation [Kalantari et al. 2016] and single-view extrapolation [Srinivasan
et al. 2017] in the context of light fields. All these methods estimate
geometry; either a single depth [Kalantari et al. 2016; Srinivasan
et al. 2017] or per-plane depth probabilities (or blending weights)
[Flynn et al. 2016; Zhou et al. 2018] are predicted for either one
input view [Srinivasan et al. 2017; Zhou et al. 2018] or each novel
view [Flynn et al. 2016; Kalantari et al. 2016]. All these methods
resolve visibilities in an implicit way, whereas our network learns
multi-view correspondence and explicitly predicts per-input-view
visibility-aware attention maps jointly with depth probability maps
at a novel viewpoint. Most importantly, we demonstrate that we
can synthesize a much wider range of new viewpoints compared to
these works.
1 2
4
5
6
3
1
2
3
4
5
6
n
Fig. 2. Acquisition configuration. Left: a regular convex icosahedron with
12 spherically symmetric vertices. Right: a projective view of our configuration, where the black background circle represents a hemisphere towards
the central view. We consider a setup with six known views (green circles,
denoted by numbers 1-6), in which one is located at a vertex and five at the
adjoining face centers. Our goal is to synthesize a novel view (orange circle,
noted by n), in the convex hull of the six known views (red dash-lines).
Geometry-free learning-based methods can directly generate pixels for a novel view from one or multiple input views [Tatarchenko
et al. 2015; Yang et al. 2015], though these methods are restricted to
specific shape classes. Other methods leverage flow prediction to
warp pixels from source views [Park et al. 2017; Zhou et al. 2016b].
Flow-based warping has been combined with per-input-view confidence maps to improve aggregation [Sun et al. 2018]. While we also
use attention maps, ours are predicted from geometric correspondences, inferred jointly with depths, and incorporate information
about occlusions, viewpoint differences, etc. We show that this leads
to results that are more realistic than flow-based methods.
Learning-based appearance acquisition. Deep learning-based methods have been applied to appearance acquisition applications like
reflectance capture [Li et al. 2017], reflectance map estimation [Rematas et al. 2016], and depth estimation [Eigen and Fergus 2015].
Using photometric images has been shown to be better for singleshot BRDF acquisition [Deschaintre et al. 2018; Li et al. 2017, 2018b],
and image-based relighting from sparse samples [Xu et al. 2018].
All these methods focus on the single-view setting. In this work, we
explore multi-view appearance acquisition using photometric measurements and achieve photorealistic novel view synthesis under a
single directional light from six sparse views.
3 ACQUISITION CONFIGURATION
Similar to previous work on reflectance field acquisiton [Debevec
et al. 2000; Weyrich et al. 2006], we choose an acquisition setup
where cameras are placed on a sphere. We assume that the cameras
are distant with respect to the scale of the scene and image the scene
with a field-of-view that ensures sufficient pixel coverage. Our goal
is to acquire the appearance of a generic scene, without making
any assumptions about scene composition. Therefore we utilize a
symmetric camera configuration, that would be optimal on average.
Given these design decisions, we need to choose a configuration
that symmetrically samples a sphere. This sampling has to balance
two constraints: we would like a sparse sampling that minimizes
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
76:4 • Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, and Ravi Ramamoorthi
Feature
Extractor
... ...
... ...
... ...
... ...
... ...
... ...
Shading
Predictor
... ...
... ... ... ...
Geometric
Warping
Geometric
Warping
Correspondence
Predictor
Depths: k = 1, 2, ..., p
Views:
i = 1, 2, ..., m
...
...
...
...
...
Multi-view
Input Images
...
...
Multi-view
Correspondence
Feature Maps
Plane Sweep Volume
 (8) Plane Sweep
Volume
Visibility-aware
Attention Maps
Attention-masked Volume Per-depth Images
Depth
Probability Maps
Weighted Images
(6) Predicted
Novel View
Image
(11) Predicted
Novel View
Depth
Corr-Branch (See Sec. 4.2)
Shade-Branch (See Sec. 4.3)
: 2D images or feature maps
: 3D color or feature volume
: 2D Conv U-Net
: 3D Conv U-Net
: Fixed geometric warping
: Network branches
(1) (2) (3)
(4) (5)
(7) (9) (10)
Fig. 3. Network Overview. Our view synthesis network consists of two branches that operate on plane sweep volumes (shown in blue) that are constructed
using geometric warping (gray boxes). Corr-Branch (bottom) extracts image features (7) and estimates attention maps (9) and depth probability maps (10).
The depth probabilites are used to estimate scene depth from the novel view (11). Shade-Branch (top) processes the input image volume using the attention
maps and depth probabilities to synthesize the novel view image (6). Please refer to the supplementary material for details of T, C and S.
acquisition cost while ensuring that views have sufficient overlap
to allow high-quality view interpolation.
Our configuration of choice, as shown in Fig. 2, is inspired by a
standard spherically symmetric shape: the regular convex icosahedron. An icosahedron has 12 vertices with 20 equilateral triangular
faces, symmetrically distributed around a sphere. Given an icosahedron, we investigate a setup with m = 6 cameras, in which one
camera is positioned on a vertex and five "boundary" cameras are
positioned at the centers of the faces that surround the central vertex. In this configuration, the angular distance from the central
view to each boundary view is about 37◦
. Note that this is a very
wide baseline, and our cameras observe very different parts of a
scene with limited overlapping regions. Our goal is to synthesize an
arbitrary view point in the convex hull of these six known views;
this represents a cone with an angular baseline of more than 60◦
.
Because of the symmetry of the icosahedron, our six-view setup
can potentially be extended for full sphere acquisition. Cameras can
be placed at every vertex and every face center, in total requiring
merely 32 views. This is a very sparse camera setup with significantly fewer views compared to previous techniques [Chen et al.
2018; Wood et al. 2000] that capture hundreds of images.
We focus on a practical case where a scene is composed of one
or a few objects that are placed on a flat platform. We capture
photometric images of this scene lit by an arbitrary directional light
from the frontal hemisphere (the black circular region in Fig. 2
right). This region is most likely to illuminate the scene and light
coming from behind the scene will have little contribution. For our
multi-view multi-light extension (Sec. 4.4), we use six fixed lights,
with one collocated with the central camera, and five located on the
surrounding neighboring five vertices. Fixing the light directions in
this scenario allows the network to leverage this structured input
and produce higher quality synthesis results.
4 ALGORITHM
Given images from our six pre-defined, calibrated views under a
single directional light, our goal is to synthesize a new image from a
specified novel view between the six inputs. Inspired by the recent
success of deep learning, we propose to train a deep CNN to directly
regress the final output image. Our method is a geometry-based
IBR method that uses scene geometry to align and blend the multiview data. However, instead of relying on a precomputed geometric
proxy [Buehler et al. 2001; Hedman et al. 2018], we design our
network to infer multi-view correspondence information for each
novel view, and predict novel view depths as side products. Since
we are using a synthetic dataset, the ground truth for both final
images and view-dependent depths can be generated and used as
supervision. Our network leverages this supervision to learn shading
and correspondence simultaneously in a single end-to-end system.
A key component in our network architecure, is that we modulate
the input views with per-view, per-scene-depth attention maps that
are learned without any supervision. We find that these attention
maps indirectly learn a combination of visibility, viewpoint-based
weighting, and other factors that when taken into account lead to
high-quality view synthesis results. In this section, we discuss the
details of our network design (Sec. 4.1, 4.2, 4.3, 4.4), data generation
(Sec. 4.5) and training details (Sec. 4.6).
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
Deep View Synthesis from Sparse Photometric Images • 76:5
4.1 Inputs and basic architecture
The inputs to our network are a fixed number of m input images,
I1,I2,..., Im from m views under a single directional light. While our
architecture can be potentially generalized to setups with other
fixed number of views, we consider m = 6 in this paper as discussed
in Sec. 3. Our network regresses an image In and the associated
depth map Dn from a novel view point. This involves aligning and
aggregating multi-view inputs, given input view camera parameters
Θ1, Θ2,..., Θm and novel view camera parameters Θn.
Our network uses a plane sweep volume representation. Plane
sweep volumes are constructed by warping colors or features from
multiple input views to a novel view at multiple pre-defined depth
planes. The warping function, denoted as W, is a homographybased geometric function, and is implemented as a non-learnable
(but differentiable) layer. Plane sweep volumes contain geometrically
aligned structured data, which is suitable for deep learning-based
IBR methods [Flynn et al. 2016]. To help the network better understand the multi-view data, we provide it with information about
the input and output viewpoints. Specifically, we supply b1, b2,...,
bm, which describe the angular distances from a novel view to each
input view, and the depth values d1, d2,..., dp of the p planes in a
plane sweep volume, as additional inputs to the network. As such,
our network is a regression function Φ:
In,Dn = Φ(I1, Θ1, b1..., Im, Θm, bm;d1, ..., dp ; Θn) (1)
= Φ({Ii }, {Θi }, {bi }; {dk
}; Θn), (2)
where {·} represents a set containing either multi-view data (denoted with subscript i) or multiple depths (denoted with subscript
k). We use this notation convention in the rest of the paper.
As shown in Fig. 3, our network uses two separate branches. The
first, Corr-Branch, seeks to analyze multi-view correspondences
and reconstruct scene geometry. The second branch, denoted as
Shade-Branch, reconstructs the output image. Both the correspondence predictor and shading predictor networks use a 3D U-Net
architecture, comprised of 3D convolutions, upsampling and downsampling operations, to process their corresponding plane sweep
volumes [Huang et al. 2018]. This allows the network to reason
about multi-view scene geometry and appearance while predicting
the output view and depth.
The plane sweep volume representation contains incorrect or
redundant features (e.g., features that are at incorrect depths or
occluded or view-dependent). Therefore, we estimate "attention"
maps that account for how much information should be used from
each view at each depth, and incorporate factors such as per-view
visibility and view weighting. Because these attention maps are
likely to be highly correlated with the scene geometry, we predict
them jointly with the depth probabilities in Corr-Branch. We premodulate the input image pixel volume with these attention maps
before it is processed by the shading predictor. This ensures that
each depth plane of the input pixel volume only has meaningful
color information at the beginning of the shading prediction, leading
to signicantly improved view synthesis results.
We provide ground truth rendered images as supervision, which
allows the network to reason about scene appearance and photo consistency. We also supervise the output depth images, which allows
the network to learn about scene geometry and correspondences.
While the learnable parameters of the correspondence and shading branches are separate, their data flows are highly correlated
because they share information coming from the image and depth
supervision (Fig. 3).
4.2 Learning multi-view correspondences: Corr-Branch
Multi-view reconstruction and re-rendering methods fundamentally rely on finding correspondences across the input images. We
achieve this using deep CNNs that process a plane sweep cost volume with deep-learned filters at multiple scales. Depth and visibility
information are highly correlated: depth expresses the distance at
which multi-view appearance is consistent, and visibility expresses
if single-view appearance is consistent with all other views. Therefore, we propose a novel correspondence estimation network, CorrBranch, to jointly infer depth and visibility-aware attention maps
for a novel view.
Corr-Branch consists of a feature extractor T and a correspondence predictor C. Photometric images are captured under directional lighting and can have high frequency view-dependent specularities which complicate correspondence reasoning. Therefore,
we apply a small U-Net style feature extractor, T, to pre-filter each
input image Ii
. T learns to extract specular-invariant features like
edges and orientations (examples shown in Fig. 3) that are meaningful for correspondence estimation. Specifically, T transforms each
3-channel RGB image Ii to an 8-channel feature map:
Mi = T (Ii). (3)
Given p discrete, pre-defined depth values {dk
|k = 1, .., p}, we
construct a plane sweep volume M at a novel view, Θn, from the
extracted per-input-view feature maps {Mi
|i = 1, ..,m}:
M = W({Mi }, {Θi }; {dk
}; Θn). (4)
W geometrically warps every Mi onto every depth plane at a distance dk using input view, Θi
, and novel view, Θn, to form the
volume M. We use Mk,i
to denote the warped Mi at the k
th depth in
M. Correspondence inference requires the network to understand
photometric consistency across multiple views. We achieve this by
processing the volume M with 3D filters with gradually expanding
receptive fields. To make it easier for the network to consider multiview consistency (and inconsistency), we pre-process the feature
volume by removing the average multi-view feature at each depth
plane as:
M˜
k,i =

Mk,i −
Í
j Mk,j
m
2
. (5)
This operation is similar to variance volumes that have been used
in other deep learning-based reconstructions methods [Yao et al.
2018]. However, while a variance volume expresses only global information across all views, M˜ only subtracts the mean and retains
per-view information in the form of per-view maps M˜
k,i
. This allows our network to infer view-dependent attention maps while also
leveraging global information. In fact, if required, the network can
compute the variance volume in subsequent convolutional layers
by averaging M˜ across views.
M˜
is thus a 3D (depth × image height × image width) volume,
where each "voxel" has 8m channels. We augment these features
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
76:6 • Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, and Ravi Ramamoorthi
with view differences bi and per-plane depth values dk
to make the
network utilize the novel view’s location (vis-a-vis the input views)
for correspondence reasoning:
Nk = M˜
k ⊕ {bi } ⊕ dk
, (6)
where ⊕ is a per-voxel/per-pixel concatenation operator. Since our
views lie on a sphere in our acquisition configuration, we use the
cosine of the angle between a pair of views as bi
. The volume, N,
thus has 9m + 1 channels.
Our correspondence predictor C is a 3D U-Net style network. It
processes volume N through a series of 3D convolutional layers,
each followed by group normalization (GN) and ReLU layers. We
use downsampling and upsampling along the depth and image
dimensions to analyze multi-view correspondence at multiple spatial
scales. The details of C are shown in Fig. 3. The correspondence
predictor outputs an (m + 1)-channel volume, in which the first m
channels represent a per-view attention volume V and the last one
channel represents a depth probability volume P:
V, P = C(N). (7)
Each channel in V corresponds to the attention information for
the corresponding input view, and incorporates both visibility and
viewpoint-based weighting information.Vk,i
is an attention map for
the i
th view at the k
th depth plane; it provides a pixel-wise attention
mask that is used during shading prediction. Pk
, on the other hand,
provides a pixel-wise depth probability for the k
th depth plane.
P is processed with a depth-wise softmax operation to produce
actual probability maps for each depth plane:
P
d = soft-max(α
d
P), (8)
where α
d
is a learnable scalar parameter. The output view depth
image is finally predicted as:
Dn =
Õ
k
dk P
d
k
. (9)
We provide ground truth depth images for Dn as supervision. We
expect the attention maps and depth estimates to be highly correlated. Therefore, we estimate them from the single correspondence
predictor network and share information until the last layer. This
ensures that the attention prediction utilizes the depth supervision
to learn meaningful features. As noted before, both the attention,
V , and depth probability, P, are provided to the shading prediction
branch to help aggregate multi-view appearance.
4.3 Learning to predict shading: Shade-Branch
Inferring scene appearance from a novel view is a highly challenging task in our configuration: our wide angular baseline input views
can see different parts of a scene with limited overlap. Morevover,
complex shading effects like high frequency specular highlights
vary significantly across views. We resolve these challenges using
our image prediction branch, Shade-Branch. As shown in Fig. 3,
Shade-Branch has a shading predictor S that reasons about scene
appearance from multi-view images {Ii }, using the multi-view correspondence information (attention maps, V , and depth probability,
P) predicted by Corr-Branch.
Similar to Eqn. (4), a plane sweep volume I is constructed from
original images {Ii } using homography-based warping W. This
volume contains the original color information warped onto multiple planes. This volume can have highly redundant and potentially
inconsistent information from multiple views due to strong occlusions. A key feature of our network is that we pre-mask this color
volume I using the visibility-aware attention maps inferred from
the correspondence branch, to disentangle this redundancy and inconsistency. The masking is achieved by a voxel-wise multiplication,
for every view at every depth:
˜Ik,i = Ik,iVk,i
. (10)
This directly connects Corr-Branch and Shade-Branch; Corr-Branch
can thus leverage appearance information from Shade-Branch to
estimate attention maps that allow the shading prediction to be as
accurate as possible.
Our shading predictor S is a 3D-Unet style network similar to the
correspondence predictor C, but with more channels at each layer
for better appearance reasoning. It processes the masked volume ˜I,
the original attention maps V and other information, and predicts a
3-channel appearance volume
A = S(˜I,V, {bi }, {dk
}), (11)
where ˜I,V , {bi }, {dk
} are concatenated voxel-wise, similar to Eqn. (6).
Supplying the original attention maps, V (in addition to the modulated appearance volume) gives the network more freedom to
reconstruct scene appearance. Note that each plane Ak
is a predicted image, containing the predicted appearance of the scene at
the k
th depth plane. These per-plane predicted images are weighted
by the depth probability maps from Corr-Branch to reconstruct the
final image as:
A˜
k = Ak P
a
k
, (12)
In =
Õ
k
A˜
k
. (13)
where P
a
is the normalized depth probability volume using soft-max
and a scalar parameter α
a = 4 similar to Eqn. (8). Each plane (A˜
k
) of
A˜
, as shown in Fig. 3, is a clean weighted image, with depth-incorrect
outlier pixels completely masked out.
We provide ground truth rendered novel view images as supervision for In, and train our network end to end. As noted before, by
transferring information between Shade-Branch and Corr-Branch
(in the form of the attention maps and depth estimates), our novel
network design consolidates both appearance synthesis and correspondence estimation, allowing the two branches to leverage each
other for better inference.
4.4 Extension to multi-light inputs.
Many photometric applications, like image-based relighting and
photometric stereo, acquire photometric images under multiple
light sources from a single viewpoint. We propose an extension of
our approach to multi-view and multi-light datasets. As noted in
Sec. 3, for this application we capture images under six fixed views
and six fixed lights.
Multi-light data allows for better geometric reconstruction by
giving us more information about scene appearance [Davis et al.
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
Deep View Synthesis from Sparse Photometric Images • 76:7
2005]. For example, specularities under one light source can disappear under another light source, and shadowed regions under one
light can be illuminated by another. This leads to better estimation
of normals, edges and other features. We design an extended feature
extractor Tq to take advantage of this for better multi-view correspondence inference. The difference between the single-light T and
the multi-light Tq feature extractors is merely the number of input
channels. Tq uses a fixed number of structured inputs; the subscript
q specifies the number of lights. For each view, we stack q images
I
1
i
, I
2
i
,..,I
q
i
under q different light sources together as a 3q-channel
feature map as an input for Tq. While Corr-Branch predicts the
attention maps and depth probability maps from multi-light data,
the input for our Shade-Branch remains the same; it still predicts a
novel view image under a single light source using the multi-view
images under that light.
We use our network to synthesize novel views for each input
light. In Section 6, we show that it can be combined with imagebased relighting methods to re-render scene appearance under novel
viewpoint and lighting — the full scene acquisition scenario.
4.5 Data generation
To the best of our knowledge, there is no existing large-scale dataset
that contains multi-view photometric images. Previous novel view
synthesis methods have trained with data from on-line videos [Zhou
et al. 2018], car driving scenes [Geiger et al. 2012] or simple objects
from specific classes [Chang et al. 2015], none of which apply to our
high-quality appearance acquisition scenario. Therefore, we create
a novel large-scale synthetic dataset. As in Xu et al. [2018], we procedurally generate shapes by combining primitives with randomly
generated bump maps and randomly merging 1 to 9 primitives. We
create 1000 training scenes and 50 testing scenes using this method.
We texture map these shapes with SVBRDFs from the Adobe Stock
3D material dataset1
. This dataset contains 1329 SVBRDFs, and we
separate it into 1129 training and 200 test materials.
We render our dataset using path tracing with 1000 samples; we
use Optix to achieve fast path tracing, similar to [Li et al. 2018b].
This physically based rendering method ensures that our images
contain realistic light transport. We render 512 × 512-resolution
images and tone-map them using gamma 2.2. We render our scenes
using the camera configuration described in Sec. 3. For each scene,
we randomly select a field-of-view angle from 5
◦
to 60◦
, and correspondingly calculate a distance for cameras based on the angle
and the scene’s size to ensure good pixel coverage. Each training
image set consists of 6 input views and one novel view under a
single directional light. For each training scene, we create 30 such
sets by randomly placing 30 novel views between the 6 input views
and selecting 30 random directional lights for rendering. In total,
we have 30000 such sets in our training set. For each test scene, we
randomly select 36 novel views and 3 directional lights, and render
each view under each light, which creates 108 image sets, for a total
of 5400 image sets in our test set.
1https://stock.adobe.com/3d-assets
4.6 Training details
Loss functions. We use an L1 loss on both the ground truth images
and depths from each novel view. Specifically, let La be the image
L1 loss and Ld be the depth L1 loss. Our final loss L is given by
L = La + βLd
. (14)
We use β = 0.1 for all our experiments.
Parameters and strategies. Our six views are radially symmetric
(see Fig. 2); we leverage this symmetry to provide structured inputs
to our network and make training easier. Given a novel view, we first
rotate the image and depth around the central viewing direction to
make its up direction point towards the central input view. We then
reorder the input views to achieve a canonical input view layout,
where the first view is the central view, and the other views are
ordered in counter-clockwise order starting with the view on the
left (see Fig. 2 right).
We assume that the test scenes are captured by a calibrated spherical gantry and that the physical size of a scene and the distance
from a camera to the center of the scene is approximately known.
This allows the depth values {dk
|k = 1, 2..., p} to be specified correspondingly. While our network is fully convolutional and can
support an arbitrary number of depth planes, p, we use p = 64 for
our training and all experiments. During training, we have perfectly
symmetric cameras distributed at a known distance, ˆd, from the
center of a scene. We also know the ground truth scene size ∆ (the
maximum difference between ˆd and each pixel depth). To get {dk
},
we set d1 = ˆd − γ∆, dp = ˆd + γ∆, and uniformly divide this range
for other dk
, which can be expressed by:
dk = ˆd −γ∆ +γ
2∆(k − 1)
p − 1
. (15)
During training, we randomly pick γ between 0.8 to 2.0, so that our
network sees different sweep volume scales; we thus only need to
specify a roughly correct distance ˆd and size ∆ at test time for real
scenes (using γ = 1).
We train our networks using three or four NVIDIA Titan Xp or
1080Ti GPUs, using a batch size of 4 for each GPU, for a total batch
size of 12 or 16. We apply group normalizations in both C and S,
and observe better performance than batch normalization with our
small per-GPU batch size, similar to [Wu and He 2018]. During
training, we randomly crop 64 × 64 patches from novel view images
and depths for data augmentation. Since our images can have large
regions of black background, we only select crops that have at least
50% non-background pixels. Our network generally converges after
training for 400 epochs (about 4 days with 4 GPUs).
5 EXPERIMENTS
We now present a comprehensive evaluation of our method on both
synthetic and real data.
Ablation study on synthetic data. We first justify the design of our
network architecture through ablations on the synthetic dataset. We
compare our proposed single-light photometric novel view synthesis
network, T CS, i.e., with a feature extractor (T) and correspondence
and shading predictors (C and S), against a number of variants. We
also compare it to our multi-light network, T6CS, with six different
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
76:8 • Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, and Ravi Ramamoorthi
Table 1. Ablation study. We evaluate different versions of our networks on
our synthetic testing dataset, and compare the image L1 error, PSNR, SSIM,
and Depth L1 error on the central 256 × 256 crops.
Image L1 Image PSNR Image SSIM Depth L1
T (CS)noV 0.0451 30.74 0.9391 0.0567
T CnoDS 0.0345 32.05 0.9520 0.1448
T CS 0.0318 32.61 0.9573 0.0437
T6CS 0.0307 33.03 0.9602 0.0246
Ground Truth
Fig. 4. Qualitative comparisons on synthetic test set between T CS and
T(CS)noV (i.e., with and without attention maps, respectively). T(CS)noV
suffers from color bleeding artifacts (red arrows), that are resolved by T CS.
directional lights as inputs for Corr-Branch and a multi-light feature
extractor. Specifically, we compare against the following networks:
T CnoDS, that doesn’t have depth supervision, and T (CS)noV, a
network that does not use attention maps. We evaluate all these
networks on our synthetic testing dataset and compare the image L1
loss, PSNR, SSIM, and depth L1 loss. To avoid biases in these metrics
from the large black backgrounds in our rendered images, we crop
the central 256 × 256 regions from all testing images for evaluation;
these crops have 75% non-background pixels on average. We also
calculate depth L1 only for the foreground as we do for training.
The numerical comparisons of these different networks are shown
in Tab. 1. As demonstrated by T CS vs T (CS)noV, our visibilityaware attention maps significantly improve reconstruction performance; image L1 loss reduces by about 30% and is accompanied by
a large improvement in PSNR and SSIM. Figure 4 shows qualitative comparisons between T CS and T (CS)noV. We observe many
color-bleeding artifacts with T CS because it is unable to resolve the
large occlusions in these scenes. We also observe that the attention
maps can be inferred and help the synthesis in an unsupervised
way; network T CnoDS with attention maps, but without depth supervision, performs much better at view synthesis than T (CS)noV
without attention maps and with depth supervision. These comparisons demonstrate the key role our visibility-aware attention maps
play in effectively eliminating the incorrect, inconsistent information in a plane sweep volume. Also, comparing T CnoDS vs T CS
shows that depth supervision improves reconstruction accuracy,
though in a subtle way.
Finally, when multi-light data is provided, our multi-light network
T6CS has the best performance. This confirms that the multi-light
feature extractor T6 extracts better features leading to both better
depth prediction and better image synthesis than our single light
version. While the multi-light version requires more acquired images, it can be naturally combined with an image-based relighting
method to enable view and lighting changes (see Fig. 8).
Real data capture. We capture our real scenes—composed of one
or more real objects placed on a platform—using a spherical gantry.
We capture each scene from the six input views under either a
central directional light (for single-light results) or six directional
lights (for multi-light results and additional applications). We also
capture 50 novel views under these lights, as ground truth to validate
our view synthesis quality. Our cameras are about 50cm away from
the platform. We thus set ˆd = 50cm for all our real scenes. While
the sizes and scales of our real scenes vary, we find that ∆ = 6cm
works well for most cases. Our network is robust to these variations
because of the randomized ∆ during training. We mask out the
background of the captured images before passing them to our
network. Our network is fully convolutional and we directly apply
our method on these images, although we are training on 64×64
crops. We use an Nvidia Titan Xp to process our real results and it
takes about 2 seconds to generate a 400×400 image.
Comparisons against previous view synthesis methods. We now
compare our method to previous state-of-the-art view synthesis
methods. We considered comparing against learning-based IBR
methods. However, these methods are usually designed for general IBR applications with densely sampled views [Flynn et al. 2016;
Hedman et al. 2018]. We tried training an implementation of Flynn
et al. [2016] on our dataset, but it failed to predict reasonable results
in our wide-baseline case. Hedman et al. [2018] require estimating
geometry using multi-view stereo, which does not work from our
sparse inputs. Besides, methods designed for specific scenarios, like
light fields [Kalantari et al. 2016; Srinivasan et al. 2017] or pair-view
extrapolation [Zhou et al. 2018] are also not easy to apply in our
case. We also tried training the released network of [Kalantari et al.
2016] on our dataset; it doesn’t converge to any reasonable results
because of a significantly more challenging task. Therefore, we
compare against [Penner and Zhang 2017], a state-of-the-art nonlearning based IBR method. Note that, Penner and Zhang [2017]
have already demonstrated that their method performs better than
[Flynn et al. 2016] and [Kalantari et al. 2016].
We also compare against a flow-based view synthesis method
[Sun et al. 2018]. Directly applying their model, trained on KITTI
[Geiger et al. 2012] or ShapeNet [Chang et al. 2015], doesn’t work
on our data. Therefore, we retrain their model using our dataset,
albeit with a white background; this is the same scenario Sun et
al.[2018] use in their paper.
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
Deep View Synthesis from Sparse Photometric Images • 76:9
Input Images
PSNRs:
SSIMs:
11.947
0.1365
8.4499
0.0880
20.450
0.3552
18.992
0.3527
29.474
0.9118
30.417
0.9466
8.2796
0.1899
10.734
0.4310
13.026
0.1967
17.290
0.5490
23.141
0.9067
30.957
0.9808
PSNRs:
SSIMs:
Sun et al. 2018 Penner and Zhang 2017
from six closer views
Penner and Zhang 2017 Our Results Ground Truth
21.987
0.4781
20.347
0.4942
15.821
0.4844
18.757
0.6977
Fig. 5. Comparisons with flow-based view synthesis [Sun et al. 2018] (second column) and Soft3D [Penner and Zhang 2017] (third column) using wide-baseline
inputs (first column, input viewing directions shown in green, and novel view direction in yellow). We also compare with [Penner and Zhang 2017] (fourth
column) that uses a much closer set of views as input (marked in grey rectangles in the first column). Our results are significantly better than other methods
and differences almost imperceptible from ground truth. We show cropped insets of all results with corresponding PSNRs and SSIMs (bottom).
In Fig. 5, we show qualitative and quantitative comparisons on
two real captured examples. The first scene has a complicated surface texture and the second has complex specularities and hard
shadows. In the second and third columns of Fig. 5, we compare
with [Sun et al. 2018] and [Penner and Zhang 2017] using the same
six-view images we use for our method. We can see that our method
performs significantly better than the two methods qualitatively,
as shown by the synthesized images (details in insets), and quantitatively, as shown by the PSNR and SSIM values. Both previous
methods fail to handle this challenging wide-baseline configuration.
Sun et al. [2018] produce blurred results with no appearance details
and many mis-aligned ghosting artifacts. The results of [Penner and
Zhang 2017] also contain serious ghosting artifacts. Our method, on
the other hand, produces photorealistic view synthesis results with
significantly higher PSNR and SSIM values. As shown in the insets,
our method recovers both the complicated texture of the first example as well as the challenging hard shadows and view-dependent
specularities in the second example.
We also select six closer views as input for [Penner and Zhang
2017]. As shown in the fourth column, these "easier" inputs improve
their results. However, their results with the small-baseline inputs
are still worse than our results from the six wide-baseline inputs,
highlighting the accuracy and robustness of our method.
View synthesis on real photometric data. Figures 1 and 10 show
our view synthesis results from our single-light network compared
with captured ground truth. Our method produces photo-realistic
novel view images for these real scenes, which accurately match the
ground truth. As demonstrated in many examples, our method generates high-quality view interpolation results even at challenging
viewing directions that are close to the boundary of the pentagonal cone, where very limited input information can be used. These
results are consistent across a wide variety of scenes, in terms of
both materials (pottery, cloth, mental, wood, plastic and candy) and
geometry (single and multiple objects; small and big objects).
Comparison between single-light and multi-light networks. We
show a challenging scene under directional lighting in Fig. 6, and
compare our single-light network with our extended multi-light
network. The scene contains thin structures (like the arms and
thumbs) which are very distinct from our training geometry primitives. These highly non-convex structures exhibit challenging cast
shadows that complicate correspondence inference. Nevertheless,
our single-light network performs well for most viewing directions
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
76:10 • Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, and Ravi Ramamoorthi
Input Images
Our single-light Results Our multi-light Results Ground Truth
 g
 h
g
a
a
b
b
c
c
d
d
e
e
f
f
h
Fig. 6. Single-light vs. multi-light network comparison. For a complex scene
captured under a challenging light direction (marked by red hollow circle), our single-light network may generate ghosting artifacts from some
challenging viewing directions (second row, marked by blue arrows). Our
multi-light network resolves these issues using images under multiple light
sources (marked by gray). For most viewing directions, our single-light
network produces high-quality results (first row). The six input images are
shown on the bottom, with light directions marked in green with labels
(a-f ), and novel views marked in yellow with labels (g,h).
Input Images Our Result Ground Truth
Fig. 7. Limitations. Our method fails to reconstruct sharp specularies that
have long-range motion (top) and highly non-convex occlusions (bottom).
(e.g. Fig. 6.g). As shown in Fig. 6.h, our single-light network may
generate obvious ghosting artifacts for some challenging directions,
but our multi-light network can resolve these issues thanks to more
reliable correspondence inference from multi-light images.
Our novel view synthesis results under six dierent directional lights
Our novel view relighting results under environment maps
1
2
3
4
5
6
1 2 3
4 5 6
Fig. 8. Novel view relighting using our novel view synthesis results. We
apply our multi-light network to synthesize six novel view images under six
directional lights (marked in gray with labels 1-6), each synthesized from
the same six views (marked in green). We use these high-quality synthesis
results as inputs for a deep image-based relighting technique [Xu et al.
2018], and create relighting results (on the bottom) under environment
maps (shown on the top left of each result).
Limitations. Our method only handles opaque scenes, which is a
limitation of our training dataset. Our network is trained on 64 × 64
cropped images, which limits the spatial scale of appearance reasoning. Consequently, long-range effects like sharp specularities
that move significantly are not reconstructed well (see Fig. 7). Our
method might blur sharp specularities (see Fig. 10). Also, our network generates blurred results with ghosting for highly non-convex
scenes with parts that are visible in only one or two views (see
Fig. 7).
6 ADDITIONAL APPLICATIONS
Our view synthesis method can be combined with other scene acquisition and rendering techniques to enable a broad set of applications.
We now demonstrate a few examples.
Novel view relighting. Our method can synthesize novel views
from images captured under different directional lights. These, in
turn, can be used with image-based relighting methods to enable
rendering under novel view and lighting. One such relighting example is shown in Fig. 1c3. We apply our multi-light network with
six different directional lights and synthesize novel view images
for each light separately (see Fig. 8 top). We train the Relight-Net
network from [Xu et al. 2018] for our six-light setting. We pass the
synthesized novel view images for the six lights to this network
to generate images under novel directional lights. Similar to [Xu
ACM Trans. Graph., Vol. 38, No. 4, Article 76. Publication date: July 2019.
Deep View Synthesis from Sparse Photometric Images • 76:11
b) Reconstruction from
56 captured images
a) Reconstruction from
6 captured images
c) Reconstruction from
56 images synthesized
from 6 images
d) Reconstruction from
56 images synthesized
from 6 images (with color)
Fig. 9. Multi-view stereo reconstruction from our synthesized images. We
synthesize 56 high-quality novel views from six images and use them as
inputs for a multi-view stereo algorithm, COLMAP [Schönberger et al. 2016],
to generate 3D reconstructions (c,d) that are comparable to a reconstruction
from 56 captured images (b). COLMAP reconstructs incomplete geometry
with holes from only the six sparse images (a).
et al. 2018], we achieve relighting under novel environment maps
by linearly combining the relit images as shown in Fig. 8 bottom.
Note that our network and the Relight-Net are trained separately,
without end-to-end refinement or any other special processing.
While our results from a novel view under changing environment
map often look realistic, some aspects still need improvment. For
example, some blurriness, incorrect shadow motion and temporal
inconsistency that become obvious when changing the view under
a novel environment map. Jointly training our network with the
Relight-Net using a larger task-specific training dataset can potentially resolve, or at least alleviate, these issues. That said, to our
knowledge, this is the first attempt at synthesizing a full reflectance
field, enabling changes to both lighting and viewpoint, from such
sparse samples (36 images from 6 views under 6 lights).
Multi-view stereo. Multi-view stereo methods often require a
dense set of input views, and can fail to reconstruct complete holefree geometry for sparse views such as ours. We apply our method
to "densify" the captured scene and synthesize 56 novel view images around a scene from six images. We pass these 56 synthesized
images to a multi-view stereo system COLMAP [Schönberger et al.
2016] and achieve 3D reconstruction of the scene (see Fig. 9 c and
d). As a baseline, we pass the 56 captured ground truth images to
COLMAP for reconstruction and we observe qualitatively similar
results as shown in Fig. 9. Note that COLMAP reconstructs incomplete geometry with missing parts from the original six sparse views.
By making MVS methods work better with such sparse viewpoints,
our method makes them more robust and general.
7 CONCLUSION AND FUTURE WORK
We have demonstrated a method to synthesize photometric scene
appearance at a wide range of novel viewpoints from a sparse set
of only six images captured at large baselines. This is in contrast
to previous methods that rely on densely sampling the scene with
hundreds of viewpoints. We achieve this by training a novel deep
CNN that can simultaneously infer correspondences and shading
from structured photometric images. Our network predicts visibilityaware attention maps that effectively address photometric and geometric inconsistencies and allow for the accurate aggregation of
multi-view scene appearance. We present evaluations and comparisons to previous view synthesis methods, and show that we
can generate significantly more accurate and photorealistic images
across a wide range of scenes. Fundamentally, our work takes a
step towards capturing and rendering scene appearance from sparse
image sets. This is a classic problem in vision and graphics and
we believe that our work can enable many other applications. For
example, we demonstrate that our synthesized images can be used
to achieve novel view relighting and multi-view stereo from sparse
images. In the future, it would be interesting to explore extensions
of our technique to other challenging scene acquisition tasks, like
multi-view BRDF reconstruction, 360◦
scene reconstruction, and
the acquisition of dynamic scene appearance from sparse images.