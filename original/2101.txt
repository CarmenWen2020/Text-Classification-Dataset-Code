In this article, we provide a detailed survey of 3D Morphable Face Models
over the 20 years since they were first proposed. The challenges in building
and applying these models, namely, capture, modeling, image formation,
and image analysis, are still active research topics, and we review the stateof-the-art in each of these areas. We also look ahead, identifying unsolved
challenges, proposing directions for future research, and highlighting the
broad range of current and future applications.
CCS Concepts: • Computing methodologies → Mesh models; Shape
analysis; Modeling methodologies; Reflectance modeling; Perception; Biometrics; Scene understanding; 3D imaging; Motion capture; Image representations; Shape representations; Appearance
and texture representations; Dimensionality reduction and manifold learning; Neural networks; Learning latent representations;
Learning linear models; • General and reference → Surveys and
overviews;
Additional Key Words and Phrases: 3D computer vision, computer graphics, statistical modeling, analysis-by-synthesis, generative models
1 INTRODUCTION
It is 20 years since 3D Morphable Face Models were first presented
at SIGGRAPH ’99. They were proposed as a general face representation and a principled approach to image analysis. Blanz and
Vetter [1999] introduced and tackled many subsidiary problems
and the results were considered groundbreaking. The impact of
the original paper has been long term, recognized by an impact
paper award, and the approach and applications are accessible to
a wide audience (the original supplementary video was one of the
most popular videos in the early days of YouTube). However, the
approach is not just of historical interest. In the past two years, 3D
Morphable Face Models have been re-discovered in the context of
deep learning and are incorporated into many state-of-the-art solutions for face analysis (see Figure 1). This survey aims to build
a starting point for researchers new to the topic, act as a reference guide for the community around 3D Morphable Models, and
to introduce exciting open research questions.
1.1 Definition
A 3D Morphable Face Model is a generative model for face shape
and appearance that is based on two key ideas: First, all faces are
in dense point-to-point correspondence, which is usually established on a set of example faces in a registration procedure and
then maintained throughout any further processing steps. Due to
this correspondence, linear combinations of faces may be defined
in a meaningful way, producing morphologically realistic faces
(morphs). The second idea is to separate facial shape and color
and to disentangle these from external factors such as illumination and camera parameters. The Morphable Model may involve a
statistical model of the distribution of faces, which was a principal
component analysis in the original work [Blanz and Vetter 1999]
and has included other learning techniques in subsequent work.
1.2 History
The initial research question behind the idea of 3D Morphable
Models (3DMM) was how a visual system, biological or artificial,
can cope with the high variety of images that a single class of objects can generate, and how objects are represented to solve vision
tasks. The leading assumption for the development of 3DMMs was
that prior knowledge about object classes plays an important role
in vision and helps to solve otherwise ill-posed problems. 3DMMs
are designed to capture such prior knowledge, and they are learned
automatically from a set of examples. The representation is general, so it may be applied to different objects and tasks.
Representations of faces and the task of face recognition have
been in the focus of vision research for a long time. An important and very influential paradigm shift in this field was the
Eigenfaces approach by Sirovich and Kirby [1987] and Turk and
Pentland [1991], which learned an explicit face representation
from examples and operated entirely on gray levels in the image domain. Eigenfaces treated images of faces as a vector space
and performed a principal component analysis, with the eigenvectors representing the main modes of variation in that space. The
drawback of Eigenfaces was not only that it was limited to a fixed
pose and illumination, but that it had no effective representation
of shape differences: When the coefficients in linear combinations
Fig. 2. The visual abstract of the seminal work by Blanz and Vetter [1999].
It proposes a statistical model for faces to perform 3D reconstruction
from 2D images and a parametric face space that enables controlled
manipulation.
of eigenvectors are changed continuously, structures will fade in
and out, rather than shift along the image plane. As a consequence,
the model fails to find a single parameter for, say, the distances between the eyes. The Eigenfaces approach was also extended to 3D
face surfaces by Atick et al. [1996] to model shading variations in
faces, yet with essentially the same limitation.
Several research groups proceeded by adding an Eigendecomposition of 2D shape variations between individual faces. This
provided both an explicit shape model, and—after warping the
images—an aligned Eigenface model without blurring and ghosting artifacts. While in the original Eigenface approach, the images
were only aligned by a single point (e.g., the tip of the nose); the
new methods established correspondence on significantly more
points. Landmark-based face warping for image analysis was introduced by Craw and Cameron [1991]. Using approximately 200
landmarks, the first statistical shape model was proposed in Active
Shape Models [Cootes et al. 1995]. While this model used shape
only, Active Appearance Models [Cootes et al. 1998] proposed a
combination of shape and appearance that turned out to be very
successful and influential. Other groups computed dense pixelwise image correspondences with optic-flow algorithms for modeling the facial shape variations [Hallinan et al. 1999; Jones and
Poggio 1998]. In all these correspondence-based approaches, images are warped to a common template, and the appearance variation is then performed in the same way as the original Eigenfaces,
but on the shape-normalized images. The shape model, however,
provides a powerful and compact representation of shape differences by shifting pixels in the image plane. However, compared to
the simple linear projection in Eigenfaces, the image analysis task
is transformed into a more challenging nonlinear model-fitting
problem.
These 2D models were efficient to cover the shape variation for a
fixed pose and illumination setting. The framework was extended
to variations across pose by Vetter and Poggio [1997] and to other
object classes, such as images of cars [Jones and Poggio 1998]. All
this groundwork demonstrated that a separation of shape and texture information in images can model the variation of faces. However, the price to pay for taking pose and illumination variations
into account was high: Eventually, it would require many separate
models, each limited to a small range of poses and illuminations.
In contrast, the progress of 3D Computer Graphics in the 1990s
ACM Transactions on Graphics, Vol. 39, No. 5, Article 157. Publication date: June 2020.
3D Morphable Face Models—Past, Present, and Future • 157:3
demonstrated that variations in pose and illumination are easy to
simulate, including self-occlusion and shadowing. Adapting methods from graphics to face modeling and computer vision led to the
new face representation in 3DMMs and the idea of using analysisby-synthesis to map between the 3D and 2D domain. Those were
the two key contributions in the first paper on 3DMMs [Blanz and
Vetter 1999], see Figure 2. The name Morphable Model was derived
from their 2D counterpart [Jones and Poggio 1998], and in fact,
Jones and Poggio strongly influenced the ideas that led to 3DMMs.
3DMMs and 2D Morphable Models rely on dense correspondence, rather than only a set of facial feature points. In the original
work, this was established by an optical flow algorithm for image registration. The image synthesis algorithm used a standard
rendering model with perspective projection, ambient and directional lighting, and a Phong model of surface reflectance that includes a specular component. However, in analysis-by-synthesis,
this approach comes at a computational price, because shapecamera [Smith 2016] and illumination-albedo [Egger 2018] ambiguities lead to a hard ill-posed optimization problem. Moreover,
the optimization is costly and is prone to end in unwanted local
optima. Just as it is already dramatically more complicated to fit
an Active Appearance Model to a 2D image, compared to the simple projection needed for Eigenfaces, the complexity of 3DMM fitting raises additional problems that have remained challenging to
researchers after 20 years of development.
At the time the initial 3DMM was developed, image-based models were dominating computer vision and even animation [Ezzat
et al. 2002], and they were rather elaborate at that time. It was a key
decision to take the best of both the 2D and the 3D world by using
3D models to manipulate existing images and applying 2D algorithms to 3D surfaces: Unlike mesh-based algorithms, the original
3DMM used optical flow, multi-resolution approaches, and interpolation algorithms on parameterized surfaces of faces. With the
initial face scanner delivering surfaces in a two-dimensional cylinder parameterization, all those steps were performed in 2D, and
most of the methods involved were replaced with their 3D equivalent only many years later. It is interesting to see that after a development towards 3D, the computer vision community came back to
2D representations by using deep learning, and now evolves again
to 3D, e.g., by integrating 3DMMs.
Over the past years, 3DMMs were applied beyond faces. Models
were built for the surface of the human body [Allen et al. 2003;
Anguelov et al. 2005; Loper et al. 2015] and for other specific parts
of the body, such as ears [Dai et al. 2018] and hands [Khamis et al.
2015], animals [Sun and Murata 2020; Zuffi et al. 2018], and even
cars [Shelton 2000]. In this survey, we focus on 3DMMs to model
the human face, though many of the techniques and challenges are
the same across different object classes.
The 3DMM was developed in a time where algorithms and data
were rarely shared across researchers and institutions. Ten years
later the first publicly available 3DMM was released [Paysan et al.
2009a], and in the last 10 years, all individual data and algorithmic
components needed to build and use 3DMMs were released by various researchers. We collected a list of all available resources and
will further maintain it [Community 2019].
The 3DMM was built as a general representation for faces, not
just aiming at one specific task. Even though the model is outperformed for some very specific applications such as face recognition, it is unique in its generality across different tasks and
applications.
1.3 Organization
There is a recent state-of-the-art report on monocular 3D reconstruction, tracking, and applications [Zollhoefer et al. 2018]. This
focuses on the most recent advances, particularly related to the
specific task of tracking and reconstruction. In contrast, in this article, we instead focus on the 3DMM, all involved methods, and
reflect the major contributions over the past 20 years while at the
same time highlighting challenges and future directions.
This survey is organized from building to applying a 3DMM.
We start with Section 2 where we present methods to acquire 3D
facial data for model building. We then describe in Section 3 the
various approaches to model the 3D shape and facial appearance.
In Section 4, we discuss the methods to generate a 2D image from
our 3D model using computer graphics. Our Section 5 surveys the
major application of 3DMMs, namely, the reconstruction of a 3D
face from a 2D image. Section 6 summarizes the impact of 3DMMs
in the recent advances in the field of deep learning and how deep
learning can be used to improve the modeling and analysis. Section 7 summarizes the various applications where 3DMMs were
used in the past 20 years. Every section summarizes the major challenges the authors see regarding the current limitations of 3DMM.
We also collect challenges that are shared across multiple sections
in Section 8, where we also venture an outlook on what we expect to see in the next 10 to 20 years and how the 3DMM will keep
impacting how faces are represented.
2 FACE CAPTURE
The key ingredient to any 3DMM is a representative set of 3D
shapes, usually coupled with corresponding appearance data. The
typical way to construct such a sample pool is by acquiring data
from the real world. In this section, we give a brief overview of
different approaches that have been used to acquire facial data as
well as data of facial parts. As we are concerned with the creation
of input datasets for 3DMMs, we limit the discussion to acquisition
under controlled conditions, as opposed to the more challenging
in-the-wild setting. Note that controlled 3D face capture may not
always be necessary. There have been attempts to learn 3DMMs
directly from images [Cashman and Fitzgibbon 2012] and stateof-the-art deep learning–based methods simultaneously learn a
3DMM and regression-based fitting from 2D training data (see
Section 6.3). In this section, we begin by covering shape acquisition methods in Section 2.1, including geometric, photometric,
and hybrid methods. Sections 2.2, 2.3, and 2.4 describe methods
for capture of appearance, face parts, and dynamics, respectively.
Section 2.5 lists publicly available 3D face datasets that could be exploited for building 3DMMs. Finally, we consider open challenges
related to face capture in Section 2.6.
2.1 Shape Acquisition
The three-dimensional shape is arguably the most important ingredient to a 3DMM. The issue of shape representation has not
been widely considered in the context of 3DMMs. By far the
ACM Transactions on Graphics, Vol. 39, No. 5, Article 157. Publication date: June 2020.
157:4 • B. Egger et al.
most commonly used representation is a triangle mesh. Rare exceptions include cylindrical [Atick et al. 1996] and orthographic
[Dovgard and Basri 2004] depth maps (though these representations do not permit meaningful dense correspondence), per-vertex
surface normals [Aldrian and Smith 2012], and, more recently, volumetric orientation fields [Saito et al. 2018] and signed distance
functions [Park et al. 2019]. Using a triangle mesh representation,
dense correspondence requires that all samples exhibit the same
topology and that the vertices encode the same semantic point on
all samples. Establishing correspondence across the samples is a
challenging topic in itself, discussed in Section 3.5. In this section,
we focus on the acquisition of raw 3D data before establishing
correspondence.
2.1.1 Geometric Methods. Geometric methods estimate directly the 3D coordinates of a shape either by observing the same
surface point from two or more viewpoints (in which case the
challenge is identifying corresponding points between images) or
by observing a projected pattern (in which case the challenge is
identifying the correspondence between the known pattern and
an image of its projection). Methods can either be considered active, i.e., they emit light or other signals into the scene, or passive. Laser scanners, Time-of-Flight sensors, and Structured Light
systems are active systems, where multi-view photogrammetry is
a passive alternative. Active multi-view photogrammetry may be
considered a hybrid active/passive approach, as it relies on passive photogrammetry to reconstruct the shape, but augments the
object with a well-defined texture projection that benefits the reconstruction [Zhang et al. 2004]. Unlike structured light, the origin
of the light does not matter, as the projected texture is solely meant
to augment the texture used for multi-view stereo matching. This
type of technology is used by the Intel® RealSenseTM D435 camera,
for example.1 In the early days of 3DMMs, active systems were the
only real option to acquire 3D shapes at a reasonable quality. The
original paper of Blanz and Vetter [1999] relied on laser scanning
[Levoy et al. 2000], where the face is rasterized via one or more
laser beams. The laser beam illuminates the face surface at a point
and using the known camera/laser arrangement the 3D position of
this point may be triangulated. The biggest drawback of laser scanners is the acquisition time, as only very few samples are gathered
at any given time—even at very high frame rates, such systems
require the subjects to sit still for several seconds.
Structured light scanners [Geng 2011] overcome this limitation
to some extent by injecting not only a few beams but leveraging
projectors that offer millions of them. The challenge here is to identify which beam is illuminating the object at a given point. This is
addressed by structuring the projected light in a way that allows to
clearly identify the origin of any ray. The simplest approach is binary encoding, which projects black-and-white patterns assigning
a unique binary code to each pixel. The required number of patterns is still quite substantial: For VGA resolution, one needs 19
distinct patterns and for 4K resolution 23 patterns, and hence this
approach is most suited for capturing static objects. However, technical improvements have begun to make these approaches viable
for dynamic capture of faces. The Intel® RealSenseTM SR300 uses
1https://www.intelrealsense.com/depth-camera-d435/.
only nine binary patterns to obtain VGA, while the most recent
RealSense depth camera produces VGA resolution at 60 depth FPS
with a scanning laser technology. Other more complex structured
light methods have been proposed, such as gray codes or (colored)
fringe patterns, which can reduce the number of required frames
further, in extreme cases even to a single frame. A very popular
commercial system that was used to create face datasets [Cao et al.
2014b] and that employs structured light is the first-generation
Kinect sensor.2 The device employs a structured dot pattern, which
allows reconstructing depth from a single frame by sacrificing spatial resolution. Resolution may be improved by accumulating several frames [Newcombe et al. 2011]. With the increased resolution
and quality of consumer cameras, passive systems have become
the method of choice in most cases, since they are simpler to assemble and operate; and off-the-shelf photogrammetry software
solutions, both commercial such as Agisoft3 or RealityCapture,4
as well as open-source solutions such as Meshroom,5 provide very
good results on human faces. Also, complete systems can be purchased that come with both hard- and software6,7,8. These methods typically do not require the aggregation of information over
time and hence offer themselves for single-shot acquisition [Beeler
et al. 2010] as well as full-frame rate performance capture [Beeler
et al. 2011; Bradley et al. 2010; Furukawa and Ponce 2009]. A potential disadvantage of the aforementioned systems is their form
factor, since they all require at least some separation between the
different participating components, i.e., the cameras or lights, often referred to as the baseline. An alternative that becomes more
and more viable due to the push of the mobile industry are timeof-flight sensors, where the elements can be located close to each
other. The second-generation Kinect sensor9 belongs to this family, as well as many depth sensors that are shipped with modern
mobile phones. A challenge that time-of-flight sensors share with
most of the active systems is that color information has to be acquired separately and is not intrinsically aligned with the 3D data,
which is another advantage of passive setups.
2.1.2 Photometric Methods. Photometric methods typically estimate surface orientation, from which the 3D shape may be recovered via integration. The challenge here is to select models that
accurately capture reflectance properties of the surface and obtaining sufficient measurements that the inversion of these models is well-posed. Compared to geometric methods, photometric
methods typically offer higher shape detail and do not rely on the
presence of matchable features (so are applicable to smooth, featureless surfaces) but often suffer from low-frequency bias in the
reconstructed positions caused by modeling errors in reflectance
and illumination. Photometric stereo [Ackermann et al. 2015] estimates the surface normal at each pixel by observing a scene
from a fixed position under at least three different illumination
conditions, which can be spectrally multiplexed [Hernández et al.
2https://en.wikipedia.org/wiki/Kinect#Kinect_for_Xbox_360_(2010). 3https://www.agisoft.com. 4https://www.capturingreality.com. 5https://alicevision.org/. 6https://www.canfieldsci.com/imaging-systems/vectra-m3-3d-imaging-system/. 7http://www.di4d.com. 8http://www.3dmd.com/. 9https://en.wikipedia.org/wiki/Kinect#Kinect_for_Xbox_One_(2013).
ACM Transactions on Graphics, Vol. 39, No. 5, Article 157. Publication date: June 2020.
3D Morphable Face Models—Past, Present, and Future • 157:5
Fig. 3. Capture of intrinsic face properties using a hybrid geometric/photometric method [Seck et al. 2016; Smith et al. 2020]. Multi view stereo (MVS)
is used to reconstruct a coarse mesh (c). A photometric light stage [Ma et al. 2007] is used to capture diffuse and specular albedo maps (a,b) and surface
normals that are merged with the MVS mesh to produce a mesh with fine surface detail (d). Together, these can be used to synthesize highly realistic images
of the face (e).
2007] to reduce the number of frames required. Early work assumed known lighting directions and perfectly diffuse reflectance.
When illumination is uncalibrated and a more suitable glossy reflectance model is used, generic face priors can be used to resolve the resulting ambiguity [Georghiades 2003]. Typically more
lighting conditions are used to increase robustness and coverage,
such as four [Zafeiriou et al. 2013] or even nine [Gotardo et al.
2015]. Gradient-based illumination takes the number of conditions
to the extreme by illuminating the subject not with discrete individual point lights, but by an ideally continuous, omnidirectional
incident illumination gradient. An advantage of this setup is that
hard light source occlusions (cast shadows) are replaced by soft
partial occlusions of the illuminating hemisphere (ambient occlusion). In practice, the omnidirectional illumination is realized via
a light-stage [Debevec et al. 2000], which discretizes the gradient
with a large number (several hundred) of light sources. The original work of Ma et al. [2007] suggests the use of four distinct gradients, which has later been extended using complementary gradients [Wilson et al. 2010]. Again, variants of temporal, spectral,
and polarization multiplexing have been proposed to reduce the
number of required conditions.
2.1.3 Hybrid Methods. Hybrid methods combine the strength
of geometric and photometric methods; specifically, they reduce
the low-frequency bias typically present in photometric methods
and increase the high-frequency details when compared to geometric methods. Nehab et al. [2005] propose a method for merging the low frequencies of positional information and the high
frequencies of surface normals. The method is particularly efficient, involving only the solution of a sparse linear system of equations, and has been used in the context of 3DMM fitting [Patel
and Smith 2012]. Various combinations of geometric and photometric methods have been considered. For example, Zivanov et al.
[2009] combine structured light with photometric stereo, Ma et al.
[2007] combine structured light with gradient-based illumination,
Ghosh et al. [2011] combine multi-view stereo with gradient-based
illumination, and Beeler et al. [2010] combine passive multi-view
photogrammetry with shape-from-shading. Figure 3(d) shows the
output of a hybrid method in which photometric surface normals
are merged with a multi-view stereo mesh.
2.2 Appearance Capture
In addition to shape, appearance is also required for many 3DMM
tasks, such as synthesizing images (see Section 4) and inverse rendering (see Section 5). Unlike shapes, which are almost exclusively
represented as triangular meshes, appearance representation
varies substantially. While in theory, every vertex of the mesh
could have an associated appearance property, typically shapes
are parameterized to the 2D domain and textures are used to store
appearance properties. Appearance can be as simple as backprojecting the color of the images onto the shapes, which causes
shading effects to be baked in. Self-occlusion, in particular when
only a single viewpoint is available, results in missing data in the
occluded areas, which must be hallucinated somehow. Booth et al.
[2018b] use 3DMM fits to in-the-wild images and Principal Component Pursuit with missing values to complete the unobserved
texture. They build their appearance model directly on the sampled textures. Such a simplistic approach, however, does not allow
intrinsic face appearance properties to be separated from shading/
shadowing (and hence illumination/geometry). A partial solution
to this problem is to control illumination conditions during
capture; for example, by using multiple light sources to create
approximately ambient lighting. Note that a truly Lambertian
convex surface observed under truly ambient light gives exactly
the albedo [Lee et al. 2005]. The appearance models in the most
popular 3DMMs [Booth et al. 2018a; Dai et al. 2017; Paysan
et al. 2009a] use this approach, combining images from multiple
cameras to provide full coverage of the face with diffuse lighting
to approximate albedo. A better approach is to explicitly separate
shading from skin color, often referred to as intrinsic decomposition. This allows relighting of the face under novel incident
illumination conditions and a 3DMM built on such data truly
models intrinsic characteristics of the face. Several approaches
ACM Transactions on Graphics, Vol. 39, No. 5, Article 157. Publication date: June 2020.
157:6 • B. Egger et al.
have been presented over the years to acquire reflectance data
suited for parametric rendering, measuring surface reflectance
[Marschner et al. 1999] and even subsurface scattering properties
[Ghosh et al. 2008]. The polarized spherical illumination environment used by Ma et al. [2007] enables diffuse albedo to be
captured in a single shot and specular albedo in two images (see
Figures 3(a) and (b)). While such approaches have predominately
used active setups, recently capture under passive conditions has
been demonstrated [Gotardo et al. 2018].
2.3 Face Part Specific Methods
Certain parts of the human face require more targeted acquisition
methods and devices, since they do not conform with the assumptions typically made by abovementioned approaches. For example,
the frontmost part of the eye, the cornea, is for obvious reasons
fully transparent and distorts the appearance of the underlying iris
due to refraction. Bérard et al. [2014] leverage a combination of
several specialized algorithms, including shape-from-specularity,
to reconstruct all visible components of the eye. Another challenging example are teeth [Wu et al. 2016a], which exhibit extremely challenging appearance [Velinov et al. 2018]. Hair violates
the common assumption that the reconstructed shape is a smooth
continuous surface and requires specialized approaches that estimate hair fibers [Beeler et al. 2012], hair strands [Hu et al. 2014a;
Luo et al. 2013], and braiding [Hu et al. 2014b], or even encode hair
as a surface [Echevarria et al. 2014] for manufacturing. While most
hair acquisition focuses on static reconstruction, some do capture
hair in motion [Xu et al. 2014] or estimate physical properties for
hair simulation [Hu et al. 2017a]. Especially challenging is the acquisition of partially or completely hidden properties, such as the
tongue [Hewer et al. 2018], the skull [Achenbach et al. 2018; Beeler
and Bradley 2014], or the jaw [Zoss et al. 2019, 2018], where oftentimes specialized imaging systems are required, such as Computer
Tomography (CT), Magnetic Resonance Imaging (MRI), or Electromagnetic Articulography (EMA). Last, even skin itself requires
specialized treatment in some areas, such as lips [Garrido et al.
2016b] or eyelids [Bermano et al. 2015], where the local appearance and deformation exceed the capabilities of the more generic
methods.
2.4 Dynamic Capture
Historically, 3DMMs have been mostly concerned with static
shapes; for example, with a set of neutral shapes from different
individuals or with a discrete set of expressions per individual, neglecting how the face transitions between expressions. Most capture systems used to build 3DMMs were hence static systems,
focused on capturing individual shapes rather than full performances. As the field begins to integrate more temporal information into the models, the need for dynamic capture systems will
rise. Active systems have been considered, both geometric [Zhang
et al. 2004] and photometric [Wilson et al. 2010]. However, passive
systems [Beeler et al. 2011; Bradley et al. 2010] are currently the
technologies of choice, since they do not require temporal multiplexing and still deliver high-quality shapes, and more recently
even per-frame reflectance data [Gotardo et al. 2018]. A beneficial
side-effect of such technologies is that they often provide shapes
that are already in correspondence, removing the need to establish
correspondence in a post-processing step (Section 3.5) and making
them attractive solutions even when only a discrete set of shapes is
desired. Available commercial solutions include Di4D,10 3dMD,11
or the Medusa system.12
2.5 Publicly Available Face Datasets
A relatively large number of publicly available datasets exist that
could be leveraged in the construction of 3DMMs, though many
have never been used for this purpose. We believe there is not
broad awareness of the range of 3D datasets available and so collect them together in Table 1. We hope that this will encourage
work that seeks to exploit multiple datasets for 3DMM building.
2.6 Open Challenges
The field of face capture is far ahead of face modeling in general and 3DMMs in particular. There is a large gap between the
quality of data that can be captured and the data actually used
to build 3DMMs. There is a further gap between the quality of
this already-deficient data and what a 3DMM is able to synthesize
(see Section 3). Hence, from the perspective of 3DMMs, the open
challenges in capture do not generally relate to improving the acquisition quality, but to the lack of publicly available data. While
there is a decent number of datasets publicly available (see Section 2.5), most of these contain only moderate quality shape data
and no appearance information, with the exception of Stratou et al.
[2011], which consists of 23 identities only. We believe that the
lack of high-quality datasets is due to a variety of reasons. On the
one hand, high-quality acquisition devices that can capture both
shape and appearance are not readily available. Most of them are
custom-built, cannot easily be purchased or licensed, and require
expert knowledge for operation. On the other hand, acquiring and
processing data may be a time- and resource-intense effort, since
many systems in the research community were not conceived for
scalable deployment but for experimental use; slow capture methods are not applicable to young or elderly people, expensive setups
are challenging to replicate on a global scale to capture whole populations, and methods requiring very bright illumination make it
unpleasant to be captured with eyes open. Furthermore, most highquality systems, in particular ones that also measure appearance,
generally require controlled lab conditions, which makes it difficult to capture large numbers of the general public. Advances in
face capture may alleviate some of these issues.
Additionally, there are many important broader questions related to data acquisition that remain unanswered. How many faces
do we really need to capture to build a representative (universal)
model? How can we ensure we capture natural expressions? Most
people are not trained to perform specific expressions (i.e., FACS13)
and will have difficulties performing naturally when put in a capture setup, leading to a biased dataset. How should we deal with
bias in general and what is the right sampling strategy with respect
to age, gender, ethnicity, and so on? Are the capture methods themselves biased? For example, capturing faces with very dark skin is
challenging for both photometric and geometric methods. Should
10http://www.di4d.com/. 11http://www.3dmd.com/. 12https://studios.disneyresearch.com/medusa/. 13https://en.wikipedia.org/wiki/Facial_Action_Coding_System.
ACM Transactions on Graphics, Vol. 39, No. 5, Article 157. Publication date: June 2020.
3D Morphable Face Models—Past, Present, and Future • 157:7
Table 1. Overview of Publicly Available 3D Shape and/or Appearance Scans of Human Faces
dataset format and resolution coverage no. samples scanner
Spacetime faces [Zhang
et al. 2004]
triangle mesh (23K vertices, consistent
topology)
inner face only 1 individual × 384 frame
dynamic sequence
structured light
CASIA 3D Face Database
[cas 2005]
640×480 depth map and texture image face, neck,
sometimes ears
123 individuals × 37–38
scans (expression, pose,
illumination)
Minolta Vivid910
BU-3DFE [Yin et al. 2006] triangle mesh (20K–35K triangles), two
texture images (1,300 × 900)
face, neck,
sometimes ears
100 individuals × 25
expressions
3dMD
BU-4DFE [Yin et al. 2008] triangle mesh (35K vertices), texture
image (1,040 × 1,329)
face, neck,
sometimes ears
101 individuals × six 100
frame expression
sequences
Dimensional
Imaging
Bosphorus [Savran et al.
2008]
1, 600 × 1, 200 depth map and texture
image
inner face only 105 individuals × up to 35
expressions per subject +
13 poses
Inspeck Mega
Capturor II
York 3D Face Database
[Heseltine et al. 2008]
depth map containing 5K–6K points,
texture image
inner face only 350 individuals × 15
expressions
projected pattern
stereo
B3D(AC)ˆ2 [Fanelli et al.
2010]
raw scan: triangle mesh (55K vertices),
780 × 580 texture image; processed:
triangle mesh (23K vertices, consistent
topology), 1,024 × 768 UV texture map
inner face only 14 individuals × around 80
dynamic sequences
(speech-4D)
structured light
stereo
Florence 3D Faces
[Bagdanov et al. 2011]
triangle mesh (60K–80K triangles), 4
MPixel texture, additonal 2D HD video
face, neck,
sometimes ears
53 individuals 3dMD
D3DFACS [Cosker et al.
2011]
triangle mesh (30K vertices), 1,024 ×
1,280 UV texture map
face, neck,
sometimes ears
10 individuals × around 52
dynamic sequences, FACS
coded
3dMD
3DRFE [Stratou et al. 2011] triangle mesh (1.2M vertices), 1,296 ×
1,944 diffuse and specular albedo maps
and hybrid normal maps
inner face, neck 23 individuals × 15
expressions
light stage
Hi4D-ADSIP [Matuszewski
et al. 2012]
triangle mesh (20K vertices), texture
image
inner face only 80 individuals × around 42
dynamic sequences
Dimensional
Imaging
BP4D-Spontaneous [Zhang
et al. 2014]
triangle mesh (30K–50K vertices),
texture image (1,040 × 1,329)
face, neck,
sometimes ears
41 individuals × eight
one-minute dynamic
sequences
Dimensional
Imaging
3D Dynamic Database for
Unconstrained Face
Recognition [Alashkar
et al. 2014]
3.5K vertices for dynamic, 50K vertices
for static, texture image
inner face only 58 individuals × one static
scan + seven dynamic
sequences
Artec
FaceWarehouse [Cao et al.
2014b]
raw: 640 × 480 RGBD; processed:
triangle mesh (11K vertices, consistent
topology)
150 individuals × 20
expressions
Microsoft Kinect
MMSE [Zhang et al. 2016a] triangle mesh (30K–50K vertices), 1,040
× 1,392 texture image
inner face only 140 individuals × four
dynamic sequences
Dimensional
Imaging
Headspace [Dai et al. 2017] triangle mesh (180K vertices), 2,973 ×
3,055 UV texture map
full head including
face, neck, ears
1,519 individuals 3dMD
4DFAB [Cheng et al. 2018] triangle mesh (60K–75K vertices), UV
texture map
face, neck, and ears 180 individuals × 4K–16K
frames of dynamic
sequences
Dimensional
Imaging
CoMA [Ranjan et al. 2018] triangle mesh (80K–140K vertices),
texture images (avg resolution
3,700 × 3,200), six raw camera images
(each 1,600 × 1,200), alignments in
FLAME topology
full head including
face, neck, ears
12 individuals × 12 extreme
expression sequences
3dMD
VOCASET [Cudeiro et al.
2019]
triangle mesh (80K–140K vertices),
texture images (avg resolution
3,700 × 3,200), six raw camera images
(each 1,600 × 1,200), alignments in
FLAME topology
full head including
face, neck, ears,
speech
12 individuals × 40
dynamic sequences
(speech-4D)
3dMD
ACM Transactions on Graphics, Vol. 39, No. 5, Article 157. Publication date: June 2020.
157:8 • B. Egger et al.
we accept that we cannot hope to capture sufficiently broad data
and therefore rely on synthesizing additional data or using captured data to build a bootstrap model that is refined on large 2D
datasets? These approaches are discussed in Section 6.
Finally, there are some philosophical and ethical issues to consider. The human face is unique and highly personal. Once a face
has been captured in high detail, it is possible to synthesize new
images that are almost indistinguishable from photos. If captured
datasets are made publicly available, it is very difficult to control
the distribution and use of such data. Obtaining proper informed
consent is, therefore, both legally and ethically important but perhaps even this does not go far enough, particularly when consent
for minors is given by parents. These issues are beyond the expertise of computer graphics and vision researchers and perhaps
suggest a need for discussion and debate with other disciplines.
3 MODELING
This section outlines how to compute a 3DMM by modeling the
variations of digitized 3D human faces. In particular, the following three types of variations are commonly considered. First, geometric variations across different identities are captured in a shape
model, as outlined in Section 3.1. Commonly used models include
global models, which represent variations of the entire face surface, and local models, which represent variations of facial parts.
Second, geometric variations across different facial expressions are
captured in an expression model, as outlined in Section 3.2. Commonly used models can be mainly classified into additive and multiplicative models. More recently, nonlinear expression models are
starting to be explored. Third, variation in appearance and illumination are captured in a separate appearance model as outlined in
Section 3.3.
It is interesting to note that the landmark paper on 3DMMs published 20 years ago [Blanz and Vetter 1999] proposed first models
for all three types of variation that are still commonly used today.
To compute shape, expression, or appearance models, statistics
are performed over a database of face data, where traditionally
3D scans of faces were used, and more recently some approaches
also learn face models directly from 2D images, as outlined in Section 6.3. This computation of statistics requires correspondence information, that is, anatomically corresponding parts of the faces
need to be compared, and hence known either explicitly or implicitly. An overview of how correspondence information is computed
for faces is given in Section 3.5. The most commonly used approach
is to compute correspondence information explicitly before computing the 3DMM. Some recent methods compute correspondence
information at the same time while the 3DMM is built.
3DMMs are generative models and the ability to synthesize
novel faces is a key feature and briefly discussed in Section 3.6.
Finally, this section provides a list of available models and discusses open challenges on 3D face modeling in Sections 3.7 and 3.8,
respectively.
3.1 Shape Models
This section considers modeling geometric variation across different subjects computed using classical modeling approaches that
use 3D data. To use a set of 3D scans as training data, we require
a distance measure between any pair of scans, and computing
a distance between raw scans consisting of different numbers
of unstructured vertices is a complex problem. Most commonly,
the community proceeds by first pre-processing the dataset
by deforming a template mesh to all scans, which establishes
anatomic correspondences between the points of the scans (see
Section 3.5). We denote the surface of such a pre-processed mesh
by S in the following. The ith vertex of S is denoted by vi ∈ R3,
and its associated vector c ∈ R3n contains the coordinates of vi
in a fixed order. All meshes share a common triangulation. We
denote the ith triangle by ti = (t 1
i ,t 2
i ,t 3
i ) ∈ {1,...,n}
3, where
t 1
i ,t 2
i ,t 3
i provide indices to the associated vertices vt 1
i
, vt 2
i
, vt 3
i
,
and we denote the complete triangulation by T = (t1,..., tm ).
Distances between shapes S1 and S2 are computed as difference
between c1 and c2 after rigidly aligning S1 and S2 in R3.
3DMMs most often follow Dryden and Mardia [2002] for their
definition of shape S as containing the geometric information remaining after having removed differences caused by translation,
rotation, and sometimes uniform scaling. While scaling is typically
not removed for human faces, this is often done in geometric morphometrics (e.g., Dryden and Mardia [2002], Section 2).
A shape space is traditionally defined as the set of all configurations of n vertices in R3 with fixed connectivity. Since we are interested in modeling human faces only in the context of 3DMMs,
in the following, the term shape space refers to a d-dimensional
parameter space (with d  n) that represents plausible 3D human
faces. In this way, each 3D face has an associated parameter vector
w ∈ Rd .
In 3DMMS, statistical shape analysis is used as generative
model, i.e., the shape space has an associated probability distribution called prior that is defined by a density function f (w) and that
measures the likelihood that a realistic 3D face would be represented by a particular vector w in shape space. With a slight abuse
of notation, we interpret c as a generator function in the following
as:
c : Rd → R3n, (1)
which maps the low-dimensional parameter vector w to the vector
of all vertex coordinates c(w) ∈ R3n. We again use vi (w) ∈ R3 to
refer to the ith vertex of the mesh given by w. While the resolution
(number of vertices) of the model is usually fixed, a progressive
mesh representation based on edge collapse simplification of the
generator function has been considered [Patel and Smith 2011].
This part considers the case where all faces in the training data
have a similar (typically neutral) expression; generator functions
that additionally model varying expressions are discussed in Section 3.2. As in Brunton et al. [2014b], our discussions distinguish
global models, which model the entire face or head area, from local
models, which perform statistics over localized areas.
3.1.1 Global Models. Let {Si}i denote the training shapes and
{ci}i their associated coordinate vectors. The seminal work on
3DMMs [Blanz and Vetter 1999] proposed a global shape model
that uses principal component analysis (PCA) to compute the linear generator function as
c(w) = c¯ + Ew, (2)
ACM Transactions on Graphics, Vol. 39, No. 5, Article 157. Publication date: June 2020.
3D Morphable Face Models—Past, Present, and Future • 157:9
where c¯ is the mean computed over the training data, E ∈ R3n×d
is a matrix that contains the d most dominant eigenvectors of the
covariance matrix computed over the shape differences {ci − c¯i},
and w is the low-dimensional shape parameter vector. One hypothesis of this model is that training faces can be linearly interpolated to generate new 3D faces. Another hypothesis is that the
3D faces in the reduced parameter space Rd follow a multivariate
normal distribution, which can be directly deduced from the eigenvalues corresponding to E. This implies that the density function
f (w) evaluating the likelihood of the parametric representation
w in shape space is simply the Mahalanobis distance of w to the
origin.
The 3DMM was originally computed over 200 subjects and has
proven to be useful in a variety of applications, thanks to its power
to generate plausible shapes and its simple underlying model. A recent study rebuilds such a model from a very large dataset containing 9,663 3D scans and revisits best practices [Booth et al. 2016],
demonstrating that the originally proposed generator function for
shape remains highly relevant in the research community.
One observation by Blanz and Vetter [1999] is that moving the
representation vector w away from the mean face increases their
distinctiveness, eventually leading to caricatures of the identity. To
model distinctive facial identities, Patel and Smith [2016] propose
an alternative density function f (w) based on the following observation: Consider the squared Mahalanobis distances from the
mean for a set of d-dimensional vectors that follow a multivariate Gaussian distribution. These distances form a χ2
d -distribution,
which has expected value d. Hence, to preserve the shape distinctiveness related to identity, Patel and Smith restrict the representation w to have Mahalanobis distance √
d from the mean. Lewis
et al. [2014b] propose a similar argument showing that, even if
faces are truly Gaussian distributed (which has been shown for the
Basel data by a Kolmogorov Smirnoff test for shape and per-vertex
color, where the marginal distribution for the shape is close to a
Gaussian [Egger et al. 2016a]), methods that make the assumption
that typical faces lie near the mean are not valid.
Recently, Lüthi et al. [2018] proposed a nonlinear shape space
that models deformations from the mean as Gaussian processes.
3.1.2 Local Models. Using a global generator function in Equation (1) is known to lead to representations that do not model finescale geometric details. To improve the modeling of important localized areas, such as the eye or nose regions, Blanz and Vetter
[1999] initially experimented manually segmenting the face into
regions and learning separate PCA models per region. Their results demonstrate that this localized modeling allows for reconstructions of higher fidelity. This idea has been extended since with
representations that achieve much higher accuracy than the global
PCA model, and this comes in general at the cost of a less compact
representation w.
First local models segmented the face manually [Basso and
Verri 2007; Kakadiaris et al. 2007; ter Haar and Veltkamp 2008].
Smet and Gool [2010] and Tena et al. [2011] propose automatic
ways of segmenting the faces into areas based on information
learned over the displacements of corresponding vertices in the
training set. Brunton et al. [2011] propose a model that combines
shape variations that are localized in different areas with a
multi-resolution framework that uses a wavelet decomposition of
the 3D face models. Fine-scale geometric detail can alternatively
be modeled using hierarchical pyramids that consider differences
between a smooth face and increasingly high-resolution geometry
representing, e.g., wrinkles [Golovinskiy et al. 2006].
It is also possible to perform localized analysis using different
statistical approaches than PCA. Neumann et al. [2013] propose
the use of sparse PCA combined with a group sparsity constraint to
identify localized deformation components over the training data.
Ferrari et al. [2015] follow a related idea and learn a dictionary of
deformation components over sampled regions for the application
of face recognition. Wu et al. [2016b] combine a local deformation
subspace model with an anatomical bone structure that acts as a
regularizer of the deformation. The local deformation subspace is
computed over overlapping localized patches, and the statistical
model explicitly factors the rigid and non-rigid deformations applied to each patch.
3.2 Expression Models
As simple linear models similar to the ones described can be used
to model expression variation for one subject, this section considers models that capture variations of both identity and expression.
Unlike simple linear models learned over a dataset of varying identities and expressions (e.g., Booth et al. [2017]), our focus is on
models that explicitly decouple the influence of identity and expression by modeling them in separate coefficients. We classify
these methods into additive, multiplicative, and nonlinear models,
depending on how the two sets of coefficients are combined.
3.2.1 Additive Models. Given two shapes of the same subject,
one with expression cexp and one neutral shape cne, Blanz and
Vetter [1999] transferred expressions between subjects by adding
the expression offsets Δc := cexp − cne to the neutral shape of
another subject.
Several other methods then built on this idea and model expression variations as an additive offset to an identity model with a
neutral expression. Formally, additive models are given by
c(ws , we ) = c¯ + Esws + Eewe , (3)
where c¯ is a mean, Es and Ee are the matrices of basis vectors of the
shape and expression space, and ws and we are the shape and expression coefficients. Note that the basis vectors of the expression
space can be interpreted as a data-driven blendshape model, where
the basis vectors are orthogonal and do not carry interpretable semantic meaning in general [Lewis et al. 2014a].
Starting with Blanz et al. [2003], several methods propose to
learn two PCA models, one over shape and one over expression
to derive Es and Ee , and to compute c¯ as the mean over training data, either in neutral expression or as sum of two means
(one over shape and one over expression). Blanz et al. [2003]
learned the expression space from a single subject captured in
multiple expressions. Amberg et al. [2008] extended this work to
include expression data from multiple subjects. This leads to a
statistical expression model that does not enable control over
specific facial expressions. It is therefore feasible for analysisby-synthesis tasks but limited for controlling or synthesizing
specific interpretable expression variation. Thies et al. [2015] use
ACM Transactions on Graphics, Vol. 39, No. 5, Article 157. Publication date: June 2020.
157:10 • B. Egger et al.
blendshapes as the basis vectors of the expression space. These
expression blendshapes are not orthogonal and hence information
of different blendshapes are potentially redundant.
3.2.2 Multiplicative Models. Another body of work models
shape and expression variations in a multiplicative manner. Li
et al. [2010] propose a method to adapt a pre-defined blendshape
model to a specific subject given a small number of static face
scans in different expressions, which provides a personalized facial rig. Bouaziz et al. [2013] combine a morphable shape model
c(ws) (Equation 2) with a set of de linear expression transfer operators Tj : R3n → R3n that transform the neutral shape to generate
personalized blendshapes. Formally, this model is defined as
c(ws , we ) =

de
j=1
we
j Tj

c(ws ) + δs 
+ δe
j , (4)
where δs and δe
j are corrective vectors to adapt the blendshapes
to the tracked subject, and we
j is the jth coefficient of we .
A commonly used multiplicative model is the multilinear model
that extends the idea of PCA of performing a singular value decomposition to tensor data by performing a higher-order tensor
decomposition (HOSVD) of 3D face data stacked into a training
tensor. In particular, given a training set of different identities all
captured in the same set of expressions, the vertex coordinates are
stacked into a data tensor on which HOSVD is performed. This allows to model correlations of shape changes caused by identities
and expressions. This model was first applied to 3D face modeling
by Vlasic et al. [2005a] and can be defined as
c(w2, w3) = M ×2 ws ×3 we , (5)
where M ∈ R3n×ds×de denotes the multilinear model tensor, and
×i denotes the tensor mode-product. Thanks to its expressiveness
and simplicity, this model is being used extensively for various
applications [Bolkart and Wuhrer 2015a; Dale et al. 2011; Fried
et al. 2016; Mpiperis et al. 2008; Yang et al. 2012]. To allow modeling localized variations, the multilinear model has been applied
to wavelet coefficients at different levels of detail [Brunton et al.
2014a].
Computing a multilinear model with HOSVD requires a complete tensor of data, where each identity needs to be present in all
expressions, and the data need to be in semantic correspondence
specified by expression labels. This severely limits the kind of data
that can be used for training. Recently, a number of methods have
been proposed to address this limitation using an optimization
approach [Bolkart and Wuhrer 2016], a custom tensor decomposition method [Wang et al. 2017], and an autoencoder structure
[Fernández Abrevaya et al. 2018], respectively.
3.2.3 Nonlinear Models. Facial shape and expression are
mostly modeled with a linear subspace, often assuming a Gaussian prior distribution. Few methods exist to model facial variations
with nonlinear transformations. Li et al. [2017] introduce FLAME,
an articulated expressive head model that provides nonlinear control over facial expressions by combining jaw articulation with linear expression blendshapes. Ichim et al. [2017] use a muscle activation model driven by physical simulation. Koppen et al. [2018]
instead of a single Gaussian distribution use a Gaussian mixture
model to represent facial shape and texture. In another line of
work, Shin et al. [2014] capture facial wrinkles in multi-scale maps
and nonlinearly transfer them to other faces to enhance realism.
Recently, several deep learning–based models were published
that fall into this group of nonlinear models [Bagautdinov et al.
2018; Lombardi et al. 2018; Ranjan et al. 2018; Tewari et al. 2019,
2018; Tran and Liu 2018a]. Section 6 covers these models in more
detail.
3.3 Appearance Models
This section describes approaches for modeling the facial appearance, where we distinguish between linear and nonlinear models.
The appearance of a face is influenced by its albedo and illumination. However, most 3DMMs do not completely separate these factors, so oftentimes the illumination is baked into the albedo. Hence,
in the following, we call the problem of statistically capturing this
information appearance modeling. The most common way to build
an appearance model is by performing statistics on appearance information of the training shapes, where the appearance information is usually either represented in terms of per-vertex values or
as a texture in uv-space.
3.3.1 Linear Per-vertex Models. Usually, color information is
modeled as a low-dimensional subspace that explains the color
variations. This leads to an analogous model to the linear shape
model:
d(wt ) = ¯
d + Etwt , (6)
where ¯
d and Et share the same number of rows as c¯ and E and wt
is the low-dimensional texture parameter vector.
Booth et al. [2017] and Booth et al. [2018b] use a convex matrix factorization formulation for learning a per-vertex appearance
model from images based on back-projection, where it is assumed
that the 3D geometry of the face in the image is known. Their
appearance model is not built using the color images directly but
rather features computed from the images, for example, SIFT. This
brings advantages that the features may be somewhat invariant to
illumination changes and also that they depend on a local neighborhood that may widen the basin of convergence. In a similar
vein, Wang et al. [2009] construct a linear model of spherical harmonic bases (see Section 4). This jointly models texture (more precisely, diffuse albedo) and fine-scale shape (surface normal orientation) such that appearance under any illumination can be synthesized as a linear function of the basis.
3.3.2 Linear Texture-space Models. A downside of per-vertex
models is that they require compatible resolutions between the
shape and appearance representation. This is rather uncommon in
computer graphics, where usually a low(er) resolution geometry
model (oftentimes including normals) is used in conjunction with
a high(er) resolution 2D texture map. Working with a 2D texture
also has other advantages, such as the possibility of using image
processing techniques to modify the texture maps. With that, such
a representation is also amenable for being processed by convolutional neural networks (CNNs), as will be addressed in the next
section.
We now turn our attention towards works that build linear appearance models in texture space. The original work by Blanz and
ACM Transactions on Graphics, Vol. 39, No. 5, Article 157. Publication date: June 2020.