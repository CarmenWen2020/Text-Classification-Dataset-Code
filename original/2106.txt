Current AR systems only track sparse geometric features but do not compute
depth for all pixels. For this reason, most AR effects are pure overlays that
can never be occluded by real objects. We present a novel algorithm that
propagates sparse depth to every pixel in near realtime. The produced depth
maps are spatio-temporally smooth but exhibit sharp discontinuities at depth
edges. This enables AR effects that can fully interact with and be occluded by
the real scene. Our algorithm uses a video and a sparse SLAM reconstruction
as input. It starts by estimating soft depth edges from the gradient of optical
flow fields. Because optical flow is unreliable near occlusions we compute
forward and backward flow fields and fuse the resulting depth edges using
a novel reliability measure. We then localize the depth edges by thinning
and aligning them with image edges. Finally, we optimize the propagated
depth smoothly but encourage discontinuities at the recovered depth edges.
We present results for numerous real-world examples and demonstrate the
effectiveness for several occlusion-aware AR video effects. To quantitatively
evaluate our algorithm we characterize the properties that make depth maps
desirable for AR applications, and present novel evaluation metrics that
capture how well these are satisfied. Our results compare favorably to a set
of competitive baseline algorithms in this context.
CCS Concepts: • Computing methodologies → Reconstruction; Mixed
/ augmented reality; Computational photography; Video segmentation;

Additional Key Words and Phrases: Augmented Reality, 3D Reconstruction,
Video Analysis, Depth Estimation, Simultaneous Localization and Mapping

1 INTRODUCTION
Augmented reality (AR) is a transformative technology that blurs
the line between reality and the virtual world by enhancing a live
video stream with interactive computer generated 3D content. AR
has many applications ranging from bringing virtual monsters into
your bedroom (gaming), to previewing virtual furniture in your
living room (shopping), or even breaking down the mechanics of
your car’s engine (learning), among many others.
Recent advancements in mobile hardware and tracking technology enable consumers to experience AR directly on their cell phone
screens. This is typically powered by SLAM algorithms, such as
Google’s ARCore1
and Apple’s ARKit2
, which track a few dozen
scene points in 3D and compute the 6-DOF trajectory of the device.
Once the camera pose is known, we can overlay 3D content on the
image, such as face masks or artificial animated characters.
However, since the tracked points are sparse, these effects cannot
interact with the scene geometry, because it is not known for most
pixels. This means that virtual objects can never be occluded by real
objects (e.g., the Pusheen plush doll in Figure 1a). The absence of
occlusion is often jarring and can break the illusion of reality.
In this work we propose a method that overcomes this limitation
by densifying the sparse 3D points, so that depth is known at every
1https://developers.google.com/ar/
2https://developer.apple.com/arkit/
ACM Transactions on Graphics, Vol. 37, No. 6, Article 194. Publication date: November 2018.
194:2 • A. Holynski and J. Kopf
pixel. This enables a much richer set of effects that fully interact
with the scene geometry and make use of occlusions (Figure 1b).
Multi-view stereo (MVS) methods can be used to reconstruct
dense geometry from multiple images, but they are often not the
best tool to achieve our goal, since they often (1) are not designed for
video sequences and produce temporal artifacts such as flickering,
(2) produce noisy results in untextured regions, (3) leave holes when
omitting unconfident pixels, (4) frequently suffer from misaligned
depth edges (“edge-fattening”), and (5) are prohibitively slow.
In addition, the requirements for dense, occlusion-aware AR applications are notably different from MVS. While the primary measure
that these methods optimize is geometric accuracy of depth and normals, this property is less critical for AR. Instead, we are interested
in a different set of objectives:
Sharp discontinuities: depth edges must be sharp and well-aligned
with image edges to produce convincing occlusions.
Smoothness: away from discontinuities, and in particular across
texture edges, the depth should be smooth to avoid spurious
intersections with virtual objects.
Temporal coherence: the depth needs to be consistent across frames
to avoid flickering.
Completeness: every pixel must have an assigned depth, so we can
apply the effect everywhere.
Speed: for real-time AR effects we need to compute results at fast
rates and with little delay.
We designed our method to satisfy all of these objectives. It takes
the sparse points computed by a SLAM system as input and propagates their depths to the remaining pixels in a smooth, depth-edge
aware, and temporally coherent fashion. The algorithm starts with
computing a soft depth edge likelihood map by merging forward and
backward optical flow fields using a novel reliability measure. Using
a future frame causes a slight delay in the output, which depends on
keyframe spacing, but is enforced to be at most 116ms in our experiments. A variant of our method can be run in a fully causal fashion,
i.e., without any latency, at the expense of somewhat lower result
quality. The depth edges are thinned and aligned with the image
edges using an extension of the Canny edge detector that takes the
soft depth edges into account. Finally, we densify the sparse points
by optimizing the propagation of their depths smoothly (except at
depth edges) and in a temporally coherent manner.
Our method runs at near-realtime rates on a desktop machine,
and modern mobile processor benchmarks indicate that a fast mobile phone implementation should also be possible. We have tested
our method on a variety of video sequences, and compared against
a set of state-of-the-art baseline algorithms. We also designed evaluation metrics that capture the objectives stated above and perform
extensive numerical evaluations. We demonstrate the effectiveness
for AR applications with two example effects that make use of dense
depth.
2 PREVIOUS WORK
In this section, we highlight some of the work in related areas.
SLAM. Simultaneous localization and mapping algorithms [Engel
et al. 2018, 2014; R. Mur-Artal and Tardos 2015] compute the camera
trajectory as well as a geometric scene representation from a video
stream. This problem is related to Structure from Motion, but a
fundamental difference is that SLAM techniques are optimized for
realtime applications and specificially designed for video sequences.
SLAM algorithms are typically used for tracking in AR applications.
Most methods, however, track only a select set of independent
image features, which results in a sparse scene representation and
limits AR effects to pure overlays.
Dense SLAM. Some SLAM methods attempt to reconstruct a dense
depth map that covers all pixels in the video [M. Pizzoli 2014; Newcombe et al. 2011]. These methods are often slower, however, and
may be less accurate (see discussion Engel et al.’s paper [2018]). Similar to MVS methods (discussed below), they are also not explicitly
designed to have sharp depth discontinuities that are well-aligned
with image edges, which might result in artifacts at occlusion contours.
There are also intermediate, semi-dense approaches that reconstruct a subset of pixels [Engel et al. 2014]. However, these methods
have the same limitation w.r.t. virtual object occlusions as sparse
methods.
MVS. Multi-view stereo methods [Furukawa and Hernández 2015;
Seitz et al. 2006] compute dense geometry from overlapping images.
However, as stated in the introduction, their depth maps are highly
optimized for geometric accuracy, but might not work well for AR
applications. For example, most algorithms drop uncertain pixels
(e.g., in untextured areas) and edges in the estimated geometry are
often not well aligned with image edges.
Video-based Depth Estimation. Most stereo algorithms are not
designed to produce temporally coherent results for video sequences,
however, there are some exceptions. Zhang et al. [2009] optimize
multiple video frames jointly with an explicit geometric coherency
term. However, similar to many MVS algorithms the runtime is
prohibitively slow for our applications (several minutes per frame).
Hosni et al. [2011] use a weighted 3D box filter to spatio-temporally
smooth cost volumes before extracting disparity values. Richardt et
al. [2010] use instead a bilateral grid to spatio-temporally smooth
a cost volume. Both methods require a desktop GPU to achieve interactive speeds. Stühmer et al [2010] estimate depth from multiple
optical flow fields. However, optical flow estimation is unreliable in
untextured regions, and it is slow at high quality settings.
Edge-aware Filtering. Sparse annotations, such as depth from
SLAM points, can be densified using edge-aware filtering techniques.
These techniques are typically very fast and differ in the kind of
smoothness constraints they impose on the solution.
Levin et al. [2004] propose a quadratic optimization to propagate
color scribbles on a grayscale image. A similar technique can be
used to propagate depth from sparse points [Shan et al. 2014].
A joint bilateral filter [Petschnigg et al. 2004] can also be used to
propagate sparse constraints while respecting intensity edges. The
bilateral solver [Barron and Poole 2016] can be used in a similar way,
e.g., in the Google Jump system [Anderson et al. 2016] it smoothes
optical flow fields in an edge-aware manner.
Weerasekera et al. [2018] use the output of a single-view depth
estimation network to propagate sparse depth values. Similarly,
ACM Transactions on Graphics, Vol. 37, No. 6, Article 194. Publication date: November 2018.
Fast Depth Densification for Occlusion-aware Augmented Reality • 194:3
(a) Input frame
(Full HD, 2 megapixels)
(b) SLAM points
(DSO-SLAM, 13.8ms)
(c) Flow to future / past
(DIS-FLOW, 0.4ms)
(d) Soft depth edges
(Section 4.2, 0.2ms)
(e) Localized depth edges
(Section 4.3, 12.6ms)
(f) Densification depth
(Section 4.4, 21.3ms)
Fig. 2. Overview of the major algorithm stages. Given an input video (a) we use existing methods to compute sparse SLAM points (b) and optical flow to a
nearby future and past frame (c). We fuse the most reliable responses from the forward and backward flow fields and compute “soft” depth edges that are not
well-localized, yet. We thin, localize, and binarize the depth edges (e). Finally, we propagate the sparse SLAM point depths to every pixel in a depth-edge-aware
manner (f ).
[Zhang and Funkhouser 2018] constrain the propagation of sparse
depth values using the output of a neural network which predicts
surface normals and occlusion boundaries. Both methods are tested
on desktop GPUs, and run at less than interactive framerates.
Park et al. [2014] describe techniques for upsampling and holefilling depth maps produced by active sensors using constrained
optimization. Bonneel et al. [2015] extend edge-aware filtering to
video and add temporal constraints to reduce flickering. A survey of
additional densification methods can be found in [Pan et al. 2018].
The drawback of many of these image-guided filtering techniques
is that the propagation stops not only at depth edges, but also at
texture edges. This can lead to false discontinuities in the depth
maps. Additionally, many of these methods are prohibitively slow
for real-time AR applications. We compare our method to three of
the above filtering techniques in Section 5.4.
3 OVERVIEW
The inputs to our algorithm are (1) a sequence of video frames,
typically captured with a cell phone camera, (2) camera parameters
at every frame, and (3) sparse depth annotation (at least for some
frames). We use an existing SLAM system [Engel et al. 2018] to
compute (2) and (3). Our algorithm propagates the sparse depth
annotation to every pixel in near realtime and with only a short
delay of a few frames.
As mentioned before, the criteria that make depth maps desirable
for AR applications are different from the goals that most MVS algorithms are optimized for, since we neither require correct absolute
(or metric) depth, nor do we require perfect object normals. Rather,
our algorithm is designed to produce approximately correct depth
that satisfies the criteria stated in Section 1:
Discontinuities: we estimate the location of depth edges and produce
sharp discontinuities across them.
Smoothness: our depth maps are smooth everywhere else, in particular across texture edges.
Temporal coherence: we optimize consistency of depth over time.
Completeness: by design we propagate depth to every pixel.
Speed: our algorithm takes on average 48.4ms per frame and the
output is delayed by at most 116ms. A causal variant of the
method has no delay.
Our algorithm proceeds in three stages:
(1) Estimate soft depth edges (Figure 2d, Section 4.2): First, we
examine the gradient of optical flow fields to estimate “soft”
depth edges that are not well-localized, yet. Because optical
flow is unreliable near occlusions we compute flow fields to a
future and past frame (Figure 2c) and fuse the resulting depth
edges using the observation that edges in the flow gradient
are only reliable when the flow vectors are diverging.
(2) Localize depth edges (Figure 2e, Section 4.3): Next, we localize
the depth edges using a modified version of the Canny edge
detector [Canny 1986]. This procedure thins the edges and
aligns them with the image gradient, so that they are precisely
localized on the center of image edges. It uses hysteresis to
avoid fluctuations in weak response regions.
(3) Densification (Figure 2f, Section 4.4): Finally, we propagate the
sparse input depth to every pixel by solving a Poisson problem.
The data term of the problem is designed to approximate the
sparse input depth and to encourage temporal continuity,
while the smoothness term encourages sharp discontinuities
at the detected depth edges and a smooth result everywhere
else.
4 METHOD
4.1 Camera Parameters and Sparse Depth
The first stage of our algorithm computes a “sparse reconstruction”
using a SLAM system. This computes two entities that are required
for the subsequent stages:
(1) Extrinsic camera parameters for every frame (i.e., rotation and
translation); we assume the intrinsic parameters are known.
ACM Transactions on Graphics, Vol. 37, No. 6, Article 194. Publication date: November 2018.
194:4 • A. Holynski and J. Kopf
(a) Past nearby frame Ipast (b) Current frame I (c) Future nearby frame Ifut
(d) Flow I → Ipast (e) Occluded in nearby (f) Flow I → Ifut
(g) Past grad-mag Mpast (h) Reliability-fused MF (i) Future grad-mag Mfut
Fig. 3. For a current frame from the Kitchen sequence (b) we select two
surrounding nearby frames (a,c) and compute optical flow (d,f ). The flow
fields are unreliable near pixels in the current frame that are occluded in
the nearby frames (e, with manual annotation for illustration). The depth
edges from the corresponding gradient magnitude images are unreliable
as well (g,i). Using our reliability measure (see Figure 4) we can compute a
fused result that contains only the most reliable pixels (h).
(2) Sparse 3D scene points (Figure 2b). Our algorithm will propagate their depth to the remaining pixels in a later stage to
generate the dense result.
We experimented with various existing systems and settled on
using DSO-SLAM [Engel et al. 2018], since it is fast and robust.
Since DSO only provides 3D points at intermittent key frames, we
reproject these to the surrounding non-key frames, so every frame
has a sparse depth source.
4.2 Soft Depth Edges
The goal of this stage is to find “soft” depth edges, which means
that they are defined by a continuous strength value and do not
need to be accurately localized (Figure 2d). They will be thinned,
binarized, and aligned with the image edges in the next section. For
performance reasons we compute the soft edges on downscaled
images (1/4 in each dimension) and upscale the results at the end of
the stage.
We start by selecting a nearby frame with sufficiently large baseline to the current frame, and compute a dense optical flow field.
In the flow field we can identify depth edges at places where the
gradient magnitude is high, because sudden changes in depth imply
corresponding changes in the flow, due to parallax (Figure 3, bottom
row).
Unfortunately, optical flow is unreliable around pixels that are
occluded in the nearby frame (compare Figures 3d and 3f with Figure 3e and note how only one of the two edges of the object are
resolved in each flow image, respectively). We alleviate the situation
by computing flow w.r.t. two nearby frames, one backward and one
forward in time. These tend to have different sets of pixels that
are unreliable (see manual annotations in Figure 3e). We fuse the
two depth edge maps using a novel optical flow reliability measure, described below, to achieve a result that contains the correct
edges from both (Figure 3h). In the supplementary material, we
show a comparison of the reliability merging and the more naive
approach of taking the per-element gradient maximum. As can be
seen in the Bones sequence, the naive approach drastically increases
the noise in the occlusion response, and thus the occlusion edges
are less prominently defined. We also provide comparisons with
off-the-shelf boundary estimation methods, such as [Dollár et al.
2006], showing that our use of flow gradients more frequently disambiguates occlusion boundaries from texture edges, such as in the
soft edge response of the Georgian sequence.
Selecting Nearby Frames. The nearby frames need to provide sufficient translational motion so we achieve a strong flow gradient
response and reduce noise. There are many sensible ways in which
these could be selected; we use the following simple heuristic. We
compute the spatial distance between the camera positions of the
two DSO key frames that bracket the current frame. Then, we look
forward and backward from the current frame and select the first
frame in either direction whose camera position is at least half that
distance away.
Optical Flow. We compute optical flow using DIS Flow [Kroeger
et al. 2016], because it is one of the fastest available methods. We use
the implementation in OpenCV and chose the ultrafast preset.
Since the results exhibit a block pattern we smooth it using a 7 × 7
median filter.
Computing the Gradient Magnitude. Let F be one of the two flow
images from the current frame to one of the nearby frames. We
compute the gradient magnitude M, at every pixel p selecting the x
or y component, whichever provides the higher value:
M(p) = max


∇Fx (p)




1
,



∇Fy (p)




1

. (1)
Fusing Forward and Backward Results. As described above, different sets of pixels are reliable in the two flow fields. We observe that
places where the edge-normal flow projections f are diverging are
generally more reliable; this situation occurs at disocclusions, when
both pixels are visible in the nearby image. The opposite is true for
converging flow projections; this indicates an occlusion: one of the
two pixels is not visible in the nearby image and therefore its flow
vector cannot be accurately estimated.
We turn this observation into a per-pixel reliability score as follows. Given a pixel of interest p, we find two helper pixels p0 and
p1 that are offset at unit distance in the gradient direction d and its
opposite. We compute the projection of the flow vectors at p0 and
ACM Transactions on Graphics, Vol. 37, No. 6, Article 194. Publication date: November 2018.
Fast Depth Densification for Occlusion-aware Augmented Reality • 194:5
Fig. 4. Our flow gradient reliability measure explained on two examples
on a crop from Figure 3. We measure the flow at two helper pixels p0, p1
on either side of the edge, and compute the projections f0, f1 onto the
line perpendicular to the gradient. Left example: the flow projections are
diverging (f1 − f0 > 0), which indicates a reliable edge, since both pixels
are visible in the nearby image. Right example: the flow projections are
converging (f1 − f0 < 0), which indicates an unreliable edge, since at least
one of the pixels might not be visible in the nearby image.
(a) Color image (b) Initial soft depth
edges MF
(c) Spatio-temporal
filtered MeF
Fig. 5. (b) Depth edges are well identified in the flow gradient image, but not
well aligned with the image edges. (c) We apply spatio-temporal filtering to
reduce noise and ensure the depth edges overlap with their corresponding
image edges. Now they are ready for alignment with the image edges (see
Figure 6).
p1 on d:
f0 = F (p0) · d, f1 = F (p1) · d, (2)
and obtain the reliability score as their difference,
r = f1 − f0. (3)
r will be positive (reliable) for diverging flow projections, and
negative (unreliable) for converging flow projections. Figure 4 illustrates this on two examples.
Now we can fuse the gradient magnitude from both nearby images
by selecting at each pixel the more reliable quantity:
MF (p) =

Mprev (p), if rprev (p) > rnext(p)
Mnext(p), else (4)
Filtering. The fused gradient magnitude MF identifies depth edges
well without getting confused by texture edges (Figure 5a-b). However, they are not well aligned with color images. To make the
alignment with image edges in the following stage easier, we blur
MF with a wide box filter of size kF = 31, to ensure edges overlap
with their corresponding image edges.
We also suppress noise by applying a temporal median filter that
includes samples from kT = 7 frames. We determine the temporal
neighbors of pixels by estimating a per-frame homography warp
from the SLAM points (using the OpenCV findHomography() function).
Finally, we normalize MF by dividing it by the 90th percentile
value. This makes the parameter settings more invariant to the video
content.
The final spatio-temporally filtered soft depth edges MeF are
shown in Figure 5c.
4.3 Localizing the Depth Edges
In this section, we accurately localize the depth edges by thinning
and binarizing them, and aligning them with the color image edges.
We achieve these goals by modifying the Canny edge detector [1986],
which performs a similar operation just on intensity image edges.
Recall the basic operation of the Canny detector:
(1) Intensity gradient magnitude: Compute the (blurred) gradient
magnitude of the intensity image MeI
(we normalize it by
dividing out the 90th percentile).
(2) Non-maximum suppression: all values of MeI except the local maxima are suppressed, to thin edges down to a width of a single
pixel.
(3) Double thresholding: Using two thresholds τhigh and τlow the
edge pixels are classified into strong (MeI >τhigh), weak (τhigh ≥
MeI ≥τlow), and suppressed edge pixels (τlow >MeI
).
(4) Hysteresis: Every strong edge pixel is selected, but weak edge
pixels are only selected when they are connected to a strong
edge. This effectively removes spurious weak edge pixels.
Figure 6c shows the result of applying the Canny detector on
a color image. The detector identifies and localizes all intensity
edges well. Notably, this set of edges includes also all major depth
edges. This indicates that we can achieve our goal by selectively
suppressing only the texture edges while keeping the depth edges.
We do so by injecting another threshold on the soft depth edge
map MeF into the algorithm. More precisely, we change the definition
of a strong edge pixel to require not just a high intensity gradient
response (MeI >τhigh) but also a high response in the soft edge map
(MeF > τflow). The definition of weak and suppressed edge pixels
remain unchanged.
With this modification we start depth edges only where the soft
edge map indicates a high confidence, but we allow continuing
them into low soft edge response regions as long as there remains
a sufficiently strong image edge. This helps bridging over gaps in
the soft edge map due to noise in the optical flow image. Figure 6e
shows the result of the modified detector. It effectively preserves
most of the depth edges while suppressing most texture edges.
4.4 Densification
In the final stage we use the localized depth edges to control the
propagation of sparse SLAM point depths to the remaining pixels. We set this up as a quadratic optimization problem, using the
following constraint terms.
ACM Transactions on Graphics, Vol. 37, No. 6, Article 194. Publication date: November 2018.
194:6 • A. Holynski and J. Kopf
(a) Image I (b) Magnitude of
intensity gradient MeI
(c) Standard Canny detector
using MeI
(d) Magnitude of
flow gradient MeD
(e) Modified Canny detector
using MeI and MeD
Fig. 6. Localizing and aligning the soft depth edges. (a-b) Input image and corresponding magnitude of intensity gradient. (c) An edge detector that uses this
image selects both texture and depth edges. Strong and weak edge pixels are drawn in bright and dark color, respectively. (d-e) We inject our soft edge map in
the detector algorithm, which results in suppressed texture edges. The remaining edges are mostly depth edges and well aligned with the intensity edges
(Note, that edges are drawn thick here for illustration purposes).
(a) Image I (b) Canny edges (c) Depth densified with Canny (d) Our depth edges (e) Our densification
Fig. 7. The standard Canny edge detector fires on texture edges (b), which causes incorrect discontinuities in the resulting depth map (c). Our algorithm
suppresses most texture edges (d), which leads to a refined depth map (e).
A unary data term encourages approximating the depth of the
SLAM points:
Edata(p) = wsparse(p)



D(p) − Dsparse(p)




2
2
. (5)
Dsparse is a depth map obtained by splatting the depths of SLAM
points into single pixels, and wsparse is 1 for all pixels that overlap a
SLAM point and 0 everywhere else.
A second unary data term encourages the solution to be temporally coherent:
Etemp(p) = wtemp(p)



D(p) − Dtemp(p)




2
2
. (6)
Dtemp is a depth map obtained by reprojecting the (dense) pixels of
the previous frame using the projection matrices estimated by the
SLAM system. wtemp is 1 for all pixels that overlap a reprojected
point and 0 everywhere else (splatting gaps, near boundaries).
We use a spatially varying pairwise smoothness term:
Esmooth(p,q) = wpq ∥D(p) − D(q)∥
2
2
, (7)
with the weight
wpq =

0, if B(p) + B(q) = 1,
max
1 − min(sp,sq), 0

, else. (8)
B denotes the binarized depth edge map, computed in the previous
section, and sp =

MeF · MeI

(p), sq =

MeF · MeI

(q). At depth edges
we set the weight to zero to allow the values to drift apart without
any penalty and form a crisp discontinuity. Everywhere else, we
enforce high smoothness if either MeI
is low (textureless regions) or
MeF is low (possibly texture edge but not a depth edge).
We obtain the following combined continuous quadratic optimization problem:
argmin
D
λd
Õ
p
Edata(p) + λt
Õ
p
Etemp(p) + λs
Õ
(p,q)∈N
Esmooth(p,q), (9)
where N is the set of horizontally and vertically neighboring pixels.
We use the balancing coefficients λd = 1, λt = 0.01, λs = 1.
The solution to Equation 9 is a set of sparse linear equations. It is,
in fact, a standard Poisson problem, for which we have specialized
ACM Transactions on Graphics, Vol. 37, No. 6, Article 194. Publication date: November 20