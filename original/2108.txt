We present a novel approach that enables photo-realistic re-animation of
portrait videos using only an input video. In contrast to existing approaches
that are restricted to manipulations of facial expressions only, we are the first
to transfer the full 3D head position, head rotation, face expression, eye gaze,
and eye blinking from a source actor to a portrait video of a target actor. The
core of our approach is a generative neural network with a novel space-time
architecture. The network takes as input synthetic renderings of a parametric
face model, based on which it predicts photo-realistic video frames for a
given target actor. The realism in this rendering-to-video transfer is achieved
by careful adversarial training, and as a result, we can create modified target
videos that mimic the behavior of the synthetically-created input. In order
to enable source-to-target video re-animation, we render a synthetic target
video with the reconstructed head animation parameters from a source
video, and feed it into the trained network – thus taking full control of the
target. With the ability to freely recombine source and target parameters,
we are able to demonstrate a large variety of video rewrite applications
without explicitly modeling hair, body or background. For instance, we can
reenact the full head using interactive user-controlled editing, and realize
high-fidelity visual dubbing. To demonstrate the high quality of our output,
we conduct an extensive series of experiments and evaluations, where for
instance a user study shows that our video edits are hard to detect.
CCS Concepts: • Computing methodologies → Computer graphics;
Neural networks; Appearance and texture representations; Animation; Rendering;
Additional Key Words and Phrases: Facial Reenactment, Video Portraits,
Dubbing, Deep Learning, Conditional GAN, Rendering-to-Video Translation1 INTRODUCTION
Synthesizing and editing video portraits, i.e., videos framed to show
a person’s head and upper body, is an important problem in computer graphics, with applications in video editing and movie postproduction, visual effects, visual dubbing, virtual reality, and telepresence, among others. In this paper, we address the problem of
ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018.
163:2 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt
synthesizing a photo-realistic video portrait of a target actor that
mimics the actions of a source actor, where source and target can be
different subjects. More specifically, our approach enables a source
actor to take full control of the rigid head pose, face expressions and
eye motion of the target actor; even face identity can be modified to
some extent. All of these dimensions can be manipulated together or
independently. Full target frames, including the entire head and hair,
but also a realistic upper body and scene background complying
with the modified head, are automatically synthesized.
Recently, many methods have been proposed for face-interior
reenactment [Liu et al. 2001; Olszewski et al. 2017; Suwajanakorn
et al. 2017; Thies et al. 2015, 2016; Vlasic et al. 2005]. Here, only
the face expression can be modified realistically, but not the full
3D head pose, including a consistent upper body and a consistently
changing background. Many of these methods fit a parametric 3D
face model to RGB(-D) video [Thies et al. 2015, 2016; Vlasic et al.
2005], and re-render the modified model as a blended overlay over
the target video for reenactment, even in real time [Thies et al.
2015, 2016]. Synthesizing a complete portrait video under full 3D
head control is much more challenging. Averbuch-Elor et al. [2017]
enable mild head pose changes driven by a source actor based on
image warping. They generate reactive dynamic profile pictures
from a static target portrait photo, but not fully reenacted videos.
Also, large changes in head pose cause artifacts (see Section 7.3),
the target gaze cannot be controlled, and the identity of the target
person is not fully preserved (mouth appearance is copied from the
source actor).
Performance-driven 3D head animation methods [Cao et al. 2015,
2014a, 2016; Hu et al. 2017; Ichim et al. 2015; Li et al. 2015; Olszewski
et al. 2016; Weise et al. 2011] are related to our work, but have
orthogonal methodology and application goals. They typically drive
the full head pose of stylized 3D CG avatars based on visual source
actor input, e.g., for games or stylized VR environments. Recently,
Cao et al. [2016] proposed image-based 3D avatars with dynamic
textures based on a real-time face tracker. However, their goal is
full 3D animated head control and rendering, often intentionally in
a stylized rather than a photo-realistic fashion.
We take a different approach that directly generates entire photorealistic video portraits in front of general static backgrounds under
full control of a target’s head pose, facial expression, and eye motion. We formulate video portrait synthesis and reenactment as
a rendering-to-video translation task. Input to our algorithm are
synthetic renderings of only the coarse and fully-controllable 3D
face interior model of a target actor and separately rendered eye
gaze images, which can be robustly and efficiently obtained via
a state-of-the-art model-based reconstruction technique. The input is automatically translated into full-frame photo-realistic video
output showing the entire upper body and background. Since we
only track the face, we cannot actively control the motion of the
torso or hair, or control the background, but our rendering-to-video
translation network is able to implicitly synthesize a plausible body
and background (including some shadows and reflections) for a
given head pose. This translation problem is tackled using a novel
space-time encoder–decoder deep neural network, which is trained
in an adversarial manner.
At the core of our approach is a conditional generative adversarial
network (cGAN) [Isola et al. 2017], which is specifically tailored
to video portrait synthesis. For temporal stability, we use a novel
space-time network architecture that takes as input short sequences
of conditioning input frames of head and eye gaze in a sliding
window manner to synthesize each target video frame. Our target
and scene-specific networks only require a few minutes of portrait
video footage of a person for training. To the best of our knowledge,
our approach is the first to synthesize full photo-realistic video
portraits of a target person’s upper body, including realistic clothing
and hair, and consistent scene background, under full 3D control of
the target’s head. To summarize, we make the following technical
contributions:
• A rendering-to-video translation network that transforms
coarse face model renderings into full photo-realistic portrait
video output.
• A novel space-time encoding as conditional input for temporally coherent video synthesis that represents face geometry,
reflectance, and motion as well as eye gaze and eye blinks.
• A comprehensive evaluation on several applications to demonstrate the flexibility and effectiveness of our approach.
We demonstrate the potential and high quality of our method in
many intriguing applications, ranging from face reenactment and
visual dubbing for foreign language movies to user-guided interactive editing of portrait videos for movie postproduction. A comprehensive comparison to state-of-the-art methods and a user study
confirm the high fidelity of our results.
2 RELATED WORK
We discuss related optimization and learning-based methods that
aim at reconstructing, animating and re-writing faces in images
and videos, and review relevant image-to-image translation work.
For a comprehensive overview of current methods we refer to a
recent state-of-the-art report on monocular 3D face reconstruction,
tracking and applications [Zollhöfer et al. 2018].
Monocular Face Reconstruction. Face reconstruction methods aim
to reconstruct 3D face models of shape and appearance from visual
data. Optimization-based methods fit a 3D template model, mainly
the inner face region, to single images [Blanz et al. 2004; Blanz
and Vetter 1999], unstructured image collections [KemelmacherShlizerman 2013; Kemelmacher-Shlizerman et al. 2011; Roth et al.
2017] or video [Cao et al. 2014b; Fyffe et al. 2014; Garrido et al. 2016;
Ichim et al. 2015; Shi et al. 2014; Suwajanakorn et al. 2014; Thies et al.
2016; Wu et al. 2016]. Recently, Booth et al. [2018] proposed a largescale parametric face model constructed from almost ten thousand
3D scans. Learning-based approaches leverage a large corpus of
images or image patches to learn a regressor for predicting either
3D face shape and appearance [Richardson et al. 2016; Tewari et al.
2017; Tran et al. 2017], fine-scale skin details [Cao et al. 2015], or
both [Richardson et al. 2017; Sela et al. 2017]. Deep neural networks
have been shown to be quite robust for inferring the coarse 3D
facial shape and appearance of the inner face region, even when
trained on synthetic data [Richardson et al. 2016]. Tewari et al.
[2017] showed that encoder–decoder architectures can be trained
fully unsupervised on in-the-wild images by integrating physical
ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018.
Deep Video Portraits • 163:3
Fig. 2. Deep video portraits enable a source actor to fully control a target video portrait. First, a low-dimensional parametric representation (left) of both
videos is obtained using monocular face reconstruction. The head pose, expression and eye gaze can now be transferred in parameter space (middle). We do not
focus on the modification of the identity and scene illumination (hatched background), since we are interested in reenactment. Finally, we render conditioning
input images that are converted to a photo-realistic video portrait of the target actor (right). Obama video courtesy of the White House (public domain).
image formation into the network. Richardson et al. [2017] trained
an end-to-end regressor to recover facial geometry at a coarse and
fine-scale level. Sela et al. [2017] use an encoder–decoder network
to infer a detailed depth image and a dense correspondence map,
which serve as a basis for non-rigidly deforming a template mesh.
Still, none of these methods creates a fully generative model for the
entire head, hair, mouth interior, and eye gaze, like we do.
Video-based Facial Reenactment. Facial reenactment methods rewrite the face content of a target actor in a video or image by transferring facial expressions from a source actor. Facial expressions
are commonly transferred via dense motion fields [Averbuch-Elor
et al. 2017; Liu et al. 2001; Suwajanakorn et al. 2015], parameters
[Thies et al. 2016, 2018; Vlasic et al. 2005], or by warping candidate
frames that are selected based on the facial motion [Dale et al. 2011],
appearance metrics [Kemelmacher-Shlizerman et al. 2010] or both
[Garrido et al. 2014; Li et al. 2014]. The methods described above
first reconstruct and track the source and target faces, which are
represented as a set of sparse 2D landmarks or dense 3D models.
Most approaches only modify the inner region of the face and thus
are mainly intended for altering facial expressions, but they do not
take full control of a video portrait in terms of rigid head pose, facial
expression, and eye gaze. Recently, Wood et al. [2018] proposed an
approach for eye gaze redirection based on a fitted parametric eye
model. Their approach only provides control over the eye region.
One notable exception to pure facial reenactment is AverbuchElor et al.’s approach [2017], which enables the reenactment of a
portrait image and allows for slight changes in head pose via image
warping [Fried et al. 2016]. Since this approach is based on a single
target image, it copies the mouth interior from the source to the
target, thus preserving the target’s identity only partially. We take
advantage of learning from a target video to allow for larger changes
in head pose, facial reenactment, and joint control of the eye gaze.
Visual Dubbing. Visual dubbing is a particular instance of face
reenactment that aims to alter the mouth motion of the target actor
to match a new audio track, commonly spoken in a foreign language
by a dubbing actor. Here, we can find speech-driven [Bregler et al.
1997; Chang and Ezzat 2005; Ezzat et al. 2002; Liu and Ostermann
2011; Suwajanakorn et al. 2017] or performance-driven [Garrido
et al. 2015; Thies et al. 2016] techniques. Speech-driven dubbing techniques learn a person-specific phoneme-to-viseme mapping from a
training sequence of the actor. These methods produce accurate lip
sync with visually imperceptible artifacts, as recently demonstrated
by Suwajanakorn et al. [2017]. However, they cannot directly control the target’s facial expressions. Performance-driven techniques
overcome this limitation by transferring semantically-meaningful
motion parameters and re-rendering the target model with photorealistic reflectance [Thies et al. 2016], and fine-scale details [Garrido
et al. 2015, 2016]. These approaches generalize better, but do not
edit the head pose and still struggle to synthesize photo-realistic
mouth deformations. In contrast, our approach learns to synthesize
photo-realistic facial motion and actions from coarse renderings,
thus enabling the synthesis of expressions and joint modification of
the head pose, with consistent body and background.
Image-to-image Translation. Approaches using conditional GANs
[Mirza and Osindero 2014], such as Isola et al.’s “pix2pix” [2017],
have shown impressive results on image-to-image translation tasks
which convert between images of two different domains, such as
maps and satellite photos. These combine encoder–decoder architectures [Hinton and Salakhutdinov 2006], often with skip-connections
[Ronneberger et al. 2015], with adversarial loss functions [Goodfellow et al. 2014; Radford et al. 2016]. Chen and Koltun [2017] were
the first to demonstrate high-resolution results with 2 megapixel
resolution, using cascaded refinement networks without adversarial
training. The latest trends show that it is even possible to train highresolution GANs [Karras et al. 2018] and conditional GANs [Wang
et al. 2018] at similar resolutions. However, the main challenge is
the requirement for paired training data, as corresponding image
pairs are often not available. This problem is tackled by CycleGAN
[Zhu et al. 2017], DualGAN [Yi et al. 2017], and UNIT [Liu et al.
2017] – multiple concurrent unsupervised image-to-image translation techniques that only require two sets of unpaired training
samples. These techniques have captured the imagination of many
people by translating between photographs and paintings, horses
ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018.
163:4 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt
and zebras, face photos and depth as well as correspondence maps
[Sela et al. 2017], and translation from face photos to cartoon drawings [Taigman et al. 2017]. Ganin et al. [2016] learn photo-realistic
gaze manipulation in images. Olszewski et al. [2017] synthesize a
realistic inner face texture, but cannot generate a fully controllable
output video, including person-specific hair. Lassner et al. [2017]
propose a generative model to synthesize people in clothing, and
Ma et al. [2017] generate new images of persons in arbitrary poses
using image-to-image translation. In contrast, our approach enables
the synthesis of temporally-coherent video portraits that follow the
animation of a source actor in terms of head pose, facial expression
and eye gaze.
3 OVERVIEW
Our deep video portraits approach provides full control of the head
of a target actor by transferring the rigid head pose, facial expression, and eye motion of a source actor, while preserving the target’s
identity and appearance. Full target video frames are synthesized,
including consistent upper body posture, hair and background. First,
we track the source and target actor using a state-of-the-art monocular face reconstruction approach that uses a parametric face and
illumination model (see Section 4). The resulting sequence of lowdimensional parameter vectors represents the actor’s identity, head
pose, expression, eye gaze, and the scene lighting for every video
frame (Figure 2, left). This allows us to transfer the head pose, expression, and/or eye gaze parameters from the source to the target,
as desired. In the next step (Figure 2, middle), we generate new
synthetic renderings of the target actor based on the modified parameters (see Section 5). In addition to a normal color rendering, we
also render correspondence maps and eye gaze images. These renderings serve as conditioning input to our novel rendering-to-video
translation network (see Section 6), which is trained to convert the
synthetic input into photo-realistic output (see Figure 2, right). For
temporally coherent results, our network works on space-time volumes of conditioning inputs. To process a complete video, we input
the conditioning space-time volumes in a sliding window fashion,
and assemble the final video from the output frames. We evaluate
our approach (see Section 7) and show its potential on several video
rewrite applications, such as full-head reenactment, gaze redirection,
video dubbing, and interactive parameter-based video control.
4 MONOCULAR FACE RECONSTRUCTION
We employ a state-of-the-art dense face reconstruction approach
that fits a parametric model of face and illumination to each video
frame. It obtains a meaningful parametric face representation for
the source Vs = {Is
f
| f = 1, . . . , Ns } and target Vt = {It
f
| f =
1, . . . , Nt } video sequence, where Ns and Nt denote the total number of source and target frames, respectively. Let P
• = {P•
f
| f =
1, . . . , N•} be the corresponding parameter sequence that fully describes the source or target facial performance. The set of reconstructed parameters encode the rigid head pose (rotation R
• ∈SO(3)
and translation t
• ∈ R
3
), facial identity coefficients α
• ∈ R
Nα (geometry, Nα = 80) and β
•
∈ R
Nβ (reflectance, Nβ = 80), expression
coefficients δ
•
∈R
Nδ (Nδ =64), gaze direction for both eyes e
• ∈R
4
,
and spherical harmonics illumination coefficients γ
• ∈ R
27. Overall,
our monocular face tracker reconstructs Np =261 parameters per
video frame. In the following, we provide more details on the face
tracking algorithm as well as the parametric face representation.
Parametric Face Representation. We represent the space of facial
identity based on a parametric head model [Blanz and Vetter 1999],
and the space of facial expressions via an affine model. Mathematically, we model geometry variation through an affine model v∈R
3N
that stacks per-vertex deformations of the underlying template mesh
with N vertices, as follows:
v(α,δ) = ageo +
Õ
Nα
k=1
αk b
geo
k
+
Õ
Nδ
k=1
δk b
exp
k
. (1)
Diffuse skin reflectance is modeled similarly by a second affine
model r∈R
3N that stacks the diffuse per-vertex albedo:
r(β) = aref +
Õ
Nβ
k=1
βk
b
ref
k
. (2)
The vectors ageo ∈ R
3N and aref ∈ R
3N store the average facial
geometry and corresponding skin reflectance, respectively. The
geometry basis {b
geo
k
}
Nα
k=1
has been computed by applying principal
component analysis (PCA) to 200 high-quality face scans [Blanz
and Vetter 1999]. The reflectance basis {b
ref
k
}
Nβ
k=1
has been obtained
in the same manner. For dimensionality reduction, the expression
basis {b
exp
k
}
Nδ
k=1
has been computed using PCA, starting from the
blendshapes of Alexander et al. [2010] and Cao et al. [2014b]. Their
blendshapes have been transferred to the topology of Blanz and
Vetter [1999] using deformation transfer [Sumner and Popović 2004].
Image Formation Model. To render synthetic head images, we
assume a full perspective camera that maps model-space 3D points
v via camera space vˆ ∈R
3
to 2D points p=Π(vˆ) ∈R
2 on the image
plane. The perspective mapping Π contains the multiplication with
the camera intrinsics and the perspective division. We assume a
fixed and identical camera for all scenes, i.e., world and camera space
are the same, and the face model accounts for all the scene motion.
Based on a distant illumination assumption, we use the spherical
harmonics (SH) basis functions Yb
: R
3 → R to approximate the
incoming radiance B from the environment:
B(ri
, ni
,γ) = ri
·
B
Õ2
b=1
γb
Yb
(ni). (3)
Here, B is the number of spherical harmonics bands, γb
∈R
3
are the
SH coefficients, and ri and ni are the reflectance and unit normal
vector of the i-th vertex, respectively. For diffuse materials, an average approximation error below 1 percent is achieved with only
B = 3 bands, independent of the illumination [Ramamoorthi and
Hanrahan 2001], since the incident radiance is in general a smooth
function. This results in B
2 =9 parameters per color channel.
Dense Face Reconstruction. We employ a dense data-parallel face
reconstruction approach to efficiently compute the parameters P
•
for both source and target videos. Face reconstruction is based on an
analysis-by-synthesis approach that maximizes photo-consistency
between a synthetic rendering of the model and the input. The
ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018.
Deep Video Portraits • 163:5
reconstruction energy combines terms for dense photo-consistency,
landmark alignment and statistical regularization:
E(X) = wphotoEphoto(X) + wlandEland(X) + wregEreg(X), (4)
with X ={R
•
, t
•
,α
•
, β
•
,δ
•
,γ
•
}. This enables the robust reconstruction of identity (geometry and skin reflectance), facial expression,
and scene illumination. We use 66 automatically detected facial
landmarks of the True Vision Solution tracker1
, which is a commercial implementation of Saragih et al. [2011], to define the sparse
alignment term Eland. Similar to Thies et al. [2016], we use a robust
ℓ1-norm for dense photometric alignment Ephoto. The regularizer
Ereg enforces statistically plausible parameter values based on the
assumption of normally distributed data. The eye gaze estimate e
•
is directly obtained from the landmark tracker. The identity is only
estimated in the first frame and is kept constant afterwards. All
other parameters are estimated every frame. For more details on
the energy formulation, we refer to Garrido et al. [2016] and Thies
et al. [2016]. We use a data-parallel implementation of iteratively
re-weighted least squares (IRLS), similar to Thies et al. [2016], to
find the optimal set of parameters. One difference to their work is
that we compute and explicitly store the Jacobian J and the residual
vector F to global memory based on a data-parallel strategy that
launches one thread per matrix/vector element. Afterwards, a dataparallel matrix–matrix/matrix–vector multiplication computes the
right- and left-hand side of the normal equations that have to be
solved in each IRLS step. The resulting small linear system (97×97
in tracking mode, 6 DoF rigid pose, 64 expression parameters and 27
SH coefficients) is solved on the CPU using Cholesky factorization
in each IRLS step. The reconstruction of a single frame takes 670 ms
(all parameters) and 250 ms (without identity, tracking mode). This
allows the efficient generation of the training corpus that is required
by our space-time rendering-to-video translation network (see Section 6). Contrary to Garrido et al. [2016] and Thies et al. [2016], our
model features dimensions to model eyelid closure, so eyelid motion
is captured well.
5 SYNTHETIC CONDITIONING INPUT
Using the method from Section 4, we reconstruct the face in each
frame of the source and unmodified target video. Next, we obtain the
modified parameter vector for every frame of the target sequence,
e.g., for full-head reenactment, we modify the rigid head pose, expression and eye gaze of the target actor. All parameters are copied
in a relative manner from the source to the target, i.e., with respect
to a neutral reference frame. Then we render synthetic conditioning
images of the target actor’s face model under the modified parameters using hardware rasterization. For higher temporal coherence,
our rendering-to-video translation network takes a space-time volume of conditioning images {Cf −o
|o=0, . . . , 10} as input, with f
being the index of the current frame. We use a temporal window of
size Nw =11, with the current frame being at its end. This provides
the network a history of the earlier motions.
For each frame Cf −o of the window, we generate three different
conditioning inputs: a color rendering, a correspondence image, and
an eye gaze image (see Figure 3). The color rendering shows the
1http://truevisionsolutions.net
Diffuse Rendering Correspondence Eye and Gaze Map
Fig. 3. The synthetic input used for conditioning our rendering-to-video
translation network: (1) colored face rendering under target illumination,
(2) correspondence image, and (3) the eye gaze image.
modified target actor model under the estimated target illumination,
while keeping the target identity (geometry and skin reflectance)
fixed. This image provides a good starting point for the following
rendering-to-video translation, since in the face region only the
delta to a real image has to be learned. In addition to this color input,
we also provide a correspondence image encoding the index of the
parametric face model’s vertex that projects into each pixel. To this
end, we texture the head model with a constant unique gradient
texture map, and render it. Finally, we also provide an eye gaze image
that solely contains the white region of both eyes and the locations
of the pupils as blue circles. This image provides information about
the eye gaze direction and blinking to the network.
We stack all Nw conditioning inputs of a time window in a 3D
tensor X of size W × H × 9Nw (3 images, with 3 channels each), to
obtain the input to our rendering-to-video translation network. To
process the complete video, we feed the conditioning space-time
volumes in a sliding window fashion. The final generated photorealistic video output is assembled directly from the output frames.
6 RENDERING-TO-VIDEO TRANSLATION
The generated conditioning space-time video tensors are the input to
our rendering-to-video translation network. The network learns to
convert the synthetic input into full frames of a photo-realistic target
video, in which the target actor now mimics the head motion, facial
expression and eye gaze of the synthetic input. The network learns to
synthesize the entire actor in the foreground, i.e., the face for which
conditioning input exists, but also all other parts of the actor, such as
hair and body, so that they comply with the target head pose. It also
synthesizes the appropriately modified and filled-in background,
including even some consistent lighting effects between foreground
and background. The network is trained for a specific target actor
and a specific static, but otherwise general scene background. Our
rendering-to-video translation network follows an encoder–decoder
architecture and is trained in an adversarial manner based on a
discriminator that is jointly trained. In the following, we explain
the network architectures, the used loss functions and the training
procedure in detail.
Network Architecture. We show the architecture of our renderingto-video translation network in Figure 4. Our conditional generative
adversarial network consists of a space-time transformation network
T and a discriminator D. The transformation network T takes the
W × H × 9Nw space-time tensor X as input and outputs a photoreal image T(X) of the target actor. The temporal input enables the
network to take the history of motions into account by inspecting
previous conditioning images. The temporal axis of the input tensor
is aligned along the network channels, i.e., the convolutions in the
ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018.
163:6 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt
Y
Bilinear Downsampling
T(X)
9�# 64 128 256 … 128 64 3 3 channels
256 D
X
…
T
64
128
256
128
64
64
32
32
64
128
TanH
256
256
128
32 64 128
=
BN LReLu Up
=
DeConv BN ReLu Refine
=
Up Refine Conv BN ReLu
=
Conv Drop Drop
in
out
in
out
3
×
3
4
×
4
Refine
prob.
prob.
stride 2
stride 2
stride 1
4
×
4
Fig. 4. Architecture of our rendering-to-video translation network for an
input resolution of 256×256: The encoder has 8 downsampling modules
with (64, 128, 256, 512, 512, 512, 512, 512) output channels. The decoder
has 8 upsampling modules with (512, 512, 512, 512, 256, 128, 64, 3) output
channels. The upsampling modules use the following dropout probabilities
(0.5, 0.5, 0.5, 0, 0, 0, 0, 0). The first downsampling and the last upsampling
module do not employ batch normalization (BN). The final non-linearity
(TanH) brings the output to the employed normalized [−1, +1]-space.
first layer have 9Nw channels. Note, we store all image data in
normalized [−1, +1]-space, i.e, black is mapped to [−1, −1, −1]
⊤ and
white is mapped to [+1, +1, +1]
⊤.
Our network consists of two main parts, an encoder for computing a low-dimensional latent representation, and a decoder for
synthesizing the output image. We employ skip connections [Ronneberger et al. 2015] to enable the network to transfer fine-scale
structure. To generate video frames with sufficient resolution, our
network also employs a cascaded refinement strategy [Chen and
Koltun 2017]. In each downsampling step, we use a convolution
(4 × 4, stride 2) followed by batch normalization and a leaky ReLU
non-linearity. The upsampling module is specifically designed to
produce high-quality output, and has the following structure: first,
the resolution is increased by a factor of two based on deconvolution (4 × 4, upsampling factor of 2), batch normalization, dropout
and ReLU. Afterwards, two refinement steps based on convolution
(3 × 3, stride 1, stays on the same resolution) and ReLU are applied.
The final hyperbolic tangent non-linearity (TanH) brings the output
tensor to the normalized [−1, +1]-space used for storing the image
data. For more details, please refer to Figure 4.
The input to our discriminator D is the conditioning input tensor
X (size W × H × 9Nw ), and either the predicted output image T(X)
or the ground-truth image, both of size W × H × 3. The employed
discriminator is inspired by the PatchGAN classifier, proposed by
Isola et al. [2017]. We extended it to take volumes of conditioning
images as input.
Objective Function. We train in an adversarial manner to find the
best rendering-to-video translation network:
T
∗ = argmin
T
max
D
EcGAN(T, D) + λEℓ1
(T). (5)
This objective function comprises an adversarial loss EcGAN(T, D)
and an ℓ1-norm reproduction loss Eℓ1
(T). The constant weight of
λ=100 balances the contribution of these two terms. The adversarial
loss has the following form:
EGAN(T, D) = EX,Y

log D(X, Y)

+ EX

log
1 − D(X, T(X))
. (6)
We do not inject a noise vector while training our network to produce deterministic outputs. During adversarial training, the discriminator D tries to get better at classifying given images as real or
synthetic, while the transformation network T tries to improve in
fooling the discriminator. The ℓ1-norm loss penalizes the distance
between the synthesized image T(X) and the ground-truth image Y,
which encourages the sharpness of the synthesized output:
Eℓ1
(T) = EX,Y

∥Y − T(X)∥1

. (7)
Training. We construct the training corpus T ={(Xi
, Yi)}i based
on the tracked video frames of the target video sequence. Typically,
two thousand video frames, i.e., about one minute of video footage,
are sufficient to train our network (see Section 7). Our training
corpus consists of Nt −(Nw −1) rendered conditioning space-time
volumes Xi and the corresponding ground-truth image Yi
(using a
window size of Nw =11). We train our networks using the TensorFlow [Abadi et al. 2015] deep learning framework. The gradients
for back-propagation are obtained using Adam [Kingma and Ba
2015]. We train for 31,000 iterations with a batch size of 16 (approx.
250 epochs for a training corpus of 2000 frames) using a base learning rate of 0.0002 and first momentum of 0.5; all other parameters
have their default value. We train our networks from scratch, and
initialize the weights based on a Normal distribution N(0, 0.2).
7 RESULTS
Our approach enables full-frame target video portrait synthesis under full 3D head pose control. We measured the runtime for training
and testing on an Intel Xeon E5-2637 with 3.5 GHz (16 GB RAM) and
an NVIDIA GeForce GTX Titan Xp (12 GB RAM). Training our network takes 10 hours for a target video resolution of 256×256 pixels,
and 42 hours for 512×512 pixels. Tracking the source actor takes
250 ms per frame (without identity), and the rendering-to-video
conversion (inference) takes 65 ms per frame for 256×256 pixels, or
196 ms for 512×512 pixels.
In the following, we evaluate the design choices of our deep video
portrait algorithm, compare to current state-of-the-art reenactment
approaches, and show the results of a large-scale web-based user
study. We further demonstrate the potential of our approach on several video rewrite applications, such as reenactment under full head
and facial expression control, facial expression reenactment only,
video dubbing, and live video portrait editing under user control.
In total, we applied our approach to 14 different target sequences
of 13 different subjects and used 5 different source sequences; see
Appendix A for details. A comparison to a simple nearest-neighbor
retrieval approach can be found in Figure 6 and in the supplemental
video. Our approach requires only a few minutes of target video
footage for training.
7.1 Applications
Our approach enables us to take full control of the rigid head pose,
facial expression, and eye motion of a target actor in a video portrait, thus opening up a wide range of video rewrite applications.
All parameter dimensions can be estimated and transfered from a
source video sequence or edited manually through an interactive
user interface.
ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018