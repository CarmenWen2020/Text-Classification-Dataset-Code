We present a fully automatic framework that digitizes a complete 3D head
with hair from a single unconstrained image. Our system offers a practical
and consumer-friendly end-to-end solution for avatar personalization in
gaming and social VR applications. The reconstructed models include secondary components (eyes, teeth, tongue, and gums) and provide animationfriendly blendshapes and joint-based rigs. While the generated face is a
high-quality textured mesh, we propose a versatile and efficient polygonal
strips (polystrips) representation for the hair. Polystrips are suitable for an
extremely wide range of hairstyles and textures and are compatible with
existing game engines for real-time rendering. In addition to integrating
state-of-the-art advances in facial shape modeling and appearance inference, we propose a novel single-view hair generation pipeline, based on
3D-model and texture retrieval, shape refinement, and polystrip patching
optimization. The performance of our hairstyle retrieval is enhanced using
a deep convolutional neural network for semantic hair attribute classification. Our generated models are visually comparable to state-of-the-art
game characters designed by professional artists. For real-time settings, we
demonstrate the flexibility of polystrips in handling hairstyle variations, as
opposed to conventional strand-based representations. We further show the
effectiveness of our approach on a large number of images taken in the wild,
and how compelling avatars can be easily created by anyone.
CCS Concepts: • Computing methodologies → Mesh geometry models; • Theory of computation → Machine learning theory;
Additional Key Words and Phrases: dynamic avatar, face, hair, digitization,
modeling, rigging, polystrip, texture synthesis, data-driven, deep learning,
deep convolutional neural network
1 INTRODUCTION
The onset of virtual reality (VR) and its entertainment applications
have highlighted how valuable and captivating the immersion of
alternate universes can be. VR and its democratization have the
potential to revolutionize 3D face-to-face communication and social
interactions through compelling digital embodiments of ourselves,
as demonstrated lately with the help of VR head mounted displays
with facial sensing capabilities [Li et al. 2015; Olszewski et al. 2016;
Thies et al. 2016b] or voice-driven technology demonstrated at Oculus Connect 3. In addition to enabling personalized gaming experiences, faithfully individualized 3D avatars could facilitate natural telepresence and interactions between remote participants in
virtual worlds, and potentially, one day, displace physical travels.
Meanwhile, companies such as Facebook and Snap are popularizing the use of augmented reality filters to alter selfie videos and
emerging tech startups such as Pinscreen [2017], FaceUnity [2017],
Loom.ai [2017], and itSeez3D [2017], are exploring the automatic
creation of 3D avatars for virtual chatting applications.
Recent progress in data-driven methods and deep learning research have catalyzed the development of high-quality 3D face modeling techniques from a single image [Cao et al. 2014b; Saito et al.
2017; Thies et al. 2016a]. Even the generation of realistic strand-level
hair models is possible from an image fully automatically [Chai et al.
2016]. However, despite efforts in real-time simulation [Chai et al.
2014], strand-based representations are still very difficult to integrate into game environments due to their rendering and simulation
complexity. Furthermore, strands are not efficient representations
for short hairstyles and ones with highly stochastic structures, such
as for curly hair. Cao et al. [2016] have recently introduced a system that uses a versatile image-based mesh representation, but it
requires the usage of multiple photographs and manual intervention, and the volumetric structure of hair is not captured. Despite
substantial advances in making avatar creation as easy as possible,
the barriers to entry are still too high for commodity user adoption.
In this paper, we present the first automatic framework that generates a complete 3D avatar from a single unconstrained image, using
high-quality optimized polygonal strips (polystrips or poly cards)
for real-time hair rendering. By eliminating the need of multiple
photographs and a controlled capture environment, we provide a
practical and consumer-friendly solution for digitizing ourselves
or others, such as celebrities, from any photograph. Our digitized
models are fully rigged with intuitive animation controls such as
blendshapes and joint-based skeletons, and can be readily integrated
into existing game engines.
We first address the challenge of predicting the 3D shape and
appearance of entire heads from partially visible 2D input data. We
carefully integrate multiple cutting edge techniques into a comprehensive facial digitization framework. An accurate 3D face model
is estimated using a modified dense analysis-through-synthesis
approach [Thies et al. 2016a] with visibility constraints on a presegmented input image, which is obtained from a convolutional
neural network for segmentation [Saito et al. 2016]. Subsequently,
a complete high-quality facial texture is synthesized using a deep
learning-based inference technique introduced by Saito et al. [2017].
While a straightforward incorporation of an existing single-view
hair modeling technique is possible [Chai et al. 2016; Hu et al. 2015],
we focus on a method that produces highly efficient polystrips rather
than strands. The use of polystrips is particularly suitable for realtime rendering and integration with existing game engines. For
games, hair models rarely exceed 100K triangles, especially when a
large number of characters need to be on screen at any given time.
With appropriate textures and alpha masks, this representation also
supports for a much larger variety of hairstyles than strands. Though
widely used in cutting edge games (e.g., Uncharted 4), the creation
of visually compelling hair polystrips is typically associated with a
tedious and time-consuming modeling and texture painting process
by skilled artists.
We introduce an automatic hair digitization pipeline for modeling polystrip-based hairstyles. Critical to reconstructing highquality hair meshes are convincing shapes and structures, such as
fringes, which are laid out manually by a modeler. We propose a
deep learning-based framework to first extract semantical hair attributes that characterizes the input hairstyle. A tractable subset
of candidate hairstyles with compatible traits is then selected from
a large hair model database. A closest hairstyle is then retrieved
from this hairstyle collection and refined to match the input. Our
deep neural network also identifies hair appearance attributes, that
describe the local structure and styling with the corresponding shading properties. Though a small set of local hairstyle textures can
generalize well for different hair models, the associated alpha masks
often introduce severe transparency artifacts and alter the overall
look of the hair model significantly. In production, the crafting of
hair polystrips typically involves a complex iterative design process
of mesh adjustments, UV layout, texturing, as well as polystrip duplication and perturbation. To this end, we develop a novel iterative
optimization technique for polystrip patching, placement, and shape
refinement based on a scalp visibility metric. For visually pleasing
animations, we also rig our hair model to the head skeleton using
inverse distance skinning [Jacobson et al. 2014].
We show the effectiveness of our approach on a wide range of subjects and hairstyles, and also demonstrate compelling animations of
our avatars with simulated hair dynamics. The output quality of our
framework is comparable to state-of-the-art game characters, as well
as cutting-edge avatar modeling systems that are based on multiple
input photographs [Cao et al. 2016; Ichim et al. 2015]. The proposed
pipeline also produces superior results than existing commercial
single view-based solutions such as Loom.ai and itSeez3D.
Contributions:
• We present a fully automatic framework for complete 3D
avatar modeling and rigging, from a single unconstrained
image that is suitable for real-time rendering in game and VR
environments. Our facial digitization pipeline integrates the
latest advances in facial segmentation, shape modeling, and
high-fidelity appearance inference.
• We develop a new single-view hair digitization pipeline that
produces highly efficient and versatile polystrip models. Our
system captures both hair shape and appearance properties.
• To ensure high-quality output hair meshes, we present a hair
attributes classification framework based on deep learning.
Furthermore, an iterative optimization algorithm for polystrip
patching is introduced to ensure a flawless scalp coverage
and correct hair shape likeness to the input.
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
Avatar Digitization From a Single Image For Real-Time Rendering • 195:3
2 RELATED WORK
Facial Modeling and Capture. Over the past two decades, a great
amount of research has been dedicated to the modeling and animation of digital faces. We refer to [Parke and Waters 2008] for a
comprehensive introduction and overview. Though artist-friendly
digital modeling tools have significantly evolved over the years, 3D
scanning and performance capture technologies provide an attractive way to scale content creation and improve realism through accurate measurements from the physical world. While expensive and
difficult to deploy, sophisticated 3D facial capture systems [Beeler
et al. 2010, 2011; Bradley et al. 2010; Debevec et al. 2000; Ghosh et al.
2011; Li et al. 2009; Ma et al. 2007; Weise et al. 2009] are widely
adopted in high-end production and have proven to be a critical
component for creating photoreal digital actors. Different rigging
techniques such as joint-based skeletons, blendshapes [Li et al. 2010;
von der Pahlen et al. 2014], or muscle-based systems [Sifakis et al.
2005; Terzopoulos and Waters 1990] have been introduced to ensure
intuitive control in facial animation and high-fidelity retargeting
for performance capture. Dedicated systems for capture, rigging,
and animation have also emerged for the treatment of secondary
components such as eyes [Bérard et al. 2016; Miller and Pinskiy
2009], lips [Garrido et al. 2016b], and teeth [Wu et al. 2016]. Despite
high-fidelity output, these capture and modeling systems are too
complex for mainstream adoption.
The PCA-based linear face models of [Blanz and Vetter 1999]
have laid the foundations for the modern treatment of image-based
3D face modeling, with extensions to multi-view stereo [Blake et al.
2007], large-scale internet pictures [Kemelmacher-Shlizerman 2013;
Liang et al. 2016], massive 3D scan datasets [Booth et al. 2016], and
the use of shading cues [Kemelmacher-Shlizerman and Basri 2011].
Blanz and Vetter have demonstrated in their original work that
compelling facial shapes and appearances with consistent parameterization can be extracted reliably from a single input image. Recent
progress in single-view face modeling demonstrate improved detail
reconstruction [Richardson et al. 2016], component separation [Kim
et al. 2017; Tewari et al. 2017], and manipulation capabilities [Shu
et al. 2017] using deep convolutional neural networks. To handle
facial expressions, vector spaces based on visemes and expressions
have been proposed [Blanz et al. 2003], which led to the development of PCA-based multi-linear face models [Vlasic et al. 2005] and
the popularization of FACS-based blendshapes [Cao et al. 2014b].
The low dimensionality and effectiveness in representing faces have
made linear models particularly suitable for instant 3D face modeling and robust facial performance capture in monocular settings
using depth sensors [Bouaziz et al. 2013; Hsieh et al. 2015; Li et al.
2013; Weise et al. 2011, 2009], as well as RGB video [Cao et al. 2014a;
Garrido et al. 2013, 2016a; Saito et al. 2016; Shi et al. 2014; Thies
et al. 2016a]. When modeling a 3D face automatically from an image, sparse 2D facial landmarks [Cootes et al. 2001; Cristinacce and
Cootes 2008; Saragih et al. 2011; Xiong and De la Torre 2013] are
typically used for robust initialization during fitting. State-of-the-art
landmark detection methods achieve impressive efficiency by using
explicit shape regressions [Cao et al. 2013; Kazemi and Sullivan
2014; Ren et al. 2014].
While linear models can estimate entire head models from a single
view, the resulting textures are typically crude approximations of
the subject, especially in the presence of details such as facial hair,
complex skin tones, and wrinkles. In order to ensure likeness to
the captured subject, existing 3D avatar creation systems often
avoid the use of a purely linear appearance model, but rely on
acquisitions from multiple views to build a more accurate texture
map. Ichim et al. [2015] introduced a comprehensive pipeline for
video-based avatar reconstruction in uncontrolled environments.
They first produce a dense point cloud using multi-view stereo
and then estimate a 3D face model using non-rigid registration. An
integrated albedo texture map is then extracted using a combination
of Poisson blending and light factorization via spherical harmonics.
Their method is limited to a controlled acquisition procedure based
on a semi-circular sweep of a hand-held sensor, and hair modeling
is omitted. Chai et al. [2015] presented a single-view system for
high-quality 2.5D depth map reconstruction of a both faces and hair,
using structural hair priors, silhouette, and shading cues. However,
their technique is not suitable for avatars, as a full head cannot be
produced nor animated. More recently, Cao et al. [2016] developed
an end-to-end avatar creation system that can produce compelling
face and hair models based on an image-based mesh representation.
While their system can handle very large variations of hairstyles and
also produce high-quality facial animations with fine-scale details,
they require up to 32 input images and some manual guidance for
segmentation and labeling. Instead of a controlled capture procedure
with multiple photographs, we propose a fully automatic system
that only needs a single image as input.
Notice that proprietary technologies for single-view avatar modeling have emerged recently in the commercial world, such as Pinscreen’s demonstration at SIGGRAPH Real Time Live! show [Li et al.
2017] and FaceUnity’s photo-to-avatar preview [FaceUnity 2017]. In
Section 6, we compare our proposed solution with two other recent
avatar creation solutions, Loom.ai [2017] and itSeez3D [2017].
Hair Modeling and Capture. Hair is an essential component of
life-like avatars and CG characters. In studio settings, human hair is
traditionally modeled, simulated, and rendered using sophisticated
design tools [Choe and Ko 2005; Kim and Neumann 2002; Weng et al.
2013; Yuksel et al. 2009]. We refer to the survey of Ward et al. [2006]
for an extensive overview. 3D hair capture techniques, analogous
to those used for face capture, have been introduced to digitize hair
from physical inputs. High-fidelity acquisition systems typically involve controlled recording sessions, manual assistance, and complex
hardware equipments, such as multi-view stereo rigs [Beeler et al.
2012; Echevarria et al. 2014; Jakob et al. 2009; Luo et al. 2013; Paris
et al. 2008] or even thermal imaging [Herrera et al. 2012].
Hu et al. [2014a] demonstrated a highly robust multi-view hair
modeling approach using a data-collection of pre-simulated hair
strands, which can fully eliminate the need for manual hair segmentation. Since physically simulated hair strands are used as shape
priors, their method can only handle unconstrained hairstyles. The
same authors later introduced a procedural method for hair patch
generation [Hu et al. 2014b] to handle highly convoluted hairstyles
such as braids. They also proposed a more accessible acquisition
approach based on a single RGB-D camera, that is swept around
the subject. Single-view hair digitization methods have been pioneered by Chai et al. [2013; 2012] but rely on high-resolution input
photographs and can only produce the frontal geometry of the hair.
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
195:4 • L. Hu, S. Saito, L. Wei, K. Nagano, J. Seo, J. Fursund, I. Sadeghi, C. Sun, Y. Chen and H. Li
input image face modeling face texture
reconstruction
faces and hair
segmentation
hairstyle
digitization hair appearance matching (shader, texture, alpha mask, bump map, color) real-time
3D avatar rendering
facial rigging (blendshape, joint-based, secondary components)
Fig. 2. Our single-view avatar creation framework is based on a pipeline that combines both complete face digitization and hair polystrip digitization—both
geometry and appearance are captured. Original image courtesy of Getty Images.
A database-driven approach by Hu et al. [2015] later showed that
the modeling of complete strand-level hairstyles is possible from a
single image, with the help of very few user strokes as guidance. A
similar, but fully automatic approach has been furthered by Chai
et al. [2016] using a larger database for shape retrieval and a deep
learning-technique for hair segmentation. While a wide range of
high-quality hair models can be digitized, many hairstyles with
multiple layers or stochastic structures—such as afros or messy
hair—are difficult to capture and not suitable for strand-based representations. Furthermore, strand-based hair models are still difficult
to integrate into real-time game environments, due to their complexity in real-time hair rendering and simulation. We introduce
a new hair digitization framework based on highly efficient and
flexible polystrips, which are widely adopted in modern games. Hair
polystrips are more efficient for rendering than hair strands, and
also can also achieve believable volumetric structures through textures with alpha masks and cut-off techniques as opposed to the
opaque textured mesh representation used by Cao et al. [2016].
3 AVATAR MODELING FRAMEWORK
Our end-to-end pipeline for face and hair digitization is illustrated
in Figure 2. An initial pre-processing step computes pixel-level
segmentation of the face and hair regions. We then produce a fully
rigged avatar based on textured meshes and hair polystrips from this
image. We decouple the digitization of face and hair since they span
entirely different spaces for shape, appearance, and deformation.
While the full head topology of the face is anatomically consistent
between subjects and expressions, the mesh of the hair model will
be unique for each person.
Image Pre-Processing. Segmenting the face and hair regions of an
input image improves the accuracy of the 3D model fitting process,
as only relevant pixels are used as constraints. It also provides
additional occlusion areas, that need to be completed during texture
reconstruction, especially when the face is covered by hair. For the
hair modeling step, the silhouette of the segmented hair region will
provide important matching cues.
We adopt the real-time and automatic semantic segmentation
technique of [Saito et al. 2016] which uses a two-stream deconvolution network to predict face and hair regions. This technique
produces accurate and robust pixel-level segmentations for unconstrained photographs. While the original implementation is
designed to process face regions, we repurpose the same convolutional neural network to segment hair. In contrast to the image
pre-processing step of [Cao et al. 2016], ours is fully automatic.
To train our convolutional neural network, we collected 9269
images from the public LFW face dataset [Huang et al. 2007] and
produce the corresponding binary segmentation masks for both
faces and hair via Amazon Mechanical Turk (AMT) as illustrated
in Figure 3. We detect the face in each image using the popular
Viola-Jones face detector [2001] and normalize their positions and
scales to a 128 × 128 image. To avoid overfitting, we augment the
training dataset with random Gaussian-distributed transformation
perturbations and produce 83421 images in total. The standard deviations are 10◦
for rotations, 5 pixels for translations, and 0.1 for
scale, and the means are 0, 0, and 1.0 respectively. We further use a
learning rate of 0.1, a momentum of 0.9, and weight decay of 0.0005
for the training. The optimization uses 50, 000 stochastic gradient
descent (SGD) iterations which take roughly 10 hours on a machine
with 16GB RAM and NVIDIA GTX Titan X GPU. We refer to the
work of [Saito et al. 2016] for implementation details. Once trained,
the network outputs a multi-class probability map (for face and hair)
from an arbitrary input image. A post-hoc inference algorithm based
on dense conditional random field (CRF) [Krähenbühl and Koltun
2011] is then used to extract the resulting binary mask. Successful
results and failure cases are presented in Figure 3.
Face Digitization. We first fit a PCA-based linear face model for
shape and appearance to the segmented face region. Next, a variant
of the efficient pixel-level analysis-through-synthesis optimization
method of [Thies et al. 2016a] is adopted to solve for the PCA coefficients of the 3D face model and an initial low-frequency albedo
map. We use our own artist-created head topology (front and back
head) with identity shapes transferred from [Blanz and Vetter 1999]
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
Avatar Digitization From a Single Image For Real-Time Rendering • 195:5
training data successful results failure cases
Fig. 3. Hair segmentation training data, successful results, and failure cases.
and expressions from [Cao et al. 2014b]. A visibility constraint is
incorporated into the model fitting process to improve occlusion
handling and non-visible regions. A PCA-based appearance model
is constructed for the textures of the full head, using artist-painted
skin textures in missing regions of the original data samples. We
then infer high-frequency details to the frontal face regions even
if they are not visible in the capture using a feature correlation
analysis approach based on deep neural networks [Saito et al. 2017].
Finally, we eliminate the expression coefficients of our linear face
model to neutralize the face. The resulting model is then translated
and scaled to fit the eye-balls using the average pupillary distance of
an adult human of 66 mm. We then translate and scale the teeth/gum
to fit pre-selected vertices of the mouth region. We ensure that these
secondary components do not intersect the face using a penetration
test for all the FACS expressions of our custom animation rig.
Hair Digitization. Our hair digitization pipeline produces a hair
mesh model and infers appearance properties for the hair shader.
We first use a state-of-the-art deep convolutional neural network
based on residual learning [He et al. 2016] to extract semantic hair
attributes such as hair length, level of baldness, and the existence
of hairlines and fringes. These hair attributes are compared with
a large hairstyle database containing artist created hair polystrip
models. We then form a reduced hairstyle dataset that only contains
relevant models with compatible hair attributes. We then search for
the closest hairstyle to our input image based on the silhouette of
its segmentation and the orientation field of the hair strands. As
the retrieved hairstyle may not match the input exactly, we further
perform a mesh fitting step to deform the retrieved hairstyle to the
input image using the silhouette and the input orientation field. We
incorporate collision handling between the deformed hair and the
personalized face model to avoid hair meshes intersecting the face
mesh. The classification network for hair attribute classification also
identifies hair appearance properties for proper rendering such as
hair color, texture and alpha maps, various shader parameters, etc.
Polystrip duplication is necessary, since the use of alpha masks for
the hair texture can cause a loss of scalp coverage during rendering.
Consequently, we iteratively identify the incomplete hair regions
using multi-view visibility map and patch them with interpolated
hair strips. The hair polystrips are alpha blended using an efficient
rendering algorithm based on order-independent transparency with
depth peeling [Bavoil and Myers 2008].
Rigging and Animation. Since our linear face model is expressed
by a combination of identity and expression coefficients [Saito et al.
2017], we can easily obtain the neutral pose. Using an examplebased approach, we can compute the face input’s corresponding
FACS-based expressions (including high-level controls) via transfer from a generic face model [Li et al. 2010]. Our generic face is
also equipped with skeleton joints based on linear blend skinning
(LBS) [Parke and Waters 2008]. The face and secondary components
(eyes, teeth, tongue, and gums) also possess blendshapes. Eye colors (black, brown, blue, and green) are detected using the same
deep convolutional neural network used for hair attribute classification [He et al. 2016] and the appropriate texture is used. Our
model consists of 71 blendshapes, and 16 joints in total. Our face rig
also abstracts the low-level deformation parameters with a smaller
and more intuitive set of high-level controls as well as manipulation handles. We implemented our rig in both the animation tool,
Autodesk Maya, and the real-time game engine, Unity. We can rig
our hair model directly with the skeleton joints of the head in order
to add a minimal amount of dynamics for simple head rotations.
For more complex hair dynamics, we also demonstrate a simple
real-time physical simulation of our polystrip hair representation
using mass-spring models with rigid body chains and hair-head
collisions [Selle et al. 2008].
4 FACE DIGITIZATION
We first build a fully textured head model using a multi-linear PCA
face model. Given a single unconstrained image and the corresponding segmentation mask, we compute a shape V , a low-frequency
facial albedo map I, a rigid head pose (R,t), a perspective transformation ΠP (V ) with the camera intrinsic matrix P, and illumination
L, together with high-frequency textures from the visible skin region. Since the extracted high-frequency texture is incomplete from
a single-view, we infer the complete texture map using a facial appearance inference method based on deep neural networks [Saito
et al. 2017].
input image without visibility
constraints
without visibility
constraints (uv map)
with visibility
constraints
with visibility
constraints (uv map)
Fig. 4. Our facial modeling pipeline with visibility constraints produces
plausible facial textures when there are occlusions such as hair.
3D Head Modeling. To obtain the unknown parameters χ =
{V, I, R,t, P, L}, we adopt the pipeline of [Thies et al. 2016a] which
is based on morphable face models [Blanz and Vetter 1999] extended with a PCA-based facial expression model and an efficient
optimization based on pixel color constraints. We further incorporate pixel-level visibility constraints using our segmentation mask
obtained using the method of [Saito et al. 2016].
We use a multi-linear PCA model to represent the low-frequency
facial albedo I and the facial geometry V with n = 10, 822 vertices
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
195:6 • L. Hu, S. Saito, L. Wei, K. Nagano, J. Seo, J. Fursund, I. Sadeghi, C. Sun, Y. Chen and H. Li
and 21, 510 faces:
V (αid , αexp ) = V¯ + Aid αid + Aexpαexp,
I(αal) = ¯I + Aalαal .
Here Aid ∈ R
3n×40
, Aexp ∈ R
3n×40, and Aal ∈ R
3n×40 are the
basis of a multivariate normal distribution for identity, expression,
and albedo with the corresponding mean: V¯ = V¯
id + V¯
exp ∈ R
3n
,
and ¯I ∈ R
3n
, and the corresponding standard deviation: σid ∈ R
40
,
σexp ∈ R
40, and σal ∈ R
40
. Aid , Aal , V¯, and ¯I are based on the Basel
Face Model database [Paysan et al. 2009] and Aexp is obtained from
FaceWarehouse [Cao et al. 2014b]. We assume Lambertian surface
reflectance and approximate the illumination using second order
Spherical Harmonics (SH).
First, we detect 2D facial landmarks fi ∈ F using the method of
Kazemi et al. [Kazemi and Sullivan 2014] in order to initialize the
face fitting by minimizing the following energy:
El an(χ) =
1
|F |
Õ
fi ∈F
∥ fi − ΠP (RVi + t)∥2
2
.
We further refine the shape and optimize the low-frequency albedo,
as well as the illumination, by minimizing the photometric difference between the input image and a synthetic face rendering. The
objective function is defined as:
E(χ) = wcEc (χ) + wl anEl an(χ) + wr eдEr eд(χ), (1)
with energy term weightswc = 1,wl an = 10, andwr eд = 2.5×10−5
for the photo-consistency term Ec , the landmark term El an, and
the regularization term Er eд. Following [Saito et al. 2017], we also
ensure that the photo-consistency term Ec is only evaluated for
visible face regions:
Ec (χ) =
1
|M |
Õ
p ∈M
∥Cinput (p) − Csynth(p)∥2,
where Cinput is the input image, Csynth the rendered image, and
p ∈ M a visibility pixel given by the facial segmentation mask. The
regularization term Er eд is defined as:
Er eд(χ) =
Õ
40
i=1

(
αid,i
σid,i
)
2 + (
αal,i
σal,i
)
2

+
Õ
40
i=1
(
αexp,i
σexp,i
)
2
.
This term encourages the coefficients of the multi-linear model to
conform a normal distribution and reduces the chance to converge
into a local minimum. We use an iteratively reweighted GaussNewton method to minimize the objective function (1) using three
levels of image pyramids. In our experiments, 30, 10, and 3 GaussNewton steps were sufficient for convergence from the coarsest
level to the finest one. After this optimization, a high-frequency
albedo texture is obtained by factoring out the shading component
consisting of the illumination L and the surface normal from the
input image. The resulting texture map is stored in the uv texture
map and used for the high-fidelity texture inference.
Face Texture Reconstruction. After obtaining the low-frequency
albedo map and a partially visible fine-scale texture, we can infer a
complete high-frequency texture map, as shown in Figure 5, using
a deep learning-based transfer technique and a high-resolution
face database [Ma et al. 2015]. The technique has been recently
introduced in [Saito et al. 2017] and is based on the concept of feature
correlation analysis using convolutional neural networks [Gatys
et al. 2016]. Given an input image I and a filter response F
l
(I) on the
layer l of a convolutional neural network, the feature correlation
can be represented by a normalized Gramian matrix G
l
(I):
G
l
(I) =
1
Ml
F
l
(I)

F
l
(I)
T
Saito et al. [2017] have found that high-quality facial details (e.g.,
pores, moles, etc.) can be captured and synthesized effectively using
Gramian matrices. Let I0 be the low-frequency texture map and Ih
be the high-frequency albedo map with the corresponding visibility
mask Mh. We aim to represent the desired feature correlationGh as a
convex combination ofG(Ii), where I1, ..., Ik are the high-resolution
images in the texture database:
G
l
h
=
Õ
k
wkG
l
(Ik
), ∀l s.t.
Õ
K
k=1
wk = 1.
We compute an optimal blending weight {wk
} by minimizing the difference between the feature correlation of the partial high-frequency
texture Ih and the convex combination of the feature correlations
in the database under the same visibility. This is formulated as the
following problem:
min
w
Í
l






Í
k wkG
l
M
(Ik
, Mh) − G
l
M
(Ih, Mh)






F
s.t. ÍK
k=1
wk = 1
wk ≥ 0 ∀k ∈ {1, . . . ,K}
,
(2)
where GM(I, M) is the Gramian Matrix computed from only the
masked region M. This allows us to transfer multi-scale features
of partially visible skin details to the complete texture. We refer
to [Saito et al. 2017] for more detail.
Once the desired Gh is computed, we update the albedo map I so
that the resulting correlation G(I) is similar to Gh, while preserving
the low frequency spatial information F
l
(I0) (i.e., position of eye
brows, mouth, nose, and eyes):
min
I
Õ
l ∈LF






F
l
(I) − F
l
(I0)






2
F
+ α
Õ
l ∈LG






G
l
(I) − Gh






2
F
, (3)
where LG is a set of high-frequency preserving layers and LF a set
of low-frequency preserving layers in VGG-19 [Simonyan and Zisserman 2014]. A weight α balances the influence of high frequency
and low frequency and α = 2000 is used for all our experiments.
Following Gatys et al. [2016], we solve Equation 3 using an L-BFGS
solver. Since only frontal faces are available in the database, we can
only enhance frontal face regions. To obtain a complete texture, we
combine the results with the PCA-based low-frequency textures of
the back of the head using Poisson blending [Pérez et al. 2003].
Secondary Components. To enhance the realism of the reconstructed avatar, we insert template models for eyes, teeth, gums, and
tongue into the reconstructed head model. The reconstructed face
model is rescaled and translated to fit a standardized pair of eye balls
so that each avatar is aligned as to avoid scale ambiguity during the
single-view reconstruction. The mouth-related template models are
aligned based on pre-selected vertices on the facial template model.
After the initial alignment, we test for intersections between the
face and the secondary components for each activated blendshape
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
Avatar Digitization From a Single Image For Real-Time Rendering • 195:7
input image input uv map inferred uv map inferred textured
face model
Fig. 5. We produce a complete and high-fidelity texture map from a partially
visible and low resolution subject using a deep learning-based inference
technique. Original image courtesy of Getty Images.
expression. The secondary models for the mouth region are then
translated by the minimal offset where no intersection is present.
The eye color texture (black, brown, green, blue) is computed using a similar convolutional neural network for semantic attribute
inference as the one used for hair color classification. The input
to this network is a cropped image of the face region based on the
bounding box around the 2D landmarks from [Kazemi and Sullivan 2014], where non-face regions are set to black and the image
centered between the two eyes.
5 HAIR DIGITIZATION
Hairstyle Database. Starting from the USC-HairSalon database
for 3D hairstyles, introduced in [Hu et al. 2015], and 89 additional
artist created models, we align all the hairstyle samples to the PCA
mean head model V¯ used in Section 4. Inspired by [Chai et al. 2015],
we also increase the number of samples in our database using a
combinatorial process, which is necessary to span a sufficiently
large variation of hairstyles. While the online model generation
approach of [Hu et al. 2015] is less memory consuming, it requires
some level of user interaction.
To extend the number of models, we first group each sample of
the USC-HairSalon database into 5 clusters via k-means clustering
using the root positions and the strand shapes as in [Wang et al.
2009]. Next, for every pair of hairstyles, we randomly pick a pair of
strands among the cluster centroids and construct a new hairstyle
using these two sampled strands as a guide using the volumetric
combination method introduced in [Hu et al. 2015]. We further
augment our database by flipping each hairstyle w.r.t. the x-axis
plane, forming a total of 100,000 hairstyles.
For each hair model, the set of all particles forms the outer surface
of the entire hair by considering each hair strand as a chain of particles. This surface can be constructed using a signed distance field
obtained by volumetric points samples [Zhu and Bridson 2005]. By
using the surface normal of this mesh, we compose close and nearly
parallel hair strands into a hair polystrip, which is a parametric
piece-wise linear patch. This thin surface structure can carry realistic looking textures that provide additional variations of hair, such as
curls, crossings, or thinner tips. Additionally, the transparency of the
texture allows us to see through the overlay of different polystrips
and provide an efficient way to achieve volumetric hair renderings.
Luo et al. [2013] proposed a method to group short hair segments
into a ribbon structure. Adopting a similar approach, we start from
the longest hair strand in the hairstyle as the center strand of the
input image
hair attribute
extraction
hair attributes
hair category
matching
hairstyle
retrieval
hair mesh
fitting
polystrip
patching
optimization
hairstyle database reduced dataset
closest hairstyle
fitted hairstyle
alpha mask reconstructed hair
segmentation
and orientation
Short
curly not spiky
...
... ...
Fig. 6. Our hair mesh digitization pipeline. Original image courtesy of Getty
Images.
polystrip. By associating the normal of each vertex on the strand to
the closest point on the hair surface, we can expand the center strand
on both sides of the binormal as well as its opposite direction. We
compute the coverage of all hair strands by the current polystrip, and
continue to expand the polystrip until no more strands are covered.
Once a polystrip is generated, we remove all the covered strands
in the hairstyle, and reinitiate process from the longest strand in
the remaining hair strand subset. Finally, we obtain a complete hair
polystrip model, once all the hair strands are removed from the
hairstyle. We refer to [Luo et al. 2013] for more details.
Hair Attribute Classification. We use 40K images from the CelebA
dataset [Liu et al. 2015] with various hairstyles and collect their
hair attributes using AMT (see Table 1 for the list of hair attributes).
Similarly, we manually label all the hair models in our database
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
195:8 • L. Hu, S. Saito, L. Wei, K. Nagano, J. Seo, J. Fursund, I. Sadeghi, C. Sun, Y. Chen and H. Li
using high level semantic attributes. We also actively ensure that
we have roughly the same quantity of images for each attribute by
resampling the training data.
These annotations are then fed into a state-of-the-art classification network, ResNet [He et al. 2016], to train multiple classifiers
predicting each hair attribute given an input image. We use the
50-layer ResNet pre-trained with ImageNet [Deng et al. 2009], and
fine-tune it using our training data under learning rate 10−4
, weight
decay 10−4
, momentum 0.9, batch size 32, and 90 epochs using the
stochastic gradient descent method. The images are augmented for
the training based on perturbations suggested by He et al. [2016]
(variations in cropping, brightness, contrast, and saturation).
During test time, input images are resized so that the maximum width or height is 256, center-cropped to 224 × 224, and fed
into the trained classifiers. Each classifier returns a normalized ndimensional vector, where n = 2 for binary attributes and n = m for
m-class attributes. The predictions of all classifiers are then concatenated into a multi-dimensional descriptor. Nearest neighbor search
is then performed to find the k-closest matching hair with smallest
Euclidean distance in the descriptor space. If the classifier detects a
bald head, the following hairstyle matching process is skipped.
Hairstyle Matching. After obtaining a reduced hair model subset
based on the semantic attributes, we compare the segmentation
mask and hair orientations at the pixel level using pre-rendered
thumbnails to retrieve the most similar hairstyle [Chai et al. 2016].
Following Chai et al. [2016], we organize our database as thumbnails
and adopt the binary edge-based descriptor from [Zitnick 2010] to
increase matching efficiency. For each hairstyle in the database,
we pre-render the mask and the orientation map as thumbnails
from 35 different views, where 7 angles are uniformly sampled
in [−π/4, π/4] as yaw and 5 angles in [−π/4, π/4] as pitch. If the
hair segmentation mask has multiple connected components due to
occlusion or if the hair is partially cropped, then the segmentation
descriptor may not be reliable; in this case, we find the most similar
hairstyle using the classifiers.
retrieved
hairstyle
deformed
hairstyle
collision
handling
input
image
Fig. 7. Our hair mesh fitting pipeline.
Hair Mesh Fitting. In order to match the retrieved model with the
silhouette and orientation of the input, we extend the hair fitting
algorithm for strands [Chai et al. 2016; Hu et al. 2015] to the polystrip
meshes. First, we perform spatial deformation in order to fit the hair
model to the personalized head model, using an as-rigid-as-possible
graph-based deformation model [Li et al. 2009]. We represent the
displacement of each vertex on the hair mesh as a linear combination
of the displacements of k-nearest vertices on the head mesh using
the following inversely weighted Gaussian approximation:
dpi =
Õ
j ∈Ni
(1 + ∥pi − qj ∥2 + ∥pi − qj ∥
2
2
)
−1
dqj
,
where p and q are vertices on the hair and mean head mesh respectively. This allows the hair model to follow the head deformation
without causing intersection. Once the scalp and the hair mesh is
aligned, we compute a smooth warping function W(·) mapping vertices on the 3D model’s silhouette to the closest points on the input’s
2D silhouette from the camera angle, and deform each polystrip
according to the as-rigid-as-possible warping function presented
in [Li et al. 2009]. Then, we deform each polystrip to follow the
input 2D orientation map as described in [Chai et al. 2016; Hu et al.
2015]. Possible intersections between the head and the hair model
due to this deformation are resolved using simple collision handling
via force repulsion [Luo et al. 2013].
input hair model multi-view scalp
visibility map iteration 1 final result
Fig. 8. Our iterative optimization algorithm for polystrip patching.
Polystrip Patching Optimization. With the benefit of having a low
computational overhead, a polystrip-based rendering with a bump
map and an alpha mask produces locally plausible hair appearance
for a wide range of hairstyles. However, such rendering is prone to a
lack of scalp coverage, especially for short hairstyles. We propose an
iterative optimization method to ensure scalp coverage via patching
with minimum increase in the number of triangles.
We measure the coverage by computing the absolute difference
between the alpha map in a model view space with and without
hair transparency from multiple view points (see Figure 8). Regions
with high error expose the scalp surface and need to be covered
by additional hair meshes. Without transparency, all polystrips are
rendered with alpha value 1.0. When a hair alpha mask is assigned
by the hair style classification, the polystrips are rendered via orderindependent transparency (OIT), resulting in alpha values of range
[0, 1]. First, we convert the error map into a binary map by thresholding if the error exceeds 0.5, and apply blob detection on the
binary map. Given the blob with highest error, a new polystrip is
then placed to cover the area.
We find the k-closest polystrips to the region with the highest
error and resample two polystrips within this set so that their average produces a new one that covers this region. We use k = 6 for all
our examples. The two polystrips are re-sampled so that they have
consistent vertex numbers for linear blending. By averaging the
polystrips, we can guarantee that the resulting strips are inside the
convex hull of the hair region. Thus, our method does not violate
the overall hair silhouette after new strips are added. We iterate this
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
Avatar Digitization From a Single Image For Real-Time Rendering • 195:9
process until the highest error has reached a certain threshold or
when no more scalp region is visible.
straight dreadlock wavy
Fig. 9. Example polystrip textures for characterizing high-frequency structures of different hair types. Each texture atlas contains a 9-uv map for
polystrips of different sizes.
Hair Rendering and Texturing. We render the resulting polystrips
using a variant of [Sadeghi et al. 2010]. The hair tangents are directly
obtained from the directions of the mesh’s UV parameterization.
We use our classification network to determine the semantic shader
parameters, such as the width and the intensity of the primary
and secondary highlights. To approximate the multiple scattering
components, we add the diffuse term from Kajiya and Kay [1989].
We perform alpha blending between the polystrips using an orderindependent transparency (OIT) algorithm based on depth peeling.
Our classification network also specifies for each input image
the most similar local hairstyle texture. As illustrated in Figure 9,
we characterize a hairstyle’s local high-frequency structure into
different categories. These textures are manually designed by an
artist based on pre-categorized images that are also used for training.
As demonstrated in many games, these type of hair textures can
represent a wide range of hair appearances. As different hair types
are associated with custom shaders, some styles may be associated
with a bump map, which is also prepared by the artist.
For the texture lookups, we use a hierarchical UV atlas which
depends on the world dimensions of individual polystrips after
the deformation step. The polystrip textures are grouped into nine
categories of sizes in a single map. Using multiple texture sizes for
each hair patch reduces stretching and compression artifacts in both
U and V directions, and also increases texture variations.
6 RESULTS
We created fully-rigged 3D avatars with challenging hairstyles and
secondary components for a diverse set of inputs from a wide range
of image sets. Even though the input resolutions are inconsistent,
there is no a-priori knowledge about the scene illumination or intrinsic camera parameters, and the subjects within the inputs may
have tilted or partially covered heads with different expressions,
we were still able to produce automatically digitized outputs. We
also processed short and long hairstyles of different local structures
including straight, wavy, and dreadlock styles. As illustrated in Figure 10, our proposed framework successfully digitizes textured face
models and reproduces the volumetric appearance of hair, which
is shown from the front and the back. Facial details are faithfully
digitized in unseen regions and fully covered hair polystrips can be
reconstructed using our iterative patching optimization algorithm.
Our accompanying video shows several animations produced by a
professional animator using the provided controls of our avatar. We
also demonstrate an avatar animation applications using a real-time
facial performance capture system, as well as the simulated hair
motions of our hair polystrip models using a mass-spring system
based on rigid body chains and hair-head collision (see Figure 13).
Evaluation. We evaluate the robustness of our system and consistency of the reconstruction using a variety of input examples
of the same subject as shown in Figure 11. Our combined facial
segmentation [Saito et al. 2016], texture inference [Saito et al. 2017]
and PCA-based shape, appearance, and lighting estimation [Thies
et al. 2016a] framework is robust to severe lighting conditions. We
can observe that the visual difference between the reconstructed
albedo map of a same person, captured under contrasting illuminations, is minimal. We also demonstrate how our linear face model
can discern between a person’s identity and its expression up to
some degree. Our visualization shows the resulting avatar in the
neutral pose. While some slightly noticeable dissimilarity in the
face and hair digitization remains, both outputs are plausible. For
large smiles in the input image, the optimized neutral pose can still
contain an amused expression.
While traditional hair database retrieval techniques [Chai et al.
2016; Hu et al. 2015] are effective for strand-based output, our hair
polystrip modeling approach relies on clean mesh structures and
topologies as they are mostly preserved until the end of the pipeline.
As shown in Figure 12, a deep learning-based hair attribute classification step is critical in avoiding wrong hair types being used
during retrieval. Table 1 lists a few annotated hair attributes, as well
as their prediction accuracies from the trained network. Although
the predictions are sometimes not accurate due to the lack of training data, we can still retrieve similar hairstyles which are further
optimized by subsequent steps in the pipeline.
attribute possible values accuracy (%)
hair_length long/short/bald 72.5
hair_curve straight/wavy/curly/kinky 76.5
hairline left/right/middle 87.8
fringe full/left/right 91.8
hair_bun 1 bun/2 buns/... 91.4
ponytail 1 tail/2 tails/... 79.2
spiky_hair spiky/not spiky 91.2
shaved_hair fully/partially shaved 81.4
baldness fully bald/receded hair 79.6
Table 1. We train a network to classify the above attributes of hairstyles,
achieving accuracies around 70-90%.
Comparison. We compare our method against several state-ofthe-art facial modeling techniques and avatar creation systems in
Figure 14. Our deep learning-based framework [Saito et al. 2017]
can infer facial textures with more details comparing to linear morphable face models [Blanz and Vetter 1999; Thies et al. 2016a], In
addition to producing high-quality hair models, our generated face
meshes and textures are visually comparable to the video-based
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
195:10 • L. Hu, S. Saito, L. Wei, K. Nagano, J. Seo, J. Fursund, I. Sadeghi, C. Sun, Y. Chen and H. Li
input image face and hair mesh face and hair mesh (side) 3D avatar 3D avatar (side) 3D avatar (animated)
Fig. 10. Our proposed framework successfully generates high-quality and fully rigged avatars from a single input image in the wild. We demonstrate the
effectiveness on a wide range of subjects with different hairstyles. We visualize the face meshes and hair polystrips, as well as their textured renderings.
Original images courtesy of Getty Images.
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
Avatar Digitization From a Single Image For Real-Time Rendering • 195:11
input output different lighting output
different expression output different pose output
Fig. 11. We evaluate the robustness of our framework by validating the
consistency of the output under different capture conditions.
input image with hair
classification
(5 attributes)
without hair
classification
(0 attributes)
with hair
classification
(10 attributes)
Fig. 12. We assess the importance of our deep learning-based hair attribute
classification. Original image courtesy of Getty Images.
Fig. 13. Real-time hair simulation using a mass-spring system.
reconstruction system of Ichim et al. [2015]. We can also reproduce
similarly compelling avatars as in [Cao et al. 2016], but using only
one out of many of their input images. While their approach is
still associated with some manual labor, our system is fully automatic. Additionally, we provide two comparisons with two existing
commercial solutions. In particular, we notice that the system of
Loom.ai [2017] fails to retrieve the correct hairstyle, while itSeez3D’s
Avatar SDK [2017] does not automatically produce hair models, nor
allows the avatar to be animated.
We further compare our polystrip-based results with the statethe-art single-view hair modeling technique from Chai et al. [2016]
as shown in Figure 15. Their methods are constrained to strandbased hairstyles and lose effectiveness on local features compared to
our polystrips method. While strand-based renderings are typically
more realistic, we argue that our representation is more versatile
(especially for very short hair) and suitable for efficient character
rendering in highly complex virtual scenes. In particular, a single
polystrip patch can approximate a large number of strands using a
input image [Thies et al. 2016] our method our method (side)
input image [Ichim et al. 2015] our method our method (side)
input image [Cao et al. 2016] our method our method (side)
input image Loom.ai our method our method (side)
input image itSeez3D our method our method (side)
Fig. 14. We compare our method with several state-of-the-art avatar creation systems. Original image (row 4) courtesy of Getty Images.
single texture with an alpha mask, which can significantly increase
rendering performance.
Performance. All our experiments are performed using an Intel
Core i7-5930K CPU with 3.5 GHz equipped with a GeForce GTX
Titan X with 12 GB memory. 3D head model reconstruction takes
5 minutes in total, consisting of 0.5 second of face model fitting,
75 s of feature correlation extraction, 14 s of computing the convex blending weight, 172 s of the final synthesis optimization. The
secondary component fitting and facial rigging are done within 1
second. Hair polystrip reconstruction takes less than 1 s to classify
the hair attributes from the input image, less than 1 s to retrieve the
closest exemplar, and 10 s to deform a hairstyle. 5 s are needed to
handle collision. Polystrip patching optimization is done within 1
minute for 2 iterations.
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
195:12 • L. Hu, S. Saito, L. Wei, K. Nagano, J. Seo, J. Fursund, I. Sadeghi, C. Sun, Y. Chen and H. Li
input image [Chai et al. 2016] our method our method
(textured)
Fig. 15. We compare our method with the latest single-view hair modeling
technique, AutoHair [Chai et al. 2016]. Original images (row 1, 2) courtesy
of Getty Images.
7 DISCUSSION
The concept of single-view modeling of avatars with hair has been
first demonstrated in [Cao et al. 2014b] as part of their “real-time
performance-based facial image animation” application. The system
is based on a hair reconstruction pipeline for portrait manipulation [Chai et al. 2012]. However, the technique is not fully automatic
and requires manual key point corrections and hair strokes.
While the automatic digitization of faces [Blanz and Vetter 1999;
Saito et al. 2017; Thies et al. 2016a] and hair [Chai et al. 2016]
from single views have been introduced separately, we demonstrate
an end-to-end framework that integrates the computation of both
components. The ability to create complete models from a single unconstrained image is particularly suitable for consumer use, as well
as for scalable content creation in virtual production. We can now
easily produce animator-friendly models of a person with intuitive
controls, as illustrated in our examples.
Previous single-view hair reconstruction techniques mostly focus
on the digitization of strand geometry; however, we also infer hair
appearance, taking into account the custom shading properties for
the rendering engine. Even though the digitization of high-quality
strands is possible, the rendering costs involved are significant for
complex multi-character virtual environments. Our focus is to provide a unified solution for capturing a wide range of hairstyles and
the ability to integrate them into existing real-time game engines
such as Unity. We have shown that polystrips are versatile hair representations and suitable for the efficient rendering and animation
of compelling avatars. We also note the importance of rendering
capabilities such as order-independent transparency for producing
convincing looking volumetric hair.
The effectiveness of our methodology is grounded on a careful
integration of state-of-the-art modeling and synthesis techniques
for faces and hair. Several key components, such as segmentation,
semantic hair attributes extraction, and eye color recognition, are
only possible due to recent advances in deep learning. Our experiments also indicate the robustness of our system, where consistent
results of the same subject can be obtained when captured from
different angles, under contrasting lighting conditions, and with
different input expressions.
Even in cases where the subject is only partially visible, the image
is of low resolution, and the illumination conditions unknown, we
can obtain high-quality textured meshes of the face and compelling
hair renderings similar to those of characters in recent games. Our
approach is qualitatively comparable to existing avatar creation
systems, which require multiple photographs and manual input [Cao
et al. 2016; Ichim et al. 2015].
While our proposed polystrip optimization algorithm is a critical
component for our automatic avatar digitization framework, we
believe that it can also be a useful tool during the design process
of polystrip-based hair models in general. Once a rough hair mesh
is created, an artist could use this patching optimization instead of
manually duplicating and perturbing with additional polystrips.
Limitations. Due to the ill-posed problem of highly incomplete
input and the low-dimensionality of our linear face models, our
shape models may not be fully accurate and our facial texture inference technique may add details in wrong places. With the dramatic
progress in deep learning research, we believe that a massive collection of high-resolution 3D faces in controlled capture settings could
be used to improve the fidelity of our face models, as well as the
performance of shape inference algorithms.
Since only a single input image is used, our face modeling pipeline
transfers a generic FACS-based linear blendshape model to every
subject. In reality these blendshapes would need to be individualized
for specific subjects. While it is possible that certain expressions
would correlate with the shape of the face, it is most likely that
multiple input images would be necessary to form accurate facial
expression models using optimization techniques as introduced by
Li et al. [2010]. In addition, the accuracy of our hair classification
network is not 100%; for example, ponytails can be ambiguous.
Similar to previous papers, our method would fail to retrieve the
correct hair model when the input hairstyle differs greatly from
those in the database (Figure 16).
We use a simple mass-spring system technique to produce motion
simulation. While the use of hair polystrips is highly efficient and a
reasonable approximation of strand-based models [Chai et al. 2016;
Hu et al. 2015], convincing strand-level simulations [Chai et al. 2014]
are not yet possible with our representation.
Though the use of polystrips and textures with alpha masks can
capture the volumetric look of hair as opposed to image-based alternatives [Cao et al. 2016], we cannot digitize props such as headwear
or glasses. Our method would also fail for longer facial hair such as
beards, since our database does not contain these objects. We believe that adding more object types as samples in our database could
make such inference possible. In addition, our system currently only
ACM Transactions on Graphics, Vol. 36, No. 6, Article 195. Publication date: November 2017.
Avatar Digitization From a Single Image For Real-Time Rendering • 195:13
input image our method our method
(textured) closest hairstyle
Fig. 16. Limitations. Wrong hairstyles can be retrieved due to incomplete visibility or insufficient hair samples in the database. Original images courtesy
of Getty Images (row 1) and Alexandra Spence (row 2).
captures a single hair color for each subject. More powerful texture analysis and synthesis techniques would be needed to generate
plausible multi-color hairstyles.
Future Work. Since our framework is designed around today’s
real-time rendering environments and facial animation systems,
we are still using commonly used parametric models for faces and
hair, and the results may still look uncanny. In the future, we plan
to explore end-to-end deep learning-based inference methods to
generate more realistic avatars with dynamic textures and more
compelling hair rendering techniques. Research in generative adversarial networks are promising directions.
