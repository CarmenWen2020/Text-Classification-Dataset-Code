This article proposes a real-time method that uses a single-view RGB-D
input (a depth sensor integrated with a color camera) to simultaneously reconstruct a casual scene with a detailed geometry model, surface albedo,
per-frame non-rigid motion, and per-frame low-frequency lighting, without
requiring any template or motion priors. The key observation is that accurate
scene motion can be used to integrate temporal information to recover the
precise appearance, whereas the intrinsic appearance can help to establish
true correspondence in the temporal domain to recover motion. Based on
this observation, we first propose a shading-based scheme to leverage appearance information for motion estimation. Then, using the reconstructed
motion, a volumetric albedo fusing scheme is proposed to complete and
refine the intrinsic appearance of the scene by incorporating information
from multiple frames. Since the two schemes are iteratively applied during
recording, the reconstructed appearance and motion become increasingly
more accurate. In addition to the reconstruction results, our experiments
also show that additional applications can be achieved, such as relighting,
albedo editing, and free-viewpoint rendering of a dynamic scene, since geometry, appearance, and motion are all reconstructed by our technique.
CCS Concepts:  Computing methodologies→Reconstruction; Motion
capture;
1. INTRODUCTION
Dynamic scene reconstruction involves capturing and reproducing various aspects of the real visual world, including static geometry, detailed motion, and intrinsic or observed appearance.
Simultaneously reconstructing all of these aspects or even part
of them enables important applications in computer vision and
graphics. For example, reconstructed geometry and surface motion as well as observed appearance can be used for free-viewpoint
video. Reconstructed kinematic motion can be transferred to new
objects or used to generate new photo-realistic animations. The
intrinsic appearance of a dynamic scene/object can be used in
applications such as appearance editing and relighting. The realtime reconstruction of geometry, motion, and appearance enables
more realistic rendering for virtual reality scenarios, for example,
Holoportation [7].
Although considerable effort has been devoted to dynamic
scene reconstruction, the problem remains challenging because of
ACM Transactions on Graphics, Vol. 36, No. 3, Article 32, Publication date: June 2017.
32:2 • K. Guo et al.
the extraordinarily large solution space, necessitating a carefully
designed capture environment [5, 34], high-quality lighting equipment [3, 9], and many video cameras [15]. Several recent works
have successfully eliminated various constraints on acquisition by
using convenient capture equipment, such as a single Kinect [32] or
binocular camera [37]. However, they require many scene priors to
constrain the problem space, such as pre-scanned model templates
[32, 37], finely embedded skeletons [37], and pre-captured lighting
environments [37].
The work on DynamicFusion [23] represents a step forward in
dynamic scene reconstruction. This work uses a single depth sensor
to fuse scene geometry from multiple frames during non-rigid registration processing, thereby gradually obtaining a static model with
fine geometry details and a motion sequence that matches the nonrigid motion in the scene. However, it has two major drawbacks.
First, it does not recover the appearance and lighting information
of a scene; thus, it does not yield a complete reconstruction of the
visual information of the scene. Second, it uses only geometric
information to estimate inter-frame motions based on the slow motion assumption, which restricts its application to only slow and
controlled motions in a scene. Recently, a concurrent work, Volume
Deform [14], has been proposed that uses image features to improve
motion estimation. However, even with sparse features, the applicability of this technique is still restricted to slow and controlled
motions, and it also does not reconstruct appearance or lighting
information.
In this article, we propose a novel method in which the surface
geometry, motion, and intrinsic appearance (albedo) of a dynamic
scene are jointly estimated. Our system utilizes both geometry and
appearance information to estimate inter-frame motions; thus, it is
able to handle casual motions. Our system requires only a single
RGB-D sensor (a depth sensor integrated with a color camera);
hence, it is very convenient to set up. Moreover, our technique requires no prior knowledge of the scene; consequently, it has no
pre-computation step and is fully automatic. Finally, our system is
performed in real time, and thus, it is applicable for online applications (shown in Figure 1).
Similarly to Reference [41] in handling static scene reconstruction, our key observation is that, on the one hand, to achieve appearance reconstruction for a dynamic scene from a single-view
input, information from multiple frames of a sequence needs to be
integrated to achieve a complete reconstruction that is robust to
noise. However, accurate integration strongly depends on accurate
temporal correspondence among frames, and the geometry-based
Iterative Closest Point (ICP) method is far from achieving accurate correspondence. On the other hand, intrinsic appearance is an
important cue for establishing correspondence, because it does not
change over time. Essentially, intrinsic appearance recovery and
correspondence estimation are highly correlated in the context of
dynamic scene reconstruction, and our key concept is to leverage
one to jointly achieve the other in the tracking procedure.
We thus propose a unified optimization procedure to introduce albedo information (intrinsic appearance) into motion reconstruction, thereby enabling more accurate estimation of the correspondence between the reconstructed geometry and the input
depth/color. Given the reconstructed correspondence, a gradually
refined surface albedo is obtained by integrating information from
multiple frames via the proposed volumetric albedo optimization
procedure. With the iteratively optimized motion and appearance,
the system achieves full visual reconstruction of a dynamic scene
and has the ability to handle challenging motions, such as the
stretching of cloth, object interactions, fast boxing, and other casual
motions.
In this article, we present a novel method that can simultaneously
fuse object geometry and surface albedo for a non-rigid scene in
real time. The specific contributions of our technique include the
following:
• Accurate and robust surface registration for non-rigid surface reconstruction in real time. By exploiting dense shading information, our system establishes temporally consistent geometric and
photometric correspondences for casual motion reconstruction
and consequently outperforms the previous methods proposed in
References [14, 23].
• Temporally coherent albedo estimation and fusion in real time.
Our system decomposes the photometric information for each
frame into albedo and low-frequency environmental lighting and
then fuses multiple frames to refine the surface albedo based on
temporally coherent correspondences, thereby enabling applications such as relighting and appearance editing.
2. RELATED WORK
The reconstruction of dynamic 3D scenes in the real world is
a widely investigated topic in computer vision and graphics.
With multi-view inputs and controlled environmental lighting,
the dynamic geometry of a scene can be reconstructed via the
shape-from-silhouette, multi-view stereo, or photometric stereo
techniques [3, 20, 29, 34]. To better constrain the solution space,
some methods utilize shape templates for captured objects. Thus,
dynamic scene reconstruction becomes a shape-tracking problem [1, 2, 5]. Furthermore, because the motion of a human character
always follows a skeleton structure, a pre-defined skeleton can
be embedded into a surface template to realize character motion
driven by joint rotations [21, 33, 44]. Note, however, that all
these techniques require either a controlled environment or careful
initialization, which significantly limits their applications.
In addition to accurate depth acquisition [25, 40], performance
capture can also be achieved using consumer depth sensors. Ye
and Yang [45] used a Gaussian mixture model in which an articulated deformation model was embedded. Wu et al. [37] further
utilized pre-captured surface reflectance and environmental lighting to achieve detailed surface reconstruction. However, the purpose
of these techniques is to reconstruct characters with skeleton structures, such as humans. It is difficult to extend these techniques to
general objects.
In the literature, surface deformation techniques have been proposed in which the skeleton restriction is removed to achieve the reconstruction of general objects. Li et al. [17] and Zollhofer et al. [48] ¨
reconstructed non-rigid motions using ICP-defined correspondence
and local rigid motion priors. Guo et al. [10, 11] further used an
L0-based motion prior to obtain improved results for articulated objects. Zhang et al. [46] trained a parametric pose model and used it
in shape estimation for humans. Although these techniques achieve
accurate dynamic reconstruction for a large variety of motions, they
still require an initial geometry prior.
To further eliminate the dependency on geometry priors, some
techniques attempt to directly reconstruct water-tight surfaces from
a dynamic scene. Liao et al. [19] focused on handling continuous and predictable motions by stitching together partial surfaces
obtained from multiple frames. Li et al. [18] achieved the accurate reconstructions of loose cloths but permitted only smooth pose
changes among multi-view scans. However, non-rigid registration
techniques, which generally assume multi-view scans [28] or involve additional motion priors [35], are also applicable to this task.
Recently, based on single-view depth inputs, fine three-dimensional
ACM Transactions on Graphics, Vol. 36, No. 3, Article 32, Publication date: June 2017.
Real-Time Geometry, Albedo, and Motion Reconstruction Using a Single RGB-D Camera • 32:3
(3D) models have been reconstructed without any motion priors by
gradually fusing multi-frame geometries whose relative motions
have been effectively removed [8, 23]. Using these techniques, it
is possible to eliminate the static modeling step prior to non-rigid
motion reconstruction. The concurrent work in Reference [14] has
added Scale-Invariant Feature Transform (SIFT) features to the ICP
registration framework, thereby improving the accuracy of motion
reconstruction. Note that neither of these recent techniques [14, 23]
using a single depth sensor seeks correct correspondence to ensure
high-quality appearance reconstruction. By contrast, in this article,
we demonstrate a technique for more accurate non-rigid motion
reconstruction in real time based on jointly solving for and leveraging the surface albedo, and we show that this accurate motion
reconstruction technique enables competent texturing and relighting applications.
In addition to surface geometry and motion reconstruction, a
rich body of work on appearance reconstruction also exists. The
majority of these works focus on static objects. A comprehensive
survey can be found in Reference [6]. Moreover, some recent works
have achieved accurate reconstruction with simple setups [41–43].
For dynamic scenes, Theobalt et al. [31] estimated a parametric
BRDF model for a dynamic human body under calibrated lights. Li
et al. [16] reconstructed motion, appearance, and lighting using a
multi-view camera array and an initial shape template. Imber et al.
[13] also utilized multi-view input to reconstruct intrinsic textures
without a smooth prior in the temporal domain. Taheri et al. [30]
jointly optimized albedo and face pose by utilizing information from
multiple video frames. However, all of these appearance-related
reconstruction techniques critically require pre-calibrated camera
arrays, controlled lighting conditions, or temporal information.
3. OVERVIEW
Our system runs in a frame-by-frame manner to process an input
sequence. Figure 2 illustrates the processing of one frame given
previous frames. In our system, we represent the static model containing both the geometry and surface albedo of the captured object
in a coordinate system defined in a frame called the canonical frame.
The first step of our system is the joint motion and lighting optimization (Section 5.1), in which the non-rigid surface motion from
the static model to the current captured frame is estimated along
with the low-frequency environmental lighting of this frame. This
is achieved by first warping the static model using the reconstructed
motion from the previous frame, followed by solving a unified optimization problem to fit the current depth and color. Because both
the lighting and motion are correlated with the observed appearance in the image, a unified framework facilitates convergence to
the global optimum through jointly solving for lighting and motion,
and the results are, consequently, more consistent with the observed
depth and color. Compared with DynamicFusion, because shading
information is considered, temporal coherence (the temporal correspondence between each pair of adjacent frames) is well preserved.
The second step is the static model update (Section 5.2). We
warp the static model using the newly solved motion and update the
warped static model with the current depth and color. As the lowfrequency lighting is solved for, the albedo is updated by solving a
volumetric optimization problem to fit the current observed image
and the original albedo as accumulated from previous frames. This
optimization makes the static model more complete, because new
parts of the object can be observed as it moves. Furthermore, because
temporal consistency is preserved in the first step, this optimization
correctly fuses multi-frame observations, thereby making the result
more robust against noise and reconstruction errors. Finally, the
updated static model is warped back to the canonical frame to be
used in the processing of the next frame.
4. PRELIMINARIES
In this section, we present the symbols and other notation used in this
article. To reconstruct a dynamic scene, a consumer RGB-D sensor,
such as a Kinect or an Xtion sensor, is used to simultaneously record
a depth sequence {Dt
} and a color sequence {Ct
} (t indicates the
frame label). The two sequences are synchronized, and the sensor is
pre-calibrated; thus, the per-pixel correspondence between Dt and
Ct is obtained by default. The output of our system includes a fused
static model S of the captured object, the per-frame low-frequency
environmental lighting Lt
, and a per-frame motion field Wt that
represents the non-rigid deformation between S and the real object
in each frame t.
We define S = {V, A}, where V and A represent the geometry
and appearance, respectively, of the captured object. The geometry
V is not represented as a set of surface points in 3D space but rather
as a truncated signed distance function (TSDF) [4] in a canonical
frame. We use the coordinate frame of the depth camera as the
canonical frame. Specifically, the canonical space is voxelized, and
each voxel x is assigned a value indicating its signed distance to
the closest surface to it in the scene. Signed distances with values
larger than a threshold τ (set to 0.01m) are truncated to τ . Mathematically, V = {[d(x) ∈ R, ω(x) ∈ R+]
T}. Here, d(x) encodes the
signed distance value for voxel x, whereas ω(x) indicates the confidence of that value; further details are provided in Section 5.2.1.
Following References [32, 38, 39], we assume the object surfaces
to be predominantly Lambertian, and the appearance A is represented by a set of RGB albedo vectors a(x) for the voxels in the
canonical frame. Note that the albedo represented on the voxels is
used to interpolate the true albedo on the object surfaces; thus, only
voxels on or near object surfaces have valid albedo values. The reason for using a volumetric data structure to represent the geometry
is that, compared with a mesh data structure, a regular volumetric
structure is more suitable for parallel memory allocation and access
on a graphics processing unit (GPU). It also enables more efficient
geometric operations (e.g., normal calculations) on a GPU. These
characteristics help to achieve real-time performance.
The environmental lighting Lt is represented by spherical harmonics (SH) [26]. For Lambertian surfaces, we use only the first
nine SH basis functions, which correspond to the low-frequency
lighting and provide a good approximation of Lambertian reflectance [12]. With this representation, the reflected irradiance B
of a voxel x is expressed as follows:
B(x) = a(x)
b2
m=1
mHm(nx). (1)
Here,{Hm}represents the SH basis functions, nx denotes the normal
of voxel x, and the {m} are the SH coefficients that define the
environmental lighting. b = 3 means that we consider only SH
basis functions of up to the second order.
We follow DynamicFusion [23] in representing the motion field
Wt by a graph-based motion representation, which can effectively
represent non-rigid deformation for a surface of any shape and can
be applied to deform voxels. Specifically, Wt = {[pj ∈ R3, σj ∈
R+, Tj ∈ SE(3)]T}, where j denotes the index of the j th node in
graph G. pj is the position of the j th node, and σj is a radius parameter related to the weight with which the j th node influences voxel
x. This weight is defined as wj (x, σj ) = exp(−
x − pj

2
/(2σ2
j )).
σj is a predefined parameter. Finally, Tj is the 6D transformation
ACM Transactions on Graphics, Vol. 36, No. 3, Article 32, Publication date: June 2017.