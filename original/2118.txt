Modeling and rendering of dynamic scenes is challenging, as natural scenes
often contain complex phenomena such as thin structures, evolving topology,
translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g.,
light field video) typically rely on constrained viewing conditions, which
limit interactivity. We circumvent these difficulties by presenting a learningbased approach to representing dynamic objects inspired by the integral
projection model used in tomographic imaging. The approach is supervised
directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two
primary components: an encoder-decoder network that transforms input
images into a 3D volume representation, and a differentiable ray-marching
operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared
to screen-space rendering techniques. The encoder-decoder architecture
learns a latent representation of a dynamic scene that enables us to produce
novel content sequences not seen during training. To overcome memory
limitations of voxel-based representations, we learn a dynamic irregular grid
structure implemented with a warp field during ray-marching. This structure
greatly improves the apparent resolution and reduces grid-like artifacts and
jagged motion. Finally, we demonstrate how to incorporate surface-based
representations into our volumetric-learning framework for applications
where the highest resolution is required, using facial performance capture
as a case in point.
CCS Concepts: • Computing methodologies → Neural networks; Rendering; Volumetric models.
Additional Key Words and Phrases: Volumetric Rendering, Volume Warping,
Ray Potentials, Differentiable Ray Marching
1 INTRODUCTION
Polygon meshes are an extremely popular representation for 3D
geometry in photo-realistic scenes. Mesh-based representations efficiently model solid surfaces and can be paired with sophisticated
reflectance functions to generate compelling renderings of natural
scenes. In addition, there has been significant progress recently in
optimization techniques to support real-time ray-tracing, allowing
for interactivity and immersion in demanding applications such as
Virtual Reality (VR). However, little of the interactive photo-real
content available today is data-driven because many real-world phenomena are challenging to reconstruct and track with high fidelity.
State-of-the-art motion capture systems struggle to handle complex
occlusions (e.g., running hands through one’s hair), to account for
reflectance variability (e.g., specularities in the sheen of a moving
object), or to track topological evolution in dynamic participating
ACM Trans. Graph., Vol. 38, No. 4, Article 65. Publication date: July 2019.
65:2 • Lombardi et al.
media (e.g., smoke as it billows upward). Where solutions exist,
they are typically specialized to an individual phenomenon [Atcheson et al. 2008; Hawkins et al. 2005; Roth and Black 2006; Xu et al.
2014], and are often aimed at either image generation [Buehler et al.
2001; Kalantari et al. 2016] or 3D reconstruction [Goesele et al. 2007;
Nießner et al. 2013], but not both. Since mesh-based representations
rely heavily on the quality of reconstruction to produce compelling
renderings, they are ill-suited to handle such cases. Nonetheless,
these kinds of phenomena are necessary to create compelling renderings of much of our natural world.
To address limitations posed by inaccurate geometric reconstructions, great progress has been made in recent years by relaxing
the physics-inspired representation of light transport, and instead
leveraging machine learning to bridge the gap between the representation and observed images of the scene. In Lombardi et al.
[2018], this technique was used to great effect in modeling the human face, where it was demonstrated that a neural network can be
trained to compensate for geometric reconstruction and tracking
inaccuracies through a view-dependent texture. Similar approaches
have also been shown to be effective for modeling general far-field
scenes [Overbeck et al. 2018]. An extreme variant of this technique
is screen-space rendering, where no geometry of the scene is used at
all [Karras et al. 2018; Wang et al. 2018]. Although these approaches
have been shown to produce high quality renderings of complex
scenes, they are limited to viewpoints available to the system at
training time. Since their neural architectures are not 3D aware, the
methods do not extrapolate to novel viewpoints in a way that is
consistent with the real world. The problem is exacerbated when
modeling near-field scenes, where variation in viewpoint is more
common as a user interacts with objects in the scene, compared with
far-field captures where there is less interactivity and the viewer is
mainly stationary.
An important insight in this work is that if both geometry and
appearance variations can be learned simultaneously, phenomena
explainable by geometric variations may be modeled as such, leading to better generalization across viewpoints. The challenge, then,
is to formulate this joint learning such that good solutions can
be found. Directly optimizing over a mesh-representation using
gradient-based optimization is prone to terminating in poor local
minima. This is the case even when a model of both appearance and
geometry are known a priori, and is exacerbated when these models
are also unknown. One of the main reasons for this difficulty is the
local support of the gradients of mesh-based representations. To address this, we propose using a volumetric representation consisting
of opacity and color at each position in 3D space, where rendering
is realized through integral projection. During optimization, this
semi-transparent representation of geometry disperses gradient information along the ray of integration, effectively widening the
basin of convergence, enabling the discovery of good solutions.
Although the volumetric representation has the ability to represent 3D geometric phenomena in a geometrically faithful way, it can
easily over-fit from image-supervision for a typical density of viewpoints. As such, additional regularization is necessary to achieve
good results. In this work, we show that a neural-network decoder is
sufficient to encourage discovery of solutions that generalize across
viewpoints. At first glance, this may appear surprising given the decoder network typically has enough capacity to reproduce solutions
found through a direct solve of the volume’s entries. We conjecture that the decoder network introduces spatial regularity into the
gradients of the volume’s entries (i.e., opacity and color), leading
to more generalizable solutions without diminishing the volumetric representation’s capacity. Additionally, the decoder network is
paired with an encoder network that produces a low-dimensional
latent space that encodes the state of the scene at each frame, enabling joint reconstruction of sequences rather than just individual
frames. Analogous to non-rigid structure from motion [Torresani
et al. 2008], this architecture can leverage a scene’s regularity across
time to improve viewpoint generalization further. This latent code
can be used to generate novel renderings of the scene’s content by
traversing the latent space, enabling realistic modifications of the
recording, or even completely new sequence animation, without
requiring object/scene/content specific solutions.
Despite these advantages of the volumetric representation, its
main drawback is limited resolution. Using voxel-based data structures to represent a scene typically requires an order of magnitude
more memory than its mesh-based counterpart to achieve similar
levels of resolution. Furthermore, much of this memory is dedicated
to modeling empty space or the inside of objects; neither of which
have an impact on the rendered result. This limitation stems from
the regular grid structure this representation exhibits. To overcome
this limitation, we employ a warping technique that indirectly escapes the restrictions imposed by a regular grid structure, allowing
the learning algorithm to make the best use of available memory.
Using this technique, we demonstrate significantly higher fidelity
than using only a conventional voxel data structure. Furthermore,
as our representation is 3D based, we can naturally combine it with
surface-based reconstruction and tracking methods when appropriate. This allows us to reach the highest levels of fidelity on objects
in the scene for which state-of-the-art reconstruction and tracking
work well, while also maintaining a complete model of the scene.
In summary, we propose a novel volumetric representation that
is object/scene agnostic, can generalize well to novel viewpoints,
reconstructs dynamic scenes jointly, facilitates novel content generation, requires only image-level supervision and is end-to-end
trainable. The resulting models afford real-time rendering and support on-the-fly adjustments, suitable for interactive applications in
VR. In §2 we cover related work, followed by an overview of our
approach in §3. Details of the encoder and decoder architectures
are covered in sections §4 and §5 respectively. Rendering through
integral projection is discussed in §6 and details of the learning
problem are covered in §7. We evaluate this architecture on a number of challenging scenes and present ablation study on various
design choices in our construction in §8. We conclude in §9 with a
discussion and directions of future work.
2 RELATED WORK
Our approach is driven by learning and rendering techniques spanning multiple domains, from volumetric reconstruction and deformable volumes to neural rendering and novel view synthesis.
ACM Trans. Graph., Vol. 38, No. 4, Article 65. Publication date: July 2019.
Neural Volumes: Learning Dynamic Renderable Volumes from Images • 65:3
Multiview Capture (Section 8) Encoder + Decoder (Section 4+5) Ray Marching (Section 6) End-to-end Training (Section 7)
z
c
Final Reconstruction Target Image
Ray Marching Output
Composite
Fig. 2. Pipeline of our method. We begin with a multi-view capture system, from which we choose a subset of the cameras as input to our encoder. The
encoder produces a latent code z which is decoded into a volume that gives an RGB and α value for each point in space, as well as a warp field used to
index into the RGBα volume. The decoder may optionally use an additional control signal (e.g., head pose) c. We then use an accumulative ray marching
algorithm to render the volume. The final output in image space is an RGB image with associated alpha mask and estimated background. We composite these
components together and minimize the L2 loss between the rendered and target images w.r.t. network parameters.
The following paragraphs discuss similarities and differences to
previous works in these areas.
2.1 Classical Surface and Volumetric Reconstruction
Point- and surface-based reconstruction techniques have a long history in computer vision (see [Furukawa and Hernández 2015] for a
review). Following successful efforts in stereo matching [Scharstein
and Szeliski 2002], most of the subsequent literature has focused
on the multi-view case, including extensions of photometric consistency [Furukawa and Ponce 2010] and depth map fusion [Merrell
et al. 2007; Zach et al. 2007]. Despite some attempts at handling more
complex materials or semi-transparent surfaces [Fitzgibbon and Zisserman 2005; Szeliski and Golland 1999], many popular multi-view
stereo (MVS) methods, such as COLMAP [Schönberger and Frahm
2016; Schönberger et al. 2016], still struggle with thin structures and
dense semi-transparent materials (e.g., hair and smoke).
Volumetric reconstruction methods side-step this problem of
explicit correspondence matching, beginning with Voxel Coloring [Prock and Dyer 1998; Seitz and Dyer 1997, 1999] and Space
Carving [Bonet and Viola 1999; Broadhurst et al. 2001; Kutulakos and
Seitz 2000]. These methods recover occupancy and color in a voxel
grid from multi-view images by evaluating the photo-consistency of
each voxel in a particular order. Our method is similar to these classical voxel-based techniques in spirit, but rather than using a strict
photometric consistency criterion, we learn a generative model that
tries to best match the input images. Since we do not assume that
objects in a scene are composed of flat surfaces, this approach also
allows us to overcome the typical limitations of MVS methods and
capture rich materials and fine geometry.
2.2 Volumetric Reconstruction with Ray Potentials
A number of more recent works on volumetric reconstruction have
explored the concept of ray potentials, i.e., cost functions between
the first surface struck by a ray and the color (or other property) of
the corresponding pixel. Ulusoy et al. [2015], Savinov et al. [2016],
and Paschalidou et al. [2018] formulate graph-based energy or inference objectives using ray potentials as constraints. A differentiable
ray consistency criterion that inspired our work is developed by Tulsiani et al. [2018, 2017], who use an encoder-decoder architecture
to predict voxel occupancy probabilities from RGB images. A loss
on ray potentials evaluated in this volume is then backpropagated
to the underlying convolutional architecture.
A fundamental difference between our work and previous works
using ray potentials is that we model voxel transparency rather
than occupancy probability, as we are focused on rendering rather
than reconstruction. This explicit image formation process allows
us to reconstruct and render dynamic scenes of semi-transparent
materials, such as smoke.
2.3 Deformable Volumes
Non-rigidly deforming objects pose challenges to both optimizationand learning-based approaches. Over the years, significant efforts
have been spent on their reconstruction and tracking from RGB-D
sensors: the DynamicFusion method of Newcombe et al. [2015]
produces a base 3D template surface using a Truncated Signed
Distance Function (TSDF) representation, and a time-dependent
warp to fuse a sequence of depth frames. Zollhöfer et al. [2014] and
Innmann et al. [2016] also deform a 3D template surface with an
as-rigid-as-possible regularizer.
Our rendering model is based on classical volume ray marching [Ikits et al. 2004; Levoy 1988], but we introduce the concept of a
warp field to it: instead of directly sampling color and opacity along
the ray, we first sample a 3D warp field encoding the location (within
a template voxel grid) from which color and opacity are sampled.
In 2D, similar techniques have been used to learn warps that align
images [Dai et al. 2017; Jaderberg et al. 2015; Shu et al. 2018]. In
our work, the warp field is not only used to model motion but also
increases the effective resolution of the voxel grid by simulating a
dynamic non-uniform sampling grid.
2.4 Neural Rendering
Deep learning-based rendering has become an active area of exploration, with some methods relying on volumetric representations.
ACM Trans. Graph., Vol. 38, No. 4, Article 65. Publication date: July 2019.
65:4 • Lombardi et al.
Nguyen-Phuoc et al. [2018] use a convolutional neural network applied on a voxel grid to produce an image, but their method requires
correspondence between the images and the voxel reconstruction.
DeepVoxels [Sitzmann et al. 2018] automatically learns a 3D feature representation for novel view synthesis but is limited to static
objects and scenes, and the employed architecture does not lend
itself to real-time inference. Martin-Brualla et al. [2018] use neural
networks to fill holes and generally improve the quality of a textured
geometric representation. Kim et al. [2018] use a U-net architecture to convert an image of rasterized attributes to realistic images,
similar to pix-to-pix [Isola et al. 2017]. Our method shares many
similarities with these approaches but has one critical difference:
machine learning is only used to generate an RGBA volume, which
is then rendered with a ray marching algorithm with no learned
parameters. This is important as it gives us an interpretable volume,
which may lead to better viewpoint generalization.
In our evaluation of hybrid rendering approaches, we employ
the Deep Appearance Model of Lombardi et al. [2018] that provides
a textured mesh representation. The method learns a Variational
Autoencoder (VAE) model representing the dynamic mesh and viewdependent texture of a specific person’s face. While we also evaluate
our model on human faces, our method does not require precise
mesh tracking or other forms of pre-processing. Instead, it is trained
end-to-end, using only raw images as supervision. Volumetric representations such as ours are also able to better represent complex
surfaces like hair that are difficult to model using meshes.
2.5 Novel View Synthesis
Novel view synthesis aims to produce RGB images of novel views
given a set of input RGB images. Typically, these methods use a
geometric proxy to assist in reprojecting 3D points back into the
input images and some blending is performed to produce a final
pixel color. Buehler et al. [2001] and Davis et al. [2012] use heuristics
to blend contributions of different images based on the rays from
the geometric proxy to each camera. Hedman et al. [2018] uses neural networks to determine blending weights, which can overcome
inaccurate geometric proxies. Zhou et al. [2018] skip the geometric
proxy altogether and use a neural network to compute blending
weights of each image projected along a set of planes. Penner and
Zhang [2017] compute a soft volumetric representation using MVS
depth maps to perform novel view synthesis. Unlike most novel
view synthesis techniques, our method operates on sequences and
creates an animatable model.
To compute geometric proxies, a multi-view stereo method is
often employed. While free-viewpoint video methods [Collet et al.
2015; Prada et al. 2016] rely on a sophisticated combination of multiview stereo techniques (including silhouettes and MVS) to reconstruct many kinds of objects, our method creates novel views of
dynamic scenes with a single generative framework. In addition, our
model’s latent embedding of the scene allows us to generate novel
animations more flexibly by producing new embedding sequences.
3 OVERVIEW
We present an end-to-end pipeline for rendering images from novel
views with only image supervision that leverages an internal 3D
volumetric representation. There are two main parts to the method:
an encoder-decoder network that converts input images into a 3D
volume V(x), and a differentiable raymarching step that renders an
image from the volume V given a set of camera parameters. The
method can be thought of as an autoencoder whose final layer is a
fixed-function (i.e., no free parameters) volume rendering operation.
Formally, we model a volume that maps 3D positions, x ∈ R3
, to
a local RGB color and differential opacity at that point,
V : R3 → R4
, V(x) =

Vrgb(x), Vα (x)

, (1)
where Vrgb(x) ∈ R3
is the color at x and Vα (x) ∈ R is its differential opacity in the range [0, ∞], with 0 representing full transparency. The purpose of a semi-transparent volume is two-fold:
first, it is a softening of a discrete volume representation which
enables gradients to flow for learning; second, it allows us to model
semi-transparent objects or bundles of thin structures that appear
translucent at limited resolutions, like hair.
Fig. 2 shows a visual representation of the pipeline. We first
capture a set of synchronized and calibrated video streams of a
performance from different viewpoints. Next, an encoder network
takes images from a subset of the cameras for each time instant and
produces a latent code z that represents the state of the scene at
that time. A volume decoder network produces a 3D volume given
this latent code, V(x; z), which yields an RGBα value at each point x.
Finally, an accumulative ray marching algorithm renders the volume
from a particular point-of-view. We train this system end-to-end by
reconstructing each of the input images and minimizing the squared
pixel reconstruction loss over the entire training set. At training
time, we run through the entire pipeline to train the weights of the
encoder-decoder network. At inference time, we produce a stream
of latent codes z (either the sequence of latent codes produced by
the training images or a novel generated sequence) and decode and
render in real time.
4 ENCODER NETWORK
The main component of our system that enables novel sequence
generation is the encoder-decoder architecture, where the scene’s
state is encoded using a consistent latent representation z ∈ R256
.
A traversal in this latent space can be decoded into a novel sequence
of volumes that can then be rendered from any viewpoint (see §5 for
details). This is in contrast to methods that rely on specialized mesh
constructions per frame which only allow for playback [Collet et al.
2015] or limited control over the generative process [Prada et al.
2016]. Moreover, this representation allows for conditional decoding,
where only part of the scene’s state is modified on playback (i.e.,
expression during speech, view-dependent appearance effects, etc.).
The encoder-decoder architecture naturally supports this capability
without requiring specialized treatment on the decoder side so long
as paired samples of the conditioning variable are available during
training.
To build the latent space, the information state of the scene at
any given time is codified by encoding a subset of views from the
multi-camera capture system using a convolutional neural network
(CNN). The architecture of the encoder is shown in Fig. 3. Each
camera view is passed through a dedicated branch before being
ACM Trans. Graph., Vol. 38, No. 4, Article 65. Publication date: July 2019.
Neural Volumes: Learning Dynamic Renderable Volumes from Images • 65:5
*…
Conv2D
*
Reshape
*…
Conv2D
*
Reshape
…
…
FCs
Concat
334 x 512 x 3
3 x 4 x 256 3072
9216 256
…
Image 1 Image K
Concat
z
c
Condition
256+L
Sample
256
Reparam
Fig. 3. Encoder architecture. Inputs are images from a subset of K-cameras,
passed through camera-specific CNNs. The latent variable z is concatenated
with the L-dimensional conditioning variable c before being passed to the
decoder (see §5).
concatenated with those from other views and further encoded
down to the final shape. Although using all camera views as input
is optimal from an information-theoretic perspective, we found that
using K = 3 views worked well in practice, while being much more
memory and computationally efficient. To maximize coverage, we
select a subset of cameras that are roughly orthogonal, although our
system is not especially sensitive to the specific choice of views. In
practice, we used the frontal, left and right-most camera views and
downsampled the images by a factor of 8 to size 334 × 512 pixels.
To generate plausible samples during a traversal through the latent space, the generative model needs to generalize well between
training samples. This is typically achieved by learning a smooth
latent space. To encourage smoothness, we use a variational architecture [Kingma and Welling 2013]. The encoder outputs parameters of a diagonal 256-dimensional Gaussian (i.e., µ and σ), whose
KL-divergence from a standard Normal distribution is used as regularization. Generating an instance involves sampling from this
distribution using the reparameterization trick, and decoding into
the volumetric components described in §5.
In addition to encouraging latent space smoothness, the variational architecture also ensures that the decoder makes use of the
conditioning variable when it is trained jointly with the encoder, as
described in §7. Specifically, since the variational bottleneck maximizes the non-informative latent dimensions [Higgins et al. 2017],
information pertaining to the conditioning variable is projected out
of the latent space, leaving the decoder no choice but to use the
conditioning variable in its reconstruction.
This method of conditioning can be applied using any auxiliary information available to the user for controlling the rendered
output. In §8 we show experiments demonstrating this for a few
types of conditioning information. Of particular importance is viewconditioning, which allows view-dependent effects, such as specularity, to be rendered correctly. When viewed in VR, the auxiliary
information in the form of a view-vector can be obtained from the
relative orientation of the headset in the virtual scene.
5 VOLUME DECODERS
In this section we discuss parameterizing the RGBα volume function V using different neural network architectures which we call
volume decoders. We discuss possible representations for the volumes (voxel grids and multi-layer perceptrons) and a method for 13
643 1283
1024 8 4
128
4
C
D3
1283
4
Reshape
1283×4
…
256+L
Reshape
1024
FCs FCs
(a) Convolutional Decoder (b) Linear Basis Decoder
* g(z) g(z) T
* … T
*
z T
c
z
c
256+L
Fig. 4. Voxel grid decoders. (a) Convolutional and (b) Linear basis decoders.
L denotes the size of the conditioning variable.
increasing the effective resolution by using warping fields. Finally,
we discuss view-conditioning for modeling view-dependent appearance.
5.1 MLP Decoders
One possible model for the volume function V(x; z) at point x with
state z, is an implicit one with a series of fully connected layers with
non-linearities. A benefit of this approach is that we’re not restricted
by voxel grid resolution or storage space. Unfortunately, in practice
an MLP requires prohibitive size to produce high-quality reconstructions. We must also evaluate the MLP at every step along each ray
in the ray-marching process (see §6), imposing an equally restrictive
upper bound on the MLP complexity for real-time applications.
5.2 Voxel Grid Decoders
Rather than trying to model the entire volume implicitly with an
MLP, we may instead assume that the volume function can be modeled as a discrete 3D grid of voxels. We produce this explicit 3D
voxel grid as the output tensor of a neural network. Let the tensor
Y ∈ RC×D×D×D represent a D×D×D grid of values in RC, with C
the channel dimensions. Define S(x; Y) : R3 → RC to be an interpolation function that samples from the grid Y by scaling continuous
values in the range [−1, 1] to the grid [1,D] along each dimension,
followed by trilinear interpolation. We can define a volume decoder
for a 3D cube with center at xo and sides of size W ,
V(x; z) = S

x − xo
W /2
; g(z)

, (2)
where g(z) is a neural network that produces a tensor of size
4×D×D×D. Note that we only evaluate the decoder function inside
the volume it covers by computing intersections with its bounding
volume.
In practice, we use either a convolutional architecture or a series
of fully-connected layers to implement g(z). In the former case, we
first apply a fully-connected layer and non-linearity to transform z
into a 1024-dimensional representation and reinterpret the resulting
vector as a 1×1×1 cube with 1024 channels. We experiment with two
convolutional architectures, one with a final size of 323
(achieved
with 5 transposed convolutions and running at 90Hz) and one with
a final size of 1283
(achieved with 7 transposed convolutions and
running at 22Hz). As an alternative, we consider a bottleneck architecture capable running at 90Hz consisting of 3 fully-connected
layers with output sizes of 128, 4, and 1283 × 4, respectively, with
the last layer representing the decoded volume. After decoding the
volume, we apply a softplus function to the RGBα values to ensure
they are non-negative. Both decoder variants are illustrated in Fig. 4.
ACM Trans. Graph., Vol. 38, No. 4, Article 65. Publication date: July 2019.
65:6 • Lombardi et al.
5.3 Warping Fields
On their own, voxel grids are limited because they can only represent
details as small as a single voxel and are expensive to compute and
store at high resolution. Additionally, they are wasteful in typical
scenes where much of the scene consists of empty space. Common
solutions to these problems are spatial acceleration structures like
octrees (e.g., Riegler et al. [2017]), but it’s difficult to modify these
to work in a learning setting. They also typically require that the
distribution of objects within the structure is known a priori, which
is not true in our case as we do not use any 3D training data.
To solve these problems, we propose to use warping fields to both
alter the effective resolution of the voxel volumes as well as model
motion more naturally. In our warping formulation, we produce a
template RGBα volume T(x) and a warp volume W−1
(x). Each point
in the warp volume gives a corresponding location in the template
volume from which to sample, making this an inverse warp as it
maps from output positions to template sample locations. As before,
both template and warp are decoded from a dynamic (per-frame)
latent code z, which we drop from the notation for conciseness.
The choice of inverse warps rather than forward warps allows
representing resolution-increasing transformations by mapping a
small area of voxels in the output space to a larger area in the
template space without requiring additional memory. Thus, the
inverse warp can represent details in the output space with higher
resolution than uniform grid sampling, but remains well-defined
everywhere in the output space, which is necessary for providing
usable gradients during learning.
Formally, we define the inverse warp field,
W−1
(x) → y x, y ∈ R3
, (3)
where x is a 3D point in the output (rendering) space and y a 3D
point in the RGBα template space. To generate the final volume
value, we first evaluate the value of the inverse warp and then
sample the template volume T at the warped point,
VRGBα (x) = TRGBα (W−1
(x)). (4)
5.4 Mixture of Affine Warps
A critical piece of including warp fields is determining how they’re
produced. As we show later, the architecture of the warp field decoder makes a large difference on the quality of the model.
A straightforward approach to decoding warp fields would be
to use deconvolutions to produce a warp field with freely-varying
template sample points at each output point. This parameterization,
however, is too flexible, resulting in overfitting and poor generalization to novel views as we show in our experimental results. We
instead take the approach that the basic building block of a warp
field should be an affine warp. Since a single affine warp can’t model
non-linear bending, we use a spatial mixture of affine warps to
produce an inverse warp field.
We write the affine mixture as,
W−1
(x) =
Õ
i
Ai (x) ai(x), (5)
with Ai(x) = Ri (si ◦ (x − ti)), ai(x) =
wi (Ai (x))
Í
j wj

Aj (x)
 , (6)
where Ai(x) is the i
th affine transformation, {Ri
, si
, ti } define the
rotation, scaling, and translation of the i
th affine transformation
parameters, ◦ is element-wise multiplication, andwi(x) is the weight
volume of the i
th warp. Note that we sample the spatial mixture
weight wi after warping (“warped weights”). The intuition behind
this is that the warped space represents different parts of the scene,
and the weighting function should be in that space as well. This
can be viewed as an extension of linear blend skinning [Lewis et al.
2000] applied to a volumetric space.
To compute the transformation parameters {Ri
, si
, ti } and the
weighting volume wi we use 2 fully-connected layers after the encoding z. For rotation, we produce a rotation quaternion vector
which is normalized and transformed into a rotation matrix. Before
outputting the values of the weighting volume, we apply exp(·) to
ensure the weights are non-negative. Unlike the voxel volume V,
we clamp samples outside the weighting volume wi to the surface
otherwise the warps can get “stuck” early in training if they land
outside the volume. In practice, we found that a mixture of 16 warps
provides sufficient expressiveness. In all experiments where warping is used, we learn an additional global warp {Rg, sg, tg} (with
parameters produced by MLP) that is applied to x before the warping field, and we use a 323 voxel grid to represent the warp field
as we found that the low resolution provides smoothness from the
trilinear interpolation that helps learning.
5.5 View Conditioning
In order to model view-dependent appearance, we opt to condition
the RGB decoder network on the viewpoint. This allows us to model
specularities in a data-driven way, without specifying a particular
functional form. To do this, we input the normalized direction of
the camera to the decoder alongside the encoding. Note that for
view-conditioned models, we use separate convolutional branches to
produce the RGB values and α values as we only want to condition
the RGB values on the viewpoint.
6 ACCUMULATIVE RAY MARCHING
We formulate a rendering process for semi-transparent volumes
that mimics front-to-back additive blending: As a camera ray traverses a volume of inhomogeneous material, it accumulates color
in proportion to the local color and density of the material at each
point along its path.
6.1 Semi-Transparent Volume Rendering
We generate images from a volume function V(x) by marching rays
through the volume it models. Critically, to model occlusions, the
ray accumulates not only color but also opacity. If the accumulated
opacity reaches 1 (for example, when the ray traverses an opaque
region), then no further color can be accumulated on the ray. In
particular, the color Irgb(p) at a pixel p in the focal plane of a camera
with center ro ∈ R3
and image-to-world transformation P
−1
is given
by raymarching in the unit direction rd = (P
−1p−ro )/∥P
−1p−ro ∥ ∈
ACM Trans. Graph., Vol. 38, No. 4, Article 65. Publication date: July 2019