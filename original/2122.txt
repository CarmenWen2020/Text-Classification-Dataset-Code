We present a new optical design for see-through near-eye displays that
is simple, compact, varifocal, and provides a wide field of view with clear
peripheral vision and large eyebox. Key to this effort is a novel see-through
rear-projection screen. We project an image to the see-through screen using
an off-axis path, which is then relayed to the user’s eyes through an on-axis
partially-reflective magnifying surface. Converting the off-axis path to a
compact on-axis imaging path simplifies the optical design. We establish
fundamental trade-offs between the quantitative parameters of resolution,
field of view, and the form-factor of our design. We demonstrate a wearable
binocular near-eye display using off-the-shelf projection displays, customdesigned see-through spherical concave mirrors, and see-through screen
designs using either custom holographic optical elements or polarizationselective diffusers.
CCS Concepts:• Human-centered computing→Displays and imagers;
• Applied computing → Physics; • Hardware → Emerging optical
and photonic technologies;
Additional Key Words and Phrases: Near eye displays, See-through Displays,
Varifocal Displays, Computational Displays, Augmented Reality Displays,
Holography, Holographic Optical elements
1 INTRODUCTION
Augmented reality (AR) has recently gained momentum in the
form of a variety of available optical see-through near-eye displays
(NEDs) including the Meta 2, the Sony Smart Eye, and the Microsoft
Hololens. Kress and Sterner [2013] described the goal of consumerlevel AR NEDs as providing a cost-effective solution to multiple
optical design challenges: resolution, transparency, eyebox, and
field of view (FoV). Cakmakci et al. [2006] point out FoV as the
major optical design challenge. There is also increasing evidence
that accommodation-aware systems improve user experience in
Virtual Reality (VR) and AR systems [Konrad et al. 2016; Kramida
and Amitabh 2016; Liu et al. 2008]. Addressing all of these characteristics is an ongoing challenge in AR NED design. We present
a new configuration for an AR NED that provides the following
contributions:
• See-through screens for near-eye displays: We introduce onaxis see-through intermediate image surface (OASIIS) screens,
which address a major problem in optical AR NED design by providing an on-axis path for optical combiners,
while avoiding significant perceptual degradation of the
real world.
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
1:2 • K. Akşit et al.
Table 1. Comparison of recent accommodation supporting see-through near eye displays (modeled after tables in Dunn et al. [2017] and Matsuda et al. [2017]).
Our prototype demonstrates a unique combination of good from factor, resolution, FoV, and eyebox. Note that, in our chart, wide field of view is defined as
≥ 60o degrees, high resolution is defined as ≥ 15 cycles per degree (cpd), and a moderate eyebox is defined as 5 − 10 mm.
Focus mechanism FoV resolution eyebox form factor compute overhead
Pinlight displays [Maimone et al. 2014] always in focus wide low moderate thin high
Free-form optics [Hua and Javidi 2014] light fields narrow high moderate moderate high
HOE [Kim et al. 2015] holographic wide high small bulky N/A
HOE [Maimone et al. 2017] holographic wide high small N/A high
Focus tunable light engine [Liu et al. 2008] varifocal narrow high small bulky moderate
Multi-focal plane display [Hu and Hua 2014] varifocal narrow high moderate bulky high
Membrane [Dunn et al. 2017] varifocal wide high large bulky low
This work varifocal wide high large moderate low
• Wide FoV: The use of on-axis beam combiners with a large
aperture size leads to a wide FoV.
• Large eyebox: The combination of traditional beam combiners and the novel see through screen leads to a large
eyebox.
• Simple varifocal mechanism: The focal plane can be adjusted
by mechanically moving thin and light optical components.
• Wearable prototype: As a proof of concept, we make and
test a wearable AR NED. Our prototype employs two offthe-shelf projectors, two concave see-through mirrors, a
mechanical linear actuator, and two OASIIS screens in a 3D
printed housing. The prototype provides brightness levels
suitable for indoor and outdoor applications.
We believe this design lends itself to straightforward miniaturization. Each component is relatively cheap to manufacture and the
design will benefit from continuing improvements of picoprojectors
and actuators. Critically, the design can support an accommodationaware system, by integrating a (commercially available) gaze tracker
and a fast linear mechanism for moving the optical components;
we also demonstrate this capability with our prototype. The least
straightforward element of our design is the OASIIS screen, which
we describe in detail in this paper.
2 RELATED WORK
Our design is a see-through AR NED with a simple varifocal mechanism aimed at future accommodation-aware systems. We here
review see-through AR screens used in non-NED contexts, other
NED AR optics designs, and finally accommodation-aware NED
designs in both AR and VR contexts. We do not review direct retinal projection AR systems [Urey 2000] nor video see-through AR
systems [Rolland and Fuchs 2000] which are completely different
approaches to AR than the beam-splitter family of designs to which
ours belongs. These different families of approaches have different
advantages and disadvantages and all three are being actively pursued by the AR community. Our prototype is contrasted with other
optical see-through AR NEDs in Table 1. These are all research prototypes so the table does not show what the limits are in the various
approaches, but they do indicate how far the various approaches
can definitely be pushed.
2.1 See-through Screens:
Researchers have explored see-through screen designs based on classical optical components. Hedili et al. [2013] describe a see-through
microlens array for a heads-up display application. Soomro and
Urey [2016] report a see-through screen based on retro-reflectors
for a head mounted projection display application. Neither of these
approaches has yet been redesigned for NEDs, nor for the expected
diffraction effects accompanying that miniaturization.
Using silver nanoparticles and a front projector, Hsu et al. [2014]
create a transparent screen that backscatters light at specific wavelengths. Yamamoto et al. [2016] also describe a different approach
to a wavelength selective front projection transparent screen using cholesteric liquid crystal dots. Both approaches scatter light in
both the forward and reverse directions which can lead to a strong
perceivable haze in a NED application.
Holographic optical elements (HOEs) are able to direct light from
a carefully placed light source towards a user’s eye while appearing
transparent to light from the environment. HOEs have been used as
optical components in transparent heads-up displays [Tolstik et al.
2009]. Lee et al. [2016] used two reflective diffusive HOEs and two
projectors to create a transparent additive light field display that
is in a similar spirit to our OASIIS screen, but is front-projection
and not for NED. Reflective and diffusive holographic combiners
have been used for transparent screens [Eisen et al. 2006; Yeom
et al. 2014]. HOEs can also be used to redirect light from an off-axis
projector into the eye [Kim et al. 2015; Maimone et al. 2017]. This
serves the same function as our HOE and beam combiner together
and has great potential for compactness and transparency but so
far has unresolved issues with eyebox, and demands more compute
before projection than our approach.
Spatial light modulators (SLMs), such as OLEDs or LCDs, can
also function as see-through screens. Görrn et al. [2006] introduced
entirely transparent displays using OLEDs with transparent contacts
and thin-film transistors.
However, the periodic aperture arrangement in SLMs can cause
diffraction-related image degradation over the observed real world;
Using LCDs directly in front of an eye, Maimone et al. [2014] shows
a typical example of such degradation. LCDs with polarizers also
block certain polarizations of the light. Tsai et al. [2015] propose a
custom aperture arrangement for avoiding degradation with OLEDs
which is similar to coded-aperture techniques [Ma et al. 2013]. Thus,
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
Near-Eye Varifocal Augmented Reality Display using See-Through Screens • 1:3
it improves some details of the real world while degrading others.
Since we do not use SLMs as see-through screens, we avoid these
shortcomings.
2.2 See-through NED Optics:
Sutherland [1968] introduced see-through NEDs using a beam combiner near the eye of a subject to superimpose the direct view of
the real world and computer-generated images. Optical systems
relying on flat combiners have progressed greatly as described by
Rotier [1989] and Cakmakci, [Cakmakci and Rolland 2006]. The
geometry of flat beam combiners along with the lenses used in optical NEDs dictates a strict trade-off: a large FoV quickly leads to a
bulky form factor. Droessler et al. [1990] propose a tilted catadioptric (reflecting and refracting) system to overcome FoV limitations
by tilting the optics with respect to a flat combiner and using a
curved combiner as the final relay surface, which provides up to
60o of rotationally symmetrical monocular FoV. Tilted catadioptric
systems are fundamentally limited in light efficiency, depend on a
complex polarized optical system, and produce a bulky form factor. Gilboa [1991] propose an off-axis single-element curved beam
combiner, and explore the associated optical design space. Today,
modern variants of off-axis single-element curved beam combiners
(e.g., [Wang et al. 2016]) are deployed in military applications and
consumer level prototypes (e.g., Meta 2). Major limitations in offaxis single-element curved beam combiners come into play while
extending FoV in horizontal directions when lit vertically; these
combiners are known to provide poor imaging characteristics with
eccentricity, and require a larger screen with a larger FoV demand.
Our optical layout builds on concepts shared with these systems. We
essentially convert off-axis projection paths to on-axis paths using
our OASIIS screens, which avoids the shortcomings of previous
optical designs and provides a compact optical layout.
Another family of see-through NEDs is based on waveguides.
Cheng et al. [2009] propose a waveguide based NED design that
fuses curved beam combiners and waveguides into a single freeform prism. They describe a tiling strategy of these prisms to increase limited-FoV, which requires multiple displays per prism. Flat
combiners have been converted into thin cascaded waveguides as
a see-through NED prototype (e.g., Lumus), however FoV related
issues are still a major problem in practice. As described by Kress
and Shin [2013], holographic methods simplify designing waveguides through holographic out-coupling and in-coupling of light.
Today such displays are present as consumer level see-through NED
prototypes (e.g., Microsoft Hololens, Sony Smart Eye), which only
report maximum 45o diagonal binocular FoV. HOEs can function
as a complete reflective and diffusive beam combiner (e.g., Li et
al. [2016], Maimone et al. [2017]) with a small eyebox. Specifically,
the work of Maimone et al. [2017] promises a glasses form factor
see-through NED design, the chief limitations relative to our design
are as follows: device runs 20 Hz refresh rate for generating fullcolor images, provides an impractical small eyebox, and demands a
large computation. We demonstrate a practical use case of HOEs
with our optical layout. The true potentials of HOEs in practical
NED designs are yet to be explored.
Notably, most of these optical layouts block a large portion of
peripheral vision, whereas our design leaves peripheral vision clear.
2.3 Accommodation Supporting See-through NEDs:
A major problem with NEDs is the vergence-accommodation conflict
(VAC), where the binocular triangulation distance conflicts with the
focusing distance [Shibata et al. 2005]. Early on, Akeley et al. [2004]
demonstrate benefits of fixed-viewpoint volumetric desktop displays using planar multi planes and generate near-correct focus
cues without tracking eye position. Using the hardware layout of
work of Akeley et al. [2004], Mackenzie et al. [2010] describe the link
between displayed spatial frequencies and human eye accommodation has a correlation. Work of Hu et al. [2014] demonstrate that
planar multiple planes can also be used to discretize the planes of focus in a see-through NED design. Most recently, work of Matsuda et
al. [2017] introduce a new concept for NED design for VR using the
concept of deformed focus planes, and work of Konrad et al. [2017]
propose an accommodation-invariant NED VR design. Kramida and
Varshney [2016] , Masia et al. [2013] and Hong Hua [2017] give a
recent and comprehensive review of different computational NED
approaches to resolve VAC. Studies show evidence that supporting
accommodative cues through a varifocal mechanism improves visual comfort [Johnson et al. 2016] and user performance [Konrad
et al. 2016] while being simpler than other methods, but most current approaches sacrifice FoV and bulk. The recent design of Dunn
et al. [2017] provides a wide FoV and a fast varifocal mechanism,
but the flexible membrane mirror used as the outer optical surface
presents cosmetic and physical robustness difficulties for long-term
usage.
Integral Imaging, first proposed by Lippmann [1908], deals with
the capture and the reproduction of lightfields which can also address VAC. Huang et al. [2015] demonstrate a factorized compressive
lightfield for conventional NED VR design. A see-through NED design [Hua and Javidi 2014] uses a free-form prism from the work of
Cheng et al. [2009], while supporting lightfields in a limited FoV. The
work of Maimone et al. [2014] promises a wide FoV with see-through
capabilities, but diffraction effects limit image quality; liquid-crystal
switching times also limit their demonstrated prototype to 12 Hz
refresh rate for full-color images. HOEs have been demonstrated in
the context of generating lightfield displays [Lee et al. 2016], though
not applied to NEDs, yet.
Similar to the mechanism we use to adjust focus, some systems
have dynamically adjusted the distance between lens and screen [Liu
et al. 2008; Padmanaban et al. 2017; Sugihara and Miyasato 1998].
Inspired by previous work, we propose a see-through NED design
with a novel application of HOEs as OASIIS screens, opening the
door to accommodation support while providing high refresh rate,
full-color, improved resolution, wide FoV, large eyebox, and low
compute overhead.
3 SYSTEM OVERVIEW
The overall structure of an optical see-through NED can be summarized as two separate building blocks: a light engine and a beam
combiner. As shown in Figure 2, we place an OASIIS screen in
front of a viewer’s eye to relay information from an off-axis rearprojection light engine to an on-axis beam combiner. This on-axis
component decreases the design complexity of the beam combiner,
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
1:4 • K. Akşit et al.
and avoids fundamental limitations in both image quality and FoV
without substantially blocking peripheral vision.
The OASIIS screen acts as an intermediate image plane. Optical
NED designs with intermediate image planes are commonly known
as pupil-forming optical layouts [Cakmakci and Rolland 2006]. In
our design, the required size of a projected image dscr depends on
the nature of the curved beam combiner. Thus, we first explore the
design space of curved beam combiners.
Fig. 2. The optical layout of our see-through Near-Eye Display proposal: An
off-axis light engine projects information on an OASIIS screen. Projected
information on the OASIIS screen is relayed back to a viewer’s eye, while
being magnified and placed at a virtual image plane by a curved beam
combiner.
Fig. 3. A sketch showing major components of light rays that are interacting
with an OASIIS screen that is located between an eye and a curved beam
combiner. Stronger combiner reflectivity Im and screen forward scattering
Is f o
lead to better image quality.
3.1 Curved Beam Combiners:
A curved optical beam combiner can be designed with specific demands in mind, such as, FoV, angular resolution, and size of eyebox,
deyebox, the volume within which the pupil can move (Figure 2). The
human visual system (HVS) spans approximately 190o FoV horizontally [Webb 1964] and, with healthy young eyes (20/20 vision),
approximately 1 arc-minute of angular resolution or 30 cycles per
degree (cpd). We target deyebox of 10 × 10 mm to accommodate
gaze changes without degrading optical quality. We use de = 20
mm, distance between the eye and the OASIIS screen, which leaves
enough free-space propagation volume for our projection-based
light engine.
Figure 3 shows a ray tracing diagram for an OASIIS screen. Light
rays originated from a projector Ip will introduce multiple components as light reflects Ir0
, Ir1
, and scatters in forward Is f 0
, Is f1
and
back Isb0
, Isb1
directions. The major component of light contributing to the formation of a target image on a retina is the light that is
bouncing from a curved beam combiner Im and making its way to
the retina. Note that Im becomes a stronger component as Is f 0
dominates the scattering terms. For explanatory purposes, consider an
ideal flat rectangular OASIIS screen. Such a screen provides forward
diffusion Is f 0
as the major component, thus leading to a strong Im
while other components of light are negligibly small.
A simple solution is to use a spherical beam combiner with a
radius of curvature r, and thus a focal length f = −r/2. The center
of an OASIIS screen can be registered to a virtual point in space
located at a desired distance dv from a spherical beam combiner by
using
1
dv
=
1
f
+
1
dsm
⇔ dv =
1
1
f
+
1
dsm
(1)
where dsm represents the distance between the mirror and the
screen. Magnification of the screen can be calculated as M = hv /hs =
−dv /dsm, where hv represents the size of a virtual image created,
and hs represents the size of the screen. Total optical path length of
our proposal using an OASIIS screen and a spherical beam combiner
is expressed as OPL = dsm + de . Using the provided equations and
an in-house ray tracer, we compile a design trade space for our
proposal, showing the optical qualities of designed beam combiners
with required physical dimensions (Figure 4). We limit our spherical
beam combiner to a f-number of f /0.6 to avoid large image degradations with increasing eccentricities. This sample choice represents
a simpler design process compared to using other conic or freeform
surfaces, each with multiple variables to tune.
3.2 OASIIS Screens:
The most important component of our design is an OASIIS screen.
We evaluated multiple approaches: rotating diffusers, polarization
selective diffusers (PSDs), and, most successfully, HOEs. We explain
the theory and construction of HOEs here. We include details which
will be known to those with a practical holography background but
may not be familiar to others. For the interested reader, multiple, accessible and detailed introductions to holography and making HOEs
exist, see for instance, Saxby and Zacharovas [2016] or Ackerman
and Eichler [2007].
The interference from two coherent light beams, traditionally
called reference and object beams, creates a diffraction pattern which
can be recorded in a holographic recording medium (HRM). When
the holographic diffraction pattern has been properly exposed and
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
Near-Eye Varifocal Augmented Reality Display using See-Through Screens • 1:5
Fig. 4. One way to analyze the properties of our design is to graph the impact of two interacting free parameters on various dependent parameters. For a
given virtual image plane at a distance dv and an OASIIS screen to beam combiner distance dsm configuration, from left to right, the first row of diagrams
shows required focal length, beam combiner aperture size towards nose and peripheral vision, and optical path length. We indicate the prototype described in
Section 4 with a star on each chart. Note that aperture size towards nose is naturally limited to half of interpupillary distance; we use 30 mm as the limit
which would support most adults. Overall designs are limited with a f-number of f /0.6 to provide good imaging characteristics. For a given configuration, the
second row of diagrams shows magnifications provided, required screen size, and horizontal monocular field of view provided.
f = 200 mm
achromat
Fig. 5. Left: during recording, RGB light sources are combined to form a white light beam that is split into a reference beam and a object beam. A holographic
recording medium captures the interference from both beams. Right: As a recorded medium is illuminated with lights from a light engine that closely resembles
one of the original recording beams, the recorded medium diffuses light towards a curved beam combiner, which relays the resulting image on the recorded
medium to the user’s eye.
developed, re-exposing the diffraction pattern to the original reference beam (without the object beam present) recreates the object
beam. The reconstructed object beam propagates in the same direction as the original object beam starting from the position of the
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
1:6 • K. Akşit et al.
Light
Engine
Curved
Beam Combiners OASIIS
Screen
Path Folding
Mirrors
3D Printed
Housing
Wearable Fixed Focus Prototype
Curved
Beam Combiners
3D Printed
Housing
Light
OASIIS Engine
Screen
Motorized
Linear Stage
Wearable Varifocal Prototype
Fig. 6. Left: Our wearable fixed focus prototype using projection light engines, path-folding mirrors, curved beam combiners, 3D printed housing, and OASIIS
screens. Right: Our wearable varifocal prototype using projection light engines, a motorized linear stage, curved beam combiners, 3D printed housing, and
OASIIS screens.
hologram. The trick, for this project and others which create HOEs,
is to come up with appropriate object and reference beams.
Figure 5 is a schematic diagram of the setup used in our experiments. We start with the recording process which is depicted in
the left-hand part of the figure. The process starts on the left, where
laser beams of three different colors (red, green, and blue) are split
into two different beams with the help of a half-wave plate (labeled
“
λ
2
”) and a polarizing beam splitter (PBS). To simplify the diagram,
only the green laser is depicted. These six beams are combined with
an x-cube to form two different “white-light” beams (one horizontally polarized and the other vertically polarized). The horizontally
polarized beam (top beam path) passes through a beam expander
constructed from achromatic lenses with focal lengths −20 mm and
200 mm spaced by 180 mm which increases the diameter of the laser
beam by a factor of 10. This beam is, then, reflected off of a mirror
which directs the laser beam through a lens with a 3 mm focal length
which forms the diverging reference beam for our experiment.
As shown in lower beam path in Figure 5, the object beam in the
writing set-up is created by the vertically polarized beam. In order
for the object and reference beams to interfere, the polarization
of the object beam must be the same as the reference beam. The
vertical polarization of the light in the lower beam path is rotated
to horizontal with the help of another half-wave plate and a PBS.
Excess light can be shunted off to a beam dump placed next to
the PBS. Immediately following this, the diameter of the beam is
increased by a factor of 100× with the help of two stages of beam
expansion – one identical to the beam expansion used in the upper
beam path and one made up of lenses with focal lengths of −40 mm
and 400 mm. Upon passing through the last lens, the beam passes
through a diffuser and forms the (diffusing) object beam.
The most demanding parts of the optical set-up are making sure
that the original red, green, and blue laser beams are sufficiently
collinear that they are well balanced at the position of the HRM and
ensuring the beams leave each stage of beam expansion collimated.
The requirements for the beams at the position of the HRM or the
relative placement of the HRM from the last lens in the reference
beam or from the diffuser are not precise. The distance of the HRM
from the diffuser can be adjusted to change the relative intensity
of the reference and object beams at the location of the HRM (but,
this is more conveniently controlled with the half waveplate in the
lower beam path). The only requirements for the final set of optics
(the last lens in the reference beam path, the diffuser, and the HRM
are that the beam intensities need to be loosely uniform and that
the ratios of beam intensities need to be controlled (the ratio of
reference to object beam intensities in our experiments were 1:10).
An additional requirements exist on the light sources used and
the difference in the path length from the lasers to the HRM taken
by light which travels either the top or the bottom beam path in
Figure 5. The crests and troughs from the waves which make up the
two beams must arrive at the location of the HRM with the same
relative phase through out the recording process. In practice, this
means that the light used in both beams need to come from the same
laser source. The phase of a laser beam periodically resets in time
to a new random value (temporal coherence). This limits the length
over which a laser beam may be thought of as a single sinusoidal
wave (the coherence length). Thus, to preserve the relative phase
of the object and reference beams, the difference in path lengths
that two beams travel must be less than the coherence length of
the laser sources used. The recording process also depends on the
wavelength. Care should also be taken to insure that the power
density at each wavelength is similar in both beams at the location
of the hologram. Exposure times should be chosen so as to achieve
the desired total dosage at each wavelength (usually specified by
the manufacturer in units of mJ/cm2
).
In our HMD design we construct the reference beam by placing
the apparent origin of the “projection cone” of the projector at the
same relative location to the HRM as location of the focus of the
reference beam in the recording process. As mentioned before, when
the hologram is illuminated with a beam which closely resembles the
reference beam, this “reference beam” is diffracted by the hologram
in such a way that it recreates the object beam. If the hologram
is only partially illuminated by a part of the reference beam, then
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
Near-Eye Varifocal Augmented Reality Display using See-Through Screens • 1:7
only that part of the object beam which would originate from the
illuminated part of the HOE is reconstructed. Also, if the hologram is
illuminated with light which does not contain all of the wavelengths
used to create the hologram, only those colors actually present in
the reference beam will be present in the reconstructed object beam.
Note that the difference between an “object beam” and a “reference
beam” are arbitrary. Once the HOE is illuminated with light which
closely resembles light from either beam, the part of the illuminated
HOE will redirect light to create the corresponding part of the
other beam, and all other beams will pass through the HOE as in
Figure 1. The result is a thin, light-weight, see-through diffuser,
which functions as an OASIIS screen.
3.3 Light Engine:
A projection-based light engine typically combines light-emitting
diodes (LEDs) or lasers with modulation technologies such as liquid crystal on silicon (LCoS), digital micromirror device (DMD)
or scanning microelectromechanical systems (MEMS). Scanning
MEMS combined with laser sources promises an always-in-focus
beam at different throw distances dthrow, whereas conventional
LCoS coupled to lasers or LEDs require focusing optics. Having a
large dthrow with LCoS or DMD modulators, however, more closely
approximates an always-in-focus beam, and using LEDs decreases
the amount of visible speckle phenomena largely. All of the mentioned light engines are applicable to our proposal as long as they
are able to generate sharp pixels on our OASIIS screens at a given
dthrow, thus current projectors are requiring a custom approach
in projection optics, which we will discuss in up coming sections.
Our design can act as a varifocal system by changing the distance
between OASIIS screens and curved beam combiners, which can
be achieved by either moving curved beam combiners or moving
OASIIS screens. Moving curved beam combiners is practically less
demanding from light engine requirements stand point, whereas
in the other case projection has to be either always-in focus or has
to provide adjustable focus to match a variable dthrow. Therefore,
in our prototypes, we choose to move curved beam combiners and
keep OASIIS screens at a fixed location.
4 IMPLEMENTATION
We provide the details for our prototypes, and explore the varifocal
capabilities of our design. The OASIIS family of NED designs relies
on a see-through screen co-axial with display and eye; we describe
three example implementations using a mechanical rotating dotpattern diffuser, PSDs, and HOEs as OASIIS screens.
4.1 Wearable Prototypes:
Our two wearable prototypes, one with fixed focus and one with
varifocal capabilities, are shown in Figure 6. Modulated light from
projection light engines is channeled through free space to OASIIS
screens either directly or via path-folding mirrors, and information
on OASIIS screens relayed to a user’s eyes with partially reflective
curved beam combiners.
Our light engines are commercially available pico-projectors that
combine RGB LEDs to create a time-multiplexed white light source.
Combined white light in our light engines is modulated using LCOS
devices (OVP921, 1280×720 pixels, 60 Hz from ImagineOptix). Modulated light from our light engines approaches on OASIIS screens
with a wide angle of 56o
, and forms a sharp image on OASIIS screens
standing at dthrow = 60−180 mm in our two prototypes. In our fixed
focus prototypes, we use silver coated off-the-shelf standard optical
mirrors to fold the dthrow = 180 mm into a compact form factor. The
OASIIS screens in our prototypes are interchangeable and can be
PSDs (samples provided by Nitto Denko Corporation) or custom
HOEs. In our varifocal prototype, location of a curved beam combiner is controlled by a Actuonix PQ12 linear actuator, and driven
by an Arduino microcontroller (uC). Response time to shift of a
beam combiner from one extreme to an another (0.2 − 5 D) is measured as 410 ms. A key question is whether a change in gaze can be
determined and the focal adjustment accomplished, without hurting
the ability to accommodate in practice. The control of accommodation is still under active investigation by vision scientists, and the
driving mechanism probably rely on several input stimuli including
chromatic characteristics [Kruger et al. 1995] and optical or computational blur [Ciuffreda et al. 2006; Del Águila-Carrasco et al. 2017].
There is also some reason to believe that varifocal systems can help
driving accommodation as the total latency for accommodation is
approximately one second [Campbell and Westheimer 1960]. There
is some empirical evidence that varifocal systems similar to ours
provide practical accomodation support in VR/AR contexts [Padmanaban et al. 2017] .
We use custom spherical concave mirrors (Diverse Optics) made
from Zeonex. The inner mirror surface is coated with silver (beamsplitter coating 80% reflective, 20% transmissive). Our spherical
mirrors have 76 mm radius of curvature and aperture size daperture,
of 60 mm. In our prototypes, de = 20 mm and dsm = 37 mm (OPL =
57 mm). All components of our prototypes are assembled with a 3Dprinted housing built using a Formlabs 2 3D printer and FreeCAD
software. We drive a prototype via two HDMI ports from a NVIDIA
GeForce GTX 1080 on a Linux computer. The computer provides
real-time imagery of 3D models using in-house built Python and
OpenGL/GL Shader Languages based software while also compensating for mirror and keystone distortions.
Fig. 7. Left: photograph showing a close look on the surface of a static
dot-pattern diffuser illuminated with a projection light engine. Right: image
formation when static dot-pattern diffuser is rotated around an axis with a
conventional electrical motor.
4.2 Time-Multiplexed Dot-pattern Diffusers:
Our initial investigations used a simple and low-cost OASIIS screen:
a dot-pattern projection surface, such as typically found in an LCD
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
1:8 • K. Akşit et al.
backlight system [Chang and Fang 2007]. Dot-pattern screens contain a spatial pattern that is partially reflective and partially transmissive. We harvested a dot-pattern diffuser from a conventional
LCD and rotated it rapidly with a small electrical motor to scan the
entire area, providing a complete image when observed directly as
demonstrated in Figure 7. Readers wishing to test OASIIS designs
without producing HOEs will find this as a viable path when bulk
and light loss are not major concerns. The significant backward
scattering component will create undesired images on the screen–
but much closer than the user’s eye can focus, creating in effect
a haze that reduces contrast but leaves the in-focus images easily
perceivable.
4.3 Polarization Selective Diffusers:
PSDs scatter light that is linearly polarized in one direction (dictated during manufacture), but pass light that is polarized in the
orthogonal direction. Extensive evaluation of the optical properties
of PSDs can be found in [Seo and Kim 2008]. In our system we
cause the PSD to diffuse the light from the light engine by placing a
polarizer immediately after the light engine. A quarter waveplate
applied on top of the PSD rotates the plane of polarization of the
light so that, when passing back through the PSD, the reflected
light is not diffused again. An anti-reflection coating applied to both
surfaces of the PSD reduces reflections from the environment. Unfortunately, adding extra layers to a PSD comes with a trade-off: the
multi-layered structure causes additional scattering. We measured
the ratio of haze-generating component Isb0
+ Is f 1
(see Figure 3)
and image-forming component Im (also Figure 3) in our PSD samples to be 8% when no coating is used and to be 14% with coatings.
PSDs typically require linearly polarized light from a light engine,
whereas in other approaches this is not the case.
4.4 Holography Setup:
Our best results for OASIIS screens come from our HOEs. We provide, here, the details of our holographic setup. A “white light” laser
beam is created by combining the light from a Coherent Genesis
MX460-500 SLM OPS Laser-Diode System, a Cobolt Samba diode
pumped solid state laser and a Cobolt Flamenco diode pumped
solid state laser operating at wavelengths of 532 − 460 − 660 nm
respectively. All three lasers have coherence lengths greater than
15 m. The holographic film used to record the HOEs was C-RT20
“Instant Hologram” Film (Litiholo). The beams were power balanced
by placing a half waveplate and a polarizing beam splitter in front of
each of the green and red lasers before combining the laser beams.
Excess power was shunted into beam dumps. Beam expansion in
Figure 5 was achieved using a Keplerian beam expander built with
conventional achromatic lenses from Thorlabs. The diffuser used
during the hologram writing process was a 250 × 250 mm 120Grit
Ground Glass Diffuser from Edmund Optics. Exposure times were
controlled by a mechanical shutter (SH1/M, Thorlabs). Following
exposure to the object and reference beams, “development” of the
holographic pattern was finalized by exposing the holographic films
to UV light (Dymax 2000-EC UV Light-Curing Flood Lamp) for
5 minutes. Once a holographic film is processed, it does not have to
go through the same long process, and it can now act as an OASIIS
screen. Note that a OASIIS screen is made to display dynamically
changing imagery.
5 EXPERIMENTAL ASSESSMENT
Here we evaluate our experimental prototypes to assess optical
qualities including FoV, eyebox, brightness, and varifocal range. We
provide our examples in the supplementary video.
Polarization Selective Di user Holographic Optical Element
Fig. 8. A pair of photographs showing image quality and visible amount
of haze while using (top left) a polarization selective diffuser with antireflection coating, and (top right) a holographic diffuser. Bottom row shows
magnified central regions of top row.
Cakmakci and Rolland [2006] define wide FoV for NED designs
as the ability to generate > 60o of monocular FoV. Depending on
the user’s facial structure, our prototypes provide a rotationally
symmetric monocular FoV of 55o − 63o
. Many standards exist for
reporting binocular FoV, including starting from a specific point
inside a person’s head (e.g., [Wearality 2015]) or starting from a
“cyclopean eye” between the user’s eyes (e.g., [Woods et al. 1993]).
Especially in the case of varifocal NEDs, the differing approaches
lead to widely varying estimates of the binocular FoV, and so we
report only the well-understood measure of monocular FoV.
Our two OASIIS screen types, PSDs and HOEs provide different
imaging characteristics as shown in Figure 8, in which brightness
levels supported by each screen and point spread functions (PSFs)
at various eccentricities are shown. We found HOEs to be more
effective in our use cases. Note that phenomena of speckle is an
artifact that can be observed for both types of OASIIS screens as can
be seen in magnified regions in Figure 8, which can be mitigated
through a dedicated light engine using moving diffusers in illumination mechanisms, broad-band illumination sources, or variation of
polarization of light [Shevlin 2012]. Phenomena of speckle is more
pronounced in HOEs, and this problem is traditionally tackled during recording process using moving diffusers [Gerritsen et al. 1968]
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
Near-Eye Varifocal Augmented Reality Display using See-Through Screens • 1:9
or using diffraction gratings [Utsugi and Yamaguchi 2013] during
cloning the already recorded HOE. Following process of [Yang et al.
2012] in recording, An HOE can also be tuned for best color. We
highlight that these are desired upgrades for our future work.
Fig. 9. A plot showing average Modulation Transfer Function (MTF) of
near eye display’s optical system at central field of view, while using a
holographic optical element as an OASIIS screen. Our measurements are
based on Slanted-Edge MTF methodology. We place a conventional desktop
(27-inch Acer XB270HU) display at each depth level, and measure MTF
of the display simultaneously using the same methodology to provide a
comparison and a validation. Input Center
15 mm to the left 15 mm above
Fig. 10. Examination of eyebox stability. Top left: an input image provided
to our light engine. Top right: A photograph taken from the center of our
prototype’s eyebox. Bottom left: A photograph taken from 15 mm to the left
with respect to center of our prototype’s eyebox. Bottom right: A photograph
taken from 15 mm above the center of our prototype’s eyebox.
We rely on off-the-shelf components so the keystone behavior
comes from both the light engine and the angle that the light engine’s projection axis makes with the normal vector of the HOE.
The variation in PSFs across the FoV is primarily due to the projection geometry in our prototypes. The lenses in front of the light
engine focus the light at a particular plane perpendicular to the light
engine’s projection axis. Since this plane is not parallel to the plane
of the OASIIS screen, the effective size of the PSF changes across the
FoV as the distance of the smallest spot size to the plane of the HOE
changes. As in Figure 9, we also provide a detailed analysis of Modulation Transfer Functions (MTFs) of our prototype using HOEs at
different depth levels. Our MTF measurements are based on a commonly accepted industry standard: ISO 122333 slanted-edge MTF
method [Burns 2000]. The bottom part of Figure 9 provides MTF
analysis of a conventional desktop display placed at highlighted
depth ranges, and this data is included as a validation point of our
analysis. The top part of Figure 9 shows the results of the same
analysis on our device. A camera is placed behind our NED to mimic
an eye position, and the camera is focused to a depth level, where
a conventional display stands. A slanted edge is displayed on the
conventional display at the position of central field of our NED,
and thus generating data for the bottom portion of the figure as
seen through our NED. Process is repeated by showing a slanted
edge on our NED, and during experiments, NED’s focus is always
matched the camera’s focus, which is at the conventional display
in all experiments. Thus, we generated data for the top portion of
the figure. Once the raw data is collected, a Gaussian blur is applied
with a small enough kernel to decrease the noise in the image. We
chose a region of interest at the FoV. We intentionally threshold the
line spread function at 10% and 90% of intensities to avoid ripples in
next step. Raw data is processed using the process in [Burns 2000],
which can be briefly summarized as taking a Fast Fourier Transform
(FFT) of a line spread function’s first derivative. Note that as the
conventional display gets closer to the camera, the angular extend
of a single pixel on the conventional display increases and MTF falls
from > 30 cpd to 12 cpd as expected. Our measurements indicate
that, with our NED prototypes, achieving 18 − 20 cpd is possible at
the central regions of our FoV. However, as there isn’t an available
off-the-shelf light engine equipped for the task, at most extreme eccentricities resolutions drops to the level of 5 cpd. We observed high
resolutions at different eccentricities, when our projection optics
tuned to be at sharp focus at those eccentricities. Thus, we would
like to highlight that resolution capabilities of our proposal are not
limited with the demonstrated prototype and highly depending on
the nature of a future projection optics. We will discuss further on
overcoming this limitation in our future work section. Contradicting with the work of Konrad et al. [2017], our MTF measurements
in Figure 9 suggest that supported resolution in cycles per degree
drops with increasing virtual image distance. The major factor for
this trend is due to large distance in between cornea and a spherical
beam combiner in our system, de +dsm ≈ 57 mm, whereas this value
is as small as 20 mm for [Konrad et al. 2017]. Effect of this distance
on expected MTF is explained in detail in our supplementary.
We demonstrate the stability of our prototype’s eyebox in Figure
10, which approximates a similar size as a consumer level conventional NED for VR. The light engines of our near eye display
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
1:10 • K. Akşit et al.
Near (4D) Mid (2D) Far (1D) Far (1D)
Scene
Input
Camera
Near eye display prototype
Near Mid
Far
Fig. 11. Examination of varifocal performance using our holographic OASIIS screens. Top: shows the input image of our prototype and a real world scene with
a near, a mid and a far target. As our curved beam combiner positioned by a linear motorized stage; the input image projected as the AR element. Middle
shows four columns of data with the complete monocular FoV while AR element is projected at the highlighted depth levels. On the bottom in each column,
insets show magnified images of the details of the targets at different depth levels and details of the AR element, which helps to interpret camera’s focus for a
given photograph.
prototype can provide up to 100 lumens while drive circuitries can
also be programmed to set the brightness levels to a lower value. By
Input Photograph from prototype
Fig. 12. Left: A rendered object provided as an AR element to our wearable
varifocal prototype, Right: A photograph showing how our input AR element
as seen through our wearable varifocal prototype.
projecting a wire-frame teapot model with 100 lumens, we evaluate
the brightness performance of our prototypes in outdoor environment as in Figure 1, and we show that it can run even under strong
sunlight.
In Figure 11, we demonstrate varifocal abilities of our design at
indoors with a brightness level of 20 lumens using HOEs. To test
for the feasibility, we translated the curved beam combiner back
and forth up to dd = 5 mm (see Figure 2) to cover a depth range
of 1 − 4 diopters. Using the same setup, as in Figure 12, we also
provide a sample photograph providing information on the quality
of a colored content.
Our display does not support occlusion of the real world by virtual
objects. However, the brightness available to our design does make it
viable the relative contrast mitigation strategy suggested by Rolland
and Fuchs [2000], even when the real world is bright.
An early experimentations with our display and gaze tracking
cameras from Pupil labs indicate that there are important items to
ACM Transactions on Graphics, Vol. 36, No. 6, Article 1. Publication date: November 2017.
Near-Eye Varifocal Augmented Reality Display using See-Through Screens • 1:11
consider for an efficient gaze tracking integration; positioning and
aiming of the camera with respect to eyebox has vital importance.
Also accuracy, latency, and robustness of gaze tracking methodology
plays an important role in defining operations or applications for
our prototypes. Expected idle location for placing cameras is directly
in front of the eye, however as this isn’t directly feasible for our
prototype without inserting a beam splitter, we placed cameras
below HOEs and this limits the quality of the capture for tracking.
The gaze tracking cameras that works under strong sunlight or
outdoors is also an important item that has to be tackled, this is
known to be a general problem for gaze tracker designers as bright
ambients can easily saturate cameras. All these open questions leave
challenges to gaze tracker manufacturers and researchers in the next
step of AR devices.
6 LIMITATIONS AND FUTURE WORK
While this design opens up new research opportunities, work remains to overcome practical challenges to producing a commercially
viable product. For both HOEs and PSDs, we observe slightly different brightness levels across the visual field as can be observed in
Figure 8. We can improve the HOE’s response to be more homogeneously distributed by improving the beam shaping optics of our
object beam.
Recording a HOE with a reference beam going through a replica
of targeted light engine optics would help resolve color issues across
all visual fields, as it truly mimics light engines behavior. Instead of
off-axis projection, using waveguides as done by Travis et al. [2013]
could also result in separate-waveguide systems at different distances to the eye, bringing the work of Lee et al. [2016] to a NED
use case. Of course, light levels emitted by the light engine can also
be adjusted in image space to even out perceived light levels. Finally, using a microlens array HOE [Yeom et al. 2014] as an OASIIS
screen, combined with the near-eye lightfield display of Lanman
and Luebke [2013], could extend our approach to AR lightfields.
Using a thicker holographic recording medium than used in this
study would increase light efficiency and allow the light engines
to use less power. This improvement comes with a design trade-off.
Thin holographic diffusers have a wide spectral response [Gu et al.
1996] which allows us to use our HOE diffusers with conventional
light engines, whereas thick HOEs provide a narrow wavelength
response and would require careful tuning of HOE to light engine
wavelength.
Image degradations across our display’s visual field and keystone
effect are both caused by off-axis projection, both items can be
corrected through a custom light engine design [Maimone et al.
2017] or possibly by a focus sweeping projection or display optics
design [Iwai et al. 2015; Konrad et al. 2017]. Linear actuators used in
our system can also be replaced with the design of Dunn et al. [2017]
as the form factor of the deformable mirror membranes shrink in
size in the near future.
While our prototype is not large by the standards of wide FoV
NED systems, with the form factor of ski goggles, it is still bulky
compared to corrective glasses. The design space shown in Figure 4
suggests some avenues for miniaturization. Work of Ren et al. [2016]
observed a significant increase in task based performance with increasing FoV, thus hinting for a need of even large FoV. We speculate that recording curved OASIIS screens similar to Guillaumee et
al. [2014] and using a more complex surface model for the combiner
may improve FoV. In the supplement to this paper we provide code
to generate design candidates for a free-form combiner that exceeds
80o
. In a commercial product, varying ambient light levels could be
detected with a photo-detector and appropriate light levels could
be displayed by a continuously varying neutral density filter (i.e.,
Thorlabs NDC-25C-2) or by adjusting the emissive power of the
light engine. Such additions may lead naturally to High Dynamic
Range [Lincoln et al. 2017; Seetzen et al. 2004] NED systems.
7 CONCLUSION
We have presented a novel family of designs for see-through neareye display devices. Our approach, dubbed OASIIS, uses an on-axis
see-through screen to form an intermediate image which is then
relayed back toward the eye by a concave mirror or a partiallytransparent beam combiner. The resulting system is simple to design
and construct, with robust and inexpensive components. The OASIIS
design advances several goals of augmented reality, including wide
field of view, large eyebox, unobstructed peripheral view beyond
the display, high image quality, no chromatic aberration, and realworld imagery undistorted by refractive or diffractive elements. Our
wearable prototypes resemble current virtual reality devices in size,
but we believe the design lends itself to a compact miniaturized
wearable device. Finally, slight deflections of the lightweight curved
beam combiners enable the display to rapidly change focal plane,
opening an intriguing avenue for future work on accommodationsupporting augmented reality display.