We present a technique to automatically animate a still portrait, making
it possible for the subject in the photo to come to life and express various
emotions. We use a driving video (of a different subject) and develop means
to transfer the expressiveness of the subject in the driving video to the target
portrait. In contrast to previous work that requires an input video of the
target face to reenact a facial performance, our technique uses only a single
target image. We animate the target image through 2D warps that imitate the
facial transformations in the driving video. As warps alone do not carry the
full expressiveness of the face, we add fine-scale dynamic details which are
commonly associated with facial expressions such as creases and wrinkles.
Furthermore, we hallucinate regions that are hidden in the input target face,
most notably in the inner mouth. Our technique gives rise to reactive profiles,
where people in still images can automatically interact with their viewers.
We demonstrate our technique operating on numerous still portraits from
the internet.
CCS Concepts: • Computing methodologies → Animation;
Additional Key Words and Phrases: face animation, facial reenactment
1 INTRODUCTION
Few objects convey as large a range of depth and meaning as the
human face. Facial expressions in humans convey not just major
emotions, but through subtle variations, a rather nuanced view into
the emotional state of a person, for example, a sad smile, blushing,
etc. (e.g., much has been said about the smile of Mona Lisa).
In this work, we are interested in animating faces in human
portraits, and in particular controlling their expressions. To avoid
crossing into the “uncanny valley”, previous facial animation techniques usually assume the availability of a video of the target face,
which exhibits variation in both pose and expression [Dale et al.
2011; Garrido et al. 2014; Thies et al. 2016]. An input video, or even
an image collection of the target face (e.g., [Cao et al. 2016]), allows
for an accurate 3D face reconstruction, over which face textures are
mapped and manipulated.
In contrast to previous work we use as input only a single image
of a target face to animate it. This makes our method more widely
We thank Peter Hedman, Noa Fish, Tal Hassner and Amit Bermano for their insightful
comments and suggestions. We also thank Ohad Fried, Justus Thies, Matthias Niessner,
Pablo Garrido, and Christian Theobalt for providing us with comparisons to their
techniques. This work is partially supported by the Israeli Science Foundation, research
program (1790/12 and 2366/16).
© 2017 Copyright held by the owner/author(s).
This is the author’s version of the work. It is posted here for your personal use. Not for
redistribution. The definitive Version of Record was published in ACM Transactions on
Graphics, https://doi.org/10.1145/3130800.3130818.
Fig. 1. Given a single image (top row), our method automatically generates
photo-realistic videos that express various emotions. We use driving videos
of a different subject and mimic the expressiveness of the subject in the
driving video. Representative frames from the videos are displayed above.
©Unsplash photographers Lauren Ferstl, Brooke Cagle, Guillaume Bolduc, Ilya Yakover,
Drew Graham and Ryan Holloway.
applicable, to the near endless supply of portrait or selfie images
on the internet. We animate the single target face image from a
driving video, allowing the target image to come alive and mimic
the expressiveness of the subject in the driving video. While most
previous work restrict themselves to only the face region, within
limits, our method animates the full head and upper body.
We animate the target image by a series of warps that imitate the
facial transformations in the driving video. Like in previous works
(e.g., [Fried et al. 2016; Leyvand et al. 2008; Yang et al. 2011]), we
manipulate the face by lightweight 2D warps. We are able to create
moderate head movement while maintaining the high realism of
the input 2D image without converting and projecting the image to
3D.
In our work, we establish a correspondence between the target
image and driving video frames by utilizing the effectiveness of
facial landmarks detection and tracking techniques, and expanding
these facial correspondences to span the entire image and over time.
As warps alone do not carry the full expression of the face, we add
fine-scale details such as wrinkles and creases that are commonly
associated with facial expressions, and hallucinate regions that are
hidden in the input target face, most notably in the inner mouth.
Figure 1 shows results of expressions which were transferred to a
single target image on the left. (The reader is encouraged to see the
videos included in the supplementary material).
ACM Transactions on Graphics, Vol. 36, No. 4, Article 196. Publication date: November 2017.
196:2 • Averbuch-Elor et al.
As our results illustrate, our technique enables bringing a still
portrait to life, making it seem as though the person is breathing,
smiling, frowning, or for that matter any other animation that one
wants to drive with. We apply our technique on highly varying facial
images, including internet selfies, old portraits and facial avatars.
Additionally, we demonstrate our results in the context of reactive
profiles – a novel application which resembles the moving portraits
from Harry Potter’s magical world, where people in photographs
move, wave, etc. By discovering portions of driving videos which
contain a number of different emotions, our technique provides the
means for the target image to react to stimulii. Our main contributions are:
• The ability to bring life to a single still portrait through 2D
warps and generate a video sequence which maintains the
realism of the input image.
• A method to continuously transfer fine-scale details including wrinkles and creases, while avoiding outlier wrinkles
that are caused by cast shadows or misalignment between
the warped video frames.
• A method to seamlessly transfer hidden regions, e.g., the
mouth interior, when necessary.
• A new reactive profile application, enabled from a single
input image.
2 RELATED WORK
Our method takes a single target image of a neutral face in frontal
pose and generates a video that expresses various emotions. Previous
works [Blanz and Vetter 1999; Breuer et al. 2008] addressed face
manipulation from a single image, but do not focus on generating
an animated video. Other works [Thies et al. 2016; Vlasic et al. 2005]
manipulate or reenact a facial performance, but these assume the
availability of a video of the target face. In what follows, we elaborate
on the most closely related works.
Most prior works require more than a single target image of a face
to automatically manipulate it. A video-to-image facial retargeting
application was previously introduced in Cao et al. [2014], but their
method is not automatic and requires some user interaction. Facial
editing using deep networks was introduced in Yeh et al. [2016]. The
method of Liu et al. [2001] enables transferring the fine-scale details
of one person’s changed expression to a neutral target image. In
our work, we extend their technique to accommodate more general
image pairs and a stable video output. Recently, Fried et al. [2016]
presented a technique to manipulate the camera viewpoint from a
single input image. Their method enables modifying the apparent
relative pose and the distance between the camera and the subject.
Other works, such as [Hassner et al. 2015], specifically address the
problem of face frontalization, as it is extremely beneficial for facial
recognition [Ding and Tao 2016].
In their seminal work, Blanz and Vetter [1999] fit a 3D morphable
model to a single input image, texture-map a face image onto a
3D mesh, and parametrically change its pose and appearance. In a
follow up work, Blanz et al. [2003] enabled animating a single image,
but focus on the mouth region. Later works, e.g. [Breuer et al. 2008],
extended the 3D morphable model technique to allow for an automatic reconstruction pipeline. However, as noted by Piotraschke
and Blanz [2016], when only a single image is provided, plausible
reconstructions often require a manual initialization. Furthermore,
the realism of the manipulated faces using these techniques in general is often lacking, as they cannot extract fine details since they
are not spanned by the principal components. To maintain the realism of the input image, one should avoid the projection of the
image onto a 3D model. This claim holds for faces as well as for
the whole body (e.g., [Zhou et al. 2010]). The commercial system
FaceApp 1
can generate a smiling still image from a neutral input
image. In the supplementary material, we demonstrate some output
stills generated by their system.
Facial manipulation techniques that require an input video of the
target face are by far more common. Vlasic et al. [2005] edit the 3D
mesh of the target face according to the expression parameters. Li
et al. [2012] utilize a facial performance database of the target face.
Dale et al. [2011] use a 3D morphable model for face reeanactment
using a source and target video. Garrido et al. [2014] present an
automatic method for face replacement in video, but unlike Dale
et al., they only replace the actor’s facial region, and keep the hair
and the rest of the head and the upper body of the source video.
The problem of face swapping was also introduced in the context
of an input target image, for example in [Korshunova et al. 2016].
Recently, Thies et al. [2016] presented a real-time facial reenactment
of a target video sequence. Unlike our animation of a single static
image, their work assumes that the target video contains rich and
sufficient data to synthesize a plausible reenactment. In our work,
we do not reenact the facial expression only, but also the respective
head motion. The technique of Kemelmacher et al. [2010] is also
related to ours, but more in the context of image retrieval. Other
video techniques, such as [Garrido et al. 2015], focus specifically on
transferring lip motion to an existing target video.
There are other works that address facial expression editing in
video, but do not explicitly follow another performer. Some examples include [Kuster et al. 2012] who aim at retargeting the gaze
of a streaming video and [Ganin et al. 2016] who manipulate the
gaze of a single image. Some works focus, for example, on synthesizing a realistic inside of the mouth (e.g., [Kawai et al. 2013, 2014]).
Bai et al. [2013] edit a facial video performance in an expressionpreserving manner, removing undesired large-scale motion. Yang et
al. [2012] magnify (or suppress) the provided expression in a target
video. In contrast, we create new photo-realistic expressions which
are significantly different from the input image. Recently, Masi et
al. [2016] addressed the problem of data augmentation by adding
expression variation in the form of a single local expression control
(closing the mouth). This expression manipulation helps in augmenting training data, without necessarily generating a plausible
realistic expression.
Some previous works address transferring expressions from a user
to a facial avatar (e.g., [Saragih et al. 2011]) or from one facial avatar
to another (e.g., [Chuang and Bregler 2005]). Although our method
enables animating non photo-realistic faces, the main challenge in
our work is to maintain a high degree of realism of a human face.
Unlike animating and manipulating the full body (e.g., [Hornung
et al. 2007; Zhou et al. 2010]), in facial animation, we, as humans,
1https://www.faceapp.com
ACM Transactions on Graphics, Vol. 36, No. 4, Article 196. Publication date: November 2017.
Bringing Portraits to Life • 196:3
Input Feature correspondence Animating a single target frame Output
Fig. 2. An overview of our method. Given an input target image and a driving video, we extract and track facial and non-facial features in the driving video
(colored in red and yellow, respectively) and compute correspondences to the target image. To generate the animated target frames, we perform a 2D warping
to generate a coarse target frame, followed by a transfer of hidden regions (i.e., the mouth interior) and fine-scale details. ©Unsplash photographer Atikh Bana.
are extremely sensitive to the finest nuances. To do so, we merely
utilize 2D tracked features in the driving videos, bypassing the need
for a precise tracking procedure such as the ones presented in Cao
et al. [2015] or Saito et al. [2016].
3 OVERVIEW
Our method uses as input a single image of a target face in neutralfrontal pose, as well as a video of a different face that drives the
animation of the target image. An overview of our method is illustrated in Figure 2. We animate the target image through a series
of 2D warps that imitate the facial transformations in the driving
video. These warps are controlled by a set of sparse correspondences
between the target face and the face in the driving video frames.
Many methods have been proposed for detecting fiducial points on
faces at fixed standard locations (eyes, mouth corners, etc., see red
points in Figure 2). However, to bring the full head of the target
image to life we need to allow the entire head to move and change
its pose. Thus, we track additional points outside the face region to
help guide the overall head.
Moreover, geometric warps alone do not encode the full range of
changes a face undergoes when the expression changes. In addition
there are many fine-scale changes, such as self-shadowing in wrinkles and creases that are necessary to convey the full expression of
the face in the driving video. Furthermore, by assuming a neutral
target face, we implicitly assume a closed mouth of the target face.
Therefore, the inner mouth of the target face is hidden, and thus
we need to hallucinate the appearance in this region if the mouth
opens in the driving video.
To meet these requirements, we develop the following technical
solutions:
Correspondence expansion. We utilize the high-fidelity of facial
landmarks detection and tracking, and expand the correspondences
in the facial region to correspondences that span the entire image
and over time. These additional landmarks are illustrated in yellow
in Figure 2. The augmented set of corresponding points allow tracking and changing the pose of the target head to follow and imitate
the one in the driving video (see Section 4).
Confidence-aware warping. We extrapolate the sparse set of correspondences to a dense vector field over the entire image. See the
illustration of the warped target at frame t. We distinguish between
the highly-confident facial region and the rest of the image, where
we have no guarantees on the quality or even the quantity of the
corresponding points, and smooth the vector field accordingly (see
Section 4).
Hidden region transfer. When needed (i.e., the mouth is open in the
driving video), we transfer the mouth interior to the animated target
frame. The composite retains as many details as possible from the
target image as only the mouth interior, and not the lips themselves,
are transferred to the animated frame. Note the hallucinated teeth
of the animated target face at frame t (see Section 5).
Detection of inlier wrinkles. To generate a realistic expression,
we transfer creases and wrinkles in the facial region. We detect
and avoid illumination changes that are caused by cast shadows or
misalignments between the warped video frames (see Section 6).
4 COARSE TARGET VIDEO SYNTHESIS
Our input consists of a target image, t
∗
, and a driving video S,
which contains a series of frames si
. Note that our notation assigns
lowercase letters to images and frames and uppercase letters to
ACM Transactions on Graphics, Vol. 36, No. 4, Article 196. Publication date: November 2017.
196:4 • Averbuch-Elor et al.
(a) Input target image (b) No smoothing (c) Constant smoothing (d) Our warped frame (e) Our final frame
Fig. 3. Confidence-aware warping. (a) The target image to be warped, with the triangulation mesh on top of it. (b) Without smoothing, discontinuities can be
observed in the non-facial region, e.g., in the region inside the yellow rectangle. (c) A constant blur kernel diminishes the animated facial expression. As can
be seen inside the blue rectangle, the mouth is no longer smiling. (d) Our confidence-aware blurring kernel keeps the facial details and smooths out the
discontinuities. (e) The animated frame on the right is our final result (after transferring hidden regions and fine-scale details). ©Unsplash photographer Jimmy Bay.
videos. Our goal is to synthesize a new video T , constructed from
frames ti
, that follows the motion of S, but maintains the identity
from t
∗
. We assume that t
∗
is an image of a neutral face and that
there is a frame, s
∗
, in S, with a neutral expression.
Each frame in the synthesized sequence ti ∈ T corresponds to a
frame in the driving video si ∈ S. To ease notations, we assume the
neutral frame of the driving video, s
∗
, to be the first frame (s0), but
this frame can be selected as well. Each frame ti ∈ T is generated
by warping t
∗
according to a sparse set of control points p
t
i
, which
mimic the offsets of the corresponding set of control points p
S
i
defined in the driving video.
Since the faces in t
∗
and s0 are not aligned, we first compute an
aligning transformation, ϕ, between t
∗
and s
∗
that compensates
for the misalignment. For each frame, ti ∈ T , t
∗
is warped by any
changes in the positions of the points in the driving video, (p
S
i
−p
S
0
)
after the transformation, ϕ, that aligns the neutral frames. Putting
it all together:
p
t
i = p
t
0
+ ϕ ·

p
S
i − p
S
0

(1)
The alignment and subsequent offsets are computed over a set of
control points. The control points that we use consist of two sets,
illustrated in Figure 2 in red and yellow points. The red points are
the fiducial points on the faces. Various methods have been proposed
for detecting fiducial points on faces. We use the implementation
available in [King 2009] which automatically detects and tracks 68
facial landmarks. This provides us with 68 corresponding landmarks
which are located in the chin, mouth, nose, eye and eyebrow regions.
The aligning transformation ϕ is defined once by estimating a
similarity transformation between the target image t
∗
and the neutral frame s
∗
. We estimate this transformation using least-squares by
approximating a rotation and scale between the landmarks located
in the eye regions and the tip of the nose.
To modify the entire image, and not just the facial region, some
correspondences outside the facial region are required. However,
there are no consistent features away from the facial region in both
the target and driving video. Thus, a robust correspondence technique cannot be reliably obtained. We therefore instead track points
in the driving video and hallucinate the corresponding locations of
points in the target image. To track points in the driving video, we
use a simple optical flow tracker [Bouguet 2001]. We then hallucinate the corresponding pixel locations in the target image using the
aligning transformation ϕ. This peripheral set of control points are
illustrated with yellow points in the Figure 2. The peripheral set also
includes points along the image boundary that do not move throughout the animation to help fix the background in place. The fiducial
points together with the peripheral points form the augmented set
of control points used to warp t
∗
at each frame.
To interpolate the offsets of the sparse set of control points to a
dense warp field for the entire image, we use a Delauney triangulation of the control points (see Figure 3(a)), which is computed on
s
∗
. Corresponding triangles on t
∗ define a simple piece-wise linear
interpolation, which works well inside the facial region, where the
points are relatively dense, but may cause noticeable discontinuities
outside the facial region.
To alleviate this issue, we smooth the dense warp field with respect to a confidence associated with each point. Inside the facial
region, where we have reliable correspondences, smoothing is unnecessary and may take away from the desired facial expression
(e.g., the warped mouth in Figure 3(c) is no longer smiling). Outside
the facial region, we smooth the warp field by convolving it with a
disk whose radius increases away from the facial region. For efficiency, we use 10 different blurring kernels with radii in the range

0, 0.05 · Sd iaд

, where Sd iaд is the size of the image diagonal. We
then use inverse mapping to calculate the origin of each pixel in the
new animated target frame. As can be observed in Figure 3(d) the
spatially-variant warp keeps the facial details, and at the same time
smooths out the discontinuities.
5 TRANSFERRING HIDDEN REGIONS
When the driving video’s subject opens its mouth, the interior is
revealed which does not exist in the target image. We thus transfer
this region from each driving frame to the target video frame.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 196. Publication date: November 2017.