Despite significant recent progress, dense, time-resolved imaging of complex,
non-stationary 3D flow velocities remains an elusive goal. In this work we
tackle this problem by extending an established 2D method, Particle Imaging
Velocimetry, to three dimensions by encoding depth into color. The encoding
is achieved by illuminating the flow volume with a continuum of light planes
(a “rainbow”), such that each depth corresponds to a specific wavelength of
light. A diffractive component in the camera optics ensures that all planes are
in focus simultaneously. With this setup, a single color camera is sufficient
for tracking 3D trajectories of particles by combining 2D spatial and 1D
color information.
For reconstruction, we derive an image formation model for recovering
stationary 3D particle positions. 3D velocity estimation is achieved with a
variant of 3D optical flow that accounts for both physical constraints as well
as the rainbow image formation model. We evaluate our method with both
simulations and an experimental prototype setup.
CCS Concepts: • Computing methodologies → Computer graphics; 3D
imaging; Motion capture;
Additional Key Words and Phrases: Fluid velocity imaging, Rainbow PIV,
Optimization
1 INTRODUCTION
Fluid capture is an active research area in computer graphics. Recent
works include efforts to image phenomena such as flames [Hasinoff and Kutulakos 2007; Ihrke and Magnor 2004], smoke [Gu et al.
2013; Hawkins et al. 2005], transparent hot air flows [Atcheson
et al. 2008], and fluid mixtures [Gregson et al. 2012]. While these
methods recover dense volumetric reconstructions, they only yield
independent scalar fields of the density of the respective phenomenon at each time step. To fully characterize the 3D flow and open
up applications beyond simple play-back, 3D velocity fields need
to be recovered as well. While there have been efforts to recover
velocities from the captured scalar fields through optical flow-style
approaches, these attempts have been limited by the relatively small
amount of high-frequency texture in the recovered data [Gregson
et al. 2014].
Fluid imaging has many significant applications in scientific and
engineering fields such as combustion research, design of airplanes
and underwater vehicles, and development of artificial heart valves.
Since 3D unsteady flows and turbulence are very common in such
domains, the main task of the fluid imaging is to allow probing
the fluid motions over a range of length scales. In other words, the
ultimate goal is to be able to obtain 3D dense measurements of the
three components of the velocity vector, known as 3D-3C.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
36:2 • J. Xiong, R. Idoughi, A.A. Aguirre-Pablo, A.B. Aljedaani, X. Dun, Q. Fu, S.T. Thoroddsen, and W. Heidrich
Over the last decades, different imaging techniques have been developed to get closer to this goal. Particle Imaging Velocimetry (PIV)
is the most commonly used of these techniques [Adrian and Westerweel 2011; Lourenco et al. 1989]. For PIV, small density-matched
tracer particles are inserted into the flow, and their advected motion
is tracked with image correlation methods, i.e. optical flow. In basic
2D PIV [Okamoto et al. 2000], this tracking is made possible by illuminating the volume with a light sheet perpendicular to the camera
line of sight (Fig. 2, left). Particles within that plane can be identified
easily, and tracked over time, so long as the flow does not move them
out of plane. This yields dense measurements of two components of
the velocity field on a two-dimensional slice of the volume (2D-2C).
Although 3D extensions such as holographic PIV [Hinsch 2002] or
tomographic PIV [Elsinga et al. 2006] exist, a dense reconstruction
of all three components of the velocity field over the full 3D volume
requires multiple cameras and remains elusive in practice (also see
Section 2). The densest volume measurements involve high-speed
imaging in combination with scanning laser-volumes [Casey et al.
2013].
This paper proposes a new approach, RainbowPIV, by combining
a suitable setup for color-based encoding of the third dimension in
volumetric PIV, as well as a powerful algorithm to retrieve both the
particle positions and the velocity vector field. For the hardware
part, a linear color filter is employed in order to obtain a continuous
wavelength-gradation pattern, i.e. a rainbow illumination. Then,
a diffractive optical element (DOE) is attached to the camera objective lens, in order to achieve a wavelength-selective focus that
coincides with the rainbow illumination planes (Fig. 2, right). With
this setup, particles with different wavelengths (different depths)
will be simultaneously in focus on the sensor plane.
The reconstruction algorithm utilizes a detailed image formation
model for this setup to retrieve the 3D location of particles in each
frame. From a sequence of successive frames, the velocity vector
field is reconstructed using an optical flow approach [Horn and
Schunck 1981; Meinhardt-Llopis et al. 2013], where physical constraints (incompressibility and temporal consistency) are introduced.
In order to improve the obtained results, we can iterate between
position and velocity estimation, effectively solving a joint optimization problem for both. The specific contributions of this paper
are:
• We propose a simple PIV setup (RainbowPIV) for measuring time-varying 3D-3C fluid velocity vector fields using a
single camera.
• We design a hybrid refractive-diffractive optical system in
order to focus all wavelength on the same sensor plane,
extending the depth-of-field while preserving high lateral
resolution.
• We formulate an image formation model for 3D particle distribution reconstruction, and apply optimization strategies
to tackle the ill-posed inverse problem.
• We introduce a physically constrained optical flow method
for recovering the fluid velocity fields, and evaluate its
effectiveness on synthetic data. Our approach allows having
a good estimation of velocity over the measurement volume
(high concentration of particles).
• We demonstrate our proposed hardware setup and algorithms on real fluid flows.
2 RELATED WORK
3D Fluid Imaging in Graphics has, as already mentioned, mostly
focused on independent scalar density fields for each time step. Examples for the physical properties recovered in this fashion include
the distribution of light emission in flames [Hasinoff and Kutulakos
2007; Ihrke and Magnor 2004], scattering density in smoke [Gu
et al. 2013; Hawkins et al. 2005], density of a fluorescent dye in
fluid mixtures [Gregson et al. 2012], as well as the refractive index
distribution in hot air plumes [Atcheson et al. 2008]. While this data
is sufficient for playback in graphics applications, other interesting
applications such as guided simulation [Wang et al. 2009], or flow
editing require velocity fields instead of just scalar densities. This
requires some form of velocity estimation or flow tracking, which
is difficult on this kind of input data (see below).
3D Particle Reconstruction is an alternative to imaging continuous densities, and is used by 3D variants of PIV. The task of the
particle reconstruction is to determine the 3D location of particles
from one or more camera views. The total number of cameras in
these settings is usually very limited due to space constraints, as well
as occlusions by solids, and is typically orders of magnitude lower
than the number of projections in x-ray tomography, for example.
Another practical issue is depth of field of the cameras, since the
whole volume needs to be in focus simultaneously, and the camera
aperture usually has to be large to collect enough light to capture
fast flows.
Some examples of 3D extensions of PIV include holographic
PIV [Hinsch 2002], which works with coherent light, and tomographic PIV [Elsinga et al. 2006; Schanz et al. 2016], which utilizes
typically 3-5 cameras. Both of these approaches are in practice very
hard to set up for many types of flow experiments.
More closely related to our approach are single-camera methods, with drastically simplified setup. Willert et al. [1992] used a
three-pin-hole mask to decode illuminated particles such that the
three-dimensional positions of each particle can be retrieved from
the image patterns on the observed image via a defocus analysis.
Since three dots would appear in the image for each particle, this
method is stuck with a low particle seeding density. Another group
of approaches made use of plenoptic cameras [Levoy et al. 2006; Ng
et al. 2005], which capture the full 4D light field. Particle positions
can be reconstructed using ray tracing based algorithms. The idea
of applying such technology for measuring volumetric particle distributions has been discussed by [Lynch et al. 2012]. However, due
to the existence of ghost particles originated from reconstruction
algorithm and reduced spatial resolution, it becomes difficult to
reveal particle locations with relatively high accuracy. This can also
be seen as limited angle tomography with a very narrow cone of
observations.
Instead of modifying the camera side, another class of volumetric
particle reconstruction approaches relies on modifying the illumination method, providing additional information on the relative
depth, as seen from the camera, by encoding it in color. For this purpose different illumination methods were used: prism [Kimura et al.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
Rainbow Particle Imaging Velocimetry • 36:3
Camera Camera
DOE in front of lens for
wavelength-selective focus
Linear filter
Laser
Light sheet
Cylindrical
lens
Collimator
White light
source
PIV setup Rainbow PIV setup
PIV image Rainbow PIV image
Rainbow of
narrow-band
light planes
DOE: multi-level
Fresnel phase plate
Fig. 2. Comparison between the measurement setups for standard PIV (left) and RainbowPIV (right). In regular PIV, a single plane of the volume is illuminated
by a light sheet, and a camera focusing on this plane is able to track particles moving within it. This yields 2 components of the velocity field on a 2D slice of
the volume. By comparison, in RainbowPIV, the 3D volume is illuminated by a continuum of planes, where the wavelength of the illumination varies linearly
with the depth. Particles in the volume can thus be modeled as narrow-band point sources, and a diffractive optical element attached to the camera lens
ensures that for each wavelength the camera is focused at the appropriate depth.
1991], laser [McGregor et al. 2007], color filter [Pick and Lehmann
2009], LCD projector [Watamura et al. 2013]. Herein, the locations
of the particles in the volume can be determined by their 2D spatial
positions and by the colors in the captured image using a mapping
between color and depth position. The primary advantage of this
setup compared to other 3D PIV methods is its simplicity. Nevertheless, the presence of random noise, optical aberrations and focus
issues, color contamination caused by secondary light scattering
from the particles, and color mixing for overlapping particles severely complicate the identification of the representative colors for
every possible particle in the observed image. McGregor et al. [2007]
used a method based on a calibration curve relating the hue of acquired images to the depth of particles within the imaged volume.
Watamura et al. [2013] proposed an algorithm to calculate particle’s
representative color by averaging hue values of the pixels where the
particle is projected on in polar coordinate. Even though it revealed
promising results for Particle Tracking Velocimetry (PTV), where
low density particles are seeded, it will fail for our task of measuring a dense set of velocity fields, where sufficiently high density of
particles is required.
Our method exploits a similar idea of encoding particle depth
by color, however we employ a combination of coded illumination
and modified camera optics to solve many of the issues of existing
methods. Moreover, we develop a new optimization-based joint
reconstruction of both particle position and velocity field.
Velocity Estimation from particle fields has been elaborately studied not only in the field of fluid mechanics, but also in the computer
vision community. Literature from the fluid mechanics field mainly
adopts correlation-based algorithms [Prasad 2000] for global velocity measurement, which computes the spatial auto-correlation or
cross-correlation of successive images, extracting average motion at
every single interrogation spot. Though significant improvements
have been made on correlation methods [Stanislas et al. 2008], they
still have issues in areas of low particle density, which is common
in 3D measurements.
In a seminal result from computer vision, Horn and Schunck [1981],
proposed a global variational optical flow method based on the assumption of brightness constancy and smoothness in the flow vector.
The connection between optical flow and fluid flow was investigated
by Liu and Shen [2008], which revealed that under certain conditions
(mass conservation, inviscid), brightness constancy is equivalent
to scalar transport equation for fluid flow. This connection lends a
physical meaning to optical flow approaches for fluid tracking. Heitz
et al. [2010] gave an overview to the applications of optical flow
based fluid motion estimation. The estimation accuracy between
optical flow and correlation approaches applied to PIV system has
been numerically evaluated [Liu et al. 2015].
Since the optical flow problem is physically connected to the continuity equation in fluid dynamics, it becomes feasible to introduce
Navier-Stokes equations, which govern real-world fluid motions,
as additional physical priors into the conventional Horn-Schunck
algorithm. Some previous literature has taken divergence-free constraints into account [Herlin et al. 2012; Ruhnau et al. 2007; Yuan
et al. 2007], while most of them suffer from the complexity of solving
higher order regularization terms. Gregson et al. [2014] simplify
this issue by connecting the pressure projection method with the
proximal operator, allowing it to be easily handled by a convex
optimization framework. Ruhnau et al. [2007] and Heitz et al. [2008]
also consider the equation for time evolution of fluid flow, imposing
temporal consistency. In our work, we adopt ideas from fluid simulation [Fedkiw et al. 2001; Foster and Metaxas 1997; Stam 1999],
which approximately solve the time-evolution of fluid flow. This
enables us to integrate the temporal coherence regularization terms
into the optical flow model, which can then be solved by a modular
optimization framework.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
36:4 • J. Xiong, R. Idoughi, A.A. Aguirre-Pablo, A.B. Aljedaani, X. Dun, Q. Fu, S.T. Thoroddsen, and W. Heidrich
3 RAINBOW PARTICLE IMAGING VELOCIMETRY
The RainbowPIV method consists of two components: a new optical
setup that encodes particle depth into a color image with a large
depth of field, and a matching new reconstruction algorithm that
jointly optimizes particle position and velocity field.
Optical Setup. A comparison between the setups for regular PIV
and RainbowPIV is shown in Figure 2. In regular PIV (left), a laser is
used to create a light sheet perpendicular to the camera line of sight.
The camera is focused on the illuminated plane, and can therefore
observe and track particles moving within the sheet of light. This
yields 2 components of the velocity field on a 2D slice of the volume
(2D-2C). In RainbowPIV, the illumination is provided by a white
light source that is collimated and filtered so that the wavelength
varies linearly with the depth within the flow volume. In this setup,
particles submersed in the fluid can be modeled as narrow-band
point lights, whose wavelength varies linearly with depth.
The second part of the optical setup is a diffractive optical element
in the form of a Fresnel phase plate, which is implemented as a thin
glass plate with a micro-scale height field structure etched into it.
This DOE provides a wavelength-selective focus in the camera optics.
Specifically, the optical system is designed such that the camera
focus for each wavelength corresponds to the depth at which that
wavelength occurs in the rainbow illumination. This design achieves
all-in-focus imaging of the particles in the interrogation volume.
Reconstruction. The reconstruction task is to estimate particle
positions from the the observed color, and then track these particles
over time to obtain a 3D velocity field to get a full 3D, 3 component
(3D-3C) measurement. This task is made more complicated by the
fact that the camera captures only RGB information, not a full
hyperspectral image, which makes the position reconstruction less
robust. To tackle this problem, we employ an iterative approach: an
initial position estimate for each time step can be used to obtain
a first estimate of the time-dependent velocity field. This velocity
field can be used to refine the position estimate by adding physical
priors that tie together all the time steps. These two iterative steps
are described in detail below. For tomoPIV a joint reconstruction of
volume and velocity was already used by [Barbu et al. 2013].
3.1 Particle Position Estimation
An inverse problem is proposed for recovering particle locations
in 3D spatial domain. We start by introducing an image formation
model that relates the obtained particle positions to the observed
image. Three regularization terms are then added to formulate an optimization problem, which can be efficiently solved with guaranteed
convergence, tackling our ill-posed inverse problem.
Image Formation Model. As mentioned above, the illumination
in the volume is designed to consist of a continuum of light sheets
(Figure 2) with a narrow-band spectrum, whose wavelength (denoted as λ) varies with depth (z coordinate). In this work, we restrict
ourselves to a linear relationship between z and λ since this setting
is easily implemented with off-the-shelf components (see Section 4).
Therefore, the location of particles in the volume can be geometrically represented as the position of the light plane, specified by
wavelength, and pixel positions in that light plane (x, λ) = (x,y, λ).
The presence of a particle at a specific point in the volume is modeled
as an occupancy probability P (x, λ).
Since we are operating with incoherent light, the imaging process of the optical system can be modeled as a set of point spread
functions (PSF), one for each color channel: дC (x, λ), where C ∈
{red, green, blue}. With these definitions, the image formation model
is
iC (x) =
Z
Λ
Z
X
дC (x − x
′
, λ) · ir (x, λ) · P (x, λ) dx
′
dλ, (1)
where iC (x) are the color channels of the captured RGB image,
and ir (x, λ) is the corresponding spectral distribution incident on
the image sensor. The spatial integral corresponds to a convolution
representing potentially imperfect focus, while the wavelength integral represents the conversion from a spectral image to an RGB
image encoding 3D particle positions.
Optimization Problem. After discretization, we can formulate
the convolution of PSFs and reflected light intensity as a matrix
A ∈ R
3N ×N L, where N is the number of image pixels, L is the
number of discretization levels along the wavelength coordinate,
and the value of 3 refers to three color channels. Moreover, it ∈ R
3N
represents the observed image at time t, and pt ∈ [0, 1]N L is the
volume of occupancy probabilities at time t. The distribution of
particles at each time step of a video can be retrieved by solving the
linear system
Apt = it
. (2)
However, this inverse problem is ill-posed as we have compressed
the full spectral information encoding the particle position into just
three color channels. To handle this ill-conditioned inverse problem,
some prior knowledge of the distribution of particles is introduced as
regularization terms, resulting the following minimization problem:
(p
∗
) = argmin
p
1
2












A






p1 |...|pT






−






i1 |...|iT


















2
2
(3)
+ κ1



diaд(w) (p1; ...; pT )



1 + Π[0,1] (p1; ...; pT )
+ κ2
X
T
t=1
Z
Ω
pt ⊙ (pt − pt+1 (ut
, −∆t))
◦2
dΩ,
where ⊙ and (·)
◦2
respectively refer to the operators for the Hadamard
(i.e. component-wise) product and square operator. The operator
Π[0,1] projects all volume occupancy probabilities onto the convex
set of valid probabilities [0, 1]N L
.
The first line in Equation 3 is a least-square data fitting term
corresponding to Equation 2. The second line defines a weighted L1
term that encourages sparse distributions of particles in the volume,
and the indicator function enforces that occupancy probabilities are
between zero and one. Finally, the term of the third line provides
temporal coherence by mandating that occupancy probabilities of
successive time frames are consistent with advection under a previously estimated flow field ut = (ut
,vt
,wt
) by −∆t units of time,
expressed as pt+1 (ut
, −∆t). We call this term the particle motion
consistency term, and it allows for refining position estimates once
a velocity field has been estimated, and ties the reconstruction of
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
Rainbow Particle Imaging Velocimetry • 36:5
all frames together into a single optimization problem. The particle
motion consistency term is discussed in more detail below.
The above optimization problem is non-smooth because of the
L1 term and the indicator function, hence it cannot be solved by
general optimization tools such as gradient descent. The strategy
tackling this kind of issue is to decouple non-smooth terms from the
original optimization problem, such that distinct parts can be handled separately. We apply this strategy using the ADMM framework
which is systematically discussed in [Boyd et al. 2011].
Algorithm 1 ADMM Framework of Computing Particle Distribution
1: procedure ComputeParticleLocation(F1,H1)
2: for from 1 to maximum ADMM iteration do
3: // p-minimization step
4: p
j+1 ← proxσ1F1
(z
j − q
j
)
5: // z-minimization step
6: z
j+1 ← proxτ1H1
(p
j+1 + q
j
)
7: // scaled dual variables update
8: q
j+1 ← q
j + p
j+1 − z
j+1
9: end for
10: end procedure
The pseudo code for solving Equation 3 using ADMM is shown
in Algorithm 1, where j is the iteration number, z is a slack variable,
q is a dual variable, or Lagrange multiplier. proxσ1F1
and proxτ1H1
are proximal operators [Parikh et al. 2014] based on F1 and H1
respectively, and we provide their derivations in the Appendix. F1
and H1 are defined as:
F1 (p) =
1
2












A






p1 |...|pT






−






i1 |...|iT


















2
2
(4)
+ κ2
X
T
t=1
Z
Ω
pt ⊙ (pt − pt+1 (ut
, −∆t))
◦2
dΩ
H1 (p) = κ1



diaд(w) (p1; ...; pT )



1 + Π[0,1] (p1; ...; pT ) (5)
Particle Sparsity. The L1 penalized term ensures a sparse distribution of particles in the volume. It is further weighted by a diagonal
matrix diaд(w). Unlike the algorithm proposed in [Candes et al.
2008], which iteratively changes the weight coefficients based on
previous results for enhancing sparsity, weights in our approach
are fixed during iterations, but vary with particle depth. The motivation for this process is to compensate for different sensitivities of
the camera to different wavelengths. For example, wavelengths in
the yellow or in the blue-green part of the spectrum elicit a strong
response in two or even three color channels, while wavelengths in
the far blue or far red parts only trigger one channel. This can result
in a non-uniform particle distribution, where particles are more
likely to be placed at certain preferred depths. The weighting term
allows us to eliminate this bias by compensating for the photometric
non-uniformity.
Particle Motion Consistency. As mentioned, particle motion consistency ensures that estimated particle locations in successive
frames are consistent with advection through a previously estimated flow field. This turns the position estimation from a set of
independent problems, one for each time step, to a single joint estimation problem for the whole sequence. This term can be improved
by adding a mask to suppress the impact of low confidence flow
estimates.
3.2 Velocity Field Reconstruction
This section describes how we estimate the fluid flow vectors from
reconstructed 3D particle distributions in a video frame. First, we
introduce the physical properties of fluid flow formulated in NavierStokes equations, and then an optimization problem is constructed
by combining conventional optical flow with those physical constraints.
Divergence Free. An incompressible flow can be described as a
solenoidal flow vector field usol, which is divergence free:
∇ · usol = 0. (6)
Based on the Helmholtz decomposition, any arbitrary vector field u
(in our case an intermediate flow vector that does not necessarily
satisfy the divergence-free constraints) can be decomposed into a
solenoidal (divergence-free) part and an irrotational (curl-free) part.
The irrotational flow vector is the gradient of some scalar function (pressure P in our case), hence we can express the Helmholtz
decomposition as
u = usol + ∇P/ρ, (7)
where ρ defines density. Taking the divergence of both sides, we
obtain
∇ · u = ∇
2P/ρ (since ∇ · usol = 0). (8)
With the intermediate vector field u, the scalar function P can be
computed by solving the above Poisson equation, and then the
solenoidal flow vector field can be simply retrieved as
usol = u − ∇P/ρ. (9)
Equations 8 and 9 represent a pressure projection ΠCD IV operation
that projects an arbitrary flow field onto the space of divergence-free
flows CD IV , and is widely used in fluid simulation. Mathematically,
this step corresponds to an operator splitting method [Gregson et al.
2014].
Temporal Coherence. The incompressible Navier-Stokes equation
describes the time evolution of fluid velocity vector fields, given by:
∂u
∂t
+ (u · ∇)u = −∇P/ρ + (∇ · τ¯¯)/ρ + f , (10)
where P is the pressure, τ¯¯ is deviatoric stress and f is an external force. For a non-viscous fluid in absence of external force and
ignoring the unknown pressure gradient term, Equation 10 becomes
∂u
∂t
+ (u · ∇)u = 0, (11)
which refers to an approximated evolution of fluid velocity over
time. On the basis of this equation, we can advect the fluid velocity
at the current time step by itself, and then project it onto a space
of divergence-free flows to generate an estimate of the subsequent
velocity field, and vice versa. This time evolution equation will be
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
36:6 • J. Xiong, R. Idoughi, A.A. Aguirre-Pablo, A.B. Aljedaani, X. Dun, Q. Fu, S.T. Thoroddsen, and W. Heidrich
introduced into the optimization problem discussed in the following
as a soft constraint.
Optimization Problem. We aim to reconstruct the fluid flow velocity vector fields based on a physically constrained optical flow
model. The extended optical flow model is formulated as:
u
∗
t = argmin
ut
Z
Ω
pt ⊙ (pt − pt+1 (ut
, −∆t))
◦2
dΩ + κ3 ∥∇ut ∥
2
2
+ κ4







M(ut − ΠCD IV (ut−1 (ut−1, ∆t)))





2
2
+






M(ut − ΠCD IV (ut+1 (ut
, −∆t)))





2
2

+ ΠCD IV (ut
),
(12)
each line of which is explained hereafter:
• the first line describes the conventional Horn-Schunck optical flow model except that the brightness constancy constraint is replaced with the masked particle motion consistency as discussed in Section 3.1.
• the second and third lines describe the temporal coherence regularization as explained above: the fluid velocity
at the current time step is approximated by either forward
warping the flow vector at the previous time step by itself,
followed by a projection operation, or by backward warping the flow vector at the next time step by the current
flow, followed again by a projection operation. The binary
mask M is employed to ensure confidence-based weighting,
giving 0 for the flow vectors near the boundary and 1 for
vectors in the central region.
• the fourth line represents an indicator function of the projection method introduced above. Gregson et al. [2014]
found that the projection operation is equivalent to the
proximal operator for the space of divergence-free velocity
field. This allows us to integrate the divergence-free constraint into the original optical flow model, which can still
be efficiently solved by well-known optimization frameworks.
We formulate this optimization problem in the ADMM framework
in Algorithm 2, where the definitions of the functions F2 and H2 are
given below. The corresponding proximal operators can be found
in the Appendix.
F2 (ut
) =
Z
Ω
pt ⊙ (pt − pt+1 (ut
, −∆t))
◦2
dΩ + κ3 ∥∇ut ∥
2
2
+ κ4







M(ut − ΠCD IV (ut−1 (ut−1, ∆t)))





2
2
+






M(ut − ΠCD IV (ut+1 (ut
, −∆t)))





2
2

(13)
H2 (ut
) = ΠCD IV (ut
) (14)
In addition, a coarse-to-fine strategy is applied to deal with large
displacements. The algorithm begins from the coarsest level, and
an initial guess of optical flow at the next finer level is obtained
by scaling up the flow computed in the coarser level. It should be
noted that in this case, the above optimization problem becomes
non-linear in ut on account of the warping term pt+1 (ut
, −∆t). To
Algorithm 2 ADMM Framework of Computing Fluid Velocity Vector Fields
1: procedure ComputeVelocity(F2,H2)
2: for from 1 to ADMM iterations do
3: // u-minimization step
4: u
j+1
t ← proxσ2F2
(z
j − q
j
)
5: // z-minimization step
6: z
j+1 ← proxτ2H2
(u
j+1
t
+ q
j
)
7: // scaled dual variables update
8: q
j+1 ← q + u
j+1
t
− z
j+1
9: end for
10: end procedure
tackle this issue, the non-linear term is linearized by using first order
Taylor expansion and ut
is updated iteratively based on fixed-point
theorem. More detailed descriptions about this approach are given
in [Meinhardt-Llopis et al. 2013].
For a sequence of fluid velocity vector fields, each of them is
solved independently in an iterative loop. The update of the flow at
one time step will impact the subsequent flows in current iteration,
and also the previous flows in the following iterations.
4 RESULTS AND DISCUSSION
4.1 Experimental setup
Figure 3 represents a picture of the experimental configuration used
to evaluate the performance of the RainbowPIV algorithm.
Light source
DOE
Lens
Linear filter
Tank
Collimator
x
z
Fig. 3. Illustration of the experimental setup. A combination of a white light
source, a collimator and a linear bandpass filter yields a parallel rainbow
beam. After reflection on the particles present in the tank, the light is
acquired by a camera. A hybrid refractive-diffractive lens (lens+DOE) is
used to ensure that all particles of the measurement volume are focused on
the same sensor plane.
Rainbow light generation. The experiments were performed using a high power plasma light source combined with a liquid light
guide (HPLS245, Thorlabs) to generate white light (output spectrum:
[390, 730 nm]). A collimator was added to obtain a parallel light
beam. It is important to have a parallel light beam, to guarantee
that two particles having the same depth will be illuminated by the
same colored light.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
Rainbow Particle Imaging Velocimetry • 36:7
To split the white light in a rainbow beam, we employed a continuously linearly varying bandpass filter (LF103245, Delta Optical
Thin Film). Other components (prism, blaze grating) were also considered for their ability to generate a rainbow beam. However after
comparison, the linear filter appeared to us as the best solution for
its effectiveness and simplicity. The generated beam encompasses
a spectral range from 480 nm to 680 nm, and corresponding to a
depth range of 18 mm in the z direction. Given the height of the
beam and the length of the used tank, the two other dimensions of
the measurement volume are 50.1 mm along the x axis and 25.6 mm
along the y axis.
Acquisition device. To record the particle images, a digital camera was used (RED SCARLET-X DSMC, sensor: MYSTERIUM-X
[30 mm × 15 mm], 4096 × 2160 pixels). A lens with a focal length of
50 mm was mounted on the camera. As can be seen in Figure 5 (b,
c, d), when a standard refractive lens is used alone, the depth of
field is very shallow, and only a small depth range can be in focus.
For these three cases the other wavelengths are out of focus, which
makes it impossible to exploit these images to retrieve the velocity
of particles.
A DOE (see Figure 4) was designed in order to overcome this
limitation. Specifically, the DOE is a Fresnel phase plate, which is
implemented as a height field with 16 discrete levels as discussed
in other recent works, e.g. [Heide et al. 2016; Peng et al. 2015]. The
design parameters for the hybrid refractive/diffractive camera lens
are summarized in the following table (please refer to the Appendix
for a description on how to derive these parameters):
Symbol Description Value
DDO E DOE diameter 16 mm
f
DO E
λ0
DOE focal length for λ0 = 563 nm 401.8 mm
γ Magnification 2.065
F # Aperture 4.125
L
′ Distance hybrid lens - sensor 66 mm
L2 Distance hybrid lens - volume 127.3 mm
3 mm
Fig. 4. Left: The designed DOE fixed on a support, and destined to be
mounted on the refractive lens. Right: A microscopic view of the designed
DOE.
The image of particles acquired using the hybrid lens is presented
in Figure 5 (a). One can notice that for this case all particles within
the measurement volume are in focus. Their size on the image is
almost the same, contrary to the defocused images obtained without
using the DOE.
a b
c d
1 mm
1 mm
1 mm
1 mm
Fig. 5. Comparison of subsections of the images acquired using a hybrid
lens and a refractive lens. (a) Image obtained with the hybrid lens (DOE
+ lens). (b, c, d) Images obtained when using only the refractive lens. The
focus is adjusted respectively for blue (b), green (c) and red (d) particles.
Measured flows. Two types of experiments were realized using
transparent, rectangular tanks made of glass plates placed on a brass
metal support:
• Experiments with a ground truth were performed using
a high viscosity transparent fluid (PSF-1,000,000 cSt Pure
Silicone Fluid). Its viscosity is one million times higher
than that of water. White particles (White Polyethylene
Microspheres, with a diameter in the range [90, 106 µm])
were introduced to this liquid. This involved heating the
liquid while stirring in the particles, followed by vacuum
treatment to eliminate bubbles. After cooling the liquid, the
particles become frozen in place. Then, experiments were
conducted by applying a known movement (translations or
rotation) to the tank using micro-meter stages. Therefore,
the particle motion is known, since they are immobile with
respect to the tank.
• Experiment without "ground truth" were realized using
the same particles, after introducing them in a tank containing tap water. A small amount of surfactant (Polysorbate
80 Water) is added in order to reduce the surface tension of
water. This is to avoid the agglomeration of particles in the
tank. In this case, the particle motion is generated manually
through stirring, pouring, and similar excitations.
4.2 Velocity Vector Field Reconstruction Results
In this section, we first evaluate our proposed approaches based on
synthetic examples for ground truth comparisons. Then, we conduct
two types of experiments, where the first one is to move particles
with known motion vector, verifying the accuracy of our methods
on real data, the second one is to work on practical fluids.
Synthetic simulations. To quantitatively assess our reconstruction method, we tested our algorithm on simulated data. A volume
with the size of 100 × 100 × 20 (X × Y × Z) was simulated and
we randomly generated 1000 particles in the volume. The particles
were advected over time by ground truth flow vectors that were
generated using the method of Stam [1999], such that we can obtain
time evolved particle distributions from a forward simulation that
is completely decoupled from our implementation. Using the image
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
36:8 • J. Xiong, R. Idoughi, A.A. Aguirre-Pablo, A.B. Aljedaani, X. Dun, Q. Fu, S.T. Thoroddsen, and W. Heidrich
formation model from Equation 1, we simulated a time sequence of
5 captured images.
We compare our proposed velocity vector reconstruction algorithm, referred to "S-T div-H&S", with the general multi-scale HornSchunck algorithm "H&S" [Meinhardt-Llopis et al. 2013] and its
extension by introducing divergence free constraint as a proximal
operator "div-H&S" [Gregson et al. 2014]. Note that the last two
approaches compute the motion between one pair of frames independently, while our approach works on a sequence of frames
simultaneously.
Time frame Time frame
Angular Error (RMS)
End Point Error (RMS)
Degree Pixel
Fig. 6. Numerical comparisons with ground truth data for different algorithms. Left: Average angular error (in degrees). Right: Average end-point
error (in pixels).
For evaluation, we use two metrics known from the optical flow
literature: the average end-point error, i.e. the average Euclidean
distance of the true and estimated particle positions after advection,
and the average angular error, i.e. the average discrepancy in flow
direction. In Figure 6 we show how both types of error accumulate
over multiple frames, which is a good indicator for the accuracy of
path lines generated through forward integration in the estimated
velocity fields. As expected, the reconstruction errors increase over
time in all methods. However, by considering temporal coherence,
our proposed method exhibits better performance compared to the
other two approaches. We point out that a temporal smoothness
regularizer may not necessarily result in improved reconstruction
results at each particular time step, however, it conveys better estimations in the temporal domain. This is essential for video frames
captured in real-world experiments.
We also ran experiments on simple analytical flows, specifically
one vortex with the rotation axis aligned with the optical axis, and
one with the rotation axis orthogonal to the optical axis. The results
(shown Figure 7) are consistent with the above results on more
complex simulated flows. In the first case, the mean of the average
end-point error for a sequence of 5 frames are 0.54, 0.52 and 0.49 in
pixels, and the mean of the average angular error are 8.06, 7.77 and
7.08 in degrees respectively for "H&S", "div-H&S" and "S-T div-H&S"
approaches. As for the latter one, the mean of the average end-point
error are 0.79, 0.77 and 0.73 in pixels, and the mean of the average
angular error are 17.15, 16.24 and 13.65 in degrees. These results
verify that the temporal smoothness term truly boosts the overall
reconstruction results for a sequence of frames. Moreover, we could
observe a better estimation for flows in the longitudinal plane than
those in the transverse plane. This will be further discussed in the
following section.
Norm of Velocity (Pixel/time unit)
Y [pixel] Y [pixel]
Fig. 7. Ground truth (left) and reconstructed (right) results for simple analytical flows. Top: Rotation around axis aligned with the optical axis. Bottom:
Rotation around axis orthogonal to optical axis.
Fig. 8. Calibrated PSFs for different layers along depth direction. From near
camera side to far-end of the camera.
Experiments with a ground truth. To evaluate the effectiveness
of our proposed methods on real captured data, we firstly conduct
the experiments with a tank containing seeded particles in high
viscosity liquid. The tank is put on a multi-dimensional translation/rotation stage such that reconstruction results of the algorithm
can be compared with ground truth movements. Three independent
tests are performed:
(1) Translation in the x direction (i.e. perpendicular to camera
line of sight): 5 frames were acquired. Between each two
successive frames, a translation of 0.2mm in the x direction
was applied.
(2) Translation in the z direction (i.e. along the camera line of
sight): 5 frames were acquired. Between each two successive frames, a translation of 0.5 mm in the z direction was
applied. In this case, the translation is larger, in order to
observe easily the color change.
(3) An approximation of rotation around the vertical (y) axis in
a clockwise direction. With our setup of “frozen” particles in
a volume, only an approximation of this rotational motion
is possible, since it is not possible to tilt the tank relative
to the camera line of sight to avoid distorting the volume
by refraction. We therefore approximate rotational flow
by rotating the rainbow illumination pattern relative to
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
Rainbow Particle Imaging Velocimetry • 36:9 Norm of Velocity (Pixel/time unit)
Z [pixel]
Y [pixel]
x
y
z
y
x z Y [pixel]
Norm of Velocity (Pixel/time unit)
Y [pixel]
Norm of Velocity (Pixel/time unit)
y
x
z
Fig. 9. The reconstructed velocity vector fields induced by moving the measurement volume with a rotation stage. Top: Translation along x direction.
Middle: Translation along z direction (towards the camera). Bottom: Rotate
along y axis in clockwise direction. The magnitude of the vectors are coded
by color.
the tank. In practice, the tank and the camera are mounted
together on a rotation table with fixed relative position, and
the lighting setup is fixed. The rotations were performed
from an angle of −8
◦
to 8
◦
(the reference is defined when
the tank is aligned with the (x,z) directions). Between each
two successive frames, a rotation of an angle equal to 4
◦
was applied.
Before processing the captured images, we first pass them through
a Gaussian filter and then downsample them by a factor of 8, hence
the resolution for the downsampled image is about 100 µm/pixel,
approximately one particle per image pixel. We discretize the wavelength coordinate into 20 levels, corresponding to 900 µm/layer. The
calibrated point spread functions for each levels are shown in Figure 8. It should be noted that the resolution along the wavelength
coordinate is about 9 times coarser than that in x − y plane.
The reconstructed velocity vector fields are visualized in Figure 9. The overall structures of the reconstructed flow in all three
cases reveal that a significant part of the real flow structures are
reproduced.
Furthermore, we can numerically analyze the reconstructed results with respect to the ground truth movements. In the experiments, the x-axis and the z-axis translations move respectively
200 µm and 500 µm in one time step, which corresponds in the captured images to 2 and 5 pixels. In the rotation test, the total rotation
π
45 rad, and the 2D plane of the test section has the physical size of
10mm×18mm (x×z), and distance from the center of the test section
to the center of the disk is 10 mm, hence the practical magnitudes
of the displacements are about 334 µm (3.3 pixel sizes) for the part
at the near-end of the disk center and 506 µm (5.1 pixel sizes) for
the part at the far-end of the disk center. The computed magnitudes
of the flow vectors are encoded by color in our represented results.
The mean of the norm of the velocity in the left translational
experiment is 1.75 pixel sizes with standard deviation of 0.15, while
the mean of that in the experiment of translating towards camera
is 3.48 pixel sizes with standard deviation of 0.79. We can see that
reconstructed flow vectors reveal higher accuracy for the flow perpendicular to the optical axis with respect to the flow in longitudinal
direction. This is reasonable since: (1) depth resolution is highly
limited compared to lateral resolution as camera is much more sensitive to the spatial change of objects in 2D plane than the change
of wavelength, which results in coarser reconstructed flow vectors
along the wavelength coordinate. (2) the error may also come from
a bias of reconstructed particle distributions. Determination of the
spatial positions of the particles along z axis involves higher uncertainties. Moreover, distortion caused by the refractive effect of the
applied high viscosity materials, arises when moving the tank along
the z axis. As the thickness of the material between camera and
illuminated particles changes, the PSFs are altered simultaneously.
Fortunately, this issue does not exist when measuring practical fluid
flow, where the particles move, instead of the light beam. Though
facing the fact of relatively low reconstruction accuracy for flow
in axial direction, not only flow in simple translational structures,
but also vortical flows are reasonably reproduced, and the error in
wavelength axis is within a certain tolerance, which, in general, is
no more than half of length of the discretization intervals.
Experiments without ground truth. Finally, we test our RainbowPIV system on four different real flows of varying complexity
(Figures 10–13). Using the setup described in Section 4.1, we captured image sequences of fluids at a frame rate of 30Hz, and downsampled the images by a factor of 8 from an original resolution of
4096×2160 to 512×270. The wavelength coordinate was discretized
into 20 levels (10nm/level), hence the maximum grid resolution for
any experiment was reach 512 × 270 × 20, although additional cropping was performed on some datasets to only reconstruct regions
with interesting flows. The voxel pitch in the (x,y) plane is 100µm,
while along the z axis it is 900µm.
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
36:10 • J. Xiong, R. Idoughi, A.A. Aguirre-Pablo, A.B. Aljedaani, X. Dun, Q. Fu, S.T. Thoroddsen, and W. Heidrich
x
y
z
x
y
z
X [pixel]
Norm of Velocity (Pixel/s)
Y [pixel] Y [pixel]
x
y
1 mm
Fig. 10. Left: 5 successively captured images (without post-processing) in a video frame. Six representative particles are tracked in the time sequential
frames to verify the reconstructed flow structure. Right: Computed flow vectors according to the given frame data, viewing from different angles. Please see
supplemental video for better visualization.
3.5
6.6
0.4
Fig. 11. Path line visualization of the dataset from Figure 10.
The parameters for the optimization method were kept the same
for all datasets (κ1 = κ2 = 0.01, κ3 = 10−5
,κ4 = 10−7
). Only two
outer loops were required for all datasets, with 30 − 50 iterations
in the inner loop of the position estimation subproblem, two inner
loops for the velocity estimation problem, and finally five loops for
each frame within each velocity estimation step. The reconstruction
time for the largest dataset was 125 minutes on a 2.50Ghz Intel Xeon
E5-2680 CPU with 128GB RAM. Roughly 1/3 of that time was spent
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
Rainbow Particle Imaging Velocimetry • 36:11
Fig. 12. Path line visualization for two more datasets, corresponding to a
drop of water being dripped into the volume from the top (top image), and
a small jet of water being injected from the bottom (bottom image).
on the position estimation, and the remaining 2/3 on the velocity
estimation.
Five successive captured images are shown in the left hand side of
Figure 10, and the reconstructed velocity vectors are visualized on
the right hand side of the same figure. Six representative particles
are manually selected to verify the accuracy of the computed flow
vectors. The first particle moves upward in the image plane, and the
color of it changes from green to cyan, which states that it moves
away from the camera. The second particle moves upward and
slightly to the right in the image plane and in the depth direction,
it moves to the far-end of the camera. The third particle moves
to the upper left, color changes from green to cyan. The fourth
particle quickly moves to the left hand side with no significant color
change. From the fourth particle, we can observe a certain amount
of motion blur due to its large velocity. The fifth and sixth particle
move downwards in the image plane and towards camera in the
wavelength domain, while the orange one moves to the right and
blue one moves to the left. Comparing the motion of these chosen
particles with the corresponding flow vectors in the reconstructed
results, it reveals that overall agreement is achieved. In addition, the
actual stirred flow structure is supposed to be a vortex, rotating in a
clockwise direction. We observe that the key features of the vortex
structure are well reconstructed by our developed methods. A path
line visualization of the same velocity data is shown in Figure 11.
Note that the particles in the visualization are seeded synthetically
and do not directly correspond to RainbowPIV particles. Please also
refer to the supplemental video for dynamic visualizations of all
results.
Figure 12 shows two more data sets, one with a drop of water
being dripped into the volume from the top, and one where a small
amount of liquid is injected into the volume form the bottom. The
recovered flow field in both cases is consistent with both the expectations and the observed RainbowPIV frames (see video).
Finally, the most complex example is shown in Figure 13. This
flow was generated by strongly stirring the fluid, and then letting it
set. After a while, the pictured two-vortex structure can be observed.
Like many fluid imaging methods, RainbowPIV has problems reconstructing flows with strong motion blur. This limits our ability to
reconstruct the early stages of this experiment. To overcome this
limitation, high speed cameras could be used in conjunction with
stronger light sources.
5 CONCLUSION
We have introduced a novel RainbowPIV system coupled with optimization strategies, which enables us to recover the 3D fluid flow
structures using a single color camera, greatly reducing the hardware setup requirements and easing calibration complexity compared to the other approaches handling 3D-3C measurements. Our
approach is implemented by illuminating particles in the volume
with "rainbow" light such that the depth information of the particles
is color-coded into the captured images, and the 3D trajectory of
particles can be tracked by analyzing the 2D spatial motion in the
image plane and the color change in the wavelength domain. A specially designed DOE helps to focus all the wavelength planes on the
sensor plane simultaneously, to achieve high lateral resolution and
relatively large depth of focus at the same time. We then formulate
an inverse problem to reconstruct the particle positions in 3D using
a sequence of frames to alleviate the ambiguity issues of identifying
particle positions from a single frame. With the recovered particle
locations at different time steps, a further step is taken to reconstruct
the fluid velocity vector fields. An optimization problem integrating
the conventional Horn-Schunck algorithm with physical constraints
is proposed to compute the flow vectors.
We demonstrate our approach both on synthetic flows induced
by moving a frozen particle volume and by using a real stirred flow.
Overall, our method can robustly reconstruct a significant part of
the flow structures at good accuracy.
The primary drawback of our system is the limited spatial resolution along the wavelength (depth) coordinate. Due to the existence
of noise and light scattering issues, and relatively low sensitivity of
the camera to the wavelength change, at current stage the wavelength coordinate is not allowed to be discretized any further. In the
future this situation could be improved by making use of the IR end
of the spectrum instead of blue light, where camera sensitivity is
rather low. Other possible improvements include the use of cameras
ACM Transactions on Graphics, Vol. 36, No. 4, Article 36. Publication date: July 2017.
36:12 • J. Xiong, R. Idoughi, A.A. Aguirre-Pablo, A.B. Aljedaani, X. Dun, Q. Fu, S.T. Thoroddsen, and W. Heidrich
4.6
2.5
0.4
Fig. 13. Path line visualization of a complex flow created by stirring the fluid. Note the two vortices that interact in a complex fashion. The visualization uses
virtual particles that do not correspond directly to real particles imaged with RainbowPIV.
with additional color primaries, or primaries that are optimized for
this task.
Furthermore, our current system can only measure velocities
within a flow volume of fixed dimensions, which are determined
in the axial direction by the wavelength spread of the generated
rainbow volume and the matching chromatic aberration in the DOE
camera optics. In the future we intend to address this issue by designing a dynamically reconfigurable rainbow light engine. In addition
to having an adjustable depth range, this Rainbow light engine will
use diffractive optics to provide better light efficiency than the currently used linear filter. On the camera side, a viable solution already
exists in the form of encoded diffractive optics [Heide et al. 2016],
which allows for diffractive lenses with dynamically adjustable focal length. However, integration of the encoded DOE and the new
light engine into a new RainbowPIV setup requires still a significant
amount of system development.
Despite these current limitations, on account of the simple setup
and good accuracy, our system can be easily implemented and applied to investigate new types of fluid flows in the future.
