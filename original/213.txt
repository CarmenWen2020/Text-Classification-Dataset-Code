n this paper, we propose a novel neural network model, called bi-hemisphere domain adversarial neural network (BiDANN) model, for electroencephalograph (EEG) emotion recognition. The BiDANN model is inspired by the neuroscience findings that the left and right hemispheres of human's brain are asymmetric to the emotional response. It contains a global and two local domain discriminators that work adversarially with a classifier to learn discriminative emotional features for each hemisphere. At the same time, it tries to reduce the possible domain differences in each hemisphere between the source and target domains so as to improve the generality of the recognition model. In addition, we also propose an improved version of BiDANN, denoted by BiDANN-S, for subject-independent EEG emotion recognition problem by lowering the influences of the personal information of subjects to the EEG emotion recognition. Extensive experiments on the SEED database are conducted to evaluate the performance of both BiDANN and BiDANN-S. The experimental results have shown that the proposed BiDANN and BiDANN models achieve state-of-the-art performance in the EEG emotion recognition.

SECTION 1Introduction
Emotion, as a common mental phenomenon, is closely related to human's cognition and behavior [1]. Although emotion can be easily captured by human beings, it is still hard to be understood by machines. As one of the most active research topics of affective computing [2], emotion recognition had received substantial attention from computer vision and pattern recognition research communities. Basically, the responses of emotion can be roughly divided into the external and the internal responses. The typical external responses include facial expression, gesture or speech of human beings, and the typical internal responses include skin conductance response, heart rate, blood pressure, respiration rate, electroencephalograph (EEG), magnetoencephalogram (MEG) [3]. From the neuroscience view of point [4], there are some major brain cortex regions, e.g., the orbital frontal cortex, ventral medial prefrontal cortex, and amygdala, are closely related to emotions [5], [6], [7], which provides us a potential way to decode emotion by recording human's brain signals over these brain regions. For example, by placing the EEG electrodes on the scalp, we can record the neural activities of the brain, which can be used to recognize human's emotions.

Traditional EEG emotion recognition system usually consists of two major parts, i.e., feature extraction part and classifier design part. EEG features can be extracted either from time domain, frequency domain, or from time-frequency domain [8]. Jenke et al. [8] provided a comprehensive survey on the EEG feature extraction approaches. For dealing with the classification problem, many EEG emotion recognition models and methods were proposed over the past several years [9], [10]. Zheng et al. [11] proposed a novel group sparse canonical correlation analysis (GSCCA) method for simultaneous EEG channel selection and emotion recognition. Li et al. [12] proposed a graph regularized sparse linear regression (GRSLR) method to deal with EEG emotion recognition problem. Recently, using deep learning methods for EEG emotion recognition had been widely adopted and had demonstrated better performance than traditional methods. In [13], Zheng et al. proposed to use Deep Belief Network (DBN) for EEG emotion classification. In [14], Song et al. used a graph to model the multichannel EEG features and then perform EEG emotion classification based on it.

Although many algorithms or models have been proposed for the EEG emotion recognition problem, most of them focused on the scenarios where both training and testing data come from the same domain, in which we usually assume that the feature distributions of training and testing data are similar. Under this assumption, the label information of the target data is predicted directly based on the classifier learned with the source domain data samples. For cross-domain EEG emotion recognition problems, however, many of the aforementioned EEG emotion recognition methods would fail because of the mismatch problem of the EEG feature distributions between the source domain and the target one. A typical example is the cross-subject EEG emotion recognition problem, in which the training and testing EEG data are from different subjects.

To deal with the challenging cross-subject EEG emotion recognition problem, Pandey et al. [15] proposed a subject independent approach for EEG emotion recognition. Li et al. [16] proposed another method for cross-subject EEG emotion recognition. Recently, Ganin et al. proposed a domain adversarial neural networks (DANN) to deal with the domain adaptation problems [17]. It uses a domain discriminator as well as a feature generator to alleviate the differences of the feature distribution between the source domain and the target domain and at the same time generate domain-invariable data features without any class information from testing data. In our preliminary work of [18], we had proposed a novel neural network model that fits the asymmetry of emotional brain into the DANN structure meanwhile utilizing time information for EEG emotion recognition task.

On the other hand, from the neuroscience point of view, it is notable that the left and right hemispheres of human's brain are not entirely symmetrical [19], [20], [21], [22]. In [23], Dimond et al. first found that there exists different ‘emotional vision’ between the right and the left hemispheres of the human brain, where the right hemisphere demonstrates to be more powerful in perceiving unpleasant emotion than the left one. In [24], [25], [26], Davidson et al. showed that the EEG signals of the left frontal cortex are closely related to positive emotions while the ones in the right frontal cortex are related to negative emotions. In [27], Costanzo et al. focused on the research of mood induction and language activation based on functional magnetic resonance imaging (fMRI) and found that sadness and happiness are processed by each hemisphere. Moreover, some researchers had also proposed methods to measure the differences between the two hemispheres, which were used to the depression detection and EEG monitoring [28], [29], [30]. Nevertheless, it is still an interesting topic of how to utilize the asymmetry property of brain to improve the emotion recognition performance.

Motivated by the recent success of the DANN method and the findings of neuroscience about the asymmetry property of human brain, in this paper we propose a novel deep neural network model, called bi-hemispheres domain adversarial neural network (BiDANN) model, to deal with EEG emotion recognition problem, which can be seen as an extension of our preliminary work of [18]. In the BiDANN model, we design a global and two local domain discriminators that work adversarially with a classifier to learn discriminative emotional features of each hemisphere and at the same time to alleviate the domain differences between source and target domains on each hemisphere. Specifically, the BiDANN model consists of three major modules, i.e., the feature extractor module, the classifier module, and the domain discriminator module. The major goal of feature extractor module is to extract discriminative deep features based on the original EEG features extracted from each cerebral hemisphere, the classifier module aims to predict the emotion class information based on the EEG deep features, and the target of the domain discriminator module is to alleviate the domain differences between the source and target domains. Moreover, considering that the subject's information, e.g., the personal identity information, may influence the EEG emotion recognition performance, we also propose an improved version of BiDANN, denoted by BiDANN-S, to deal with subject-independent EEG emotion recognition. The basic idea is to ensure that the extracted EEG features are robust to the variation of subjects so as to improve the generality of the recognition model.

The remainder of this paper is organized as follows: In Section 2, we give a brief overview of preliminary work. In Sections 3 and 4, we propose the BiDANN model and the BiDANN-S model, respectively, for EEG emotion recognition. The experiments will be presented in Section 5, and finally, in Section 6, we will conclude the paper.

SECTION 2Preliminaries
In this section, we will briefly review the preliminary work of long short term memory (LSTM) method [31] and the domain adversarial neural networks (DANN) method [32].

2.1 LSTM
In the traditional neural networks methods, the inputs are independent with each other and hence they neglect sequence information. In contrast to the traditional neural network methods, the recurrent neural networks (RNN) method [33] aims to map the input sequence into a series of hidden states through a complex dynamics transform with a chain of repeating modules of neural network and remember and forget the information in the whole sequence adaptively [34]. The LSTM network [31] is a special type of RNN, which replaces the repeating module of RNN with a more complex structure consisting of three gates and a cell state to deal with long-term dependence problem.

2.2 DANN
The DANN model was proposed by Ganin et al. [17] to deal with domain adaptation problem under the assumption that the desired features should not be able to discriminate the data samples between the source and target domains. To this end, DANN borrows the idea of generative adversarial networks (GAN) [32] whose goal is to design an adversarial process that simultaneously learns a generative model and a discriminative model, in which the discriminative model estimates the probability of a sample coming from the real data set or a fake generative data set whereas the generative model aims to estimate the distribution of real training data. Based on the GAN method, the DANN method aims to generate domain-invariant data features that are discriminative for the classification task whereas in-discriminative for the shift between the source and target domains, in which a gradient reversal layer (GRL) is introduced for this purpose.

SECTION 3BiDANN for EEG Emotion Recognition
In this section, we will first address the BiDANN model and then extend it to deal with subject-independent EEG emotion recognition problem. Before addressing the detailed information of the model, we will first provide a list of the major notations and the corresponding definitions in Table 1.

TABLE 1 The Major Notations and the Corresponding Definitions Used in BiDANN
Table 1- 
The Major Notations and the Corresponding Definitions Used in BiDANN
In addition, we use the subscript notations of f, c, d to denote the feature extractor, the classifier and the discriminator, respectively. Similarly, we use the superscript notations of l, r to denote left and right hemisphere, respectively, and use the subscript notations of S, T to denote subscript of source and target domain respectively. For example, we use Xl and XS to denote the EEG feature matrix associated with the left hemisphere and the EEG feature matrix associated with the source domain, respectively, and use XlS to denote the EEG feature matrix corresponding to the left hemisphere of the source domain.
3.1 BiDANN
The target of BiDANN is to alleviate the possible feature distribution difference between source and target domains, either in each hemisphere or the whole brain cortex area. To achieve this goal, we borrow the basic idea of the DANN method by leverage the adversarial operation between the source and the target domains into the discriminative EEG feature learning module, and also make use of the neuroscience findings that the left and right hemispheres of human's brain are asymmetric to the emotional response to further enhance the discriminative ability of the EEG features. Fig. 1 illustrates the framework of the BiDANN model, which consists of the following major parts: (1) two feature extractors (fl and fr); (2) a global discriminator (Dg) and two local discriminators (Dl and Dr); (3) three GRL, and a classifier (C). In Fig. 1, the black lines and arrows refer to source domain path that the EEG data matrix XS is from the source domain, while the red lines and arrows refer to target domain path that the EEG data matrix XT is from the target domain.

Fig. 1. - 
The framework of BiDANN. The black lines and arrows refer to source domain path, while the red lines and arrows refer to target domain path.
Fig. 1.
The framework of BiDANN. The black lines and arrows refer to source domain path, while the red lines and arrows refer to target domain path.

Show All

In BiDANN model, the two feature extractors fl and fr aim to capture the dynamic features of two hemispheric EEG signal separately. The three discriminators are trained on a binary domain label set D={0,1}, in which the domain labels of source samples are set to 0 while the domain labels of target samples are set to 1. The global discriminator aims to narrow the features’ distribution gap between source and target domains, while the two local discriminators complementally eliminate the left and right hemispheric features’ distribution difference between source and target domains. The use of GRL is to maximize the loss of discriminators by leaving the input data unchanged during forward propagation and reversing the gradient by multiplying it with a negative scalar during backpropagation [17]. The classifier maps the emotion-related and domain-invariant features into class label space to predict the class labels. In what follows, we will specify the BiDANN model with respect to feature extractor, local discriminator, global discriminator and classifier:

3.1.1 Feature Extractor
The goal of feature extractor is to extract more discriminative features from handcraft EEG features so as to improve the EEG classification performance. The whole feature extraction procedure is summarized in Fig. 2, which consists of the handcraft EEG feature extraction part and the discriminative deep feature learning part.

Fig. 2. - 
The feature extraction procedures in the BiDANN model, which consists of the handcraft EEG feature extraction part and the discriminative deep feature extraction part.
Fig. 2.
The feature extraction procedures in the BiDANN model, which consists of the handcraft EEG feature extraction part and the discriminative deep feature extraction part.

Show All

To extract the handcraft EEG features, we first decompose the EEG signals into five frequency bands, i.e., δ band (1-3 Hz), θ band (4-7 Hz), α band (8-13 Hz), β band (14-30 Hz), and γ band (31-50Hz). For each EEG frequency band, we segment the EEG signals of each trial into some clips using a non-overlapped Hanning window [13], in which the EEG signals of each clip is used to extract 62 differential entropy (DE) features, resulting in a 310-dimensional feature vector. To explore the temporal dependence among the EEG feature vectors, every 9 neighboring EEG feature vectors determined by a slicing window with the size of 9 EEG clips are chosen to form an EEG feature matrix, denoted by X∈IRdx×tx, and this matrix is used to serve as one sample. By slicing the window with an interval of one EEG clip, we finally obtain a set of EEG feature matrices as the input EEG data.

Suppose that Xl=[xl1,…,xltx]∈IRdlx×tx and Xr=[xr1,…,xrtx]∈IRdrx×tx denote two handcraft EEG feature matrices of an EEG clip sequence associated with the left and right hemisphere, respectively, where dlx and drx denote the dimensions of the EEG feature vector, and tx is the number of EEG clips. Let
X=[Xl,Xr]=[xl1,…,xltx,xr1,…,xrtx].
View SourceTo capture the temporal dependence among the EEG feature sequence, we adopt the LSTM model to extract more discriminative EEG features, resulting in the following LSTM feature matrices Yl and Yr:
Yl=[yl1,…,yltx]=[f(xl1),…,f(xltx)],(1)
View Source
Yr=[yr1,…,yrtx]=[f(xr1),…,f(xrtx)],(2)
View Sourcewhere f(⋅) denote the LSTM feature learning operation, yli=f(xli) and yri=f(xri) (i=1,…,tx).

Let G=[Gik]tx×K be a tx×K coefficient matrix and σ(⋅) be a nonlinear mapping. Then, under the transformation of G and the operation of σ(⋅), we obtain the following deep feature matrices Hl and Hr:
Hl=σ(YlG+B)=σ(f(Xl)G+B),(3)
View Source
Hr=σ(YrG+B)=σ(f(Xr)G+B),(4)
View Sourcewhere B is a bias matrix.

Now suppose that XlS (XrS) and XlT (XrT) are the input EEG feature matrices of the left (right) hemisphere associated with the source domain and the target domain, respectively. Let HlS (HrS) and HlT (HrT) be the corresponding deep feature matrices obtained using (3) and (4). Then, by combining HlS (HrS) with HlT (HrT), we obtain the following new matrices:
HlS,T=[HlS,HlT]=Ef(XlS,XlT),(5)
View SourceRight-click on figure for MathML and additional features.
HrS,T=[HrS,HrT]=Ef(XrS,XrT),(6)
View SourceRight-click on figure for MathML and additional features.where Ef(⋅) denotes the feature extractor function.

3.1.2 Local, Global Discriminators and Classifier
The use of local discriminator aims to alleviate the feature distribution difference between source and target domains, in either left hemisphere or right one. In contrast to the local discriminators, the target of global discriminator is also to eliminate the feature distribution difference between source and target domains, but across the whole brain cortex area instead of only the hemisphere area.

Different from both local and global discriminators, the purpose of the classifier part is to learn the discriminative features across source and target domains, which is implemented by applying the softmax function to the transformed hidden states to predict the class label, where the loss function of class label prediction is expressed as:
Lc(XS;θlf,θrf,θc)=∑c−τ(y,c)×logP(c|XS),(7)
View SourceRight-click on figure for MathML and additional features.where c is the label of c-th emotion class, y is the ground-truth label associated with the data sample XS, and
τ(y,c)={1,0,if y=c,otherwise.(8)
View SourceRight-click on figure for MathML and additional features.

The optimal parameters of feature extractors are determined by minimizing the loss of classifier and maximizing the loss of both local and global domain discriminators. This leads to an adversarial learning between classifier and domain discriminators for emotion-related and domain-invariant EEG features.

According to the above analysis, we obtain that the BiDANN problem can be formulated as the optimization problem of minimizing the overall loss function L(θlf,θrf,θc,θld,θrd,θgd) defined as the following form:
minL(θlf,θrf,θc,θld,θrd,θgd)=Lc(XS;θlf,θrf,θc)−Lld(XlS,T;θlf,θld)−Lrd(XrS,T;θrf,θrd)−Lgd(XS,T;θlf,θrf,θgd),(9)
View SourceRight-click on figure for MathML and additional features.where Lc(⋅), Lld(⋅), Lrd(⋅) and Lgd(⋅) denote the loss function of classifier, the loss function of local discriminator associated with the left and right hemisphere, and the loss function of global discriminator, respectively, and the parameters θc, θld, θrd, θgd, θlf and θrf denote the corresponding model parameters.

3.2 Optimization of BiDANN
We can iteratively train the classifier and three discriminators and update the parameters with the similar approach of standard deep learning methods by chain rule. Specifically, to solve the optimal solution of BiDANN, we adopt the stochastic gradient descent (SGD) algorithm [35] to find optimal model parameters (θlf, θrf, θc) in (9). Moreover, we also introduce three gradient reversal layers for the three domain discriminators (see Fig. 1), which keep the gradient sign but reverse it while performing the back-propagation operation.

By iteratively minimizing the loss function of L defined in (9), we can finally obtain the optimal parameters. More specifically, suppose that the optimal parameters of θ^gd, θ^ld and θ^rd are obtained. Then, the optimal parameters of θ^lf, θ^rf, θ^c can be obtained by minimizing the loss function L(XS,T;(θlf,θrf,θc),θ^gd,θ^ld,θ^rd), i.e.,
(θ^lf,θ^rf,θ^c)=argminθlf,θrf,θcL(XS,T;(θlf,θrf,θc),θ^gd,θ^ld,θ^rd).
View Source

On the other hand, after obtaining the optimal parameters of θ^lf, θ^rf, θ^c, the optimal parameters of θ^gd, θ^ld, and θ^rd can be updated via solving the following optimization problems:
θ^gd=argmaxθgdL(XS,T;θ^lf,θ^rf,θ^c,θgd,θ^ld,θ^rd),(10)
View Source
θ^ld=argmaxθldL(XlS,T;θ^lf,θ^rf,θ^c,θ^gd,θld,θ^rd),(11)
View SourceRight-click on figure for MathML and additional features.
θ^rd=argmaxθrdL(XrS,T;θ^lf,θ^rf,θ^c,θ^gd,θ^ld,θrd).(12)
View SourceRight-click on figure for MathML and additional features.

Here it should be noted that the parameters before the GRL module (i.e., the parameters of feature extractors) will take the reverse direction of the gradients during back-propagation operation so as to generate data representations that minimize the loss of classifier while maximize the loss of discriminators. The optimization procedure of BiDANN is summarized in Algorithm 1.

Algorithm 1. Procedures of Training BiDANN Model
Input: Source data set {XS} and Target data set {XT};

Ground-truth label set LS={y} of source data set;

Source domain label set DS=[DlS,DrS]={0} and target domain label set DT=[DlT,DrT]={1};

Initialize model parameters and learning rate α;

repeat

Update the parameters of the classifier: θc←θc−α∂Lc∂θc, θlf←θlf−α∂Lc∂θlf, and θrf←θrf−α∂Lc∂θrf;

Update the parameters of the global discriminator: θgd←θgd−α∂Lgd∂θgd, θlf←θlf+α∂Lgd∂θlf, and θrf←θrf+α∂Lgd∂θrf;

Update the parameters of the local discriminator corresponding to the left hemisphere: θld←θld−α∂Lld∂θld, θlf←θlf+α∂Lld∂θlf;

Update the parameters of the local discriminator corresponding to the right hemisphere: θrd←θrd−α∂Lrd∂θrd, θrf←θrf+α∂Lrd∂θrf;

If the model has been trained for 100 epochs, then α←0.9×α and goto step 3;

until The iterations satisfies the predefined condition, i.e., the loss function satisfies:
L(θlf,θrf,θc,θld,θrd,θgd)<10−3.
View Source

Output:

Parameters: θ^lf,θ^rf,θ^c,θ^ld,θ^rd,θ^gd.


SECTION 4BiDANN-S for Subject-Independent EEG Emotion Recognition
The aforementioned BiDANN model provides a useful way to deal with the mismatching feature distribution problem between source domain and the target one. Nevertheless, it is notable that we may still suffer from the difficulties of handling the cross-subject EEG emotion recognition problem, in which the EEG feature distributions of different subjects would be different even if these subjects are from the same source domain. Consequently, it is very desirable to design an effective subject-independent model for the cross-subject EEG emotion recognition problem.

To handle such a subject-independent EEG emotion recognition problem, in this section we propose the BiDANN-S method based on the aforementioned BiDANN model. The basic idea of BiDANN-S is to lower the influences of the personal information of subjects to the EEG emotion recognition as more as possible. This goal is achieved by imposing an additional subject discriminator (denoted by Ds) onto the BiDANN model, in which the subject discriminator tries to lower the discrepancy for the recognition of different subjects. In designing the subject discriminator, we use the subject identity of the labeled source EEG data to serve as another class label set, denoted by Ds={1,2,…,N}, where N is the number of subject in training dataset. In this case, the subject discriminator will work adversarially with the emotion classifier in the BiDANN-S model to result in the subject-independent discriminative EEG features. Fig. 3 illustrates the subject-independent framework of BiDANN-S.

Fig. 3. - 
The framework of the proposed BiDANN-S. Compared with BiDANN, BiDANN-S has an additional subject discriminator to decrease the effect of identity information to the discriminative emotion features.
Fig. 3.
The framework of the proposed BiDANN-S. Compared with BiDANN, BiDANN-S has an additional subject discriminator to decrease the effect of identity information to the discriminative emotion features.

Show All

As a result, the loss function of the BiDANN-S model contains an additional part, denoted by Lsd(XS,T;θlf,θrf,θsd), compared with the BiDANN method. To achieve the subject-independent features, we should maximize the loss function Lsd(XS,T;θlf,θrf,θsd), where θsd is the parameter of subject discriminator. In this case, the entire loss function of BiDANN-S can be expressed as following form:
L(θlf,θrf,θc,θld,θrd,θgd,θsd)=Lc(XS;θlf,θrf,θc)−Lld(XlS,T;θlf,θld)−Lrd(XrS,T;θrf,θrd)−Lgd(XS,T;θlf,θrf,θgd)−Lsd(XS,T;θlf,θrf,θsd).(13)
View SourceRight-click on figure for MathML and additional features.

By minimizing the loss function L(θlf,θrf,θc,θld,θrd,θgd,θsd) in (9), we can iteratively solve the optimal model parameters θlf, θrf, θc, θld, θrd, θgd, θsd of BiDANN-S. The solution approach is similar with that shown in Algorithm 1.

SECTION 5Experiments
In this section, we will conduct extensive experiments on the SJTU Emotion EEG Dataset (SEED) [13] to evaluate the proposed BiDANN and BiDANN-S methods in EEG emotion recognition. The SEED database contains 15 subjects’ EEG signals recorded from 62 electrode channels using ESI NeuroScan with a sampling rate of 1000 Hz. The EEG data are collected when the participants are watching three kinds of emotional film clips, i.e., positive emotion, neutral emotion, negative emotion. To evaluate the proposed BiDANN method, we adopt both subject-dependent and subject-independent strategy to design experiments. In the subject-dependent experiment, we follow the same experimental protocol as that of [13] by choosing the EEG signals associated with two of the three sessions for each subject to serve as the experimental data, in which each session contains 15 trials of EEG signals. In the subject-independent experiment, on the other hand, we also adopt the same experimental protocol as that of [36] by choosing one of the EEG signals associated with one of the three sessions to serve as the experimental data.

Moreover, to evaluate the contributions of different parts of BiDANN to the EEG emotion recognition, we also propose two simplified versions of BiDANN, denoted by BiDANN-R1 and BiDANN-R2, respectively, and compare the recognition performance among them under the same experimental settings. In the BiDANN-R1 model, we use two feature extractors to capture the emotion information from source and target data without considering the hemispheric differences. In contrast to the BiDANN-R1 model, the BiDANN-R2 model uses two feature extractors to capture each brain hemispheric emotion information from source and target data separately. The frameworks of BiDANN-R1 and BiDANN-R2 are shown in Figs. 4 and 5, respectively.

Fig. 4. - 
The framework of BiDANN-R1, which uses two feature extractors to capture the emotion information from source and target data.
Fig. 4.
The framework of BiDANN-R1, which uses two feature extractors to capture the emotion information from source and target data.

Show All


Fig. 5.
The framework of BiDANN-R2, which uses two feature extractors to capture each brain hemispheric emotion information from source and target data separately.

Show All

5.1 Subject-Dependent EEG Emotion Recognition Experiments
In this experiment, we adopt the experimental protocol of Zheng et al. [13] by using 9 trails of EEG data per session of each subject as source (training) domain data whereas using the remaining 6 trials per session as target (testing) domain data. In this way, we can obtain 1938 samples in total as the training data and 1336 samples in total as the testing data. For comparison purpose, we use the linear support vector machine (SVM) [37], canonical correlation analysis (CCA) [39], group sparse canonical correlation analysis [11], deep believe network [13], graph regularization sparse linear regression [12], graph convolutional neural network (GCNN) [40] and dynamical graph convolutional neural network (DGCNN) [14] to conduct the same experiment, respectively. Moreover, in this experiment we also conduct additional experiments for the BiDANN-R1 method by using hemispheric EEG data to investigate which hemisphere contributes more to the emotion recognition. Table 2 summarizes the experimental results of the various methods. In addition, we also show the confusion matrices of the BiDANN method in Fig. 6 to see the confusions of BiDANN in recognizing the three emotions.


Fig. 6.
The confusion matrices of the subject-dependent EEG emotion recognition results using the BiDANN method on the SEED database.

Show All

TABLE 2 The Mean Accuracies (ACC) and Standard Deviations (STD) on SEED Database for Conventional EEG Emotion Recognition Experiment

From Table 2, we can see that the proposed BiDANN method achieves the best recognition result among the various methods. Especially, compared with the other deep learning based methods such as DBN, GCNN and DGCNN, the proposed BiDANN method increases the classification accuracy of 6.30, 4.98 and 1.98 percent, respectively. In addition, from Table 2, we can see that the methods with domain discriminator, such as DANN and BiDANN-R1, achieve higher recognition accuracies than the other methods. On the other hand, from Fig. 6, we can see that the average recognition accuracies of BiDANN in recognizing the three types of emotions are 86.15 (negative), 93.61 (neutral), and 96.89 percent (positive), which means that positive emotion would be much easier to be recognized than both negative and neutral emotions.

Moreover, we also adopt the BiDANN-R1 method to investigate the differences between left and right hemispheres in the perception of the three types of emotions. The experimental results are shown in Fig. 7, from which we can see that, the right hemisphere shows more powerful perception ability than the left hemisphere in perceiving the negative emotion, whereas the left hemisphere demonstrates better perception ability than the right hemisphere in perceiving the positive emotion. The experimental results are consistent with the neuroscience finding about the emotion perception of the two hemispheres [23].


Fig. 7.
The experimental results of using the BiDANN-R1 method to recognizing the three emotions based on the left and the right hemispheres EEG signals, respectively.

Show All

It is notable that the previous work of Zheng et al. [11] showed that different frequency bands of the EEG signals may play different roles in the emotion recognition task. In this experiment, we also conduct experiments to verify this point. Table 3 summarizes the experimental results of the various emotion recognition methods across the five EEG frequency bands. From Table 3, we can see that the experimental results are consistent with the findings of Zheng et al. in [11], i.e., the higher frequency bands (β and γ) would achieve better recognition performance than lower frequency bands (δ, θ and α). From Table 3, we can also see that the BiDANN method achieves better performance than the other methods for each frequency band.

TABLE 3 The Mean Accuracies (and Standard Deviations) Using Different Frequency Bands for Subject-Dependent EEG Emotion Recognition Experiment on SEED Database

5.2 Subject-Independent EEG Emotion Recognition Experiments
In this experiment, we will focus our attention on the subject-independent EEG emotion recognition problem. For this purpose, we adopt the leave-one-subject-out (LOSO) cross-validation strategy to evaluate the performance of the proposed BiDANN-S method, in which we use the EEG signals of one subject as testing data and the rest of ones of 14 subjects as training data to conduct the experiment. This procedure is repeated such that the EEG signals of each subject have been used once as testing data. For comparison purpose, we also conduct the same experiments using various methods, including the methods of Kullback-Leibler importance estimation procedure (KLIEP) [41], unconstrained least-squares importance fitting (ULSIF) [42], selective transfer machine (STM) [43], linear SVM [37], kernel principal component analysis (KPCA) [44], transfer component analysis (TCA) [45], transfer kernel learning (TKL) [46], subspace alignment (SA) [47], geodesic flow kernel (GFK) [48], transductive SVM (T-SVM) [49], transductive parameter transfer (TPT) [50], and DGCNN [14]. The experimental results of the various methods are shown in Table 4.1

TABLE 4 The Mean Accuracies (ACC) and Standard Deviations (STD) on SEED Database for Subject-Indepent EEG Emotion Recognition Experiment
Table 4- 
The Mean Accuracies (ACC) and Standard Deviations (STD) on SEED Database for Subject-Indepent EEG Emotion Recognition Experiment
From Table 4, we can see that the proposed BiDANN-S method achieves the best recognition result among the various methods. The better recognition performance of BiDANN-S is very likely due to the fact that the subject discriminator of BiDANN-S can effectively reduce the subjects’ personal identity information such that it is more powerful for the subject-independent emotion recognition problem. Moreover, we can also see that the BiDANN-R2 method achieves better recognition result than the BiDANN-R1 method, which indicates the importance of considering the discrepancy information between left and right cerebral hemispheric data in the EEG emotion recognition. Furthermore, we can also see that the BiDANN method with local discriminators also achieves better performance than BiDANN-R2, which indicates that the local discriminators are useful to improve the EEG emotion recognition performance.

In addition, we also depict the confusion matrices corresponding to the experimental results of BiDANN and BiDANN-S to see the confusions of the three emotions. Fig. 8 shows the confusion matrices of the experimental results of BiDANN and BiDANN-S, respectively. From Fig. 8, we can see that both BiDANN and BiDANN-S perform well in recognizing all the three types of emotion. Compared with BiDANN, it is interesting to see that BiDANN-S perform much better in recognizing both negative and neutral emotions than BiDANN, indicating the effectiveness of the subject discriminator in the BiDANN-S model.

Fig. 8. - 
The confusion matrices of the subject-independent EEG emotion recognition results using BiDANN and BiDANN-S method on the SEED database. (a) Confusion matrix of BiDANN; (b) Confusion matrix of BiDANN-S.
Fig. 8.
The confusion matrices of the subject-independent EEG emotion recognition results using BiDANN and BiDANN-S method on the SEED database. (a) Confusion matrix of BiDANN; (b) Confusion matrix of BiDANN-S.

Show All

Finally, to investigate the different contributions of the different frequency bands to the emotion recognition, we also conduct the experiments on each of the five frequency bands using the LOSO experimental strategy. Table 5 summarizes the experimental results of the various methods with respect to the five frequency bands. Again, from Table 5, we can see that the higher frequency bands achieve better recognition performance than lower frequency bands. Moreover, from Table 5 we can also see that the proposed BiDANN and BiDANN-S methods achieve better recognition performance than the other methods for the five frequency bands.

TABLE 5 The Mean Accuracies (and Standard Deviations) Using Different Frequency Bands for Subject-Independent EEG Emotion Recognition on SEED Database

5.3 Discussions
In the aforementioned experiments, we had demonstrated that the BiDANN-S method can further improve the recognition performance in contrast to the BiDANN method. This mainly attributes to the fact that the subject discriminator used in BiDANN-S can largely remove the influences of the subject information from the EEG features and hence is able to improve the discriminative ability of the EEG features for emotion recognition.

To further verify this point, we conduct an additional experiment on the two-class HR-EEG4EMO database [51], which consists of the EEG signals of 27 subjects. In this experiment, we first adopt the same band power (BP) EEG feature as that used in [51] to extract 64 handcraft features associated with 64 EEG electrodes from each subject. Then, we visualize the feature distributions associated with five subjects using the t-SNE embeddings method [52]. Fig. 9 shows the visualized results of the raw EEG handcraft features and the discriminative features learnt by BiDANN and BiDANN-S respectively, where the feature distribution corresponding to each subject is obtained based on BiDANN or BiDANN-S using the EEG data of the other 26 subjects. From Fig. 9, we can see that the raw EEG feature points associated with positive emotion are relatively hard to be recognized compared with the feature points associated with negative emotion. In contrast to the raw features, we can see that the separability between the positive emotion and negative emotion becomes much easier when BiDANN or BiDANN-S is used. On the other hand, compared with the BiDANN method, the BiDANN-S method achieves better separability for most cases. The better separability of BiDANN-S than BiDANN mainly attributes to the reduction of the subject personal information in the EEG features.


Fig. 9.
The t-SNE visualization before and after domain adaptation. (a)-(e) are raw data. (f)-(j) are feature distributions obtained by BiDANN. (k)-(o) are feature distributions obtained by BiDANN-S. The red color denotes the negative emotion and the blue color denotes the positive emotion.

Show All

SECTION 6Conclusions
In this paper, we had proposed a novel BiDANN method to deal with EEG emotion recognition, which is inspired by the neuroscience findings about the asymmetry property of human brain to the emotion response. The extensive experiments on the SEED EEG database demonstrated that the proposed BiDANN method achieves better EEG emotion recognition performance than the state-of-the-art methods. The better recognition performance of BiDANN mostly attributes to the fact that the BiDANN method can not only make use of the advantages of adversarial discriminative domain adaptation to improve the EEG recognition performance, but also is able to make use of the asymmetry of the left and right hemispheres in the perception of different emotions. Based on the BiDANN method, we also proposed the BiDANN-S method to further improve the BiDANN method to deal with the cross subject EEG emotion recognition problem. The experimental results on the SEED database had confirmed that the BiDANN-S method would achieve higher recognition accuracy than BiDANN.