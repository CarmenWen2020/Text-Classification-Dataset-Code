We present an algorithm for constructing 3D panoramas from a sequence of
aligned color-and-depth image pairs. Such sequences can be conveniently
captured using dual lens cell phone cameras that reconstruct depth maps
from synchronized stereo image capture. Due to the small baseline and
resulting triangulation error the depth maps are considerably degraded and
contain low-frequency error, which prevents alignment using simple global
transformations. We propose a novel optimization that jointly estimates the
camera poses as well as spatially-varying adjustment maps that are applied
to deform the depth maps and bring them into good alignment. When fusing
the aligned images into a seamless mosaic we utilize a carefully designed
data term and the high quality of our depth alignment to achieve two orders of magnitude speedup w.r.t. previous solutions that rely on discrete
optimization by removing the need for label smoothness optimization. Our
algorithm processes about one input image per second, resulting in an endto-end runtime of about one minute for mid-sized panoramas. The final 3D
panoramas are highly detailed and can be viewed with binocular and head
motion parallax in VR.
CCS Concepts: • Computing methodologies → Image-based rendering; Reconstruction; Computational photography;
Additional Key Words and Phrases: 3D Reconstruction, Image-stitching,
Image-based Rendering, Virtual Reality
1 INTRODUCTION
Virtual reality (VR) is a fascinating emerging technology that creates
lifelike experiences in immersive virtual environments, with highend headsets now widely available. Most content that is consumed
in VR today is synthetic and needs to be created by professional
artists. There is no practical way for consumers to capture and share
their own real-life environments in a form that makes full use of
the VR technology.
That is perhaps not surprising when considering the formidability
of this problem: we are looking for a method that does not require
expensive hardware and is easy to use even for novice users. Yet,
it should create high-quality and truly immersive content, i.e., a
3D representation that supports binocular vision and head-motion
parallax. Finally, consumers demand very fast processing times, on
the order of seconds at most.
Panoramic images and video can be easily captured now with
consumer 360° cameras. While the surrounding imagery provides
some immersion, the realism is limited due to lack of depth and
parallax. Stereo panoramas [Peleg and Ben-Ezra 1999] provide binocular depth cues by delivering different images to the left and right
eye, but these images are static and do not provide motion parallax
when the user turns or moves their head.
Full immersion can only be achieved using a 3D representation,
such as the 3D models generated by multi-view stereo methods.
However, when applied to typical panorama datasets, captured
“inside-out” from a single vantage point, these methods have to
deal with a very small baseline, which causes noisy results and
holes in the reconstruction. The methods are also very sensitive
to even slight scene motion, such as wind-induced motion in trees.
The Casual 3D Photography system [Hedman et al. 2017] achieves
some improvement in reconstruction quality, but it is slow with a
runtime of several hours per scene.
ACM Transactions on Graphics, Vol. 37, No. 4, Article 101. Publication date: August 2018.
101:2 • P. Hedman and J. Kopf
In this paper, we present a new algorithm that constructs 3D
panoramas from sequences of color-and-depth photos produced
from small-baseline stereo dual camera cell phones, such as recent
iPhones. We take these sequences with a custom burst capture app
while casually moving the phone around at a half-arm’s distance.
The depth reconstruction is essentially free since it is integrated into
native phone OS APIs and highly optimized. Using depth maps from
dual cameras makes our algorithm somewhat robust to scene motion,
because the synchronous stereo capture enables reconstructing
depth even for dynamic objects, though stitching them might still
result in visible seams.
Our method is fast and processes approximately one input image
per second, about the same time it takes to capture. We stress the
importance of this point, since we found that as our system became
faster, it made our own behavior with regards to capture more
opportunistic: we were suddenly able to capture spontaneously on
the go and even “iterate” on scenes to try different viewing angles.
The output of our algorithm is a detailed 3D panorama, i.e., a
textured, multi-layered 3D mesh that can be rendered with standard
graphics engines. Our 3D panoramas can be viewed with binocular
and head-motion parallax in VR, or using a parallax viewer on
normal mobile and web displays (see the accompanying video for
examples). We can also generate interesting geometric effects using
the 3D representation (Figure 1, right).
We faced two challenges when developing this algorithm, which
lead to our two main technical contributions:
(1) Due to the very small baseline of dual phone cameras depth
estimation is highly uncertain, and it requires strong edgeaware filtering to smooth the resulting noise. However, this
leads to low frequency errors in the depth maps that prevent
simple alignment using global transformations. We present
a novel optimization method that jointly aligns the depth
maps by recovering their camera poses as well as solving for
a spatially-varying adjustment field for the depth values. This
method is able to bring even severely degraded input depth
maps into very good alignment.
(2) Existing image fusion methods using discrete optimization are
slow. We utilize a carefully designed data term and the high
quality of our depth alignment to remove the need for label
smoothness optimization, and replace it with independently
optimizing every pixel label after filtering the data term in a
depth-guided edge-aware manner. This achieves a speedup
of more than two orders of magnitude.
After stitching, we convert the 3D panorama into a multi-layered
representation by converting it to a mesh, tearing it at strong depth
edges, and extending the back-layer into occluded regions while
hallucinating new colors and depths. When viewing the panorama
away from the default viewpoint this new content is revealed in
disocclusions.
We demonstrate our algorithm on a wide variety of captured
scenes, including indoor, outdoor, urban, and natural environments
at day and night time. We also applied our algorithm to several
datasets where the depth maps were estimated from single images
using CNNs. These depth maps are strongly deformed from their
ground truth and lack image-to-image coherence, but nevertheless our algorithm is able to produce surprisingly well-aligned and
consistent stitched panoramas. A large number of these results is
provided in the supplementary material and accompanying video.
2 PREVIOUS WORK
360° Photo and Video: Using dedicated consumer hardware, such
as the Ricoh Theta, it is now easy to capture full 360° × 180° panoramas. Although this is often marketed as capture for VR, it does not
make use of the most interesting capabilities of that technology, and
the lack of binocular and motion parallax limits the realism.
Stereo Panoramas: Binocular depth perception can be enabled by
stitching appropriate pairs of left-eye and right-eye panoramic images. This representation is often called omnidirectional stereo [Anderson et al. 2016; Ishiguro et al. 1990; Peleg et al. 2001; Richardt
et al. 2013]. Recent systems enable the capture of stereo panoramic
videos using multiple cameras arranged in a ring [Anderson et al.
2016; Facebook 2016], or using two spinning wide-angle cameras
[Konrad et al. 2017].
Omnidirectional stereo has a number of drawbacks. In particular,
the rendered views are not in a linear perspective projection and
exhibit distortions such as curved straight lines and incorrect stereo
parallax away from the equator [Hedman et al. 2017]. Even more
importantly, the representation does not support motion parallax,
i.e., the rendered scene does not change as the user moves their head,
which considerably limits depth perception, and, hence, immersion.
Parallax-aware Panorama Stitching: Some panorama stitching
methods compute warp-deformations to compensate for parallax in
the input images. While this reduces artifacts, it does not address
the fundamental limitation that this representation does not support
viewpoint changes at runtime.
Zhang and Liu [2014] stitch image pairs with large parallax by
finding a locally consistent alignment sufficient for finding a good
seam. Perazzi et al. [2015] extend this work to the multi-image
case and compute optimal deformations in overlapping regions to
compensate parallax and extrapolate the deformation field smoothly
in the remaining regions. Lin et al. [2016] handle two independently
moving cameras whose relative poses change over time.
Recent work [Zhang and Liu 2015] demonstrates that these approaches also extend to omni-directional stereo. However, this line
of work has not yet produced explicit 3D geometry, making them
unable to produce head-motion parallax in VR.
Panoramas with Depth: An alternative to generating a left-right
pair of panoramic images is to augment a traditional stitched panoramic image with depth information. Im et al. [2016] construct
a panorama-with-depth from small baseline 360° video. However,
the fidelity of the depth reconstruction does not seem sufficient for
viewpoint changes (and this has not been demonstrated.) Lee et
al. [2016] use depth information to compute a spatially varying 3D
projection surface to compensate for parallax when stitching images
captured with a 360° rig. However, similar to before mentioned work
the surface is a low-resolution grid mesh. Zheng et al. [2007] create
a layered depth panorama using a cylinder-sweep multi-view stereo
algorithm. However, their algorithm creates discrete layers at fixed
depths and cannot reconstruct sloped surfaces.
ACM Transactions on Graphics, Vol. 37, No. 4, Article 101. Publication date: August 2018.
Instant 3D Photography • 101:3
Stage
Capture and pre-processing
(Section 4.1)
Deformable alignment
(Section 4.2)
Stitching
(Section 4.3)
Multi-layer processing
(Section 4.4)
Stage output
Color-and-depth images
IMU orientations
Feature point matches
Camera poses
Depth adjustment fields
Color-and-depth panorama Triangle mesh
Texture atlas
Fig. 2. Breakdown of the major algorithms stages and their outputs, which form the inputs to the next respective stage.
Multi-view Stereo: A long line of research in computer vision
is concerned with producing depth maps or surface meshes from
multiple overlapping images using multi-view stereo (MVS) algorithms [Seitz et al. 2006]. MVS algorithms are used in commercial
photogrammetric tools for 3D reconstruction of VR scenes [Realities
2017; Valve 2016]. Huang et al. [2017] use MVS to obtain dense point
clouds from video sequences captured with a single 360° camera.
MVS methods work best if the camera baseline is large, which
is unfortunately not the case in the panorama capture scenario. In
this case, it is difficult for the methods to deal with the triangulation
uncertainty, which leads to artifacts, such as noisy reconstructions
and missing regions. These methods are usually also slow, with
runtimes ranging from minutes to hours. Hedman et al. [Hedman
et al. 2017] improve the quality of reconstructed 3D panoramas, but
their algorithm requires several hours of processing.
Light Fields: The light field representation [Gortler et al. 1996;
Levoy and Hanrahan 1996] can generate highly realistic views of
a scene with motion parallax and view-dependent effects. Recent
work addresses unstructured acquisition with a hand-held camera
[Davis et al. 2012]. The main disadvantage of this representation
is that it requires a very large number of input views that need
to be retained to sample from at runtime, and a custom rendering
algorithm.
Bundle Adjustment with Depth: The popularisation of consumer
depth cameras has inspired research on aligning and fusing multiple
depth maps into globally consistent geometry [Izadi et al. 2011]. Dai
et al. [Dai et al. 2017b] present system which integrates new depth
maps in real-time, using bundle adjustment on 3D feature point
correspondences to continuously maintain and refine alignment.
There has also been work on non-rigid deformations to refine
alignment with active depth sensors. Zhou and Koltun [Zhou and
Koltun 2014] perform 3D camera calibration during scanning, correcting for non-linear distortion associated with the depth camera.
Whelan et al. [Whelan et al. 2015] show how to correct for drift
by non-rigidly deforming the 3D geometry which has already been
scanned.
In general, methods designed for depth cameras cannot directly
be applied to narrow baseline stereo data, which is of much lower
quality. Unlike the depth maps used in this paper, depth sensors
provide absolute scale, maintain frame-to-frame consistency and
can often be rigidly aligned to a high degree of accuracy.
3 OVERVIEW
The goal of our work is to enable easy and rapid capture of 3D
panoramas using readily available consumer hardware.
3.1 Dual Lens Depth Capture
Dual lens cameras capture synchronized small-baseline stereo image
pairs for the purpose of reconstructing an aligned color-and-depth
image using depth-from-stereo algorithms [Szeliski 2010]. The depth
reconstruction is typically implemented in system-level APIs and
highly optimized, so from a programmer’s and a user’s perspective,
the phone effectively features a “depth camera”. Several recent flagship phones feature dual cameras, including the iPhone 7 Plus, 8
Plus, X, and Samsung Note 8. Such devices are already in the hands
of tens of millions of consumers.
The small baseline is both a blessing and a curse: the limited search
range enables quickly establishing dense image correspondence but
also makes triangulation less reliable and causes large uncertainty
in the estimated depth. For this reason, most algorithms employ
aggressive edge-aware filtering [Barron et al. 2015; He et al. 2010],
which yields smoother depth maps with color-aligned edges, but
large low-frequency error in the absolute depth values. In addition,
the dual lenses on current-generation phones constantly move and
rotate during capture due to optical image stabilization, changes
in focus, and even gravity1
. These effects introduce a non-linear
and spatially-varying transformation of disparity that adds to the
low-frequency error from noise filtering mentioned above.
1
see http://developer.apple.com/videos/play/wwdc2017/507 at 17:20-20:50, Slides 81-89.
ACM Transactions on Graphics, Vol. 37, No. 4, Article 101. Publication date: August 2018.
101:4 • P. Hedman and J. Kopf
Stereo image pair (a) iPhone 7+ depth map (b) Monodepth [Godard et al. 2017] (c) DfUSMC depth map [Ha et al. 2016]
Fig. 3. Estimating depth maps using various algorithms. Note relative scale difference and low-frequency deformations between different maps. (a) Small
baseline stereo depth computed by the native iOS algorithm on an iPhone 7+. (b) Single image CNN depth map [Godard et al. 2017]. (c) Depth from accidental
motion result [Ha et al. 2016] (we actually used a short video clip to produce this result).
In Figure 3, you can see depth maps reconstructed using different
stereo algorithms on this kind of data. As revealed in the figure,
there is a significant amount of low-frequency error in the depth
maps. Since our focus is not stereo matching, we use the depth
maps from the native iPhone 7 Plus stereo algorithm for all of our
experiments.
An important detail to note is that many small baseline stereo
methods (including the one running on the iPhone) do not estimate
absolute depth, but instead produce normalized depth maps. So,
aligning such depth map involves estimating scale factors for each of
them, or, in fact, sometimes even more complicated transformations.
3.2 Algorithm Overview
Our 3D panorama construction algorithm proceeds in four stages:
Capture (Section 4.1, Figure 2a): The input to our algorithm is a
sequence of aligned color-and-depth image pairs, which we capture
from a single vantage point on a dual lens camera phone using a
custom burst capture app.
Deformable Depth Alignment (Section 4.2, Figure 2b): Due to the
small camera baseline and resulting triangulation uncertainty, the
input depth maps are not very accurate, and it is not possible to align
them well using global transformations. We resolve this problem
using a novel optimization method that jointly estimates the camera
poses as well as spatially-varying adjustment maps that are applied
to deform the depth maps and bring them into good alignment.
Stitching (Section 4.3, Figure 2c): Next, we stitch the aligned colorand-depth photos into a panoramic mosaic. Usually this is formulated as a labeling problem and solved using discrete optimization
methods. However, optimizing label smoothness, e.g., using MRF
solvers, is very slow, even when the problem is downscaled. We
utilize a carefully designed data term and the high quality of our
depth alignment, to replace label smoothness optimization with
independently optimizing every pixel after filtering the data term in
a depth-guided edge-aware manner. This achieves visually similar
results with more than an order of magnitude speedup.
Multi-layer Mesh Generation (Section 4.4, Figure 2d): In the last
stage, we convert the panorama into a multi-layered and textured
mesh that can be rendered on any device using standard graphics
engines. We tear the mesh at strong depth edges and extend the
backside into the occluded regions, hallucinating new color and
depth values in occluded areas. Finally, we simplify the mesh and
compute a texture atlas.
4 ALGORITHM
4.1 Capture and Preprocessing
We perform all of our captures with an iPhone 7 Plus using a custombuilt rudimentary capture app. During a scene capture session, it
automatically triggers the capture of color-and-depth photos (using
the native iOS stereo algorithm) at 1 second intervals.
The capture motion resembles how people capture panoramas
today: the camera is pointed outwards while holding the device at
half-arms’ length and scanning the scene in an arbitrary up-, down-,
or sideways motion. Unfortunately, the field-of-view of the iPhone 7
Plus camera is fairly narrow in depth capture mode (37◦ vertical), so
we need to capture more images than we would with other cameras.
A typical scene contains between 20 and 200 images.
The captured color and depth images have 720 × 1280 pixels and
432 × 768 pixels resolution, respectively. We enable the automatic
exposure mode to capture more dynamic range of the scene. Along
with the color and depth maps, we also record the device orientation
estimate provided by the IMU.
Feature extraction and matching: As input for the following alignment algorithm, we compute pairwise feature matching using standard methods. We detect Shi-Tomasi corner features [Shi and Tomasi
1994] in the images, tuned to be separated by at least 1% of the
image diagonal. We then compute DAISY descriptors [Tola et al.
2010] at the feature points. We use the IMU orientation estimate to
choose overlapping image pairs, and then compute matches using
the FLANN library [Muja and Lowe 2009], taking care to discard
outliers with a ratio test (threshold = 0.85) and simple geometric
filtering, which discards matches whose offset vector deviates too
much from the median offset vector (more than 2% of the image
diagonal). All this functionality is implemented using OpenCV.
4.2 Deformable Depth Alignment
Our first goal is to align the depth maps. Since the images were
taken from different viewpoints, we cannot deal with this in 2D
image space due to parallax. We need to recover the extrinsic camera
poses (orientation and location), so that when we project out the
depth maps they align in 3D.
ACM Transactions on Graphics, Vol. 37, No. 4, Article 101. Publication date: August 2018.
Instant 3D Photography • 101:5
Coefficient of variation Stitched depth map
(a) Our global affine alignment (Eq. 4) (b) Global alignment to SFM point cloud (c) Our deformable alignment (Eq. 8)
Fig. 4. Aligning depth maps with low-frequency errors. We show stitches and the coefficient of variation (see text) for various methods. (a) Our algorithm with
a global affine model (Eq. 4). Many depth maps got pushed to infinity. (b) Aligning each depth map independently with Eq 4 to a high-quality reconstruction.
The result is better, but there are many visible seams and floaters due to the impossibility to fit the inaccurate depth maps with simple global transformations.
(c) Our algorithm with the spatially-varying affine model yields excellent alignment.
4.2.1 Rigid alignment: We achieve this goal by minimizing the
distance between reprojected feature point matches. Let f
i
A
be a
feature point in image A and M =

(f
i
A
, f
i
B
)
	
the set of all matched
pairs. We define a reprojection loss as follows:
Ereprojection =
Õ
(f
i
A
, f
i
B
)∈M
ρ
 


PA→B

f
i
A

− f
i
B




2
2

, (1)
where ρ(s) = log(1+s) is a robust loss function to reduce sensitivity
to outlier matches, and PA→B(f ) is a function that projects the 2D
point f from image A to image B:
PA→B (f ) = n

R
T
B

RA
3D point in camera A’s coord
z }| {
˜
f dA(f ) + tA
| {z }
3D point in world space
− tB


, (2)
where (RA, tA) and (RB, tB) are the rotation matrix and translation
vectors for images A and B, respectively, ˜
f is the homogeneousaugmented version of f , dA(f ) is the value of image A’s depth map
at location f , and n

[x,y, z]
T

= [
x
z
,
y
z
]
T
. Note, that this formulation
naturally handles the wrap-around in 360° panoramas.
Similar reprojection losses are common in geometric computer
vision and have been used with great success in many recent reconstruction systems [Schönberger and Frahm 2016]. However, our
formulation has a subtle but important difference: since we have
depth maps, we do not need to optimize the 3D location of feature
point correspondences. This significantly simplifies the system in
several ways: (1) it drastically reduces the number of variables that
need to be estimated, to just the camera poses, (2) we do not have
to link feature point matches into long tracks, and (3) the depth
maps helps reduce uncertainty, making our system robust to small
baselines and narrow triangulation angles.
Eq. 2 assumes that the camera intrinsics as well as lens deformation characteristics are known and fixed throughout the capture. If
this is not the case, extra per-camera variables could be added to
this equation to estimate these values during the optimization.
Minimizing Eq. 1 w.r.t. the camera poses is equivalent to optimizing a rigid alignment of the depth maps. However, since most small
baseline depth maps are normalized (including the ones produced
by the iPhone), they cannot be aligned rigidly.
4.2.2 Global transformations: We resolve this problem by introducing extra variables that describe a global transformation of each
depth map. Our first experiment was trying to estimate a scale factor
sA for each depth map, i.e., by replacing dA(f ) in Eq. 1 with
d
scale
A
(f ) = sA dA(f ), (3)
where sA is an extra optimization variable per image. However, this
did not achieve good results, because, as we learned, many depth
maps are normalized using unknown curves. We tried a variety of
other classes of global transformations, and achieved the best results
with an affine transformation in disparity space (i.e., 1/d):
d
affine
A
(f ) =

sA d
−1
A
(f ) + oA)
−1
, (4)
sA and oA are per-image scale and offset coefficients, respectively.
Figure 4a shows a typical result of minimizing Eq 1 with the
affine model. Many depth maps are incorrectly pushed at infinity,
because the optimizer could not find a good way to align them
otherwise. In the bottom row we visualize the coefficient of variation
of depth samples per pixel, i.e., the ratio of the standard deviation
to the mean. This is a scale-independent way of visualizing the
amount of disagreement in the alignment. As a sanity check we also
tried to independently align each depth map to a high quality SFM
reconstruction of the scene (computed with COLMAP [Schönberger
and Frahm 2016]) that can be considered ground truth (Figure 4b).
Even with this “best-possible” result for the model the stitch is
severely degraded by seams and floaters.
ACM Transactions on Graphics, Vol. 37, No. 4, Article 101. Publication date: August 20