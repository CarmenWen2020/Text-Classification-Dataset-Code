Despite remarkable advances in emotion recognition, they are severely restrained from either the essentially limited property of the employed single modality, or the synchronous presence of all involved multiple modalities. Motivated by this, we propose a novel crossmodal emotion embedding framework called EmoBed, which aims to leverage the knowledge from other auxiliary modalities to improve the performance of an emotion recognition system at hand. The framework generally includes two main learning components, i.e., joint multimodal training and crossmodal training. Both of them tend to explore the underlying semantic emotion information but with a shared recognition network or with a shared emotion embedding space, respectively. In doing this, the enhanced system trained with this approach can efficiently make use of the complementary information from other modalities. Nevertheless, the presence of these auxiliary modalities is not demanded during inference. To empirically investigate the effectiveness and robustness of the proposed framework, we perform extensive experiments on the two benchmark databases RECOLA and OMG-Emotion for the tasks of dimensional emotion regression and categorical emotion classification, respectively. The obtained results show that the proposed framework significantly outperforms related baselines in monomodal inference, and are also competitive or superior to the recently reported systems, which emphasises the importance of the proposed crossmodal learning for emotion recognition.

SECTION 1Introduction
Automatic emotion recognition endows machines with the capability of natural and empathic communication with humans. Thus, it is considered to be essential to sustain long-term human–machine interactions and is even critical in shifting the present artificial intelligence into the next generation enhanced with emotional characteristics [1], [2], [3], [4]. Over the past decades, significant advances have been made to improve the accuracy and robustness of emotion recognition systems in either a monomodal or multimodal scenario.

The monomodal emotion recognition systems normally independently explore the prominent features for the emotions of interest, from one specific modality, such as audio, video, image, text, or physiology [5], [6], [7], [8]. With the advent of deep learning, such modality-specific systems have continually achieved promising performance [9], [10], [11]. In contrast, the multimodal systems tend to jointly utilise several modalities, with an aim to take advantage of complementary or supplementary information from different media cues [2], [12], [13]. Benefited from this, such systems have been consistently evaluated to be superior to the monomodal systems in numerous previous works [14], [15], [16].

Albeit the notable advantages, in the evaluation phase, most of the multimodal emotion recognition systems require the synchronous presence of all modalities that are employed in the previous training phase [12], [13], [14], [15], [16]. This severely impedes their application in real life, since it is a common case that information from some particular modalities is missing. For example, a camera could be not always fixed in front of a user, or could not work in darkness, which results in invalid or missing visual signals. Likewise, a user could be silent although she/he is emotional, leading to the missing of audio data. The absence of any involved modalities often leads to corruption or performance degradation of a pre-trained multimodal system [2].

A straightforward way to address this issue often makes use of the integration of an additional component, such as a voice activity detector and a face detector, in front of the multimodal recognition systems [2]. Once the absence of a particular modality is detected, the prediction process can be automatically re-directed to another system that is trained via an accordingly reduced number of modalities. Nevertheless, such a system is normally inferior to the system with all modalities as aforementioned.

To embrace the advantages and avoid the shortages of both systems, in this article, we propose a novel crossmodal Emotion emBedding framework, namely EmoBed. The underlying idea of this framework is to transfer the knowledge from other auxiliary modalities to a target modality, in order to enhance the performance of a monomodal emotion recognition model. Basically, it consists of two main processes: thejoint multimodal training process and thecrossmodal training process. The former process utilises the data from multiple modalities to jointly train a shared network, with an assumption that the knowledge from different modalities could be implicitly transferred to or fused by the network. Meanwhile, the latter process utilises a triplet loss [17], [18] to minimise the distance of intra-class representations while maximising the inter-class ones, regardless of their corresponding modality types. In doing this, it forces the extracted high-level representations cross modalities shared a similar space, where the intra-class representations have a close distance while the inter-class ones have a long distance.

This framework holds two main advantages compared with traditional multimodal emotion recognition systems [12], [13], [14], [15], [16]. First, it only requires the data from auxiliary modalities in the training stage, where the knowledge is supposed to be transferred to the target modality. In the inference stage, the auxiliary modalities are not needed anymore. Therefore, it overcomes the synchronous presence problem of traditional multimodal systems. Second, when training the network, it does not demand a paired data. That is, the signals from different modalities are unnecessarily time-aligned. Thus, the data from heterogeneous corpora (i.e., modality mismatch) can be used for training our framework. This advantage largely releases the signal-alignment requirement of traditional multimodal systems.

Furthermore, our work is partially inspired by the multi-task learning paradigm, where multiple tasks are jointly trained with a shared network and several task-specific networks. It has been repeatedly demonstrated including in affective computing that such a learning process can lead to a better generalisation of the representations learnt from the shared networks [19], [20], [21], [22], [23]. Similarly, in this work, we assume that multiple modalities could also benefit for training a monomodal framework, through optimising the parameters of a shared network.

Overall, the major contributions of this work include: (i) We propose a novel learning framework – EmoBed – to explore knowledge from auxiliary modalities for an emotion recognition system; (ii) we jointly train the network with the heterogeneous data, which, however, is unnecessary to be paired; (iii) we extract modality-invariant emotion embeddings in a latent space via a triplet loss. Although triplet loss has been implemented in the emotion recognition literature [24], [25], it was merely utilised to distil discriminative representations within speech signals, which differs from the proposed work that aims to distil the modality-invariant emotion embeddings; finally, (iv) we comprehensively investigate the effectiveness and robustness of the model for both dimensional continuous emotion regression and categorical discrete emotion classification. Note that, for the sake of clarification, we define the traditional emotion recognition system without any crossmodal training as a classic emotion recognition system, whereas the system enhanced by our proposed crossmodal technologies as an enhanced emotion recognition system. Despite the fact that our proposed approaches can be used for more than one modality, for the sake of simplicity, in the present article, we mainly focus on the visual and audio modalities for emotion recognition, as cameras and microphones are pervasive in the world. Thus, the audio-only or video-only based system is assumed to be monomodal system.

The remainder of this article is structured as follows. First, we brief related works for crossmodal and multimodal emotion recognition systems in Sections 2. After that, in Section 3, we elaborately describe the proposed EmoBed framework in detail. Then, in Sections 4 and 5, we perform comprehensive experiments on two audiovisual emotional databases, to assess the performance of the proposed approach in both emotion regression and classification tasks. Finally, in Section 6, we draw conclusions and point out some promising related research directions.

SECTION 2Related Work
In this section, we summarise the most related and recently reported works on the crossmodal training and multimodal emotion recognition systems for emotion recognition, respectively.

2.1 Crossmodal Training
Recently, there have been an increasing number of studies to investigate approaches to transfer knowledge across domains or modalities [26], [27], [28], [29]. One notable relevant work is SoundNet [28] for large-scale natural sound representation learning under a student-teacher training procedure. In particular, a teacher vision network can guide the student sound network to recognise acoustic events in an unsupervised learning manner. During training, KL-divergence between the posterior probabilities of the teacher and student networks is minimised, and thus, knowledge is transferred from well established visual recognition models into audio ones. Of more relevance is a recent work by Albanie et al. [30], where the authors proposed to produce aligned embeddings for speech emotion recognition, by distilling the knowledge of a strong teacher network for facial emotion recognition.

Our approach differs from the one proposed by Albanie et al. [30] mainly in two key aspects. First, although the student network is learnt without access to any form of labelled audio, the main limitation of the approach is that it requires a well-trained complex teacher network, which arguably has not yet been developed in emotion recognition. In our method, however, the network is trained from scratch. Second, the results reported in the work [30] indicate that the performance of the student network falls short of the performance of the teacher network, and cannot compete with a fully supervised network. In contrast, our primary focus goes to provide additional knowledge from a second modality to assist the targeted modality. As a consequence, the performance of our approach is expected to be superior to a fully supervised monomodal network.

2.2 Multimodal Emotion Recognition
As mentioned in Section 1, multimodal fusion approaches have been widely conducted to exploit complementary information, to improve the performance and robustness of emotion recognition systems measurably [14], [31]. To this aim, various data fusion strategies have been extensively utilised, and they normally can be classified into three categories, namely, feature-level fusion, decision-level, and model-level fusion [2]. Typically, feature-level fusion (also known as early fusion) straightforwardly concatenates audio and visual features into one combined feature vector, which is then used as the input for modelling [32], [33]. In contrast, decision-level fusion (also known as late fusion) combines the predictions, rather than the features, from the modality-specific models for a final decision by the use of certain suitable criteria [32], [34]. In addition, model-level fusion fuses the intermediate representations instead, and in this manner learns to model the potential hidden correlations among features [14], [35], [36]. Compared with the former two fusion strategies, model-level fusion is supposedly more bio-inspired from the cognitive perspective [37]. Overall, these three fusion strategies are practical and helpful, to different extents, for audiovisual emotion recognition.

However, these approaches focus on multimodal scenarios and rely heavily on the existence of signals from all sensors. In contrast to these works, we exploit the hidden correlation of multiple modalities in an implicit fusion manner, and thus it later can be implemented in a more flexible setting, as information from auxiliary modalities is not required during inference.

SECTION 3Crossmodal Emotion Embedding System
In this work, we aim to attain a shared embedding space to explore the latent correlation between audio and video signals for monomodal emotion recognition, and the training stage of the proposed EmoBed framework is depicted in Fig. 1. Typically, after extracting audio and video descriptors via several standard and essential processing steps, we jointly train two modality-specific networks to project these multimodal descriptors into a common space, the representations of which can then be applied to predict emotions.


Fig. 1.
The proposed crossmodal Emotion emBedding (EmoBed) framework for monomodal emotion recognition.

Show All

Mathematically, the two embedding functions fA:RM→RE and fV:RN→RE, aim at mapping audio inputs in RM and visual inputs in RN onto embedded representations accordingly in a shared coordinate space RE. To learn such embedding functions, we first introduce joint training with audiovisual data in Section 3.1. Moreover, to learn useful semantic representations, we further employ crossmodal triplet loss in the learning process in Section 3.2. Lastly, the proposed EmoBed framework is given in Section 3.3, by integrating the merits of the joint training and the crossmodal training.

3.1 Joint Training with Audiovisual Data
In this subsection, we demonstrate how to learn a common embedding space for monomodal emotion recognition with joint training loss using audiovisual emotional data. Before that, we first briefly differentiate our joint training from other related structures in the following paragraphs, as depicted in Fig. 2.


Fig. 2.
Structure comparison among the proposed joint audiovisual training (e), and other related multimodal learning frameworks (i.e., early fusion (a), late fusion (b), model-level fusion (c)), and multi-task learning (d).

Show All

Three conventional multimodal fusion paradigms are demonstrated in Figs. 2a, 2b, and 2c. Note that, although information of multiple modalities is fused at different levels, they all contribute to multimodal emotion recognition systems. Concretely, given x(⋅), e(⋅), and y(⋅) denoting the monomodal input feature, the learnt hidden-layer representation, and the output prediction for audio A and video V, respectively, the combination of audio and video knowledge is in forms of [xA;xV] for feature-level fusion, weighted averaging based on yA and yV for decision-level fusion, or [eA;eV] for model-level fusion. It should be noted that, these models can be utilised, if and only if both xA and xV are available as inputs of the model, and there is no need for eA and eV to be of the same dimensions. In the proposed joint training model, albeit the constraint of the existing of both xA and xV remains during training, the model can then be applied under a monomodal setting.

Furthermore, our model is also dissimilar to multi-task learning, which is illustrated in Fig. 2d. In multi-task learning, during the training phase, an auxiliary task benefits the main task by updating the parameters in the shared front-end feature-learning network. In contrast, in the proposed model, we assume inputs of an auxiliary modality can help improve the emotion prediction of the main modality, by optimising the parameters of the shared back-end predicting network.

Let us denote an audio feature vector as xA∈RM and its corresponding visual feature vector as xV∈RN, where M and N are the dimensions of the audio and visual vectors, respectively. As depicted in Fig. 2e, xA and xV are fed into two modality-specific subnetworks, the process of which can be formulated as follows:
eA=fA(xA),eV=fV(xV),(1)
View Sourcewhere the function fA(⋅):RM→RE and the function fV(⋅):RN→RE map each input of different modalities into the same subspace, resulting in corresponding E-dimensional representations eA and eV. After that, the following shared layers are applied to estimate the final predictions, and this process can be formulated as follows:
yA=f(eA),yV=f(eV),(2)
View Sourcewhere the function f(⋅):RE→R estimates final predictions yA and yV, separately.

To efficiently aggregate the advantages of different modalities for monomodal emotion recognition (i.e., speech emotion recognition or facial emotion recognition), the model is trained with a set of audiovisual features {(xA,xV)}. When the model is applied for speech emotion recognition, the joint loss function J(θ) is calculated by:
J(θ)=LA+α⋅LV,(3)
View Sourcewhere θ denotes the network parameters to be optimised, LA and LV stand for the loss of audio and video data, respectively, and α denotes the weight of the video prediction loss to regulate its contribution to J(θ). The term α⋅LV enforces the optimisation to take the auxiliary modality information into account. Similarly, for facial emotion recognition, the joint loss function in Eq. (3) is altered into
J(θ)=LV+α⋅LA.(4)
View SourceMoreover, the value of α is optimised on the development set, by achieving the best performance for the selected modality.

3.2 Crossmodal Triplet
The primary focus of this subsection is to learn emotion-discriminative embeddings using crossmodal data via a triplet loss function. In general, triplet loss forces to project the original descriptors into a latent space where instances with similar semantics are pulled together while instances with dissimilar semantics are pushed away. Consequently, the similarity of instances with the same semantic information is preserved in the learnt representations. While the triplet constraint has been investigated intensively for images and text [38], [39], they are now starting to gain traction in audio and video studies [24], [40]. Motivated by these successes, we propose a crossmodal triplet framework by adopting crossmodal triplet loss to supervise the learning process.

To form a triplet tri={e,e+,e−}, for the embedding of a given instance e, we select embeddings of another two instances, i.e., e+ and e−. In particular, {e,e+} (denoted as a positive pair) are embeddings from the same class, with {e,e−} (denoted as a negative pair) belonging to different classes. To this end, we first calculate the semantic similarity of paired instances over a batch of embeddings. Typically, given a pair of embeddings {e,e′}, their similarity D can be computed as follows:
De,e′=∥e−e′∥2,(5)
View Sourcewhere ∥⋅∥2 denotes the euclidean distance between two embeddings in the pair. Therefore, a pairwise euclidean distance matrix can be generated by computing the distance between all embeddings. Note that, the diagonal of the obtained matrix is null, as the distance between one embedding and itself is zero.

Subsequently, for each e, we explore the distance matrix to find e+ and e− from the batch, to form the hardest positive pair {e,e+} and the hardest negative pair {e,e−}, by considering their emotion labels accordingly. Particularly, e+ is one embedding which has the maximum distance from e, among all embeddings having the same label as e. Conversely, e− is another embedding which has the minimum distance from e, among all embeddings with different labels from e.

Once e+ and e− are obtained and applied together with e to construct the hard triplet for each embedding, the triplet loss constraint LT can be estimated over all hard triplets by
LT=∑(Dei,e+i−Dei,e−i).(6)
View Source

Note that, to compute the crossmodal triplet loss LVAT, we first combine the audio embeddings eA and the video embeddings eV, to form a double-sized batch of embeddings in the form of {eA;eV}. Then, a pairwise euclidean distance matrix is obtained by computing the distance between all paired embeddings. Afterwards, for each embedding (either audio or video), another two embeddings are chosen from the same batch, to form a hard triplet. It is worth mentioning that, when generating the hardest positive or negative pair, we take both the intermodal and intramodal similarity into consideration. In this manner, the learning process forces the model to narrow the distribution gap of embeddings from different modalities, and to keep the specific emotional semantics intact in the meantime.

Supervised by the crossmodal triplet loss LVAT, the model is forced to minimise the optimisation objective J(θ), which can be formulated as:
J(θ)=Lmon+β⋅LVAT,(7)
View Sourcewhere Lmon denotes the conventional monomodal discriminative loss, which is, in our case, LV for facial emotion recognition or LA for speech.

3.3 Crossmodal Emotion Embedding
In this subsection, we illustrate the proposed EmoBed framework, which integrates the triplet constraint (see Section 3.2) into the joint training approach (see Section 3.1). The overall training process is demonstrated in Fig. 1.

Generally, after extracting monomodal descriptors from standard pre-processing procedures, embedding functions fA(⋅) and fV(⋅) are estimated by two embedded neural networks, respectively, which project audio and video descriptors into a common latent space. Subsequently, the audio and visual embeddings are fed into a shared emotion recognition neural network, which is trained via a joint training loss. Concurrently, the training process is supervised by the triplet loss of the audio and visual embeddings.

Mathematically, when applying the EmoBed framework for audio emotion recognition, the objective function can be formatted as:
J(θ)=LA+α⋅LV+β⋅LVAT+λ⋅R(θ),(8)
View Sourcewhere LA and LV represent the discriminative loss function of audio and visual data, respectively, while LVAT represents the triplet loss function of both audio and visual data. Moreover, the hyperparameters α and β are introduced to weight the contribution of the video data and the triplet loss. Furthermore, λ is applied to control the importance of the regularisation term R(θ) (L2). Similarly, when training the EmoBed framework for facial emotion recognition, the objective function in Eq. (8) can be modified by exchanging LA and LV.

After the model has finished training, the components associated with the auxiliary modality can be discarded, while the rest is retained and utilised to recognise emotions.

SECTION 4Experimental Implementation
To evaluate our approach comprehensively, we conducted extensive experiments on two multimodal emotional datasets for two tasks, respectively. Specifically, the RECOLA dataset was chosen for dimensional emotion regression, whereas the OMG-Emotion dataset for categorical emotion classification. In this section, we briefly introduce the two datasets. Then, we describe the experimental setup in detail for the sake of experiment replication, and also the evaluation metrics for a performance comparison.

4.1 Evaluated Databases and Features
4.1.1 RECOLA
This database is widely used for audiovisual dimensional emotion recognition, and a standard database which was previously applied in the AVEC since 2015 [41], [42]. It contains audiovisual recordings of spontaneous and natural interactions from 27 French-speaking participants in order to investigate socio-affective behaviours in the context of remote collaborative tasks. Moreover, time- and value-continuous dimensional emotion annotations in terms of arousal and valence are given with a constant frame rate of 40 ms for the first five minutes of each recording, by averaging all six annotators and meanwhile taking the inter-evaluator agreement into consideration [43]. The dataset is further equally divided into three disjoint parts, by balancing the gender, age, and mother tongue of the participants. Therefore, each part consists of nine unique recordings, resulting in 67.5 k segments in total for each part (training, development, or test).

When conducting experiments on RECOLA, we employed the same acoustic and visual features as the features utilised to compute the AVEC 2015 and 2016 baselines [41], [44], for a fair comparison with other methods. Specifically, as for the acoustic features, the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS [45]) was extracted on all audio recordings by our open-source openSMILE toolkit [46]. This resulted in a set of 88 acoustic features per segment.

In relation to the visual features, we utilised both the appearance and geometric standard features of the AVEC challenges. That is, we investigated handcrafted video features rather than learnt features from a pre-trained VGG-Face net as shown in Fig. 1, as the inputs of visual embedding nets. This is for a fair comparison with other methods. Similar to the acoustic features, the arithmetic mean and the standard derivation were computed over the sequential handcrafted visual features of each frame using a sliding window of 8 s with a step size of 40 ms. This process led to 168 appearance and 632 geometric visual features.

4.1.2 OMG-Emotion
In our experiments, the One-Minute Gradual-Emotional (OMG-Emotion) Behavior dataset [47] was employed for categorical emotion classification. The OMG-Emotion dataset is composed of 567 emotional monologue videos collected from Youtube, with an average length of one minute. These videos were then divided into utterance-level clips, and annotated by at least five annotators [47]. Seven categorical emotions were considered, i.e., neutral, happiness, sadness, anger, surprise, fear, and disgust. Majority voting was then applied to compute the gold standard based on all annotations of the same segment. Moreover, the dataset is split into training, development, and test sets, resulting in 2 440, 617, and 2 229 segments for each partition, respectively. Note that, in this work, we performed experiments and reported performances only on the development set, as labels of the test set are not yet accessible.

To extract acoustic features on the OMG-Emotion dataset, we used the eGeMAPS feature set [45], resulting in 88 features for each utterance. For visual descriptors, first, a multi-task cascaded CNN [48] was applied for face detection and alignment on each frame. After that, frame-level intermediate deep features of size 4 096 were extracted from the “fc-7” layer of the VGG-Face model [49], which was pre-trained on a large number of facial images. Lastly, average pooling was conducted on all frames of the same clip to deliver the final utterance-level video descriptors.

4.2 Experimental Setup
The proposed EmoBed networks were implemented with GRU-RNNs. Compared with LSTM units, the employed GRUs have fewer parameters owing to the lack of separate memory cells and output gates, which results in a faster training process and a less-training-data demand for achieving a good generalisation [50]. Moreover, many empirical evaluations have shown that GRUs perform as competitively as LSTM units [51].

For the RECOLA experiments, we fixed the number of hidden layers for the modality-specific subnetworks (i.e., for audio and video) and the modality-shared subnetwork to be two, respectively. Each hidden layer has 120 hidden nodes. To train the network, we utilised the Adam optimisation algorithm with an initial learning rate of 0.001. Moreover, we employed a regularisation term (L2) with a weight decay of 10−4, to improve the model generality. Furthermore, to facilitate the training process, we set the mini-batch size to be four. Additionally, an online standardisation was always applied to the input data by using the means and variations of the training set. All these settings were empirically recommended by our previous work on the RECOLA database after a grid search evaluation strategy [15].

For the OMG-Emotion experiments, we kept in line with the network and the training hyper-parameters, but used one hidden layer as the modality-specific or the modality-shared subnetworks due to the limited size of the OMG-Emotion dataset, and used 64 as the mini-batch size.

When training the network in a crossmodal scenario, we randomly chose the audio and video data, rather than the aligned data pair across audio and video, as the mini-batch. The advantage of this method is that, it does not require the synchronous presence of both modalities in the training phase. This means that we can principally mix the audio-only and video-only databases to complete the network training process.

Additionally, when continuously assessing emotions, the annotators have to take time to perceive acoustic events, understand them, and report the emotional states [52]. To address this annotation delay problem, we took a widely used explicit compensation approach. That is, we shifted the gold standard back in time (i.e., 2.4 s) with respect to the features for all modalities and tasks [41], with an assumption that the delay is invariant with annotators, annotator states, modalities, and tasks.

To refine the obtained predictions for continuous emotion recognition, we further performed a chain of post-processing, including median filtering, centring, scaling, and time-shifting, as suggested by Valstar et al. [41]. The filtering window size W (from 0.12 s to 0.44 s at a rate of 0.08 s) and the time-shifting delay D (from 0.04 s to 0.60 s at a step of 0.04 s) were optimised using a grid search method. All the post-processing parameters were optimised on the development set and then applied to the test set.

4.3 Evaluation Metrics
To evaluate the performance of the continuous emotion regression model, we took the Concordance Correlation Coefficient (CCC), which was officially recommended by the AVEC 2015-2018 challenges [41]. The CCC is defined by:
rc=2rσxσyσ2x+σ2y+(μx−μy)2,(9)
View Sourcewhere r represents Pearson’s correlation coefficient (PCC) between two time series (e.g., prediction and gold-standard), μx and μy denote the mean of each time series, and σ2x and σ2y stand for the corresponding variances. Compared with PCC, the CCC considers not only the shape similarity between the two series but also the value precision. This is especially relevant for estimating the performance of time-continuous emotion prediction models, as both the trends as well as absolute prediction values are relevant for describing the performance of a model. The CCC metric falls into the range of [−1, 1], where +1 represents perfect concordance, −1 total discordance, and 0 no concordance at all.

As to the discrete emotion classification tasks, we chose F1 as the evaluation metric, mainly due to the fact that i) it can provide an overview performance in a multi-class setting as it is calculated by the harmonic mean of unweighted precision and recall; ii) it was employed by the OMG-Emotion challenge in 2018 [47]. In general, a higher CCC or F1 indicates a better prediction performance.

Further, to evaluate the statistical significance of performance improvement, unless stated otherwise, we undertook a Fisher r-to-z transformation [53] for dimensional continuous emotion regression and a one-tailed z-test for categorical discrete emotion classification. Only if the p-value was lower than .05, a significant difference was triggered.

SECTION 5Experimental Results and Discussions
For the sake of fair performance comparison, in this section, we report on conducted experiments on two emotional databases (i.e., RECOLA and OMG-Emotion) with their corresponding standard testbeds of the AVEC 2016 [41] and OMG-Emotion challenges 2018 [47].

5.1 Results on RECOLA
As suggested in [41], for these experiments, we took two visual feature sets (i.e., appearance and geometric) and one acoustic feature set (i.e., eGeMAPS), as aforementioned in Section 4.1.1. This leads to four experimental scenarios: audio (+video-app.), video-app. (+audio), audio (+video-geo.), and video-geo. (+audio), where the modalities shown in the parentheses indicate the employed auxiliary modalities for the corresponding single modalities. In addition, we evaluated the systems on both dimensional arousal and valence regressions as well. For the classic monomodal systems, we independently performed the training process on either audio or video data. That was achieved by setting α to be 0.0 in Eqs. (3) and (4), respectively.

The obtained results for arousal and valence predictions are presented in Tables 1 and 2, respectively. From the two tables, it can be seen that our classic monomodal systems outperform the challenge benchmarks that used the ‘SVR + offset’ approach [41] in most cases. One exception is the arousal prediction with audio signals, which probably is attributed to the fact that a fixed network structure, rather than the optimised one on the arousal prediction, was employed in our experiments (see Section 4.2). These results further confirm that GRU-RNNs hold the powerful capability to capture long-term context dependence.

TABLE 1 Performance Comparison in Terms of CCC for the Arousal Prediction Among the Proposed EmoBed Systems, Related Baselines, and Other State-of-the-Art Systems

TABLE 2 Performance Comparison in Terms of CCC for the Valence Prediction Among the Proposed EmoBed Systems, Related Baselines, and Other State-of-the-Art Systems

Furthermore, when jointly training audio and video data (see Section 3.1), one may notice that the corresponding monomodal systems (based on either audio-eGeMAPS, video-appearance, or video-geometric) can deliver higher CCCs compared with the classic monomodal systems, in seven out of eight cases for the arousal prediction and six out of eight cases for the valence prediction, respectively. This observation implies that such a joint training process can somewhat transfer shared semantic information from other heterogeneous data to the target modality thanks to the implementations of i) a shared subnetwork and ii) a multi-task learning framework.

Rather than the joint audiovisual training, when performing the triplet constraint across the audio and video modalities (see Section 3.2), the obtained CCCs (the third lines in Tables 1 and 2) show that the introduced enhanced monomodal systems again generally significantly outperform the classic monomodal systems in most cases (Fisher r-to-z transformation, p<.05). This suggests that the implementation of the triplet constraint is helpful to distil emotional discriminative representations, not only in a monomodal scenario [24] but also in a crossmodal scenario as investigated in this article.

Finally, when we simultaneously carried out the crossmodal triplet training as well as the joint audiovisual training processes, it can be seen that the EmoBed systems achieve the best performance in most cases, i.e., six out of eight cases for the arousal regression and five out of eight cases for the valence regression. For example, the obtained CCCs on the test set of the audio-based model are boosted from .605 to .644 and .639 for arousal regression, and from .381 to .434 and .439 for valence regression, when respectively integrating with video-appearance and video-geometric feature sets in the training process. Furthermore, the best CCCs achieved by the video-based models reach to .475 and .417 with appearance and geometric feature sets, respectively, with absolute CCC increases of .064 and .018 compared with the classic monomodal systems for arousal regression. Such an observation, however, cannot be found for the video-based valence regression models on the test set. This exception possibly is attributed to the distribution mismatch between the development and test partitions. Overall, it is concluded that the proposed EmoBed can largely supply additional knowledge from audio signals to alleviate the shortage of video signals, and vice verse.

Meanwhile, as presented in Tables 1 and 2, the EmoBed systems achieve comparable or better performance to other state-of-the-art methods, such as the end-to-end systems [54], curriculum learning systems [56], multi-task learning systems [21], and Dynamic Difficulty Awareness Training (DDAT) systems [15]. Although most of these reported systems utilised a variety of multimodal fusion approaches to boost their final performance, these systems, in the reference stage, demand the simultaneous occurrence of all modalities that appear in the training stage, to the best of our knowledge. It also has to be noted that the best achieved results are delivered by using the end-to-end system for the arousal predictions when using audio signals (i.e., .715 of CCC) and for the valence predictions when using video signals (i.e., .620 of CCC). This observation further confirms that optimising the whole network from the raw signals to the target can benefit the efficient extraction of high-level representations. Therefore, in the future we will further implement the EmoBed system in an end-to-end fashion, which is expected to further enhance the performance of monomodal emotion recognition.

5.2 Results on OMG-Emotion
For our experiments on the OMG-Emotion database, we conducted seven-class categorical emotion classification tasks on audio and visual signals. Table 3 presents the performance of the models in terms of F1 on the development set (note that the annotations of the test set are not publicly available). From the table, we can see that on this database, our classic monomodal models outperform the other methods reported in the literature [47], i.e., Support Vector Machine (SVM) and Random Forest (RF). More specifically, our classic monomodal models yields higher F1 than SVM (36.5 versus 33.0 percent) for audio, and than RF (37.9 versus 37.0 percent) for video.

TABLE 3 Performance of the Proposed EmoBed Systems, Related Baselines, and Other Reported Systems in Terms of F1 on the Development Set of the OMG-Emotion Dataset

Additionally, comparing the proposed joint audiovisual training models with the classic monomodal systems, it is noticed that the former approach outperforms the latter one by a large margin, i.e., 40.2 versus 36.5 percent for audio and 42.1 versus 37.9 percent for video. These experimental results again indicate that, the proposed joint audiovisual training approach is plausible to promote performances of monomodal emotion classification. Furthermore, similar results are also obtained when utilising the triplet training approach to distil the salient representations across multiple modalities. Nevertheless, the highest F1s are achieved by means of the EmoBed systems, which deliver 5.2 and 6.0 percent absolute performance gain compared with the classic monomodal systems when using audio or video signals, respectively. All these observations further confirm the findings discovered from the RECOLA database.

5.3 Visualisation of Emotion Embeddings
To investigate how the proposed crossmodal learning framework benefit emotion recognition, we extracted the learnt representations from the classic monomodal systems and the proposed EmoBed systems. Fig. 3 illustrates the distribution of the learnt representations on the development set of the RECOLA database by means of t-Distributed Stochastic Neighbour Embedding (t-SNE). It can be seen that with the classic monomodal systems, the learnt representations can be easily distinguished into three parts by the modalities they stem from, in either arousal (cf. Fig. 3a) or valence (cf. Fig. 3c) prediction. Specifically, the representations learnt from different modalities almost have no overlap albeit they belong to the same emotion states. In stark contrast, the representations extracted from EmoBed systems are visibly clustered together based on their emotional properties (cf. Figs. 3b and 3d for arousal and valence, respectively).


Fig. 3.
Visualisation of the learnt representations of the development set of the RECOLA database when using the proposed EmoBed systems or the classic monomodal systems. Red, green, and yellow markers: representations from audio (eGeMAPS), video (appearance), and video (geometric) modalities; circle and cross markers: high and low arousal/valence.

Show All

Such an observation is even more noticeable on the OMG-Emotion database (cf. Fig. 4). Note that, for the sake of simplicity, we merely chose two emotional categories (i.e., sad and neutral) for visualisation. Likewise, one can find that the representations belonging to the same emotional category share almost the same latent space.


Fig. 4.
Visualisation of the learnt representations of the development set of the OMG-Emotion database when using the proposed EmoBed systems or the classic monomodal systems. Red and green markers: Representations from audio and video modalities; circle and cross markers: neutral and sad categories.

Show All

These findings indicate that the representations learnt by the proposed EmoBed are somewhat invariant to the modalities. By making use of the emotion embedding space, the emotional representations extracted from audio and video signals are able to implicitly fuse the knowledge from each other. Thus, the exploitation of mutual information possibly leads to performance improvement for a monomodal system.

5.4 Discussion
To demonstrate the importance of the learning from auxiliary modalities for the monomodal emotion recognition system, we independently investigated the impact of weight change with respect to the counterpart modality, when in a joint audiovisual training process or in a crossmodal triplet training process.

Figs. 5a and 6a depict the relationship between the obtained CCCs and the weight α (cf. Eqs. (3) and (4)) on the RECOLA database. It is noted that the model performance is improved when the weight increases to some values for the video-based arousal regression models (green and cyan lines in Fig. 5a). Similar observations can be made as well for the audio-based valence regression models (blue and red lines in Fig. 6a). Therefore, this behaviour again indicates that learning from other modalities indeed can help the enhancement of traditional monomodal systems. Yet, it is also noted that the audio-based arousal and the video-based valence regression models almost remain without obvious performance improvement. This might suggest that transferring the information from the modality with richer knowledge to the one with sparse knowledge is much easier than the other way around, as audio signals often lead to higher CCCs for arousal regression while video signals for valence regression.

Fig. 5. - 
Impact of the joint auxiliary modality loss on the joint audiovisual training systems (a), and impact of the crossmodal triplet loss on the crossmodal triplet training systems (b), with the RECOLA database for arousal regression. The best performed $\alpha$α or $\beta$β is indicated in each case.
Fig. 5.
Impact of the joint auxiliary modality loss on the joint audiovisual training systems (a), and impact of the crossmodal triplet loss on the crossmodal triplet training systems (b), with the RECOLA database for arousal regression. The best performed α or β is indicated in each case.

Show All

Fig. 6. - 
Impact of the joint auxiliary modality loss on the joint audiovisual training systems (a), and impact of the crossmodal triplet loss on the crossmodal triplet training systems (b), with the RECOLA database for valence regression. The best performed $\alpha$α or $\beta$β is indicated in each case.
Fig. 6.
Impact of the joint auxiliary modality loss on the joint audiovisual training systems (a), and impact of the crossmodal triplet loss on the crossmodal triplet training systems (b), with the RECOLA database for valence regression. The best performed α or β is indicated in each case.

Show All

Further, Figs. 5b and 6b illustrate the relationship between the obtained CCCs and the weight β (cf. Eq. (7)) on the RECOLA database. Obviously, it can be seen that the obtained CCCs remarkably grow with the increase of weight β in all cases for arousal (cf. Fig. 5b) and valence (cf. Fig. 6b) regression. Specifically, when β=1.0, i.e., the triplet training contributes equally as the traditional emotion regression training, the systems yield the best CCCs in all cases for arousal regression. Nevertheless, the audio- and video-based valence regression systems deliver the best CCCs only when β=0.4 and β=0.8/1.2, respectively. The lower contribution from triplet loss implies that it might be more difficult to distil the valence-salient representations than the arousal-salient representations by means of triplet training.

Moreover, we conducted a similar investigation on the OMG-Emotion database for categorical emotion classification. Fig. 7 explicitly quantifies the contributions of joint audiovisual training (a) and crossmodal triplet training (b) when in a crossmodal learning framework. For a joint audiovisual training system, when α=0.0, i.e., no contribution from the auxiliary modality, the model is learnt based on only the loss of each modality, separately. When α increases, i.e., the contribution of the auxiliary modality during training increases, the performance of monomodal emotion recognition (audio or video) is improved first, until a point where the contribution of the auxiliary modality might actually penalise the learning objective too much and even harm the learning of the main modality, and thus performances start to decrease. Similar observations can be found for crossmodal triplet training systems.


Fig. 7.
Impact of the joint auxiliary modality loss on the joint audiovisual training systems (a), and impact of the crossmodal triplet loss on the crossmodal triplet training systems (b), with the OMG-Emotion database. The best performed α or β is indicated in each case.

Show All

To this end, proper values of the weight α and β need to be identified for the tasks at hand. We can observe from the figure that, the best performance for both audio and video emotion classification is reached when α=0.5 in joint audiovisual training systems; whereas the best performance for audio and video emotion classification is achieved when β=0.3 and β=0.8, respectively, in crossmodal triplet training systems.

SECTION 6Conclusion
Different from previous emotion recognition works which have focused on either the traditional monomodal (i.e., modality-specific) systems or multimodal systems, in this article, we proposed an enhanced system through exploring the information across auxiliary modalities. To implement this system with exemplary audio and visual modalities, we, on the one hand, utilised a shared emotion recognition network for both audio and video data, so that the complementary information from an auxiliary modality can be implicitly transferred to the target modality. On the other hand, we applied a triplet constraint over acoustic and visual representations to distil emotional embeddings that are invariant to the modalities. The proposed learning frameworks were systematically evaluated on the two benchmark databases RECOLA and OMG-Emotion. Experimental results have demonstrated that the proposed methods significantly improve the prediction performance of a monomodal system, by fusing an additional modality in the training process.

Albeit the efficiency, the proposed learning framework could be further developed in the future. For example, in the triplet training process, the annotation uncertainty information could be utilised as a new distance measure between the learnt representations. In addition, it is also worth to train the model by using large-scale heterogeneous datasets from a variety of domains. Moreover, in this work, we independently conducted the feature extraction process before performing the crossmodal learning process. It might be helpful if we further combine the two processes together as an end-to-end framework.