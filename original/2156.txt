Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level (what the constituents are) and clip-level (how those constituents are organized) . We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by selecting the data to visualize instead of manually drawing the graphical marks . Our system can be generalized to other racket sports (e.g ., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.

SECTION 1Introduction
Video is a popular medium for presenting and broadcasting sports events. In recent years, thanks to the advance of techniques such as Computer Vision (CV) and Image Processing (IP), there has been a growing practice of augmenting sports videos with embedded visualizations [16]. These augmented sports videos combine visualizations with video effects (e.g., slow motion, camera rotation) to embed the data in the actual scenes (Fig. 1c). Given the ability to explicate player strategies in an engaging manner, augmented sports videos have been used widely by TV channels [15] to engage the fans and by sports analysts [33], [41] to communicate analytical findings.

Fig. 1: - Left: A raw sports video. Middle: viscommentator extracts data from the raw video by using machine learning models, allows users to select the augmenting data by directly interacting with the objects in the video, and suggests visuals for the data to generate an augmented video. Right: an augmented video reveals the probability distribution of the ball placement on the table.
Fig. 1:
Left: A raw sports video. Middle: viscommentator extracts data from the raw video by using machine learning models, allows users to select the augmenting data by directly interacting with the objects in the video, and suggests visuals for the data to generate an augmented video. Right: an augmented video reveals the probability distribution of the ball placement on the table.

Show All

Despite their popularity, these augmented videos, by nature, are difficult to create, especially for sports analysts, who usually lack sufficient video editing skills compared to the proficient video creators at TV companies. Based on our collaboration with sports analysts who provide data analysis services for national sports teams, we are informed that augmenting videos are particularly useful for sports analysts, allowing them to effectively communicate the analytical insights to the coaching team and players. However, sports analysts usually cannot create these videos as the creation process involves complex design decisions and video editing. Thus, fast prototyping tools that allow sports analysts to create augmented videos rapidly are in great demand.

Various systems and techniques have been proposed to augment sports video over the last years. On the one hand, industrial companies have developed commercial tools, such as Viz Libero [46] and Piero [4], for TV sportscasts. Yet, these tools target proficient video editors and focus on the editing of graphical elements. On the other hand, although increasing research has been conducted on sports data visualization, visualizing data in sports videos has received relatively less attention from the community [33]. A notable exception comes from Stein et al. [40], [41], who developed a system that automatically extracts and visualizes data from and in soccer videos. Nevertheless, except for soccer, there are no publicly available methodologies and tools that help sports analysts create augmented sports videos by providing data-driven features (e.g., mapping and binding between data and visuals, visualizations recommendation).

To facilitate the creation of augmented sports videos, we are particularly interested in four questions that should be considered at two levels. The first level, namely, element-level, is about identifying the building blocks for augmenting sports videos. Similar to all data visualizations, the building blocks of augmented sports videos include both the data and visuals: what kinds of data can be used to augment a sports video (Q1)?; what visuals are used to present these data (Q2)? The next level, clip-level, is about the organization of these building blocks: how to organize and present the data for various narrative purposes (Q3)?; how to organize the visuals in temporal order with respect to the raw video (Q4)? Without regard for these considerations, the visualizations embedded in the fast-moving video will easily overwhelm the audience, let alone engage them.

In this work, we focus on ball sports and aim to facilitate the creation of augmented videos for sports analysts. To understand the design practices of augmented sports videos, we first systematically reviewed a corpus of 233 examples collected from reputable sources, such as TV channels, leagues, and teams. Our analysis resulted in a design space that characterizes augmented sports videos at element-and clip-levels, which answer the aforementioned four questions. The four design dimensions (i.e., Data Type, Visual Type, Data Level, and Narrative Order) of the design space and their combination frequency provide guidance for selecting data insights and visual effects for various purposes, such as entertainment or education. Informed by the design space, we design and implement VisCommentator, a proof-of-concept system that allows sports experts to rapidly prototype augmented table tennis videos. VisCommentator takes a raw table tennis video as the input, extracts the data based on data levels by using machine learning (ML) models, allows users to select the augmenting data by directly interacting with the objects in the video, and suggests visual effects based on the narrative order and data selected by the user (Fig. 1b). Due to the data-driven nature, VisCommentator can be generalized to other racket ball sports such as tennis and badminton once data and the underlying models are available. A reproduction-based user study with seven sports analysts demonstrates the overall usability of the system. We further report the analysts' feedback gathered from the post-study interviews, which implies future improvements and opportunities. In summary, the main contributions of this research include 1) a design space derived from existing augmented sports videos, 2) the design and implementation of a fast prototyping tool for augmenting table tennis videos, and 3) a reproduction-based user study with seven domain experts. The corpus, created videos, and other materials can be found in https://viscommentator.github.io.

SECTION 2Related Work
Video-Based Sports Visualization
Due to its advantages of presenting data in actual scenes, video-based sports visualization has been used widely to ease experts' analysis [41] and engage the audiences [26]. Based on the presentation method, video-based visualizations can be divided into side-by-side [62], overlaid [42], and embedded [41]. This work focuses on embedded visualizations in sports videos.

Perin et al. [33] comprehensively surveyed the visualization research on sports data, indicating that only a few works can be considered as video-based visualizations. Among these few works, a representative example was developed by Stein et al. [41] for soccer. Their system takes raw footage as the input and automatically visualizes certain tactical information as graphical marks in the video. Later, Stein et al. [40] extended their work by proposing a conceptual framework that semi-automatically selects proper information to be presented at a given moment. Recently, Fischer et al. [16] found that video-based soccer visualization systems from the industry are actually ahead of most of the academic research. For example, Piero [4] and Viz Libero [46] are both developed for sportscasting and provide a set of powerful functions to edit and annotate a sports video. Besides, CourtVision [10], developed by Second Spectrum [39] for basketball, is another industry product that automatically tracks players' positions and embeds status information to engage the audiences. In summary, as detailed by Fischer et al. [16], the research on video-based sports visualization is still in its infancy, while the strong market demand has already spawned very successful commercial systems. Nevertheless, these commercial systems target proficient video editors, leading to a steep learning curve for sports analysts. Moreover, they mainly augment the video with graphical elements, while the goal of sports analysts is to augment sports videos with data. Given that very little is known about the design practices of augmented sports videos, it remains unclear how to support augmenting sports videos with data. In this work, we explore this direction and aim to ease the creation by a systematic study of existing augmented sports videos collected from reputable sources.

Data Videos for Storytelling
Data video, as one of the seven genres of narrative visualization categorized by Segel and Heer [36], is an active research topic and has attracted interest from researchers. Amini et al. [2] systematically analyzed 50 data videos from reputable sources and summarized the most common visual elements and attention cues in these videos. They further deconstructed each video into four narrative categories by using the visual narrative structure theory introduced by Cohn [11]. Their findings reveal several design patterns in data videos and provide implications for the design of data video authoring tools. Build upon that, Amini et al. [3] further contributed DataClips, an authoring tool that allows general users to craft data videos with predefined templates. Recently, Thompson et al. [45] contributed a design space of data videos for developing future animated data graphic authoring tools. Cao et al. [6] analyzed 70 data videos and proposed a taxonomy to characterize narrative constructs in data videos. Although these studies provide insights into data video design, our scenario is inherently different from theirs and thus yields new challenges. Specifically, these studies use video as a medium to tell the story of data, whereas we focus on augmenting existing videos with data. An existing video imposes extra constraints on narrative orders, visual data forms, etc. With these constraints, how to visually narrate data in videos remains underexplored. Perhaps the most relevant work is from Stein et al. [40], who annotated the data in soccer videos in a linear way. To fully explore the ways for augmenting sports videos with data, we analyze 233 real-world examples and summarize six narrative orders and their common usage scenarios. The results provide guidance for designing authoring tools for augmented sports videos.

Intelligent Design Tools
Designing visual data stories usually requires the skills to analyze complex data and map it into proper visualizations, both of which require considerable skills. Therefore, to lower the entry barrier to visual data stories, many researchers develop intelligent creation tools to automate or semi-automate the design process. One widely used approach to ease the visual mapping process is templates. For example, DataClips [3] and Timeline StoryTeller [5] use templates manually summarized from existing examples to enable semi-automatic creation of data animation and timeline infographics, respectively. On the basis of Timeline StoryTeller, Chen et al. [9] proposed a method to automatically extract extensible templates from existing designs to automate the timeline infographics design. Besides automating the visual mapping process, some tools further facilitate the data analysis process by automatically suggesting data insights. DataShot [50] adopts an auto-insight technique to recommend interesting data insights based on their significance for factsheet generation. DataToon [21], an authoring tool for data comic creation, uses a pattern detection engine to suggest salient patterns of the input network data.

However, few, if any, tools exist that provide the aforementioned kinds of data-driven support for creating augmented sports videos. The challenges of developing such a tool not only exist in the engineering implementations but also in the integration between the workflows of visualization authoring and video editing. We draw on the line with prior visualization design tools and design VisCommentator to support visualizing data sports videos in a video editing process.

Data Extraction from Sports Videos
Due to the advancement of deep learning on CV, such as object detection [35], segmentation [34], and action recognition [61], more and more data is available to be extracted from a video [25], [53], [59], [60]. Shih [38] presented a comprehensive survey on content-aware video analysis for sports. He summarized the state-of-the-art techniques to parse sports videos from a low-semantic to a high-semantic level: 1) object level techniques extract the key objects in the video, 2) event level recognizes the action of the key object, and 3) conclusion level generates the semantic summarization of the video. Besides models tailored to sports videos, researchers have also developed models [31] to detect and segment humans from general images and videos. A particular challenge in table tennis is that the ball is small (15 pixels on average [47]) and move fast (e.g., 30 m/s). To tackle this challenge, Voeikov et al. [47] proposed TTNet, a convolution-based neural network, to detects the ball in a high-resolution video. Besides, TTNet can further segments the ball and detect ball events such as bounces and net hits. In this work, we composite multiple state-of-the-art machine learning models [17], [44], [47] to extract the data from table tennis videos, such as ball and players' position, actions, events, and key strokes.

SECTION 3Design Space of Augmented Videos
To understand the design practices of augmented videos for ball sports, we collected and analyzed 233 videos from TV channels, teams, and leagues to characterize augmentations at element-level (what the constituents are) and clip-level (how those constituents are organized).

3.1 Methodology
Data Sources and Videos
We collected a video corpus of six popular ball sports, including three team sports (i.e., basketball, soccer, and American football) and three racket sports (i.e., tennis, badminton, and table tennis). Specifically, we searched the videos in Google Videos by using keyword combinations such as “breakdown videos + SPORT”, “analysis video + SPORT”, and “AR + SPORT”, where SPORT is one of the six ball sports mentioned above. The searching processing was repeated recursively on each returned site until no more augmented videos were found. To ensure the quality and representative of the videos, we only included videos that are watched by more than tens of thousands of times and created by official organizations, such as TV companies, sports teams. We also purchased some subscriptions (such as ESPN+, CCTV VIP) to watch member-only videos during the collection. Three of the authors went through the videos to exclude the problematic ones (e.g., with no or only a few augmentations, not ball sports, or not cover a sports event). In this process, we noticed that most of the videos focused on one sports event (e.g., a goal) and thus were less than 3mins. Some videos were sports events collections and too long (e.g., 45mins). Thus, we sliced these videos into pieces to ensure that each piece is shorter than 3mins and includes at least one sports event. To control the diversity of the videos, we randomly sampled a subset of videos from the raw corpus (which contains more than 1000 videos) following this priority: the number of 1) team sports vs. racket sports, 2) different sports types, and 3) different video sources. Finally, our corpus includes 233 videos. Fig. 2a and b present the number of videos of different sports types and with different time duration. Fig. 3 presents some examples from our corpus.

Qualitative Analysis
We conducted a qualitative analysis of this corpus. Specifically, we first followed the definition in [2] to segment the augmented sports videos into clips. In our corpus, a video can have more than one sports event, each of which can be segmented into multiple clips. Three of the authors independently reviewed and segmented the videos following this process: each of them segmented 1/3 videos and verified another 1/3 videos segmented by others. Gathering the reviews resulted in 871 clips. We then probed into the content of 261 (30%) clips to examine the element-level design. The investigation also referred to prior research [2], [33], [45] and derived the code-sets of two element-level dimensions, Data Type and Visual Type. Next, three of the authors independently conducted open coding on the 261 clips to characterize how these visual elements are organized. Disagreements were resolved through multiple rounds of discussion and iterations with another three authors, one of whom was a senior sports analyst and had more than ten years' experience in providing consulting services for national sports teams and sports TV channels, until we reached a Cohen's κ above 0.7 for all codes of videos. Finally, we came to two clip-level dimensions, namely, Data Level and Narrative Order. The design dimensions were further refined in the following process.

Annotations
We further counted the occurrences of these design dimensions in the corpus. Five postgraduate students were recruited to annotate these videos following our codes. All of them were sports fans. We introduced the details of the design dimensions to the five students, asked them to practice annotation on 15 curated example videos, and started the actual annotation when they were confident enough. After the annotation, cross-validation was further conducted. In total, each student annotated 45 videos and validated 45 videos from two others. Questions and discussions were encouraged throughout the process, which helped us refine the design dimensions. Finally, three of the authors scanned through the annotations and calculated the statistics.

3.2 Element-Level Design
The augmentations of a sports video are presented as visual elements. We characterize these visual elements with Data Type and Visual Type.

Dimension I: Data Type
Sports videos are augmented with different types of data. Perin et al. [33] identified three types of data used in sports visualizations: tracking (in-game actions and movements), box-score (historical statistics), and meta (sports rules and player information). Given the in-game nature of sports videos, most of the augmented data on a sports video belongs to tracking data. Thus, we simplify the three types into two:

Tracking Data is the data collected or extracted from a specific game, such as the moving trajectories and the actions of players. In recent years, the advances of CV techniques lead to increased tracking data that ranges from low-level physical data to high-level tactical data. This data is always associated with a specific space and time in the video and can be naturally embedded into the video. Thus, most of the data visualized in augmented videos is tracking data.

Non-tracking Data refers to the data not captured from a specific game, including historical data, rules, and player information. Augmented videos usually provide this data as supplemental information to explain or comment on the situations in the game.

Fig. 2c shows the top-10 frequently presented data types in our corpus. Most of (9/10) the data is tracking data except for name, which belongs to non-tracking data.

Dimension Ii: Visual Type
The data is presented as different types of visuals. Usually, in data videos, data is presented as graphical marks [3], such as bar, pictograph, and map. However, in augmented sports videos, the video content in the raw footage such as players and the court can also be used to encode data. We categorize these visual representations as Graphical Marks and Video Effects:

Graphical Marks are the visual elements added to the raw footage. Augmented sports videos usually present data as primitive marks, such as dots, lines, and areas, while common data videos [2] contain more complex visualizations, such as donut, pie, scatter plot. We also see some dedicated marks that rarely have been used in common data videos, such as spotlight, skeleton, and field of view. Besides, we only found two types of animation used for the marks in augmented sports videos, namely, Creation and Destruction, while eight [3] are used in common data videos.

Video Effects are visual effects based on the existing content of the raw footage, such as segmenting and moving a player, showing a slow-motion, or rotating the camera. In a certain aspect, these effects are similar to the Attention Cues in [2]. However, in augmented sports videos, these effects are driven or controlled by data, with the goal of not only drawing the viewer's attention but also revealing deeper insights into a sports event. For example, the video can move a player to show a what-if situation or rotate the camera to present another point-of-view. These effects are limited by the quality of the raw footage (frame rate, resolution) and video processing techniques.

Fig. 2d shows the top-10 frequently used visual types in our corpus. 7/10 are graphical marks, while only 3 belong to video effects (pause, slow, and repeat).

3.3 Clip-Level Design
A more important question is how to organize the data and visual types in a sports video. To cope with this question, we identify two design dimensions, Data Level and Narrative Order, that should be considered for selecting and presenting data in the video.

Dimension Iii: Data Level
Augmented videos usually present sports data for different purposes. Specifically, some videos present data for entertainment (e.g., showing the jumping height of a player who is dunking) while others are for education (e.g., highlighting the formation of the team). We notice that the data for entertainment is usually with low semantics (e.g., positions and distances) that can be easily perceived from the raw footage by general audiences, while the one for education is more often with high semantics (e.g., techniques and tactics) that provide extra knowledge to the audiences. Based on this observation, we categorize the data from low to high semantic levels:

Fig. 2: - The number of videos a) of different ball sports and b) with different time duration. The top-10 frequently presented c) data types and d) visual types in augmented sports videos.
Fig. 2:
The number of videos a) of different ball sports and b) with different time duration. The top-10 frequently presented c) data types and d) visual types in augmented sports videos.

Show All

Image Level includes the frames of a raw footage. The data at thislevelhas the largest quantity and will be used as the input of a system. A video without any augmentations presents this level's data.

Object Level includes the physical data of objects detected from the image-level data, such as the postures of a player, the positions of the ball, and empty areas of the court. This level of data can be naturally understood from the video by audiences without any prior knowledge. An augmented video that only presents the data at this level is mainly for entertainment purposes.

Event Level contains the data that can only be interpreted from the video with domain specific knowledge. Typical examples include the player's techniques, the formation of the team, and the status of the ball. These kinds of data may be familiar to experienced fans but provide extra knowledge to novices. Therefore, when presenting event-level data, an augmented video is considered to be in the middle between for entertainment and for education.

Tactic Level presents the reasoning results of a sports video, explaining why a team wins or loses a score. Tactic-level data is usually drawn by experts who analyze the data from other levels, indicating the key factors that lead to the result. Hence, an augmented video with tactic-level data is mainly for education purposes since it provides the most additional knowledge for audiences.

Fig. 4 shows the statistics of clips with different data levels. Overall, most of the clips present tactic- (52.6%) and event-level (43.9%) data, while only 3.6% of clips present object-level data. These statistics indicate that most the augmented videos target sports fans and provide expert analysis. Data Level is not meant to be a taxonomy of sports data but a design dimension that needs to be considered in creating an augmented video. For example, if a video designer wants to engage the novice audience, she should select and present the object-level data.

Dimension Iv: Narrative Order
A sports video usually presents the sports events in linear and rarely employs non-linear narrative structures. However, we notice that the data in these videos are not always presented in chronological order. For example, to explain the tactic of a player, it is common to pause the video and foreshadow the trajectories of the next several movements of the player. Considering the presenting order of data and its actual chronological order, we borrow the idea of narrative order [27] to depict how the data is presented in these augmented videos:

Linear presents data in chronological order (Fig. 3a), which is the most common way to present data in sports videos. Pausing is used in Linear when there is too much data to show in one moment.

FlashForward foreshadows the data that will happen later than what is being told. It is frequently used for Tactic Level data. Thevideo is usually paused when using FlashForward. For example, Fig. 3b shows the positions of one player in the next several seconds.

TimeFork shows data that never happens in the game and is pri{###}- nullmary used to present a what-if analysis, explaining the results ofdifferent choices of players (Fig. 3d). A typical pattern in TimeFork is to show the hypothetical data first, reject these options, and visualize the actual data at the end.

FlashBack presents the data that took place earlier than what is nullhappening. FlashBack can be used for both entertainment (e.g., to emphasize the achievements of a player) and education (e.g., explain the causality). For example, Fig. 3c highlights a player's previous position to reveal how fast a player ran.

nullZigZag plays the video in reverse for a short period and then forwards this part again (Fig. 3f). Basic usage of ZigZag is tohighlight some key events, such as crossover and spin, for both entertainment and education purposes.

Fig. 3: - Video examples in the corpus of a) table tennis in linear, b) soccer in flashforward, c) basketball in flashback, d) badminton in timefork, e) tennis in grouped, and f) football in zigzag.
Fig. 3:
Video examples in the corpus of a) table tennis in linear, b) soccer in flashforward, c) basketball in flashback, d) badminton in timefork, e) tennis in grouped, and f) football in zigzag.

Show All

Fig. 4: - A clip-level design space for augmented sports videos: data level and narrative order. The number in cells depict their combination occurrences in our corpus. Darker cells mean more occurrences. The last row and column present the ratio of each option to its dimension.
Fig. 4:
A clip-level design space for augmented sports videos: data level and narrative order. The number in cells depict their combination occurrences in our corpus. Darker cells mean more occurrences. The last row and column present the ratio of each option to its dimension.

Show All

 Grouped presents multiple sports scenes by using picture-in-picture or multiple views (Fig. 3e). These events are grouped based on some criteria, such as technique types, players, or correlation, and can be played in parallel or series.

Fig. 4 provides the statistic of these Narrative Order in the video corpus. In general, Linear (52.5%) and FlashForward (24.9%) are the two most frequently used order, Grouped (1%) is the least frequently used one, and the remaining three-FlashBack, ZigZag, and TimeFork-all have around 5% to 10% cases.

3.4 Patterns at Clip-Level Design
Based on the annotations of our corpus, we identify some common design patterns at clip-level:

Data + Linear
Linear is the most commonly used null(52.5% clips) order to augment sports videos in all data levels. Presenting data in linear is natural since the underlying sports videos are played in linear. Thus, an augmented video creation tool can use the Linear order as the default choice for all data levels. An interesting finding is that object-level data is seldomly narrated in orders other than Linear. After discussing with our experts, we consider this is primary because: 1) the physical data can easily be understood by the audiences, and 2) when augmenting a video with physical data to engage the audiences, using linear without pausing can avoid interfering the audiences' game-watching experience.

 Event Level + Flashforward
We also see that many clips show event-level data using FlashForward. Throughwatching the videos in our corpus, we notice that using FlashForward allows to preview and explain complex data in the following events without affecting the actual watching experience.

Tactic Level+ Flashforward
An interesting pat- tern is that compared to event-level data, tactic-level data is less frequently presented in Linear but more in FlashForward. By digging into the corpus, we consider this is mainly due to another advantage of FlashForward-enhancing the connection between the current and following events to emphasize the causality between them.

 Tactic Level+zigzag
ZigZag can be used to augment one event from different angles since it can repeat a specific event multiple times. For example, a video can highlight a player in an event and then use ZigZag to replay the same event but with another player highlighted. Besides, ZigZag can be seen as a composition of two “single” narrative orders. For instance, some examples use Linear to show certain data and then use ZigZag to roll back and show another data using FlashForward. Given these characteristics, ZigZag has been frequently used to present tactic-level data.

SECTION 4Viscommentator
Based on the design space, we design VisCommentator, a proof-of-concept system that augments the videos for table tennis. While the implementation of VisCommentator is specific to table tennis, its design can be generalized to other racket ball sports once the underlying dataset and ML models are available.

4.1 Design Goals
We conceived the design goals of VisCommentator based on the design space, prior research [21], [37], [50] on intelligent data visualization design tools, and weekly meetings with two domain experts over the past eleven months. One of the experts is a sports science professor who provides data analysis and consultancy services for national table tennis teams. The other is a Ph.D. candidate in sports science with a focus on table tennis analytics. In the meetings, we demonstrated several prototype systems and collected feedback to refine our design goals.

G1. Extract the Data from the Video Based on Data Levels
According to our design space, augmented sports videos usually present data from different levels (i.e., object-, event-, and tactic-levels) for entertainment or education purposes. However, manually preparing the data of different levels might be tedious and time-consuming. Thus, to support these augmentations, the system should automatically extract and organize the data from a video based on the data levels.

G2. Interact with the Data Instead of Graphical Marks
The fundamental goal of sports analysts is to visualize the data, such as showing the ball trajectory. However, most of the video editing tools support the user to augment the raw footage with graphical marks, such as drawing a line in the scene. Hence, the system should allow the analysts to select the data to visualize instead of asking them to select the graphical mark to draw. Furthermore, the user should be able to directly interact with the data in the video where it is originated from.

G3. Recommend Visualizations for Different Narrative Orders
Visualization recommendation systems can ease the creation process as the sports analysts may lack sufficient knowledge of data visualization. Prior systems [28], [52] usually recommend visualizations based on the effectiveness of visual channels. However, in our scenarios, the narrative order may result in different availability of visual effects. For example, when using a Linear order, we cannot move a player in the video out of his position. Consequently, the system should further consider the narrative order in the recommendation.

4.2 Usage Scenario
To illustrate how VisCommentator encompasses the design goals, we introduce the workflow to create an augmented video taken by Jon, a hypothetical sports analyst who needs to analyze a table tennis rally and present his findings to the coaching team and athletes. He also needs to create a video with highlight moments to engage the team fans.

Brush the Timeline
Jon plans to present his analytical findings that the last two turns are the key events, in which the ball rotation speed is so fast that the player in red cloth (Playerred) can only return it to a narrow area and thus gives his opponent a chance to attack. Thus, Jon loads the raw video to VisCommentator. It's user interface (UI) is familiar to Jon as it follows the design of general video editing tools, which usually have a video preview (Fig. 5a), a main video timeline to be edited (Fig. 5b), and an edit panel (with tactic-level data and Linear order in default) in the right-hand side (Fig. 5c). The events of the rally are automatically detected and visualized under the timeline (G1). Jon brushes on the timeline to select the last two turns.

Select the Data
The two players and ball are automatically detected and highlighted by using bounding boxes in the main view (Fig. 5a, G1). Jon first moves the pointer to the frame when Playerredperforms the stroke (Fig. 5b1) and right-clicks the ball in the main view. A context menu popups, allowing Jon to directly select which data to be augmented (Fig. 5a1, G2). Based on his analysis, Jon selects the “Ball rotation speed”, “Potential placements”, and “Potential routes” in the context menu. The system automatically recommends visuals for these data and lists the mappings in the “Visual Mapping” list (G3).


Fig. 5
The user interface of the system, including a) a video preview, b) a timeline, and c) an edit panel. A basic authoring workflow includes four steps: 1) brush the timeline, 2) choose the key event, 3) select the narrative purpose and narrative order, and 4) select the augmented data by directly interacting with the video objects. The system will suggest the visuals for the selected data in the edit panel (c1).

Show All

By playing the clip, Jon finds that the clip pauses and visualizes the ball rotation speed by using a label (Fig. 6a), followed by an animation that shows the potential routes and placements possibility of the ball (Fig. 6b). Satisfied with these augmentations, Jon moves the pointer to the next turn, right-clicks the player in black cloth (Playerblack), and selects the “Stroke Technique” data. Consequently, VisCommen-tator augments the video to highlight the active attack of Playerblack (Fig. 6c). Although impressed with these augmentations, Jon is curious about the “Narrative Order” and chooses “Flash Forward” to check the augmentation effects. The system presents another augmented clip that foreshadows the action of the Playerblack in the video right after showing the probability distributions of the ball placement (Fig. 6d). Jon recognizes that this narrative method explains the causality between the ball placement and the “Playerblack 's attack in a stronger manner.

Edit and Fine-Tune
Besides the coaching team and athletes, Jon also needs to present the highlight moments to engage the team fans. Jon thinks that the tactical data is too complex for the general fans. Thus, he chooses the object-level data in the edit panel to augment the video. Instead of annotating the players' key movements, Jon now can select some physical data to augment the video. He thus directly interacts with the objects in the video to select “Player Object”, “Player Position”, “Ball Trajectory”, and “Ball Placement”. VisCommentator automatically recommends visual effects to present these data, so that Jon only needs to perform minimal editions, e.g., specify the colors. Satisfied with the results, Jon exports the augmented video.

4.3 Processing the Data Through Ml Models
To incorporate G1, we implement a bottom-up approach to extract the data from table tennis videos (Fig. 7a1).

Object-Level Data
We assemble multiple state-of-the-art deep learning-based models to extract object-level data from the input video. Specifically, in each frame, we aim to detect the positions of the ball, the table, and the two players. For the players, we further extract their postures. To achieve these goals, we first extract the feature map of each frame by using a ImageNet [13] pre-trained RestNet-50 [17] backbone. The feature map is used for the following detections:

To detect the ball, we use TTNet [47], a multi-tasks model that can detect and segment the ball, table, and players, as well as classifying ball events such as ball bounce and net hit. We only use TTNet to detect the ball and table since TTNet cannot detect player postures. Given that the ball is small and moves fast, TTNet employs a two-scales architecture on a stack of consecutive frames to detect the ball. The detection is represented by a bounding box. Besides, we interpolate the ball position when it is occluded.

The table can be easily detected as it is usually fixed in the video.

For the players, we adopt BodyPix [44] to detect the two players, segment their pixels from the raw image, and detect their postures. BodyPix is an industry-level model for real-time human body pose estimation. The output for each player is represented as the bounding box, the pixels, and the posture key points.


Fig. 6:
A) the rotation speed of the ball is 7000 rounds per minute. B) the probability distribution of the ball placement. C) the player attacks the ball to win the rally. D) the video uses a flashforward to preview the action of the player after showing the potential ball placement.

Show All

Finally, based on these data, we can further calculate other object-level data, such as ball velocity, ball trajectory, player moving direction. Given the “off-the-shelf” nature of these models, we only tested our approach in a 6-seconds long table tennis video, whose resolution is 1920 × 1080 and frame rate is 50FPS, and obtained 90%+ average precision with intersection over union equals 0.5 for all the objects.

Event-Level Data
An event usually covers multiple frames. Based on the object-level data, we extract two types of events, i.e., ball, player:

Ball events include ball bounce and net hit. We reuse the event spotting branch integrated in TTNet to classify whether there is a ball bounce or net hit event within in a frame.

The main player event is stroking. To detect the stroke event, we leverage the ball velocity and the distance between the ball and the player's right hand that is detected as a key point of the player's posture. Given that the player's right hand sometimes may be miss detected due to occlusion, we also use the player's neck (another key point of the posture) as a fallback. We classify a frame as belonging to a player stroke event when the distance reaches a bottom and flag it as a “hit” frame if the ball velocity changes direction.

Fig. 7: - The pipeline to augment a raw footage. A) the system (1) adopts a bottom-up approach to extract object-, event-, and tactic-levels data from the video frames, and (2) organizes them into a hierarchical manner. B) the user can directly interact with the objects in the video to select the data. The narrative order will affect how the data is rendered. C) the system automatically (1) suggests visuals for the data and (2) schedules their rendering relatively to the video frames based on the narrative order. The data and video frames are rendered in two separated tracks.
Fig. 7:
The pipeline to augment a raw footage. A) the system (1) adopts a bottom-up approach to extract object-, event-, and tactic-levels data from the video frames, and (2) organizes them into a hierarchical manner. B) the user can directly interact with the objects in the video to select the data. The narrative order will affect how the data is rendered. C) the system automatically (1) suggests visuals for the data and (2) schedules their rendering relatively to the video frames based on the narrative order. The data and video frames are rendered in two separated tracks.

Show All

Finally, based on the events, we further compute other event-level data, such as the placement region based on the ball position in bounce event, the stroke technique based on the player posture in the hitting frame.

Tactic-Level Data
Tactic-level data usually involves multiple events and deep domain knowledge and thus can hardly be detected from the video by using CV models. Instead, we adopt a rule-based method to acquire some kinds of tactic-level data. Specifically, we use a set of rules provided by our domain experts to infer the player tactic, potential ball routes and placements, and stroke effect of a player. For example, given the rule that “if a player received the ball at his end line, he can only return the ball to the end line of the opponent”, we can calculate the potential ball placements based on the event-level data. In addition, we use a pre-trained causal graph [32], [56] to detect the key stroke that contributes most to the rally result (i.e., why one player win). Nevertheless, there are still many kinds of tactic-level data that cannot be automatically obtained from the video due to the limitation of the state-of-the-art techniques. As a trade-off, our system allows the analyst to import external data from other tools [12], [49]. Further technical details can be found in the supplemental materials.

4.4 Interacting with the Data Through Direct Manipulation
Different from general video editing, the ultimate goal of sports analysts is to augment the video with data rather than graphical marks (G2). To this end, a straightforward method is to allow the analyst to select the data in the edit panel (Fig. 5c1). However, this method leads to a large distance [19] between the user's intentions and the meaning of expressions in the interface. For example, the analyst wants to select the data of the objects in the video, but she has to interact with the edit panel. To reduce the analyst's cognitive load, we leverage the extracted data to decorate the video, thereby increasing the directness of interactions. Specifically, we visualize the events along with the timeline and encode their event type (i.e., ball or player event) with color, so that the user can quickly identify and navigate to a specific turn in the rally. Besides, the players and ball in the main view (Fig. 5a) are right-clickable, by which the user can directly select the data for augmentation. All the selections will be mapped to the underlying data (Fig. 7b) for mapping to visuals.

Fine-Tuning in the Edit Panel
Meanwhile, all the selected data will be listed in the edit panel (Fig. 5c1) and encoded with the visuals suggested by the system. The user can further adjust and fine-tune the visual effects, such as modifying the color, line width.

4.5 Visualizing the Data Through Recommendation
VisCommentator integrates a visualization recommendation engine to map the data to visuals based on the user-selected narrative order (G3).

Visuals-Maximum Conditional Probability
Previous research proposed several methods to suggest visuals based on the data, such as rule-based [52], constraint-based [28], decision tree [50], and neural network [18]. Considering our specific domain, we intend to use the collected videos as prior knowledge to recommend the visuals. Specifically, we model the visual mappings using conditional probability distribution: p=f((d,v)| O), where d, v, and O is the data, visual, and narrative order, respectively. Intuitively, this model represents the probability distribution of data-visual mappings under different narrative orders. This probability distribution can be estimated based on the occurrence frequency of the combinations of data, visual, and narrative orders in our corpus. Consequently, given d and O, our system will search the v that maximizes p and suggest it to the user. We also use a rule-based method [52] as the default to select effective visual channels for the data if there are no mapping records of the data under a narrative order. Our method is simple but effective since it is built on videos from reputable sources, and can be extended and optimized in the future. Finally, since the position of the objects in each frame are detected, we can easily render the visual effects of the objects in the screen space.

Narrative Order-Double-Track Rendering
To correctly rendering the visuals based on the user-selected narrative order, we employ a double-track rendering method (Fig. 7c2). Specifically, we render the frames and data visualizations in two parallel tracks (video track and data track) to control their appearance order. For example, in Fig. 7b, the user selects the data of four frames and renders them by using a FlashForward, which presents the data that will happen later than the current frame. Our system will pause the video track after playing the first frame and keep rendering the data for the next three frames. The video track will be resumed after rendering all the selected data. The duration of the visualizations in a non-linear order is set with a default value. The current system only implements the common patterns mentioned in Sec. 3.4.

SECTION 5Implementation
VisCommentator is implemented in a browser/server architecture. The browser part, which is built upon HTML + CSS + JavaScript, is responsible for the UI and rendering of the video. We mainly use HTML Canvas to achieve the rendering of augmented videos. To improve the web-page efficiency, we use the OffscreenCanvas [29] functionality that leverages the worker, a multi-thread like technique in a modern browser, to accelerate heavy rendering. The server part is implemented based on Node.js + TypeScript. To extract the data from a video, we employ PyTorch [1] and TensorFlow.js [43] that support running pre-trained deep learning models by using Node.js.

SECTION 6User Study
We conducted a qualitative user study to assess the usability of Vis-Commentator. The study aimed to evaluate whether sports analysts, the target users, can create augmented table tennis videos with our system and to observe their creation process to reflect on future improvements.

Participants
We recruited 7 table tennis analysts (P1-P7; 4 male; age: 20–30, one didn't disclose) from a university sports science department. All experts majored in Sports Training with proficient experience in analyzing table tennis matches. All the experts had the experience of using lightweight video editors, e.g., VUE [48], but no experience on advanced video editing tools such as Adobe Premiere and Camtasia. The scale of expert participants is consistent with similar sports visualization research [40], [54]. Each participant received a gift card worth $14 at the beginning of the session, independent of their performance.


Fig. 8:
A) the video highlights that (1) the player in red first uses his left hand. (2) He switches to his right hand to stroke the ball to (3) obtain more chances to hit the ball to the corner, and (4) thus successfully hits the ball to the empty area. B) the video visualizes (1) the ball rotation speed and foreshadows (2) the ball route and placement, as well as (3) the player's next movement. C) the augmentation shows that (1) when the player in black plans to move to the right, his opponent strokes the ball to the left. (2) The player in black tries his best to return the ball. (3) After returning the ball, his opponent can stroke the ball to the two table corners. The opponent strokes to the farther corner and win the rally.

Show All

Tasks
The participants were asked to finish a training task and two reproduction tasks by using our system. We prepared three augmented videos based on three ITTF top 10 rallies in 2019 [20]: T1 (Fig. 8b) is augmented by event-level data and presented by FlashForaward with 5 visual-data mappings; T2 (Fig. 8c) is augmented by tactic-level data and presented by TimeFork with 10 visual-data mappings; T0 (Fig. 8a) includes two augmented clips and covers all the features in T1 and T2. All three augmented videos were created by our domain experts in the collaborations. For the tasks, we provided the augmented videos, background information, the system manual, a digital version of the design space, and the data extracted from the video.

Procedure
The study began with the introduction (15min) of the study purpose, the concept of augmented sports videos, and the design space with 15 curated example videos. We moved to the training phase (20min) when the experts had no more questions. The participants were walked through with a step-by-step instruction to reproduce T0.

After the training, participants were provided with the materials of the two reproduction tasks, T1 and T2 (15min for each). We encouraged them to ask questions about the materials. A task was started when a participant was confident to begin, and ended when the participant confirmed finishing. To assess the efficiency and effectiveness of the system, we recorded the completion time and successfulness for each task. A successful reproduction should correctly reproduce the data level, narrative order, and all the visual-data mappings of the given video in 15 minutes (based on our pilot study). All measures were explicitly explained before the tasks. The participants were encouraged to explore the system (20min) after the tasks.

The session ended with a semi-structured interview and a post-study questionnaire (15min). Each session was run in the lab, using a 24-inch monitor, with a think-aloud protocol, and lasted around 90 minutes.

6.1 Results
We consider the study assessed the system qualitatively rather than quantitatively due to the small sample size. All participants could successfully reproduce the two augmented videos in a few minutes (T1: 5mins; T2: 9mins). Some participants (P1, P3, P4) could not memorize the details of the targeted videos and thus frequently replayed the videos to check the augmentations during the authoring. This is the reason why these participants took a longer time to finish the tasks. Besides, we also noticed that P2 and P5 explored other augmentations during the tasks even they had already successfully created the results, leading to extra time to finish the tasks. Overall, these results qualitatively demonstrated the effectiveness and efficiency of our system. The participants' feedback is summarized as follows:

Usability
Overall, our system was rated as easy to learn (μ=6.00) and easy to use (μ=6.86) I by the participants. The participants (P1-4, P6) commented that “••• this system allows me to do more with less effort.” P3 provided in detail that “I have used other video editors•••I can't imagine how I can use them to create augmented videos•••”When we asked the participants whether they need more customization of the augmentations such as the order of visuals, most of them agreed that on one hand, the tool should strike a balance between customizability and simplicity as their goal is “rapid prototyping”; on the other hand, the tool should allow editing of the details when the user needs to.

Usefulness
The key designs-data extractor, direct manipulation UI, and visuals recommendation-of our system were particularly lauded by the experts and confirmed to be useful (μ=6.29, μ=6.86, and μ=6.57, respectively). The participants thought that extracting and organizing data based on data levels were useful. P2 commented that “organizing data into the three levels is not surprised to me, but connecting them to different narrative purposes inspires me a lot.” All the participants spoke highly of how we support the augmentation through direct manipulation. “Clearly revealing the object to be augmented” is the main reason why the participants favored it. P7 further pointed out that the UI design made the task “cognitively easy.” The participants found the visualization recommendation was particularly useful as they “were not familiar with mapping data to visuals.” Besides, P5 suggested allowing “comparison between different recommendations.”

Satisfaction
The rating also reflected a high user satisfaction for the data extractor (μ=6.29), direct manipulation design (μ=6.43) and visualization recommendation (μ=6.43). The participants said that the direct manipulation design was “intuitive” (P1-P7) and the visualization recommendation “helps guys like us (sports analysis)•••” (P1, P6) Comments also implied further improvements of the data extractor: “The available tactic data is a bit limited••” (P3, P6) We explained that this was mainly due to the limitations of ML models and our system allows the user to import external tactic data.

6.2 Observations and Feedback
Increasing the Player's Memorability in a Tense Game
We were interested in how the participants usually presented their analytical findings and the advantages of using augmented videos. All the participants responded that voiceover on the video is the most common way to present their findings. They also prepared slides to add annotations to the video. Some participants (P3, P5, P6) also created simple charts, such as bar and pie charts, to visualize the data. Compared to these methods, a particular benefit of augmented videos is “increasing the memorability.” (P1, P3-P7) This is extremely important for the players. As pointed out by P5, the game is so tense that the player usually can only “recall the visual scenes rather than the numbers.”

Direct Manipulation with the Video Objects
While our system allows the user to directly interact with the video elements to select the data, we observed that the participants wanted to achieve more than that. For example, P3 and P7 wanted to navigate the video by directly moving the player; P2 tried to pan the video to adjust the camera. These behaviors revealed that the users not only directly interact with the video elements to manipulate the data but also the time and space. Prior research [14], [30] proposed techniques to allow users to navigate the video by directly moving the objects. DimpVis [23] applies the similar idea for time-varying visualizations. Yet, how to unify the direct manipulation of time, space, and data in a data-augmented video remains unclear. Further exploration of the design space is thus suggested.

Bridging the Gap Between Analyzing and Communicating Data
While visual analytics systems have been increasingly used for data analysis, it remains challenging for analysts to visually communicate their analytical findings. For example, although all the participants expressed strong interest in authoring augmented videos, they had never created augmented videos before since “it is too complicated to create them•••” (P2) P7 detailed that “the design tools mainly focus on the visual parts” so that the analyst has to “manually map the data insights to the visual elements.” Thus, all the participants appreciated that our system allows data-driven design as “the ultimate goal of drawing a curve is to present the trajectory data.” (P7) Nonetheless, the participants also suggested that our system should be integrated more closely with visual analytics systems. For example, the system should allow the analyst to import the analytical findings and automatically generate augmented videos. We consider this suggestion as an important future direction of bridging the gap between visual analytics and storytelling.

Translating High-Level Insights Into Specific Data
During the study, we observed that a main obstacle for the participants is to translate a high-level insight into specific data visualization. For example, the participants could easily perceive that Fig. 8c1 showcases an attacking both sides tactic. However, to reproduce this clip, the participants needed to break down this tactic into specific data such as ball trajectory, moving direction of the ball and player. This “translation” process usually led to a trial-and-error situation. In the interview, the participants commented that this process is similar to a reverse analysis in which the analyst needs to “generate data from insights.” (P7) Moreover, P1 provided that in some cases, there is no data equivalent of insights, e.g., “the player is in low spirits.” This finding implies future study to translate the analyst's high-level findings into specific data. A potential solution is to leverage natural language processing techniques, which have already been employed in some creativity support tools [22], [24].

Suggestions
Most limitations identified by the participants were related to system engineering maturity. For example, transitions between clips were not supported; some parameters (e.g., playback rate) were fixed to default values. Our system also involves certain inherent limitations due to the underlying ML models. First, the input video needs to cover the whole court with a fixed camera angle. Although this is the common case for racket sports, videos of team sports such as football and basketball do not satisfy this requirement, which necessitates additional steps to concatenate different views into a panoramic view [41]. Second, the data extracted from the video is still limited. For example, the server type, hitting area, and other historical data are unavailable. To mitigate this issue, we allow importing data from dedicated data annotation systems (e.g., EventAnchor [12]). Third, the system only supports augmentation in screen space instead of world space. In other words, our system does not reconstruct the 3D scene of the video and thus cannot properly handle z-order and occlusions.

SECTION 7Future Work and Limitations
Significance-Communicating Data Insights Through Videos
After decades of research efforts, many systems and methodologies have been proposed to lower the barrier to data visualizations, allowing data analysts and even general users to communicate data insights through visualizations. However, communicating data through video-based visualizations is still a challenging task [45]. On the other hand, it has become increasingly popular in recent years to disseminate information through videos (e.g., YouTube, TikTok) as they can convey information in an engaging and intuitive manner. We explore this direction by investigating augmenting sports videos with data. Feedback from our user study with sports analysts demonstrates the potential of communicating data insights through videos. We expect our work can increase interest and inspire future research in this promising direction.

Generalizability-Extending from Table Tennis to Other Ball Sports
Although our prototype system focuses on augmenting table tennis videos, its main design can be extended to other racket sports once the underlying ML models and datasets are available. As for team sports (e.g., basketball, soccer), future studies are required as these sports involve more complex spatio-temporal data [51], [55] and need more powerful ML models [57]. The design space, which covers both racket and team sports, can provide guidance for designing more extensive authoring tools to augment sports videos of other ball sports.

Potentiality-e-Augmenting Not Only Videos but Also Reality
Our research can be a step-stone towards SportsXR [26], which focuses on using augmented reality (AR) techniques to augment real-world sports activities with data visualizations, thereby supporting in-situ decision making and engaging sports enthusiasts. As delineated by Stein et al. [41], the study of augmenting videos with visualizations is intter-connected to AR visualization [8]. However, although AR opens up new opportunities for both sports analytics and sports watching experience’ it also introduces many challenges that yet to be tackled, such as scene understanding, streaming decision making, and data visualization in 3D real-world canvas [7], [58]. We believe our work adds to this direction by exploring the ways to present data in real-world canvas.

Study Limitations
Similar to other sports visualization research [40], the sample size of our user study is small since the access to sports experts is naturally limited. Thus, the study results are considered to be qualitative rather than quantitative. The design space is derived from a corpus of a limited number of videos. Our system only implements the most common patterns in the design space. Other less frequently used combinations of data level and narrative order are left for future implementation. Finally, although the experts and participants were satisfied with the created augmented videos, we didn't evaluate the videos from the audience's perspective. Future study is suggested once the system is engineering mature.

SECTION 8Conclusion
This work is motivated by the close collaboration with sports analysts who have a strong demand to augment sports videos with data. To ease the creation of augmented sports videos, we first systematically review 233 augmented sports videos collected from reputable sources and derive a design space that characterizes augmented sports videos at element-level (what are the constituents) and clip-level (how the constituents are organized). Informed by the design space, we present VisCommentator, a prototype system that allows sports analysts to augment table tennis videos efficiently. VisCommentator extracts data from a table tennis video based on data levels, allows the user to select the augmentation by interacting with the objects in the video, and suggests visuals for entertainment or education purposes. A user study with seven sports analysts confirmed the effectiveness, efficiency, and high satisfaction of the system. We have also discussed the observations and feedback from the study, which suggest future research.