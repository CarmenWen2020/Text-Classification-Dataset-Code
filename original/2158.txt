Analysts often make visual causal inferences about possible data-generating models. However, visual analytics (VA) software tends to leave these models implicit in the mind of the analyst, which casts doubt on the statistical validity of informal visual “insights”. We formally evaluate the quality of causal inferences from visualizations by adopting causal support —a Bayesian cognition model that learns the probability of alternative causal explanations given some data—as a normative benchmark for causal inferences. We contribute two experiments assessing how well crowdworkers can detect (1) a treatment effect and (2) a confounding relationship. We find that chart users' causal inferences tend to be insensitive to sample size such that they deviate from our normative benchmark. While interactively cross-filtering data in visualizations can improve sensitivity, on average users do not perform reliably better with common visualizations than they do with textual contingency tables. These experiments demonstrate the utility of causal support as an evaluation framework for inferences in VA and point to opportunities to make analysts' mental models more explicit in VA software.
SECTION 1Introduction
Data analysts engaged in sensemaking [9], [22], [46], [48] infer the compatibility of different causal explanations with their data. During exploratory analysis or provisional statistical modeling, analysts implicitly or explicitly compare their data to patterns they expect as logical consequences of hypothesized data generating processes [2], [16], [17], [25], [26]. Visualizations play a critical role in causal inference both because externalization reduces cognitive load [12] and because human capabilities for inference rely heavily on sensory expectations (e.g., mental imagery) and comparisons between expectations and experiences [7], [30].

Fig. 1: - Modeling causal inferences with visualizations: ⓐ users view and may interact with data visualizations; ⓑ ideally, users reason through a series of comparisons that allow them to allocate subjective probabilities to possible data generating processes; and ⓒ we elicit users' subjective probabilities as a dirichlet distribution across possible causal explanations and compare these causal inferences to a computed benchmark of causal support, which we derive from bayesian inference across possible causal models.
Fig. 1:
Modeling causal inferences with visualizations: ⓐ users view and may interact with data visualizations; ⓑ ideally, users reason through a series of comparisons that allow them to allocate subjective probabilities to possible data generating processes; and ⓒ we elicit users' subjective probabilities as a dirichlet distribution across possible causal explanations and compare these causal inferences to a computed benchmark of causal support, which we derive from bayesian inference across possible causal models.

Show All

Data analysts and software designers need to anticipate how human capabilities for causal inference may be error prone. For instance, perceptual biases such as underestimation of sample size (e.g., [33]) contribute to errors in causal inferences insofar as perceived associations seem to be the basis for causal inferences [1], [59]. Analysts also err in their causal interpretations of data when the mapping between a potential causal explanation and an expected pattern in the data is unclear [4], [60]. For example, imagine an analyst trying to detect confounding in experiment results on the effectiveness of a treatment at preventing disease (Fig. 1 Ⓐ). To detect that ‘gene’ is a confounding factor, the analyst must see effects of gene on both treatment effectiveness (i.e., a difference between the top and bottom cells in the right column of table Ⓐ) and overall rate of disease (i.e., a difference between the top and bottom rows of Ⓐ). Attributing these signals in the data to confounding requires the analyst to know what they are looking for, rather than passively detecting the appropriate patterns.

To assess how well visual analytics (VA) tools support such causal inferences, we need to compare analysts' inferences to a benchmark that is roughly ‘normative’, that captures important aspects of good causal inference. We adopt causal support, a model from mathematical psychology [21], as a benchmark for evaluating causal inferences from visualizations. Causal support models one's belief in a set of possible data generating processes as a Bayesian update. Causal support is a good normative model for three reasons. (1) Causal support has numerous proprieties of valid statistical inference in light of the analyst's prior knowledge. It captures the fact that belief in evidence should be stronger as sample size increases. It accounts for unknown unknowns about the space of possible models, such that causal support assigns no posterior probability to models that the analyst does not explicitly consider. (2) Prior work [21] shows through a system of experiments that causal support accounts for otherwise hard-to-explain patterns in human causal inferences, e.g., that subjective belief in a causal relationship varies as a function of the potential to detect that relationship in a given data set. (3) Since causal support is extensible to any generative model (i.e., models that can assign likelihood to data), it can be applied in a wide range of VA applications to evaluate causal inferences.

We contribute two experiments using causal inference problems involving count and proportion data to (1) study the utility of causal support for gaining insight into inferences from visualizations and (2) evaluate how well visualizations common in visual analytics (VA) software support causal inferences about possible data generating processes. We compare three common visual encodings for count and proportion data—bar charts, icon arrays, and text tables as a baseline—and we investigate how the ability to interactively aggregate or cross-filter data in bar charts impacts causal inferences. In Experiment 1, we ask participants to differentiate whether a treatment is effective by allocating probability across two different data generating processes. We find that chart users' causal inferences are far from normative with all visualizations we tested, and interacting with visualizations can improve sensitivity to signal in predictable ways. Ultimately, however, no visualization reliably outperforms text tables. We also see that chart users are more sensitive to evidence against a treatment effect than evidence in favor of one, suggesting an unequal weighting of falsifying versus verifying evidence. In Experiment 2, we replicate the main findings Experiment 1 but with a confounding detection task where participants allocate probability across four alternative data generating processes. Experiment 2 demonstrates how casual support can be extended to study causal inferences about more complex data generating processes.

SECTION 2Related Work
2.1 Visualization for Casual Inference
Much of the psychology and statistics literature on visual aids for causal reasoning focuses on contingency tables (e.g., [1], [4], [11], [20], [21], [49]). Contingency tables support causal inferences by using layout to encode conditional probabilities, the same way trellis plots afford grouping by factors during visual data analysis [5], [53]. Whether or not a factor seems to be collapsible—whether or not patterns the data seem to change depending on whether the data are grouped by that factor—can be a visual signal for reasoning about causal relationships such as confounding [20]. However, empirical research on interpretation strategies for contingency tables [4] suggests that analysts often misinterpret signals like collapsability because they don't ascertain the mapping between these visual signals and hypothesized causal relationships. Tools like Tableau enable users to explore collapsability by interactively grouping data. We investigate whether the ability to interactively control data aggregation improves the quality of causal inferences.

Research on visual analytics (VA) employs a broader range of representations to support causal reasoning, including parallel coordinates [55], [56], bar charts [60], “diff bar charts” showing counterfactual outcomes under different conditions as layered bars [58], and novel techniques using animation to show event sequences (e.g., [13], [28], [29]).

Some of these tools also incorporate directed acyclic graphs (DAGs) as interfaces to models and visualizations (e.g., [55], [56], [58], [60]). DAGs are devices for causal reasoning which have garnered attention in recent years [44]. DAGs encode hypothesized relationships among variables (e.g., Fig. 1 Ⓑ), making causal relationships and the assumptions they entail explicit and in some cases testable [3], [41]–[42][43]. We use DAGs to present differences between alternative causal explanations for data sets that we ask participants to judge (Figs. 2 & 9).

VA systems frequently use interaction techniques such as cross-filtering linked views of data (e.g., [55], [56]) and click- or drag-and-drop-based chart construction (e.g., [52], [58], [60]). The most similar prior research to our study1 tests whether constructing charts by clicking on variables versus dragging variables onto a DAG makes a difference in analysts' ability to differentiate between different kinds of causal relationships [60], specifically identifying mediating variables. Although they do not find an effect of interaction method on causal inferences, the authors provide a detailed strategy analysis extending evidence from psychological studies [4] that analysts struggle to reason about the exact set of visual signals they should look for to verify or falsify a causal relationship. We extend this line of work by studying whether the ability to (un)facet charts or cross-filter coordinated multiple views impacts the quality of untrained analysts' causal inferences.

Prior work in visualization [36], [38] and risk communication [15], [50] suggests that icon arrays can improve Bayesian inferences, perhaps in part because of cognitive benefits of framing probabilities as frequencies of events [18], [23], [27], [32]. We compare icon arrays to text tables and bar charts since these visualizations span the design space for showing count data and are also easy to create in VA software like Tableau.

2.2 Modeling Causal Reasoning
In the present study, we draw on and extend a model of causal reasoning called causal support, first proposed by Griffiths and Tenenbaum [21]. Causal support formulates causal inferences as a Bayesian update on the log odds of a finite set of causal explanations given some observed data. Mathematically, causal support has similar properties to a Chi-squared test (i.e., Are the data in each cell of a contingency table likely generated by the same process?), which prior work analogizes to the kind of comparisons between data and model predictions that analysts visualize in “model checks” [16], [17], [26] such as QQ-plots. However, unlike a Chi-squared test, causal support relies on Monte Carlo simulations to assign likelihoods under alternative causal explanations, making causal support extensible to any finite set of generative causal models. For instance, Pacer and Griffiths extend causal support to handle continuous data [40] and event streams [39]. Similarly, in Experiment 2, we present an extension of causal support to evaluate inferences about more than two possible data generating models.

Previous cognitive models of causal inference explored in psychology share more in common with parameter estimation than statistical inference per se, a subtle but important distinction. One such model delta p posits that that people judge differences in conditional proportions of observed events when making causal inferences about count data [1]. Another such model causal power posits that people judge the magnitude of effect size when making causal inferences [11]. Both of these predecessors to causal support make the assumption that causal inferences are fundamentally a perceptual judgment, however, causal power rescales delta p based on the potential to detect any signal whatsoever within the observed data. In contrast, causal support assumes that the signal for causal inferences depends on the possible data generating models that the analyst has in mind and represents these alternative models explicitly. This makes causal support more flexible, with higher predictive validity for human judgments than delta p, causal power, and even Chi-squared [21]. Causal support reflects analysts' natural tendency to dichotomize, for better or worse, reasoning about whether or not causal relationships exist rather than how strong they are.

SECTION 3Experiment 1
How well do different visualization designs that are common in visual analytics (VA) software support causal inferences about possible data generating processes? We evaluate visualizations of count data including text contingency tables, icon arrays, grouped bar charts, bars that users can interactively aggregate, and linked bars that users can interactively cross-filter. In Experiment 1, we investigate chart users' ability to infer whether a treatment prevents a disease. By asking chart users about a treatment effect in count data, we build on the task and structural equation models used by Griffiths and Tenenbaum [21] to propose and validate causal support. Count data are also ideal for evaluating bar charts. The design requirements for supporting our causal inference task with count data are that visualizations should express both the proportion of people with disease and sample size. Based on these requirements, we ruled out testing pie charts and heatmaps, common ways of encoding proportions that do not encode sample size.

3.1 Method
We set out to study how well different visualizations support causal reasoning by using causal support as a benchmark for causal inferences.

3.1.1 Task Scenario & Response Elicitation
Participants played the role of an analyst hired by a company to interpret samples of data on the effectiveness of experimental treatments at preventing various diseases. We showed participants visualizations of the number of people in each sample who did or did not receive treatment, get a disease, and have a gene known to cause the disease.

We asked participants to judge the underlying causal relationships in the data, rating their degree of belief that treatment protects against disease by allocating probability across the two DAGs in Figure 2. We chose to study causal inferences about treatments, genes, and diseases in order to create a scenario where users would find the possible causal explanations feasible, coherent, and memorable.

Question & Elicitation
We asked participants the following question: How much do you believe in each of the causal explanations described below? Imagine you have 100 votes to allocate across the two possible explanations. Split your 100 votes between explanations based on your degree of belief. For example, if you think one explanation is twice as likely as the other, you might give 67 votes (roughly two thirds) to that explanation and 33 votes (roughly one third) to the other. Assume no other explanations are possible.


Fig. 2:
DAGs representing possible causal explanations participants were asked to consider in experiment 1.

Show All

Participants responded with two complementary probabilities. We used form validation to make sure their responses were both numbers between 0 and 100 that summed to 100. Following prior work on eliciting Dirichlet distributions [10], [37] (i.e., probabilities allocated across alternatives), when participants gave their first response, we imputed what the second response would need to be in order for their responses to sum to 100. This imputed value and a corresponding prompt, “Adjust your responses until both numbers reflect your beliefs.”, were both highlighted with the same color to indicate this imputation. We elicited probabilities as “votes out of 100” because frequency framing tends to reduce bias in probability estimates [18], [23], [37]. Participants received no feedback on their responses. We transformed these responses into perceived causal support, which we compared to our benchmark.

Perceived Causal Support
The dependent variable in our study was a measure of the perceived log odds of a target explanation over other possible causal explanations. Specifically, in Experiment 1 we targeted explanation A, which posited a treatment effect, requiring us to transform participants' responses into a log response ratio (lrrA),
lrrA=log(responseAresoonseB)
View Sourcewhere responseA and responseB were the probabilities participants allocated to causal explanations A and B, respectively, on each trial. We used a log odds scale in order to make participants' perceived causal support comparable to our normative benchmark of causal support.

Payment
Participants received a guaranteed reward of $2 plus a bonus of $0.25 for every trial where their estimate of the probability of causal explanation A2 was within 5 percentage points of the ground truth.

Apparatus
We collected data using a Flask application deployed on Heroku with a Firebase database and visualizations created with D3.3

3.1.2 Visualization Conditions
Our visualizations show the number of people with and without disease in each cell of a 2×2 contingency table faceted by treatment and gene, with the exception of cross-filter bars which use a different layout. We aimed to test visualizations of count data similar to what an analyst could produce using visual analytics software like Tableau.


Fig. 3:
Non-interactive visualizations evaluated in our study: ⓐ text contingency tables; ⓑ faceted icon arrays; and ⓒ faceted bar charts.

Show All


Fig. 4:
Aggregating bars mimic shelf construction and faceting.

Show All

Text contingency tables showed the number of people with disease as a fraction of the total number of people in each cell of a faceted table (Fig. 3 Ⓐ). Text tables, which have been studied in prior research on causal support, served as a baseline comparison for other visualizations.

Icon arrays showed counts of people with and without disease as filled and open circles, respectively (Fig. 3 Ⓑ). We set the number of dot columns to minimize the aspect ratio on each trial, similar to how analysts might create roughly square icon arrays in Tableau. Icon arrays express both proportion and sample size as natural frequencies, which prior work finds beneficial for statistical reasoning (e.g., [15], [18], [23]).

Bar charts showed counts of people with and without disease using a length/position encoding on a common scale (Fig. 3 Ⓒ). On each trial, we set the y-axis scale to the maximum count of the data in view, allowing scales to change from trial to trial as they do when users load a new data set in Tableau. Bar charts are ubiquitous for count data.

Aggregating bars (aggbars) were similar to bar charts, however, users could interactively toggle faceting by treatment or gene by clicking on the table headers (Fig. 4). On each trial, we set the y-axis scale to the maximum count of the fully aggregated data. We designed aggbars to roughly mirror Tableau's shelf interactions, where users control faceting by direct manipulation of table headers. Interactive faceting may facilitate causal inferences by enabling users to explore whether “collapsing” [20] over a factor changes patterns in the data.

Cross-filtering bars were three bar charts showing the number of people with and without treatment, the gene, and disease, respectively (Fig. 5). We linked these bar charts such that clicking on one bar cross-filtered the rest of the data in view. When users applied a filter (e.g., only show people who received treatment), the corresponding axis label became bold and gray bars persisted in the background, so chart users could compare filtered and unfiltered views without relying on working memory. Users “reset filters” by clicking a button below the charts. Filterbars emulated coordinated multiple views such as a Tableau dashboard. Interactive cross-filtering might assist in causal inferences because conditioning the data in view based on a specific event is analogous to Pearl's do operator (e.g., do(treatment = yes)), a notation for reasoning about counterfactuals in a causal networks [44].

3.1.3 Experimental Design
We manipulated both the visualizations participants used for the task and the data sets that we showed. We randomly assigned each participant to use one of five visualizations (see Section 3.1.2), making comparisions of visualizations between-subjects. We showed each participant a total of 18 trials, which included 16 data conditions (see Section 3.1.4) presented within-subjects and two attention checks which we used for exclusion criteria (see Section 3.1.6). We randomized trial order for each participant, inserting attention checks on trials 7 and 13.

3.1.4 Stimulus Generation
We evaluated causal inferences on realistic data sets, which spanned a range of ground truth causal support. Generating data sets required (1) manipulating data attributes which signaled causal support to participants, and (2) labeling each data set with ground truth causal support.

Fig. 5: - Cross-filtering bars mimic coordinated multiple views.
Fig. 5:
Cross-filtering bars mimic coordinated multiple views.

Show All

Our goal was to generate 16 data conditions (i.e., trials in our experiment) that varied delta p and sample size, two data attributes which in turn manipulate ground truth causal support. Delta p described the difference in the proportion of people with disease in each data set depending on whether they received treatment. Positive values of delta p indicated evidence that the treatment protected against disease; negative values indicated evidence against treatment effectiveness. Sample size was the number of people in each data set we showed participants.

Data Conditions
To generate our 16 data conditions, we simulated data from structural models with one parameter per DAG arrow in Figure 2. We manipulated both the probability that treatment prevents disease (4 levels: {0, 0.1, 0.2, 0.4}) and sample size (4 levels: {100, 500, 1000, 1500}). We controlled the probability of disease due to the gene (0.5), probability of disease due to unobserved causes (0.2), base rate of the gene (0.4), and the proportion of each sample with treatment (0.5). We selected these parameters iteratively by sampling data sets and labeling ground truth until half of the trials had greater than a 50% chance of being generated by causal explanation A.

For each of the resulting 16 data conditions, we simulated many data sets using a binomial random number generator to approximate realistic sampling error. By simulating sampling error, we prevented the count data from appearing contrived. This sampling error resulted in a distribution of ground truth causal support under each data condition, with more variability in the ground truth at smaller sample sizes. To guarantee that each participant saw trials spanning a consistent range of causal support, we selected 16 data sets representing 16 quantiles of the ground truth distribution per data condition, and we counterbalanced the quantile shown for each data condition across participants within each visualization condition using a balanced latin square. For our attention check trials, we selected the two simulated data sets that had the minimum and maximum ground truth causal support.

Labeling Ground Truth Causal Support for Each Data Set
We operationalized the ground truth for causal inferences using Griffiths and Tenenbaum's causal support, a Bayesian cognition model that estimates the posterior log odds of a target data generating model over a set of alternative data generating models, given a data set. In Experiment 1, we targeted causal support for explanation A over explanation B:
csA=log(Pr(C|modelA)Pr(C|modelB))+log(Pr(modelA)Pr(modelB))
View Sourcewhere C is the data set we label with ground truth, and models modelA and modelB correspond to causal explanations A and B (Fig. 2).

The first term in the formula for csA is a log likelihood ratio representing the relative compatibility of a given data set with causal explanations A and B. We computed the log likelihood of each data set given modelA and modelB using Monte Carlo simulations (Alg. 1, lines 28-30), based on structural models similar to those we used to generate data sets. In practical scenarios, we would not know the true data generating parameters, so we used Monte Carlo simulations of possible parameter values under each model to calculate likelihoods without needing to know the ground truth a priori. Under modelA we sampled all three parameters uniformly on the interval [0, 1], representing the assumptions that there is a treatment effect and that both gene and unobserved factors cause disease. Under modelB we sampled Pr(D|G) and Pr(D) uniformly, but we fixed Pr(¬D|T) at zero, representing an assumption of no treatment effect (i.e., omitting the DAG arrow between treatment and gene in Fig. 2). In each simulation, we averaged log likelihood of a given data set over m=10000 Monte Carlo iterations (Alg. 1, lines 25-26), marginalizing over sampled parameter values.

The second term in the formula for csA is a log ratio of the prior probability of explanations A versus B. Following Griffiths and Tenenbaum [21], we assume a uniform prior to be normative, assigning 50% probability to both explanations A and B (Alg. 1, lines 31-32). The prior encodes a bias in belief allocation across a finite set of alternative causal explanations. We assume a uniform prior because we want our benchmark to reflect no a priori bias toward causal explanations.4

Algorithm 1 Monte Carlo Simulation to Calculate Causal Support in Experiment 1. Algorithm for Experiment 2 is Similar
Input: (8, 1) vector of contingency table counts (no disease vs disease, no gene vs gene, no treatment vs treatment) C, Monte Carlo iterations m, set of parameters to fix at zero θ0 (i.e., parameters representing DAG arrows to omit from the data generating process)

Output: MONTE_CARLO returns log likelihood of the given data generating process log_lik; Main returns causal support for the target explanation (Fig. 2, Explanation A) csA

# Monte Carlo simulation to calculate likelihood

function MONTE_CARLO (C,m,θ0):

Parameters corresponding to each DAG arrow in Fig. 2:

θP={# initialize parameters

Pr(D), # p disease due to unknown causes

Pr(D|G), #p disease due to gene

Pr(¬D|T)#p no disease due to treatment

}

for parameter θ∈θP do # assign parameters

if θ∈θ0 then Fix parameter at zero: θ=Zeros(m)

else Uniformly sample probabilities: θ=Random(0,1,m)

Calculate probabilities corresponding to contingency table:

P=[ # p no disease vs p disease given…

(1−Pr(D)), #no treat ¬T,no gene ¬G

Pr(D),

(1−Pr(D))+Pr(D)⋅Pr(¬D|T),#T,¬G

Pr(D)⋅(1−Pr(¬D|T)),

(1−Pr(D|G))⋅(1−Pr(D)),#¬T,G

Pr(D|G)+Pr(D)−Pr(D|G)⋅Pr(D),

(Pr(D|G)+Pr(D)−Pr(D|G)⋅Pr(D))#T,G

⋅Pr(¬D|T)+(1−Pr(D|G))⋅(1−Pr(D)),

(Pr(D|G)+Pr(D)−Pr(D|G)⋅Pr(D))

⋅(1−Pr(¬D|T))

]

return average log likelihood of data:

log_lik=∑i∈{1,…,m}∑(C⋅log(P))−log(m)

# Main: causal support calculation

Calculate likelihood of data given causal explanations A and B:

log_likA=MONTE_CARLO(C,10000,{})

log_likB=MONTE_CARLO(C,10000,{Pr(¬D|T)})

return causal support for explanation A: # Bayesian update

csA=(log_likA−log_likB)+(log(0.5)−log(0.5))

Algorithm 
Algorithm 


Algorithm 
Algorithm 
Algorithm 
Algorithm 
Algorithm 



















3.1.5 Performance Evaluation
We wanted to measure how much participants' causal inferences deviated from our normative benchmark, causal support.

The Linear in Log Odds Model & Causal Support
By choosing to model perceived causal support lrrA (see Section 3.1.1) as a function of ground truth causal support on a log odds scale, we leverage a linear in log odds (LLO) model to extend causal support from a normative cognitive model into a descriptive one. Prior work shows that the LLO model accurately describes natural distortions in mental representations of probability [19], [24], [51], [61]. For example, visualization researchers [31] used the LLO model to measure perceptual distortions in probabilistic judgments about intervention effectiveness. Our normative model of causal support itself (see Section 3.1.4) is a sum in log odds units.

Derived Measures
Using a LLO model to measure the correspondence between normative and perceived causal support enables us to estimate (1) participants' sensitivity to changes in ground truth causal support and (2) bias in perceived causal support. From our model, we derive sensitivity and bias per condition as LLO slopes and intercepts, respectively. LLO slopes describe sensitivity to ground truth causal support such that a slope of one indicates ideal sensitivity. One can think of slopes as the weight participants assign to changes in the ground truth log likelihood ratio of explanations A versus B. LLO intercepts describe bias in participants' probability allocations when causal support is zero such that an intercept of 50% indicates no bias. One can think of intercepts as the average prior probability that participants allocate to explanation A when there is no signal in the data.


Fig. 6:
Sensitivity (y-axes) conditioned on two attributes of visual signal for treatment effectiveness (rows, x-axes) and visualizations (columns).

Show All


Fig. 7:
Linear in log odds (LLO) slopes per visualization condition.

Show All

Approach
We used the brms package [8] in R to fit Bayesian hierarchical models on perceived causal support. We adopted a Bayesian workflow called model expansion [14], where we started with a simple model and iteratively added predictors to build up to more complex models, running prior predictive checks, model diagnostics, posterior predictive checks, and leave-one-out cross validation for each version of the model. We centered each prior to reflect a null hypothesis of ideal performance and no bias, and we scaled each prior to be weakly informative while providing sufficient regularization for models to converge. We provide more details about our modeling workflow in our preregistrations5 and Supplemental Materials.6

Model Specification
We used the following model (Wilkinson-Pinheiro-Bates notation [8], [45], [57]) to evaluate participants' responses:
lrrA∼Normal(μA,σA)  μA=csA∗delta_p∗n∗vis+(csA∗delta_p+csA∗n|worker−id)
View Sourcewhere lrrA was perceived causal support for a treatment effect, csA was our normative benchmark, delta_p was the difference in the proportion of people with disease given treatment versus no treatment, n was the sample size as a factor, vis was a dummy variable for visualization condition, and worker_id was a unique identifier for each participant.

We primarily modeled effects on the mean of perceived causal support μA, but our model also learned the residual standard deviation σA. Both σA and the random effects in the μA submodel helped account for the empirical distribution, differentiating between response noise and effects of interest. The term csA∗delta_p∗n∗vis enabled our model to learn how the slope on causal support varies as a function of the visual signal on each trial (delta_p and n) and visualization condition.

3.1.6 Participants & Exclusions
We recruited participants on Amazon Mechanical Turk. Workers had a HIT acceptance rate of at least 97% and were located in the US. We aimed to recruit a total of 400 participants after exclusions using our attention check trials, 80 per visualization condition. We determined this target sample size using a heuristic power analysis based on pilot data and the assumption that the width of confidence intervals would be inversely proportional to N−−√. We recruited a total of 548 participants, and after exclusions we used data from 408 participants in our analysis. We slightly overshot our target sample size because we could not anticipate perfectly how many participants would miss our attention checks (see Section 3.1.4). Although we preregistered that we would exclude participants who failed to allocate at least 50% subjective probability to the most likely causal explanation on either attention check, this criterion proved too strict and would have excluded 48% of our sample. Instead, we opted to use only the easier of the two attention checks for exclusions, resulting in the exclusion of 26% of our sample. All participants were paid regardless of exclusions. We compensated the average participant $2.50 for about 9 minutes.

3.2 Results
We evaluate chart users' causal inferences using a linear in log odds (LLO) model to assess sensitivity to the ground truth and bias in probability allocations when each causal explanation is equally likely.

Sensitivity
A LLO slope of one indicates one-to-one correspondence between the ground truth and users' probability allocations. In all visualization conditions (Fig. 7), we see slopes far below one, indicating that users are much less sensitive than ideal. The only reliable differences between visualization conditions are that filterbars users who do not interact are less sensitive than users in other conditions.

When filterbars users do not interact with the charts, slopes are approximately zero indicating that users are insensitive to signal. Performance improves reliably when users interact with the visualization by applying cross-filters to coordinated multiple views. This is expected because filterbars hide visual signal for the task behind interactions.

Surprisingly, when aggbars users interact with the charts to group by gene or treatment, this leads to lower sensitivity, though this difference is not reliable. To make sense of this result, we analyze interaction log data to see which variables chart users condition on. We find that agg-bars users group the data by gene almost as often as treatment. Compare this to filterbars users, who condition on treatment much more often than gene (see Supplemental Material). This suggests that interacting with visualizations only improves sensitivity to causal support when users deliberately generate views of the data which show counterfactual predictions that can distinguish competing causal explanations.


Fig. 8:
LLO intercepts per visualization condition.

Show All

Visual Signal Effects on Sensitivity
We examine sensitivity in each visualization as function of attributes of the visual signal for our task. In Experiment 1, the signal breaks down into two data attributes, delta p and sample size (see Section 3.1.4). Normatively, LLO slopes equal one regardless of delta p and sample size, however, our model measures differences in sensitivity depending on these data attributes.

Figure 6 shows that in the conditions where slopes are largest-text, icons, bars, aggbars without interaction, and filterbars with interaction-users are more sensitive to causal support at negative values of delta p (e.g., Fig. 6, top inset). The average user in these conditions responds more to evidence against treatment (i.e., falsification) than evidence in favor of a treatment effect (i.e., verification). At positive values of delta p (e.g., Fig. 6, top inset), LLO slopes are similar across conditions, suggesting that differences in performance between conditions are driven in part by differences in sensitivity to falsifying evidence.

We also see in Figure 6 that users of icons, bars, and aggbars are more sensitive to signal when sample size is smaller. This finding is consistent with prior work showing that chart users tend to underestimate sample size when making inferences with data [33], which may be driven by logarithmic perception [54], [61]. Alternatively, we could interpret this result as a cognitive bias where users are unwilling to be certain even when sample size is large enough to support unambiguous inferences, related to non-belief in the law of large numbers [6].

Bias
Intercepts in the LLO model describe bias in users' probability allocations when ground truth causal support indicates that explanation A (i.e., treatment effect) is just as likely as explanation B (i.e., no treatment effect). Under this condition, a normative observer would allocate equal probability to both causal explanations. We derive expected probability allocated to explanation A based on a logistic transform of LLO intercepts, and compare this to the normative benchmark of 50%.

With all visualizations except for filterbars, probability allocations are far below 50% indicating substantial bias (Fig. 8). On average when causal support is zero, users of text tables, icons, bars, and aggbars allocate too little probability to causal explanation A. Users of filterbars, on the other hand, allocate approximately 50% to explanation A. We see the most extreme bias of up to 20% with icons arrays.

Unfortunately, we can only speculate about possible reasons for these biases. We expected that LLO intercepts would indicate average responses near 50% in the absence of signal for all conditions (i.e., a uniform prior), simply because this follows from the structure of the task. Because this pattern of biases across visualizations results from a non-preregistered exploratory comparison, we investigate in Experiment 2 whether these biases replicate for a more complex task.

SECTION 4Experiment 2
In Experiment 2, we evaluate the same visualization designs on a more difficult task. We asked participants to detect confounding in the presence of a known treatment effect by allocating probability across four possible “backdoor paths” [44] (Fig. 9). We extend causal support to handle more than two alternative causal explanations, demonstrating how causal support can be employed in more complex analyses.


Fig. 9:
Dags for possible causal explanations in experiment 2.

Show All

4.1 Method
Experiment 2 was the same as Experiment 1 except for the following changes to response elicitation, modeling, and experimental design.

4.1.1 Task Scenario & Response Elicitation
Participants judge the influence of a gene on both disease and treatment effectiveness by allocating probability across the four DAGs in Figure 9, separately assessing each DAG arrow in a confounding relationship.

Question & Elicitation
We asked participants a similar question as in Experiment 1, where participants allocated 100 votes (i.e., subjective probability) across alternative causal explanations. However, in Experiment 2 we elicited a Dirichlet distribution with four alternatives. Following Chalone et al. [10], [37] and extending our interface from Experiment 1, each time participants allocated a number of votes between 0 and 100 to an option, the remaining votes out of 100 were uniformly distributed across unused response options. These imputed responses were highlighted along with a prompt to, “Adjust your responses until all the numbers reflect your beliefs.” Participants iteratively set and adjusted their probability allocations. We combined these responses into perceived causal support, which we compared to our benchmark.

Perceived Causal Support
When estimating perceived causal support in Experiment 2, we separately evaluated multiple target explanations. Primarily, we targeted belief in explanation D (llrD). confounding:
lrrD=log(responseD∑i={A,B,C}responsei)
View Sourcewhere responseA,responseB,responseC, and responseD were participants' probability allocations to causal explanations A through D, respectively, on each trial. We also separately targeted belief in both of the component DAG arrows that constitute a confounding relationship (Fig. 9): (llrBD) the effect of gene on disease, which appears in explanations B and D; and (llrCD) the effect of gene on treatment effectiveness, which appears in explanations C and D. We define llrBD as follows,
lrrBD=log(∑i={B,D}responsei∑i={A,C}responsei)
View Sourceand we define llrCD similarly. We compare log response ratios llrD,llrBD, and llrCD to corresponding causal support csD,csBD and csCD. Strategy. At the end of the experiment, we asked participants,

How did you use the charts to complete the task? Please tell us what patterns you looked for in the data and what comparisons you made.

We analyzed these qualitative responses to assess whether participants understood how to use the charts for the confounding detection task.

4.1.2 Experimental Design
We manipulated both the visualizations (between-subjects) and the data sets we showed (within-subjects). We showed each participant 19 trials, 18 data conditions (see Section 4.1.3) and one attention check used for exclusions (see Section 4.1.5). We randomized trial order for each participant, inserting the attention check on trial 10.

4.1.3 Stimulus Generation
We generated data sets that spanned a range of ground truth causal support for confounding. Creating these data sets required (1) manipulating data attributes which signaled whether the gene was a confounding factor, and (2) labeling each data set with ground truth causal support.

Our goal was to generate 18 data conditions that varied delta p disease, delta p treatment, and sample size, data attributes which manipulate causal support for confounding. Delta p disease described the difference in the proportion of people with disease in each data set depending on whether they had the gene. Negative values of delta p disease indicated evidence that the gene caused disease, whereas values near zero indicated evidence against a gene effect on disease. Delta p treatment described the difference in the proportion of people with disease within the treatment group depending on whether they had the gene. Negative values of delta p treatment indicated evidence that the gene stopped the treatment from preventing disease, whereas values near zero indicated evidence against a gene effect on treatment. Sample size was the number of people in each data set we showed chart users.

Data Conditions
To generate 18 data conditions, we simulated data from structural models with one parameter per DAG arrow in Figure 9. We manipulated the probability that gene causes disease (3 levels: {0, 0.35, 0.7}), the probability that gene prevents treatment from working (3 levels: {0,0.35,0.7}), and sample size (2 levels: {100, 1000}). We controlled the probability that treatment prevents disease (0.8), probability of disease due to unobserved causes (0.2), base rate of the gene (0.4), and the proportion of each sample with treatment (0.5). We selected these parameters iteratively by sampling data sets and labeling ground truth until half of trials had greater than a 25% chance of having been generated by causal explanation D.

As in Experiment 1, we simulated many data sets for each data condition, and we counterbalanced quantiles of sampling error across participants (see Section 3.1.4). For our attention check trial, we selected the simulated data set that maximized causal support for confounding.

Labeling Ground Truth Causal Support
We extended Griffiths and Tenenbaum's model of causal support [21] to account for more than two alternative causal explanations. We primarily targeted causal support for causal explanation D over explanations A, B or, C,
csD=log(Pr(C|modelD)∑i={A,B,C}Pr(C|modeli))+log(Pr(modelD)∑i={A,BC}Pr(modeli))
View SourceRight-click on figure for MathML and additional features.where C is the data set we label with ground truth, and modelA,modelB,modelC, and modelD, correspond to causal explanations A through D (Fig. 9), respectively. Since we separately targeted belief in both of the component DAG arrows that constitute a confounding relationship (see Section 4.1.1, perceived causal support), we needed to calculate (csBD) ground truth causal support for explanations B or D over A or C:
csBD=log(∑i={B,D}Pr(C|modeli)∑i={A,C}Pr(C|modeli))+log(∑i={B,D}Pr(C|modeli)∑i={A,C}Pr(C|modeli))
View SourceRight-click on figure for MathML and additional features.

We similarly calculated (csBD) causal support for explanations C or D.

The first terms in the formulae for csD,csBD, and csCD, are log likelihood ratios representing the relative compatibility of a given data set with causal explanations A, B, C, and D. We calculated the log likelihood of each data set we showed participants given modelA,modelB,modelC, and modelD using Monte Carlo simulations similar to Algorithm 1. In Experiment 2, we introduced one more parameter Pr(¬T|G) to our structural models, representing the probability that the gene prevents the treatment effect. We incorporate this parameter into our Monte Carlo simulations (Alg. 1) by making the following substitutions:
20:(Pr(D|G)+Pr(D)−Pr(D|G)⋅Pr(D))⋅(Pr(¬D|T)21: .(1−Pr(¬T|G))+(1−Pr(D|G))⋅(1−Pr(D)),22:(Pr(D|G)+Pr(D)−Pr(D|G)⋅Pr(D))23: .((1−Pr(¬D|T))+Pr(¬D|T)⋅Pr(¬T|G))
View Source

Under modelA we sampled Pr(D), and Pr(¬D|T) uniformly on the interval [0, 1] and fixed Pr(D|G) and Pr(¬T|G) at zero, representing assumptions that the gene impacts neither disease or treatment. Under modelB we sampled Pr(D),Pr(D|G), and Pr(¬D|T) uniformly and fixed Pr(¬T|G) at zero, representing the assumption that the gene has no effect on treatment. Under modelC we sampled Pr(D),Pr(¬T|G), and Pr(¬D|T) uniformly and fixed Pr(D|G) at zero, representing the assumption that the gene has no effect on disease. Under modelD we sampled all four parameters uniformly to represent confounding.

The second terms in the formulae for csD,csBD, and csCD are log ratios of the prior probabilities of the target explanation(s) versus other possible explanations. Again, we assumed a uniform prior to create an unbiased benchmark for our task such that 25% was the normative prior probability for each causal explanation A, B, C, and D, respectively.

4.1.4 Performance Evaluation
Again, we used linear in log odds (LLO) models [19], [61] to describe discrepancies between perceived and normative causal support. We also conducted a qualitative analysis of participants' reported strategies.

Model Specification
We used three inferential models because we had three dependent variables, representing perceived causal support for confounding (lrrD) and for the two constituent effects of confounding (lrrBD and lrrCD), Here, we show only the models on lrrD and lrrBD because the models on lrrCD and lrrBD are identical in form, with csCD and delta_p_t replacing csBD and delta_p_d as predictors:
lrrDμDlrrBDμBD∼Normal(μD,σD)=csD∗delta_p−d∗delta_p−t∗n∗vis+(csD∗delta_p−d+csD∗delta_p−t+csD∗n|worker−id)∼Normal(μBD,σBD)=csBD∗delta_p_d∗n∗vis+(csBD|worker−id)
View Sourcewhere lrrD,lrrBD, and lrrCD were perceived causal support for a confounding, the gene effect on disease, and the gene effect on treatment, respectively, csD,csBD, and csCD were our normative benchmarks corresponding to each log response ratio, delta_p_d was the difference in the proportion of people with disease given gene versus no gene, delta_p_t was the difference in the proportion of people with disease among those in the treatment group given gene versus no gene, n was the sample size as a factor, vis was a dummy variable for visualization condition, and worker_id was a unique identifier for each participant.

We primarily modeled effects on the mean of perceived causal support μD,μBD, and μCD, but our models also learned residual standard deviations σD,σBD, and σCD. The residual standard deviations and random effects in each model helped us separate patterns in responses from noise and individual differences. In the first model, we used the term cs5D∗delta_p_d∗delta_p_t∗n∗vis to learn how sensitivity to causal support for confounding varies as a function of sample size n and visualization vis. In the second and third models, we used the terms csBD∗delta_p_d∗n∗vis and csCD∗delta_p_d∗n∗vis to learn how sensitive users in each visualization condition were to the gene effects on disease delta_p_d and treatment delta_p_t, respectively.

Qualitative Analysis
We wondered how well participants would intuit how to perform the confounding detection task, considering it was more difficult than the task in Experiment 1, and we provided no training. To address this we applied a deductive coding scheme. We coded participants' strategy descriptions as uninformative if they didn't describe a strategy. Otherwise, we coded whether or not participants described adequate strategies for judging delta p disease, delta p' treatment, or sample size (see Section 4.1.3), and we coded confusion if they stated they were confused or described an incorrect strategy.

4.1.5 Participants & Exclusions
We used a similar approach to power analysis as in Experiment 1 to determine a target sample size of 500 participants after exclusions. We recruited a total of 703 participants, and after exclusions we used data from 519 participants in our analysis. Although we preregistered that we would exclude participants who allocated less than 25% probability to confounding on an attention check trial where confounding was very likely (see Section 4.1.3), this criterion would have excluded 39% of our sample. We relaxed the cutoff to less than 20% probability of confounding to allow for additional response error, resulting in a 26% exclusion rate. We paid participants $3.04 for 14 minutes on average.


Fig. 10:
Sensitivity (y-axes) conditioned on three attributes of visual signal for confounding (rows, x-axes) and visualization conditions (columns).

Show All

Fig. 11: - Linear in log odds (LLO) slopes per visualization condition.
Fig. 11:
Linear in log odds (LLO) slopes per visualization condition.

Show All

Fig. 12: - LLO intercepts per visualization condition.
Fig. 12:
LLO intercepts per visualization condition.

Show All

4.2 Results
We use a linear in log odds (LLO) model to describe performance in terms of sensitivity to ground truth causal support and bias in probability allocations when all four causal explanations are equally likely.

Sensitivity
A LLO slope of one indicates ideal sensitivity to the log likelihood of the data given a set of causal explanations Similar to Experiment 1, slopes in all visualization conditions are closer to zero than one (Fig. 11), indicating under-sensitivity to the ground truth.

Interacting with filterbars seems to improve sensitivity, while interacting with aggbars seems to decrease sensitivity, although these differences are not reliable. It is surprising to see a similar pattern of results for interactive visualizations in both experiments, since we expected interactive visualizations to be more helpful for detecting confounding than for detecting a treatment effect. Detecting confounding requires users to look for more complex counterfactual patterns in order to distinguish between causal explanations, and manipulating data aggregation and filtering should help users to query visualizations for these patterns. When we analyze interaction logs (see Supplemental Materials), we see that filterbars users interacted with the visualizations more frequently and created more task-relevant views of the data than aggbars users, which may help to explain why interacting with filterbars was somewhat more helpful than interacting with aggbars.

Visual Signal Effects on Sensitivity
We examine sensitivity in each visualization condition to the three visual signals for confounding in our task (delta p disease, delta p treatment, and sample size; see Section 4.1.3). Normatively, slopes are one regardless of these visual signals.

Figure 10 shows users are more sensitive to causal support at values of delta p disease and delta p treatment near zero, with the exception of filterbars users who don't interact. This pattern is consistent with the findings of Experiment 1 in that chart users respond more to evidence against a given causal effect than evidence in favor of an effect.

In Figure 10, we also see that users of every visualization but filter-bars are more sensitive when sample size is smaller. This pattern is consistent with prior work [6], [33] and the results of Experiment 1.

Bias
LLO intercepts describe bias in probability allocations when the data are equally likely under each alternative causal explanation. We derive expected probability allocated to explanation D based on LLO intercepts and compare this to the normative benchmark of 25%.

Figure 12 shows that, with all visualizations but text tables, users underestimate the probability of confounding in the absence of signal. The fact that biases for each visualization condition differ between Experiments 1 and 2 suggests that these results are task-specific. Future work should study reasons for these biases and what visual analytics software can do to help calibrate analysts' probability allocations.

Strategies
We assess users' self-reported strategy descriptions. 235 of 519 (45%) users included in our analysis gave uninformative responses and were excluded from further analysis. 42 of 284 (15%) remaining users either stated they were confused or described an incorrect strategy.

However, many users intuited the important signals in the data:

“I relied more on the ‘no treatment’ cells to consider whether the gene causes the [disease], trying to look at ratio of ‘disease’ and ‘no disease’ within those two quadrants … [I] tried to consider the actual counts remembering that small numbers mean loose estimates but this was easy to overlook. Then I compared the two purple bars in the ‘gene no’ top-half of graph to estimate the treatment effect… and did the same for the two lower purple bars to see if treatment equally effective in those with the gene.”

222 of 284 (79%) described an adequate strategy for inferring the gene effect on disease. 81 of 284 (29%) mentioned sample size information. 168 of 284 (59%) described an adequate strategy for inferring the gene effect on treatment effectiveness. These results suggest that much of our data represent a reasonable understanding of the task, yet participants still appeared to struggle to use the visualizations effectively.

SECTION 5Discussion
We demonstrate the utility of causal support for evaluating inferences with visualizations, successfully measuring expected patterns in the quality of chart users' causal inferences. For example, filterbars users should not have been able to perform either task without interacting because the visual signals required to perform the tasks were hidden behind interactions. Our method shows that filterbars users were completely insensitive to the signal in data when they did not interact. Similarly, our models corroborate prior work suggesting that chart users underweight sample size when making inferences [6], [33]. Findings like these reassure us that causal support can help us understand how users struggle to use visualizations to evaluate causal hypotheses.

Our findings point to unsolved design challenges for supporting causal inferences with visual analytics (VA) tools. Contrary to what we might expect given the emphasis of visualization research on evaluating encodings and interaction techniques, using different encodings for count data doesn't appear to improve sensitivity to evidence for causal inferences beyond text contingency tables. Similarly, common interaction techniques in VA tools, such as manipulating data aggregation or cross-filtering coordinated multiple views, don't seem to improve causal inferences beyond what users can achieve with simpler static visualizations. Interacting with visualizations seems to help or hurt sensitivity depending on how deliberately signal-seeking users are and whether interacting is necessary in order to expose the visual signal in the data. This suggests that VA tools designed to optimize easy exposure of data are not sufficient for supporting causal inferences.

We also find systematic biases in the way that chart users respond to specific visual signals in charts. Chart users seem ubiquitously more sensitive to falsifying evidence than they are to verifying evidence. This may reflect a cognitive bias where analysts are more responsive to discrepancies, between observed data and the counterfactual patterns expected under a given causal explanation, than they are to similarities between observed data and counterfactual patterns. Interestingly, this bias may be somewhat rational to the extent that verifying an inference is probabilistic, whereas the logic of falsification is deductive and thus “more powerful” in that it can definitively rule out an explanation [47].

Insensitivity to sample size remains a major challenge for informal statistical inferences, and it appears not to be sufficiently addressed by common chart types for showing count data. Even icon arrays, which emphasize sample size as the number of equal-sized dots, don't seem to mitigate this problem. Prior work [6], [33] suggests this may be due to perceptual underestimation of sample size and cognitive bias against claiming certainty in inferences. Additionally, our qualitative results suggest chart users may not intuitively pay as much attention to sample size as they do to other signals when making causal inferences.

Consistent with an aversion to believing causal relationships exist, we find that chart users tend to underestimate the probability of a given DAG arrow. In the absence of any signal differentiating between causal explanations, chart users allocate more probability to explanations that posit fewer relationships, rather than allocating probability uniformly across alternatives. Though this tendency interacts with task and visualization in ways that warrant further study, it may reflect an overall cognitive bias toward believing in simpler causal explanations.

5.1 Limitations & Future Work
We set out to run a proof-of-concept study establishing causal support as an evaluation method for VA tools, and our study raises many unanswered questions. A primary limitation of this work is that we recruited participants on Mechanical Turk, who may be less sensitive to causal support than real data analysts to the extent that they may use VA tools less deliberately. However, our qualitative analysis suggests that many participants understood the task and used reasonable strategies. Future work may find causal support helpful in evaluating current practices or novel interfaces with smaller pools of participants, insofar as real data analysts give less noisy responses than crowdworkers. Questions remain about whether our findings generalize for other data types (e.g., continuous [40] and event stream data [39]), for domains outside of medicine, and for analysis scenarios with more complex possible data generating models. Though we suspect our findings will persist in some form across user populations and analysis scenarios, visualizations probably will support some other causal inference tasks better than they support differentiating possible data generating processes.

5.2 Improving Visual Analytics for Causal Inference
A theme in visual causal inference is that analysts do not always know what to look for in data [4], [60]. Causal inferences differentiating between possible data generating processes (DGPs) require comparisons between patterns in observed data and counterfactual patterns under a specific DGP [20], [44]. Users of VA software may struggle with causal inferences insofar as they fail to imagine counterfactual predictions.

Prior work in statistics and visualization argues for model checks that make comparisons between data and model predictions explicit [16], [17], [26]. For example, workflows in Bayesian statistics frequently employ prior and posterior predictive checks [14]. Visualizing model predictions alongside empirical data could support causal inference by externalizing discrepancies and similarities between observed and expected patterns.

We envision a VA workflow where analysts cycle between interactively specifying models (e.g., [35]) and generating model checks to gauge model compatibility with their data. This echos calls to make models themselves a primary goal of visual data analysis [2]. Causal support solves an important problem in realizing this vision, defining a “good” model check as one which supports sensitive inferences among a set of candidate DGPs. Though it may be difficult to come up with an exhaustive set of DGPs in many real world applications, we think that this approach would be fruitful even with a relatively simple set of models that a knowledgeable analyst might provisionally entertain. Causal support cannot guard against analysts ignoring possible models, but it can be used to evaluate visualization and interaction designs intended to help analysts collate and compare alternative models.

SECTION 6Conclusion
We contribute two crowdsourced experiments demonstrating an approach to evaluating causal inferences with visual analytics (VA) tools. No visualization or interaction designs we tested lead to reliably better causal inferences than text contingency tables, suggesting that common VA tools designed for data exposure may not be sufficient for supporting causal inferences. We point to perceptual and cognitive biases which seem to make visual causal inferences difficult, including tendencies to underweight both evidence verifying a causal relationship and evidence from large samples. We discuss how formal models of causal support can be used to evaluate VA systems that place an emphasis on helping users reason about possible data generating processes.