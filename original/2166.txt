We present an approach utilizing Topological Data Analysis to study the structure of face poses used in affective computing, i.e., the process of recognizing human emotion. The approach uses a conditional comparison of different emotions, both respective and irrespective of time, with multiple topological distance metrics, dimension reduction techniques, and face subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our topology-based approach captures known patterns, distinctions between emotions, and distinctions between individuals, which is an important step towards more robust and explainable emotion recognition by machines.
SECTION 1Introduction
Affective computing, computer-based detection of human affects, has applications that span education (e.g., judging learners' confidence), healthcare (e.g., judging pain), and product marketing (e.g., measuring consumers' response to products). Early work in measuring affect began in the late 1960's spearheaded by Ekman and Friesen [23]. Their work culminated in a classification of six basic emotions: anger, disgust, fear, happiness, sadness, and surprise [24], which were later expanded [22]. The field of affective computing has seen significant growth since the seminal work from Rosalind Picard [65]. The vast majority of research in affective computing has been focused on machine learning algorithms trained on emotion data to classify affect. Like many machine learning solutions, these neural networks focused on classifying the input emotion and ignored data inspection and decision-making explainability.

Fig. 1. - Our affective computing visualization provides numerous options for comparing and contrasting the data. For example, the small multiples view (left) is comparing a male (left) subject to a female (right) subject considering different subsets of facial landmarks (columns), across emotions (rows) of anger, disgust, fear, happiness, sadness, and surprise. Each point represents one facial pose. The embedding graph (top right) compares all facial poses across all emotions, in this case showing the female full face using MDS on non-metric topology. The 3D landmarks (lower right) show a single facial pose per emotion. Settings are selected on the bottom.
Fig. 1.
Our affective computing visualization provides numerous options for comparing and contrasting the data. For example, the small multiples view (left) is comparing a male (left) subject to a female (right) subject considering different subsets of facial landmarks (columns), across emotions (rows) of anger, disgust, fear, happiness, sadness, and surprise. Each point represents one facial pose. The embedding graph (top right) compares all facial poses across all emotions, in this case showing the female full face using MDS on non-metric topology. The 3D landmarks (lower right) show a single facial pose per emotion. Settings are selected on the bottom.

Show All

To inspect the data, an effective visual representation of emotion data must address numerous challenges. First, the affect data are quite large, captured by multiple high-speed video cameras. Fortunately, previous affective computing research already partially addressed this issue by reducing the data to 83 landmark points tracked temporally. Nevertheless, the problem remains challenging becAUse patterns in emotion occur over extended time periods, represented by a series of 83-landmark poses. Furthermore, patterns of interest may occur in different time sequences lasting for different lengths of time, making alignment and comparison non-trivial. Finally, changes in landmarks are simultaneously subtle and subject to noise from the extraction process, making them difficult to observe.

This paper presents a visual analytics approach utilizing Topological Data Analysis (TDA) to examine emotion data respective and irrespective of time. By using TDA to address this problem, our approach can capture and track the topological “shape” of facial landmarks over time in a manner robust to noise [20]. After analysis, the data are presented for investigation using familiar visualizations, e.g., timelines (see Fig. 5 top) and scatterplots (see Fig. 1 top right), and through landmark-based representations (see Fig. 1 bottom right or Fig. 10). These interfaces enable tracking facial movement, comparing emotions, and comparing individuals, while also providing the ability to derive precise explanations for features identified in the data.

A natural question at this point would be, why is TDA well-suited to this problem? Our approach utilizes one of the foundational tools of TDA, namely persistent homology. There are four main advantages to this tool. (1) Persistent homology has a solid mathematical grounding, and its output is explainable. (2) Persistent homology extracts homology groups, which in our context are (connected) components and tunnels/cycles. These fundamental shapes match well with the shapes of a face. (3) The homology groups are extracted at multiple scales, without the need to specify any thresholds or other parameters. This means that persistent homology captures all of the topological structures without any user intervention1. (4) Finally, it classifies features by their importance with a measure called persistence, which automatically differentiates topological signal from noise [13].

The specific contributions of this paper are:

a mapping of affective computing data to TDA (see Sect. 4), including a novel non-metric formulation of geometry for faster and more accurate topology extraction (see Sect. 4.3);

a visual analytics interface that enables analyzing, comparing, and contrasting multiple data configurations (see Sect. 6);

an evaluation that uses our methodology to explain features in data that were extracted by state-of-the-art emotion detection machine learning algorithms (see Sect. 7.2); and

an evaluation of the ability of TDA to differentiate emotions within the same individual (see Sect. 7.3) and differentiate multiple individuals showing the same emotion (see Sect. 7.4).

Perhaps most importantly, our approach opens the door to explainability in a way that may help to unlock open questions in the affective computing community.

SECTION 2Background in Affective Computing
Affective computing has applications in fields as varied as medicine [85], entertainment [31], and security [45]. Most notably, the expression recognition sub-field focuses on detecting subjects' affective states automatically.

2.1 Expression Recognition
While successful 2D facial-expression image recognition exists [29], [43], [49], the approaches suffer from weaknesses, such as occlusion from, e.g., a rotating head. We focus our discussion instead on a few representative 3D facial recognition approaches. Zhen et al. [91] developed a model that localized points within each muscular region of the face and extracted features that include coordinate, normal, and shape index [46]. The features were then used to train a Support Vector Machine (SVM) [77] to recognize expressions. Xue et al. [82] proposed a method for 4D (3D + time) expression recognition, which showed promise differentiating difficult emotions, such as anger and sadness. The method extracted local patch sequences from consecutive 3D video frames and represented them with a 3D discrete cosine transform. Then, a nearest-neighbor classifier was used to recognize the expressions. Hariri et al. [36] proposed an approach to expression recognition using manifold-based classification. The approach sampled the face by extracting local geometry as covariance regions, which were used with an SVM to recognize expressions.

Some recent techniques showed that not all regions of the face carry the same importance in emotion recognition. Hernandez-Matamoros et al. [37] found that segmenting the face based on the eyes and mouth resulted in improved expression recognition. Fabiano et al. [28] further illustrated that different areas of the face carry different levels of importance for emotions, e.g., one subject happiness had more important features on the right eye and eyebrow, while embarrassment had more on the left eye and eyebrow. We utilize this information in our visualization design by targeting specific subsets of facial features.

2.2 Affective Computing in Visualization
There has been limited work in the visualization community on affective computing; what exists has been primarily focused on visualizing affective states, i.e., considering valence and arousal, not inspecting the landmarks used as input to affective computing algorithms.

Early work on visualizing affective states concerns the glyph-based Self-Assessment Manikin (SAM), which measures pleasure, arousal, and dominance of a person's affective state [7]. Cernea et al. [9] later described guidelines for conveying the user emotion through the use of widgets that depict the affective states of valence and arousal. The widgets employed emotion scents, hue-varied colormaps representing either valance or arousal, e.g., red and green represent negative and positive valance, respectively. Emotion-prints was an early system to provided real-time feedback of valance and arousal to users using touch-displays [10]. More recently, Kovacevik et al. [47] employed ideas from SAM and emotion scents to create a glyph for simultaneous representation of valence and arousal. Their research focused on video game players' and developers' awareness of emotions elicited from a particular gaming experience. For visualizing affect over extended periods, AffectAura provided an interface that enabled users to visualize emotional states over time for the purpose of reflection [56].

There has also been some work visualizing the affective state of multiple individuals using, e.g., virtual agents in collaborative work [11] or using a visual analytics interface to access the emotional state of students in a classroom [86]. Qin et al. [66] created HeartBees, which was an interface to demonstrate the affect of a crowd using physiological data. The interface used an abstract flocking behavior to demonstrate the collective emotional state.

In contrast to all of these prior approaches, our work focuses on using TDA and visualization to investigate the data used in classifying expression, i.e., the input data, not the emotional state itself. There has been some recent work that looked at the explainability of deep networks in expression recognition, e.g., [62]. These approaches focus on visualizing heatmaps that highlight what parts of the image most influenced decision making, not necessarily why.

2.3 Dataset
To evaluate our approach, we use the BU4DFE 3D facial expression dataset [84], which has been extensively used for expression recognition [12], [27], [63], [74], 3D shape reconstruction [32], [33], [53], face tracking [8], [64], and face recognition [3], [30], [42], [72]. The dataset contains 101 subjects (58 female and 43 male) from multiple ethnicities, including Caucasian, African American, Asian, and Hispanic, with an age range of 18–45 years old. Each modality has the six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. For each sequence, the expression is the result of gradually building from neutral to low then high intensity and back again. Each of the video sequences is 3–4 seconds in length.

The data are captured using the Di3D dynamic face capturing system [17], which consists of three cameras, two to capture stereo and one to capture texture. Passive stereophotogrammetry is used on each pair of stereo images to create the 3D facial pose models with an RMS accuracy of 0.2 mm. Each 3D model contains 83 facial landmarks (see Fig. 2), which correspond to the key areas of the face that include the mouth, eyes, eyebrows, nose, and jawline. The landmarks are the result of using an active appearance model [14] that detects the landmarks on the 2D texture images, which are aligned and projected into the corresponding 3D models.

SECTION 3Overview of the Pipeline
TDA has received significant attention in the visualization community, e.g., [73]. We utilize a foundational tool of TDA, persistent homology, which has been studied in graph analysis [34], [35], [67], [71], high-dimensional data analysis [78], and multivariate analysis [68]. We utilize persistent homology to capture the topology of the landmarks of each facial pose into a structure known as a persistence diagram. We then compare the topology of different subsets of facial poses to reveal their relationships. Our processing pipeline contains three main stages, which are fed into the visualization (see Sect. 6).

The first stage is extracting the topology of a single facial pose. We offer two variations, a Euclidean metric-based approach (see Sect. 4.1) and a novel non-metric-based approach (see Sect. 4.3).

Once the topologies of individual poses are extracted, we compare the topology to that of other poses to determine their pairwise dissimilarity (see Sect. 5.1).

Finally, using the topological dissimilarity, we utilize a variety of dimension reduction techniques to highlight different aspects of the dissimilarity between groups of facial poses (see Sect. 5.2).

 - 
 - 
 - 
 - 
 - 
Fig. 2. - An illustration of persistent homology on the 83 facial landmarks on the female subject F001. (a) The persistent homology is calculated by forming a rips filtration and tracking/extracting the associated homology groups. Starting at $r=0$, if the pairwise distance between any two or three points is less than $r$, an edge or triangle is formed, respectively. As $r$ increases, components merge, and tunnels form and disappear. (b) The topology is visualized with a persistence diagram. Square points are H0 components (the triangle indicates a single infinite ho component), and the hollow circles are the H1 tunnels. The horizontal position of points is their birth $r_{b_{i}}$ and their vertical position is their death $r_{d_{i}}$. Distance from the dotted diagonal, as well as object size, is proportional to its persistence.
Fig. 2.
An illustration of persistent homology on the 83 facial landmarks on the female subject F001. (a) The persistent homology is calculated by forming a rips filtration and tracking/extracting the associated homology groups. Starting at r=0, if the pairwise distance between any two or three points is less than r, an edge or triangle is formed, respectively. As r increases, components merge, and tunnels form and disappear. (b) The topology is visualized with a persistence diagram. Square points are H0 components (the triangle indicates a single infinite ho component), and the hollow circles are the H1 tunnels. The horizontal position of points is their birth rbi and their vertical position is their death rdi. Distance from the dotted diagonal, as well as object size, is proportional to its persistence.

Show All

SECTION 4Topological Data Analysis of Facial Landmarks
We consider two variations for extracting the topology of facial poses, a Euclidean metric approach, followed by a novel non-metric variant.

4.1 Euclidean Metric Persistent Homology on Landmarks
Homology deals with the topological features of a space. Given a topological space X, we are interested in extracting the H0(X) and H1(X) homology groups, which correspond to (connected) components and tunnels/cycles of X, respectively2. In practice, there may not exist a single scale that captures the topological structures of the data. Instead, we use a multi-scale notion of homology, called persistent homology, to describe the topological features of a space at different spatial resolutions. We briefly describe persistent homology in our limited context. Nevertheless, understanding persistent homology can be daunting for those who are unfamiliar with it. For a high-level overview, see [79], or for detailed background, see [19].

To calculate the persistent homology of a single facial pose, we first calculate the Euclidean distance between all 83 landmarks. We then apply a geometric construction, the Rips complex, R(r), on the point set. In brief, for a given distance, r, the Rips complex has all 0-simplicies, i.e., points, for all values of r. A 1-simplex, i.e., an edge, between two points is formed iff r is greater than or equal to their distance. A 2-simplex, i.e., a triangle, is formed among three points iff r is greater than or equal to every pairwise distance between the points.

To extract the persistent homology (see Fig. 2(a)), we consider a finite sequence of increasing distances, 0=r0≤r1≤⋯≤rm=∞. A sequence of Rips complexes, known as a Rips filtration, is connected by inclusions, R(r0)→R(r1)→⋯→R(rm), and the homology of each is calculated, tracking the homomorphisms induced by the inclusions, H(R(r0))→H(R(r1))→⋯→H(R(rm)). As the distance increases, topological features, i.e., components and tunnels, appear and disappear. The appearance is known as a birth event, rbi, and the disappearance is known as a death event, rdi. The birth and death of all features are stored as a multi-set of points in the plane, (rbi,rdi), known as the persistence diagram, which is often visualized in the scatterplot display (see Fig. 2(b)). From the points, we devise an importance measure, called persistence, which helps to differentiate signal from noise. The persistence is simply the difference between the birth and death of a feature, i.e., rdi−rbi. Furthermore, in visualizations of the persistence diagram, such as Fig. 2(b), distance from the diagonal dotted line represents the persistence of a feature.

In addition to considering all the topology of all landmarks, we provide the user the functionality to consider only related subsets of features. In particular, they have the option of including/excluding jawline, mouth, nose, left/right eyes, and left/right eyebrows in the calculation of the topology.

4.2 Interpolating Known Geometry
Our computation using facial landmarks ignores an important aspect of the data, namely the known connectivity between landmarks. In other words, landmarks of, e.g., the mouth, have known connectivity to their neighboring landmarks. Fig. 3(f) shows this connectivity. This raises two questions. First, does our failure to consider this connectivity impact the features we extract, and second, how do we efficiently consider the connectivity?

We first consider using interpolation of the connectivity to supersample additional landmarks. For our experiment, we take the known connectivity and interpolate across each edge, such that points are no further than a user-defined ε apart. Fig. 3(b) through Fig. 3(e) show four examples with ever-smaller ε values. As expected, as ε gets smaller, the data looks increasingly similar to the known connectivity in Fig. 3(f).

We now consider the impact of the connectivity by comparing the persistence diagrams of H1features in the original data in Fig. 3(a) to the lowest ε data in Fig. 3(e). The persistence diagrams are clearly different (the H0 features are also different but more difficult to observe pictorially). The difference is exceedingly important becAUse it means using the 83 landmark points alone is insufficient to capture the topological structure of the data.

To overcome this limitation, we considered using the supersampled landmarks for calculations. However, there are three interrelated problems to this approach. (1) The first is the challenge of selecting an appropriate ε value. The smaller the value, the closer the representation is to the geometric structure. For example, Fig. 3(c) appears sufficient for this example, but it is unclear if this is sufficient for all of the data, leading one to perhaps select an even smaller ε. (2) However, the second challenge is that the smaller the ε, the longer the computation time for detecting the topological features. Fig. 4(a) shows that as ε is divided in half, the compute time grows exponentially. (3) The third related challenge is that the smaller ε, the greater the number of topological features generated. Fig. 4(b) shows this extreme growth. To make matters worse, the vast majority of these features are topological noise with very low persistence. In other words, they do not contribute to our understanding of the shape of the face.

4.3 A Non-Metric Variant of Persistent Homology
We instead use a novel modification to the persistent homology calculation to utilize this connectivity as follows. Instead of considering 83 landmark points, we consider the relationship between 81 landmark edges formed by the known connectivity of the landmark points (see Fig. 3(f)). We calculate a distance matrix representation of the landmark edges, where the distance is the shortest Euclidean distance between line segments. Finally, we run persistent homology calculations on this distance matrix.

One immediate question should be the appropriateness of this configuration for persistent homology calculations, particularly considering that this representation breaks two important axioms of a metric space, namely the identity of indiscernibles and the triangle inequality. Fortunately, persistent homology calculations themselves do not explicitly require a metric space—they have a weaker requirement of inclusion [19]. In other words, as long as in the filtration R(ri)⊂R(ri+1), the calculation can proceed.



Fig. 3.
Illustration of supersampling and non-metric persistent homology shows the data and the persistence diagram of H1 features. (a) The original 83 landmarks from a single pose on the female subject F001. (b-e) Supersampling the landmarks with different ε values shows a significant difference in the persistence diagram between the (a) original and (e) supersampled data. (f) Our non-metric representation of the data requires significantly less data and produces a persistence diagram similar to that of (e) supersampling.

Show All

The challenge is that the Rips complex does require that the underlying space is metric. We define a new non-metric Rips complex that satisfies the inclusion property, where: 0-simplicies, representing landmark edges, are present for all values of r; 1-simplicies appear when r is strictly greater than the non-metric distance between a pair of 0-simplicies; and 2-simplices appear when r is strictly greater than all of the non-metric distances of the three related 1-simplicies. Fortunately, this definition is similar enough to the standard Rips complex that careful ordering of inclusions (i.e., observing the strictly greater than cases) in the filtration allows us to utilize conventional persistent homology tools on our non-metric distances.

Fig. 3(f) shows the landmark edges and the persistence diagram of the associated H1 features. Our non-metric approach overcomes all three limitations of supersampling. (1) The result is very similar to the output of the supersampling in Fig. 3(e) without the need for specifying any ε parameter. (2) Furthermore, Fig. 4(a) shows that the compute time for our non-metric approach is approximately the same as that of the original 83 landmark points. (3) Finally, Fig. 4(b) shows the number of topological features output is small (i.e., we avoid outputting extraneous topological noise).

SECTION 5Comparing Facial Pose Topology
Thus far, we have introduced a method for extracting the topological features from a single facial pose. We now describe how we compare the topology of multiple facial poses. We start by describing the notion of topological distance between persistence diagrams, which serves as a pairwise dissimilarity between them (see Sect. 5.1). Next, we discuss how dimension reduction is used on all pairwise dissimilarities to cluster, compare, and summarize changes in topology (see Sect. 5.2).



Fig. 4.
Plots of (a) The compute time and (b) The number of topological features generated for the original 83 landmarks, six levels of supersampling (ε=8, 4, 2, 1, 0.5, and 0.25), and our non-metric representation. The regression line in (a) only considers the supersampling data points.

Show All

5.1 Dissimilarity Between Poses
Once persistence diagrams are calculated, we wish to explore the relationship between them by performing pairwise comparisons of features of the persistence diagrams. This type of pairwise comparison is commonly performed using bottleneck or Wasserstein distance [20]. Intuitively speaking, these measures find the best match between the features of two persistence diagrams and report the topological feature of the largest distortion, in the case of bottleneck distance, or the average topological distortion, in the case of Wasserstein distance.

Technically speaking, consider two persistence diagrams, X and Y, let η be a bijection, with all diagonal points, (x,x), added for infinite cardinality [44]. The bottleneck distance is W∞(X,Y)=infη:X→Ysupx∈X∥x−η(x)∥∞, and the 1-Wasserstein distance, which we use, is W1(X,Y)=infη:X→YΣx∈X∥x−η(x)∥∞. Our implementation computes the bottleneck and 1-Wasserstein distance for Ho and H1 features separately, and combines the results. In other words, for bottleneck, W∞¯¯¯¯¯¯¯¯¯(X,Y)=max(W∞(XH0,YH0),W∞(XH1,YH1)), and for 1-Wasserstein, W1¯¯¯¯¯¯¯(X,Y)=W1(XH0,YH0)+W1(XH1,YH1).

5.2 Summarizing Topological Dissimilarity
Once the set of all persistence diagrams is calculated, we explore the relationship between them by calculating all pairwise dissimilarities between poses, forming a dissimilarity matrix representing all of the topological variations between facial poses. However, a dissimilarity matrix, such as this, is difficult to explore directly. We investigated several options to represent and evaluate the relationship between different facial poses and emotions. Importantly, each technique preserves a different aspect of the dissimilarity matrix, providing different perspectives on the data.

The first approach we used is 1D relative distance. In this approach, a keyframe or focal pose is selected by the user. All other facial poses are positioned by their relative distance (i.e., pairwise distance) to that keyframe. Relative distance perfectly preserves the relationship between the keyframe and all other frames. It does not, however, provide information about the relationship between other pairs of frames.

Next, we consider two dimension reduction techniques, with each using the pairwise dissimilarity matrix directly. We first consider Multidimensional Scaling (MDS) [48], which tries to preserve pairwise distances between the topology of poses. Second, we use t-SNE [76] and UMAP [57], which attempt to preserve the clustering structure by considering a local neighbor. Both t-SNE and UMAP contain hyperparameters that can impact the structures visible to the user. We have performed a structured evaluation of various hyperparameters and found that the structures visible in our results are, by-and-large, stable across a wide variety of parameter values (see our supplement for an example). Therefore, we use the default parameters in our evaluation. To measure the dimension reduction quality for all methods, we calculate the goodness-of-fit using the Spearman rank correlation of the Shepard diagram (denoted in the lower right of images as Rank).

Note that none of these approaches directly consider time. Nevertheless, the temporal components of the data are presented in the visualization when relevant.

SECTION 6Visualization
To examine the topological structure of facial landmark data, we built a visualization (see Fig. 1 and Fig. 10) with the following design criteria:

provide multiple ways to evaluate temporal and non-temporal aspects of the data (e.g., animated, static, and non-temporal visualizations);

provide multiple conditional perspectives (e.g., bottleneck vs. Wasserstein, MDS vs. t-SNE vs. UMAP, etc.) on the topology;

allow comparison of data between two or more emotions;

allow for investigating subsets of landmarks; and

provide direct explanations for the topological differences between facial poses.

Small Multiples (Fig. 1 Left)
Our interface features a small multiples display for comparing different data conditions. The interface features a comparison between two conditions, including comparing two subjects, bottleneck vs. Wasserstein distance, metric vs. non-metric topology, t-SNE vs. MDS, etc. [D2]. The interface further divides each column into comparisons of different subsets of facial features, including full face, eyes+nose, mouth+nose, and eyebrows+nose [D4]. Finally, each row represents one of the six main emotions, anger, disgust, fear, happiness, sadness, and surprise. The user selects the data for the embedding by selecting a column and enabling/disabling specific emotions of interest by selecting rows [D3]. Each small multiple is shown using the visualization modality chosen in the settings at the bottom.

Embedding Graph (Fig. 1 Top Right)
The primary visualization tool in our approach is the embedding graph, which is either a line chart or scatterplot representation of time-varying topological data for the selected emotions [D1]. For the line chart, time is plotted horizontally, while the 1D relative distance is plotted vertically (see Fig. 5 top) [D2]. Each selected emotion is overlaid for time-dependent comparison [D3]. The keyframe is user-selectable, and the visualization updates as the keyframe is modified. The scatterplot representation uses 2D dimension reduction for both the horizontal and vertical axes. The choice of MDS, t-SNE, or UMAP is provided for the user. The data are either shown as points or connected via a path (see Fig. 6(a)) if the user wants a temporal context [D1].

The plot is also interactive-selecting a point updates the time-index used in other visualizations, e.g., the 3D landmarks.

3D Landmarks (Fig. 1 Lower Right)
The 3D landmarks represent the data of the current time-index for the respective emotion [D1]. The faces are placed side-by-side for comparison [D3][D5]. The sliders beneath each can be used to animate or adjust their time-index [D1].

Persistence Diagrams (Fig. 10)
When additional details about a given facial pose are desired, the persistence diagram captured by persistent homology is represented by a scatterplot [D5]. The persistence diagram plots feature birth horizontally and death vertically. In this context, H0features are represented as solid squares, and H1features are represented as rings. The size of each element is proportional to its persistence (i.e., importance). Furthermore, the distance from the dashed diagonal to a feature is also a measure of persistence.

Representative Components and Cycles (Fig. 10)
A byproduct of the calculation of persistent homology is a structure known as generators, which are the landmark elements that generated a particular topological feature. For H0, the generators are the 0-simplices representing the joining of two components. For H1features, the data are output in the form of a representative cycle3. Each topological feature is associated with a generator that we use to identify what input data generated that topological feature, with a general focus on the high persistence features in the data [D5].

SECTION 7Evaluation
We evaluate our approach by first performing a detailed evaluation of two individuals—one female (‘F001’) and one male (‘M001’) from the BU4DFE expression dataset [84]. We then evaluate the ability of our approach to differentiate individuals using the entire dataset of 101 subjects. Each of these individuals has approximately 600 facial poses (6 emotions × ∼ 100 frames per emotion). Since the data provided are large and time-varying, our approach allows conditional observation of the topology of emotions based upon individuals, emotions, selected subset of facial features (full face, eyes+nose, mouth+nose, eyebrows+nose), topological dissimilarity, and dimension reduction technique. Our evaluation looks at how these conditional comparisons can be matched to known phenomena in affective computing. We note that one of the coauthors of this paper, Shaun Canavan is a researcher in affective computing and provided detailed feedback at every stage of the design.

7.1 Implementation and Performance
We implemented our approach using Python for data management, non-metric distance, and dimension reduction calculations, ripser [6] for persistent homology calculations, Hera [44] for topological distance, and D3.js for the user interface. Persistent homology and topological dissimilarity are pre-calculated for all combinations of landmark subsets. Dimension reduction is performed at run-time, taking at most a few seconds; as this data is calculated, it is also stored in a short-term cache to improve performance. The user interface is interactive. Our source code is available at https://github.com/USFDataVisualization/AffectiveTDA.

We evaluated the computational performance of the persistent homology and bottleneck and Wasserstein dissimilarity matrix calculations for F001 and M001 in Table 1. The calculations were performed on a Linux workstation with a 3.40GHz Intel i7-6700 CPU and 48 GB of RAM. In this table, we compare the metric landmark point-based approach and our novel non-metric landmark edge-based approach. Comparing persistent homology calculations, our non-metric approach took approximately twice as long as the metric approach. This is entirely attributable to the extra cost of calculating segment-segment distance (instead of point-point distance). The performance benefit of the non-metric approach comes with the calculation of the dissimilarity matrices, which saw a 10 x - 15 x speedup over the metric approach. This is attributable to the reduced number of noise features created by the non-metric approach, as described in Sect. 4.3. Overall, our approach saw a speedup of ∼ 7.5x.

Table 1. Computation time for metric (M) and non-metric (NM) approaches to extract persistent homology features and calculate the dissimilarity matrix for all frames from each subject, including subsets of facial landmarks (full face, eyes+nose, mouth+nose, eyebrows+nose). The number of landmarks input to each method is similar, 83 for full face metric and 81 for non-metric. In addition, we show the average number of H0 and H1 features generated per frame.












Table 1.- Computation time for metric (M) and non-metric (NM) approaches to extract persistent homology features and calculate the dissimilarity matrix for all frames from each subject, including subsets of facial landmarks (full face, eyes+nose, mouth+nose, eyebrows+nose). The number of landmarks input to each method is similar, 83 for full face metric and 81 for non-metric. In addition, we show the average number of H0 and H1 features generated per frame.
Table 1.- Computation time for metric (M) and non-metric (NM) approaches to extract persistent homology features and calculate the dissimilarity matrix for all frames from each subject, including subsets of facial landmarks (full face, eyes+nose, mouth+nose, eyebrows+nose). The number of landmarks input to each method is similar, 83 for full face metric and 81 for non-metric. In addition, we show the average number of H0 and H1 features generated per frame.



7.2 Relative Distance Topology and Action Units (AUs)
In affective computing, there are various approaches for recognizing expressions, as detailed in Sect. 2.1. One promising approach is the use of action units (AUs) [25], which are facial muscle movements linked to expression. AUs are represented as an intensity from [0, 5], where 0 is inactive, and > 0 is an active AU, with higher values representing more intense movement. Specific configurations of active AUs have been shown to be useful for recognizing facial expressions [51], [52], [54], [69], [81].

AUs are generally created in one of two ways. Either an expert manually annotates video frames, or a machine learning algorithm extracts them from the data. While the former is a slow and tedious process, the latter is fast but lacks any explainability in measuring the activity of AUs. We automatically detect 17 AUs (see Table 2) using the publicly available OpenFace toolkit [4], which is commonly used in affective computing literature [75]. However, coming from a machine learning model, the extracted AUs lack specific explainability.

We now demonstrate how our topology-based system can explain certain AU features detected by OpenFace by comparing the output of each. Our approach is as follows, since all sequences begin with a neutral pose, we consider relative distance with respect to the first frame of the sequence. We hypothesized that we would observe similar signals in the AUs and their associated facial features using our topology-based approach. Fig. 5 shows two examples of this relationship. In Fig. 5(a), we compare the activity of the nose and eyes to AU45 (blink) for the F001 disgust emotion. The relative distance shows three clear spikes at the same frames as AU45 (approximately frames 28, 52, and 75). However, AU45 does not tell the entire story of the activity that the topology is capturing.

Instead, we hypothesize that the topology is a combination of multiple AUs. Fig. 5(b) shows an example comparing the mouth+nose to AU14 (dimple) and AU25 (lips part) for the F001 fear emotion. In this case, a linear combination of both AUs seems to capture a more complete picture of the activity represented in the topology. A broader analysis of both subjects, multiple emotions, and multiple facial features, as seen in Fig. 7 and Fig. 8, revealed these relationships are widely observable. This confirms our hypothesis of a strong similarity between topology features and AUs.

Nevertheless, there is still not a perfect one-to-one relationship between the topology and AUs. The AUs go through further contextual processing than the topology does, e.g., to separate the activity of AU7 (eyelid tightening) from AU45 (blinking) and other related movements. One challenge with the contextual processing in the state-of-the-art in affective computing is the lack of explainability, which our topology-based approach provides (see Sect. 8).


Fig. 5.
A comparison of relative distance on non-metric topology (top) to action units (AUs) (bottom) on F001. The results demonstrate the similarity between the features extracted by the topology and AUs, which are commonly used in affective computing.

Show All

Table 2. Action units (AUs) and their corresponding facial muscle movements as used in our evaluation.

7.3 Comparing and Differentiating Expressions
Next, we consider whether the topological features are sufficient for differentiating the six different emotions present in the data. To perform this evaluation, we look at the full face topology for the female subject using t-SNE (Fig. 6(a), Shepard fitness: 0.79) and MDS (Fig. 1, Shepard fitness: 0.88), and male subject using t-SNE (Fig. 6(b), Shepard fitness: 0.78) and MDS (Fig. 6(c), Shepard fitness: 0.88).

We begin by examining the female and male subjects using t-SNE, as seen in Fig. 6(a) and Fig. 6(b), respectively. We make three important observations about the resulting images. (1) First, for both subjects, the emotional states tend to form separate clusters, indicating that they are indeed differentiable. This is particularly important if the topology were to be used for predicting unknown emotional states. (2) Second, most of the emotions begin and end towards the centers of the plots. This colocation is cAUsed by the neutral facial pose that subjects were asked to begin and end with each sequence. (3) The final observation is that the facial poses form temporally coherent ‘strings.’ This observation is particularly poignant, considering that nowhere in calculating the topological dissimilarity does it utilize temporal information.

We next consider the MDS projections for the female subject and male subject in Fig. 1 and Fig. 6(c), respectively. With the female subject, we observe that happiness, surprise, fear, and sadness largely cluster into separate regions of the plot with limited overlap or mixing. The anger and disgust emotions, on the other hand, overlap significantly, which corresponds with the recent literature in the affective computing community that considers them to be similar expressions [59]. Another interesting finding is that the emotional states of the male are less differentiated than those of the female. Interestingly, it is commonly accepted in affective computing that, in general, the expressiveness of females is more differentiable than that of males [15]. Given that we are only observing two subjects, no broad gender-based conclusions can be made in our case. Nevertheless, this female's expressions are more differentiated than this male's expressions.


Fig. 6.
Evaluation using (a-b) t-SNE and (c) MDS to determine how effective our non-metric topology-based approach is at differentiating emotions. F001 MDS can be found in fig. 1.

Show All


Fig. 7.
Comparison of F001 non-metric topology (top) and AUs (bottom) shows a similarity between eyes+nose (column 1), mouth+nose (column 2), and eyebrows+nose (column 3) and AUs associated with those facial regions.

Show All

7.4 Comparing and Differentiating Individuals
Finally, we consider how topological features allow the differentiation of each of the 101 subjects in the BU4DFE dataset.

To perform this evaluation, we look at the full face topology of a subset of 10 subjects (F001-F010) using t-SNE (Fig. 9(a)–9(f)). We notice that, for all six emotions, all ten subjects form relatively independent clusters; this is particularly true of the anger (Fig. 9(a)) and sadness (Fig. 9(e)) emotions, while some minor overlap occurs for a few of the individuals in the other emotions.

We performed a similar evaluation for all 101 subjects (58 female and 43 male) (Fig. 9(g)–9(l)). We can see that the clustering behavior seen with only 10 subjects scales to all the subjects of the dataset. To test the robustness of this t-SNE result to variations in hyperparameters, we ran the tests with four different perplexities (30, 40, 50, and 100) and found that the clusters remained roughly constant throughout (see supplemental material). The clustering phenomenon present in the t-SNE dimension reduction images was also present when we used UMAP (see supplemental materials).

SECTION 8Discussion
8.1 Contribution to Affective Computing
Due to the challenging nature of detecting AUs and determining emotion from expression, we hypothesize that our TDA-based approach can be used to provide new insights into these challenges. As it has been shown that temporal AU information can make recognizing emotions easier, our approach evaluates temporal facial expressions (i.e., AUs), which allows us to visualize a new representation of this data. As shown in Sect. 7, this representation shows the similarity between the topological signals and the AU signals over time, which provides the following insight, as validated by the coauthor on this paper, who is a researcher in affective computing.

Validation that AUs Are Correctly Detected
A limitation of current machine learning AU detection models is their accuracy, as little improvement has been made compared to previous models [39]. This is mainly due to the models detecting AUs that are not active, as well as not detecting AUs that are active. Our TDA-based approach can validate that the detected AUs are correctly capturing the muscle movement of the face. More specifically, the proposed approach will ensure that the AUs that have been detected are correct. As shown in Fig. 5(a), AU45 has high-intensity values three times during the sequence. This correctly corresponds to the three blinks that occur in the data, which are also captured in the topological signal. If the blinks were not captured in the topological signal, then the spikes in the AU intensity signal could be attributed to mislabelling or noise. This could facilitate more intelligent active learning [1] that would improve the machine learning detection models.

Fig. 8. - Comparison of M001 non-metric topology (top) and AUs (bottom) shows a similarity between eyes+nose (column 1), mouth+nose (column 2), and eyebrows+nose (column 3) and AUs associated with those facial regions.
Fig. 8.
Comparison of M001 non-metric topology (top) and AUs (bottom) shows a similarity between eyes+nose (column 1), mouth+nose (column 2), and eyebrows+nose (column 3) and AUs associated with those facial regions.

Show All

Relationship Between Which AUs Occur Together
AU cooccurrences [70] and patterns [39] can have a significant impact on the accuracy of machine learning models. Our approach can also give insight into multiple AUs that occur together, including specific AUs that occur when an emotion is expressed. This is detailed in Fig. 5(b), where AU14 and AU25 are active at different intensities at different times during the sequence. The topological signal shows a combination of the two AU signals corresponding to the most intense segments of the active AUs. AU14 is a dimple, and AU25 is active when the lips part, which are common muscle movements that could occur during a wide smile. When a smile occurs, AU6 and AU12 are commonly found together, according to the Facial Action Coding System (FACS) [25]. However, according to Barrett et al. [5], expressions vary across cultures and situations meaning, AU6 and AU12 may not be active in all smiles. The proposed approach can provide insight into this phenomenon, allowing investigations of the relationships of new AUs, over time, for different expressions.

Detecting Facial Expressions
There are many successful approaches to detecting facial expressions in affective computing [49], [50], [58], [83], [87]. Considering this, the purpose of the proposed approach is not to detect facial expressions but to provide greater analysis and explainability of the data. The insight provided by our visualization will allow new insight not previously seen in affective computing, as we can analyze the movement of the face using the proposed TDA-based approach, which directly corresponds to AU movements. This will result in new, more accurate ways of building facial expression detection models.

Explainability of Machine Learning Models
Machine learning has given us many advancements in fields as diverse as medicine [80], security [2], and education [16]. However, one of the main limitations is the lack of explainability [18]. Considering this, one of the key advantages of TDA over machine learning is the explainability of the features identified in the process. We demonstrate this using an example of four facial poses from the female surprise data, as shown in Fig. 10. These examples focus on the opening and closing of the eyes and mouth, which is commonly associated with a surprised expression. Given a machine learning model that successfully detected the AUs associated with this expression, with the long list of possible muscle movements (e.g., AU1, AU2, AU5, AU25, and AU26), it is difficult to understand why such a model detected them. This is especially true given the black-box nature of neural networks [61]. In Fig. 10, we can directly see the features that change the data. In the persistence diagrams, the number and persistence of the most important features are clearly different. Furthermore, when evaluating the representative cycles, we can further associate the landmark geometry of each high persistence feature in the data. This explainability will facilitate more accurate emotion recognition systems. This is due to the insight that the explainability will give affective computing researchers to better understand and tune their models, resulting in more accurate models.

8.2 Topology Doesn't Capture Everything
Limits of Topology
Some of the advantages of topology over geometry are also its biggest weaknesses. There are certain shapes of the data that may not be captured by topology alone. For example, smiles and frowns may have the same topological shape. That said, the relationship between the smile, nose, and jawline may be sufficient enough to disambiguate between smiles and frowns. Furthermore, smiles and frowns will also be associated with other changes in facial features, e.g., changes in eye or eyebrow shape. At the very least, our evaluation showed that smile emotions, e.g., happiness, and frown emotions, e.g., sadness, were differentiable in Sect. 7.3.

Differences Between TDA and Machine Learning
Topology does not capture all features of AUs. The AU extraction may utilize other data, nonlinearities, knowledge of physiological relationships of AUs, etc., to determine the extent of the activation of the AUs. That said, it is also important to understand that the AU information is not ground truth. It is the output of a machine learning technique, and it may, in fact, not be showing genuine AU activation. On the other hand, all of the topological features we observed are in the data, and for any matching feature, TDA provides evidence as to why the machine learning algorithm classified AU activation as it did.


Fig. 9.
Clustering of 10 subjects (F001-f010) on the top and all 101 subjects on the bottom. For 10 subjects, each subject is colored differently. For 101 subject tests, 12 colors were mapped to 101 by creating roughly 10 shades per color. Each plot includes the associated shepard fitness (sf).

Show All

8.3 Challenges and Limitations of Automatic AU Detection
The use of AUs for expression recognition is a promising approach. However, there are significant challenges in the detection of them [26]. Many works that have developed approaches for automatically detecting AUs using machine learning have focused on learning single AUs. However, it has been shown that patterns of AUs can have a significant impact on detection [39]. This leads to a bigger limitation of current machine learning approaches to AU detection, namely the data. State-of-the-art machine learning models require a large amount of good data to be accurate. Current models are trained on data that has biases in ethnicity [55], as well as significant imbalances in the distribution of AUs [88], [89]. Along with these data biases and imbalances, the ground truth data is often manually annotated, which is subjective and can lead to errors [60]. This results in machine learning models that are not learning how to represent an AU but learning the distribution of data [39]. In machine learning, many times, the solution to the problem is to collect more data. In the case of AUs, it has the additional challenge that multiple AUs occur simultaneously [70], resulting in an unresolvable imbalance of data for the AUs that occur more often (e.g., AU6 or AU12) [39]. These challenges lead to machine learning models that often fail to recognize AUs that are active, as well as recognize AUs that are not active [90].


Fig. 10.
Illustration of the explainability of our TDA-based approach using four poses from F001 surprise. For each, the persistence diagrams are shown (right), along with the highest persistence representative cycles (left). The number and persistence of features explains the difference between poses, while the representative cycles explain which landmarks generated those topological features.

Show All

Along with challenges in detecting AUs, there is a larger discussion of how humans learn and express emotions [41]. Broadly, this discussion can be categorized into two hypotheses. The first hypothesis states that emotions can be recognized from facial expressions (AUs) [24]. This hypothesis is the basis for the Facial Action Coding System (FACS) AUs [25] and is a significant motivation for the field of affective computing. The second hypothesis contradicts that and states that expressions vary across cultures, situations, and people in the same situation. BecAUse of this, emotions cannot be recognized from facial expressions alone [5]. Instead, other factors such as context, physiology, age, and gender should be considered. While these two hypotheses contradict one another, recent work has shown validity in both hypotheses. More specifically, while it is difficult to determine emotion from AUs given a single facial image when temporal AU information and context are considered, the task becomes easier [38]. Along with this, it has also been shown that the fusion of physiological signals, such as heart rate and respiration, along with AUs can be used to accurately recognize pain in subjects [40]. Although it is a challenging problem to automatically detect AUs using machine learning, these works show that AUs are still a promising approach to solve the challenging problem of emotion from expression.

SECTION 9Conclusions
In this paper, we have demonstrated that TDA can be used to discern and better understand patterns that exist between emotions. Paired with machine learning approaches to affective computing, TDA provides a means to evaluate particular aspects of the data to discern what parts of the face may be cAUsing the machine learning algorithm to recognize the data as a particular emotion or explain the shortcomings or misdiagnoses that the machine learning algorithm provides, e.g., if the algorithm recognizes a happiness emotion as anger, TDA may help to discern what led to this misdiagnosis. The next phase of this work is to begin evaluating these affective computing hypotheses, particularly those discussed in Sect. 8, using our TDA-based analysis and interface.