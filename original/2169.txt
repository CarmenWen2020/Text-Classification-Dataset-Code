Scientific ray tracing now can include realistic shading and material properties, but tracing rays of various depths to conclusion through partitioned data is inefficient. For such data, many ray scheduling methods have demonstrated improved rendering performance. However, synchronicity and non-adaptivity inherent in prior methods hinder further performance optimizations. In this paper, we attempt to relax these constraints. Specifically, we incorporate prediction models capable of dynamically adjusting levels of speculation in ray-data queries, making ray scheduling highly adaptable to a spectrum of scene characteristics. In addition, we organize rays in a tree of speculation nodes, where speculation is coordinated pairwise within a subtree of adaptive ray groups, facilitating concurrency and parallelism. Compared to prior non-predictive methods, we achieve up to three times higher throughput for volume and geometry rendering on a distributed system, making our method fit for both interactive and offline applications.
SECTION 1Introduction
With petascale supercomputing, large-scale scientific simulations can produce up to terascale data in minutes to hours [3], [17], [19], [30]. Large data generated by a simulation is typically a group of domains, each responsible for a spatial partition of the total volume. This is relevant particularly for distributed-memory settings because most computational simulations decompose a volume into subregions and assign them to processes to exploit data parallelism. As the subsets of a simulation data are generated, the simulation either writes them to permanent storage for post hoc visualization [6] or sends them to active visualization processes for in situ visualization [7], [8], [31]–, which may or may not be co-located with simulation processes.

Fig. 1: - We have path traced the distributed partitions of the lambda2 dataset [15], [18] of simulated vortices using the frontera supercomputer at the texas advanced computing center. With our predictive scheduling method, we are able to achieve a throughput of 7–33 mrays/s while sending rays across 4–128 distributed compute nodes. Lambda2 contains an aggregate of 2.3 billion unique triangles in 1024 domains extracted from a 77.3GB hdf5 file.
Fig. 1:
We have path traced the distributed partitions of the lambda2 dataset [15], [18] of simulated vortices using the frontera supercomputer at the texas advanced computing center. With our predictive scheduling method, we are able to achieve a throughput of 7–33 mrays/s while sending rays across 4–128 distributed compute nodes. Lambda2 contains an aggregate of 2.3 billion unique triangles in 1024 domains extracted from a 77.3GB hdf5 file.

Show All

Regardless of the visualization method employed, simulations are continuously adopting ray tracing as a vehicle for visualization [6], [27], [30] since modern ray tracers can generate high-fidelity images at high throughput rates [2], [33]. One of the challenges for tracing rays through partitioned data is that visibility queries for each ray evolve from a single task in the whole scene space into many discrete tasks in individual subspaces, incurring large overhead due to deferred task finalizations. Moreover, assigning domains to distinct remote processes exacerbates the problem even further, making the system less work efficient, thus demanding an effective scheduling method to orchestrate tasks of local and remote processes.

Generally, the tasks of tracing and forwarding rays are processed in front-to-back order based on the spatial ordering of domains affected by groups of rays [2], [22], [28]. The ordering requirement, however, may become a limiting factor for performance especially as more tasks tend to finalize in the back of the order. We can address this problem by allowing the system to speculatively process the tasks and defer the resolution of unordered tracing outcomes until all redundant tasks speculatively issued are processed [9], [16], [24]. Nevertheless, a secondary effect is that in case the majority of tasks tends to finalize early in the order, the excessive amount of work may negate the effect of improved parallelism enabled by speculation. Based on this reasoning, we argue that the premise of each method may be suited to restricted settings at best, and thus neither of these methods may act as a panacea for all rendering scenarios.

Inspired by this insight, we develop a scheduling method that demonstrates good performance consistently over a wide range of rendering scenarios. To achieve this nontrivial goal, we unify the two existing methods into a single method such that it dynamically adjusts to the characteristics of underlying data by means of prediction models. Furthermore, we introduce a generalized parallel scheduling framework that is flexible enough to accommodate existing scheduling methods as well as our unified, prediction-based method. In essence, the framework encapsulates groups of rays with a speculation context into runnable tasks and assigns each of the tasks to the work queue of an available processor for parallel processing.

SECTION 2Background
2.1 Ray Tracing Distributed Data
Distributed-memory ray tracing can be a combination of sort-first, sort-middle, and sort-last. If a dataset fits into the size of a process, it is common to use the sort-first approach. Primary rays are initially assigned to individual processes, which in turn trace them to completion using the dataset replicated in each process. Most interactive ray tracers to date [10], [33] are based on this rendering classification.

This sort-first distributed-memory rendering approach essentially means concurrently running multiple Whitted-style recursive ray tracers on different processes and combining the pixel values generated by each process into the frame buffer of a display process. Therefore, only job assignments and pixel values are communicated between processes, but rays and scene data remain in each local process.

As for rendering distributed data spanning multiple processes, the sort-last approach is typically used. Every process participates in evaluating all pixels based on the partial data it owns, and the images from all processes are lastly composited into a single image based on the depth information. When it comes to ray tracing, this approach in modern ray tracers [14], [32], [33] provides users with interactivity; however, global secondary effects cannot be computed since each process views only its local data and can only generate localized effects such as ambient occlusion with a bounded ray length.

To achieve full-fledged global illumination on distributed data, data must be brought into where rays are, or rays must be sent to where data are. In the former case, geometry caching [12], [26] is effective in reducing data transfer between processes. In addition, out-of-core rendering methods [13], [24], [26]–, which page in required data from storage, are another way of effectively amortizing the cost over multiple tasks. However, both approaches become intractable with limited I/O bandwidth as the amount of data tends to easily reach terascale levels in petascale computing and beyond. Therefore, our work adopts the latter approach where rays are periodically sorted and redistributed among processes based on the domain each ray belongs to. Many methods following this sort-middle approach have shown promising results in prior work [2], [22], [24], [28] and commonly utilize ray reordering [26].

2.2 Ray Scheduling
Ray tracing distributed data can be perceived as a problem of two-level visibility queries. Given a ray, the domains of a scene are queried to find the first domain closest to the ray origin, and then the data within that domain is further queried to process the ray inside. Thus, the domains overlapping the ray have to be marched through and searched until secondary rays are spawned or the ray is retired as a pixel.

A preferred way of achieving efficiency in multi-level visibility queries is to process rays in groups. This is typically done by sorting the order of the domains that a ray enters and queuing the ray into the first domain prior to collectively processing the ray group in each domain. This deferred approach allows the system to exploit parallelism within and across the sorted ray groups and to further amortize various data management costs over many rays. For this reason, such queuing ray tracers have shown superb performance and scalability in distributed address space settings [2], [21], [22], [24], [28] as well as in single address space settings [4], [23], [26].

Fig. 2: - An abstract view of the ray tracing pipeline for distributed data rendering. The pipeline places the incoming rays to the queues of spatial domains, assigns each queue to the process owning the data, and performs visibility queries and shading on assigned rays to update the frame buffer, which is composited with others to form an image.
Fig. 2:
An abstract view of the ray tracing pipeline for distributed data rendering. The pipeline places the incoming rays to the queues of spatial domains, assigns each queue to the process owning the data, and performs visibility queries and shading on assigned rays to update the frame buffer, which is composited with others to form an image.

Show All

As illustrated in Fig. 2, there are mainly three functional stages involved in processing the unsorted incoming rays before the image compositing stage: ray placement, ray assignment, and ray processing. The first two stages in particular, often collectively referred to as ray scheduling, are of our primary interest because they act as an overhead to actual work in the last ray processing stage, but simultaneously, they are responsible for assigning work to processes, directly impacting workload balance. The performance impact of image compositing is negligible with regard to achieving global illumination on distributed data, thus it is beyond the scope of our work.

SECTION 3Problems and Goals
3.1 Ray Placement Policies
Placing a ray into domains can be non-speculative or speculative. The non-speculative placement policy is what most queuing ray tracers commonly adopt in single address space [26] and distributed address space [2], [22], [28]. In this policy, a ray is placed into the next closest domain that it enters. As scheduling progresses, the affected domains for each ray are examined one after another from the closest to the farthest until each ray finally terminates. The tracing results in all domains being undisputed, the scheduler can immediately issue the next actions to follow: migrating each of the rays into the next domain, spawning new rays, or updating pixel values. We refer to the ray scheduled in this way as a shallow ray.

The speculative placement policy has also been studied in single address space [5] and distributed address space [9], [16], [24]. This policy permits placement of rays into all affected domains, relaxing the constraint that domains are examined one at a time and thus boosting levels of concurrency with simultaneous tracing. This approach not only makes ray placement speculative but also makes tracing carried out in all domains inherently speculative. The tracing results being inconclusive, the scheduler needs an extra step to resolve the unordered tracing outcomes prior to initiating the aforementioned next actions. We refer to the ray scheduled in this way as a deep ray.

3.2 Data Awareness
Relying on shallow or deep rays alone is not amenable to variations in system input. Regardless of the ray type, the distance to ray termination can extensively vary depending on scene characteristics and system configurations. For instance, in geometry rendering of matte surfaces, the overall density of geometries determines ray termination; rays in dense regions tend to terminate earlier than those in sparse regions. In volume rendering of scalar fields, however, alpha compositing determines ray termination; rays in opaque regions tend to terminate earlier than those in transparent regions. For this reason, input characteristics play a crucial role in altering the dynamics of spatial and temporal ray distribution, impacting system workloads considerably. Consequently, scheduling with deep rays may outperform scheduling with shallow rays, and vice versa, depending on the characteristics of the data. Therefore, we dynamically adjust the scheduling depth of each ray to achieve a highly adaptive, data-aware system that consistently demonstrates good performance over a wide range of input variations.

3.3 Concurrency
All distributed-memory ray tracers to date, whether they rely on shallow scheduling [2], [28] or deep scheduling [9], [16], [24]–, improve performance by exploiting levels of parallelism within and across processes. One breed of the deep scheduling system [9], [24] processes work in a way similar to the bulk synchronous parallel model where work is divided into phases and in each phase all processes independently run tasks assigned to them until reaching the next synchronization point. Similarly, another breed of the shallow or deep scheduling system adopts a hybrid programming model of message passing combined with multithreading [2], [16] or SIMT [28] but decomposes work into much finer granularities for asynchronous processing. In light of our comparative studies between these approaches, we find that regardless of the scheduling depth, shallow or deep, the latter approach is much appreciated when it comes to performance. For the adaptive system we desire to build, the challenge is to devise a scheduling framework that can seamlessly manage all the complexities involved in coordination of the fine-grained tasks of speculation while meeting all the constraints and goals discussed in this section.

3.4 Performance
Distributed-memory ray tracers are designed with different performance goals in mind. In one approach, interactive performance is emphasized over global illumination, exhibiting local lighting effects only. To render global effects, interactive distributed-memory ray tracers [14], [33] require replications of entire subsets of data in all visualization processes, making it impractical to visualize the data larger than a single-process memory. Another approach [2], [24] emphasizes more on global illumination by creating an image database offline, providing a limited interactivity when the user explores the database with an image-based analysis tool such as Cinema [3]. One of our ambitious goals is to fulfill both of these perspectives by building a high-throughput ray scheduling system, enabling interactive and offline rendering of distributed data with the rays sent across processes.

3.5 Versatility
For scientific visualization, it is often required to render various data types such as volumes, geometries, or a mixture of the two. However, the scope of prior work is restricted to one type of rendering or scheduling only. For systems supporting deep scheduling, some are tailored to render geometry data [16], [24], [28] whereas some are tailored to render volume data [9]. Although Galaxy [2] can render both volumes and geometries, it can perform shallow scheduling only. OSPRay [33] supports a variety of data types, including adaptive mesh refinement data, but for distributed data, it can only support local effects of illumination because it does not send rays across processes. That said, our next goal is to make the scheduling framework general enough to support a broad range of rendering capabilities on both distributed and non-distributed data while achieving global illumination.

SECTION 4Predictive Scheduling
Scheduling a ray in a partitioned scene can be perceived as determining the order in which different regions of space, or domains, are traversed for one or more rays. The traversal for a ray continues until it finds a termination point along the ray, which in turn triggers the following actions depending on the rendering method used. In volume rendering, as ray marching reaches a point of high opacity, the corresponding pixel is updated with the color value accumulated along the ray. In geometry rendering, as a ray intersects a point on a surface, secondary rays are newly spawned and traced within the domain. Each of the newly spawned rays is then conditionally scheduled if it travels beyond the boundaries of the region without being terminated.

The existing scheduling methods adopt one of the two fixed ordering policies: the non-speculative front-to-back traversal for shallow scheduling [2], [22], [28] and the fully speculative unordered traversal for deep scheduling [9], [16], [24]. In shallow scheduling, a ray is moved from one domain to another until a condition for ray termination is met. Between the moves, the ray is queued along with other rays sharing the same destination domain, and the ray group in the queue is forwarded to the process owning the data, which then performs ray-data queries and queues both the non-terminated rays and the newly generated rays for the next round of scheduling. As long as many rays are available, this approach provides the system with levels of parallelism across ray groups as well as across the rays within a ray group. However, as more rays tend to terminate early, the amount of active rays is drastically reduced, limiting the level of available parallelism over time.

Providing the system with a sufficient amount of independent work can address the problem of reduced parallelism. The deep scheduling method implements this principle by unconditionally inserting a ray into all affected domains, processing the redundantly placed rays in parallel, and finally resolving the outcomes from different domains before scheduling the newly created rays. This approach greatly improves parallelism with increased processor utilization. However, it requires additional synchronizations to resolve speculations, and an exceedingly large amount of speculative work may incur high communication overhead. Moreover, if rays tend to terminate early because of certain scene characteristics, the vast majority of rays processed in farther domains may turn unnecessary after all.

In this section, we evolve these two extremes of the scheduling method to a unified, generic method that has much relaxed scheduling constraints. While elaborating on this new approach, we use the example scene drawn in Fig. 3 containing a triangle in the third domain that basically symbolizes the termination point of a ray.

4.1 Adjusting Speculation Levels
Given a list of domains affected by a ray, there are fundamentally two questions that need to be answered in each scheduling step where the ray is assigned to different domains: In what order should we process the domains? And given the order, what domains should we assign the ray to? The shallow scheduling method uses the front-to-back ordering policy and assigns the ray to the next closest domain. The deep scheduling method does not constrain the spatial order but rather assigns the ray to all affected domains. Therefore, depending on the scheduling method used, the ray is assigned to one or all of the domains affected.

Our method, however, relaxes the spatial ordering constraint by using what we call in-order or out-of-order speculation. For in-order speculation, a ray is assigned to one or more domains in front-to-back order. For out-of-order speculation, a ray is assigned to one or more domains using a different ordering constraint other than the front-to-back order used in the shallow scheduling method. In-order speculation is useful when the front-to-back order is warranted, which is especially true for radiance rays but is equally relevant to shadow rays as well. On the other hand, out-of-order speculation can alternatively be used only for shadow rays because the main role of shadow rays is to query the scene for occlusion from light sources, i.e., to find any hit along a ray, not the first hit point. Fig. 3c and Fig. 3e illustrate both of these scheduling approaches.

In addition, our method further relaxes the constraint on the number of domains that a ray is assigned to. Our approach first sorts the domains by some order constrained by the in-order or out-of-order speculation policy and chooses the first d domains for ray assignments. We define d as a scheduling depth or speculation level. Ideally, d has to be chosen such that redundant ray-data queries and additional rounds of scheduling are minimized. For example, as shown in Fig. 3d, if the traversal in the first round of scheduling fails to finalize, another round of scheduling is necessary to perform additional ray-data queries in untraversed domains.

4.2 The Predictive Queuing System
With the controllability incorporated, the question now becomes what criteria and strategies we should use for sorting the domains and choosing the scheduling depth d. Our short answer to this question is the prediction-based, adaptive queuing system shown in Fig. 4. The basic idea is to predict the domain in which a ray is mostly likely to terminate based on the statistics of probability or depth values measured over frames. Given a ray, the queuing system sorts the affected domains according to a predefined sorting criterion and evaluates the value d as a function of the measured statistics. The system then places the ray into the first d domains in the sorted list, and once each domain is populated with the ray, it distributes copies of the ray across processes and aggregates the resulting statistics for the next round of scheduling.


Listing 1:
A depth-based queuing method for radiance and shadow rays.

Show All


Fig. 3:
Our predictive scheduling framework generalizes existing ray scheduling methods to a unified method that adapts to the characteristics of underlying data by adjusting the scheduling depth of each ray. Although a single ray is used in this example, rays are scheduled in groups, not individually.

Show All


Fig. 4:
A high-level view of our predictive, statistics-driven queuing system.

Show All

4.3 Depth-Based Prediction
One straightforward way of predicting a scheduling depth is to directly use past scheduling depth values. In this case, the predicted depth can simply be expressed as d = dµwhere dµis the mean of the measured depth values averaged over the previous frames. One caveat is that when shadow rays are scheduled with out-of-order speculation, this depth-based approach may become invalid because the sorted order of the domains may be inconsistent between the measured and the predicted. Sorting the domains in front-to-back order is required for the depth-based prediction so that the assumption of the prediction holds true.

In addition, it is worth noting that the method makes use of binning to localize predictions to coherent groups of rays with similar directional or geometric properties. For primary rays, image tiles are subdivided into bins of smaller subspaces, and a bin ID is assigned to each ray based on the subspace that each ray belongs to. For secondary rays, each domain is associated with bins of the outgoing ray direction (±x, ±y, and ±z), forming a total of 8n virtual subspaces for n domains. In this case, a bin ID is assigned based on the domain that each ray originates from and one of the eight subspaces that each ray belongs to, which is evaluated based on the signs of vector components of ray direction.


Listing 2:
A probability-based queuing method for radiance rays.

Show All


Listing 3:
A probability-based queuing method for shadow rays.

Show All

Listing 1 is the pseudocode for the depth-based prediction method. The algorithm first populates a list of domains affected by a ray and sorts the list by distance to arrange the domains in front-to-back order. With a bin ID assigned, the algorithm simply evaluates d as the mean of the previous depth values in the corresponding virtual subspace indexed by the bin ID. With the estimated value of d in place, the algorithm speculatively places the ray into the first d domains of the sorted list, finalizing the ray placement. Notice that for correctness, d must be properly clamped to the valid range prior to the ray placement. Also notice that at the end of each ray placement, the scheduling depth is cached to avoid placing the ray, in the next round of scheduling, into the domains already traversed.

4.4 Probability-Based Prediction
In addition to the depth-based prediction method, we introduce another prediction model that makes predictions by evaluating the probability of ray termination in each domain. Unlike the depth model, this method does not require binning. However, it maintains counter values for hits and misses in each domain (i.e., terminating and nonterminating). Whenever ray marching or ray-data queries are performed in each domain, the number of terminations are tallied into the counter values.

With all the statistics aggregated over frames, the main goal of this method becomes choosing the scheduling depth d such that the overall probability of ray termination is maximized. To explain the approach at a high level, the probability of ray termination is first estimated on a list of sorted domains, then scanning over the sorted list, the scheduler continuously inserts the ray into the domain as long as the aggregate probability evaluated so far does not exceed the target probability defined by the user.

In probability-based prediction, the scheduling system employs slightly different prediction models for different types of rays. For radiance rays, we use an additive prediction model expressed as phit,j=Σjj=0(1−phit,i−1)×hi, where phit,j is the estimated probability of a ray having a termination point between domains 0 and j, and hi is the measured probability of a ray having a termination point in domain i. phit,j is initially set to phit,−1=0 For shadow rays, we use a multiplicative prediction model expressed as pmiss,j=∏jj−0pmiss,i−1×mi where pmiss,j is the estimated probability of a ray not having a termination point between domains 0 and j, and miis the measured probability of a ray not having a termination point in domain i. Pmiss,jis initially set to pmiss,−1=1 We can then compute the probability of having any termination point between domains 0 and j by using the expression panyhit,j=1−pmiss,j. The algorithms in Listing 2 and Listing 3 each implement one of these prediction models for radiance and shadow rays, respectively.

One advantage of using this approach is that the same statistics can be applicable to both types of rays. Another advantage is that the counter values are easy to maintain because of the lack of binning. On the other hand, when evaluating the scheduling depth d, it involves more computations than the depth model in which a single lookup of the average depth value is sufficient.

4.5 Aggregating Statistics
To collect the statistics discussed so far, we subdivide each frame into subframes and aggregate the statistics over multiple subframes or frames depending on the statistic used. The idea of employing subframes naturally fits into the framework of progressive rendering, which is a widely used technique to achieve interactive rendering in many applications.

For the depth values acquired from 2D binning, assuming the camera position changes every frame, the statistics are aggregated over multiple subframes within a frame but need to be refreshed at the beginning of every frame. In contrast, because the depth values or hit/miss counters acquired from 3D space are more dependent on the geometric properties of the rays, the statistics can be aggregated over different frames as well as over different subframes. However, it is worth noting that accumulating values over many frames can cause overflows in numbers, thus our implementation computes running averages of statistics instead of allowing continued aggregations.

SECTION 5A Scheduling Framework
With the prediction models established, we need a generic scheduling framework that can efficiently implement this high-level idea on modern supercomputers. The main goal is to provide a high-throughput ray processing capability for rendering large-scale distributed data. We achieve this goal by viewing the whole system as a giant tree of speculative or non-speculative ray groups. With this perspective, we compartmentalize the tree of rays into many fine-grained subtrees and turn the nodes of each subtree into runnable concurrent tasks having different levels of dependency within and across subtrees. We then map and run these tasks on a system that employs a hybrid programming model of inter-process messaging and multi-threading. In order to build a system bearing such design philosophy, there are many design choices we have to make along the way, which eventually leads to many challenging questions that we have to answer in order to achieve the performance goal: How do we compose such subtrees? How do we manage and resolve the dependencies within and across subtrees? What needs to be processed within a subtree? How do we translate subtrees into tasks of computation and communication? How do we make the framework versatile such that various rendering algorithms and the prediction models are seamlessly incorporated? In this section, we strive to answer all these questions as we build such a sophisticated system.


Fig. 5:
A hash table-based distributed consensus protocol for nonblocking work management.

Show All

5.1 Task-Based Execution and Work Management
The notion of encapsulating rays into a tree of speculation nodes makes the implementation highly amenable to parallel computing because of a high degree of inherent concurrency that exists within and across subtrees. In order to efficiently process the whole evolving tree, we make use of a task-based parallel computing framework similar to the one used in Galaxy [2]. We innovate beyond Galaxy-like task scheduling by the use of a tree-based abstraction to organize the speculation levels, described in Sect. 5.2. Essentially, the framework is a hybrid programming model of inter-process messaging and multi-threading which incorporates a number of different types of threads in each process. Specifically, each process maintains a round-robin task scheduler acting as a receiver thread and as it receives a task from a local or remote process, it assigns the task to one of the worker threads capable of executing different types of speculation nodes in the tree that are defined as runnable tasks. In addition, if a new task is to be sent to another process, it is first pushed to a message queue, and a dedicated sender thread eventually sends it to the destination process.

Using this framework, all processes continue to run until they finish processing all the nodes in the tree that no longer grows in size. Especially when all tasks are highly distributed and are created asynchronously, devising an efficient consensus system is challenging. To address this challenge, we have developed a hash table-based distributed consensus protocol, which allows non-blocking notifications of work progression.

Fig. 5 shows a high-level view of our work management system. Each process maintains a dedicated thread for work management, allocating a list of hash tables for bookkeeping. To minimize conflicts in the hash table, we use a separate hash table for each image tile, which can be owned by any process in the system. Thus, the ownership of a tile determines the ownership of the corresponding hash table as well as all the subtrees derived from it. After creating and scheduling child subtrees, the node being processed notifies the tile owner process of its work status with the ID of the parent subtrees, the number of completed tasks, the ID of the child subtrees, and the number of newly scheduled child tasks. Upon receiving the notification, the owner process updates the hash table entries of task counters indexed by the IDs of the parent and child subtrees. Once all hash tables of tiles of each process become empty, each process notifies the root process, and if all hash tables across processes turn out to be empty, the root process notifies all processes to finalize the current render frame.

5.2 Speculation Nodes
We employ a tree-based abstraction to organize the speculation levels. First, we define primitives to compose the tree. A speculation node is an abstraction that encapsulates a group of rays, which can be speculative or non-speculative. The encapsulation simplifies the complex management of many different types of rays.

Speculation nodes are broadly classified into sender nodes and receiver nodes. As the names imply, the sender and receiver nodes are responsible for sending and receiving messages from one process to another. The sender node, acting as the parent of a subtree, encompasses a group of non-speculative rays as the input. It places the input rays into domains and sends each of the ray groups in different domains to the process owning the data. A receiver node, acting as a child of a subtree, encompasses a group of rays received from the parent sender node. Depending on the ray placement policy used in the sender node, the ray groups created in the sender node and the input rays of the receiver node can be speculative or non-speculative. But the input rays of the sender node are always non-speculative.


Fig. 6:
Progression of speculation nodes as part of non-speculative subtrees (top) and speculative subtrees (bottom).

Show All

If the rays are speculative, a receiver node is tightly coupled to the parent sender node for the duration of its lifetime. In this case, the receiver node performs ray-data queries on the input rays and sends the results back to their parent node. Unless all the rays are terminated, the receiver node is still alive and waits for the next message to arrive from the parent node. If the rays are non-speculative, the receiver node is not bound to its parent and is free to play its role without blocking on the parent node. That is, it recursively traces all the rays within the domain that it is assigned to and subsequently initiates new subtrees by creating new sender nodes encapsulating the rays that migrate into other domains.

The sender nodes that perform shallow scheduling on the input rays terminate their lifetime immediately upon sending groups of rays to the owner processes of the receiver nodes. In contrast, the sender nodes that result in speculative ray groups block on the results coming from the receiver nodes. Once a sender node receives a message from one of the receiver nodes, it updates the results into what we call a visibility buffer. Then, as the sender node updates the buffer for the last child (receiver node), it resolves speculations for all child nodes and individually sends the resolved ray indices to all child nodes that are still alive. Simultaneously, since the visibility buffer is correct at this point after the resolution, the sender node retires the contributions of all the input rays as pixel values. It is worth noting that serialization only occurs while updating the visibility buffer, and the remaining parts of the whole process are highly parallel.

Finally, upon receiving the ray indices, each of the receiver nodes recursively traces the resolved rays to completion until the maximum ray depth within the domain is reached or until the rays traverse out of the domain bounds. The rays that must be moved to other domains are again encapsulated into sender nodes, and the whole process repeats as described. Therefore, sender nodes are created whenever a new non-speculative ray group is available as the input. The non-speculative ray group can be either the primary rays generated from an image tile or the secondary rays spawned by a receiver node. Fig. 6 clarifies some of these roles by illustrating the progression of the trees that are either speculative or non-speculative.

5.3 The Subtree Structure
As already discussed, each subtree comprises a sender node and one or more receiver nodes. Because radiance and shadow rays each serve different purposes, we define two separate subtree types: a radiance subtree and a shadow subtree. Grouping the same type of rays together allows the execution of ray-data queries to follow similar control paths, mitigating the complexity of managing different ray types. As shown in Fig. 7, the radiance subtree contains a radiance sender node ST and one or more radiance receiver nodes RT. Likewise, the shadow subtree contains a shadow sender node SO and one or more shadow receiver nodes RO.


Fig. 7:
A generalized form of N-nary subtrees created for radiance and shadow rays. With proper coordination between subtree nodes, we can enable asynchronous ray speculation on volume and geometry data.

Show All

At the end of the lifetime of a radiance subtree, each of the radiance receiver nodes RT creates up to two sender nodes including a radiance sender node ST and a shadow sender node SO, initiating two child subtrees. Notice that a child subtree is constructed only if there exist rays crossing the domain boundaries; otherwise, no child subtrees are created for the current subtree. Unlike radiance subtrees, shadow subtrees do not result in a child subtree because shadow rays do not spawn secondary rays.

5.4 Messaging Within a Subtree
We can view the sender and receiver nodes of a subtree as state memories and the edges connecting the nodes as channels for messaging. Therefore, implementing a variety of rendering algorithms with different types of data in place implies defining different memory entities in each node and defining messages communicated within each N-nary subtree. We additionally need to define a sequence of algorithmic steps driven by each message in order to orchestrate all the state changes within each node.

For geometry rendering, each radiance sender node ST maintains the depth information {ti} in the visibility buffer, and each shadow sender node SO maintains the occlusion information {oi} in the visibility buffer. On the other hand, for volume rendering, each radiance sender node maintains the color values {rgbαi,domain} obtained from different receiver nodes RT in the visibility buffer. Since volume rendering only involves integrating color values by means of ray marching, shadow subtrees are unnecessary.

For geometry rendering, the state values stored on the sender side are updated through messaging between senders and receivers. The update operations are carried out by taking the minimum of t values and the logical OR of occlusion flags. Once all values are updated and merged, the sender node in the radiance subtree individually sends the resolved ray indices {i} to the receiver nodes, which is unnecessary for shadow rays because they do not create secondary rays.

For volume rendering, however, the sender node instead blends the partial color composites with proper depth ordering. Basically, we implement the same sorting approach suggested by Binyahib et al. [9]. As each receiver node RT receives a group of rays, it performs ray marching within the assigned domain and returns partially composited color values to the sender node ST, which in turn composites the partial values into a single, final color for each ray.

5.5 Compacting Communication Data
Although the abstraction of organizing ray groups in a tree of nodes greatly overlaps computation and communication by exploiting parallelism within and across subtrees, we can further reduce the communication cost by reducing the amount of communication between the nodes. One optimization we apply to our system is to avoid sending raw t values after processing each radiance receiver node RT. We instead send only 1-bit hit/miss information for each radiance ray, reducing the payload size by 32 or 64 times depending on the floating-point precision used for t values. This is possible under the assumption that the domains are organized as a regular grid without overlaps between the domains, which is typically the case for most scientific data.

Fig. 8: - Deferred error handling for predictive scheduling. Error tasks are queued within the current subframe and are carried over to the next subframe in the context of progressive rendering.
Fig. 8:
Deferred error handling for predictive scheduling. Error tasks are queued within the current subframe and are carried over to the next subframe in the context of progressive rendering.

Show All

Table 1: Configurations of the noise dataset used for performance analysis. Isosurface values shown in parentheses.
Table 1:- Configurations of the noise dataset used for performance analysis. Isosurface values shown in parentheses.
If a receiver node's flag indicates a hit for a ray, the sender node retrieves the ray and updates the visibility buffer with its scheduling depth and domain ID. Upon merging all of the depth values, the sender node resolves the visibility by searching for the speculative ray having the same domain ID as the one in the buffer. For shadow rays, we achieve data reduction by maintaining 1-bit occlusion flags in the visibility buffer; each latest buffer entry indicates the occlusion status. For volume rendering, we can pack each RGBA color value into a smaller size, but we have not implemented it in our system because of potential precision issues.

5.6 Handling Special Cases
When predictive scheduling is performed, inaccurate predictions lead to insufficient visibility information, delaying the finalization of ray processing. When mispredictions occur, instead of immediately launching another subtree for the error rays, we create an error node and defer processing it until the next subframe in the context of progressive rendering. Our error handling approach shown in Fig. 8 not only addresses the issue of incurring additional delays in handling errors but also reduces the amount of work near the end of a subframe. The basic insight is to hide the misprediction penalty by handing over error nodes to the next subframe such that they can be processed along with other nodes initiated in the next subframe, improving overall throughput with abundant work available.

When one or more rays exclusively overlap only one domain, all the ray processing results in the domain are non-speculative and complete, obviating the need for additional error handling steps. This basically implies falling back to shallow scheduling where all the nodes are loosely coupled without requiring any coordination between them. As shown in Fig. 6, once a sender node creates and schedules receiver nodes, the receiver nodes are able to independently trace all the rays within the domain on their own. However, such instances may still initiate another round of speculative scheduling, making our system capable of switching between shallow, deep, and predictive scheduling.

SECTION 6Evaluation
In this section, we evaluate the effectiveness of our asynchronous scheduling framework and compare the performance of predictive and non-predictive scheduling methods.

6.1 Setup
We use both synthetic and real datasets throughout our evaluation process. For synthetic datasets, we procedurally generate the regular grid volumes shown in Fig. 9 using the Perlin noise function [1], [25]. To evaluate the performance of predictive scheduling on scenes with a wide range of characteristics, we vary opacity levels by altering color maps for volume rendering and vary density levels by altering isosurface values for surface rendering. For real datasets, we use the benchmark datasets shown in Fig. 10. For each dataset, we uniformly assign domains to processes in z-curve order so that spatially adjacent domains are mapped to the same process in case the domain count exceeds the process count.


Fig. 9:
Synthetic noise datasets. Surfaces with isosurface values of 0.1, 0.4, 0.7, and 1.0 (or sparsity levels of 0, 1, 2 and 3) from left to right (top row). Regular volumes with transparency levels of 0, 1, 2, and 3 from left to right (bottom row). See Table 1 for configurations.

Show All


Fig. 10:
Benchmark datasets used for performance comparison.

Show All

For all experiments, we use the Frontera supercomputer [29] at the Texas Advanced Computing Center and launch a single process on each compute node, comprising dual sockets of 2.7GHz 28-core Intel Xeon Platinum 8280 Cascade Lake processors and 192GB DDR4 memory. All compute nodes on Frontera are interconnected via the Mellanox 100Gb/s InfiniBand system. We compile our hybrid MPI and multithreaded C++ code with the Intel compiler 19.1.1 and the Intel IMPI library 19.0.9. We do not explicitly vectorize the code besides activating the compiler's default optimization features. On the rendering side, we use the Intel Embree ray tracing library 3.12.1 for visibility queries and the Intel OpenVKL volume kernel library 0.11.0 for volume sampling.

We evaluate our approach on both surface and volume data. For surface rendering, we use implicit isosurfacing on volume data, and we use ambient occlusion and path tracing on geometry data. For shading and materials, we include a diffuse hemisphere light source in each scene and use the pure diffuse material for all surfaces. For volume rendering, we perform ray marching on volume data to integrate the scalar values sampled along each ray. For implicit isosurfacing, we allow one ray bounce and generate one shadow ray for each hit point. For ambient occlusion, we allow one ray bounce and generate eight ambient occlusion rays for each hit point. For path tracing, we allow four ray bounces and generate one radiance ray and one shadow ray for each hit point. For each test case, we generate a total of 2500 subframe images at a resolution of 10242 pixels in the context of progressive rendering; we integrate 50 samples per pixel over 50 subframes to form a fully integrated frame and capture the performance distributions of 50 different viewpoints orbiting around each dataset.


Fig. 11:
Adaptive performance behavior of predictive scheduling methods with variations of volume transparency and surface density of the noise dataset.

Show All

Table 2: Throughput comparison of synchronous and asynchronous scheduling methods. Implicit isosurfacing was used to render a set of weak scaling benchmarks of the noise dataset. Throughput measured in mrays/s. Speedup shown in parentheses.

For all performance measurements, we use throughput as a performance metric. We specifically define throughput as the number of newly generated non-speculative rays processed per unit time, measured in MRays/s. We do not count duplicate ray copies or pointers traversing different domains in the scene but rather count them only once as they are created in a certain domain or at the camera position.

6.2 Results
Table 2 shows the effectiveness of our asynchronous scheduling approach compared to the synchronous scheduling approach [9], [24]. All throughput measurements are as a result of performing shallow and deep scheduling on a set of weak scaling benchmarks of the Noise dataset using implicit isosurfacing. It is clearly seen that our asynchronous scheduling framework dominates in performance by orders of magnitude for a range of node counts and isosurface values.

Table 3: Effects of out-of-order scheduling of shadow rays when the probability-based scheduling method is used to render the noise dataset. Throughput measured in mrays/s. Speedup shown in parentheses.

Fig. 11 compares the performance of depth-based and probability-based predictive scheduling to shallow and deep scheduling. In this performance comparison, we evaluate the impact of variations in surface density and volume transparency on the performance of different scheduling methods when rendering the Noise dataset with various rendering algorithms, including volume rendering, implicit isosurfacing, ambient occlusion, and path tracing.

When it comes to surface rendering, our general expectation on performance behavior is that for dense geometry data, predictive scheduling should perform as good as shallow scheduling, and for sparse geometry data, predictive scheduling should perform as good as deep scheduling. This is because rays typically terminate early within dense geometry (i.e., there are more opportunities for an intersection), whereas rays typically terminate late within sparse geometry. A similar duality applies for volume rendering. For highly opaque volume data, predictive scheduling should match the performance of shallow scheduling, and for highly transparent volume data, predictive scheduling should match the performance of deep scheduling. Again, similar to the case of geometry rendering, this is because rays typically terminate early in scenes with high opacity whereas rays typically terminate late in scenes with low opacity (i.e., high transparency).

We consistently observe such performance behavior in Fig. 11 for both methods of depth-based and probability-based predictive scheduling. For volume rendering, however, although predictive scheduling shows good performance on 64 nodes, the performance does not scale well on 128 nodes particularly for highly opaque volumes (not included in Fig. 11) because the large amount of communication required to composite partial color values limits the overall performance. Neverthe-less, we find that depending on the viewpoints, predictive scheduling can also be effective even for opaque volumes rendered on 128 nodes.

Fig. 11 shows that particularly for path tracing with many incoherent secondary rays, depth-based scheduling outperforms probability-based scheduling because the measurement of depth-based scheduling is more fine-grained by design, which groups the statistics of depth values into eight different directions using 3D binning. However, such disparities in performance can be overcome by scheduling shadow rays out of order using the probability-based scheduling method. Table 3 shows that except for implicit isosurfacing performed on 16 nodes, out-of-order scheduling achieves up to 4.5× higher throughput when rendering the Noise dataset with implicit isosurfacing and path tracing. Because shadow rays, unlike radiance rays, do not require the order of domain traversal to be front to back, and because shadow rays can terminate upon any hit, shadow rays can traverse only a portion of what is required by radiance rays, thus reducing the amount of computation and communication, improving performance greatly.

Predictive scheduling attempts to find the right amount of speculation when it comes to placing a ray into domains. The right amount implies the actual distance to ray termination as a result of resolving the visibility of a ray after performing visibility queries on all of its speculative rays placed in different domains. To demonstrate the correctness of our prediction models, we measure the amount of redundancy incurred by different scheduling methods in terms of the total scheduling depth of all rays. With the depth values, we calculate the percentage of overhead defined as the ratio of a redundant scheduling depth and a required scheduling depth. For the domain traversal of a radiance ray, the required depth can be one or more of the affected domains, but for the domain traversal of a shadow ray, the required depth can be one or all of the affected domains since a single hit is sufficient to determine the occlusion of a shadow ray. Fig. 12 shows the overhead measured separately for radiance and shadow rays. As can be seen, both methods of depth-based and probability-based predictive scheduling effectively limit overhead to shallow or even sub-shallow levels, drastically reducing the overhead incurred by deep scheduling.


Fig. 12:
Percentages of scheduling overhead for rendering the noise dataset with ambient occlusion on 128 nodes. Predictive scheduling effectively limits overhead to shallow or sub-shallow levels, reducing the significant amount of redundancy incurred by deep scheduling.

Show All


Fig. 13:
Effects of adjusting the amount of speculation with the probability knob when the noise dataset is rendered on 16 nodes.

Show All

Although users do not have control over the depth-based prediction model, the model automatically adjusts speculation levels between a shallow scheduling depth and a deep scheduling depth based on the statistics measured from previous rays in flight. In addition to the self-control capability, the probability-based prediction model provides users with a control knob for adjusting speculation levels. Fig. 13 shows such controllability for various rendering algorithms.

Fig. 14 shows the performance comparison of different scheduling methods used to render the benchmark datasets. For volume rendering of the RM dataset [11], predictive scheduling achieves up to 1.7 × speedup compared to shallow scheduling and up to 1.8 × speedup compared to deep scheduling. Both deep and predictive scheduling methods begin to outperform shallow scheduling as the node count increases. For implicit isosurfacing of the RM dataset, predictive scheduling achieves up to 2 × speedup compared to shallow scheduling and up to 2.3 × speedup compared to deep scheduling. Unlike the case of volume rendering, both shallow and deep scheduling methods perform similarly, but predictive scheduling consistently outperforms non-predictive scheduling for all node counts.


Fig. 14:
Throughput results for the benchmark datasets. The error bars show standard deviation due to 50 different camera positions orbiting around each dataset.

Show All

As for rendering geometry data with path tracing, predictive scheduling outperforms shallow and deep scheduling. For the DNS dataset [18], predictive scheduling achieves up to 3 × speedup compared to shallow scheduling and up to 6.8 × speedup compared to deep scheduling. For the Lambda2 dataset [15], [18], predictive scheduling achieves up to 2.3 × speedup compared to shallow scheduling and up to 7.8 × speedup compared to deep scheduling. For the DNS and Lambda2 datasets, shallow scheduling outperforms deep scheduling overall.

For implicit isosurfacing of the Exajet dataset [20], predictive scheduling achieves up to 2.4 × speedup compared to shallow scheduling and up to 2.5 × speedup compared to deep scheduling. Both shallow and deep scheduling methods achieve similar throughput. As indicated by the error bars, the throughput variations for predictive scheduling are relatively high compared to other datasets due to irregular distributions of unstructured hexahedral cells captured from different viewpoints.

SECTION 7Conclusion
Scientific simulations are increasingly producing larger datasets reaching up to terascale levels and continuously demand high-fidelity images with realistic shading and illumination models. With such trends, highperformance rendering for in situ visualization on distributed data plays an important role. In this paper, we have developed a data-aware predictive scheduling method and its system for distributed-memory ray tracing. Specifically, we have established a communication abstraction to form a scheduling framework for asynchronous speculation. In addition’ we have incorporated simple heuristic prediction models, making the framework highly adaptable to a spectrum of scene characteristics. The framework is flexible enough to support a wide range of rendering techniques, including many variants of volume rendering and geometry rendering. By exploiting concurrency and parallelism, our scheduling method achieves many-times higher throughput on a multi-node, distributed system compared to prior methods, which makes our method fit for both interactive and offline applications. For future work, it would be interesting to consider a machine learning-based prediction model incorporated into distributed-memory ray tracers. Looking forward, we believe that our studies on predictive scheduling have established some groundwork for such a research avenue ahead.