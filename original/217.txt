Social context plays an important role in everyday emotional interactions, and others' faces often provide contextual cues in social situations. Investigating this complex social process is a challenge that can be addressed with the use of computer-generated facial expressions. In the current research, we use synthesized facial expressions to investigate the influence of socioaffective inferential mechanisms on the recognition of social emotions. Participants judged blends of facial expressions of shame-sadness, or of anger-disgust, in a target avatar face presented at the center of a screen while a contextual avatar face expressed an emotion (disgust, contempt, and sadness) or remained neutral. The dynamics of the facial expressions and the head/gaze movements of the two avatars were manipulated in order to create an interaction in which the two avatars shared eye gaze only in the social interaction condition. Results of Experiment 1 revealed that when the avatars engaged in social interaction, target expression blends of shame and sadness were perceived as expressing more shame if the contextual face expressed disgust and more sadness when the contextual face expressed sadness. Interestingly, perceptions of shame were not enhanced when the contextual face expressed contempt. The latter finding is probably attributable to the low recognition rates for the expression of contempt observed in Experiment 2.
SECTION 1Introduction
Social context plays an important role in everyday emotional interactions and others' faces often provide contextual cues in social situations. This social information is particularly useful when we are confronted with uncertainty-inducing situations, in which relevant information can be inferred from the facial expressions of others to correctly evaluate the situation [1], [2]. Investigating such complex social processes is a challenge that can be addressed with the use of computer-generated facial expressions.

Numerous lines of research have shown that emotional contextual information can strongly modulate the perception of emotion in faces (for a review, see [3]). Because we often perceive people when they are surrounded by other people, the faces of others are common contextual cues in social situations and provide crucial information. However, it is important to make a distinction between the effect of general affective information presented in contextual faces (e.g., contextual faces looking sad may make perceivers more likely to see sadness in a target face) from a more specific inferential process that occurs when the emotions of others appear to be directed at a certain person (e.g., contextual faces looking angrily at someone may make perceivers more likely to see fear in a target face). On this account, a minor physical difference in the observed interaction (i.e., a change in gaze direction) could have strong psychological effects.

Recent research using synthesized facial expressions shows that emotional reactions apparently directed towards a target avatar exert specific influences on the perception of the target's facial expressions [4], [5]. This socioaffective inferential mechanism is particularly strong when two emotions share a functional relationship. For instance, a subtle facial expression of fear in a target face was better recognized as expressing fear if another synthesized face looking at the target expressed anger than if it looked away or expressed a neutral emotion. Observers therefore based their judgments on the whole simulated social situation and inferred that another person's angry facial expression implied that the target was feeling afraid.

The functional relation between the emotion pairing of anger and fear proved to be strong in a simulated social situation, when the contextual face looked at the target [4], [5]. However, in these studies the two avatars did not ‘interact,’ in the sense that there was no mutual gaze. This leaves open the possibility that, in the absence of clear social interaction between both avatars, the subtle facial expression of fear on the target face was not perceived as a reaction to the contextual person's emotional expression, but instead as a reaction to an environmental threat. To understand how socioaffective inferential mechanisms that occur during social interaction influence emotion recognition, it makes sense to investigate emotions such as shame, embarrassment, jealousy or admiration that are, by definition, fundamentally dependent on other people's thoughts, feelings or actions [6]. An inferential mechanism for such emotions could be observed, for instance, in a situation where other people's disgusted facial expressions imply that a target should feel ashamed [7].

In the current research, we investigate the influence of such socioaffective inferential mechanisms on the recognition of social emotions. Modeling such a process using synthesized facial expressions allowed us to focus not simply on a restricted subset of facial movements, but also to include other movements (e.g., head movements) that serve communicative purposes, adjusting dynamically to changing events. Specifically, we focused on the recognition of facial expression blends of shame-sadness because of the functional relation that exists between shame, on the one hand, and the emotions of contempt and disgust, on the other. We expected that, in a social interaction condition, such expression blends would be perceived as expressing greater shame when the contextual face expressed disgust or contempt, relative to a control condition in which the two avatars did not interact. From a participant's perceptive, the interaction between the avatars could reflect a functional relation between a disgusted contextual face, expressing a signal of rejection or disapproval, and a target face that is perceived as expressing shame. We also expected a ‘contagion’ effect where there was congruency between the emotion expressed by the target and the contextual faces because, from a participant's perspective, the target face could be seen as reacting in the same way to a shared situation.

Investigating the specificity of a socioaffective inferential mechanisms to the situation in which the emotion expressed by both faces share a functional relation was important for our study, so we also used a second target expression blend (anger-disgust) in which there was a less clear-cut functional relation with the emotion expressed by the contextual face. Although it could be argued that one person could in principle respond angrily to another person's expression of disgust, this is arguably a less ‘natural’ functional relation than the one between shame and disgust. We therefore did not expect perceptions of anger in the target face to be enhanced by the presence of a disgusted contextual face, especially given the fact that the target face breaks eye contact with the contextual face, rather than engaging in the confrontational gaze typical of anger. However, perceptions of an anger-disgust target expression could be influenced by assumptions about emotional contagion if the contextual face also expressed disgust. We therefore expected that target expression blends of anger and disgust would be perceived as expressing greater disgust, relative to the control condition, when the contextual face also expressed disgust. One technical advantage of selecting blend expressions of shame-sadness and anger-disgust is that, in both cases, the respective prototypical facial expressions share common facial features and are commonly confused.

SECTION 2Experiment I
2.1 Method
Participants. Sixty-seven undergraduate students (64 females, 3 males; mean age: 19.2±1.1 years) at Cardiff University, UK, participated in partial fulfillment of a course requirement. The sample size was defined in advance on the basis of results from previous experiments [4], [5].

Stimuli. Dynamic emotional facial expressions were generated by using FACSGen (software developed at the Swiss Center of Affective Sciences [8], [9]). The software was designed to manipulate expressions in three-dimensional faces with exact control of the muscle parameters derived from the Facial Action Coding System [10]. This tool allowed us to perform highly controlled manipulations of temporal features for gaze movement and unfolding of dynamic facial expression. Photorealistic skin textures were mapped onto FACSGen faces using FaceGen Modeller [11]. These photofits give a human-like appearance to the faces [8]. Seven photofits were generated on the basis of male human faces selected from the Radboud Faces Database [12]. Three of these were used for the target face. Each displayed four blends of shame-sadness expressions and four blends of anger-disgust expressions. A pilot experiment conducted to control the ambiguity of the four expression blends of shame-sadness is presented as online material available on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TAFFC.2018.2799593, and information about the four expression blends of anger-disgust was published as supplementary materials in a previous publication [5]. The other four photofits were used for the contextual face and these displayed expressions of contempt, disgust, sadness or a neutral state. These facial expressions were validated in previous work [8].

Procedure. Participants were told that two faces would be presented on the screen, one at the center, the other in the periphery, and that their task would be to assess the emotion expressed by the central face using four rating scales. Each trial began with a fixation cross for 500 ms, followed by a dynamic sequence in which the head/gaze movements of the two avatars were manipulated in order to create the illusion of a social interaction between the two avatars. As shown in Fig. 1, in the social interaction condition the two faces looked at each other and engaged in a mutual gaze, while in the mere context condition the two faces looked in opposite directions (i.e., away from the other face and from the participant) and did not share any gaze contact. In both conditions, the head/gaze movements of the two faces were followed by (1) the emotional expression displayed on the peripheral face (hereafter, contextual face: contempt, disgust, sadness or neutral) and then by the emotional expression displayed by the central face (hereafter, target face: blends of shame-sadness or blends of anger-disgust). The duration of head/gaze movement of the two faces was the same in the two conditions (social interaction and mere context). The total duration of the sequence in all conditions was 5.07s (+500ms for the fixation cross presented at the beginning of each trial). A response window containing four rating scales was presented next. The emotion labels for these scales were disgust, sadness, anger, and shame. Participants reported the extent to which these four emotions were perceived in the target face by moving a slider between 0 and 10. Participants completed all emotion scales (but could choose 0 if they felt that a given emotion was not present in the target face). Video S1 in the Supplemental Material, available online illustrates the task performed by the participants. The order of emotion category scales on the screen was constant for any given participant, but was randomized across participants. The order of stimulus presentation was counterbalanced across participants.

Fig. 1. - 
Illustration of the dynamic sequence presented to the participants in the two context conditions (social interaction vs mere context). After the presentation of the fixation cross, both faces appeared on the screen (1), followed by a shift of the head/gaze of both faces (2). In the social interaction condition, the faces looked at each other and shared a mutual gaze, while in the mere context condition they looked into opposite directions. Following the head/gaze shift, the contextual face expressed an emotion (3), and then the target face expressed an emotion (4).
Fig. 1.
Illustration of the dynamic sequence presented to the participants in the two context conditions (social interaction vs mere context). After the presentation of the fixation cross, both faces appeared on the screen (1), followed by a shift of the head/gaze of both faces (2). In the social interaction condition, the faces looked at each other and shared a mutual gaze, while in the mere context condition they looked into opposite directions. Following the head/gaze shift, the contextual face expressed an emotion (3), and then the target face expressed an emotion (4).

Show All

All participants took part in all 16 experimental conditions: 2 (context: social interaction and mere context) × 2 (target emotion: blends of anger-disgust and blends of shame-sadness) × 4 (contextual emotion: contempt, disgust, sadness, and neutral). Each condition was represented by four trials, making a total of 64 trials per participant.

Data analysis. We computed two indices reflecting which of two emotions sharing similar facial features was judged to be more present in the target face. The shame index characterized the response of each participant to facial expression blends of shame-sadness, and reflected how much shame relative to sadness was perceived in these expressions. It was calculated as the difference between the scores on the rating scales for shame and sadness. Positive scores indicate that participants made higher ratings of shame than of sadness. Similarly, the disgust index reflected the response of each participant to facial expression blends of anger-disgust. It was calculated as the difference between the scores on the rating scales for disgust and anger, with positive scores reflecting higher ratings of disgust than of anger. Means and standard deviations for each rating scale are available as online Supplemental Material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TAFFC.2018.2799593. Table S1 presents values for target facial expression blends of shame-sadness, whereas Table S2 presents values for target facial expression blends of anger-disgust.

2.2 Results
Perception of shame. A repeated-measures analysis of variance (ANOVA) was performed on shame index scores, with Condition (social interaction vs mere context) and Contextual Emotion (neutral, contempt, disgust, sadness) as within-subjects factors. There was a significant main effect of Contextual Emotion, F(3,198)=20.03, p<.001, η2p=.233, and an interaction between the Condition and Contextual Emotion, F(3,198)=7.16, p<.001, η2p=.098. Confirming our primary hypothesis, planned contrasts revealed that shame-sadness blends were perceived as expressing more shame when the avatars interacted and the contextual face expressed disgust. As shown in Fig. 2, when the contextual face expressed disgust, scores on the shame perception index were significantly higher in the social interaction condition than in the mere context condition, F(1,66)=15.59, p<.001, η2p=.19. Interestingly, this effect was specific for a contextual expression of disgust; it was not observed when the contextual emotion was contempt, F(1,66)=0.5, p=.480, η2p=.01.

Fig. 2. - 
Mean rating on the shame perception index for target facial expression blends of shame-sadness, when the contextual face was neutral or was expressing contempt, disgust or sadness, and when the two faces looked at each other and shared a mutual gaze (social interaction condition), or when they looked in opposite directions (mere context condition). Error bars indicate within participant 95 percent confidence intervals. The asterisks indicate a significant difference between context conditions ($\ast p< .05$*p<.05; $\ast\! \ast\! p< .01$**p<.01).
Fig. 2.
Mean rating on the shame perception index for target facial expression blends of shame-sadness, when the contextual face was neutral or was expressing contempt, disgust or sadness, and when the two faces looked at each other and shared a mutual gaze (social interaction condition), or when they looked in opposite directions (mere context condition). Error bars indicate within participant 95 percent confidence intervals. The asterisks indicate a significant difference between context conditions (∗p<.05; ∗∗p<.01).

Show All

Planned contrasts also revealed that shame-sadness blends were perceived as expressing more sadness in the social interaction condition when the contextual face also expressed sadness. Scores on the shame perception index were significantly lower in the social interaction condition than in the mere context condition when the contextual face expressed sadness, F(1,66)=4.96, p=.029, η2p=.07. There was no difference between the social interaction and the mere context condition when the contextual face was neutral, F(1,66)=2.42, p=.124, η2p=.04.

Perception of disgust. A repeated-measures ANOVA on disgust index scores with Condition (mere context, social interaction) and Contextual Emotion (neutral, contempt, disgust, sadness) as within-subjects factors revealed no significant effects (all ps>.1).

SECTION 3Experiment II
Results of Experiment 1 clearly showed an effect on emotion perception in the social interaction condition when the contextual face expressed disgust, but not when it expressed contempt. However, from a theoretical perspective, at least in its complex forms, disgust reflects a social function of protecting the self from nonphysical threats [13] whereas contempt is entirely defined by its social component that implies a negative evaluation of others [14]. Therefore, although both of these contextual emotions provide a clear signal of rejection or disapproval in a social situation, a stronger effect should have been observed when the contextual face was expressing contempt, because it signals a punitive social sentiment. Given the fact that the emotional expressions of contempt used in Experiment I were previously validated in a French-speaking population in Geneva, a possible explanation for the observed results is that our English-speaking participants in Cardiff did not recognize the facial expression of contempt as expressing such emotion. Experiment 2 was designed to test this hypothesis by checking how well a new set of Cardiff participants recognized the emotions expressed by the contextual face used in Experiment 1.

3.1 Method
Participants. Thirty undergraduate students (26 females, 3 males; mean age: 19±0.7 years) at Cardiff University, UK, took part in the study. They were paid £3.00 for their participation.

Procedure. Facial expressions displayed by the contextual face in Experiment 1 (contempt, disgust and sadness) were presented as part of a web-based experiment designed to evaluate the recognition of dynamic emotional expressions in faces. Participants were shown the same animated sequence performed by the contextual face in Experiment 1: head/gaze movements of the contextual face to the right or to the left, followed by the emotional expression. A response window containing seven rating scales was presented next (labeled as disgust, sadness, anger, happiness, contempt, embarrassment, and shame). A definition of each emotion term was presented at the start of the experiment (details in online material). As in Experiment 1, participants used these scales to report the extent to which they perceived each emotion. Participants responded using all scales. The order of the emotion scales was kept constant for any given participant, but was randomized across participants. The order of stimulus presentation was counterbalanced across participants. All participants evaluated each facial expression on two different avatars and with a head/gaze movement of the face to the right and to the left, for a total of four measures for each expression.

3.2 Results
Mean scores on each emotion scale for the displayed emotions of contempt, disgust and sadness are shown in Table 1. A repeated measures analysis of variance was conducted on each portrayed emotion with the Head/Gaze Direction (right or left) and the Emotion Rating (disgust, sadness, anger, happiness, contempt, embarrassment and shame) as within-subject factors. Analyses revealed that for each portrayed emotion, there was a significant main effect of Emotion Rating (contempt: F(6,168)=14.27, p<.001, η2p=.338; disgust: F(6,168)=95.38, p<.001, η2p=.773; sadness: F(6,168)=111.03, p<.001, η2p=.799) but no significant effect of the Head/Gaze Direction (p>.1) and no significant two-way interaction (p>.1).

TABLE 1 Means of Ratings

Planned comparisons in which the displayed emotion served as the reference category revealed that expressions of disgust and sadness were well recognized. Scores on the disgust and sadness scales were significantly higher than the scores on each of the other six emotion scales (p<.01). By contrast, the expression of contempt was not recognized as expressing contempt, but instead was confused with happiness. Indeed, there was no significant difference between scores on the contempt and happiness scales, F(1,28)=.23, p=.634, η2p=.008, although the scores on the contempt scale were significantly higher than the scores on each of the other five emotion scales (p<.01).

SECTION 4Discussion
We found support for the prediction that when two avatars are seen to be engaged in a simulated social interaction, the emotion perceived in the target's facial expression is influenced by the emotion expressed by the contextual face. In Experiment 1, when avatars were engaged in a social interaction (relative to when they were not), expression blends of shame and sadness were perceived as expressing more shame when the contextual face expressed disgust. The functional relationship between disgust and shame means that a target is seen as feeling ashamed because of the disapproval message conveyed by the disgust contextual face [15]. Future studies could test other functionally related emotion pairings, such as an angry contextual face signaling that a target feels guilt, or an amused contextual face signaling that a target feels embarrassment.

The interpretation of these findings is limited by the fact that our hypotheses were tested by comparing a situation in which two faces looked at each other (social interaction condition) with a condition in which there was no mutual gaze (mere context condition). From a participant's perspective, the mere context condition could be seen as an unusual dyadic interaction that might in turn influence the emotion perceived in the target face. For example, not looking at another person could encourage perceivers to interpret the interaction as an aversive one. The rationale for comparing the social interaction condition with the mere context condition in order to test our specific hypotheses was that, from a perceptual point of view, the same emotional and social information was present in both cases, in the sense that the same expressions were present in the two faces, and the two avatars moved their heads in the same way. The only difference was that these head movements enabled or prevented mutual gaze. Thus, our findings show that a relatively minor change in the situation (i.e., whether or not the avatars looked at each other) had a strong impact on how perceivers judged the target face.

Results also revealed that what appeared to be emotional contagion between avatars led participants to perceive more sadness in the same facial expression blends of shame and sadness. When avatars were engaged in social interaction, expression blends of shame and sadness were perceived as expressing more sadness when the contextual face also expressed sadness. These findings may be specific to certain expression blends, because it was not observed when contextual disgust faces were engaged in social interaction with a target face that expressed a blend of anger and disgust, perhaps because disgust is less susceptible to emotional contagion.

A further possible limitation is that the judgment of target expressions was influenced by seeing the target's head movement, given that a sideways and downward head movement is argued to signal shame or embarrassment [16]. However, given that this movement was seen in all conditions, such an influence could not account for the fact that shame was judged to be more intense when the contextual face expressed disgust than when it expressed sadness or neutrality, but only if there appeared to be social interaction between the avatars.

Findings of Experiment 2 revealed that the expression of disgust displayed by the contextual face used in Experiment 1 was unambiguously recognized as expressing disgust. However, the corresponding facial expression of contempt was not recognized as expressing contempt, but instead was confused with happiness. It follows that the contextual expression of contempt would not have conveyed a clear message of disapproval that could be used to disambiguate the situation [14]. This explains why, in the social interaction condition of Experiment 1, the contextual facial expression of contempt did not enhance ratings of shame when participants judged blends of shame and sadness expressions. Interestingly, the facial expression of contempt used in Experiment 1 had previously been validated in a French-speaking population in Geneva, Switzerland [8]. By contrast, Experiment 2 was conducted in an English-speaking population in Cardiff, UK. The difference in these findings could be explained by cultural variation in the perception of facial contempt [17], [18], [19]. Indeed, there is evidence of a cultural bias on the part of English speakers, who appear to be unable to label contempt expressions as “contempt” [20].

A final limitation worth acknowledging is that the social interactions that served as stimuli in the current study were clearly simulations. Emotional expressions in computer-generated virtual faces have become increasingly realistic and can be used as well-controlled and dynamic stimuli in emotion research [21], [22], [23]. Although the validity of virtual emotion expressions in comparison to real emotion displays is still debated, experimental research tends to show that emotions expressed by a virtual face are recognized in a comparable way to emotions expressed by natural faces [24] but are influenced by specific factors such as the type of emotion and participant's age [25]. The major advantage of these stimuli are that they can be easily animated and systematically varied according to the experimenter's needs. Achieving the same degree of experimental control over the stimulus material using real faces would be highly challenging, but it would obviously be beneficial to replicate the current findings using naturalistic stimuli.

SECTION 5Conclusion
The purpose of our research was to investigate the influence of socioaffective inferential mechanisms on the recognition of social emotions. Investigating this complex mechanism was made possible thanks to synthesized facial expressions that allowed us to create a simulated social interaction between two avatars, while having a total control over our stimuli.

Generally speaking, our research highlights the importance of social contextual information in disambiguating facial expressions. Although our results cannot be generalized to the perception of all ambiguous facial expressions, the present findings suggest that future models of emotion recognition should be revised to include the influence of contextual factors, and especially the socioaffective inferential mechanisms that play an important role in everyday emotional interactions.

We believe that these findings open new perspectives such as using synthesized faces to investigate how affective mechanisms influence personality impression from facial appearance [26] and decision-making processes [27], [28].

