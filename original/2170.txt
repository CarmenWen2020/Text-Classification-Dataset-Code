Embeddings of high-dimensional data are widely used to explore data, to verify analysis results, and to communicate information. Their explanation, in particular with respect to the input attributes, is often difficult. With linear projects like PCA the axes can still be annotated meaningfully. With non-linear projections this is no longer possible and alternative strategies such as attribute-based color coding are required. In this paper, we review existing augmentation techniques and discuss their limitations. We present the Non-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation strategy for projected data (rangesets) with interactive analysis in a small multiples setting. Rangesets use a set-based visualization approach for binned attribute values that enable the user to quickly observe structure and detect outliers. We detail the link between algebraic topology and rangesets and demonstrate the utility of NoLiES in case studies with various challenges (complex attribute value distribution, many attributes, many data points) and a real-world application to understand latent features of matrix completion in thermodynamics.
SECTION 1Introduction
High-dimensional data is omnipresent in the form of tabular data. It occurs in economy, biology, chemistry, political science, astronomy, and physics to only name a few [34], [41]. For such data a large variety of analysis techniques is offered in modern data analysis libraries ranging from cluster analysis, to regression, to outlier detection, and dimensionality reduction [44]. The exploratory data analysis principles requests for such techniques to show for each drawn conclusion a raw data plot that convincingly shows such [56]. In many applications this raw data plot is an embedding of the high-dimensional data using methods like linear projections (e.g. PCA) or non-linear techniques like multi-dimensional scaling (MDS) or t-distributed stochastic neighbor embedding (t-SNE). Linear techniques have the advantage that the resulting axes still have meaning, but often they cannot uncover complex structures in high-dimensional space. Non-linear projections often nicely reveal complex structure in high-dimensional space, but no longer offer direct annotation of the projected space. Hence, it is paramount to equip these widely used techniques with mechanisms that help users properly read the projected data and relate original data attributes to the computed features.


Fig. 1:
Explaining machine learning: the embedding (MDS, top right) shows a non-linear projection of the 4D latent feature space as computed by machine learning for 240 chemical compounds [26]. Color-based augmentation helps relate structures in the embedded point cloud to input attributes (bottom, continuous variables) and domain knowledge (top, categorical variables). Our new approach enabled domain scientists detect previously unrecognizable patterns and outliers in their data.

Show All



Fig. 2:
Same data, different chart types: all charts present the same MDS embedding of the wine dataset [1], [20] with various augmentation strategies for the attribute alcohol (b-e). Colorcoding (b) Correctly represents that data but suffers from occlusion, clutter, and poor outlier highlighting. Field-based techniques (d+e) cannot correctly depict projection ambiguity. Rangesets (c) Combine the advantages of both techniques.

Show All

Going through examples in high-dimensional data analysis libraries [44] and recent papers [34], we observed that the gold standard for this task is still color-coding glyphs and 2D interpolation-based scalarfield reconstruction [31] as is also confirmed by the survey by Nonato et al. [41]. Fig. 2 compares these standard techniques for an MDS embedding of the wine dataset [1], [20] augmented with the alcohol levels of each sample point. Color-coding assigns each projected data point a color using one of the original attributes (Fig. 2b). This technique is easy to implement and to comprehend, but suffers from occlusion and visual clutter and makes outlier detection difficult [37]. Scalarfield reconstruction techniques [31] reconstruct a 2D scalar function for a given attribute, which serves as input to a spatial color-coding (Fig. 2d + 2e). This results in readily visible spatial patterns, but cannot account for the fact that data points with different attribute values are projected onto the same position in 2D space. In our new technique, we combine the strengths of these two directions. Our goals are to design a technique and system that are easy to use and comprehend, that are applicable to all types of embedding techniques, that can be directly integrated into existing analysis pipelines, and that enable the user to quickly and correctly understand attribute value distributions in the embedded data. We also remark that the goal of this paper is not to explain the projection, i.e., the precise nature of the data transformation from high-dimensional to 2D space, as is done for example in [16], [57] who visually encode least-varying dimensions, but only the final outcome, i.e., the embedding of the data points in the plane.

In this paper, we present NoLiES, an interactive system that enables the user to explore linear and non-linear embeddings with respect to the original attributes. The code is open source1 and a live demo is provided via MyBinder2. NoLiES inherently relies on a novel augmentation technique for multidimensional projections, which we call rangesets. Rangesets, as presented in Fig. 2c, visually group datapoints with similar attribute values into non-convex a-hulls. The method allows for outlier filtering and user defined adjustments which is explained in detail in Sect. 4. To answer questions relating to multiple attributes, an interactive analytics system is required, which will be presented in Sect. 5. Several use cases, a real-world application problem of machine learning (ML) in thermodynamics (Sect. 6), and feedback from an informal expert user study (Sect. 7) are provided to demonstrate the capabilities of the novel technique.

In summary, our contributions are as follows:

We review the state-of-the-art of direct augmentation techniques for multivariate projections and highlight strengths and interpretation challenges with these techniques.

We detail a novel visualization technique (rangesets) to augment embeddings.

We detail the connection of rangesets to algebraic topology and provide first steps on how this theory can be used to further improve augmentations of embeddings.

We present an interactive analysis system and detail the analysis workflow to interpret linear and non-linear embeddings, along with several case studies using examples from ML data bases and real world applications.

SECTION 2Related Work
NoLiES builds upon concepts for augmented multidimensional projections, improved scatterplots, and topological analysis of high-dimensional data, which we will review in the following.

2.1 Augmentation of Multidimensional Projections
Non-linear dimensionality reduction techniques are widely used for data exploration [41]. Much of the work centers around finding better projections, controlling and communicating error, and automatic detection and visualization of features. A critical aspect that receives far less support is the interpretation of projected data. Common techniques are (i) direct enrichment, (ii) cluster-based enrichment, and (iii) spatially-structured enrichment [41]. Direct techniques augment the embedding for example with attribute-based color and text labels to provide additional information [34]. A variety of techniques is presented by Aupetit [3] using color-coding on the glyphs [19], [32], [33], [52], (hex)bin visualizations for local densities and values [43], and multivariate glyph-based approaches. All these techniques have in common that they can represent per pixel only attribute values of a single data point. However, most multidimensional projections (linear and nonlinear alike) suffer from projection ambiguity, i.e., data points with different attribute values are projected onto the same 2D coordinate, which needs to be communicated. The methods closest to our approach are classified as spatially-structured enrichment, which first partition the space using techniques such as Voronoi diagrams or treemaps and afterwards enrich these regions [8], [51]. Most of these applications, however, aim at a non-overlapping partitioning, which we specifically want to integrate to correctly reflect the nature of the data.

Several techniques aim at the reconstruction of a continuous scalarfield that can be directly visualized using field visualization techniques, e.g., ProbingProjection [52] or DataContextMap [13]. These spatial techniques directly avoid overplotting and group coherent regions [37], but need special strategies to cope with projection ambiguity as will be discussed in more detail in Sect. 4.1.

The third line of research that also follows the concept of a continuous field tries to recover the non-linear axes or illustrate regions of maximal attribute values. The DataContextMap [14] enriches the embedding with additional data points that locate regions of high attribute values and augment the visualization with additional attribute-based contours on the reconstructed scalar field [13]. DimReader [23] augments the embedding with non-linear grid lines and prolines [11] display the non-linear axes. The t-viSNE system [12] presents an analytics tool to explore t-SNE projections [59] using for enrichment color-coded glyphs augmented with interactive exploration. An excellent overview of interaction with dimensionality reduction is given by Sacha et al. [47]. The techniques in this third category are complimentary to our approach and can be combined with the here proposed rangesets.

2.2 Improving Scatterplots
Scatterplot visualization and the treatment of its challenges like over-plotting and clutter have been researched in their own right [4]. Micallef et al. [39] present techniques to optimize parameter settings for scatterplots like size and opacity to automatically improve the visual results. Contours have been applied in a variety of approaches to represent set relationships in scatterplots to improve perception and to lower cognitive load [13], [15], [37], [50]. Bubble Sets [15] employ density estimates and contour lines to determine outlines for a set. Butterfly Plots [48] use convex hulls and refined convex hulls to enclose data points of the same class. Alpha Shapes, as used in our technique, were previously used by Joia et al. [28] to highlight 2D clusters. Simonetto et al. [50] start with a planar graph connecting the points of a set for which a geometric hull is computed. From a theoretical perspective, this approach is similar to ours, though we assume more data points per set, which allows us to go for geometry directly generated through a triangulation. Mayorga and Gleicher [37] present Splatterplots that combine density plots with contours to solve the overplotting problem for large numbers of points and at the same time be agnostic to outliers. This approach is conceptually very similar to ours and inspired the presented rangesets. Challenges that we wanted to further improve are locally non-uniform density and cognitive load. A more detailed discussion is given in Sect. 4.1.

2.3 Topological Methods for High-Dimensional Data
The filtration of the Delaunay triangulation, which is a simplicial complex, allows us to draw from the well researched theory of algebraic topology [21], [36]. A brief summary of the theory and how it relates to our approach are given in Sect. 4.2.2. Topological methods have a long history in high-dimensional data analysis [34]. Commonly, they are applied on the high-dimensional data in a preprocessing step to extract relevant structures, e.g. the contour tree [35] or features [45], [46], [49], [52], that help simplify the data and create abstractions that are easier to represent. Topological methods have also been used to control and evaluate the projection process [3], [18], [45] integrating Voronoi cells for quality control [3]. With this paper, we hope to initiate a connection from the embedding side, so that high-dimensional and two-dimensional topology can mutually augment each other for more insightful data analysis.

SECTION 3Problem Definition and Workflow
NoLiES supports the visual interpretation of dimensionality reduction schemes. Fig. 3 illustrates the three steps of the workflow including screenshots of the two GUI elements. NoLiES is implemented in a Jupyter Notebook [29] that, when combined with the python panel package [54], comes in two flavors: (Step 1) Scripting and static charts in the Jupyter Notebook and (Step 2) an interactive web-application that supports user interactions. When talking about NoLiES, we primarily refer to the interactive GUI that is used for the data analytics part.

Jupyter Notebook comprises data preprocessing and knowledge storage (Fig. 3 (left)): In the notebook, the user specifies data loading and cleaning routines. Additionally, they can provide custom parameter values and selections like considered data attributes, slider ranges, filter threshold, or colormaps. In the multi-dimensional projection section, the user can choose their preferred dimensionality reduction method and couple it with appropriate control mechanisms like data scaling and correlation checks.

NoLiES for interactive exploration in the GUI (Fig. 3 (right)): Once the user is happy with the data preprocessing, they change to the interactive GUI view in the browser. In this view, only the chart elements from the notebook are visible and are now linked interactively. The GUI shows the embedded data in a scatterplot and provides overview over included data attributes and attribute value distributions.

The survey by Nonato and Aupetit [41] gives an excellent overview over analytical tasks performed using multi-dimensional projections. NoLiES supports tasks relating to the two main categories Explore Dimensions (Axes) and Explore Items in Enriched Layout, i.e., it helps understand the mapped data and explore structures in the projected data respectively. In particular, we support the following fine grained tasks (names in italic are tasks as defined by Nonato and Aupetit): (i) Explore dimensions (Map Synthesized Dimension to Original Dimension, Discover Relation btw. Visual Pattern & Original Dim.): We provide a visual link between the position in 2D space and the original data attributes, which explains the attribute value distribution in the projected data. (ii) Cluster-based analysis (Name Cluster, Discover Clusters in Map, Match Clusters and Classes in Map, Brush in Data Space): The user observes regions in the embedding that are denser than their surroundings, i.e., clusters. They now want to understand what discriminates the cluster from the surrounding data and what are characteristics that points in the cluster have in common. Small multiples and interactive selections that are shared between all plots support these tasks. (iii) Outlier analysis (Discover an Outlier in Map, Discover Class-Outlier in Map): Outliers are points that are unusually far away from other points compared to average point distance. Here, the user would like to understand what sets this point apart from the surrounding points. We support this task by outlier highlighting and the cluster strategies as above.

User in the loop Commonly, the user will need to make adjustments to the default settings during analysis. Hence, the analysis runs in cycles. Additionally, the user can augment the notebook with obtained knowledge to use it in the next run and preserve it for future analysis.


Fig. 3:
Analysis workflow: NoLiES is implemented in a jupyter notebook that is well suited for script-based data preprocessing and can then be served as an interactive web-application. Knowledge that the user obtains during the analysis process, e.g. Best parameters or color codes, can be stored in the notebook.

Show All

SECTION 4Method
The centerpiece of NoLiES are the proposed rangesets (Fig. 2 (center)), which we introduce in this section. We start with a detailed review of existing augmentation strategies, continue with the description of the contour extraction algorithm including references to levelset computation from algebraic topology, and close with the visual encoding.

4.1 Discussion of Existing Approaches
In the presented rangeset approach, we wanted to overcome challenges we faced when using state-of-the-art techniques to explore non-linear projections. Starting point was the data displayed in Fig. 1, which consists of 240 points in 4D. Using standard approaches it was not possible to relate the structure in the point cloud to the input variables or domain knowledge. In this section, we review the three most widely-used concepts for scatterplot augmentation: Glyph-based colorcoding, scalarfield reconstruction, and set-based visualization. For illustration purposes, we use the UCI wine dataset [1], [20], which features the same challenges, but is more widely known and accessible (Fig. 2).

Glyph-based colorcoding (Fig. 2b) is easy to implement and gives an intuitive sense of value locations, but suffers from occlusion and over-plotting as detailed before [37]. Assessing the amount of group/color overlap and rapid detection of outliers, however, are difficult.



Fig. 4:
Parameter study for the contour parameter: the contour algorithm connects all points that have maximal distance ε. (b) Choosing small values results in fragmented contours. (e) Allowing arbitrary distance results in the convex hull.

Show All

Fig. 2d + 2e represent scalar fields as reconstructed from the point data. Two popular approaches are nested filled isocontours and triangulation-based [17] renderings. Both methods are implemented in matplotlib. While both techniques give a good sense of attribute value distributions, they have difficulties in representing regions that suffer from projection ambiguity, i.e., where points with different attribute values are projected onto each other or close to each other. Here, field-based approaches have to either work with local averages (isocontouring) or create many small color patches (triangulations).

A third concept that is used particularly for categorical attributes on scatterplots are set-based visualizations [41]. The two primary directions in which the outlines of sets can be obtained are geometric/algebraic approaches and statistical approaches. The geometric approaches operate on a simplicial complex (graph or triangulation) and derives the boundary from this construct through filtering or additional geometric operations like dilation. The convex hull of a set of points is an example of a geometrically created boundary. The statistical approach relies on a density estimate for which an isocontour is drawn. Both approaches, statistical and geometric, rely on parameters that control the outlier filtering. In the statistical case, this is achieved through the isovalue of the boundary contour. A known problem is the challenge in handling point clouds with locally varying density, which are treated, for example, with adaptive-KDE-methods [58], which, however, are hardly implemented in data analysis software packages. Geometric approaches, on the other hand, are commonly controlled by geometric criteria like maximal distance to identify outliers.

In conclusion, we want to state that all methods have use cases where they are best suited. Glyph coloring is easy to implement and works very well with a limited number of data points. Statistical approaches carry the interesting notion of probability that points may be located in a certain area of the plot and scalar-field techniques can handle an arbitrary number of color-levels if this is requested. For the rangesets presented in this paper, we chose the geometric set-based approach as it uses the strengths of spatial augmentations, can handle projection ambiguity, and is intuitive to control by a single distance parameter with good default value heuristics.

4.2 Computation of Contours
Rangesets use geometric contours to visually group data points with similar values. For categorical attributes a contour is drawn per category (see Fig. 6). For continuous variables, we first discretize the value range of the attribute and then draw a contour per bin, i.e., for a range of attribute values (see Fig. 9). In the following, we detail the algorithms used to compute the contour(s) per value bin and to filter outliers. The final algorithm then consists of the following five steps as illustrated in Fig. 5: (i) select an attribute bin, (ii) filter data points in the respective range, (iii) compute Delaunay triangulation of the filtered points, (iv) remove triangles with unwanted properties, (v) compute the boundary of the triangulation and find all points in the current range that do not belong to the contour for highlighting.


Fig. 5:
Contour computation: the algorithm consists of five steps and the key tasks are highlighted in the illustrations.

Show All




Fig. 6:
Effects of the distance parameter ε on final layout: the thermodynamics embedding is augmented with color for 20 chemical classes. Small values for ε (left: 0.75) result in core regions of the classes. Large values (right: 10) result in convex hulls that strongly overlap.

Show All

4.2.1 Non-Convex Hulls
Given a set S of n points in the plane (n being a positive integer), we are looking for a hull that tightly encloses the points of S and allows for filtering of outliers. Non-convex or minimum area hulls [2] feature these properties. Following the discussion for the choice of a non-convex hull for scagnostics [62], we use α-hulls as they can be computed efficiently and allow for filtering of outliers. The α-hull of S is the intersection of all closed complements of discs with radius −1/α that contain all the points of S for arbitrary negative reals α. As shown in [21], α-hulls can be computed efficiently from the Delaunay triangulation [17] of the point set S by excluding triangles that contain an edge whose length exceeds a given threshold ε. For the distance computation we use the Euclidean distance d(⋅,⋅). The filtered Delaunay graph Dε now has the vertex set V={0,1,…,n−1} and an edge set of E={(u,v)∈V×V|d(u,v)≤ε}, i.e., two vertices u and v are connected if and only if their distance is less than or equal to the selected distance threshold ε. The parameter ε controls the “tightness” of the computed contour. Large values include all edges of the Delaunay triangulation resulting in the convex hull [17] of the point set. Very small values will result in a strong fragmentation.

An example that illustrates the progression of the contours with increasing ε-values is shown in Fig. 4. Here, all points of the wine dataset are included in the contouring process and the resulting contours are shown for increasing ε-thresholds. Note how the set of contours changes from small fragments, over a smooth tight boundary, to the full convex hull. The choice of the “tightness” of the boundary has strong effects on the final augmentation for the embedding where multiple contours are drawn. An additional complex example is given in Fig. 6 for the thermodynamics dataset where 20 categorical classes are used to explain the embedding. For small ε-values, the contours for each class focus on dense core regions (left, ε=1). For large values, the contours overlap strongly and are no longer helpful (right). Finding a good ε-value is an important aspect of the algorithm and will be discussed in the next section.

4.2.2 Topological Filtration to Control ε
To better understand the progression of the size of the contour, we now look into algebraic topology. The above outlined algorithm induces a filtration of the simplicial complex, i.e., the subcomplexes Dε1 and Dε2 form a nesting hierarchy with Dε1 being a subset of Dε2 if and only if ε1≤ε2. On the operational level, this results in the nice property that increasing the threshold ε can only add triangles to the non-convex hull, but never remove them. For ε=0 no contour is created and each data point is an outlier and for ε=εmax, where εmax is the longest edge in the Delaunay graph, we obtain the convex hull of the set of points in the plane. For ε-values within this range, the number of outliers will decrease and the number of contours will vary. Algebraic topology is a research field that studies topological properties and changes of the simplicial complex under filtrations. For our analysis, we currently consider the zero Betti-number b0, which captures the number of connected components. Another helpful characteristic is the first Betti-number, which counts the number of holes in polygons.

To obtain a better understanding of the effects of ε, we additionally provide a topological summary. In simply connected domains like ours, this information is often presented as a contour tree [10], which depicts the merging of the contours as ε changes. As the more critical information for our application is the number of outliers and connected components for any ε, we provide this information in an area chart. You can think of this chart as a marginal of the merge tree differentiating between connected components containing exactly one and those containing multiple vertices.

An example of the topology chart is given in Fig. 4. Connected components with more than one vertex are depicted in blue and outliers in gray. The plot is truncated at the top as in the beginning g ε∈(0,0.7) most data points are outliers. Alternatively, the user can switch to a logarithmic y-axis. The graph presented here is typical for all datasets that we investigated. In the beginning, with ε=0, all datapoints are outliers. With increasing ε-values many small contours form (ε∈(0.2,0.7)), which eventually merge to larger more stable contours. Starting from ε=1, we only have one contour and the threshold parameter only controls the number of outliers. Note that the presented ε-values are not universal as they refer to edge length as computed by the embedding. Scaling the embedding can help, but one still has to account for varying aspect ratios and point densities due to number of points.

Wilkinson et al. [62] propose a default value for ε based on edge lengths in the minimal spanning tree (MST):
ε=q75+1.5⋅(q75−q25)
View Sourcewhere q75 is the 75th percentile of the MST edge lengths and the expression in the parentheses is the interquartile range of the edge lengths. It is important to note that the minimum spanning tree of a set P of point sites (in any dimension) is a subgraph of the Delaunay triangulation. Hence, we use this criterion to provide a default value for the filter threshold ε. In most practical applications we found this value too restrictive and manually raised the threshold to obtain smoother contours. An example is given in Fig. 4 where the suggested ε is 0.96 (similar to Fig. 4(c)), but was overwritten with ε=2 for the analysis.

4.2.3 Discussion of Triangle Filter Criteria
The above described algorithm demonstrates how topological analysis can be used to control the contours and we used maximal edge length for the distance function. Alternative choices, however, are possible. The three widely used local criteria when controlling triangulation quality are edge length, triangle area, and inner angles. Optimizing inner angles is already ensured through the Delaunay triangulation, which creates an angle optimal triangulation, i.e., from all possible triangulations the one with largest smallest angle is chosen [17]. This ensures that long spiky triangles are avoided as much as possible.




Fig. 7:
Comparison of filtration attributes: (left) Filtration based on edge length with thresholds of 0.9, 1.55, εmax. (right) Area-based filtration with thresholds 0.2, 0.5, Δmax, which features unwanted properties like spiky triangles and holes.

Show All

Table 1: Timings on the covertype dataset: A subset of the 500k points was sampled and the three important algorithmic steps (embedding: UMAP, error computation and ε-estimation: Minimal spanning tree MST, rangeset computation) were timed. Timings are given in seconds.


In the above algorithm, we used edge length for the filtration. An alternative choice of distance metric for the filtration that we explored during the development of NoLiES is the area of triangles, i.e., triangles and their bounding edges are included in Dε if they do not exceed the threshold size ε. The goal was to exclude large triangles that cover space without additional points inside the triangle. A comparison of the two distance metrics can be seen in Fig. 7. This choice of metric, however, proved to be much harder to control and resulted in unexpected holes in the hull and long spiky triangles not being removed.

4.2.4 Scalability and Stability
Algorithms from algebraic topology have great analytical power, but are known to be computationally expensive [5]. In our practical examples, we found the system to be interactive for datasets with up to 1,000 data points. Up to 5,000 data points required acceptable waiting times of a few seconds. We conducted a systematic study to assess running times for larger datasets. For the analysis we sampled the covertype dataset [6] which consists of 500k data points. For the embedding we used the UMAP algorithm [38] which was able to project the data within a couple of second for various sample sizes (Table 1). Assessing projection quality requires the computation of the minimal spanning tree (MST - second row) [40] which in our test cases commonly took longest and was prohibitive beyond 20k data points. We tested implementations based on the python packages scikit-learn and scipy for this task and found the distance computation in scikit-learn to be faster. Rangesets were computed for five bins and averaged across variables. Fig. 8 illustrates the range of timings for the ten variables. Depending of the distribution of points and the discretization, we observe variations of 30% of the mean value. Overall the plot shows that rangesets scale linear with the number of data points. Up to 5k data points we found a live computation of the rangesets acceptable (about 7sec waiting time). With more data points we recommend to downsample the dataset or precompute the rangesets.

Regarding the numbers of attributes, we were able to successfully use NoLiES for up to 16 attributes. Fig. 10 shows the GUI for a dataset with 11 attributes. To help users comprehend the connection between many attributes interactive selections are shared between all plots. To not obscure the underlying information, the outline curve is offset by edge width [22]. In Fig. 10 the gray outline was manually drawn, optimized with the rangeset algorithm, and shared between all plots. In this way, the user can now go through all attributes, directly find the region of interest, and deselect irrelevant plots to focus on fewer attributes.

Fig. 8: - Benchmark for the covertype dataset: the UMAP embedding is a stochastic approach designed for handling large-scale datasets. Rangesets scale linearly with the number of datapoints. Timings are given for the rangeset of one variable per bin, i.e., the total time for one chart is the sum of times as given in table 1.
Fig. 8:
Benchmark for the covertype dataset: the UMAP embedding is a stochastic approach designed for handling large-scale datasets. Rangesets scale linearly with the number of datapoints. Timings are given for the rangeset of one variable per bin, i.e., the total time for one chart is the sum of times as given in table 1.

Show All

To help the user assess truthfulness of the projection we automatically include projection quality [40] as auxiliary attribute in the GUI.

4.3 Attribute Range Discretization
The contour algorithm described so far computes the contour for a single bin of the attribute histogram and, hence, data binning plays an important role for the rangesets. Histogram computation has been intensively studied in the statistics community and we can draw from excellent prior research [24]. We use the numpy histogram function with settings for the following parameters: min-max-range of the attribute that is considered, number of bins for the discretization, step size of the discretization (uniform vs. non-uniform), and handling of values outside the selected range. We provide default values for all attributes following the reasoning below that can be overwritten by the user in the Jupyter Notebook. A screenshot of the entire system including small-multiples with histograms is given in Fig. 10.

Initially, min-max-ranges for each attribute are defined by the extremal values in the data. Adjustments may be necessary to obtain bins with easy to read values or to account for the range of possible values (e.g. 0-100% when values only range from 37-82%). In NoLiES, this can be done interactively via the range-sliders below each attribute in the attribute panel (see Fig. 10 (left) for an example). Upon dragging the slider, the contours update automatically giving instant feedback on the effects of the changes. Three samples of this interaction process on the wine dataset (attribute alcohol) are depicted in Fig. 9b. Like in this example, we experienced in general that the contours were fairly stable and the exact choice of the absolute range was not too critical. Customized ranges can be stored in the notebook and are automatically used in the next iteration. Automatic outlier detection [61] and choosing thresholds with human-interpretable values [53] may be used to further automize the process.

We discretize the selected range in the default case into five bins with equidistant boundaries. In our examples we found five bins sufficient to model any underlying distribution. Fig. 10 depicts the histograms for five bins in color and the version as computed by numpy.histogram below in gray. Using more contours resulted in hard to read images which aligns with the findings by Kraus et al. [30], who report for continuous scalar fields difficulties in precise readings of continuous maps on scatterplots, which aligns with findings by Tory et al. [55]. A comparison of a perceptually uniform continuous colormap and our discrete one are given in Fig. 9a. We are aware that “rainbow”-colormaps are not a popular choice. However, the colors in this colormap are easy to name and highly distinct, which makes comparison in a small-multiples setting (like ours) easy and allows for easy and unambiguous communication. Hence, we assign each of the five bins a fixed color [blue, green, yellow, orange, red] and a label [very low, low, medium, high, very high]. Again the colormap can be easily changed to personal preferences in the notebook and the Bokeh-library offers a rich set of default colormaps to choose from. We explored non-uniform discretization to better adapt to local structures in the point cloud, but found this to be misleading when interpreting the color distribution.


Fig. 9:
Effects of discretization: (a) Color-coded glyphs using mat-plotlib's perceptually uniform plasma colormap vs a discretized colormap based on bokeh spectral5. (b) Using the discretized colormap from (a) with additional contour outlines based on rangesets. The range can be interactively adjusted and we present three examples: (min, max), (11.7, 14.1), (12,14). In our example, we always include values below the minimum and above maximum in the extremal bins.

Show All

An additional augmentation of the histograms in NoLiES is the use of the negative y-axis. Here, we encode data points that are outside the ranges. Datapoints outside the min-max-range are encoded as glyphs below the y-axis. Datapoints that are outliers for a particular rangeset (i.e., their scalar value falls within one of the bins, but their distance to the nearest point is larger than ε) are counted and encoded as bars extending in the negative y-direction. In this way, the user can directly see how parameter settings affect the topology of the rangeset chart.

4.4 Visual Encoding
To visually represent the rangesets, we augment the scatterplot with polygons and color-coded glyphs. The contour for each bin is rendered as a filled transparent polygon (alpha value is 0.5) and is rendered below the scatterplot. We additionally render (a subset of) the datapoints including those inside contours, to provide the user with a sense of data density. Custom contours (gray outline) can be added to highlight a selected group of points. To highlight outliers, we chose the channel glyph size increasing the radius of the point glyphs. Additionally, outliers are plotted on top of the contours to not be hidden by multiple transparent layers. In the histogram view, we highlight outliers by counting them on the negative side of the y-axis.

SECTION 5Interactive System
NoLiES is implemented in python using the panel library for interaction support (Fig. 3). This allows to run the application both in the browser as a Jupyter notebook and as a standalone application. In the analysis process, we alter between the two views using each where it is strongest. The software is written in modules. Generic code like the rangeset computation is stored in an extra module. We create one notebook per dataset and can use it in this way as a storage location for knowledge that was derived during the analysis process. We intentionally do not include too many settings in the GUI to make it easy to use and provide places to store information in a uniform way.


Fig. 10:
How happy are you? the MDS embedding of the OECD better life data reveals multiple clusters that partially align with self perceived life satisfaction. The small multiples reveal patterns in six of the original attributes. Countries in the cluster of very happy countries (gray outline) have in common that they have no category (attribute) with very low ratings. Countries with very low life satisfaction (greece, south africa, portugal) feature each their own challenges (i.a. Unemployment, health, and/or safety).

Show All

Jupyter Notebook
The notebook provides three sections that guide the user through the analysis process. Part 1 handles data loading, preprocessing, and cleaning. Additionally, we include placeholders for commonly used system parameters like custom ranges for sliders, attribute filters, and the rangeset threshold. Part 2 handles the multidimensional projection. Multiple widely used dimensionality reduction methods as implemented in the scikit-learn library [44] are included and can be selected by the user. We also include best practices for dimensionality reduction like attribute scaling and tests for correlation. Part 3 takes care of the visual design and interactions. The system is implemented in python and can be rapidly manipulated by users with limited programming experience as we found in an informal user study (see Discussion).

Interactive App
The GUI of NoLiES consists of three major components (Figure 3): (i) An attribute view that lists all included attributes from the raw data, their ranges, and the selected sub-ranges to be used for the binning. (ii) The embedding renders the projected data as a point cloud with optional labels. The user can interactively alter the displayed rangesets in a dropdown menu. The title of the chart automatically includes the applied projection method as defined in the notebook. (iii) The small multiples view provides a quick overview over all selected attributes distributions and present the histograms for the binned attributes. Views can be interactively switched on and off in the attribute view with a checkbox.

The attribute sliders are interactive and upon moving the sliders, the outlines and the histograms are updated interactively. This procedure helps to understand attribute value locations and the effect of the discretization. The user can also use this technique to manually filter for outliers and set tighter value bounds for the displayed contours. Once they found good default values, these can be stored in the notebook and will be set at defaults in the next run.

Implementation
NoLIES is implemented in python in the Jupyter Notebook. The GUI is realized using the panel library [54] and charts are created using bokeh [7]. Geometric operations are realized with the shapely library [25]. Multidimensional projections and data pre-processing are provided through scikit-learn [44]. Data handling is realized using pandas [60]. Topological data analysis is implemented in NoLiES and supported through methods in the shapely library [25]. NoLiES is available on GitHub3.

SECTION 6Case Studies
In the following, we present three case studies with increasing complexity. The Better Life dataset is easy to comprehend and follow. The forest covertype dataset contains more than 4000 data points and is difficult to project. The real-world study in thermodynamics targets explainable machine learning and large number of contours in the rangeset.

6.1 OECD Better Life
The OECD Better Life dataset [42] measures 25 attributes for 40 countries (+ OECD mean). The goal is to understand which factors promote a society's well-being. For illustration purposes we chose 11 attributes that covered the general mix of topics, had varying attribute distribution profiles, and did not alter the general structure of the projected data too much. Fig. 10 shows the entire GUI view with all attributes enabled in the small multiples view.

The central chart renders the rangeset for self-reported life satisfaction (range (0,10)). The histogram on the left tells us that values range from 4.7 (South Africa - requires interaction to be determined) to 7.6 (Denmark, Finland, Norway) which were discretized into bins of width 0.5 ranging from satisfaction values of 5 to 7.5 (out of 10). Countries outside the histogram range are included in the two extremal bins. The small chart on projection quality shows that the MDS projection works in general very well and can retain the neighborhood structure (except for Luxembourg). We can observe that life satisfaction strongly correlates with the visual clusters in the 2D embedding and that the red and green class (very high and low happiness) contain most data points. The very unhappy countries (blue points) are spread across the plot. They are not grouped in a rangeset contour, but form all outliers which is reflected in the histogram (bar below the zero-line). We also observe some outliers in the orange class (large dots in the scatterplot; Chile and Mexico) who report high happiness, but are in many categories close to more unhappy countries.

To obtain an overview over the distribution of assessed attributes, we first look at the small multiples display in Figure 10 (right). Attributes with a blue checkbox are presented in the small multiples summary on the right (here all of them). We observe that the attribute ‘Feeling safe walking alone' decreases from bottom-left to top-right. The histogram below the small multiple depicts the attribute distribution. We see that countries colored in red have 80–90% agreement and blue countries less than 50%, i.e., people in countries contained in the red polygons agree to 80–90% with the statement that they feel safe when walking alone. Other attributes like ‘Labour market insecurity’ and ‘Student skills’ feature a more complex and harder to describe distribution of attribute values, which is expected in an MDS embedding.

Fig. 11: - Forest covertype dataset with 4.200 data points and 9 attributes: 70% classification accuracy can be achieved for this dataset [6] with common confusion of spruce/fir and lodgepole pine (blue vs orange) as well as douglas-fir and ponderosa pine (brown vs green) (a) which is illustrated by overlapping contours in the embedding (b). (c) Original attributes help characterize classes and analyze the algorithm.
Fig. 11:
Forest covertype dataset with 4.200 data points and 9 attributes: 70% classification accuracy can be achieved for this dataset [6] with common confusion of spruce/fir and lodgepole pine (blue vs orange) as well as douglas-fir and ponderosa pine (brown vs green) (a) which is illustrated by overlapping contours in the embedding (b). (c) Original attributes help characterize classes and analyze the algorithm.

Show All

Fig. 12: - Comparison of augmentation techniques: (a) Rangesets quickly outline regions of different parameter values. The size of outliers can be interactively increased. (b) Augmentation based on glyph-based color coding.
Fig. 12:
Comparison of augmentation techniques: (a) Rangesets quickly outline regions of different parameter values. The size of outliers can be interactively increased. (b) Augmentation based on glyph-based color coding.

Show All

Next we focus on the region outlined in gray, which contains all countries with very high life satisfaction. The outline was drawn manually using the selection tool. Concentrating on the colors inside the outline for each attribute, we can quickly observe that countries in the selection have diverse, but generally positive values in all attributes, except for labour market insecurity where low values are good. In summary, we can state that countries with very high life satisfaction do well in all assessed categories. This also discriminates the two major clusters (very happy (red) and unhappy (green) countries). Countries in the green cluster have at least one problematic category with low and very low values, disposable income, self-reported health, and safety being the most prominent ones.

The US and Luxembourg form their own small cluster close to the very happy countries, but only rate themselves with high life satisfaction. Comparing these two countries to colors inside the selection polygon for the very happy countries, we can identify attributes in which they diverge. Looking for color differences, we identify students skills (lower than in selection polygon) and household net income (higher than in selection polygon). From our investigation we can conclude that money alone does not make happy and that it probably is a combination of factors that result in (only) high life satisfaction in these two countries.

6.2 Forest Covertype
The forest covertype dataset [6] covers 581k datapoints and 54 cartographic attributes like elevation, slope, and shade. The goal is to predict the forest type (7 classes) using these attributes only. Blackard et al. [6] report 70% accuracy using an ANN. The misclassification as depicted in the confusion matrix (Fig. 11a) are extracted from their paper. Rows in the confusion matrix correspond to actual classes and columns to the predicted ones. We observe major misclassification between spruce/fir (SF) and lodgepole pine (LP) as well as douglas-fir (DF) and ponderosa pine (PP). Multiple other misclassifications occur (mainly close to the diagonal) and some classes are never confused, e.g. CW with KR/SF/LP/AS.

For the dimensionality reduction we sampled 600 data points from each of the 7 classes resulting in 4,200 data points. The embedding was computed using the 10 numeric attributes. The discarded 44 categorical attributes distinguish between wilderness areas (4 attributes) and soil types (40 attributes). The t-SNE-based embedding (perplexity: 30, early exaggeration: 30) is depicted in Figure 11b. The rangeset coloring encodes the ground truth class labels. We observe that there is a lot of overlap between colors. On closer inspection, we find that only certain colors overlap, which is in agreement with the confusion analysis of the ANN model. We observe overlap, e.g., between blue/orange/pink for SF/LP/KR or brown/green for DF/PP. Additionally, we observe that some colors only occur in particular regions, e.g. red areas are located at top-left and in multiple small regions on the southern boundary of the purple region.

It is important to note that structural analysis of t-SNE plots is challenging and may easily lead to misreadings [59]. The overlay with rangesets can help to counteract common misperceptions. As stated by Wattenberg, cluster sizes mean nothing in t-SNE [59]. With the overlayed attribute-based rangesets, the user can reconstruct the underlying distances between datapoints. Figures 11c+12 augment the plot with two of the original attributes. Regions in the same color are close in high-dimensional space with respect to this attribute. For example, low elevation regions are split into two groups by t-SNE (top-left and bottom-right). For the red Cottonwood/Willow covertype, we can thus deduce that they mainly grow in areas with low elevation and high shade values at 9am. These observations can also be made using classical glyph-based color coding. We found in our experiments that the trust in the observations was higher using rangesets – a detailed elucidation, however, is subject to future studies.

6.3 Matrix Completion in Thermodynamics
The prediction of fluid properties plays a central role in chemical engineering, e.g., for process design and optimization, since experimental measurements are usually cumbersome and expensive. Methods to predict the properties of binary mixtures are of particular importance, since also the properties of multi-component mixtures can often be described based on information on the binary ‘submixtures’ [9]. Matrix completion is a novel promising ML approach for this purpose [26], [27]. However, while data-driven matrix completion methods (MCM) yield great performance in predicting fluid properties, they are not intuitive and therefore difficult to understand from a physical perspective. This strongly reduces confidence among engineers and natural scientists and hampers their application. Hence, tools that enable a physical interpretation of MCM are paramount. In this case study, we apply NoLiES for this purpose. Note that alternative approaches using glyph coloring and clustering failed in communicating any relationship between latent MCM features and domain knowledge (Fig. 1). Figure 13 shows MDS projections of four MCM features of 240 solutes, which were trained to experimental data for the activity coefficients of these solutes at infinite dilution in 250 solvents (at a temperature of 298.15 K) [26]. Hence, these MCM features constitute latent descriptors of the solutes that are derived from mixture properties (here: activity coefficients) only.

 - 
Fig. 13: - MDS-embedding of the four MCM features of 240 solutes trained to data for activity coefficients in binary mixtures [26]. The ground truth as captured by domain knowledge covers 20 chemical classes (color code, top-left) and information on the molar mass and polarity (bottom). Comparing domain knowledge and latent MCM features $u_{i}$ (fig. 1) helps explain black-box machine learning techniques.
Fig. 13:
MDS-embedding of the four MCM features of 240 solutes trained to data for activity coefficients in binary mixtures [26]. The ground truth as captured by domain knowledge covers 20 chemical classes (color code, top-left) and information on the molar mass and polarity (bottom). Comparing domain knowledge and latent MCM features ui (fig. 1) helps explain black-box machine learning techniques.

Show All

The embedding features multiple clusters, which we overlay with expert knowledge on the chemical classes the solutes are allocated to (denoted by the color code of rangesets and glyphs) in Figure 13(top) and observe a surprising coherence. We learn that the chemical class of a solute is a suitable descriptor regarding the solute's MCM features (and its activity coefficients in mixtures to which the MCM features were trained). We furthermore observe that some chemical classes are very characteristic, e.g., alcohol (right, light green), aldehyde (top right, light purple), and nitrile (bottom, dark brown), whereas the contours of others strongly overlap (mainly located in the center). Also note that water and deuterium oxide (heavy water) appear as exceptional solutes (bottom right edge, light olive green), which fits well with their exceptional macroscopic properties. In addition, correlations of the MCM features with other physical descriptors, such as the solute's molar mass and polarity (Figure 13(bottom)), are found.

Looking at the rangesets for the four MCM features (Fig. 1(bottom)), we observe clear spatial structure. A detailed analysis of the link between MCM features and physical properties of the solutes is subject to future research and not within the scope of this paper. However, the results shown here indicate that NoLiES offers exciting physical insights into latent MCM features, which will serve as basis for a targeted enhancement of MCM, e.g., for selecting suitable physical descriptors to support the data-driven approach or predicting the MCM features of additional solutes based on readily available information on the solutes.

SECTION 7Discussion and Future Work
The goals that we set out to accomplish were as follows: Design a technique and system (i) that is easy to use and comprehend, (ii) that is applicable to all types of embedding techniques, (iii) that can be directly integrated into existing analysis pipelines, and most importantly (iv) that enables the user to quickly and correctly understand attribute value distributions in the embedded data. Goals (i + iv) are demonstrated in the use cases and were assessed in an informal expert user study with five domain scientists from various application fields that need to interpret high-dimensional data. We demonstrated NoLiES and gave them access to the notebooks. They all used data from their own work and explored structure and outliers therein (one application is reported in sect. 6.3). All experts commented directly that the visualization is visually appealing and easy to comprehend. One user commented that rangesets reminded him of cartography, which we deem an interesting analogy. The interactive GUI part proved directly accessible to all levels of computer literacy. The users with background in programming/python were additionally able to download NoLiES from GitHub and customize the notebook for the exploration of their own data. Goals (ii + iii) are ensured by the implementation in Python and Jupyter Notebooks and the lack of coupling to the embedding. We demonstrate rangesets for MDS and t-SNE, which are often used to reveal inherent structure and clusters in the data. We also tested rangesets on PCA and SVM projections with similar results. Rangesets are computed in a post-processing step and the code can be easily used outside of NoLiES.

Limitations that we encountered relate mainly to scalability issues. With increasing number of data points interactivity slows down. Computing contours in the forest covertype dataset with 4k data points takes about one second, which also holds for the computation of the embedding on a regular desktop PC. For both routines, we use external libraries that we cannot easily improve. As rangesets and the embedding are computed only once, we found the latency acceptable. Similarly, reading rangesets became challenging with many classes as in the thermodynamics example with 20 chemical classes. We currently use a bokeh default colormap. Colormaps optimized for perception that are aware of spatial overlap may further increase visibility [37].

The geometrical nature of rangesets directly suggests several extensions like the support of Boolean operations on the sets. This concept could also be applied for a further automation of the analysis process of cluster properties, which we currently did fully manually.

SECTION 8Conclusion
In this paper, we presented NoLiES, an interactive system for the interpretation of embeddings of multi-dimensional data projections. We introduced rangesets, an augmentation strategy for embeddings that outline datapoints with similar values in multiple non-convex contours. Rangesets have a dedicated handling of outliers and their only parameter is the maximal acceptable edge length between connected points. We discussed the relationship between rangesets and algebraic topology and demonstrated how the theory can be used to control the rangeset parameter. To work with rangesets of multiple data attributes, NoLiES integrates an interactive small multiples concept that is linked by selections and color coding. Important knowledge obtained during the analysis of a dataset can be stored in the notebook and used in future analysis.