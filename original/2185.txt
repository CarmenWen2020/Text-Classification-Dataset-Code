Abstract
Unsupervised domain adaptation has been a popular approach for cross-domain person re-identification (re-ID). There are two solutions based on this approach. One solution is to build a model for data transformation across two different domains. Thus, the data in source domain can be transferred to target domain where re-ID model can be trained by rich source domain data. The other solution is to use target domain data plus corresponding virtual labels to train a re-ID model. Constrains in both solutions are very clear. The first solution heavily relies on the quality of data transformation model. Moreover, the final re-ID model is trained by source domain data but lacks knowledge of the target domain. The second solution in fact mixes target domain data with virtual labels and source domain data with true annotation information. But such a simple mixture does not well consider the raw information gap between data of two domains. This gap can be largely contributed by the background differences between domains. In this paper, a Suppression of Background Shift Generative Adversarial Network (SBSGAN) is proposed to mitigate the gaps of data between two domains. In order to tackle the constraints in the first solution mentioned above, this paper proposes a Densely Associated 2-Stream (DA-2S) network with an update strategy to best learn discriminative ID features from generated data that consider both human body information and also certain useful ID-related cues in the environment. The built re-ID model is further updated using target domain data with corresponding virtual labels. Extensive evaluations on three large benchmark datasets show the effectiveness of the proposed method.

Access provided by University of Auckland Library

Introduction
Unsupervised Domain Adaptation (UDA) person re-ID-entification (re-ID) is a rising task since the annotation of all images in target domain is expensive and infeasible (Yang et al. 2019; Wu et al. 2019; Zhong et al. 2019; Song et al. 2019; Qi et al. 2019; Liu et al. 2019; Li et al. 2019; Chen et al. 2019c). Compared with conventional fully supervised training re-ID task (Zheng et al. 2019, 2017b; Huang et al. 2018, 2017b; Sun et al. 2021; Yu et al. 2019; Guo et al. 2019; Dai et al. 2019; Luo et al. 2019; Chen et al. 2019a, b; Zhou et al. 2019), UDA re-ID is more challenging due to the large bias between training (i.e., source domain) and testing domains (i.e., target domain) (Huang et al. 2019). For instance, person images captured from two different campuses have distinct illumination conditions and backgrounds (e.g., Market-1501 (Zheng et al. 2015) and DukeMTMC-reID (Ristani et al. 2016; Zheng et al. 2017b) datasets). Therefore, the bias between two domains becomes large. Directly training a classifier from one dataset often produces a degraded performance when testing is conducted on another dataset. Thus, it is important to investigate solutions for such a UDA issue. For person re-ID, UDA solutions have drawn attention in recent years (Bak et al. 2018; Deng et al. 2018; Wang et al. 2018; Wei et al. 2018; Zhong et al. 2018).

Recent UDA person re-ID methods usually adopt (or resort to) Generative Adversarial Network (GAN) to learn the domain variations through data transformation (Bak et al. 2018; Deng et al. 2018; Wei et al. 2018; Zhong et al. 2018; Qi et al. 2019; Liu et al. 2019; Li et al. 2019; Chen et al. 2019c). These approaches can be categorized into two main types: (1) general inter-domain style transfer (Bak et al. 2018; Deng et al. 2018; Wei et al. 2018; Qi et al. 2019; Liu et al. 2019; Li et al. 2019; Chen et al. 2019c); (2) inter-camera style transfer (Zhong et al. 2018; Qi et al. 2019). All of them may perform well in certain cases, i.e., domain style change or camera style change. The former is desired to transfer person images from source domain and keep their identities, while presenting similar styles with person images in target domain. The latter aims at addressing intra-domain variations caused by different camera configurations. However, these methods do not consider another factor which is a significant contributor causing differences between domains. That is BackGround (BG). For instance, when a network is trained based on limited BG information presented in a source domain, such network may not well distinguish essential pedestrian features against noise caused by BG variations in a target domain. Unfortunately, BG in target domain is normally very different from source domain. In this paper, this problem is formulated as a BG shift problem that may significantly degrade the overall performance of UDA person re-ID.

The BG difference between source domain and target domain can be categorized into two types: (1) general style difference caused by variations of illumination and camera configuration and (2) content difference caused by changes of environment (e.g., botanies, buildings, decorations, etc). Existing inter-domain style transfer approaches (e.g., SPGAN (Deng et al. 2018) and PTGAN (Wei et al. 2018)) may capture a certain type of BG difference between domains. However, due to the lack of BG information suppression, this kind of approach can only capture the general style difference in BG. The content difference in BG cannot be dealt with. Unlike the inter-domain style transfer approach, the main focus of this paper is to capture the content difference in BG in order to reduce the domain gap. By generating images with BG suppression, the content difference in BG can be weakened to better reduce the domain gap between training and testing data. In addition, such BG suppression also reduces domain gap caused by style differences.

One possible solution to sort out BG shift is to directly remove BGs using ForeGround (FG) masks in a hard manner (i.e., applying the binary masks on original images) (Farenzena et al. 2010; Huang et al. 2016; Song et al. 2018; Tian et al. 2018). However, it is observed that methods, such as JPPNet (Liang et al. 2018) and Mask-RCNN (Abdulla 2017; He et al. 2017), specifically being designed for removing BG may damage the FG information too. By simply removing BGs, this hard manner solution does improve the performance of UDA person re-ID to a certain extent (see Table 1). At the same time, it can be seen that this is still an open problem. “Is there a way to better suppress BG shift to improve UDA re-ID performance?” This paper makes the first effort to generate images where BGs are suppressed moderately instead of being completed removed in a hard manner.

Fig. 1
figure 1
Comparison between different input images for UDA person re-ID. Images from Market-1501 and DukeMTMC-reID show distinct BG shift. Images generated by SPGAN (Deng et al. 2018) and PTGAN (Wei et al. 2018) do not suppress the BG noise, and have the BG shift problem. The hard-mask solution, i.e., JPPNet (Liang et al. 2018) damages the FG. The proposed SBSGAN takes all impacts into consideration

Full size image
To address the problem above, a Suppression of BG Shift Generative Adversarial Network (SBSGAN) is proposed. Compared with hard-mask solutions, images generated by the proposed SBSGAN can be regarded as FG images, where BG is suppressed moderately. The generated images by SBSGAN can also be called as soft-mask images. In addition, previous works (Deng et al. 2018; Wei et al. 2018) show that keeping certain style consistency between data of different domains (i.e., domain style transfer) can improve the performance of UDA person re-ID. Such an idea is integrated into SBSGAN to reduce the domain gap further. Figure 1 shows images selected from two different person re-ID datasets. The BGs are very different. A model trained on one dataset may easily be biased on another one due to the BG shift problem mentioned above. Images generated by recent UDA re-ID approaches, such as SPGAN (Deng et al. 2018) and PTGAN (Wei et al. 2018) still present some undesirable results. If FG masks obtained by JPPNet (Liang et al. 2018) is directly used to zero out BGs, the FG can be badly damaged by the noisy masks. On the contrary, every pixel in the generated images is preserved in a soft manner using the proposed SBSGAN. For instance, the blurred bag in the left corner image in Fig. 1 indicates that some pixels in the blurred bag are regarded as the BG when SBSGAN is used to generate this soft-mask image. Thus, the values of these pixels are close to, but not equal to zero according to the degree of a pixel being considered as background. Compared with other solutions, Fig. 1 shows the proposed SBSGAN can generate better images which further reduce the domain differences caused by BG shift.

In order to well utilize the generated FG information and integrate useful ID-related cues from context into the network, a Densely Associated 2-Stream (DA-2S) network is proposed. This work is to argue that certain context information, e.g., companions and vehicles in BG can provide ID-related cues. Both images with suppressed BGs (generated by the proposed SBSGAN) and the original images with full BGs are respectively fed into two individual streams of DA-2S. Unlike previous 2-stream methods (e.g., (Ahmed et al. 2015; Chen et al. 2018; Zheng et al. 2017a)), this paper proposes Inter-Stream Densely Connection (ISDC) modules as new components between two streams of DA-2S. With ISDCs, the relationship between two streams can be enhanced by signals coming from two streams in training.

Fig. 2
figure 2
Schematic diagrams of the difference between this work (c) and previous attempts (a, b). a By only considering the gap between data of different domains (e.g., using generated data by SBSGAN), features learned by a re-ID model (only using labelled source domain data for training) cannot perceive knowledge from target domain, which can yield poor performance. b Ignoring the data level gap between domains, the performance can also be compromised when the knowledge learned from source domain (pre-train) is directly transferred to target domain (update) in the feature level. c A comprehensive solution jointly considers the gap between different domains from data level to feature level

Full size image
Although SBSGAN is able to mitigate BG shift and thus to reduce the gap between data in source and target domains, so far, the proposed DA-2S is not able to fully explore the data in target domain for training because target domain does not have label information for each image in the training set. To learn more discriminative ID features, this paper makes use of data in target domain as training data with virtual labels to further update DA-2S. Inspired by recently published UDA re-ID works (Fan et al. 2018; Song et al. 2020; Fu et al. 2019; Zhang et al. 2019), unsupervised clustering methods (i.e., DBSCAN Ester et al. 1996) are used to assign virtual labels for the unlabelled target domain training images. Specifically, the proposed DA-2S network is first pre-trained with labelled images from source domain. However, unlike (Fan et al. 2018; Song et al. 2020; Fu et al. 2019; Zhang et al. 2019) they use raw data from source domain to pre-train a re-ID model, this paper uses BG suppressed images generated by the proposed SBSGAN, which have fewer domain gaps. Then, such a pre-trained DA-2S network is used to extract features of each training image in the target domain. DBSCAN is used to classify these unlabelled images into different clusters. Each unlabelled training image in target domain is then assigned a virtual ID label according to the corresponding cluster. It is clear that the effectiveness of virtual label estimation highly depends on the quality of the clustering result. Therefore, a Dynamic Clustering Confidence Value (DCCV) is proposed for each image when it is selected to update the DA-2S network. Specifically, given a cluster j, the mean value of features of every candidate image in j is used as the density center of j. Then, all candidate images in j are used to calculate the distance to the density center. The average distance is employed as DCCV to measure the reliability of an image in the corresponding cluster. The images with DCCV<0 (close to the density center) are selected for DA-2S update, the others are discarded. Once images in the target domain training set have proper label information, they can be utilized to further update DA-2S previously trained by source domain data only. During the update, the natural characteristics of target domain data are explicitly considered into re-ID model (i.e., DA-2S) training. Thus, the performance is further improved.

The contributions of this paper are:

(1):
BG shift is comprehensively investigated to analyze its impact on UDA person re-ID. SBSGAN is proposed to make the first effort by generating soft-mask images in order to reduce domain gaps. In addition, compared with previous methods, BGs are mitigated rather than completely removed in generated images.

(2):
A DA-2S CNN network with the proposed ISDC components is presented to facilitate complementary information between the generated data and the ID-related cues from the BG.

(3):
The pre-trained DA-2S network can be further updated by target domain training images with estimated virtual labels produced by DBSCAN clustering method. DCCV is proposed to improve virtual label estimation quality.

(4):
Comprehensive experiments are given to show the effectiveness of soft-mask images for mitigating domain gaps and the performance of DA-2S model for UDA person re-ID.

A preliminary version of this work is reported in Huang et al. (2019). Compared with the earlier study, there are a few key differences introduced: (i) Instead of only considering the domain gap between inputs (data level) to DA-2S, this study presents a more comprehensive UDA person re-ID solution that reduces domain gap from both data level (reducing domain gap through image/visual content transformation by SBSGAN) and feature level (increasing robustness of feature description across domains and feature discriminability across IDs by training DA-2S on target domain data). In addition, unlike recently published works that only consider two different levels (Fig. 2a, b), both levels are comprehensively integrated into a whole framework (Fig. 2c). (ii) Compared with the preliminary work, the performance is significantly improved on three large person re-ID datasets by exploring the knowledge of data from target domain when training DA-2S, which is lacking in Huang et al. (2019). (iii) Besides introducing new methods, this paper further conducts more comprehensive evaluations and analyses to demonstrate insights.

Related Work
Data Transformation for UDA Person Re-ID
Following image-to-image translation approaches (e.g., CycleGAN (Zhu et al. 2017) and StarGAN (Choi et al. 2018)), some studies focus on inter-domain style transfer to mitigate domain gaps for person re-ID. Deng et al. (2018) propose SPGAN to transfer general image style between domains. Wei et al. (2018) introduce PTGAN to transfer body pixel values and generate new BGs with similar statistic distribution of target domain. Unlike SPGAN, PTGAN explicitly considers BG shift problem between domains. However, PTGAN overlooks the fact that BGs should be suppressed rather than retained because the BG shift may degrade the UDA re-ID performance. Liu et al. (2019) present an ATNet that can decompose cross-domain style transfer into a set of factor-wise sub-transfers (e.g., illumination, resolution, etc.). Then these factor-wise transfers can be fused by an ensemble strategy to magnitude different sub-factors in cross-domain image generation. In addition to the inter-domain style transfer, Zhong et al. (2018) propose to transfer the style of images between cameras to reduce the domain gap by using StarGAN (Choi et al. 2018). Qi et al. (2019) take both cross-domain and cross-camera issues into consideration to alleviate discrepancies between domains and different cameras. Li et al. (2019) add a pose disentanglement scheme to improve the cross-domain image transfer process. Chen et al. (2019c) transfer source domain images to target domain with diverse target domain contexts. A synthetic dataset is proposed to generalize illumination between different light conditions for UDA person re-ID in Bak et al. (2018). Cycle-consistency translation of GAN is employed to retain identities of the synthetic dataset. Unlike these approaches, the proposed SBSGAN concentrates on the BG shift issue by generating soft-mask images amongst different domains to reduce the domain gap.

Background Mask Solutions
To deal with the BG shift problem, one possible solution is to completely remove BGs using binary body masks obtained by semantic segmentation or human parsing methods. Currently, methods such as Mask-RCNN (He et al. 2017) and JPPNet (Liang et al. 2018) can obtain body masks with a pre-trained model on large-scale datasets, e.g., MS COCO (Lin et al. 2014) and LIP (Liang et al. 2018). However, masks obtained by these methods often contain errors due to reasons such as low-resolution person images, and highly dynamic person poses. Directly using noisy masks may further jeopardize UDA re-ID performance. Instead, the first effort is made in this paper to suppress the BG noise by generating soft-mask images. Previous work such as Kalayeh et al. (2018) embeds the concept of ‘soft’ to learn more informative features using probability maps of different body parts on feature maps. The proposed SBSGAN focuses on the data level that tries to deal with the BG shift problem for UDA person re-ID.

The 2-Stream Models
The 2-stream models have been used in many computer vision tasks (Ahmed et al. 2015; Chen et al. 2018; Sun et al. 2015; Zheng et al. 2017a, b; Huang et al. 2018). Generally, the learning objectives of 2-stream models are categorized into two types. One verifies inputs of two individual streams belonging to the same or different classes, e.g., (Ahmed et al. 2015; Zheng et al. 2017a, b; Huang et al. 2018) in person re-ID and Sun et al. (2015) in face recognition. The other type is to enrich the representation by considering the complementarity between inputs, e.g., (Chen et al. 2018) in person search. Motivated by the methods of the second type, this paper proposes a DA-2S model. Unlike the above-mentioned 2-Stream models, ISDC is introduced between two individual streams in DA-2S to strengthen the inter-stream relationship and explore a stronger complementarity between input images.

Clustering-Based UDA Person Re-ID
So far, several works attempt to use clustering-based methods to explore natural characteristics of images in target domain for UDA re-ID (Fan et al. 2018; Song et al. 2020; Fu et al. 2019; Zhang et al. 2019). All of these works use raw source domain images with ground-truth ID labels to pre-train a re-ID model. Then, this pre-trained model is used to extract features of training images in target domain. An unsupervised clustering approach (e.g., k-means++ (Arthur et al. 2006), DBSCAN (Ester et al. 1996), or HDBSCAN (Campello et al. 2013)) is adopted to classify such features into different clusters. Each image is assigned a virtual label according to the clustering results. The images, along with the corresponding virtual labels, are used to update the pre-trained CNN model. Both (Fan et al. 2018; Song et al. 2020) employ the classical ID-discriminative Embedding (IDE) network (Zheng et al. 2016; Deng et al. 2018; Zheng et al. 2017b) as the pre-trained CNN model on source domain. Based on the IDE network, Fu et al. (2019) promote the performance of clustering-based solution by adopting body part partition. Zhang et al. (2019) integrate multiple losses in a conservative stage and a promoting stage to enrich the discriminability of features for UDA re-ID.

Unlike (Fan et al. 2018; Song et al. 2020; Fu et al. 2019; Zhang et al. 2019), (1) this paper adopts the proposed DA-2S network to learn natural characteristics from target domain data through a clustering-based strategy (i.e., DBSCAN). (2) Instead of using raw source domain images to pre-train a CNN model, the data generated by SBSGAN are used as the inputs to DA-2S, where the data distribution is already close to target domain. In this way, the quality of virtual label estimation can be improved via more robust ID features extracted from DA-2S. (3) This paper proposes DCCV to further promote the reliability of images selected by clustering results to update the proposed DA-2S network.

UDA
Besides UDA for person re-ID, a few approaches consider UDA in a wide range of areas. These approaches can be roughly categorized into three types: (1) adversarial learning methods (Tzeng et al. 2017; Wang et al. 2019; Kurmi et al. 2019), (2) statistical moment matching methods (Long et al. 2015; Peng et al. 2019), and (3) optimal transport based methods (Courty et al. 2016; Damodaran et al. 2018). The first type addresses domain shift using an adversarial discriminator that aims to distinguish between samples drawn from the source and target domains. For instance, Tzeng et al. (2017) propose to learn a mapping of target domain images to the source feature space by fooling a domain discriminator. Wang et al. (2019) adversarially learn transferable feature representations by focusing on transferable regions and images between two domains. Kurmi et al. (2019) incorporate probabilistic certainty estimate into discriminator to obtain predominant regions for improving classification on target domain. The second type adopts networks to seek domain-invariant features in task-specific layer via statistical moment matching techniques (e.g., maximum mean discrepancy Gretton et al. 2006). Long et al. (2015) reduce domain discrepancy using multi-kernel MMD. Peng et al. (2019) dynamically align moments of feature distributions between source and target domains. The third type transfers knowledge from a source domain to a target domain by finding transportation of minimal cost moving. Courty et al. (2016) utilize discrete optimal transport to match the shifted marginal distributions of two domains. Damodaran et al. (2018) measure the discrepancy on joint representations based on optimal transport to align the source domain to target domain. These approaches normally assume that class labels are the same across domains. Thus, they are not suitable for cross-domain person re-ID since different re-ID datasets contain different person identities (Deng et al. 2018).

Proposed Method
Fig. 3
figure 3
Pipeline of the proposed approaches

Full size image
The proposed method is divided into three steps. The first step is soft-mask image generation to mitigate domain gaps by the proposed SBSGAN. In the second step, the generated source domain images with ground-truth ID labels are used to pre-trained the DA-2S re-ID model. In the third step, the pre-trained (or updated) DA-2S is utilized to extract features of each image in the target domain training set. The DBSCAN clustering method is adopted to classify these features into different clusters. Virtual labels are assigned to images according to the corresponding cluster. Finally, images with virtual labels are used to update DA-2S. The step three repeats several times until DA-2S can converge to an approximate optimal solution. Figure 3 illustrates the pipeline of the proposed approach in this paper.

The details of each step are introduced from Sect. 4 to Sect. 5, respectively. Specifically, the SBSGAN for soft-mask image generation is introduced in Sect. 4 (step 1). The details of the DA-2S network and pre-training DA-2S model using labelled source domain are given in Sect. 5.1 (step 2). Virtual label estimation for target domain training data and DA-2S update are presented in Sect. 5.2 (step 3).

SBSGAN for Soft-Mask Image Generation
Fig. 4
figure 4
Overview of SBSGAN. Three domains are used as an example (i.e., Domain1, 2, 3). a shows the training process of generator G. Given an input image from Domain1, G can generate the corresponding soft-mask image, and transfer the input image to different domain styles (e.g., Domain1 to Domain2) according to the indicators. The FG mask is obtained by JPPNet. b All real and fake images are used to minimize the adversarial loss and domain classification loss in D

Full size image
Network Architecture. The generator (G) of SBSGAN is adopted based on Zhu et al. (2017), given an input image, two down-sampling convolutional layers are used followed by six residual blocks (He et al. 2016) in G. Unlike (Zhu et al. 2017), two branches (without parameters sharing) are respectively used to generate soft-mask and auxiliary style-transferred images followed by the output of the last residual block. Each branch contains two up-sampling transposed convolutional layers with a stride of 2. For the discriminator (D), the PatchGAN (Isola et al. 2017; Zhu et al. 2017) structure is used without any change.

Motivation. The main purpose of SBSGAN is to suppress the impact of BG and also maintain image style consistency across domains as much as possible. There are two tasks in G. The main task is to generate soft-mask images with suppressed BGs to reduce the domain gap between training and testing data. In order to suppress the BG, a BG suppression loss is proposed for the soft-mask image generation (see Eq. 3). The auxiliary task is to generate inter-domain style-transferred images (retain BG) to normalize the style of soft-mask images across domains via the proposed style consistency loss (see Eq. 4). The cross-domain style normalization can further reduce the impact caused by the domain gap. Other two loss functions are involved in G to ensure the color and content of generated images are close to real images (see Eqs. 1 and 2). D is used to distinguish the real against fake images via a conventional adversarial loss (Goodfellow et al. 2014), and classify these images to their corresponding domains via a domain classification loss (see Eq. 5). Figure 4 shows the proposed SBSGAN.

Formulation. Specifically, given an input image (e.g., 𝐼𝔻𝑠) from source domain 𝔻𝑠, G can generate its corresponding soft-mask image 𝐼𝔻¯ by 𝐺(𝐼𝔻𝑠,𝔻¯)→𝐼𝔻¯. G takes both image (e.g., 𝐼𝔻𝑠) and indicator (e.g., 𝔻¯, refer to Sect. 4.2) as inputs. In addition, G can also transfer the style of 𝐼𝔻𝑠 to a target domain 𝔻𝑘 (𝑘≠𝑠) via 𝐺(𝐼𝔻𝑠,𝔻𝑘)→𝐼𝔻𝑘. The proposed SBSGAN is able to support multi-domain data as inputs. If there are K domains in training, then, all 𝐼𝔻𝑘(𝑘∈[1,𝐾]∩𝑘≠𝑠) and the input image 𝐼𝔻𝑠 are used to normalize the style of 𝐼𝔻¯, ensuring the style of 𝐼𝔻¯ is consistent across K domains.

Objective Functions in SBSGAN
In order to achieve the function above, several loss functions are adopted to train SBSGAN. There are four important losses in terms of the data generation, which control four different things: color, content, BG, and style consistency.

(1) ID Constraint Loss (color). Without any constraint, G may change the color of generated style-transferred images (see Fig. 8) when the style-transferred images are directly applied to normalize the style of soft-mask images across multiple domains (see Eq. 4). Therefore, in order to well preserve the color information of all generated style-transferred images, the ID Constraint (IDC) loss (Taigman et al. 2017) is adopted to preserve the underlying color information of image for the auxiliary style-transferred image generation. The IDC loss is defined as follows:

𝑖𝑑𝑐=𝔼𝐼𝔻𝑠,𝔻𝑘[‖‖𝐺(𝐼𝔻𝑠,𝔻𝑘)−𝐼𝔻𝑠‖‖1].
(1)
IDC enforces the similarity between generated image 𝐺(𝐼𝔻𝑠,𝔻𝑘) and source domain image 𝐼𝔻𝑠 via 𝐿1 norm constraint.

(2) Reconstruction Loss (content). G can generate an input image to different data according to the indicator it receives. However, there is no strong pixel-level supervision to ensure this generation process sufficiently reliable. Therefore, a REConstruction (REC) loss is required to ensure the content consistency between an input image and the corresponding generated image. REC is a conventional objective function for the domain-to-domain image style transfer when strong pixel-level supervision is unavailable (Choi et al. 2018; Deng et al. 2018; Wei et al. 2018; Zhu et al. 2017). The REC loss is given as follows:

𝑟𝑒𝑐=𝔼𝐼𝔻𝑠,𝔻𝑘∨𝔻¯[‖‖𝐺(𝐺(𝐼𝔻𝑠,𝔻𝑘∨𝔻¯),𝔻𝑠)−𝐼𝔻𝑠‖‖1],
(2)
where ∨ is ‘or’ operator. By adopting REC, when a soft-mask image (or style-transferred image) being generated, it enforces the image content of FG (or FG+BG) to be consistent with the corresponding real image. Only the domain-related information in images is expected to be changed by the G.

(3) BG Suppression Loss (BG). In order to generate a soft-mask image, this paper proposes a BG Suppression (BGS) loss to suppress BG in data generation. The BGS loss is formulated as follows:

𝑏𝑔𝑠=𝔼𝐼𝔻𝑠,𝔻¯[‖‖𝐼𝔻𝑠⊙𝑀(𝐼𝔻𝑠)−𝐺(𝐼𝔻𝑠,𝔻¯)‖‖2].
(3)
An auxiliary FG mask 𝑀(𝐼𝔻𝑠) produced by JPPNet (Liang et al. 2018) is used to suppress BG of the input image 𝐼𝔻𝑠. 𝐿2 distance is applied to minimize the loss. The mask produced by JPPNet contains certain segmentation errors. The generated soft-mask image by the proposed SBSGAN is able to tolerate (or refine) such segmentation errors during the data generation process (see Fig. 7).

(4) Style Consistency Loss (style consistency). With suppressed BG, the domain gap can be reduced in the generated soft-mask images. In order to further reduce the domain gap between target and source domain data, a Style Consistency (SC) Loss is proposed to encourage the style of soft-mask images to be consistent across all input domains. The SC loss is given as follows:

𝑠𝑐=𝔼𝐼𝔻𝑠,𝔻¯,𝔻𝑘[‖‖𝐺(𝐼𝔻𝑠,𝔻¯)−𝐼𝔻𝑠⊙𝑀(𝐼𝔻𝑠)‖‖1+∑𝑘=1,𝑘≠𝑠𝐾‖‖𝐺(𝐼𝔻𝑠,𝔻¯)−𝐺(𝐼𝔻𝑠,𝔻𝑘)⊙𝑀(𝐼𝔻𝑠)‖‖1].
(4)
As shown in Fig. 4a, all style-transferred images (i.e., Domain1 to Domain2 and 3) and the original input image are used to encourage the style of 𝐺(𝐼𝔻𝑠,𝔻¯) to be consistent across three domains. Since the information of soft-mask images concentrates on FG, the FG mask is imposed on 𝐼𝔻𝑠. All style-transferred images 𝐺(𝐼𝔻𝑠,𝔻𝑘) in Eq. 4 are used to normalize the FG information of soft-mask images across all input domains.

(5) Other Common Loss Functions. Besides the above-mentioned loss functions, the conventional adversarial loss (𝑎𝑑𝑣) (Goodfellow et al. 2014) is added to distinguish real and fake images in training. In addition, domain classification loss 𝑟𝑐𝑙𝑠 (Choi et al. 2018) is employed to classify the source domains of real images for optimizing D, and 𝑓𝑐𝑙𝑠 (Choi et al. 2018) is used to classify the target domains of fake images for optimizing G. Since the style of 𝐼𝔻¯ is normalized across K domains, a uniform distribution (i.e., 1𝐾) over K domains is used as the domain label of 𝐼𝔻¯ (refer to Fig. 4b).

The overall objective functions of G and D are given as follows:

𝐷=𝑎𝑑𝑣+𝑟𝑐𝑙𝑠,
(5)
𝐺=𝑎𝑑𝑣+𝑓𝑐𝑙𝑠+𝜆𝑟𝑒𝑐𝑟𝑒𝑐+𝜆𝑖𝑑𝑐𝑖𝑑𝑐+𝜆𝑏𝑔𝑠𝑏𝑔𝑠+𝜆𝑠𝑐𝑠𝑐,
(6)
where 𝜆 is hyper-parameter to control the contribution weights of different loss functions. This paper empirically sets 𝜆𝑟𝑒𝑐=10 and 𝜆𝑖𝑑𝑐=𝜆𝑏𝑔𝑠=𝜆𝑠𝑐=5 in experiments.

Fig. 5
figure 5
Overview of DA-2S. In a ISDC, GAP, FC, and CE respectively represent Inter-Stream Densely Connection, Global Average Pooling, Fully-Connected layer, and Cross-Entropy loss. ⊕ represents element-wise summation

Full size image
Indicators for Data Generation
The proposed SBSGAN supports multi-domain images as inputs. In experiments, images from three domains (datasets) are used in training. When images are fed into G, an indicator is concatenated after each image on the dimension of channel to let G knows what types of image should be generated. A 3D tensor 𝔻 is used as the indicator. The height and width of 𝔻 equal to the input image. There are K channels in 𝔻. For the auxiliary style-transferred image generation, 𝔻 is denoted as 𝔻𝑘; all values in the k-th channel of 𝔻𝑘 are set to 1, and other values in the remaining 𝐾−1 channels are set to 0. For the soft-mask image generation, 𝔻 is denoted as 𝔻¯; all values of 𝔻¯ are set to 1𝐾.

DA-2S
Initial DA-2S Training
One of the main contributions of this paper is to deal with the UDA person re-ID task based on the proposed inter-domain BG shift suppression. Moreover, in order to explore helpful ID-related BG cues (i.e., context information), a DA-2S network is proposed. Besides FG, the context information in BG, e.g., companions and vehicles, is also useful in UDA person re-ID. The proposed DA-2S network is to enrich person representations by learning features from both FG and BG. Figure 5a shows the DA-2S network. A pair of input images from source domain (a soft-mask image and its style-transferred image to the target domain) is fed into two ImageNet-trained Densenet-121 (Huang et al. 2017a) networks (without parameters sharing). It can be observed that the companion in white clothes is suppressed as BG in the soft-mask image. To use the companion as an ID-related cue presented in BG, a style-transferred image (without BG suppression) is fed into the second stream. In order to learn the complementarity between two inputs, ISDC is proposed and applied on the first pooling layer and every Dense Block. Specifically, the input information of each ISDC module is accumulated from the outputs of both streams as well as previous ISDC module. The output of each ISDC module can be defined as:

𝑂𝐼𝑆𝐷𝐶𝑛=𝛿((𝑦⋅𝑂𝐼𝑆𝐷𝐶𝑛−1⊕[𝑂𝑆1𝑛,𝑂𝑆2𝑛],{𝐖𝑛})),
(7)
where S1 and S2 respectively represent two streams, 𝑂𝑆1𝑛 and 𝑂𝑆2𝑛 are the outputs of two streams after the first pooling layer or after each Dense Block, 𝑛∈[1,4] represents the index of ISDC modules,  is a CNN encoder parameterized by 𝐖𝑛, ⊕ is element-wise summation, [⋅] refers to concatenation along channel dimension, y indicates whether this is the first (i.e., 𝑛=1) ISDC module between two streams. If 𝑛=1, 𝑦=0, it refers to the first ISDC module. If 𝑛∈[2,...,4], 𝑦=1, element-wise summation is used to transfer the knowledge from the previous ISDC module to the next one. 𝛿 denotes ReLU (Nair and Hinton 2010). Also, Batch Normalization (BN) (Ioffe and Szegedy 2015) is used in front of each ReLU activation function.

The final output of two Densenet-121 backbone networks (after concatenation by channel) is re-weighted using SEBlock (Hu et al. 2018) to emphasize informative features. The output of the last ISDC is directly connected to the re-weighted feature maps by an element-wise summation. Then, a Global Average Pooling (GAP) layer is used followed by a fully-connected layer (FC1), BN, and ReLU. Another fully-connected layer (FC2) is used with N neurons, where N is the number of training identities. At last, a cross-entropy loss is adopted by casting the training process as an ID classification problem.

Notably, instead of using ResNet-50, DenseNet-121 is adopted as the backbone network of two streams in DA-2S because: (1) DenseNet-121 demonstrates similar performance to ResNet-50 in some person re-ID models (e.g., the widely used IDE person re-ID architecture (Zheng et al. 2016; Deng et al. 2018; Zheng et al. 2017b)), and (2) using DenseNet-121 is able to reduce the chance of over-fitting since parameters of two streams in DA-2S are not shared, and the number of parameters in DenseNet-121 is three times less than ResNet-50.

DA-2S Update
In Sect. 5.1 a preliminary DA-2S is trained using labelled source domain data. Although the data distribution of inputs (i.e., the source domain data) of DA-2S is already close to target domain, it is still difficult to achieve good UDA re-ID performance without perceiving knowledge from target domain. That is, when DA-2S is pre-trained, only data from source domain is adopted. In this case, if there are some identities with yellow clothes in target domain, DA-2S may not learn (or perceive) features from the yellow clothes when there is no identity of yellow colour in source domain. Consequently, the discriminability of the pre-trained DA-2S is limited. To sort out this problem, this paper encourages DA-2S to learn more characteristics from target domain. The details of learned knowledge (or natural characteristic) from the target domain onto the pre-trained DA-2S network are introduced in this section.

Virtual Label Assignment for Unlabelled Data on Target Domain
In order to learn knowledge from target domain, the pre-trained DA-2S network is used to extract features of training images from each target domain. As shown in Fig. 5b, the features 𝑓𝑡𝑥𝑦, which contains knowledge from both FG and ID-related BG information, are used as deep representations of images in target domain. The extracted target domain features are denoted as follows:

𝐹𝑡𝑥𝑦={𝑓𝑡𝑥𝑦1,𝑓𝑡𝑥𝑦2,...,𝑓𝑡𝑥𝑦𝑁𝑡},
(8)
where 𝑁𝑡 is the number of training images in the target domain.

Then, unsupervised clustering is employed to classify these deep representations into different clusters. Because the number of IDs is unknown in target domain (Ester et al. 1996), the DBSCAN (Ester et al. 1996) (as in Song et al. (2020); Fu et al. (2019); Zhang et al. (2019)) is adopted for clustering since DBSCAN does not require a pre-defined number of clusters. After clustering, images that are not classified as outliers (do not belong to any cluster) by DBSCAN are assigned with virtual labels according to the clustering result:

𝑋𝔻𝑡={𝐼𝑖:(𝑦𝑖∥−1)}𝑁𝑡𝑖=1,𝑦𝑖∈[1,...,𝑁𝑐]
(9)
where 𝑋𝔻𝑡 represents the training set of target domain with virtual labels, 𝑁𝑐 is the number of clusters. Each image 𝐼𝑖 in 𝑋𝔻𝑡 is assigned a virtual label 𝑦𝑖∈[1,...,𝑁𝑐] according to the cluster that 𝐼𝑖 belongs to. If 𝐼𝑖 does not belong to any cluster, it is regarded as an outlier, and is assigned a label -1. Images with label -1 do not participate in DA-2S update.

Clustering Refinement
After clustering, a training set 𝑋𝔻𝑡 from target domain can be obtained, where each image has a virtual label. However, since the effectiveness of virtual label estimation highly depends on the quality of clustering result, Dynamic Clustering Confidence Value (DCCV) is proposed to further refine the clustering result dynamically after DBSCAN. The DCCV is used to select more reliable image samples in 𝑋𝔻𝑡, and discard samples with confidence value below a threshold. To achieve this, first, the density center of each cluster is calculated. The density center can be regarded as the mean value of feature vectors belonging to one specified cluster. For instance, given all images that belong to a cluster j, the density center 𝑓𝑡𝑥𝑦(𝑗)⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯ of cluster j can be formulated as follows:

𝑓𝑡𝑥𝑦(𝑗)⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯=∑𝑁𝑡𝑖=1[𝑓𝑡𝑥𝑦𝑖⋅(𝑦𝑖==𝑗)]𝑁𝑗𝑡,
(10)
where 𝑁𝑗𝑡 is the total number of images that belong to the cluster j. If 𝑦𝑖==𝑗, (𝑦𝑖==𝑗)=1, otherwise (𝑦𝑖==𝑗)=0.

Fig. 6
figure 6
The effectiveness of DCCV. Some samples become outliers after clustering (left figure). After conducting DCCV, unreliable samples that are far from their density centers are marked as outliers (right figure)

Full size image
Finally, given an image 𝐼𝑖 of cluster j, DCCV calculates the average Euclidean distance between each image of cluster j and the density center 𝑓𝑡𝑥𝑦(𝑗)⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯. This average distance is then used to compare the distance between 𝐼𝑖 and 𝑓𝑡𝑥𝑦(𝑗)⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯:

𝐷𝐶𝐶𝑉(𝐼𝑖)=‖‖‖𝑓𝑡𝑥𝑦(𝑗)⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯−𝑓𝑡𝑥𝑦𝑖‖‖‖2−∑𝑁𝑡𝑖=1[‖‖𝑓𝑡𝑥𝑦(𝑗)⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯−𝑓𝑡𝑥𝑦𝑖‖‖2⋅(𝑦𝑖==𝑗)]𝑁𝑗𝑡.
(11)
According to the definition of DCCV, if 𝐷𝐶𝐶𝑉(𝐼𝑖)=>0, 𝐼𝑖 is far away from its density center comparing with other samples of cluster j. Thus, 𝐼𝑖 is regarded as an unreliable sample, and its virtual label is then set to -1. That is, this sample is regarded as an outlier. Consequently, this sample 𝐼𝑖 does not participate in the DA-2S update. If 𝐷𝐶𝐶𝑉(𝐼𝑖)<0, 𝐼𝑖 is close to its density center. Therefore, 𝐼𝑖 along with its virtual label are used for DA-2S update. Figure 6 illustrates the function of DCCV. It can be observed that, after DCCV, some unreliable samples in different clusters are marked as outliers. The remaining samples are used for updating the pre-trained DA-2S model.

Loss Function for DA-2S Update
After DBSCAN and clustering refinement, preliminary 𝑁𝑐 clusters (refer to Sect. 5.2.1 and 5.2.2) are obtained. These clustering IDs are assigned to images on target domain as the virtual labels. However, the actual number of person IDs may not be 𝑁𝑐. Therefore, DA-2S is updated 𝑁𝑖𝑡𝑒𝑟 times until it reaches an approximate optimal solution. During every update, 𝑁𝑒𝑝𝑜𝑐ℎ training epochs are used, and 𝑁𝑐 is progressively updated when DBSCAN is carried out every time. In the DA-2S update stage, the CE loss is replaced with the batch-hard triplet loss (Hermans et al. 2017). This is because (1) the clusters achieved by DBSCAN do not have any semantic identity, and (2) the number of clusters is not fixed during every update. By doing so, the triplet loss can encourage the network to distinguish different classes in order to distinguish images according to the clusters they belong to. This process only needs to know which images belong to the same/different class/es (or cluster/s) according to the similarity of appearance features, which does not require any semantic identity. In addition, the CE loss is not suitable in the DA-2S update stage because CE loss needs a fully-connected layer with a fixed number of neurons to represent different semantic identities. The batch-hard triplet loss is given as follows:

𝐿𝑡𝑟𝑖𝑝(𝑓)=∑𝑖=1𝑃∑𝑎=1𝑄[𝛼+𝑚𝑎𝑥𝑝𝑜𝑠=1...𝑄‖‖𝑓𝑖,𝑎−𝑓𝑖,𝑝𝑜𝑠‖‖2ℎ𝑎𝑟𝑑𝑒𝑠𝑡𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑚𝑖𝑛𝑛𝑒𝑔=1...𝑄𝑗=1...𝑃𝑗≠𝑖‖‖𝑓𝑖,𝑎−𝑓𝑗,𝑛𝑒𝑔‖‖2ℎ𝑎𝑟𝑑𝑒𝑠𝑡𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒]+,
(12)
where P and Q represent the number of clusters (IDs) and instances of each cluster in a training mini-batch respectively. Thus, the batchsize is 𝑃×𝑄. f represents feature used to calculate the distance between an anchor sample 𝑓𝑖,𝑎 and the corresponding hardest positive (negative) mined sample 𝑓𝑖,𝑝 (𝑓𝑖,𝑛). More specifically, in a training mini-batch, 𝐿𝑡𝑟𝑖𝑝 enforces the largest gap of features belonging to the same cluster (i.e., the hardest positive sample pair) to be larger than the smallest gap of features belonging to different clusters (i.e., the hardest negative sample pair).

Finally, in the DA-2S update stage, features extracted from the soft-mask image in target domain (i.e., 𝑓𝑡𝑥), the corresponding original image (i.e., 𝑓𝑡𝑦), and their joint output (i.e., 𝑓𝑡𝑥𝑦) are used to calculate the batch-hard triplet loss as follows:

𝐿𝑗𝑜𝑖𝑛𝑡=𝐿𝑡𝑟𝑖𝑝(𝑓𝑡𝑥)+𝐿𝑡𝑟𝑖𝑝(𝑓𝑡𝑦)+𝐿𝑡𝑟𝑖𝑝(𝑓𝑡𝑥𝑦).
(13)
Three different features can refer to Fig. 5b. Experiments show that the combination of losses using different features is able to improve UDA re-ID performance (refer to Table 3).

Experiments
Comprehensive evaluations are carried out to verify the effectiveness of the proposed SBSGAN and the DA-2S network. The effectiveness of soft-mask images generated by SBSGAN and virtual label assignment to target domain data are verified qualitatively. The performance of DA-2S for UDA person re-ID is evaluated quantitatively. The experiments are mainly conducted using Market-1501 (Zheng et al. 2015) for training and DukeMTMC-reID (Ristani et al. 2016; Zheng et al. 2017b) for testing since both datasets have fixed training/testing splits. Other results are given on three widely used person re-ID datasets, including Market-1501 (Zheng et al. 2015), DukeMTMC-reID (Zheng et al. 2017b), and CUHK03 (Li et al. 2014).

Datasets for Evaluations
Market-1501 is collected from six cameras in Tsinghua University campus. It contains 751 IDs with 12,936 images for training. The average training images per ID is 17.2, making it a widely used large person re-ID dataset. The test set contains 750 IDs with 3,368 query images and 19,732 gallery images.

DukeMTMC-reID is collected from eight cameras in Duke University campus. The original dataset is used for multi-target pedestrian tracking (Ristani et al. 2016). Its re-ID version is used here for evaluation (Zheng et al. 2017b). This dataset contains 702 IDs with 16,522 images for training. Another 702 IDs are used for testing. In the testing set, there are 2,228 query images and 17,661 gallery images.

CUHK03 is captured by six cameras in the CUHK campus. It contains 1,467 IDs with 14,097 images in total. The CUHK03 dataset contains two image settings: one is annotated by hand-drawn bounding boxes, the other one is produced by the DPM detector (Felzenszwalb et al. 2009). Only the detected images are used in the experiment, which is more challenging.

Evaluation Protocol
All query images are used to retrieve person images in the galley set. Single-query evaluation is adopted. The conventional rank-n accuracy and mean Average Precision (mAP) are used as evaluation protocols (Zheng et al. 2015).

Implementation Details
SBSGAN. All images of three datasets (𝐾=3) are used to train the proposed SBSGAN. Only domain labels are used for training. Input images and their corresponding body masks are resized to 256×128. Kingma and Ba (2015) is used with 𝛽1=0.5 and 𝛽2=0.999. The batchsize is set to 16. To train G, 𝐾+116 images of each mini-batch are randomly selected for soft-mask images generation as well as the auxiliary style-transferred images generation. The remaining images in a mini-batch are used for the general style transfer to stabilize the performance of data generation in G. The learning rate is initially set to 0.0001 for both G and D, and the model stops training after 5 epochs. One G update is performed after five D updates as in Gulrajani et al. (2017). In testing, an indicator (i.e., 𝔻¯) and an original image (i.e., 𝐼𝔻𝑠) are concatenated for the soft-mask image generation. Notably, there is no need to use any FG or body mask in testing.

Initial DA-2S Training. Both soft-mask and style-transferred images (to the target domain) are used to pre-train DA-2S (refer to Sect. 5). The soft-mask images are generated by the proposed SBSGAN. PTGAN (Wei et al. 2018) is used to get the general style-transferred images as the input to DA-2S. The batchsize is set to 50. Input images are resized to 256×128 with random horizontal flipping. The SGD is used with momentum 0.9. The initial learning rate is set to 0.1, and decayed to 0.01 after 40 epochs. DA-2S stops training after the 60-th epoch. A reduction rate of 16 is used for SEBlock as in Hu et al. (2018). A dropout layer with the rate of 0.5 is inserted after FC1 (see Fig. 5a) to reduce the risk of over-fitting. The FC1 layer has 512 neurons. According to the number of training identities, FC2 has 751, 702, and 1,367 neurons when training is conducted on Market-1501, DukeMTMC-reID, and CUHK03 respectively. For each convolutional layer of ISDC, kernel size=3, and padding=1. In addition, stride=2 is used for the first three ISDC modules and stride=1 for the last ISDC module. The number of channels is doubled by each ISDC. Finally, 2,048 channels are obtained after four ISDC modules. The pre-trained DA-2S model is trained using labelled source domain data.

Fig. 7
figure 7
Comparison between hard-mask and soft-mask images. Images are selected from three different person re-ID datasets. The original images are listed in the first row. The second and the third rows respectively show hard-mask images by Mask-RCNN (Abdulla 2017; He et al. 2017) and JPPNet (Liang et al. 2018). The last row shows soft-mask images generated by the proposed SBSGAN

Full size image
DA-2S Update. DBSCAN is used for virtual label estimation in the DA-2S update stage. During every update, DBSCAN can automatically divide the data into 𝑁𝑐 clusters when the number of identities in target domain is unknown. There are two hyper-parameters that need to give DBSCAN before it does its works:

Epsilon (eps): It is defined as the maximum distance between two points to be considered as the same cluster.

Minimum points (minPts): It defines the minimum number of data points needed to determine a single cluster when the eps distance requirement is satisfied.

This paper empirically sets: 𝑒𝑝𝑠=𝑚𝑒𝑎𝑛(𝑑𝑠𝑜𝑟𝑡[1:𝑟ℎ𝑜×𝑁𝑑𝑠𝑜𝑟𝑡]) and 𝑚𝑖𝑛𝑃𝑡𝑠=4, where rho=1e-3, 𝑑𝑠𝑜𝑟𝑡 represents distance vector sorted in descending order. Values in 𝑑𝑠𝑜𝑟𝑡 are the distance between any two data points in target domain. For minPts, this paper follows the setting of minPts=4 in Ester et al. (1996). That is, a single cluster at least contains 4 images.

To update the pre-trained DA-2S model, both original target domain training images and the corresponding soft-mask images are used as inputs (refer to Sect. 5.2). The batchsize is set to 64 (16 IDs with 4 instances of each ID). Thus, in Eq. 12𝑃=16 and 𝑄=4. The input images are also resized to 256×128 with random horizontal flipping. During update, FC1, FC2, and the CE loss are removed (see in Fig. 5a). Other parts in the pre-trained model are directly loaded, including two Densenet-121 streams, ISDC modules, and SEBlock. The 𝑁𝑖𝑡𝑒𝑟=30 and 𝑁𝑒𝑝𝑜𝑐ℎ=120 (refer to Sect. 5.2.3). Thus, DBSCAN is conducted 30 times for virtual label estimation, and for each time, 120 training epochs are executed. In each update process, the initial learning rate is set to 6e-3, and it decays to 6e-4 after 100 training epochs. The 𝛼 in Eq. 12 is set to 0.5. Following the same setting of the pre-trained DA-2S, SGD optimizer is used with momentum 0.9. In testing, 4,096-dim CNN features (𝑓𝑡𝑥, 𝑓𝑡𝑦, and 𝑓𝑡𝑥𝑦 as shown in Fig. 5b) are extracted for each testing image. The Euclidean distance is used to compute the similarity between query and gallery images.

Qualitative Evaluation
Soft-Mask Images Are Better Than Hard-Mask Images in Suppression of BG Shift. In Fig. 7, a comparison is given between the generated soft-mask images and hard-mask images. The hard-mask images are respectively obtained by JPPNet (Liang et al. 2018) and Mask-RCNN (Abdulla 2017; He et al. 2017). Both methods have shown compelling performance in person parsing or object instance segmentation. However, it can be observed that both methods cannot perform well in body segmentation from BG on existing person re-ID datasets. As shown in Fig. 7, when people carry objects (e.g., bags), these objects are regarded as BGs and removed by noisy FG masks with segmentation errors. However, such features are significant to person re-ID, which should be retained rather than removed. On the contrary, in soft-mask images, important cues such as bags and body parts can be well generated and retained. This is because the binary body mask is not directly utilized on original images to remove the BGs. Although the FG mask obtained by JPPNet (the third row in Fig. 7) is also used to suppress the BG (refer to Eq. 3), the generated images by the proposed SBSGAN show better results. This phenomenon also shows that the proposed SBSGAN is robust to the noisy masks in the data generation.

The Effectiveness of Loss Functions in SBSGAN. The proposed SBSGAN jointly optimizes over several loss functions (see Eqs. 5 and 6). Fig. 8 shows images generated by SBSGAN using different loss functions. The effectiveness of 𝑖𝑑𝑐, 𝑏𝑔𝑠, and 𝑠𝑐 are verified. The others are conventional GAN-based loss functions, and their effectiveness is already evaluated by several previous works (Arjovsky et al. 2017; Choi et al. 2018; Gulrajani et al. 2017; Isola et al. 2017; Taigman et al. 2017; Zhu et al. 2017). It can be observed in Fig. 8 that when 𝑖𝑑𝑐 and 𝑏𝑔𝑠 are removed, the color information of original images cannot be well preserved. In addition, the BG cannot be well suppressed. By only removing 𝑠𝑐, SBSGAN can generate soft-mask images that are close to the objective. The 𝑠𝑐 is proposed to encourage the style of generated soft-mask images to be consistent (refer to Eq. 4). Apart from the qualitative comparison in Fig. 8, a quantitative evaluation can be found in Table 1 to verify the effectiveness of 𝑠𝑐 further.

Fig. 8
figure 8
The effectiveness of different loss functions. Best viewed in color (Color figure online)

Full size image
Table 1 Baseline performance of UDA person re-ID
Full size table
Fig. 9
figure 9
Data visualization. 5000 images are randomly selected from Market-1501 and DukeMTMC-reID to learn data distributions via the Barnes-Hut t-SNE (Van Der Maaten 2014), respectively. Another 200 images of each domain are used for visualization. The red circle and blue triangle respectively represent images belonging to Market-1501 and DukeMTMC-reID. The center points (i.e., ‘C-’) are shown using their corresponding domain color. Domain distance (i.e., 𝐿1 distance) is given between center points (Color figure online)

Full size image
Fig. 10
figure 10
The change of estimated ID number (a) and rank-1 accuracy (b) come with the increases of 𝑁𝑖𝑡𝑒𝑟 (from 1 to 30). This experiment is conducted with DukeMTMC-reID dataset as the target domain

Full size image
Reducing the BG Shift Is Effective to Reduce Domain Gaps: Visualization of Data Distributions Between Two Domains. The distance between different domains is visualized using different types of data, including the popular style-transferred images, hard-mask images, and the generated soft-mask images. Three recently published methods SPGAN (Deng et al. 2018), PTGAN (Wei et al. 2018), and StarGAN (Choi et al. 2018) are used to transfer the image style from Market-1501 to DukeMTMC-reID, respectively. Figure 9 shows the result. Compared with the general style transferred results, the hard-mask and soft-mask images can reduce the domain gap by a large margin. This phenomenon verifies the effectiveness of reducing domain gaps by considering the BG shift problem. The domain distance of hard-mask images is on par with the soft-mask images (10.19 vs. 10.90). However, compared with hard-mask images, the generated soft-mask images show better performance in UDA person re-ID (e.g., rank-1: 43.3% vs. 38.6%, see Table 1). Naturally, it is unfair to compare the domain distance between soft-mask and hard-mask images directly. This is because many pixel values of hard-mask images are simply zeroed out, making approximately half of the information of hard-mask images already being discarded in the comparison. Alternatively, soft-mask images suppress BGs rather than simply removing them.

Fig. 11
figure 11
Feature distributions between training and testing data. 5000 training data from source (Market-1501) and target (DukeMTMC-reID) domains are selected respectively to extract 𝑓𝑠𝑥𝑦 and 𝑓𝑡𝑥𝑦 via pre-trained DA-2S and updated DA-2S, respectively. Another 5000 testing data are selected from the target domain to extract 𝑓𝑠𝑥𝑦 and 𝑓𝑡𝑥𝑦 using pre-trained DA-2S and updated DA-2S respectively. Barnes-Hut t-SNE (Van Der Maaten 2014) is used to learn the feature distributions between testing and training data extracted via two different DA-2S models. The distribution center point is denoted by ‘C-’. 𝐷(⋅,⋅) represents the distribution distance (i.e., 𝐿1 distance) between center points

Full size image
Table 2 Ablation study of DA-2S
Full size table
DA-2S Update Can Encourage the Number of Estimated IDs to Be Close to the Number of Real IDs. As shown in Fig. 10a, the number of estimated IDs are recorded when the DBSCAN clustering approach is conducted every time. The real ID number in target domain training set is 702 (i.e., DukeMTMC-reID). At beginning, for example the first five 𝑁𝑖𝑡𝑒𝑟, the estimated ID number increases fast after DA-2S updated each time. It is close to the real ID number when 𝑁𝑖𝑡𝑒𝑟=16. Then, the number of estimated ID gradually reaches a stable range (i.e., [720, 728] when 𝑁𝑖𝑡𝑒𝑟=[20,30]). Although the estimated ID number is not exactly the same as the real ID number, this experiment demonstrates that DA-2S update can effectively explore the natural characteristics on target domain according to the quality of virtual ID label estimation. In Fig. 10b, it can be observed that the rank-1 accuracy increases dramatically when 𝑁𝑖𝑡𝑒𝑟=[1,5] (i.e., from 58.3% to 70.4%), which is in line with the increase of estimated ID number. After that, the rank-1 accuracy grows steadily, and it achieves the best result when 𝑁𝑖𝑡𝑒𝑟=29. Finally, 𝑁𝑖𝑡𝑒𝑟 is set to 30 according to this experiment.

DA-2S Update Can Effectively Reduce the Feature Distribution Gap Between Training and Testing Data. The feature distributions of training and testing data are visualized in Fig. 11. The features are extracted by DA-2S. As shown in Fig. 11a, when only using the pre-trained DA-2S network, the feature distribution of source domain training data is far from the feature distribution of target domain testing data (i.e., 40.92). On the contrary, when DA-2S is updated after 𝑁𝑖𝑡𝑒𝑟 times, it can clearly see that the feature distributions of target domain training data are close to the feature distribution of target domain testing data (i.e., 2.76). This experiment demonstrates that DA-2S update can effectively reduce the gap of features between training and testing domains.

Table 3 Ablation study of DA-2S update
Full size table
Quantitative Evaluation
Soft-mask Images vs. Other Types of Images. The widely used IDE model (Zheng et al. 2016, 2017b; Deng et al. 2018) with ImageNet-trained DenseNet-121 as backbone network is adopted to compare UDA re-ID performance across different types of images, including the generated soft-mask images, the general style-transferred images, and hard-mask images. Table 1 lists the performance. By directly using the original images, the performance is inferior (mAP: 17.7%, rank-1: 33.5%). A clear performance improvement is achieved by directly removing BGs from both training and testing images using FG masks obtained by JPPNet and Mask-RCNN, respectively. However, the performance of soft-mask images outperforms hard-mask images by +4.7% in rank-1 accuracy (43.3% vs. 38.6%). This is because hard-mask images normally contain segmentation errors. The general style-transferred results such as PTGAN and SPGAN achieve competitive performance. However, the soft-mask images obtain the best rank-1 accuracy (43.3%), which shows the effectiveness by considering the BG shift problem in UDA person re-ID. In addition, without 𝑠𝑐, images generated by SBSGAN can satisfy the visual requirement (see Fig. 8), but the performance is dropped by 1.1% in mAP and 1.6% in rank-1 accuracy. This is because 𝑠𝑐 is used to normalize the style of soft-mask images across multiple domains, by which the inter-domain gap can be further reduced. Since SPGAN and PTGAN only support images of two domains as inputs, the proposed SBSGAN is also trained in the same way instead of using images from three domains. Without interference from images of the third domain (i.e., CUHK03), the performance gains by Soft-mask2−𝐷𝑜𝑚𝑎𝑖𝑛𝑠 (mAP: 23.5%, rank-1: 44.2%). However, multiple domains as inputs are still used in all experiments to generate soft-mask images, through which only one model is trained instead of training multiple models between any two domains.

Ablation Study of Pre-Trained DA-2S. An ablation study of the proposed DA-2S network is given in Table 2. Without SEBlock and ISDC (i.e., baseline), DA-2S achieves 28.8% in mAP and 50.2% in rank-1 accuracy. By using SEBlock, the performance is improved from 50.2% to 50.5% in rank-1 accuracy. To strengthen the inter-stream relationship, Basel.+SEBlock+ISDC produces the best performance (mAP: 30.8%, rank-1: 53.5%), demonstrating the effectiveness of the proposed ISDC modules. If SEBlocks are added to every ISDC module (ISDC-SE), the performance is dropped by 2% in rank-1 accuracy. This is because additional SEBlocks produce more parameters, which can potentially increase the risk of over-fitting. Moreover, the inputs of 2-stream DA-2S is changed to style-transferred images or soft-mask images only (i.e., the network receives two style-transferred images or two soft-mask images) to show the performance (i.e., DA-2S† and DA-2S‡). The results demonstrate that the combination of two types of images is better than using them independently.

Table 4 Comparison with SOTA methods
Full size table
Ablation Study of DA-2S Update. The ablation study of DA-2S update is given in Table 3. The pre-trained DA-2S is used as baseline (mAP: 30.8%, rank-1: 53.5%). It can be seen that when only using one pre-trained stream for updating (without loading the ISDC and SEBlock modules), the performance can be improved from 53.5% to 61.1% and 68.7% in rank-1 accuracy. However, compared with using both streams (i.e., rank-1: 74.9%), the performance of using one stream is still inferior. This phenomenon verifies both FG and ID-related information in BG should be jointly considered in DA-2S update. The experiment also tries to remove a part of loss functions in Eq. 13 to testify the effectiveness of each loss when full pre-trained DA-2S model is used for updating. It can be observed that both 𝑓𝑡𝑥 and 𝑓𝑡𝑦 are useful when they participate in DA-2S update. If removing one or both of them, the rank-1 accuracy can be reduced from 74.9% to 73.6% (w/o 𝐿𝑡𝑟𝑖𝑝(𝑓𝑡𝑥)), 73.8% (w/o 𝐿𝑡𝑟𝑖𝑝(𝑓𝑡𝑦)), and 73.5% (w/o 𝐿𝑡𝑟𝑖𝑝(𝑓𝑡𝑥) & 𝐿𝑡𝑟𝑖𝑝(𝑓𝑡𝑦) ), respectively. This experiment demonstrates that features extracted from both FG and ID-related information in BG can improve UDA re-ID performance in DA-2S. At last, the experiment attempts to only remove the DCCV module when DA-2S update is conducted. The performance is reduced from 76.9% to 74.9% in rank-1 accuracy. This experiment demonstrates the effectiveness of DCCV.

Comparison with State-of-the-Art UDA Person Re-ID Methods. The performance of the proposed approach is compared with several recently published State-Of-The-Art (SOTA) UDA re-ID methods, including PUL (Fan et al. 2018), PTGAN (Wei et al. 2018), SPGAN+LMP (Deng et al. 2018), TJ-AIDL (Wang et al. 2018), HHL (Zhong et al. 2018), ATNet (Liu et al. 2019), PAUL (Yang et al. 2019), ECN (Zhong et al. 2019), CR-GAN (Chen et al. 2019c), PDA-Net (Li et al. 2019), UCDA-CCE (Qi et al. 2019), CASCL (Wu et al. 2019), PCB-R-PAST (Zhang et al. 2019), SSG (Fu et al. 2019), and Theory (Song et al. 2020). For all methods, training images from source have ground-truth ID labels, and ID labels in target domain are unavailable. Table 4 lists the comparison results. It is clear to see that the proposed method achieves the best UDA re-ID performance. The proposed approach outperforms the SOTA method SSG++ by 3.7% (1.7%) in rank-1 accuracy when testing is conducted on DukeMTMC-reID (Market-1501). Besides, if CUHK03 is used to pre-train DA-2S, the proposed approach outperforms the SOTA approach PCB-R-PAST 2.9% (6.7%) in rank-1 accuracy when testing is conducted on DukeMTMC-reID (Market-1501). It can be seen that SSG++ achieves competitive performance compared with SBSGAN+DA-2S-I (without using re-ranking post-processing trick). This is because SSG++ uses: (1) body part partition, and (2) a joint training strategy via clustering-guided semi-supervised training. This paper does not try to further divide the input images into different body parts and conduct clustering on each part. In addition, the clustering-guided semi-supervised training needs to double the loss items in the final objective function (i.e., Eq. 13). The proposed approach of this paper only focuses on the whole images by considering the importance of FG and ID-related information in BG. Through DA-2S update that benefits from knowledge of target domain data, the proposed approach significantly improves the performance of the preliminary work (Huang et al. 2019) (e.g., the rank-1 accuracy is improved from 53.5% to 79.7% when testing is conducted on DukeMTMC-reID). In addition, compared with existing clustering-based UDA approaches (i.e., PUL, PCB-R-PAST, SSG, and Theory listed in Table 4), the proposed DA-2S method, which is based on soft-mask images generated by SBSGAN, achieves the best performance.

Comparison with Other UDA Approaches. TADA (Wang et al. 2019) and CADA (Kurmi et al. 2019) are two UDA methods following the adversarial domain adaptation framework, which was first presented in ADDA (Tzeng et al. 2017). The difference is that both TADA and CADA further consider transferable regions of the foreground (“relevant” foreground regions) under the adversarial framework by introducing an attention mechanism, which is close to the motivation of the proposed method in somehow (i.e., BG suppression for UDA). TADA does not have shared code. Therefore, CADA and ADDA are used for comparison. Both methods are used for the UDA classification task. In order to use them for UDA re-ID, in the testing stage, only the feature extractor in Kurmi et al. (2019) (or called target domain encoder in Tzeng et al. (2017)) is used to extract person representations for re-ID. The re-ID matching is conducted using these representations in testing. The experiment results indicate that both methods are inferior to UDA re-ID approaches. For instance, ADDA (CADA) only achieves 1.6% (2.3%) in rank-1 accuracy when Market1501 is used as the source domain, and DukeMTMC-reID is used as the target domain.

These methods are powerful benchmarks for the classic UDA classification tasks. The adversarial framework presented in these methods is used to learn a mapping of each target image to the source feature space. This process is achieved by fooling a domain discriminator to distinguish target image features (encoded by target domain encoder) from the source domain (refer to Tzeng et al. (2017) for details). Both methods can be successfully utilized for the UDA classification task since it assumes that the class labels are the same across source and target domains (e.g., for digits adaptation, both source and target domain contain images with the number from 0 to 9). The mapping between a target domain and a source domain can be established with the aligned class space. On the contrary, in UDA person re-ID, the source domain and target domain contain entirely different person identities/classes (Deng et al. 2018). Thus, under the same adversarial framework, representations extracted from the target domain encoder will be hard to distinguish different IDs since the discriminator has to force the encoder to learn mappings between the target domain and the source domain. However, the IDs are non-overlapped across two domains. Therefore, it is observed that CADA and ADDA are not suitable for UDA person re-ID even CADA also considers “relevant” foreground regions.

Fig. 12
figure 12
Background suppression images with randomly erased regions (marked by red bounding boxes) (Color figure online)

Full size image
Quantification of Impact Caused by Incorrect BG Suppression for Domain Adaptation. Quantifying the possible limitations of the proposed method by downgrading the quality of BG suppression should be more comprehensive. To achieve this, the proposed DA-2S is trained by (1) using images with incorrect BG suppression on both source and target domains and (2) using images with incorrect BG suppression on one domain only (source or target). In order to mimic generated images by SBSGAN with incorrect BG, each generated image has a P possibility to randomly remove a region (𝑃={25%,50%,75%}, see Fig. 12) to make up an incorrect BG suppression training data set. These data are used to train the DA-2S network. The result is shown in Table 5. It is observed that compared with incorrect BG suppression on source domain, only (see Table 5\textcircled2) the adaptation performance drops dramatically when the BG cannot be well suppressed on target domain (i.e., Table 5\textcircled1 and \textcircled3). It also can observe that when the ratio of incorrect BG suppression cases increases, (i.e., P increases from 0 to 75%) the performance also suffers. This experiment quantifies a possible limitation that the adaptation performance is more sensitive to the BG suppression performance on target domain. This is because source images are only used for providing some prior knowledge in DA-2S initialization. The final performance is significantly contributed by target images that may cause severe performance degradation if the BG suppression cannot work well on the target domain.

Table 5 Adaptation performance of downgrading the quality of BG suppression of generated images
Full size table
Brief Analysis of Model Complexity
The proposed SBSNGAN adopts commonly used architectures. For example, other GAN-based UDA re-ID approaches, such as SPGAN (Deng et al. 2018), PTGAN (Wei et al. 2018), StarGAN (Choi et al. 2018) as well as the proposed SBSGAN use PatchGAN (Isola et al. 2017; Zhu et al. 2017) as the discriminator (see Sect. 4). For the generator, minor modifications are made by these methods. For instance, SPGAN (Deng et al. 2018) adopts an extra SiaNet; PTGAN (Wei et al. 2018) introduces 3 extra residual layers; The proposed SBSGAN uses 2 extra up-sampling transposed convolutional layers. However, in general, the number of parameters is similar between these methods. Note that, the four different loss functions defined in Sect. 4.1 can be regarded as significant contributors to the quality of generated data by the proposed SBSGAN, which do not introduce additional parameters. Besides parameters, the proposed SBSGAN shows high efficiency for training. It only needs to train one single generator that can support multi-domain data transfer with only 5 training epochs (see Sect. 6.3). Previous methods, such as PTGAN (Deng et al. 2018) and SPGAN (Wei et al. 2018) should train one new generator between any two domains. The StarGAN (Choi et al. 2018) used in HHL (Zhong et al. 2018) also needs training on each individual domain to learn the inter-cameras style transfer. In terms of model training, the proposed SBSGAN takes 2.9 h against SPGAN (29.6 h), PTGAN (160 h), and StarGAN (62.3 h). This comparison is conducted on one single Nvidia-1080Ti GPU using all datasets. The proposed SBSGAN model uses much less running time.

The virtual labelling (achieved by clustering) is conducted on the proposed DA-2S network after DA-2S initialization. This process does not need extra parameters beyond parameters in DA-2S. It can be seen in Table 3 that different components (i.e., each stream and the ISDC module) in the proposed DA-2S contribute differently to improve the UDA performance.

Conclusion
In this paper, the BG shift issue is first considered to reduce domain gaps for UDA person re-ID. SBSGAN is proposed to generate soft-mask images with BG being suppressed. Compared with hard-mask solutions, soft-mask images are able to suppress BG in a moderate way. Compared with general inter-domain style-transferred approaches, soft-mask images can further reduce the domain gap by considering the BG shift problem. A DA-2S model is introduced along with the proposed ISDC module to make use of helpful BG cues. To further explore/learn the natural characteristics from unlabelled target domain training data. An update strategy is given based on the proposed DA-2S network and images generated by SBSGAN. Based on DBSCAN clustering results, the proposed DCCV is used to improve the virtual label estimation quality. Experiment results demonstrate the effectiveness of the proposed approach in both qualitative and quantitative evaluations. SOTA performance is achieved.