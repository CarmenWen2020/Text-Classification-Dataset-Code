Multiple Object Tracking (MOT) in the wild has a wide range of applications in surveillance retrieval and autonomous driving. Tracking-by-Detection has become a mainstream solution in MOT, which is composed of feature extraction and data association. Most of the existing methods focus on extracting targets’ individual features and optimizing the association by hand-crafted algorithms. In this paper, we specially consider the interrelation cue between targets and we propose Human-Interaction Model (HIM) to extract interaction features between the tracked target and its surrounding. The interaction model has more discriminative features to distinguish objects, especially in crowded (dense) scene. Meanwhile we propose an efficient end-to-end model, Deep Association Network (DAN), to optimize the association with graph-based learning mechanism. Both HIM and DAN are constructed by three kinds of deep networks, which include Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Graph Neural Network (GNN). The CNNs extract appearance features from bounding box images, the RNNs encoder motion features from historical positions of trajectory. And then the GNNs aim to extract interaction features and optimize graph structure to associate the objects in different frames. In addition, we present a novel end-to-end training strategy for Deep Association Network and Human-Interaction Model. Our experimental results demonstrate performance of our method reaches the state-of-the-art on MOT15, MOT16 and DukeMTMCT datasets.
Introduction
Multiple Object Tracking (MOT) is one of the most significant components in computer vision technology, which has been widely applied to video surveillance retrieval, scene understanding and autonomous driving. MOT in the wild aims to track the targets in crowded scene. A tracker identifies targets according to their features and associates the same target within different frames as contiguous trajectory. The trajectory is commonly utilized for human behavior analysis, action recognition and human feature supplement. However, MOT is still a challenging task due to unfavorable factors such as occlusion, scene complexity and indistinguishable objects. Existing methods merely consider individual features (e.g. appearance and motion) rather than interrelation cue between objects. Some complicated scenes (as shown in Fig. 1) are therefore difficult to be processed by existing methods. In addition, these methods are composed of multiple independent algorithms, which are not combined together as an integral deep network architecture. In this paper, we specially focus on inter-relation cue between objects to extract interaction features between tracked-target and its surrounding. The interaction model has more discriminative features to distinguish objects, especially in crowded scene. Meanwhile, we additionally design a novel end-to-end training method to optimize the graph structure instead of hand-crafted method.

Fig. 1
figure 1
The most popular datasets on MOT in the wild currently, a:MOT15 (Leal-Taix et al. 2015) b:MOT16/MOT17 (Milan et al. 2016) c:Duke-MTMCT (Ristani et al. 2016) d:MOT20 (Dendorfer et al. 2019) e:JRDB (Martín-Martín et al. 2019)

Full size image
Tracking-by-detection (TBD) has been widely used for the MOT task in recent years, which is usually based on the bounding boxes detected by leveraging off-the-shelf object detectors such as Cascade RCNN (Cai and Vasconcelos 2018), Faster RCNN (Ren et al. 2015) and Yolov3 (Redmon and Farhadi 2018). TBD associates the bounding boxes according to their temporal-spatial information in different frames. Specifically, under the assumption that the same individual from different frames has similar characteristics. TBD calculates similarity between bounding boxes according to the extracted features (e.g. appearance, motion, shape). And the tracker associate the bounding boxes which have high-similarity. The bounding boxes are linked together to form a trajectory. Therefore, traditional methods generally are divided into two modules: feature extraction and bounding box association.

Feature extraction aims to comprehensively describe an object using discriminative features, which include colors, textures, positions, boundaries, velocity, structural feature, etc. Recently, deep neural networks such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) have been widely used for the MOT task. However, most existing methods learn features from single individual but they neglect the relationship between the tracked-target and its surroundings information, which causes ”lost-tracking” or ”mis-tracking” for target in the crowd scenario. Figure 2a shows an example of complex scene at the 146-th, 222-rd and 301-st frame of sequence, respectively. The red bounding box on the left of each picture indicates a tracked person, who are occluded frequently by the other person and even sometime are occluded completely. Traditional methods are not able to track the target effectively. Fortunately, the persons around the target relatively reserve some information such as their appearance and the relative-position with target (as shown in Fig. 2b). Therefore, human-interaction detail becomes a powerful and effective feature for tracking, especially in complex scene.

Fig. 2
figure 2
Tracking person in crowd by deep interaction feature. a: Target and video sequence b: Corresponding interaction feature for each frame

Full size image
Bounding boxes association is usually formulated as a Graph Optimization problem, which is about associating and removing the edge between nodes on graph structure constructed by bounding boxes and their similarity. Each bounding box is regarded as a node on the graph, meanwhile the similarity of nodes represents edge weight between nodes. The optimized and pruned graph reserves several sub-graphs, and the nodes on each sub-graph are considered as the same individual, which are labeled the same ID subsequently.

Fig. 3
figure 3
Our method pipeline: 1.Extracting feature (appearance, motion and interaction) for each node from detection results; 2.Constructing the graph structure for all of nodes; 3.Optimizing the graph structure and obtaining the association result

Full size image
Currently, the graph optimization algorithm on MOT hasn’t relied on deep learning strategy, and existing approaches continue to utilize the traditional solution such as Hungarian Algorithm (Sahbani and Adiprawita 2017) and Network Flow Algorithm (Schulter et al. 2017). MOT is a high-level semantic task, compared with low-level semantic or image processing task such as image deblurring, defogging, deraining and even semantic segmentation, MOT is more difficult to construct an end-to-end network by CNN. So far feature extraction and graph optimization are still treated as two independent tasks, they still haven’t been combined as an end-to-end model for training together.

In this paper, we propose an end-to-end Deep Association Network (DAN) (as illustrated in Fig. 3), which can jointly learn and process the feature extraction and data association together. The DAN is divided into two parts: 1. Feature Extraction; 2. Graph Optimization. In the feature extraction part, besides the two individual feature extractors (appearance and motion), we additionally propose a novel inter-relation feature extractor, Human-Interaction Model (HIM), whose interaction feature is extracted by Graph Neural Network (GNN). In the Graph Optimization part, we also utilize GNN to replace hand-crafted algorithm to optimize the graph. GNN has the ability to learn and process the topological data from a large amount of data. The advantage of GNN is that it can input arbitrary graph structures. We utilize specific loss function and large-scale tracking training data to train GNN, thus the interactive information can be extracted by GNN, ultimately nodes which belong to the same individual tend to be together. We connect the two parts sequentially for end-to-end training. Our contributions of the framework is as follows:

End-to-end MOT model framework: We firstly combine the feature extraction and graph optimization from two independent tasks to form an unified task as an end-to-end model. The framework is named as Deep Association Network (DAN).

More discriminative deep interaction feature: we propose a Human-Interaction Model (HIM) to extract the interrelation details of tracked-target and its surrounding, which is more effective for the targets with frequent occlusion in the crowded scene.

Special Training Strategy: We specially create graph-structured dataset for training DAN. In order to make it converge better, we set different learning rates for training different layers and train the multiple models stage-by-stage.

Developable MOT baseline: Deep Association Network is an unprecedented model structure for multiple object tracking, therefore DAN is worth continuing to be explored and be researched on how to improve the performance of MOT.

Our proposal greatly enhances the effectiveness of tracking in the high-density crowds and complex scenes. Our experiments demonstrate that our method achieves superior effectiveness and robustness over state-of-the-arts.

Related Work
Multiple Object Tracking has attracted researchers’ attention recently. The performance of MOT improves gradually at the MOT benchmark (Milan et al. 2016). Among the methods of MOT, (Henschel et al. 2018; Tang et al. 2017; Xiang et al. 2015; Choi 2015; Kim et al. 2015; Chen et al. 2017) designed an ingenious data association or multiple hypothesis. Schulter et al. (2017) is the first to combine feature extraction and hand-crafted graph structure to learn together. Bergmann et al. (2019) accomplishes tracking without specifically targeting any of these tasks. Keuper et al. (2018) combines point trajectories and clustering of bounding boxes to track objects. Chen et al. (2019) addresses the association method at the tracklet-level. Levinkov et al. (2017); Maksai et al. (2017); Ma et al. (2019) presented network flow and graph optimization which are powerful approaches. (Shen et al. 2018) aims at gluing feature learning and data association into a unity by a bi-level optimization formulation.

The appearance model aims to extract human features from an image. Tang et al. (2017); Sadeghian et al. (2017); Yang et al. (2019) train the CNN on the basis of person re-identification (Yang et al. 2020, 2019, 2020) to extract the image features, and Son et al. (2017) utilizes the quadruplet loss to enhance the feature expression. Chu et al. (2017) builds the CNN model to generate visibility maps to solve the occlusion problem. And , Henschel et al. (2018) uses a novel multi-object tracking formulation to incorporate several detectors into an integrated tracking system. Kim et al. (2015) extends the multiple hypothesis by enhancing the detection model. (Ma et al. 2018) address a sophisticated model to process trajectories. Zhu et al. (2018); Gao et al. (2018) propose spatial and temporal attention mechanisms to enhance the performance of MOT. Wang et al. (2019) combines temporal and appearance information together as a unified framework.

Fig. 4
figure 4
The Framework of Deep Association Network. The top of figure describes tracking progress, the bottom of figure illustrates the DAN structure, which is a sequential processing, the figure only shows initial stage and the first two stages

Full size image
The motion model defines the rule of object movement, which is utilized for prediction of trajectory position in the future by their historical positions. Motion model generally is divided into linear position prediction (Son et al. 2017) and non-linear position prediction (Dicle et al. 2013). Hong Yoon et al. (2016) designs the structural constraint by the location of people to optimize assignment. Following the success of RNN models for sequence prediction tasks, (Alahi et al. 2016) proposes social-LSTM to predict the position of each person in a scene.

The interaction model extracts interaction features which describe the inter-relation information between the tracked-target and its neighboring targets. The interaction features not only express the relative position information between targets, it additionally include the visual features of the targets. Nowadays typical interaction models such as social force models and crowd motion pattern models have been widely used on Pedestrian (Crowd) Simulation (Chen et al. 2018; Yang et al. 2020), Anomaly Detection (Zhang et al. 2020; Cai et al. 2020) and Trajectory Forecasting (Sadeghian et al. 2019; Kosaraju et al. 2019). In social force models, targets are considered as agents which determine their velocity, acceleration and position based on characteristics of other objects and the environment. Motion pattern model utilizes collective spatial-temporal structure and various modalities of motion to analyze the behavior of pedestrians. Sadeghian et al. (2019); Kosaraju et al. (2019) predict the behavior and position of targets in the future by interaction model, where (Sadeghian et al. 2019) presents Sophie model for path prediction for multiple interacting agents in a scene, and Kosaraju et al. (2019) addresses Social-BiGAT that generates realistic multi-model trajectory prediction by better modeling the social interactions of pedestrians in a scene. However, interaction models haven’t been utilized adequately yet on MOT, a few methods (Sadeghian et al. 2017; Lan et al. 2018; Wang et al. 2015) only utilized the relative positions between the tracked-target and its surrounding targets to extract a simple interaction feature. Hong Yoon et al. (2016) designs a structural constraint by the location of people to optimize assignment. However, for the targets in highly-crowded scenery, only using motion information cannot distinguish the persons who stand close together, so appearance information becomes the most discriminative features. Thus, combining motion and appearance feature together can describe targets’ interaction features better.

GNN was previously applied to Natural Language Programming (NLP), physical simulation and etc. For instance, (Battaglia et al. 2018) summarizes the principle and the applications of GNN. Li et al. (2016); Kipf and Welling (2016); Veličković et al. (2018) respectively proposed the GNN variant structure, GGSNN (Gated Graph Sequence Neural Network), GCN (Graph Convolutional Network) and GAT (Graph Attention Network). Duvenaud et al. (2015) focuses on molecule feature descriptor, and each molecule is composed by atoms as a graph structure. Kipf et al. (2018) aims to research physical simulation by GNN, more specifically the interaction of dynamical particles system, meanwhile they realize basketball player trajectories’ prediction. Recently, GNN has been utilized for computer vision. GNN-based few-shot transfer learning is presented by Garcia et al. (2017), and polygon refinement for instance segmentation is addressed by Acuna et al. (2018). Yan et al. (2018) adopts spatial-temporal skeleton graph for action recognition, and Shen et al. (2018) constructs relationship graph to train Re-identification model. GNN is able to extract the topological data such as molecule structure, body skeleton, etc. Interaction feature also includes the topological structure (relative position) and node attributes (appearance and motion information), therefore, GNN becomes the most suitable technique to extract interaction features according to inter-relationship and targets’ information.

Deep Association Framework
In this Section, we introduce the modules of Deep Association Network one-by-one. The framework of our network and the definition of the tracking task are described in Sec.3.1. The state transitions for nodes are introduced in Sec.3.2. The regular feature extraction is explained in Sec.3.3. The details for Human-Interaction Model are introduced in Sec.3.4. We describe the strategy of the graph construction in Sec.3.5. Lastly Sec.3.6 gives the strategy of the training of GNN.

Fig. 5
figure 5
The architecture of Feature Extraction, which is consisted of Motion Encoder, Appearance Extractor and Human-Interaction Model

Full size image
Deep Association Pipeline
Deep Association Network (DAN) is composed by three kinds of feature extractors (Appearance Extractor, Motion Encoder and Human-Interaction Model) to parallelly extract the targets’ features, and then the features are fed into Graph Neural Network (GNN) to optimize the graph structure (as described in Fig. 4). Firstly, we obtain the detection results for each frame by the detectors. The bounding boxes for each image are treated as nodes on the graph. The details of each node are extracted by Feature Extraction modules, which are divided into three parts, the architecture of Feature Extraction is illustrated in Fig. 5. Appearance Extractor (AE) is applied to extract appearance features from the cropped images according to the bounding boxes, meanwhile Motion Encoder (ME) is utilized for encoding the corresponding bounding boxes’ information, which contains the position, width, height and velocity of the bounding boxes as the motion features. Human-Interaction Model (HIM) generates an interaction feature, which not only includes the relative position between the tracked-target and its neighboring targets, but also fuses the appearance features of the targets together. And then, we concatenate the three types of features together as the nodes’ characteristics. After feature extraction, we construct an adjacent matrix according to the spatial-temporal relationship of the bounding boxes within the frames to connect the nodes on the graph structure. The adjacent matrix and the concatenated features are fed into GNN to optimize the graph. GNN propagates node features on the graph structure and learns the relationship between the nodes. Finally, the features of the nodes close to each other have higher similarity, which can be identified as the same person, the nodes are sequentially linked to form a complete trajectory.

We formulate the near-online tracking as a local bounding box association task between the tracked candidates and the current detection results in a video fragment. We define the set of detection results in the t-th to (𝑡+𝜂)-th frames as 𝑡 (𝑑𝑘𝜉∈𝑡,𝜉∈[𝑡,𝑡+𝜂]), where 𝑑𝑘𝜉 is the k-th detection in frame 𝜉, and 𝜂 is the length of the sub-sequence. 𝑡 (𝑐𝑘𝜉∈𝑡,𝜉<𝑡) indicates the tracked results from initial frame to the t-th frame, where 𝑐𝑘𝜉 is k-th tracked-node in frame 𝜉. Bounding boxes association can be perceived as graph optimization. Therefore, we construct a global graph structure  (𝐺𝑡∈,𝐺𝑡=(𝑡,𝑡)), the global graph  consists of several local graphs {𝐺1,𝐺1+𝛿,𝐺1+2𝛿...,𝐺1+𝑛𝛿,𝑛𝛿≤𝐿}, where 𝐺∗ indicates local graph constructed from the video fragment from t-th to (𝑡+𝜂)-th frames, 𝛿 is the stride of video fragment on timeline. L indicates total length of the video. 𝑡 (𝑣𝑘𝜉∈𝑡,𝜉∈[1,𝑡+𝜂],𝑡=𝑡∪𝑡) indicates the set of nodes in the graph, each node stands for a bounding box and 𝑣𝑘𝜉 denotes the k-th node in frame 𝜉, and nodes are defined as 7 dimensions [t, id, x, y, w, h, s] which are the tracklet id by tracker, the object time, the center position (x, y), width and height of the bounding box, and the state s of the node. 𝑒𝑖𝑗𝜉∈𝑡 is the edge between 𝑣𝑖𝜉1 and 𝑣𝑗𝜉2 on 𝐺𝑡. The cost function of our method is given by

𝑎𝑟𝑔𝑚𝑖𝑛⎛⎝⎜⎜∑𝐺𝑡∈𝐹𝑆(𝑣𝑖𝜉,𝑣𝑗𝜉)𝑒𝑖𝑗𝜉+∑𝐺𝑡1,𝐺𝑡2∈𝐹𝑆(𝑣𝑖𝜀,𝑣𝑗𝜀)𝑒𝑖𝑗𝜀⎞⎠⎟⎟𝑠.𝑡. 𝐺𝑡1∩𝐺𝑡2≠∅
(1)
The first term in Eq.(1) measures the accuracy of the data association in single graphs 𝐺𝑡∈ according to output of model, where 𝜉∈[𝑡, 𝑡+𝜂], 𝑣𝑖𝜉,𝑣𝑗𝜉∈𝐺𝑡, and 𝑒𝑖𝑗𝜉∈{0,1} indicate whether two nodes 𝑣𝑖𝜉 and 𝑣𝑗𝜉 belong to the same person, 𝐹𝑆(𝑣𝑖𝜉,𝑣𝑗𝜉) measures the similarity between the nodes. The second term in Eq.(1) checks whether the association results of adjacent graphs 𝐺𝑡1,𝐺𝑡2 in overlap region are consistent, where 𝜀∈[𝑡2, 𝑡1+𝜂], 𝑡2=𝑡1+𝛿 indicate the frame index in the overlap region. 𝑣𝑖𝜀,𝑣𝑗𝜀 come from the same graph 𝐺𝑡1 or 𝐺𝑡2. Since the outputs of 𝐺𝑡1 or 𝐺𝑡2 are different, and 𝑣𝑖𝜀,𝑣𝑗𝜀 exist in both of 𝐺𝑡1 or 𝐺𝑡2, so we need to calculate the cost twice for the same two-nodes. Eq.(1) belongs to a target function, we minimize the cost value of Eq.(1) by adjusting the architecture, hyper-parameter, etc to improve the effectiveness of the model indirectly.

Node State Transitions
In Fig. 4, there are 5 kinds of colors to represent the state s of nodes (e.g. Purple: “Unassigned”, Blue and Indigo:“Tracked”, Red : “Lost”, Black: “Quitted”). The blue and indigo dots are linked (associated) by the corresponding nodes in the next frame, we named these nodes as ‘tracked’, where blue dot is the first node of a tracklet. The red dots indicate the nodes at the current frame or the nodes not being associated by other nodes in their next frames, the nodes are named as ‘lost’. Only one node for each tracklet can be a ‘lost’ node, which is in the last frame of a tracklet. All of the purple dots come from the detection bounding boxes in the current frame, they are labelled as ‘unassigned’ nodes.

There are four situations that the node will be switched to other state:

“Lost”→“Tracked”: If the ‘lost’ nodes are associated by an ’unassigned’ node in next few frames, the ‘lost’ nodes will be turned as ‘tracked’ nodes.

“Unassigned”→“Lost”: If ‘unassigned’ nodes are associated by ‘lost’ nodes, they will be linked to tracklet as the last node and its state turns to a new ‘lost’ node.

“Unassigned”→“Quitted”: If the ’unassigned’ nodes aren’t associated by any nodes, they will be labelled as ’quitted’.

“Lost”→“Quitted”: We use RNN to predict the position in the future for every tracklets after each stage by their historical positions to determine whether the node leaves the scene. If the tracklet goes out of the scene, the ‘lost’ node of the tracklet will be labelled as the ‘quitted’ node. If a tracklet has not been associated for long time, in general, the difference between the last frame of the tracklet and the current frame is greater than the stride of sequence, 𝛿, the ‘lost’ node will be labelled as ‘quitted’.

Regular Feature Extractor
Regular Feature Extraction is used to extract the classical characteristics of the individual to distinguish the differences between the nodes (bounding boxes). For the same individual in different frames, it has similar features for a period of time such as wearing, position, body size and velocity. These cues are totally summarized as two parts: appearance features and motion features. In our framework, the appearance features are extracted by several shared-weight CNNs, and the motion features are encoded by RNNs.

Appearance Extractor focuses on the pedestrian features (e.g. color, shape and texture) from each bounding box located by the detection model. we treat the appearance model as a person re-idenidentification (Re-ID) task initially to obtain the pre-trained model for CNNs of DAN. We combine the three public Re-ID datasets (Market1501 Zheng et al. 2015), DukeMTMC-ReID Zheng et al. 2017) and CUHK03 Zhong et al. 2017)) to train the homostructural CNNs model of DAN. 𝑓𝑖𝑎,𝜉 and 𝑓𝑖𝑐𝑙𝑠_𝑎,𝜉 indicate the outputs of CNN’s appearance feature and classification vector, respectively, for node 𝑣𝑖𝜉, the 𝑓𝑖𝑎,𝜉 is the n-dimensional vector, and the 𝑓𝑖𝑐𝑙𝑠_𝑎,𝜉 is mapped to the K-dimensional vector by a fully-connected layer from 𝑓𝑖𝑎,𝜉, n denotes the training set class number. 𝐹𝑎(∗) represents the model forward function of appearance model:

𝑓𝑖𝑐𝑙𝑠𝑎,𝜉,𝑓𝑖𝑎,𝜉=𝐹𝐴(𝐼𝑖𝜉),𝜉∈[𝑡,𝑡+𝜂]
(2)
where 𝐼𝑖𝜉 indicates the croped image of the node 𝑣𝑖𝜉. We use the cross-entropy loss 𝑎𝑝𝑝(∗) in the multi-class classification task for the identification:

𝑎𝑝𝑝(𝑓𝑖𝑐𝑙𝑠_𝑎,𝜉,𝑝𝑖𝑎,𝜉)=∑𝑘=1𝐾−𝑝𝑖𝑎,𝜉[𝑘]⋅log(𝑝̂ 𝑖𝑎,𝜉[𝑘])𝑝̂ 𝑖𝑎,𝜉=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑓𝑖𝑐𝑙𝑠_𝑎,𝜉)
(3)
where 𝑝̂ 𝑖𝑎,𝜉∈ℝ1∗𝐾 is the probability by prediction, 𝑝̂ 𝑖𝑎,𝜉[𝑘] indicates the probability for the k-th class. 𝑝𝑖𝑎,𝜉∈ℝ1∗𝐾 indicates the ground truth label. If the i-th target belongs to the k-th class, then 𝑝𝑘𝑎,𝜉=1, others elements=0.

When the identification loss tends to converge, all of the parameters will be loaded into CNNs from DAN as the pre-trained model.

Motion Encoder analyzes the pedestrian movement and predicts the position in the future. The inputs of the motion model include historical locations of tracklet and its corresponding timestamps. Motion Encoder is utilized for encoding the bounding boxes’ information (position and shape). The ME (Motion Encoder) model projects the 4 dimensional vector into a m-dimensional vector by LSTM for the assigned nodes, 𝐹𝑙𝑀(∗), and by fully-connected layers for the unassigned nodes, 𝐹𝑓𝑀(∗). LSTM is able to learn sequential data. 𝑓𝑖𝑚,𝑡 denotes the motion feature for node 𝑣𝑖𝑡, 𝐹𝑀(∗)={𝐹𝑙𝑀(∗),𝐹𝑓𝑀(∗)} is the model forward function of motion model:

𝑓𝑖𝑚,𝜉={𝐹𝑙𝑀([𝐵𝑖(𝜉−𝐿𝑖),...,𝐵𝑖(𝜉−1),𝐵𝑖𝜉]), 𝑣𝑖𝜉∈𝐴𝑠𝑠𝑖𝑔𝑛𝑒𝑑𝐹𝑓𝑀(𝐵𝑖𝜉),  𝑣𝑖𝜉∈𝑈𝑛𝑎𝑠𝑠𝑖𝑔𝑛𝑒𝑑
(4)
where 𝜉∈[𝑡,𝑡+𝜂] is the frame number, 𝐿𝑖 indicates the length of the i-th tracklet, 𝐵𝑖𝜉 represents the bounding box x, y position, width and height, [𝑥𝑖𝜉,𝑦𝑖𝜉,𝑤𝑖𝜉,ℎ𝑖𝜉], of 𝑣𝑖𝜉, which are normalized to [0, 1] by the original image size. We use the position loss to train the motion model in advance, which is described as:

𝑚𝑜𝑡=∑𝜉=1𝐿𝑖−1∥𝑓𝑖𝑚,𝜉−𝑓𝑖𝑚,𝜉+1∥22
(5)
The motion features 𝑓𝑖𝑚,𝜉 and 𝑓𝑖𝑚,𝜉+1 are generated by LSTM model and FC model, respectively.

Human-Interaction Model Extractor
Human-Interaction Model (HIM) is a novel model for extracting features for MOT, which combines the information of tracked-target and its neighboring targets. Compared with the previous interaction extractor, HIM not only utilizes the relative position information between tracked node and its surrounding nodes, but also fuses appearance features of the targets together. HIM describes where the persons are located around the tracked-target and additionally describes what the persons look like. Interaction feature has more effectiveness for the target who is occluded frequently or heavily in the highly-crowded scenery.

Fig. 6
figure 6
The Illustration of Building Relationships and Interaction Extraction

Full size image
To build relationships between the target and the other objects, we set up a connecting rule to filter the nodes which are unsuitable to extract features for tracked-target (as shown in in Fig. 6). The set of node candidates are defined as 𝑢𝑗𝜉∈𝑈. The connecting rule includes two constraints, distance constraint and direction constraint. The candidate node 𝑢𝑗𝜉 which is over the maximum distance limit or against the direction with tracked-target 𝑣𝑖𝜉 cannot be connected in relationship. The distance constraint is described as:

{1,  ∥𝑃𝑖𝜉−𝑃𝑗𝜉∥22≤𝜚𝑑𝑖𝑠0,  ∥𝑃𝑖𝜉−𝑃𝑗𝜉∥22>𝜚𝑑𝑖𝑠
(6)
where 𝑃𝑖𝜉, 𝑃𝑗𝜉 are center positions of the bounding boxes 𝐵𝑖𝜉, 𝐵𝑗𝜉, respectively. 𝜚𝑑𝑖𝑠 is the maximum distance limit parameter. The direction constraint is defined as:

{1,  𝑐𝑜𝑠𝑖𝑛𝑒(𝑣𝑖𝜉[𝑣𝑣𝜉],𝑢𝑗𝜉[𝑣𝑣𝜉],)≥0 ∨ 𝑢𝑗𝜉[𝑣𝑣𝜉]=00,  𝑐𝑜𝑠𝑖𝑛𝑒(𝑣𝑖𝜉[𝑣𝑣𝜉],𝑢𝑗𝜉[𝑣𝑣𝜉],)<0
(7)
where 𝑣𝑖𝜉[𝑣𝑣𝜉],𝑢𝑗𝜉[𝑣𝑣𝜉] are the corresponding velocities of the target and the candidate at [𝑡,𝑡+𝜂]-th frame. The kept candidates are connected to the target node as the graph structure 𝜓𝑖𝜉∈ℝ(𝑝+1)∗(𝑝+1), where p is the number of reserved candidates. The candidates’ features are regarded as node characteristics 𝑋𝑖𝜓,𝜉∈ℝ(𝑝+1)∗(𝑛+𝑚), which are composed of the corresponding nodes’ appearance feature 𝑓𝑖𝑎,𝜓 from AE and the relative position 𝑓𝑖𝛥,𝜓=𝐹𝐶([[𝛥𝑡𝑎𝑟,1𝑥,𝛥𝑡𝑎𝑟,1𝑦];...[𝛥𝑡𝑎𝑟,(𝑝+1)𝑥,𝛥𝑡𝑎𝑟,(𝑝+1)𝑦]]), where 𝛥𝑡𝑎𝑟,∗𝑥 and 𝛥𝑡𝑎𝑟,∗𝑦 indicate the distance on x, y axis between the target and the corresponding neighboring node, 𝑓𝑖𝛥,𝜓∈ℝ(𝑝+1)∗𝑚 is the output of FC. The HIM is a Graph Neural Network structure, 𝐹𝜓(∗) represents the model forward function of the interaction model, which is described as:

Fig. 7
figure 7
Illustration of the Graph Construction. The whole sequence is divided into several sub-sequence, and the strategy constructs the graph by bounding boxes overlap and time interval in sub-sequence

Full size image
𝑓𝑖𝑐𝑙𝑠_𝜓,𝜉,𝑓𝑖𝜓,𝜉=𝐹𝜓(𝐴𝑖𝜓,𝜉,𝑋𝑖𝜓,𝜉)
(8)
where 𝐴𝑖𝜓,𝜉∈ℝ(𝑝+1)∗(𝑝+1) is the adjacent matrix, which represents the connection between node i and its surrounding nodes at frame 𝜉. p is the number of nodes around node i. 𝑋𝑖𝜓,𝜉∈ℝ(𝑝+1)∗(𝑛+𝑚) is the feature matrix of these nodes. 𝑓𝑖𝑐𝑙𝑠_𝜓,𝜉 and 𝑓𝑖𝜓,𝜉 are the interaction feature and classification feature of node i, respectively. To process graph data using convolution operation, we transform the adjacent matrix to the frequency domain by Laplace Transform (Kipf and Welling 2016). In this way, an adjacent matrix is transformed to a Laplace Matrix. We adopt the Symmetric Normalized Laplacian Matrix L in implementation, which is defined as:

𝐿=𝛤−12𝐴𝛤−12
(9)
where 𝛤 is Degree Matrix, which is the diagonal matrix where each element 𝛤[𝑖,𝑖] is the degree of the vertex i (number of edges attached to the vertex i). A (Adjacent Matrix) is a square matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph. For Laplacian Transform, convolution in time domain equals point-wise multiplication in frequency domain. The graph convolution simplifies to:

𝑔𝜃⋆𝑥≈𝜃(𝛹𝑁+𝛤−12𝐴𝛤−12)𝑥
(10)
where ⋆ indicates convolution, and 𝑔𝜃 is a filter parameterized by 𝜃 in the Fourier domain. x is the feature matrix, 𝜃 is a learnable parameter, and 𝛹𝑁 is the identity matrix. Note that 𝛹𝑁+𝛤−12𝐴𝛤−12 has eigenvalues in the range [0, 2]. Repeated application of Eq.(10) leads to numerical instabilities and exploding/vanishing gradients when used in a deep neural network model. To alleviate this problem, we introduce the following re-normalization trick: 𝛹𝑁+𝛤−12𝐴𝛤−12→𝛤̃ −12𝐴̃ 𝛤̃ −12, with 𝐴̃ =𝐴+𝛹𝑁, 𝛤̃ [𝑖,𝑖]=∑𝑗𝐴̃ [𝑖,𝑗], the Eq.(10) is approximated as follows:

𝑋̂ =𝛤̃ −12𝐴̃ 𝛤̃ −12𝑋𝛩
(11)
where 𝑋∈ℝ𝑁∗𝐶 is feature matrix, N is the number of nodes, C is input channel (C-dimensional feature vector). 𝛩∈ℝ𝑁∗𝐹 is a learnable parameter of GCN. 𝑋̂ ∈ℝ𝑁∗𝐹 is the convolved feature matrix. The Eq.(8) is expended as:

𝑋̂ 𝑖𝜓,𝜉=𝑅𝑒𝐿𝑈(𝛤̃ −12𝐴̃ 𝑖𝜓,𝜉,𝛤̃ −12𝑋𝑖𝜓,𝜉𝛩)
(12)
𝐴̃ =𝐴+𝛹𝑁, 𝛤̃ [𝑖,𝑖]=∑𝑗𝐴̃ [𝑖,𝑗]
(13)
𝛹𝑁 is the identity self-connections matrix, and the 𝐴̃  is the combination of adjacency matrix and self-connections. 𝛤̃  indicates a degree matrix of graph G, and 𝛩∈ℝ(𝑝+1)∗(𝑛+𝑚) is a learnable parameters on GNN, and feature matrix 𝑋̂ 𝑖𝜓,𝜉∈ℝ(𝑝+1)∗𝑙 indicates output the GNN. And then, 𝑋̂ 𝑖𝜓,𝜉 are global Max Pooling on node-level:

𝑓𝑖𝜓,𝜉=𝑀𝑎𝑥𝑃𝑜𝑜𝑙𝑖𝑛𝑔(𝑋𝑖𝜓,𝜉)
(14)
𝑓𝜓,𝜉 is a l-dimensional vector, which represents the interaction feature. We also adopt the classification loss 𝑖𝑛𝑡 for training HIM, which is defined as:

𝑖𝑛𝑡(𝑓𝑖𝑐𝑙𝑠_𝜓,𝜉,𝑝𝑖𝜓,𝜉)=∑𝑘=1𝐾−𝑝𝑖𝜓,𝜉[𝑘]⋅log(𝑝̂ 𝑖𝜓,𝜉[𝑘])𝑝̂ 𝑖𝜓,𝜉=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑓𝑖𝑐𝑙𝑠_𝜓,𝜉)
(15)
where the value of 𝑝𝑖𝜓,𝜉 represents ground truth label for the i-th target.

The features generated by AE, ME and HIM are concatenated together, which is defined as:

𝑓𝑖𝜉=𝐶𝑜𝑛𝑐𝑎𝑡𝑒𝑛𝑎𝑡𝑒[𝑓𝑖𝑎,𝜉,𝑓𝑖𝑚,𝜉,𝑓𝑖𝜓,𝜉]
(16)
where 𝑓𝑖𝜉 is the feature of node 𝑣𝑖𝜉, and then we concatenate all of the nodes within [𝑡,𝑡+𝜂] as:

𝑋𝑡=𝐶𝑜𝑛𝑐𝑎𝑡𝑒𝑛𝑎𝑡𝑒[𝑓1𝑡;...;𝑓𝑖𝑡;𝑓1𝑡+1;...;𝑓𝑗𝑡+𝜂]
(17)
where 𝑋𝑡∈ℝ𝑁∗(𝑛+𝑚+𝑙) indicates the feature matrix, N is the number of all of nodes within [𝑡,𝑡+𝜂].

Graph Construction
Before feeding into GNN, we firstly divide the video sequence as several sub-sequences, and pre-process the bounding boxes for each sub-sequence. The bounding boxes are constructed and normalized as graph-structured data. The graph structure consists of nodes and edges represented as 𝐺=(,) (mentioned in Sec.3.1), where the nodes 𝑣∈ represent the bounding boxes, and the edges 𝑒∈ denote the spatial-temporal relationship between nodes. The graph construction is described in Fig. 7. We divide the whole video sequence into several video fragments (sub-sequence). 𝜂 is the length of video fragments, for instance in order to generate the graph 𝐺𝑡, we firstly extract the t-th frame to the (𝑡+𝜂)-th frame images and select nodes from sub-sequence in terms of the node state. The state of the nodes can be divide into “Tracked”, “Lost”, “Unassigned”, and “Quitted” (described in Sec.3.2).Only ‘lost’ and ‘unassigned’ nodes will be selected for graph construction. We calculate the node’s bounding boxes IoU (Intersection over Union) between adjacent frames, the edge weight is proportional to IoU value and inversely proportional to the frame interval. The edge weight 𝑒𝜀𝑖,𝜉𝑗 of graph 𝐺𝑡 between 𝐵𝑖𝜀 and 𝐵𝑗𝜉 is defined as:

𝑒𝜀𝑖,𝜉𝑗=𝐼𝑜𝑈(𝐵𝑖𝜀,𝐵𝑗𝜉)∗(1−𝜇∗𝑚𝑖𝑛(𝑎𝑏𝑠(𝜀−𝜉),𝜆))𝑠.𝑡.  𝜀,𝜉∈[𝑡,𝑡+𝜂],  𝜀<𝜉,  𝜇,𝜆>0
(18)
where

𝐼𝑜𝑈(𝐵𝑖𝜀,𝐵𝑗𝜉)=𝑎𝑟𝑒𝑎(𝐵𝑖𝜀∩𝐵𝑗𝜉)𝑎𝑟𝑒𝑎(𝐵𝑖𝜀∪𝐵𝑗𝜉)
(19)
where constant 𝜇 is used for adjusting the influence of frame interval on edge weight, and constant 𝜆 is the upper limit of frame interval. The representation of graph 𝐺𝑡 includes adjacent matrix 𝐴𝑡∈ℝ𝑁∗𝑁,𝐴𝑡[𝑖,𝑗]=𝑒𝑖,𝑗 and features matrix 𝑋𝑡∈ℝ𝑁∗(𝑛+𝑚+𝑙),𝑋𝑡[𝑖]=[𝑓𝑖𝑎,𝑓𝑖𝑚,𝑓𝑖𝜓], which are introduced in Eq.(16) and Eq.(17).

Graph Optimization by GNN
Graph Neural Network (GNN) aims to learn the topological data pattern to represent the graph structure feature, which encodes the node features and updates the representation vector in the graph. Better than CNN and RNN, GNN has more significant effects on the graph structure based task, such as molecule classification and particle interaction simulation.

The target of multiple object tracking task is to locate every pedestrian’s position at each moment. So we associate the node ID and connect the nodes which belong to the same person as the tracking result. The MOT method address this problem by Data Association, which involves network flow, graph-cut and feature clustering, so GNN is able to optimize the graph node feature and edge weights between nodes. we adopt the Graph Convolutional Network (GCN) (Kipf and Welling 2016) as the network backbone. The adjacent matrix 𝐴𝑡 and feature matrix 𝑋𝑡 are denoted as the GCN input, and the GCN outputs include updated 𝐴𝑡^ and 𝑋𝑡^. 𝐹𝐺(∗) indicates the model forward function of GCN:

𝐴̂ ,𝑋̂ =𝐹𝐺(𝐴,𝑋)
(20)
where forward function 𝐹𝐺 is similar to Eq.(12) and Eq.(13)

𝑋̂ =𝑅𝑒𝐿𝑈(𝛤̃ −12𝐴̃ 𝛤̃ −12𝑋𝛩)
(21)
where, feature matrix 𝑋̂ ∈ℝ𝑁∗𝑞 denotes one of the GCN output, each node feature is a q-dimension vector. The updated adjacency matrix 𝐴̂ ∈ℝ𝑁∗𝑁 is given by:

𝐴̂ =(𝑋̂ 𝑛𝑜𝑟𝑚∗𝑋̂ 𝑇𝑛𝑜𝑟𝑚)+12
(22)
where

𝑋̂ 𝑛𝑜𝑟𝑚[𝑖]=𝑋̂ [𝑖]||𝑋̂ [𝑖]||,𝑖∈[1,𝑁]
(23)
where Eq.(23) indicates row-wise normalization, 𝑋̂ [𝑖] indicates the i-th row of 𝑋̂ . Every element of 𝐴̂  is between 0 to 1. 𝐴̂ [𝑖,𝑗] indicates the association probability between the i-th node and the j-th node. The multi-layers GCN feedward function is shown as:

𝐴1^,𝑋1^=𝐹𝐺1(𝐴,𝑋)
(24)
𝐴𝜁^,𝑋𝜁^=𝐹𝐺𝜁−1(𝐴𝜁−1^,𝑋𝜁−1^), 𝜁>1
(25)
Finally, to train the DAN, we design a Graph Loss 𝐺(∗), which is defined as:

Fig. 8
figure 8
The explanation of DAN training and Loss Function

Full size image
𝐺(𝐴𝜁^,𝐺𝑔𝑡𝑡)=∑𝑒𝑔𝑡𝑖𝑗∈𝑔𝑡𝑡(𝑒𝑔𝑡𝑖𝑗−𝑒𝑖𝑗)+∑𝑒𝑔𝑡𝑖𝑗∉𝑔𝑡𝑡𝜎∗(𝑒𝑖𝑗−𝑒𝑔𝑡𝑖𝑗)
(26)
where 𝐺𝑔𝑡𝑡 is the ground truth graph structure which is computed previously, 𝐺𝑔𝑡𝑡=(𝑔𝑡𝑡,𝑔𝑡𝑡), 𝑒𝑔𝑡𝑖𝑗∈𝑔𝑡𝑡, 𝑒𝑔𝑡𝑖𝑗={0,1} , and 𝜎 is the loss weight of Graph Loss. The total loss  of the training DAN includes cross-entropy loss 𝑎𝑝𝑝, 𝑖𝑛𝑡 for AE and HIM, position loss 𝑚𝑜𝑡 for ME, and graph loss 𝐺 for GCN (described in Fig. 8).

Experiments
MOT Datasets
Our method is evaluated on public benchmark (MOT Challenge (Milan et al. 2016)), which includes MOT15 (Leal-Taix et al. 2015), MOT16 (Milan et al. 2016), Duke-MTMCTT (Ristani et al. 2016) and MOT20 (Dendorfer et al. 2019). All datasets contain large-scale video sequences from different cameras and scenes. The training sets provide ground truth bounding boxes and IDs by annotator, and testing sets only give the detection results by detector.

MOT15 includes 22 sequences which are divided into one half for training and the other half for testing. The testing data contains over 10 minutes of footage and 61440 annotated bounding boxes, the videos are from moving cameras or static cameras, respectively. MOT15 additionly provides the detection results by DPM (Felzenszwalb et al. 2010) as the tracker inputs.

MOT16 is a classical evaluation dataset comparing several tracking methods on MOT Challenge, which includes 14 sequences captured from surveillance, hand-held shooting and driving recorder by static cameras and moving cameras. The length of each video is about 500-1500 frames. And the dataset also provides the detections by DPM (Felzenszwalb et al. 2010).

DukeMTMCT is a large scale dataset for multiple-camera multiple-object tracking, which are captured by 8 surveillance cameras at different viewing angles include 2800 identities (person) in Duke University . The video duration of each camera is 86 minutes, which is split into training set (0-50 min) and testing set (50-86 min). In addition, the dataset provided DPM (Felzenszwalb et al. 2010) and Openpose (Cao et al. 2018) detection results for each frame as the tracker input.

MOT20 has been carefully selected to challenge trackers and detectors on extremely crowded scenes. In contrast to previous challenges, the total dataset contains 8 sequences. All sequences are filmed in high resolution from an elevated viewpoint, and the mean crowd density reaches 246 pedestrians per-frame which is 10 times larger than the previous benchmark. MOT20 utilizes a Faster R-CNN (Ren et al. 2015) with ResNet101 backbone on the MOT20 training sequences as the tracking input.

Implementation Details
In our experiments, DAN consists of feature extraction and graph optimization. For feature extraction, appearance extractor is composed of CNN, whose architecture backbone is SeResNet-50 (Hu et al. 2018). The tracked-targets’ images are resized to 256×256 from the cropped images and the outputs of CNN produces appearance feature 𝑓𝑎,𝑡, a 2048-dimensional vector to describe the image. Motion Encoder (ME) is composed of 3-layers of LSTM network, batch-normalization and ReLU. Bounding box information [x,y,w,h], a 4-dimensional vector is raised to 4→64→512 dimensional vector finally by ME. Human-Interaction Model is a GNN structure. To build the relationship, we set the maximum distance by the sum of average width and height of all the bounding boxes in the current frame. The relatively position [[𝛥𝑡𝑎𝑟,1𝑥,𝛥𝑡𝑎𝑟,1𝑦];...[𝛥𝑡𝑎𝑟,𝑝𝑥,𝛥𝑡𝑎𝑟,𝑝𝑦]] are input into 3-layers fully-connected network. The corresponding appearance feature from feature extractor and relative position are jointly fed into the GNN model. The output of GNN is a 2048-dimensional vector to express the target surrounding feature. To construct relationship rapidly, we adopt a function “radius_neighbors_graph()” from sklearn library to implement graph construction for each target. The function rapidly connects the nodes whose distance is less than radius as an adjacent matrix. We only generate two adjacent matrices for each frame, one representing the adjacent matrix 𝐴𝑑𝑖𝑠∈ℝ𝑁∗𝑁 with distance constraint, the other denoting the adjacent matrix 𝐴𝑜𝑟𝑖∈ℝ𝑁∗𝑁 with orientation constraint, where 𝐴𝑑𝑖𝑠/𝑜𝑟[𝑖,𝑗]∈0,1, and then Inter-connection Matrix 𝐴𝜓 is calculated by 𝐴𝑑𝑖𝑠 & 𝐴𝑜𝑟𝑖. The training set graph size is restricted to 64-512 nodes per graph, the length of sub-sequence 𝜂 is 30 frames, and the stride of sequence 𝛿 is 20. For each step of epoch, we replace data batch size with graph size to feed CNN model. For graph construction, the frame interval impact factor 𝜇 is 0.08 and the upper limit constant 𝜆 is 10. The input of GCN is a N × 2560-dimensional vector, where N is the number of nodes on graph, and we adopt two-layers GCN and the graph loss weight 𝜎 is 2. The training optimizer is the AdamOptimizer (Kingma et al. 2014). We load the pre-trained weight on CNN, the learning rate is less than the others models. The initial learning rate for CNN is set to 1e-5 and learning rates for the others are set to 0.001. The model converges finally at the 150th epoch.

Ablation Study
Table 1 shows the tracking performance with different components and network on validation datasets. The table is divided into three groups, the first group compares results between without and with Human-Interaction Model. The interaction feature is able to associate the same target before and after occlusion in the crowd, as a result, it can drastically reduce Id-switch (IDSw.) from 1286 to 874 , Fragments (Frag.) from 2181 to 1862 and improve MOTA by 4.1% and IDF1 by 4.2%.

The second group demonstrates the difference between Hungarian Algorithm and Graph Network Association. MOTA, IDF1 are increased by 1.5%, 0.7%, respectively, when hand-crafted optimization (H) is replaced by GNN (𝐺256). On this basis, adding interaction model can further enhance the tracking performance.

The third group shows the results on graph structures with different number of nodes, where 𝐺256 indicates that the graph structure includes no more than 256 nodes. We firstly replace (H) by 𝐺64, however the result is not satisfactory, because the fewer nodes we get, the shorter sub-sequence 𝜂 is. GNN is difficult to associate the occluded target with un-occluded target in short time. Increasing the number of nodes can enlarge the length of sub-sequence, the same targets before and after occlusion can be associated. Therefore, MOTA and IDF1 are improved gradually until the number of nodes equals 256. The same target’s appearances change (illumination, angle, scale, etc.) in long-term, too many nodes in graph leads the network to be unable to distinguish the feature of the same and different targets well. As a consequence, 𝐺512 decreases the performance slightly. Finally, we adopt 𝐺256 as the graph optimization part.

Table 1 Tracking results on validation dataset (MOT15-PETS09-S2L1, MOT16-04, MOT20-05) with different components. (A): Appearance Extractor; (M): Motion Encoder; (H): Hungarian Algorithm; (I): Human-Interaction Model; (G): Graph Network Association
Full size table
figure a
Fig. 9
figure 9
The performance of Inter-relationship Building. a: Video frames and tracked-targets at different times. b: Corresponding Inter-relationship structure of targets

Full size image
Fig. 10
figure 10
a: Cosine similarity between appearance features of targets 𝐼1→𝐼7 from Fig. 9.a; b: Cosine similarity between interaction features of targets 𝜓1→𝜓7 from Fig. 9.b

Full size image
Table 2 Results on the MOT16 test dataset
Full size table
Table 3 Results on the DukeMTMCT test dataset
Full size table
Table 4 Results on the MOT15&16 on static camera test set
Full size table
DAN training Details
To train the DAN, we firstly divide training sequences into many short sub-sequence, and the length of each sub-sequences is about 20-30 frames. If every frame includes 15-20 pedestrians, each sub-sequence has 64-512 nodes and we construct them as a graph. We set the maximum number of nodes at graph construction phase, 𝐺256 indicates that the number of nodes is no more than 256, otherwise the rest of the nodes are moved to the next sub-sequences. For the whole sequence, we can get about 2k-10k graphs, and the number of graph depends on sub-sequence length and sample stride length. And we shuffle these graphs for each epoch, and our experiment implements on the Pytorch framework by 4 Nvidia Titan X GPUs, and device 0,1,2 are used for loading the CNN model to extract appearance features, and then the features transfers to device 3 to calculate motion encoder and GNN model. The processing of network forward and backward are shown as Algorithm 1. Shuffling graph is only used in training phase. However, in inference phase, the model processes the sequence stage-by-stage according to the order of sequence as shown in Fig. 8. For each epoch, we need to pass all of training set once (We used 12 sequences for training, each sequence has 450-1200 frames). However, we set the length of sub-seqence = 30 and stride = 20, so actually we have o pass 1.5 times length of sequence for each epoch. It takes about 80-90 minutes (about 4500s). It takes about 7-8 days to finish 150 epochs.

Inter-relationship Building Results
Human Interaction Model is utilized for extracting interaction features between target and neighbors. The inter-relationship buildup is illustrated in Fig. 9, where Fig. 9a shows the video frames at different times. The cropped images 𝐼1→5 on the left of each frame indicates the tracked-target. Due to the frequently occlusion and illumination variation, the target’s appearance and bounding box shape change quite much, furthermore, the most prominent feature (a red knapsack) of target is erased in some frames, which causes the appearance features for the same target at different frames to become indistinguishable. Fig. 9b shows the results of inter-relationship structure 𝜓1→5 for 𝐼1→5. Besides, the figure additionally shows the target’s neighbor 𝐼6→7 and relationship 𝜓6→7 to contrast the difference of feature with the target, where one of the neighbor has a similar red knapsack as the target, another neighbor has the different appearance from the target.

Appearance and Interaction Comparison
Fig. 10 demonstrates the appearance and interaction cosine similarity between the same target at different frames and different target at the same frames. For appearance similarity estimation, we calculate cosine similarity as a 7×7 matrix 𝑆𝐼 between targets 𝐼1→7, where 𝑆𝐼[𝑖,𝑗] indicates the similarity between 𝐼𝑖 and 𝐼𝑗 (as shown in Fig. 10a). In addition, we also calculate cosine similarity as 𝑆𝜓 between groups 𝜓1→5 (as shown in Fig. 10b). We firstly compare appearance feature similarity of the same target 𝐼1→5, where the value ranges between [0.556, 0.726]. However, due to the highly similar appearance between the target 𝐼1→5 and the first neighbor 𝐼6, it’s difficult to distinguish them by similarity value ([0.528, 0.659]). On the contrary, the interaction feature reserves some discriminative characteristics, the similarity value ranges between targets 𝜓1→5 is limited to [0.816,0.926], meanwhile the value between targets 𝜓1→5 and the first neighbor 𝜓6 is decreased to [0.324, 0.471]. The similarity is already distinguishable in appearance feature (such as targets 𝐼1→5 and the second neighbor 𝐼7), their interaction features 𝜓1→5 and 𝜓7 also remain discriminative.

MOT Evaluation Metrics
The MOT Challenge Benchmark adopts the standard CLEAR-MOT mapping (Bernardin and Stiefelhagen 2008) and ID measures (Ristani et al. 2016) for evaluating MOT performance. The main metrics for MOT are MOTA and IDF1. MOTA (Multiple Object Tracking Accuracy) measures the effect of tracking for each tracklet, which depends on True Positives (TP) ,False Positives (FP), False Negatives (FN) and Id Switches (IDSw), 𝑀𝑂𝑇𝐴=1−𝐹𝑃+𝐹𝑁+𝐼𝐷𝑆𝑇𝑃. Total number of Fragment (Frag), Mostly tracked targets (MT), Mostly lost targets (ML) are used for evaluating tracklet integrity as the reference indexes. The IDF1 (ID F1 Score) is the ratio of correctly identified detection over the average number of true and computed detections, which depends on ID True Positives (IDTP), ID False Positives (IDFP) and ID False Negatives (IDFN), 𝐼𝐷𝐹1=2∗𝐼𝐷𝑇𝑃2∗𝐼𝐷𝑇𝑃+𝐼𝐷𝐹𝑁+𝐼𝐷𝐹𝑃. IDP (ID Precision) indicates fraction of computed detections that are correctly identified, 𝐼𝐷𝑃=𝐼𝐷𝑇𝑃𝐼𝐷𝑇𝑃+𝐼𝐷𝐹𝑃. IDR (ID Recall) means fraction of ground-truth detections that are correctly identified, 𝐼𝐷𝑅=𝐼𝐷𝑇𝑃𝐼𝐷𝑇𝑃+𝐼𝐷𝐹𝑁.

Tracker Results Comparison
Our method is evaluated on the MOT Challenge testing set. We compare the performance with the state-of-the-art methods on the benchmark. The comparison of methods on MOT16 and DukeMTMCT are shown in Table 2 and Table 3. The most crucial evaluating parameters include MOTA and IDF1. Compared with the previous methods, our proposal aims to extract the more efficient features to avoid the ”mis-tracking” and ”lost-tracking”. Consequently, Table 2 shows that DHIAN (ours) achieves the best performance on MOTA, MT. The others parameters (such as IDF1, ML, etc.) reaches relatively high score for MOT16 dataset. As shown in Table 3, our method exceeds the previous highest MOTA score by 3.3%, and DHIAN also achieves the excellent score on MT, ML, FN. The video from Duke-MTMCT dataset is captured from surveillance, but MOT16 blends moving videos and static videos. Since moving camera affects trajectory prediction and relationship construction, our methods has better performance on Duke-MTMCT than MOT16. Therefore, we specially compare performance on static video of MOT15 and MOT16 (as shown in Table 4). Our results ranks the 1st or 2nd place with MOTA and IDF1 among the start-of-the-art methods. Fig. 11 illustrates the DHIAN visualization results on MOT15 (the 1st-3rd rows), MOT16 (the 4th-6th rows), Duke-MTMCT (the 7th row) and MOT20 (the 8th row).

Fig. 11
figure 11
The visualization results on MOT15, MOT16, Duke-MTMCT and MOT20

Full size image
Conclusion and Future Work
In this paper, we concentrate on Multiple Object Tracking (MOT) in the wild. Frequent occlusion and rapid illumination variation influence the tracked- targets’ feature description. In addition, existing methods use the hand-crafted method to optimize graph structure, which causes that feature extraction module and graph optimization module cannot be combined as a whole end-to-end network. Therefore, we specially consider the interrelation cue between objects and design Human-Interaction Model (HIM) to extract the details of target and its surrounding. Meanwhile we propose an efficient end-to-end model, Deep Association Network (DAN), to optimize the association with graph-based learning mechanism. Our proposal is evaluated on 4 public datasets (MOT15, MOT16, Duke-MTMCT and MOT20). The algorithm achieves MOTA up to 50.0, 86.7 and IDF1 up to 54.3, 82.0 on MOT16 and DukeMTMCT, respectively. The visualization results are shown in Fig. 11. Our method has better performance in complex crowd scene and static cameras.

In the future, we intend to optimize our model and improve processing efficiency. And we will continue to explore the novel model structure and relationship building strategy to extract interaction feature more effectively. To solve the frequent occlusion problem, we intend to combine the head-detector and body-detector as collaborative tracking of pedestrians. And we will design a newer motion encoder to replace the current RNN structure for trajectory prediction. Lastly, we attempt to utilize transfer learning to improve the model robustness.