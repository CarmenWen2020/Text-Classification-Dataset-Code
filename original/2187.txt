Multiple Object Tracking (MOT) in the wild has a wide range of applications in surveillance retrieval and autonomous driving. Tracking-by-Detection has become a mainstream solution in MOT, which is composed of feature extraction and data association. Most of the existing methods focus on extracting targetsâ€™ individual features and optimizing the association by hand-crafted algorithms. In this paper, we specially consider the interrelation cue between targets and we propose Human-Interaction Model (HIM) to extract interaction features between the tracked target and its surrounding. The interaction model has more discriminative features to distinguish objects, especially in crowded (dense) scene. Meanwhile we propose an efficient end-to-end model, Deep Association Network (DAN), to optimize the association with graph-based learning mechanism. Both HIM and DAN are constructed by three kinds of deep networks, which include Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Graph Neural Network (GNN). The CNNs extract appearance features from bounding box images, the RNNs encoder motion features from historical positions of trajectory. And then the GNNs aim to extract interaction features and optimize graph structure to associate the objects in different frames. In addition, we present a novel end-to-end training strategy for Deep Association Network and Human-Interaction Model. Our experimental results demonstrate performance of our method reaches the state-of-the-art on MOT15, MOT16 and DukeMTMCT datasets.
Introduction
Multiple Object Tracking (MOT) is one of the most significant components in computer vision technology, which has been widely applied to video surveillance retrieval, scene understanding and autonomous driving. MOT in the wild aims to track the targets in crowded scene. A tracker identifies targets according to their features and associates the same target within different frames as contiguous trajectory. The trajectory is commonly utilized for human behavior analysis, action recognition and human feature supplement. However, MOT is still a challenging task due to unfavorable factors such as occlusion, scene complexity and indistinguishable objects. Existing methods merely consider individual features (e.g. appearance and motion) rather than interrelation cue between objects. Some complicated scenes (as shown in Fig. 1) are therefore difficult to be processed by existing methods. In addition, these methods are composed of multiple independent algorithms, which are not combined together as an integral deep network architecture. In this paper, we specially focus on inter-relation cue between objects to extract interaction features between tracked-target and its surrounding. The interaction model has more discriminative features to distinguish objects, especially in crowded scene. Meanwhile, we additionally design a novel end-to-end training method to optimize the graph structure instead of hand-crafted method.

Fig. 1
figure 1
The most popular datasets on MOT in the wild currently, a:MOT15 (Leal-Taix et al. 2015) b:MOT16/MOT17 (Milan et al. 2016) c:Duke-MTMCT (Ristani et al. 2016) d:MOT20 (Dendorfer et al. 2019) e:JRDB (MartÃ­n-MartÃ­n et al. 2019)

Full size image
Tracking-by-detection (TBD) has been widely used for the MOT task in recent years, which is usually based on the bounding boxes detected by leveraging off-the-shelf object detectors such as Cascade RCNN (Cai and Vasconcelos 2018), Faster RCNN (Ren et al. 2015) and Yolov3 (Redmon and Farhadi 2018). TBD associates the bounding boxes according to their temporal-spatial information in different frames. Specifically, under the assumption that the same individual from different frames has similar characteristics. TBD calculates similarity between bounding boxes according to the extracted features (e.g. appearance, motion, shape). And the tracker associate the bounding boxes which have high-similarity. The bounding boxes are linked together to form a trajectory. Therefore, traditional methods generally are divided into two modules: feature extraction and bounding box association.

Feature extraction aims to comprehensively describe an object using discriminative features, which include colors, textures, positions, boundaries, velocity, structural feature, etc. Recently, deep neural networks such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) have been widely used for the MOT task. However, most existing methods learn features from single individual but they neglect the relationship between the tracked-target and its surroundings information, which causes â€lost-trackingâ€ or â€mis-trackingâ€ for target in the crowd scenario. Figure 2a shows an example of complex scene at the 146-th, 222-rd and 301-st frame of sequence, respectively. The red bounding box on the left of each picture indicates a tracked person, who are occluded frequently by the other person and even sometime are occluded completely. Traditional methods are not able to track the target effectively. Fortunately, the persons around the target relatively reserve some information such as their appearance and the relative-position with target (as shown in Fig. 2b). Therefore, human-interaction detail becomes a powerful and effective feature for tracking, especially in complex scene.

Fig. 2
figure 2
Tracking person in crowd by deep interaction feature. a: Target and video sequence b: Corresponding interaction feature for each frame

Full size image
Bounding boxes association is usually formulated as a Graph Optimization problem, which is about associating and removing the edge between nodes on graph structure constructed by bounding boxes and their similarity. Each bounding box is regarded as a node on the graph, meanwhile the similarity of nodes represents edge weight between nodes. The optimized and pruned graph reserves several sub-graphs, and the nodes on each sub-graph are considered as the same individual, which are labeled the same ID subsequently.

Fig. 3
figure 3
Our method pipeline: 1.Extracting feature (appearance, motion and interaction) for each node from detection results; 2.Constructing the graph structure for all of nodes; 3.Optimizing the graph structure and obtaining the association result

Full size image
Currently, the graph optimization algorithm on MOT hasnâ€™t relied on deep learning strategy, and existing approaches continue to utilize the traditional solution such as Hungarian Algorithm (Sahbani and Adiprawita 2017) and Network Flow Algorithm (Schulter et al. 2017). MOT is a high-level semantic task, compared with low-level semantic or image processing task such as image deblurring, defogging, deraining and even semantic segmentation, MOT is more difficult to construct an end-to-end network by CNN. So far feature extraction and graph optimization are still treated as two independent tasks, they still havenâ€™t been combined as an end-to-end model for training together.

In this paper, we propose an end-to-end Deep Association Network (DAN) (as illustrated in Fig. 3), which can jointly learn and process the feature extraction and data association together. The DAN is divided into two parts: 1. Feature Extraction; 2. Graph Optimization. In the feature extraction part, besides the two individual feature extractors (appearance and motion), we additionally propose a novel inter-relation feature extractor, Human-Interaction Model (HIM), whose interaction feature is extracted by Graph Neural Network (GNN). In the Graph Optimization part, we also utilize GNN to replace hand-crafted algorithm to optimize the graph. GNN has the ability to learn and process the topological data from a large amount of data. The advantage of GNN is that it can input arbitrary graph structures. We utilize specific loss function and large-scale tracking training data to train GNN, thus the interactive information can be extracted by GNN, ultimately nodes which belong to the same individual tend to be together. We connect the two parts sequentially for end-to-end training. Our contributions of the framework is as follows:

End-to-end MOT model framework: We firstly combine the feature extraction and graph optimization from two independent tasks to form an unified task as an end-to-end model. The framework is named as Deep Association Network (DAN).

More discriminative deep interaction feature: we propose a Human-Interaction Model (HIM) to extract the interrelation details of tracked-target and its surrounding, which is more effective for the targets with frequent occlusion in the crowded scene.

Special Training Strategy: We specially create graph-structured dataset for training DAN. In order to make it converge better, we set different learning rates for training different layers and train the multiple models stage-by-stage.

Developable MOT baseline: Deep Association Network is an unprecedented model structure for multiple object tracking, therefore DAN is worth continuing to be explored and be researched on how to improve the performance of MOT.

Our proposal greatly enhances the effectiveness of tracking in the high-density crowds and complex scenes. Our experiments demonstrate that our method achieves superior effectiveness and robustness over state-of-the-arts.

Related Work
Multiple Object Tracking has attracted researchersâ€™ attention recently. The performance of MOT improves gradually at the MOT benchmark (Milan et al. 2016). Among the methods of MOT, (Henschel et al. 2018; Tang et al. 2017; Xiang et al. 2015; Choi 2015; Kim et al. 2015; Chen et al. 2017) designed an ingenious data association or multiple hypothesis. Schulter et al. (2017) is the first to combine feature extraction and hand-crafted graph structure to learn together. Bergmann et al. (2019) accomplishes tracking without specifically targeting any of these tasks. Keuper et al. (2018) combines point trajectories and clustering of bounding boxes to track objects. Chen et al. (2019) addresses the association method at the tracklet-level. Levinkov et al. (2017); Maksai et al. (2017); Ma et al. (2019) presented network flow and graph optimization which are powerful approaches. (Shen et al. 2018) aims at gluing feature learning and data association into a unity by a bi-level optimization formulation.

The appearance model aims to extract human features from an image. Tang et al. (2017); Sadeghian et al. (2017); Yang et al. (2019) train the CNN on the basis of person re-identification (Yang et al. 2020, 2019, 2020) to extract the image features, and Son et al. (2017) utilizes the quadruplet loss to enhance the feature expression. Chu et al. (2017) builds the CNN model to generate visibility maps to solve the occlusion problem. And , Henschel et al. (2018) uses a novel multi-object tracking formulation to incorporate several detectors into an integrated tracking system. Kim et al. (2015) extends the multiple hypothesis by enhancing the detection model. (Ma et al. 2018) address a sophisticated model to process trajectories. Zhu et al. (2018); Gao et al. (2018) propose spatial and temporal attention mechanisms to enhance the performance of MOT. Wang et al. (2019) combines temporal and appearance information together as a unified framework.

Fig. 4
figure 4
The Framework of Deep Association Network. The top of figure describes tracking progress, the bottom of figure illustrates the DAN structure, which is a sequential processing, the figure only shows initial stage and the first two stages

Full size image
The motion model defines the rule of object movement, which is utilized for prediction of trajectory position in the future by their historical positions. Motion model generally is divided into linear position prediction (Son et al. 2017) and non-linear position prediction (Dicle et al. 2013). Hong Yoon et al. (2016) designs the structural constraint by the location of people to optimize assignment. Following the success of RNN models for sequence prediction tasks, (Alahi et al. 2016) proposes social-LSTM to predict the position of each person in a scene.

The interaction model extracts interaction features which describe the inter-relation information between the tracked-target and its neighboring targets. The interaction features not only express the relative position information between targets, it additionally include the visual features of the targets. Nowadays typical interaction models such as social force models and crowd motion pattern models have been widely used on Pedestrian (Crowd) Simulation (Chen et al. 2018; Yang et al. 2020), Anomaly Detection (Zhang et al. 2020; Cai et al. 2020) and Trajectory Forecasting (Sadeghian et al. 2019; Kosaraju et al. 2019). In social force models, targets are considered as agents which determine their velocity, acceleration and position based on characteristics of other objects and the environment. Motion pattern model utilizes collective spatial-temporal structure and various modalities of motion to analyze the behavior of pedestrians. Sadeghian et al. (2019); Kosaraju et al. (2019) predict the behavior and position of targets in the future by interaction model, where (Sadeghian et al. 2019) presents Sophie model for path prediction for multiple interacting agents in a scene, and Kosaraju et al. (2019) addresses Social-BiGAT that generates realistic multi-model trajectory prediction by better modeling the social interactions of pedestrians in a scene. However, interaction models havenâ€™t been utilized adequately yet on MOT, a few methods (Sadeghian et al. 2017; Lan et al. 2018; Wang et al. 2015) only utilized the relative positions between the tracked-target and its surrounding targets to extract a simple interaction feature. Hong Yoon et al. (2016) designs a structural constraint by the location of people to optimize assignment. However, for the targets in highly-crowded scenery, only using motion information cannot distinguish the persons who stand close together, so appearance information becomes the most discriminative features. Thus, combining motion and appearance feature together can describe targetsâ€™ interaction features better.

GNN was previously applied to Natural Language Programming (NLP), physical simulation and etc. For instance, (Battaglia et al. 2018) summarizes the principle and the applications of GNN. Li et al. (2016); Kipf and Welling (2016); VeliÄkoviÄ‡ et al. (2018) respectively proposed the GNN variant structure, GGSNN (Gated Graph Sequence Neural Network), GCN (Graph Convolutional Network) and GAT (Graph Attention Network). Duvenaud et al. (2015) focuses on molecule feature descriptor, and each molecule is composed by atoms as a graph structure. Kipf et al. (2018) aims to research physical simulation by GNN, more specifically the interaction of dynamical particles system, meanwhile they realize basketball player trajectoriesâ€™ prediction. Recently, GNN has been utilized for computer vision. GNN-based few-shot transfer learning is presented by Garcia et al. (2017), and polygon refinement for instance segmentation is addressed by Acuna et al. (2018). Yan et al. (2018) adopts spatial-temporal skeleton graph for action recognition, and Shen et al. (2018) constructs relationship graph to train Re-identification model. GNN is able to extract the topological data such as molecule structure, body skeleton, etc. Interaction feature also includes the topological structure (relative position) and node attributes (appearance and motion information), therefore, GNN becomes the most suitable technique to extract interaction features according to inter-relationship and targetsâ€™ information.

Deep Association Framework
In this Section, we introduce the modules of Deep Association Network one-by-one. The framework of our network and the definition of the tracking task are described in Sec.3.1. The state transitions for nodes are introduced in Sec.3.2. The regular feature extraction is explained in Sec.3.3. The details for Human-Interaction Model are introduced in Sec.3.4. We describe the strategy of the graph construction in Sec.3.5. Lastly Sec.3.6 gives the strategy of the training of GNN.

Fig. 5
figure 5
The architecture of Feature Extraction, which is consisted of Motion Encoder, Appearance Extractor and Human-Interaction Model

Full size image
Deep Association Pipeline
Deep Association Network (DAN) is composed by three kinds of feature extractors (Appearance Extractor, Motion Encoder and Human-Interaction Model) to parallelly extract the targetsâ€™ features, and then the features are fed into Graph Neural Network (GNN) to optimize the graph structure (as described in Fig. 4). Firstly, we obtain the detection results for each frame by the detectors. The bounding boxes for each image are treated as nodes on the graph. The details of each node are extracted by Feature Extraction modules, which are divided into three parts, the architecture of Feature Extraction is illustrated in Fig. 5. Appearance Extractor (AE) is applied to extract appearance features from the cropped images according to the bounding boxes, meanwhile Motion Encoder (ME) is utilized for encoding the corresponding bounding boxesâ€™ information, which contains the position, width, height and velocity of the bounding boxes as the motion features. Human-Interaction Model (HIM) generates an interaction feature, which not only includes the relative position between the tracked-target and its neighboring targets, but also fuses the appearance features of the targets together. And then, we concatenate the three types of features together as the nodesâ€™ characteristics. After feature extraction, we construct an adjacent matrix according to the spatial-temporal relationship of the bounding boxes within the frames to connect the nodes on the graph structure. The adjacent matrix and the concatenated features are fed into GNN to optimize the graph. GNN propagates node features on the graph structure and learns the relationship between the nodes. Finally, the features of the nodes close to each other have higher similarity, which can be identified as the same person, the nodes are sequentially linked to form a complete trajectory.

We formulate the near-online tracking as a local bounding box association task between the tracked candidates and the current detection results in a video fragment. We define the set of detection results in the t-th to (ğ‘¡+ğœ‚)-th frames as îˆ°ğ‘¡ (ğ‘‘ğ‘˜ğœ‰âˆˆîˆ°ğ‘¡,ğœ‰âˆˆ[ğ‘¡,ğ‘¡+ğœ‚]), where ğ‘‘ğ‘˜ğœ‰ is the k-th detection in frame ğœ‰, and ğœ‚ is the length of the sub-sequence. îˆ¯ğ‘¡ (ğ‘ğ‘˜ğœ‰âˆˆîˆ¯ğ‘¡,ğœ‰<ğ‘¡) indicates the tracked results from initial frame to the t-th frame, where ğ‘ğ‘˜ğœ‰ is k-th tracked-node in frame ğœ‰. Bounding boxes association can be perceived as graph optimization. Therefore, we construct a global graph structure îˆ³ (ğºğ‘¡âˆˆîˆ³,ğºğ‘¡=(î‰‚ğ‘¡,îˆ±ğ‘¡)), the global graph îˆ³ consists of several local graphs {ğº1,ğº1+ğ›¿,ğº1+2ğ›¿...,ğº1+ğ‘›ğ›¿,ğ‘›ğ›¿â‰¤ğ¿}, where ğºâˆ— indicates local graph constructed from the video fragment from t-th to (ğ‘¡+ğœ‚)-th frames, ğ›¿ is the stride of video fragment on timeline. L indicates total length of the video. î‰‚ğ‘¡ (ğ‘£ğ‘˜ğœ‰âˆˆî‰‚ğ‘¡,ğœ‰âˆˆ[1,ğ‘¡+ğœ‚],î‰‚ğ‘¡=îˆ¯ğ‘¡âˆªîˆ°ğ‘¡) indicates the set of nodes in the graph, each node stands for a bounding box and ğ‘£ğ‘˜ğœ‰ denotes the k-th node in frame ğœ‰, and nodes are defined as 7 dimensions [t, id, x, y, w, h, s] which are the tracklet id by tracker, the object time, the center position (x, y), width and height of the bounding box, and the state s of the node. ğ‘’ğ‘–ğ‘—ğœ‰âˆˆîˆ±ğ‘¡ is the edge between ğ‘£ğ‘–ğœ‰1 and ğ‘£ğ‘—ğœ‰2 on ğºğ‘¡. The cost function of our method is given by

ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›â›ââœâœâˆ‘ğºğ‘¡âˆˆîˆ³ğ¹ğ‘†(ğ‘£ğ‘–ğœ‰,ğ‘£ğ‘—ğœ‰)ğ‘’ğ‘–ğ‘—ğœ‰+âˆ‘ğºğ‘¡1,ğºğ‘¡2âˆˆîˆ³ğ¹ğ‘†(ğ‘£ğ‘–ğœ€,ğ‘£ğ‘—ğœ€)ğ‘’ğ‘–ğ‘—ğœ€ââ âŸâŸğ‘ .ğ‘¡. ğºğ‘¡1âˆ©ğºğ‘¡2â‰ âˆ…
(1)
The first term in Eq.(1) measures the accuracy of the data association in single graphs ğºğ‘¡âˆˆîˆ³ according to output of model, where ğœ‰âˆˆ[ğ‘¡, ğ‘¡+ğœ‚], ğ‘£ğ‘–ğœ‰,ğ‘£ğ‘—ğœ‰âˆˆğºğ‘¡, and ğ‘’ğ‘–ğ‘—ğœ‰âˆˆ{0,1} indicate whether two nodes ğ‘£ğ‘–ğœ‰ and ğ‘£ğ‘—ğœ‰ belong to the same person, ğ¹ğ‘†(ğ‘£ğ‘–ğœ‰,ğ‘£ğ‘—ğœ‰) measures the similarity between the nodes. The second term in Eq.(1) checks whether the association results of adjacent graphs ğºğ‘¡1,ğºğ‘¡2 in overlap region are consistent, where ğœ€âˆˆ[ğ‘¡2, ğ‘¡1+ğœ‚], ğ‘¡2=ğ‘¡1+ğ›¿ indicate the frame index in the overlap region. ğ‘£ğ‘–ğœ€,ğ‘£ğ‘—ğœ€ come from the same graph ğºğ‘¡1 or ğºğ‘¡2. Since the outputs of ğºğ‘¡1 or ğºğ‘¡2 are different, and ğ‘£ğ‘–ğœ€,ğ‘£ğ‘—ğœ€ exist in both of ğºğ‘¡1 or ğºğ‘¡2, so we need to calculate the cost twice for the same two-nodes. Eq.(1) belongs to a target function, we minimize the cost value of Eq.(1) by adjusting the architecture, hyper-parameter, etc to improve the effectiveness of the model indirectly.

Node State Transitions
In Fig. 4, there are 5 kinds of colors to represent the state s of nodes (e.g. Purple: â€œUnassignedâ€, Blue and Indigo:â€œTrackedâ€, Red : â€œLostâ€, Black: â€œQuittedâ€). The blue and indigo dots are linked (associated) by the corresponding nodes in the next frame, we named these nodes as â€˜trackedâ€™, where blue dot is the first node of a tracklet. The red dots indicate the nodes at the current frame or the nodes not being associated by other nodes in their next frames, the nodes are named as â€˜lostâ€™. Only one node for each tracklet can be a â€˜lostâ€™ node, which is in the last frame of a tracklet. All of the purple dots come from the detection bounding boxes in the current frame, they are labelled as â€˜unassignedâ€™ nodes.

There are four situations that the node will be switched to other state:

â€œLostâ€â†’â€œTrackedâ€: If the â€˜lostâ€™ nodes are associated by an â€™unassignedâ€™ node in next few frames, the â€˜lostâ€™ nodes will be turned as â€˜trackedâ€™ nodes.

â€œUnassignedâ€â†’â€œLostâ€: If â€˜unassignedâ€™ nodes are associated by â€˜lostâ€™ nodes, they will be linked to tracklet as the last node and its state turns to a new â€˜lostâ€™ node.

â€œUnassignedâ€â†’â€œQuittedâ€: If the â€™unassignedâ€™ nodes arenâ€™t associated by any nodes, they will be labelled as â€™quittedâ€™.

â€œLostâ€â†’â€œQuittedâ€: We use RNN to predict the position in the future for every tracklets after each stage by their historical positions to determine whether the node leaves the scene. If the tracklet goes out of the scene, the â€˜lostâ€™ node of the tracklet will be labelled as the â€˜quittedâ€™ node. If a tracklet has not been associated for long time, in general, the difference between the last frame of the tracklet and the current frame is greater than the stride of sequence, ğ›¿, the â€˜lostâ€™ node will be labelled as â€˜quittedâ€™.

Regular Feature Extractor
Regular Feature Extraction is used to extract the classical characteristics of the individual to distinguish the differences between the nodes (bounding boxes). For the same individual in different frames, it has similar features for a period of time such as wearing, position, body size and velocity. These cues are totally summarized as two parts: appearance features and motion features. In our framework, the appearance features are extracted by several shared-weight CNNs, and the motion features are encoded by RNNs.

Appearance Extractor focuses on the pedestrian features (e.g. color, shape and texture) from each bounding box located by the detection model. we treat the appearance model as a person re-idenidentification (Re-ID) task initially to obtain the pre-trained model for CNNs of DAN. We combine the three public Re-ID datasets (Market1501 Zheng et al. 2015), DukeMTMC-ReID Zheng et al. 2017) and CUHK03 Zhong et al. 2017)) to train the homostructural CNNs model of DAN. ğ‘“ğ‘–ğ‘,ğœ‰ and ğ‘“ğ‘–ğ‘ğ‘™ğ‘ _ğ‘,ğœ‰ indicate the outputs of CNNâ€™s appearance feature and classification vector, respectively, for node ğ‘£ğ‘–ğœ‰, the ğ‘“ğ‘–ğ‘,ğœ‰ is the n-dimensional vector, and the ğ‘“ğ‘–ğ‘ğ‘™ğ‘ _ğ‘,ğœ‰ is mapped to the K-dimensional vector by a fully-connected layer from ğ‘“ğ‘–ğ‘,ğœ‰, n denotes the training set class number. ğ¹ğ‘(âˆ—) represents the model forward function of appearance model:

ğ‘“ğ‘–ğ‘ğ‘™ğ‘ ğ‘,ğœ‰,ğ‘“ğ‘–ğ‘,ğœ‰=ğ¹ğ´(ğ¼ğ‘–ğœ‰),ğœ‰âˆˆ[ğ‘¡,ğ‘¡+ğœ‚]
(2)
where ğ¼ğ‘–ğœ‰ indicates the croped image of the node ğ‘£ğ‘–ğœ‰. We use the cross-entropy loss îˆ¸ğ‘ğ‘ğ‘(âˆ—) in the multi-class classification task for the identification:

îˆ¸ğ‘ğ‘ğ‘(ğ‘“ğ‘–ğ‘ğ‘™ğ‘ _ğ‘,ğœ‰,ğ‘ğ‘–ğ‘,ğœ‰)=âˆ‘ğ‘˜=1ğ¾âˆ’ğ‘ğ‘–ğ‘,ğœ‰[ğ‘˜]â‹…log(ğ‘Ì‚ ğ‘–ğ‘,ğœ‰[ğ‘˜])ğ‘Ì‚ ğ‘–ğ‘,ğœ‰=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘“ğ‘–ğ‘ğ‘™ğ‘ _ğ‘,ğœ‰)
(3)
where ğ‘Ì‚ ğ‘–ğ‘,ğœ‰âˆˆâ„1âˆ—ğ¾ is the probability by prediction, ğ‘Ì‚ ğ‘–ğ‘,ğœ‰[ğ‘˜] indicates the probability for the k-th class. ğ‘ğ‘–ğ‘,ğœ‰âˆˆâ„1âˆ—ğ¾ indicates the ground truth label. If the i-th target belongs to the k-th class, then ğ‘ğ‘˜ğ‘,ğœ‰=1, others elements=0.

When the identification loss tends to converge, all of the parameters will be loaded into CNNs from DAN as the pre-trained model.

Motion Encoder analyzes the pedestrian movement and predicts the position in the future. The inputs of the motion model include historical locations of tracklet and its corresponding timestamps. Motion Encoder is utilized for encoding the bounding boxesâ€™ information (position and shape). The ME (Motion Encoder) model projects the 4 dimensional vector into a m-dimensional vector by LSTM for the assigned nodes, ğ¹ğ‘™ğ‘€(âˆ—), and by fully-connected layers for the unassigned nodes, ğ¹ğ‘“ğ‘€(âˆ—). LSTM is able to learn sequential data. ğ‘“ğ‘–ğ‘š,ğ‘¡ denotes the motion feature for node ğ‘£ğ‘–ğ‘¡, ğ¹ğ‘€(âˆ—)={ğ¹ğ‘™ğ‘€(âˆ—),ğ¹ğ‘“ğ‘€(âˆ—)} is the model forward function of motion model:

ğ‘“ğ‘–ğ‘š,ğœ‰={ğ¹ğ‘™ğ‘€([ğµğ‘–(ğœ‰âˆ’ğ¿ğ‘–),...,ğµğ‘–(ğœ‰âˆ’1),ğµğ‘–ğœ‰]), ğ‘£ğ‘–ğœ‰âˆˆğ´ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘ğ¹ğ‘“ğ‘€(ğµğ‘–ğœ‰),  ğ‘£ğ‘–ğœ‰âˆˆğ‘ˆğ‘›ğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘
(4)
where ğœ‰âˆˆ[ğ‘¡,ğ‘¡+ğœ‚] is the frame number, ğ¿ğ‘– indicates the length of the i-th tracklet, ğµğ‘–ğœ‰ represents the bounding box x, y position, width and height, [ğ‘¥ğ‘–ğœ‰,ğ‘¦ğ‘–ğœ‰,ğ‘¤ğ‘–ğœ‰,â„ğ‘–ğœ‰], of ğ‘£ğ‘–ğœ‰, which are normalized to [0, 1] by the original image size. We use the position loss to train the motion model in advance, which is described as:

îˆ¸ğ‘šğ‘œğ‘¡=âˆ‘ğœ‰=1ğ¿ğ‘–âˆ’1âˆ¥ğ‘“ğ‘–ğ‘š,ğœ‰âˆ’ğ‘“ğ‘–ğ‘š,ğœ‰+1âˆ¥22
(5)
The motion features ğ‘“ğ‘–ğ‘š,ğœ‰ and ğ‘“ğ‘–ğ‘š,ğœ‰+1 are generated by LSTM model and FC model, respectively.

Human-Interaction Model Extractor
Human-Interaction Model (HIM) is a novel model for extracting features for MOT, which combines the information of tracked-target and its neighboring targets. Compared with the previous interaction extractor, HIM not only utilizes the relative position information between tracked node and its surrounding nodes, but also fuses appearance features of the targets together. HIM describes where the persons are located around the tracked-target and additionally describes what the persons look like. Interaction feature has more effectiveness for the target who is occluded frequently or heavily in the highly-crowded scenery.

Fig. 6
figure 6
The Illustration of Building Relationships and Interaction Extraction

Full size image
To build relationships between the target and the other objects, we set up a connecting rule to filter the nodes which are unsuitable to extract features for tracked-target (as shown in in Fig. 6). The set of node candidates are defined as ğ‘¢ğ‘—ğœ‰âˆˆğ‘ˆ. The connecting rule includes two constraints, distance constraint and direction constraint. The candidate node ğ‘¢ğ‘—ğœ‰ which is over the maximum distance limit or against the direction with tracked-target ğ‘£ğ‘–ğœ‰ cannot be connected in relationship. The distance constraint is described as:

{1,  âˆ¥ğ‘ƒğ‘–ğœ‰âˆ’ğ‘ƒğ‘—ğœ‰âˆ¥22â‰¤ğœšğ‘‘ğ‘–ğ‘ 0,  âˆ¥ğ‘ƒğ‘–ğœ‰âˆ’ğ‘ƒğ‘—ğœ‰âˆ¥22>ğœšğ‘‘ğ‘–ğ‘ 
(6)
where ğ‘ƒğ‘–ğœ‰, ğ‘ƒğ‘—ğœ‰ are center positions of the bounding boxes ğµğ‘–ğœ‰, ğµğ‘—ğœ‰, respectively. ğœšğ‘‘ğ‘–ğ‘  is the maximum distance limit parameter. The direction constraint is defined as:

{1,  ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’(ğ‘£ğ‘–ğœ‰[ğ‘£ğ‘£ğœ‰],ğ‘¢ğ‘—ğœ‰[ğ‘£ğ‘£ğœ‰],)â‰¥0 âˆ¨ ğ‘¢ğ‘—ğœ‰[ğ‘£ğ‘£ğœ‰]=00,  ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’(ğ‘£ğ‘–ğœ‰[ğ‘£ğ‘£ğœ‰],ğ‘¢ğ‘—ğœ‰[ğ‘£ğ‘£ğœ‰],)<0
(7)
where ğ‘£ğ‘–ğœ‰[ğ‘£ğ‘£ğœ‰],ğ‘¢ğ‘—ğœ‰[ğ‘£ğ‘£ğœ‰] are the corresponding velocities of the target and the candidate at [ğ‘¡,ğ‘¡+ğœ‚]-th frame. The kept candidates are connected to the target node as the graph structure ğœ“ğ‘–ğœ‰âˆˆâ„(ğ‘+1)âˆ—(ğ‘+1), where p is the number of reserved candidates. The candidatesâ€™ features are regarded as node characteristics ğ‘‹ğ‘–ğœ“,ğœ‰âˆˆâ„(ğ‘+1)âˆ—(ğ‘›+ğ‘š), which are composed of the corresponding nodesâ€™ appearance feature ğ‘“ğ‘–ğ‘,ğœ“ from AE and the relative position ğ‘“ğ‘–ğ›¥,ğœ“=ğ¹ğ¶([[ğ›¥ğ‘¡ğ‘ğ‘Ÿ,1ğ‘¥,ğ›¥ğ‘¡ğ‘ğ‘Ÿ,1ğ‘¦];...[ğ›¥ğ‘¡ğ‘ğ‘Ÿ,(ğ‘+1)ğ‘¥,ğ›¥ğ‘¡ğ‘ğ‘Ÿ,(ğ‘+1)ğ‘¦]]), where ğ›¥ğ‘¡ğ‘ğ‘Ÿ,âˆ—ğ‘¥ and ğ›¥ğ‘¡ğ‘ğ‘Ÿ,âˆ—ğ‘¦ indicate the distance on x, y axis between the target and the corresponding neighboring node, ğ‘“ğ‘–ğ›¥,ğœ“âˆˆâ„(ğ‘+1)âˆ—ğ‘š is the output of FC. The HIM is a Graph Neural Network structure, ğ¹ğœ“(âˆ—) represents the model forward function of the interaction model, which is described as:

Fig. 7
figure 7
Illustration of the Graph Construction. The whole sequence is divided into several sub-sequence, and the strategy constructs the graph by bounding boxes overlap and time interval in sub-sequence

Full size image
ğ‘“ğ‘–ğ‘ğ‘™ğ‘ _ğœ“,ğœ‰,ğ‘“ğ‘–ğœ“,ğœ‰=ğ¹ğœ“(ğ´ğ‘–ğœ“,ğœ‰,ğ‘‹ğ‘–ğœ“,ğœ‰)
(8)
where ğ´ğ‘–ğœ“,ğœ‰âˆˆâ„(ğ‘+1)âˆ—(ğ‘+1) is the adjacent matrix, which represents the connection between node i and its surrounding nodes at frame ğœ‰. p is the number of nodes around node i. ğ‘‹ğ‘–ğœ“,ğœ‰âˆˆâ„(ğ‘+1)âˆ—(ğ‘›+ğ‘š) is the feature matrix of these nodes. ğ‘“ğ‘–ğ‘ğ‘™ğ‘ _ğœ“,ğœ‰ and ğ‘“ğ‘–ğœ“,ğœ‰ are the interaction feature and classification feature of node i, respectively. To process graph data using convolution operation, we transform the adjacent matrix to the frequency domain by Laplace Transform (Kipf and Welling 2016). In this way, an adjacent matrix is transformed to a Laplace Matrix. We adopt the Symmetric Normalized Laplacian Matrix L in implementation, which is defined as:

ğ¿=ğ›¤âˆ’12ğ´ğ›¤âˆ’12
(9)
where ğ›¤ is Degree Matrix, which is the diagonal matrix where each element ğ›¤[ğ‘–,ğ‘–] is the degree of the vertex i (number of edges attached to the vertex i). A (Adjacent Matrix) is a square matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph. For Laplacian Transform, convolution in time domain equals point-wise multiplication in frequency domain. The graph convolution simplifies to:

ğ‘”ğœƒâ‹†ğ‘¥â‰ˆğœƒ(ğ›¹ğ‘+ğ›¤âˆ’12ğ´ğ›¤âˆ’12)ğ‘¥
(10)
where â‹† indicates convolution, and ğ‘”ğœƒ is a filter parameterized by ğœƒ in the Fourier domain. x is the feature matrix, ğœƒ is a learnable parameter, and ğ›¹ğ‘ is the identity matrix. Note that ğ›¹ğ‘+ğ›¤âˆ’12ğ´ğ›¤âˆ’12 has eigenvalues in the range [0, 2]. Repeated application of Eq.(10) leads to numerical instabilities and exploding/vanishing gradients when used in a deep neural network model. To alleviate this problem, we introduce the following re-normalization trick: ğ›¹ğ‘+ğ›¤âˆ’12ğ´ğ›¤âˆ’12â†’ğ›¤Ìƒ âˆ’12ğ´Ìƒ ğ›¤Ìƒ âˆ’12, with ğ´Ìƒ =ğ´+ğ›¹ğ‘, ğ›¤Ìƒ [ğ‘–,ğ‘–]=âˆ‘ğ‘—ğ´Ìƒ [ğ‘–,ğ‘—], the Eq.(10) is approximated as follows:

ğ‘‹Ì‚ =ğ›¤Ìƒ âˆ’12ğ´Ìƒ ğ›¤Ìƒ âˆ’12ğ‘‹ğ›©
(11)
where ğ‘‹âˆˆâ„ğ‘âˆ—ğ¶ is feature matrix, N is the number of nodes, C is input channel (C-dimensional feature vector). ğ›©âˆˆâ„ğ‘âˆ—ğ¹ is a learnable parameter of GCN. ğ‘‹Ì‚ âˆˆâ„ğ‘âˆ—ğ¹ is the convolved feature matrix. The Eq.(8) is expended as:

ğ‘‹Ì‚ ğ‘–ğœ“,ğœ‰=ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ›¤Ìƒ âˆ’12ğ´Ìƒ ğ‘–ğœ“,ğœ‰,ğ›¤Ìƒ âˆ’12ğ‘‹ğ‘–ğœ“,ğœ‰ğ›©)
(12)
ğ´Ìƒ =ğ´+ğ›¹ğ‘, ğ›¤Ìƒ [ğ‘–,ğ‘–]=âˆ‘ğ‘—ğ´Ìƒ [ğ‘–,ğ‘—]
(13)
ğ›¹ğ‘ is the identity self-connections matrix, and the ğ´Ìƒ  is the combination of adjacency matrix and self-connections. ğ›¤Ìƒ  indicates a degree matrix of graph G, and ğ›©âˆˆâ„(ğ‘+1)âˆ—(ğ‘›+ğ‘š) is a learnable parameters on GNN, and feature matrix ğ‘‹Ì‚ ğ‘–ğœ“,ğœ‰âˆˆâ„(ğ‘+1)âˆ—ğ‘™ indicates output the GNN. And then, ğ‘‹Ì‚ ğ‘–ğœ“,ğœ‰ are global Max Pooling on node-level:

ğ‘“ğ‘–ğœ“,ğœ‰=ğ‘€ğ‘ğ‘¥ğ‘ƒğ‘œğ‘œğ‘™ğ‘–ğ‘›ğ‘”(ğ‘‹ğ‘–ğœ“,ğœ‰)
(14)
ğ‘“ğœ“,ğœ‰ is a l-dimensional vector, which represents the interaction feature. We also adopt the classification loss îˆ¸ğ‘–ğ‘›ğ‘¡ for training HIM, which is defined as:

îˆ¸ğ‘–ğ‘›ğ‘¡(ğ‘“ğ‘–ğ‘ğ‘™ğ‘ _ğœ“,ğœ‰,ğ‘ğ‘–ğœ“,ğœ‰)=âˆ‘ğ‘˜=1ğ¾âˆ’ğ‘ğ‘–ğœ“,ğœ‰[ğ‘˜]â‹…log(ğ‘Ì‚ ğ‘–ğœ“,ğœ‰[ğ‘˜])ğ‘Ì‚ ğ‘–ğœ“,ğœ‰=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘“ğ‘–ğ‘ğ‘™ğ‘ _ğœ“,ğœ‰)
(15)
where the value of ğ‘ğ‘–ğœ“,ğœ‰ represents ground truth label for the i-th target.

The features generated by AE, ME and HIM are concatenated together, which is defined as:

ğ‘“ğ‘–ğœ‰=ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¡ğ‘’[ğ‘“ğ‘–ğ‘,ğœ‰,ğ‘“ğ‘–ğ‘š,ğœ‰,ğ‘“ğ‘–ğœ“,ğœ‰]
(16)
where ğ‘“ğ‘–ğœ‰ is the feature of node ğ‘£ğ‘–ğœ‰, and then we concatenate all of the nodes within [ğ‘¡,ğ‘¡+ğœ‚] as:

ğ‘‹ğ‘¡=ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¡ğ‘’[ğ‘“1ğ‘¡;...;ğ‘“ğ‘–ğ‘¡;ğ‘“1ğ‘¡+1;...;ğ‘“ğ‘—ğ‘¡+ğœ‚]
(17)
where ğ‘‹ğ‘¡âˆˆâ„ğ‘âˆ—(ğ‘›+ğ‘š+ğ‘™) indicates the feature matrix, N is the number of all of nodes within [ğ‘¡,ğ‘¡+ğœ‚].

Graph Construction
Before feeding into GNN, we firstly divide the video sequence as several sub-sequences, and pre-process the bounding boxes for each sub-sequence. The bounding boxes are constructed and normalized as graph-structured data. The graph structure consists of nodes and edges represented as ğº=(î‰‚,îˆ±) (mentioned in Sec.3.1), where the nodes ğ‘£âˆˆî‰‚ represent the bounding boxes, and the edges ğ‘’âˆˆîˆ± denote the spatial-temporal relationship between nodes. The graph construction is described in Fig. 7. We divide the whole video sequence into several video fragments (sub-sequence). ğœ‚ is the length of video fragments, for instance in order to generate the graph ğºğ‘¡, we firstly extract the t-th frame to the (ğ‘¡+ğœ‚)-th frame images and select nodes from sub-sequence in terms of the node state. The state of the nodes can be divide into â€œTrackedâ€, â€œLostâ€, â€œUnassignedâ€, and â€œQuittedâ€ (described in Sec.3.2).Only â€˜lostâ€™ and â€˜unassignedâ€™ nodes will be selected for graph construction. We calculate the nodeâ€™s bounding boxes IoU (Intersection over Union) between adjacent frames, the edge weight is proportional to IoU value and inversely proportional to the frame interval. The edge weight ğ‘’ğœ€ğ‘–,ğœ‰ğ‘— of graph ğºğ‘¡ between ğµğ‘–ğœ€ and ğµğ‘—ğœ‰ is defined as:

ğ‘’ğœ€ğ‘–,ğœ‰ğ‘—=ğ¼ğ‘œğ‘ˆ(ğµğ‘–ğœ€,ğµğ‘—ğœ‰)âˆ—(1âˆ’ğœ‡âˆ—ğ‘šğ‘–ğ‘›(ğ‘ğ‘ğ‘ (ğœ€âˆ’ğœ‰),ğœ†))ğ‘ .ğ‘¡.  ğœ€,ğœ‰âˆˆ[ğ‘¡,ğ‘¡+ğœ‚],  ğœ€<ğœ‰,  ğœ‡,ğœ†>0
(18)
where

ğ¼ğ‘œğ‘ˆ(ğµğ‘–ğœ€,ğµğ‘—ğœ‰)=ğ‘ğ‘Ÿğ‘’ğ‘(ğµğ‘–ğœ€âˆ©ğµğ‘—ğœ‰)ğ‘ğ‘Ÿğ‘’ğ‘(ğµğ‘–ğœ€âˆªğµğ‘—ğœ‰)
(19)
where constant ğœ‡ is used for adjusting the influence of frame interval on edge weight, and constant ğœ† is the upper limit of frame interval. The representation of graph ğºğ‘¡ includes adjacent matrix ğ´ğ‘¡âˆˆâ„ğ‘âˆ—ğ‘,ğ´ğ‘¡[ğ‘–,ğ‘—]=ğ‘’ğ‘–,ğ‘— and features matrix ğ‘‹ğ‘¡âˆˆâ„ğ‘âˆ—(ğ‘›+ğ‘š+ğ‘™),ğ‘‹ğ‘¡[ğ‘–]=[ğ‘“ğ‘–ğ‘,ğ‘“ğ‘–ğ‘š,ğ‘“ğ‘–ğœ“], which are introduced in Eq.(16) and Eq.(17).

Graph Optimization by GNN
Graph Neural Network (GNN) aims to learn the topological data pattern to represent the graph structure feature, which encodes the node features and updates the representation vector in the graph. Better than CNN and RNN, GNN has more significant effects on the graph structure based task, such as molecule classification and particle interaction simulation.

The target of multiple object tracking task is to locate every pedestrianâ€™s position at each moment. So we associate the node ID and connect the nodes which belong to the same person as the tracking result. The MOT method address this problem by Data Association, which involves network flow, graph-cut and feature clustering, so GNN is able to optimize the graph node feature and edge weights between nodes. we adopt the Graph Convolutional Network (GCN) (Kipf and Welling 2016) as the network backbone. The adjacent matrix ğ´ğ‘¡ and feature matrix ğ‘‹ğ‘¡ are denoted as the GCN input, and the GCN outputs include updated ğ´ğ‘¡^ and ğ‘‹ğ‘¡^. ğ¹ğº(âˆ—) indicates the model forward function of GCN:

ğ´Ì‚ ,ğ‘‹Ì‚ =ğ¹ğº(ğ´,ğ‘‹)
(20)
where forward function ğ¹ğº is similar to Eq.(12) and Eq.(13)

ğ‘‹Ì‚ =ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ›¤Ìƒ âˆ’12ğ´Ìƒ ğ›¤Ìƒ âˆ’12ğ‘‹ğ›©)
(21)
where, feature matrix ğ‘‹Ì‚ âˆˆâ„ğ‘âˆ—ğ‘ denotes one of the GCN output, each node feature is a q-dimension vector. The updated adjacency matrix ğ´Ì‚ âˆˆâ„ğ‘âˆ—ğ‘ is given by:

ğ´Ì‚ =(ğ‘‹Ì‚ ğ‘›ğ‘œğ‘Ÿğ‘šâˆ—ğ‘‹Ì‚ ğ‘‡ğ‘›ğ‘œğ‘Ÿğ‘š)+12
(22)
where

ğ‘‹Ì‚ ğ‘›ğ‘œğ‘Ÿğ‘š[ğ‘–]=ğ‘‹Ì‚ [ğ‘–]||ğ‘‹Ì‚ [ğ‘–]||,ğ‘–âˆˆ[1,ğ‘]
(23)
where Eq.(23) indicates row-wise normalization, ğ‘‹Ì‚ [ğ‘–] indicates the i-th row of ğ‘‹Ì‚ . Every element of ğ´Ì‚  is between 0 to 1. ğ´Ì‚ [ğ‘–,ğ‘—] indicates the association probability between the i-th node and the j-th node. The multi-layers GCN feedward function is shown as:

ğ´1^,ğ‘‹1^=ğ¹ğº1(ğ´,ğ‘‹)
(24)
ğ´ğœ^,ğ‘‹ğœ^=ğ¹ğºğœâˆ’1(ğ´ğœâˆ’1^,ğ‘‹ğœâˆ’1^), ğœ>1
(25)
Finally, to train the DAN, we design a Graph Loss îˆ¸ğº(âˆ—), which is defined as:

Fig. 8
figure 8
The explanation of DAN training and Loss Function

Full size image
îˆ¸ğº(ğ´ğœ^,ğºğ‘”ğ‘¡ğ‘¡)=âˆ‘ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—âˆˆîˆ±ğ‘”ğ‘¡ğ‘¡(ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—âˆ’ğ‘’ğ‘–ğ‘—)+âˆ‘ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—âˆ‰îˆ±ğ‘”ğ‘¡ğ‘¡ğœâˆ—(ğ‘’ğ‘–ğ‘—âˆ’ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—)
(26)
where ğºğ‘”ğ‘¡ğ‘¡ is the ground truth graph structure which is computed previously, ğºğ‘”ğ‘¡ğ‘¡=(î‰‚ğ‘”ğ‘¡ğ‘¡,îˆ±ğ‘”ğ‘¡ğ‘¡), ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—âˆˆîˆ±ğ‘”ğ‘¡ğ‘¡, ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—={0,1} , and ğœ is the loss weight of Graph Loss. The total loss îˆ¸ of the training DAN includes cross-entropy loss îˆ¸ğ‘ğ‘ğ‘, îˆ¸ğ‘–ğ‘›ğ‘¡ for AE and HIM, position loss îˆ¸ğ‘šğ‘œğ‘¡ for ME, and graph loss îˆ¸ğº for GCN (described in Fig. 8).

Experiments
MOT Datasets
Our method is evaluated on public benchmark (MOT Challenge (Milan et al. 2016)), which includes MOT15 (Leal-Taix et al. 2015), MOT16 (Milan et al. 2016), Duke-MTMCTT (Ristani et al. 2016) and MOT20 (Dendorfer et al. 2019). All datasets contain large-scale video sequences from different cameras and scenes. The training sets provide ground truth bounding boxes and IDs by annotator, and testing sets only give the detection results by detector.

MOT15 includes 22 sequences which are divided into one half for training and the other half for testing. The testing data contains over 10 minutes of footage and 61440 annotated bounding boxes, the videos are from moving cameras or static cameras, respectively. MOT15 additionly provides the detection results by DPM (Felzenszwalb et al. 2010) as the tracker inputs.

MOT16 is a classical evaluation dataset comparing several tracking methods on MOT Challenge, which includes 14 sequences captured from surveillance, hand-held shooting and driving recorder by static cameras and moving cameras. The length of each video is about 500-1500 frames. And the dataset also provides the detections by DPM (Felzenszwalb et al. 2010).

DukeMTMCT is a large scale dataset for multiple-camera multiple-object tracking, which are captured by 8 surveillance cameras at different viewing angles include 2800 identities (person) in Duke University . The video duration of each camera is 86 minutes, which is split into training set (0-50 min) and testing set (50-86 min). In addition, the dataset provided DPM (Felzenszwalb et al. 2010) and Openpose (Cao et al. 2018) detection results for each frame as the tracker input.

MOT20 has been carefully selected to challenge trackers and detectors on extremely crowded scenes. In contrast to previous challenges, the total dataset contains 8 sequences. All sequences are filmed in high resolution from an elevated viewpoint, and the mean crowd density reaches 246 pedestrians per-frame which is 10 times larger than the previous benchmark. MOT20 utilizes a Faster R-CNN (Ren et al. 2015) with ResNet101 backbone on the MOT20 training sequences as the tracking input.

Implementation Details
In our experiments, DAN consists of feature extraction and graph optimization. For feature extraction, appearance extractor is composed of CNN, whose architecture backbone is SeResNet-50 (Hu et al. 2018). The tracked-targetsâ€™ images are resized to 256Ã—256 from the cropped images and the outputs of CNN produces appearance feature ğ‘“ğ‘,ğ‘¡, a 2048-dimensional vector to describe the image. Motion Encoder (ME) is composed of 3-layers of LSTM network, batch-normalization and ReLU. Bounding box information [x,y,w,h], a 4-dimensional vector is raised to 4â†’64â†’512 dimensional vector finally by ME. Human-Interaction Model is a GNN structure. To build the relationship, we set the maximum distance by the sum of average width and height of all the bounding boxes in the current frame. The relatively position [[ğ›¥ğ‘¡ğ‘ğ‘Ÿ,1ğ‘¥,ğ›¥ğ‘¡ğ‘ğ‘Ÿ,1ğ‘¦];...[ğ›¥ğ‘¡ğ‘ğ‘Ÿ,ğ‘ğ‘¥,ğ›¥ğ‘¡ğ‘ğ‘Ÿ,ğ‘ğ‘¦]] are input into 3-layers fully-connected network. The corresponding appearance feature from feature extractor and relative position are jointly fed into the GNN model. The output of GNN is a 2048-dimensional vector to express the target surrounding feature. To construct relationship rapidly, we adopt a function â€œradius_neighbors_graph()â€ from sklearn library to implement graph construction for each target. The function rapidly connects the nodes whose distance is less than radius as an adjacent matrix. We only generate two adjacent matrices for each frame, one representing the adjacent matrix ğ´ğ‘‘ğ‘–ğ‘ âˆˆâ„ğ‘âˆ—ğ‘ with distance constraint, the other denoting the adjacent matrix ğ´ğ‘œğ‘Ÿğ‘–âˆˆâ„ğ‘âˆ—ğ‘ with orientation constraint, where ğ´ğ‘‘ğ‘–ğ‘ /ğ‘œğ‘Ÿ[ğ‘–,ğ‘—]âˆˆ0,1, and then Inter-connection Matrix ğ´ğœ“ is calculated by ğ´ğ‘‘ğ‘–ğ‘  & ğ´ğ‘œğ‘Ÿğ‘–. The training set graph size is restricted to 64-512 nodes per graph, the length of sub-sequence ğœ‚ is 30 frames, and the stride of sequence ğ›¿ is 20. For each step of epoch, we replace data batch size with graph size to feed CNN model. For graph construction, the frame interval impact factor ğœ‡ is 0.08 and the upper limit constant ğœ† is 10. The input of GCN is a N Ã— 2560-dimensional vector, where N is the number of nodes on graph, and we adopt two-layers GCN and the graph loss weight ğœ is 2. The training optimizer is the AdamOptimizer (Kingma et al. 2014). We load the pre-trained weight on CNN, the learning rate is less than the others models. The initial learning rate for CNN is set to 1e-5 and learning rates for the others are set to 0.001. The model converges finally at the 150th epoch.

Ablation Study
Table 1 shows the tracking performance with different components and network on validation datasets. The table is divided into three groups, the first group compares results between without and with Human-Interaction Model. The interaction feature is able to associate the same target before and after occlusion in the crowd, as a result, it can drastically reduce Id-switch (IDSw.) from 1286 to 874 , Fragments (Frag.) from 2181 to 1862 and improve MOTA by 4.1% and IDF1 by 4.2%.

The second group demonstrates the difference between Hungarian Algorithm and Graph Network Association. MOTA, IDF1 are increased by 1.5%, 0.7%, respectively, when hand-crafted optimization (H) is replaced by GNN (ğº256). On this basis, adding interaction model can further enhance the tracking performance.

The third group shows the results on graph structures with different number of nodes, where ğº256 indicates that the graph structure includes no more than 256 nodes. We firstly replace (H) by ğº64, however the result is not satisfactory, because the fewer nodes we get, the shorter sub-sequence ğœ‚ is. GNN is difficult to associate the occluded target with un-occluded target in short time. Increasing the number of nodes can enlarge the length of sub-sequence, the same targets before and after occlusion can be associated. Therefore, MOTA and IDF1 are improved gradually until the number of nodes equals 256. The same targetâ€™s appearances change (illumination, angle, scale, etc.) in long-term, too many nodes in graph leads the network to be unable to distinguish the feature of the same and different targets well. As a consequence, ğº512 decreases the performance slightly. Finally, we adopt ğº256 as the graph optimization part.

Table 1 Tracking results on validation dataset (MOT15-PETS09-S2L1, MOT16-04, MOT20-05) with different components. (A): Appearance Extractor; (M): Motion Encoder; (H): Hungarian Algorithm; (I): Human-Interaction Model; (G): Graph Network Association
Full size table
figure a
Fig. 9
figure 9
The performance of Inter-relationship Building. a: Video frames and tracked-targets at different times. b: Corresponding Inter-relationship structure of targets

Full size image
Fig. 10
figure 10
a: Cosine similarity between appearance features of targets ğ¼1â†’ğ¼7 from Fig. 9.a; b: Cosine similarity between interaction features of targets ğœ“1â†’ğœ“7 from Fig. 9.b

Full size image
Table 2 Results on the MOT16 test dataset
Full size table
Table 3 Results on the DukeMTMCT test dataset
Full size table
Table 4 Results on the MOT15&16 on static camera test set
Full size table
DAN training Details
To train the DAN, we firstly divide training sequences into many short sub-sequence, and the length of each sub-sequences is about 20-30 frames. If every frame includes 15-20 pedestrians, each sub-sequence has 64-512 nodes and we construct them as a graph. We set the maximum number of nodes at graph construction phase, ğº256 indicates that the number of nodes is no more than 256, otherwise the rest of the nodes are moved to the next sub-sequences. For the whole sequence, we can get about 2k-10k graphs, and the number of graph depends on sub-sequence length and sample stride length. And we shuffle these graphs for each epoch, and our experiment implements on the Pytorch framework by 4 Nvidia Titan X GPUs, and device 0,1,2 are used for loading the CNN model to extract appearance features, and then the features transfers to device 3 to calculate motion encoder and GNN model. The processing of network forward and backward are shown as Algorithm 1. Shuffling graph is only used in training phase. However, in inference phase, the model processes the sequence stage-by-stage according to the order of sequence as shown in Fig. 8. For each epoch, we need to pass all of training set once (We used 12 sequences for training, each sequence has 450-1200 frames). However, we set the length of sub-seqence = 30 and stride = 20, so actually we have o pass 1.5 times length of sequence for each epoch. It takes about 80-90 minutes (about 4500s). It takes about 7-8 days to finish 150 epochs.

Inter-relationship Building Results
Human Interaction Model is utilized for extracting interaction features between target and neighbors. The inter-relationship buildup is illustrated in Fig. 9, where Fig. 9a shows the video frames at different times. The cropped images ğ¼1â†’5 on the left of each frame indicates the tracked-target. Due to the frequently occlusion and illumination variation, the targetâ€™s appearance and bounding box shape change quite much, furthermore, the most prominent feature (a red knapsack) of target is erased in some frames, which causes the appearance features for the same target at different frames to become indistinguishable. Fig. 9b shows the results of inter-relationship structure ğœ“1â†’5 for ğ¼1â†’5. Besides, the figure additionally shows the targetâ€™s neighbor ğ¼6â†’7 and relationship ğœ“6â†’7 to contrast the difference of feature with the target, where one of the neighbor has a similar red knapsack as the target, another neighbor has the different appearance from the target.

Appearance and Interaction Comparison
Fig. 10 demonstrates the appearance and interaction cosine similarity between the same target at different frames and different target at the same frames. For appearance similarity estimation, we calculate cosine similarity as a 7Ã—7 matrix ğ‘†ğ¼ between targets ğ¼1â†’7, where ğ‘†ğ¼[ğ‘–,ğ‘—] indicates the similarity between ğ¼ğ‘– and ğ¼ğ‘— (as shown in Fig. 10a). In addition, we also calculate cosine similarity as ğ‘†ğœ“ between groups ğœ“1â†’5 (as shown in Fig. 10b). We firstly compare appearance feature similarity of the same target ğ¼1â†’5, where the value ranges between [0.556, 0.726]. However, due to the highly similar appearance between the target ğ¼1â†’5 and the first neighbor ğ¼6, itâ€™s difficult to distinguish them by similarity value ([0.528, 0.659]). On the contrary, the interaction feature reserves some discriminative characteristics, the similarity value ranges between targets ğœ“1â†’5 is limited to [0.816,0.926], meanwhile the value between targets ğœ“1â†’5 and the first neighbor ğœ“6 is decreased to [0.324, 0.471]. The similarity is already distinguishable in appearance feature (such as targets ğ¼1â†’5 and the second neighbor ğ¼7), their interaction features ğœ“1â†’5 and ğœ“7 also remain discriminative.

MOT Evaluation Metrics
The MOT Challenge Benchmark adopts the standard CLEAR-MOT mapping (Bernardin and Stiefelhagen 2008) and ID measures (Ristani et al. 2016) for evaluating MOT performance. The main metrics for MOT are MOTA and IDF1. MOTA (Multiple Object Tracking Accuracy) measures the effect of tracking for each tracklet, which depends on True Positives (TP) ,False Positives (FP), False Negatives (FN) and Id Switches (IDSw), ğ‘€ğ‘‚ğ‘‡ğ´=1âˆ’ğ¹ğ‘ƒ+ğ¹ğ‘+ğ¼ğ·ğ‘†ğ‘‡ğ‘ƒ. Total number of Fragment (Frag), Mostly tracked targets (MT), Mostly lost targets (ML) are used for evaluating tracklet integrity as the reference indexes. The IDF1 (ID F1 Score) is the ratio of correctly identified detection over the average number of true and computed detections, which depends on ID True Positives (IDTP), ID False Positives (IDFP) and ID False Negatives (IDFN), ğ¼ğ·ğ¹1=2âˆ—ğ¼ğ·ğ‘‡ğ‘ƒ2âˆ—ğ¼ğ·ğ‘‡ğ‘ƒ+ğ¼ğ·ğ¹ğ‘+ğ¼ğ·ğ¹ğ‘ƒ. IDP (ID Precision) indicates fraction of computed detections that are correctly identified, ğ¼ğ·ğ‘ƒ=ğ¼ğ·ğ‘‡ğ‘ƒğ¼ğ·ğ‘‡ğ‘ƒ+ğ¼ğ·ğ¹ğ‘ƒ. IDR (ID Recall) means fraction of ground-truth detections that are correctly identified, ğ¼ğ·ğ‘…=ğ¼ğ·ğ‘‡ğ‘ƒğ¼ğ·ğ‘‡ğ‘ƒ+ğ¼ğ·ğ¹ğ‘.

Tracker Results Comparison
Our method is evaluated on the MOT Challenge testing set. We compare the performance with the state-of-the-art methods on the benchmark. The comparison of methods on MOT16 and DukeMTMCT are shown in Table 2 and Table 3. The most crucial evaluating parameters include MOTA and IDF1. Compared with the previous methods, our proposal aims to extract the more efficient features to avoid the â€mis-trackingâ€ and â€lost-trackingâ€. Consequently, Table 2 shows that DHIAN (ours) achieves the best performance on MOTA, MT. The others parameters (such as IDF1, ML, etc.) reaches relatively high score for MOT16 dataset. As shown in Table 3, our method exceeds the previous highest MOTA score by 3.3%, and DHIAN also achieves the excellent score on MT, ML, FN. The video from Duke-MTMCT dataset is captured from surveillance, but MOT16 blends moving videos and static videos. Since moving camera affects trajectory prediction and relationship construction, our methods has better performance on Duke-MTMCT than MOT16. Therefore, we specially compare performance on static video of MOT15 and MOT16 (as shown in Table 4). Our results ranks the 1st or 2nd place with MOTA and IDF1 among the start-of-the-art methods. Fig. 11 illustrates the DHIAN visualization results on MOT15 (the 1st-3rd rows), MOT16 (the 4th-6th rows), Duke-MTMCT (the 7th row) and MOT20 (the 8th row).

Fig. 11
figure 11
The visualization results on MOT15, MOT16, Duke-MTMCT and MOT20

Full size image
Conclusion and Future Work
In this paper, we concentrate on Multiple Object Tracking (MOT) in the wild. Frequent occlusion and rapid illumination variation influence the tracked- targetsâ€™ feature description. In addition, existing methods use the hand-crafted method to optimize graph structure, which causes that feature extraction module and graph optimization module cannot be combined as a whole end-to-end network. Therefore, we specially consider the interrelation cue between objects and design Human-Interaction Model (HIM) to extract the details of target and its surrounding. Meanwhile we propose an efficient end-to-end model, Deep Association Network (DAN), to optimize the association with graph-based learning mechanism. Our proposal is evaluated on 4 public datasets (MOT15, MOT16, Duke-MTMCT and MOT20). The algorithm achieves MOTA up to 50.0, 86.7 and IDF1 up to 54.3, 82.0 on MOT16 and DukeMTMCT, respectively. The visualization results are shown in Fig. 11. Our method has better performance in complex crowd scene and static cameras.

In the future, we intend to optimize our model and improve processing efficiency. And we will continue to explore the novel model structure and relationship building strategy to extract interaction feature more effectively. To solve the frequent occlusion problem, we intend to combine the head-detector and body-detector as collaborative tracking of pedestrians. And we will design a newer motion encoder to replace the current RNN structure for trajectory prediction. Lastly, we attempt to utilize transfer learning to improve the model robustness.