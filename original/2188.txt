Recent works on crowd counting mainly leverage Convolutional Neural Networks (CNNs) to count by regressing density maps, and have achieved great progress. In the density map, each person is represented by a Gaussian blob, and the final count is obtained from the integration of the whole map. However, it is difficult to accurately predict the density map on dense regions. A major issue is that the density map on dense regions usually accumulates density values from a number of nearby Gaussian blobs, yielding different large density values on a small set of pixels. This makes the density map present variant patterns with significant pattern shifts and brings a long-tailed distribution of pixel-wise density values. In this paper, we aim to address such issue in the density map. Specifically, we propose a simple and effective Learning to Scale (L2S) module, which automatically scales dense regions into reasonable closeness levels (reflecting image-plane distance between neighboring people). L2S directly normalizes the closeness in different patches such that it dynamically separates the overlapped blobs, decomposes the accumulated values in the ground-truth density map, and thus alleviates the pattern shifts and long-tailed distribution of density values. This helps the model to better learn the density map. We also explore the effectiveness of L2S in localizing people by finding the local minima of the quantized distance (w.r.t. person location map), which has a similar issue as density map regression. To the best of our knowledge, such localization method is also novel in localization-based crowd counting. We further introduce a customized dynamic cross-entropy loss, significantly improving the localization-based model optimization. Extensive experiments demonstrate that the proposed framework termed AutoScale improves upon some state-of-the-art methods in both regression and localization benchmarks on three crowded datasets and achieves very competitive performance on two sparse datasets. An implementation of our method is available at https://github.com/dk-liang/AutoScale.git.
Introduction
Crowd counting has recently attracted great interest owing to its importance in a wide range of applications, e.g., video monitor (Kang et al. 2018), public security and city management (Sindagi and Patel 2018). Although the de facto CNN-based methods (Zhao et al. 2016; Zhang et al. 2016; Shi et al. 2019; Liu et al. 2019; Sam et la. 2017) have made significant progresses over traditional methods (Chan et al. 2008; Chen et al. 2012; Ge and Collins 2009; Idrees et al. 2015), it is still very difficult to accurately reason the count especially in dense regions where the crowd gathers. Whereas, the very crowded scene full with people is very common in real life, such as large gathering, train station, stadium.

In particular, most methods aim to estimate accurate and high-quality density maps that represent people through Gaussian blobs and can be integrated to obtain the final counts. However, there exist multiple Gaussian blob overlaps in dense regions, thus the density value of one pixel can be accumulated from many different nearby Gaussian blobs. Meanwhile, these accumulated density values are usually quite crucial to the final count yet hard to accurately predict. For instance, as shown in Fig. 1a, we can observe that the Gaussian blobs in the sparse region are separated and similar to each other, while gather and overlap in the dense region. Statistically, as shown in Fig. 1b, different from the distribution of density values in the sparse region, it presents a long-tailed shape in the dense region, which hinders the model from accurate density map prediction for the following reasons:

1) Open end of density values: The density values on dense regions are usually accumulated in multiple ways from nearby Gaussian blobs, leading to an open end of the density values in dense regions.

2) Density value imbalance: Pixels in the dense region usually have large density values and only occupy a small part of the whole density map, which poses density value imbalance. However, the count on dense regions is crucial for accurate crowd counting.

3) Density distribution gap: As show in Fig. 1b, there exists a huge density distribution gap between the sparse and dense regions. Besides, the density distributions of dense regions from different images also vary a lot because of variant value accumulations. This further gives rise to pattern shifts.

Fig. 1
figure 1
The intuitive and statistical distribution of the density map. a Intuitively, we can observe that the Gaussian blobs distribute separately and are similar to each other in the sparse region (e.g., within the green box), while variant overlaps exist in the dense region (e.g., within the red box). Re-scaling the dense region via closeness normalization alleviates the pattern shifts between dense and sparse regions. b Statistically, the dense region (e.g., in the red box) presents a long-tailed distribution of density values. The proposed AutoScale alleviates this issue and reduces the density value distribution gap between dense and sparse regions, thus facilitating density map regression on dense regions. Note that the rescaled GT is obtained using the predicted scale factor and the Y-axis of (b) is in logarithmic scale (Color figure online)

Full size image
In fact, both the pattern shift and long-tailed distribution bring challenge for accurate prediction in dense regions. In this paper, to the best of our knowledge, we are the first to try to mitigate such issue in density map for crowd counting. To this end, we propose a simple yet effective Learning to Scale (L2S) module to automatically learn reasonable scale factors, and then rescale the dense regions into similar and appropriate closeness levels reflecting image-plane distance between neighboring people. This helps to separate the overlapped blobs and decompose the original accumulated density values in density map. Therefore, normalizing the closeness alleviates the issue of pattern shift and long-tailed distribution by pattern normalization, hence facilitates the regression of density map. It is noteworthy to mention that since there is no ground-truth for the scale factor suggesting how much a given dense region should be zoomed ideally, the proposed L2S performs in an unsupervised clustering way via the center loss. An example showing the effect of the proposed L2S is illustrated in Fig. 1a, b. It can be observed that through L2S, the long-tailed density distribution and pattern shift are mitigated.

The localization-based method is currently attracting much attention recently. Instead of detecting each person, we propose to regard the local minima of the quantized distance (w.r.t. person location) map as the final person localization result. Specifically, we term such quantized distance map as distance-label map, which divides different distances into a number of categories, representing different distance ranges. The local minima of such distance-label map correspond to the localization of people. To the best of our knowledge, we are the first to leverage the local minima of such distance-label map for localization-based crowd counting. Similar to the density map representation, there exist class imbalance and distribution variances. This motivates us to employ L2S on distance-label map to separate the closed blobs and mitigate the distribution variances to improve the localization accuracy. Besides, we also design a customized dynamic cross-entropy (DCE) loss to guide the distance-label map learning. Specifically, different from the widely used static weighted cross-entropy loss, the weights are generated by the multiplication between the prediction possibilities and the absolute difference between predicted class and ground truth class, which is to dynamically change according to the prediction.

We leverage a FPN-like baseline model to frame both of our regression-based method and localization-based method with the proposed L2S module, which is termed AutoScale. Precisely, the baseline model provides an initial prediction, giving count for sparse regions and helping to automatically select a dense region for further refinement. The proposed L2S module generates an appropriate scale factor to rescale the selected dense region. A second count uses the same FPN-like model is performed on the rescaled dense region, and replaces the initial count on that region. We adopt the similar pipeline to the localization-based method by simply changing the output from the density map to the distance-label map.

Extensive experiments demonstrate that AutoScale outperforms some state-of-the-art methods on UCF-QNRF, JHU-Crowd++, NWPU-Crowd datasets for both regression-based and localization-based methods, and achieves very competitive performance on ShanghaiTech Part A and ShanghaiTech Part B dataset. On the extracted dense regions, where the accurate counting is very challenging, the proposed L2S achieves significant improvements. Besides, applying the L2S to some other popular crowd counting methods consistently improves their corresponding performance. Moreover, we also conduct the experiments on the TRANCOS dataset, further showing the superiority of the distance-label map with customized dynamic cross-entropy loss for localization-based method.

The main contribution of this paper lie in three folds: (1) we are the first to explore the long-tailed distribution of pixel-wise values in density map for crowd counting and propose the Learning to Scale (L2S) module to mitigate this issue. (2) We are -to the best of our knowledge- the first to localize people by finding the local minima of distance-label map in localization-based crowd counting. We also propose a novel customized dynamic cross-entropy loss, which takes advantage of the geometrical meaning of distance-label map. This mitigates the class imbalance and significantly improves the baseline performance of the localization-based method. (3) The proposed regression-based AutoScale (resp. localization-based AutoScale) based on a simple baseline consistently outperforms some regression-based (resp. localization-based) state-of-the-art methods on three public dense datasets and achieves very competitive performance on two sparse datasets. Besides, the proposed L2S is also helpful in improving the performance of some other popular methods.

The current paper extends the preliminary study of this work (Xu et al. 2019) in the following four major aspects:

First, we reformulate the intuition from the point of view of long-tailed distribution, which reveals better the mechanism of L2S in improving the counting accuracy. To the best of our knowledge, L2S is the first attempt that explicitly explores the pixel-level long-tailed distribution issue in density map regression and localization-based crowd counting.

Second, in addition to density map regression, we also adapt the proposed L2S to localization-based crowd counting. Specifically, we novelly leverage the distance-label map to count by localizing the local minima, and design the customized dynamic cross-entropy (DCE) loss. Sufficient experiments have been conducted to demonstrate that the proposed L2S is also effective on the localization-based method.

Third, we improve the original method in the conference version Xu et al. (2019) via some more reasonable designs (e.g., adaptive dense area selection instead of regular patches given by evenly dividing the image domain and the distance-based density measurement). The average distance between nearby persons is a more intuitive and efficient indication about how dense the crowd is.

Last, we conduct many more comparison experiments on more challenging datasets to demonstrate the effectiveness of the proposed L2S and DCE loss. We also now deeply analyze the effectiveness of L2S on dense regions and in combination with different baseline methods.

Related Work
We shortly review some related works on crowd counting in Sect. 2.1, other vision tasks involving scaling operation in Sect. 2.2 and long-tailed distribution problems in Sect. 2.3.

Crowd Counting
Current mainstreams for crowd counting consist of two kinds of methods, localization-based methods and regression-based methods. We shortly review some representative works in the following.

Regression-based methods Regression-based methods are existing mainstreams in crowd counting, thanks to the widely used density map. Before the era of deep learning, previous works (Chan et al. 2008; Chen et al. 2012; Ge and Collins 2009; Idrees et al. 2015; Liu and Vasconcelos 2015; Arteta et al. 2014) resort to different regression strategies, e.g., linear regression, Gaussian regression, ridge regression. Current regression-based methods (Zhang et al. 2016; Sam et la. 2017; Cao et al. 2018; Li et al. 2018; Sindagi and Patel 2017; Wang et al. 2019; Wan et al. 2019; Liu et al. 2019; Zhao et al. 2019, 2019; Lian et al. 2019; Jiang et al. 2019; Shi et al. 2018; Zhang et al. 2019) leverage CNNs to regress density maps, based on which not only the count but also the approximate distribution can be reasoned.

Though density map regression-based methods have achieved significant progress, there still exist several challenges, such as scale variations, perspective distortions, and noisy background interference. Multi-scale feature fusion (Zhang et al. 2016; Cao et al. 2018; Onoro-Rubio and López-Sastre 2016; Jiang et al. 2019; Sindagi and Patel 2019; Sindagi et al. 2020; Sindagi and Patel 2017; Ranjan et al. 2018; Zhang and Chan 2019; Liu et al. 2020) is an effective way to improve the ability of coping with different scales. AMSNet (Hu et al. 2020) relies on network architecture search (NAS) to automatically adopt a multi-scale architecture, addressing the scale variation issue in crowd counting. Some works (Kang and Chan 2018; Sajid et al. 2020) aim to scale the image/attended region according to fixed scale factors. The strategy of dividing and conquering is also applied to crowd counting. Sam et la. (2017) and Babu Sam et al. (2018) adopt the scale classifiers (Sam et la. 2017; Babu Sam et al. 2018) to predict the scale levels of different regions and further design models to separately deal with them. (Liu et al. 2018) attempt to count people in sparse regions through detection methods and regress density maps in dense regions. S-DCNet (Xiong et al. 2019) transforms the open-set problem into the close-set problem by dividing spatial planes on the feature map. Attention mechanism is another trend of to cope with spatial relations in crowd counting, such as self-attention (non-local) module (Zhang et al. 2019; Wan and Chan 2019) and other customized attention blocks (Zhang et al. 2019; Miao et al. 2020; Hossain et al. 2019; Liu et al. 2018, 2018; Miao et al. 2020).

Some auxiliary tasks are widely combined to improve density map estimation, e.g., foreground and background segmentation (Liu et al. 2019; Zhao et al. 2019; Jiang et al. 2019; Shi et al. 2019), depth predictions (Zhao et al. 2019; Lian et al. 2019), crowd velocity estimations (Zhao et al. 2016), uncertainty estimation (Oh et al. 2020), and target correction (Bai et al. 2020). The foreground masks (Liu et al. 2019; Zhao et al. 2019; Shi et al. 2019) or trimap (Arteta et al. 2016) given by thresholding the distance map are usually used for filtering out noisy predictions in the background so that the estimation biases are relatively removed and the predictions become more accurate. The depth estimations (Zhao et al. 2019; Lian et al. 2019) provide information about people scales, which are beneficial for density map estimation. (Liu et al. 2018, 2019) leverage the multi-task (learn to count and learn to rank) strategy to train the simple baseline (Simonyan and Zisserman 2015) and effectively facilitate the estimations of density maps. CAN (Liu et al. 2019), PACNN (Shi et al. 2019), 3DCC (Zhang and Chan 2020), PRNet (Yang et al. 2020), and PGCNet (Yan et al. 2019) extract perspective knowledge to help CNNs adapt to diverse scales. ASNet (Jiang et al. 2020) learns several extra masks of multiplication rates to automatically adjust the density estimation of each corresponding sub-region. HyGnn (Luo et al. 2020) leverages hybrid graph neural network to learn localization map as the auxiliary task to enhance density map prediction for crowd counting.

Besides the above model design mechanisms, the objective function is also an important direction. In particular, Bayesian loss (Ma et al. 2019) is proposed to regard the density map as a probability map and compute the probability of each pixel. Cheng et al. (2019) proposed Maximum Excess over Pixels (MEP) loss, which finds pixel-level region with high difference to the ground truth, and then the region is selected for optimization. DSSINet Liu et al. (2019) utilizes a Dilated Multiscale Structural Similarity (DMSSSIM) loss to produce locally consistent density maps.

Localization-based methods. Traditional methods for crowd counting try to count by detecting person faces (Chen et al. 2010; Zhao et al. 2009) or directly detecting pedestrians (Wang and Wang 2011; Viola et al. 2005; Brostow and Cipolla 2006; Rodriguez et al. 2011). Nevertheless, box annotations for detection are quite laborious especially in extreme dense regions. A compromised kind of annotation is to point out the exact location (e.g., center point of head) of each person. Such annotation in terms of individual points hinders the use of powerful object detection pipelines (Girshick 2015; Ren et al. 2015; He et al. 2017). Besides, detection methods usually suffer from severe occlusions in highly congested regions. Despite these difficulties, detection/localization methods have witnessed great progresses. Laradji et al. (2018) proposes a watershed split loss based on the distance map for separating nearby people. Ribera et al. (2019) propose a loss based on weighted Hausdorff distance (WHD) and formulate the object-localization problem as the minimization of distances between points. Liu et al. (2019) observe that zooming in (at a fixed rate) the dense region is effective for further localization. To tackle the shortage of the box annotations, Liu et al. (2019) and Sam et al. (2020) propose an impressive method to detect bounding boxes under the supervision of point-level annotations. Idrees et al. (2018) attempt to localize people based on local maxima of predicted density map with a small Gaussian kernel.

Our work is different from the above methods. We explore how to learn better density maps from the aspect of distribution of density values. We propose a novel L2S module to effectively alleviate the long-tailed density distribution in dense regions, helping to better regress density maps. We also demonstrate the effectiveness of proposed method in localizing people in dense regions by regarding the local minima of distance-label map as person localization result.

Scaling in Vision Tasks
Scaling plays an important role in many vision tasks, e.g., object detection (Singh and Davis 2018; Singh et al. 2018; Najibi et al. 2019) and fine-grained classification (Zheng et al. 2017; Recasens et al. 2018). Singh and Davis (2018) propose a method called scale normalization of image pyramid by selecting objects with relative similar scales. Instead of processing an entire image pyramid, SINPER Singh et al. (2018) processes context regions around ground-truth instances at the appropriate scale. Najibi et al. (2019) attempt to design an efficient algorithm to automatically focus on small objects that are usually hard to detect, then process them at finer scales. In the field of fine-grained classification, zooming in attended regions is an effective method to better recognize specific objects. For example, Zheng et al. (2017) propose an attention method to search for key regions with important features for specific fine-grained classes and zoom in the regions to see better. Similar to Zheng et al. (2017),Recasens et al. (2018) also attempt to find salient regions and zoom in them for better fine-grained object classification. It is noteworthy to mention that STN Jaderberg et al. (2015) also changes the original scales by learning parameters of affine transformation without specific supervision.

Most existing methods (Recasens et al. 2018; Jaderberg et al. 2015; Liu et al. 2019; Sajid et al. 2020; Kang and Chan 2018; Singh and Davis 2018; Singh et al. 2018) involving scaling operations mainly implicitly find attended or important regions and rescale them to regions of fixed size (or at a fixed zooming rate) for better detection or recognition. The proposed AutoScale differs from those existing methods in scaling purpose, motivation, and computation of scale factor. Typically, many works (Liu et al. 2019; Sajid et al. 2020; Kang and Chan 2018; Singh and Davis 2018; Singh et al. 2018) aim to scale the image according to fixed scale factors in order to have a more accurate attention. AutoScale aims to explicitly select dense regions and then rescale them into similar closeness levels, thus alleviating the long-tailed issues in dense regions.

Fig. 2
figure 2
The diagram of the proposed AutoScale. There are two major modules: (1) the basic counting module 𝑓𝜃 dedicated for both initial prediction 𝑌𝑖 on original image I and re-prediction 𝑌′𝑑 on the rescaled dense region 𝐼′𝑑 for both regression-based and localization-based counting; (2) the L2S module that generates an appropriate scale factor 𝑟𝑑 to rescale the selected dense region 𝑅𝑑 for refining count on the dense region 𝑅𝑑. The final count 𝑁𝑓 is composed of initial count on sparse regions 𝑁𝑠=𝑠𝑢𝑚(𝑌𝑖)−𝑠𝑢𝑚(𝑌𝑑), where 𝑌𝑑 is the initial prediction on the selected dense region 𝑅𝑑, and re-predicted count on the selected dense region 𝑠𝑢𝑚(𝑌′𝑑)

Full size image
Long-Tailed Distribution in Vision Tasks
Long-tailed distribution is extremely common in natural data, and has been extensively studied (Zhu et al. 2014; Salakhutdinov et al. 2011; Zhu et al. 2016; Ouyang et al. 2016; Van Horn and Perona 2017; Cui et al. 2018; He and Garcia 2009; Geng et al. 2020). It is very classical He and Garcia (2009) to mitigate the long-tailed distribution by under-sampling the major classes, over-sampling the minor classes and re-weighting the data. Most recent works mainly focus on improving objective functions (Lin et al. 2017; Zhang et al. 2017; Oh Song et al. 2016; Cao et al. 2019), training strategies Dong et al. (2017) and model design (Wang et al. 2017; Ha et al. 2016). The improvement by studying the long-tailed problem mainly exist in the current mainstreaming vision applications, such as recognition, object detection and segmentation.

There still exist severe long-tailed distribution issue yet rarely explored in crowd counting, especially for the aspect of density map. The most related work to our proposed method is S-DCNet (Xiong et al. 2019), which also aims to tackle the open-set problem in crowd counting by dividing the spatial planes to maintain the count under an closed range. Although S-DCNet (Xiong et al. 2019) also explores the long-tailed issue in crowd counting, it is quite different with our AutoScale. In fact, we explore the pixel-level long tailed distribution of density values caused by overlapped Gaussian blobs on the highly congested regions of the density map. Whereas, S-DCNet (Xiong et al. 2019) addresses the long-tailed distribution of image/patch-level people number. Besides, dividing the spatial planes is hardly to cope with the long-tailed distribution resulting from the accumulations of pixel value. Different from this, we propose L2S to separate the overlapped blobs and decompose the original accumulated values, which is dynamical and with less hand-craft operations. Furthermore, the proposed method is also beneficial to improve the localization accuracy for methods based on distance label maps.

Learn to Scale for Crowd Counting
Overview
The long-tailed distribution in density map poses great challenges to crowd counting, yet few works focus on mitigating it. Specifically, the accumulated density values in dense regions result in the open end, value imbalance and huge distribution gap between dense and sparse regions. Nevertheless, these density values in dense regions are quite crucial to the final prediction.

Consequently, we propose a learning to scale (L2S) module, acting as an unsupervised clustering that leverages the center loss to rescale all dense regions into similar and reasonable closeness levels, which mitigates the open end, transforms the distributions into as similar as sparse regions and reduces the distribution gap between dense regions. We build the framework using a FPN-like baseline network with L2S to demonstrate the effectiveness of the proposed method. We first apply such L2S module into the baseline model that regresses density map and demonstrates the effectiveness of the proposed L2S. Then we simply change the output into distance-label map that can be used for counting by localization, which further demonstrates the effectiveness of L2S on localization-based method. Besides, we propose a novel dynamic cross-entropy loss for distance-label map to better learn the distance-label map representation, further boosting the localization accuracy. Note that we provide two kinds of methods, regression-based AutoScale and Localization-based AutoScale, which share similar networks but are independent with each other and used for counting by regression and counting by localization, respectively.

Both the regression and localization-based AutoScale with L2S are end-to-end trainable. The overall framework is depicted in Fig. 2. The whole pipeline consists of two parts: (1) Counting network based on a widely used backbone (e.g., FPN Lin et al. (2017) in this paper) for the estimations of density maps and distance-label maps; (2) L2S dedicated for generating appropriate scale factors for selected dense regions. We will detail the proposed methods in the following.

Fig. 3
figure 3
The density maps with different Gaussian kernels and the corresponding distributions of density values

Full size image
Analysis on Density Map Representation
Most current crowd counting methods rely on density map regression to count people. However, few works aim to explore the distribution of pixel values in density map. Generally, most recent crowd counting datasets provide point-level annotations, which can be represented as binary maps B. For each pixel in the image domain 𝑝∈𝛺, we have 𝐵(𝑝)=∑𝑃𝑖=1𝛿(𝑝−𝑝𝑖), where each head position 𝑝𝑖 is modeled as a delta function 𝛿(𝑝−𝑝𝑖), and P refers to the total number of people in the image. The density map D on each pixel p is then generated by convolving B(p) with a Gaussian kernel G (Zhang et al. 2016): 𝐷(𝑝)=∑𝑃𝑖=1𝛿(𝑝−𝑝𝑖)∗𝐺. Meanwhile, the kernel size of G is a key hyper-parameter for crowd counting (Zhang et al. 2016; Wan and Chan 2019).

In particular, a larger kernel size is prone to bring in more overlaps, therefore the pixel accumulations occur more, while a smaller kernel size leads to more severe value imbalances. Although adaptive Gaussian kernel proposed in Zhang et al. (2016) can reduce the overlaps of Gaussian blobs by setting the kernel size according to the K nearest neighborhood distance, the distances are quite variant, making an open end distribution. An example is presented in Fig. 3. Except for Gaussian kernel equal to 1, others present the long-tailed distribution because of multiple pixel accumulations from the overlaps of Gaussian blobs. Yet, as for the Gaussian kernel equal to 1, there exist severe pixel value imbalance, which is also difficult for CNNs to learn. Kernel generator Wan and Chan (2019) is a promising method to cope with the dilemma of kernel selection. However, it is not explicit to deal with the distribution of the density values.

To the best of our knowledge, we are the first to explore how to better learn the density map from the aspect of pixel value distribution and propose the Learn to Scale (L2S) to dynamically change the distribution of density map in the dense regions, which makes the model be able to better learn and represent the density map during both training and inference phase.

Learn to Scale
Learning to scale aims to modulate dense regions of different scales to similar and appropriate closeness levels, which tackles the problem of long-tailed distribution for density map. Precisely, for a given region R, we define a closeness level S for R via ground-truth as

𝑆=∑𝑃𝑅𝑖=1𝑑𝑖𝑃𝑅,
(1)
where 𝑑𝑖 denotes the distance between i-th person and its nearest person in R, and 𝑃𝑅 stands for the overall number of people in the region R. Note that different from the conference version (Xu et al. 2019) that relies on the average people number to define the density level, we now leverage the average distance. Actually, dense (resp., sparse) region intrinsically contains people that lie very close to each other (resp., far away from each other). Therefore, the closeness level for both dense and sparse region could be naturally measured by the average distance between each person and its nearest person within the region. More importantly, different from patch-wise and pixel-wise crowd number, the average distance is not influenced by the area without people, the closeness level defined in Eq. (1) is thus more reliable and intuitive.

For a given dataset consisting of 𝑁𝑅 regions to be rescaled, we target scaling each region of interest 𝑅𝑖(𝑖=1,2,…,𝑁𝑅) such that the closeness level of each rescaled region approaches a similar scale 𝑆⎯⎯⎯⎯. For that, we need to generate an appropriate scale factor 𝑟𝑖 for each region 𝑅𝑖. However, there is no explicit target scale factor suggesting how much the region 𝑅𝑖 should be zoomed and also no target similar closeness level 𝑆⎯⎯⎯⎯ indicating which should be approached. Therefore, we propose a learn to scale module that acts as an unsupervised clustering by using the center loss on closeness levels.

Specifically, for each region 𝑅𝑖(𝑖=1,2,…,𝑁𝑅) to be rescaled, we attempt to generate a corresponding scale factor 𝑟𝑖. We apply a simple CNN consisting of three 3×3 convolutional layers and two fully connected layers on the backbone feature of region 𝑅𝑖 to produce 𝑟𝑖, which is then used to rescale the region 𝑅𝑖 via bilinear upsampling. Such L2S module is learned using the following training objective in terms of the center loss on closeness levels

𝐿𝑠=12∑𝑖=1𝑀‖‖𝑆𝑖×𝑟2𝑖−𝑆⎯⎯⎯⎯‖‖22,
(2)
where M refers to the number of dense regions in every T iterations for parameter updating and 𝑆𝑖 refers to the closeness level of region 𝑅𝑖 following Eq. (1). Note that we limit the scale factor 𝑟𝑖 between 0.5 and 3 to avoid the degenerated solution of 𝑟𝑖=0 and 𝑆⎯⎯⎯⎯=0 and potential image distortion (for too large scale factors). Meanwhile, we allow 𝑟𝑖<1 to make the zooming-out regions also participate in the training of center loss with more samples. It is also noteworthy to mention that we leverage the STN Jaderberg et al. (2015) to achieve the scale operation. Specifically, the scale factor of the affine matrix in the STN is set to our learned rescale factor, making the whole pipeline differentiable. Thus, the gradients are back-propagated through 𝑟𝑖 as well as 𝑆⎯⎯⎯⎯. The derivative of 𝐿𝑠 with respect to 𝑟𝑖 is given as follows:

∂𝐿𝑠∂𝑟𝑖=2𝑆𝑖(𝑆𝑖×𝑟3𝑖−𝑆⎯⎯⎯⎯×𝑟𝑖).
(3)
The center of closeness level 𝑆⎯⎯⎯⎯ is also learnable rather than manually set. We first randomly initialize 𝑆⎯⎯⎯⎯. Then, we follow the standard process of updating the center:

𝛥𝑆⎯⎯⎯⎯𝑡=∑𝑀𝑖=1(𝑆⎯⎯⎯⎯𝑡−𝑆𝑖×𝑟2𝑖)1+𝑀,𝑆⎯⎯⎯⎯𝑡+𝑇=𝑆⎯⎯⎯⎯𝑡−𝛼⋅𝛥𝑆⎯⎯⎯⎯𝑡,
(4)
where 𝛼 is the learning rate for updating the center.

The L2S is a simple yet effective module that can improve the performance of both regression-based method and localization-based method for counting in dense regions. We detail the proposed regression-based and localization-based AutoScale using L2S for crowd counting in the following.

Regression-Based Counting
Regression model: We first frame AutoScale in a simple manner for density map regression. Precisely, following previous works (Li et al. 2018; Liu et al. 2018), we adopt a simple VGG16-based FPN (Lin et al. 2017) as the backbone network and discard the last pooling layer and all following fully connected layers, as well as the pooling layer between stage4 and stage5 to preserve sufficient spatial information for accurate counting. As shown in Fig. 2, the FPN-based backbone for counting first generates an initial density map where the estimation is relatively accurate in the sparse regions. Yet, for the dense regions, it is usually difficult to accurately regress the density maps. We propose to apply the L2S module on the dense regions to rescale all dense regions into similar closeness levels, so that the accumulated pixel values are decomposed and the distribution is transformed to be similar. To this end, we first threshold the initially predicted density map with twice of its mean density on the whole image, yielding a set of connected regions having density larger than twice of the mean value. Then we box out the maximum connected region as the selected dense region 𝑅𝑑 for each image.

We then re-estimate the density map for the selected dense region 𝑅𝑑. For that, we crop the backbone feature on 𝑅𝑑 and pool the cropped feature to the size of 14×14, which is then fed into the L2S module. The L2S module generates a scale factor 𝑟𝑑 to rescale the dense region 𝑅𝑑. We re-estimate the density map for 𝑅𝑑 by applying the same counting network sharing parameters with the initial prediction on the rescaled region 𝐼′𝑑.

The final count is made of initially predicted density map and the re-predicted density map. Specifically, as shown in Fig. 2, the final count 𝑁𝑓 is given by the sum of two counts: 𝑁𝑓=𝑁𝑠+𝑠𝑢𝑚(𝑌′𝑑), where 𝑁𝑠 is the count on the sparse region (image domain excluding the selected dense region) and 𝑠𝑢𝑚(𝑌′𝑑) is the refined count on the rescaled dense region 𝐼′𝑑. 𝑁𝑠 is given by the original count on the whole image 𝑠𝑢𝑚(𝑌𝑖) minus the original count on the selected dense region 𝑠𝑢𝑚(𝑌𝑑). For the regression model, the count sum(Y) is obtained by integrating over the estimated density map. Note that if the size of the maximal region is smaller than a proportion 𝐽𝑟 of the input image size, no dense region is selected, implying that the underlying image mainly contains sparse regions for which the initial density map prediction is accurate enough.

Training objective for regression model In the training phase of regression-based counting model, we follow previous density map regression methods to train the counting network. Specifically, we adopt Mean Square Error (MSE) loss function to optimize the counting network, given by

𝐿𝑚=||𝐷−𝐷̂ ||2,
(5)
where D and 𝐷̂  are the ground-truth and predicted density map, respectively. It is noteworthy to mention that we adopt an online ground-truth update for the density map re-prediction on selected dense regions. Precisely, we first rescale the binary annotation in terms of points on selected dense region by multiplying the original annotated dot coordinates with the learned scale factor. Then we regenerate its corresponding ground-truth density map on the rescaled binary map using the same Gaussian kernel as for the initial ground-truth density map on the whole image.

The total training objective 𝐿𝐷 for optimizing the whole density regression-based model is given by

𝐿𝐷=𝐿𝑖𝑚+𝐿𝑑𝑚+𝜆1×𝐿𝑠,
(6)
where 𝐿𝑖𝑚 and 𝐿𝑑𝑚 stands for the MSE loss for the initial prediction and re-prediction on the selected dense region using Eq. (5), 𝐿𝑠 is the center loss (see Eq. (2)) involved in optimizing the L2S module, and 𝜆1 is a hyper-parameter.

Fig. 4
figure 4
Ground-truth distance-label map generation based on distance transformation from original binary location annotation B. The distance-label map is obtained by dividing different distances of distance map into different classes

Full size image
Localization-Based Counting
Though regression-based counting provides accurate count, it does not indicate the exact person locations. Whereas, the location information is also important in many applications such as person tracking and general crowd analysis. In this section, we detail the proposed localization-based counting using a distance-label map (learned with a novel dynamic cross-entropy loss) to represent person head locations. More specifically, we first transform the binary head location annotation into a distance map through distance transformation. Then we divide the range of distances into different categories, resulting in a distance-label map (see Fig. 4). The head locations correspond to local minima of such distance-label map. In consequence, we frame the localization-based counting problem as a dense pixel-wise classification problem, which is similar to semantic segmentation. We also resort to L2S to improve the localization accuracy in dense regions.

Generation of Distance-Label Map We first apply the distance transformation Baxes (1994) on the original annotation in terms of binary head location map. Then we classify the obtained distance map into a distance-label map C by assigning different distance ranges to different classes. In this paper, the total number of distance classes 𝑁𝑐 is set to 11. An example of generating such distance-label map C is shown in Fig. 4. The label in each pixel defines a distance level respect to its nearest head locations. Note that pixels near head locations have smaller distance label and pixels far away from head locations have larger distance label, ensuring that the distance-label blobs represent different heads without overlaps.

Different from the existing method Idrees et al. (2018) that localizes Gaussian blob local maxima where the Gaussian blobs are blur and the localization is interfered by the severe overlaps between nearby heads, the adopted distance-label maps are more discriminative and there is no overlap between nearby heads. On the other hand, compared to the binary classification method Liu et al. (2019) that directly localizes the head position, distance-label maps also roughly provides information on the closeness level via the geometrical meaning of each class, which indicates the approximate distance to the corresponding nearest head. It is noteworthy that Olmschenk et al. (2019) also leverages the principle of distance transformation by generating iKNN maps for crowd counting. Specifically, iKNN makes use of the distance transformation for regressing a counting value. We are the first to perform person localization via distance-label map, which quantizes the continuous distance map into the discrete distance-label map.

Fig. 5
figure 5
Qualitative illustration of the purpose for DCE loss. a represents a local blob of distance-label map with sequential labels. b–d are different predicted examples with the same predicted probability on each ground-truth label. The predicted result in (b) and (c) have smaller DCE loss than (d), and preserve better the geometry structure of (a)

Full size image
Localization Model We adopt the similar pipeline (see Fig. 2) described in Sect. 3.4 for regression-based counting. We simply change the output target from density maps to distance-label maps. The counting network is responsible for classifying each pixel into different class labels, which provides an initial distance-label map. Similarly, this initial distance-label map prediction is accurate in sparse regions, but has difficulty in dense regions. In fact, since nearby heads in dense regions lie very close to each other, making the predicted labels prone to be the same, which hinders the accurate localization via local minima of distance-label maps. To address this, we also leverage the proposed L2S. Specifically, we threshold the initially predicted distance-label map by selecting pixels with class labels smaller than c (set to 8), forming a set of candidate dense regions. We select the maximal connected region and regard its bounding box as the dense region 𝑅𝑑 for the underlying image. Similarly to the above regression-based counting with L2S, we crop the backbone feature on 𝑅𝑑 and resize it to the spatial size of 14×14. The L2S takes the resized feature and outputs a scale factor 𝑟𝑑 for rescaling 𝑅𝑑. The rescaled region image 𝐼′𝑑 is then fed into the same counting network sharing parameters with initial distance-label map prediction, leading to a re-predicted distance-label map for the selected dense region 𝑅𝑑.

The final output is given by the sum of number of local minima in initially predicted distance-label map on sparse regions and the number of local minima in re-predicted distance-label map on the selected dense region. Note that we also discard the selected dense region whose size is smaller than a proportion 𝐽𝑙 of the input image size.

Table 1 Settings of all involved hyper-parameters for the regression-based and localization-based AutoScale with L2S
Full size table
Training Objective for Localization Model In the training phase of localization-based counting model, we propose a dynamic cross-entropy loss function to optimize localization-based counting network. Specifically, the counting network outputs a 𝑁𝑐 channel probability map Pr that classifies each pixel into a corresponding label category. We weight the cross-entropy loss based on the probability of each label category and the corresponding absolute difference with the ground-truth label value. The dynamic cross-entropy loss 𝐿𝑐𝑒 for distance-label map classification is given by

𝐿𝑐𝑒=−∑𝑝∈𝛺(∑𝑖=0𝑁𝑐−1(|𝐶(𝑝)−𝑖|+1)×𝑃𝑟(𝑝)𝑖)×𝑙𝑜𝑔(𝑃𝑟(𝑝)𝐶(𝑝)),
(7)
where 𝑃𝑟(𝑝)𝑖 denotes the the predicted probability of pixel p belonging to the i-th class, and 𝑃𝑟(𝑝)𝐶(𝑝) is the predicted probability of pixel p being the ground-truth class C(p). The dynamic cross-entropy loss in Eq. (7) explicitly makes use of the distance between the prediction and the ground-truth. Indeed, each distance-label category has explicit geometrical meaning, implying the relative distance to the annotated dots. The corresponding absolute difference between each predicted label class and the ground-truth label value also measures how far is the prediction to the ground-truth. The relative error between the predicted and GT labels roughly reflects the relative difference between the predicted and GT distance to dots. The weighting mechanism in the DCE loss penalizes large relative difference, preserving better the geometry structure of GT distance-label map. For example, as shown in Fig. 5, a local blob is represented as sequential ordinary labels, i.e., 0, 1, 2, 3 ... (starting from the center). If the pixel of ‘2’ is predicted as ‘1‘ or ‘3‘, the localization of local minima is not influenced. Nevertheless, if the pixel of ‘2‘ is predicted as ‘7’, it will introduce a false local minimum. Therefore, we propose to penalize more for larger difference, helping to preserve the geometry structure of the distance-label map, and providing thus more accurate localization result. It is noteworthy that for the re-prediction on selected dense regions, the ground-truth distance-label maps are regenerated online for the rescaled dense regions, in the similar way as regenerating rescaled ground-truth density maps. This ensures that the close and similar distance labels on selected dense regions can be distinguished after rescaling.

The total training objective 𝐿𝐶 for optimizing the whole localization-based model based on distance class label map representation is given by

𝐿𝐶=𝐿𝑖𝑐𝑒+𝐿𝑑𝑐𝑒+𝜆2×𝐿𝑠,
(8)
where 𝐿𝑖𝑐𝑒 and 𝐿𝑑𝑐𝑒 refers to the dynamic cross-entropy loss for the initial distance-label map prediction and re-prediction on the selected dense region using Eq. (7), 𝐿𝑠 is the center loss for scale factor learning given by Eq. (2) optimizing the L2S module, and 𝜆2 is a hyper-parameter.

Experiments
Datasets and Evaluation Protocol
We conduct experiments on the JHU-Crowd++ Sindagi et al. (2020), the NWPU-Crowd (Wang et al. 2020), the UCF-QNRF (Idrees et al. 2018) as well as the ShanghaiTech (Zhang et al. 2016) Part A and Part B datasets to demonstrate the effectiveness of both the proposed regression-based and localization-based crowd counting with L2S. Besides, we also conduct an experiment on the TRANCOS (Guerrero-Gómez-Olmedo et al. 2015) dataset using the proposed distance-label map representation with dynamic cross-entropy loss, demonstrating its superiority in vehicle localization and counting.

NWPU-Crowd (Wang et al. 2020) is currently the largest existing congest dataset with 2,133,238 annotations, containing 3109 training images, 500 val images and 1500 test images. We present the result by the provided online evaluation benchmark website.

JHU-Crowd++ (Sindagi et al. 2020) is an extension of JHU-Crowd Sindagi et al. (2019) containing 2722 training images, 500 validation images, and 1600 test images, which is collected from diverse scenarios and weather conditions. Besides, the dataset provides rich annotations, including image-level, head-level and point-level annotations. The total number of people in each image ranges from 0 to 25791.

UCF-QNRF (Idrees et al. 2018) is a challenging and dense dataset, containing 1201 training and 334 test high-resolution (up to 9000×6000) images. The scales of the people in this dataset vary significantly. The total number of people in each image ranges from 49 to 12865.

ShanghaiTech (Zhang et al. 2016) consists of Part A and Part B with a total number of 1198 images. Images in Part A are scrawled from the internet, and are of different scenes and significantly varied densities. Part A is split into 300 training images and 182 test images. Part B is taken from the metropolis in Shanghai city, containing 400 images for training and 316 images for testing.

TRANCOS (Guerrero-Gómez-Olmedo et al. 2015) is a vehicle counting benchmark dataset containing 1244 low resolution images captured by the publicly available video surveillance cameras in Spain. The dataset provides the split of training, validation, and test.

Counting evaluation metrics We follow standard metrics widely adopted in previous works to evaluate the proposed AutoScale, including mean average error (MAE) and root mean squared error (MSE) which are defined as

𝑀𝐴𝐸=1𝑁𝐼∑𝑖=1𝑁𝐼|𝑃𝑖−𝑃𝑖^|,
(9)
𝑀𝑆𝐸=1𝑁𝐼∑𝑖=1𝑁𝐼|𝑃𝑖−𝑃𝑖^|2‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾⎷,
(10)
where 𝑁𝐼 denotes the number of total images in a dataset, 𝑃𝑖 and 𝑃̂ 𝑖 are the ground-truth and predicted number of people in each image.

Localization Evaluation Metrics We calculate the Precision, Recall, and F-measure metrics to evaluate the localization performance. Specifically, when the distance between a given predicted point 𝑝𝑝 and ground truth point 𝑝𝑔 is less than a distance threshold 𝜎, it means that 𝑝𝑝 and 𝑝𝑔 are successfully matched. For ShanghaiTech Part A, we calculate the localization metrics with four different 𝜎 (4, 8, 16, and KNN distance). For the UCF-QNRF dataset, we report the average precision, average recall, and average F-measure at different distance tolerance thresholds 𝜎: (1,2,3,…,100) pixels, following (Idrees et al. 2018). For the NWPU-Crowd dataset (Wang et al. 2020), we follow (Wang et al. 2020) that chooses two adaptive thresholds 𝜎𝑠=𝑚𝑖𝑛(𝑤,ℎ)/2 and 𝜎𝑙=𝑤2+ℎ2‾‾‾‾‾‾‾‾√/2 for each individual person, where w and h is the width and height of the annotated bounding box for the corresponding person. Note that we use the implementation codeFootnote1 provided by Wang et al. (2020) to compute the localization evaluation metrics for all datasets.

Implementation Details
For NPWU-CROWD, JHU-Crowd++, and UCF-QNRF dataset, we resize all the images by setting the longer side size to 2048 while maintaining the corresponding ratio. The original image size is used on the ShanghaiTech Part A and Part B dataset. We follow the procedure described in Sect. 3.2 to generate ground-truth density maps for the regression-based AutoScale with different spread parameters, which are set to 4, 8, 8, 6, and 8 on the ShanghaiTech Part A, Part B, UCF-QNRF, NWPU-Crowd, JHU-Crowd++ datasets, respectively. For the localization-based AutoScale, we generate the ground-truth distance-label maps as described in Sect. 3.5 and depicted in Fig. 4 for all involved datasets.

During the training phase, original or downsampled images are fed to the network with data augmentations, including horizontal flipping, adding noise, and random scaling. The settings for all involved hyper-parameters are depicted in Table 1. Specifically, the area ratio threshold J for discarding small dense regions is set to 0.1 and 0.02 for the regression and localization-based AutoScale, respectively. The weight of the center loss 𝐿𝑠 on scale factors 𝜆1 in Eq. (6) and 𝜆2 in Eq. (8) involved in training objective for regression-based and localization-based AutoScale are both set to 1. The parameters in L2S are initialized with Gaussian random values and are updated each epoch.

We use Adam (Kingma and Ba 2014) to optimize both training objectives with batch size set to 1. The weight decay is set to 5×10−4. The learning rate 𝛼 for updating the center in Eq. (4) is set to 10−3. The learning rate 𝛾 for the whole network is set to 10−7 and 10−4 for the regression-based and localization-based AutoScale, respectively. During the test phase, we use the same area ratio threshold J as the training phase. The proposed AutoScale is implemented in Pytorch. All experiments are carried out on a workstation with an Intel Xeon 16-core CPU (3.5GHz), 32GB RAM, and a single Tesla V100 GPU.

Table 2 Quantitative results on val/test set of NWPU-Crowd dataset. Scene level is divided into five different ranges:0, (0,100], (100,500], (500,5000], and >5000. L0, L1 and L2 denote three luminary levels respectively:[0,0.25], (0.25,5], (0.5,0.75]. Bolditalic, italics, and bold respectively indicate the first, second and third place in this table. The methods with ∗ denotes the localization-based methods
Full size table
Table 3 Quantitative results on the val/test sets of JHU-Crowd++ dataset. “Low”, “Medium” and “High” respectively indicates three categories based on different ranges:[0,50], (50,500], and >500. Bolditalic, italics, and bold respectively indicates the first, second and third place in this table. The methods with ∗ denote the localization-based methods
Full size table
Fig. 6
figure 6
Qualitative visualization of density maps generated by the proposed AutoScale. From left to right: original images, ground-truth density maps, baseline results, and results with L2S. The enclosed regions are the automatically selected dense regions, which are rescaled via L2S for re-prediction

Full size image
Table 4 Quantitative comparison (in terms of MAE and MSE) of the regression-based AutoScale and localization-based AutoScale with state-of-the-art methods on three widely adopted benchmark datasets. The methods with ∗ represents localization-based methods. Bolditalic, italics, and bold respectively indicate the first, second and third place in this table
Full size table
Fig. 7
figure 7
Qualitative visualization of distance-label maps given by the proposed AutoScale. From left to right: original images, ground-truth distance-label maps, baseline results, and results with L2S. The enclosed regions are the automatically selected dense regions, which are rescaled via L2S for re-prediction

Full size image
Fig. 8
figure 8
Qualitative visualization of detected person locations by the localization-based AutoScale. Red points are the ground-truth. To more clearly present our localization results, we generate bounding boxes (green boxes) according to the KNN distance of each point, which follows and compares with LSC-CNN (Sam et al. 2020) (Color figure online)

Full size image
Table 5 Quantitative evaluation of localization-based methods on the ShanghaiTech Part A dataset using Precision (P), Recall (R), and F-measure (F) at different distance tolerance thresholds 𝜎 for one-to-one correct match between the predicted and ground-truth localization result
Full size table
Experimental Comparison
Regression-Based Crowd Counting Result We first evaluate the regression-based AutoScale using L2S. Figure 6 presents some sample qualitative density maps. Qualitatively, the proposed L2S helps to improve the density map prediction on dense regions, boosting the count accuracy. The quantitative comparison with state-of-the-art methods on NWPU-Crowd, JHU-Crowd++, ShanghaiTech, and UCF-QNRF datasets is depicted in Tables 2, 3 and 4, respectively.

Our VGG16-based AutoScale outperforms the other VGG16-based methods (including VGG16-based CG-DRCN Sindagi et al. (2020)) on JHU-Crowd++ and NWPU-Crowd dataset, and is very competitive to the other methods including most VGG16-based related CVPR/ECCV/AAAI 2020 methods on UCF-QNRF, ShanghaiTech Part A and ShanghaiTech Part B datasets. Specifically, AutoScale has superior performance on the extremely dense set, e.g., the “High” part of JHU-Crowd++ dataset and the “S4” in NWPU-Crowd dataset, which demonstrates the effectiveness of the proposed L2S to dense regions in crowd counting. On the relative sparse datasets such as ShanghaiTech Part A and Part B, L2S still improves the baseline model significantly.

It is noteworthy that the state-of-the-art methods BL (Ma et al. 2019) and CG-DRCN (Sindagi et al. 2020) leverage VGG19 and ResNet101 as the backbone, which is stronger than our adopted VGG16 backbone on all datasets. For a fair comparison on the JHU-Crowd++ dataset, We also implement our AutoScale with VGG19 backbone on the JHU-Crowd++ dataset, and observe that the VGG19-based AutoScale achieves 65.9 MAE and 264.8 MSE on the JHU-Crowd++ dataset, improving VGG19-based BL (Ma et al. 2019) (resp. ResNet101-based CG-DRCN Sindagi et al. (2020)) by 9.1 MAE (resp. 5.1) and 35.1 (13.8) MSE. Besides, as depicted in Tables 2 and 4, despite the less strong backbone network, our VGG16-based AutoScale outperforms VGG19-based BL (Ma et al. 2019) and ResNet101-based CG-DRCN (Sindagi et al. 2020) by 1.2 (resp. 7.0) and 8.0 (resp. 16.5) MAE (resp. MSE) on the UCF-QNRF dataset, respectively, and improves VGG19-based BL (Ma et al. 2019) and ResNet101-based SFCN†(Wang et al. 2019) by 11.3 (resp. 66.0) and 11.6 (resp. 35.9) MAE (resp. MSE) on the NWPU-Crowd dataset, respectively. On the UCF-QNRF dataset, AutoScale performs slightly worse than AMRNet (Liu et al. 2020) and worse than ADSCNet (Bai et al. 2020). Compared with AMRNet which adopts some specific designs such as multi-scale fusion, and multi-activation fusion, AutoScale is simply to automatically zoom in the dense regions for refinement without other tricks, and achieves slightly worse MAE than AMRNet (Liu et al. 2020) (87.5 vs 86.6 MAE) but better MSE (147.8 vs 152.2 MSE). Compared with the ADSCNet (Bai et al. 2020) which proposes the adaptive dilations and ground-truth correction mechanism (bringing 18.8 MAE improvement), AutoScale targets at a different perspective, i.e., a general module to refine the prediction on the dense region.

It is also noteworthy that the proposed method performs better on dense crowds than sparse ones, which further confirms that the learning to scale module works well. Since the proposed method is dedicated to improving the counting accuracy on dense regions causing long-tailed distribution problem, it is reasonable that the performance improvement on the sparse dataset is not as good as that on dense datasets. Overall, the proposed method improves the performance on very crowded scenes (such as large gatherings, train stations, stadiums), and does not harm the performance on sparse scene that does not suffer from long-tailed distribution issue.

Table 6 Quantitative evaluation of localization-based methods on the UCF-QNRF dataset. We report the average precision, average Recall, and average F-measure at different distance thresholds 𝜎: (1,2,3,…,100) pixels
Full size table
Table 7 Quantitative comparison of the localization performance on the NWPU-Crowd dataset in terms of precision (P), recall (R), and F-measure (F) using different adaptive distance tolerance thresholds 𝜎𝑙 and 𝜎𝑠 following (Wang et al. 2020)
Full size table
Localization-Based Crowd Counting Result We then evaluate the localization-based AutoScale using L2S. Some qualitative results in terms of distance-label maps are illustrated in Fig. 7. We also show some examples of person localization results in Fig. 8. We present the bounding box on each location of people, generating from the KNN distance of each predicted local minima, which is similar as LSC-CNN Sam et al. (2020). Comparied with LSC-CNN (Sam et al. 2020), AutoScale gives competitive bounding boxes and has better counting by localization performance in dense crowds. Qualitatively, the proposed distance-label map representation combined with the introduced dynamic cross-entropy loss is effective for localizing people even in dense regions. The proposed L2S effectively improves the localization and thus counting accuracy. The quantitative comparison with some other detection/localization-based methods is also depicted in Tables 2, 3 and 4, respectively. The localization-based AutoScale consistently outperforms state-of-the-art methods thanks to the L2S, the proposed localization method based on the local minima of distance-label map, and DCE loss on the ShanghaiTech Part A, UCF-QNRF, JHU-Crowd++ and NWPU-Crowd datasets. We have comparable performance with LSC-CNN (Sam et al. 2020) on the ShanghaiTech Part B dataset.

To further demonstrate the effectiveness of the localization-based AutoScale, following (Wang et al. 2020), we evaluate the accuracy of localization using the common metrics in terms of precision, recall, and F-measure on the ShanghaiTech Part A, UCF-QNRF, and NWPU-Crowd dataset. The corresponding localization-based evaluation result is displayed in Tables 5, 6, and 7, respectively.

For the ShanghaiTech Part A, as depicted in Table 5, the proposed method improves the state-of-the-art method LSC-CNN (Sam et al. 2020) by 22.6% F-measure for the most strict setting 𝜎=4, and consistently improves upon the other methods for the less strict settings.

For the dense dataset UCF-QNRF, as shown in Table 6, our localization-based AutoScale outperforms the state-of-the-art method LSC-CNN (Sam et al. 2020) by 3.17% F-measure.

Finally, we compare the proposed method with some baselines such as (Ren et al. 2015; Hu and Ramanan 2017; Gao et al. 2019; Liu et al. 2019) on the new released large-scale localization benchmark dataset NWPU-Crowd. As illustrated in Table 7, it can be observed that the proposed method largely improves the popular detection baseline Faster RCNN (Ren et al. 2015) and TinyFaces (Hu and Ramanan 2017) by distinguishing margins in terms of both F-measure for localization and in terms of MAE/MSE for counting, even though they use a stronger backbone. Compared with the methods (Gao et al. 2019; Liu et al. 2019) dedicated for localization-based crowd counting, our localization-based AutoScale also outperforms them by at least 2.2% for 𝜎𝑙 (2.7% for 𝜎𝑠) F-measure.

Counting and Localization Result on the Extracted Dense Regions We also compare the performance of baseline and AutoScale on the extracted dense regions on the Shanghaitech Part A, Part B, UCF-QNRF, JHU-Crowd++, and NWPU-Crowd dataset. Since NWPU-Crowd does not release the ground-truth for the test set, we report results on the val set instead of test set for the other four datasets. As shown in Table 8, the proposed L2S achieves significant improvement compared with the baseline for both regression-based and localization-based counting on the extracted dense regions for all datasets.

Table 8 Quantitative comparison of baseline and AutoScale on the extracted dense regions
Full size table
Ablation Study
We conduct the ablation study mainly on the widely adopted ShanghaiTech Part A dataset. In the following, we study the effectiveness of the proposed L2S module, different designs in L2S, and the effectiveness of distance-label map with dynamic cross-entropy (DCE) loss.

Ablation Study on Effectiveness of the Proposed L2S Module We study the effectiveness of the proposed L2S from three aspects: (1) Effectiveness of L2S in alleviating the long-tailed distribution issue; (2) Effectiveness of applying L2S to different baseline methods; (3) Effectiveness of L2S compared with using fixed scale factors.

Fig. 9
figure 9
Density of original selected dense regions and rescaled dense regions using L2S on the ShanghaiTech Part A dataset. ∗ stands for localization-based methods. We compute the density as the ratio between the total number of people in the region and the region size. The proposed L2S effectively brings all dense regions of different scales into similar density

Full size image
Effectiveness of L2S in alleviating the long-tailed distribution issue: We study the effect of alleviating the long-tailed distribution issue for both regression-based and localization-based AutoScale. Specifically, for a given region R, we compute its density as the ratio between the total number of people in R and the size of R. As depicted in Fig. 9, for both regression and localization-based AutoScale, the automatically selected dense regions are of significantly varied density, presenting the lont-tailed distribution. The proposed L2S which normalizes the closeness effectively rescales all selected dense regions of different density into similar. Consequently, the appropriate scale factors lead pixel values to be decomposed and the overlapped blobs to be separated, meanwhile the similar closeness level and density makes the distribution of pixel values similar between sparse and dense regions, reducing the gaps between different regions in image and between different images and mitigating the long-tailed issue.

Effectiveness of applying L2S to different baseline methods: Since L2S can improve the long-tailed distribution issue by rescaling the images, both training and inference phase thus benefit the model prediction and improve the accuracy. As shown in Tables 2, 3, and 4, the proposed L2S consistently improves the baseline FPN over a significant stage on all the datasets for both regression-based method and localization-based method. Furthermore, as shown in Fig. 6 and Fig. 7, the peak of each blob is more discriminative after the re-prediction by L2S. This is because the model is trained under a suitable pixel value/label distribution of ground truth, which helps the model to predict more accurately when an appropriate rescaled image is fed. Note that the resolution of the AutoScale density map is the same as the baseline density map. For visualization purpose, we simply resize the refined prediction for the rescaled dense region to the original size of the selected dense region, and replace the original density map on that dense region with such resized one.

Specifically, for the regression-based AutoScale, L2S improves the baseline model by 1.5/14.2 in MAE and 7.6/80.9 in MSE on the val/test set of NWPU-Crowd dataset, by 4.1/5.1 in MAE and 14.5/11.2 in MSE on the val/test set of JHU-Crowd++ dataset, by 4.5 in MAE and 11.3 in MSE on the UCF-QNRF dataset, by 5.1 in MAE and 11.5 in MSE on the ShanghaiTech Part A dataset. The improvement of L2S for localization-based AutoScale is more significant. Precisely, L2S improves the baseline model by 14.3/19.3 in MAE and 34.1/87.7 in MSE on the val/test set of NWPU-Crowd dataset, by 14.7/15.3 in MAE and 61.3/66.9 in mse on the val/test set of JHU-Crowd++ dataset, by 20.4 in MAE and 60.5 in MSE on the UCF-QNRF dataset, by 9.9 in MAE and 38.3 in MSE on the ShanghaiTech Part A dataset. It is noteworthy L2S improves the baseline not as significantly as other datasets on the ShanghaiTech Part B since it is a relatively sparse dataset.

To demonstrate that the L2S can generalize to different models, we implement the MCNN (Zhang et al. 2016), CSRNet (Li et al. 2018), CAN (Liu et al. 2019), BL (Ma et al. 2019), and SFCN (Wang et al. 2019) with the proposed L2S on the ShanghaiTech Part A dataset. The quantitative results are listed in Table 9, where we can observe that the proposed L2S is helpful to these baseline methods, consistently achieving noticeable improvements on the ShanghaiTech Part A dataset.

Table 9 The effectiveness of L2S on different density regressors. ∗ means reproduction results using their corresponding officially released code
Full size table
Fig. 10
figure 10
The histogram of learned scale factors and the effectiveness of the proposed L2S compared with fixed scale factors

Full size image
Effectiveness of L2S compared with fixed scale factors To verify that the improvement is not simply given by zooming in dense regions for re-prediction, we conduct experiments by zooming in the selected dense regions at fixed scale factors. Based on the histogram of predicted rescale factors shown in Fig. 10a, b for both regression-based and localization-based AutoScale, we fix the scale factors to 1.5, 2.0, 2.5, and 3. As shown in Fig. 10c, though zooming in at fixed factors may improve the prediction on dense regions, the proposed L2S outperforms the alternatives using fixed scale factors, which is reasonable. In fact, since the selected regions are usually very dense, zooming in for re-prediction is beneficial for accurate counting, because it can relatively mitigate the long-tailed distribution. Nevertheless, since the selected dense regions are of different closeness levels (see Fig. 9), we need adaptive scale factors for better counting. Indeed, L2S generates adaptive and appropriate scale factors (given by the ratio between the original closeness level and the re-scaled closeness level in Fig. 9), which mitigates the long-tailed distribution in an adaptive and learnable manner and thus leads to more improvement of counting accuracy. We can also observe that in Fig. 10a, b, there exist scale factors smaller than 1, which indicates that the center loss also forces the selected relatively sparse regions close to the central closeness level.

Ablation Study on Different Designs in L2S We study the effect of some designs for L2S module including the density measure, the area-ratio threshold, and the ground-truth regeneration.

Effect of using different density measures We compare the performance of variants of AutoScale conducted with different density measures: the average people number and the average distance. As presented in Table 10, the average-distance measure consistently improves the performance over the average people-number, demonstrating the effectiveness of using the average distance as the density measure.

Table 10 Performance comparison of using different density measures in the L2S module on the ShanghaiTech Part A and test set of JHU-Crowd++
Full size table
Fig. 11
figure 11
The impact of different settings of 𝐽𝑟 and 𝐽𝑙 on the performance in terms of MAE

Full size image
Table 11 Ablation study on kernel setting during training phase. “Fixed” means that the Gaussian kernel size/distance ranges for generating distance-label map are fixed even though the distances between Gaussian blobs /local minima are changed according to the learned scale factors, while “Multiplied” (resp., “Divided”) means that the Gaussian kernel size/distance ranges are multiplied (resp., divided) by the learned scale factors. Note that when using “Multiplied” (resp., “Divided”), we rescale (resp., keep) the size of the selected dense region
Full size table
Effect of using different area-ratio threshold values: We threshold the selected regions via 𝐽𝑟 (resp. 𝐽𝑙) for the regression-based AutoScale (resp. localization-based AutoScale). The performance changes with different 𝐽𝑟 and 𝐽𝑙 are presented in Fig. 11. For very small values of 𝐽𝑟/𝐽𝑙, we select to refine some very small regions which may be some noisy background regions, leading to slightly decreased performance. Using too large 𝐽𝑟/𝐽𝑙 may ignore the refinement of some dense regions, which decreases a bit the performance. The good news is that the performance in terms of MAE is rather stable (varying one to three MAE) and the L2S consistently improves the baseline for a wide range of different values of 𝐽𝑟 and 𝐽𝑙. We roughly set 𝐽𝑟 to 0.1 and 𝐽𝑙 to 0.02 for all datasets and all experiments based on the experiments on the ShanghaiTech Part A dataset.

Effect of regenerating the ground-truth on extracted dense region To further demonstrate that the key to improve the performance is to mitigate the long-tailed distribution rather than simply zooming in the dense regions to an appropriate scale, we conduct an experiment that aims to compare fixed Gaussian kernels of ground-truth density map with scaled Gaussian kernels during the training phase. We perform two straightforward scaled Gaussian kernels: (1) rescale the size of selected region and multiply the kernel size with the scale factor; (2) keep the size of the selected region and divide the kernel size with the scale factor. For the former variant, there still exist overlaps and pixel accumulations from different Gaussian blobs even though their centers are separated when the kernels becomes larger. In fact, rescaling both images and Gaussian blobs does not change the long-tailed distribution of the density values since the blobs are still overlapped the same relative amount. For the second variant, though using small kernels can mitigate the long-tailed distribution, it does not normalize the person size, which is beneficial to match the feature extractors of CNN. The adopted fixed Gaussian kernel with rescaled region effectively alleviates the long-tailed issue and normalizes the person size by rescaling all dense regions into similar and reasonable closeness level, leading to better performance. Indeed, as shown in Table 11, even though zooming in the images, the Scaled Gaussian kernel brings in some improvements, while the fixed Gaussian kernel significantly improves the baseline in terms of the density map regression. The same mechanism and observation also hold for the localization based on the distance-label map.

Table 12 Comparison of counting by localization based on classical density maps with different Gaussian kernels and the proposed distance label map combined with different loss functions. DCE loss denotes our proposed dynamic cross-entropy loss
Full size table
Fig. 12
figure 12
Two examples of localization-based counting using the ordinal regression loss and the proposed DCE loss

Full size image
Ablation Study on Effectiveness of the Distance-Label Map with Dynamic Cross-Entropy Loss We study in the following the effect of the proposed distance-label map and the customized cross-entropy loss in localizing people.

The effectiveness of utilizing distance-label map for counting by localization To demonstrate the localization effectiveness of using the local minima of distance-label map representation, we compare with experiments using density map representation on the baseline model. We discard the L2S module for this ablation study. For the localization based on distance maps, the local minima represent exact person locations. For the density map representation, when a small spread parameter is used, there is few overlaps of Gaussian blobs between nearby people in dense regions. Therefore, person locations correspond to local maxima of density maps. Whereas, when a large spread parameter is used, the Gaussian blobs of nearby people in dense regions severely overlap each other, the maxima of such density map may not be accurate. Thus, we conduct experiments for localization with density maps using small spread parameters: {1, 2, 4, 8}. As depicted in Table 12, for localization using density map representation, a smaller spread parameter indeed yields a better count accuracy. Localization using the proposed distance-label map significantly outperforms localization based on density maps by 27.2 in MAE and 53.4 in MSE.

The effectiveness of utilizing the dynamic cross-entropy loss As shown in Table 12, the proposed dynamic cross-entropy loss significantly improves classical cross-entropy loss in predicting distance-label maps. Specifically, for the localization model without L2S (resp. with L2S), the dynamic cross-entropy loss improves the results by 6.6 (resp. 5.7) in MAE and 9.4 (resp., 15.5) in MSE. Moreover, the proposed dynamic cross-entropy loss for the introduced distance-label map further boosts the localization accuracy, improving the localization using density maps by 33.8 in MAE and 62.8 in MSE.

Besides, we also compare the proposed DCE loss with widely used focal loss (Lin et al. 2017) and ordinal regression loss (Fu et al. 2018). Specifically, the proposed DCE loss outperforms the focal loss by 3.6 MAE (resp. 4.3 MAE) and 9.8 MSE (resp. 10.8 MSE) for the baseline (resp. AutoScale). The ordinal regression loss is dedicated for regressing ordinal numbers (e.g., 1,2,3,…). We observe that the baseline with DCE loss achieves 23.3 MAE improvement over the ordinal regression loss. In fact, as shown in Fig. 12, though the ordinal regression loss guides the model to learn distance-label maps with similar patterns as that given by the proposed DCE loss. Yet, the local minima are not as accurate as the result given by the proposed DCE loss. A possible reason for this is that the regression loss usually results in blurred prediction, thus making it hard to localize the local minima.

Cross-Dataset Validation
Since scene variation usually leads to significant performance drop, cross-dataset evaluation gradually attracts more attention in crowd counting (Zhang et al. 2015; Shi et al. 2018; Wan et al. 2019; Ma et al. 2019). In practice, a crowd counting method with strong generalizibility is usually expected. To verify the transferability of the proposed AutoScale, we evaluate the performance of the AutoScale under different cross-dataset validations. As depicted in Table 13, both the regression-based and localization-based AutoScale with L2S outperform state-of-the-art methods under cross-dataset evaluation, which demonstrates the superior generalizability of the proposed AutoScale. Specifically, the regression-based AutoScale with L2S outperforms RRSP (Wan et al. 2019) (resp. D-ConvNet (Shi et al. 2018)) by 9.1 (resp. 18.2) in MAE and 25.9 (resp. 56.6) in MSE on ShanghaiTech Part A crossing to Part B. Meanwhile, AutoScale with L2S outperforms BL (Ma et al. 2019) consistently under the same cross-validation settings.

The localization-based AutoScale with L2S also improves the LSC-CNN (Sam et al. 2020) by 18.9 in MAE and 34.3 in MSE on ShanghaiTech Part B crossing to Part A and by 25.1 in MAE and 24.7 in MSE on UCF-QNRF crossing to ShanghaiTech Part A. LSC-CNN is slightly better than us on the ShanghaiTech Part A crossing and UCF-QNRF crossing to ShanghaiTech Part B, because the proposed method L2S focuses more on the dense dataset. Moreover, we can see that the proposed L2S significantly improves the baseline models for both regression-based and localization-based counting, demonstrating the effectiveness of the proposed L2S under cross-dataset validation.

Table 13 Experimental results on the transferability of different methods under cross-dataset evaluation. The proposed L2S improves the transferability. * means localization-based methods
Full size table
Fig. 13
figure 13
Qualitative results in localizing vehicles using the proposed distance class label map and dynamic cross-entropy loss

Full size image
Table 14 Quantitative comparison in counting vehicles on the TRANCOS dataset using the proposed distance-label map and dynamic cross-entropy loss. ∗ stands for localization-based methods
Full size table
Evaluation on Vehicle Counting
We also conduct an experiment on vehicle counting to further demonstrate the effectiveness of the distance-label map representation and the proposed dynamic cross-entropy loss other than on crowd counting. For that, we conduct experiments on the TRANCOS dataset (Guerrero-Gómez-Olmedo et al. 2015). We evaluate the proposed method using the associated metric Grid Average Mean Absolute Error (GAME) for this dataset, given by

𝐺𝐴𝑀𝐸(𝑛)=1𝑁𝐼∑𝑖=1𝑁𝐼(∑𝑗=14𝑛|𝑃𝑗𝑖−𝑃̂ 𝑗𝑖|),
(11)
where 𝑁𝐼 is the number of images in the testing set, 𝑃𝑗𝑖 and 𝑃̂ 𝑗𝑖 are the ground-truth and predicted count result of the j-th region in the i-th input image, and n means that we evenly divide the whole image into 4𝑛 non-overlapping regions. The GAME(n) is the sum of MAE in each of these non-overlapping regions. Note that GAME(0) is equivalent to the MAE metric in Eq. (10).

Some qualitative results in terms of distance-label maps are illustrated in Fig. 13. Using the baseline FPN model with distance-label maps and dynamic cross-entropy loss accurately localizes and counts the vehicles. The quantitative comparison with some state-of-the-art methods on this dataset is depicted in Table 14. The proposed method outperforms the state-of-the-art methods (Sam et al. 2020) under GAME0, GAME1, and GAME2 evaluations, and performs competitively with other methods under GAME3 evaluation. It is noteworthy to mention that the TRANCOS dataset provides regions of interest that mask the dense regions, therefore we only use the baseline model instead of the whole AutoScale to conduct the counting by localization experiment. This further confirms the effectiveness of the proposed distance-label map representation and dynamic cross-entropy loss in localizing vehicles other than people.

Fig. 14
figure 14
Some failure cases of the regression-based and localization-based AutoScale. The enclosed regions are the automatically selected dense regions, which are rescaled via L2S for re-prediction

Full size image
Failure Case and Related Discussion
Though both the regression and localization-based AutoScale improve the counting accuracy on dense regions in most cases. There are still some problems in very difficult images such as incorrect predictions on noisy backgrounds and very dense regions. Some failure cases are shown in Fig. 14. In the first row, the initial prediction has difficulty in noisy background regions (containing “fake” people that do not count), leading to inappropriate dense region selection. Zooming in the noisy background region results in an uncontrollable count.

Table 15 The effect of choosing at most one or two disconnected dense areas. * means our localization-based method
Full size table
Another case of unsatisfied counting is shown in the second row, where there are more than one very dense regions. Since the proposed AutoScale selects the largest dense region to rescale and refine the count, we still need to improve the count on the other dense regions. Therefore, it requires further exploration of the dense region selection methods. For that, we conduct experiments by simply choosing at most two disconnected dense areas for refinement by AutoScale. As depicted in Table 15, refining at most two dense regions slightly increases the performance on the ShanghaiTech Part A and JHU-Crowd++ dataset. Yet, the improvement is rather limited (less than 0.7 MAE and 2.8 MSE).

In practice, crowd people usually gather together, leading to very few images having more than one disconnected dense area (See Figs. 6 and 7 for example). Statistically, we find that none of ShanghaiTech Part A images and only 18 of 1600 images in JHU-Crowd++ test set have more than one disconnected dense area for the regression-based counting. For the localization-based counting, since we use a small 𝐽𝑙=0.02, 45 of 182 images in ShanghaiTech Part A and 344 of 1600 images have more than one disconnected dense area. Yet, most of the rest candidate dense areas (except the largest one) are relatively sparse. This explains why the improvement is limited. Therefore, although it is true that choosing more dense regions for refinement makes more sense and brings slight improvement, we simply refine the prediction with L2S on the largest dense region. The currently adopted dense region selection strategy is still somehow heuristic, which could be improved and bring further performance gain in the future work.

Conclusion
In this paper, we explore how to learn and represent the density map better from the aspect of long-tailed distribution of pixel-wise density values for crowd counting. The proposed L2S performs in an unsupervised clustering way that leverages the center loss to bring all dense regions to similar and appropriate closeness levels for accurate counting. This is used for decomposing the accumulated density values and reducing the density distribution gap, helping to better learn the density maps. The effectiveness of L2S is validated on both density regression method and distance-label map localization method for crowd counting. For the later method, we also present a dedicated dynamic cross-entropy loss function to further improve the effectiveness of localizing people in addition to count people. Extensive experiments on five challenging datasets demonstrate the effectiveness of the proposed L2S module and the superior performance of the proposed localization-based counting in localizing people with the proposed dynamic cross-entropy loss. Besides, both the proposed density map regression AutoScale and localization-based AutoScale with L2S achieve appealing performance on the cross-dataset evaluation, showing a good transferability in crowd counting. Moreover, the proposed L2S can be applied to many other baseline methods, achieving consistent improvements. Furthermore, the localization-based AutoScale also performs competitively with other methods in localizing and counting vehicles other than the crowd.

In the future, we would like to improve the dense region selection strategy, which is currently somehow heuristic, and continue to explore the issue of long-tailed distribution in the density map. We are also interested in investigating the effectiveness of L2S in other vision tasks than the crowd or the vehicle counting and localization.