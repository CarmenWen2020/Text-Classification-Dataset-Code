Forecasting scene layout is of vital importance in many vision applications, e.g., enabling autonomous vehicles to plan actions early. It is a challenging problem as it involves understanding of the past scene layouts and the diverse object interactions in the scene, and then forecasting what the scene will look like at a future time. Prior works learn a direct mapping from past pixels to future pixel-wise labels and ignore the underlying object interactions in the scene, resulting in temporally incoherent and averaged predictions. In this paper, we propose a learning framework to forecast semantic scene layouts (represented by instance maps) from an instance-aware perspective. Specifically, our framework explicitly models the dynamics of individual instances and captures their interactions in a scene. Under this formulation, we are able to enforce instance-level constraints to forecast scene layouts by effectively reasoning about their spatial and semantic relations. Experimental results show that our model can predict sharper and more accurate future instance maps than the baselines and prior methods, yielding state-of-the-art performances on short-term, mid-term and long-term scene layout forecasting.
Introduction
Human beings are remarkably capable of forecasting the future states of a scene given past observations of it. We can do this mainly because we have built a mental model of scene dynamics by observing many examples of how objects move and interact in real-world scenes. A machine equipped with a similar capability to predict what a scene will look like in the near future will enable intelligent agents to plan their actions early based on past observations. For example, self-driving cars and social robots need to predict the future in order to plan ahead and react to the environment more quickly (Dosovitskiy and Koltun 2017; Shalev-Shwartz et al. 2016).

Developing machinesâ€™ capability to anticipate the future is very challenging, as it requires understanding various appearance changes, complex motion dynamics, and diverse object interactions in a scene. Towards this objective, there has been a line of research on forecasting scene layouts [represented by semantic segmentation maps (Luc et al. 2017; Å ariÄ‡ et al. 2019) or instance maps (Luc et al. 2018)] given the observed past frames in an input video. However, all these works adopt an end-to-end, per-pixel prediction framework of an existing segmentation model, which is extended to the temporal domain to directly map past pixels to future pixel-wise semantic labels. Such a design would lead to blurry and averaged predictions, where instances have degraded shapes and may disappear unexpectedly, especially for long-term prediction. It may also suffer from temporally incoherent predictions of instance shapes and locations, if partial occlusions are present in the input frames. Consider Fig. 1 as an example. The shape of the car (marked in green) generated by the state-of-the-art F2F model (Luc et al. 2018) is inaccurate compared to that of the ground truth. In addition, the car marked in yellow is missing due to the lack of temporal consistency.

Our key observation is that to anticipate what a scene will look like at a future time, human beings would typically recognize and localize individual instances in the scene first, and then reason about their spatial and semantic interactions to make the prediction. Inspired by this observation, rather than performing direct pixel-level prediction as in existing works, we address the problem from an instance-aware perspective. We model the dynamics of individual instances separately and reason their interactions with each other explicitly. By doing so, we can enforce constraints at instance-level, which help produce sharper instance shapes and temporally stable predictions. We can also predict plausible instance motions due to the explicit modeling of subtle instance-to-instance interactions.

Fig. 1
figure 1
Scene layout forecasting. Given four past image frames (ğ¼ğ‘¡âˆ’3ğ›¥,ğ¼ğ‘¡âˆ’2ğ›¥,ğ¼ğ‘¡âˆ’1ğ›¥,ğ¼ğ‘¡) of a video as input (top row), our goal is to predict a future instance map ğ¼ğ‘¡+ğ‘›ğ›¥, where ğ‘›ğ›¥ is the number of frames ahead in the future (bottom row). State-of-the-art methods, e.g., F2F (Luc et al. 2018), tend to predict inaccurate object shapes and temporally incoherent objects, especially over a long time span. In contrast, our model can predict a more accurate and sharper future instance map over different time spans (see Sect. 4.4)

Full size image
Fig. 2
figure 2
Illustration of the existing pixel-level prediction (left) and the proposed instance-aware modeling (right) for scene layout forecasting. Pixel-level prediction directly maps the input pixels to per-pixel semantic labels, while instance-aware modeling considers each instance separately and enforces spatial and temporal constraints at instance level. ğ‘…ğ‘˜ is the k-th instance representation, and ğ¿ğ‘˜ is the future layout of the k-th instance

Full size image
Based on the above idea, we propose a separation and composition framework to predict future instance maps (i.e., scene layouts) from past image frames. As shown in Fig. 2, unlike pixel-level prediction, our framework explicitly models the dynamics of individual instances and the relations among them in a scene. Given a sequence of image frames, we first generate a sequence of consistent instance maps by an off-the-shelf instance segmentation network (He et al. 2017). The instance maps along with the corresponding image frames are separated in an instance-wise manner, generating a set of instance-wise representations. Each representation captures the structure and appearance dynamics of an instance over the time span of the input frames. The framework then learns per-instance features by encoding the instance-wise representation and modeling its spatial and semantic relations with other instances via an instance relation module. The learned per-instance features can then be used to predict instance layouts, which are finally composed to form the predicted future instance map. Figure 1 shows that our approach can generate much more accurate scene layouts, compared with the state-of-the-art methods.

To evaluate the performance of our method, we use the popular Cityscapes dataset (Cordts et al. 2016) for short-term, mid-term and long-term future instance map predictions. Extensive quantitative and qualitative comparisons are performed against the baselines and state-of-the-art methods. Results show that our method is able to forecast sharper and more accurate scene layouts, yielding state-of-the-art performances.

In summary, the main contributions of this work are:

To the best of our knowledge, we are the first to propose an instance-aware approach to address the scene layout forecasting problem. Our approach is able to produce sharp instance shapes and plausible instance motions.

We propose a novel separation and composition framework for predicting future instance maps. It separately models the dynamics of individual instances and explicitly captures their interactions in a scene.

We extensively validate our method and show that it achieves state-of-the-art performances in short-term, mid-term and long-term future predictions.

Related Work
Future forecasting, though challenging, is important to many real-world applications such as autonomous driving. Learning to forecast the future has received a significant amount of attention in the computer vision community. There has been much research effort on this problem by parameterizing the future in different forms, such as object movement trajectories (Yuen and Torralba 2010), event categories (Hoai and De la Torre 2014), motion fields (optical flow) (Walker et al. 2015), human body poses (Chao et al. 2017), human behaviors (Guan et al. 2020), and action categories (Vondrick et al. 2016; Gammulle et al. 2019). Unlike these works that predict future object locations, scene layout forecasting needs to jointly predict object positions, sizes and shapes (Luc et al. 2017; Jin et al. 2017; Luc et al. 2018). Thus, all the aforementioned works are not amenable to our problem. In this section, we focus our discussion on recent works about image frame and scene layout forecasting.

Image Frame Forecasting There is a large body of research on predicting raw pixel values of future frames in a video sequence. Ranzato et al. (2014) proposed the first unsupervised deep model for next frame prediction. Mathieu et al. (2016) improved the predictions using the adversarial loss and a gradient difference loss to avoid blurry results. A similar training strategy was employed for future frame predictions in time-lapse videos (Zhou and Berg 2016). Rather than predicting unconstrained pixel intensities directly, Vondrick and Torralba (2017) learned pixel transformation and synthesized future frames by transforming pixels from existing frames. Kwon and Park (2019) used a single network to predict future and past frames retrospectively to enforce the consistency of bi-directional prediction. Kim et al. (2020) proposed an adaptive online updating network for future frame prediction. All these approaches suffer from blurry and averaged prediction, due to the difficulty of pixel-level prediction. In addition, pixel-level fine-grained predictions are often not necessary for intelligent systems to make decision. In contrast to these works that predict pixel intensities/colors, we focus on predicting a mid-level representation (i.e., instance maps). Our work is similar in spirit to the works on future frame synthesis based on motion decomposition (Qi et al. 2019) and foreground-background separation (Saric et al. 2020). However, instead of decomposing motion or separating the foreground and background, we decompose a scene into instances, model the interactions among these instances, and then predict a future layout of the instances.

Fig. 3
figure 3
We propose an instance-aware separation and composition framework for scene layout forecasting, by modeling instances and their relations. Given a sequence of input frames ğ¼ğ‘¡âˆ’3ğ›¥:ğ‘¡, we use Mask R-CNN as a pre-process to obtain a sequence of instance maps ğ‘†ğ‘¡âˆ’3ğ›¥:ğ‘¡. ğ¼ğ‘¡âˆ’3ğ›¥:ğ‘¡ and ğ‘†ğ‘¡âˆ’3ğ›¥:ğ‘¡ are decomposed into instance-wise representation ğ‘…ğ‘˜ and background representation ğ‘…ğ‘ğ‘”. These representations are then embedded into per-instance features and updated by the instance relation module, resulting in new features ğ‘“ğ‘Ÿğ‘˜ that capture interactions among the instances. ğ‘“ğ‘Ÿğ‘˜ are then used to predict instance layouts ğ¿ğ‘˜, which are finally composed to output a future instance map

Full size image
Scene Layout Forecasting Our work is in line with a series of recent works on scene layout forecasting. Instead of synthesizing (or predicting) pixel values for future frames, these works aimed to predict future image content in a more abstract way, i.e., semantic scene layout. Jin et al. (2017) trained a model to predict the semantic segmentation of the next frame from the preceding input frames. Luc et al. (2017) directly trained a single network by taking several segmentation masks as input to predict the semantic segmentation of a future frame. Rochan et al. (2018) proposed a LSTM to model the temporal information of the input frames and predicted the semantic segmentation of a future frame. Hu et al. (2020) proposed a deep learning model to jointly predict ego-motion, static scene, and the motion of dynamic agents in a probabilistic manner. While the above approaches are relevant to scene layout forecasting, they focus on producing object category labels instead of instance labels. Jin et al. (2017) further divided a scene layout into three groups and proposed a multi-task learning framework to jointly predict optical flow and semantic segmentation. However, it only considered three groups in the scene and required optical flow annotations, which are difficult to obtain.

Notably, our work is closely related to Luc et al. (2018), which also predicted future instance maps. They first predicted the features for a future frame, and then integrated the predicted features into the Mask R-CNN pipeline (He et al. 2017) for instance segmentation. However, their method still follows the traditional instance segmentation framework without explicitly accounting for instances. Graber et al. (2021) further extended the future instance map prediction task to future panoptic segmentation by learning the dynamics of background stuff and foreground objects. However, they do not consider the interactions among instances. In contrast, our framework models individual instances and captures their spatial and semantic relationships explicitly. Our experiments in Sect. 4.4 show that while previous works tend to average different object classes into blurry future predictions, our method can preserve the properties of individual objects better due to its ability to exert instance-level control over the predictions, especially for mid-term and long-term predictions.

Approach
The scene layout forecasting task is to predict instance shapes, sizes as well as motions in a scene at a future time. The inputs to our method include four image frames, ğ¼ğ‘¡âˆ’3ğ›¥:ğ‘¡={ğ¼ğ‘¡âˆ’3ğ›¥,ğ¼ğ‘¡âˆ’2ğ›¥,ğ¼ğ‘¡âˆ’ğ›¥,ğ¼ğ‘¡} (where ğ›¥ denotes the time interval), which capture the appearance dynamics of a scene. Our goal is to predict a future instance map ğ‘†Ë†ğ‘¡+ğ‘›ğ›¥ (where n is the number of time intervals ahead in the future) that describes the semantic and structural information of the scene at time step ğ‘¡+ğ‘›ğ›¥.

Figure 3 shows our framework. It has three main tasks. First, after segmenting the input frames ğ¼ğ‘¡âˆ’3ğ›¥:ğ‘¡ into a sequence of instance maps ğ‘†ğ‘¡âˆ’3ğ›¥:ğ‘¡ using Mask R-CNN (He et al. 2017), we separate ğ¼ğ‘¡âˆ’3ğ›¥:ğ‘¡ and ğ‘†ğ‘¡âˆ’3ğ›¥:ğ‘¡ in an instance-wise manner to generate disjoint sets of instance-wise frames, each of which captures the structure and appearance dynamics of an instance over the input time span 3ğ›¥. We also segment the background from the input frames and the instance maps into a set of background frames. Second, we use background and instance encoders to extract background features and per-instance features, respectively, from the decomposed maps and frames. The features of each instance are then refined by the instance relation module to model its relation with other instances and the background. Third, we feed the refined instance features to LayoutNet to help predict the layouts of individual instances, which are then combined by a composition module to form a predicted scene layout. A sequence of future scene layouts can be generated by applying the framework recursively. We present the details of these three tasks below.

Instance-Wise Separation
In order to carry out future instance map prediction in an instance-aware manner, we first decompose the input frames and instance maps into a set of instance-wise representations. As shown in Fig. 3, for each instance i, we obtain its masked instance maps ğ‘†ğ‘–ğ‘¡âˆ’3ğ›¥:ğ‘¡ and masked frames ğ¼ğ‘–ğ‘¡âˆ’3ğ›¥:ğ‘¡ by tracking the instance across the frames and zeroing out the values outside the instance in ğ‘†ğ‘¡âˆ’3ğ›¥:ğ‘¡ and ğ¼ğ‘¡âˆ’3ğ›¥:ğ‘¡. We then convert the masked instance maps ğ‘†ğ‘–ğ‘¡âˆ’3ğ›¥:ğ‘¡ to instance-wise semantic maps ğ‘†Ì‚ ğ‘–ğ‘¡âˆ’3ğ›¥:ğ‘¡ by filling the instance silhouette of each instance map with one-hot encoding of the instanceâ€™s category. We concatenate ğ‘†Ì‚ ğ‘–ğ‘¡âˆ’3ğ›¥:ğ‘¡ and ğ¼ğ‘–ğ‘¡âˆ’3ğ›¥:ğ‘¡ to construct an instance-wise representation ğ‘…ğ‘–. This instance-wise representation ğ‘…ğ‘– captures the structure and appearance dynamics of the instance over the time span 3ğ›¥. To model the effect of background contents on the instances of interest, we also consider the background (non-instance regions) as an instance by assigning it a distinctive label, and create a background representation ğ‘…ğ‘ğ‘” similarly.

Note that since the instance maps are generated by applying Mask R-CNN to each input frame independently, there is no inherent correspondence between instances across different frames. Thus, before the separation step, we track each instance across different frames using a simple tracking method as described in the dataset processing part of Sect. 4.2.

Feature Embedding
Given the background and instance-wise representations, we use a background encoder (BKG encoder) and an instance encoder (Insta encoder) to extract background features and instance features that encode structure and appearance dynamics of the background and the instances, respectively. However, since each instance would likely plan its future states according to the behaviors of other instances in a scene, encoding each instance alone may not capture the subtle interactions among the instances, which is crucial to predicting its future state. To address this problem, we introduce an instance relation module to refine the features of each instance by modeling its spatial and semantic relation with other instances in the scene. Taking as input the background and instance features {ğ‘“ğ‘ğ‘”,ğ‘“1,ğ‘“2,â€¦,ğ‘“ğ‘›}, the relation module outputs the refined instance features {ğ‘“ğ‘Ÿ1,ğ‘“ğ‘Ÿ2,â€¦,ğ‘“ğ‘Ÿğ‘›} as the final embedded instance features.

Instance Relation Module To predict the future state of an instance, the states of some neighboring instances might be more relevant than the others. Hence, we adopt an attention mechanism (Li et al. 2019) to incorporate relational information into the features of each instance. Note that our framework is general, and any differentiable relation module can be used in principle. Specifically, the refined features of instance i are a linear combination of the features of the other instances and the background as:

ğ‘“ğ‘Ÿğ‘–=ğ‘“ğ‘–+âˆ‘ğ‘—â‰ ğ‘–ğ‘›ğœ€ğ‘–ğ‘—â‹…(ğ‘Šğ‘£1ğ‘“ğ‘—)+ğœ€ğ‘ğ‘”ğ‘–â‹…(ğ‘Šğ‘£2ğ‘“ğ‘ğ‘”),
(1)
where ğ‘Šğ‘£1 and ğ‘Šğ‘£2 are learnable weight matrices. ğœ€ğ‘–ğ‘— indicates the relative importance of instance j to i. ğœ€ğ‘ğ‘”ğ‘– indicates the relationship of instance i to the background. Both ğœ€ğ‘–ğ‘— and ğœ€ğ‘ğ‘”ğ‘– are attention weights that can be computed dynamically via an attention mechanism:

ğœ€ğ‘–ğ‘—=ğ‘šğ‘ğ‘¥(0,ğ‘Šğ‘„ğ‘“ğ‘–+ğ‘Šğ‘ƒğ‘“ğ‘—),
(2)
ğœ€ğ‘ğ‘”ğ‘–=ğ‘šğ‘ğ‘¥(0,ğ‘Šğ¾(ğ‘“ğ‘–+ğ‘“ğ‘ğ‘”)),
(3)
where ğ‘Šğ‘„, ğ‘Šğ‘ƒ and ğ‘Šğ¾ are learnable weight vectors that project instance features ğ‘“ğ‘– and ğ‘“ğ‘— to scalar values. max(0, x) is used to ensure the attention weights â©¾ 0, with 0 indicating no interaction with instance i. In this way, the refined features of each instance will encode not only the information of the instance itself but also the contextual information from other instances and the background.

Fig. 4
figure 4
Details of the LayoutNet module (top) and the composition module (bottom)

Full size image
Layout Prediction and Composition
Having generated the embedded features for individual instances in the scene, we use them to predict an instance map that describes the semantic layout of the scene at a future time, by first predicting a layout for each instance and then composing all the instance layouts to output a future instance map.

Layout Prediction For each input instance, we use its features to compute an instance layout map by predicting a shape mask and a bounding box using the LayoutNet module (Qiao et al. 2019), which is illustrated in Fig. 4(top). We note that the moving patterns of different instances in a scene are different, depending on many factors such as the size of each instance and its distance from the camera. In particular, we have empirically found that directly predicting the future layouts of small instances cannot give satisfying results. It is possibly because the motion magnitudes of small objects are often very small. This may result in the loss function being dominated by those instances with large motions, causing the model to generate unreasonable predictions for the small instances.

To address the above problem, we train the LayoutNet module to additionally estimate how confident the network is in predicting the layout map of each instance. A low confidence score indicates that the network prediction result is unreliable. In particular, the input to the LayoutNet module is the instance-wise features ğ‘“ğ‘˜, which are fed into a shape regression branch to predict a soft binary shape mask ğ‘šğ‘– and a position regression branch to predict a bounding box of the instance with five parameters (ğ‘¥ğ‘–,ğ‘¦ğ‘–,ğ‘¤ğ‘–,â„ğ‘–,ğ‘ğ‘–). (ğ‘¥ğ‘–,ğ‘¦ğ‘–) refer to the center location of the box, (ğ‘¤ğ‘–,â„ğ‘–) refer to the width and height of the box, and ğ‘ğ‘– is the confidence score of the box. Note that both rigid and non-rigid objects can be represented by the predicted shape masks. The generated shape is then warped to the position of the corresponding bounding box using bilinear interpolation (Jaderberg et al. 2015), generating a layout map ğ¿ğ‘˜ that represents the shape, size, position and confidence score of the instance in the scene. For the instances with low confidence scores, rather than predicting with the object layout network, we directly estimate their layouts using their motion trajectories in the instance maps. In particular, for each low-confidence instance, we compute an average scaling factor and movement vector based on the corresponding instance masks across all the input instance maps. We then apply the scaling factor and movement vector to the mask of the instance in the last instance map to obtain its future layout map.

Layout Composition Given all the instance layout maps {ğ¿1,ğ¿2,â€¦,ğ¿ğ‘›}, the composition module aims to compose them to generate a coherent scene layout in the form of an instance map. However, partial occlusions among the instances may occur during the composition process. If two instances are found to overlap each other, we first determine their front-back order. Inspired by the element composition process in Qi et al. (2018), we first train an ordering network to indicate if an instance should be in front of another. The inputs to the ordering network are two instance layout maps, and the output is a binary label that indicates if an instance is on top of another when they are composed together. To train the network, the ground truth order is obtained based on the depth maps from Cityscapes. Here, the network focuses on learning the prior that explains the instance-instance depth relations in natural scenes. Based on the outputs from the ordering network, we then composite these instances onto a single canvas, resulting in a scene layout. Note that some artifacts like holes and unreasonable shapes may still exist after the composition process. We further refine the composed scene layout via a cascaded refinement network (Chen and Koltun 2017), to remove these artifacts.

Training
We first train the ordering network to obtain the relative order of two instance layout maps. We then train our entire network end-to-end by minimizing a multi-task objective. In particular, for instance shape prediction, we use a binary cross entropy loss to penalize the pixel-wise difference between each predicted shape mask m and the corresponding ground truth ğ‘šÌ‚  as:

ğ¿ğ‘ â„ğ‘ğ‘ğ‘’=âˆ’âˆ‘ğ‘¥âˆ‘ğ‘¦ğ‘šÌ‚ ğ‘¥,ğ‘¦logğ‘šğ‘¥,ğ‘¦+(1âˆ’ğ‘šÌ‚ ğ‘¥,ğ‘¦)log(1âˆ’ğ‘šğ‘¥,ğ‘¦),
(4)
where ğ‘šğ‘¥,ğ‘¦ is the presence probability of an instance at spatial location (x, y).

For instance bounding box prediction, we denote the parameters and the confidence score of the predicted bounding box as b and ğ‘ğ‘–ğ‘›ğ‘ ğ‘¡, respectively. We define the L1 loss between b and the corresponding ground truth ğ‘Ì‚  as:

ğ¿ğ‘ğ‘ğ‘œğ‘¥=ğ‘ğ‘–ğ‘›ğ‘ ğ‘¡â€–ğ‘âˆ’ğ‘Ì‚ â€–1.
(5)
Since we do not have access to the ground truth confidence score, we apply a cross entropy loss with a constant target label of 1 on ğ‘ğ‘–ğ‘›ğ‘ ğ‘¡ as:

ğ¿ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’=âˆ’log(ğ‘ğ‘–ğ‘›ğ‘ ğ‘¡).
(6)
This will encourage the network to minimize the loss in Eq. 5, while allowing for a certain amount of relaxation to discount the instances with low confidence scores.

In addition, we also make use of adversarial learning (Goodfellow et al. 2014) to encourage the generated instance map to appear realistic. Specifically, we train our model adversarially against a discriminator network D, which attempts to classify an input instance map as real or fake by minimizing the objective:

ğ¿ğ‘ğ‘‘ğ‘£=ğ„ğ‘¥âˆ¼ğ‘ğ‘Ÿğ‘’ğ‘ğ‘™logğ·(ğ‘¥)+ğ„ğ‘¥âˆ¼ğ‘ğ‘“ğ‘ğ‘˜ğ‘’log(1âˆ’ğ·(ğ‘¥)),
(7)
where ğ‘¥âˆ¼ğ‘ğ‘“ğ‘ğ‘˜ğ‘’ are predicted instance maps. ğ‘¥âˆ¼ğ‘ğ‘Ÿğ‘’ğ‘ğ‘™ are the ground truth instance maps.

In summary, we train our model with a total loss:

ğ¿=ğ›¼âˆ‘ğ‘–ğ¿ğ‘–ğ‘ â„ğ‘ğ‘ğ‘’+ğ›½âˆ‘ğ‘–ğ¿ğ‘–ğ‘ğ‘ğ‘œğ‘¥+ğ›¾ğ¿ğ‘ğ‘‘ğ‘£+ğœâˆ‘ğ‘–ğ¿ğ‘–ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’,
(8)
where ğ›¼, ğ›½, ğ›¾ and ğœ are the loss weights.

Experiments
Implementation Details
Training The proposed framework is implemented under the Pytorch framework. The network parameters are initialized using the truncated normal initializer. We downsample the original frames to a resolution of 128Ã—256 in both training and testing stages. We optimize the parameters of our model by the Adam optimizer (Kingma and Ba 2014) with an initial learning rate of 0.005, ğ›½1=0.9 and ğ›½2=0.9999. The batch size is set to be 8. We empirically set the loss weights ğ›¼, ğ›½, ğ›¾ and ğœ to 1, 1, 1, 0.1. Following Luc et al. (2017), we train our model by sampling four frames with an interval of 3 frames (i.e., 0.17s).

Inference We apply our model for three types of time spans: (1) Short-term: predicting the 3rd frame after t, i.e., ğ‘†ğ‘¡+3 (up to 0.17s); (2) Mid-term: predicting the 9th frame after t, i.e., ğ‘†ğ‘¡+9 (up to 0.5s); (3) Long-term: predicting the 27th frame after t, i.e., ğ‘†ğ‘¡+27 (up to 1.6s).

We only train our model for short-term prediction and adopt a recursive approach as in Luc et al. (2017) for mid-term and long-term prediction, by predicting a future instance map given a temporal window of 4 past instance maps. Initially, the window only contains the input instance maps and the corresponding image frames. In later iterations, it contains the instance maps predicted from the previous iterations. Note that we also need to obtain the corresponding masked frames before passing them to the network. When using a predicted instance map as input to our model, for each instance on it, we estimate its masked image frame by masking the last input image frame based on the spatial transformation between its current bounding box and that in the last input frame. We empirically set the threshold of the confidence score to 0.3. When the confidence score of an instance is lower than 0.3, we use the linear motion strategy described in Sect. 3.3 to obtain its future layout. Otherwise, we use the network to predict the future instance layout.

Experimental Setup
Dataset Following existing works (Luc et al. 2017, 2018; Jin et al. 2017), we use the Cityscapes dataset (Cordts et al. 2016) for training, which consists of urban scene videos recorded from a car while driven on the street. It contains 2,975 training, 500 validation and 1,525 test video sequences. Each video has 30 frames of resolution 1024Ã—2048. However, the ground truth instance map annotation is only available for the 20th frame of each video sequence in the dataset. To get the instance maps for all the frames, we use the Mask R-CNN model pre-trained on COCO (Lin et al. 2014) and fine-tuned on Cityscapes using a ResNet-50-FPN backbone (He et al. 2016). Given the image frames in the dataset, the predicted instance segmentations from Mask R-CNN are regarded as the ground-truth annotations to supervise our network.

The instance label IDs may not be consistent across frames due to the independent application of Mask R-CNN on each frame. To perform instance-level decomposition (Sect. 3.1) robustly, we need to filter out unreliable instances and build correspondences of the instances across frames. To this end, we propose a simple tracking method by combining multiple cues (Yang et al. 2019), including semantic consistency, spatial correlation and detection confidence. Given a sequence of 5 frames (4 inputs and 1 output), assuming that we already have a set of existing instance IDs denoted as ğ›º, a new instance can be assigned to an instance ID or discarded based on its confidence score. Specifically, we compute the confidence score ğ‘£ğ‘–ğ‘¡ for instance i at frame t:

ğ‘£ğ‘–ğ‘¡=ğ‘šğ‘ğ‘¥ğ‘—[logğ‘ ğ‘–ğ‘¡+ğˆğ¨ğ”(ğ‘ğ‘–ğ‘¡,ğ‘ğ‘—)+ğ›¿(ğ‘ğ‘–ğ‘¡,ğ‘ğ‘—)],
(9)
where ğ‘ ğ‘–ğ‘¡ is the detection score of instance i from Mask R-CNN. ğ‘ğ‘–ğ‘¡ and ğ‘ğ‘–ğ‘¡ are its bounding box and category, respectively. ğˆğ¨ğ”(â‹…) is the IoU between two instance bounding boxes, and ğ›¿(â‹…) is the Kronecker delta function. It is equal to 1 when ğ‘ğ‘– and ğ‘ğ‘› are equivalent, and 0 otherwise. If ğ‘£ğ‘–ğ‘¡ is larger than a threshold ğ‘‰ğ‘¡â„, we assign it an existing ID j that maximizes the value of ğ‘£ğ‘–ğ‘¡ in Eq. 9, and a new ID otherwise. We record the number of times that the confidence score of each instance ID is higher than threshold ğ‘‰ğ‘¡â„. This number represents how many times the instance ID appears across different frames. We iteratively update this set of instance IDs until all the instances have been processed. Finally, those instances that appear in less than 3 frames will be discarded.

Fig. 5
figure 5
Qualitative results for short-term prediction. In each example, we show the first and last image frames of a four-frame sequence. We then show the instance map prediction results of an optical flow-based method (Optical Flow), F2F (Luc et al. 2018), PS (Graber et al. 2021)), our method (Ours), and the ground truth (GT)

Full size image
Fig. 6
figure 6
Qualitative results for mid-term (a-c) and long-term (d) predictions. In each example, we show the first and last image frames of a four-frame sequence. We then show the instance map prediction results of an optical flow-based method (Optical Flow), F2F (Luc et al. 2018), PS (Graber et al. 2021), our method (Ours), and the ground truth (GT)

Full size image
Evaluation Metrics Since our objective is to predict accurate positions and shapes of the instances in the future, we employ the following metrics. We first use the Jaccard index, i.e., intersection over union (IoU) between the predicted mask and ground truth mask. We also adopt the contour-based F-measure (Ehrig and Ã©rome Euzenat 2005) to evaluate the quality of the predicted object shapes. In particular, given the pixel-wise boundaries of a predicted mask and the ground truth mask, we use a distance threshold ğœŒ to define a boundary precision ğ‘ƒğ‘– and recall ğ‘…ğ‘–. A predicted boundary pixel is regarded as positive only if it is within a distance of ğœŒ from any ground truth boundary pixel. The boundary-based F-measure ğ¹ğ‘– is computed as:

ğ¹ğ‘–=(1+ğ›½2)Ã—ğ‘ƒğ‘–Ã—ğ‘…ğ‘–ğ›½2Ã—ğ‘ƒğ‘–+ğ‘…ğ‘–,
(10)
where ğ›½2=0.3 as in Achanta et al. (2009) to emphasize the precision. It is worth noting that AP (an evaluation metric used in instance segmentation) is not applicable in our setting as it is computed by thresholding on IoU and class confidence, while our model directly predicts the future position and shape of each instance without predicting its class probabilities.

Fig. 7
figure 7
Layout predictions of individual instances by different methods over different time spans. For each case, we show the first and last RGB frames of a four-frame sequence along with their respective instance maps computed by He et al. (2017). We show the instance layouts predicted by an optical flow based algorithm (Optical Flow), F2F (Luc et al. 2018), PS (Graber et al. 2021), our method (Ours), and oracle (He et al. 2017) at time steps ğ‘¡+3 (2nd row), ğ‘¡+6 (3rd row) and ğ‘¡+9 (4th row)

Full size image
Compared Methods
Note that our objective of this work is to predict the future state of each instance in the scene (i.e., a future instance map), rather than semantic segmentation as in Å ariÄ‡ et al. (2019); Saric et al. (2020), which do not differentiate between different instances of each class. Thus, we compare our model with the following methods:

Identity. The instance map for the future frame ğ‘†ğ‘¡+ğ›¥ is copied from the last input instance map ğ‘†ğ‘¡ directly.

Linear Motion. For each object in the scene, we directly estimate its average scaling factor and movement vector from the given instance maps, and apply them to its mask in the last instance map to get its future instance map.

Optical Flow. We first compute the optical flow field (Chen et al. 2016) from the last two input image frames. We then warp each instance mask in the last input instance map based on the inverted flow field on it. For long-term prediction, we recurrently apply the computed optical flow to the latest predicted instance map to make the next prediction.

F2F (Luc et al. 2018). This method first predicts a future feature representation, and then send it to Mask R-CNN to predict an instance map of the future.

Panoptic Segmentation (PS) (Graber et al. 2021). This is the state-of-the art method for predicting the future panoptic segmentation that contains both background stuff and foreground objects. For fair comparison, we compare our results with the foreground object prediction results in Graber et al. (2021).

Oracle. We also report the result of Mask R-CNN as a performance upper bound of our method, which is obtained by running Mask R-CNN on the future frames whose instance maps are to be predicted (recall that our model is supervised by the instance maps predicted by Mask R-CNN).

Results
Qualitative Results We first show visual comparison with the existing methods in Figs. 5 and 6. For short-term prediction shown in Fig. 5, we find that Optical Flow tends to produce artifacts around the instance boundaries, F2F may introduce new instances that do not exist in the input scenes, and PS may generate inaccurate instance shapes, sizes and positions. Consider Fig. 5a as an example. Optical Flow produces some artifacts on the cars due to the inaccurate flow field estimation. Although F2F can predict plausible shape and position of the car in the red box, it introduces a truck unexpectedly inside the green box, which breaks the temporal coherency. The main reason is that its result is based on a predicted high-level, abstract future representation, which is unable to guarantee instance-level temporal consistency between the input frames and the output. In addition, without considering the relationships among the instances, PS generates inaccurate shape, size and position for the car in the red box. In contrast, our method can preserve the instances in the input frames better and generate temporally more coherent predictions. This is because our method considers each instance separately and models the interactions among these instances explicitly.

Mid-term and long-term predictions are much more challenging than short-term prediction, as the uncertainty increases when the time span becomes larger. For the mid-term prediction shown in Fig. 6aâ€“c, in comparison to other methods, our method can predict sharper object shapes and more accurate object positions. For example, for the car inside the green box in Fig. 6a, Optical Flow predicts a wrong position, hiding it behind another car, while F2F produces a blurry shape. Although the result of PS is better, the shapes of the two cars are still worse than those of our method. For long-term prediction shown in Fig. 6d, we can see that Optical Flow fails miserably by having many instances diminished, due to the challenging task of estimating the motion field over a long period of time. F2F and PS tend to generate â€œaverageâ€ predictions, where the shapes of the instances degrade considerably (e.g., the cars inside the green box). In contrast, our method can still give clear instance shapes and plausible instance positions.

Table 1 Quantitative results of predicting instance maps for short-term, mid-term and long-term future
Full size table
Table 2 Quantitative results on car and person
Full size table
To further investigate the effect of different time spans (i.e., from ğ‘¡+3 to ğ‘¡+9) on the prediction performance, we show the layout predictions of individual instances in Fig. 7. We can observe that the car shapes predicted by our method closely match with the oracle (He et al. 2017), while Optical Flow, F2F and PS tend to produce degraded car shapes over time. These results confirm the distinctive advantage of our instance-aware modeling over the pixel-level prediction in generating high-quality object shapes.

Fig. 8
figure 8
Performance versus Threshold. We change the confidence score threshold from 0 to 1, and use Jaccard index and F-measure to evaluate the performance over different time spans. Higher scores indicate better performances

Full size image
Table 3 Results of the ablation study
Full size table
Quantitative Results Table 1 shows the quantitative results over different time spans. Our method performs significantly better than the baselines, F2F and PS. For short-term prediction, F2F works well and is close to our method on the Jaccard index. Predicting the immediate future is a relatively easier task since the pixels of immediate future are strongly correlated with the past pixels. Thus, pixel-level prediction models can give good results, as demonstrated by the reasonably good performances of Identity and Optical Flow, along with the modest performance gaps between the Oracle and different methods. However, it is worth noting that our results have consistently better boundary accuracy (i.e., higher F-measure values) across different time spans, which confirms the distinctive advantage of our instance-wise framework in instance shape prediction. For mid-term and long-term predictions, despite the significant performance drops of all the methods, PS performs better than F2F and other baselines by modeling the motion and appearance history of individual instances. However, our method still outperforms PS by a large margin. This demonstrates the superiority of our method, particularly for a relatively long time span.

We further investigate the performances of our method on some individual object categories. In Table 2, we show quantitative results on car (a rigid object) and person (a highly deformable object). Our method consistently outperforms the other methods on both categories. We note that our performance gains are more pronounced on rigid objects (i.e., car) than on non-rigid objects (i.e., person), as non-rigid objects can deform in numerous ways, making it difficult to predict their motions and shapes accurately.

When predicting the future layout of an instance, we chose to threshold on its confidence score to determine if we use the network or a naive linear motion predictor for prediction. Here, we experiment with different confidence score thresholds to study the effectiveness of this design choice. When the threshold is set to 0 or 1, we only use our network or the linear motion predictor, respectively, for prediction. When the threshold is in (0, 1), we use the two prediction models simultaneously and determine which one to use for an instance based on its predicted confidence score. Figure 8 shows the results. We can see that as the threshold increases from 0 (i.e., as we add the linear motion predictor to handle instances with relatively low confidence scores), the performance improves gradually for all the time spans, until when the threshold reaches around 0.3 where the performance drops significantly. This confirms the necessity of our confidence-based dynamic model choice strategy.

Ablation Study
To investigate the effectiveness of our design choices, we compare our model with its ablated versions:

w/o image frames: We use a sequence of instance maps as input to our model.

w/o background representation: We remove the background encoder from our model.

w/o instance relation module: We remove the instance relation module from our model.

w/o ordering network: We remove the ordering network from our model.

w/o adversarial loss: We train the model without using the adversarial loss.

The results of the ablation study are shown in Table 3. Without utilizing image frames or background representation, the performance drops. This indicates that the visual appearances of objects and the background are useful signals for forecasting scene layouts. When the instance relation module is removed, the performance becomes worse, which implies that modeling instance-instance interactions is crucial to predicting the future motions of instances in a scene. In particular, we note that the performance for long-term prediction drops more significantly without this module. This indicates that modeling instance-instance interactions is of particular importance to long-term prediction. Without the ordering network, the F-measure is affected more than the Jaccard index. This is possibly because the incorrect instance order would significantly change some instance boundaries. Finally, without using the adversarial loss, the performance also drops slightly. This indicates that the adversarial training process could help generate instance maps with more realistic details.

Conclusion
In this work, we have studied the problem of forecasting scene layouts with an instance-aware approach. We have presented a learning framework to forecast the instance map of a scene at a future time from past image frames, by explicitly modeling the motion dynamics of the instances in the scene as well as instance-instance interactions. Through extensive experiments, we show that learning with instance-wise formulation is able to produce more accurate and sharp predictions, as compared with prior methods, and yields state-of-the-art performances.

Despite promising results, our method may fail to predict plausible future shapes for some highly non-rigid objects in a scene (e.g., people as shown in Fig. 9). This is because these objects can deform in many different ways, making it very challenging to predict their shapes. A possible solution to this problem is to group these highly deformable objects according to their semantic labels (e.g., people), and design a branch to especially model their shape dynamics, which can be an interesting future work.

Fig. 9
figure 9
Failure case in short-term prediction. When a scene contains many people with diverse poses, our model may not be able to predict plausible shapes due to the inherent ambiguity of predicting the shapes of highly non-rigid objects

