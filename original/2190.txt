Object detection typically assumes that training and test samples are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch may lead to a significant performance drop. In this work, we present Scale-aware Domain Adaptive Faster R-CNN, a model aiming at improving the cross-domain robustness of object detection. In particular, our model improves the traditional Faster R-CNN model by tackling the domain shift on two levels: (1) the image-level shift, such as image style, illumination, etc., and (2) the instance-level shift, such as object appearance, size, etc. The two domain adaptation modules are implemented by learning domain classifiers in an adversarial training manner. Moreover, we observe that the large variance in object scales often brings a crucial challenge to cross-domain object detection. Thus, we improve our model by explicitly incorporating the object scale into adversarial training. We evaluate our proposed model on multiple cross-domain scenarios, including object detection in adverse weather, learning from synthetic data, and cross-camera adaptation, where the proposed model outperforms baselines and competing methods by a significant margin. The promising results demonstrate the effectiveness of our proposed model for cross-domain object detection. The implementation of our model is available at https://github.com/yuhuayc/sa-da-faster.
Introduction
As a fundamental problem in computer vision, the goal of object detection is to identify and localize all object instances of certain categories in an input image. Driven by the recent success of deep convolutional neural networks (CNN) (Krizhevsky et al. 2012), numerous CNN-based object detection approaches have been proposed (Gidaris and Komodakis 2015; Girshick 2015; Girshick et al. 2014; Li et al. 2016; Liu et al. 2016; Sermanet et al. 2013) in recent years, drastically improving detection accuracy.

While good performance has been achieved on standard benchmark datasets (Everingham et al. 2010; Lin et al. 2014), object detection in the real world still faces challenges due to the large variance in viewpoints, object appearance, backgrounds, illumination, image quality, etc., which may cause a considerable domain shift between training and test data. Taking autonomous driving as an example, the camera type and setup used in a car may differ from that used to collect training data, and the car may be in a different city where the appearance of objects is different. Moreover, the autonomous driving system is expected to work reliably under diverse weather conditions (e.g. in rain and fog), while the training samples are usually collected in clear weather with better visibility. The recent trend of using synthetic data for training deep CNN models presents a similar challenge due to the visual mismatch between simulation and reality. As an example, several datasets on autonomous driving are illustrated in Fig. 1, where a considerable domain shift can be observed.

Such domain shifts have been observed to cause significant performance drop (Gopalan et al. 2011). Although collecting more training data could possibly alleviate the domain shift problem to some extent, it is still non-trivial in many real-world scenarios, as manually annotating bounding boxes is expensive and time-consuming. Therefore, it is highly desirable to develop algorithms to adapt object detection models to a new target domain, without the need of collecting additional ground-truth labels in the target domain.

Fig. 1
figure 1
Illustration of different datasets for autonomous driving. From top to bottom-right, example images are taken from: KITTI (Geiger et al. 2013), Cityscapes (Cordts et al. 2016), Foggy Cityscapes (Sakaridis et al. 2018), SIM10K (Johnson-Roberson et al. 2017). Although all datasets cover urban scenes, images across datasets vary in style, resolution, illumination, object size, etc. The visual difference between these datasets presents a challenge for applying an object detection model learned in one domain to another domain

Full size image
Many techniques have been proposed to tackle the domain adaptation problem for the task of image classification. However, such techniques are often designed for aligning features for the entire image, which may not be readily applicable to the task of object detection. The reasons are largely two-fold. On the one hand, since object detection aims to simultaneously predict object bounding boxes and class labels. A small shift in localization may lead to wrong class prediction, therefore the model is more vulnerable to data variation. On the other hand, the variation of objects across domains is often complex. In particular, as multiple instances are contained in one image, the domain shift could occur on both the image level (e.g., image scale, style, illumination, etc.) and the instance level (e.g., object appearance, size, etc.). The domain shift on these two levels can also be different.

In this work, we aim to address this cross-domain object detection problem, and propose a new Scale-aware Domain Adaptive Faster R-CNN (SA-DA-Faster) model to tackle these challenges. In particular, we consider the unsupervised domain adaptation scenario: full supervision is given in the source domain while no supervision is available in the target domain. Thus, the object detection accuracy in the target domain should be improved at no additional annotation cost.

We build our model based on the Faster R-CNN framework (Ren et al. 2015). On the one hand, in order to deal with the diverse domain shift both on the image and the instance level, we augment Faster R-CNN in the training with two adaptation modules, which minimize the domain discrepancy on the image level and the instance level respectively. In each component, we train a domain classifier and employ the adversarial training strategy to learn robust features that are domain-invariant. On the other hand, considering that localization is crucial for robustness of object detection models, we further incorporate a consistency regularization between image-level and instance-level domain classifier to learn a domain-invariant region proposal network (RPN) in the Faster R-CNN model. We refer to this model as Domain Adaptive Faster R-CNN (DA-Faster), which was mainly described in our preliminary work (Chen et al. 2018a).

To further equip the detection model against data variation in cross-domain scenarios, we additionally consider the challenge brought by the variation of object scales in cross-domain adaptation, and propose the Scale-aware Domain Adaptive Faster R-CNN model. In particular, the scale of objects in natural images can vary dramatically. For instance, in autonomous driving, due to the perspective projection effect, vehicles far away are usually very small, while the near ones are relatively larger in the image. As a result, performing a uniform feature alignment across all scales for domain adaptation would be difficult. Instead, it is more desirable to perform individual alignment between corresponding scale for two domains (i.e., aligning small objects to small ones, and large objects to large ones). Moreover, in some scenarios such as driving in foggy weather, the domain shift also varies across object scales. Fog generally obscures distant objects much more than closer ones. This also poses a demand for adapting differently for objects at different scales.

To address this issue, we explicitly take the object scale into consideration in the domain alignment process. Specifically, we adopt the feature pyramid network (FPN) (Lin et al. 2016) to produce multiple feature maps of different scales, which are used to detect instances of different sizes. To align the features, we build an image-level domain classifier respectively at each scale to align the representation of individual scales. Also, an instance-level domain classifier is built at each scale to align the instance features of individual scales. The domain classifiers at different scales do not share weights, so the feature alignment is performed separately at each scale. Consequently, each domain classifier only needs to focus on a certain scale and solve a relatively more local alignment problem, rather than a global feature alignment across all scales. We term the two newly proposed adaptation modules as scale-aware image-level adaptation and scale-aware instance-level adaptation respectively. The two modules can be easily incorporated into the Faster RCNN model, which leads to our final Scale-aware Domain Adaptive Faster R-CNN approach.

We conduct extensive experiments to evaluate our Domain Adaptive Faster R-CNN using multiple datasets including Cityscapes (Cordts et al. 2016), KITTI (Geiger et al. 2013), SIM 10k (Johnson-Roberson et al. 2017), and Foggy Cityscapes (Sakaridis et al. 2018). The experimental results clearly show the effectiveness of our proposed approach for addressing the domain shift of object detection in multiple scenarios with domain discrepancies.

A preliminary version of this work has been published in Chen et al. (2018a). Compared to the conference version, this paper makes the following additional contributions:

To tackle the problem caused by the large variation of scales in natural scenes, we develop a scale-aware adaptation module, which can leverage the scale information for better feature alignment.

We validate the proposed scale-aware adaptation module by extensive experiments and analyses. The promising results suggest that the scale-aware adaptation strategy is highly effective in different cross-domain scenarios. Our SA-DA-Faster model achieves new state-of-the-art results in cross-domain detection.

Additional experiments are conducted for DA-Faster model, including more advanced backbones, to further validate our two-level adaptation strategy.

Related Work
In this work, we are interested in cross-domain object detection. We provide an overview of the most related works in object detection and domain adaptation.

Object Detection
The aim of object detection is to detect all instances of objects of certain class(es) in an input image, by predicting their bounding box coordinates and semantic class. It is a fundamental problem in computer vision, which dates back to a long time ago, resulting in a plenitude of approaches. Early works formulated object detection as a sliding window classification problem (Dalal and Triggs 2005; Felzenszwalb et al. 2010; Viola and Jones 2001). The successes of deep learning (Krizhevsky et al. 2012) in computer vision have led to a paradigm shift in object detection. Numerous CNN-based detection models have been proposed (Gidaris and Komodakis 2015; Girshick 2015; Girshick et al. 2014; Li et al. 2016; Liu et al. 2016; Sermanet et al. 2013). Among them, the seminal work of region-based CNN (R-CNN) (Girshick 2015; Girshick et al. 2014; Zhang et al. 2016) has attracted significant attention due to its effectiveness and simplicity. This line of work was pioneered by R-CNN (Girshick et al. 2014), which extracts region of interest (RoI) using a proposal algorithm (Uijlings et al. 2013) and then classifies each RoI with a trained network. This method was then improved in Fast R-CNN (Girshick 2015) by sharing the convolution feature map among RoIs. In the same vein, Faster R-CNN (Girshick et al. 2014) further proposed to produce object proposals directly within CNN, with a Region Proposal Network (RPN), instead of relying on external proposal generation algorithms. Faster R-CNN achieved highly competitive performance and laid the foundation for many works in object detection (Gidaris and Komodakis 2015; Liu et al. 2016; Li et al. 2016; Lin et al. 2016; Zhang et al. 2016), and for other tasks such as instance segmentation (Dai et al. 2016; He et al. 2017). In the same framework, the feature pyramid network (FPN) (Lin et al. 2016) was proposed to deal with the scale variation in object detection, by building a feature pyramid within the convolutional neural network. In this work, we adopt FPN as a way to extract features of different scales.

Object detection literature has mainly focused on the within-domain setting, where training and validation are performed on the same data distribution, while ignoring the domain shift issue for object detection in the wild. In this work, we adopt the Faster R-CNN framework (Lin et al. 2016; Zhang et al. 2016) due to its effectiveness, and aim to improve its generalization to new target domains.

Domain Adaptation for Image Classification
Domain adaptation has been widely studied for image classification (Duan et al. 2012a, b; Fernando et al. 2013; Ganin and Lempitsky 2015; Ghifary et al. 2016; Gong et al. 2012; Gopalan et al. 2011; Kulis et al. 2011; Li et al. 2017; Long et al. 2015; Motiian et al. 2017; Panareda Busto and Gall 2017; Sener et al. 2016; Sun et al. 2015). Conventional methods include domain transfer multiple kernel learning (Duan et al. 2012a, b), asymmetric metric learning (Kulis et al. 2011), subspace interpolation (Gopalan et al. 2011), geodesic flow kernel (Gong et al. 2012), subspace alignment (Fernando et al. 2013), covariance matrix alignment (Sun et al. 2015), etc. Recent works aim to improve the domain adaptability of deep neural networks, including (Ganin and Lempitsky 2015; Ghifary et al. 2016; Haeusser et al. 2017; Li et al. 2017; Long et al. 2015; Lu et al. 2017; Maria Carlucci et al. 2017; Motiian et al. 2017; Panareda Busto and Gall 2017; Sener et al. 2016). A lot of methods have also been recently presented for performing unpaired image translation between two sets of data, which can be seen as pixel-level domain adaptation (Gong et al. 2019; Kim et al. 2017; Liu et al. 2017; Yi et al. 2017; Zhu et al. 2017).

However, these techniques are typically designed to align the feature distribution for the entire image, and it is non-trivial to repurpose them for other applications such as object detection. Different from these works, we focus on the object detection problem, which is arguably more challenging as domain shift can affect both object location and category prediction.

Domain Adaptation Beyond Classification
Compared to image classification, domain adaptation for other tasks has become to receive more attention only in the last few years.

Fig. 2
figure 2
Overview of the proposed Domain Adaptive Faster R-CNN (DA-Faster) model. ‘GRL’ is the gradient reverse layer. ‘Per-RoI’ represents the per-RoI (instance) feature extractor. We tackle the domain shift on two levels, the image level and the instance level. A domain classifier is built on each level, trained in an adversarial training manner. A consistency regularizer is incorporated within these two classifiers to learn a domain-invariant RPN for the Faster R-CNN model

Full size image
For the task of detection, Xu et al. (2014) proposed to mitigate the domain shift problem of the deformable part-based model (DPM) by introducing an adaptive SVM. Raj et al. (2015) proposed to use a subspace alignment method to align the features extracted using the R-CNN model. There also exist works on learning detectors from alternative sources, such as from images to videos (Tang et al. 2012), from 3D models (Peng et al. 2015; Sun and Saenko 2014), or from synthetic models (Hattori et al. 2015). The above works either cannot be trained in an end-to-end fashion, or focus on a specific case. Our preliminary work DA Faster R-CNN (Chen et al. 2018a) represents the first attempt towards building an end-to-end trainable model for cross-domain object detection. In the model, two levels of feature alignment are used to learn domain invariant features. Later on, various techniques have been proposed to tackle the domain shift in object detection for deep models. In particular, SCDA (Zhu et al. 2019) introduced a model which focuses on aligning the discriminative regions. MTOR (Cai et al. 2019) explored object relation in region-level consistency, inter-graph consistency and intra-graph consistency for cross-domain object detection. SWDA (Saito et al. 2019) utilized strong and weak domain classifiers to align local and global features respectively. Similar multi-level design has been studied in MAF (He and Zhang 2019) and by Xie et al. (2019). Pixel-level adaptation has been explored for the task of cross-domain object detection. In more detail, Shan et al. (2019) employed an image translation technique to reduce the domain discrepancy in pixel space. DM (Kim et al. 2019) additionally proposed to tackle the imperfections in image translation and the source-biased discriminativity problem. SPLAT (Tzeng et al. 2018) leveraged information in aligned image pairs. Label-level adaptation has also been discussed in a few works (Khodabandeh et al. 2019; Kim et al. 2019; RoyChowdhury et al. 2019) for cross-domain detection and yielded improved performance.

There are also some works discussing domain adaptation for other tasks such as semantic segmentation (Chen et al. 2018b, 2019; Gong et al. 2019; Hoffman et al. 2016; Tsai et al. 2018; Zhang et al. 2017), fine-grained recognition (Gebru et al. 2017), monocular depth estimation (Zhao et al. 2019) etc., which are also related to our work.

Preliminaries
Faster R-CNN Framework
We briefly review the Faster R-CNN (Zhang et al. 2016) framework. Faster R-CNN is a two-stage detector consisting of three major components: shared bottom convolutional layers, a region proposal network (RPN) and a region-of-interest (RoI) based classifier. The architecture is illustrated in the left part of Fig. 2.

First, an input image is represented as a convolutional feature map produced by the shared bottom convolutional layers. Based on that feature map, RPN generates candidate object proposals, whereafter the RoI-wise classifier predicts the category label from a feature vector obtained by pooling the feature map within the proposal. The training loss consists of the loss of the RPN and the loss of the RoI classifiers

𝑑𝑒𝑡=𝑟𝑝𝑛+𝑟𝑜𝑖.
(1)
The training losses of the RPN and the RoI classifier both have two terms: one pertains to classification and measures the accuracy of the predicted class-level probabilities, and the other is a regression loss on the box coordinates for better localization. Readers are referred to Zhang et al. (2016) for more details about the framework and the training procedure.

Distribution Alignment with -Divergence
The -divergence (Ben-David et al. 2010) is designed to measure the divergence between two sets of samples with different distributions. Let us denote by 𝐱 a feature vector. A source domain sample can be denoted as 𝐱 and a target domain sample as 𝐱. We also denote by ℎ:𝐱→{0,1} a domain classifier, which aims to predict the source domain samples 𝐱 to be 0, and the target domain samples 𝐱 to be 1. Supposing  is the set of all possible domain classifiers, -divergence defines the distance between two domains as

𝑑(,)=2(1−minℎ∈(𝑒𝑟𝑟(ℎ(𝐱))+𝑒𝑟𝑟(ℎ(𝐱)))),
where 𝑒𝑟𝑟 and 𝑒𝑟𝑟 are the prediction errors of ℎ(𝐱) on source and target domain samples respectively. The above definition implies that the domain distance 𝑑(,) is inversely proportional to the error rate of the domain classifier h. In other words, if the error is high for the best domain classifier, the two domains are hard to distinguish, so they are close to each other, and vice versa.

In deep neural networks, the feature vector 𝐱 usually comprises the activations after a certain layer. Let us denote by f the network that produces 𝐱. To align the two domains, we therefore need to enforce the networks f to output feature vectors that minimize the domain distance 𝑑(,) (Ganin and Lempitsky 2015), which leads to

min𝑓𝑑(,)⇔max𝑓minℎ∈{𝑒𝑟𝑟(ℎ(𝐱))+𝑒𝑟𝑟(ℎ(𝐱))}
This can be optimized in an adversarial training manner. Alternatively, this objective can also be optimized by reversing the gradient direction, as shown in Ganin and Lempitsky (2015), where a gradient reversal layer (GRL) is integrated into a CNN for image classification in the unsupervised domain adaptation scenario.

Domain Adaptation for Object Detection
Following the common terminology in domain adaptation, we refer to the domain of the training data as source domain, denoted by , and to the domain of the test data as target domain, denoted by . For instance, when using the Cityscapes dataset for training and the KITTI dataset for testing,  is the Cityscapes domain and  stands for the KITTI domain. We adopt the unsupervised domain adaptation protocol, where we have access to images and full supervision in the source domain (i.e., bounding box and object categories), but only have access to the unlabeled images in the target domain. The aim is to learn an object detection model adapted to a target domain by only using unlabeled data in that domain.

A Probabilistic Perspective
The object detection problem can be viewed as learning the posterior P(Y, B|I), where I is the image representation, B is the bounding-box of an object and 𝑌∈{1,…,𝐾} the category of the object (K being the total number of categories).

Let us denote the joint distribution of training samples for object detection as P(Y, B, I). Then we denote the source domain joint distribution as 𝑃(𝑌,𝐵,𝐼), and the target domain joint distribution as 𝑃(𝑌,𝐵,𝐼). Note that here we use 𝑃(𝑌,𝐵,𝐼) to analyze the domain shift problem, although the bounding box and category annotations (i.e., B and Y) are unknown during training. In the presence of domain shift, 𝑃(𝑌,𝐵,𝐼)≠𝑃(𝑌,𝐵,𝐼).

Image-Level Adaptation Based on the Bayes’s Formula, the joint distribution can be decomposed as

𝑃(𝑌,𝐵,𝐼)=𝑃(𝑌,𝐵|𝐼)𝑃(𝐼).
(2)
Similar to the classification problem, we make the covariate shift assumption for objection detection, i.e., the conditional probability P(Y, B|I) remains the same for the two domains, and the domain distribution shift is caused by the difference on the marginal distribution P(I). In other words, the detector is consistent between two domains: given an image, the detection results should be the same regardless of which domain the image belongs to. In the Faster R-CNN model, the image representation I is actually the feature map output of the base convolutional layers. Therefore, to handle the domain shift problem, we should enforce the distribution of image representation from two domains to be the same (i.e., 𝑃(𝐼)=𝑃(𝐼)), which is referred to as image-level adaptation.

Instance-Level Adaptation On the other hand, the joint distribution can also be decomposed as

𝑃(𝑌,𝐵,𝐼)=𝑃(𝑌|𝐵,𝐼)𝑃(𝐵,𝐼).
(3)
With the covariate shift assumption, i.e., the conditional probability P(Y|B, I) is the same for the two domains, we have that the domain distribution shift is from the difference in the marginal distribution P(B, I). Intuitively, this implies the semantic consistency between two domains: given the same image region containing an object, its category labels should be the same regardless of which domain it comes from. Therefore, we can also enforce the distribution of instance representation from two domains to be the same (i.e., 𝑃(𝐵,𝐼)=𝑃(𝐵,𝐼)). We refer to it as instance-level alignment.

Here the instance representation (B, I) refers to the features extracted from the image region in the ground truth bounding box for each instance. Although the bounding-box annotation is unavailable for the target domain, we can obtain it via 𝑃(𝐵,𝐼)=𝑃(𝐵|𝐼)𝑃(𝐼), where P(B|I) is a bounding box predictor (e.g., RPN in Faster R-CNN). This holds only when P(B|I) is domain-invariant, for which we provide a solution below.

Joint Adaptation Ideally, one can perform domain alignment on either the image or instance level. Considering that 𝑃(𝐵,𝐼)=𝑃(𝐵|𝐼)𝑃(𝐼) and the conditional distribution P(B|I) is assumed to be the same and non-zero for two domains, thus we have

𝑃(𝐼)=𝑃(𝐼)⇔𝑃(𝐵,𝐼)=𝑃(𝐵,𝐼).
(4)
In other words, if the distributions of the image-level representations are identical for two domains, the distributions of the instance-level representations are also identical, and v.v. Yet, it is generally non-trivial to perfectly estimate the conditional distribution P(B|I). The reasons are two-fold: 1) in practice it may be hard to perfectly align the marginal distributions P(I), which means the input for estimating P(B|I) is somehow biased, and 2) the bounding box annotation is only available for source domain training data, therefore P(B|I) is learned using the source domain data only, which is easily biased toward the source domain.

To this end, we propose to perform domain distribution alignment on both the image and instance levels, and to apply a consistency regularization to alleviate the bias in estimating P(B|I). As introduced in Sect. 3.2, to align the distributions of two domains, one needs to train a domain classifier ℎ(𝐱). In the context of object detection, 𝐱 can be the image-level representation I or the instance-level representation (B, I). From a probabilistic perspective, ℎ(𝐱) can be seen as estimating a sample 𝐱’s probability belonging to the target domain.

Thus, by denoting the domain label as D, the image-level domain classifier can be viewed as estimating P(D|I), and the instance-level domain classifier can be seen as estimating P(D|B, I). By using the Bayes’ theorem, we obtain

𝑃(𝐷|𝐵,𝐼)𝑃(𝐵|𝐼)=𝑃(𝐵|𝐷,𝐼)𝑃(𝐷|𝐼).
(5)
In particular, P(B|I) is a domain-invariant bounding box predictor, and P(B|D, I) a domain-dependent bounding box predictor. Recall that in practice we can only learn a domain-dependent bounding box predictor P(B|D, I), since we have no bounding box annotations for the target domain. Thus, by enforcing the consistency between two domain classifiers, i.e., 𝑃(𝐷|𝐵,𝐼)=𝑃(𝐷|𝐼), we could learn P(B|D, I) to approach P(B|I).

Domain Adaptation Modules
This section introduces the domain adaptation components used in our model: image-level adaptation and instance-level adaptation, used to align the feature representation distributions on those two different levels.

Image-Level Adaptation
In the Faster R-CNN model, the image-level representation refers to the feature map outputs of the base convolutional layers (see the green parallelogram in Fig. 2). To eliminate the domain distribution mismatch on the image level, we employ a patch-based domain classifier as shown in the lower right part of Fig. 2.

In particular, we train a domain classifier on each activation from the feature map. Since the receptive field of each activation corresponds to an image patch of the input image, the domain classifier actually predicts the domain label for each image patch.

The benefits of this choice are twofold: (1) aligning image-level representations generally helps to reduce the shift caused by the global image difference such as image style, image scale, illumination, etc. A similar patch-based loss has shown to be effective in recent work on style transfer (Johnson et al. 2016), which also deals with the global transformation, and (2) the batch size is usually very small for training an object detection network, due to the use of high-resolution input. This patch-based design is helpful to increase the number of training samples for training the domain classifier. More formally, we denote by z the domain label of a training image, with 𝑧=0 for the source domain and 𝑧=1 for the target domain. We denote as C the feature map of the input image after the base convolutional layer, and 𝐶(𝑢,𝑣) the activation located at (u, v). The output of the domain classifier is denoted as 𝑝(𝑢,𝑣). Using the cross entropy loss, the image-level adaptation loss for an input image can be written as

𝑖𝑚𝑔=−∑𝐶(𝑢,𝑣)∈𝐶[𝑧log𝑝(𝑢,𝑣)+(1−𝑧)log(1−𝑝(𝑢,𝑣))].
(6)
As discussed in Sect. 3.2, in order to align the domain distributions, we should simultaneously optimize the parameters of the domain classifier to minimize the above domain classification loss, and optimize the parameters of the base network to maximize this loss. For the implementation we use the gradient reverse layer (GRL) (Ganin and Lempitsky 2015), whereas the ordinary gradient descent is applied for training the domain classifier. The sign of the gradient is reversed when passing through the GRL layer to optimize the base network.

Instance-Level Adaptation
The instance-level representation refers to the RoI-based feature vectors before feeding into the final per-RoI category classifiers and box regressor. Aligning the instance-level representations helps to reduce the local instance difference such as object appearance, size, viewpoint etc. Similar to the image-level adaptation, we train a domain classifier for the feature vectors to align the instance-level feature distribution. We denote all instance features as Q, and the feature extracted from the i-th instance as 𝑄𝑖. For an instance feature 𝑄𝑖, the output of the instance-level domain classifier is represented as 𝑝𝑖. The instance-level adaptation loss of an input image can now be written as

𝑖𝑛𝑠=−∑𝑄𝑖∈𝑄[𝑧log𝑝𝑖+(1−𝑧)log(1−𝑝𝑖)].
(7)
Similarly with image-level adaptation, we add a gradient reverse layer before the domain classifier to apply the adversarial training strategy.

Consistency Regularization
As analyzed in Sect. 4.1, enforcing consistency between the domain classifier on different levels helps to learn the cross-domain robustness of bounding box predictor (i.e., RPN in the Faster R-CNN model). Therefore, we further impose a consistency regularizer. Since the image-level domain classifier produces an output for each activation of the image-level representation C, we take the average over all activations in the image as its image-level probability, which is implemented as a global average pooling operation. The consistency regularizer can be written as

𝑐𝑠𝑡=∑𝑄𝑖∈𝑄‖1|𝐶|∑𝐶(𝑢,𝑣)∈𝐶𝑝(𝑢,𝑣)−𝑝𝑖‖1
(8)
where |C| denotes the total number of activations in the convolutional feature map, and ‖⋅‖1 is the ℓ1 distance.

Scale-Aware Domain Adaptive Faster R-CNN
We have presented above a Domain Adaptive Faster R-CNN framework for cross-domain adaptation. However, as discussed in the Introduction, the data variation across different domains is often complex (e.g., image scale, style, illumination, object appearance, size, etc.). These variances occur both within a single domain and across domains, rendering the alignment of the feature distributions between two domains extremely challenging. Although the previously introduced techniques can help to reduce the feature discrepancy between the source and target domains, considerable misalignment can still exist between the two domains due to the large data variance. Among the factors of misalignment, scale is very important. Therefore, we place particular interest on the scale issue and design a Scale-aware Domain Adaptive Faster R-CNN (SA-DA-Faster) model.

Fig. 3
figure 3
Illustration on the influence of scale on domain shift. A clear image from Cityscapes is shown on the left, while the corresponding foggy image from Foggy Cityscapes is shown on the right. Note the difference in domain shift in the close car (shown in blue box) and the distant car (shown in red box)

Full size image
In particular, the scale of objects in natural images can vary dramatically (Chen et al. 2016; Lin et al. 2016). For example, cars that are far away are usually very small in an image, while the near ones are relatively larger. Thus, the distributions of features extracted from objects at different scales can be very different. Besides, the domain shift can also be different at different scales. We illustrate such effect with an example in Fig. 3. Therefore, a uniform feature alignment across all scales, as in Domain Adaptive Faster R-CNN, may not be sufficient. Instead it is more feasible to perform individual alignment between corresponding scale for two domains (i.e., aligning small objects to small ones, and large object to large ones).

Fig. 4
figure 4
Illustration of the proposed Scale-aware Domain Adaptive Faster R-CNN (SA-DA-Faster) model. We employ a feature pyramid network to split the features for objects of different size. In the example shown in the figure, we show only three pyramid levels due to space limitation, while in experiments five pyramid levels are used. We build an independent module for each pyramid level to independently align the features of a certain scale. ‘DA’ block stands for our proposed scale-aware adaptation module. ‘GRL’ is the gradient reverse layer. ‘Per-RoI’ represents the per-RoI(instance) feature extractor

Full size image
Motivated by this observation, we further incorporate the scale variance issue into our Domain Adaptive Faster R-CNN to explicitly model the scale change in the feature space, and leverage such additional information for a better feature alignment. In our new SA-DA-Faster model, we first build a feature pyramid in the detector to extract the features from different scales. Then, we generalize the previously introduced adaptation modules to the scale-aware case for aligning features at corresponding scales. In the following, we will first briefly review the feature pyramid network (FPN), and then present our domain alignment modules based on it.

Feature Pyramid for Object Detection
In typical CNN designs, scale is coupled with the level of abstraction, as features from earlier layers in networks are more local and represent low-level features such as edge, while the features from higher layers are coarser in resolution and represent more complicated structure, such as semantics. In contrast to the previous works that additionally aligns local features (He and Zhang 2019; Saito et al. 2019; Xie et al. 2019), in this work our main objective is to align features for different scales. Therefore feature scale needs to be decoupled from the level of abstraction. The work of feature pyramid network provides a solution by building a pyramid of features within convolutional neural network. We use a network with five blocks (which is typical in ResNet or VGG) as an example to introduce the FPN structure. FPN features two pathways: a bottom-up pathway and a top-down pathway. The concept is shown with an example in the left part of Fig. 4. The bottom-up pathway is identical with a typical feedforward network. We denote the original conv1, conv2, conv3, conv4, conv5 features as {𝐶1,𝐶2,𝐶3,𝐶4,𝐶5}. A pyramid stage is defined for each stage. Each stage naturally has a different scale due to the difference feature resolution of that stage. Then a top-down pathway is built to propagate the semantically stronger high-level feature to lower layers of the network. In more details, 𝑃5 is generated by passing 𝐶5 through a 1×1 convolutional layer. Then at each stage, the feature from the high-level 𝑃𝑖, up sample by 2, is summed with the feature from 𝐶(𝑖−1) passed through a 1×1 convolutional layer. In this way, the feature of each layer can be representative for the scale at that stage, and at the same time contains high-level information. Thus, the scale can be decoupled from the level of abstraction, as we desired. The resulting feature map set is denoted as {𝑃1,𝑃2,𝑃3,𝑃4,𝑃5}, which corresponds respectively to the same scale/resolution of {𝐶1,𝐶2,𝐶3,𝐶4,𝐶5}. But in contrast to {𝐶1,𝐶2,𝐶3,𝐶4,𝐶5}, which become increasingly more semantic, the features in {𝑃1,𝑃2,𝑃3,𝑃4,𝑃5} are all aware to semantic information, and can thus be used for object detection.

FPN backbone is widely used with Faster R-CNN framework. With a FPN backbone, a RPN can built on each pyramid stage (e.g., {𝑃1,𝑃2,𝑃3,𝑃4,𝑃5}) to produce object proposals of different scales. Then the RoI features are extracted from each object proposal for the final classifier. The same set of loss introduced in Sect. 4 is used to learn the detector. Readers are referred to Lin et al. (2016) for more details regarding Faster R-CNN with FPN.

Scale-Aware Adaptation
As the features of different scales are divided into different stages by FPN, scale-aware feature adaptation can then be performed by aligning separately each stage of feature pyramid. We extend the concept of image-level and instance-level to enhance the scale awareness of the two adaptation components, introduced as follows

Scale-Aware Image-level Adaptation The image-level feature representation is divided into several levels in the feature pyramid i.e., {𝑃1,𝑃2,…,𝑃𝑠,…}, based on the scale. To align the image-level features, we proposed a Scale-aware Image-level Adaptation module, which builds s image-level domain classifiers, one for each stage at {𝑃1,𝑃2,…,𝑃𝑠,…}. Each domain classifier has the same architecture as the one introduced in Sect. 5.1 which is similar to a PatchGAN and can produce domain label prediction for the activation in a dense manner.

We denote by z the domain label of a training image, with 𝑧=0 for the source domain and 𝑧=1 for the target domain. We denote as 𝑃𝑠(𝑢,𝑣) the activation located at (u, v) of the feature map 𝑃𝑠. By denoting the output of the domain classifier as 𝑝𝑠(𝑢,𝑣) and using the cross entropy loss, the image-level adaptation loss for an input image can be obtained by summing the loss among all scales, and all locations. Thus the loss is written as

𝑠𝑎−𝑖𝑚𝑔=−∑𝑠∑𝑃𝑠(𝑢,𝑣)∈𝑃𝑠[𝑧log𝑝𝑠(𝑢,𝑣)+(1−𝑧)log(1−𝑝𝑠(𝑢,𝑣))].
(9)
Each discriminator is connected to the image feature 𝑃𝑠 by a gradient reverse layer, which reverses the gradient direction in the backward pass, for the purpose of adversarial training to align feature distribution.

Scale-Aware Instance-level Adaptation Similarly, the concept of scale-aware adaptation can generalize to instance-level adaptation, as each feature map is responsible for producing detection of different scales. We denote the instance-level features from each scale as {𝑄1,𝑄2,…,𝑄𝑠,…}. Then an individual instance-level domain classifier is built on each pyramid stage to discriminate the instance feature of a certain scale.

Similar to before, z is the domain label of a training image. We denote as 𝑝𝑠𝑖 the prediction of domain classifier for the feature 𝑄𝑠𝑖, which is the i-th feature at pyramid stage s. The instance-level adaptation loss is derived by applying softmax loss between z and 𝑝𝑠𝑖. By summing up all instances among all pyramid stages, we have the scale-aware instance adaptation loss

𝑠𝑎−𝑖𝑛𝑠=−∑𝑠∑𝑄𝑠𝑖∈𝑄𝑠[𝑧log𝑝𝑠𝑖+(1−𝑧)log(1−𝑝𝑠𝑖)].
(10)
In the training, the gradient from the domain classifier is reversed by gradient reverse layer, thus to make the instance-level feature domain invariant.

Network Overview
We illustrate our DA-Faster model in Fig. 2, and SA-DA-Faster model in Fig. 4.

The left part of Fig. 2 is the original Faster R-CNN model. The bottom convolutional layers are shared between all components. Then the RPN and RoI pooling layers are built on top, followed by RoI head to extract the instance-level features. We augment the Faster R-CNN base architecture with our domain adaptation components, which leads to our Domain Adaptive Faster R-CNN model.

Three novel components are introduced in our Domain Adaptive Faster R-CNN. The image-level domain classifier is added after the last convolution layer and the instance-level domain classifier is added to the end of the RoI-wise features. The two classifiers are linked with a consistency loss to encourage the RPN to be domain-invariant. The final training loss of the proposed network is a summation of each individual part, which can be written as

=𝑑𝑒𝑡+𝜆(𝑖𝑚𝑔+𝑖𝑛𝑠+𝑐𝑠𝑡),
(11)
where 𝜆 is a trade-off parameter to balance the Faster R-CNN loss and our newly added domain adaptation components. The network can be trained in an end-to-end manner using a standard SGD algorithm. Note that the adversarial training for domain adaptation components is achieved by using the GRL layer, which automatically reverses the gradient during propagation. The overall network in Fig. 2 is used in the training phase. During inference, one can remove the domain adaptation components, and simply use the original Faster R-CNN architecture with adapted weights.

With the new scale-aware adaptation module, the SA-DA-Faster model can be trained in similar manner, simply by replacing 𝑖𝑚𝑔 with the scale-aware image-level adaptation loss 𝑠𝑎−𝑖𝑚𝑔 defined in Eq. 9, and replacing 𝑖𝑛𝑠 with the scale-aware instance-level loss 𝑠𝑎−𝑖𝑛𝑠 in Eq. 10.

Experiments
Experimental Setup
We adopt the unsupervised domain adaptation protocol in our experiments. The training data consists of two parts: the source training data for which images and their annotations (bounding boxes and instance categories) are provided, and the target training data for which only unlabeled images are available. The adapted model is on a test split with the same data distribution as the target training data. For all experiments, mean average precision (mAP) with a threshold of 0.5 is used as the main evaluation metric.

Unless otherwise specified, we use ResNet-50 as the detection backbone, with FPN (Lin et al. 2016) to split the features into different scales. All training and test images are resized so that the shorter side has a length of 800 pixels to fit in GPU memory. The model is initialized using weights pretrained on ImageNet. Each batch consists of 1 source image and 1 target image. We set 𝜆=0.1 for the adaptation modules in Equation 11. The model is trained for 60k iterations, with initial learning rate of 0.0025, and weight decay of 0.0001.

Main Experimental Results
In this section we evaluate our proposed Domain Adaptive Faster R-CNN model for object detection in four cross-domain scenarios: (1) learning from synthetic data, where the training data is captured from video games, while the test data comes from the real world; (2) driving in adverse weather, where the training data pertain to decent weather conditions, while the test data to foggy weather; (3) cross-camera adaptation, where the training data and test data are captured with different camera setups; (4) adaptation to paintings, where the training data consists of annotated real images, while the test data painting images.

To better assess the individual contribution of each components, we include the results from the ablated versions of our model.

Learning from Synthetic Data
As computer graphics technique advances, synthetic data has became an attractive source to train neural networks, as the annotation can be acquired at a much lower cost. However, synthetic data still exhibits a clear visual difference with real world images, and usually a notable performance drop is observed when compared with models trained on real data. Thus, our first experiment is to examine the effectiveness of the proposed model in such scenario. We use the SIM 10k (Johnson-Roberson et al. 2017) dataset as the source domain, and the Cityscapes dataset as the target domain, which we briefly introduce in the following.

Datasets SIM 10k (Johnson-Roberson et al. 2017) consists of 10, 000 images which are generated by the video game Grand Theft Auto (GTAV). In SIM 10k, bounding boxes of 58, 701 cars are provided in the 10, 000 training images. All images are used in the training. The Cityscapes (Cordts et al. 2016) dataset is an urban scene dataset depicting driving scenarios. The images are captured by a car-mounted video camera. The dataset includes 2975 images in the training set, and 500 images in the validation set. We use the unlabeled images from the training set as the target domain to adapt our detector, and the results are reported on the validation set. There are 8 categories with instance labels in Cityscapes, but only car is used in this experiment since car is the only category annotated in SIM 10k. Since only instance masks are provided in Cityscapes, to generate the bounding boxes, we take the tight rectangle of each instance mask as the ground-truth bounding box.

Table 1 Evaluation on SIM10K → Cityscapes
Full size table
Table 2 Evaluation on Cityscapes → Foggy Cityscapes
Full size table
Results The experimental results are summarized in Table 1. Faster R-CNN with a ResNet-50 backbone achieves 32.9% in AP. Compared with the non-adaptive Faster R-CNN baseline, we achieve 36.2% in AP (+3.3% gain over baseline) by using the image-level adaptation component only, and 39.5% in AP (+6.6% gain over baseline) using instance-level alignment only. This proves that the proposed image-level adaptation and instance-level adaptation components can reduce the domain shift on each level effectively. Combining those two components yields an AP of 40.7% (+7.8% gain over baseline), which validates our conjecture on the necessity of reducing domain shifts on both levels. The consistency regularization brings a slight improvement of +0.5%, resulting a AP of 41.2%, which is also our Domain Adaptive Faster R-CNN (DA-Faster) model.

In order to build our Scale-aware Domain Adaptive Faster R-CNN (SA-DA-Faster) model, first we replace the backbone with a ResNet-50 with FPN. This non-adaptive baseline has resulted 36.7% in AP, outperforming the ResNet-50 backbone. However, the improvement is rather modest compared to the within-domain scenario, where FPN shows a remarkable improvement for object detection. This suggests that it is not guaranteed that the improvement due to backbone will be kept for the cross-domain case, so adaptation module is needed to address this issue. By applying the proposed scale-aware adaptation module, our proposed SA-DA-Faster model achieves 55.8 in AP, with a significant improvement of +19.1% compared to the non-adaptive FPN baseline. The improvement is also considerably larger when compared to the DA-Faster model, which demonstrates the effectiveness of the proposed scale-aware adaptation strategy.

Driving in Adverse Weather
We proceed with further evaluation by studying domain shift between weather conditions. Weather is an important source of domain discrepancy, because scenes are visually different as weather conditions change. Whether a detection system can operate reliably in different weather conditions is critical for a safe autonomous driving system (Narasimhan and Nayar 2002; Sakaridis et al. 2018). In this section, we investigate the ability to detect objects when we adapt a model from normal to foggy weather.

Datasets Cityscapes is used as our source domain, with images predominantly obtained in clear weather. In this experiment we report our results on categories with instance annotations: person, rider, car, truck, bus, train, motorcycle and bicycle. Similar with the previous experiment, we obtain the ground-truth bounding box by taking the tightest rectangle around the respective instance mask.

For the target domain, we use the Foggy Cityscapes dataset (Sakaridis et al. 2018). Foggy Cityscapes is a synthetic foggy dataset in that it simulates fog on real scenes. The foggy images are rendered using the original clear-weather images as well as the depth maps of Cityscapes. Examples can be found at Figs. 1 and 3 and in the related paper (Sakaridis et al. 2018). The semantic annotations and data split of Foggy Cityscapes are inherited from Cityscapes, making it ideal to study the domain shift caused by variation in visibility. Foggy Cityscapes contains multiple versions, each with a fixed level of fog density, however, in our experiments we restrict ourselves to the version with level of fog density corresponding to a visibility range of 150 meters (Sakaridis et al. 2018).

We use the training set from Cityscapes as the (labeled) source training set, and the training set from Foggy Cityscapes as the (unlabeled) target training set. The results are reported on the validation set from Foggy Cityscapes.

Results Table 2 presents our results and those of other baselines. Similar observations apply in this scenario. Our DA-Faster model improves the cross-domain performance from 25.6% to 41.3%, while the SA-DA-Faster model improves the non-adaptive baseline from 30.3% to 44.0%. The results again confirm the effectiveness of our proposed two-level adaptation modules, and of the scale-aware approach. Besides, we notice a general improvement across most of the categories, which shows that the proposed technique can reduce domain discrepancy across different object classes.

Cross-Camera Adaptation
Domain shift commonly exists even between real datasets taken under similar weather conditions, as different datasets may differ in setups for capture, image quality/resolution etc. Thus each dataset usually exhibits some bias when collecting the data (Torralba and Efros 2011). For detection, different datasets also vary drastically in scale, size and class distribution, sometimes it is difficult to determine the source of a domain shift. In this part, we focus on studying adaptation between two real datasets, for which we use KITTI and Cityscapes in this experiment.

Table 3 Evaluation on Cityscapes ↔ KITTI
Full size table
Table 4 Evaluation on Pascal → Watercolor
Full size table
Datasets We use KITTI training set which contains 7481 images. The dataset is used in both adaptation and evaluation. Images have original resolution of 1250×375, and are resized so that shorter length is 500 pixels long. Cityscapes is used as the other domain. Consistent with the first experiment, we evaluate our method using AP of car,

Results We apply the proposed method in both adaptation directions, we denote KITTI to Cityscapes as 𝐾→𝐶 and vice versa. Table 3 summarizes our results, where we can observe a clear performance gain by using the proposed adpation modules. And our method is useful for both adaptation directions 𝐾→𝐶 and 𝐶→𝐾.

Adaptation to Paintings
In this experiment, we study the effectiveness of the proposed approach on adapting real imagery to painted imagery. Following the protocol adopted in the previous work (Saito et al. 2019), we use the Pascal VOC dataset as the source domain. This dataset contains ground-truth bounding boxes for objects in 20 categories. We combine PASCAL VOC 2007 and 2012 training and validation set for training, resulting around 15k training images. Then either the Clipart or the Watercolor dataset (Inoue et al. 2018) is used as the target domain. The two datasets contain painted images of different styles. Clipart consits of 1000 images with the same 20 categories with PASCAL, the images are used for training for adaptation and reporting the results. While the Watercolor dataset contains 2000 images and a subset of 6 categories from PASCAL. We use the training set for adapting, and evaluate on the 1000 images in the testing set. Note that no ground-truth label from the target domain is used in the training for the unsupervised domain adaptation protocol.

Table 5 Evaluation on Pascal → Clipart
Full size table
We summarize the results on Clipart in Table 5, and the results on Watercolor in Table 4. In both scenarios, the proposed modules can help to reduce the domain discrepancy and improve the cross-domain detection performance. The results suggest that the proposed method is useful in a wide range of applications.

Experimental Analysis
This section presents additional experimental analysis, aiming at providing further understanding of the proposed models. Firstly, to understand the different roles of the proposed image-level and instance-level adaptation, we provide a study to analyze the detection error in Sect. 7.3.1, and a study on the influence of input size in Sect. 7.3.2. Then we investigate the proposed scale-aware adaptation module in more details, by testing various adaptation designs in Sect. 7.3.3. Lastly we visualize the feature distribution aligned by different adaptation strategies in in Sect. 7.3.4. We use the 𝑆𝑖𝑚10𝑘→Cityscapes for all the experiments in this section. Unless otherwise specified, all settings remain the same with Sect. 7.2.1.

Error Analysis on Top Ranked Detections
In the previous sections, we have shown that both image-level and instance-level alignment help to decrease domain discrepancy. To further validate the individual effect of image-level adaptation and instance-level adaptation, we analyze the accuracies caused by most confident detections for models using adaptation components on different levels.

Fig. 5
figure 5
Error analysis of top ranked detections. Results are reported using the 𝑆𝑖𝑚10𝑘→Cityscapes setup. Top 20, 000 detections are categorized into three types: ‘Cor’ (correct), ‘MisLoc’ (mis-localized), and ‘BG’ (background)

Full size image
Fig. 6
figure 6
A study on different target input sizes. Results are reported using the 𝑆𝑖𝑚10𝑘→Cityscapes setup. Source images from sim10k are fixed at a scale of 800 pixels, and we resize the target images from Cityscapes to different scales, as shown in x-axis

Full size image
We use 𝑆𝑖𝑚10𝑘→Cityscapes as the study case. We select 20, 000 predictions with highest confidence for the vanilla Faster R-CNN model, our model with only image-level adaptation, and our model with only instance-level adaptation, respectively. Inspired by (Hoiem et al. 2012), we categorize the detections into three types: correct: The detection has an overlap greater than 0.5 with ground-truth. mis-localized: The detection has an overlap with ground-truth between 0.3 and 0.5, and background: the detection has an overlap smaller than 0.3, which means it takes a background as a false positive.

Table 6 A study on the domain adaptation in feature pyramid network
Full size table
Fig. 7
figure 7
t-SNE visualization of the instance-level features. We show the features extracted with different models. The first row shows the scale label, while the second row presents the domain label

Full size image
Table 7 Per scale evaluation
Full size table
The results are shown in Fig. 5. From the figure we can observe that each individual component (image-level or instance-level adaptation) improves the number of correct detections (blue color), and dramatically reduces the number of false positives (other colors). Moreover, we also observe that the model using instance-level alignment gives higher background error than the model using image-level alignment. The reason might be the image-level alignment improves RPN more directly, which produces region proposals with better localization performance.

Table 8 Comparison with state-of-the-arts on SIM10K → CS
Full size table
Table 9 Comparison with state-of-the-arts on Cityscapes → Foggy Cityscapes
Full size table
Image-Level Versus Instance-Level Alignment
To further analyze the impact of image-level and instance-level adaptation, we conduct an experiment on Sim10k→Cityscapes by varying the image scales. Because different cameras are used in the two datasets, the different camera parameters might lead to a scale drift between two domains.

In particular, we refer to the shorter length of an image as its scale. To study how image scale affects our two domain adaptation components, we vary the size of images in the target domain to see how this affects the behavior of the two components while the scale in the source domain is fixed to 800 pixels.

Table 10 Comparison with state-of-the-arts on Cityscapes ↔ KITTI
Full size table
Table 11 Comparison with state-of-the-arts on Watercolor
Full size table
We plot the performance of different models in Fig. 6. By varying the scale of target images, we observe that the performance of the vanilla Faster R-CNN (i.e., non-adapt) drops significantly when the scales are mismatched. Comparing the two adaptation models, the image-level adaptation model is more robust to scale change than the instance-level adaptation model.

The reason behind this is that the scale change is a global transformation, which affects all instances and background. And in our design, global domain shift is mainly tackled by image-level alignment, and instance-level alignment is used to minimize instance-level discrepancy. When there is a serious global domain shift, the localization error of instance proposals goes up, thus the accuracy of instance-level alignment is damaged by deviating proposals. Nevertheless, using both always yields the best results across all scales. Contrary to the vanilla Faster R-CNN, our model can benefit from high resolution of target images, and performs increasingly better as the input size rises.

Ablation Study on Scale-Aware Alignment
In this study we investigate the performance of various strategies for aligning feature representation. We use Sim10k→Cityscapes for this study. For a fair comparison, all experiments are conducted with ResNet-50 with FPN. To be consistent with the notation in Sect. 6.1. We denote the convolutional feature in bottom-up pathway as 𝐶1,𝐶2,𝐶3,𝐶4,𝐶5, and the pyramid feature as 𝑃1,𝑃2,𝑃3,𝑃4,𝑃5. We denote by {⋅} as building a domain classifier shared among all the features within the bracket.

We present the results in Table 6. The non-adaptive baseline achieves 36.7% in AP. First we investigate different strategies for image-level feature alignment. The image-level adaptation can be represented as {𝐶5}, as only the last convolutional feature is aligned. This approach achieves +4.3% performance gain. Another performance gain of +1.4% is achieved by further aligning {𝐶3}, which is in line with the findings in works that additionally align local features (He and Zhang 2019; Xie et al. 2019; Saito et al. 2019). The scale-aware image-level adaptation can be represented as {𝑃1},{𝑃2},{𝑃3},{𝑃4},{𝑃5}, due to that an independent domain classifier is built at each scale. Scale-aware Image-level adaption achieves 45.0, significantly better than the previous approaches. To further demonstrate the benefits, we also test a variant of scale-agnostic image-level adaptation, where we apply a shared domain classifier among all the pyramid features. This approach can be denoted as {𝑃1,𝑃2,𝑃3,𝑃4,𝑃5}, which produces a inferior result of 43.0 in AP, suggesting a uniform feature alignment is insufficient to align features across different scales. For instance-level adaptation, the scale-aware version outperforms the scale-agnostic version by 1.9%.

By performing together the scale-aware instance-level adaptation and scale-aware image-level adaptation, we achieve an AP of 55.8%, significantly outperforming a competing adaptation strategy (Shen et al. 2019) with the same ResNet-50-FPN backbone, which further validate the benefits of our scale-aware adaptation modules.

Scale-Wise Analysis
Lastly, to gain further understanding in the influence of feature alignment, we conduct a scale-wise analysis, where we visualize the instance-level feature from different object scales, as well as provide a scale-wise quantitative evaluation. We examine four different models: (a) non-adaptive Faster R-CNN with ResNet-50 backbone (NonAdapt), (b) non-adaptive Faster R-CNN with FPN backbone (FPN), (c) our Domain Adaptive Faster R-CNN (DA-Faster) and (d) our Scale-aware Domain Adaptive Faster R-CNN (SA-DA-Faster).

Table 12 Comparison with state-of-the-arts on Pascal → Clipart
Full size table
We use Sim10k→ Cityscapes in this study. First 5120 instance-level features are uniformly sampled for each model. And we then visualize the feature embedding by T-SNE. The visualization is shown in Fig. 7. On the top row we show the instance scale, where we categorize each instance into three categories, based on the instance size: small (<202 pixels), medium (202–1002 pixels), and large (>1002 pixels). And on bottom row we show the domain label.

Fig. 8
figure 8
Qualitative results on Cityscapes. Sim10k is used as source domain. From left to right are the results from NonAdapt, DA-Faster, SA-DA-Faster model

Full size image
Fig. 9
figure 9
Qualitative results on Foggy Cityscapes. Cityscapes is used as source domain. From left to right are the results from NonAdapt, DA-Faster, SA-DA-Faster model

Full size image
Fig. 10
figure 10
Qualitative results on Clipart and Watercolor dataset. Pascal VOC dataset is used as source domain. From left to right are the results from NonAdapt, DA-Faster, SA-DA-Faster model

Full size image
As shown in Fig. 7, NonAdapt (a) and FPN (b), features of different scales are spanned across the feature space, and a notable domain shift can be observed. DA-Faster (c) performs a uniform domain alignment, which is agnostic to the scale. As a result, the features are aligned between the two domains to some extent. However, the alignment produces a side effect of wrongly aligning features across different scales. In contrast, our SA-DA-Faster (d) is able to take advantage of the scale information, and maintains the scale discriminability when aligning the features. This has resulted an observable better feature alignment.

With respect to quantitative results, we evaluate objects of different scales separately. We report MAP for each scale and summarize the results in Table 7. We observe that the proposed modules also demonstrate better quantitative results across scales.

Comparing to State of the Arts
Since the conference version of our work, many end-to-end cross-domain detection models have been proposed. To facilitate the comparison with other competing methods, we also include the results of multiple recent cross-domain adaptation methods.

We summarize the results on SIM10K→Cityscapes in Table 8, the results on Cityscapes→Foggy Cityscapes in Table 9, the results on Cityscapes↔KITTI in Table 10, the results on Pascal→Watercolor in Table 11, and the results on Pascal→Clipart in Table 12. Due to the fact that different backbone architectures are used across different works, we also list the backbone architecture of each method.

Compared to competing methods, the proposed SA-DA-Faster model achieves top performance, and sets a new state of the art in all benchmarks. Notably in many cases our model outperforms competing methods by a large margin. For example, on SIM10K→Cityscapes our model is 9.2% better than MTOR, which is based on a similar backbone with our model. The results demonstrate the effectiveness of our scale-aware adaptation approach. Note that our approach is conceptually different to other works that aim to conduct multi-level feature alignment (He and Zhang 2019; Xie et al. 2019; Saito et al. 2019), as in this work we explicitly take the object scale into consideration in the feature alignment process.

Finally, we provide some qualitative results on several datasets in Figs. 8, 9, and 10. Where we show the results of non-adaptive baseline, DA-Faster and SA-DA-Faster model. We can clearly observe an improvement in the detection quality. The results show that the proposed scale-aware adaptation method improves the performance significantly, especially for small objects.

Conclusion
In this work, we have presented Scale-aware Domain Adaptive Faster R-CNN for cross-domain object detection. The proposed model aligns the domain discrepancy in two levels: (1) the image-level shift, such as image style, illumination, etc., and (2) the instance-level shift, such as object appearance, size, etc. The two domain adaptation modules are implemented by learning domain classifiers in an adversarial training manner. Moreover, due to the difficulty caused by the large variation in object scales, the feature alignment process is further improved by explicitly incorporating the object scale into the adversarial training. We evaluate our proposed model comprehensively on multiple cross-domain scenarios, including object detection in adverse weather, learning from synthetic data, and cross-camera adaptation, and show that our model outperforms baselines and competing methods by a significant margin.