Abstract
Existing salient instance detection (SID) methods typically learn from pixel-level annotated datasets. In this paper, we present the first weakly-supervised approach to the SID problem. Although weak supervision has been considered in general saliency detection, it is mainly based on using class labels for object localization. However, it is non-trivial to use only class labels to learn instance-aware saliency information, as salient instances with high semantic affinities may not be easily separated by the labels. As the subitizing information provides an instant judgement on the number of salient items, it is naturally related to detecting salient instances and may help separate instances of the same class while grouping different parts of the same instance. Inspired by this observation, we propose to use class and subitizing labels as weak supervision for the SID problem. We propose a novel weakly-supervised network with three branches: a Saliency Detection Branch leveraging class consistency information to locate candidate objects; a Boundary Detection Branch exploiting class discrepancy information to delineate object boundaries; and a Centroid Detection Branch using subitizing information to detect salient instance centroids. This complementary information is then fused to produce a salient instance map. To facilitate the learning process, we further propose a progressive training scheme to reduce label noise and the corresponding noise learned by the model, via reciprocating the model with progressive salient instance prediction and model refreshing. Our extensive evaluations show that the proposed method plays favorably against carefully designed baseline methods adapted from related tasks.
Introduction
Salient Object Detection (SOD) is a long-standing vision task that aims to segment visually salient objects in a scene. It often serves as a core step for downstream vision tasks like video object segmentation (Wang et al. 2015), object proposal generation (Alexe et al. 2012), and image cropping (Wang and Shen 2017). Recent deep learning-based SOD methods have achieved a significant performance progress (Wang et al. 2017; Zhuge et al. 2018; Xu et al. 2019; Su et al. 2019; Zhao and Wu 2019; Hou et al. 2017; Wang et al. 2018), benefited from the powerful representation learning capability of neural networks and large-scale pixel-level annotated training data. Since annotating pixel-level labels is extremely tedious, there are some works (Wang et al. 2017; Zeng et al. 2019) that aim to explore cheaper image-level labels (e.g, class labels) to train SOD models in a weakly-supervised manner.

Fig. 1
figure 1
Our key idea is to leverage complementary image-level labels (class and subitizing) to train a salient instance detection model in a weakly-supervised manner, via synergically learning to predict salient objects, detecting object boundaries and locating instance centroids

Full size image
Salient Instance Detection (SID) goes further from SOD as it aims to differentiate individual salient instances. This instance-level saliency information can benefit vision tasks that require fine-grained scene understanding, e.g, object rank (Siris et al. 2020), image captioning (Karpathy and Fei-Fei 2015), image editing (Yang et al. 2018) and semantic segmentation (Tan et al. 2021). However, existing SID methods (Fan et al. 2019; Li et al. 2017; Zhang et al. 2016) still rely on large-scale annotated ground truth masks in order to learn how to segment salient instances with their boundaries delineated. Hence, it is worthwhile studying the SID problem from the weakly-supervised perceptive by using cheaper image-level labels.

A straightforward solution to the weakly-supervised SID problem is to use class labels for training, like the weakly-supervised SOD methods (Wang et al. 2017; Zeng et al. 2019). However, using just class labels to learn a SID model is non-trivial for two reasons. First, while class labels can help detect semantically predominant regions (Zhou et al. 2016), there is no guarantee that the detected regions are visually salient. Second, objects of the same class may not be easily distinguished due to their high semantic affinity. We observe that subitizing refers to the number of certain objects and is therefore naturally related to saliency instance detection. By predicting the number of salient objects, we may use it as a global supervision to help separate instances of the same class while clustering different parts of an instance with diverse appearances into one. Inspired by this insight, we propose to learn a weakly-supervised SID network (denoted as WSID-Net) using class and subitizing labels.

Our WSID-Net consists of three synergic branches: a salient object detection branch is proposed to locate candidate salient objects while a boundary detection branch is proposed to delineate their boundaries, both by exploiting semantics from the class labels; and a centroid detection branch is proposed to detect the centroid of each salient instance, by leveraging saliency cues from the subitizing labels. This information is fused to obtain the salient instance map. To facilitate the learning process, we propose a Progressive Training Scheme (PTS) to reduce the noise generated in our salient object detection branch (e.g, incomplete object proposals and cluttered background objects), by reciprocally updating the branch using generated pseudo labels and refreshing the branch in a self-supervised manner. To demonstrate the effectiveness of the proposed model, we compare it with a variety of baselines adapted from related tasks on the standard benchmark (Li et al. 2017).

To summarize, this work has four main contributions:

To the best of our knowledge, we propose the first weakly-supervised method for salient instance detection, which only requires image-level class and subitizing labels to obtain salient instance maps.

We propose a novel network (WSID-Net), with a novel centroid-based subitizing loss to exploit salient instance number, a novel Boundary Enhancement module to learn instance boundaries, and a novel Cross-layer Attention module to enhance cross-layer context feature learning of centroids and the boundaries.

We propose a novel Progressive Training Scheme, to facilitate the learning of the saliency detection branch by reducing the noise in a self-supervised manner.

We conduct extensive experiments to analyze the proposed method, and verify its superiority against baselines adapted from related state-of-the-art approaches.

Related Work
Salient Instance Detection
Existing SID methods are fully-supervised. Zhang et al. (2016) propose to detect salient instances with bounding boxes, and propose a MAP-based optimization framework to regress a large amount of pre-defined bounding boxes into a compact number of instance-level bounding boxes of high confidences. However, this method based on bounding boxes cannot detect salient instances with accurately delineated boundaries. Other works predict pixel-wise masks for the detected salient instances, and typically rely on large amount of manually annotated ground truth labels. Specifically, Li et al. (2017) propose to first predict the saliency mask and instance-aware saliency contour, and then apply the Multi-scale Combinatorial Grouping (MCG) algorithm (ArbelÃ¡ez et al. 2014) to extract instance-level masks. Fan et al. (2019) propose an end-to-end SID network based on the object detection model FPN (Lin et al. 2017), with a segmentation branch to segment the salient instances.

Unlike these existing SID methods, we propose in this paper to train a weakly-supervised network, which only requires image-level class and subitizing labels.

The work presented in this paper extends our BMVC oral paper (Tian et al. 2020) in three aspects. First, we provide a more comprehensive literature survey on the weakly supervised salient instance detection task and other relevant works. Second, we note that the earlier method (Tian et al. 2020) typically suffers from the salient instance incompleteness problem, due to the noise generated in both salient object detection and boundary detection branches. To address this problem, we propose a Cross-layer Attention module here to learn boundary and centroid features, and a self-supervised Progressive Training Scheme to reduce the noise in the salient object detection branch. Third, we perform more experiments to analyze the properties of our method and show its effectiveness over existing state-of-the-art approaches.

Salient Object Detection
SOD methods aim at detecting salient objects in a scene without differentiating the detected instances. Liu et al. (2007) formulate the SOD task as a binary segmentation problem for segmenting out the visually conspicuous objects of an image via color and contrast histogram based priors.

Traditional methods propose to leverage different hand-crafted priors to detect salient objects, e.g, image colors and luminance (Achanta et al. 2009), global and local contrast priors (Perazzi et al. 2012; Cheng et al. 2014), and background geometric distance prior (Yang et al. 2013). Recently, deep learning based SOD methods achieve superior performances on the standard SOD benchmarks (Yang et al. 2013; Li et al. 2014; Shi et al. 2015; Jiang et al. 2013; Wang et al. 2017; Cheng et al. 2014). Among them, several methods explore boundary information for salient object detection. Xu et al. (2019) propose a CRF-based architecture to refine boundaries of both deep features and saliency maps in a coarse-to-fine manner. Some methods (Zhuge et al. 2018; Zhou et al. 2020; Wei et al. 2020; Su et al. 2019) propose to formulate saliency and edge detection with two network branches as multi-task learning. Feature fusion strategies have also been widely explored in salient object detection. DSS (Hou et al. 2017), DGRL (Wang et al. 2018), and MINet (Pang et al. 2010) integrate multi-level features in the top-down direction. In GBMPM (Zhang et al. 2018b) and PAGE-Net (Wang et al. 2019), multi-level saliency features are fused in both top-down and bottom-up directions, to detect salient objects of varying scales. F3Net (Wei et al. 2020) and PFPN (Wang et al. 2020) propose to fuse features progressively, to enrich saliency features with recurrent feedback information. Attention mechanism has also been exploited to reweigh multi-scale features in order to suppress noise and enhance context learning, via dynamic weight decay scheme (Gao et al. 2020), mutual relation learning of object parts (Chen et al. 2020), gate-based interference control (Zhao et al. 2020), and spatial-/channel-wise attentions on different features (Zhao and Wu 2019). In particular, He et al. (He et al.) propose to leverage numerical representation of subitizing to enrich spatial representations of salient objects. These methods are typically benefited from the powerful learning ability of deep neural networks as well as large-scale annotated ground truth data.

To alleviate the data annotation efforts, many weakly-supervised SOD methods are proposed, by investigating different approaches of generating pseudo saliency labels. A method leverage subitizing information alone for refining saliency prediction Some methods (Zhang et al. 2017, 2018a; Liu et al. 2021) propose to use traditional SOD methods to generate pseudo labels for training deep saliency models. Some other methods (Wang et al. 2017; Zeng et al. 2019) propose to train weakly-supervised deep models using object class labels and class activation maps (CAMs) (Zhou et al. 2016). There are also some methods (Li et al. 2018; Zhang et al. 2020) that propose to combine pre-trained contour networks with segment proposals (Li et al. 2018) or scribbles (Zhang et al. 2020) to generate pseudo labels for training saliency detection networks.

However, existing weakly-supervised SOD methods cannot be directly applied to our problem, as class labels along do not provide instance-level information. In this paper, we propose to use class and subitizing labels to train our SID model.

Fig. 2
figure 2
Pipeline overview. Our SID model is trained using only image-level class and subitizing labels. It has three synergic branches: (1) a Boundary Detection Branch for detecting object boundaries using class discrepancy information; (2) a Saliency Detection Branch for detecting objects using class consistency information; and (3) a Centroid Detection Branch for detecting salient instance centroids using subitizing information. A random walk method is further applied to fuse these information to obtain a final salient instance mask

Full size image
Noise Reduction
Noise commonly exists in a weakly-supervised setting, typically when the task is a pixel-level prediction and the supervision is provided at the image-level. Existing methods typically rely on auxiliary full-annotated labels (referred to as clean labels) or pre-trained models to regularize the noise. Hu et al. (2019) formulate it as a multi-task learning problem, in which the networks trained on a small set of clean labels can help regularize the noise in the networks trained on a large set of weak labels. Zhang et al. (2020) propose a noise-aware method for learning a disentangled clean saliency detector from noisy labels. Lu et al. (2016) propose a sparse learning model to learn the noise statistics from over-segmented superpixels, while Zhu et al. (2019) propose to filter out noisy segment proposals with low matching scores. Both methods rely on additional pre-trained models for noise reduction.

Unlike the above methods, we do not leverage clean labels or pre-trained proposals as assistances. We achieve this goal by training our salient object detection branch in a progressive manner, via model refreshing and pseudo label regeneration.

Methodology
Class labels are widely explored in weakly-supervised SOD methods for learning to localize candidate objects, based on the pixel-level semantic affinity derived from the network responses to the class labels. However, class labels lack instance-level information, causing over- and under-detection when salient instances are from the same category. We note that subitizing, which is a cheap image-level label that denotes the number of salient instances of a scene, can serve as a complementary supervision to the class labels to provide instance-related information. Hence, we propose to use both class and subitizing labels to address our weakly-supervised SID problem. To this end, we propose a weakly-supervised SID network (WSID-Net), as shown in Fig. 2.

The proposed WSID-Net has three branches:

1.
A Saliency Detection Branch for locating candidate salient objects. This saliency detection branch is based on Deeplab (Chen et al. 2017) by modifying its last layer for binary prediction. We propose a novel Progressive Training Scheme (PTS) to self-correct the noise coming from the weak labels and the corresponding noise learned by this saliency branch.

2.
A Centroid Detection Branch for detecting the centroids of salient instances, where subitizing knowledge is utilized in a novel loss function to provide regularization on the global number of instance centroids.

3.
A Boundary Detection Branch for delineating salient instance boundaries, where a novel Boundary Enhancement (BE) module is introduced to resolve the discontinuity problem of detected boundaries.

Finally, we propose a novel Cross-layer Attention (CA) module for the Centroid Detection Branch and Boundary Detection Branch to learn the context information for detecting centroids and boundaries, respectively.

Centroid Detection Branch
Detecting object centroids is crucial to separating object instances in a weakly-supervised scheme. Unlike existing semantic (instance) segmentation methods (Ahn et al. 2019; Neven et al. 2019; Zhou et al. 2018; Cholakkal et al. 2019; Zhu et al. 2019; Laradji et al. 2019) that detect the centroids based on network responses to the class labels, we propose to introduce subitizing information to explicitly supervise the salient centroid detection process.

Centroid-Based Subitizing Loss
It has been shown that penalizing the centroid loss (Ahn et al. 2019; Neven et al. 2019) helps cluster local pixels with high semantic affinities. However, this typically fails when salient instances from the same object category have varying shapes and appearances. The reason is that the clustering process of local pixels lacks global saliency supervision. Hence, we introduce the centroid-based subitizing loss îˆ¸îˆ¿î‰ to resolve this problem. We use subitizing to explicitly supervise the number of predicted centroids, which is implicitly related to the learned offset vectors as îˆ¸ğ‘ ğ‘¢ is back-propagated to the centroid detection branch during training, to guide the centroid-aware pixel clustering process. The detailed formulation is discussed below.

The Centroid Detection Branch predicts an offset vector map î‰‚âˆˆâ„ğ‘ŠÃ—ğ»Ã—2, where ğ‘ŠÃ—ğ» denotes the spatial size of the map, and each 2D vector ğ‘£ğ‘–âˆˆî‰‚ indicates the vertical and horizontal distances of the ith pixel from its associated instance centroid. We follow (Ahn et al. 2019) to iteratively derive î‰‚ as:

ğ‘£ğ‘š+1ğ‘–=ğ‘£ğ‘šğ‘–+ğ‘£ğ‘£ğ‘šğ‘–+ğ‘ğ‘–,
(1)
where ğ‘ğ‘– is the coordinates of the ith pixel, m is the iteration number, and ğ‘£ğ‘šğ‘–+ğ‘ğ‘– indexes the the current centroid that the offset of the ith pixel points to. Ideally, Eq. 1 would converge within a few iterations when ğ‘£ğ‘š+1ğ‘–=ğ‘£ğ‘šğ‘– and the offset of the centroid is zero, and yields a set of centroids that represent the instances. Pixel i can then be assigned to its corresponding centroid ğ‘ğ‘› by measuring its distance from the centroid, as described by:

ğ‘ğ‘–â†’ğ‘›=argminğ‘›  â€–ğ‘£ğ‘–+ğ‘ğ‘–âˆ’ğ‘ğ‘ğ‘›â€–.
(2)
We then use the saliency map îˆ¿ of the saliency detection branch to filter out non-salient instances by computing their ğ¼ğ‘œğ‘ˆ=(îˆ¿îˆµğ‘›âˆ©îˆ¿)/îˆ¿îˆµğ‘›>ğœƒ, to obtain a set of saliency instances îˆ¿îˆµâˆ—={îˆ¿îˆµ1,îˆ¿îˆµ2,...,îˆ¿îˆµğ‘‡âˆ—}, where ğ‘‡âˆ— represents the number of predicted salient instances. Finally, we use MSE to measure îˆ¸ğ‘ ğ‘¢ as:

îˆ¸ğ‘ ğ‘¢=ğ‘€ğ‘†ğ¸(ğ‘‡âˆ—,ğ‘‡),
(3)
where T is the subitizing ground truth, and ğ‘‡âˆ— denotes the number of predicted centroids extracted from the offset vectors of the pixels in the salient region. Note that the loss îˆ¸ğ‘ ğ‘¢ is back-propagated to update the offset vectors only in the salient region, which avoids the learning process of instance centroid detection being distracted by the non-salient background. The gradient ğ›¿ of îˆ¸ğ‘ ğ‘¢ is calculated as:

ğ›¿=1ğ¾â‹…âˆ‚îˆ¸ğ‘ ğ‘¢âˆ‚î‰‚âˆ—,
(4)
where î‰‚âˆ— are the offset vectors in the salient region, and K is the total number of offset vectors in î‰‚âˆ—.

Figure 3 visualizes the results from centroid detection and the corresponding instance segmentation, with and without using the centroid-based subitizing îˆ¸îˆ¿î‰ loss function. We can see that the network groups the two dogs into one when not using îˆ¸îˆ¿î‰, as these two dogs have similar appearances and lie next to each other (Fig. 3b, e). By introducing îˆ¸îˆ¿î‰, the network is able to predict a correct number of centroids, and generate reasonable salient instance masks compared with the ground truth (Fig. 3c, f).

Network Structure
We adopt the image-to-image translation scheme, where our network outputs a 2D centroid map, in which the value of each pixel location indicates the offset vector to its instance centroid. The bottom part of Fig. 2 shows the network structure of our centroid detection branch. Given an input image, we first extract multi-scale backbone features ğ‘“1 to ğ‘“5 and feed them to the Cross-layer Attention (CA) modules with boundary-aware features for joint refinement (to be discussed in Sect. 3.3). We then fuse the high-level features to obtain ğ‘“â„: ğ‘“â„=ğ¶ğ‘œğ‘›ğ‘£(ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘“4âˆ—,ğ‘“5âˆ—)), which is further fused with the low-level features to produce the centroid map î‰‚: î‰‚=ğœ(ğ¶ğ‘œğ‘›ğ‘£(ğ¶ğ‘œğ‘›ğ‘£(ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘“â„,ğ‘“1âˆ—,ğ‘“2âˆ—,ğ‘“3âˆ—)))).

Fig. 3
figure 3
Visualization of the centroid detection branch with and without îˆ¸îˆ¿î‰. Using class labels alone fails to train the network to detect instance centroids (denoted by red stars) if they have similar appearances (b), resulting in wrong segmentations (e). In contrast, our proposed subitizing loss can segment these salient objects in instance-level (f), by learning to identify the correct number of salient instances (c)

Full size image
Boundary Detection Branch
Boundaries provide strong cues for separating salient instances. Unlike fully-supervised SID methods that learn boundary-aware information based on pixel-level ground truth masks, we propose the Boundary Enhancement module to leverage the Canny prior (John 1986) to delineate continuous instance boundaries.

Boundary Enhancement (BE) Module
We apply a random walk algorithm to search a salient instance from a centroid to its boundary. However, it may fail when part of the boundary is discontinuous as the random walk algorithm will also search the region outside the boundary. Hence, we propose the BE module to incorporate the edge prior for learning continuous instance boundaries, as shown in Fig. 4. Specifically, we first extract low-level features along the horizontal and vertical directions from the input image, by two 1Ã—7 and 7Ã—1 convolution layers. These low-level features are then fed into three Residual Blocks (He et al. 2016) for feature refinement, which are further concatenated with enriched edges computed from the Canny operator (John 1986). To compute the final enriched boundary features, another 1Ã—1 convolution layer is applied.

Fig. 4
figure 4
Boundary enhancement (BE) module

Full size image
Figure 5 visualizes two examples of boundary detection and the corresponding salient instance detection with and without the BE module. We can see that our BE module helps detect the boundaries between objects, which is crucial to salient instance segmentation.

Fig. 5
figure 5
Visualization of the boundary detection branch with and without the BE module, which shows the effectiveness of our proposed BE module in mining continuous boundary information for separating salient objects of same classes

Full size image
Network Structure
The top part of Fig. 2 shows the architecture of the boundary detection branch. Given an input image îˆµ, the backbone network produces multi-scale features (ğ‘“1 to ğ‘“5), each of which is enhanced by a CA module (to be discussed in Section 3.3) before they are concatenated and computed to predict the boundary map. We also feed the input image into the BE module to obtain enhanced edge features ğ‘“ğ‘. The output boundary map îˆ® is then computed as: îˆ®=ğœ(ğ¶ğ‘œğ‘›ğ‘£(ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘“1âˆ—,â€¦,ğ‘“5âˆ—,ğ‘“ğ‘1,ğ‘“ğ‘2))), where ğœ is the sigmoid activation function.

Cross-Layer Attention (CA) Module
Detecting instance centroids and boundaries are two highly coupled sub-tasks, i.e., they can influence each other and further affect the SID performance. To effectively learn these two sub-tasks, we propose the Cross-layer Attention (CA) module for refining backbone features before they are used for these two sub-tasks. Its design is based on two observations. First, low-level features contain high-resolution but noisy information for delineating salient instance boundaries, while high-level features have low-resolution but robust information for salient instance localization. Second, since salient instances may have various shapes and they may correspond to different class labels, we need to model both long-range spatial and cross-channel contextual information. Unlike existing dual attention mechanisms (Woo et al. 2018; Fu et al. 2019) that only enhance the feature representation capacity of one fixed layer, our CA module first incorporates a Cross-layer Feature Mixing (CFM) unit to enhance the communication across different levels of backbone features and then uses multiple Mutual Attention (MA) units to learn hierarchical channel-wise and spatial-wise attentive features for each sub-task. The CFM unit shares its parameters to allow information exchanges across the boundary and centroid branches. Figure 6 shows the module structure.

Fig. 6
figure 6
Cross-layer Attention (CA) module

Full size image
Fig. 7
figure 7
Cross-layer Feature Mixing (CFM) unit

Full size image
Structure of CFM Unit
Figure 7 shows the structure of the CFM unit. ğ‘“1 to ğ‘“5 are the features from the pyramid layers of the ResNet backbone. We first upsample ğ‘“2, ğ‘“3, ğ‘“4, and ğ‘“5 such that the given feature maps have the same resolution, and apply 1Ã—1 convolution on the five feature maps such that they have the same channel depth (256). We get features ğ‘“1â€² to ğ‘“5â€² that have the same shape for the following operation. We then apply CFM on the pyramid features to generate cross-layer features. CFM is implemented via a concatenation-split-concatenation operation on the feature channels. We concatenate features ğ‘“1â€² to ğ‘“5â€² as ğ‘“ğ‘, with 1280 (256Ã—5) channels. The split-concatenation operation could be considered as a reshape-transpose-reshape process. We reshape channel dimension of ğ‘“ğ‘ to 2 dimensions (i.e., [5, 256]), transpose it to [256, 5], and then flatten it to 1280. Finally, we concatenate the features before/after CFM, and feed these concatenated features to 1Ã—1 convolutional filters to generate the final enhanced features (ğ‘“1â€³ to ğ‘“5â€³).

Structure of MA Unit
Figure 8 shows the structure of our MA unit. The top and bottom branches are channel-wise and spatial-wise attention blocks, respectively. Specifically, given the input features ğ‘“ğ‘›â€³, we compute the channel-wise attention features îˆ²ğ‘ as:

îˆ²ğ‘=ğœ(ğ‘€ğ¿ğ‘ƒ(ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™ğ‘(ğ‘“ğ‘›â€³))+ğ‘€ğ¿ğ‘ƒ(ğ‘€ğ‘ğ‘¥ğ‘ƒğ‘œğ‘œğ‘™ğ‘(ğ‘“ğ‘›â€³))),
(5)
where ğ‘€ğ‘ğ‘¥ğ‘ƒğ‘œğ‘œğ‘™ğ‘ and ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™ğ‘ denote two channel-wise pooling operations, and MLP is the multi-layer perception with one hidden layer to generate the attention features. We also compute the spatial-wise attention features îˆ²ğ‘  as:

îˆ²ğ‘ =ğœ(ğ¶ğ‘œğ‘›ğ‘£7Ã—7([ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™ğ‘ (ğ‘“ğ‘›â€³);ğ‘€ğ‘ğ‘¥ğ‘ƒğ‘œğ‘œğ‘™ğ‘ (ğ‘“ğ‘›â€³)])),
(6)
where ğ¶ğ‘œğ‘›ğ‘£7Ã—7 is a convolutional layer with kernel size 7. The final attention features ğ‘“ğ‘›âˆ— are then computed as:

ğ‘“ğ‘›âˆ—=ğ‘“ğ‘›â€³Ã—îˆ²ğ‘+ğ‘“ğ‘›â€³Ã—îˆ²ğ‘ ,
(7)
where Ã— denotes the dot product operation, and + is the element-wise summation operation.

Figure 9 shows the effectiveness of the proposed CA module in enhancing the boundary and centroid detection performances.

Fig. 8
figure 8
Mutual attention (MA) unit

Full size image
Fig. 9
figure 9
Illustration on how the CA module benefits both boundary and centroid tasks. We can see that the CA module influence both the continuity of the detected boundaries and the accuracy of detected centroids

Full size image
Fig. 10
figure 10
Progressive training scheme for our saliency detection branch. Each training iteration in the dashed box contains two steps, model refreshing and pseudo label regeneration, leading the model to correct the noise in a self-supervised manner

Full size image
Progressive Training Scheme (PTS)
Weak annotated labels would inevitably introduce noise into the learning process. To reduce the noise, previous works propose to use temporal ensemble learning (Laine and Aila 2016; Tarvainen and Valpola 2017) in a semi-supervised setting, where latent knowledge learned from labeled data can be applied to noisy unlabeled data. We extend this idea in our weakly-supervised setting. Since we do not have fully-annotated data, we explore this ensemble learning strategy in a progressive self-supervised manner, i.e., by reciprocally training the salient object detection branch using newly predicted salient instance maps. It mainly contains two iterative steps: pseudo label generation, and model refreshing. Figure 10 shows the overview of the proposed Progressive Training Scheme, and Algorithm 1 summarizes the main steps.

Pseudo Label Generation
Since we do not have any fully-annotated labels to learn a noise-free representation, we propose to use our WSID-Net as a pseudo-label generator and refine its salient object detection branch in a self-supervised manner. This is because the output of our WSID-Net has more accurate boundaries with the help of other two branches, compared with its salient object detection branch. On the other hand, the re-trained saliency detection branch can further boost the performance of our WSID-Net due to the improved salient object localization. To this end, we first forward the WSID-Net to generate salient instance masks, and use them as the initial pseudo training labels to update the network parameters of the salient object detection branch. In the next training iteration, we update the pseudo training labels using the re-trained WSID-Net.

Model Refreshing
Before we update the pseudo training labels in the next iteration, we need to refresh our salient object detection branch. Note that we do not have any clean data (fully-annotated labels in our case) that can be used for learning noise-free features. It is possible that our model may overfit the noise in the pseudo labels and converge to a local minimum. To avoid these problems, we adopt the Exponential Moving Average method to update the model parameters with a weighted sum of model parameters in the current and former iterations. We define ğœ”ğ‘Ÿ as the model weight in iteration r, and the model refreshing is formulated as:

ğœ”ğ‘Ÿ+1=ğ›¼ğœ”ğ‘Ÿâˆ’1+(1âˆ’ğ›¼)ğœ”ğ‘Ÿ,
(8)
where ğ›¼ is the smoothing hyper-parameter that balances the contributions of model parameters from different iterations.

figure a
Fig. 11
figure 11
Illustration of how the proposed PTS benefits our task progressively in a self-supervised manner. While WSID-Net initially fails due to inaccurate detection (e), the output of WSID-Net (f) is still better than (e) with the help of the centroid (c) and boundary (d) detections. By using (f) to further refine the saliency detection branch (g), our WSID-Net could also enjoy the performance boost (h)

Full size image
Figure 11 shows one example of how the proposed PTS benefits our task in a self-supervised manner: given a challenging input image of two persons leaning on each other (a), our saliency detection branch fails to detect the majority of the salient regions due to the diverse foreground appearance and cluttered background (e), which further causes the failure of our WSID-Net (f). However, since (f) is still better than (e) due to the help of the centroid and boundary detections ((c) and (d)), after we have applied the proposed PTS training strategy, we can see that our saliency detection branch enjoys a better performance in detecting the saliency region as a whole (g), leading to an accurate salient instance detection result (h).

Experiments
Implementation Details
We implement WSID-Net on the Pytorch framework (Paszke et al. 2019). Both training and testing are performed on a PC with an i7 4GHz CPU and a GTX 1080Ti GPU. CRF is used to generate or refine pseudo labels. The hyper parameters of CRF are set as ğ‘¤1=4.0, ğ‘¤2=3.0, ğœğ›¼=49.0, ğœğ›½=5.0, and ğœğ›¾=3.0. We choose ResNet50 as the backbone for all three branches in WSID-Net. The backbone are initialized as in Simonyan and Zisserman (2014). Input images are resized to 512Ã—512 resolution. To minimize the loss function, we use the SGD optimizer with batch size 6 and initial learning rate 0.01. The learning rate decreases following poly policy (ğ‘™ğ‘Ÿitr=ğ‘™ğ‘Ÿinit(1âˆ’itrmaxitr)ğ›¾). We train our WSID-Net for 5 epoches. The proposed PTS training is further applied to refine the saliency detection branch for another 6 iterations, of which each iteration contains 8 epoches (R and E in Algorithm 1). ğ›¼ in Eq. 5 is set to ğ‘Ÿğ‘Ÿ+1, where r is the current iteration index. The learning rate begins with 0.0001, and the decay follows the aforementioned poly policy.

Training and Evaluation Details
Datasets. Our WSID-Net is trained on two kinds of image-level labels, class and subitizing. We use the class labels from the PASCAL VOC 2012 (Everingham et al. 2010) dataset (which is originally proposed for semantic and instance segmentation) to train our network. For subitizing labels, we count the numbers of salient instances from the ILSO (Li et al. 2017) dataset, and train our network on this training set. For the proposed progressive training scheme, we augment the training data by using the unlabeled training data from C2SNet (Li et al. 2018). For testing, following the existing fully-supervised SID method (Fan et al. 2019), we perform SID evaluations on the test set of ILSO (Li et al. 2017).

Training and Inference. We train the boundary and centroid branches together with different losses, and train the saliency branch independently. We train the centroid detection branch using the proposed centroid-based subitizing loss together with the centroid loss introduced in Ahn et al. (2019); Neven et al. (2019). We train the boundary detection branch using the boundary loss introduced in Ahn and Kwak (2018); Ahn et al. (2019). To train the saliency detection branch, we follow existing weakly-supervised SOD methods to use pseudo masks derived from class labels. Specifically, we first compute class activation maps via (Zhou et al. 2016). We then feed these maps together with the input image to a Conditional Random Field (CRF) (KrÃ¤henbÃ¼hl and Koltun 2011) to generate pseudo object maps, and use these pixel-level pseudo labels to train the saliency detection branch. We further utilize the proposed PTS with model refreshing and self-generated pseudo labels to retrain the saliency detection branch.

During inference, given an input image, WSID-Net first computes the centroids, boundaries, and saliency maps. We first obtain the initial saliency instance map îˆ¿îˆµâˆ— via the saliency map and centroid map, as discussed in Sect. 3.1.1. We then use the boundary map to refine the initial saliency instance map with the random walk algorithm. The transition probability matrix îˆ¹ is defined as:

îˆ¹=îˆ°âˆ’1îˆ´ğœ’,
(9)
where îˆ´ is the affinity matrix of the learned boundary map îˆ®, and îˆ° is a diagonal matrix relating to îˆ´. The element in îˆ´ is defined as: â„ğ‘˜=1âˆ’maxğ‘˜âˆˆÎ ğ‘–ğ‘—îˆ®(ğ‘¥ğ‘˜), where Î ğ‘–ğ‘— is a set of pixels on the line between boundary pixels ğ‘¥ğ‘– and ğ‘¥ğ‘—. In addition, îˆ´ğœ’ is the self production of îˆ´ with power ğœ’ for affinity distillation, and îˆ°â€™s diagonal element îˆ°ğ‘–ğ‘– equals to âˆ‘â„ğœ’ğ‘–ğ‘— for summarizing values of îˆ´ğœ’ by row. The random walk algorithm for instance-wise saliency value propagation is conducted as:

ğ‘£ğ‘’ğ‘(îˆ¿îˆµâ¯â¯â¯â¯â¯â¯â¯â¯âˆ—ğ‘›)=îˆ¹ğ‘–ğ‘£ğ‘’ğ‘(îˆ¿îˆµâˆ—ğ‘›(1âˆ’îˆ®)),
(10)
where vec() refers to the vectorization of the matrix, and îˆ¿îˆµâ¯â¯â¯â¯â¯â¯â¯â¯âˆ—ğ‘› is our final saliency instance map.

Evaluation Metrics We use the mean Average Precision (mAP) metric (Hariharan et al. 2014) to evaluate the SID performance. The IoU is set to 0.5 and 0.7 for this metric.

Comparing to the State-of-the-Art Methods
As we are the first to propose a weakly-supervised SID method, we compare our method to 2 existing fully-supervised state-of-the-art SID methods: S4Net (Fan et al. 2019) and MSRNet (Li et al. 2017). We also prepare the following baselines from related tasks for evaluation. We choose 6 state-of-the-art weakly-supervised methods, with two from the SOD task C2SNet (Li et al. 2018) and NLDF (Luo et al. 2017); one from the SID task MAP (Zhang et al. 2016); one from the object detection (OD) task, DeepMask (Pinheiro et al. 2015); and two from the Semantic Instance Segmentation task, PRM+D (Cholakkal et al. 2019) and IRN (Ahn et al. 2019). We adapt them by adding different post-processing strategies to these methods for deriving instance-level saliency maps from their original outputs, or modifying their networks and retrain them using our training data. Details are summarized as follows:

We choose â€œMCG (ArbelÃ¡ez et al. 2014) + MAP (Zhang et al. 2016)â€ as the post-processing strategy for the weakly-supervised SOD methods (i.e., C2SNet (Li et al. 2018) and NLDF (Luo et al. 2017)), inspired by the fully-supervised SID method MSRNet (Li et al. 2017). It has been shown in (Li et al. 2017) that MCG (ArbelÃ¡ez et al. 2014) can be used to produce segments given the contour maps as input and assign these segments with confidence scores. Segments with low confidences can then be filtered out by MAP (Zhang et al. 2016). Considering that both C2SNet (Li et al. 2018) and NLDF (Luo et al. 2017) learn to produce contour maps, we find this post-process strategy suitable for weakly-supervised SOD methods with contour predictions.

We select CRF (KrÃ¤henbÃ¼hl and Koltun 2011) as the post-processing strategy for fully-supervised bounding-box-based SID method MAP (Zhang et al. 2016), due to the fact that CRF is a popular graphical model used as post-processing for boosting segmentation performance. Considering that MAP (Zhang et al. 2016) can generate instance-level bounding boxes, CRF can be used to obtain instance maps by refining the boundaries, which gives a performance boost of 3%.

We choose a weakly-supervised SOD method as post-processing for filtering out non-salient segments produced by DeepMask (Pinheiro et al. 2015), as DeepMask (Pinheiro et al. 2015) is a class-agnostic object detection method that is not aware of saliency information. We choose WSS (Wang et al. 2017) as the weakly-supervised SOD method for a fair comparison, as it performs closely to our Saliency Detection Branch in our preliminary experiment. This strategy improves the performance by 5%.

IRN (Ahn et al. 2019) produces class-specific instance segmentation maps, which do not have saliency information. To adapt its results from class-specific to class-agnostic, we remove the CAM in their method and directly use their centroid and boundary maps to obtain instance maps. We then utilize WSS (Wang et al. 2017) to select salient instances.

PRM+D (Cholakkal et al. 2019) is trained with class and per-class subitizing labels to predict semantic instance maps. However, this method can only response to the instances with pre-defined class labels. To adapt it to class-agnostic, we merge its per-class outputs (originally 20 output maps for 20 classes) into one class-agnostic map by adding an additional convolutional layer, and then retrain it using our training data.

Quantitative Comparisons We quantitatively evaluate our method in Table 1.Footnote1 It is worth noting that our method achieves a significantly better performance of about 20% over the second-place weakly-supervised baseline, on the mAP@0.7 metric (which is very challenging as it requires the IoU value to be over 70%). These results show that our method achieves the best performance using just two image-level labels.

Table 1 Quantitative evaluation of our method against six baseline methods and state-of-the-art fully-supervised SID methods
Full size table
Qualitative Comparisons We further qualitatively evaluate our method with fully-supervised methods and baselines in Fig. 12. The visual results verify that our method is able to delineate the instance boundaries clearly, and output accurate numbers of segmented salient instances directly for different scenes, i.e. scenes with single instances, small instances, (non-)adjacent instances, similar/varied instances, and cluttered contents. In contrast, the compared methods exhibit different limitations as follows:

PRM+D and IRN fail to detect integral instances with inferior detected boundaries (e.g, rows 1, 13).

C2SNet and NLDF tend to recognize texture boundaries, causing fragmented instances (e.g, rows 10, 12, 14).

DeepMask and S4Net suffer from the over-detection problem, as they fail to distinguish instance proposals belonging to the same instance (e.g, rows 2, 3, 13).

MAP is a bounding-box based method. It fails to get clear instance boundaries even post-processed by CRF (e.g, rows 1, 2, 7).

Overall, our method outperforms all these baselines, as a result of the centroid-based subitizing loss, the carefully designed BE and CA modules, and the progress training scheme.

Fig. 12
figure 12
Qualitative results of our method, compared with existing fully-supervised methods [S4Net (Fan et al. 2019) and MAP (Zhang et al. 2016)] and modified baselines (PRM+D (Cholakkal et al. 2019), DeepMask (Pinheiro et al. 2015), C2SNet (Li et al. 2018), NLDF (Luo et al. 2017), and IRN (Ahn et al. 2019)). Refer to Sect. 4.3 and Table 1 on how we modify and train these baselines, in order to carry out a fair comparison

Full size image
Internal Analysis and Discussions
Ablation Study of Network Design
We begin by investigating the effectiveness of the proposed network design, including the proposed Boundary Enhancement module, Cross-layer Attention module, Progressive Training Scheme, and îˆ¸îˆ¿î‰ loss. Table 2 shows the results. We can see that the SID performance would drop if we remove any of the components from the network. This shows that these components can help boost the performances of the saliency, centroid and boundary detection sub-tasks, which play a vital role in detecting salient instances. Figures 3, 5, and 9 provide additional visual comparisons to demonstrate the effectiveness of these components.

Evaluation of the CA Module
We then investigate our CA module on its design choices and intermediate feature visualization.

Design Choices: we examine our CA module against its variants, as reported in Table 3. First, we study the benefits of the CFM and MA units. Row 1 shows that removing the CFM unit leads to a performance drop, due to the reduction in communication enhancement among the pyramid feature layers. Row 2 also shows a performance drop without the MA unit, as we are not able to learn long-range dependencies. Second, we study the connection styles (parallel and cascade) of the attention mechanisms in the MA unit. Rows 3, 4 and 5 show that parallel connection of the spatial- and channel-wise attentions performs better than the cascade one. This may be because cascade connection may lose the learned context information of the former attention mechanism.

Table 2 Ablation study of network design
Full size table
Table 3 Evaluation of different designs of the CA module
Full size table
Fig. 13
figure 13
Visualization of the multi-level intermediate features learned by our CA module

Full size image
Feature Visualization we visualize multi-level intermediate features learned by our CA module in Fig. 13. Given multi-level backbone features ğ‘“1 âˆ¼ğ‘“5 (1st row), the CFM unit in the CA module first generates multi-level mixed context features ğ‘“âˆ—1 âˆ¼ğ‘“âˆ—5 (2nd row), which are then used for learning boundary features ğ‘“âˆ—1â‡’îˆ® âˆ¼ğ‘“âˆ—5â‡’îˆ® (3rd row) and centroid features ğ‘“âˆ—1â‡’î‰‚ âˆ¼ğ‘“âˆ—5â‡’î‰‚ (4th row), respectively. First, we can see that our CFM unit is able to highlight the salient objects via aggregating multi-level backbone features, as shown in row 2. Second, the boundary-aware feature maps have high responses in different regions as shown in row 3, which suggests that determining the instance boundaries also require multi-level features. Third, in row 4, the visualization of centroid-aware features generally corresponds to the centroid map î‰‚, where the instance boundaries are generally highlighted, and pixel values of the centroid locations are close to zero. Overall, our proposed CA module is able to help adapt the backbone features into different task-specific features.

In addition, our CA module differs from CAM in two aspects. First, CAM is conditioned on the class label input, but our CA module learns class-agnostic attentions from pseudo labels. Second, CAM is unable to delineate clear boundary and instance information, while our CA module can learn this information to complement the CAM for detecting salient instances, as shown in Fig. 14.

Fig. 14
figure 14
Visual comparison between CAM and CA features. As shown in column 7, CAM itself cannot delineate the boundaries between two persons and locate their centroids. Hence, we only use CAM in the saliency detection branch. Our CA module successfully learns this information for complementing CAM

Full size image
Evaluation of the BE Module
We conduct ablation studies to investigate the effect of the Canny filter in the Boundary Enhancement (BE) module. We compare our BE module to two ablated versions: removing the Canny filter from the BE module (denoted as BE Module w/o Canny), and using Canny filter only (denoted as Canny Only). Results are shown in Table 4. We can see that our method outperforms both ablated versions. The Canny filter is used to enrich the high-level boundary features with low-level edge information. Without the Canny filter, the BE module may not detect small boundaries accurately. However, relying only on the Canny filter cannot obtain high-level boundary information, which typically leads to over-segmentation of instances.

Table 4 Evaluation on the Canny filter
Full size table
Evaluation of Parameter Settings for the Canny Filter
We empirically set the thresholds (i.e., ğœƒğ‘¢ğ‘ and ğœƒğ‘™ğ‘œğ‘¤ for controlling the connectivity and density of the detected edges) in the Canny operator to be automatically determined by the channel median of the gray-scale image. We find that this works well in our experiments.

To further investigate how these two thresholds affect the performance, we provide both qualitative and quantitative comparisons between our threshold choice with two manual choices described below:

Large range: we manually set ğœƒğ‘™ğ‘œğ‘¤ and ğœƒğ‘¢ğ‘ to 30 and 200, respectively, so that the Canny operator is sensitive to textures and can detect more edges.

Small range with large values: we manually set ğœƒğ‘™ğ‘œğ‘¤ and ğœƒğ‘¢ğ‘ to 230 and 260, respectively, so that only structural edges of objects can be detected.

Table 5 shows that both manual strategies would degrade the performance. Figures 15 and 16 show two scenes that these manual strategies fail. In row 2 of Fig. 15, if the Canny edge map provides insufficient object structure information and the learned boundaries are partially weak, our method fails to separate nearby instances. In row 1 of Fig. 16, the Canny edge contains extensive non-structure textures that affect the learned boundaries, making it difficult for the saliency values to propagate to the target boundaries from the centroid for determining the instance. In contrast, our choice successfully detects accurate instances since we can obtain high-quality boundaries, as shown in rows 1 and 3 in Fig. 15, and rows 2 and 3 in Fig. 16. This visually verifies that the Canny edge generated under our automatic setting is more stable to provide pleasant instance boundaries.

Table 5 Evaluation on different parameter settings for the Canny operator in the BE module
Full size table
Fig. 15
figure 15
Visual comparison between results using different parameters of the Canny operator

Full size image
Fig. 16
figure 16
Visual comparison between results using different parameters of the Canny operator

Full size image
Evaluation of PTS
We study how PTS helps improve the saliency detection performance iteration by iteration. Figures 17 and 18 show the progressively improving results over six training iterations. Intermediate results in both figures verity that our PTS does not only penalize the background distraction but also recover the integral objects. Overall, our PTS is able to reduce noise and improve the performance in a self-supervised manner.

Fig. 17
figure 17
Visualization of intermediate results over six training rounds. The dashed red regions denote background distraction noise. We can see that the noise is progressively suppressed over the iterations

Full size image
Fig. 18
figure 18
Visualization of intermediate results over six training rounds. The dashed red regions denote missed salient parts that are progressively recovered over the iterations. We can see that the detected objects are becoming more and more complete

Full size image
Evaluation of the Saliency Detection Branch
Since our WSID-Net relies on the performance of the saliency detection branch in detecting salient objects, we are particularly interested in the question of to what extent that the quality of the saliency object detection maps may affect the SID performance. To answer this question, we replace the outputs of our weakly-supervised salient object detection branch with five state-of-the-art full-supervised SOD methods (i.e., DSS (Wang et al. 2018), PiCANet (Liu et al. 2018), EGNet (Zhao et al. 2019), ITSD (Zhou et al. 2020), and SCRN (Wu et al. 2019)), as well as the ground truth saliency maps, to generate the salient instance maps. Results are reported in Table 6. We can see that the performance generally increases when inferring salient instance masks using the fully-supervised SOD results. This is because the fully-supervised methods are more robust to background distractions and able to delineate full object masks. However, we can still observe that the performance would not be saturated even if we feed the ground truth saliency maps to generate the SID maps. This is because the instance boundaries are still very difficult to detect, especially when these salient instances overlap each other. This suggests that developing an effective method for detecting salient instance boundaries in a weakly-supervised setting would be a promising solution.

Table 6 Investigation on how the SID performance is affected by the quality of the SOD maps
Full size table
Evaluation of CRF in the Saliency Branch
CRF is used to produce pseudo ground truth saliency maps given the coarse CAM activation maps, so that the saliency detection branch can learn more accurate boundary information. Figure 19 shows that CRF helps produce more accurate pseudo ground truth saliency maps. We also provide quantitative results in Table 7, from which we can see that the performance drops without CRF refinement.

Fig. 19
figure 19
Visualization of the effect of CRF on refining the coarse CAM

Full size image
Table 7 Evaluation of the effect of CRF to the saliency branch
Full size table
Conclusion and Future Work
In this paper, we have proposed the first weakly-supervised SID method, called WSID-Net, which is trained on class and subitizing labels. Our WSID-Net learns to predict object boundaries, instance centroids, and salient regions. By using the proposed Boundary Enhancement module, Cross-layer Attention module, Progressive Training Scheme, and centroid-based subitizing loss, our method can identify and segment salient instances effectively. Both quantitative and qualitative experiments demonstrate the effectiveness of the proposed method compared with baseline methods.

Our method does have its limitation. It may fail when the images are taken with improper exposures. Therefore, our method cannot detect salient objects/instances with low contrast to their surroundings. As a future work, we are currently exploring the use of a discriminative network of generative adversarial learning to overcome this limitation. We would also like to extend this work for videos.

