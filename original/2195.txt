Multi-object tracking (MOT) is an important problem in computer vision which has a wide range of applications. Formulating MOT as multi-task learning of object detection and reID in a single network is appealing since it allows joint optimization of the two tasks and enjoys high computation efficiency. However, we find that the two tasks tend to compete with each other which need to be carefully addressed. In particular, previous works usually treat reID as a secondary task whose accuracy is heavily affected by the primary detection task. As a result, the network is biased to the primary detection task which is not fair to the reID task. To solve the problem, we present a simple yet effective approach termed as FairMOT based on the anchor-free object detection architecture CenterNet. Note that it is not a naive combination of CenterNet and reID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. The resulting approach achieves high accuracy for both detection and tracking. The approach outperforms the state-of-the-art methods by a large margin on several public datasets. The source code and pre-trained models are released at https://github.com/ifzhang/FairMOT.
Introduction
Multi-Object Tracking (MOT) has been a longstanding goal in computer vision (Bewley et al. 2016; Wojke et al. 2017; Chen et al. 2018a; Yu et al. 2016). The goal is to estimate trajectories for objects of interest presented in videos. The successful resolution of the problem can immediately benefit many applications such as intelligent video analysis, human computer interaction, human activity recognition, and even social computing.

Most of the existing methods such as Mahmoudi et al. (2019), Zhou et al. (2018), Fang et al. (2018), Bewley et al. (2016), Wojke et al. (2017), Chen et al. (2018a), Yu et al. (2016) attempt to address the problem by two separate models: the detection model firstly detects objects of interest by bounding boxes in each frame, then the association model extracts reIDentification (reID) features from the image regions corresponding to each bounding box, links the detection to one of the existing tracks or creates a new track according to certain metrics defined on features.

There has been remarkable progress on object detection (Ren et al. 2015; He et al. 2017; Zhou et al. 2019a; Redmon and Farhadi 2018) and reID (Zheng et al. 2017a; Chen et al. 2018a) respectively recently which in turn boosts the overall tracking accuracy. However, these two-step methods suffer from scalability issues. They cannot achieve real-time inference speed when there are a large number of objects in the environment because the two models do not share features and they need to apply the reID models for every bounding box independently in the video.

With the maturity of multi-task learning (Kokkinos 2017; Chen et al. 2018b), one-shot trackers which estimate objects and learn reID features using a single network have attracted more attention (Wang et al. 2020b; Voigtlaender et al. 2019). For example, Voigtlaender et al. (2019) add a reID branch to Mask R-CNN to extract a reID feature for each proposal (He et al. 2017). It reduces inference time by re-using backbone features for the reID network. But the performance drops remarkably compared to the two-step models. In fact, the detection accuracy is still good but the tracking performance drops a lot. For example, the number of ID switches increases by a large margin. The result suggests that combining the two tasks is a non-trivial task and should be treated carefully.

In this paper, we investigate the reasons behind the failure, and present a simple yet effective solution. Three factors are identified to account for the failure. The first issue is caused by anchors. Anchors are originally designed for object detection (Ren et al. 2015). However, we show that anchors are not suitable for extracting reID features for two reasons. First, anchor-based one-shot trackers such as Track R-CNN (Voigtlaender et al. 2019) overlook the reID task because they need anchors to first detect objects (i.e. using RPN Ren et al. 2015) and then extract the reID features based on the detection results (reID features are useless when detection results are incorrect). So when competition occurs between the two tasks, it will favor the detection task. Anchors also introduce a lot of ambiguity during training the reID features because one anchor may correspond to multiple identities and multiple anchors may correspond to one identity, especially in crowded scenes.

The second issue is caused by feature sharing between the two tasks. Detection task and reID task are two totally different tasks and they need different features. In general, reID features need more low-level features to discriminate different instances of the same class while detection features need to be similar for different instances. The shared features in one-shot trackers will lead to feature conflict and thus reduce the performance of each task.

The third issue is caused by feature dimension. The dimension of reID features is usually as high as 512 (Wang et al. 2020b) or 1024 (Zheng et al. 2017a) which is much higher than that of object detection. We find that the huge difference between dimensions will harm the performance of the two tasks. More importantly, our experiments suggest that it is a generic rule that learning low-dimensional reID features for “joint detection and reID” networks achieves both higher tracking accuracy and efficiency. This also reveals the difference between the MOT task and the reID task, which is overlooked in the field of MOT.

Fig. 1
figure 1
Overview of our one-shot tracker FairMOT. The input image is first fed to an encoder-decoder network to extract high resolution feature maps (stride = 4). Then we add two homogeneous branches for detecting objects and extracting reID features, respectively. The features at the predicted object centers are used for tracking

Full size image
Table 1 Comparison of different reID feature extraction (sampling) strategies on the validation set of MOT17
Full size table
In this work, we present a simple approach termed as FairMOT which elegantly address the three issues as illustrated in Fig. 1. FairMOT is built on top of CenterNet (Zhou et al. 2019a). In particular, the detection and reID tasks are treated equally in FairMOT which essentially differs from the previous “detection first, reID secondary” framework. It is worth noting that it is not a naive combination of CenterNet and reID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies.

Figure 1 shows an overview of FairMOT. It has a simple network structure which consists of two homogeneous branches for detecting objects and extracting reID features, respectively. Inspired by Zhou et al. (2019a, 2019b), Law and Deng (2018), Duan et al. (2019), the detection branch is implemented in an anchor-free style which estimates object centers and sizes represented as position-aware measurement maps. Similarly, the reID branch estimates a reID feature for each pixel to characterize the object centered at the pixel. Note that the two branches are completely homogeneous which essentially differs from the previous methods which perform detection and reID in a two-stage cascaded style. So FairMOT eliminates the unfair disadvantage of the detection branch as reflected in Table 1, effectively learns high-quality reID features and obtains a good trade-off between detection and reID.

We evaluate FairMOT on the MOT Challenge benchmark via the evaluation server. It ranks first among all trackers on the 2DMOT15 (Leal-Taixé et al. 2015), MOT16 (Milan et al. 2016), MOT17 (Milan et al. 2016) and MOT20 (Dendorfer et al. 2020) datasets. When we further pre-train our model using our proposed single image training method, it achieves additional gains on all datasets. In spite of the strong results, the approach is very simple and runs at 30 FPS on a single RTX 2080Ti GPU. It sheds light on the relationship between detection and reID in MOT and provides guidance for designing one-shot video tracking networks.

Our contributions are as follows:

We empirically demonstrate that the prevalent anchor-based one-shot MOT architectures have limitations in terms of learning effective reID features which has been overlooked. The issues severely limit the tracking performance of those methods.

We present FairMOT to address the fairness issue. FairMOT is built on top of CenterNet. Although the adopted techniques are mostly not novel by themselves, we have new discoveries which are important to MOT. These are both novel and valuable.

We show that the achieved fairness allows our FairMOT to obtain high levels of detection and tracking accuracy and outperform the previous state-of-the-art methods by a large margin on multiple datasets such as 2DMOT15, MOT16, MOT17 and MOT20.

Related Work
The best-performing MOT methods (Bergmann et al. 2019; Brasó and Leal-Taixé 2020; Hornakova et al. 2020; Yu et al. 2016; Mahmoudi et al. 2019; Zhou et al. 2018; Wojke et al. 2017; Chen et al. 2018a; Wang et al. 2020b; Voigtlaender et al. 2019) usually follow the tracking-by-detection paradigm, which first detect objects in each frame and then associate them over time. We classify the existing works into two categories based on whether they use a single model or separate models to detect objects and extract association features. We discuss the pros and cons of the methods and compare them to our approach.

Detection and Tracking by Separate Models
Detection Methods
Most benchmark datasets such as MOT17 (Milan et al. 2016) provide detection results obtained by popular methods such as DPM (Felzenszwalb et al. 2008), Faster R-CNN (Ren et al. 2015) and SDP (Yang et al. 2016) such that the works that focus on the tracking part can be fairly compared on the same object detections. Some works such as Yu et al. (2016), Wojke et al. (2017), Zhou et al. (2018), Mahmoudi et al. (2019) use a large private pedestrian detection dataset to train the Faster R-CNN detector with VGG-16 (Simonyan and Zisserman 2014) as backbone, which obtain better detection performance. A small number of works such as Han et al. (2020) use more powerful detectors which are developed recently such as Cascade R-CNN (Cai and Vasconcelos 2018) to boost the detection performance.

Tracking Methods
Most of the existing works focus on the tracking part of the problem. We classify them into two classes according to the type of cues used for association.

Location and Motion Cues based Methods SORT (Bewley et al. 2016) first uses Kalman Filter (Kalman 1960) to predict future locations of the tracklets, computes their overlap with the detections, and uses Hungarian algorithm (Kuhn 1955) to assign detections to tracklets. IOU-Tracker (Bochinski et al. 2017) directly computes the overlap between the tracklets (of the previous frame) and the detections without using using Kalman filter to predict future locations. The approach achieves 100K fps inference speed (detection time not counted) and works well when object motion is small. Both SORT and IOU-Tracker are widely used in practice due to their simplicity.

However, they may fail in challenging cases of crowded scenes and fast motion. Some works such as Xiang et al. (2015), Zhu et al. (2018), Chu and Ling (2019), Chu et al. (2019) leverage sophisticated single object tracking methods to get accurate object locations and reduce false negatives. However, these methods are extremely slow especially when there are a large number of people in the scene. To solve the problem of trajectory fragments, Zhang et al. (2020) propose a motion evaluation network to learn long-range features of tracklets for association. MAT (Han et al. 2020) is an enhanced SORT, which additionally models the camera motion and uses dynamic windows for long-range re-association.

Appearance Cues based Methods Some recent works (Yu et al. 2016; Mahmoudi et al. 2019; Zhou et al. 2018; Wojke et al. 2017) propose to crop the image regions of the detections and feed them to reID networks (Zheng et al. 2017b; Hermans et al. 2017; Luo et al. 2019a) to extract image features. Then they compute the similarity between tracklets and detections based on reID features and use Hungarian algorithm (Kuhn 1955) to accomplish assignment. The method is robust to fast motion and occlusion. In particular, it can re-initialize lost tracks because appearance features are relatively stable over time.

There are also some works (Bae and Yoon 2014; Tang et al. 2017; Sadeghian et al. 2017; Chen et al. 2018a; Xu et al. 2019) focusing on enhancing appearance features. For example, Bae and Yoon (2014) propose an online appearance learning method to handle appearance variations. Tang et al. (2017) leverage body pose features to enhance the appearance features. Some methods (Sadeghian et al. 2017; Xu et al. 2019; Shan et al. 2020) propose to fuse multiple cues (i.e. motion, appearance and location) to get more reliable similarity. MOTDT (Chen et al. 2018a) proposes a hierarchical data association strategy which uses IoU to associate objects when appearance features are not reliable. A small number of works such as Mahmoudi et al. (2019), Zhou et al. (2018), Fang et al. (2018) also propose to use more complicated association strategies such as group models and RNNs.

Offline Methods The offline methods (or batch methods) (Zhang et al. 2008; Wen et al. 2014; Berclaz et al. 2011; Zamir et al. 2012; Milan et al. 2013; Choi 2015; Brasó and Leal-Taixé 2020; Hornakova et al. 2020) often achieve better results by performing global optimization in the whole sequence. For example, Zhang et al. (2008) build a graphical model with nodes representing detections in all frames. The optimal assignment is searched using a min-cost flow algorithm, which exploits the specific structure of the graph to reach the optimum faster than Linear Programming. Berclaz et al. (2011) also treat data association as a flow optimization task and use the K-shortest paths algorithm to solve it, which significantly speeds up computation and reduces parameters that need to be tuned. Milan et al. (2013) formulate multi-object tracking as minimization of a continuous energy and focus on designing the energy function. The energy depends on locations and motion of all targets in all frames as well as physical constraints. MPNTrack (Brasó and Leal-Taixé 2020) proposes trainable graph neural networks to perform a global association of the entire set of detections and make MOT fully differentiable. Lif_T (Hornakova et al. 2020) formulates MOT as a lifted disjoint path problem and introduces lifted edges for long range temporal interactions, which significantly reduces id switches and reIDentify lost.

Advantages and Limitations For the methods which perform detection and tracking by separate models, the main advantage is that they can develop the most suitable model for each task separately without making compromise. In addition, they can crop the image patches according to the detected bounding boxes and resize them to the same size before estimating reID features. This helps to handle the scale variations of objects. As a result, these approaches (Yu et al. 2016; Henschel et al. 2019) have achieved the best performance on the public datasets. However, they are usually very slow because the two tasks need to be done separately without sharing. So it is hard to achieve video rate inference which is required in many applications.

Detection and Tracking by a Single Model
With the quick maturity of multi-task learning (Kokkinos 2017; Ranjan et al. 2017; Sener and Koltun 2018) in deep learning, joint detection and tracking using a single network has begun to attract more research attention. We classify them into two classes as discussed in the following.

Joint Detection and reID The first class of methods (Voigtlaender et al. 2019; Wang et al. 2020b; Liang et al. 2020; Pang et al. 2021; Lu et al. 2020) perform object detection and reID feature extraction in a single network in order to reduce inference time. For example, Track-RCNN (Voigtlaender et al. 2019) adds a reID head on top of Mask R-CNN (He et al. 2017) and regresses a bounding box and a reID feature for each proposal. Similarly, JDE (Wang et al. 2020b) is built on top of YOLOv3 (Redmon and Farhadi 2018) which achieves near video rate inference. However, the accuracy of these one-shot trackers is usually lower than that of the two-step ones.

Joint Detection and Motion Prediction The second class of methods (Feichtenhofer et al. 2017; Zhou et al. 2020; Pang et al. 2020; Peng et al. 2020) learn detection and motion features in a single network. D&T (Feichtenhofer et al. 2017) propose a Siamese network which takes input of adjacent frames and predicts inter-frame displacements between bounding boxes. Tracktor (Bergmann et al. 2019) directly exploits the bounding box regression head to propagate identities of region proposals and thus removes box association. Chained-Tracker (Peng et al. 2020) proposes an end-to-end model using adjacent frame pair as input and generating the box pair representing the same target. These box-based methods assume that bounding boxes have a large overlap between frames, which is not true in low-frame rate videos. Different from these methods, CenterTrack (Zhou et al. 2020) predicts the object center displacements with pair-wise inputs and associate by these point distances. It also provides the tracklets as an additional point-based heatmap input to the network and is then able to match objects anywhere even if the boxes have no overlap at all. However, these methods only associate objects in adjacent frames without re-initializing lost tracks and thus have difficulty handling occlusion cases.

Our work belongs to the first class. We investigate the reasons why one-shot trackers get degraded association performance and propose a simple approach to address the problems. We show that the tracking accuracy is improved significantly without heavy engineering efforts. A concurrent work CSTrack (Liang et al. 2020) also aims to alleviate the conflicts between the two tasks from the perspective of features, and propose a cross-correlation network module to enable the model to learn task-dependent representations. Different from CSTrack, our method tries to address the problem from three perspectives in a systematic way and obtains notably better performances than CSTrack. CenterTrack (Zhou et al. 2020) is also related to our work since it also uses center-based object detection framework. But CenterTrack does not extract appearance features and only links objects in adjacent frames. In contrast, FairMOT can perform long-range association with the appearance features and handle occlusion cases.

Multi-task Learning There is a large body of literature (Liu et al. 2019; Kendall et al. 2018; Chen et al. 2018b; Guo et al. 2018; Sener and Koltun 2018) on multi-task learning which may be used to balance the object detection and reID feature extraction tasks. Uncertainty (Kendall et al. 2018) uses task-dependent uncertainty to automatically balance the single-task losses. MGDA is proposed in Sener and Koltun (2018) to update the shared network weights by finding a common direction among the task-specific gradients. GradNorm (Chen et al. 2018b) controls the training of multi-task networks by simulating the task-specific gradients to be of similar magnitude. We evaluate these methods in the experimental sections.

Video Object Detection
Video Object Detection (VOD) (Feichtenhofer et al. 2017; Luo et al. 2019b) is related to MOT in the sense that it leverages tracking to improve object detection performances in challenging frames. Although these methods were not evaluated on MOT datasets, some of the ideas may be valuable for the field. So we briefly review them in this section. Tang et al. (2019) detect object tubes in videos which aims to enhance classification scores in challenging frames based on their neighboring frames. The detection rate for small objects increases by a large margin on the benchmark dataset. Similar ideas have also been explored in Han et al. (2016), Kang et al. (2016, 2017), Tang et al. (2019), Pang et al. (2020). One main limitation of these tube-based methods is that they are extremely slow especially when there are a large number of objects in videos.

Fig. 2
figure 2
a Track R-CNN treats detection as the primary task and reID as the secondary one. Both Track R-CNN and JDE are anchor-based. The red boxes represent positive anchors and the green boxes represent the target objects. The three methods extract reID features differently. Track R-CNN extracts reID features for all positive anchors using ROI-Align. JDE extracts reID features at the centers of all positive anchors. FairMOT extracts reID features at the object center. b The red anchor contains two different instances. So it will be forced to predict two conflicting classes. c Three different anchors with different image patches are response for predicting the same identity. d FairMOT extracts reID features only at the object center and can mitigate the problems in b, c

Full size image
Unfairness Issues in One-Shot Trackers
In this section, we discuss three unfairness issues that arise in the existing one-shot trackers which usually lead to degraded tracking performances.

Unfairness Caused by Anchors
The existing one-shot trackers such as Track R-CNN (Voigtlaender et al. 2019) and JDE (Wang et al. 2020b) are mostly anchor-based since they are directly modified from anchor-based object detectors such as YOLO (Redmon and Farhadi 2018) and Mask R-CNN (He et al. 2017). However, we find that the anchor-based design is not suitable for learning reID features which result in a large number of ID switches in spite of the good detection results. We explain the problem from three perspectives in the following.

Overlooked reID task Track R-CNN (Voigtlaender et al. 2019) operates in a cascaded style which first estimates object proposals (boxes) and then pools features from them to estimate the corresponding reID features. The quality of reID features heavily depends on the quality of proposals during training (reID features are useless if proposals are not accurate). As a result, in the training stage, the model is seriously biased to estimate accurate object proposals rather than high quality reID features. So the standard “detection first, reID secondary” design of the existing one-shot trackers makes the reID network not fairly learned.

One anchor corresponds to multiple identities The anchor-based methods usually use ROI-Align to extract features from proposals. Most sampling locations in ROI-Align may belong to other disturbing instances or background as shown in Fig. 2. As a result, the extracted features are not optimal in terms of accurately and discriminatively representing the target objects. Instead, we find in this work that it is significantly better to only extract features at a single point, i.e. the estimated object centers.

Multiple anchors correspond to one identity In both Voigtlaender et al. (2019) and Wang et al. (2020b), multiple adjacent anchors, which correspond to different image patches, may be forced to estimate the same identity as long as their IOU is sufficiently large. This introduces severe ambiguity for training. See Fig. 2 for illustration. On the other hand, when an image undergoes small perturbation, e.g. due to data augmentation, it is possible that the same anchor is forced to estimate different identities. In addition, feature maps in object detection are usually down-sampled by 8/16/32 times to balance accuracy and speed. This is acceptable for object detection but it is too coarse for learning reID features because features extracted at coarse anchors may not be aligned with object centers.

Unfairness Caused by Features
For one-shot trackers, most features are shared between the object detection and reID tasks. But it is well known that they actually require features from different layers to achieve the best results. In particular, object detection requires deep features to estimate object classes and positions but reID requires low-level appearance features to distinguish different instances of the same class. From the perspective of the multi-task loss optimization, the optimization objectives of detection and reID have conflicts. Thus, it is important to balance the loss optimization strategy of the two tasks.

Unfairness Caused by Feature Dimension
The previous reID works usually learn very high dimensional features and have achieved promising results on the benchmarks of their field. However, we find that learning lower-dimensional features is actually better for one-shot MOT for three reasons: (1) high-dimensional reID features notably harms the object detection accuracy due to the competition of the two tasks which in turn also has negative impact to the final tracking accuracy. So considering that the feature dimension in object detection is usually very low (class numbers + box locations), we propose to learn low-dimensional reID features to balance the two tasks; (2) the MOT task is different from the reID task. The MOT task only performs a small number of one-to-one matchings between two consecutive frames. The reID task needs to match the query to a large number of candidates and thus requires more discriminative and high-dimensional reID features. So in MOT we do not need that high-dimensional features; (3) learning low dimensional reID features improves the inference speed as will be shown in our experiments.

FairMOT
In this section, we present the technical details of FairMOT including the backbone network, the object detection branch, the reID branch as well as training details.

Backbone Network
We adopt ResNet-34 as backbone in order to strike a good balance between accuracy and speed. An enhanced version of Deep Layer Aggregation (DLA) (Zhou et al. 2019a) is applied to the backbone to fuse multi-layer features as shown in Fig. 1. Different from original DLA (Yu et al. 2018), it has more skip connections between low-level and high-level features which is similar to the Feature Pyramid Network (FPN) (Lin et al. 2017a). In addition, convolution layers in all up-sampling modules are replaced by deformable convolution such that they can dynamically adjust the receptive field according to object scales and poses. These modifications are also helpful to alleviate the alignment issue. The resulting model is named DLA-34. Denote the size of input image as 𝐻image×𝑊image, then the output feature map has the shape of 𝐶×𝐻×𝑊 where 𝐻=𝐻image/4 and 𝑊=𝑊image/4. Besides DLA, other deep networks that provide multi-scale convolutional features, such as Higher HRNet (Cheng et al. 2020), can be used in our framework to provide fair features for both detection and reID.

Detection Branch
Our detection branch is built on top of CenterNet (Zhou et al. 2019a) but other anchor-free methods such as Duan et al. (2019), Law and Deng (2018), Dong et al. (2020), Yang et al. (2019) can also be used. We briefly describe the approach to make this work self-contained. In particular, three parallel heads are appended to DLA-34 to estimate heatmaps, object center offsets and bounding box sizes, respectively. Each head is implemented by applying a 3×3 convolution (with 256 channels) to the output features of DLA-34, followed by a 1×1 convolutional layer which generates the final targets.

Heatmap Head
This head is responsible for estimating the locations of the object centers. The heatmap based representation, which is the de facto standard for the landmark point estimation task, is adopted here. In particular, the dimension of the heatmap is 1×𝐻×𝑊. The response at a location in the heatmap is expected to be one if it collapses with the ground-truth object center. The response decays exponentially as the distance between the heatmap location and the object center.

For each GT box 𝐛𝑖=(𝑥𝑖1,𝑦𝑖1,𝑥𝑖2,𝑦𝑖2) in the image, we compute the object center (𝑐𝑖𝑥,𝑐𝑖𝑦) as 𝑐𝑖𝑥=𝑥𝑖1+𝑥𝑖22 and 𝑐𝑖𝑦=𝑦𝑖1+𝑦𝑖22, respectively. Then its location on the feature map is obtained by dividing the stride (𝑐˜𝑖𝑥,𝑐˜𝑖𝑦)=(⌊𝑐𝑖𝑥4⌋,⌊𝑐𝑖𝑦4⌋). Then the heatmap response at the location (x, y) is computed as 𝑀𝑥𝑦=∑𝑁𝑖=1exp−(𝑥−𝑐˜𝑖𝑥)2+(𝑦−𝑐˜𝑖𝑦)22𝜎2𝑐 where N represents the number of objects in the image and 𝜎𝑐 represents the standard deviation. The loss function is defined as pixel-wise logistic regression with focal loss (Lin et al. 2017b):

𝐿heat=−1𝑁∑𝑥𝑦{(1−𝑀̂ 𝑥𝑦)𝛼log(𝑀̂ 𝑥𝑦),(1−𝑀𝑥𝑦)𝛽(𝑀̂ 𝑥𝑦)𝛼log(1−𝑀̂ 𝑥𝑦)𝑀𝑥𝑦=1;otherwise,
(1)
where 𝑀̂  is the estimated heatmap, and 𝛼,𝛽 are the pre-determined parameters in focal loss.

Box Offset and Size Heads
The box offset head aims to localize objects more precisely. Since the stride of the final feature map is four, it will introduce quantization errors up to four pixels. This branch estimates a continuous offset relative to the object center for each pixel in order to mitigate the impact of down-sampling. The box size head is responsible for estimating height and width of the target box at each location.

Denote the output of the size and offset heads as 𝑆̂ ∈ℝ2×𝐻×𝑊 and 𝑂̂ ∈ℝ2×𝐻×𝑊, respectively. For each GT box 𝐛𝑖=(𝑥𝑖1,𝑦𝑖1,𝑥𝑖2,𝑦𝑖2) in the image, we compute its size as 𝐬𝑖=(𝑥𝑖2−𝑥𝑖1,𝑦𝑖2−𝑦𝑖1). Similarly, the GT offset is computed as 𝐨𝑖=(𝑐𝑖𝑥4,𝑐𝑖𝑦4)−(⌊𝑐𝑖𝑥4⌋,⌊𝑐𝑖𝑦4⌋). Denote the estimated size and offset at the corresponding location as 𝑠̂ 𝑠̂ 𝑖 and 𝑜̂ 𝑜̂ 𝑖, respectively. Then we enforce 𝑙1 losses for the two heads:

𝐿box=∑𝑖=1𝑁‖𝐨𝑖−𝑜̂ 𝑜̂ 𝑖‖1+𝜆𝑠‖𝐬𝑖−𝑠̂ 𝑠̂ 𝑖‖1.
(2)
where 𝜆𝑠 is a weighting parameter and is set 0.1 as the original CenterNet (Zhou et al. 2019a).

reID Branch
reID branch aims to generate features that can distinguish objects. Ideally, affinity among different objects should be smaller than that between same objects. To achieve this goal, we apply a convolution layer with 128 kernels on top of backbone features to extract reID features for each location. Denote the resulting feature map as 𝐄∈ℝ128×𝐻×𝑊. The reID feature 𝐄𝑥,𝑦∈ℝ128 of an object centered at (x, y) can be extracted from the feature map.

reID Loss
We learn reID features through a classification task. All object instances of the same identity in the training set are treated as the same class. For each GT box 𝐛𝑖=(𝑥𝑖1,𝑦𝑖1,𝑥𝑖2,𝑦𝑖2) in the image, we obtain the object center on the heatmap (𝑐˜𝑖𝑥,𝑐˜𝑖𝑦). We extract the reID feature vector 𝐄𝑐˜𝑖𝑥,𝑐˜𝑖𝑦 and use a fully connected layer and a softmax operation to map it to a class distribution vector 𝐏={𝐩(𝑘),𝑘∈[1,𝐾]}. Denote the one-hot representation of the GT class label as 𝐋𝑖(𝑘). Then we compute the reID loss as:

𝐿identity=−∑𝑖=1𝑁∑𝑘=1𝐾𝐋𝑖(𝑘)log(𝐩(𝑘)),
(3)
where K is the number of all the identities in the training data. During the training process of our network, only the identity embedding vectors located at object centers are used for training, since we can obtain object centers from the objectness heatmap in testing.

Training FairMOT
We jointly train the detection and reID branches by adding the losses (i.e., Eqs. (1), (2) and (3)) together. In particular, we use the uncertainty loss proposed in Kendall et al. (2018) to automatically balance the detection and reID tasks:

𝐿detection=𝐿heat+𝐿box,
(4)
𝐿total=12(1𝑒𝑤1𝐿detection+1𝑒𝑤2𝐿identity+𝑤1+𝑤2),
(5)
where 𝑤1 and 𝑤2 are learnable parameters that balance the two tasks. Specifically, given an image with a few objects and their corresponding IDs, we generate heatmaps, box offset and size maps as well as one-hot class representation of the objects. These are compared to the estimated measures to obtain losses to train the whole network.

In addition to the standard training strategy presented above, we propose a single image training method to train FairMOT on image-level object detection datasets such as COCO (Lin et al. 2014) and CrowdHuman (Shao et al. 2018). Different from CenterTrack (Zhou et al. 2020) that takes two simulated consecutive frames as input, we only take a single image as input. We assign each bounding box a unique identity and thus regard each object instance in the dataset as a separate class. We apply different transformations to the whole image including HSV augmentation, rotation, scaling, translation and shearing. The single image training method has significant empirical values. First, the pre-trained model on the CrowdHuman dataset can be directly used as a tracker and get acceptable results on MOT datasets such as MOT17 (Milan et al. 2016). This is because the CrowdHuman dataset can boost the human detection performance and also has strong domain generalization ability. Our training of the reID features further enhances the association ability of the tracker. Second, we can finetune it on other MOT datasets and further improve the final performance.

Online Inference
In this section, we present how we perform online inference, and in particular, how we perform association with the detections and reID features.

Network Inference
The network takes a frame of size 1088×608 as input which is the same as the previous work JDE (Wang et al. 2020b). On top of the predicted heatmap, we perform non-maximum suppression (NMS) based on the heatmap scores to extract the peak keypoints. The NMS is implemented by a simple 3×3 max pooling operation as in Zhou et al. (2019a). We keep the locations of the keypoints whose heatmap scores are larger than a threshold. Then, we compute the corresponding bounding boxes based on the estimated offsets and box sizes. We also extract the identity embeddings at the estimated object centers. In the next section, we discuss how we associate the detected boxes over time using the reID features.

Online Association
We follow MOTDT (Chen et al. 2018a) and use a hierarchical online data association method. We first initialize a number of tracklets based on the detected boxes in the first frame. Then in the subsequent frame, we link the detected boxes to the existing tracklets using a two-stage matching strategy. In the first stage, we use Kalman Filter (Kalman 1960) and reID features to obtain initial tracking results. In particular, we use Kalman Filter to predict tracklet locations in the following frame and compute the Mahalanobis distance 𝐷𝑚 between the predicted and detected boxes following DeepSORT (Wojke et al. 2017). We fuse the Mahalanobis distance with the cosine distance computed on reID features: 𝐷=𝜆𝐷𝑟+(1−𝜆)𝐷𝑚 where 𝜆 is a weighting parameter and is set to be 0.98 in our experiments. Following JDE (Wang et al. 2020b), we set Mahalanobis distance to infinity if it is larger than a threshold to avoid getting trajectories with large motion. We use Hungarian algorithm (Kuhn 1955) with a matching threshold 𝜏1=0.4 to complete the first stage matching.

In the second stage, for unmatched detections and tracklets, we try to match them according to the overlap between their boxes. In particular, we set the matching threshold 𝜏2=0.5. We update the appearance features of the tracklets in each time step to handle appearance variations as in Bolme et al. (2010), Henriques et al. (2014). Finally, we initialize the unmatched detections as new tracks and save the unmatched tracklets for 30 frames in case they reappear in the future.

Experiments
Datasets and Metrics
There are six training datasets briefly introduced as follows: the ETH (Ess et al. 2008) and CityPerson (Zhang et al. 2017) datasets only provide box annotations so we only train the detection branch on them. The CalTech (Dollár et al. 2009), MOT17 (Milan et al. 2016), CUHK-SYSU (Xiao et al. 2017) and PRW (Zheng et al. 2017a) datasets provide both box and identity annotations which allows us to train both branches. Some videos in ETH also appear in the testing set of the MOT17 which are removed from the training dataset for fair comparison. The overall training strategy is described in Sect. 4.4, which is the same as Wang et al. (2020b). For the self-supervised training of our method, we use the CrowdHuman dataset (Shao et al. 2018) which only contains object bounding box annotations.

We evaluate our approach on the testing sets of four benchmarks: 2DMOT15, MOT16, MOT17 and MOT20. We use Average Precision (AP) to evaluate detection results. Following Wang et al. (2020b), we use True Positive Rate (TPR) at a false accept rate of 0.1 for evaluating reID features. In particular, we extract reID features which correspond to ground truth boxes and use each feature to retrieve N most similar candidates. We report the true positive rate at false accept rate 0.1 (TPR@FAR=0.1). Note that TPR is not affected by detection results and faithfully reflects the quality of reID features. We use the CLEAR metric (Bernardin and Stiefelhagen 2008) (i.e. MOTA, IDs) and IDF1 (Ristani et al. 2016) to evaluate overall tracking accuracy.

Implementation Details
We use a variant of DLA-34 proposed in Zhou et al. (2019a) as our default backbone. The model parameters pre-trained on the COCO dataset (Lin et al. 2014) are used to initialize our model. We train our model with the Adam optimizer (Kingma and Ba 2014) for 30 epochs with a starting learning rate of 10−4. The learning rate decays to 10−5 at 20 epochs. The batch size is set to be 12. We use standard data augmentation techniques including rotation, scaling and color jittering. The input image is resized to 1088×608 and the feature map resolution is 272×152. The training step takes about 30 hours on two RTX 2080 Ti GPUs.

Ablative Studies
In this section, we present rigorous studies of the three critical factors in FairMOT including anchor-less reID feature extraction, feature fusion and feature dimensions by carefully designing a number of baseline methods.

Anchors
We evaluate four strategies for sampling reID features from the detected boxes which are frequently used by previous works (Wang et al. 2020b; Voigtlaender et al. 2019). The first strategy is ROI-Align used in Track R-CNN (Voigtlaender et al. 2019). It samples features from the detected proposals using ROI-Align. As discussed previously, many sampling locations deviate from object centers. The second strategy is POS-Anchor used in JDE (Wang et al. 2020b). It samples features from positive anchors which may also deviate from object centers. The third strategy is “Center” used in FairMOT. It only samples features at object centers. Recall that, in our approach, reID features are extracted from discretized low-resolution maps. In order to sample features at accurate object locations, we also try to apply Bi-linear Interpolation (Center-BI) to extract more accurate features.

We also evaluate a two-stage approach to first detect object bounding boxes and then extract reID features. In the first stage, the detection part is the same as our FairMOT. In the second stage, we use ROI-Align (He et al. 2017) to extract the backbone features based on the detected bounding boxes and then use a reID head (a fully connected layer) to get reID features. The main difference between the two-stage approach and the one-stage “ROI-Align” approach is that the reID features of the two-stage approach rely on the detection results while those of the one-stage approach do not during training.

The results are shown in Table 1. Note that the five approaches are all built on top of FairMOT. The only difference lies in how they sample reID features from detected boxes. First, we can see that our approach (Center) obtains notably higher IDF1 score and True Positive Rate (TPR) than ROI-Align, POS-Anchor and the two-stage approach. This metric is independent of object detection results and faithfully reflects the quality of reID features. In addition, the number of ID switches (IDs) of our approach is also significantly smaller than the two baselines. The results validate that sampling features at object centers is more effective than the strategies used in the previous works. Bi-linear Interpolation (Center-BI) achieves even higher TPR than Center because it samples features at more accurate locations. The two-stage approach harms the quality of the reID features.

Balancing Multi-task Losses
We evaluate different methods for balancing the losses of different tasks including Uncertainty (Kendall et al. 2018), GradNorm (Chen et al. 2018b) and MGDA-UB (Sener and Koltun 2018). We also evaluate a baseline with fixed weights obtained by grid search. We implement two versions for the uncertainty-based method. The first is “Uncertainty-task” which learns two parameters for the detection loss and reID loss, respectively. The second is “Uncertainty-branch” which learns four parameters for the heatmap loss, box size loss, offset loss and reID losses, respectively.

Table 2 Comparison of different loss weighting strategies on the validation set of the MOT17 dataset
Full size table
Table 3 Comparison of different backbones on the validation set of MOT17 dataset
Full size table
The results are shown in Table 2. We can see that the “Fixed” method gets the best MOTA and AP but the worst IDs and TPR. It means that the model is biased to the detection task. MGDA-UB has the highest TPR but the lowest MOTA and AP, which indicates that the model is biased to the reID task. Similar results can be found in Wang et al. (2020b), Vandenhende et al. (2021). GradNorm gets the best overall tracking accuracy (highest IDF1 and second highest MOTA), meaning that ensuring different tasks to have similar gradient magnitude is helpful to handle feature conflicts. However, GradNorm takes longer training time. So we use the simpler Uncertainty method which is slightly worse than GradNorm in the rest of our experiments.

Multi-layer Feature Fusion
We compare a number of backbones such as vanilla ResNet (He et al. 2016), Feature Pyramid Network (FPN) (Lin et al. 2017a), High-Resolution Network (HRNet) (Wang et al. 2020a), DLA (Zhou et al. 2019a), HarDNet (Chao et al. 2019) and RegNet (Radosavovic et al. 2020). Note that the rest of the factors of these approaches such as training datasets are all controlled to be the same for fair comparison. In particular, the stride of the final feature map is four for all methods. We add three up-sampling operations for vanilla ResNet and RegNet to obtain feature maps of stride four. We divide these backbones into two categories, one without multi-layer fusion (i.e. ResNet and RegNet) and one with (i.e. FPN, HRNet, DLA and HarDNet).

The results are shown in Table 3. We also list the ImageNet (Russakovsky et al. 2015) classification accuracy Acc in order to demonstrate that a strong backbone in one task does not mean it will also get good results in MOT. So detailed studies for MOT are necessary and useful.

By comparing the results of ResNet-34 and ResNet-50, we find that blindly using a larger network does not notably improve the overall tracking result measured by MOTA. In particular, the quality of reID features barely benefits from the larger network. For example, IDF1 only improves from 67.2 to 67.7% and TPR improves from 90.9 to 91.9%, respectively. In addition, the number of ID switches even increases from 435 to 501. By comparing ResNet-50 and RegNetY-4.0GF, we can find that using a even more powerful backbone also achieves very limited gain. The reID metric TPR of RegNetY-4.0GF is the same as ResNet-50 (91.9) while the ImageNet classification accuracy improves a lot (79.4 vs. 77.8). All these results suggest that directly using a larger or a more powerful network cannot always improve the final tracking accuracy.

In contrast, ResNet-34-FPN, which actually has fewer parameters than ResNet-50, achieves a larger MOTA score than ResNet-50. More importantly, TPR improves significantly from 90.9 to 94.2%. By comparing RegNetY-4.0GF-FPN with RegNetY-4.0GF, we can see that adding multi-layer feature fusion structure (Lin et al. 2017a) to RegNet brings considerable gains (+1.9 MOTA, +1.3 IDF1, -36.9% IDs, +2.2 AP, +2.3 TPR), which suggests that multi-layer feature fusion has clear advantages over simply using larger or more powerful networks.

In addition, DLA-34, which is also built on top of ResNet-34 but has more levels of feature fusion, achieves an even larger MOTA score. In particular, TPR increases significantly from 90.9 to 94.4% which in turn decreases the number of ID switches (IDs) from 435 to 299. Similar conclusions can be obtained from the results of HRNet-W18. The results validate that feature fusion (FPN, DLA and HRNet) effectively improves the discriminative ability of reID features. On the other hand, although ResNet-34-FPN obtains equally good reID features (TPR) as DLA-34, its detection results (AP) are significantly worse than DLA-34. We think the use of deformable convolution in DLA-34 is the main reason because it enables more flexible receptive fields for objects of different sizes - it is very important for our method since FairMOT only extracts features from object centers without using any region features. We can only get 65.0 MOTA and 78.1 AP when replacing all the deformable convolutions with normal convolutions in DLA-34. As shown in Table 5, we can see that DLA-34 mainly outperforms HRNet-W18 on middle and large size objects. When we further use a more powerful backbone HarDNet-85 with more multi-layer feature fusion structures, we achieve even better results than DLA-34 (+2.1 MOTA, +1.7 IDF1, -33.8% IDs, +1.4 AP, +1.4 TPR). Although HRNet-W18, DLA-34 and HarDNet-85 get lower ImageNet classification accuracy than ResNet-50 and RegNetY-4.0GF, they achieve much higher tracking accuracy. Based on the experimental results above, we believe that multi-layer feature fusion is the key to solve the “feature” issue.

Table 4 Demonstration of feature conflict between the detection and reID tasks on the validation set of the MOT17 dataset
Full size table
Table 5 The impact of different backbones on objects of different scales
Full size table
Table 6 Evaluation of reID feature dimensions of JDE and FairMOT on the validation set of MOT17
Full size table
Table 7 Evaluation of the three ingredients in the data association model
Full size table
To validate the existence of feature conflict between the detection and reID tasks, we introduce a baseline ResNet-34-det which only trains the detection branch (reID branch is randomly initialized). We can see from Table 4 that the detection result measured by AP improves by 1 point if we do not train the reID branch which shows the conflict between the two tasks. In particular, ResNet-34-det even gets higher MOTA score than ResNet-34 because the metric favors better detection than tracking results. In contrast, DLA-34, which adds multi-layer feature fusion over ResNet-34, achieves better detection as well as tracking results. It means multi-layer feature fusion helps alleviate the feature conflict problem by allowing each task to extract whatever it needs for its own task from the fused features.

Feature Dimension
The previous one-shot trackers such as JDE (Wang et al. 2020b) usually learns 512 dimensional reID features following the two-step methods without ablation study. However, we find in our experiments that the feature dimension actually plays an important role in balancing detection and tracking accuracy. Learning lower dimensional reID features causes less harm to the detection accuracy and improves the inference speed. We conduct experiments on different one-shot trackers and find it is a generic rule that low-dimensional (i.e. 64) reID features achieves better performance than high-dimensional (i.e. 512) reID features.

We evaluate multiple choices for reID feature dimension of JDE and FairMOT in Table 6. For JDE, we can see that 64 achieves better performance than 512 on all the metrics. For FairMOT, we can see that 512 achieves higher IDF1 and TPR scores which indicates that higher dimensional reID features lead to stronger discriminative ability. However, the MOTA score improves when we decrease the dimension from 512 to 64. This is mainly caused by the conflict between the detection and reID tasks. In particular, we can see that the detection result (AP) improves when we decrease the dimension of reID features. Different from the reID task, low-dimensional reID features achieves better performance and efficiency on the MOT task.

Data Association Methods
This section evaluates the three ingredients in the data association step including bounding box IoU, reID features and Kalman Filter (Kalman 1960). These are used to compute the similarity between each pair of detected boxes. With that we use Hungarian algorithm (Kuhn 1955) to solve the assignment problem. Table 7 shows the results. We can see that only using box IoU causes a lot of ID switches. This is particularly true for crowded scenes and fast camera motion. Using reID features alone notably increases IDF1 and decreases the number of ID switches. In addition, adding Kalman filter helps obtain smooth (reasonable) tracklets which further decreases the number of ID switches. When an object is partly occluded, its reID features become unreliable. In this case, it is important to leverage box IoU, reID features and Kalman filter to obtain good tracking performance.

We also present a detailed runtime breakdown of different components including detection, reID matching, Kalman Filter and IoU matching. We test runtime on sequences with different density (average number of pedestrians per frame). The results are shown in Fig. 4. The time spent on joint detection and reID is minimally affected by density. The time spent on Kalman Filter and IoU matching are around 1 ms or 2 ms and can be ignored. The time spent on reID matching increases linearly with the increase of density. This is because a large amount of time is cost on updating the reID feature of each tracklet.

Visualization of reID Similarity
We use reID similarity maps to demonstrate the discriminative ability of reID features in Fig. 3. We randomly choose two frames from our validation set. The first frame contains the query instance and the second frame contains the target instance that has the same ID. We obtain the reID similarity maps by computing the cosine similarity between the reID feature of the query instance and the whole reID feature map of the target frame, as described in Sects. 5.3.1 and 5.3.3 respectively. By comparing the similarity maps of ResNet-34 and ResNet-34-det, we can see that training the reID branch is important. By comparing DLA-34 and ResNet-34, we can see that multi-layer feature aggregation can get more discriminative reID features. Among all the sampling strategies, the proposed Center and Center-BI can better discriminate the target object from surrounding objects in crowded scenes.

Single Image Training
We first pre-train FairMOT on the CrowdHuman dataset (Shao et al. 2018). In particular, we assign a unique identity label for each bounding box and train FairMOT using the method described in Sect. 4.4. Then we finetune the pre-trained model on the target dataset MOT17.

Fig. 3
figure 3
Visualization of the discriminative ability of the reID features. Query instances are marked as red boxes and target instances are marked as green boxes. The similarity maps are computed using reID features extracted based on different strategies (e.g., Center, Center-BI, ROI-Align and POS-Anchor as described in Sect. 5.3.1) and different backbones (e.g., ResNet-34 and DLA-34). The query frames and target frames are randomly chosen from the MOT17-09 and the MOT17-02 sequence

Full size image
Fig. 4
figure 4
Time spent on different parts of our whole MOT system. We run tracking on sequences with different density from the MOT17 dataset and the MOT20 dataset

Full size image
Table 8 shows the results. First, the pre-trained model can be directly used as a tracker and get acceptable results on MOT datasets such as MOT17. This is because the CrowdHuman dataset can boost the human detection performance and also has strong domain generalization ability. Our training of the reID features further enhances the association ability of the tracker. Second, pre-training on CrowdHuman outperforms directly training on the MOT17 dataset by a large margin. Third, the single image training model even outperforms the model trained on the “MIX” and MOT17 datasets with identity annotations. The results validate the effectiveness of the proposed single image pre-training, which saves lots of annotation efforts and makes FairMOT more attractive in real applications.

Table 8 Effects of single image training on the validation set of MOT17
Full size table
Table 9 Comparison of the state-of-the-art one-shot trackers on the 2DMOT15 dataset
Full size table
Table 10 Comparison of the state-of-the-art methods under the “private detector” protocol
Full size table
Results on MOTChallenge
We compare our approach to the state-of-the-art (SOTA) methods including both the one-shot methods and the two-step methods.

Comparing with One-Shot SOTA MOT Methods
There are two published works of JDE (Wang et al. 2020b) and TrackRCNN (Voigtlaender et al. 2019) that jointly perform object detection and identity feature embedding. We compare our approach to both of them. Following the previous work (Wang et al. 2020b), the testing dataset contains 6 videos from 2DMOT15. FairMOT uses the same training data as the two methods as described in their papers. In particular, when we compare to JDE, both FairMOT and JDE use the large scale composed dataset described in Sect. 5.1. Since Track R-CNN requires segmentation labels to train the network, it only uses 4 videos of the MOT17 dataset which has segmentation labels as training data. In this case, we also use the 4 videos to train our model. The CLEAR metric (Bernardin and Stiefelhagen 2008) and IDF1 (Ristani et al. 2016) are used to measure their performance.

Table 11 Results of the MOT17 test set when using different datasets for training
Full size table
Fig. 5
figure 5
Example tracking results of our method on the test set of MOT17. Each row shows the results of sampled frames in chronological order of a video sequence. Bounding boxes and identities are marked in the images. Bounding boxes with different colors represent different identities. Best viewed in color

Full size image
The results are shown in Table 9. We can see that our approach remarkably outperforms JDE (Wang et al. 2020b). In particular, the number of ID switches reduces from 218 to 80 which is big improvement in terms of user experience. The results validate the effectiveness of the anchor-free approach over the previous anchor-based one. The inference speed is near video rate for the both methods with ours being faster. Compared with Track R-CNN (Voigtlaender et al. 2019), their detection results are slightly better than ours (with lower FN). However, FairMOT achieves much higher IDF1 score (64.0 vs. 49.4) and fewer ID switches (96 vs. 294). This is mainly because Track R-CNN follows the “detection first, reID secondary” framework and use anchors which also introduce ambiguity to the reID task.

Comparing with Other SOTA MOT Methods
We compare our approach to the state-of-the-art trackers including the two-step methods in Table 10. Since we do not use the public detection results, the “private detector” protocol is adopted. We report results on the testing sets of the 2DMOT15, MOT16, MOT17 and MOT20 datasets, respectively. Note that all of the results are directly obtained from the official MOT challenge evaluation server.

Our approach ranks first among all online and offline trackers on the four datasets. In particular, it outperforms other methods by a large margin. This is a very strong result especially considering that our approach is very simple. In addition, our approach achieves video rate inference. In contrast, most high-performance trackers such as (Fang et al. 2018; Yu et al. 2016) are usually slower than ours. Our approach also ranks second under a very recent local MOT metric ALTA (Valmadre et al. 2021), which further indicates that our approach achieves very high tracking performance (Table 10).

Training Data Ablation Study
We also evaluate the performance of FairMOT using different amount of training data (Table 11). We can achieve 69.8 MOTA when only using the MOT17 dataset for training, which already outperforms other methods using more training data. When we use the same training data as JDE (Wang et al. 2020b), we can achieve 72.9 MOTA, which remarkably outperforms JDE. In addition, when we perform single image training on the CrowdHuman dataset, the MOTA score improves to 73.7. The results suggest that our approach is not data hungry which is a big advantage in practical applications.

Qualitative Results
Figure 5 visualizes several tracking results of FairMOT on the test set of MOT17 (Milan et al. 2016). From the results of MOT17-01, we can see that our method can assign correct identities with the help of high-quality reID features when two pedestrians cross over each other. Trackers using bounding box IOUs (Bewley et al. 2016; Bochinski et al. 2017) usually cause identity switches under these circumstances. From the results of MOT17-03, we can see that our method perform well under crowded scenes. From the results of MOT17-08, we can see that our method can keep both correct identities and correct bounding boxes when the pedestrians are heavily occluded. The results of MOT17-06 and MOT17-12 show that our method can deal with large scale variations. This mainly attributes to the using of multi-layer feature aggregation. Our method can detect small objects accurately as shown in the results of MOT17-07 and MOT17-14.

Summary and Future Work
Start from studying why the previous one-shot methods (Wang et al. 2020b) fail to achieve comparable results as the two-step methods, we find that the use of anchors in object detection and identity embedding is the main reason for the degraded results. In particular, multiple nearby anchors, which correspond to different parts of an object, may be responsible for estimating the same identity which causes ambiguities for network training. Further, we find the feature unfairness issue and feature dimension issue between the detection and reID tasks in previous MOT frameworks. By addressing these problems in an anchor-free single-shot deep network, we propose FairMOT. It outperforms the previous state-of-the-art methods on several benchmark datasets by a large margin in terms of both tracking accuracy and inference speed. Besides, FairMOT is inherently training data-efficient and we propose single image training of multi-object trackers only using bounding box annotated images, which both make our method more appealing in real applications.

